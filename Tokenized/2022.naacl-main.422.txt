  Haozhe Ji , Rongsheng Zhang , Zhenyu Yang , Zhipeng Hu , Minlie Huang   Abstract   Although Transformers with fully connected   self - attentions are powerful to model long - term   dependencies , they are struggling to scale to   long texts with thousands of words in language   modeling . One of the solutions is to equip the   model with a recurrence memory . However , ex-   isting approaches directly reuse hidden states   from the previous segment that encodes con-   texts in a uni - directional way . As a result , this   prohibits the memory to dynamically interact   with the current context that provides up - to-   date information for token prediction . To rem-   edy this issue , we propose Look - Ahead Memory   ( LaMemo)that enhances the recurrence mem-   ory by incrementally attending to the right - side   tokens , and interpolating with the old mem-   ory states to maintain long - term information in   the history . LaMemo embraces bi - directional   attention and segment recurrence with an addi-   tional computation overhead only linearly pro-   portional to the memory length . Experiments   on widely used language modeling benchmarks   demonstrate its superiority over the baselines   equipped with different types of memory .   1 Introduction   Language modeling is an important task that tests   the ability of modeling long - term dependencies by   predicting the current token based on the previous   context ( Mikolov and Zweig , 2012 ; Merity et al . ,   2017 ) . Recently , Transformer - based language mod-   els achieved remarkable performance by enabling   direct interaction between long - distance word pairs .   However , as the computation overhead grows with   the length of the input sequence , Transformers can   only process a fixed length segment at a time . To   allow long - term information flow across individual   segments , existing approaches augment the modelFigure 1 : Attention weights on the context ( in log - scale )   in the final layer of Transformer - XL and LaMemo av-   eraged on 15 K tokens . Transformer - XL quickly loses   attention to older contexts , while LaMemo maintains   awareness to the history with the grow of the context   length .   with a recurrence memory that stores hidden states   computed in previous time steps ( Dai et al . , 2019 )   and their compressions ( Rae et al . , 2020 ; Martins   et al . , 2021 ) for the target tokens to attend to .   One limitation of this approach is that the recur-   rence memory is only aware of older contexts since   they are previously computed to predict the next   word from left to right . As a result , distant memory   states become outdated and less activated by the   current context , as illustrated in Figure 1 . When   humans read or write a document , they maintain a   memory that records important information from   the past and often refresh them under the current   context to keep it up - to - date .   In this paper , we propose Look - Ahead Memory   ( LaMemo ) where memory states “ look ahead ” to   future time steps by attending to the token represen-   tations on their right side to provide up - to - date con-   textualization . To maintain information from the   long - term history , we propose memory interpola-   tionto take both past and future tokens into consid-   eration , which mimics the bi - directional attention .   Note that , directly applying bi - directional attention   to update the memory representations brings an   additional complexity of O(M)(Mis the mem-5747ory length ) . This is expensive when the memory is   very long . LaMemo incrementally attends to the   right and accumulate the weighted attention sum   from previous segments to simulate the full atten-   tion in only O(M×N)complexity ( Nis the tar-   get sequence length ) , which does not increase the   attention complexity of Transformer - XL , namely   O(N+M×N ) . We provide an illustration of   this mechanism in Figure 3 .   Another technique proved to be effective in lan-   guage modeling is the relative positional encod-   ing ( Shaw et al . , 2018 ; Huang et al . , 2018 ; Dai   et al . , 2019 ) , which biases the pair - wise attention   score purely based on the relative distance of the   two tokens . However its ability to generalize to   the attention of the future tokens remains unknown ,   since both the distance and the direction need to   be taken into consideration . In preliminary experi-   ments , we observed the unstability of directly ap-   plying the relative positional encoding of Dai et al .   ( 2019 ) to this setting . We propose a simple yet ef-   fective modification based on Dai et al . ( 2019 ) that   disentangles the bias of the relative distance and the   attention direction which facilitates the training of   LaMemo . We give both theoretical and empirical   analysis to the unstability issue and demonstrate the   effectiveness of the proposed disentangled relative   positional encoding method .   To sum up , our contributions are as follows :   ( 1 ) We propose LaMemo , a memory mechanism   that incrementally attends to the right - side tokens ,   and interpolates with the old memory , which en-   ables bi - directional interaction with a complexity   linear in memory length .   ( 2 ) We propose disentangled relative positional   encoding , a simple yet effective solution that dis-   entangles the relative distance and the attention   direction that can better generalize to the attention   of the future tokens .   ( 3 ) We conduct experiments on standard lan-   guage modeling benchmarks and demonstrate   LaMemo ’s superiority over various baselines equp-   pied with different types of memory mechanisms ,   despite some having an access to longer contexts .   Comprehensive comparisons show the benefits of   learning memory representations contextualized   with up - to - date information.2 Background   2.1 Transformer for Language Modeling   A Transformer ( Vaswani et al . , 2017 ) is composed   of multiple layers of identical blocks , including a   multi - head self - attention ( Bahdanau et al . , 2015 )   that calculates pair - wise token interaction and a   feed - foward layer for position - wise projection with   a non - linear activation . Both two modules are fol-   lowed by residual connections ( He et al . , 2016 ) and   layer normalization ( Ba et al . , 2016 ) to facilitate   optimization .   Given the input sequence representations of the   current τ - th segment X= [ x,···,x]∈   Rwhere Nis the target sequence length and d   is the hidden state size , they are first mapped into   queries Q , keys Kand values Vby learned weight   matrix to compute self - attention :   Q = XW , K = XW , V = XW ,   ( 1 )   where W , W , W∈Rare learnable projec-   tion matrices . To perform multi - head self - attention ,   Q , K , V are further split into Hheads . For sim-   plicity , we only consider the case of a single head .   In language modeling , the attention map is always   added by a causal mask to avoid information leak-   age from the future when predicting the next token :   C = Causal - Attn ( Q , K , V )   = softmax / parenleftigQK√   d / parenrightig   V , ( 2 )   where softmax ( · ) masks position j > i for the   i - th row of the input matrix with −∞ before taking   the softmax . The resulted context representations   are concatenated and then projected to the final   outputs O∈Rwith a learnable projection   matrix W∈R. Finally , the self - attention   outputs Oare added by the input representations   Xand fed to the following point - wise non - linear   transformation , denoted as f ( · ):   f(x ) = LN / parenleftig   FFN / parenleftbig   LN(x)/parenrightbig   + LN(x)/parenrightig   , ( 3 )   where LN(·)is the layer normalization and FFN ( · )   is the feed - forward layer , both of which are applied   to each row vector individually . The final output of   this Transformer layer is f(O+X ) .   Outputs of the final layer are projected to the   vocabulary to predict Pr(w|w , · · · , w ) . The   joint probability of predicting the whole segment5748   is the product of these conditional factors . The   final objective is to maximize the following log-   likelihood :   log Pr ( w ) = /productdisplaylog Pr ( w|w , · · · , w).(4 )   2.2 Recurrence Memory Mechanism   To enable the Transformer to consider more con-   textual information from previous segments , Dai   et al . ( 2019 ) proposed to augment the Transformer   with a recurrence memory which stores the hidden   states of previous time steps as extended keys and   values , as shown in Figure 2 . Concretely , let us   consider a memory length of Mand memory repre-   sentations X= [ x,···,x]∈R.   The extended key and value matrices are obtained   by prepend XtoXbefore projection :   ˜X= [ sg(X) ◦ X]∈R,(5 )   where sg(·)stands for stop - gradient which disables   gradient propagation to previous segments , and   [ · ◦ · ] indicates concatenation of hidden states along   the length dimension . Extended by the recurrence   memory , each query vector can consider contexts   even beyond the total context length of the atten-   tionM+N. As illustrated by Dai et al . ( 2019 ) ,   the effective context length grows linearly to the   number of layers and the attention context length   due to layer - wise reusing .   Another technique necessary to the recurrence   memory is the relative positional encodings . By   considering only the relative distance between   two tokens when computing the attention score ,   it avoids temporal confusion caused by indexing   the same position across segments and injects use-   ful relative bias . Transformer - XL uses the fixed   sinusoidal encoding matrix ( Vaswani et al . , 2017 )   to provide relative distance bias and learns global   bias terms shared across different layers , which can   extrapolate to longer contexts with a great reduc-   tion of parameters compared to Shaw et al . ( 2018 ):   A = XWWX+XWWR   + uWX+vWR , ( 6 )   where Ris the sinusoid encoding matrix , u , vare   learnable weight vectors governing the global con-   tent and position bias , and W , Ware separate   key projection matrices for the content and position   respectively .   3 Method   In this section , we describe our method in detail   with our motivation to learn better representations   for the memory .   3.1 Look - Ahead Attention   Human language is sequential with one word   following another , but humans process informa-   tion usually in a non - sequential way and re-   contextualize certain contents for several times . For   example , when countering complicated contents   during reading , humans usually first store them   temporarily in the memory and continue to scan for   relevant information if any , and revisit those old   contents to refresh their meaning quite often . This   dynamic memory refreshing mechanism enables us   to thoroughly understand the passage under current   contexts .   Existing recurrence memory however , lacks this   dynamic contextualization ability . As the represen-   tations in the recurrence memory are previously   computed conditioned on their past , they are not   aware of the current contexts which provide more5749relevant information for the current token predic-   tion .   To address this limitation , we propose a look-   ahead attention that allow the memory to attend to   the contexts on their right . Formally , we reuse the   notation X= [ x,···,x]∈Rfor   the representations of the current target sequence   andX= [ x,···,x]∈Rfor the   representations of the memory .   Let us consider the i - th position of the memory   X , xcan attend to position xon its right   ( j > i ) without causing information leakage as   long as j≤τ+1 . Though appealing , this naïve ap-   proach requires to calculate an MbyMattention   map , which would become inefficient and redun-   dant when Mis significantly greater than N. Ac-   tually , since the target segment moves forward N   positions at each iteration , we devise an incremen-   tal manner of look - ahead attention computation   that only requires the newest Npositions on the   right as key - value pairs .   ˜X= [ x,···,x]∈R.(7 )   Then the look - ahead attention results computed   previously can be effectively reused and interpo-   lated with the current ones ( § 3.2 ) . Concretely , we   formalize the look - ahead attention as follows :   ˜K=˜XW,˜V=˜XW , ( 8)   C = LookAhead - Attn ( Q,˜K,˜V )   = softmax / parenleftigQ˜K√   d / parenrightig   ˜V , ( 9 )   where softmax ( · ) masks position j≤ifor the i-   th row of the input matrix with −∞ before softmax .   Qis obtained by Eq . ( 1 ) , and the projection   matrices of query , key and value are all shared with   the causal attention . We illustrate this in Figure   3 where the look - ahead attention ( yello paths ) in-   creases the attention window of each memory state   toMtokens on its right .   3.2 Memory Interpolation   To save computations for looking - ahead and effec-   tively reuse the attention results of the past , we   propose memory interpolation that smoothly inter-   polates attention results from both the future and   the past to provide bi - directional contextualization .   Recall that in the previous iteration , we have   calculated the causal context representations C   ofXusing Eq . 2 , where each row is a linear   combination of the weighted token representations   of the previous tokens . In Sec . 3.1 , we describe   the look - ahead attention which enables Xto   attend to the contexts on their right and computes   Cusing Eq . 9 . Here , we formulate the memory   interpolation as the interpolation between the old   representations Cand the new ones Cwith   a coefficient vector α∈Rcontrolling the   memorization of the past activations :   C = Mem - Interp ( C , C , α )   = αsg(C ) + ( 1 −α)C.   ( 10 )   The resulted Cwhich attend to contexts from   both directions , are further fed to the non - linear   transformation defined in Eq . 3 to update represen-   tations in higher layers .   Forα , we define it to be the sum of the nor-   malized attention weights on the previous tokens   when calculating C(Eq . 2 ):   α = sg(s )   sg(s ) + s+ε , ( 11 )   where sis the sum of the unnormalized atten-   tion score of C , which is the denominator of   the softmax in Eq . 2 . Similarly , sis the denom-   inator of the softmax in Eq . 9 . εis a small value   to prevent zero division error in practice . Then Eq .   10 can be derived into a form that resembles the   bi - directional attention with the queries attending   to positions on both sides(Appendix A ) . Figure 4   shows the architecture of LaMemo.5750Note that the difference between the hidden state   reuse in the recurrence memory and our memory   interpolation is that they simply reuse the static   representations to extend the contexts for attention   while we update the memory representations by   aggregating weighted attention sum of the history   without the need to recompute them .   3.3 Disentangled Relative Positional   Encodings   As the look - ahead attention allows the memory   to attend to future tokens on its right , we need a   relative positional encoding scheme that can gen-   eralize to this setting . We start by considering the   relative positional encoding in Transformer - XL ,   as described by Eq . 6 . When the i - th query vec-   tor attending to a position j = i+ ∆ > i , we   haveR = R. As defined by Vaswani et al .   ( 2017 ) , R∈Ris composed of sine and cosine   functions with different frequencies . Since the sine   function is odd , sin(−ω∆ ) = −sin(ω∆ ) , we have   R̸=Rso that it can represent attention in   different directions ( ±sign of ∆ ) with the same   relative distance ( absolute value of ∆ ) .   However , this approach solely relies on the fixed   sinusoid encodings to represent the relative dis-   tance and the attention direction . We argue that   disentangling them is more effective in capturing   these two types of temporal biases and also miti-   gates the numerical unstability issue . Specifically ,   we propose to learn two direction - aware global po-   sition biases to parameterize the sign and query R   with the absolute value of the relative distance :   A = XWWX+XWWR   + uWX+vWR , ( 12 )   where v = vifi≥jelsev . The global   positional bias now explicitly separates the contri-   butions of sgn(i−j)and|i−j| , which can better   generalize to long distance in both forward and   backward directions .   To illustrate the numerical unstability caused by   adapting Eq . 6 to j > i , we derive the variance   of the dot product xRwhere xis a random   vector . We show that the variance undergoes an   oscillation and can not be properly bounded every-   where when ishifts from i≥jtoi < j . Detailed   analysis are presented in Appendix B.4 Experiments   We evaluate LaMemo on both word - level and   character - level language modeling tasks and com-   pare with existing Transformer baselines aug-   mented with different types of memory .   4.1 Datasets and Metrics   For word - level language modeling task , we con-   sider Wikitext-103 ( Merity et al . , 2017 ) , which is   the most widely used word - level language model-   ing benchmark . It contains 103 million tokens for   training from 28 thousand wikipedia articles , with   an average length of 3.6 thousand tokens per arti-   cle and a vocabulary size around 260K. We report   perplexity ( ppl ) on the dev and test set .   We also evaluate on two character - level language   modeling benchmarks enwik8 and text8 ( Ma-   honey , 2011 ) . Both datasets contain 100 million   Wikipedia characters . While enwik8 is unpro-   cessed , text8 is preprocessed by case lowering and   filtering to include only 26 letters from atozand   space . On both datasets , we report bit per character   ( bpc ) on the dev and test set .   4.2 Baselines   To directly compare with different types of memory ,   we consider Transformer - XL and its variations with   the same model architecture but different memory   mechanism .   Transformer+RPE is the vanilla Trans-   former ( Vaswani et al . , 2017 ) that uses relative   positional encodings from Dai et al . ( 2019 ) but   does not extend the context with additional   memory .   Transformer - XL ( Dai et al . , 2019 ) is a Trans-   former model equipped with relative positional en-   codings and a recurrence memory comprised of   hidden states computed in previous time steps to   extend the context length of the attention .   Compressive Transformer ( Rae et al . , 2020 )   extends Transformer - XL with an external compres-   sive memory that stores compressed hidden states   at the temporal level using convolutional networks .   ∞-former ( Martins et al . , 2021 ) uses continuous   space attention to attend over the external memory   which consists of continuous signals . They also   updated the external memory with recent hidden   states to enable unbounded memory capacity.5751   4.3 Implementation Details   We follow the standard architecture of the   Transformer - XL ( Dai et al . , 2019 ) that has differ-   ent configurations for different tasks . Specifically ,   on Wikitext-103 , we use a 16 - layer Transformer   with 10 attention heads and head dimension 41   equipped with adaptive embeddings ( Baevski and   Auli , 2019 ) . We control the target sequence length   to be 150 and the memory length 150 for all mod-   els following the setting of Dai et al . ( 2019 ) . For   the Compressive Transformer and ∞-former , we   additionally use an external memory of size 150 fol-   lowing the setting of Martins et al . ( 2021).On the   text8 and enwik8 datasets , we use a 12 - layer Trans-   former with 8 heads and head dimension 64 . The   length of the target sequence and the recurrence   memory are both set to 512 . In the main results   we use the identical evaluation setting to the train-   ing phase on all datasets and do not use a longer   memory . We use the Pytorch framework ( Paszke   et al . , 2019 ) and Apex for mixed - precision training .   In practice , we found that calculating the expo-   nentials ( § 3.2 ) may lead to numerical overflow in   mixed - precision mode , so we compute the loga-   rithm of the exponential sum using logsumexp   andlogaddexp operator . Further details of the   dataset and the hyperparameter settings are de-   scribed in the Appendix C.   4.4 Main Results   We show the results of word - level language mod-   eling benchmark Wikitext-103 in Table 1 . We first   observe that all the models extended with memo-   ries significantly outperforms Transformer+RPE .   Under the same memory length , LaMemo outper-   forms Transformer - XL with a clear margin , which   demonstrates the effectiveness of learning dynamic   memory representations over static ones . When   compared to the compressive memory and the un-   bounded memory that take longer contexts into   account , LaMemo still achieves lower perplexity .   This indicates that the look - ahead memory allows   the language model to exploit the recent contexts   to gain performance , while simply increasing the   context length yields marginal improvement . This   is in accordance with previous findings of how lan-   guage models utilize contexts ( Khandelwal et al . ,   2018 ; Sun et al . , 2021 ) . In terms of the parameters ,   LaMemo has the same number of parameters as   the Transformer - XL while other baselines use ad-   ditional parameters in CNN to compress or smooth   the hidden states . Lastly , we show the number of   FLOPS necessary for computing one step predic-   tion.∞-former has the highest number of FLOPS   for resampling enough points from the continu-   ous signal to update the memory using smoothing   techniques . LaMemo also incurs additional com-   putations to re - contextualize the memory under the   current context . Note that although the Compres-   sive Transformer has lower number of FLOPS than   LaMemo , it has an external memory that consumes   more GPU memory .   We also present the results of character - level lan-   guage modeling on text8 and enwik8 datasets in   Table 2 . We observe similar trends as the results on5752   the word - level benchmark , where LaMemo outper-   forms Transformer - XL by 0.04 on text8 and 0.02   on enwik8 with the same context length . Addition-   ally , we observe that all models exhibit overfitting   on text8 , which might be caused by the extremely   small vocabulary size of the dataset .   4.5 Ablation Study   We conduct ablation studies on Wikitext-103 to   examine the effects of the proposed techniques , i.e. ,   look - ahead attention , memory interpolation , and   disentangled relative positional encodings .   We use the same model achitecture and the same   target and memory length as the main results . We   first study three configurations , including ( 1 ) using   theFull model setting , ( 2 ) ablating the memory   interpolation module ( w/o mem interp ) , i.e. , set   the memorizing coeffecient α= 0 , and ( 3 )   ablating the look - ahead attention ( w/o look - ahead ) ,   i.e. , only use the causal context representations   Cin each layer . As shown in the First three   rows in Table 3 , both the memory interpolation   and the look - ahead attention are indispensible for   achieving the best performance . Additionaly , we   found that cancelling out memory interpolation   leads to a worse performance , which indicates that   the distant past still provides additional information   beyond the current context .   The second study targets at studying different en-   coding schemes . We substitute our encodings with   the RPE of Transformer - XL Dai et al . ( 2019 ) and   run multiple experiments with 3 different random   seeds , but all the models fail to converge . We plot   the training curves using two encodings in Figure   8 in Appendix B , where we observe that our dis-   entangled RPE is more stable during training and   achieves lower perplexity .   5 Extrapolating to Longer Contexts   In this section , we extrapolate the models to longer   contexts during inference to study the effect of   dynamic contextualization to the distant past .   We fix the length of the target sequence to   64and extrapolate the trained models to longer   memory length 64×mduring inference , where   m= 1,···,10 . We compare the perplexity of   LaMemo and Transformer - XL trained on Wikitext-   103 when augmented by a memory with different   length . As shown in Figure 5 , LaMemo consis-   tently achieves lower perplexity than Transformer-   XL when extraploating to longer contexts , while   the performance of both models saturate when m   is over 7 . Additionally , we observe that the gap   of perplexity between the two models increases   when taking longer contexts into account . This   demonstrates the effectiveness of dynamically re-   freshing the distant memory representations under   the current context.57536 Attention Analysis   In this section , we analyze the attention distribution   of LaMemo to validate the effectiveness of utilizing   bi - directional contexts with look - ahead attention .   We first visualize the memorizing coefficient α   which stands for the portion of the past activations   in the current memory representations . As show   in Figure 6 , we plot αin different layers as a func-   tion of the memory index averaged on 100 text   segments . We observe that in lower layers the   memory mainly attends to the past ( α≈1.0 ) . We   conjecture that long - term bi - directionality is not   necessary for low - level representations such as lex-   ical features . In higher layers , the memory sub-   stantially utilizes the future contents to refresh the   high - level representations , especially for the old   memory state with a small memory index .   Next , we visualize the attention weight distribu-   tion on the context tokens when predicting each   target token in Figure 1 . For every token , we take   the maximal attention weight in each interval of   5 tokens on its left and scale to a context length   of 100 . The result indicates that LaMemo learns   better memory represetations by attending to the   right - side tokens , which increases the memory uti-   lization when predicting the target token .   7 Case Study   We present the generated texts of LaMemo and   Transformer - XL trained on Wikitext-103 in Ap-   pendix D. Both models maintain a memory size   of 512 , and we seed them with the same context   randomly sampled from the test set and generate   256 tokens using top - p sampling ( Holtzman et al . ,   2020 ) with p= 0.95 .   8 Related Work   The Transformer ( Vaswani et al . , 2017 ) , with its   pair - wise modeling ability of the input , becomes   prevailing for sequence modeling , especially long   sequence processing tasks , such as long text gener-   ation ( Tan et al . , 2021 ; Ji and Huang , 2021 ) , long   document QA ( Beltagy et al . , 2020 ; Ainslie et al . ,   2020 ) , language modeling ( Dai et al . , 2019 ; Rae   et al . , 2020 ) , video processing ( Wu et al . , 2019 ) ,   and etc . Specifically , language modeling ( Merity   et al . , 2017 ) which requires processing documents   with thousands of tokens has become a naturaltestbed for benchmarking this long - term process-   ing ability . However , due to the quadratic time and   space complexity of self - attention , scaling to in-   puts with thousands of tokens is computationally   prohibitive .   One line of work investigated the linear - time   attention mechanism to mitigate the scability is-   sue of Transformer . Linformer ( Wang et al . , 2020 )   projects the inputs to lower dimension in length   and approximates the full attention with a low - rank   factorization . Linear Transformer ( Katharopoulos   et al . , 2020 ) regards the self - attention as a kernel   function and uses a linear dot - product as a substi-   tute . Choromanski et al . ( 2021 ) and Peng et al .   ( 2021 ) proposed to approximate the softmax more   precisely with the expectation of the dot - product   of random features . Although achieving substan-   tial improvements on benchmarks designated for   long inputs ( Tay et al . , 2021 ) . These methods , how-   ever , focus on approximating the full attention with   low - rank factorizations or kernel functions , which   compromise the expressiveness and robustness of   the original softmax attention , are reported to be   inferior to the simple local attentions on real world   language processing tasks ( Xiong et al . , 2021 ) .   Our work falls in another line , which aug-   ments the Transformer with a parametrized mem-   ory to store critical history information . Memory-   augmented networks ( Graves et al . , 2014 ; Weston   et al . , 2015 ; Sukhbaatar et al . , 2015 ) have been stud-   ied in the context of recurrent neural networks for   a long time , but are mostly restricted to small and   synthetic datasets . With the rapid development of   Transformer , various works start to adapt memories   to this architecture .   Dai et al . ( 2019 ) first extended Transformer with   a recurrence memory that caches hidden states com-   puted in previous steps for the target tokens to at-   tend to . Rae et al . ( 2020 ) further extended the   context with an external memory that stores com-   pressed hidden states at the temporal level . Martins   et al . ( 2021 ) used continuous space attention to   attend over the old history and updated the mem-   ory with recent hidden states to enable unbounded   memory capacity . Wu et al . ( 2021 ) proposed to   use the encoder - decoder architecture to encode the   memory states with previous text segments and   pass this memory to future time steps . Instead of   using a fixed - size attention span for different layers ,   Sukhbaatar et al . ( 2019 ) and Correia et al . ( 2019 )   proposed to learn dynamic attention spans for dif-5754ferent attention heads , which greatly reduced the   computations . These works focused on enabling   the Transformer to access contents in long distance ,   but did not consider to learn better memory repre-   sentations by refreshing the old memory under the   current context . Our work is orthogonal to learning   adaptive attention spans and can be combined with   this technique to reduce the complexity .   9 Conclusion   We present LaMemo , a memory mechanism that   allows the memory states to incrementally attend   to the right - side tokens and interpolates with the   old memory states on the left side , which enables   the memory to interact with bi - directional contexts   with a complexity linear in memory length . Experi-   ments on three language modeling datasets demon-   strate the superiority of LaMemo over baselines   with various types of memory mechanisms . We   also found that LaMemo increases the utilization of   older memory states when predicting the target to-   kens , and yields a higher performance boost when   extrapolating to longer memory length , which in-   dicates the effectiveness of recontextualizing the   memory under the current context .   Acknowledgments   This work was supported by the National Science   Foundation for Distinguished Young Scholars ( with   No . 62125604 ) and the NSFC projects ( Key project   with No . 61936010 and regular project with No .   61876096 ) . This work was also supported by the   Guoqiang Institute of Tsinghua University , with   Grant No . 2019GQG1 and 2020GQG0005 . This   work was also sponsored by Tsinghua - Toyota Joint   Research Fund .   References575557565757A Derivation of Memory Interpolation   We derive Eq . 10 into the form of standard self-   attention in the following :   C = αsg(C ) + ( 1 −α)C.   We consider the i - th row of C , denoted as c.   We omit the stop - grad operation sg(·)and substi-   tuteαwith the result from Eq . 11 :   c = αc+ ( 1−α)c   = s   s+sc+s   s+sc ,   where s , sis the denominator of the softmax   when computing c , crespectively :   s=/summationdisplayexp / parenleftigqk√   d / parenrightig   = /summationdisplaysim(q , k ) ,   s=/summationdisplayexp / parenleftigqk√   d / parenrightig   = /summationdisplaysim(q , k ) ,   where ( q , k)and(q , k)are two sets of query-   key vectors computed in the previous and this text   segment respectively for the same position pair   ( i , j ) . Then we have :   c=/summationtextsim(q , k)/summationtextsim(q , k ) + /summationtextsim(q , k)c   + /summationtextsim(q , k)/summationtextsim(q , k ) + /summationtextsim(q , k)c   = /summationtextsim(q , k)v+/summationtextsim(q , k)v / summationtextsim(q , k ) + /summationtextsim(q , k )   = /summationdisplayβ˜v ,   where / summationtextβ= 1 . Finally , we derive cas the   weighted sum of the value vectors ˜vfrom both the   past ( j≤i ) and the future ( j > i ) of the position i.   B Unstability Analysis of the RPE in   Transformer - XL   We conjecture that the unstability of Eq . 6 stems   from the terms involving the dot - product of R   and another vector . So we start by considering the   variance of xRwhere x∈Ris a random   vector . Without loss of generality , we assume that   xhas zero mean and a variance of σ :   E(x ) = 0 , ∀k∈[1 , · · · , d ]   Var(x ) = σ,∀k∈[1 , · · · , d ]   Cov(x , x ) = σ,∀l̸=k∈[1 , · · · , d]Leti−j= ∆ . According to Vaswani et al . ( 2017 ) ,   Rtakes the following form :   R=[sin ( ω∆),cos(ω∆ ) ,   · · · , sin(ω∆),cos(ω∆ ) ] ,   where w= 10000 . Then the dot - product   xRcan be derived into the linear combination   of sine and cosine functions :   xR=/summationdisplayxsin(ω∆ ) + xcos(ω∆ ) ,   where we can easily derive that E(xR ) = 0 .   According to the variance - expectation formula :   Var(x ) = E[x]−E[x ] , we can simplify the vari-   ance Var ( xR)in the following :   Var(xR )   = E / bracketleftig / parenleftbig / summationdisplayxsin(ω∆ ) + xcos(ω∆)/parenrightbig / bracketrightig   = /summationdisplayE[x ] sin(ω∆ ) + E[x ] cos(ω∆ )   + 2 / summationdisplay / summationdisplayE[xx ] sin(ω∆ ) cos ( ω∆ ) .   We further simplify the above equation by assum-   ing that all the elements have the same variance   σ , and all pairs of distinct elements have the same   covariance σ :   Var(xR ) = /summationdisplayσ[sin(ω∆ ) + cos(ω∆ ) ]   + 2 / summationdisplay / summationdisplayσsin(ω∆ ) cos ( ω∆ )   = d   2σ+ 2σg(∆ ) ,   where g(x ) = /summationtext / summationtextsin(ωx ) cos ( ωx )   is an odd function .   We consider the value of g(x)when x≈0.5758   Since sin(ωx)≈ωx , cos(ωx)≈1 , we have :   g(x)≈/summationdisplay / summationdisplayωx   = d   2 / summationdisplaywx   = xd   2 / summationdisplay / parenleftig1   10000 / parenrightig   ≈d   2((10)−1)·x = γ·x .   Since a≈1 + xlnawhen x≈0 , we derive that   γ≈with the grow of d. This causes g(x )   to have a very steep slope near 0 . Since g(x)is an   odd function , the value of g(∆ ) andg(−∆)will   have a huge gap ( ∆is a small positive value ) . To   validate this , we plot the function of g(x)when   d= 64 in Figure 7 .   Overall , the variance of xRis composed of   two terms , the first being σmultiplied by a con-   stant factor d/2 , and the second being σmultiplied   byg(∆ ) . Note that σis strictly positive , while σ   does not have this restriction . Due the asymptotic   behavior of g(∆)near 0 , i.e. , O(d∆ ) , we can not   find a proper σthat makes Var(xR)bounded   byO(dσ)for every ∆that takes its value from   both the positive and negative integers .   Finally , we plot the training curves of the two   models using the RPE in Transformer - XL ( xl - rpe )   and our disentangled RPE ( dis - rpe ) in Figure 8   where we observed that the xl - rpe suffers from   numerical unstability during training .   C Experimental Details   C.1 Dataset Details   Wikitext-103 dataset is extracted from the set of   verified Good and Featured articles on English   Wikipedia . The dataset retains the original case ,   punctuation and numbers , and covers a broad range   of domains , e.g. , science , culture , bibliography ,   and etc . The dataset is available under the Creative   Commons Attribution - ShareAlike ( CC BY - SA ) Li-   cense .   enwik8 dataset is the test set data of the Large   Text Compression Benchmark which contains the   first 100 million bytes of English Wikipedia dump   on Mar. 3 , 2006 . All characters are encoded in   UTF-8 . This dataset is licensed under the CC BY-   SA License .   text8 dataset contains the first 100 million bytes   of the clean text of Wikipedia that retains only   regular articles and image captions . All the letters   are converted into lower case , and only letters in   the 27 character alphabet , namely letters a - z and   nonconsecutive spaces , are preserved . This dataset   is licensed under the CC BY - SA License .   The statistics of the three datasets is shown in   Table 4.5759C.2 Model Configurations   We follow the base model configuration of Dai   et al . ( 2019 ) . On Wikitext-103 , we use the Trans-   former model with 16 layers , 10 attention heads   with a head dimension of 41 . The inner dimension   size of the feedforward layer is 2100 . We use a   dropout rate of 0.1 and no attention dropout . To   cope with the large vocabulary , we use the adap-   tive embeddings ( Baevski and Auli , 2019 ) . We set   the memory length to 150 and the target sequence   length to 150 as well . On text8 and enwik8 datasets ,   we use the Transformer model with 12 layers , 8   attention heads with a head dimension of 64 . The   inner dimension size of the feedforward layer is   2048 . We use a dropout rate of 0.1 and no attention   dropout . We set the memory length to 512 and   the target length to 512 . Specifically , our LaMemo   uses the disentangled relative positional encodings   described in Sec . 3.3 . The look - ahead attention   shares the query , key and value projection matrices   with those in the causal attention .   C.3 Training Settings   We trained the models using Adam ( Kingma and   Ba , 2015 ) optimizer , with no warmup . We used   a learning rate of 2.5×10which decayed to   0 at the end of training with a cosine schedule .   On Wikitext-103 , we trained the model with 250 K   steps using a batch size of 64 . On enwik8 and   text8 , we trained the model with 100Ksteps using   a batch size of 40 . We conducted our experiments   on 2 Tesla V100 .   C.4 Hyperparameters   We present the hyperparameter search space in Ta-   ble 5 . The number of hyperparameter search trials   was 10 . We adopted a manual search to select the   hyperparameters , and the selection criterion was   ppl / bpc on the dev set . We did not use early stop-   ping during training .   D Generated Examples   In this section , we present the examples gener-   ated by LaMemo and Transformer - XL trained on   the Wikitext-103 dataset . Both models maintain   a memory with a length of 512 . We randomly se-   lect a piece of text from the test set as the context   and allow both models to generate 256 tokens fol-   lowing the context . We use top - p sampling with   p= 0.95and detokenize the context and the gen-   erated texts to facilitate reading . We present the   exmples in Table 6 and 7 . We present our major   findings below :   •Both models are able to hallucinate imaginary   contents fairly relevant to the limited contexts   given as prompts .   •Transformer - XL sometimes generates topic-   irrelevant contents without further elaboration   ( marked by underline ) , while LaMemo stays   on topic more closely during the course of   generation .   •Transformer - XL suffers more sever repetition   issues ( marked in boldface ) than LaMemo   both lexically and semantically.576057615762