iiiiic   2022 Association for Computational Linguistics   Order copies of this and other ACL proceedings from :   Association for Computational Linguistics ( ACL )   209 N. Eighth Street   Stroudsburg , PA 18360   USA   Tel : +1 - 570 - 476 - 8006   Fax : +1 - 570 - 476 - 0860   acl@aclweb.org   ISBN 978 - 1 - 955917 - 22 - 3   iv   Welcome to ACL 2022 , the 60th Annual Meeting of the Association for Computational Linguistics ! The   conference will be held in Dublin , the capital of Ireland , on May 22‚Äì27 , 2022 .   ACL 2022 will be a hybrid conference . After two fully virtual editions , ACL 2020 and ACL 2021 , due to   the covid-19 pandemic , this year we are gradually coming back to normality , estimating , at the moment   of writing this message , that about 50 % of the registered participants will be able to attend the conference   in - person , enjoying the atmosphere of the CCD congress center , the social events of the conference , and   the many opportunities in Dublin . On the other side , virtual attendees will have the possibility to interact   almost like they were in Dublin , thanks to a sophisticated virtual conference platform .   There are few important innovations this year . The most relevant is that ACL 2022 adopted a new   reviewing process , based on ‚Äú rolling review ‚Äù ( ARR ) , with the goal of coordinating and making more   efÔ¨Åcient the paper reviews of the ACL conferences . This initiative was shared with NAACL 2022 , resul-   ting in a coordinated effort . As a side effect of moving to ARR , we have been working on a new version   of the software , called ACLPUB2 , used to produce both the conference proceedings and the conference   schedule . I would like to thank all the people who contributed to those achievements . Finally , this year   we celebrate the 60th anniversary of the ACL conference . Thanks to the enthusiastic contributions of   many organizations , coordinated by the Diversity and Inclusion co - chairs , we are preparing a very spe-   cial initiative for our community , which , at the time of writing this message , is still secret and that will   be disclosed during the opening of the conference .   I was very lucky to work together with three fantastic Program Chairs : Preslav Nakov , Smaranda Mure-   san and Aline Villaviciencio . I could not thank you more for the dedication and the capacity with which   you have organized a very exciting scientiÔ¨Åc program and for the help in all the phases of the conference   organization .   Thanks to the local organizers in Dublin , Andy Way and John Kelleher , and to the PCO , who managed the   local organization in a period in which we have had very few certainties , and many more uncertainties .   We are extremely grateful to all sponsors for their continuing and generous support to help our conferen-   ces be very successful . Thank you to Chris Callison - Burch , the ACL Sponsorship Director , for managing   the relations between the sponsors and ACL 2022 .   I am also very grateful to the chairs of the previous years ‚Äô conferences , who were always ready to help   and to provide advice , contributing to the transmission , from year to year , of all the know - how and   collective memory . Thanks to all the members of The ACL Executive Committee , they were always   supportive , particularly when feedback on delicate issues was needed .   Many thanks to the senior area chairs , the area chairs , the reviewers , our workshop organizers , our tutorial   instructors , the authors and presenters of papers , and the invited speakers .   ACL requires a long process , involving a large team of committed people . It is an honor for me to have   coordinated such a team of talented people , who kindly volunteered their time to make this conference   possible . I would like to thank the members of the organizing committee for their dedication and hard   work , often under a tight schedule :   Workshop Co - Chairs : Elena Cabrio , Sujian Li , Mausam ;   Tutorial Co - Chairs : Naoaki Okazaki , Yves Scherrer , Marcos Zampieri ;   Demo Co - Chairs : Valerio Basile , Zornitsa Kozareva , Sanja ≈†tajner ;   Student Research Workshop Co - Chairs : Samuel Louvan , Brielen Madureira , Andrea Madotto ;   vSRW Faculty Advisors : Cecile Paris , Siva Reddy , German Rigau ;   Publication Co - Chairs ( also publication co - chairs for NAACL 2022 ): Danilo Croce , Ryan Cotte-   rell , Jordan Zhang ;   Conference Handbook Chair : Marco Polignano ;   Diversity & Inclusion Co - chairs : Mona Diab , Martha YiÔ¨Åru Tachbelie ;   Ethic advisor committee : Su Lin Blodgett , Christiane Fellbaum ;   Technical OpenReview Chair : Rodrigo Wilkens ;   Publicity and Social Media Co - chairs : Isabelle Augenstein , Emmanuele Chersoni , Diana May-   nard , Soujanya Poria , Joel Tetreault ;   Local Arrangement Committee : Fiona McGillivray , Greg Carew , Laird Smith ;   Student V olunteer Coordinators : Filip Klubicka , Vasudevan Nedumpozhimana , Guodong Xie ,   Pintu Lohar ;   Internal Communications Chair : Marcely Boito Zanon .   Let me deserve a special thanks to Priscilla Rasmussen . She has been the pillar not only of this year ‚Äôs   ACL , but of the ACL conferences for many years . She has offered her invaluable experience to the   organizing committee , and her presence has always given us a pleasant sense of security .   Finally , I would like to thank all the participants , both in - person and virtual , who will be the main   actors from May 22 to May 27 , 2022 . I am convinced that we will experience a fantastic conference ,   scientiÔ¨Åcally exciting and full of fond memories .   Welcome and hope you all enjoy the conference !   Bernardo Magnini ( FBK , Italy )   ACL 2022 General Chair   vi   Welcome to the 60th Annual Meeting of the Association for Computational Linguistics ( ACL 2022 ) .   ACL 2022 has a special historical signiÔ¨Åcance , as this is the 60th Anniversary edition . It is also the Ô¨Årst   hybrid ACL conference after two years of a fully virtual format for ACL in 2020 and 2021 due to the   COVID-19 pandemic . Finally , it is the Ô¨Årst * ACL conference to fully embrace the ACL Rolling Review   ( ARR ) as a reviewing process . Below , we discuss some of these changes and we highlight the exciting   program that we have put together with the help from our community .   In coordination with the NAACL 2022 team and the ACL executive committee , we decided to fully   adopt the ACL Rolling Review ( ARR ) as the only reviewing platform for ACL 2022 . ARR is a new   review system for * ACL conferences , where reviewing and acceptance of papers to publication venues is   done in a two - step process : ( i ) centralized rolling review via ARR , and ( ii ) commitment to a publication   venue , e.g. , ACL 2022 . The purpose of the ACL Rolling Review is to improve the efÔ¨Åciency and the   turnaround of reviewing in * ACL conferences while keeping diversity ( geographic and otherwise ) and   editorial freedom .   As ACL 2022 is the Ô¨Årst conference to fully adopt the ARR review process , we worked very closely   with ARR and we coordinated our efforts with the NAACL 2022 PC chairs . In particular , given the short   distance between ACL 2022 and NAACL 2022 , we allowed authors to commit their papers to ACL 2022   and simultaneously to submit a revision to ARR in January , which were eligible for NAACL 2022 . We   also joined ARR as Guest Editors - in - Chief ( EiCs ) to help with the September ‚Äì November submissions   to ARR , which primarily targeted ACL 2022 . We worked together to integrate ARR and some of the   conference workÔ¨Çows to ensure scaling up , and to maintain the quality and the timely processing of the   submissions for November , and thus to guarantee that all papers submitted by the November 15 , 2021   ARR deadline could be considered for ACL 2022 if the authors decided to commit them . This required   making sure we had all reviews and meta - reviews ready in time , which we managed to achieve thanks   to the combined efforts of the ARR and the ACL 2022 teams . We would also like to note that this is a   community effort , and we are grateful for the support of the authors , the reviewers , the Action Editors   ( AEs ) , and the Senior Area Chairs ( SACs ) , who have been constructively engaging and helping with   ARR and ACL 2022 .   The commitment form for ACL 2022 asked the authors to provide a link to their paper in ARR : we   asked for a link to the latest version of the paper that had reviews and a meta - review . The authors also   needed to select an area ( including the Special Theme area ) they were submitting their paper to ( this   was needed as ACL 2022 had areas , while ARR did not ) . Finally , the authors were allowed to submit   optional comments to the ACL 2022 Senior Area Chairs ( SACs ) . Note that these comments were only   visible to the SACs , and they were not sent to the reviewers or to the Action Editors : the rationale was   that responding to reviewers and Action Editors should be handled in a response letter if the authors   decided to do a resubmission in ARR , which is a completely different process than committing a paper   to ACL 2022 . These comments to the SACs were designed mainly to raise concerns about objective   misunderstandings by the reviewers and/or by the Action Editor about the technical aspect of the paper   that the authors believed might help the SACs in their decision - making process .   Areas While ARR did not have areas , ACL 2022 did : it had 23 areas , including the 22 areas from ACL   2021 plus our Special Theme . Our special theme was on ‚Äú Language Diversity : from Low - Resource to   Endangered Languages , ‚Äù to commemorate the 60th anniversary of ACL with the goal of reÔ¨Çecting and   viistimulating a discussion about how advances in computational linguistics and natural language proces-   sing can be used to promote language diversity from low - resource to endangered languages . We invited   papers that discuss and reÔ¨Çect on the ‚Äú role of the speech and language technologies in sustaining langua-   ge use ‚Äù ( Bird , 2020 ) for the large variety of world languages with focus on under - resourced , indigenous ,   and/or endangered languages . We were interested in the challenges for developing and scaling up the   current NLP technologies for the rich diversity of human languages and in the ethical , cultural , and po-   licy implications of such technologies for local communities . We also have a best Theme paper award   category .   As ACL 2022 submissions in ARR , we count all papers from September , October , and November , which   we advertised as ACL 2022 months , after removing all re - submissions and also nine papers that selected   NAACL 2022 as a preferred venue ( a total of 3,360 papers ) + the papers from the May ‚Äì August period   that were actually committed to ACL 2022 and that were not resubmissions ( a total of 18 papers ) , for a   total of 3,378 papers .   This number is on par with the number of submissions to ACL 2021 , which received 3,350 submissions .   Subsequently , 1,918 papers were committed to ACL 2022 ( i.e. , 57 % ) . After the review process , 701   papers ( 604 long and 97 short ) were accepted into the main conference .   Acceptance Rates for the Main Conference   The quality of a conference is often perceived based on the acceptance rate of the papers submitted there ,   and thus it is important to have an acceptance rate that adequately represents the difÔ¨Åculty of publishing   a paper in the conference . Given the adoption of ARR , it is also important to allow for consistency   across various conferences . Thus , ACL 2022 ( and NAACL 2022 ) adopted the following two ways of   calculating the acceptance rates :   ( a)(Number of accepted papers at ACL 2022 ) / ( Number of papers that selected ACL 2022 as the   preferred venue in ARR or were committed to ACL 2022 ) . For ACL 2022 , for the denominator we   consider the 3,378 papers as explained above . Thus , the acceptance rate is 701 / 3,378 = 20.75 %   for the Main conference .   ( b)(Number of accepted papers at ACL 2022 ) / ( Number of papers committed to ACL 2022 ) . For the   denominator , we had 1,918 papers committed to ACL 2022 , and thus , the acceptance rate is 701 /   1,918 = 36.54 % for the Main conference .   Note that option ( a ) is closer to the way the acceptance rate was computed at previous * ACL conferences ,   where submitting and committing a paper was done in one step and papers were rarely withdrawn after   the reviews , the meta - reviews , and the corresponding scores were released . However , one issue with this   option for ACL 2022 was that indicating a preferred venue was only enabled starting with the October   ARR submissions , and it was not available for earlier months . As mentioned above , we removed a small   number of papers from our denominator that selected NAACL 2022 as a preferred venue in October   and November ( a total of 9 papers ) and we considered the ARR submissions only for the months of   September , October , and November , as these months were advertised in our CFP , plus any papers that   were committed to ACL 2022 from earlier months ( May ‚Äì July ) and which were also not resubmissions .   Option ( b ) yields a higher ‚Äú acceptance rate ‚Äù , as many authors with low reviewing scores chose not to   commit their paper to ACL 2022 .   Best Paper Awards   From the committed ACL 2022 papers , we selected 32 papers as candidates for the following Best Paper   awards , based on nominations by the Senior Area Chairs : Best Research Paper , Best Special Theme   viiiPaper , Best Resource Paper , and Best Linguistic Insight Paper . These papers were assessed by the Best   Paper Award Committee . The selected best papers will be presented in a dedicated plenary session for   Best Paper Awards on May 24 , 2022 .   Findings of ACL 2022   Given the success of the Findings at EMNLP 2020 and 2021 and ACL - IJCNLP 2021 , we also have Fin-   dings of ACL 2022 papers , which are papers that were not accepted for publication in the main confe-   rence , but nonetheless were assessed by the Program Committee as solid work with sufÔ¨Åcient substance ,   quality , and novelty . A total of 361 papers were offered to be included in the Findings of ACL 2022 .   Given the two ways of computing acceptance rates described above , this results in a 10.68 % acceptance   rate in option ( a ) , and 19.82 % in option ( b ) . Out of the 361 papers , 30 papers declined the offer , leading   to 331 papers to be published in the Findings of ACL 2022 . In order to increase the visibility of the   Finding of ACL 2022 papers , we offered the authors of these 331 papers the possibility to present their   work as a poster at ACL 2022 , in addition to making a 6 - minute or a 3 - minute video to be included in   the virtual conference site ( for long and for short papers , respectively ) . The authors of 305 of the 331   papers accepted our invitation to present their work as a poster at ACL 2022 .   TACL and Computational Linguistics   Continuing the tradition from previous years , ACL 2022 also features 43 articles that were published   at the Transactions of the Association for Computational Linguistics ( TACL ) and 8 papers from the   Computational Linguistics journal .   Another highlight of our program are the keynotes , which we run in three different formats :   a keynote talk by Angela Friederici ( Max Planck Institute for Human Cognitive and Brain Scien-   ces ) on ‚Äú Language in the Human Brain ‚Äù ;   a keynote Ô¨Åre - side chat on ‚Äú The Trajectory of ACL and the Next 60 years ‚Äù with Barbara Grosz   ( Harvard University ) and Yejin Choi ( University of Washington and Allen Institute for ArtiÔ¨Åcial   Intelligence ) , moderated by Rada Mihalcea ( University of Michigan ) ;   a keynote panel on ‚Äú How can we support linguistic diversity ? ‚Äù led by Steven Bird ( Charles   Darwin University ) , with panelists representing a variety of world languages , including ( currently   conÔ¨Årmed ) Teresa Lynn ( Irish ) , Robbie Jimerson ( Seneca ) , Heather Long ( Creole languages ) , and   Manuel Mager ( Wixaritari ) .   We further had two additional invited talk initiatives :   Spotlight Talks by Young Research Stars ( STIRS ) by Eunsol Choi ( University of Texas at Au-   stin ) , Ryan Cotterell ( ETH Zurich ) , Sebastian Ruder ( Google , London ) , Swabha Swayamdipta   ( Allen Institute for AI ) , and Diyi Yang ( Georgia Tech ) ;   Next Big Ideas Talks by Marco Baroni ( Pompeu Fabra University ) , Eduard Hovy ( The Univer-   sity of Melbourne and Carnegie Mellon University ) , Heng Ji ( UIUC ) , Mirella Lapata ( Universi-   ty of Edinburgh ) , Hang Li ( Bytedance Technology ) , Dan Roth ( University of Pennsylvania and   Amazon ) , and Thamar Solorio ( University of Houston ) .   ix   ACL 2022 is the result of a collaborative effort and a supportive community , and we want to acknowledge   the efforts of so many people who have made signiÔ¨Åcant efforts into the organization of ACL 2022 ! First   of all , we would like to thank our Program Committee ( the full list of names is quite long and it is   included in the Program Committee pages of the Proceedings ):   Our awesome 82 Senior Area Chairs who were instrumental in every aspect of the review process ,   from liaising with ARR , to supporting the implementation of a two - stage reviewing system , re-   commending Action Editors and reviewers , working on paper acceptance , and nomination of best   papers and outstanding reviewers . For all of them , this involved familiarizing themselves with a   new protocol to accommodate the integration of ARR reviews and a new system , and for many of   them , the scope of their responsibilities was equivalent to chairing a small conference .   The 363 ARR Action Editors ( from the June ‚Äì November ARR cycles ) , who had the role of ACL   2022 Area Chairs interacting with reviewers , leading paper review discussions , and writing meta-   reviews .   The 2,323 ARR reviewers ( from the June ‚Äì November ARR cycles ) , who contributed for the ACL   2022 reviewing cycles , providing valuable feedback to the authors .   The emergency ARR Action Editors and reviewers , who provided their support at the last minute   to ensure a timely reviewing process .   The amazing ARR team , who collaborated in the challenge of managing and implementing the   ARR reviewing needed for the scale of ACL 2022 . In particular , we acknowledge Amanda Stent   and Goran Glava≈° as Guest ARR Editors - in - Chief for ACL 2022 , Graham Neubig as Guest ARR   Chief Technical OfÔ¨Åcer for ACL 2022 , and Sara Goggi as Guest ARR Editorial Manager for ACL   2022 .   ACL 2022 counted on the contributions of many wonderful committees , including :   Our Best Paper Selection Committee , who selected the best papers and the outstanding papers :   Tim Baldwin , Kathleen McKeown , David Chiang , Min - Yen Kan , and Taro Watanabe .   Our Ethics Advisory Committee , chaired by Christiane Fellbaum and Su Lin Blodgett , for their   hard work to ensure that all the accepted papers addressed the ethical issues appropriately , under a   very tight schedule and on a new platform .   Our amazing Publication Chair Danilo Croce , our Handbook Chair Marco Polignano , the Techni-   cal OpenReview Chair Rodrigo Wilkens , and the Scheduler Chair Jordan Zhang , who jointly with   the NAACL 2022 Publication Chair , Ryan Cotterell , made an enormous contribution to the com-   munity by implementing the integration scripts for generating the proceedings , the handbook and   the schedule from the OpenReview platform .   Our Publicity Chairs Isabelle Augenstein , Emmanuele Chersoni , Diana Maynard , Soujanya Poria ,   and Joel Tetreault , for their work on managing the communications on social media platforms .   The Internal Communications Chair Marcely Boito Zanon for streamlining the processes .   The wonderful Technical OpenReview Chair Rodrigo Wilkens , who went above and beyond to   ensure that the typical ACL conference functionalities were translated to a new environment .   We would also like to thank many people who helped us with various software used for the conference :   The ARR Tech team , in particular Sebastin Santy and Yoshitomo Matsubara , who served as Guest   ARR Tech Team for ACL 2022 .   xThe OpenReview team , in particular Nadia L‚ÄôBahy , Celeste Martinez Gomez , and Melisa Bok ,   who helped to implement the integration of ARR as a reviewing platform for ACL 2022 .   The whole Underline team , in particular Sol Rosenberg , Jernej Masnec , Damira Mr≈°i ¬¥ c , and Mateo   Antonic , who created a virtual site for the conference .   As Program chairs , we had to deal with many tasks , including handling new protocols and situations and   a new conference management environment . We would not be able to complete these tasks without the   advice from our colleagues , including   Our fantastic General Chair Bernardo Magnini , who provided invaluable support and feedback   throughout the whole process , including collaborating on the efforts to take on the challenge of   reengineering the conference reviewing processes and pipeline .   The Program Co - Chairs of NAACL 2022 Marine Carpuat , Marie - Catherine de Marneffe , and Ivan   Vladimir Meza Ruiz , and the NAACL 2022 General Chair , Dan Roth , for collaborating in the   challenge of coordinated adoption of ARR reviewing in a full scale for ACL 2022 and NAACL   2022 .   The Program Co - Chairs of previous editions of * ACL conferences , in particular the ACL - IJCNLP   2021 PC chairs Roberto Navigli , Fei Xia , and Wenjie Li , as well as the EMNLP 2021 PC chairs Lu-   cia Specia , Scott Wen - tau Yih , and Xuanjing Huang for providing amazing guidance and support ,   and sharing their experience and answering our many questions , often on short notice .   The ACL Executive Committee , especially Tim Baldwin ( the ACL President ) , Rada Mihalcea ( the   ACL Past President ) , Shiqi Zhao ( Secretary ) , Priscilla Rasmussen ( Business Manager ) , and the   members of the ACL executive committee for providing invaluable feedback and for helping us   sort through various issues .   The Computational Linguistics Editor - in - Chief Hwee Tou Ng , the TACL Editors - in - Chief Ani   Nenkova and Brian Roark , and the TACL Editorial Assistant Cindy Robinson , for coordinating the   Computational Linguistics and the TACL presentations at ACL 2022 .   We would also like to thank all the authors who submitted / committed their work to ACL 2022 . Although   we were only able to accept a small percentage of the submissions , your hard work makes this conference   exciting and our community strong . Our huge thanks goes to the * ACL communities for the kind and   patient support during a year of major changes in our submission and reviewing processes .   Last , but not least , we thank our students , interns , postdocs , colleagues , and families for being so under-   standing and supportive during this intense year , and especially when we were swamped by countless   conference deadlines and meetings . Our deepest gratitude is to all of you . We hope you will enjoy this   60th Anniversary edition of ACL .   Smaranda Muresan ( Columbia University and Amazon AWS AI Labs , USA )   Preslav Nakov ( Qatar Computing Research Institute , HBKU )   Aline Villavicencio ( University of ShefÔ¨Åeld , UK )   ACL 2022 Program Committee Co - Chairs   xi   Back in March 2020 , just after the Ô¨Årst COVID-19 lockdown , we submitted our bid for Dublin to host   ACL 2022 , conference that you are currently attending . In November 2020 , we learned that our bid had   been successful , which we were of course delighted to hear . Of course , at that stage ‚Äì and at many points   in between ‚Äì we have wondered whether we would be able to meet face - to - face at all , and it is great   that we are able to host you in the wonderful city of Dublin where we are privileged to live , as well as   accommodating many of you online .   ACL is an opportunity to welcome not just our European friends and colleagues , but also those from   farther aÔ¨Åeld . Ireland punches above its weight in the areas of NLP and Machine Learning , principally   through the SFI - funded e100 million ADAPT Centre for Digital Content Technology , which comprises   experts from 4 local Dublin universities as well as 4 further universities from across the country in a   range of disciplines in AI . We have internationally renowned groups in machine translation , information   retrieval , speech technology , parsing and grammar Induction , among others , so we believe it is appro-   priate that ACL is being held in our country for the Ô¨Årst time . We are of course grateful to everyone   who submitted a paper ; whether your work was selected for presentation or not , if no - one had submitted ,   we would n‚Äôt have had a conference . For those of you whose work was selected for presentation , many   thanks for coming to Dublin , or for presenting online .   Along the way , we have been helped greatly by the General Chair Bernardo Magnini , and by Priscilla   Rasmussen and others from the ACL executive team , to whom we are extremely thankful . However , by   far the biggest thanks are due to Greg Carew and his team in Abbey Conference and Events for their   professional support of the conference . You will have met them at registration , and they are available   throughout the event to ensure your needs are met . We have been engaging with them for 2 years now on   ACL , and for longer as they helped Andy host the MT Summit in 2019 . We could not have made a better   choice of PCO to assist us with all the requirements involved in hosting the best - regarded conference in   our area . This has been a true partnership that has made this journey an enjoyable one .   We are also extremely grateful to F√°ilte Ireland for their extremely generous support of this conference ,   and to our PostDocs Guodong Xie & Pintu Lohar ( with Andy at DCU ) , and Vasudevan Nedumpozhimana   & Filip Klubi Àácka ( with John at TUD ) for their huge efforts to recruit and manage the small army of   student volunteers . Finally , we really hope that you all enjoy the conference , that you beneÔ¨Åt from   the excellent programme that has been assembled , and that you go away from here having made new   friends . We are fortunate indeed that many of our very best friends are in the computational linguistics   community , and we will try our very best to meet as many of you as possible during the event .   Andy Way ( Dublin City University , Ireland )   John Kelleher ( TU Dublin , Ireland )   Local Chairs , ACL 2022   xii   General Chair   Bernardo Magnini , FBK , Italy   Program Chairs   Smaranda Muresan , Columbia University and Amazon AWS AI Labs , USA   Preslav Nakov , Qatar Computing Research Institute , HBKU , Qatar   Aline Villavicencio , University of ShefÔ¨Åeld , UK   Local Organization Chairs   John Kelleher , TU Dublin , Ireland   Andy Way , Dublin City University , Ireland   Workshop Chairs   Elena Cabrio , Universit√© C√¥te d‚ÄôAzur , France   Sujian Li , Pekin University , China   Mausam , IIT Delhi , India   Tutorial Chairs   Luciana Benotti , National University of C√≥rdoba , Argentina   Naoaki Okazaki , Tokyo Institute of Technology , Japan   Yves Scherrer , University of Helsinki , Finland   Marcos Zampieri , Rochester Institute of Technology , USA   Demo Chairs   Valerio Basile , University of Turin , Italy   Zornitsa Kozareva , Facebook AI Research , USA   Sanja ≈†tajner , Symanto Research , Germany   Student Research Workshop Chairs   Samuel Louvan , FBK , Italy   Andrea Madotto , HKUST , Hong Kong   Brielen Madureira , University of Potsdam , Germany   Student Research Workshop : Faculty Advisors   Cecile Paris , CSIRO , Australia   Siva Reddy , McGill University , Canada   German Rigau , Basque Country University , Spain   xiiiPublicity and Social Media Chairs   Isabelle Augenstein , University of Copenhagen , Denmark   Emmanuele Chersoni , The Hong Kong Polytechnic University , Hong Kong   Diana Maynard , University of ShefÔ¨Åeld , UK   Soujanya Poria , Singapore University of Technology , Singapore   Joel Tetreault , Dataminr , USA   Publication Chairs   Danilo Croce , University of Rome Tor Vergata , Italy   Ryan Cotterell , ETH Z√ºrich , Switzerland   Jordan Zhang   Handbook Chair   Marco Polignano , University of Bari Aldo Moro , Italy   Technical OpenReview Chair   Rodrigo Wilkens , Universit√© catholique de Louvain , Belgium   Conference App Chair   Pierluigi Cassotti , University of Bari Aldo Moro , Italy   Diversity and Inclusion Chairs   Mona Diab , Facebook AI Research & GWU , USA   Martha YiÔ¨Åru Tachbelie , Addis Abada University , Ethiopia   Ethic Advisor Committee   Su Lin Blodgett , Microsoft Research Montr√©al , Canada   Christiane Fellbaum , Princeton University , USA   Student Volunteer Coordinators   Filip Klubi Àácka , ADAPT Centre , Ireland   Pintu Lohar , ADAPT Centre , Ireland   Vasudevan Nedumpozhimana , ADAPT Centre , Ireland   Guodong Xie , ADAPT Centre , Ireland   Internal Communications Chair   Marcely Boito Zanon , University of Avignon , France   Best Paper Selection Committee   Tim Baldwin , MBZUAI and The University of Melbourne , Australia   Kathleen McKeown , Columbia University , USA and Amazon AWS AI Labs   David Chiang , University of Notre Dame , USA   xivMin - Yen Kan , National University of Singapore , Singapore   Taro Watanabe , Nara Institute of Science and Technology , Japan   Guest ARR Editors - in - Chief for ACL 2022   Amanda Stent , Colby College , USA   Goran Glava≈° , University of Mannheim , USA   Guest ARR Chief Technical OfÔ¨Åcer for ACL 2022   Graham Neubig , Carnegie Mellon University , USA   Guest ARR Editorial Manager for ACL 2022   Sara Goggi , CNR - ILC , Italy   Guest ARR Tech Team for ACL 2022   Yoshitomo Matsubara , UC Irvine , USA   Sebastin Santy , University of Washington , USA   Guest OpenReview Team for ACL 2022   Nadia L‚ÄôBahy , OpenReview   Celeste Martinez Gomez , OpenReview   Melisa Bok , OpenReview   Underline   Sol Rosenberg , Underline   Jernej Masnec , Underline   Damira Mr≈°i ¬¥ c , Underline   Mateo Antonic , Underline   Luka ≈†imi ¬¥ c , Underline   Conference Advisor   Priscilla Rasmussen , ACL   Conference Registration   Nicole Ballard , Yes Events   Terah Shaffer , Yes Events   Local PCO   Greg Carew , Abbey   Fiona McGillivray , Abbey   Laird Smith , Abbey   xv   Program Chairs   Smaranda Muresan , Columbia University and Amazon AWS AI Labs   Preslav Nakov , Qatar Computing Research Institute , HBKU   Aline Villavicencio , University of ShefÔ¨Åeld   Computational Social Science and Cultural Analytics   Tanmoy Chakraborty , Indraprastha Institute of Information Technology Delhi   David Jurgens , University of Michigan   Diyi Yang , Georgia Institute of Technology   Dialogue and Interactive Systems   Srinivas Bangalore , Interactions LLC   Yun - Nung Chen , National Taiwan University   David Traum , University of Southern California   Dilek Hakkani - Tur , Amazon Alexa AI   Zhou Yu , Columbia University   Discourse and Pragmatics   Manfred Stede , Universit√§t Potsdam   Junyi Jessy Li , University of Texas , Austin   Ethics and NLP   Saif M. Mohammad , National Research Council Canada   Malvina Nissim , University of Groningen   Generation   Claire Gardent , CNRS   Asli Celikyilmaz , Facebook AI Research   Chenghua Lin , University of ShefÔ¨Åeld   Michael Elhadad , Ben Gurion University of the Negev   Information Extraction   Heng Ji , University of Illinois , Urbana - Champaign and Amazon Alexa AI   Marius Pasca , Google Research   Alan Ritter , Georgia Institute of Technology   Veselin Stoyanov , Facebook   Satoshi Sekine , RIKEN   Information Retrieval and Text Mining   xviHang Li , Bytedance Technology   Marti Hearst , University of California Berkeley   Jing Jiang , Singapore Management University   Interpretability and Analysis of Models for NLP   Yonatan Belinkov , Technion , Technion   Anders S√∏gaard , Copenhagen University   Anna Rogers , University of Copenhagen   Hassan Sajjad , Qatar Computing Research Institute , HBKU   Language Grounding to Vision , Robotics and Beyond   William Yang Wang , UC Santa Barbara   Marie - Francine Moens , KU Leuven   Linguistic theories , Cognitive Modeling and Psycholinguistics   Frank Keller , The University of Edinburgh   Afra Alishahi , Tilburg University   Machine Learning for NLP   Mohit Bansal , University of North Carolina at Chapel Hill and Amazon Alexa AI   Nikolaos Aletras , University of ShefÔ¨Åeld , University of ShefÔ¨Åeld and Amazon   Andre Martins , Instituto Superior T√©cnico and Unbabel   Andreas Vlachos , Facebook and University of Cambridge   Kristina Toutanova , Google   ShaÔ¨Åq Joty , SalesForce and Nanyang Technological University   Machine Translation and Multilinguality   Taro Watanabe , Nara Institute of Science and Technology   Rico Sennrich , University of Zurich and University of Edinburgh   Francisco Guzm√°n , Facebook   Philipp Koehn , Facebook and Johns Hopkins University   Kenneth HeaÔ¨Åeld , The University of Edinburgh   Thamar Solorio , University of Houston   NLP Applications   Joel R. Tetreault , Dataminr   Karin Verspoor , Royal Melbourne Institute of Technology   Jimmy Lin , University of Waterloo   Horacio Saggion , Universitat Pompeu Fabra   Wei Gao , Singapore Management University   Beata Beigman Klebanov , Educational Testing Service   Phonology , Morphology and Word Segmentation   Ryan D Cotterell , ETH Z√ºrich   xviiAlexis Palmer , University of Colorado , Boulder   Question Answering   Mohit Iyyer , University of Massachusetts Amherst   Sanda Harabagiu , University of Texas at Dallas   Alessandro Moschitti , Amazon Alexa AI   Resources and Evaluation   Torsten Zesch , University of Duisburg - Essen   Agata Savary , Universit√© Paris - Saclay   Katrin Erk , University of Texas , Austin   Pablo Gamallo , Universidad de Santiago de Compostela   Bonnie L. Webber , The University of Edinburgh   Semantics : Lexical   Carlos Ramisch , Aix Marseille University   Ekaterina Shutova , University of Amsterdam   Ivan Vuli ¬¥ c , University of Cambridge and PolyAI Limited   Semantics : Sentence - level Semantics , Textual Inference and Other areas   Samuel R. Bowman , New York University   Goran Glava≈° , University of Mannheim   Valeria de Paiva , Topos Institute   Renata Vieira , Universidade de Evora   Wei Lu , Singapore University of Technology and Design   Sentiment Analysis , Stylistic Analysis , and Argument Mining   Yulan He , The university of Warwick   Iryna Gurevych , TU Darmstadt   Roman Klinger , University of Stuttgart   Bing Liu , University of Illinois at Chicago   Special Theme   Emily M. Bender , University of Washington   Laurent Besacier , Naver Labs Europe   Steven Bird , Charles Darwin University and International Computer Science Institute   Speech and Multimodality   Grzegorz Chrupa≈Ça , Tilburg University   Yang Liu , Amazon Alexa AI   Summarization   Kathleen McKeown , Columbia University and Amazon AWS AI Labs   xviiiAnnie Louis , Google Research   Dragomir Radev , Yale University   Syntax : Tagging , Chunking and Parsing   Barbara Plank , IT University of Copenhagen   Joakim Nivre , Uppsala University   Action Editors   Zeljko Agic , Alan Akbik , Md Shad Akhtar , Firoj Alam , Nikolaos Aletras , Malihe Alikhani , Tanel   Alum√§e , Sophia Ananiadou , Antonios Anastasopoulos , Mark Anderson , Jacob Andreas , Xiang   Ao , Marianna Apidianaki , Yuki Arase , Mikel Artetxe , Ehsaneddin Asgari , Giuseppe Attardi   Niranjan Balasubramanian , Timothy Baldwin , Miguel Ballesteros , David Bamman , Mohamad   Hardyman Barawi , Jeremy Barnes , Loic Barrault , Roberto Basili , Ali Basirat , Jasmijn Bastings ,   Daniel Beck , Iz Beltagy , Luciana Benotti , Steven Bethard , Chandra Bhagavatula , Lidong Bing ,   Alexandra Birch , Steven Bird , Yonatan Bisk , Eduardo Blanco , Danushka Bollegala , Antoine Bos-   selut , Florian Boudin , Leonid Boytsov , Chlo√© Braud , Chris Brew , Wray Buntine   Elena Cabrio , Aoife Cahill , Andrew Caines , Ruken Cakici , Marie Candito , Yanan Cao , Ziqiang   Cao , Cornelia Caragea , Xavier Carreras , Paula Carvalho , Andrew Cattle , Daniel Cer , Alessandra   Cervone , Tanmoy Chakraborty , Muthu Kumar Chandrasekaran , Angel X Chang , Kai - Wei Chang ,   Snigdha Chaturvedi , Boxing Chen , Danqi Chen , Kehai Chen , Kuan - Yu Chen , Lei Chen , Yun - Nung   Chen , Colin Cherry , Jackie CK Cheung , Hai Leong Chieu , Luis Chiruzzo , Jinho D. Choi , Monojit   Choudhury , Khalid Choukri , Grzegorz Chrupa≈Ça , Oana Cocarascu , Trevor Cohn , John M Conroy ,   Mathieu Constant , Caio Filippo Corro , Marta Ruiz Costa - juss√† , Stefano Cresci , Aron Culotta   Giovanni Da San Martino , Raj Dabre , Walter Daelemans , Daniel Dakota , Dipanjan Das , Johannes   Daxenberger , Ga√´l De Chalendar , Miryam De Lhoneux , Pascal Denis , Leon Derczynski , Barry   Devereux , Mona T. Diab , Liang Ding , Georgiana Dinu , Jesse Dodge , Li Dong , Ruihai Dong , Yue   Dong , Eduard Dragut , Kevin Duh , Nadir Durrani , Greg Durrett   Liat Ein - Dor , Michael Elhadad , Katrin Erk , Allyson Ettinger   Angela Fan , Anna Feldman , Naomi Feldman , Yang Feng , Yansong Feng , Raquel Fern√°ndez , Fran-   cis Ferraro , Elisabetta Fersini , Simone Filice , Mark Fishel , Annemarie Friedrich , Pascale Fung   Michel Galley , Matthias Gall√© , Zhe Gan , Yang Gao , Marcos Garcia , Sebastian Gehrmann , Al-   borz Geramifard , Debanjan Ghosh , Goran Glava≈° , Kyle Gorman , Jiatao Gu , Qing Gu , Honglei   Guo , Qipeng Guo , Francisco Guzm√°n   Ivan Habernal , Christian Hardmeier , David Harwath , Luheng He , Yulan He , Zhongjun He , Da-   niel Hershcovich , Julia Hockenmaier , Enamul Hoque , Baotian Hu , Junjie Hu , Shujian Huang ,   Xuanjing Huang   Dmitry Ilvovsky , Kentaro Inui , Ozan Irsoy , Srini Iyer , Mohit Iyyer   Cassandra L Jacobs , Alon Jacovi , Kokil Jaidka , Hyeju Jang , Yangfeng Ji , Antonio Jimeno Ye-   pes , ShaÔ¨Åq Joty , Preethi Jyothi   Sarvnaz Karimi , Shubhra Kanti Karmaker , Daisuke Kawahara , Daniel Khashabi , Jin - Dong Kim ,   xixSeokhwan Kim , Taeuk Kim , Judith Lynn Klavans , Roman Klinger , Hayato Kobayashi , Ekaterina   Kochmar , Mamoru Komachi , Grzegorz Kondrak , Parisa Kordjamshidi , Amrith Krishna , Udo Kru-   schwitz , Marco Kuhlmann , Sumeet Kumar , Jonathan K Kummerfeld   Wai Lam , Zhenzhong Lan , Mark Last , Hady W. Lauw , Carolin Lawrence , John Lawrence , Ales-   sandro Lenci , Lori Levin , Omer Levy , Mike Lewis , Jing Li , Junhui Li , Juntao Li , Junyi Jessy Li ,   Liangyou Li , Piji Li , Sujian Li , Tianrui Li , Wenjie Li , Zongxi Li , Constantine Lignos , Chenghua   Lin , Dekang Lin , Marco Lippi , Pengfei Liu , Qun Liu , Wu Liu , Xuebo Liu , Yang Liu , Yang Liu ,   Zhiyuan Liu , Kyle Lo , Wei Lu , Thang Luong , Anh Tuan Luu   Wilson Ma , Craig MacDonald , Nitin Madnani , Andrea Madotto , Navonil Majumder , Prodromos   Malakasiotis , Igor Malioutov , Thomas Mandl , Vukosi Marivate , Eugenio Martinez - Camara , Bru-   no Martins , Yuji Matsumoto , Mausam , David McClosky , Mahnoosh Mehrabani , Ivan Vladimir   Meza Ruiz , Margot Mieskes , Makoto Miwa , Daichi Mochihashi , Saif M. Mohammad , Mohamed   Morchid , David R Mortensen , Alessandro Moschitti , Lili Mou , Philippe Muller , Kenton Murray   Nona Naderi , Courtney Napoles , Shashi Narayan , Franco Maria Nardini , Tristan Naumann , Mark-   Jan Nederhof , Vincent Ng , Dat Quoc Nguyen , Thien Huu Nguyen , Jan Niehues , Qiang Ning   Diarmuid O Seaghdha , Brendan O‚ÄôConnor , Jose Ochoa - Luna , Kemal OÔ¨Çazer , Maciej Ogrodnic-   zuk , Alice Oh , Naoaki Okazaki , Manabu Okumura , Matan Orbach , Miles Osborne , Jessica Ouyang   Hamid Palangi , Ankur P Parikh , Joonsuk Park , Seong - Bae Park , Yannick Parmentier , Tomma-   so Pasini , Rebecca J. Passonneau , Viviana Patti , Haoruo Peng , Nanyun Peng , Gabriele Pergola ,   Fabio Petroni , Maxime Peyrard , Juan Pino , Emily Pitler , Edoardo Ponti , Simone Paolo Ponzetto ,   Kashyap Popat , Maja Popovic , Soujanya Poria , Vinodkumar Prabhakaran , Daniel Preotiuc - Pietro ,   Emily Prud‚Äôhommeaux   Tieyun Qian , Xipeng Qiu , Xiaojun Quan   Colin Raffel , Ganesh Ramakrishnan , Siva Reddy , Ines Rehbein , Roi Reichart , Xiang Ren , Yafeng   Ren , Sebastian Riedel , Sara Rosenthal , Joseph Le Roux , Alla Rozovskaya , Attapol Rutherford   Mrinmaya Sachan , Beno√Æt Sagot , Hassan Sajjad , Chinnadhurai Sankar , Maarten Sap , Nathan   Schneider , Hinrich Schuetze , H. Schwartz , Lane Schwartz , Rico Sennrich , Minjoon Seo , Bei Shi ,   Tianze Shi , Lei Shu , Melanie Siegel , Kevin Small , Noah Smith , Luca Soldaini , Vivek Srikumar ,   Shashank Srivastava , Efstathios Stamatatos , Gabriel Stanovsky , Pontus Stenetorp , Amanda Stent ,   Veselin Stoyanov , Karl Stratos , Emma Strubell , Sara Stymne , Jinsong Su , Yu Su , Saku Sugawara ,   Jun Suzuki   Dima Taji , Zeerak Talat , Duyu Tang , Amalia Todirascu , Antonio Toral , Paolo Torroni , Kristina   Toutanova , Amine Trabelsi , Trang Tran , Chen - Tse Tsai , Junichi Tsujii , Kewei Tu   Stefan Ultes   Olga Vechtomova , Giulia Venturi , Suzan Verberne , Yannick Versley , David Vilares , Serena Vil-   lata , Thuy Vu , Ivan Vuli ¬¥ c , Yogarshi Vyas   Byron C Wallace , Xiaojun Wan , Jingjing Wang , Longyue Wang , Shuai Wang , Xin Eric Wang ,   Zhiguang Wang , Leo Wanner , Shinji Watanabe , Taro Watanabe , Bonnie L. Webber , Zhongyu Wei ,   Michael White , Alina Wr√≥blewska , Lijun Wu   xxTong Xiao , Deyi Xiong , Hainan Xu , Wei Xu   Rui Yan , Min Yang , Jin - Ge Yao , Wenpeng Yin , Koichiro Yoshino , Dian Yu , Jianfei Yu , Kai Yu ,   Mo Yu , Tao Yu , Fran√ßois Yvon   Marcos Zampieri , Fabio Massimo Zanzotto , Luke Zettlemoyer , Justine Zhang , Weinan Zhang ,   Xiangliang Zhang , Xingxing Zhang , Yi Zhang , Yue Zhang , Zhe Zhang , Xiaoqing Zheng , Michael   Zock   ARR reviewers   Micheal Abaho , Ahmed Abdelali , Mostafa Abdou , Muhammad Abdul - Mageed , Omri Abend , Ab-   dalghani Abujabal , Lasha Abzianidze , Manoj Acharya , Heike Adel , David Ifeoluwa Adelani , So-   mak Aditya , Vaibhav Adlakha , Stergos D. Afantenos , Sachin Agarwal , Vibhav Agarwal , Rodrigo   Agerri , Manex Agirrezabal , Ameeta Agrawal , Priyanka Agrawal , Sweta Agrawal , Gustavo Agui-   lar , Roee Aharoni , Wasi Uddin Ahmad , Benyamin Ahmadnia , Aman Ahuja , Chaitanya Ahuja ,   Kabir Ahuja , Xi Ai , Laura Aina , Akiko Aizawa , Alan Akbik , Md Shad Akhtar , Nader Akoury ,   Ekin Aky√ºrek , Ozge Alacam , Firoj Alam , Mehwish Alam , Chris Alberti , Georgios Alexandridis ,   David Alfter , Bashar Alhafni , Raquel G. Alhama , Tariq Alhindi , Hamed Alhoori , Hassan Alhu-   zali , Mohammad Aliannejadi , Afra Alishahi , Tamer Alkhouli , Emily Allaway , Miguel A. Alonso ,   Sawsan Alqahtani , Emily Alsentzer , Milad Alshomary , Christoph Alt , Tanel Alum√§e , Fernando   Alva - Manchego , Rami Aly , Maxime Amblard , Prithviraj Ammanabrolu , Reinald Kim Amplayo ,   Chantal Amrhein , Aixiu An , Guozhen An , Ashish Anand , Sophia Ananiadou , Raviteja Anantha ,   Antonios Anastasopoulos , Carolyn Jane Anderson , Nicholas Andrews , Ion Androutsopoulos , Ga-   bor Angeli , Diego Antognini , Kaveri Anuranjana , Emilia Apostolova , Jun Araki , Rahul Aralikat-   te , Eiji Aramaki , Yuki Arase , Arturo Argueta , Mozhdeh Ariannezhad , Ignacio Arroyo - Fern√°ndez ,   Katya Artemova , Yoav Artzi , Masayuki Asahara , Akari Asai , Meysam Asgari , Elliott Ash , Zheni-   sbek Assylbekov , Duygu Ataman , Dennis Aumiller , Eleftherios Avramidis , Parul Awasthy , Hosein   Azarbonyad , Wilker Aziz   Rohit Babbar , Sanghwan Bae , Ebrahim Bagheri , Dzmitry Bahdanau , Ashutosh Baheti , Fan Bai , He   Bai , Yu Bai , JinYeong Bak , Vidhisha Balachandran , Mithun Balakrishna , Anusha Balakrishnan ,   Niranjan Balasubramanian , Ioana Baldini , Livio Baldini Soares , Kalika Bali , Nicolae Banari , Juan   M Banda , Pratyay Banerjee , Sameer Bansal , Trapit Bansal , Forrest Sheng Bao , Hangbo Bao , Jian-   zhu Bao , Junwei Bao , Siqi Bao , Yu Bao , Zuyi Bao , Ankur Bapna , Roy Bar - Haim , Edoardo Barba ,   Francesco Barbieri , Denilson Barbosa , M Saiful Bari , Ken Barker , Gianni Barlacchi , Jeremy Bar-   nes , Maria Barrett , Valentin Barriere , James Barry , Max Bartolo , Pierpaolo Basile , Valerio Basile ,   Somnath Basu Roy Chowdhury , John A. Bateman , Riza Batista - Navarro , Anil Batra , Khuyagbaa-   tar Batsuren , Daniel Bauer , Timo Baumann , Rachel Bawden , Kathy Baxter , Tilman Beck , Lee   Becker , Lisa Beinborn , Ahmad Beirami , Giannis Bekoulis , N√∫ria Bel , Eric Bell , G√°bor Bella , Me-   riem Beloucif , Iz Beltagy , Eyal Ben - David , Emily M. Bender , Michael Bendersky , Luisa Bentivo-   gli , Adrian Benton , Jonathan Berant , Alexandre Berard , G√°bor Berend , Taylor Berg - Kirkpatrick ,   Toms Bergmanis , Rafael Berlanga , Delphine Bernhard , Dario Bertero , Laurent Besacier , Chandra   Bhagavatula , Rishabh Bhardwaj , Aditya Bhargava , Suma Bhat , Parminder Bhatia , Sumit Bha-   tia , Kasturi Bhattacharjee , Pushpak Bhattacharyya , Satwik Bhattamishra , Shruti Bhosale , Rajarshi   Bhowmik , Bin Bi , Wei Bi , Federico Bianchi , Laura Biester , Yi Bin , Lidong Bing , Philippe Blache ,   Fred Blain , Eduardo Blanco , Terra Blevins , Rexhina Blloshmi , Jelke Bloem , Michael Bloodgood ,   Valts Blukis , Ben Bogin , Nikolay Bogoychev , Ondrej Bojar , Gemma Boleda , Danushka Bollegala ,   Marcel Bollmann , Valeriia Bolotova , Daniele Bonadiman , Francis Bond , Claudia Borg , Mihae-   xxila Bornea , Aur√©lien Bossard , Antoine Bosselut , Robert Bossy , Nadjet Bouayad - Agha , Florian   Boudin , Zied Bouraoui , Samuel R. Bowman , Jordan Lee Boyd - Graber , Johan Boye , Kristy Eliza-   beth Boyer , Faeze Brahman , Arthur Brazinskas , Thomas Brochhagen , Samuel Broscheit , Thomas   Brovelli , Christopher Bryant , Pawe≈Ç Budzianowski , Emanuele Bugliarello , Wray Buntine , Joan   Byamugisha , Bill Byrne   Sky CH - Wang , Subalalitha CN , Elena Cabrio , Avi Caciularu , Samuel Cahyawijaya , Deng Cai , Han   Cai , Hengyi Cai , Jon Cai , Pengshan Cai , Yi Cai , Andrew Caines , Agostina Calabrese , Iacer Calix-   to , Jose Camacho - Collados , Erik Cambria , Oana - Maria Camburu , Giovanni Campagna , Leonardo   Campillos - Llanos , Daniel F Campos , Jon Ander Campos , Marie Candito , Jie Cao , Juan Cao , Kris   Cao , Meng Cao , Qingqing Cao , Qingxing Cao , Ruisheng Cao , Steven Cao , Yixin Cao , Yu Cao ,   Yuan Cao , Yue Cao , Yunbo Cao , Annalina Caputo , Doina Caragea , Dallas Card , Ronald Carde-   nas , R√©mi Cardon , Danilo Carvalho , Tommaso Caselli , Justine Cassell , Vittorio Castelli , Giusep-   pe Castellucci , Thiago Castro Ferreira , Paulo Cavalin , Christophe Cerisara , Alessandra Cervone ,   Arun Tejasvi Chaganty , Soumen Chakrabarti , Abhisek Chakrabarty , Tuhin Chakrabarty , Tanmoy   Chakraborty , Bharathi Raja Chakravarthi , Ilias Chalkidis , Jon Chamberlain , Nathanael Chambers ,   Angel X Chang , Baobao Chang , Haw - Shiuan Chang , Jonathan P. Chang , Serina Chang , Xuankai   Chang , Lidia S. Chao , WenHan Chao , Akshay Chaturvedi , Aditi Chaudhary , Vishrav Chaudha-   ry , Wanxiang Che , Bei Chen , Bo Chen , Chenhua Chen , Chung - Chi Chen , Danqi Chen , Daoyuan   Chen , Guanhua Chen , Guanyi Chen , Hanjie Chen , Hongshen Chen , Howard Chen , Huimin Chen ,   Jiaze Chen , Jifan Chen , John Chen , Kehai Chen , Lei Chen , Lin Chen , Long Chen , Lu Chen ,   Luoxin Chen , Maximillian Chen , Mei - Hua Chen , Meng Chen , Mingda Chen , Minhua Chen , Mu-   hao Chen , Pei Chen , Pinzhen Chen , Qian Chen , Qianglong Chen , Qingcai Chen , Sanxing Chen ,   Shizhan Chen , Tao Chen , Wang Chen , Wei - Fan Chen , Wenhu Chen , Wenliang Chen , Wenqing   Chen , Xilun Chen , Xinchi Chen , Xiusi Chen , Xiuying Chen , Yen - Chun Chen , Yubo Chen , Yue   Chen , Yufeng Chen , Yulong Chen , Yun Chen , Yun - Nung Chen , Yunmo Chen , Zhi Chen , Zhihong   Chen , Zhuang Chen , Zhumin Chen , Zhuohao Chen , Fei Cheng , Hao Cheng , Jianpeng Cheng ,   Liying Cheng , Lu Cheng , Minhao Cheng , Pengxiang Cheng , Pengyu Cheng , Weiwei Cheng , Yong   Cheng , Yu Cheng , Emmanuele Chersoni , Ethan A Chi , Ta - Chung Chi , Zewen Chi , Yew Ken Chia ,   David Chiang , Ting - Rui Chiang , Patricia Chiril , Francisco Javier Chiyah - Garcia , Jaemin Cho ,   Sangwoo Cho , Won Ik Cho , Eleanor Chodroff , Eunsol Choi , Jaesik Choi , Jinho D. Choi , Seung-   taek Choi , Shamil Chollampatt , Jaegul Choo , Leshem Choshen , Prafulla Kumar Choubey , Monojit   Choudhury , Jishnu Ray Chowdhury , Md Faisal Mahbub Chowdhury , Shammur Absar Chowdhury ,   Christos Christodoulopoulos , Fenia Christopoulou , Alexandra Chronopoulou , Chenhui Chu , Chri-   stopher Chu , Zewei Chu , Tat - Seng Chua , Jin - Woo Chung , Yi - Ling Chung , Kenneth Church , Abu   Nowshed Chy , Mark Cieliebak , Manuel Rafael Ciosici , V olkan Cirik , Christopher Clark , Elizabe-   th Clark , Kevin Clark , Miruna Clinciu , Louis Clouatre , Trevor Cohen , Jeremy R. Cole , Marcus   D. Collins , Simone Conia , Mathieu Constant , Danish Contractor , Robin Cooper , Anna Corazza ,   Luciano Del Corro , Ryan D Cotterell , Josep Crego , Danilo Croce , Paul A. Crook , James Cross ,   Ferm√≠n L. Cruz , Heriberto Cuayahuitl , Lei Cui , Leyang Cui , Shaobo Cui , Yiming Cui , Washington   Cunha , Anna Currey , Tonya Custis , Erion √áano   Luis Fernando D‚ÄôHaro , Jennifer D‚ÄôSouza , Giovanni Da San Martino , Raj Dabre , Deborah A.   Dahl , Damai Dai , Falcon Z Dai , Hongliang Dai , Wenliang Dai , Xiang Dai , Xinyu Dai , Yinpei   Dai , Siddharth Dalmia , Sandipan Dandapat , Ankit Dangi , Marina Danilevsky , Verna Dankers ,   Anubrata Das , Rajarshi Das , Sarthak Dash , Pradeep Dasigi , Debajyoti Datta , Hal Daum√© Iii , Sam   Davidson , Brian Davis , Ernest Davis , Ga√´l De Chalendar , Christine De Kock , Kordula De Kuthy ,   Miryam De Lhoneux , Marie - Catherine De Marneffe , Gerard De Melo , Jos√© G. C. De Souza , Iria   De - Dios - Flores , Steve DeNeefe , Alok Debnath , Mathieu Dehouck , Flor Miriam Plaza Del Arco ,   Marco Del Tredici , Agust√≠n D. Delgado , Louise Del√©ger , David Demeter , √áa Àògatay Demiralp , Yang   Deng , Yuntian Deng , Zhongfen Deng , Tejaswini Deoskar , Jan Milan Deriu , Franck Dernoncourt ,   xxiiTim Dettmers , Daniel Deutsch , Sunipa Dev , Joseph Dexter , Kuntal Dey , Bhuwan Dhingra , Luigi   Di Caro , Barbara Di Eugenio , Shizhe Diao , Ga√´l Dias , Chenchen Ding , Haibo Ding , Kaize Ding ,   Liang Ding , Ning Ding , Shuoyang Ding , Xiao Ding , Stefanie Dipper , Nemanja Djuric , Ngoc Bich   Do , Simon Dobnik , Jesse Dodge , Charles Dognin , Miguel Domingo , Lucia Donatelli , Domenic   Donato , Li Dong , MeiXing Dong , Qian Qian Dong , Yue Dong , Bonaventure F. P. Dossou , Longxu   Dou , Zi - Yi Dou , Doug Downey , A. Seza Do Àògru√∂z , Mark Dras , Markus Dreyer , Rotem Dror , An-   drew Drozdov , Jingfei Du , Jinhua Du , Lan Du , Li Du , Mengnan Du , Pan Du , Wanyu Du , Xinya   Du , Yupei Du , Junwen Duan , Xiangyu Duan , Kumar Avinava Dubey , Pablo Duboue , Philipp Duf-   ter , Jonathan Dunn , G√©rard M Dupont , Ondrej Dusek , Ritam Dutt , Subhabrata Dutta , Chris Dyer ,   Nouha Dziri , Herv√© D√©jean   Abteen Ebrahimi , Aleksandra Edwards , Steffen Eger , Markus Egg , Koji Eguchi , Yo Ehara , Vladi-   mir Eidelman , Bryan Eikema , Jacob Eisenstein , Asif Ekbal , Wassim El - Hajj , Aparna Elangovan ,   Yanai Elazar , Heba Elfardy , Michael Elhadad , AbdelRahim A. Elmadany , Micha Elsner , Denis   Emelin , Guy Emerson , Akiko Eriguchi , Liana Ermakova , Patrick Ernst , Carlos Escolano , Arash   Eshghi , Ramy Eskander , Cristina Espa√±a - Bonet , Luis Espinosa - Anke , Kawin Ethayarajh , Allyson   Ettinger , Kilian Evang , Ben Eyal   Alexander Fabbri , Marzieh Fadaee , Tiziano Fagni , Farzane Fakhrian , Neele Falk , Tobias Falke ,   Chuang Fan , Feifan Fan , Kai Fan , Lu Fan , Wei Fang , Yimai Fang , Yuwei Fang , Adam Faulk-   ner , Maryam Fazel - Zarandi , Amir Feder , Hao Fei , Nils Feldhus , Naomi Feldman , Mariano Felice ,   Jiazhan Feng , Shaoxiong Feng , Shi Feng , Shi Feng , Xiachong Feng , Zhangyin Feng , Manos Ferga-   diotis , James Ferguson , Patrick Fernandes , Raquel Fern√°ndez , Daniel Fern√°ndez - Gonz√°lez , Elisa   Ferracane , Francis Ferraro , Besnik Fetahu , Oluwaseyi Feyisetan , Alejandro Figueroa , Simone Fi-   lice , Catherine Finegan - Dollak , Orhan Firat , Nicholas FitzGerald , Margaret M. Fleck , Lucie Flek ,   Antske Fokkens , Marina Fomicheva , Jos√© A.r . Fonollosa , Marco Fonseca , Tommaso Fornacia-   ri , Paula Fortuna , Eric Fosler - Lussier , George Foster , Jennifer Foster , Mary Ellen Foster , Anette   Frank , Stella Frank , Thomas Fran√ßois , Alexander Fraser , Kathleen C. Fraser , Marjorie Freedman ,   Dayne Freitag , Markus Freitag , Lea Frermann , Daniel Fried , Guohong Fu , Jie Fu , Peng Fu , Qia-   nkun Fu , Tsu - Jui Fu , Zuohui Fu , Yoshinari Fujinuma , Atsushi Fujita , Kotaro Funakoshi , Adam   Funk , Richard Futrell , Michael F√§rber   Devi G , Matteo Gabburo , Saadia Gabriel , David Gaddy , Marco Gaido , Andrea Galassi , Mark   Gales , Boris Alexandrovich Galitsky , Ygor Gallina , Diana Galvan , Bj√∂rn Gamb√§ck , Leilei Gan ,   Yujian Gan , Zhe Gan , Kuzman Ganchev , Sudeep Gandhe , Balaji Ganesan , Rashmi Gangadharaiah ,   Varun Gangal , Revanth Gangi Reddy , Debasis Ganguly , Ge Gao , Jun Gao , Shen Gao , Tianyu Gao ,   Wei Gao , Yang Gao , Yanjun Gao , Yifan Gao , Yingbo Gao , Utpal Garain , Cristina Garbacea , Diego   Garcia - Olano , Matt Gardner , Sarthak Garg , Siddhant Garg , Dan Garrette , Aina Gar√≠ Soler , Kiril   Gashteovski , Albert Gatt , Manas Gaur , Eric Gaussier , Dipesh Gautam , Yubin Ge , Sebastian Ge-   hrmann , Michaela Geierhos , Ruiying Geng , Shijie Geng , Xinwei Geng , Xiubo Geng , Ariel Gera ,   Mor Geva , Hamidreza Ghader , Demian Gholipour Ghalandari , Sarik Ghazarian , Mozhdeh Gheini ,   Deepanway Ghosal , Deepanway Ghosal , Debanjan Ghosh , Sayan Ghosh , Soumitra Ghosh , Sou-   rav Ghosh , Daniel Gildea , Salvatore Giorgi , V oula Giouli , Adri√† de Gispert , Mario Giulianelli ,   Michael Glass , Goran Glava≈° , AlÔ¨Åo Gliozzo , Pranav Goel , Vaibhava Goel , Nazli Goharian , Tejas   Gokhale , Elizaveta Goncharova , Heng Gong , Hongyu Gong , Karthik Gopalakrishnan , Philip John   Gorinski , Matthew R. Gormley , Koustava Goswami , Akhilesh Deepak Gotmare , Isao Goto , Cy-   ril Goutte , Edward Gow - Smith , Kartik Goyal , Naman Goyal , Pawan Goyal , Tanya Goyal , Mario   Graff , Christophe Gravier , Yulia Grishina , Milan Gritta , Lo√Øc Grobol , Dagmar Gromann , Roman   Grundkiewicz , Jia - Chen Gu , Jing Gu , Yue Gu , Jian Guan , Saiping Guan , Yi Guan , Marco Guerini ,   Lin Gui , Tao Gui , Vincent Guigue , Liane Guillou , Camille Guinaudeau , Kalpa Gunaratna , Chulaka   Gunasekara , Tunga Gungor , Jiang Guo , Jiaqi Guo , Junliang Guo , Ruocheng Guo , Yinpeng Guo ,   xxiiiYuhang Guo , Zhijiang Guo , Arpit Gupta , Arshit Gupta , Nitish Gupta , Prakhar Gupta , Shashank   Gupta , Sonal Gupta , Vivek Gupta , Izzeddin Gur , Suchin Gururangan , Joakim Gustafson , Ximena   Gutierrez - Vasques , Carlos G√≥mez - Rodr√≠guez   Jung - Woo Ha , Nizar Habash , Ivan Habernal , Kais Haddar , Christian Hadiwinoto , Reza Haf , Mi-   chael Hahn , Zhen Hai , Huda Hakami , Dilek Hakkani - Tur , Kishaloy Halder , Jiale Han , Jiawei Han ,   Namgi Han , Rujun Han , Ting Han , Wenjuan Han , Xianpei Han , Xiaochuang Han , Abram Hand-   ler , Viktor Hangya , Greg Hanneman , Jie Hao , Yaru Hao , Momchil Hardalov , Mareike Hartmann ,   Thomas Hartvigsen , Sadid A. Hasan , Peter Hase , Chikara Hashimoto , Nabil Hathout , Robert D.   Hawkins , Hiroaki Hayashi , Katsuhiko Hayashi , Yoshihiko Hayashi , Shirley Anugrah Hayati , De-   vamanyu Hazarika , Hangfeng He , Jiangen He , Junxian He , Keqing He , Liang He , Luheng He ,   Shizhu He , Tianxing He , Wanwei He , Xuanli He , Zhongjun He , Marti Hearst , Michael Heck ,   Behnam Hedayatnia , Benjamin Heinzerling , Matthew Henderson , Iris Hendrickx , Leonhard Hen-   nig , Sophie Henning , Daniel Hershcovich , Jonathan Herzig , Jack Hessel , John Hewitt , Ryuichiro   Higashinaka , Swapnil Hingmire , Tsutomu Hirao , Tatsuya Hiraoka , Cuong Hoang , Hieu Hoang ,   Johannes Hoffart , Valentin Hofmann , Chris Hokamp , Eben Holderness , Nora Hollenstein , Ari   Holtzman , Takeshi Homma , Ukyo Honda , Pengfei Hong , Mark Hopkins , Helmut Horacek , Md   Mosharaf Hossain , Nabil Hossain , Mohammad Javad Hosseini , Feng Hou , Lei Hou , Yufang Hou ,   Yutai Hou , Dirk Hovy , David M Howcroft , Estevam Hruschka , Shu - Kai Hsieh , Chao - Chun Hsu ,   Chun - Nan Hsu , I - Hung Hsu , Wei - Ning Hsu , Phu Mon Htut , Baotian Hu , Chi Hu , Guangneng   Hu , Huang Hu , Jennifer Hu , Jinyi Hu , Junjie Hu , Linmei Hu , Minghao Hu , Pengwei Hu , Po Hu ,   Renfen Hu , Wei Hu , Yue Hu , Zhe Hu , Ziniu Hu , Xinyu Hua , Yiqing Hua , Chao - Wei Huang ,   Chenyang Huang , Chieh - Yang Huang , Danqing Huang , Fei Huang , Haoran Huang , He Huang ,   Hen - Hsen Huang , Heyan Huang , Jiaji Huang , Jie Huang , Jimmy Huang , Jing Huang , Kuan - Hao   Huang , Kung - Hsiang Huang , Lifu Huang , Minlie Huang , Quzhe Huang , Ruihong Huang , Shujian   Huang , Siyu Huang , Xiaolei Huang , Xinting Huang , Xuancheng Huang , Zhen Huang , Zhongqiang   Huang , Ziming Huang , Patrick Huber , Binyuan Hui , Kai Hui , Dieuwke Hupkes , Ben Hutchinson ,   Jena D. Hwang , Sung Ju Hwang , Ali H√ºrriyeto Àòglu   Ignacio Iacobacci , Georgiana Ifrim , Oana Ignat , Ryu Iida , Gabriel Ilharco , Filip Ilievski , Niko-   lai Ilinykh , Irina Illina , Dmitry Ilvovsky , Kenji Imamura , Oana Inel , Naoya Inoue , Radu Tudor   Ionescu , Daphne Ippolito , Hitoshi Isahara , Tatsuya Ishigaki , Etsuko Ishii , Tunazzina Islam , Haya-   te Iso , Dan Iter , Itay Itzhak , Julia I ve , Tomoya Iwakura , Kenichi Iwatsuki , Rishabh K Iyer , Srini   Iyer , Gautier Izacard   Aaron Jaech , Sarthak Jain , Masoud Jalili Sabet , Abhik Jana , Hyeju Jang , Tommi Jauhiainen , S√©ba-   stien Jean , Sungho Jeon , Minwoo Jeong , Yacine Jernite , Kevin Jesse , Rahul Jha , Harsh Jhamtani ,   Feng Ji , Zongcheng Ji , Chen Jia , Renee Jia , Robin Jia , Ruipeng Jia , Yuxiang Jia , Ping Jian , Daxin   Jiang , Haoming Jiang , Hongfei Jiang , Meng Jiang , Ming Jiang , Nan Jiang , Nanjiang Jiang , Wen-   bin Jiang , Xin Jiang , Yong Jiang , Zhengbao Jiang , Zhuolin Jiang , Zhuoren Jiang , Zhuoxuan Jiang ,   Wenxiang Jiao , Zhanming Jie , Antonio Jimeno Yepes , Di Jin , Hailong Jin , Hanqi Jin , Lisa Jin ,   Peng Jin , Xiaolong Jin , Zhijing Jin , Baoyu Jing , Yohan Jo , Richard Johansson , Melvin Johnson ,   Erik Jones , Gareth J. F. Jones , Siddhartha Jonnalagadda , Aditya Joshi , Dhanya Jothimani , ShaÔ¨Åq   Joty , Xincheng Ju , Jaap Jumelet , Heewoo Jun , David Jurgens , Prathyusha Jwalapuram   Jad Kabbara , Indika Kahanda , Sylvain Kahane , Ivana Kajic , Mihir Kale , Oren Kalinsky , Aikaterini-   Lida Kalouli , Ehsan Kamalloo , Hidetaka Kamigaito , Jaap Kamps , Min - Yen Kan , Hiroshi Kanaya-   ma , Nikhil Kandpal , Masahiro Kaneko , Dongyeop Kang , Minki Kang , Diptesh Kanojia , Evan-   gelos Kanoulas , Jiun - Yu Kao , Pavan Kapanipathi , Georgi Karadzhov , Alina Karakanta , Giannis   Karamanolakis , Siddharth Karamcheti , Mladen Karan , B√∂rje F. Karlsson , Sanjeev Kumar Karn ,   Jungo Kasai , Omid KasheÔ¨Å , Yosuke Kashiwagi , Zden Àáek Kasner , Nora Kassner , Denys Kateren-   xxivchuk , Divyansh Kaushik , Pride Kavumba , Anna Kazantseva , Hideto Kazawa , Ashkan Kazemi ,   Abe Kazemzadeh , Pei Ke , Zixuan Ke , Chris Kedzie , Katherine A. Keith , Yova Kementchedjhieva ,   Brendan Kennedy , Casey Kennington , Tom Kenter , Daniel J Kershaw , Santosh Kesiraju , Salam   Khalifa , Dinesh Khandelwal , Urvashi Khandelwal , Simran Khanuja , Mitesh M Khapra , Eugene   Kharitonov , Daniel Khashabi , Mikhail Khodak , Tushar Khot , Johannes Kiesel , Halil Kilicoglu ,   Byeongchang Kim , Dong - Jin Kim , Dongkwan Kim , Doo Soon Kim , Gene Louis Kim , Geonmin   Kim , Gunhee Kim , Gyuwan Kim , Hyounghun Kim , Hyunwoo Kim , Jihyuk Kim , Jin - Dong Kim ,   Joo - Kyung Kim , Jooyeon Kim , Jung - jae Kim , Juyong Kim , Kang - Min Kim , Seokhwan Kim , Yea-   chan Kim , Yoon Kim , Yunsu Kim , Milton King , Tracy Holloway King , Christo Kirov , Nikita   Kitaev , Hirokazu Kiyomaru , Shun Kiyono , Judith Lynn Klavans , Ayal Klein , Bennett Kleinberg ,   Jan - Christoph Klie , Mateusz Klimaszewski , Miyoung Ko , Hideo Kobayashi , Sosuke Kobayashi ,   Thomas H Kober , Jordan Kodner , Svetla Peneva Koeva , Mare Koit , Noriyuki Kojima , Alexander   Koller , Keshav Kolluru , Mamoru Komachi , Rik Koncel - Kedziorski , Lingkai Kong , Luyang Kong ,   Valia Kordoni , Yuta Koreeda , Mandy Barrett Korpusik , Katsunori Kotani , Lili Kotlerman , Fajri   Koto , Venelin Kovatchev , Josip Krapac , Sebastian Krause , Elisa Kreiss , Ralf Krestel , Julia Kreut-   zer , Florian L. Kreyssig , Kalpesh Krishna , Nikhil Krishnaswamy , Reno Kriz , Canasai Kruengkrai ,   Udo Kruschwitz , Germ√†n Kruszewski , Alexander Ku , Lun - Wei Ku , Marco Kuhlmann , Mayank   Kulkarni , Sayali Kulkarni , Vivek Kulkarni , Artur Kulmizev , Devang Kulshreshtha , Ashutosh Ku-   mar , Dhruv Kumar , Sachin Kumar , Sawan Kumar , Shankar Kumar , Varun Kumar , Vishwajeet   Kumar , Anoop Kunchukuttan , Souvik Kundu , Shuhei Kurita , Kemal Kurniawan , Sadao Kuroha-   shi , Robin Kurtz , Andrey Kutuzov   Peifeng LI , Matthieu Labeau , Faisal Ladhak , Nikolaos Lagos , Cheng - I Lai , Viet Dac Lai , Yuxuan   Lai , Yash Kumar Lal , Divesh Lala , John P. Lalor , Tsz Kin Lam , Wai Lam , Matthew Lamm , Vasi-   leios Lampos , Gerasimos Lampouras , Man Lan , Yanyan Lan , Yunshi Lan , Lukas Lange , Ni Lao ,   Guy Lapalme , Egoitz Laparra , Mirella Lapata , Gabriella Lapesa , Ekaterina Lapshinova - Koltunski ,   Stefan Larson , Jey Han Lau , Anne Lauscher , Alberto Lavelli , John Lawrence , Dawn Lawrie , Hung   Le , Phong Le , Andrew Lee , Dongkyu Lee , Fei - Tzin Lee , Hung - yi Lee , Hwanhee Lee , Hwaran Lee ,   Hyunju Lee , I - Ta Lee , Ji - Ung Lee , Jinhyuk Lee , Katherine Lee , Kenton Lee , Kyungjae Lee , Mina   Lee , Moontae Lee , Nayeon Lee , Roy Ka - Wei Lee , Sang - Woo Lee , Seolhwa Lee , Taesung Lee ,   Young - Suk Lee , Artuur Leeuwenberg , Els Lefever , Jie Lei , Tao Lei , Wenqiang Lei , Zeyang Lei ,   Jochen L. Leidner , Alessandro Lenci , Yichong Leng , Haley Lepp , Piyawat Lertvittayakumjorn ,   Guy Lev , Lori Levin , Gina - Anne Levow , Bai Li , Baoli Li , Bei Li , Binyang Li , Bo Li , Bowen Li ,   Chen Li , Chen Li , Chenliang Li , Chenliang Li , Chunyuan Li , Dianqi Li , Dingcheng Li , Dong-   fang Li , Fei Li , Haizhou Li , Haoran Li , Hongyu Li , Huayang Li , Irene Li , Jialu Li , Jicheng Li ,   Jinchao Li , Jing Li , Jing Li , Jingye Li , Juanzi Li , Juncheng B Li , Junyi Jessy Li , Lei Li , Lin Li ,   Lucy Li , Manling Li , Miao Li , Minglei Li , Peng Li , Piji Li , Qi Li , Quanzhi Li , Ruizhe Li , Shang-   Wen Li , Shaohua Li , Sheng Li , Shuangyin Li , Si Li , Tao Li , Wei Li , Xiang Lisa Li , Xiang Li ,   Xiang Lorraine Li , Xiangci Li , Xiaonan Li , Xin Li , Xinjian Li , Xintong Li , Xiujun Li , Yaliang   Li , Yanran Li , Yanzeng Li , Yaoyiran Li , Yingjie Li , Yingya Li , Yinqiao Li , Yitong Li , Yitong   Li , Yiyuan Li , Yuan - Fang Li , Zhenghua Li , Zhongli Li , Zhongyang Li , Zhoujun Li , Zichao Li ,   Zongxi Li , Zuchao Li , Bin Liang , Chao - Chun Liang , Chen Liang , Paul Pu Liang , Xiaobo Liang ,   Yunlong Liang , Lizi Liao , Jind Àárich Libovick√Ω , Chaya Liebeskind , Wang Lijie , Gilbert Lim , Kwan   Hui Lim , Bill Yuchen Lin , Chih - Jen Lin , Chu - Cheng Lin , Hongfei Lin , Junyang Lin , Lucy H. Lin ,   Peiqin Lin , Ting - En Lin , Weizhe Lin , Xi Victoria Lin , Xiang Lin , Yankai Lin , Ying Lin , Zehao   Lin , Zhaojiang Lin , Zheng Lin , Zhouhan Lin , Zi Lin , Zhen - Hua Ling , Tal Linzen , Pierre Lison ,   Johann - Mattis List , Robert Litschko , Patrick William Littell , Marina Litvak , Bin Liu , Bing Liu ,   Chen Cecilia Liu , Chi - Liang Liu , Dairui Liu , Danni Liu , Dexi Liu , Fangyu Liu , Fei Liu , Feifan   Liu , Han Liu , Haochen Liu , Haokun Liu , Haoyan Liu , Hui Liu , Jiachang Liu , Jian Liu , Jiangming   Liu , Jing Liu , Junhao Liu , Kang Liu , Lemao Liu , Ling Liu , Linqing Liu , Liyuan Liu , Maofu Liu ,   Ming Liu , Nelson F. Liu , Peng Liu , Qian Liu , Qianchu Liu , Shujie Liu , Shulin Liu , Siyang Liu ,   xxvTianyu Liu , Weijie Liu , Xianggen Liu , Xiao Liu , Xiao Liu , Ximeng Liu , Xin Liu , Xinchen Liu ,   Xudong Liu , Xuebo Liu , Xueqing Liu , Yang Janet Liu , Yijia Liu , Yijin Liu , Yingchi Liu , Yong   Liu , Zemin Liu , Zhengyuan Liu , Zhiyuan Liu , Zhun Liu , Zihan Liu , Zoey Liu , Farhana Ferdousi   Liza , Nikola Ljube≈°i ¬¥ c , Kyle Lo , Guodong Long , Yunfei Long , Jos√© David Lopes , Lucelene Lopes ,   Natalia Loukachevitch , Ismini Lourentzou , Sharid Lo√°iciga , Di Lu , Wei Lu , Weiming Lu , Yao Lu ,   Yaojie Lu , Yichao Lu , Nurul Lubis , Stephanie M. Lukin , Hongyin Luo , Huaishao Luo , Ling Luo ,   Ping Luo , Renqian Luo , Ruotian Luo , Tianyi Luo , Anh Tuan Luu , Kelvin Luu , Shangwen Lv , Xin   Lv , Teresa Lynn , Chenyang Lyu , Michael Lyu , Samuel L√§ubli   Ji Ma , Jianqiang Ma , Kaixin Ma , Mingyu Derek Ma , Qianli Ma , Xiaofei Ma , Xinyin Ma , Xuezhe   Ma , Xutai Ma , Sean MacAvaney , Aman Madaan , Avinash Madasu , Mounica Maddela , Pranava   Madhyastha , Andrea Madotto , Walid Magdy , Manuel Mager , Suchismit Mahapatra , Adyasha Ma-   harana , Debanjan Mahata , Rahmad Mahendra , Ayush Maheshwari , Gaurav Maheshwari , Kyle Ma-   howald , Wolfgang Maier , Jean Maillard , Olga Majewska , Bodhisattwa Prasad Majumder , M√°rton   Makrai , Prodromos Malakasiotis , Chaitanya Malaviya , Andreas Maletti , Ankur Mali , Eric Malmi ,   Christopher Malon , Radhika Mamidi , Saab Mansour , Ramesh Manuvinakurike , Emaad Manzoor ,   Jiaxin Mao , Wenji Mao , Xian - Ling Mao , Xin Mao , Yuren Mao , Vladislav Maraev , Ana Maraso-   vic , Diego Marcheggiani , Daniel Marcu , Piotr Mardziel , Andreas Marfurt , Katerina Margatina ,   Benjamin Marie , Zita Marinho , Antonis Maronikolakis , Edison Marrese - Taylor , Hector Martinez   Alonso , Pedro Henrique Martins , Yuval Marton , Sameen Maruf , Claudia Marzi , Sandeep Mathias ,   Prashant Mathur , Puneet Mathur , David Martins De Matos , S√©rgio Matos , Yuichiroh Matsubaya-   shi , Takuya Matsuzaki , Yevgen Matusevych , Evgeny Matusov , Kaushal Kumar Maurya , Nickil   Maveli , Jonathan May , Stephen Mayhew , Karen Mazidi , Sahisnu Mazumder , Arya D. McCarthy ,   John Philip McCrae , Matthew B.a . McDermott , Denis Jered McInerney , Alexander Mehler , Shikib   Mehri , Nikhil Mehta , Hongyuan Mei , Hardik Meisheri , Clara Isabel Meister , Dheeraj Mekala , Tel-   mo Menezes , Fandong Meng , Rui Meng , Tao Meng , Yu Meng , Yuanliang Meng , Zaiqiao Meng ,   Zhao Meng , Rakesh R Menon , Samuel Mensah , Wolfgang Menzel , Paola Merlo , William Merrill ,   Mohsen Mesgar , Florian Metze , Donald Metzler , Marie - Jean Meurs , Haitao Mi , Yisong Miao , Ju-   lian Michael , Paul Michel , Lesly Miculicich , Sabrina J Mielke , Margot Mieskes , Todor Mihaylov ,   Tsvetomila Mihaylova , Elena Mikhalkova , Simon Mille , Timothy A Miller , Tristan Miller , Eleni   Miltsakaki , David Mimno , Bonan Min , Sewon Min , Pasquale Minervini , Xu Mingzhou , Hideya   Mino , Shachar Mirkin , Seyedabolghasem Mirroshandel , Paramita Mirza , Abhijit Mishra , Swa-   roop Mishra , Kanishka Misra , Masato Mita , Prasenjit Mitra , Jelena Mitrovi ¬¥ c , Arpit Mittal , Vibhu   O. Mittal , Makoto Miwa , Yusuke Miyao , Takashi Miyazaki , Daichi Mochihashi , Ashutosh Modi ,   Hans Moen , Aditya Mogadala , Nikita Moghe , Alireza Mohammadshahi , Muqeeth Mohammed ,   Hosein Mohebbi , Diego Molla , Natawut Monaikul , Nicholas Monath , Ishani Mondal , Joel Ruben   Antony Moniz , Syrielle Montariol , Manuel Montes , Seungwhan Moon , Ray Mooney , NaÔ¨Åse Sadat   Moosavi , Mehrad Moradshahi , Vlad I Morariu , Erwan Moreau , Jose G Moreno , Mathieu Morey ,   Gaku Morio , Makoto Morishita , John Xavier Morris , David R Mortensen , Ahmadreza Mosallane-   zhad , Marius Mosbach , Lili Mou , Xiangyang Mou , Seyed Mahed Mousavi , Maximilian Mozes ,   Yassine Mrabet , Frank Martin Mtumbuka , Hamdy Mubarak , Pramod Kaushik Mudrakarta , Aaron   Mueller , David Mueller , Matteo Muffo , Animesh Mukherjee , Phoebe Mulcaire , Matthew Mulhol-   land , Deepak Muralidharan , Masayasu Muraoka , Elena Musi , Sheshera Mysore , Mark - Christoph   M√ºller , Mathias M√ºller , Thomas M√ºller   Seung - Hoon Na , Nona Naderi , Masaaki Nagata , Ajay Nagesh , Saeed NajaÔ¨Å , Tetsuji Nakagawa ,   Diane Napolitano , Jason Naradowsky , Karthik R Narasimhan , Tahira Naseem , Sudip Kumar Na-   skar , Alexis Nasr , Vivi Nastase , Anandhavelu Natarajan , Tristan Naumann , Roberto Navigli , Mat-   teo Negri , Graham Neubig , G√ºnter Neumann , Mariana Neves , Denis Newman - GrifÔ¨Ås , Dai Quoc   Nguyen , Hoang Van Nguyen , Huyen Nguyen , Thanh V Nguyen , Thanh - Tung Nguyen , Thien Huu   Nguyen , Truc - Vien T. Nguyen , Hoang - Quoc Nguyen - Son , Jianmo Ni , Garrett Nicolai , Massimo   xxviNicosia , Vlad Niculae , Feng Nie , Yixin Nie , Jan Niehues , Christina Niklaus , Fedor Nikolaev ,   Giannis Nikolentzos , Vassilina Nikoulina , Qiang Ning , Takashi Ninomiya , Nobal B. Niraula , Ko-   suke Nishida , Kyosuke Nishida , Noriki Nishida , Masaaki Nishino , Sergiu Nisioi , Guanglin Niu ,   Tong Niu , Xing Niu , Hiroshi Noji , Tadashi Nomoto , Damien Nouvel , Michal Nov√°k , Pierre Nu-   gues , Claire N√©dellec , Aur√©lie N√©v√©ol   Alexander O‚ÄôConnor , Yusuke Oda , Stephan Oepen , Maciej Ogrodniczuk , Barlas Oguz , Alice Oh ,   Yoo Rhee Oh , Kiyonori Ohtake , Naoaki Okazaki , Tsuyoshi Okita , Manabu Okumura , Hugo Go-   n√ßalo Oliveira , Antoni Oliver , Arturo Oncevay , Yasumasa Onoe , Juri Opitz , Shereen Oraby , John   Ortega , Pedro Ortiz Suarez , Yohei Oseki , Malte Ostendorff , Naoki Otani , Myle Ott , Zhijian Ou ,   Zijing Ou , Hiroki Ouchi , Nedjma Ousidhoum , Robert √ñstling , Lilja √òvrelid   Maria Leonor Pacheco , Inkit Padhi , Aishwarya Padmakumar , Santanu Pal , Sukomal Pal , Chester   Palen - Michel , Alexis Palmer , Endang Wahyu Pamungkas , Boyuan Pan , Liangming Pan , Liang   Pang , Richard Yuanzhe Pang , Sheena Panthaplackel , Alexandros Papangelis , Nikolaos Pappas ,   Emerson Cabrera Paraiso , Letitia Parcalabescu , Natalie Parde , Antonio Pareja - Lora , Cecile Pa-   ris , ChaeHun Park , Chanjun Park , Hyunji Hayley Park , Jungsoo Park , Kunwoo Park , Lucy Park ,   Youngja Park , Ioannis Partalas , Niko Tapio Partanen , Prasanna Parthasarathi , Md Rizwan Parvez ,   Gabriella Pasi , Tommaso Pasini , Ramakanth Pasunuru , Or Patashnik , Arkil Patel , Kevin Patel , Raj   Patel , Roma Patel , Sangameshwar Patil , Barun Patra , Braja Patra , Jasabanta Patro , Manasi Patwar-   dhan , Siddharth Patwardhan , Debjit Paul , Silviu Paun , John Pavlopoulos , Pavel Pecina , Jiaxin Pei ,   Stephan Peitz , Viktor Pekar , Baolin Peng , Hao Peng , Haoruo Peng , Siyao Peng , Wei Peng , Xi   Peng , Xutan Peng , Yifan Peng , Lis Pereira , Martin Pereira , Julien Perez , Gabriele Pergola , Jan-   Thorsten Peter , Ben Peters , Matthew E Peters , Pavel Petrushkov , Sandro Pezzelle , Jonas Pfeiffer ,   Minh - Quang Pham , Quan Pham , Van - Thuy Phi , Maciej Piasecki , Massimo Piccardi , Karl Pichot-   ta , Mohammad Taher Pilehvar , Tiago Pimentel , Aidan Pine , Juan Pino , Yuval Pinter , Flammie A   Pirinen , Benjamin Piwowarski , Lonneke Van Der Plas , Bryan A. Plummer , Brian Pl√ºss , Sylvain   Pogodalla , Martin Popel , Octavian Popescu , Andrei Popescu - Belis , Fred Popowich , Fran√ßois Por-   tet , Matt Post , Martin Potthast , Christopher Potts , Amir Pouran Ben Veyseh , Sandhya Prabhaka-   ran , Vinodkumar Prabhakaran , Shrimai Prabhumoye , Aniket Pramanick , Jakob Prange , Animesh   Prasad , Archiki Prasad , Judita Preiss , Audi Primadhanty , Victor Prokhorov , Prokopis Prokopidis ,   Haritz Puerto , Rajkumar Pujari , Matthew Purver , Valentina Pyatkin , Juan Antonio P√©rez - Ortiz   Fanchao Qi , Jianzhong Qi , Peng Qi , Tao Qi , Dong Qian , Kun Qian , Yujie Qian , Libo Qin , Yu-   jia Qin , Liang Qiu , Long Qiu , Xipeng Qiu , Chen Qu , Lizhen Qu , Xiaoye Qu   Ella Rabinovich , Gorjan Radevski , Alessandro Raganato , Dinesh Raghu , Vipul Raheja , Afshin Ra-   himi , Hossein Rajaby Faghihi , Sara Rajaee , Dheeraj Rajagopal , Sanguthevar Rajasekaran , Pavithra   Rajendran , Geetanjali Rakshit , Dhananjay Ram , Ori Ram , Taraka Rama , Deepak Ramachandran ,   Anil Ramakrishna , Ganesh Ramakrishnan , Owen Rambow , Alan Ramponi , Gabriela Ram√≠rez De   La Rosa , Tharindu Ranasinghe , Surangika Ranathunga , Priya Rani , Peter A. Rankel , Jinfeng Rao ,   Yanghui Rao , Ahmad Rashid , Hannah Rashkin , Abhinav Rastogi , Vipul Kumar Rathore , Vikas   Raunak , Shauli Ravfogel , Abhilasha Ravichander , Vinit Ravishankar , Anirudh Ravula , Avik Ray ,   Soumya Ray , Manny Rayner , Julia Rayz , Traian Rebedea , Sravana Reddy , Hanumant Harichandra   Redkar , Georg Rehm , Marek Rei , Nils Reimers , Navid Rekabsaz , Da Ren , Feiliang Ren , Feiliang   Ren , Pengjie Ren , Ruiyang Ren , Shuhuai Ren , Shuo Ren , Xiang Ren , Xuancheng Ren , Zhaochun   Ren , Adi Renduchintala , Mehdi Rezagholizadeh , Saed Rezayi , Leonardo F. R. Ribeiro , Caitlin   Laura Richter , Sebastian Riedel , Stefan Riezler , German Rigau , Shruti Rijhwani , Mat ¬Øƒ±ss Rikters ,   Darcey Riley , Laura Rimell , Eric Ringger , Annette Rios , Anthony Rios , Miguel Rios , Brian Roa-   rk , Kirk Roberts , Christophe Rodrigues , Pedro Rodriguez , Melissa Roemmele , Lina Maria Rojas-   Barahona , Roland Roller , Stephen Roller , Alexey Romanov , Salvatore Romeo , Srikanth Ronanki ,   xxviiSubendhu Rongali , Rudolf Rosa , Aiala Ros√° , Michael Roth , Sascha Rothe , Salim Roukos , Dmitri   Roussinov , Bryan R. Routledge , Aurko Roy , Subhro Roy , Jos Rozen , Alla Rozovskaya , Dongyu   Ru , Raphael Rubino , Sebastian Ruder , Koustav Rudra , Frank Rudzicz , Federico Ruggeri , Thomas   Ruprecht , Alexander M Rush , Irene Russo , Phillip Rust , Attapol Rutherford , Max Ryabinin , Maria   Ryskina , Andreas R√ºckl√©   C S , Ashish Sabharwal , Mrinmaya Sachan , Fatiha Sadat , Arka Sadhu , Marzieh Saeidi , Niloofar   SaÔ¨Å Samghabadi , Kenji Sagae , Horacio Saggion , Monjoy Saha , Swarnadeep Saha , Tulika Saha ,   Saurav Sahay , Gaurav Sahu , Sunil Kumar Sahu , Hassan Sajjad , Keisuke Sakaguchi , Sakriani Sa-   kti , Elizabeth Salesky , Alexandre Salle , Avneesh Saluja , Tanja Samardzic , Younes Samih , Danae   Sanchez Villegas , Chinnadhurai Sankar , Malaikannan Sankarasubbu , Sashank Santhanam , Ma-   rina Santini , Bishal Santra , Sebastin Santy , Maarten Sap , Naomi Saphra , Maya Sappelli , Zahra   Sarabi , Sheikh Muhammad Sarwar , Felix Sasaki , Shota Sasaki , Ryohei Sasano , Giorgio Satta ,   Danielle Saunders , Agata Savary , Aleksandar Savkov , Beatrice Savoldi , Apoorv Umang Saxena ,   Asad B. Sayeed , Thomas Schaaf , Shigehiko Schamoni , Tatjana SchefÔ¨Çer , Christian Scheible , Yves   Scherrer , Timo Schick , Marten Van Schijndel , Frank Schilder , Viktor Schlegel , Jonathan Schler ,   Helmut Schmid , Tyler Schnoebelen , Steven Schockaert , Alexandra SchoÔ¨Åeld , Sabine Schulte I m   Walde , Claudia Schulz , Hannes Schulz , Elliot Schumacher , Anne - Kathrin Schumann , Sebastian   Schuster , Tal Schuster , Roy Schwartz , Robert Schwarzenberg , Stefan Schweter , Johannes Sch√§fer ,   Djam√© Seddah , Jo√£o Sedoc , Satoshi Sekine , David Semedo , Nasredine Semmar , Sina Semnani ,   L√ºtÔ¨Å Kerem Senel , Rico Sennrich , Minjoon Seo , Yeon Seonwoo , Christophe Servan , Lei Sha , Iz-   hak Shafran , Darsh Jaidip Shah , Kashif Shah , Samira Shaikh , Cory Shain , Chao Shang , Guokan   Shang , Jingbo Shang , Mingyue Shang , Chenze Shao , Nan Shao , Yutong Shao , Zhihong Shao , Ori   Shapira , Naomi Tachikawa Shapiro , Amr Sharaf , Arpit Sharma , Ashish Sharma , Vasu Sharma ,   Serge Sharoff , Rebecca Sharp , Hassan Shavarani , Peter Shaw , Qiaoqiao She , Zaid Sheikh , Artem   Shelmanov , Hua Shen , Jiaming Shen , Lei Shen , Qinlan Shen , Sheng Shen , Shiqi Shen , Tao Shen ,   Xiaoyu Shen , Yikang Shen , Yilin Shen , Yongliang Shen , Emily Sheng , Qiang Sheng , Tom Sher-   borne , Chuan Shi , Freda Shi , Jiatong Shi , Jiaxin Shi , Ning Shi , Peng Shi , Shuming Shi , Tianze   Shi , Weijia Shi , Weiyan Shi , Xing Shi , Yangyang Shi , Zhouxing Shi , Tomohide Shibata , Nobuyuki   Shimizu , Anastasia Shimorina , Jamin Shin , Yow - Ting Shiue , Boaz Shmueli , Eyal Shnarch , Lin-   jun Shou , Mohit Shridhar , Akshat Shrivastava , Manish Shrivastava , Kai Shu , Lei Shu , Raphael   Shu , Kurt Shuster , Vered Shwartz , Chenglei Si , Mei Si , Aditya Siddhant , A.b . Siddique , Carina   Silberer , Miikka Silfverberg , Khalil Sima‚Äôan , Patrick Simianer , Kathleen Siminyu , Arabella Jane   Sinclair , Sameer Singh , Karan Singla , Koustuv Sinha , Kairit Sirts , Amy Siu , Milena Slavcheva ,   Noam Slonim , David A. Smith , Felipe Soares , Christine Soh , Haoyu Song , Hyun - Je Song , Kai   Song , Kaiqiang Song , Linfeng Song , Mingyang Song , Ruihua Song , Wei Song , Xingyi Song , Yi-   ping Song , Sandeep Soni , Rishi Sonthalia , Claudia Soria , Alexey Sorokin , Daniil Sorokin , William   Eduardo Soto Martinez , Sajad Sotudeh , Marlo Souza , Lucia Specia , Matthias Sperber , Vivek Sri-   kumar , Balaji Vasan Srinivasan , Tejas Srinivasan , Shashank Srivastava , Edward P. Stabler , Felix   Stahlberg , Ieva Staliunaite , Marija Stanojevic , Gabriel Stanovsky , David Stap , Katherine Stasa-   ski , Manfred Stede , Mark Steedman , Benno Stein , Shane Steinert - Threlkeld , Elias Stengel - Eskin ,   Amanda Stent , Mark Stevenson , Ian Stewart , Matthew Stone , Kevin Stowe , Karl Stratos , Kristina   Striegnitz , Heiner Stuckenschmidt , Nikolaos Stylianou , Sara Stymne , Dan Su , Hui Su , Jinsong Su ,   Keh - Yih Su , Shang - Yu Su , Weifeng Su , Yu Su , Yusheng Su , Nishant Subramani , Lakshmi Sub-   ramanian , Sanjay Subramanian , Katsuhito Sudoh , Saku Sugawara , Hiroaki Sugiyama , Alessandro   Suglia , Yoshihiko Suhara , Dianbo Sui , Zhifang Sui , Elior Sulem , Md Arafat Sultan , Changzhi   Sun , Chengjie Sun , Fei Sun , Haipeng Sun , Haitian Sun , Huan Sun , Jian Sun , Jingyi Sun , Kai Sun ,   Kai Sun , Ming Sun , Mingming Sun , Si Sun , Simeng Sun , Siqi Sun , Tianxiang Sun , Yawei Sun ,   Yibo Sun , Yifan Sun , Yu Sun , Zequn Sun , Zhiqing Sun , Dhanasekar Sundararaman , Mujeen Sung ,   Hanna Suominen , Mihai Surdeanu , Anshuman Suri , Shiv Surya , Simon Suster , Mirac Suzgun , Jun   Suzuki , Masatoshi Suzuki , Swabha Swayamdipta , Benjamin Sznajder , Stan Szpakowicz , Felipe   xxviiiS√°nchez - Mart√≠nez , G√∂zde G√ºl ¬∏ Sahin   Ryuki Tachibana , Oyvind Tafjord , Shabnam Tafreshi , Hiroya Takamura , Ryuichi Takanobu , Sho   Takase , Ece Takmaz , Aarne Talman , Derek Tam , George Tambouratzis , Fabio Tamburini , Akihiro   Tamura , Chuanqi Tan , Fei Tan , Liling Tan , Samson Tan , Xu Tan , Zeqi Tan , Kumiko Tanaka - Ishii ,   Buzhou Tang , Gongbo Tang , Hao Tang , Qingming Tang , Raphael Tang , Shuai Tang , Siliang Tang ,   Yi - Kun Tang , Zhiwen Tang , Ludovic Tanguy , Xavier Tannier , Chongyang Tao , Shiva Taslimipoor ,   Sandeep Tata , Yuka Tateisi , Michiaki Tatsubori , Marta Tatu , Hillel Taub - Tabib , Yi Tay , Andon   Tchechmedjiev , Christoph Teichmann , Selma Tekir , Serra Sinem Tekiroglu , Eric S. Tellez , Irina   Temnikova , Zhiyang Teng , Ian Tenney , Hiroki Teranishi , Silvia Terragni , Alberto Testoni , Nithum   Thain , Khushboo Thaker , Urmish Thakker , Nandan Thakur , Kilian Theil , Jesse Thomason , Laure   Thompson , Sam Thomson , Camilo Thorne , James Thorne , Junfeng Tian , Ran Tian , Yingtao Tian ,   Zhiliang Tian , J√∂rg Tiedemann , Tiago Timponi Torrent , Erik Tjong Kim Sang , Gaurav Singh To-   mar , Nadi Tomeh , Nicholas Tomlin , Sara Tonelli , Mariya Toneva , MeiHan Tong , Antonio Toral ,   Kentaro Torisawa , Samia Touileb , Julien Tourille , Quan Hung Tran , Dietrich Trautmann , Marcos   Vinicius Treviso , Hai - Long Trieu , Alina Trifan , Enrica Troiano , Tuan Quoc Truong , Chen - Tse   Tsai , Bo - Hsiang Tseng , Masaaki Tsuchida , Yoshimasa Tsuruoka , Kewei Tu , Lifu Tu , Mei Tu ,   Zhaopeng Tu , Iulia Raluca Turc , Martin Tutek , Francis M. Tyers , Andre T√§ttar   Rutuja Ubale , Ana Sabina Uban , Takuma Udagawa , Umair Ul Hassan , Stefan Ultes , Shyam Upa-   dhyay , L. Alfonso Ure√±a , Ricardo Usbeck   Keyon Vafa , Sowmya Vajjala , Jannis Vamvas , Tim Van De Cruys , Benjamin Van Durme , Emiel   Van Miltenburg , Rik Van Noord , Keith N VanderLinden , Lucy Vanderwende , David Vandyke , Na-   talia Vanetik , Daniel Varab , Siddharth Varia , Lucy Vasserman , Julien Velcin , Alakananda Vempala ,   Sriram Venkatapathy , Giulia Venturi , Suzan Verberne , Gaurav Verma , Rakesh M Verma , Giorgos   Vernikos , Yannick Versley , Karin Verspoor , Anvesh Rao Vijjini , David Vilar , Jes√∫s Vilares , Serena   Villata , Aline Villavicencio , √âric Villemonte De La Clergerie , Veronika Vincze , Krishnapriya Vi-   shnubhotla , Ngoc Phuoc An V o , Rob V oigt , Elena V oita , Soroush V osoughi , Thang Vu , Thuy Vu ,   Thuy - Trang Vu , Tu Vu , Xuan - Son Vu , Yogarshi Vyas , Ekaterina Vylomova   Henning Wachsmuth , Takashi Wada , Joachim Wagner , Byron C Wallace , Mengting Wan , Mingyu   Wan , Stephen Wan , Yao Wan , Yu Wan , Alex Wang , Bailin Wang , Baoxin Wang , Baoxun Wang ,   Bin Wang , Bingqing Wang , Boxin Wang , Changhan Wang , Chao Wang , Chenguang Wang , Chen-   gyu Wang , Cunxiang Wang , Daling Wang , Dingmin Wang , Fei Wang , Guangrun Wang , Guoyin   Wang , Hai Wang , Han Wang , Hanrui Wang , Hao Wang , Hao Wang , Haohan Wang , Haoyu Wang ,   Hong Wang , Hongfei Wang , Hua Wang , Jin Wang , Jin Wang , Jingang Wang , Jingkang Wang ,   Jue Wang , Ke Wang , Liang Wang , Lidan Wang , Lingzhi Wang , Liwen Wang , Lucy Lu Wang ,   Ping Wang , Pinghui Wang , Qiang Wang , Qifan Wang , Qingyun Wang , Quan Wang , Rui Wang ,   Rui Wang , Runze Wang , Shaonan Wang , Shi Wang , Shuo Wang , Shuohang Wang , Sinong Wang ,   Tong Wang , Tong Wang , Wei Wang , Wei Wang , Weiyue Wang , Wen Wang , Wenbo Wang , Wenhui   Wang , Wenya Wang , Xiaojie Wang , Xiaolin Wang , Xiaozhi Wang , Xin Wang , Xing Wang , Xi-   nyi Wang , Xuezhi Wang , Yan Wang , Yaqing Wang , Yequan Wang , Yifei Wang , Yijue Wang , Yile   Wang , Yingyao Wang , Yiran Wang , Yizhong Wang , Yong Wang , Yue Wang , Yue Wang , Yujing   Wang , Zhen Wang , Zhichun Wang , Zhongqing Wang , Zijian Wang , Ziqi Wang , Zirui Wang , Leo   Wanner , Nigel G. Ward , Alex Warstadt , Christian Wartena , Koki Washio , Ingmar Weber , Leon   Weber , Noah Weber , Kellie Webster , Julie Weeds , Jason Wei , Johnny Wei , Junqiu Wei , Penghui   Wei , Wei Wei , Xiangpeng Wei , Xiaochi Wei , Shira Wein , David Weir , Ralph M. Weischedel , Char-   les Welch , Orion Weller , Haoyang Wen , Lijie Wen , Rongxiang Weng , Peter West , Taesun Whang ,   Michael White , Michael Wiegand , Sarah Wiegreffe , Adam Wiemerslage , Derry Wijaya , Gijs Wijn-   holds , Ethan Wilcox , Rodrigo Wilkens , Jake Ryland Williams , Jennifer Williams , Shomir Wilson ,   xxixSteven R. Wilson , Genta Indra Winata , Shuly Wintner , Sam Wiseman , Guillaume Wisniewski ,   Magdalena Wolska , Derek F. Wong , Tak - Lam Wong , Dina Wonsever , Zach Wood - Doughty , Bo   Wu , Bowen Wu , Chien - Sheng Wu , Chuhan Wu , Chun - Kai Wu , Dayong Wu , Di Wu , Fangzhao   Wu , Jian Wu , Junshuang Wu , Lianwei Wu , Lijun Wu , Lingfei Wu , Qianhui Wu , Qingyang Wu ,   Shijie Wu , Shuangzhi Wu , Sixing Wu , Stephen Wu , Tongshuang Wu , Wei Wu , Xianchao Wu ,   Xiaobao Wu , Yanan Wu , Youzheng Wu , Yu Wu , Yuanbin Wu , Yunfang Wu , Yuting Wu , Zeqiu   Wu , Zhen Wu , Zhiyong Wu , Zhonghai Wu , Joern Wuebker   Congying Xia , Jingbo Xia , Mengzhou Xia , Patrick Xia , Qingrong Xia , Rui Xia , Yikun Xian ,   Jiannan Xiang , Rong Xiang , Chaojun Xiao , Chunyang Xiao , Huiru Xiao , Jinghui Xiao , Lin Xiao ,   Liqiang Xiao , Min Xiao , Tong Xiao , Wen Xiao , Yanghua Xiao , Boyi Xie , Jun Xie , Qianqian Xie ,   Ruobing Xie , Tianbao Xie , Yuqiang Xie , Ji Xin , Frank Xing , Deyi Xiong , Wenhan Xiong , Ben-   feng Xu , Boyan Xu , Can Xu , Canwen Xu , Chen Xu , Dongkuan Xu , Frank F. Xu , Hongfei Xu ,   Hu Xu , Jia Xu , Jiacheng Xu , Jinan Xu , Jing Xu , Jingjing Xu , Jitao Xu , Jun Xu , Kun Xu , Lu Xu ,   Peng Xu , Peng Xu , Qiantong Xu , Qiongkai Xu , Ruifeng Xu , Runxin Xu , Ruochen Xu , Shusheng   Xu , Wang Xu , Weijia Xu , Weiran Xu , Weiwen Xu , Wenduan Xu , Xinnuo Xu , Yan Xu , Yang Xu ,   Yumo Xu , Zenglin Xu , Zhen Xu , Zhiyang Xu   Shuntaro Yada , Vikas Yadav , Yadollah Yaghoobzadeh , Ikuya Yamada , Ivan P. Yamshchikov , Hanqi   Yan , Jun Yan , Lingyong Yan , Yu Yan , Yuanmeng Yan , Baosong Yang , Changbing Yang , Cheng-   hao Yang , Fan Yang , Haiqin Yang , Jie Yang , Jun Yang , Linyi Yang , Min Yang , Mingming Yang ,   Muyun Yang , Ruosong Yang , Sen Yang , Sen Yang , Songlin Yang , Tsung - Yen Yang , Wei Yang ,   Wenmian Yang , Yilin Yang , Yinfei Yang , Yujiu Yang , Zhao Yang , Zhen Yang , Zhichao Yang ,   Zhilin Yang , Ziqing Yang , Ziyi Yang , Jianmin Yao , Liang Yao , Shunyu Yao , Wenlin Yao , Ziyu   Yao , Mark Yatskar , Deming Ye , Qinyuan Ye , Reyyan Yeniterzi , Jinyoung Yeo , Xiaoyuan Yi , Seid   Muhie Yimam , Da Yin , Pengcheng Yin , Qingyu Yin , Xuwang Yin , Yichun Yin , Sho Yokoi , Zheng   Xin Yong , Kang Min Yoo , Seunghyun Yoon , Masashi Yoshikawa , Steve Young , Safoora YouseÔ¨Å ,   Bei Yu , Bowen Yu , Changlong Yu , Chen Yu , Dian Yu , Dian Yu , Dong Yu , Heng Yu , Hong Yu ,   Jifan Yu , Juntao Yu , Kai Yu , Mo Yu , Tao Yu , Tiezheng Yu , Wenhao Yu , Xiaodong Yu , Yue Yu ,   Caixia Yuan , Jianhua Yuan , Nicholas Jing Yuan , Yu Yuan , Zheng Yuan , Xiang Yue , Hyokun Yun   Annie Zaenen , Wajdi Zaghouani , Marcos Zampieri , Marcely Zanon Boito , Alessandra Zarcone ,   Sina Zarrie√ü , Vicky Zayats , Rabih Zbib , Albin Zehe , Rowan Zellers , Yury Zemlyanskiy , Daojian   Zeng , Fengzhu Zeng , Jiali Zeng , Jichuan Zeng , Qi Zeng , Shuang Zeng , Weixin Zeng , Xingshan   Zeng , Zhiyuan Zeng , Thomas Zenkel , Deniz Zeyrek , Hanwen Zha , Fangzhou Zhai , Haolan Zhan ,   Li - Ming Zhan , Runzhe Zhan , Biao Zhang , Bowen Zhang , Bowen Zhang , Chen Zhang , Chen   Zhang , Chiyu Zhang , Chuheng Zhang , Danqing Zhang , Dawei Zhang , Delvin Ce Zhang , Den-   ghui Zhang , Dong Zhang , Dongdong Zhang , Dongxu Zhang , Dongyu Zhang , Guanhua Zhang ,   Haibo Zhang , Hainan Zhang , Haisong Zhang , Hao Zhang , Hao Zhang , Haoyu Zhang , Hongming   Zhang , Hu Zhang , Jiajun Zhang , Jianguo Zhang , Jieyu Zhang , Jinchao Zhang , Jingqing Zhang , Ke   Zhang , Kun Zhang , Lei Zhang , Lei Zhang , Li Zhang , Licheng Zhang , Longyin Zhang , Meishan   Zhang , Meng Zhang , Michael JQ Zhang , Mike Zhang , Min Zhang , Ningyu Zhang , Peng Zhang , Qi   Zhang , Richong Zhang , Rui Zhang , Ruixiang Zhang , Sheng Zhang , Shiyue Zhang , Shujian Zhang ,   Shuo Zhang , Tianlin Zhang , Tong Zhang , Tongtao Zhang , Wei Zhang , Wei Emma Zhang , Weinan   Zhang , Wen Zhang , Wen Zhang , Xiang Zhang , Xiao Zhang , Xiaotong Zhang , Xingxing Zhang ,   Xinliang Frederick Zhang , Xinsong Zhang , Xinyuan Zhang , Xuanwei Zhang , Xuanyu Zhang , Xu-   chao Zhang , Yan Zhang , Yan Zhang , Yao Zhang , Yichi Zhang , Yu Zhang , Yu Zhang , Yuan Zhang ,   Yuanzhe Zhang , Yue Zhang , Yuhao Zhang , Yuhui Zhang , Yunyi Zhang , Yusen Zhang , Zeyu Zhang ,   Zheng Zhang , Zhengyan Zhang , Zhihao Zhang , Zhirui Zhang , Zhisong Zhang , Zhuosheng Zhang ,   Ziqi Zhang , Chao Zhao , Chen Zhao , Dongyan Zhao , Guangxiang Zhao , Jieyu Zhao , Kai Zhao ,   Mengjie Zhao , Sanqiang Zhao , Tiancheng Zhao , Tianyu Zhao , Tiejun Zhao , Yang Zhao , Yanpeng   xxxZhao , Yao Zhao , Yilun Zhao , Zhenjie Zhao , Zhou Zhao , Bo Zheng , Changmeng Zheng , Chujie   Zheng , Renjie Zheng , Xiaoqing Zheng , Yinhe Zheng , Zaixiang Zheng , Ming Zhong , Peixiang   Zhong , Victor Zhong , Wanjun Zhong , Zexuan Zhong , Ben Zhou , Chunting Zhou , Deyu Zhou ,   Dong Zhou , Giulio Zhou , Guangyou Zhou , Jiawei Zhou , Jie Zhou , Jie Zhou , Jingbo Zhou , Junpei   Zhou , Junsheng Zhou , Li Zhou , Long Zhou , Meng Zhou , Pei Zhou , Qingyu Zhou , Shuyan Zhou ,   Wangchunshu Zhou , Wenxuan Zhou , Xiang Zhou , Xiangyang Zhou , Yaqian Zhou , Yi Zhou , Yi-   chao Zhou , Yichu Zhou , Yilun Zhou , Yucheng Zhou , Zhengyu Zhou , Zhihan Zhou , Conghui Zhu ,   Hao Zhu , Jian Zhu , Jun Zhu , Junnan Zhu , Kenny Q. Zhu , Lixing Zhu , Muhua Zhu , Qi Zhu , Qing-   fu Zhu , Qinglin Zhu , Su Zhu , Wei Zhu , Xiaoyan Zhu , Yilun Zhu , Yong Zhu , Zining Zhu , Fuzhen   Zhuang , Yimeng Zhuang , Caleb Ziems , Roger Zimmermann , Heike Zinsmeister , Ayah Zirikly , Shi   Zong , Bowei Zou , Yanyan Zou , Amal Zouaq , Arkaitz Zubiaga , Pierre Zweigenbaum   Outstanding Action Editors   Antonios Anastasopoulos , David Bamman , Steven Bethard , Leonid Boytsov , Paula Carvalho ,   Snigdha Chaturvedi , Raj Dabre , Daniel Dakota , Johannes Daxenberger , Leon Derczynski , Greg   Durrett , Michael Elhadad , Allyson Ettinger , Goran Glava≈° , David Harwath , Shubhra Kanti Karma-   ker , Daniel Khashabi , Mamoru Komachi , Carolin Lawrence , John Lawrence , Constantine Lignos ,   Saif M. Mohammad , Philippe Muller , Rebecca J. Passonneau , Emily Prud‚Äôhommeaux , Mrinmaya   Sachan , Lane Schwartz , Kevin Small , Efstathios Stamatatos , Amanda Stent , Amalia Todirascu ,   Junichi Tsujii , Suzan Verberne , Antonio Jimeno Yepes , Fran√ßois Yvon , Luke Zettlemoyer , Justine   Zhang   Outstanding Reviewers   Nader Akoury , Gianni Barlacchi , Rachel Bawden , G√°bor Bella , Delphine Bernhard , Shruti Bhosa-   le , Michael Bloodgood , Ondrej Bojar , Iacer Calixto , R√©mi Cardon , Thiago Castro Ferreira , Tuhin   Chakrabarty , Verna Dankers , Yupei Du , Micha Elsner , Antske Fokkens , Stella Frank , Alexander   Fraser , Dayne Freitag , Daniel Fried , Dan Garrette , Philip John Gorinski , Dagmar Gromann , Liane   Guillou , Jack Hessel , Nanjiang Jiang , Gareth J. F. Jones , Min - Yen Kan , Anna Kazantseva , Faj-   ri Koto , Julia Kreutzer , Kalpesh Krishna , Dawn Lawrie , Andrew Lee , Jordan Lee Boyd - Graber ,   Gina - Anne Levow , Xiang Lisa Li , Patrick William Littell , Kaixin Ma , Vladislav Maraev , Alexan-   der Mehler , Florian Metze , Julian Michael , Paul Michel , Elena Musi , Sheshera Mysore , Denis   Newman - GrifÔ¨Ås , Tong Niu , Michal Nov√°k , Siddharth Patwardhan , Karl Pichotta , Yuval Pinter ,   Peng Qi , Surangika Ranathunga , Vikas Raunak , Pedro Rodriguez , Sebastian Ruder , Alexander M.   Rush , Elizabeth Salesky , Thomas Schaaf , Yves Scherrer , Viktor Schlegel , Elliot Schumacher , Ian   Stewart , Naomi Tachikawa Shapiro , Emiel van Miltenburg , Peter West , Adam Wiemerslage , Jitao   Xu , Yue Yu , Yury Zemlyanskiy   xxxi   Angela D. Friederici   Max Planck Institute for Human Cognitive and Brain Sciences , Leipzig , Germany   Abstract : Language is considered to be a uniquely human faculty . The different aspects of the language   system , namely phonology , semantics and syntax have long been discussed with respect to their species-   speciÔ¨Åcity . Syntax as the ability to process hierarchical structures appears to be speciÔ¨Åc to humans . The   available neuroscientiÔ¨Åc data allow us to deÔ¨Åne the functional language network which involves Broca ‚Äôs   area in the inferior frontal cortex and the posterior superior temporal cortex . Within this network , the   posterior part of Broca ‚Äôs area plays a special role as it supports the processing of hierarchical syntactic   structures , in particular the linguistic computation Merge which is at the root of every language . This   part of Broca ‚Äôs area is connected to the posterior temporal cortex via a dorsally located white matter   Ô¨Åber tract hereby providing to structural basis for the functional interplay of these regions . It has been   shown that the maturation of this white matter pathway is directly correlated with the ability to process   syntactically complex sentences during human development . Moreover , this dorsal pathway appears to   be weak in the prelinguistic infant and in the non - human primate . These Ô¨Åndings suggest that the dorsal   pathway plays a crucial role in the emergence of syntax in human language .   Bio : Angela D. Friederici is a cognitive neuroscientist in the domain of language . She is director at the   Max Planck Institute for Human Cognitive and Brain Sciences ( MPI CBS ) in Leipzig , Germany and the   Founding director of this institution founded in 1994 .   She graduated in linguistics and psychology at the University of Bonn ( Germany ) and spent a postdoc-   toral year at MIT ( USA ) . She was a research fellow at the Max Planck Institute in Nijmegen ( NL ) , at the   University Rene Descartes , Paris ( F ) and University of California , San Diego ( USA ) . Prior to joining the   Max Planck Society as a director , she was professor for Cognitive Sciences at the Free University Berlin .   Friederici is honorary professor at the University of Leipzig ( Psychology ) , the University of Potsdam   ( Linguistics ) and the Charit√© Universit√§tsmedizin Berlin ( Neurology ) and she holds a Doctor honoris   causa from the University of Mons , Belgium . Between 2014 and 2020 she was Vice President for the   Human Sciences Section of the Max Planck Society .   Her main Ô¨Åeld of research is the neurobiology of language . She published about 500 scientiÔ¨Åc papers on   this topic in major international journals . She received a number of scientiÔ¨Åc awards : 1987 Heisenberg   Fellowship of the German Research Foundation , 1990 Alfried Krupp Award of the Alfried Krupp von   Bohlen and Halbach - Stiftung , 1997 Gottfried Wilhelm Leibniz Prize of the German Research Founda-   tion , and 2011 Carl Friedrich Gauss Medal of the Brunswick ScientiÔ¨Åc Society . She is member of the   Berlin - Brandenburg Academy of Sciences and Humanities , member of the national German Academy   of Sciences ‚Äô Leopoldina ‚Äô and member of the Academia Europaea .   xxxii   For the 60th Anniversary of ACL 2022 , we will feature a keynote Ô¨Åre - side chat on ‚Äú The Trajectory of   ACL and the Next 60 years ‚Äù with two keynote talks in dialogue : Barbara Grosz and Yejin Choi followed   by a moderated discussion lead by Rada Mihalcea .   Barbara J. Grosz   Harvard University SEAS   Abstract : Research in computational linguistics and spoken language systems has made astonishing   progress in the last decade . Even so , the challenge remains of achieving human - level Ô¨Çuent dialogue   conversational capabilities beyond narrowly deÔ¨Åned domains and tasks . Findings of earlier ACL times   research on dialogue hold some lessons for breaking the ‚Äú dialogue boundary ‚Äù in computational lingui-   stics yet again , if ways can be found to integrate them into deep - learning language models . These models   raise some of the most serious ethical challenges of current computing research and technologies . Ex-   panding their powers in this direction will raise more . In discussing these topics , I will raise questions   for Prof. Choi and our subsequent discussion .   Bio : Barbara J. Grosz is Higgins Research Professor of Natural Sciences in the Paulson School of En-   gineering and Applied Sciences at Harvard University . Her contributions to AI include fundamental   advances in natural - language dialogue processing and in theories of multi - agent collaboration as well   as innovative uses of models developed in this research to improve healthcare coordination and science   education . She co - founded Harvard ‚Äôs Embedded EthiCS program , which integrates teaching of ethical   reasoning into core computer science courses . A member of the National Academy of Engineering ,   the American Philosophical Society , and the American Academy of Arts and Sciences , she is a fellow   of several scientiÔ¨Åc societies and recipient of the 2009 ACM / AAAI Allen Newell Award , the 2015 IJ-   CAI Award for Research Excellence , and the 2017 Association for Computational Linguistics Lifetime   Achievement Award .   Yejin Choi   Paul G. Allen School of Computer Science & Engineering at the University of Washington   Abstract : In this talk , I will wander around reÔ¨Çections on the past of ACL and speculations on the future   of ACL . This talk will be purposefully imaginative and accidentally controversial , by emphasizing on the   importance of deciphering the dark matter of intelligence , by arguing for embracing all the ambiguous   aspects of language at all pipelines of language processing , by highlighting the counterintuitive contin-   uum across language , knowledge , and reasoning , and by pitching the renewed importance of formalisms ,   algorithms , and structural inferences in the modern deep learning era . Looking back , at the 50‚Äôth ACL ,   I could n‚Äôt possibly imagine that I would be one day giving this very talk . For that reason , I will also   share my personal anecdotes on the lasting inspirations from the previous lifetime achievement award   speeches , how I believe talent is made , not born , and the implication of that belief for promoting diversity   and equity .   xxxiiiBio : Yejin Choi is Brett Helsel Professor at the Paul G. Allen School of Computer Science & Engi-   neering at the University of Washington and a senior research manager at AI2 overseeing the project   Mosaic . Her research investigates commonsense knowledge and reasoning , neuro - symbolic integration ,   neural language generation and degeneration , multimodal representation learning , and AI for social good .   She is a co - recipient of the ACL Test of Time award in 2021 , the CVPR Longuet - Higgins Prize in 2021 ,   a NeurIPS Outstanding Paper Award in 2021 , the AAAI Outstanding Paper Award in 2020 , the Borg   Early Career Award in 2018 , the inaugural Alexa Prize Challenge in 2017 , IEEE AI ‚Äôs 10 to Watch in   2016 , and the ICCV Marr Prize in 2013 .   xxxiv   Chair : Steven Bird , Charles Darwin University   Panelists and languages represented :   Robert Jimerson , Rochester Institute of Technology ( Seneca , USA )   Fajri Koto , The University of Melbourne ( Minangkabau , Indonesia )   Heather Lent , University of Copenhagen ( Creole languages )   Teresa Lynn , Dublin City University ( Irish )   Manuel Mager , University of Stuttgart ( Wixaritari , Mexico )   Perez Ogayo , Carnegie Mellon University ( Luo and Kiswahili , Kenya )   How do the tools and techniques of computational linguistics serve the full diversity of the world ‚Äôs lan-   guages ? In particular , how do they serve the people who are still speaking thousands of local languages ,   often in highly multilingual , post - colonial situations ? This 60th meeting of the ACL features a special   theme track on language diversity with the goal of ‚Äú reÔ¨Çecting and stimulating discussion about how the   advances in computational linguistics and natural language processing can be used for promoting lan-   guage diversity ‚Äù . This keynote talk - panel will showcase the special theme and identify key learnings   from the conference . We hope this session will help to shape the future agenda for speech and language   technologies in support of global linguistic diversity . The session will be organised around a series of   questions under three headings .   Diverse Contexts . What is the situation of local languages where panel members are working ? Are   there multiple languages with distinct functions and ideologies ? What are the local aspirations for the   future of these languages . How are people advocating for language technology on the ground ? How did   the work begin ? What does success look like ?   Understanding Risks . Do the people who provide language data fully understand the ways their da-   ta might be used in future , including ways that might not be in their interest ? What beneÔ¨Åt are local   participants promised in return for their participation , and do they actually receive these beneÔ¨Åts ? Are   there harms that come with language standardisation ? What principles of doing no harm can we adopt ?   New Challenges . How can we provide beneÔ¨Åts of text technologies without assuming language stan-   dardisation , ofÔ¨Åcial orthography , and monolingual usage ? When working with local communities , do   we always require data in exchange for technologies , or is a non - extractive NLP possible ? How do we   decolonise speech and language technology ? At the beginning of the International Decade of Indigenous   Languages 2022‚Äì2032 , we ask : how do we respond as a community , and how can our Ô¨Åeld be more   accessible to indigenous participation ?   xxxv   BitFit : Simple Parameter - efÔ¨Åcient Fine - tuning for Transformer - based Masked Language - models   Elad Ben Zaken , Yoav Goldberg and Shauli Ravfogel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1   Are Shortest Rationales the Best Explanations for Human Understanding ?   Hua Shen , Tongshuang Wu , Wenbo Guo and Ting - Hao Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10   Analyzing Wrap - Up Effects through an Information - Theoretic Lens   Clara Isabel Meister , Tiago Pimentel , Thomas Hikaru Clark , Ryan D Cotterell and Roger P. Levy   20   Have my arguments been replied to ? Argument Pair Extraction as Machine Reading Comprehension   Jianzhu Bao , Jingyi Sun , Qinglin Zhu and Ruifeng Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29   High probability or low information ? The probability ‚Äì quality paradox in language generation   Clara Isabel Meister , Gian Wiher , Tiago Pimentel and Ryan D Cotterell . . . . . . . . . . . . . . . . . . . . .36   Disentangled Knowledge Transfer for OOD Intent Discovery with UniÔ¨Åed Contrastive Learning   Yutao Mou , Keqing He , Yanan Wu , Zhiyuan Zeng , Hong Xu , Huixing Jiang , Wei Wu and Weiran   Xu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46   Voxel - informed Language Grounding   Rodolfo Corona , Shizhan Zhu , Dan Klein and Trevor Darrell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54   P - Tuning : Prompt Tuning Can Be Comparable to Fine - tuning Across Scales and Tasks   Xiao Liu , Kaixuan Ji , Yicheng Fu , Weng Lam Tam , Zhengxiao Du , Zhilin Yang and Jie Tang . 61   On EfÔ¨Åciently Acquiring Annotations for Multilingual Models   Joel Ruben Antony Moniz , Barun Patra and Matthew R. Gormley . . . . . . . . . . . . . . . . . . . . . . . . . . 69   Automatic Detection of Entity - Manipulated Text using Factual Knowledge   Ganesh Jawahar , Muhammad Abdul - Mageed and Laks V . S. Lakshmanan . . . . . . . . . . . . . . . . . . 86   Does BERT Know that the IS - A Relation Is Transitive ?   Ruixi Lin and Hwee Tou Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94   Buy Tesla , Sell Ford : Assessing Implicit Stock Market Preference in Pre - trained Language Models   Chengyu Chuang and Yi Yang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100   Pixie : Preference in Implicit and Explicit Comparisons   Amanul Haque , Vaibhav Garg , Hui Guo and Munindar P. Singh . . . . . . . . . . . . . . . . . . . . . . . . . . 106   Counterfactual Explanations for Natural Language Interfaces   George Tolkachev , Stephen Mell , Stephan Zdancewic and Osbert Bastani . . . . . . . . . . . . . . . . . . 113   Predicting DifÔ¨Åculty and Discrimination of Natural Language Questions   Matthew Alexander Byrd and Shashank Srivastava . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119   How does the pre - training objective affect what large language models learn about linguistic proper-   ties ?   Ahmed Alajrami and Nikolaos Aletras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131   The Power of Prompt Tuning for Low - Resource Semantic Parsing   Nathan Schucher , Siva Reddy and Harm de Vries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148   xxxviData Contamination : From Memorization to Exploitation   Inbal Magar and Roy Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157   Detecting Annotation Errors in Morphological Data with the Transformer   Ling Liu and Mans Hulden . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .166   Estimating the Entropy of Linguistic Distributions   Aryaman Arora , Clara Isabel Meister and Ryan D Cotterell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175   Morphological ReinÔ¨Çection with Multiple Arguments : An Extended Annotation schema and a Georgian   Case Study   David Guriel , Omer Goldman and Reut Tsarfaty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196   DQ - BART : EfÔ¨Åcient Sequence - to - Sequence Model via Joint Distillation and Quantization   Zheng Li , Zijian Wang , Ming Tan , Ramesh Nallapati , Parminder Bhatia , Andrew Arnold , Bing   Xiang and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203   Learning - by - Narrating : Narrative Pre - Training for Zero - Shot Dialogue Comprehension   Chao Zhao , Wenlin Yao , Dian Yu , Kaiqiang Song , Dong Yu and Jianshu Chen . . . . . . . . . . . . . 212   Kronecker Decomposition for GPT Compression   Ali Edalati , Marzieh S. Tahaei , Ahmad Rashid , Vahid Partovi Nia , James J. Clark and Mehdi   Rezagholizadeh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219   Simple and Effective Knowledge - Driven Query Expansion for QA - Based Product Attribute Extraction   Keiji Shinzato , Naoki Yoshinaga , Yandi Xia and Wei - Te Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227   Event - Event Relation Extraction using Probabilistic Box Embedding   EunJeong Hwang , Jay - Yoon Lee , Tianyi Yang , Dhruvesh Patel , Dongxu Zhang and Andrew   McCallum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235   Sample , Translate , Recombine : Leveraging Audio Alignments for Data Augmentation in End - to - end   Speech Translation   Tsz Kin Lam , Shigehiko Schamoni and Stefan Riezler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245   Predicting Sentence Deletions for Text SimpliÔ¨Åcation Using a Functional Discourse Structure   Bohan Zhang , Prafulla Kumar Choubey and Ruihong Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255   Multilingual Pre - training with Language and Task Adaptation for Multilingual Text Style Transfer   Huiyuan Lai , Antonio Toral and Malvina Nissim . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262   When to Use Multi - Task Learning vs Intermediate Fine - Tuning for Pre - Trained Encoder Transfer Lear-   ning   Orion Weller , Kevin Seppi and Matt Gardner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272   Leveraging Explicit Lexico - logical Alignments in Text - to - SQL Parsing   Runxin Sun , Shizhu He , Chong Zhu , Yaohan He , Jinlong Li , Jun Zhao and Kang Liu . . . . . . . 283   Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning   Zixuan Li , Saiping Guan , Xiaolong Jin , Weihua Peng , Yajuan Lyu , Yong Zhu , Long Bai , Wei Li ,   Jiafeng Guo and Xueqi Cheng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290   Mismatch between Multi - turn Dialogue and its Evaluation Metric in Dialogue State Tracking   Takyoung Kim , Hoonsang Yoon , Yukyung Lee , Pilsung Kang and Misuk Kim . . . . . . . . . . . . . 297   LM - BFF - MS : Improving Few - Shot Fine - tuning of Language Models based on Multiple Soft Demon-   stration Memory   Eunhwan Park , Donghyeon Jeon , Seonhoon Kim , Inho Kang and Seung - Hoon Na . . . . . . . . . . 310   xxxviiTowards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn - level Perfor-   mances   Suvodip Dey , Ramamohan Kummara and Maunendra Sankar Desarkar . . . . . . . . . . . . . . . . . . . . 318   Exploiting Language Model Prompts Using Similarity Measures : A Case Study on the Word - in - Context   Task   Mohsen Tabasi , Kiamehr Rezaee and Mohammad Taher Pilehvar . . . . . . . . . . . . . . . . . . . . . . . . . 325   Hierarchical Curriculum Learning for AMR Parsing   Peiyi Wang , Liang Chen , Tianyu Liu , Damai Dai , Yunbo Cao , Baobao Chang and Zhifang Sui333   PARE : A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation   Extraction   Vipul Kumar Rathore , Kartikeya Badola , Parag Singla and Mausam .. . . . . . . . . . . . . . . . . . . . . .340   To Find Waldo You Need Contextual Cues : Debiasing Who ‚Äôs Waldo   Yiran Luo , Pratyay Banerjee , Tejas Gokhale , Yezhou Yang and Chitta Baral . . . . . . . . . . . . . . . 355   Translate - Train Embracing Translationese Artifacts   Sicheng Yu , Qianru Sun , Hao Zhang and Jing Jiang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362   C - MORE : Pretraining to Answer Open - Domain Questions by Consulting Millions of References   Xiang Yue , Xiaoman Pan , Wenlin Yao , Dian Yu , Dong Yu and Jianshu Chen . . . . . . . . . . . . . . . 371   k - Rater Reliability : The Correct Unit of Reliability for Aggregated Human Annotations   Ka Wong and Praveen Paritosh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378   An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model   Tokenizers   Valentin Hofmann , Hinrich Schuetze and Janet B. Pierrehumbert . . . . . . . . . . . . . . . . . . . . . . . . . 385   SCD : Self - Contrastive Decorrelation of Sentence Embeddings   Tassilo Klein and Moin Nabi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394   Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words   Kaitlyn Zhou , Kawin Ethayarajh , Dallas Card and Dan Jurafsky . . . . . . . . . . . . . . . . . . . . . . . . . . 401   Revisiting the Compositional Generalization Abilities of Neural Sequence Models   Arkil Patel , Satwik Bhattamishra , Phil Blunsom and Navin Goyal . . . . . . . . . . . . . . . . . . . . . . . . . 424   A Copy - Augmented Generative Model for Open - Domain Question Answering   Shuang Liu , Dong Wang , Xiaoguang Li , Minghui Huang and Meizhen Ding . . . . . . . . . . . . . . . 435   Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation   Soyeong Jeong , Jinheon Baek , Sukmin Cho , Sung Ju Hwang and Jong C. Park . . . . . . . . . . . . . 442   WLASL - LEX : a Dataset for Recognising Phonological Properties in American Sign Language   Federico Tavella , Viktor Schlegel , Marta Romeo , Aphrodite Galata and Angelo Cangelosi . . . 453   Investigating person - speciÔ¨Åc errors in chat - oriented dialogue systems   Koh Mitsuda , Ryuichiro Higashinaka , Tingxuan Li and Sen Yoshida . . . . . . . . . . . . . . . . . . . . . . 464   Direct parsing to sentiment graphs   David Samuel , Jeremy Barnes , Robin Kurtz , Stephan Oepen , Lilja √òvrelid and Erik Velldal . 470   XDBERT : Distilling Visual Information to BERT from Cross - Modal Systems to Improve Language Un-   derstanding   Chan - Jan Hsu , Hung - yi Lee and Yu Tsao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479   xxxviiiAs Little as Possible , as Much as Necessary : Detecting Over- and Undertranslations with Contrastive   Conditioning   Jannis Vamvas and Rico Sennrich . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .490   How Distributed are Distributed Representations ? An Observation on the Locality of Syntactic Infor-   mation in Verb Agreement Tasks   Bingzhi Li , Guillaume Wisniewski and Benoit Crabb√© . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501   Machine Translation for Livonian : Catering to 20 Speakers   Mat¬Øƒ±ss Rikters , Marili Tomingas , Tuuli Tuisk , Valts Ern≈°treits and Mark Fishel . . . . . . . . . . . . . 508   Fire Burns , Sword Cuts : Commonsense Inductive Bias for Exploration in Text - based Games   Dongwon Kelvin Ryu , Ehsan Shareghi , Meng Fang , Yunqiu Xu , Shirui Pan and Reza Haf . . . 515   A Simple but Effective Pluggable Entity Lookup Table for Pre - trained Language Models   Deming Ye , Yankai Lin , Peng Li , Maosong Sun and Zhiyuan Liu . . . . . . . . . . . . . . . . . . . . . . . . . 523   S - Tuning : A Simple Cross - lingual Sub - network Tuning Method   Runxin Xu , Fuli Luo , Baobao Chang , Songfang Huang and Fei Huang . . . . . . . . . . . . . . . . . . . . 530   Region - dependent temperature scaling for certainty calibration and application to class - imbalanced   token classiÔ¨Åcation   Hillary Dawkins and Isar Nejadgholi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538   Developmental Negation Processing in Transformer Language Models   Antonio Laverghetta Jr. and John Licato . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545   Canary Extraction in Natural Language Understanding Models   Rahil Parikh , Christophe Dupuy and Rahul Gupta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552   On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representa-   tions   Yang Trista Cao , Yada Pruksachatkun , Kai - Wei Chang , Rahul Gupta , Varun Kumar , Jwala Dha-   mala and Aram Galstyan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561   Sequence - to - sequence AMR Parsing with Ancestor Information   Chen Yu and Daniel Gildea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571   Zero - Shot Dependency Parsing with Worst - Case Aware Automated Curriculum Learning   Miryam De Lhoneux , Sheng Zhang and Anders S√∏gaard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578   PriMock57 : A Dataset Of Primary Care Mock Consultations   Alex Papadopoulos KorÔ¨Åatis , Francesco Moramarco , Radmila Sarac and Aleksandar Savkov 588   UniGDD : A UniÔ¨Åed Generative Framework for Goal - Oriented Document - Grounded Dialogue   Chang Gao , Wenxuan Zhang and Wai Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599   DMix : Adaptive Distance - aware Interpolative Mixup   Ramit Sawhney , Megh Thakkar , Shrey Pandit , Ritesh Singh Soun , Di Jin , Diyi Yang and Lucie   Flek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606   Sub - Word Alignment is Still Useful : A Vest - Pocket Method for Enhancing Low - Resource Machine Tran-   slation   Minhan Xu and Yu Hong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613   HYPHEN : Hyperbolic Hawkes Attention For Text Streams   Shivam Agarwal , Ramit Sawhney , Sanchit Ahuja , Ritesh Singh Soun and Sudheer Chava . . . 620   xxxixA Risk - Averse Mechanism for Suicidality Assessment on Social Media   Ramit Sawhney , Atula Tejaswi Neerkaje and Manas Gaur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 628   When classifying grammatical role , BERT does n‚Äôt care about word order ... except when it matters   Isabel Papadimitriou , Richard Futrell and Kyle Mahowald . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .636   Triangular Transfer : Freezing the Pivot for Triangular Machine Translation   Meng Zhang , Liangyou Li and Qun Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644   Can Visual Dialogue Models Do Scorekeeping ? Exploring How Dialogue Representations Incremen-   tally Encode Shared Knowledge   Brielen Madureira and David Schlangen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651   Focus on the Target ‚Äôs Vocabulary : Masked Label Smoothing for Machine Translation   Liang Chen , Runxin Xu and Baobao Chang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .665   Contrastive Learning - Enhanced Nearest Neighbor Mechanism for Multi - Label Text ClassiÔ¨Åcation   Xi‚Äôao Su , Ran Wang and Xinyu Dai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672   NoisyTune : A Little Noise Can Help You Finetune Pretrained Language Models Better   Chuhan Wu , Fangzhao Wu , Tao Qi and Yongfeng Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 680   Adjusting the Precision - Recall Trade - Off with Align - and - Predict Decoding for Grammatical Error Cor-   rection   Xin Sun and Houfeng Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .686   On the Effect of Isotropy on VAE Representations of Text   Lan Zhang , Wray Buntine and Ehsan Shareghi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694   EfÔ¨Åcient ClassiÔ¨Åcation of Long Documents Using Transformers   Hyunji Hayley Park , Yogarshi Vyas and Kashif Shah . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702   Rewarding Semantic Similarity under Optimized Alignments for AMR - to - Text Generation   Lisa Jin and Daniel Gildea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710   An Analysis of Negation in Natural Language Understanding Corpora   Md Mosharaf Hossain , Dhivya Chinnappa and Eduardo Blanco . . . . . . . . . . . . . . . . . . . . . . . . . . . 716   Primum Non Nocere : Before working with Indigenous data , the ACL must confront ongoing colonialism   Lane Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724   Unsupervised multiple - choice question generation for out - of - domain Q&A Ô¨Åne - tuning   Guillaume Le Berre , Christophe Cerisara , Philippe Langlais and Guy Lapalme . . . . . . . . . . . . . 732   Can a Transformer Pass the Wug Test ? Tuning Copying Bias in Neural Morphological InÔ¨Çection Models   Ling Liu and Mans Hulden . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .739   Probing the Robustness of Trained Metrics for Conversational Dialogue Systems   Jan Milan Deriu , Don Tuggener , Pius V on D√§niken and Mark Cieliebak . . . . . . . . . . . . . . . . . . . 750   Rethinking and ReÔ¨Åning the Distinct Metric   Siyang Liu , Sahand Sabour , Yinhe Zheng , Pei Ke , Xiaoyan Zhu and Minlie Huang . . . . . . . . . 762   How reparametrization trick broke differentially - private text representation learning   Ivan Habernal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 771   Towards Consistent Document - level Entity Linking : Joint Models for Entity Linking and Coreference   Resolution   Klim Zaporojets , Johannes Deleu , Yiwei Jiang , Thomas Demeester and Chris Develder . . . . . 778   xlA Flexible Multi - Task Model for BERT Serving   Tianwen Wei , Jianwei Qi and Shenghuan He . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .785   Understanding Game - Playing Agents with Natural Language Annotations   Nicholas Tomlin , Andre Wang He and Dan Klein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797   Code Synonyms Do Matter : Multiple Synonyms Matching Network for Automatic ICD Coding   Zheng Yuan , Chuanqi Tan and Songfang Huang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 808   CoDA21 : Evaluating Language Understanding Capabilities of NLP Models With Context - DeÔ¨Ånition   Alignment   L√ºtÔ¨Å Kerem Senel , Timo Schick and Hinrich Schuetze . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 815   On the Importance of Effectively Adapting Pretrained Language Models for Active Learning   Katerina Margatina , Loic Barrault and Nikolaos Aletras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .825   A Recipe for Arbitrary Text Style Transfer with Large Language Models   Emily Reif , Daphne Ippolito , Ann Yuan , Andy Coenen , Chris Callison - Burch and Jason Wei 837   DiS - ReX : A Multilingual Dataset for Distantly Supervised Relation Extraction   Abhyuday Bhartiya , Kartikeya Badola and Mausam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 849   ( Un)solving Morphological InÔ¨Çection : Lemma Overlap ArtiÔ¨Åcially InÔ¨Çates Models ‚Äô Performance   Omer Goldman , David Guriel and Reut Tsarfaty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 864   Text Smoothing : Enhance Various Data Augmentation Methods on Text ClassiÔ¨Åcation Tasks   Xing Wu , Chaochen Gao , Meng Lin , Liangjun Zang and Songlin Hu . . . . . . . . . . . . . . . . . . . . . . 871   xli   Elad Ben - ZakenShauli RavfogelYoav GoldbergComputer Science Department , Bar Ilan UniversityAllen Institute for Artificial Intelligence   { benzakenelad , shauli.ravfogel , yoav.goldberg } @gmail.com   Abstract   1 Introduction   Large pre - trained transformer based language mod-   els , and in particular bidirectional masked language   models from the BERT family ( Devlin et al . , 2018 ;   Liu et al . , 2019 ; Joshi et al . , 2019 ) , are responsible   for significant gains in many NLP tasks . Under   the common paradigm , the model is pre - trained   on large , annotated corpora with the LM objec-   tive , and then finetuned on task - specific supervised   data . The large size of these models make them   expensive to train and , more importantly , expensive   to deploy . This , along with theoretical questions   on the extent to which finetuning must change the   original model , has led researchers to consider fine-   tuning variants where one identifies a small subset   of the model parameters which need to be changed   for good performance in end - tasks , while keeping   all others intact ( ¬ß 2 ) .   We present a simple and effective approach to   fine tuning ( ¬ß 3 ) , which has the following benefits :   1.Changing very few parameters per fine - tuned   task .   2.Changing the same set of parameters for every   tasks ( task - invariance).3.The changed parameters are both isolated and   localized across the entire parameter space .   4.For small to medium training data , changing   only these parameters reaches the same task   accuracy as full fine - tuning , and sometimes   even improves results .   Specifically , we show that freezing most of the   network and fine - tuning only the bias - terms is   surprisingly effective . Moreover , if we allow the   tasks to suffer a small degradation in performance ,   we can fine - tune only two bias components ( the   ‚Äú query ‚Äù and ‚Äú middle - of - MLP ‚Äù bias terms ) , amount-   ing to half of the bias parameters in the model , and   only 0.04 % of all model parameters .   This result has a large practical utility in de-   ploying multi - task fine - tuned models in memory-   constrained environments , as well as opens the way   to trainable hardware implementations in which   most of the parameters are fixed . Additionally , it   opens up a set of research directions regarding the   role of bias terms in pre - trained networks , and the   dynamics of the fine - tuning process .   2 Background : fine - tuning and   parameter - efficient fine - tuning   In transfer - learning via model fine - tuning , a pre-   trained encoder network takes the input and pro-   duces contextualized representations . Then , a task-   specific classification layer ( here we consider linear   classifiers ) is added on top of the encoder , and the   entire network ( encoder+task specific classifiers ) is   trained end - to - end to minimize the task loss .   Desired properties . While fine - tuning per - task   is very effective , it also results in a unique , large   model for each pre - trained task , making it hard to   reason about what was changed in the fine - tuning   process , as well as hard to deploy , especially as the   number of tasks increases . Ideally , one would want   a fine - tuning method that :   ( i ) matches the results of a fully fine - tuned model;1(ii ) changes only a small portion of the model ‚Äôs   parameters ; and ( iii ) enables tasks to arrive in a   stream , instead of requiring simultaneous access to   all datasets . For efficient hardware based deploy-   ments , it is further preferred that ( iv ): the set of   parameters that change values is consistent across   different tasks .   Learning vs. Exposing . The feasibility of fulfill-   ing the above requirements depends on a fundamen-   tal question regarding the nature of the fine - tuning   process of large pre - trained LMs : to what extent   does the fine - tuning process induces the learning of   new capabilities , vs. the exposing of existing capa-   bilities , which were learned during the pre - training   process .   Existing approaches . Two recent works have   demonstrated that adaptation to various end - tasks   can in fact be achieved by changing only a small   subset of parameters . The first work , by Houlsby   et al . ( 2019 ) ( ‚Äú Adapters ‚Äù ) , achieves this goal by in-   jecting small , trainable task - specific ‚Äú adapter ‚Äù mod-   ules between the layers of the pre - trained model ,   where the original parameters are shared between   tasks . The second work , by Guo et al . ( 2020 )   ( ‚Äú Diff - Pruning ‚Äù ) , achieves the same goal by adding   a sparse , task - specific difference - vector to the orig-   inal parameters , which remain fixed and are shared   between tasks . The difference - vector is regular-   ized to be sparse . Both methods allow adding only   a small number of trainable parameters per - task   ( criteria ii ) , and each task can be added without   revisiting previous ones ( criteria iii ) .   They also partially fulfill criteria ( i ) , suffering   only a small drop in performance compared to full   fine - tuning . The Adapter method , but not the Diff-   Pruning method , also supports criteria ( iv ) . How-   ever , Diff - Pruning is more parameter efficient than   the Adapter method ( in particular , it adds no new   parameters ) , and also achieves better task scores .   We compare against Diff - Pruning and Adapters in   the experiments section , and show that we perform   favorably on many tasks while also satisfying crite-   ria ( iv ) .   3 Bias - terms Fine - tuning ( BitFit )   We propose a method we call BitFit(BIas - Term   FIne - Tuning ) , in which we freeze most of the   transformer - encoder parameters , and train only the   bias - terms and the task - specific classification layer . BitFit has three key properties : ( i ) match the re-   sults of fully fine - tuned model . ( ii ) enable tasks   to arrive in a stream , this way it does not require   simultaneous access to all datasets . ( iii ) fine - tune   only a small portion of the model ‚Äôs parameters .   The approach is parameter - efficient : each new   task requires storing only the bias terms parameter   vectors ( which amount to less than 0.1 % of the   total number of parameters ) , and the task - specific   final linear classifier layer .   Concretely , the BERT encoder is composed of   Llayers , where each layer ‚Ñìstarts with Mself-   attention heads , where a self attention head ( m , ‚Ñì )   haskey , query andvalue encoders , each taking the   form of a linear layer :   Q(x ) = Wx+b   K(x ) = Wx+b   V(x ) = Wx+b   Where xis the output of the former encoder layer   ( for the first encoder layer xis the output of the   embedding layer ) . These are then combined using   an attention mechanism that does not involve new   parameters :   h = att    Q , K , V , .. , Q , K , V   and then fed to an MLP with layer - norm ( LN ):   h = Dropout    W¬∑h+b   ( 1 )   h = g‚äô(h+x)‚àí¬µ   œÉ+b(2 )   h = GELU    W¬∑h+b   ( 3 )   h = Dropout    W¬∑h+b   ( 4 )   out = g‚äô(h+h)‚àí¬µ   œÉ+b(5 )   The collection of all matrices Wand vectors   g , b , indicated in blue and purple are the net-   work ‚Äôs parameters Œò , where the subset of purple   vectors bare the bias terms .   The bias terms are additive , and correspond to a   very small fraction of the network , in BERT   and BERT bias parameters make up 0.09 %   and 0.08 % of the total number of parameters in   each model , respectively .   We show that by freezing all the parameters   Wandgand fine - tuning only the additive2   bias terms b , we achieve transfer learning perfor-   mance which is comparable ( and sometimes bet-   ter ! ) than fine - tuning of the entire network ,   We also show that we can fine - tune only a subset   of the bias parameters , namely those associated   with the query and the second MLP layer ( only   bandb ) , and still achieve accuracies that   rival full - model fine - tuning .   4 Experiments and Results   Datasets . We evaluate BitFit on the GLUE bench-   mark ( Wang et al . , 2018).Consistent with previ-   ous work ( Houlsby et al . , 2019 ; Guo et al . , 2020 )   we exclude the WNLI task , on which BERT models   do not outperform the majority baseline .   Models and Optimization . We use the publicly   available pre - trained BERT , BERT ( De-   vlin et al . , 2018 ) and RoBERTa ( Liu et al . ,   2019 ) models , using the HuggingFace ( Wolf et al . ,   2020 ) interface and implementation .   Appendix ¬ß A.2 lists optimization details .   Comparison to Diff - Pruning and Adapters ( Ta-   ble 1 ) In the first experiment , we compare Bit-   Fit to Diff - Pruning method and Adapters method ,   when using a fewer number of parameters . Table 1   reports the dev - set and test - set performance com-   pared to the Diff - Pruning and Adapters numbers   reported by Guo et al . ( 2020 ) and Houlsby et al .   ( 2019 ) ( respectively ) . This experiment used the   BERT model .   On validation set , BitFit outperforms Diff-   Pruning on 4 out of 9 tasks , while using 6x fewer   trainable parameters . As for test - set results , two   clear wins compared to Diff - Pruning and 4 clear   wins compared to Adapters while using 45x fewer   trainable parameters .   Different Base - models ( Table 2 ) We repeat   the BERT results on different base - models   ( the smaller BERT and the better performing   RoBERTa ) . The results in Table 2 show that   the trends remain consistent .   Are bias parameters special ? Are the bias pa-   rameters special , or will any random subset do ? We   randomly sampled the same amount of parameters   as in BitFit from the entire model , and fine - tuned   only them ( ‚Äú rand uniform ‚Äù line in Table 3 ) . The   results are substantially worse across all tasks ; sim-   ilar patterns are observed when the random param-   eters are sampled as complete rows / columns in the   parameter matrices ( ‚Äú rand row / col ‚Äù line in Table   3 ) .   Fewer bias parameters ( Table 3 ) Can we fine-   tune on only a subset of the bias - parameter ?   We define the amount of change in a bias vector   bto be‚à•b‚àíb‚à• , that is , the average   absolute change , across its dimensions , between the   initial LM values band its fine - tuned values b.   Figure 1 shows the change per bias term and layer ,   for the RTE task ( other tasks look very similar ,   see Appendix ¬ß A.4 ) . The ‚Äò key ‚Äô bias bhas zero3   change , consistent with the theoretical observation   in Cordonnier et al . ( 2020 ) . In contrast , b , the bias   of the queries , and b , the bias of the intermediate   MLP layers ( which take the input from 768 - dims   to 3072 ) , change the most . Table 3 reports dev-   set results when fine - tuning only the bandb   bias terms , for the BERT model . Results are   only marginally lower than when tuning all bias   parameters . Tuning either borbalone yields   substantially worse results , indicating both bias   types are essential . As expected , using a frozen   BERT model yields much worse results .   Generalization gap . While in most cases full   fine - tuning reaches nearly 100 % train accuracy , we   find that the generalization gap ( Shalev - Shwartz   and Ben - David , 2014)‚Äîthe difference between   training error and test error ‚Äî is substantially   smaller for the BitFit models .   Token - level tasks . The GLUE tasks are all sen-   tence level . We also experimented with token - level   PTB POS - tagging . Full - FT results for BERT ,   BERT and RoBERTa are 97.2 , 97.4 ,   97.2 , while BitFit results are 97.2 , 97.4 , 97.1 .   Size of training data . The GLUE results suggest   a reverse correlation between BitFit ability to reach   Full - FT performance , and training set size . To test   this ( and to validate another token - level task ) , we   train on increasing - sized subsets of SQuAD v1.0   Rajpurkar et al . ( 2016a ) . The results on Figure   2 show a clear trend : BitFit dominates over Full-   FT in the smaller - data regime , while the trend is   reversed when more training data is available . We   conclude that BitFit is a worthwhile targetted fine-   tuning method in small - to - medium data regimes .   5 Related Work   The problem of identifying the minimal set of pa-   rameters that need to be fine - tuned to achieve good   performance in end - tasks relates both to practi-   cal questions of model compression , and also to   more fundamental question on the nature of the   pre - training and finetuning process , the ‚Äú linguis-   tic knowledge ‚Äú induced by each of them , and the   extent to which it generalizes to different tasks .   Over - parameterization Large LM models were   shown to be over - parameterized : they contain   more parameters than needed in inference ( Bucilu Àáa   et al . , 2006 ; Hinton et al . , 2015 ; Urban et al . , 2017 ;   Karnin , 1990 ; Reed , 1993 ; Augasta and Kathir-   valavakumar , 2013 ; Liu et al . , 2014 ; Han et al . ,   2015 ; Molchanov et al . , 2017 ) . Gordon et al . ( 2020 )   have demonstrated that overparmeterization can be   exploited in finetuning : pruned network perform4well in transfer setting . We work in a complemen-   tary setting , where the entire model is kept , but   only some parameters are updated . The remarkable   success of those works have sparked interest the   lottery - ticket hypothesis ( Frankle and Carbin , 2019 ;   Chen et al . , 2020 ; Prasanna et al . , 2020 ): the con-   jecture that large models are needed in pretraining   only to induce ( in high probability ) the existing of   sub - networks initialized with the correct inductive   bias for learning , and the findings that those sparse   networks often transfer well to different tasks .   Bias terms Bias terms and their importance   are rarely discussed in the literature . Zhao   et al . ( 2020 ) describe a masking - based fine - tuning   method , and explicitly mention ignoring the bias   terms , as handling them ‚Äú did not observe a positive   effect on performance ‚Äù .   An exception is the work of Wang et al . ( 2019 )   who analyzed bias terms from the perspective of   attribution method . They demonstrate that the   last layer bias values are responsible for the pre-   dicted class , and propose a way to back - propagate   their importance . Michel and Neubig ( 2018 ) fine-   tuned the biases of the output softmax in an NMT   systems , to personalize the output vocabulary ,   and Frankle et al . ( 2020 ) have demonstrated that   randomly - initialized CNNs achieve reasonable ac-   curacy after training the batch - norm layers alone .   Finally , and closest to our work , Cai et al . ( 2020 )   demonstrate that bias - only fine - tuning similar to   ours is effective also for adaptation of pre - trained   computer vision models . Our work empirically   shows the importance and power of the bias param-   eters to substantially change the networks ‚Äô behav-   ior , calling for further analysis and attention on the   bias terms .   6 Conclusions   We propose BitFit , a novel method for localized ,   fast fine - tuning of pre - trained transformers for end-   tasks . The method focuses the finetuning on a spe-   cific fraction of the model parameters ‚Äî the biases ‚Äî   and maintains good performance in all GLUE tasks   we evaluated on . The focus on modifying a small   group of parameters eases deployment , as the vast   majority of the parameters of the model are shared   between various NLP tasks . It also allows for ef-   ficient hardware implementations that hard - wiremost of the network computation with the pre-   trained weights , while only allowing few change-   able parts for inference time .   Besides its empirical utility , the remarkable ef-   fectiveness of bias - only fine - tuning raises intrigu-   ing questions on the fine - tuning dynamics of pre-   trained transformers , and the relation between the   bias terms and transfer between LM and new tasks .   Acknowledgments   This project has received funding from the Euro-   pean Research Council ( ERC ) under the European   Union ‚Äôs Horizon 2020 research and innovation   programme , grant agreement No . 802774 ( iEX-   TRACT ) .   References567A Appendices   A.1 Layer naming   For convenience , we relate the notation used in the   paper with the names of the corresponding parame-   ters in the popular HuggingFace ( Wolf et al . , 2020 )   implementation .   A.2 Training Details   To perform classification with BERT , we follow the   approach of Devlin et al . ( 2018 ) , and attach a linear   layer to the contextual embedding of the [ CLS ]   token to predict the label . The GLUE tasks are fed   into BERT using the standard procedures .   We optimize using AdamW ( Loshchilov and Hut-   ter , 2017 ) , with batch sizes of 16 . For full fine-   tuning , we used initial learning rates in { 1e-5 , 2e-5 ,   3e-5 , 5e-5 } , and for the bias - only experiments we   used initial learning rates in { 1e-4 , 4e-4 , 7e-4 , 1e-   3}as the smaller rates took a very long time to   converge on some of the tasks . With the larger   learning rates , the bias - only fine - tuning converged   in 8 or fewer epochs for most tasks , and up to 20   epochs on the others . We did not perform hyper-   parameter optimization beyond the minimal search   over 4 learning rates . In each evaluation we report   X¬±Y where X is the average result for training   5 models with 5 different random seeds , Y is the   standard deviation .   To perform classification with RoBERTa , we   follow the above details but without hyperparam-   eter search over the learning rates , for bias - only   fine - tuning we used 1e-4 as learning rate and for   full fine - tuning we used 1e-5 as learning rate .   As Mosbach et al . ( 2020 ) show , fine - tuning   BERT and RoBERTa is a unstable due   to vanishing gradients . BitFit allows for the usage   of bigger learning rates , and overall the optimiza-   tion process is much more stable , when compared   with a full fine - tuning .   A.3 GLUE Benchmark   We provide information on the GLUE tasks we   evaluated on , as well as on the evaluation metrics .   We test our approach on the following subset of   the GLUE ( Wang et al . , 2018 ) tasks : The Corpus   of Linguistic Acceptability ( CoLA ; Warstadt et al .   ( 2018 ) ) , The Stanford Sentiment Treebank ( SST-   2 ; Socher et al . ( 2013 ) ) , The Microsoft Research   Paraphrase Corpus ( MRPC ; Dolan and Brockett   ( 2005 ) ) , The Quora Question Pairs ( QQP ; Iyer et al .   ( 2017 ) ) , The Semantic Textual Similarity Bench-   mark ( STS - B ; Cer et al . ( 2017 ) ) , The Multi - Genre   Natural Language Inference Corpus ( MNLI ; Bow-   man et al . ( 2015 ) ) , The Stanford Question Answer-   ing Dataset ( QNLI ; Rajpurkar et al . ( 2016b ) ) and   The Recognizing Textual Entailment ( RTE ; Dagan   et al . ( 2005 ) ) .   The metrics that we used to evaluate GLUE   Benchmark are in Table 5 . Learning rate config-   urations for best performing models are in Table   6 . For all the experiments we used the common   train : dev : test partition of GLUE .   A.4 Amount of change in bias terms8   A.5 SQuAD F1 Results9   Hua ShenyTongshuang Wu}Wenbo GuoyTing - Hao ‚Äò Kenneth ‚Äô Huang y   yCollege of Information Sciences and Technology , Pennsylvania State University   } Paul G. Allen School of Computer Science and Engineering , University of Washington   { huashen218,wzg13,txh710}@psu.edu   wtshuang@cs.washington.edu   Abstract   1 Introduction   While neural networks have recently led to large   improvements in NLP , most of the models make   predictions in a black - box manner , making them   indecipherable and untrustworthy to human users .   In an attempt to faithfully explain model decisions   to humans , various work has looked into extract-   ingrationales from text inputs ( Jain et al . , 2020 ;   Paranjape et al . , 2020 ) , with rationale deÔ¨Åned as   the ‚Äú shortest yet su cient subset of input to predict   the same label ‚Äù ( Lei et al . , 2016 ; Bastings et al . ,   2019 ) . The underlying assumption is two - fold : ( 1 )   by retaining the label , we are extracting the texts   used by predictors ( Jain et al . , 2020 ) ; and ( 2 ) short   rationales are more readable and intuitive for end-   users , and thus preferred for human understand-   ing ( Vafa et al . , 2021 ) . Importantly , prior work   has knowingly traded o  some amount of model   performance to achieve the shortest possible ratio-   nales . For example , when using less than 50 % of   text as rationales for predictions , Paranjape et al .   ( 2020 ) achieved an accuracy of 84.0 % ( compared   to 91.0 % if using the full text ) . However , the as-   sumption that the shortest rationales have better   human interpretability has not been validated by10human studies ( Shen and Huang , 2021 ) . Moreover ,   when the rationale is too short , the model has much   higher chance of missing the main point in the full   text . In Figure 1 A , although the model can make   the correct positive prediction when using only 20 %   of the text , it relies on a particular adjective , ‚Äú life-   arming , ‚Äù which is seemingly positive but does   not reÔ¨Çect the author ‚Äôs sentiment . These rationales   may be confusing when presented to end - users .   In this work , we ask : Are shortest rationales re-   ally the best for human understanding ? To answer   the question , we Ô¨Årst design Limited Ink , a self-   explaining model that Ô¨Çexibly extracts rationales   at any target length ( Figure 1 A).Limited Inkallows   us to control and compare rationales of varying   lengths on input documents . Besides controls on   rationale length , we also design Limited Ink ‚Äôs sam-   pling process and objective function to be context-   aware ( i.e. ,rank words based on surrounding con-   text rather than individually , Figure 1 B ) and co-   herent ( i.e. ,prioritize continuous phrases over dis-   crete tokens , Figure 1 C ) . Compared to existing   baselines ( e.g. ,Sparse - IB ) , Limited Inkachieves   compatible end - task performance and alignment   with human annotations on the ERASER ( DeY-   oung et al . , 2020 ) benchmark , which means it can   represent recent class of self - explaining models .   We use Limited Inkto conduct user studies to   investigate the e  ect of rationale length on human   understanding . SpeciÔ¨Åcally , we ask MTurk par-   ticipants to predict document sentiment polarities   based on only Limited Ink - extracted rationales . By   contrasting rationales at Ô¨Åve di  erent length lev-   els , we Ô¨Ånd that shortest rationales are largely not   the best for human understanding . In fact , humans   do not perform better prediction accuracy and con-   Ô¨Ådence better than using randomly masked texts   when rationales are too short ( e.g. , 10 % of input   texts ) . In summary , this work encourages a rethink-   ing of self - explaining methods to Ô¨Ånd the right   balance between brevity and su ciency .   2 L imited Ink   2.1 Self - Explaining Model DeÔ¨Ånition   We start by describing typical self - explaining meth-   ods ( Lei et al . , 2016 ; Bastings et al . , 2019 ; Paran-   jape et al . , 2020 ) . Consider a text classiÔ¨Åcation   dataset containing each document input as a tu-   ple(x;y ) . Each input xincludes nfeatures ( e.g. ,   sentences or tokens ) as x=[x;x;:::;x ] , and   yis the prediction . The model typically consistsof an identiÔ¨Åer idn()to derive a boolean mask   m=[m;m;:::;m ] , where m2f1;0gindicates   whether feature xis in the rationale or not . Note   that the mask mis typically a binary selection   from the identiÔ¨Åer ‚Äôs probability distribution , i.e. ,   midn(x ) . Then it extracts rationales zby   z = m  x , and further leverages a classiÔ¨Åer cls( )   to make a prediction ybased on the identiÔ¨Åed ratio-   nales as y = cls(z ) . The optimization objective is :   minEL(cls(z);y)|                    { z                    } +    ( m)| { z } ( 1 )   whereandare trainable parameters of iden-   tiÔ¨Åer andclassiÔ¨Åer .   ( m)is the regularization func-   tion on mask and is the hyperparameter .   2.2 Generating Length Controllable   Rationales with Contextual Information   We next elaborate on the deÔ¨Ånition and method of   controlling rationale length in Limited InkAssum-   ing that the rationale length is kas prior knowledge ,   we enforce the generated boolean mask to sum up   tokask = P(m ) , where m = idn(x;k ) . Exist-   ing self - explaining methods commonly solve this   by sampling from a Bernoulli distribution over in-   put features , thus generating each mask element m   independently conditioned on each input feature   x(Paranjape et al . , 2020 ) . For example , in Fig-   ure 1 B ) , ‚Äú life a rming ‚Äù is selected independent   of the negation context ‚Äú not ‚Äù before it , which con-   tradicts with the author ‚Äôs intention . However , these   methods potentially neglect the contextual input   information . We leverage the concrete relaxation   of subset sampling technique ( Chen et al . , 2018 )   to incorporate contextual information into ratio-   nale generation process ( see Figure 1 B ) , where   we aim to select the top - k important features over   allnfeatures in input xvia Gumbel - Softmax Sam-   pling ( i.e. ,applying the Gumbel - softmax trick to   approximate weighted subset sampling process ) .   To further guarantee precise rationale length con-   trol , we deploy the vector and sort regularization   on mask m(Fong et al . , 2019 ) . See more model   details in Appendix A.1 .   2.3 Regularizing Rationale Continuity   To further enforce coherent rationale for human   interpretability , we employ the Fused Lasso to en-   courage continuity property ( Jain et al . , 2020 ; Bast-   ings et al . , 2019 ) . The Ô¨Ånal mask regularization is:11   For BERT - based models , which use subword-   based tokenization algorithms ( e.g. , WordPiece ) ,   we assign each token ‚Äôs importance score as its sub-   tokens ‚Äô maximum score to extract rationales during   model inference ( see Figure 1 C ) .   3 Model Performance Evaluation   We Ô¨Årst validate Limited Inkon two common ratio-   nale evaluation metrics , including end - task perfor-   mance and human annotation agreement .   3.1 Experimental Setup   We evaluate our model on Ô¨Åve text classiÔ¨Åcation   datasets from the ERASER benchmark ( DeYoung   et al . , 2020 ) . We design the identiÔ¨Åer module   inLimited Inkas a BERT - based model , followed   by two linear layers with the ReLU function and   dropout technique . The temperature for Gumbel-   softmax approximation is Ô¨Åxed at 0.1 . Also , we   deÔ¨Åne the classiÔ¨Åer module as a BERT - based se-   quence classiÔ¨Åcation model to predict labels . We   train Ô¨Åve individual self - explaining models of dif-   ferent rationale lengths with training and validation   sets , where we set the rationale lengths as f10 % ,   20%,30%,40%,50%gof all input text . Then we   select one out of the Ô¨Åve models , which has the best   weighted average F1 score , to compare with cur-   rent baselines on end - task performance and human   annotation agreement on test sets . Note that we   use all models with Ô¨Åve rationale lengths in human   evaluation described in Section 4 .   Baselines . We compare Limited Inkwith four   baselines . Full - Text consists of only the clas-   siÔ¨Åer module with full - text inputs . Sparse - N en-   forces shortest rationales by minimizing rationale   mask length ( Lei et al . , 2016 ; Bastings et al . , 2019 ) .   Sparse - C controls rationale length by penalizing   the mask when its length is less than a thresh-   old ( Jain et al . , 2020 ) . Sparse - IB enables length   control by minimizing the KL - divergence betweenthe generated mask with a prior distribution ( Paran-   jape et al . , 2020 ) . See Appendix A.1 for more   model and baseline details .   3.2 Evaluation Results   End - Task Performance . Following metrics   in DeYoung et al . ( 2020 ) , we report the weighted   average F1 scores for end - task classiÔ¨Åcation   performance . Among Ô¨Åve Limited Inkmodels   with di  erent rationale lengths , Table 1 reports   the model with the best end - task performance on   the test set . We observe that Limited Inkperforms   similarly to or better than the self - explaining   baselines in all Ô¨Åve datasets . See ablation studies   in Appendix A.2 .   Human - Annotated Rationale Agreement . We   calculate the alignment between generated ratio-   nales and human annotations collected in the   ERASER benchmark ( DeYoung et al . , 2020 ) . As   also shown in Table 1 , we report the Token - level   F1 ( F1 ) metric along with corresponding Precision   ( P ) and Recall ( R ) scores . The results show that   Limited Inkcan generate rationales that are consis-   tent with human annotations and comparable to   self - explaining baselines in all datasets .   4 Human Evaluation   Equipped with Limited Ink , we next carry out hu-   man studies to investigate the e  ect of rationale   length on human understanding .   4.1 Study Design   Our goal is to quantify human performance on pre-   dicting the labels and conÔ¨Ådence based solely on   the rationales with di  erent lengths . To do so , we   control Limited Inkto extract rationales of di  er-   ent lengths , and recruit Mechanical Turk ( MTurk )   workers to provide predictions and conÔ¨Ådence .   Dataset & rationale extraction . We focus on   sentiment analysis in user study , and randomly sam-   ple 100 reviews from the Movie Reviews ( Zaidan12   and Eisner , 2008 ) test set that have correct model   predictions . Then , we extract Ô¨Åve rationales for   each review using Limited Ink , with lengths from   10 % to 50 % , with an increment of 10 % .   Since human accuracy likely increases when par-   ticipants see more words ( i.e. ,when the lengths of   rationales increase ) , we also create a Random ratio-   nale baseline , where we randomly select words of   the same rationale length on the same documents   ( 10 % to 50 % ) while taking the continuity constraint   into consideration . More details of Random base-   line generation are in Appendix A.3.1 .   Study Procedure . The study is completed in two   steps . First , we posted a qualiÔ¨Åcation Human In-   telligence Tasks ( HITs , $ 0.50 per assignment ) on   MTurk to recruit 200 qualiÔ¨Åed workers . Next ,   the 200 recruited workers can participate the task   HIT ( $ 0.20 per assignment , 7 assignments posted )   which contains Ô¨Åve distinct movie reviews , with   varying rationale lengths ( 10%-50 % ) . In task HIT ,   as key components shown in Figure 2 , we only dis-   play the rationales and mask all other words with   ellipses of random length , such that participants   can not infer the actual review length . Then partic-   ipants are asked to guess the sentiment of the full   review , and provide their conÔ¨Ådence level based on   a Ô¨Åve - point Likert Scale ( Likert , 1932 ) . The full   user interface is in Appendix A.3.2 .   Participants recruiting and grouping . With   each review having ten distinct rationales ( Ô¨Åve from   Limited Inkand Ô¨Åve Random ) , if these rationale con-   ditions were randomly assigned , participants are   likely to see the same review repeatedly and grad-   ually see all the words . We carefully design our   study to eliminate such undesired learning e  ect .   More speciÔ¨Åcally , we group our 100 reviews into   20 batches , with Ô¨Åve reviews in each batch ( Step 1   in Figure 3 ) . For each batch , we create Ô¨Åve HITs   forLimited InkandRandom , respectively , such that   all the rationale lengths of Ô¨Åve reviews are covered   by these 10 HITs ( Step 2 in Figure 3 ) . Further , we   make sure each participant is only assigned to one   unique HIT , so that each participant can only see   a review once . To do so , we randomly divide the   200 qualiÔ¨Åed workers into 10 worker groups ( 20   workers per group ) , and pair one worker group with   only one HIT in each batch . This way , each HIT   can only be accomplished by one worker group . As   our participant control is more strict than regular   data labeling tasks on MTurk , we keep the HITs   open for 6 days . 110 out of 200 distinct workers   participated in the main study , and they completed   1,169 of 1,400 assignments .   4.2 Results   We show the human prediction accuracy and con-   Ô¨Ådence results in Figure 4 . We Ô¨Ånd that the   best explanations for human understanding are   largely not the shortest rationales ( 10 % length   level ): here , the human accuracy in predicting   model labels is lower than for the random base-   line ( 0.61 vs. 0.63 ) , indicating that the shortest   rationales are not the best for human understand-   ing . There is a signiÔ¨Åcant di  erence in human pre-   dicted labels ( i.e. ,‚Äúpositive ‚Äù = 1,‚Äúnegative ‚Äù = 2 ) be-   tween Limited Ink(M=1.24,SD = 0.71 ) and Random13   ( M=1.32,SD = 0.54 ) ; t(1169 ) = 2.27 , p = 0.02 . Ta-   ble 2 shows human performance for each category .   Additionally , notice that the slope of our model ‚Äôs   accuracy consistently Ô¨Çattens as the rationale in-   creases , whereas the random baseline does not dis-   play any apparent trend and is obviously lower than   our model at higher length levels ( e.g. , 40 % ) . We   hypothesize that this means our model is ( 1 ) indeed   learning to reveal useful rationales ( rather than just   randomly displaying meaningless text ) , and ( 2 ) the   amount of information necessary for human un-   derstanding only starts to saturate at around 40 %   of the full text . This creates a clear contrast with   prior work , where most studies extract 10 - 30 % of   the text as the rationale on the same dataset ( Jain   et al . , 2020 ; Paranjape et al . , 2020 ) . The eventually   Ô¨Çattened slope potentially suggests a sweet spot   to balance human understanding on rationales and   sucient model accuracy .   5 Discussion   By examining human prediction performance on   Ô¨Åve levels of rationale lengths , we demonstrate that   the shortest rationales are largely not the best for   human understanding . We are aware that this work   has limitations . The Ô¨Åndings are limited to Movie   Reviews dataset , and we only evaluate human per-   formance with rationales generated by the pro-   posed Limited Ink . Still , our Ô¨Åndings challenge the   ‚Äú shorter is better ‚Äù assumption commonly adopted   in existing self - explaining methods . As a result , we   encourage future work to more cautiously deÔ¨Åne   the best rationales for human understanding , and   trade o  between model accuracy and rationale   length . More concretely , we consider that ratio-   nale models should Ô¨Ånd the right balance betweenbrevity and su ciency . One promising direction   could be to clearly deÔ¨Åne the optimal human inter-   pretability in a measurable way and then learn to   adaptively select rationales with appropriate length .   6 Related Work   Self - explaining models . Self - explaining models ,   which condition predictions on their rationales , are   considered more trustworthy than post - hoc expla-   nation techniques ( Rajagopal et al . , 2021 ) . How-   ever , existing e  orts often enforce minimal ratio-   nale length , which degrade the predictive perfor-   mance ( Yu et al . , 2019 ; Bastings et al . , 2019 ; Jain   et al . , 2020 ) . Paranjape et al . ( 2020 ) improves this   by proposing an information bottleneck approach   to enable rationale length control at the sentence   level . In this paper , Limited Inkfurther enables   length control at the token level to allow more Ô¨Çex-   ibility needed for our human studies .   Human - grounded evaluation . A line of stud-   ies evaluated model - generated rationales by com-   paring them against human - annotated explana-   tions ( Carton et al . , 2020 ; Paranjape et al . , 2020 ) .   Some other studies collect feedback from users to   evaluate the explanations , such as asking people   to choose a preferred model ( Ribeiro et al . , 2016 )   or to guess model predictions only based on ratio-   nales ( Lertvittayakumjorn and Toni , 2019 ; Shen   and Huang , 2020 ) .   7 Conclusion   To investigate if the shortest rationales are best un-   derstandable for humans , this work presents a self-   explaining model , Limited Ink , that achieves com-   parable performance with current self - explaining   baselines in terms of end - task performance and   human annotation agreement . We further use Lim-   itedInkto generate rationales for human studies   to examine how rationale length can a  ect human   understanding . Our results show that the shortest   rationales are largely not the best for human un-   derstanding . This would encourage a rethinking of   rationale methods to Ô¨Ånd the right balance between   brevity and su ciency .   8 Acknowledgment   We thank Chieh - Yang Huang for helpful comments   on the paper , Bhargavi Paranjape for technical dis-   cussion of methods , and the crowd workers for   participating in this study . We also thank the anony-   mous reviewers for their constructive feedback.149 Ethical Considerations   This work shows that the shortest rationales are   often not the best for human understanding . We   thus advocate for studying how users interact with   machine - generated rationales . However , we are   aware that using rationales to interpret model pre-   diction could pose some risks for users . Rationales   omit a signiÔ¨Åcant portion of the contents ( in our   case , 50 % to 90 % of the words in a movie review   are omitted ) , which could convey information in-   correctly or mislead users . Furthermore , machine-   learned rationales could encode some unwanted   biases ( Chuang et al . , 2021 ) . We believe that such   risks should be explicitly communicated with users   in real - world applications .   References15A Appendix   A.1 Model Details and Hyperparameters   A.1.1 Methodology Details   Concrete Relaxation of Subset Sampling Pro-   cess . Given the output logits of identiÔ¨Åer , we   use Gumbel - softmax ( Jang et al . , 2017 ) to gen-   erate a concrete distribution as c=[c;:::c]   Concrete ( idn(x ) ) , represented as a one - hot vec-   tor over nfeatures where the top important fea-   ture is 1 . We then sample this process ktimes in   order to sample top - k important features , where   we obtain kconcrete distributions as fc;:::;cg .   Next we deÔ¨Åne one n - dimensional random vec-   tormto be the element - wise maximum of these k   concrete distributions along nfeatures , denoted as   m = maxfcg . Discarding the overlapping fea-   tures to keep the rest , we then use mas the k - hop   vector to approximately select the top - k important   features over document x.   Vector and sort regularization . We deploy a   vector and sort regularization on mask m(Fong   et al . , 2019 ) , where we sort the output mask min   a increasing order and minimize the Lnorm be-   tween mand a reference ÀÜmconsisting of n kzeros   followed by kones .   A.1.2 Model Training Details   Training and inference . During training , we se-   lect the Adam optimizer with the learning rate at 2e-   5 with no decay . We set hyperparameters in Equa-   tion 5 and 2 as =1e 4,v=0:5andv=0:3   and trained 6 epochs for all models . Furthermore ,   we train Limited Inkon a set of sparsity levels as   k = f10%;20%;30%;40%;50%gand choose mod-   els with optimal predictive performance on valida-   tion sets .   A.1.3 Details of Self - Explaining Baselines   We compare our method with state - of - the - art self-   explaining baseline models .   Sparse - N ( Minimization Norm ) . This method   learns the short mask with minimal LorL   norm ( Lei et al . , 2016 ; Bastings et al . , 2019 ) , which   penalizes for the total number of selected words in   the explanation .   minEL(cls(z);y)+jjmjj(3 )   Sparse - C ( Controlled Norm Minimization ) .   This method controls the mask sparsity through16a tunable predeÔ¨Åned sparsity level  ( Chang et al . ,   2020 ; Jain et al . , 2020 ) . The mask is penalized as   below as long as the sparsity level  is passed .   minEL(cls(z);y)+max(0;jjmjj   N   )   ( 4 )   where N is the input length and jjmjjdenotes   mask penalty with Lnorm .   Sparse IB ( Controlled Sparsity with Informa-   tion Bottleneck ) . This method introduces a prior   probability of z , which approximates the marginal   p(m)of mask distribution ; and p(mjx)is the para-   metric posterior distribution over mconditioned   on input x(Paranjape et al . , 2020 ) . The sparsity   control is achieved via the information loss term ,   which reduces the KL divergence between the pos-   terior distribution p(mjx)that depends on xand a   prior distribution r(m ) that is independent of x.   minEL(cls(z);y)+KL[p(mjx);r(m ) ]   ( 5 )   A.2 Ablation Study on Model Components   We provide an ablation study on the Movie dataset   to evaluate each loss term ‚Äôs inÔ¨Çuence on end - task   prediction performance , including Precision , Re-   call , and F1 scores . The result is shown in Table 3 .   SetupsEnd - Task Prediction   Precision Recall F1   No Su ciency 0.25 0.50 0.34   No Continuity 0.82 0.81 0.81   No Sparsity 0.80 0.79 0.79   No Contextual 0.83 0.83 0.83   Our Model 0.91 0.90 0.90   A.3 Additional Details of Human Study   A.3.1 Generating Random Baselines   Human accuracy likely increases when participants   can see more words , i.e. , when the lengths of ra-   tionales increase . If a rationale and a random text   span have the same number of words , the rationale   should help readers predict the label better . We   created a simple baseline that generated rationales   by randomly selecting words to form the rationales . We could control ( 1 ) how many words to select and   ( 2 ) how many disjointed rationales to produce . In   the study , we set these two numbers to be identical   to that of L imited Inkat each length level .   In detail , given the rationale length k , we Ô¨Årst got   the count of total tokens in rationale as # tokens = k.   Next , we computed the average number of rationale   segments m , which are generated by Limited Ink ,   over the Movie dataset . We randomly selected m   spans with total tokens ‚Äô count as # tokens from the   full input texts , thus obtaining the random baselines .   We evenly separated 10 worker groups to Ô¨Ånish Ô¨Åve   random baseline HITs and Limited InkHITs each .   We determined that good model rationales should   get higher human accuracy compared with same-   length random baselines .   A.3.2 Human Evaluation User Interface   We provide our designed user interfaces used in the   human study . SpeciÔ¨Åcally , we show the interface   of the human study panel in Figure 5 ( B ) . We also   provide the detailed instructions for workers to un-   derstand our task , the instruction inteface is shown   in Figure 6.171819   Clara Meister Tiago Pimentel Thomas Hikaru ClarkRyan Cotterell Roger LevyETH Z√ºrich University of Cambridge Massachusetts Institute of Technology   clara.meister@inf.ethz.ch tp472@cam.ac.uk thclark@mit.edu   ryan.cotterell@inf.ethz.ch rplevy@mit.edu   Abstract   1 Introduction   Reading puts the unfolding of linguistic input in   the hands ‚Äî or , really , the eyes ‚Äî of the reader . Con-   sequently , it presents a unique opportunity to gain   a better understanding of how humans comprehend   written language . The rate at which humans choose   to read text ( and process its information ) should   be determined by their goal of understanding   it . Ergo , examining where a reader spends their   time should help us to understand the nature of   language comprehension processes themselves .   Indeed , studies analyzing reading times have been   employed to explore a number of psycholinguistic   theories ( e.g. , Smith and Levy , 2013 ; Futrell et al . ,   2020 ; Van Schijndel and Linzen , 2021 ) .   One behavior revealed by such studies is the   tendency for humans to spend more timeon   the last word of a sentence or clause . While theexistence of such wrap - up effects is well - known   ( Just et al . , 1982 ; Hill and Murray , 2000 ; Rayner   et al . , 2000 ; Camblin et al . , 2007 ) , the cognitive   processes giving rise to them are still not fully   understood . This is likely ( at least in part ) due   to the dearth of analyses targeting naturalistic   sentence - final reading behavior . First , most studies   of online processing omit data from these words   to explicitly control for the confounding factors   wrap - up effects introduce ( e.g. , Smith and Levy ,   2013 ; Goodkind and Bicknell , 2018 ) . Second ,   the few studies on wrap - up effects rely on small   datasets , none of which analyze naturalistic text   ( Just and Carpenter , 1980 ; Rayner et al . , 2000 ;   Kuperberg et al . , 2011 ) . This work addresses this   gap , using several large corpora of reading time   data . Specifically , we study whether information-   theoretic concepts ( such as surprisal ) provide   insights into the cognitive processes that occur   at a sentence ‚Äôs boundary . Notedly , information-   theoretic approaches have been proven effective for   analyzing sentence - medial reading time behavior .   We follow the long line of work that has   connected information - theoretic measures and   psychometric data ( Frank et al . , 2015 ; Goodkind   and Bicknell , 2018 ; Wilcox et al . , 2020 ; Meister   et al . , 2021 , inter alia ) , employing similar methods   to build models of sentence- and clause - final RTs .   Using surprisal estimates from state - of - the - art lan-   guage models , we search for a link between wrap-   up effects and the information content within a   sentence . We find that the distribution of surprisals   of prior context is often predictive of sentence- and   clause - final reading times ( RTs ) , while not adding   significant predictive power to models of sentence-   medial RTs . This result suggests that the nature   of cognitive processes involved during the reading   of these boundary words may indeed be different   than those at other positions . Such findings lend   support to several prior hypotheses regarding   which processes may underlie wrap - up effects20(e.g . , the resolution of prior ambiguities ) , while   providing evidence against other speculations ( e.g. ,   that the time spent at sentence boundaries can be   quantified with a constant factor , independent of   the processing difficulty of the text itself ) .   2 The Process of Reading   Decades of research on reading behavior have   improved our understanding of the cognitive   processes involved in reading comprehension ( Just   and Carpenter , 1980 ; Rayner and Clifton , 2009 ,   inter alia ) . Here , we will briefly describe overar-   ching themes that are relevant for understanding   wrap - up effects .   2.1 Incrementality and its Implications   It is widely accepted that language processing is   incremental in nature , i.e. , readers process text   one word at a time ( Hale , 2001 , 2006 ; Rayner and   Clifton , 2009 ; Boston et al . , 2011 , inter alia ) . Con-   sequently , much can be uncovered about reading   comprehension via studies that analyze cognitive   processing at the word - level . Many pyscholin-   guistic studies make use of this notion , taking   per - word RTs in self - paced reading ( SPR ) or eye-   tracking studies to be a direct reflection of the pro-   cessing load of that word ( e.g. , Smith and Levy ,   2013 ; Van Schijndel and Linzen , 2021 ) . This   RT ‚Äì processing effort relationship then allows us   to identify relationships between a word ‚Äôs pro-   cessing load and its attributes ( e.g. , surprisal or   length)‚Äîwhich in turn hints at the underlying cog-   nitive processes involved in comprehension . One   prominently studied attribute is word predictabil-   ity ; a notion naturally quantified by surprisal ( also   known as Shannon ‚Äôs ( 1948 ) information content ) .   Formally , the surprisal of a word wis defined as   s(w)=‚àílogp(w|w ) , i.e. , a unit ‚Äôs negative   log - probability given the prior sentential context   w. Notedly , this operationalization provides a   way of quantifying how our prior expectations can   affect our ability to process a linguistic signal .   There are several hypothesis about the math-   ematical nature of the relationship between per-   word surprisal and processing load . While there   has been much empirical proof that surprisal es-   timates serve as a good predictor of word - level   RTs ( Smith and Levy , 2013 ; Goodkind and Bick-   nell , 2018 ; Wilcox et al . , 2020 ) , the data observedfrom sentence - final words appears not to follow the   same relationship . Specifically , in comparison to   sentence - medial words , sentence- or clause - final   words are associated with increased RTs in self-   paced studies ( Just et al . , 1982 ; Hill and Murray ,   2000 ) and both increased fixation and regression   times in eye - tracking studies ( Rayner et al . , 2000 ;   Camblin et al . , 2007 ) . Such behavior has also   been observed in controlled settings ‚Äî for exam-   ple , Rayner et al . ( 1989 ) found that readers fixated   longer on a word when it ended a clause than when   the same word did not end a clause .   Such wide - spread experimental evidence sug-   gests sentence - final and sentence - medial reading   behaviors differ from each other , and that other   cognitive processes ( besides standard word - level   processing ) effort may be at play . Yet unfortunately ,   these wrap - up effects have received relatively little   attention in the psycholinguistic community : Most   reading time studies simply exclude sentence - final   ( or even clause - final ) words from their analyses ,   claiming that the ( poorly - understood ) effects are   confounding factors in understanding the reading   process ( e.g. , Frank et al . , 2013 , 2015 ; Wilcox   et al . , 2020 ) . Rather , we believe this data can   potentially provide new insights in their own right .   2.2 Wrap - up Effects   It remains unclear what exactly occurs in the mind   of the reader at the end of a sentence or clause .   Which cognitive processes are encompassed by the   term wrap - up effects ? Several theories have been   posited . First , Just and Carpenter ( 1980 ) hypoth-   esize that wrap - up effects include actions such as   ‚Äú the constructions of inter - clause relations . ‚Äù Second ,   Rayner et al . ( 2000 ) suggest they might involve   attempts to resolve previously postponed compre-   hension problems , which could have been deferred   in the hope that upcoming words would resolve   the problem . Third , Hirotani et al . ( 2006 ) posit the   hesitation when crossing clause boundaries is out   of efficiency ( Jarvella , 1971 ) ; readers do not want   to have to return to the clause later , so they take the   extra time to make sure there are no inconsistencies   in the prior text .   While some prior hypotheses have been largely   dismissed ( see Stowe et al . , 2018 for a more   detailed summary ) due to , e.g. , the wide - spread   support of theories of incremental processing ,   most others lack formal testing in naturalistic   reading studies . We attempt to address this gap.21Concretely , we posit the relationship between   text ‚Äôs information - theoretic attributes and its   observed wrap - up times can provide an indication   of the presence ( or lack ) of several cognitive   processes that are potentially a part of sentence   wrap - up . For example , high - surprisal words in the   preceding context may correlate with the presence   of ambiguities in the text ; they may also correlate   with complex linguistic relationships of the current   text with prior sentences ‚Äî which are two driving   forces in the theories given above . Consequently ,   in this work , we ask whether the reading behavior   observed at the end of a sentence or clause can be   described ( at least partially ) by the distribution of   information content in the preceding context , as   this may give insights for several prior hypotheses   about wrap - up effects .   3 Language Models as Predictors of   Psychometric Data   Formally , a language model bpis a probability dis-   tribution over natural language sentences . In the   case when bpis locally normalized , which is the pre-   dominant case for today ‚Äôs neural language models ,   bpis defined as the product of conditional probabil-   ity distributions : bp(y ) = Qbp(y|y ) , where   eachbp(¬∑|y)is a distribution with support over   linguistic units y(typically words ) from a set vocab-   ularyV , which includes a special end - of - sequence   token . Consequently , we can use bpto estimate in-   dividual word probabilities . Model parameters are   typically estimated by minimizing the negative log-   likelihood of a corpus of natural language strings   C , i.e. , minimizing L(bp ) = ‚àíPlogbp(y ) .   One widely embraced technique in information-   theoretic psycholinguistics is the use of these lan-   guage models to estimate the probabilities required   for computing surprisal ( Hale , 2001 ; Demberg and   Keller , 2008 ; Mitchell et al . , 2010 ; Fernandez Mon-   salve et al . , 2012 ) . It has even been observed that a   language model ‚Äôs perplexitycorrelates negatively   with the psychometric predictive power provided   by its surprisal estimates ( Frank and Bod , 2011 ;   Goodkind and Bicknell , 2018 ; Wilcox et al . , 2020 ) .   If these language models keep improving at their   current fast pace ( Radford et al . , 2019 ; Brown et al . ,   2020 ) , exciting new results in computational psy-   cholinguistics may follow , connecting reading be-   havior to the statistics of natural language .   Predicting Reading Times . In the computa-   tional psycholinguistics literature , the RT ‚Äì surprisal   relationship is typically studied using predictive   models : RTs are predicted using surprisal estimates   ( along with other attributes such as number of char-   acters ) for the current word . The predictive power   of these models , together with the structure of the   model itself ( which defines a specific relationship   between RTs and surprisal ) , is then used as   evidence of the studied effect . While this paradigm   is successful in modeling sentence - medial RTs   ( Smith and Levy , 2013 ; Goodkind and Bicknell ,   2018 ; Wilcox et al . , 2020 ) , its effectiveness for   modeling sentence- and clause - final times is   largely unknown due to the omission of this data   from the majority of RT analyses .   A priori , we might expect per - word surprisal to   be a similarly powerful predictor of sentence and   clause - final RTs . Yet in Fig . 1 , we see that when   our baseline linear model ( described more precisely   in ¬ß 4 ) is fit to sentence - medial RTs , the residuals   for predictions of clause - final RTs appear to be   neither normally distributed nor centered around 0 .   Further , these trends appear to be different for eye-   tracking and SPR data , where the latter are skewed   towards lower values for all datasets . These re-22sults provide further confirmation that clause - final   data does not adhere to the same relationship with   RT as sentence - medial data , a phenomenon that   may perhaps be accounted for by additional fac-   tors at play in the comprehension of clause - final   words . Thus , we ask whether taking into account   information from the entire prior context can give   us a better model of these clause - final RTs .   To this end , we operationalize the information   contentin text w(of length T ) as:(w)=Ps(w)(k‚â•0 ) ( 1 )   where wmay be an entire sentence , or only its first   Twords . Notably , the case of k= 0 returns T ;   under k= 1 , we get the total information content   ofw . For k > 1 , moments of high - surprisal will   disproportionately drive up the value of(w ) .   Such words may indicate , e.g. , moments of   ambiguity or uneven distributions of information   in text . Thus , how well(w)(as a function of   k ) predicts model sentence- and clause - final RTs   may indicate which attributes of prior text ( if any )   can be linked to the additional cognitive processes   involved in wrap - up effects .   4 Experiments   Data . We use reading time data from 5 corpora   over 2 modalities : the Natural Stories ( Futrell et al . ,   2018 ) , Brown ( Smith and Levy , 2013 ) , and UCL   ( SP ) ( Frank et al . , 2013 ) Corpora , which contain   SPR data , as well as the Provo ( Luke and Chris-   tianson , 2018 ) , Dundee ( Kennedy et al . , 2003 ) and   UCL ( ET ) ( Frank et al . , 2013 ) Corpora , which con-   tain eye movements during reading . All corpora are   in English . For eye - tracking data , we take reading   time to be the sum over all fixation times on that   word . We provide an analysis of regression ( a.k.a .   go - past ) time in App . B. We provide further details   regarding pre - processing in App . A.   Estimating Surprisal . We obtain surprisal esti-   mates from three language models : GPT-2 ( Rad-   ford et al . , 2019 ) , TransformerXL ( Dai et al . , 2019 )   and a 5 - gram model , estimated using Modified   Kneser ‚Äì Essen ‚Äì Ney Smoothing ( Ney et al . , 1994 ) .   We compute per - word surprisal as the sum of sub-   word surprisals , when applicable . Additionally ,   punctuation is included in these estimates , although   see App . B for results omitting punctuation , whichare qualitatively the same . More details are given   in App . A.   Evaluation . Following Wilcox et al . ( 2020 ) and   Meister et al . ( 2021 ) , we quantify the predictive   power of a variable of interest ( ( w)here ) as   the mean difference in log - likelihood ‚àÜLogLik of   a ( held - out ) data point when using a model with and   without that predictor . In other words , we train two   models to predict RTs ‚Äî one with and one without   access to(w)‚Äîthe difference in their pre-   dictive power is ‚àÜLogLik . A positive ‚àÜLogLik   value indicates the model with this predictor fits the   observed data more closely than a model without   this predictor . We use 10 - fold cross - validation to   compute ‚àÜLogLik values so as to avoid overfitting ,   taking the mean across the held - out folds as our   final metric . Our baseline model for predicting per-   word RTs contains predictors for surprisal , unigram   log - frequency , character length , and the interaction   of the latter two . These values , albeit computed on   the previous word , are also included to account for   spill - over effects ( Smith and Levy , 2013 ) . Surprisal   from two words back is included for SPR datasets .   Unless otherwise stated , GPT-2 estimates are used   for baseline surprisal estimates in all models .   Results . Here we explore the additional predic-   tive power thatgives us when modeling   clause - final RTs . In Fig . 2 , we observe that often   the additional information provided by(w )   indeed leads to better models of clause - final RTs .   In most cases , at some value of k > 0leads   to larger gains in predictive power than k= 0 .   Ergo , the information content of the preceding   text is more indicative of wrap - up behavior than   length alone . Further , while often within standard   error,(w)atk > 1provides more predictive   power than at k= 1across the majority of datasets .   This indicates that unevenness in the distribution   of surprisal is stronger than the total surprisal con-   tent alone as a predictor of clause - final RTs . The   same experiments for sentence - medial words show   these quantities are less helpful when modeling   their RTs . Note that these effects hold above and   beyond the spill - over effects from the window im-   mediately preceding the sentence boundary . The   effect of the distribution of surprisal throughout the   sentence is stronger for eye - tracking data than for   SPR ; further , the trends are even more pronounced   when measuring regression times for eye - tracking   data ( see App . B).23   Notably , we see some variation in trends across   datasets . Due to the nature of psycholinguistic   studies , it is natural to expect some variation due   to , e.g. , data collection procedures or inaccuracies   from measurement devices . Another ( perhaps more   influential ) factor in the difference in trends comes   from the variation in dataset sizes . We see that   with the smaller datasets ( e.g. , UCL and Provo ) ,   there may not be enough data to learn accurate   model parameters . This artifact may manifest as   the noisiness or a lack of a significant increase   in log - likelihood ( on a held - out test set ) over the   baseline that we observe in some cases .   When considering prior theories of wrap - up   processes , these results have several implications .   For example , they can be interpreted as supporting   and extending Rayner et al . ‚Äôs ( 2000 ) hypothesis ,   which suggests the extra time at sentence bound-   aries is spent resolving prior ambiguities . In this   case , the observed correlation between wrap - up   times and(w)may potentially be linked to   two factors : ( 1 ) contextual ambiguities increasing   variation in per - word information content ; and ( 2 )   contextual ambiguities being resolved at clause   ends . On the other hand , these results provide   evidence against the hypothesis that the cognitive   processes occurring during the comprehension   of sentence - medial and clause - final words are the   same . Further , it also goes against Hirotani et al . ‚Äôs   ( 2006 ) hypothesis ( discussed in ¬ß 2.2 ) , as the dif-   ferences in sentence - medial and clause - final times   can not be purely quantified by a constant factor.5 Conclusion   We attempt to shed light on the nature of wrap - up   effects by exploring the relationship between   clause - final RTs and information - theoretic at-   tributes of text . We find that operationalizations of   the information contained in preceding context lead   to better predictions of these RTs , while not adding   significant predictive power for sentence - medial   RTs . This suggests that information - theoretic   attributes of text can shed light on the cognitive   processes happening during the comprehension of   clause - final words . Further , these processes may   indeed be different in nature than those required for   sentence - medial words . In short , our results pro-   vide evidence ( either in support or against ) about   several theories of the nature of wrap - up processes .   Ethics Statement   All studies involving human evaluations were con-   ducted outside of the scope of this paper . The   authors foresee no ethical concerns with the work   presented in this paper .   Acknowledgments   RC acknowledges support from the Swiss National   Science Foundation ( SNSF ) as part of the ‚Äú The   Forgotten Role of Inductive Bias in Interpretability ‚Äù   project . TP is supported by a Facebook Fellowship   Award . RPL acknowledges support from NSF grant   2121074.24References2526A Experimental Setup   A.1 Data Pre - processing   We use the Moses decodertokenizer and punctua-   tion normalizer to pre - process all text data . Some   of the Hugging Face tokenizers for respective neu-   ral models performed additional tokenization ; we   refer the reader to the library documentation for   more details . We determine clause - final words as   all those ending in punctuation . Capitalization was   kept intact albeit the lowercase version of words   were used in unigram probability estimates . We es-   timate unigram log - probabilities on WikiText-103   using the KenLM ( Heafield , 2011 ) library with de-   fault hyperparameters . We removed outlier word-   level reading times ( specifically those with a z-   score > 3when the distribution was modeled as   log - linear ) .   A.2 Surprisal Estimates   We use pre - trained neural language models to com-   pute most surprisal estimates . For reproducibil-   ity , we employ the model checkpoints provided   by Hugging Face ( Wolf et al . , 2020 ) . Specifi-   cally , for GPT-2 , we use the default OpenAI ver-   sion ( gpt2 ) ; for TransformerXL , we use a ver-   sion of the model ( architecture described in Dai   et al . ( 2019 ) ) that has been fine - tuned on WikiText-   103 ( transfo - xl - wt103 ) ; for BERT , we use the   bert - base - cased version . Notably , BERT mod-   els the probability of a word given both prior   andlater context , which means it can only give   us pseudo estimates of surprisal . Both GPT-2   and BERT use sub - word tokenization . We ad-   ditionally use surprisal estimates from a 5 - gram   model trained on WikiText-103 using the KenLM   ( Heafield , 2011 ) library with default hyperparame-   ters for Kneser ‚Äì Essen ‚Äì Ney smoothing . B Additional Results   B.1 Regression Times Analysis2728   Jianzhu Bao , Jingyi Sun , Qinglin Zhu , Ruifeng XuHarbin Institute of Technology ( Shenzhen ) , ChinaJoint Lab of China Merchants Securities and HITSZPeng Cheng Laboratory , Shenzhen , China   { jianzhubao , sunjingyihit}@gmail.com   zhuqinglin@stu.hit.edu.cn , xuruifeng@hit.edu.cn   Abstract   1 Introduction   As a salient part of argument mining ( AM ) , the   analysis of dialogical argumentation has received   increasing research attention ( Morio and Fujita ,   2018 ; Chakrabarty et al . , 2019 ; Ji et al . , 2021 ;   Cheng et al . , 2021 ; Yuan et al . , 2021 ) . Argument   pair extraction ( APE ) , proposed by Cheng et al .   ( 2020 ) , is a new task within this field that focuses   on extracting interactive argument pairs from two   interrelated documents ( e.g. , peer reviewer and re-   buttal ) . Figure 1 presents an example of APE where   two interrelated documents are segmented into ar-   guments and non - arguments at sentence level . Two   arguments from different documents that discuss   the same issues are regarded as an argument pair .   Previous works ( Cheng et al . , 2020 , 2021 ) com-   monly address APE by decomposing it into two   sentence - level subtasks , i.e. , a sequence labeling   task and a sentence relation classification task .   These methods identify arguments by sentence-   level sequence labeling and determine whether two   sentences belong to the same argument pair by   sentence relation classification . Afterwards , the   argument pairs are inferred indirectly by certain   rules combining the results of the two subtasks .   However , such a paradigm only considers sentence-   level relations , while the holistic argument - level   relations can not be well modeled .   In this paper , we argue that APE can be con-   sidered as a multi - turn machine reading compre-   hension ( MRC ) task with two phases , i.e. , an AM   phase and an APE phase . Specifically , in the first   turn , a special AM query is employed to identify   all the arguments in the first document ( AM phase ) .   Afterwards , in each subsequent turn , every identi-   fied argument is treated as an APE query to extract   its paired arguments from the second document   ( APE phase ) . Similarly , this process can also be   performed in another direction , that is , using the29arguments identified in the second document as   queries to extract the paired arguments from the   first document . We train these two phases jointly   in a single MRC model , allowing them to benefit   each other . By considering arguments as queries ,   our proposed MRC framework can better capture   the interactions between each query argument and   the queried document , thus extracting the argument   pairs at the argument level . In addition , consider-   ing the long length of the documents , we utilize   Longformer ( Beltagy et al . , 2020 ) to model longer   contexts .   We evaluate our method on the large benchmark   dataset ( Cheng et al . , 2020 ) . Results show that our   proposed method significantly outperforms the cur-   rent state - of - the - art method by 7.11 % in Fscore .   2 Related Work   2.1 Argument Mining   Argument mining aims to analyze the structure of   argumentation , and it contains various subtasks ,   such as argument component identification ( Moens   et al . , 2007 ; Goudas et al . , 2015 ; Ajjour et al . ,   2017 ; Jo et al . , 2019 ) , argument relation predic-   tion ( Nguyen and Litman , 2016 ; Cocarascu et al . ,   2020 ; Jo et al . , 2021 ) , argumentation structure pars-   ing ( Stab and Gurevych , 2017 ; Kuribayashi et al . ,   2019 ; Morio et al . , 2020 ; Bao et al . , 2021 ) , argu-   mentation strategy analysis ( Khatib et al . , 2018 ;   Morio et al . , 2019 ) , etc .   Most previous works mainly focus on monologi-   cal argumentation , while dialogical argumentation   ( Morio and Fujita , 2018 ; Chakrabarty et al . , 2019 )   is relatively less emphasized . Recently , the anal-   ysis of dialogical argumentation has attracted in-   creasing attention in the field of argument mining .   Cheng et al . ( 2020 ) propose the APE task which   involves identifying arguments and extracting ar-   gument pairs in peer review and rebuttal . Ji et al .   ( 2021 ) identify interactive argument pairs in online   debate forums based on the discrete variational au-   toencoders . Cheng et al . ( 2021 ) address the APE   task based on a table - filling approach . Yuan et al .   ( 2021 ) construct a dialogical argumentation knowl-   edge graph for identifying argument pairs .   2.2 Machine Reading Comprehension   Machine reading comprehension ( MRC ) aims to   extract answer spans from a passage according to   a given query ( Seo et al . , 2017 ; Chen et al . , 2017 ;   Devlin et al . , 2019 ; Wen et al . , 2021 ) . FormulatingNLP tasks as MRC tasks has been a rising trend   in recent years , such as dependency parsing ( Gan   et al . , 2021 ) , relation extraction ( Levy et al . , 2017 ) ,   named entity recognition ( Li et al . , 2020 ) , senti-   ment analysis ( Chen et al . , 2021 ; Mao et al . , 2021 ) .   Unlike previous studies above , we employ a MRC   framework to analyze the complex argumentative   relations between two documents with excessively   long length .   3 Methodology   3.1 Task Formulation   We assume that two interrelated documents D=   ( s , s , ... , s)andD= ( s , s , ... , s)are   given , where sdenotes the j - th sentence in doc-   ument i. We need to extract the collection of ar-   gument pairs P={(arg , arg ) } , where arg   andargrespectively represent the arguments in   document DandD , and they compose the i - th   argument pair . Note that each argument consists of   one or more consecutive sentences . For example ,   arg= ( s , s , ... , s)where start and   enddenote the start and end sentence index .   To frame APE as a multi - turn MRC task , two   types of queries are constructed , i.e. , the argument   mining ( AM ) query and the argument pair extrac-   tion ( APE ) query . Intuitively , we could consider   the process of extracting argument pairs from the   perspective of two directions , i.e. , D‚ÜíDand   D‚ÜíD. For the D‚ÜíDdirection , we first   construct an AM query using a special token whose   corresponding answers are all the arguments in   document D. After recognizing all arguments   through the AM query , each recognized argument   is considered as an APE query whose correspond-   ing answers are its paired arguments in document   D. Similarly , for the D‚ÜíDdirection , we first   query document Dwith the AM query , and then   generate the APE queries for document D. Fi-   nally , the argument pairs can be derived by fusing   the answer results of all APE queries .   3.2 MRC Framework   3.2.1 Encoder   Since APE is a document - level task with exces-   sively long text , we adopt Longformer to capture   contextual information over longer distances . For   brevity , we only describe the MRC process in the   D‚ÜíDdirection below , and the D‚ÜíD   direction can be performed similarly.30Formally , we use a special token ‚Äú [ AM ] ‚Äù to rep-   resent the AM query q , which aims to identify   all the arguments A={arg}in document   Dwhere argindicates the k - th argument in D.   Then , each identified argument argis considered   as an APE query q , i.e. , q = arg=   ( s , ... , s ) . Note that we use gold arguments   as APE queries during training .   With these queries , we first concatenate the AM   query qand the document Das an input se-   quence for AM :   I= ( [ s ] , q,[/s],[s ] , s , s , ... , s,[/s ] )   ( 1 )   Also , we concatenate each APE query q   and the document Dto obtain multiple input se-   quences for APE :   I= ( [ s ] , q,[/s],[s ] , s , s , ... , s,[/s ] )   ( 2 )   where [ s]and[/s]are special tokens of Longformer .   Subsequently , for each sequence above , we feed   it into Longformer to get the hidden representation   of each token in the input document . Specifically ,   to enable Longformer to better learn argument-   specific representations , we add global attention   to the tokens of the query . Afterwards , we de-   rive the hidden representation of each sentence   through mean pooling on token representations in   this sentence . Further , to better model the long-   term dependency among sentences , the hidden rep-   resentations of sentences are fed into LSTM to de-   rive the contextual sentence representation matrix   H= ( h , h , . . . , h ) .   3.2.2 Answer Span Prediction   For each turn , one or more answer spans will be   extracted as arguments . Note that , in each direction ,   the first turn aims to extract all arguments , while   the following turns aim to extract arguments that   can form pairs with the query argument .   Specifically , inspired by Li et al . ( 2020 ) , we fed   Hinto two binary classifiers to predict the start and   end sentence positions of arguments . After obtain-   ing all start and end positions , we further employ   another binary classifier to determine whether each   start and end position pair ( matched by Cartesian   product ) forms an answer span . Note that the input   of this span classifier is the concatenation of the   start and end sentence representations from H.3.2.3 Training   During training , the three classifiers described in   Section 3.2.2 yield three cross - entropy losses , i.e. ,   a start loss , an end loss , and a span loss . We simply   sum these losses up as the training objective of our   model . In addition , the AM phrase and the APE   phrase are trained jointly in a single MRC model .   3.2.4 Inference   During inference , the D‚ÜíDdirection uses the   trained MRC model to first identify all the argu-   ments in Dby the AM query and then extract all   the argument pairs in Dby the APE queries . Sim-   ilarly , the D‚ÜíDdirection can be performed in   the same manner by simply exchanging the order   ofDandD. Each APE query in both directions   yields one or more argument pairs , where each ar-   gument pair contains the query argument and one   extracted argument . We simply merge all argument   pairs extracted by all APE queries into a union set   to obtain the final inference results .   4 Experiments   4.1 Experimental setup   4.1.1 Dataset   Our experiments are conducted on the large APE   benchmark dataset , namely the Review - Rebuttal   ( RR ) dataset ( Cheng et al . , 2020 ) , which contains   4,764 pairs of review - rebuttal passages of ICLR .   Following the setup of ( Cheng et al . , 2021 ) , we   also evaluate our method on two versions of the   train / dev / test ( 8:1:1 ) split , i.e. , RR - Passage - v1 and   RR - Submission - v2 . Note that in our method , we   view review passage and rebuttal passage as docu-   mentDand document D , respectively .   4.1.2 Implementation Details   We adopt Longformer - base-4096as base encoder ,   and we use sliding window attention with the win-   dow size of 512 . We train our model 6 epochs with   a batch size of 4 . AdamW ( Kingma and Ba , 2015 )   is used as the optimizer , and the learning rates for   Longformer and other layers are 1e-5 and 1e-3 .   The evaluation metrics contain two aspects ,   namely AM and APE . Different from ( Cheng et al . ,   2021 , 2020 ) , sentence pairing is not included as a   metric because we extract argument pairs directly.31   We select the best parameters based on the perfor-   mance ( i.e. , average Fscores of AM and APE )   on the dev set . All scores are averaged across 5   distinct trials using different random seeds .   4.1.3 Baselines   We compare our model with several baselines . PL-   H - LSTM - CRF ( Cheng et al . , 2020 ) independently   trains an argument mining task and a sentence pair-   ing task , while MT - H - LSTM - CRF ( Cheng et al . ,   2020 ) trains two subtasks in a multi - task frame-   work . MLMC ( Cheng et al . , 2021 ) is an attention-   guided model based on a table - filling approach ,   which is the current state - of - the - art method .   Furthermore , we implement two additional base-   lines . For a fair comparison with MLMC , MRC-   APE - Bert replaces Longformer with Bert , where   documents with excessively long length are splited   into several segments . Instead of jointly training   AM and APE phases , MRC - APE - Sep. trains the   two phases separately .   4.2 Results and Analysis   4.2.1 Main Results   As shown in Table 1 , our model achieves the best   performance on both versions of the RR dataset .   Concretely , on RR - Submission - v2 , our model sig-   nificantly outperforms the current state - of - the - art   model MLMC by at least 7.11 % in APE Fscore .   On RR - Passage - v1 , our model obtains at least   a 6.54 % higher APE Fscore than the MLMC .   Also , our model achieves the best performance on   AM . Furthermore , without applying Longformer   as the base encoder , MRC - APE - Bert still outper-   forms MLMC in APE Fscore , demonstrating   that our improvement is not only brought by Long-   former . However , for the AM task , MAC - APE - Bert   achieves slightly lower Fscore than MLMC . The   reason may be that , in MLMC , the predictions of   the AM task are influenced by the APE task through   a complex attention interaction mechanism . How-   ever , our model does not require such a complex   design and can achieve much better results on the   APE task . Besides , our MRC - APE achieves better   results than MRC - APE - Sep. on both AM and APE   tasks , indicating that jointly training two phases in   a single MRC model could maximize the mutual   benefits of the two phases .   In addition , to analyze the error propagation   from the first phase to the second phase , we use the   true label of AM task to predict APE task . Under   this setting , our model can achieve around 59.44 %   Fscore for APE task , showing effectiveness in   identifying argument pairs .   4.2.2 Ablation Study   The ablation study results are shown in Table 2 .   It can be observed that using two directions con-   tributes greatly to our method . Also , using the   arguments recognized in Dto extract the paired   arguments in Dis more critical in the RR dataset ,   removing it causes a 6.51 % decrease in APE F   score . Without the LSTM to capture the long-32term dependency among sentences , the APE F   score decreases by 0.86 % . Furthermore , the perfor-   mance drops heavily without the global attention ,   because it enables more interactions between the   query argument and the queried document , thus   better argument - specific representations could be   learned .   5 Conclusion   In this paper , we propose to frame the argument   pair extraction ( APE ) task as a machine reading   comprehension ( MRC ) task . Our MRC framework   addresses APE through two phases with two types   of queries , that is , argument mining ( AM ) query   and argument pair extraction ( APE ) query . Our   proposed method can better model the argument-   level interactions , thus facilitating the extraction   of argument pairs . Experimental results on a large   benchmark dataset demonstrate that our proposed   method achieves state - of - the - art performance .   Acknowledgments   This work was partially supported by the National   Natural Science Foundation of China ( 61876053 ,   62006062 , 62176076 ) , the Shenzhen Foundational   Research Funding ( JCYJ20200109113441941 ,   JCYJ20210324115614039 ) , Joint Lab of HITSZ   and China Merchants Securities .   References333435   Clara Meister Gian Wiher Tiago Pimentel Ryan CotterellETH Z ¬®urich University of Cambridge   clara.meister@inf.ethz.ch gian.wiher@inf.ethz.ch   tp472@cam.ac.uk ryan.cotterell@inf.ethz.ch   Abstract   1 Introduction   Today ‚Äôs probabilistic neural language models are   often trained on millions ‚Äî if not billions ‚Äî of lines   of human text ; thus , at least at an intuitive level ,   we would expect high - probability generations   to be human - like . Yet the high - qualitytexts   these models have become famous for producing   ( Brown et al . , 2020 ; Clark et al . , 2021 ) are usually   not those assigned the highest probability by the   model ( Fan et al . , 2018 ; Holtzman et al . , 2020 ;   Basu et al . , 2021 ; DeLucia et al . , 2021 ) . Rather ,   the relationship between probability and qualityappears to have an inflection point , i.e. , quality   and probability are positively correlated only until   a certain threshold , after which the correlation   becomes negative . While the existence of such a   trend has received informal explanations ( see , e.g. ,   Ippolito et al . ( 2019 ) and Zhang et al . ( 2021 ) for a   qualitative discussion about the trade - off between   diversity and quality ) , it lacks a more fundamental   understanding . Why does the lower probability text   produced by stochastic decoding methods ‚Äî such   as nucleus or top- ksampling ‚Äî outperform text gen-   erated using probability - maximizing approaches ?   In this note , we take an information - theoretic   approach in an attempt to answer this question .   In information theory , probability has another   interpretation : its negative log quantifies in-   formation content . In the context of natural   language , the notion of information content is   intuitive ; humans use strings as a means to convey   information . Further , less predictable text , i.e. , text   which would be harder for us to anticipate , conveys   more information . If we assume that the goal of   human communication is to transmit messages   efficiently and reliably ( Gibson et al . , 2019 ) , we   may predict that these strings ‚Äô information content   should concentrate inside a specific interval . At   one extreme , strings with more - than - expected   information may be hard to process , and thus   ought to be disfavored when producing language .   At the other extreme , low - information strings may   be seen as boring and uninformative .   Collectively , these concepts lead us to propose   theexpected information hypothesis : Text   perceived as human - like should have an infor-   mation content within a small interval around   the expected information ‚Äî i.e. , the entropy ‚Äî of   natural language strings . Such a hypothesis offers36an intuitive explanation for the trends observed   in natural language generation ( NLG ) , i.e. , why   desirable text seems to exist not always at the   high end of the probability spectrum but around a   certain inflection point . Moreover , it also gives us   atestable hypothesis : given a language generation   model qwhose entropy we can empirically   estimate , we can evaluate whether high - quality   text indeed has an information content that falls   within an interval around this quantity .   To test our hypothesis , we perform an analysis   comparing human and model - generated text ,   investigating multiple common decoding strategies   and NLG tasks . Specifically , our analysis focuses   exclusively on English text . We indeed observe   that the information content of highly ranked text   ( as judged by humans ) often falls within a standard   deviation of model entropy ; there is statistically   significant evidence that this is not due to chance .   Further , the best - performing decoding methods   appear to select strings with an information content   within this interval . We take these observations as   empirical support for our hypothesis , helping to   explain the probability ‚Äì quality paradox observed   in language generation .   2 Probabilistic Language Generators   In this work , we focus on probabilistic models   for language generation tasks . Formally , these   models are probability distributions qover natural   language strings y‚àà Y , where Yis the ( countably   infinite ) set consisting of all possible strings that   can be constructed from a set vocabulary V :   Y={ ‚ó¶ v ‚ó¶ |v‚àà V } ( 1 )   Here , and stand for special reserved   beginning- and end - of - string tokens , respectively ,   andVdenotes the Kleene closure of V. In   practice , we limit the set of strings we consider to   Y‚äÇ Y for some maximum sequence length N.   Note that qmay be a conditional model . For   instance , we may model q ( ¬∑ |x)where xis an   input text , as in the case of machine translation , or   an input image , as in the case of image captioning .   However , for notational brevity , we omit this   explicit dependence in most of our subsequent   analyses . In order to estimate q , it is standard   practice to maximize the log - probability of atraining corpus Cunder the model with respect   to the model ‚Äôs parameters Œ∏ . This is equivalent to   minimizing its negative log - probability :   L(Œ∏;C ) = ‚àíXlogq(y ) ( 2 )   There are many different decision rules one can em-   ploy for generating natural language strings from a   model q ; such sets of rules are generally referred to   as decoding strategies ; see Wiher et al . ( 2022 ) for   an in - depth review . Given the probabilistic nature   of the models we consider , an intuitive strategy for   decoding would be to choose the string with the   highest probability under q , an approach referred   to as maximum - a - posteriori ( MAP ) decoding . Yet   recent research has shown that solutions to MAP   decoding ‚Äî or , even more generally , to heuristic   mode - seeking methods such as beam search ‚Äî are   often not high - quality , even in state - of - the - art NLG   models . For example , in the domain of machine   translation , the most probable string under the   model is often the empty string ( Stahlberg and   Byrne , 2019 ) . Similarly , in the domain of open-   ended generation , mode - seeking methods produce   dull and generic text ( Holtzman et al . , 2020 ) .   Where maximization has failed , authors have   turned to stochastic methods , taking random   samples from q. While the resulting text is often   assigned much lower probability than the mode ,   it can be qualitatively much better . This peculiarity   has puzzled the language generation community   for the last few years , with only qualitative   intuitions being offered as explanation . This paper   in turn offers a quantitative explanation .   3 Language as Communication   While many aspects of natural language may not   perfectly adhere to Shannon ‚Äôs mathematical theory   of communication , there are several characteristics   of human language that canfruitfully be described   using an information - theoretic framework . Here   we employ this framework for explaining recent   phenomena observed in probabilistic NLG.373.1 Measuring Information   We can precisely compute the information content   of a string given the true ( perhaps conditional )   probability distribution pover natural language   strings . Fortunately , this is the exact distribution   our language generation models in ¬ß 2 are trained to   approximate . Assuming qapproximates pwell ( as   quantified by metrics such as perplexity ) , we may   thus use it to estimate such attributes of natural   language strings . In this work , we will measure   the amount of information a specific realization   ycontains , which we denote(y)=‚àílogq(y ) ,   as well as the expected amount of information   a random y‚àà Ydrawn from qcontains , also   termed the entropy of q :   E[(y ) ] = H ( q ) = ‚àíXq(y ) logq(y)(3 )   Note that Pimentel et al . ( 2021b , Theorem 2 ) prove   that , as long as the probability of under qis   bounded below by some œµ > 0 , then the entropy of   qis finite . In our case we restrict qto a finite subset   YofY , which also implies that Eq . ( 3 ) is finite .   3.2 The Expected Information Hypothesis   Language is used as a means for transferring   information . This property of language has in fact   motivated several theories of language evolution ;   many have posited , for instance , that natural   language has developed to optimize for reliable   and efficient data communication , subject to   cognitive resources ( Zipf , 1949 ; Hockett , 1960 ;   Hawkins , 2004 ; Piantadosi et al . , 2011 ) . The   above theories arguably imply that humans tend   to produce natural language strings with a certain   amount of information ; they also imply that , on the   receiving end of communication , humans would   expect similar strings . We argue that this amount   is intuitively close to the language ‚Äôs entropy , i.e. ,   close to the average string ‚Äôs information content .   Expected Information Hypothesis . Text per-   ceived as human - like typically encodes an amount   of information close to the expected information   content of natural language strings , i.e. , in the in-   terval [ H(p)‚àíŒµ , H(p ) + Œµ]for a natural languagestring distribution pand some Œµ . Text that falls out-   side of this region is likely perceived as unnatural .   This viewpoint can be applied to the problem of   decoding neural text generators . In the context of   a model qof the distribution p , this implies that ‚Äî   when qis a good approximation ‚Äî human - like text   should typically have a negative log - probability   close to the entropy of q. In ¬ß 4 , we provide   empirical evidence for this hypothesis .   Relationship to the typical set . The set of   strings that we discuss has an intuitive relationship   to the typical set ( Shannon , 1948 ) , an information-   theoretic concept defined for stationary ergodic   stochastic processes . However , generation from   standard neural probabilistic language models can-   not be framed as such a process . While we can not   utilize the formal mathematical underpinnings of   typicality , the connection can still be useful for un-   derstanding why strings with a given information   content exhibit certain characteristics . An overview   of the concept is in App . A for the interested reader ;   also see Dieleman ( 2020 ) for further insights on   typicality in the context of generative models .   4 Experiments   Our experiments present an analysis of the distri-   bution of information content in text generated by   both humans and probabilistic models . Specifically ,   we look at the relationship between information   content and quality ‚Äî as measured by human judg-   ments . We perform experiments on two natural   language generation tasks : abstractive summariza-   tion and story generation . We present the results   for story generation here , while the results for sum-   marization can be found in App . B due to space   constraints . A recreation of the probability versus   quality plots of Zhang et al . ( 2021 ) can also be   found in App . B.   We use the following Monte Carlo estimator for   the entropy , i.e. , expected information content , of38   our model q :   bH(q ) = 1   MX‚àílogq(y ) ( 4 )   where we sample y‚àºq . Algorithmically ,   taking these samples may be done in linear time   using ancestral sampling . All computations are   performed with the test sets of respective datasets .   Note that for both abstractive summarization and   story generation , where we condition on some input   x , we must compute the conditional entropy for   each input , i.e. , using q ( ¬∑ |x)instead of q ( ¬∑ ) . For   eachx , we take M= 100 to estimate bH(q ( ¬∑ |x ) ) .   4.1 Setup   Models and Data . We only conduct experiments   on the English language . For story generation , we   fine - tune GPT-2 ( medium ) ( Radford et al . , 2019 )   ( checkpoint made available by OpenAI ) on the   W P dataset ( Fan et al . , 2018 ) . For   abstractive summarization , we use BART ( Lewis   et al . , 2020 ) , fine - tuned on the CNN / D   dataset ( Nallapati et al . , 2016 ) . We rely on the   open - sourced code - base from the HuggingFace   framework ( Wolf et al . , 2020 ) for reproducibility .   Decoding Strategies . We explore text generated   according to a number of different decoding strate-   gies . Unless otherwise stated , we use the imple-   mentation provided by Hugging Face for each of   the decoding algorithms . Along with standard an-   cestral sampling , we experiment with the following   six decoding strategies :   ‚Ä¢greedy search ;   ‚Ä¢beam search with beam sizes k= 5 and   k= 10 ;   ‚Ä¢diverse beam search ( Vijayakumar et al . ,   2016 ) with Hamming distance as a dissimilar-   ity function and Œª= 0.7andG = k= 5 ;   ‚Ä¢ancestral sampling ;   ‚Ä¢top - ksampling ( Fan et al . , 2018 ) with k=   30 ;   ‚Ä¢nucleus sampling ( Holtzman et al . , 2020 )   withp= 0.85 ;   ‚Ä¢minimum Bayes risk decoding ( MBR ;   Eikema and Aziz 2020)with 32Monte   Carlo samplesfromqand BEER ( Stanojevi ¬¥ c   and Sima‚Äôan , 2014 ) as the utility function .   Human Evaluations . We use the prolific   platform to obtain human judgments of text   quality ( according to 2 criteria per task ) from 5   different annotators on 200 examples per decoding   strategy ‚Äì per task . This gives us a total of > 3000   annotated examples . We largely follow the   guidelines recommended by van der Lee et al .   ( 2021 ) in setting up our evaluations : For abstrac-   tive summarization , we ask annotators to rate   quality andaccuracy while for story generation ,   annotators rate fluency and naturalness . More   details on our setup can be found in App . B.1 .   4.2 Results   In Fig . 1 , we plot the distribution of information   content assigned by qto four different sets of   strings : our reference ( human - generated ) text , the39   top and bottom ranked ( according to human an-   notators ) strings generated from qvia our differ-   ent decoding strategies , and strings sampled i.i.d .   from q. Note that the latter should represent the   distribution of negative log - probabilities assigned   to strings by the model . We see that both the refer-   ences and the top - ranked model - generated strings ‚Äî   both of which we assume are of relatively high   quality ‚Äî contain an amount of information clus-   tered around the ( estimated ) model entropy . On the   other hand , the distribution of the information con-   tent of poorly rated strings is skewed towards much   lower values . The same trends hold when look-   ing at information normalized by string length , i.e. ,(y)/|y|(see App . B ) , demonstrating these trends   are not purely an artifact of string length . We note   that in our human evaluations , the reference string   was ranked first in 47 % of cases and it was tied   for first in an additional 16 % of the cases . This   suggests that the quality of the reference strings is   on par with ‚Äî if not higher than ‚Äî the set of ‚Äú top 1 ‚Äù   model - generated strings .   Fig . 2 shows the distribution of deviations of   strings ‚Äô information content from the model en-   tropy;results are shown for both reference strings   and top - ranked model - generated strings . Because   these values are distributed quite evenly around 0 ,   we take this as additional evidence that high - quality   text usually has information content close to H(q ) .   Further , the shapes of these curves motivate us to   perform our next set of tests using Œµ = œÉ , the stan-   dard deviation of information values under q.   We employ statistical hypothesis testing to   see if the percentage of high - quality strings   whose information content falls in the interval[H(q)‚àíœÉ , H(q ) + œÉ]is greater than chance .   For each input x(i.e . , either a story prompt   or article ) , we compute the information con-   tent of the reference and top-3 human - ranked   strings . We then compute the percentage   of items ( among these four ) that fall within   [ H(q ( ¬∑ |x))‚àíœÉ , H(q ( ¬∑ |x ) ) + œÉ ] . We compare   this percentage to the percentage of strings sampled   directly from q ( ¬∑ |x)that falls within this interval .   The former should ( in expectation ) be greater than   the latter if the probability of high - quality strings   having information content within this interval is   greater than chance . Specifically , we test this using   a paired , unequal - variance t - test , where samples   with the same input are paired . At significance   levelŒ±= 0.01 , we reject our null hypothesis ‚Äî i.e. ,   we reject that the percentage of highly rated strings   ( reference plus top-3 human - ranked strings ) that   fall within this interval is equal to ( or less than )   what we should expect by chance . Further , using   a simple unpaired t - test , we find that the mean   human score of strings ( across all decoding strate-   gies ) within this region is significantly higher than   those outside of this region . This characteristic is   visualized in Fig . 3 , where we plot the distributions   of human quality ratings for strings inside and   outside of this interval . We include a version of   Fig . 3 further broken down by whether strings fall   above orbelow this interval in App . B.   Additional plots reinforcing these observations   can be found in App . B. Also see Meister et al .   ( 2022 ) for follow - up experiments to this work .   5 Conclusion   In this work , we present the expected information   hypothesis , which states that human - like strings   typically have negative log - probability close to the   expected information content of the probabilistic   model from which they were generated . We use   this hypothesis to explain why high - quality text   seems to exist not necessarily at the high end of   the probability spectrum but , rather , close to the en-   tropy of the model . We provide empirical evidence   in support of our hypothesis in an analysis of both   human and machine - generated text , demonstrating   that , overwhelmingly , high - quality text indeed has   information content in the proposed region .   Ethics Statement   In order to complete our human evaluation , we   used a crowdsourcing platform . For each task , we40estimated the amount of time we expected the task   to take and made sure that the crowdworkers would   be paid ( at minimum ) a wage of $ 15 per hour . A   further ethical consideration of this work is in the   context of the use of language models for text gen-   eration . Language models have been used for the   generation of malicious text , e.g. , fake news and   triggering content . The results in this work may   provide insights for those using language models   for such purposes as to how generations can be   chosen to seem more ‚Äú human - like . ‚Äù   Acknowledgments   The authors would like to thank the anonymous re-   viewers for their helpful recommendations , as well   as Sander Dieleman for feedback on a preliminary   draft of this paper .   References4142A The Typical Set   Let us imagine flipping Nbiased coins ; specifically , let X‚àºpbe an indicator random variable that takes   values HandT. Take p(X = H ) = 0 .6andp(X = T ) = 0 .4 . Flipping Nbiased coins is then equivalent to   taking Ni.i.d . samples x‚àºp . For reasonably large N , what might you expect the sequence x , . . . , x   to look like ? Few people would answer ‚Äú all heads , ‚Äù even though this is technically the highest probability   sequence . Rather , intuition tells you : an expected sequence would be one comprised of approximately   60 % heads and 40 % tails .   The samples that fall into the latter category have a distinctive characteristic : they contain a near - average   amount of information w.r.t the support of the distribution over X , . . . , X , where the information   content of a realization x , . . . , xis defined as its negative log - probability . More formally , the ( weakly )   ( Œµ , N)-typical set Afor a chosen Œµ > 0is the set of assignments x , . . . , xto random variables‚àí ‚ÜíX = X , . . . , Xsuch that   2‚©Ωp(x , . . . , x)‚©Ω2   where H(p)=‚àíPp(x ) logp(x)is the entropy ‚Äî or equivalently , the expected value of the information   content ‚Äî of the random variable X. Under this definition we can prove that , for every Œµ > 0 , there exists an   Nsuch that for all N > N , we have that the ( Œµ , N)-typical set contains at least ( 1‚àíŒµ)of the probability   mass of the joint distribution over‚àí ‚ÜíX. The concept of the typical set also generalizes to stochastic processes   when we can actually compute their average information rate ‚Äî or equivalently , their entropy rate .   B Experimental Design   B.1 Human Evaluations   For story generation and abstractive summarization , the raters are first presented with a news article / prompt .   Next , they are presented , in random order , with the corresponding reference and the summaries / stories   generated by different decoders . For each of two rating criteria , a score from 0 to 7 is assigned . For story   generation the criteria are and while for abstractive summarization   and are used . We provide the following short descriptions of the criteria to the raters :   F : How fluent is the English text ?   N : Does the text seem to be natural English text ?   Q : How high is the overall quality of the text ?   A : How well does the summary summarize the article ?   After we obtain the ratings , we reject ratings that have not been filled out with care . Specifically , a rater is   rejected if he assigns high scores to multiple examples that do not fulfill the specified criteria at all . If a   rater has been rejected , we obtain a fresh set of ratings from a new rater .   C Additional Figures   We provide several additional results , looking further into the relationship between text information   content and perceived quality . We see that in general , the distribution of information content of reference   strings is quite close to that of the model . While the distribution of information content of top 1 ranked   strings is also closer to the model distribution than many of the individual decoding strategies , the overlap   is not as high as for reference strings.434445   Yutao Mou , Keqing He , Yanan Wu , Zhiyuan Zeng   Hong Xu , Huixing Jiang , Wei Wu , Weiran XuPattern Recognition & Intelligent System LaboratoryBeijing University of Posts and Telecommunications , Beijing , ChinaMeituan Group , Beijing , China   { myt,yanan.wu,zengzhiyuan,xuhong,xuweiran}@bupt.edu.cn   { hekeqing,jianghuixing,wuwei30}@meituan.com   Abstract   1 Introduction   Out - of - domain ( OOD ) intent discovery aims to   group new unknown intents into different clusters ,   which helps improve the dialogue system for future   development . Compared to existing text clustering   tasks , OOD discovery considers how to leverage   the prior knowledge of known in - domain ( IND )   intents to enhance discovering unknown OOD in-   tents , which makes it challenging to directly apply   existing clustering algorithms ( MacQueen , 1967 ;   Xie et al . , 2016 ; Chang et al . , 2017 ; Caron et al . ,   2018 ) to the OOD discovery task .   Previous unsupervised OOD discovery models   ( Hakkani - T√ºr et al . , 2015 ; Padmasundari and Ban-   galore , 2018 ; Shi et al . , 2018 ) only model OOD   data but ignore prior knowledge of in - domain data   thus suffer from poor performance . Therefore , re-   cent work ( Lin et al . , 2020 ; Zhang et al . , 2021 ) fo-   cus more on the semi - supervised setting where they   Ô¨Årstly pre - train an in - domain intent classiÔ¨Åer then   perform clustering algorithms on extracted OOD   intent representations by the pre - trained IND intent   classiÔ¨Åer . For example , Lin et al . ( 2020 ) Ô¨Årstly   pre - trains a BERT - based ( Devlin et al . , 2019 ) IND   intent classiÔ¨Åer then uses intent representations to   perform a pairwise clustering algorithm ( Chang   et al . , 2017 ) . Further , Zhang et al . ( 2021 ) proposes   an iterative clustering method , DeepAligned , to   obtain pseudo supervised signals using K - means   ( MacQueen , 1967 ) . However , all of these meth-   ods ignore the matching between IND pre - training   stage and OOD clustering stage because they for-   mulate IND pre - training as the classiÔ¨Åcation task   while OOD clustering as the text clustering task .   The different learning objectives make it hard to   transfer prior IND knowledge to OOD . Besides ,   previous work only transfer a single intent repre-   sentation from the pre - trained IND classiÔ¨Åer to   OOD clustering . Considering the entanglement of   the intent representation , simply transferring IND   features may harm OOD clustering . For example ,   there exist two levels of intent features , instance-   level and class - level knowledge in the pre - trained   IND classiÔ¨Åer . Decoupling different levels of intent   features helps better knowledge transferability .   To solve the issues , we propose a novel   Disentangled Knowledge Transfer method ( DKT )   via a uniÔ¨Åed multi - head contrastive learning frame-   work to transfer disentangled IND intent repre-   sentations to OOD clustering . The main intuition   is how to perform better knowledge transfer . As   shown in Fig 1 , we decouple the pre - trained intent   representations into two independent subspaces ,   instance - level and class(cluster)-level using a uni-46Ô¨Åed contrastive learning framework . Different from   existing OOD discovery work , we equip the tradi-   tional IND pre - training stage with a similar con-   trastive objective as the clustering stage . SpeciÔ¨Å-   cally , we Ô¨Årstly learn intent features using a con-   text encoder like BERT , then add two independent   transformation heads ( instance - level head fand   class - level head g ) on top of BERT . In the IND   pre - training stage , we use the head fto perform su-   pervised instance - level contrastive learning ( Chen   et al . , 2020 ; Khosla et al . , 2020 ; Gunel et al . , 2021 ;   Zeng et al . , 2021 ) and the head gto compute tra-   ditional classiÔ¨Åcation loss like cross - entropy . In   the OOD clustering stage , we employ similar ob-   jectives for these two heads where fis still used   for instance - level contrastive learning and gis used   to perform class(cluster)-level contrastive learning   ( Li et al . , 2021 ) . We leave the details in the follow-   ing Section 2 . Using the uniÔ¨Åed contrastive objec-   tives for pre - training and clustering bridges the gap   between the two stages . Besides , the two indepen-   dent heads decouple the instance- and cluster - level   contrastive learning to learn disentangled intent   representations for better knowledge transfer . Sec-   tion 4 demonstrates the effectiveness of multi - head   disentanglement .   Our contributions are three - fold : ( 1 ) We propose   a novel disentangled knowledge transfer method   for OOD discovery to better leverage prior IND   knowledge . ( 2 ) We propose a uniÔ¨Åed multi-   head contrastive learning framework to bridge the   gap between IND pre - training and OOD cluster-   ing . ( 3 ) Experiments and analysis on two bench-   mark datasets demonstrate the effectiveness of our   method for OOD discovery .   2 Approach   Problem Formulation Given a set of labeled in-   domain data ( X;Y)and unlabeled OOD   data(X;Y ) , OOD discovery aims to clus-   ter OOD groups from unlabeled OOD data using   prior knowledge from labeled IND data . Note that   IND data has no overlapping with OOD data . Gen-   erally , OOD discovery includes two stages , IND   pre - training which aims to obtain a decent intent   representation via labeled IND data , and OOD clus-   tering which aims to group OOD intents into dif-   ferent clusters .   Overall Architecture Fig 2 shows the overall   architecture of our proposed DKT model . We   Ô¨Årstly use the same BERT ( Devlin et al . , 2019 )   backbone to extract intent representations as the   previous work DeepAligned ( Zhang et al . , 2021 ) .   Then we decouple the intent representations into   two independent subspaces and use a uniÔ¨Åed con-   trastive learning framework to perform both IND   pre - training and OOD clustering .   IND Pre - training Different from existing meth-   ods that regard IND pre - training as a single intent   classiÔ¨Åcation task , we formulate it as an instance-   wise discriminative task and a class - wise classiÔ¨Å-   cation task via contrastive learning . Given an IND   intent example x , we Ô¨Årstly obtain its intent repre-   sentationzusing a BERT encoder and a pooling   layer . Then we use two independent transforma-   tion headsfandgto get two disentangled latent   vectorsf = f(z)andg = g(z).On top of the   instance - level head f , we perform supervised con-   trastive learning ( SCL ) ( Khosla et al . , 2020 ; Zeng   et al . , 2021 ) as follows :   L = X 1   N 1X11   logexp ( ff=  ) P1exp ( ff=  )   whereNis the total number of examples in the   batch that have the same label as yand1is an   indicator function . Following Gao et al . ( 2021 ) ;   Yan et al . ( 2021 ) , we employ simple dropout ( Sri-   vastava et al . , 2014 ) as data augmentation . SCL   can model instance - wise semantic similarities by   pulling together IND intents belonging to the same   class while pushing apart samples from different47   classes . Therefore , SCL helps maximize inter - class   variance and minimize intra - class variance , further   improves OOD clustering . On top of the class - level   headg , we use a cross - entropy classiÔ¨Åcation loss to   learn class(cluster)-wise distinction . Section 4 con-   Ô¨Årms both the objectives improve the performance   and SCL has a larger effect .   OOD Clustering The key challenge of OOD   clustering is how to learn intent representations   and cluster assignments . Previous state - of - the - art   model DeepAligned ( Zhang et al . , 2021 ) iteratively   repeats the two stages which results in poor cluster-   ing efÔ¨Åciency and accuracy . Thus , we propose an   end - to - end contrastive clustering method ( Li et al . ,   2021 ) to jointly learn representations and cluster   assignments . SpeciÔ¨Åcally , given an OOD example   x , we Ô¨Årstly use the pre - trained BERT encoder   and transformation heads to get OOD intent latent   vectorsfandg . Then , on top of the instance-   level headf , we perform instance - level contrastive   learning(ILCL ) ( Chen et al . , 2020 ) as follows :   ` =  logexp ( sim ( f;f)=  ) P1exp ( sim ( f;f)=  )   wherefdenotes the dropout - augmented OOD   sample and  denotes temperature . On top of   the cluster - level head g , we perform contrastive   clustering following Li et al . ( 2021 ) . SpeciÔ¨Åcally ,   given an OOD cluster - level latent vector g , we   Ô¨Årstly project it to a vector with dimension K which   equals to the pre - deÔ¨Åned cluster number . Suppose   we input a batch of OOD samples so we can get   a feature matrix of NK. Then we regard i - th   column of the matrix as the i - th cluster represen-   tationyand construct cluster - level CL(CLCL ) asfollows :   ` =  logexp ( sim ( y;y)=  ) P1exp ( sim ( y;y)=  )   whereyis the dropout - augmented cluster rep-   resentation of yandsimdenotes cosine distance .   Following Li et al . ( 2021 ) , we also add a regular-   ization item to avoid the trivial solution that most   instances are assigned to the single cluster . For   training , we simply add the above objectives in the   experiments . For inference , we only use the cluster-   level contrastive head and compute the argmax to   get the cluster results without additional K - means .   Generally , the instance - CL focuses on distinguish-   ing different intent samples while the cluster - CL   identiÔ¨Åes distinct OOD categories . Combining the   two stages , our proposed uniÔ¨Åed contrastive learn-   ing framework can effectively bridge the gap be-   tween IND pre - training and OOD clustering .   3 Experiment   3.1 Datasets   We show the detailed statistics of CLINC(Larson   et al . , 2019 ) and BANKING(Casanueva et al . ,   2020 ) datasets in Table 2 . CLINC contains 22,500   queries covering 150 intents and Banking contains   13,083 customer service queries with 77 intents . To   construct IND / OOD data , we ramdomly divided   the two datasets in three ramdom runs , according   to the speciÔ¨Åed OOD ratio(10 % , 20 % , 30 % for   CLINC , 10 % for Banking ) , and the rest is IND   data . Note that we only use the IND data for pre-   training and use OOD data for clustering . To avoid   the randomness of splitting IND / OOD , we average   results over three random runs . For each run , all   the models use the same divided dataset . Differ-   ent from previous work Zhang et al . ( 2021 ) , we   assume that the unlabeled data only contains OOD   data instead of a mixture of IND and OOD , aiming   to fairly evaluate the OOD clustering performance.48   In real scenarios , we can use OOD detection mod-   els ( Xu et al . , 2020 ; Zeng et al . , 2021 ) to collect   high - quality OOD data for OOD intent discovery .   3.2 Baselines   We mainly compare our method with semi-   supervised baselines : PTK - means ( k - means with   IND pre - training ) , DeepCluster ( Caron et al . , 2018 )   and two state - of - the - art OOD discovery methods   CDAC+ ( Lin et al . , 2020 ) and DeepAligned ( Zhang   et al . , 2021 ) . We also report the unsupervised re-   sults ( without IND pretraining ) of these methods   for a comprehensive comparison . For fairness , we   use the same BERT backbone as the baselines . We   leave the detailed baselines in the appendix A.1 .   3.3 Evaluation Metrics   We adopt three widely used metrics to evaluate the   clustering results : Accuracy ( ACC ) , Normalized   Mutual Information ( NMI ) , and Adjusted Rand   Index ( ARI ) . To calculate ACC , we use the Hun-   garian algorithm ( Kuhn , 1955 ) to obtain the map-   ping between the predicted classes and ground-   truth classes .   3.4 Implementation Details   For a fair comparison with previous work , we use   the pre - trained BERT model ( bert - base - uncased ,   with 12 - layer transformer ) as our network back-   bone , and add a pooling layer to get intent repre-   sentation(dimension=768 ) . Moreover , we freeze   all but the last transformer layer parameters to   achieve better performance with BERT backbone ,   and speed up the training procedure as suggested   in ( Zhang et al . , 2021 ) . During the pre - training   phase , the training batch size is 128 , and during   the clustering phase , the training batch size is 512   for CLINC-10 % , CLINC-30 % , Banking-10 % , and   400 for CLINC-20 % . The learning rate is 5e-5 in   the pre - training phase and 0.0003 in the cluster-   ing phase . Notably , We use dropout ( Gao et al . ,   2021 ) to construct augmented examples for con-   trastive learning with dropout rate 0.1 . For the   instance - level contrastive head , the dimensionality   of the row space is set to 128 , and the tempera-   tures of SCL and instance - level CL are 0.5 . Asfor the cluster - level contrastive head , the dimen-   sionality of the column space is naturally set to   the number of IND classes / OOD clusters , and the   cluster - level temperature parameter  = 1.0 is used   for all datasets . We use SC of validation OOD data   ( still unlabeled data ) to choose the best checkpoint .   The pre - training stage of our model lasts about   30 minutes and clustering runs for 10 minutes on   CLINC-10 % , both using a single Tesla T4 GPU(16   GB of memory ) .   3.5 Main Results   Table 1 shows the performance comparison of dif-   ferent models on two datasets . Under both un-   supervised and semi - supervised settings , our pro-   posed DKT consistently outperforms all the base-   lines . In this paper , we mainly focus on the lat-   ter setting . For the Semi - sup setting on CLINC-   10 % , DKT outperforms the previous state - of - the-   art DeepAligned by 2.67%(ACC ) , 5.35%(ARI ) ,   2.84%(NMI ) . Similar improvements are observed   on other datasets . The results prove the effective-   ness of our proposed disentangled knowledge trans-   fer for OOD discovery . Comparing Unsup DKT   with Semi - sup DKT , the latter signiÔ¨Åcantly outper-   forms the former by 23.56%(ACC ) , 33.79%(ARI ) ,   20.30%(NMI ) , which demonstrates the effective-   ness of IND pre - training(see details in appendix   A.2 ) .   4 Qualitative Analysis   Effect of Disentangled Intent Representations   Tab 3 shows performance comparison of DKT and   KT under two settings . We Ô¨Ånd Disentangled KT   signiÔ¨Åcantly outperforms KT both on two settings ,   which proves the effectiveness of representation   disentanglement for knowledge transfer .   Visualization To conÔ¨Årm the effectiveness of DKT ,   we perform OOD intent representation visualiza-   tion of DeepAligned , KT and DKT in Fig 3 . Note   that we use the same representation following the   pooling layer for fair comparison . We Ô¨Ånd both   DeepAligned and KT have some mixed OOD clus-   ters while DKT forms clearly separate decision   boundaries between clusters , which shows our pro-   posed DKT obtains discriminative OOD representa-   tions for OOD discovery . Besides , Section 4 further   explore the effect of different layer and representa-   tions after MLP ggets the best performance .   Error Analysis We further analyze the error cases   of DeepAligned and DKT in Fig 5 . We Ô¨Ånd that for49   similar OOD intents , DeepAligned is probably con-   fused but our DKT can effectively distinguish them .   For example , DeepAligned incorrectly groups ac-   cept_reservation intents into cancel_reservation   ( 14 % error rate ) vs DKT(7 % ) , which proves DKT   helps separate semantically similar OOD intents .   Ablation Study To understand the effect of differ-   ent objectives of DKT , we perform abalation study   in Tab 4 by removing each loss . Results show all   the losses contribute to the performance especially   SCL , ILCL and CLCL , which conÔ¨Årms the effec-   tiveness of our uniÔ¨Åed contrastive framework .   Intent Representations at Different Layers In   order to further explore the effectiveness of disen-   tangled representation , we visualize the output vec-   tors of instance - level head and cluster - level head   and compare them with the output vector after   BERT + pooling in Fig 4 . We can Ô¨Ånd that the   output obtained by instance - level head forms a nar-   row and long cluster distribution , while the output   obtained by cluster - level head forms a more com-   pact and uniform cluster distribution . We argue   that this reÔ¨Çects the effect of decoupling , that is ,   instance - level head decouples the uniqueness of   each sample , and cluster - level head decouples the   category characteristics of each sample .   5 Conclusion   In this paper , we propose a novel disentangled   knowledge transfer method ( DKT ) via a uniÔ¨Åed   multi - head contrastive learning framework to trans-   fer disentangled IND intent representations to OOD   clustering . Experiments and analysis on two bench-   marks demonstrate the effectiveness of DKT for   OOD discovery . We hope to explore more self-   supervised representation learning methods for   OOD discovery in the future.50Acknowledgements   We thank all anonymous reviewers for their helpful   comments and suggestions . This work was par-   tially supported by National Key R&D Program of   China No . 2019YFF0303300 and Subject II No .   2019YFF0303302 , DOCOMO Beijing Communi-   cations Laboratories Co. , Ltd , MoE - CMCC " ArtiÔ¨Å-   cal Intelligence " Project No . MCM20190701 .   Broader Impact   Task - oriented dialogue systems have demonstrated   remarkable performance across a wide range of   applications , with the promise of a signiÔ¨Åcant posi-   tive impact on human production mode and lifeway .   Intent classiÔ¨Åcation is an important component of   Task - oriented dialogue system . The existing intent   classiÔ¨Åcation models follow a closed set assump-   tion and can only identify a limited number of pre-   deÔ¨Åned intent types . However , the real world is   open . During the online deployment of dialogue   system , out - of - domain ( OOD ) or unknown intents   will appear continually . Recently , out - of - domain in-   tent detection task has been widely studied , which   can be used to collect these new intent data . The   OOD intent discovery task studied in this paper is   to make further use of these new intent data . It   aims to cluster these OOD samples according to in-   tents , so as to mine new intent types automatically ,   guide the future development of the system , and   expand the classiÔ¨Åcation ability of intent classiÔ¨Åca-   tion models .   References51   A Appendix   A.1 Baselines   The details of baselines are as follows :   ‚Ä¢PTK - means A method based on k - means   with IND pre - training . And the IND pre-   training objectives uses CE + SCL proposed   in this paper .   ‚Ä¢DeepCluster An iterative clustering algo-   rithm proposed by ( Caron et al . , 2018 ) , in each   iteration , Ô¨Årstly , k - means is used to assign   pseudo label to the unlabeled samples , and   then the cross - entropy objective is used for   representation learning . The cluster header pa-   rameters need to be reinitialized during each   iteration . In the semi - supervised setting , we   use the same IND pre- training objective as   DeepAligned ( Zhang et al . , 2021 )   ‚Ä¢CDAC+ The Ô¨Årst work of new intent discov-   ery proposed by ( Lin et al . , 2020 ) , and it Ô¨Årstly   pre - trains a BERT - based ( Devlin et al . , 2019 )   in - domain intent classiÔ¨Åer then uses intent rep-   resentations to calculate the similarity of OOD   intent pairs as weak supervised signals .   ‚Ä¢DeepAligned The second work of new intent   discovery proposed by ( Zhang et al . , 2021).It   is an improved version of DeepCluster . It   designed a pseudo label alignment strategy to   produce aligned cluster assignments for better   representation learning .   A.2 Effect of IND Data   We analyze the effect of IND data for OOD dis-   covery from two perspectives , the number of IND   classes and samples per class . Figure 6(a ) shows   the trend of the number of different IND classes ,   and Figure 6(b ) shows the trend of the number of   different samples in each class . Results show DKT   outperforms baselines under all settings and gets   the smallest varying degrees of performance drop ,   which proves the robustness and stability of our   method .   A.3 Visualization at Different Training   Epochs   To see the evolution of our method in the training   process , we show a visualization at four different   timestamps throughout the training process in Fig   7 . Results show representation vector of different   intent classes are mixed in the beginning and clus-   ter assignments become increasingly visible and   distinct as the training process goes.5253   Rodolfo Corona Shizhan Zhu Dan Klein Trevor Darrell   Computer Science Division , University of California , Berkeley   { rcorona , shizhan_zhu , klein , trevordarrell}@berkeley.edu   Abstract   1 Introduction   Embodied robotic agents hold great potential for   providing assistive technologies in home environ-   ments ( Pineau et al . , 2003 ) , and natural language   provides an intuitive interface for users to interact   with such systems ( Andreas et al . , 2020 ) . For these   systems to be effective , they must be able to re-   liably ground language in perception ( Bisk et al . ,   2020 ; Bender and Koller , 2020 ) .   Despite typically being paired with 2D images ,   natural language that is grounded in vision de-   scribes a fundamentally 3D world . For example ,   consider the grounding task in Figure 1 , where the   agent must select a target chair against a distrac-   tor given the description ‚Äú the swivel chair with 6   wheels . ‚Äù Although the agent is provided with multi-   ple images revealing all of the wheels on each chair ,   it must be able to properly aggregate information   across images to successfully differentiate them ,   something that requires reasoning about their 3D   geometry at some level .   In this work , we show how language grounding   performance may be improved by leveraging 3D   prior knowledge . Our model , V oxel - informed Lan-   guage Grounder ( VLG ) , extracts 3D voxel maps us-   ing a pre - trained volumetric reconstruction model ,   which it fuses with multimodal features from a   large - scale vision and language model in order to   reason jointly over the visual and 3D geometric   properties of objects .   We focus our investigation within the context   of SNARE ( Thomason et al . , 2021 ) , an object ref-   erence game where an agent must ground natural   language describing common household objects   by their geometric and visual properties , showing   that grounding accuracy significantly improves by   incorporating information from predicted 3D vol-   umes of objects . At the time of writing , VLG   achieves SOTA performance on SNARE , attain-   ing an absolute improvement of 2.0 % over the next   closest baseline . Code to replicate our results is   publicly available .   2 Related Work   Prior work has studied deriving structured represen-   tations from images to scaffold language ground-   ing . However , a majority of systems use represen-   tations such as 2D regions of interest ( Anderson   et al . , 2018 ; Wang et al . , 2020 ) or symbolic graph-54based representations ( Hudson and Manning , 2019 ;   Kulkarni et al . , 2013 ) , which do not encode 3D   properties of objects .   Most prior work tying language to 3D repre-   sentations has largely focused on generating 3D   structures conditioned on language , rather than us-   ing them as intermediate representations for lan-   guage grounding as we do here . Specifically , prior   work has performed language conditioned gen-   eration at the scene ( Chang et al . , 2014 , 2015a ) ,   pose ( Ahuja and Morency , 2019 ; Lin et al . , 2018 ) ,   or object ( Chen et al . , 2018 ) level . More recently ,   a line of work has explored referring expression   grounding in 3D by mapping referring expressions   of objects to 3D bounding boxes localizing them   in point clouds of indoor scenes ( Achlioptas et al . ,   2020 ; Chen et al . , 2020 ; Zhao et al . , 2021 ; Roh   et al . , 2022 ) . Standard approaches follow a two-   tiered process where an object proposal system will   first provide bounding boxes for candidate objects ,   and a scoring module will then compute a compat-   ibility score between each box and the referring   expression in order to ground it . At a more granu-   lar level , Koo et al . ( 2021 ) learn alignments from   language to object parts by training agents on a   reference game over point cloud representations of   objects .   In contrast , in this work we focus on augmenting   language grounding over 2D RGB images using   structured 3D representations derived from them .   For the task of visual language navigation , prior   work has shown how a persistent 3D semantic map   may be used as an intermediate representation to   aid in selecting navigational waypoints ( Chaplot   et al . , 2020 ; Blukis et al . , 2021 ) . The semantic   maps , however , represent entire scenes with indi-   vidual voxels representing object categories , rather   than their geometry . In this work , we show how   a more granular occupancy map representing ob-   jects ‚Äô geometry can improve language grounding   performance .   Closest to our work is that of Prabhudesai et al .   ( 2020 ) , which presents a method for mapping   language to 3D features within scenes from the   CLEVR ( Johnson et al . , 2017 ) dataset . Their sys-   tem generates 3D feature maps inferred from im-   ages and then grounds language directly to 3D   bounding boxes or coordinates . Their method as-   sumes , however , that dependency parse trees are   provided for the natural language inputs , and it is   trained with supervised alignments between nounphrases and the 3D representations , which VLG   does not require .   3 Voxel - informed Language Grounder   We consider a task where an agent must cor-   rectly predict a target object vagainst a dis-   tractor vgiven a natural language description   w={w , ... , w}of the target . For each ob-   ject , the agent is provided with n2D views v=   { x , ... , x},x‚ààR.   An agent for this task is represented by a scor-   ing function s(v , w)‚àà[0,1 ] , computing the com-   patibility between the target description and the   2D views of each object , and is used to select the   maximally scoring candidate . We first use uni-   modal encoders to encode the language description   intoe = h(w)and the object view images into   a single aggregate visual embedding e = g(v )   before fusing them with a visiolinguistic module   e = f([e;e ] ) . Prior approaches to this   problem ( Thomason et al . , 2021 ) directly input this   fused representation to a scoring module to pro-   duce a score s(e ) . They do not explicitly reason   about the 3D properties of the observed objects ,   requiring the models to learn them implicitly .   In contrast , our V oxel - informed Language   Grounder augments the scoring function swith   explicit 3D volumetric information e = o(v)ex-   tracted from a pre - trained multiview reconstruc-   tion model . The volumetric information ( in the   form of a factorization of a voxel occupancy map   inR ) is first fused into a joint representa-   tion with the language using a multimodal voxel-   language module e = f([e;e ] ) . The scor-   ing function then produces a score based on all   three modalities s([e;e ] ) .   3.1 Model Architecture   VLG ( Figure 2 ) consists of two branches : a   visiolinguistic module for fusing language and   2D RGB features , and a voxel - language module   for fusing language with 3D volumetric features .   A scoring function is then used to reason jointly   over the output of the two branches , producing a   compatibility score .   Visiolinguistic Module . The architecture of   our visiolinguistic module f(left panel , Figure   2 ) largely mirrors the architecture of MATCH   from Thomason et al . ( 2021 ) . A pre - trained   CLIP - ViT ( Radford et al . , 2021 ) model is used to55   encode the language description and view images   into vectors in R. The image embeddings are   max - pooled and concatenated to the description   embedding before being passed into an MLP   which generates a fused representation .   Voxel - Language Module . We use represen-   tations extracted from a ShapeNet ( Chang   et al . , 2015b ; Wu et al . , 2015 ) pre - trained Lego-   FormerM ( Yagubbayli et al . , 2021 ) , a multi - view   3D volumetric reconstruction model , as input to   our voxel - language module f. LegoFormer   is a transformer ( Vaswani et al . , 2017 ) based   model whose decoder generates volumetric maps   factorized into 12 parts . Each object factor is   represented by a set of three vectors x , y , z ‚ààR ,   which we concatenate to use as input tokens for   our voxel - language module . A triple cross - product   overx , y , z may be used to recover a 3D volume   V ‚ààRfor each factor . The full volume   for the object is generated by aggregating the   factor volumes through a sum operation . For more   details on LegoFormer , we refer the reader to   Yagubbayli et al . ( 2021 ) . We use a cross - modal   transformer ( Vaswani et al . , 2017 ) encoder to   fuse the language and object factors ( Figure 2 ,   right ) . The cross - modal transformer takes as   input language tokens , in the form of CLIP word   embeddings , and the 12 object factors output   by the LegoFormer decoder , which contain the   inferred geometric occupancy information of   the object . We use a CLS token as an aggregaterepresentation of the language and object factors .   Scoring Function . The scoring function is   represented by an MLP which takes as input the   concatenation of the visiolinguistic module output   and the cross - modal transformer ‚Äôs CLS token .   4 Language Grounding Evaluation   Evaluation . We test our method on the SNARE   benchmark ( Thomason et al . , 2021 ) . SNARE   is a language grounding dataset which augments   ACRONYM ( Eppner et al . , 2021 ) , a grasping   dataset built off of ShapeNetSem ( Savva et al . ,   2015 ; Chang et al . , 2015a ) , with natural language   annotations of objects .   SNARE presents an object reference game where   an agent must correctly guess a target object against   a distractor . In each instance of the game , the agent   is provided with a language description of the tar-   get as well as multiple 2D views of each object .   SNARE differentiates between visual andblind-   folded object descriptions . Visual descriptions pri-   marily include attributes such as name , shape , and   color ( e.g. ‚Äú classic armchair with white seat ‚Äù ) . In   contrast , blindfolded descriptions include attributes   such as shape andparts ( e.g. ‚Äú oval back and verti-   cal legs ‚Äù ) . The train / validation / test sets were gener-   ated by splitting over ( 207 / 7 / 48 ) ShapeNetSem   object categories , respectively containing ( 6,153 /   371 / 1,357 ) unique object instances and ( 39,104   / 2,304 / 8,751 ) object pairings with referring ex-   pressions . Renderings are provided for each object56VALIDATION TEST   Model Visual Blind All Visual Blind All   ViLBERT 89.5 76.6 83.1 80.2 73.0 76.6   MATCH 89.2 ( 0.9 ) 75.2 ( 0.7 ) 82.2 ( 0.4 ) 83.9 ( 0.5 ) 68.7 ( 0.9 ) 76.5 ( 0.5 )   MATCH90.6 ( 0.4 ) 75.7 ( 1.2 ) 83.2 ( 0.8 ) - - -   LAGOR 89.8 ( 0.4 ) 75.3 ( 0.7 ) 82.6 ( 0.4 ) 84.3 ( 0.4 ) 69.4 ( 0.5 ) 77.0 ( 0.5 )   LAGOR89.8 ( 0.5 ) 75.0 ( 0.4 ) 82.5 ( 0.1 ) - - -   VLG ( Ours ) 91.2 ( 0.4 ) 78.4(0.7 ) 84.9(0.3 ) 86.0 71.7 79.0   instance over 8 canonical viewing angles .   Because ShapeNet and ShapeNetSem represent   different splits of the broader ShapeNet database ,   we pre - train the LegoFormerM model on a modi-   fied dataset to avoid dataset leakage . Specifically ,   any objects which appear in both datasets are re-   assigned within the pre - training dataset used to   train LegoFormerM to match its split assignment   from SNARE .   ShapeNetSem images are resized to 224√ó224   when inputting them to LegoFormerM in order to   match its ShapeNet pre - training conditions .   Baselines . We compare VLG against the   set of models provided with SNARE.All   SNARE baselines except ViLBERT use a CLIP   ViT - B/32 ( Radford et al . , 2021 ) backbone for   encoding both images and language descriptions :   MATCH first uses CLIP - ViT to embed the   language description as well as each of the 8   view images . Next , the view embeddings are   mean - pooled and concatenated to the descrip-   tion embedding . Finally , a learned MLP is   used over the concatenated feature vector in   order to produce a final compatibility score .   ViLBERT fine - tunes a 12 - in-1 ( Lu et al . ,   2020 ) pre - trained ViLBERT(Lu et al . , 2019 )   as the backbone for MATCH instead of using   CLIP - ViT. Each object is presented to ViL-   BERT in the form of a single tiled image con-   taining all 14 views from ShapeNetSem , in-   stead of just the canonical 8 presented in the   standard task . ViLBERT tokenizes images byextracting features from image regions , with   the ground truth bounding boxes for each re-   gion ( i.e. view ) being provided . Because this   baseline is not open - source , we report the orig-   inal numbers from Thomason et al . ( 2021 ) .   LAGOR ( Language Grounding through   Object Rotation ) fine - tunes a pre - trained   MATCH module and is additionally regular-   ized through the auxiliary task of predicting   the canonical viewing angle of individual view   images , which it predicts using an added out-   put MLP head . Following Thomason et al .   ( 2021 ) , the LAGOR baseline is only provided   with 2 random views of each object both dur-   ing training and inference .   For more details on the baseline models , we   refer the reader to Thomason et al . ( 2021 ) .   Training Details . Apart from the dataset   split re - assignments mentioned in Section 4 , we   use the codeand hyperparameters presented   by Yagubbayli et al . ( 2021 ) to train LegoFormerM.   For training on SNARE , we follow Thomason   et al . ( 2021 ) and train all models with a smoothed   binary cross - entropy loss ( Achlioptas et al . , 2019 ) .   We train each model for 75 epochs , reporting per-   formance of the best performing checkpoint on the   validation set . For our replication of the SNARE   MATCH and LAGOR baselines , we use the code   and hyperparameters provided by Thomason et al .   ( 2021 ) . For all variants of our VLG model we   use the AdamW ( Loshchilov and Hutter , 2017 ) op-   timizer with a learning rate of 1e-3 and a linear   learning rate warmup of 10 K steps.57Model Visual Blind All   VGG16 91.4 ( 0.5 ) 76.5 ( 0.9 ) 84.0 ( 0.2 )   MLP 91.1 ( 0.8 ) 77.9 ( 0.9 ) 84.6 ( 0.1 )   no - CLIP 71.0 ( 0.6 ) 65.8 ( 0.7 ) 68.4 ( 0.1 )   VLG 91.2 ( 0.4 ) 78.4 ( 0.7 ) 84.9 ( 0.3 )   5 Results   We present test set performance for VLG and the   SNARE baselines reported by Thomason et al .   ( 2021 ) . We also present average performance for   trained models over 3 seeds with standard devia-   tions on the validation set .   5.1 Comparison to SOTA   In Table 1 we can observe reference game perfor-   mance for all models . VLG achieves SOTA perfor-   mance with an absolute improvement on the test   set of 2.0 % over LAGOR , the next best leaderboard   model . Although there is a general improvement   of 1.7 % in visual reference grounding , there is an   improvement of 2.3 % in blindfolded ( denoted as   Blind in tables to conserve space ) reference ground-   ing . This suggests that the injected 3D information   provides a greater boost for disambiguating be-   tween examples referring to geometric properties   of target objects . VLG generally improves over all   baselines and conditions for blindfolded examples ,   with the exception of ViLBERT , which may be due   to the additional information ViLBERT receives   in the form of 14 viewing angles of each object   instead of 8 .   Improvements on the Blind and All conditions   of the validation set are statistically significant with   p < 0.1under a Welch ‚Äôs two - tailed t - test .   5.2 Ablation Study   We present a variety of ablations on the validation   set to investigate the contributions of each piece of   our model . All results can be observed in Table 2 .   VGG16 Embeddings . LegoFormer uses an   ImageNet ( Deng et al . , 2009 ) pre - trained   VGG16 ( Simonyan and Zisserman , 2014 ) as a   backbone for extracting visual representations ,   which is a different dataset and pre - training taskthan what the CLIP - ViT image encoder is trained   on . This presents a confounding factor which   we ablate by performing an experiment feeding   our model ‚Äôs scoring function VGG16 features   directly instead of LegoFormer object factors   ( VGG16 in Table 2 ) . Despite getting comparable   results to VGG16 on visual reference grounding ,   VLG provides a clear improvement in blindfolded   ( and therefore overall ) reference performance ,   suggesting that the extracted 3D information is   useful for grounding more geometrically based   language descriptions , with the VGG16 features   being largely redundant in terms of visual signal .   Architecture . We ablate the contribution of   our cross - modal transformer branch by comparing   it against an MLP mirroring the structure of the   SNARE MATCH baseline . This model ( MLP in   Table 2 ) max - pools the LegoFormer object factors   and concatenates the result to the CLIP visual   and language features before passing them to an   MLP scoring function . The MLP model overall   outperforms the SNARE baselines from Table 1 ,   highlighting the usefulness of the 3D information   for grounding , but does not result in as large an   improvement as the cross - modal transformer . This   suggests that the transformer is better able at   integrating information from the multi - view input .   CLIP Visual Embeddings . Finally , we evaluate   the contribution of the visiolinguistic branch of   the model by removing it and only using the   cross - modal transformer over language and object   factors . As may be observed , there is a large drop   in performance ( 16.5 % overall ) , particularly for   visual references ( 20.2 % ) . These results suggest   that maintaining visual information such as color   and texture is critical for performing well on this   task , since the LegoFormer outputs contain only   volumetric occupancy information .   6 Discussion   We have presented the V oxel - informed Language   Grounder ( VLG ) , a model which leverages explicit   3D information from predicted volumetric voxel   maps to improve language grounding performance .   VLG achieves SOTA results on SNARE , and ab-   lations demonstrate the effectiveness of using this   3D information for grounding . We hope this paper   may inspire future work on integrating structured   3D representations into language grounding tasks.58Acknowledgements   We would like to thank Karttikeya Mangalam and   Nikita Kitaev for their helpful advice and discus-   sions on transformer models . Mohit Shridhar and   Jesse Thomason for their help with setting up   SNARE . And thanks to the anonymous review-   ers for their constructive feedback . This work was   supported by DARPA under the SemaFor program   ( HR00112020054 ) . The content does not neces-   sarily reflect the position or the policy of the gov-   ernment , and no official endorsement should be   inferred . RC is supported by an NSF Graduate   Research Fellowship .   References5960   Xiao Liu , Kaixuan Ji , Yicheng Fu , Weng Lam Tam , Zhengxiao Du ,   Zhilin Yang , Jie TangTsinghua University , KEGBeijing Academy of Artificial Intelligence ( BAAI)Shanghai Qi Zhi Institute   { liuxiao21,jkx19,fyc19}@mails.tsinghua.edu.cn   Abstract   1 Introduction   Pretrained language models ( Radford et al . , 2019 ;   Devlin et al . , 2018 ; Yang et al . , 2019 ; Raffel et al . ,   2019 ) improve performance on a wide range of   natural language understanding ( NLU ) tasks . A   widely - used method , fine - tuning , updates the en-   tire set of model parameters for a target task .   While fine - tuning obtains good performance , it is   memory - consuming during training because gradi-   ents and optimizer states for all parameters must be   stored . Moreover , keeping a copy of model param-   eters for each task during inference is inconvenient   since pre - trained models are usually large .   Prompting , on the other hand , freezes all param-   eters of a pre - trained model and uses a natural lan-   guage prompt to query a language model ( Brown   et al . , 2020 ) . For example , for sentiment analy-   sis , we can concatenate a sample ( e.g. , " Amazing   movie ! " ) with a prompt ‚Äú This movie is [ MASK ] ‚Äù   and ask the pre - trained language model to predict   the probabilities of masked token being ‚Äú good ‚Äù and   ‚Äú bad ‚Äù to decide the sample ‚Äôs label . Prompting re-   quires no training at all and stores one single copy   of model parameters . However , discrete prompt-   ing ( Shin et al . , 2020 ; Gao et al . , 2020 ) can lead to   suboptimal performance in many cases compared   to fine - tuning .   Prompt tuningis an idea of tuning only the   continuous prompts . Specifically , Liu et al . ( 2021 ) ;   Lester et al . ( 2021 ) proposed to add trainable   continuous embeddings ( also called continuous   prompts ) to the original sequence of input word   embeddings . Only the continuous prompts are up-   dated during training . While prompt tuning im-   proves over prompting on many tasks ( Liu et al . ,   2021 ; Lester et al . , 2021 ; Zhong et al . , 2021 ) , it still   underperforms fine - tuning when the model size is   not large , specifically less than 10 billion parame-   ters ( Lester et al . , 2021 ) . Moreover , as shown in   our experiments , prompt tuning performs poorly   compared to fine - tuning on several hard sequence   labeling tasks such as extractive question answer-   ing ( Cf . Section 4.2).61Our main contribution in this paper is a novel   empirical finding that properly optimized prompt   tuning can be comparable to fine - tuning universally   across various model scales and NLU tasks . In con-   trast to observations in prior work , our discovery   reveals the universality and potential of prompt   tuning for NLU .   Technically , our approach P - tuning v2 is not con-   ceptually novel . It can be viewed as an optimized   and adapted implementation of Deep Prompt Tun-   ing(Li and Liang , 2021 ; Qin and Eisner , 2021 )   designed for generation and knowledge probing .   The most significant improvement originates from   appling continuous prompts for every layer of the   pretrained model , instead of the mere input layer .   Deep prompt tuning increases the capacity of con-   tinuous prompts and closes the gap to fine - tuning   across various settings , especially for small models   and hard tasks . Moreover , we present a series of   critical details of optimization and implementation   to ensure finetuning - comparable performance .   Experimental results show that P - tuning v2   matches the performance of fine - tuning at differ-   ent model scales ranging from 300 M to 10B pa-   rameters and on various hard sequence tagging   tasks such as extractive question answering and   named entity recognition . P - tuning v2 has 0.1 %   to 3 % trainable parameters per task compared to   fine - tuning , which substantially reduces training   time memory cost and per - task storage cost .   2 Preliminaries   NLU Tasks . In this work , we categorize NLU   challenges into two families : simple classification   tasks andhard sequence labeling tasks .Simple   classification tasks involve classification over a la-   bel space . Most datasets from GLUE ( Wang et al . ,   2018 ) and SuperGLUE ( Wang et al . , 2019 ) are in   this category . Hard sequence labeling tasks involve   classification over a sequence of tokens , such as   named entity recognition and extractive question   answering .   Prompt Tuning . LetVbe the vocabulary of   a language model Mand let ebe the em-   bedding layer of M. In the case of discrete   prompting ( Schick and Sch√ºtze , 2020 ) , prompt   tokens { " It " , " is " , " [ MASK ] " } ‚äÇ V can be   used to classify a movie review . For exam - ple , given the input text x="Amazing movie ! " ,   the input embedding sequence is formulated as   [ e(x),e("It"),e("is"),e("[MASK ] " ) ] .   Lester et al . ( 2021 ) and Liu et al . ( 2021 ) in-   troduce trainable continuous prompts as a sub-   stitution to natural language prompts for NLU   with the parameters of pretrained language mod-   els frozen . Given the trainable continuous embed-   dings [ h , ... , h ] , the input embedding sequence   is written as [ e(x ) , h , ... , h , e("[MASK ] " ) ] , as il-   lustrated in Figure 2 . Prompt tuning has been   proved to be comparable to fine - tuning on 10-   billion - parameter models on simple classification   tasks ( Lester et al . , 2021 ; Kim et al . , 2021 ; Liu   et al . , 2021 ) .   3 P - Tuning v2   3.1 Lack of Universality   Lester et al . ( 2021 ) ; Liu et al . ( 2021 ) have been   proved quite effective in many NLP applica-   tions ( Wang et al . , 2021a , b ; Chen et al . , 2021 ;   Zheng et al . , 2021 ; Min et al . , 2021 ) , but still fall   short at replacing fine - tuning due to lack of univer-   sality , as discussed below .   Lack of universality across scales . Lester et al .   ( 2021 ) shows that prompt tuning can be comparable   to fine - tuning when the model scales to over 10 bil-   lion parameters . However , for medium - sized mod-   els ( from 100 M to 1B ) that are widely used , prompt   tuning performs much worse than fine - tuning .   Lack of universality across tasks . Though Lester   et al . ( 2021 ) ; Liu et al . ( 2021 ) have shown superior-   ity on some of the NLU benchmarks , the effective-   ness of prompt tuning on hard sequence tagging   tasks is not verified . Sequence tagging predicts a se-   quence of labels for each input token , which can be   harder and incompatible with verbalizers ( Schick   and Sch√ºtze , 2020 ) . In our experiments ( Cf . Sec-   tion 4.2 and Table 3 ) , we show that Lester et al .   ( 2021 ) ; Liu et al . ( 2021 ) perform poorly on typical   sequence tagging tasks compared to fine - tuning .   Considering these challenges , we propose P-   tuning v2 , which adapts deep prompt tuning ( Li   and Liang , 2021 ; Qin and Eisner , 2021 ) as a uni-   versal solution across scales and NLU tasks .   3.2 Deep Prompt Tuning   In ( Lester et al . , 2021 ) and ( Liu et al . , 2021 ) , con-   tinuous prompts are only inserted into the input   embedding sequence ( Cf . Figure 2 ( a ) ) . This leads62   to two challenges . First , the number of tunable   parameters is limited due to the constraints of se-   quence length . Second , the input embeddings have   relatively indirect impact on model predictions .   To address these challenges , P - tuning v2 em-   ploys the idea of deep prompt tuning ( Li and Liang ,   2021 ; Qin and Eisner , 2021 ) . As illustrated in Fig-   ure 2 , prompts in different layers are added as pre-   fix tokens . On one hand , P - tuning v2 have more   tunable task - specific parameters ( from 0.01 % to   0.1%-3 % ) to allow more per - task capacity while be-   ing parameter - efficient ; on the other hand , prompts   added to deeper layers have more direct impact on   model predictions ( see analysis in Appendix B ) .   3.3 Optimization and Implementation   There are a few useful details of optimization and   implementation for achieving the best performance .   Reparameterization . Prior works usually leverage   a reparameterization encoder such as an MLP ( Li   and Liang , 2021 ; Liu et al . , 2021 ) to transform train-   able embeddings . However , for NLU , we discover   that its usefulness depends on tasks and datasets .   For some datasets ( e.g. , RTE and CoNLL04 ) , MLP   brings a consistent improvement ; for the others ,   MLP leads to minimal or even negative effects on   the results ( e.g. , BoolQ and CoNLL12 ) . See Ap-   pendix B for more analysis .   Prompt Length . The prompt length plays a crit-   ical role in P - Tuning v2 . We find that different   NLU tasks usually achieve their best performance   with different prompt lengths ( Cf . Appendix B ) .   Generally , simple classification tasks prefer shorter   prompts ( less than 20 ) ; hard sequence labeling   tasks prefer longer ones ( around 100 ) .   Multi - task Learning . Multi - task learning jointly   optimizes multiple tasks with shared continuous   prompts before fine - tuning for individual tasks .   Multi - task is optional for P - Tuning v2 but can be   used for further boost performance by providing a   better initialization ( Gu et al . , 2021 ) .   Classification Head . Using a language modeling   head to predict verbalizers ( Schick and Sch√ºtze ,   2020 ) has been central for prompt tuning ( Liu et al . ,   2021 ) , but we find it unnecessary in a full - data   setting and incompatible with sequence labeling .   P - tuning v2 instead applies a randomly - initialized   classification head on top of the tokens as in BERT   ( Devlin et al . , 2018 ) ( Cf . Figure 2 ) .   To clarify P - tuning v2 ‚Äôs major contribution , we   present a conceptual comparison to existing prompt   tuning approaches in Table 1 .   4 Experiments   We conduct extensive experiments over different   commonly - used pre - trained models and NLU tasks   to verify the effectiveness of P - tuning v2 . In this   work , all methods except for fine - tuning are con-   ducted with frozen language model backbones ,   which accords with ( Lester et al . , 2021 ) ‚Äôs setting   but differs from ( Liu et al . , 2021 ) ‚Äôs tuned setting .   Ratios of task - specific parameters ( e.g. , 0.1 % ) are63   derived from comparing continuous prompts ‚Äô pa-   rameters with transformers ‚Äô parameters . Another   thing to notice is that our experiments are all con-   ducted in the fully - supervised setting rather than   few - shot setting .   NLU Tasks . First , we include datasets from Su-   perGLUE ( Wang et al . , 2019 ) to test P - tuning v2 ‚Äôs   general NLU ability . Additionally , we introduce a   suite of sequence labeling tasks , including named   entity recognition ( Sang and De Meulder , 2003 ;   Weischedel et al . , 2013 ; Carreras and M√†rquez ,   2004 ) , extractive Question Answering ( Rajpurkar   et al . , 2016 ) , and semantic role labeling ( Carreras   and M√†rquez , 2005 ; Pradhan et al . , 2012)).Pre - trained Models . We include BERT - large ( De-   vlin et al . , 2018 ) , RoBERTa - large ( Liu et al . ,   2019 ) , DeBERTa - xlarge ( He et al . , 2020 ) , GLM-   xlarge / xxlarge ( Du et al . , 2021 ) for evaluation .   They are all bidirectional models designed for NLU   tasks , covering a wide range of sizes from about   300 M to 10B.   Multitask Learning . For the multi - task setting ,   we combine the training sets of the datasets in each   task type ( e.g. , combing all training sets of seman-   tic role labeling ) . We use separate linear classi-   fiers for each dataset while sharing the continuous   prompts ( Cf . Appendix A).64   4.1 P - tuning v2 : Across Scales   Table 2 presents P - tuning v2 ‚Äôs performances across   model scales . In SuperGLUE , performances of   Lester et al . ( 2021 ) and P - tuning at smaller scales   can be quite poor . On the contrary , P - tuning v2   matches the fine - tuning performance in all the tasks   at a smaller scale . P - tuning v2 even significantly   outperforms fine - tuning on RTE .   In terms of larger scales ( 2B to 10B ) with   GLM ( Du et al . , 2021 ) , the gap between Lester   et al . ( 2021 ) ; Liu et al . ( 2021 ) and fine - tuning is   gradually narrowed down . On 10B scale , we have   a similar observation as Lester et al . ( 2021 ) re-   ports , that prompt tuning becomes competitive to   fine - tuning . That said , P - tuning v2 is always com-   parable to fine - tuning at all scales but with only   0.1 % task - specific parameters needed comparing   to fine - tuning .   4.2 P - tuning v2 : Across Tasks   From Table 3 , we observe that P - tuning v2 can be   generally comparable to fine - tuning on all tasks . P-   tuning and Lester et al . ( 2021 ) show much poorer   performance , especially on QA , which might be the   most challenging of the three tasks . We also notice   that there are some abnormal results of Lester et al .   ( 2021 ) and P - tuning on SQuAD 2.0 . This is prob-   ably because SQuAD 2.0 contains unanswerable   questions , which causes optimization challenges   for single - layer prompt tuning . Multi - task learn-   ing generally brings significant improvements to   P - Tuning v2 over most tasks except for QA .   4.3 Ablation Study   Verbalizer with LM head v.s. [ CLS ] label with   linear head . Verbalizer with LM head has been a   central component in previous prompt tuning ap-   proaches . However , for P - tuning v2 in a supervised   setting , it is affordable to tune a linear head with   about several thousand parameters . We present our   comparison in Table 4 , where we keep other hyper-   parameters and only change [ CLS ] label with linear   head to verbalizer with LM head . Here , for simplic-   ity , we use ‚Äú true ‚Äù and ‚Äú false ‚Äù for SST-2 , RTE and   BoolQ ; ‚Äú true ‚Äù , ‚Äú false ‚Äù and ‚Äú neutral ‚Äù for CB . Re-   sults indicate that there is no significant difference   between performances of verbalizer and [ CLS ] .   Prompt depth . The main difference between   Lester et al . ( 2021 ) ; ( Liu et al . , 2021 ) and P - tuning   v2 is the multi - layer continuous prompts . To ver-   ify its exact influence , given a certain number of k   layers to add prompts , we select them in both as-   cending and descending order to add prompts ; for   the rest layers , we left them untouched . As shown   in Figure 3 , with the same amount of parameters   ( i.e. , num of transformer layers to add prompts ) ,   adding them in the descending order is always bet-   ter than in the ascending order . In the RTE case ,   only adding prompts to layers 17 - 24 can yield a   very close performance to all layers .   5 Conclusions   We present P - tuning v2 , a prompt tuning method .   Despite its relatively limited technical novelty , it   contributes to a novel finding that prompt tuning   can be comparable to fine - tuning universally across   scales ( from 330 M to 10B parameters ) and tasks .   With high accuracy and parameter efficiency , P-   Tuning v2 can be a potential alternative for fine-   tuning and a strong baseline for future work .   ACKNOWLEDGEMENT   We would like to thank the anonymous reviewers   for their suggestions and comments . Jie Tang is   supported by the NSFC for Distinguished Young   Scholar ( 61825602 ) and NSFC ( 61836013 ) . Kaix-   uan Ji is supported by Tsinghua University Initia-   tive Scientific Research Program and DCST Stu-   dent Academic Training Program.65References66A Problem Formulation on Sequence   Tagging   Name entity recognition ( NER ) . NER aims to   predict all spans of words that represent some   given classes of entity with a sentence . We   adopted CoNLL03 ( Sang and De Meulder , 2003 ) ,   OntoNotes 5.0 ( Weischedel et al . , 2013 ) and   CoNLL04 ( Carreras and M√†rquez , 2004 ) . For   CoNLL03 and CoNLL04 , we trained our model on   the standard train - develop - test split . For OntoNotes   5.0 , we use the same train , develop , test split as ( Xu   et al . , 2021 ) . All the datasets are labeled in IOB2   format . We use sequence tagging to solve NER   tasks by assigning labels marking the beginning   and inside some classes of entity . The language   models generate a representation for each token ,   and we use a linear classifier to predict the labels .   We use the official scripts to evaluate the results .   For the multi - task setting , we combine the training   set of the three datasets for pre - training . We use   different linear classifiers for each dataset while   sharing the continuous prompts .   ( Extractive ) Question Answering ( QA ) . Extrac-   tive QA is designed to extract the answer from the   context given the context and a question . We use   SQuAD ( Rajpurkar et al . , 2016 ) 1.1 and 2.0 , in   which each answer is within a continuous span of   the context . Following tradition , we formulate the   problem as sequence tagging by assigning one of   the two labels : ‚Äò start ‚Äô or ‚Äò end ‚Äô to each token and at   last selecting the span of the most confident start-   end pair as the extracted answer . If the probability   of the most confident pair is lower than a threshold ,   the model will assume the question unanswerable .   For the multi - task setting , our training set for pre-   training combines the training sets of SQuAD 1.1   and 2.0 . When pre - training , we assume that all the   questions , regardless of their origin , are possibly   unanswerable .   Semantic Role Labeling ( SRL ) . SRL assigns la-   bels to words or phrases in a sentence that indicate   their semantic roles in the sentence . We evaluate   P - tuning v2 on CoNLL05 ( Carreras and M√†rquez ,   2005 ) and CoNLL12 ( Pradhan et al . , 2012 ) . Since a   sentence can have multiple verbs , we add the target   verb token to the end of each sentence to help recog-   nize which verb is used for prediction . We classify   each word with a linear classifier based on the cor-   responding semantic role representation . For multi-   task setting , the pre - train training set is a combina-67   tion of the training set of CoNLL05 ( Carreras and   M√†rquez , 2005 ) , CoNLL12 ( Pradhan et al . , 2012 )   and propbank - release ( a common extend data used   for training SRL ) . The multi - task training strategy   is similar to NER .   B More Ablation Study   Due to the page limit , we present hyper - parameters   and architecture designs ablations regarding repa-   rameterization and prompt length in this section .   Embedding v.s. MLP reparameterization . In   both prefix - tuning ( Li and Liang , 2021 ) and P-   tuning ( Liu et al . , 2021 ) , authors discover the repa-   rameterization to be useful in improving training   speed , robustness and performance . However , we   conduct experiments to show that the reparameteri-   zation effect is inconsistent across different NLU   tasks and datasets .   As shown in Figure 4 , in RTE and CoNLL04 ,   MLP reparameterization generally indicates better   performance than embedding for almost all prompt   lengths . However , in BoolQ , MLP and embed-   ding ‚Äôs results are competitive ; in CoNLL12 , the   embedding consistently outperforms MLP .   Prompt Length . Prompt length is yet another influ-   ential hyper - parameter for P - tuning v2 , and its op-   timal value varies from task to task . From Figure 4 ,   we observe that for simple NLU tasks , usually , a   shorter prompt is enough for the best performance ;   for hard sequence tasks , usually , a longer prompt   than 100 would be helpful .   We also discover that reparameterization has a   close bond with optimal prompt length . For exam-   ple , in RTE , CoNLL04 , and BoolQ , MLP reparam-   eterization achieves its optimal result earlier than   embedding . This conclusion may contribute some   thoughts on P - tuning ‚Äôs optimization properties.68   Joel Ruben Antony Moniz , Barun Patra   { jramoniz , barunpatra95}@gmail.com   Matthew R. Gormley   Carnegie Mellon University   mgormley@cs.cmu.edu   Abstract   1 Introduction   While neural networks have become the de - facto   method of tackling NLP tasks , they often require   a lot of annotated data to do so . This task of data   annotation is especially challenging while build-   ing systems aimed at serving numerous languages .   Motivated by this , in this paper , we tackle the fol-   lowing problem :   Given the requirement of building systems for   an NLP task in a multilingual setting with a fixed   annotation budget , how can we efficiently acquire   annotations to perform the task well across multi-   ple languages ?   The traditional approach to this problem has   been building a separate model to serve each lan-   guage . In this scenario , the annotation budgetis split equally for all languages , and a model   is trained for each one separately . Recently , an-   other direction that has gained popularity has been   leveraging multilingual pre - trained language mod-   els ( MPLMs ) which inherently map multiple lan-   guages to a common embedding space ( Devlin   et al . , 2019 ; Conneau et al . , 2020 ) . The popular   method for leveraging these models has been lever-   aging their zero - shot transfer ability : training on   an English - only corpus for the task , and then using   the models zero - shot for the other languages .   Another orthogonal line of work aimed at build-   ing models under a constrained budget has been   active learning ( AL ) ( Shen et al . , 2018 ; Ein - Dor   et al . , 2020 ) . While this has shown to improve   annotation efficiency , the predominant approach   has been to train one model per language , using   the ( language specific ) model for AL ( Shen et al . ,   2018 ; Erdmann et al . , 2019 ) .   In this work , we show that a single MPLM   trained on all languages simultaneously performs   much better than training independent models for   specific languages for a fixed total annotation bud-   get . Further , while the benefits of using AL in con-   junction with MPLMs has been studied for a mono-   lingual setup ( Ein - Dor et al . , 2020 ) , we show that   AL also yields benefits in the multilingual setup .   Concretely , we show that an AL acquisition on   a single language helps improve zero - shot perfor-   mance on all other languages , regardless of the   language of the seed data . Furthermore , we show   that AL also yields benefits for our proposed single   model scenario . We demonstrate that our results   are consistent on 3 different tasks across multiple   languages : classification , sequence tagging and   dependency parsing . Our approach removes the   requirement of maintaining ndifferent models , and   uses1{nthe parameters than when independent   models are trained . Our analysis reveals that the   model arbitrates between different languages based   on its performance to form a multilingual curricu-69lum .   We release our code at https://github .   com / codedecde / SMAL .   2 Related Work   Effective utilization of annotation budgets has been   the area of focus for numerous active learning   works , showing improvements for different tasks   like POS tagging ( Ringger et al . , 2007 ) , sentiment   analysis ( Karlos et al . , 2012 ; Li et al . , 2013 ; Brew   et al . , 2010 ; Ju and Li , 2012 ) , syntactic parsing   ( Duong et al . , 2018 ) , and named entity recognition   ( Settles and Craven , 2008 ; Shen et al . , 2018 ) . The   focus of most of these works , however , has been   on learning for a single language ( often English ) .   Prior work on AL that uses a multilingual setup   or cross - lingual information sharing and that goes   beyond training a separate model for each language   has thus been limited . The closest work where mul-   tiple languages influence each other ‚Äôs acquisition is   that of Qian et al . ( 2014 ) ; however , they still train   a separate model for each language .   For transfer to multiple languages , recent ad-   vances in building MPLMs ( Devlin et al . , 2019 ;   Conneau et al . , 2020 ; Liu et al . , 2020 ; Xue et al . ,   2020 ) have been extremely effective , especially in   zero - shot transfer ( Pires et al . , 2019 ; Liu et al . ,   2020 ) . Ein - Dor et al . ( 2020 ) studied the data-   effectiveness of these models when used in con-   junction with AL , but , as with other AL work , with   a single language focus . Finally , Lauscher et al .   ( 2020 ) studied the effectiveness of the zero - shot   setup , showing that adding a few examples to a   model trained on English improves performance   over zero - shot transfer . However , this assumes the   availability of a full English task - specific corpus .   3 Methodology   3.1 Task Specific Models   We use the multilingual - BERT - cased model   ( mBERT ) as the base model for all the tasks . We   use the standard training methodology for the tasks :   Forclassification , we use a single layer over the   [ CLS ] embedding . For sequence tagging , we use   a single layer for each word to predict its tag . For   dependency parsing , we follow Kondratyuk and   Straka ( 2019 ) and use mBERT embeddings with   the graph - based bi - affine attention parser ( Dozat   and Manning , 2017 ) ; refer Appendix A for details.3.2 Budget Allocation Settings   To understand data acquisition in a multilingual   setting , we consider multilingual datasets in the   3 tasks . For each task t , letLbe the set of lan-   guages ( n‚Äú|L| ) . We then define sto be the seed   size , bto be the total annotation budget and vto   be total number of annotated validation examples   available to t. We compare our proposed Single   Model Acquisition ( SMA ) setup to two baseline   settings ‚Äì Monolingual Acquisition ( MonoA ) and   Multi Model Acquisition ( MMA ):   MonoA In this setting , the seed data as well as   the validation data ( s , v ) is acquired from a single   language . Further , the entire annotation budget ( b )   is assigned to the same language . We evaluate the   test data performance on that language and on the   other n¬¥1languages in a zero - shot setting .   MMA For this setting , we train nindividual mod-   els , one for each language . Each model starts with   a seed of s{n , a validation set of v{n , and is as-   signed an acquisition budget of b{n . At test time ,   we evaluate the performance of the model on the   language it was trained with .   SMA For this setting , we consider a single model   for which both training and acquisition is done on   allnlanguages simultaneously . The seed data and   the validation set comprises of a random subset   drawn from data corresponding to all languages .   The whole of s , bandvare thus assigned to this   single model . We compute the performance on the   test data of each of the languages .   3.3 Active Learning Acquisition Strategies   The field of active AL tends not to reveal explicit   winners ‚Äî though there is a general consensus that   AL does indeed outperform passive learning ( Set-   tles , 2009 ) . Thus , we adopt the simplest confidence   based strategies to demonstrate their efficacy for   each task : Least Confidence ( LC ) for classification ,   Maximum Normalized Log Probability ( MNLP )   ( Shen et al . , 2018 ) for sequence tagging , and nor-   malized log probability of decoded tree ( NLPDT )   ( Li et al . , 2016 ) for dependency parsing   Maximum Normalized Log Probability ( MNLP )   This strategy chooses instances for which the log   probability of the model prediction , normalized by   sequence length , is the lowest . This AL strategy   has been shown to be extremely effective for NER70(Shen et al . , 2018 ) and hence we adopt it in our   setting .   Least Confidence ( LC ) This strategy chooses   those instances for which the model confidence cor-   responding to the predicted class is the least . This   acquisition strategy has been commonly applied in   classification tasks , and although simple , has been   consistently shown to often perform extremely well   ( Settles , 2009 ) ; consequently , we adopt it in our   setting .   Normalized Log Probability of the Decoded   Tree ( NLPDT ) This strategy selects the instances   with the minimum log probability of the de-   coded tree generated das generated by the Chu-   Liu / Edmonds algorithm ( refer A for additional de-   tails ) . Following ( Li et al . , 2016 ) , we also normal-   ize this score by the number of tokens N.   To the best of our knowledge , this is the first   work to explore an AL - augmented single model for   multiple languages .   4 Experiments   4.1 Dataset Details   Classification We consider Sentiment Analysis ,   using the Amazon Reviews dataset ( Prettenhofer   and Stein , 2010 ) . The dataset consists of reviews   and their binary sentiments for 4 languages : En-   glish ( en ) , French ( fr ) , Japanese ( ja ) , German ( de ) .   Sequence Tagging We choose Named Entity   Recognition , and use the CoNLL02/03 datasets   ( Sang , 2002 ; Tjong Kim Sang and De Meulder ,   2003 ) with 4 languages : English ( en ) , Spanish ( es ) ,   German ( de ) and Dutch ( nl ) , and 4 named entities :   Location , Person , Organization and Miscellaneous .   Dependency Parsing We use a subset of tree-   banks with 5 languages ( English ( en ) , Spanish ( es ) ,   German ( de ) , Dutch ( nl ) , Japanese ( ja ) ) from the   full Universal Dependencies v2.3 corpus ( Nivre   et al . , 2018 ) ; a total of 11 treebanks .   4.2 Experimental Settings   For each experiment , we run 4 training rounds : one   training on initial seed data , followed by 3 acqui-   sition rounds . We set s‚Äúb‚Äúvin all cases . Forclassification , we set s=300sentences . For NER   and Dependency Parsing , we use s‚Äú‚Äû10kand   s‚Äú‚Äû17.5ktokens respectively ( refer Appendix   B ) . We report accuracy for classification , F1 - Score   for the NER , and unlabeled and labeled attachment   scores ( UAS and LAS ) for dependency parsing .   For each task , we run the 3 settings ( ¬ß 3.2 ) across   multiple languages . For each setting , we also train   an AL model with a task - specific acquisition func-   tion ( ¬ß 3.3 ) . In addition , we train both the SMA and   MMA with all available data , i.e. , we use all data   to train one model for all languages and one model   per language respectively . We report an average of   5runs for each experiment . Refer Appendix C for   hyperparameters and training details .   5 Results and Analysis   Model Performance Figure 1 shows the perfor-   mance of NER on Spanish ( refer Appendix G for   the plots of all other languages and tasks ) . Al-   though acquiring data independently per language   ( MMA ) performs well , SMA outperforms MMA .   Unsurprisingly , MonoA with es performs the best   in the category , since it allocates its entire budget   to acquiring es data ; it thus forms an upper - bound   of the model performance . However , SMA out-   performs MonoA when its seed language and in-   ference language differ . Finally , AL consistently   provides gains over random acquisition .   To analyze the performance across all languages ,   we present the performance for each round of ac-   quisition , aggregated across all languages for Clas-   sification ( Figure 2 ) ( refer Appendix G for De-   pendency Parsing and NER plots ) . Here , SMA   consistently outperforms MMA for every round of   acquisition because MMA suffers from a poorly   utilized budget , potentially wasting annotation bud-   get on languages where the task is easier . In con-   trast , SMA improves budget utilization while also   benefiting from cross - lingual information . Finally ,   SMA , by virtue of performing well irrespective of   language , consistently outperforms MonoA.   For a concise overview , we present the aggregate   metrics across all rounds for each task in Table 1 .   We observe that SMA does much better compared   to its counterparts ; both with and without AL . We   also observe these models to be extremely data ef-   ficient : with AL , a model with access to less than   5 % of the data achieves a ( relative ) performance   of around 88 % accuracy ( for classification ) , 95.5 %   F1 - score ( for NER ) and 93.5 % LAS ( for depen-71   dency parsing ) when compared to a model trained   with all available data ( see Table 2 for full data   performance ) . Further , along with its superior per-   formance , SMA also affords substantial parameter   savings : requiring only a single model , compared   to a number of models linear in n(thereby usingparameters compared to MMA ) .   MM Full vs SM Full To analyze how effectively   a single model performs on the languages in ques-   tion despite using 1{nthe parameters , we train   a single model on all data and compare it with   nlanguage - specific models , where each of the n   models has the same number of parameters as the   single model ; this also serves as an upper - bound   for our AL experiments . Table 2 shows that having   a single model does not adversely impact perfor - mance . A more detailed discussion is present in   Appendix D.   The effectiveness of AL in MonoA We consis-   tently observe AL in the source language improv-   ing performance across all languages , irrespective   of whether inference is being run for the source lan-   guage or zero - shot on a different target language ,   both for NER and classification ( Table 3 ) . We hy-   pothesize that the model selects semantically diffi-   cult or ambiguous examples that generalize across   languages by virtue of mBERT ‚Äôs shared embed-   ding representation . To the best of our knowledge ,   this work is the first to demonstrate that AL can   improve the data efficiency of both classification   and NER in a zero - shot inference setup .   In the case of dependency parsing , we observe   mixed results when the source and target languages72differ . We hypothesize that this is because depen-   dency parsing is a syntactic problem , making it   more language specific , and zero - shot inference   inherently harder . This is in contrast with both   classification and NER , which are more semantic ,   making hard examples more generalizable across   languages . Refer Appendix E for more details .   What does SMA+AL acquire ? One advantage   of the SMA+AL setup is that the model can ar-   bitrate between allocating its acquisition budget   across different languages as training progresses .   This is in contrast with training one model per lan-   guage , where the models for languages with a high   performance waste the overall budget by acquiring   more than necessary , while models on languages   where performance is n‚Äôt as good under - acquire .   To investigate this , for each language and each   round , we plot the relative difference ( % ) between   cumulative tokens acquired by the SMA+AL model   for that language , and the tokens acquired in ex-   pectation if acquisition was done randomly ( refer   Appendix F for more details ) . For each language ,   we also plot the relative performance difference of   the language at that round compared to the perfor-   mance when 100 % data is available .   Figure 3 reveals the added benefit of SMA+AL   for data acquisition for NER ( refer Appendix F   for other tasks ): a single model can arbitrate be-   tween instances across languages automatically .   The model initially acquires data from the high   resource language ( English ) . But as the training   proceeds , the model favors acquiring data from   languages it is uncertain about ( Spanish and Ger-   man ) . This ‚Äú multilingual curriculum ‚Äù thus allows   the model to be more effective in its use of the   annotation budget . We find SMA+AL eventually   achieves a similar relative difference from 100 %   data performance for all languages consistently   across tasks as a consequence .   6 Conclusion   In this work , we consider the problem of efficiently   building models that solve a task across multiple   languages . We show that , contrary to traditional ap-   proaches , a single model arbitrating between mul-   tiple languages for data acquisition considerably   improves performance in a constrained budget sce-   nario , with AL providing additional benefits .   References737475A Task Specific Details   In this section , we elaborate on the task specific   adaptations :   Classification : As is common practice , we use   a single linear layer over [ CLS ] embeddings gen-   erated by the BERT model to generate logits for   the classification task , and the model is trained to   minimize the cross - entropy loss .   Sequence Tagging : We apply a linear layer to the   word embeddingsgenerated by the BERT model   to generate the tag logits , and the model is trained   to minimize the negative log - likelihood of the ob-   served tags .   Dependency Parsing : We use a graph - based bi-   affine attention parser introduced in ( Dozat and   Manning , 2017 ) . Following ( Kondratyuk and   Straka , 2019 ) , we use the output of the last BERT   layer in place of the embeddings generated by   the Bi - LSTM layers . These embeddings are then   concatenated with the POS embeddings . A head   feed - forward network and a child feed - forward   network then generate embeddings for each head   and dependant word of a dependency respectively .   This is combined with a biaffine attention mod-   ule to generate a probability distribution for each   word to predict its head , as well as a bilinear   layer to predict the label for each dependency re-   lationship . Let œÑ‚Äútph , d , l|h√±   dwith label lube the igold dependency   tree in the dataset . The model is then trained to   maximize the log probability of the gold tree as :   max√ø√ølog `   Pph|dqÀò   ` log `   Ppl|h√±dqÀò(1 )   During inference , the best dependency parse is   generated by decoding with Chu - Liu / Edmonds al-   gorithm ( Chu , 1965 ; Edmonds , 1967 ) .   For all the models mentioned above , all layers   of mBERT are fine - tuned during training .   B Dataset statistics   We report the detailed dataset statistics in Table 4 .   Note that the seed was chosen to be roughly 5 % of   the size of the English training data , shown in the   rightmost column of the table . C Experimental Details   Hyperparameters All experiments performed   in this paper are averaged over 5 runs . For each   experiment , we perform an LR search over ( 1e-5 ,   2e-5 , 3e-5 , 4e-5 and 5e-5 ) , and choose the best LR   according to the performance on the appropriate   validation ( sub)set , as recommended in ( Devlin   et al . , 2019 ) . In all experiments , we set the batch   size to 32 and use an Adam ( Kingma and Ba , 2015 )   optimizer . Each round of training is run with a   patience of 25 epochs , for at most 75 epochs in   total .   Data Preprocessing To avoid out - of - memory is-   sues on the GPU , we pre - process the data so that   the examples in the train set of length larger than   175 and with larger than 256 word - pieces are fil-   tered out for the NER . For classification , we simply   truncate all instances at 256 word - pieces . We also   de - duplicate the train set , to ensure that during all   AL acquisition stages , no duplicates are selected at   any point .   Code All code used in this work was imple-   mented using Python , PyTorch and AllenNLP   ( Gardner et al . , 2018 ) , using pre - trained models   released by HuggingFace ( Wolf et al . , 2020 ) .   D SM Full vs MM Full Performance   Given that the SMA setup uses 1{nthe number   of parameters , an interesting question is whether   fewer parameters leads to a loss in any expressive   power for the single model , which might potentially   lead to poorer performance ( curse of multilingual-   ity ( Conneau et al . , 2020 ) ) . To answer this question ,   we train a single model on all data and compare   it with nlanguage - specific models , where each of   thenmodels has the same number of parameters   as the single model .   From the 100 % ( rightmost ) columns of Table   2 , we find that having a single model does not ad-   versely impact performance and these trends hold   irrespective of whether all the languages in the task   are etymologically close ( as in NER ) or distant ( ja   for classification and dependency parsing ) . This   might not be the case when there are a large number   of languages , however ; investigating how well this   observation scales with the number of languages   would be an interesting line of future work.76   E Active Learning for the MonoA Setup   An interesting observation from Table 3 is that AL   in the source language helps improve performance   across all languages , irrespective of whether the   inference is being run for the source language in   question or zero - shot on a different target language   without any training . We observe this to be the   case consistently for both the NER and the clas-   sification tasks ( refer Figure 4 ) , regardless of the   source language . We hypothesize that this is be-   cause the model selects semantically difficult or   ambiguous examples that generalize across lan-   guages by virtue of mBERT ‚Äôs shared embedding   representation , in contrast with random selection   where easy examples the model can already tackle   might be selected . We observe this even in the case   of etymologically distant languages , such as when   the model is trained in English and zero - shot in-   ference is done in Japanese ( or vice versa ) . Thus ,   the AL selection does not overfit on the specific   language in question , instead choosing difficult but   generalizable examples .   We observe mixed results for the MonoA setup   for dependency parsing : AL improves substantially   over Random when the target language and the   source language are the same ; however , when they   differ , the results are mixed . We hypothesize that   this discrepancy is a consequence of dependency   parsing being a syntactic problem , making it more   language specific , in turn making zero - shot an in-   herently harder problem . This is in contrast with   both classification and NER , which are more se-   mantic tasks . Consequently , hard examples for   the latter tasks might be more generalizable across   languages , resulting in their improved AL perfor-   mance , when compared with the dependency pars-   ing task.77F Acquisition Ablation Details and   Curriculum   In this section , we describe the analysis of inves-   tigating the acquisitions of SMA+AL in more de-   tail . Let Œ±¬®¬®¬®Œ±be the language specific amount   of data present in the entire dataset ( i.e Œ±‚Äú0.3   implies that 30 % of the entire dataset ( training +   unlabeled ) is of language i ) , and let Œ≤¬®¬®¬®Œ≤   represent the amount of data acquired for every lan-   guage at every round ( i.e Œ≤indicates the amount   of data acquired by language jat round i ) . Then ,   for a task t , for each round iand language j , we   plot .   Figures 5 and 6 show the acquisition curriculum .   We observe a similar for both the tasks as that for   dependency parsing .   G Detailed Results   This section the additional plots as well as the de-   tailed tables and results for all the experiments   presented in the paper .   G.1 Per Acquisition Round Performance for   Dependency Parsing   Figures 7 and 8 show the UAS and LAS for each   round of acquisition for dependency parsing , ag - gregated across all languages .   G.2 Per Acquisition Round Performance for   NER   Figure 9 shows the F - Score for each round of ac-   quisition for NER , aggregated across all languages.78G.3 Experiments for NER   Tables 5 , 6 , 7 and 8 show the performance of the different AL settings on English , Spanish , Dutch and   German respectively . Each table shows the F - score across 4 acquisition rounds , both with and without   MNLP ( ¬ß 3.2).79   G.4 Experiments for Classification   Tables 9 , 10 , 11 and 12 show the performance of the different AL settings on English , French , Japanese   and German respectively . Each table shows the accuracy across 4 acquisition rounds , both with and   without LC ( ¬ß 3.2).80   G.5 Experiments for Dependency Parsing   Table 13 compares the performance ( LAS and UAS ) of the single model trained on all data to the   performance of one model trained per language . Table 14 gives the detailed breakdown of each AL setup   for each of the dependency parsing datasets , aggregated across all the acquisition rounds.8182G.6 Language Specific Acquisition Plots   Analogous to Figure 1 in the main paper , each fig-   ure in this section presents the performance of the   different methods for a specific language and a spe-   cific task , at each round of acquisition . The trends   observed are fairly consistent : SMA and MMA   both do consistently well , with SMA outperform-   ing MMA . MonoA for the specific language does   well , but with all other languages performs worse .   AL consistently improves performance .   G.6.1 NERG.6.2 Classification83G.6.3 Dependency Parsing : LAS   For dependency parsing , the MonoA performance   of Japanese ( MonoA[ja ] ) is poor on all other lan-   guages ( Fig . 17 , 18 , 20 , 21 , 22 , 23 , 25 , 26 ) , while   the performance of all other languages is poor on   Japanese ( Fig . 19 , 24 ) . Consequently , the graphs   below have a kink in order to capture this difference   in the range of performance of the languages.84G.6.4 Dependency Parsing : UAS85   Ganesh JawaharMuhammad Abdul - MageedLaks V . S. LakshmananDeep Learning & Natural Language Processing Group , Data Management & Mining Group   The University of British Columbia   Abstract   1 Introduction   A type of fake news that has received little atten-   tion in the research community is manipulated text .   Manipulated text is typically created by manip-   ulating a human written news article minimally   ( e.g. , replacing every occurrence of a particular en-   tity , ‚Äò Obama ‚Äô in a news article with another Amer-   ican politician entity ) . Current fake news detec-   tors that exploit stylometric signals from the text   ( e.g. , choice of specific words to express false state-   ments ) are clearly insufficient for distinguishing   manipulated text from human written text ( Zhou   et al . , 2019 ; Schuster et al . , 2020 ) as the style un-   derlying the manipulated text is virtually identical   to human writing style . In this work , we focus on   this problem of distinguishing manipulated news   articles from human written news articles .   We consider a particular type of text manipu-   lation ‚Äî entity perturbation ( Zhou et al . , 2019 ) ,   where a manipulated news article is created by mod-   ifying a fixed number of entities in a human writ-   ten news article ( e.g. , replacing them with entities   generated from a text generative model ) . E.g. , in   Table 1 , to mislead humans , the entity ‚Äò Relay Ven-   tures ‚Äô can be replaced by ‚Äò Samsung ‚Äô ( a candidate   replacement entity generated by the generative pre-   training-2 model ( GPT-2 ) ( Radford et al . , 2019 ) ) ,   which is locally consistent as some of the other   companies in the original text are also into device   manufacturing .   To distinguish a manipulated news article from   the original human written news article , we propose   a neural network based detector that jointly utilizes   the textual information along with the the factual   knowledge explicitly by building entity - relation   graphs which capture the relationship between dif-   ferent entities present in the news article . The fac-   tual knowledge is encoded by a graph convolutional   neural network ( Kipf and Welling , 2017 ) that cap-   tures the interactions between different entities and   relations , which we hypothesize , carries discrimina-   tory signals for the manipulated text detection task.86Our major contributions include : ( i ) a detector that   exploits factual knowledge to overcome the limi-   tations of relying only on stylometric signals , ( ii )   an approach to generate challenging manipulated   news article dataset using GPT-2 , and ( iii ) a collec-   tion of challenging datasets by considering various   strategies to generate the replacement entity .   2 Background and Related Work   The manipulated text detection task is related to   diverse research areas such as fake news detection ,   natural language understanding , and knowledge   bases .   Fake news detection . Research on Fake news de-   tection typically deals with challenges such as un-   derstanding the news content ( Schuster et al . , 2020 ) ,   claim verification ( Thorne and Vlachos , 2018 ) , ver-   ifying the credibility of the source ( Castillo et al . ,   2011 ) , and exploiting fake news propagation pat-   terns ( V osoughi et al . , 2018 ) . Our work is primarily   focused on detecting fake news in the form of ma-   nipulated text , by understanding the news content .   In the traditional problem setting , both fake and real   news is assumed to be written by a human ( Shu   et al . , 2017 ; Oshikawa et al . , 2020 ) . Since humans   tend to make stylistic choices ( e.g. , choosing some   specific language for writing false statements ) , the   fake news detector can perform reasonably on the   task by picking up on these stylometric signals .   One can also create fake news by manipulating a   human written news article minimally . Such ma-   nipulations include : entity perturbation ( e.g. , ‚Äò 12   people were injured in the shooting ‚Äô to ‚Äò 24 people   were killed in the shooting ‚Äô ) ( Zhou et al . , 2019 ) ,   subject - object exchange ( e.g. , ‚Äò A gangster was shot   by the police ‚Äô to ‚Äò A policeman was shot by the   gangster ‚Äô ) ( Zhou et al . , 2019 ) , and adding / deleting   negations ( e.g. , ‚Äò Trump does n‚Äôt like Obamacare ‚Äô to   ‚Äò Trump likes Obamacare ‚Äô ) ( Schuster et al . , 2020 ) .   These manipulations do not typically affect the   style and hence stylometric signals alone can not   help in building accurate manipulated text detection   models ( Zhou et al . , 2019 ; Schuster et al . , 2020 ) .   Natural language understanding . Pre - trained lan-   guage models such as BERT ( Devlin et al . , 2019 )   and RoBERTa ( Liu et al . , 2019 ) achieve strong   performance in diverse NLP tasks . Specifically ,   RoBERTa is the state - of - the - art detector when fine-   tuned for detection of synthetic text ( Solaiman   et al . , 2019 ; Jawahar et al . , 2020 ) . These mod-   els can also capture implicit world knowledge ( e.g. ,Paris is the capital of France ) that occurs frequently   in the text ( Petroni et al . , 2019 ) . However , it is   insufficient for solving our task ( Schuster et al . ,   2020 ) , as it is limited to frequent patterns .   Knowledge bases ( KBs ) . Knowledge bases ( e.g. ,   YAGO ( Tanon et al . , 2020 ) ) containing typically   a collection of facts ( e.g. , subject - relation - object   triples ) , provide specialized knowledge for down-   stream NLP tasks ( e.g. , question answering ( Baner-   jee and Baral , 2020 ) ) . One can integrate such sym-   bolic knowledge into pre - trained language models   during pre - training ( Zhang et al . , 2019 ) and finetun-   ing ( Liu et al . ( 2020 ) ; Zhong et al . ( 2020 ) , which   we follow in this work ) .   3 Manipulated Text Creation   In this work , we focus on a particular type of manip-   ulation ‚Äî entity perturbation ( Zhou et al . , 2019 ) ,   where all occurrences of a fixed number of ran-   domly picked entities from a human written news   article are replaced with different replacement en-   tities . We replace named entities of three types :   person , organization and location ( recognized us-   ing spaCy ‚Äôs named entity recognizer ( NER ) ( Hon-   nibal et al . , 2020 ) ) . We ensure the replacement   ( new ) entity belongs to the same type as the origi-   nal ( old ) entity . We create challenging manipulated   text datasets by considering various strategies to   identify the new replacement entity : random most   frequent entity ( pick randomly from among the top   5000 entities ) , random least frequent entity ( pick   randomly from the bottom 5000 entities ) , and entity   generated by GPT-2 . Sample manipulated entities   obtained from different replacement strategies are   shown in Table 2 .   GPT-2 generated entity replacement . Strategies   that randomly identify the replacement entity ig-   nore the context provided by the news article . For   example , in news portion ( 1 ) , a random replace-   ment entity for ‚Äò Relay Ventures ‚Äô can be ‚Äò Sales-   force ‚Äô . However , it is likely locally inconsistent as   ‚Äò Salesforce ‚Äô is not into device manufacturing unlike87many other co - occurring companies in the origi-   nal text . We propose a novel approach that makes   use of the state - of - the - art text generative model   GPT-2 to pick replacement entities that are locally   consistent . Revisiting the news portion ( 1 ) , let the   randomly selected entity to be replaced be ‚Äò Re-   lay Ventures ‚Äô . We treat the fragment of text from   the beginning of the article up to the tokens be-   fore the first occurrence of the target entity ( ‚Äò Relay   Ventures ‚Äô ) as the prompt . We provide this prompt   to GPT-2 , which can then generate the next few   tokens . We call the generated token sequence a   candidate replacement entity if the sequence starts   with an entity ( e.g. , ‚Äò Samsung ‚Äô ) of same type as the   target entity ( ‚Äò Relay Ventures ‚Äô ) and has no string   overlap with the target entity . If the constraints are   not met , we ask GPT-2 to create the generated se-   quence again up to a maximum of 10 attempts . The   candidate replacement entity thus obtained will be   used to replace all occurrences of the target entity .   For the news portion ( 1 ) , the candidate replacement   entity generated by GPT-2 is ‚Äò Samsung ‚Äô , which is   locally consistent : similar to other companies in   the original text , Samsung manufactures devices .   4 Manipulated Text Detection   The goal of this work is to build a detector that dis-   tinguishes manipulated news article from human   written news article with high accuracy . In prior   work , Zhou et al . ( 2019 ) conclude that the manipu-   lated article can possibly be detected by checking   the facts underlying the article with knowledge   bases and Schuster et al . ( 2020 ) show that humans   can identify the manipulated text well when they   are allowed to consult external sources ( e.g. , inter-   net ) . Building on these findings , we hypothesize   thatfactual knowledge underlying the news arti-   cle can provide discriminatory signals for manip-   ulated text detection . To this end , we embody the   RoBERTa detector with explicit factual knowledge   so that the detector can reason about facts present   in the news article , whose details we discuss next .   Factual knowledge . For factual knowledge , we   leverage a variant of YAGO 4 KB ( Tanon et al . ,   2020 ) that contains only instances that have an En-   glish Wikipedia article . We then extract the facts in   a given document by first identifying all the entities   present in the document using spaCy ‚Äôs NER . For   each target entity , we grab all the triples in the KB   where the subject matches with the target entity   at surface level . These triples can be seen as thefirst hop neighbors of the target entity in the KB .   For a given document , the set of triples collected   over all identified entities is used to build the cor-   responding factual graph . A node can be an entity   or a relation . A directed edge is added between   subject and relation , as well as relation and object .   This factual graph contains rich factual information   about entities present in the document , which can   be exploited to reason about facts mentioned in the   article for correctness .   Integrating factual knowledge with RoBERTa .   Our proposed detector is an integration of the   RoBERTa model with factual knowledge . This   allows the detector to reason about facts men-   tioned in the article . To embed the factual knowl-   edge , we employ graph convolutional networks   ( GCNs ) ( Kipf and Welling , 2017 ) , where we stack   lGCN layers and the definition of the hidden rep-   resentation of each node vof the factual graph as   layerk+ 1 , in a graph G= ( V , E ):   whereW , b , h , N(v)correspond to layer spe-   cific model weights , biases , node representation ,   and neighbors of v in Grespectively . Note that   hdenotes the initial node features , which can be   initialized randomly or using a pre - trained entity   embedding such as Wikipedia2vec ( Yamada and   Shindo , 2019 ) .   Detector prediction . The factual knowledge about   entities present in the article is captured in the node   embeddings ( h ) corresponding to the last layer l   of the GCN model . The textual knowledge corre-   sponding to the document can be obtained from the   last layer representation ( r ) of the RoBERTa   model corresponding to the first token ( ‚Äò [ CLS ] ‚Äô ,   special classification token ) of the RoBERTa input .   We combine the factual and the textual knowledge   by simply averaging all the GCN ‚Äôs entity embed-   dings and concatenating the entity average with   the RoBERTa ‚Äôs document embedding . Thus , the   unnormalized prediction probabilities ( mf(d ) ) of   our detector for the document dcan be given by :   where [ ; ] corresponds to the concatenation opera-   tion and W , bcorrespond to the affine trans-   formation specific model parameters for manipu-88   lated text detection . The output from mf(d)passes   through dropout followed by ReLU layer .   Identifying manipulated entities . To enable hu-   mans to understand our detector ‚Äôs decision and per-   form further investigation , we introduce a subtask   for the detector , namely identify the manipulated   entities among different entities present in the doc-   ument . For this subtask , we build on the entity   representations output by the last layer of the GCN   model . The unnormalized class prediction proba-   bilities ( ef(v ) ) for a given entity vfrom the article   can be given by :   where hdenotes the hidden representation at last   layerlfor the entity v , andW , bcorrespond to   the affine transformation specific model parame-   ters for entity classification . The overall objective   function of the proposed detector can be given by :   whereL , mf , andsresp . correspond to the func-   tion that computes the negative log - probability of   the correct label , detection prediction function , and   softmax function . ydenotes the entity manipu-   lation class label , which is 1if the entity eis ma-   nipulated , and 0otherwise . ydenotes the article   manipulation class label , which is 1if at least one   entity in article iis manipulated , and 0otherwise .   5 Experiments and Results   Dataset and Detector Settings . The human writ-   ten news articles used in our study are taken fromthe RealNews dataset ( Zellers et al . , 2019 ) , which   contains 5000 , 2000 , and8000 news articles in the   training , validation , and test set respectively . We   randomly pick half of the news articles in each   set for human written news article category and   the rest in each set for manipulation based on the   chosen replacement strategy . We also create three   different datasets for each replacement strategy by   varying the maximum number of entities to be ma-   nipulated from 1 to 3 . Detailed statistics of the   proposed datasets is in A.1 . The hyperparameter   search space for all detectors is offered in A.2 .   Hardest detection task . Table 3 presents the de-   tection accuracy results . We observe that the most   challenging dataset for the state - of - the - art detector   is surprisingly from random most frequent entity   replacement strategy with exactly one entity re-   placement . The random strategies fail to create a   challenging dataset with high ( e.g. , 3 ) number of   entity replacements , which indicates that the de-   tection task becomes easier with increase in the   number of locally inconsistent entities . Neverthe-   less , our proposed GPT-2 based entity replacement   strategy keeps the detection task harder even for   large number of replacements , thanks to the ability   of the strategy to generate locally consistent enti-   ties mostly . Regardless of the replacement strate-   gies , the detection performance of all the detectors   increases with the increase in the number of en-   tities that are manipulated in a document , that is ,   more the manipulations in a document , the easier   the detection task . This result is similar to pre-   vious research which performs manipulation by   adding / deleting negations in news articles ( Schus-   ter et al . , 2020 ) . A fake news propagator can thus89   manipulate exactly one entity in the news article to   make the detection task harder .   Detector performance . Nevertheless , our pro-   posed detector performs similarly to or outper-   forms the state - of - the - art detector on all replace-   ment strategies across different numbers of entity   replacements . This result validates our hypothesis   that leveraging both factual and textual knowledge   can improve detection performance , overcoming   the limitations of relying only on textual knowl-   edge . Improvements of our proposed detector on   the GPT-2 generated entity manipulation task are   not significantly high due to sizeable increase in   manipulated entities absent in the knowledge base   ( e.g. ,‚àº50 % , see last three rows in Table 6 ) .   Entity identification performance . Our proposed   detector is equipped to identify entities that are ma-   nipulated in a news article . This task is harder due   to the imbalanced nature of the task as most of the   entities present in the news article are not manipu-   lated . As shown in Table 3 , our proposed detector   achieves high precision ( ‚â•70 % ) in identifying   manipulated entities , which makes our detector ap-   pealing for applications that favor precision . The   recall is very low ( < 15 % ) , which indicates the   difficulty of the task . We also experiment with a   baseline RoBERTa model trained at the token level   to identify spans of manipulated entities . How-   ever , the model seems overwhelmed by the major-   ity class ( token not part of the manipulated entity   span ) and predicts all the tokens to belong to the   majority class . We believe there is a lot of room   for improvement in this subtask .   Detecting articles with unknown manipulated   entities . Table 4 shows performance of the detector   on manipulated articles when all the manipulated   entities are not present in the knowledge base . We   observe that our proposed detector can rely on the   relations corresponding to the non - manipulated en-   tities and pretrained textual representations to out - perform , or at least be on par with , the RoBERTa   model .   Quality gap between human and manipulated   text . Table 5 shows how the quality of the ma-   nipulated text changes with respect to human writ-   ten text across different replacement strategies , for   different numbers of replacements . We utilize   MAUVE ( Pillutla et al . , 2021 ) , a metric to measure   the closeness of machine generated text to human   language based on divergence frontiers . Since the   proposed manipulations touch only limited spans   ( i.e. , entities ) in the entire document , the overall   quality of the manipulated text does not change   much with more replacements .   6 Conclusion   We presented the first principled approach for de-   veloping a model that can detect entity - manipulated   text articles . In addition to textual information , our   proposed detector exploits explicit factual knowl-   edge from a knowledge base to overcome the limi-   tations of relying only on stylometric signals . We   constructed challenging manipulated datasets by   considering various entity replacement strategies ,   including with random selection and GPT-2 gen-   eration . On all the experimental settings , our pro-   posed model outperforms ( or is at least on par with )   the baseline detector in overall detection accuracy .   Our results show that manipulated text detection re-   mains challenging . We hope that our work will trig-   ger further research on this important but relatively   understudied subfield of fake news detection.90Acknowledgements   We gratefully acknowledge support from the Nat-   ural Sciences and Engineering Research Council   of Canada ( NSERC ; RGPIN-2018 - 04267 ) , the So-   cial Sciences and Humanities Research Council   of Canada ( SSHRC ; 435 - 2018 - 0576 ) , Canadian   Foundation for Innovation ( CFI ; 37771 ) , Com-   pute Canada ( CC),UBC ARC - Sockeye , and Ad-   vanced Micro Devices , Inc. ( AMD ) . Any opinions ,   conclusions or recommendations expressed in this   material are those of the author(s ) and do not nec-   essarily reflect the views of NSERC , SSHRC , CFI ,   CC , ARC - Sockeye , or AMD . We also thank Ayushi   Dalmia for proofreading and helpful discussions .   References91A Appendices   A.1 Summary Statistics of Proposed Datasets .   Table 6 displays the statistics of proposed datasets .   A.2 Hyperparameter Search Space for All   Detectors   Table 7 displays the search space for hyperparame-   ters used to tune all the detectors.9293   Ruixi Lin and Hwee Tou Ng   Department of Computer Science   National University of Singapore   { ruixi,nght}@comp.nus.edu.sg   Abstract   1 Introduction   The IS - A relation denotes a subclass relation . If   Ais - aB , then the concept A is a subclass of the   concept B , or A is subsumed by B. The IS - A re-   lation is frequently encoded in lexical taxonomies .   The IS - A relation has great significance since it em-   powers generalization , and generalization is at the   core of machine inference for text understanding .   The IS - A hierarchy is inherently transitive , i.e. , for   three concepts ( or word senses ) A , B , and C , A   is - aB and B is - aC entail A is - aC. For example ,   knowing that humanoid is a type of automaton , and   automaton is a type of artifact , then by transitivity ,   the relation humanoid is an artifact also holds .   The concept of transitivity is easy to compre-   hend by humans . However , deep learning mod-   els , including pre - trained language models such   as BERT ( Devlin et al . , 2019 ) , are known to lacksome human - level generalization capacities in text   understanding , or it may show some capacities for   making correct predictions but for the wrong rea-   sons , including being insensitive to negation and ex-   ploiting only surface features ( Kassner and Schutze ,   2020 ; Ettinger , 2020 ) , lacking understanding of per-   ceptual properties ( Forbes et al . , 2019 ; Weir et al . ,   2020 ) , and surface form competition ( Holtzman   et al . , 2021 ) .   Despite the issues raised above , previous work   has shown that BERT ‚Äôs layers align with the NLP   pipeline , and representations in the different layers   of BERT are found to capture different levels of   textual understanding , from syntactic ( e.g. , part-   of - speech tagging ) to semantic ( e.g. , semantic role   labeling ) as the layers go from the lower to higher   layers ( Tenney et al . , 2019a , b ) . Recent studies   also suggest that BERT can capture lexical relation   clues from words in contexts ( Vuli ¬¥ c et al . , 2020 ;   Misra et al . , 2020 ) . Researchers begin to recog-   nize BERT as an open knowledge source and query   BERT for information ( Petroni et al . , 2019 ) . More-   over , BERT , even without fine - tuning on down-   stream tasks , possesses a fair ability to produce con-   textualized embeddings that cluster to word senses   ( Wiedemann et al . , 2019 ; Haber and Poesio , 2020 ;   Mickus et al . , 2020 ; Loureiro et al . , 2021 ) . These   findings suggest that BERT has some understand-   ing of the building blocks of language . Following   these findings , since an IS - A taxonomy can be built   on top of explicit word senses , do contextualized   embeddings learned from BERT for word senses   ( in particular contexts ) respect the properties of the   IS - A taxonomy , specifically transitivity ? That is ,   does BERT make logically consistent predictions   that enforce the transitivity constraint of the IS - A   relation ?   In this paper , we introduce a minimalist probing   method to investigate whether BERT knows that   the IS - A relation is transitive . We first quantify   how well BERT predicts the IS - A relation . Next,94we measure the extent to which BERT enforces the   transitivity constraint . That is , given that BERT   predicts A is - aB and B is - aC , does it then predict   Ais - aC ?   In our work , we make use of WordNet ( Fell-   baum , 1998 ) and propose a method to sample word   sense pairs with contexts from WordNet example   sentences to build a probing dataset . We use a near-   est neighbor classifier for probing , which does not   require any parameter tuning . Our findings indi-   cate that BERT can predict IS - A relations with an   accuracy score of 72.6 % . However , when BERT   predicts Ais - aBandBis - aC , it only predicts A   is - aC82.4 % of the time . This suggests that simply   treating BERT as is as a knowledge base ( Petroni   et al . , 2019 ) is not completely satisfactory , and ad-   ditional work needs to be done to incorporate the   transitivity constraint in natural language inference   when using BERT .   2 Related Work   A key weakness of deep learning models is that   they are black - box models and do not offer explain-   able and interpretable predictions . This has led to a   large body of research regarding their interpretabil-   ity ( Linardatos et al . , 2021 ) . The pre - trained lan-   guage model BERT has been extensively analyzed   since its release . In particular , feature - based probes   have been proposed to show how a particular layer ,   head , or neuron of BERT works on a downstream   NLP task . Usually with a small set of additional pa-   rameters , a probe is trained in a supervised manner   using feature representations from the pre - trained   BERT , e.g. , contextualized embeddings , to solve   a particular task ( Wu et al . , 2020 ) . Attention and   structural probes have been invented to investigate   different aspects of BERT and linguistic proper-   ties ( Lin et al . , 2019 ; Jawahar et al . , 2019 ; Clark   et al . , 2019 ; Hewitt and Manning , 2019 ; Manning   et al . , 2020 ; Tenney et al . , 2019a , b ; Pruksachatkun   et al . , 2020 ) . Latent ontology of contextual embed-   dings has also been investigated via cluster analysis   ( Michael et al . , 2020 ) . On probing contextualized   representations for lexico - semantic relations , pre-   vious studies have investigated BERT for lexical   relation classification via a neural network probe   on type - level embeddings ( Vuli ¬¥ c et al . , 2020 ) .   Our work differs from prior work by our goal to   explicitly investigate how much BERT understands   the IS - A relation and more importantly , obeys the   transitivity constraint . That is , we aim to determine   if and how often BERT makes logically consistent   predictions for the IS - A relation . Moreover , we   focus on investigating sense - based IS - A relation ,   which is associated with explicit word senses in   their contexts , so contextualized embeddings can   be clearly mapped to word senses .   3 Experimental Setup   We probe if and how well BERT can predict the   IS - A relation and its transitivity .   Task Definition For a dataset of interest , we de-   note it as D={[(u , v ) , y],¬∑¬∑¬∑,[(u , v ) , y ] } .   ( u , v ) = [ ( u , v)¬∑¬∑¬∑,(u , v)]are repre-   sentations for pairs of word senses and y=   ( y , ¬∑ ¬∑ ¬∑ , y)are labels for the IS - A relation clas-   sification task , where 1 denotes the positive IS - A   relation and 0 otherwise . In our probing task , we   quantify the extent to which ( u , v)encode   relations y. To probe BERT , we use the contex-   tualized embedding ( i.e. , BERT ‚Äôs final hidden state   output ) of a word in a given context as the repre-   sentation for the sense associated with the word ‚Äôs   meaning in that context .   Contextualized Embeddings In this work , we fo-   cus on the BERT - base model . Given a target word   wand its context c , BERT produces a final hid-   den state output as the contextualized embedding   ofor the target word w. Ifwis tokenized into   subwords , we take the average over all subwords   to be the contextualized embedding .   3.1 Probing Dataset   WordNet ( Fellbaum , 1998 ) is a rich lexical database   of word senses connected via the IS - A relation   with example sentences for sense usages , making   it a natural resource for probing . A 1 - hop IS - A   relation is illustrated in Figure 1 . By transitivity ,   ann - hop IS - A relation is formed from a chain of   nparent - child IS - A links . We focus on noun pairs   in this work as nouns make up 70 % of all senses in   WordNet , and the path lengths for nouns are often95longer than for verbs . We propose a path - based   sampling method to generate pairs from WordNet ,   as follows :   1 . LetL={s|s‚àà Sand hypo(s ) = ‚àÖ}denote   all leaf senses from WordNet , where Srep-   resents the set of all senses in WordNet , and   hypo(s ) denotes the set of hyponym ( children )   senses of sense s.   2 . For IS - A , sample a leaf sense Nuniformly   at random from L , connect Nto the root R ,   which gives a path p(N‚ÜíR ) . For not IS-   A , similarly sample two leaf senses N , N   randomly from Land obtain two paths p   ( N‚ÜíR ) andp(N‚ÜíR ) .   3 . For IS - A , randomly sample three senses   A , B , C frompand ensure that example sen-   tences exist for senses A , B , C . This results   in the 3 - tuple ( A , B , C ) and three positive ex-   amples ( A , B),(B , C),(A , C ) . For not IS - A ,   randomly sample AandBfrom pandp   respectively , ensuring that example sentences   exist for senses AandB. IfAis not on the   path of B‚ÜíRand vice versa , then we ob-   tain a negative example ( A , B ) ; else return   to step 2 .   4 . Repeat step 2 and 3 to sample more positive   and negative examples , until the desired num-   ber of examples is reached .   In our probing dataset , We have 1,665 3 - tuples   resulting in 4,995 positive examples , as well as   4,995 negative examples , where each example is a   pair of senses .   3.2 Probing Method   Since our goal is to determine what BERT as a pre-   trained language model knows about transitivity ,   we use a simple nearest neighbor ( 1 - nn ) classifier   without further fine - tuning of BERT ‚Äôs parameters .   We also adopt a 1 - nn classifier instead of a more   complex classifier so that we are measuring what   BERT knows and not what is learned by a subse-   quent complex classification model .   Our1 - nn probing classifier works by finding the   closest example in the training set for a test ex-   ample , and using the closest training example ‚Äôs   label as the prediction . Euclidean distance is used   as the distance metric for our 1 - nn probing classi-   fier . We represent each example , which is a pairof senses , by the concatenation of the contextual-   ized embeddings of the pair . For a pair of target   words ( w , w ) and their respective contextualized   embeddings ( o , o),r(o , o)denotes the relation   embedding of the pair :   r(o , o ) = [ o;o ] ( 1 )   Letrandrdenote two examples , and let mdenote   the dimension of the relation embeddings . The   Euclidean metric d(r , r)is computed as follows :   d(r , r ) = vuutX(r‚àír ) ( 2 )   3.3 Evaluation   Model and Hyperparameters For our BERT   model , we use the basic bert - base - uncased model ,   which has 12 layers with a hidden dimension of   768 . For 1 - nn , we adopt the scikit - learn ( Pedregosa   et al . , 2011 ) KNeighborsClassifier implementation .   Training and Test Data for Probing Classifier   Following similar sizes of other probing datasets   ( Vuli ¬¥ c et al . , 2020 ; Tenney et al . , 2019b ) , we set   aside a test set consisting of 1,998 positive ( IS - A )   examples ( generated from 666 3 - tuples ) and 1,998   negative ( not IS - A ) examples . We split the remain-   ing examples into 3 equal training sets , each con-   sisting of 999 positive examples ( generated from   333 3 - tuples ) and 999 negative examples . We re-   port the average score over the 3 runs .   For the transitive examples   [ ( A , B),(B , C),(A , C)]in the test set , the   average numbers of hops for ( A , B)and(B , C )   are 1.5 and 2.1 respectively . This difference is due   to the fact that the senses with at least an example   sentence are not evenly distributed along a path   for nouns in WordNet . On average , only 46 % of   senses on a sampled path have example sentences ,   out of which 72 % of the senses in the bottom   half ( i.e. , the half closer to the leaves ) of the path   are associated with example sentences , whereas   only 17 % of the top half have example sentences .   Therefore , when a sense Cis sampled from the top   half of the path , it is likely to be further away from   sense B.   Evaluation Metric We adopt accuracy as our eval-   uation metric , which measures the percentage of   test examples correctly predicted by the probing   classifier . All accuracy scores are computed using   the scikit - learn package.96PairsIS - A IS - A and not IS - A   # examples Acc . # examples Acc .   All 1998 65.2 ¬±2.8 3996 72.6 ¬±0.9   1 - hop 702 65.8 ¬±2.4 1404 72.3 ¬±0.9   2 - hop 560 64.4 ¬±3.5 1120 73.2 ¬±1.8   3 - hop 377 68.1 ¬±3.7 754 72.4 ¬±0.8   4 - hop 212 65.6 ¬±2.4 424 74.1 ¬±1.1   5 - hop 67 60.7 ¬±4.6 134 71.6 ¬±1.2   6 - hop 40 58.3 ¬±8.2 80 70.0 ¬±4.7   p(AB ) p(BC ) p(AC ) p(AC|   AB , BC )   63.1 ( 1.9 ) 66.3 ( 3.9 ) 66.2 ( 2.9 ) 82.4 ( 3.1 )   4 Experimental Results   4.1 Results Grouped by Number of Hops   The accuracy scores for the test set are shown in   Table 1 . The overall accuracy score for all pairs of   both IS - A and not IS - A classes is 72.6 % , suggest-   ing that BERT correctly predicts IS - A relations to   some extent . We also provide a breakdown of the   accuracy scores according to different number of   IS - A hops . The scores indicate that BERT predicts   IS - A relations with higher accuracy for smaller   number of hops ( 1‚Äì4 ) than for larger number of   hops ( 5‚Äì6 ) , although the prediction accuracy does   not drop by a large amount when the number of   hops increases , and the accuracy does not vary too   much within 1‚Äì4 hops .   4.2 Prediction Ability for Transitivity   We quantify BERT ‚Äôs prediction ability for transitiv-   ity by measuring how often BERT makes logically   consistent predictions for IS - A relations . Specif-   ically , suppose word senses ( A , B , C ) form the   following transitive IS - A relations : Ais - aBis - a   C. We measure how often BERT correctly predicts   the IS - A relation ( A , C)given that it correctly pre-   dicts ( A , B)and(B , C ) . Table 2 shows the ac-   curacy scores for the 666 transitive 3 - tuples . In   the table , p(AB)denotes the percentage of cor - rectly predicted ( A , B)in the 666 ( A , B)pairs .   Similar definitions apply to p(BC)andp(AC ) .   p(AC|AB , BC ) denotes the percentage of cor-   rectly predicted ( A , C ) , given that ( A , B)and   ( B , C)are correctly predicted . The conditional   probability in Table 2 indicates that when BERT   predicts that Ais - aBandBis - aC , it correctly   predicts that Ais - aC82.4 % of the time . That   Ais - aCis not always predicted correctly ( given   that BERT correctly predicts Ais - aBandBis - a   C ) suggests that BERT lacks the ability to make   logically consistent predictions .   5 Conclusion   In this paper , we have investigated how much   BERT agrees with the transitivity constraint of the   IS - A relation , via a minimalist probing setting . Our   findings indicate that although BERT can predict   IS - A relations to some extent , it does not always   make logically consistent predictions . Allowing   BERT and more generally neural network models   to enforce the transitivity constraint of the IS - A   relation would be a worthy future research goal .   Besides the IS - A relation , there are other transitiv-   ity relations like after , before , larger than , smaller   than , etc . It would also be interesting to investi-   gate to what extent BERT also enforces or fails to   enforce these other transitivity relations in future   work .   Acknowledgments   The computational work for this article was   partially performed on resources of the Na-   tional Supercomputing Centre , Singapore   ( https://www.nscc.sg ) . We thank the anonymous97reviewers for their helpful comments , and Wei Lu   for his constructive suggestions and feedback on   this work .   References9899   Cheng Yu ChuangYi YangDepartment of Mathematics and Economics , Department of Information Systems and Operations Management ,   Hong Kong University of Science and Technology   cychuangab@connect.ust.hk , imyiyang@ust.hk   Abstract   1 Introduction   Pre - trained language models ( PLM ) have achieved   superior performance on many NLP tasks ( Devlin   et al . , 2018 ; Liu et al . , 2019 ; Radford et al . , 2019 ) .   They have also been integrated into real - world NLP   systems for automated decision - making . Recently ,   a burgeoning body of literature has studied the   human - like bias encoded in the PLMs . For ex-   ample , in the mask token prediction task , BERT   fill - in the [ MASK ] in the sentence ‚Äú He / she works   as a [ MASK ] ‚Äù with ‚Äú doctor / nurse ‚Äù , reflecting gen-   der stereotype biased associations ( Garimella et al . ,   2021 ; May et al . , 2019 ) . Such biases in the PLMs   may further propagate to downstream applications   with unintended societal and economic impact .   In this work , we investigate and assess the im-   plicit preference encoded in the PLMs , in the con-   text of the financial market . We examine if the   PLMs prefer one company over the other compa-   nies . We also examine if such implicit preference   in individual stocks also manifests at the industry   sector level . Our core idea is based on the assump-   tion that an NLP system designed to be widely   applicable should ideally produce scores that are   independent of the identities of name entities men-   tioned in the text ( Prabhakaran et al . , 2019 ) .   Table 1 illustrates the potential stock market im-   plicit preferences in the BERT ( Devlin et al . , 2018 )   and its finance - domain specific variation FinBERT   ( Yang et al . , 2020 ) . Clearly , we see a favor of Tesla   over Ford in both PLMs . This implicit association   may be rooted in the training data : While BERT   is trained on fairly neutral corpora , FinBERT is   trained on financial communication corpora , in-   cluding earnings conference calls and analyst re-   ports . If a company ‚Äôs name is often mentioned in   negative contexts ( such as losses , disruptions ) , a   trained model might inadvertently associate nega-   tivity to that name , resulting in biased predictions   on sentences with that name .   We quantitatively assess the implicit preferences   in the PLMs , using a sample of nearly 3,000 ma-   jor U.S. market stocks . Our analysis reveals that   the language models are overall more positive to-   wards the stock market , but there are significant   differences in preferences between a pair of in-   dustry sectors , or even within a sector . Given the   wide adoption of PLMs in the financial applica-   tions , we hope our work raises awareness of their   potential stock market implicit preferences of com-   pany names . Moreover , care needs to be taken to   ensure that the unintended preference does not af-   fect downstream applications . Awareness of such   matters can help practitioners to build more robust100and accountable financial NLP systems .   2 Background   Humans have ( irrational ) preferences in the   stock markets . Humans are irrational ( Becker ,   1962 ) . Human decision - makers are often in-   fluenced by emotion , biases , and cognitive er-   rors . Human ( irrational ) preferences in the stock   markets are well documented in behavioral fi-   nance / economics literature . For example , the   home - bias refers to investors ‚Äô strong preference for   domestic stocks or concentrated exposure to their   employer ‚Äôs stock ( French and Poterba , 1991 ; Tesar   and Werner , 1995 ) . Bhattacharya et al . ( 2018 ) find   that the Mandarin - speaking individual investors   submit disproportionately more limit orders at 8   than at 4 , because of the belief that the number 8   is lucky and the number 4 is unlucky ‚Äî and those   superstitious investors lose money .   Why is the implicit stock market preference   in PLMs an issue ? Automated NLP technique   for financial decision making is expected to mini-   mize human irrationality . However , PLMs that are   trained with a human - written corpus may inherit   such human preferences ( we do find it is the case ) .   This resembles the allocational harms that ‚Äú arise   when an automated system allocates resources or   opportunities unfairly to different social groups ‚Äù   ( Blodgett et al . , 2020 ) . In the financial markets , the   disproportional allocation of resources , i.e. , capital ,   also has unintended consequences . First , the strong   favoritism to a stock can attract more investors to in-   vest in the stock and increase the company ‚Äôs capital   value , which helps the company ‚Äôs growth and de-   velopment ( Beck and Levine , 2002 ) . This implies   that less favored companies may struggle with cap-   ital access . Second , the disproportional resource   allocation may result in high trading activities and   increased volatility of certain stocks , which creates   uncertainty and instability in the market .   3 Data and PLMs   Data : We choose Russell 3000 constituent firms   as our target companies because of their impor-   tance and tractability . This index includes the 3,000   largest publicly held companies incorporated in the   United States as measured by total market cap-   italization , and it represents approximately 98 %   of the U.S. public equity market . We also obtain   an industry sector label for each firm in our sam-   ple , based on the Global Industry ClassificationStandard ( GICS ) . GICS is a widely used industry   classification for market analysis , and it consists   of 11 sectors . For example , company Apple ( NAS-   DAQ : AAPL ) is in the Information Technology sec-   tor , while the company Walmart ( NASDAQ : WMT )   is in the Consumer Staples sector . The GICS sector   allows us to examine the implicit preference at the   industry sector level . The total number of stocks   in our sample is 2,653 . The detailed breakdown of   GICS sectors in our sample is presented in Table 2 .   GICS Sector Number of stocks   Financials 495   Industrials 391   Health Care 379   Information Technology 351   Consumer Discretionary 310   Real Estate 162   Energy 144   Materials 136   Communication Services 110   Consumer Staples 104   Utilities 71   PLM : We choose two BERT - based pre - trained lan-   guage models in our analysis : BERT and FinBERT .   BERT is one of the most widely used PLMs that   is trained on Wikipedia and BookCorpus ( Devlin   et al . , 2018 ) . In addition to BERT , we choose Fin-   BERT , which is a domain - specific BERT model   that is pre - trained on financial communications text ,   including annual reports , analyst reports , and earn-   ings conference call transcripts ( Yang et al . , 2020 ) .   The vocabulary of FinBERT is different from the   BERT model as it contains finance - domain specific   terms , including company names . It has shown   to outperform the general - domain BERT ( Huang   et al . , 2020 ) on financial downstream tasks . We   load both base - uncased BERT and FinBERT from   thetransformers library ( Wolf et al . , 2020 ) .   4 Assessing Implicit Preference in   Masked Token Prediction   Since BERT and FinBERT use a masked language   modeling objective , we directly probe the model   using the masked token prediction task , using cloze-   style prompts . Prior work also uses this approach   to assess the social biases ( May et al . , 2019 ) , or   the knowledge learned by PLMs ( Petroni et al . ,   2019 ) . For each firm , we create a simple tem-101plate containing the attribute word for which we   want to measure the preference ( e.g. buy or sell )   and the company name as the target word ( e.g. ,   Microsoft ) . We then mask the attribute words   and target words accordingly , to get the condi-   tional probability of producing the buy orsell   token . Specifically , for firm i , we use the tem-   plate sentence ‚Äú We should [ MASK ] the { name }   stock ‚Äù and query the probability of masked token :   P = P([MASK ] = buy|name = i ) , and   P = P([MASK ] = sell|name = i ) . We then   normalize the two conditional probabilities .   4.1 Implicit preferences in the market   Our first evaluation simply assesses if the PLM is   lean more towards buy or sell across companies .   We obtain the normalized conditional probability   P for each firm i , and we plot the boxplot of   P in Figure 1 . An ideal model would have   a conditional probability close to 0.5 for all firms .   Clearly , it is not the case in the BERT and FinBERT .   Figure 1 shows that the mean value of P is sig-   nificantly different from 0.5 . FinBERT ‚Äôs average   buy probability is even higher than 0.9 , indicating   as a stronger preference for predicting buy token   over the sell token . This tendency could be ex-   plained by two reasons . First , prior literature shows   that there is a universal positive bias in the human   language ( Dodds et al . , 2015 ) . Second , compared   to BERT which is trained on a fairly neutral corpus ,   FinBERT is trained on financial communication   corpora such as analyst reports . Therefore , the   higher buy probability may imply that the overall   market sentiment over the years is positive.4.2 Implicit preferences between industries   It may not be surprising that the PLMs are overall   positive . Therefore , we examine if certain industry   sectors are more favored than the other industries .   We use a univariate regression analysis . For firm   i , we use the P , the probability of predicting   the masked token ‚Äú buy ‚Äù , as the response variable ,   and we use the firm ‚Äôs sector Xas the dummy in-   dependent variable , i.e. , Xis 1 if stock ibelong   to the sector j , otherwise 0 . Since we have a total   of 11 sectors , we set up 11 univariate regression   models and examine the relationship between the   probability of ‚Äú buy ‚Äù and the dummy industry sec-   tor variable . The univariate regression is specified   as follows , and œµis the error term .   For sector j : y = Œ≤x+œµ ( 1 )   The univariate regression results are presented   in Table 3 . We can see that both models have pref-   erences of one sector over the other sectors . For   example , both BERT and FinBERT find companies   in the Financial sectors less preferred in terms of   predicting the buy token , as seen from the negative   Œ≤value and significant p - values . From Table 2 , we   can see that the most preferred sectors in BERT are   Materials , Consumer Staples , and Utilities ; while   for FinBERT , the most preferred sector is Materi-   als and Industrials . Moreover , we find that , while   FinBERT has a stronger buy preference across all   companies than BERT , it has less preference when   comparing to the industry sector level , as we see   there is a fewer number of sectors with significant   p - values . In other words , FinBERT has positive   preference across most of sectors , while BERT has   positive preference only in certain sectors .   We further compare the implicit preference be-   tween a pair of sectors . To do so , we conduct   Cohen ‚Äôs dtest and calculate the effect size of the   distributions of pair of industry Aand industry B.   Specifically , Cohen ‚Äôs ddetermines the mean dif-   ference between industry A and B in terms of the   probability P. A positive value indicates that   the PLM has a stronger buy preference for indus-   tryAthan for industry B. We plot the heatmap   between pairs of industries in Figure 2 . The figure   shows that both models have an implicit preference   between sectors . Consistently , Financial is the least   preferred industry sector.102GICS Sector BERT FinBERT   Financials -0.88 * * * -0.83 * * *   Industrials 0.43 0.40 * * *   Health Care 0.00 * * * 0.10   Information Technology 0.12 * * * 0.7   Consumer Discretionary 0.17 * * -0.94   Real Estate -1.88 * * * 0.07 *   Energy 0.15 0.72 *   Materials 2.22 * * * 1.09 * *   Communication Services -0.07 -0.98   Consumer Staples 0.73 * * -0.30   Utilities 0.61 * * * 1.345 Assessing Implicit Preferences within   an Industry Sector   The masked token prediction is only one way of   probing the PLMs . Recent NLP literature has pro-   posed the word association tests to measure the   human - like biases in the static word embedding   ( Bolukbasi et al . , 2016 ; Caliskan et al . , 2017 ) or   contextualized word embedding ( May et al . , 2019 ) .   The word association test in the contextualized   embedding model is called Sentence Encoder As-   sociation Test ( SEAT ) . Essentially , SEAT evalu-   ates whether the contextualized representations for   words from an attribute word set tend to be more   closely associated with the contextualized represen-   tations for words from a target word set . Templates   such as ‚Äú this is a [ word ] ‚Äù are used to obtain the   word contextualized representations .   In this work , we create a template sentence   ‚Äú { name } is a stock ‚Äù where { name } is a stock ‚Äôs com-   pany name , and we obtain the [ CLS ] embedding   as its embedding . For preference words buy and   sell , we create a template ‚Äú We should buy / sell a   stock " , and we obtain the [ CLS ] embedding as its   embedding . Let sim andsim be the co-   sine similarity between the embedding of company   i ‚Äôs name and the embedding of buy / sell . Given   an industry sector Scontaining a set of stocks ,   we calculate the SEAT association effect - size as :   d= _ . An ef-   fect size with absolute value closer to 0 indicates   lower implicit preference . We present the individ-   ual sector ‚Äôs SEAT score in Table 4 , which leads to   the following observations . First , we see consistent   implicit preferences within individual sectors . For   example , both BERT and FinBERT regard Finan-   cials as the least preferred sector ( negative effect   size ) . Since this is a within - in sector study , it im-   plies that some Financial stocks are preferred over   the other Financial stocks . Second , we see that the   majority of the sectors have a positive effect size ,   indicating that both PLMs exhibit a positive bias   within the sector .   6 Conclusion   In this paper , we study the implicit stock market   preference in PLMs . Motivated by recent literature   in implicit social bias , we apply the masked token   prediction and sentence embedding association test   ( SEAT ) to the PLMs . We find that there is a con-   sistent implicit preference of the stock market in   the PLMs , and the preferences exist at the whole-103GICS Sector BERT FinBERT   Financials -0.65 -0.15   Industrials 0.19 0.34   Health Care 0.06 -0.03   Information Technology 0.44 0.06   Consumer Discretionary 0.25 0.26   Real Estate 0.00 0.29   Energy -0.56 0.44   Materials -0.15 0.15   Communication Services 0.10 -0.06   Consumer Staples -0.08 0.18   Utilities -0.19 0.12   market , between - industry , and within - industry level .   Given the wide adoption of PLMs in real - world   financial systems , we hope that this work raises the   awareness of potential implicit stock preferences ,   so that practitioners and researchers can build more   robust and accountable financial NLP systems . Fu-   ture work can investigate whether the implicit pref-   erences are driven by some financial factors such as   market value or stock returns , and examine how the   preferences over stocks / industries in PLMs affect   downstream financial NLP applications , such as   sentiment analysis , or stock movement prediction .   Acknowledgement   This work was supported by HKUST - Kaisa Group   Seed Project on Fintech ‚Äú HKJRI3A-057 ‚Äù .   References104105   Amanul Haque andVaibhav Garg andHui Guo andMunindar P. Singh   Department of Computer Science   North Carolina State University   Raleigh , NC 27695 , USA   { ahaque2 , vgarg3 , hguo5 , mpsingh}@ncsu.edu   Abstract   1 Introduction   Online user reviews contain a cornucopia of in-   formation on user expectations about a product .   Users often express their opinions on a product   by comparing it against competitors . Understand-   ing preferences in natural language is crucial in   capturing user ‚Äôs opinions and expectations . Pre-   vious studies show that app reviews include rich   insights about user expectations and problems of   mobile apps that are valuable for app developers   ( Palomba et al . , 2015 ; Maalej and Nabil , 2015 ; Guo   and Singh , 2020 ) . We found that app reviews often   include comparative sentences , from which we can   determine a reviewer ‚Äôs preferences .   Identifying the preferred entity from an app re-   view involves ( 1 ) Comparative Sentence Identifica-   tion(CSI ) ( Jindal and Liu , 2006 ) , i.e. , identifying   sentences that contain a comparison , and ( 2 ) Com-   parative Preference Classification ( CPC ) ( Ganap-   athibhotla and Liu , 2008 ; Panchenko et al . , 2019 ) ,   i.e. , identifying the preferred entity in a compara-   tive sentence . We focus on the second task .   Prior work on CPC focuses on explicit compar-   isons , where all compared entities are explicitly   mentioned . Extracting comparative sentences by   matching keywords or patterns ( Jindal and Liu ,   2006 ; Li et al . , 2017 ; Feldman et al . , 2007 ) over-   looks indirect comparisons which lack comparative   quantifiers and adjectives .   Staab and Hahn ( 1997 ) identify omitted comple-   ment as a comparative sentence type that has been   overlooked by prior research . An omitted comple-   ment refers to one of the entities under comparison   that is omitted but can be inferred based on the   context . We have found that comparative sentences   in user - generated text such as reviews sometimes   imply instead of explicitly mentioning the target   entity being reviewed ( e.g. , Sin Table 1 ) . Compar-   isons in reviews often lack comparative linguistic   cues , such as comparative quantifiers , adjectives , or   structures ( i.e. , indirect , e.g. , Sin Table 1 ) . Such   sentences are comparative by virtue of expressing   a preference and are common in reviews but have   been understudied by prior research .   We present Pixie ( Preference in Implicit and Ex-   plicit Comparisons ) , a dataset for preference clas-   sification , created from online user reviews . As   shown in Table 1 , Pixie includes indirect compar-   isons ( i.e. , sentences lacking comparative linguistic   cues , e.g. , S ) and implicit comparisons ( omitting   compliments , i.e. , mentioning only one entity being   compared , e.g. , S ) in addition to direct compar-   isons ( comparing entities with a direct comparative   structure , e.g. , S ) and explicit comparisons ( men-   tioning both entities being compared , e.g. , S ) .   We experiment with traditional machine learn-   ing methods and transformer - based models on   Pixie . We use segment embeddings to demar-106cate the compared entities before fine - tuning the   transformer - based models . We also compare our   results with ED - GAT ( Ma et al . , 2020 ) , a state-   of - the - art model for preference classification . We   find that transformer - based pretrained language   models , fine - tuned on Pixie , achieve a higher F1-   score ( 83.34 % ) than the state - of - the - art ( F1 - score   73.99 % ) or traditional machine learning models   ( F1 - score 71.86 % ) trained on Pixie . Further er-   ror analysis of misclassifications reveals substan-   tial differences between ED - GAT and transformer-   based pretrained language models ‚Äô performance .   Current research on preference classification is   lacking and far from practical use . Real world com-   parisons can present characteristics that complicate   the task , such as indirect comparisons , implicit   comparisons , and ambiguous statements . The low   F1 - score of the existing state - of - the - art and notice-   able differences in misclassifications across differ-   ent models call for a more thorough research effort   on preference classification in text .   2 Related work   Comparative sentence structures have been a sub-   ject of syntactic and semantic theories ( Bresnan ,   1973 ; Stechow , 1984 ; van Rooij , 2011 ) . Early stud-   ies in computational linguistics include syntactic   and semantic handling of comparative construc-   tions ( Rayner and Banks , 1988 , 1990 ) , comparative   structures in question answering ( Ballard , 1988 ) ,   using quantifiers to identify comparisons ( Fried-   man , 1989 ) , and semantic interpretation of compar-   atives ( Staab and Hahn , 1997 ) .   Jindal and Liu ( 2006 ) present a binary classi-   fication dataset containing comparative and non-   comparative sentences . They present a classifier   based on Class Sequential Rules ( CSR ) and lever-   age comparative keywords to identify comparative   sentences . Ganapathibhotla and Liu ( 2008 ) extend   this work by annotating comparative sentences with   the preferred entity .   Kessler and Kuhn ( 2014 ) annotate comparative   sentences by identifying comparison predicates , en-   tities being compared , aspect of comparison , and   comparison type ( gradable or non - gradable ) . How-   ever , they focus on reviews of only one product   type ( digital cameras ) to create their dataset . Hence ,   their dataset lacks diversity in topics .   Panchenko et al . ( 2019 ) create CompSent-19 ,   a cross - domain dataset for comparative argument   mining . They propose a gradient boosting modelbased on pretrained sentence embeddings to iden-   tify the preferred entity . Ma et al . ( 2020 ) propose   a model called Entity - aware Dependency - based   Deep Graph Attention Network ( ED - GAT ) that   consists of a multihop graph attention network with   dependency relations to identify the preferred en-   tity . The ED - GAT model achieves a micro F1 - score   of 87.43 % on the CompSent-19 dataset .   Previous work on preference classification has   overlooked implicit and indirect comparisons com-   mon in user - generated text such as app reviews .   Further , existing datasets are either too small with   a few comparative sentences or have a skewed dis-   tribution . For example , Ganapathibhotla and Liu ‚Äôs   dataset contains only 837 comparative sentences ,   84 % of which have the first mentioned entity in the   text as preferred . Only 15 % of Kessler and Kuhn ‚Äôs   dataset constitutes comparative sentences . Only   27 % of the sentences in CompSent-19 ( Panchenko   et al . , 2019 ) contain a preference , 70 % of which   prefer the first mentioned entity in the sentence .   Further , existing datasets consider the order of   the appearance of compared entities in a sentence   to annotate the preferred entity . For instance , anno-   tations for CompSent-19 ( Panchenko et al . , 2019 )   and Ganapathibhotla and Liu ‚Äôs dataset are both de-   termined based on the order of appearance of the   entity in a sentence ( i.e. , is the first appearing entity   in the sentence preferred or the second ) .   3 Method   We introduce the essential concepts below .   Comparative sentence : A sentence that contains   information on similarity , dissimilarity , or   preference between two entities .   Pixie includes ( 1 ) comparative sentences that   lack comparative quantifiers , adjectives , or key-   words , i.e. , indirect comparisons , ( 2 ) implicit com-   parisons where only one of the compared entities   is mentioned , and ( 3 ) explicit comparisons which   mention both ( including pronominal references ) .   Preferred entity : an entity that is chosen over an-   other based on a stated or implied preference .   A preferred entity can be the app ( e.g. ,   Sin Table 2 ) , app ( e.g. , Sin Table 2 ) ,   or ( i.e. , ambiguous or no preference , e.g. ,   Sin Table 2 or where non - gradable comparatives   ( such as like , as . . . , and similar to ) link the entities ,   e.g. ,Sin Table 2).107   3.1 Dataset   We selected 179 popular apps on Apple App Store   and collected their reviews . After some prelimi-   nary investigation , we excluded widely mentioned   brand names such as Google , Microsoft , and Face-   book , because they often appear in broader contexts   than as a product . We removed app names synony-   mous with or formed of common words , such as   Box ( cloud storage ) and Line ( communication app )   for higher precision in extracting comparative sen-   tences . We were left with 141 apps , which we   manually grouped into 23 genres , including bank-   ing , airline , and communication . Apps in the same   genre are direct competitors . For example , airline   apps include Delta , American , and United .   We extracted sentences that mention a com-   petitor from each review and labeled each ex-   tracted sentence for comparison and preferred en-   tity . When identifying mentions , we included com-   mon aliases or abbreviations on our name list , e.g. ,   Insta for Instagram , BAfor Bank of America , and   AAfor American Airlines to improve recall . Fo-   cusing on mentions of competitors ensures that   Pixie includes indirect comparisons because such   sentences are more likely to contain comparisons .   The dataset was annotated in three phases . In   Phase 1 , the authors annotated a sample dataset of   300 sentences based on an initial set of definitions   and resolved any disagreements via discussions .   We repeated this process for three iterations and   produced annotation instructions for Phase 2 . In   Phase 2 , each author annotated an equal number   of sentences , and the disagreements were resolved   by the first author , producing 4,793 annotated sen-   tences . The interrater agreement ( Krippendorff   alpha ) was 0.74 and 0.82 between the two anno-   tators for comparison and preferred entity , respec-   tively . Phase 3 involved crowdsourcing with 42   annotators , students in Natural Language Process-   ing ( NLP ) course , each annotating around 400 sen-   tences . 5,559 data points were labeled in Phase 3   with an interrater agreement ( Krippendorff alpha )   between the three annotators of 0.51 and 0.74 forcomparison and preferred entity , respectively . We   obtained the Institutional Review Board ( IRB ) ap-   proval for this task .   Once we removed duplicate and noncomparative   sentences , we were left with 8,890 comparative sen-   tences annotated for comparison type ( or ) and preferred entity ( , ,   or ) . Table 3 shows the distribution of labels   for each class in Pixie .   Comparison Type   Preferred Entity Implicit Explicit Total   C 1,910 2,097 4,007   O 2,199 1,069 3,268   N 758 857 1,615   Total 4,867 4,023 8,890   To ensure that the dataset can be used to train   a general - purpose preference classification model ,   wemask app mentions in each sentence . With   no masking , the model may learn to differentiate   between classes based on what users prefer more   ( app A or app B ) in our dataset . Masking app men-   tions ensures that the model learns comparative   and preference revealing linguistic structures and   semantics instead of simply learning to differenti-   ate between preferred entities in an exhaustive list   of compared entities . We defined two tags for mask-   ing , current_app for the apps being reviewed and   other_app for the competitor apps . App mentions   are identified using the competitor app list for apps   referred to by name , and pronoun references are   substituted manually . Treating pronoun references   as an explicit reference to app mentions ensures   consistent based on our definitions , i.e. , all explicit   comparisons have two mentioned entities being   compared , while all implicit comparisons have one .   A portion of the dataset , ‚âà2,100 ( ‚àº23.62 % ) sen-   tences , had pronoun references that were resolved .   Table 4 shows sentences masked for app mentions .   For a quick sanity check , whether Pixie contains   indirect comparative sentences , we examine how   many of the sentences in Pixie contain a compara-   tive word . For this , we combine the list of opinion   words from ( Hu and Liu , 2004 ) and the list of com-   parative cue words from ( Panchenko et al . , 2019 ) .   Only 3,781 sentences ( 42.5 % of Pixie ) contain a   comparative or opinion word showing that most of   the sentences in Pixie lack comparative cues ( i.e. ,108are indirect comparisons ) .   Unlike prior datasets on preference classification   ( Ganapathibhotla and Liu , 2008 ; Panchenko et al . ,   2019 ) , Pixie does not consider the order of appear-   ance of compared entities for annotations . Pixie   also offers a more balanced dataset than the existing   ones for the task . For explicit comparisons ( when   both entities are present ) , 1909 sentences ( 47.45 % )   prefer entity that appears first , 1257 ( 31.25 % ) sen-   tences prefer entity that appears later , and 857 sen-   tences ( 21.30 % ) reveal no or mixed preference .   Implicit comparisons mention only one entity so   the order of appearance is irrelevant .   Pixie is publicly availableand contains original   and masked sentences .   3.2 Experiments   Among traditional machine learning approaches ,   we experiment with AdaBoost ( Hastie et al . , 2009 ) ,   Random Forest ( Breiman , 2001 ) and Support Vec-   tor Machine ( SVM ) ( Chang and Lin , 2011 ) . We use   SBERT ( SentenceBERT ) ( Reimers and Gurevych ,   2019 ) to obtain sentence embeddings for each   masked sentence .   For transformer - based language models , we fine-   tune variations of BERT ( Bidirectional Encoder   Representations from Transformers ) ( Devlin et al . ,   2019 ) and XLNet ( Yang et al . , 2019 ) . We experi-   ment with BERT ( Devlin et al . , 2019 ) , ALBERT   ( A Lite BERT ) ( Lan et al . , 2019 ) , and DeBERTa   ( Decoding - enhanced BERT with disentangled at-   tention ) ( He et al . , 2020 ) . We fine - tune each model   for 20 epochs using AdamW Optimizer with a   learning rate of 5e-5 and a weight decay of 0.01 .   We use the train - test - validation split of 60 - 20 - 20 .   We use segment embeddings to improve the per-   formance of the transformer - based models . We   assign different segment token ids to the competi-   tor app ( other_app ) and the rest of the sentence to   separate the entities being compared . We fine - tune   pretrained models with segment embeddings along   with token embeddings and attention masks .   To compare our results with ED - GAT , we con-   vert the sentences in Pixie to follow the CompSent-   19 format . Specifically , we add a token ( [ THIS ] )   for the current app in the front of each implicit sen-   tence and map the labels C andO   toB andW , as applicable . N la-   bels stay the same . We implemented ED - GAT with   BERT embeddings and used eight GAT layers . Weuse the Hugging Face ( Wolf et al . , 2020 ) library for   all transformer - based experiments .   To test the quality of Pixie , we run some cross-   dataset experiments as well . We train a DeBERTa   model on Pixie and test on CompSent-19 and vice-   versa . Since the CompSent-19 dataset is highly   skewed , we balanced both datasets to have the same   train and test data split across all three classes via   random oversampling with replacement . We keep   all other model parameters and configurations the   same and leverage the same number of samples for   training and testing .   4 Results   Table 5 contains results for models trained and   tested on Pixie . SVM achieves the highest   weighted F1 - score of 71.86 % ( among the tra-   ditional approaches ) , and DeBERTa ( F1 - score   83.34 % ) , among transformer - based models .   Segment embeddings enhanced BERT and XL-   Net model ‚Äôs performance in terms of weighted av-   erage F1 - scores , but a slight decline for DeBERTa ‚Äôs   and ALBERT ‚Äôs performance .   The and classes consistently   achieve the lowest and the highest F1 - scores , re-   spectively , for all models . The class was also   the most ambiguous class to annotate manually . Re-   call for the class is lower than precision for   all models except ED - GAT . All transformer - based   models achieve a higher recall than precision for   the class except for ALBERT ( without   segment embeddings ) and ED - GAT .   ED - GAT ( Ma et al . , 2020 ) trained on Pixie   achieves a weighted average F1 - score of 73.99 % ,   with the highest F1 - score ( 80.57 % ) for the- class and lowest ( 51.54 % ) for .   Upon further analysis , we found that most of the   incorrect classifications in transformer - based mod-   els are for the class ( 71.64 % ) , whereas , for   ED - GAT , only 8.77 % of the misclassified sentences   belong to the class . ED - GAT yielded most   misclassifications for the class ( 55.27 %   of misclassified instances ) while only 14.93 % of   misclassifications for the transformer - based models   belong to the class .   Table 6 shows the results for the cross - dataset   experiments . The weighted average F1 - score im-   proves by 4.08 % with plain vanilla fine - tuning and   6.30 % with segment embeddings when trained on   Pixie and tested on CompSent-19 . While the ac-   curacy improves by 5.11 % for plain vanilla fine-109   tuning and 6.28 % with segment embeddings . The   improvement primarily is in the recall , demonstrat-   ing that Pixie includes more diverse comparative   sentences than CompSent-19 .   5 Conclusion   Masking compared entities ensure that Pixie can   be used to train a general - purpose preference clas-   sification model . Additional analysis is needed to   claim the domain generality of our dataset ‚Äî that is ,   whether a model trained on Pixie can identify the   preferred entity in texts from other domains such as   scientific papers and news . Comparative sentences   in Pixie are limited to user - generated text and may   not generalize well over more formal texts .   Both BERT and XLNet show improvements with   segment embeddings , suggesting that the demarca-   tion of the other app helps the model identify the   preferred entity . The traditional machine learning   models perform worst and the transformer - based   pretrained models fine - tuned on Pixie achieve asubstantially better performance than the state - of-   the - art approaches for preference classification .   Identifying preferences in user reviews can   aid developers in understanding user expectations   about mobile apps . Users often express their likes   and dislikes about an app or feature by comparing   it with alternative apps and features . Understand-   ing user preferences can be particularly valuable   in enhancing the functionality as well as security   and privacy features of apps . A user ‚Äôs preferences   regarding apps would depend not only on how well   the app is constructed relative to its competitors   but also on how easily the app is used by end - users .   For example , security concerns may be signaled by   descriptions of steps to access sensitive financial or   medical data ( Guo and Singh , 2020 ) expressed in   association with comparisons . A follow - on direc-   tion is to extract and prioritize user expectations by   identifying the specific features of an app of great-   est influence on the indirect or direct comparisons   in a review.110Acknowledgments   This research is supported by the US Department   of Defense under the Science of Security Lablet   ( SoSL ) grant to NC State University . We thank the   ACL reviewers for their helpful comments .   References111112   George Tolkachev   University of Pennsylvania   georgeto@seas.upenn.eduStephen Mell   University of Pennsylvania   sm1@seas.upenn.edu   Steve Zdancewic   University of Pennsylvania   stevez@seas.upenn.eduOsbert Bastani   University of Pennsylvania   obastani@seas.upenn.edu   Abstract   1 Introduction   Semantic parsing is a promising technique for en-   abling natural language user interfaces ( Ge and   Mooney , 2005 ; Artzi and Zettlemoyer , 2013 ; Be-   rant et al . , 2013 ; Wang et al . , 2015 ) . However , a key   challenge facing semantic parsing is the richness of   human language , which can often encode concepts   ( e.g. , ‚Äú circle ‚Äù ) that do not exist in the underlying   system or are encoded using different language   ( e.g. , ‚Äú ball ‚Äù ) . Thus , human users can have trouble   providing complex compositional commands in the   form of natural language to such systems .   One approach to addressing this issue is to de-   velop increasingly powerful models for understand-   ing natural language ( Gardner et al . , 2018 ; Yin and   Neubig , 2018 ) . While there has been enormous   progress in this direction , there remains a wide gap   between what these models are capable of com-   pared to human understanding ( Lake and Baroni ,   2018 ) , manifesting in the fact that these models canfail in unexpected ways ( Ribeiro et al . , 2016 ) . This   gap can be particularly problematic for end users   who do not understand the limitations of machine   learning models , since it encourages the human   user to provide complex commands , but then per-   forms unreliably on such commands .   Thus , an important problem is to devise tech-   niques for explaining these models . Generally   speaking , a range of techniques have recently been   developed for explaining machine learning mod-   els . The Ô¨Årst technique is to use models that are   intrinsically explainable , such as linear regression   or decision trees . However , in the case of seman-   tic parsing , such models may achieve suboptimal   performance , and furthermore it is not clear that   the structure of these models would be useful to   end users . A second technique is to train a black-   box model , and then approximate it using an in-   terpretable model . Then , the interpretable model   can be shown to the human user to explain the   high - level decision - making process underlying the   blackbox model . However , this approach also suf-   fers from the fact that showing a decision tree or   regression model is likely not useful to an end user .   Instead , we consider an alternative form of   explanation called a counterfactual explana-   tion ( Wachter et al . , 2017 ) . These explanations   are designed to describe alternative outcomes to   the user . In particular , given a prediction for a spe-   ciÔ¨Åc input , they tell the user how they could have   minimally modiÔ¨Åed that input to achieve a different   outcome . As an example , suppose a bank is using   a machine learning model to help decide whether   to provide a loan to an individual ; if that individual   is denied the loan , then the bank can provide them   with a counterfactual explanation describing how   they could change their covariates ( e.g. , increase   their income ) to qualify for a loan .   We propose a novel algorithm for computing   counterfactual explanations for semantic parsers .   In particular , suppose that a user provides a com-113User command 1 : ‚Äú Go to the blue circle ‚Äù   User command 2 : ‚Äú Go to the top right ‚Äù   Our explanation : ‚Äú Go to the blue ball ‚Äù   mand in the form of a natural language utterance .   If the natural language interface fails to provide the   desired result , then our goal is to explain how the   user could have modiÔ¨Åed their utterance to achieve   the desired result . To this end , we have the human   additionally provide the desired result . Then , we   compute an alternative utterance that the semantic   parser correctly processes while being as similar   as possible to the original utterance . Intuitively ,   this explanation enables the user to modify their   language to reliably achieve their goals in future   interactions with the system .   We evaluate our approach on the BabyAI envi-   ronment ( Chevalier - Boisvert et al . , 2018 ) , where   the human can provide a virtual agent with com-   mands to achieve complex tasks such as ‚Äú pick up   the green ball and place it next to the blue box ‚Äù . We   perform two user studies , which demonstrate that   our approach both produces correct explanations   ( i.e. , match the user ‚Äôs desired intent ) , and that it   substantially improves the user ‚Äôs ability to provide   valid commands .   Example . In Figure 1 , we show an example of a   BabyAI task along with a user - provided utterance   commanding the agent to go to the blue ball . The   Ô¨Årst command corresponds to a valid program , but   can not be understood by the semantic parser due to   the use of the terminology ‚Äú circle ‚Äù instead of ‚Äú ball ‚Äù .   The second command uses the construct ‚Äú top right ‚Äù   that does not exist in the language . In both cases ,   the user provides a demonstration where the agent   navigates next to the blue ball , upon which our   approach generates the explanation shown .   Related work . There has been a great deal of re-   cent interest in providing explanations of black - box machine learning models , focusing on explain-   ing why the model makes an individual predic-   tion ( Ribeiro et al . , 2016 ; Lei et al . , 2016 ; Ribeiro   et al . , 2018 ; Alvarez - Melis and Jaakkola , 2018 ;   Liu et al . , 2018 ) , or achieving better understanding   of the limitations of models ( Wallace et al . , 2019 ;   Ribeiro et al . , 2020 ) . In contrast , our goal is to   explain how the input can be changed to achieve   a desired outcome , which is called a counterfac-   tual explanation ( Wachter et al . , 2017 ; Ustun et al . ,   2019 ) . There has been interest in improving the   performance of semantic parsers through interac-   tion ( Wang et al . , 2016 , 2017 ) ; our approach is   complementary to this line of work , since it aims   to make the system more transparent to the user .   There has also been work on leveraging natural lan-   guage descriptions to help generate counterfactual   explanations for image classiÔ¨Åers ( Hendricks et al . ,   2018 ) , but not tailored at counterfactual predictions   for natural language tasks ; speciÔ¨Åcally , while their   approach produces counterfactual explanations in   natural language , they are for image predictions   rather than text predictions .   For natural language processing tasks , a key chal-   lenge is that the input space is discrete ( e.g. , a nat-   ural language utterance ) ; for such settings , there   has been work on algorithms for searching over   combinatorial spaces of counterfactual explana-   tions ( Ross et al . , 2021b ; Wu et al . , 2021 ; Ross   et al . , 2021a ) . However , even for these approaches ,   the output space is typically small ( e.g. , a binary   sentiment label ) . In contrast , semantic parsing has   highly structured outputs ( i.e. , programs ) , requiring   signiÔ¨Åcantly different search procedures to Ô¨Ånd an   explanation that produces the correct output . To ad-   dress this challenge , we deÔ¨Åne a search space over   counterfactual explanations for semantic parsing   such that search is tractable .   2 Algorithm   Problem formulation . We consider the problem   of computing counterfactual explanations for a se-   mantic parsing model f : !. In particular ,   we assume the user provides a command in the   form of an utterance s2 , with the goal of ob-   taining some denotation y2Y. To achieve the   user ‚Äôs goal , the semantic parsing model produces   a program=f(s)2 , and then executes the   program to obtain denotation y = JK2Y , where   JK : !Y ( called the semantics of ) maps   programs to outputs.114In this context , our goal is to provide explana-   tions to the user to help them understand what ut-   terances can be correctly understood and executed   by the underlying system . In particular , we assume   the user has provided an utterance s , but the out-   put Jf(s)Kis not the one that they desired . Then ,   we ask the user to provide their desired output ,   after which we provide them with an alternative   utterancesthat is semantically similar to sbut   successfully achieves y. Formally :   DeÔ¨Ånition 2.1 . Given an utterance s2and a   desired output y2Y , the counterfactual explana-   tionforsandyis the sentence   s= arg mind(s;s)subj . to Jf(s)K = y ;   wheredis a semantic similarity metric and L   is the search space of possible explanations .   The goal is that examining sshould help the   user provide utterances that are more likely to be   correctly processed in future interactions .   Search space of explanations . A key challenge   in generating natural language expressions is how   to generate expressions that appear natural to the   human user . To ensure that our explanations are   natural , we restrict to sentences generated by a   context - free grammar ( CFG ) C. In particular , we   consider explanations in the form of sentences s2   L(C)(where is the vocabulary and L(C )   is the language generated by C ) . We restrict to   sentences with parse trees of bounded depth din   C ; we denote this subset by L(C ) . In addition ,   we assume sentences s2L(C)are included in   the dataset used to train the semantic parser fto   ensure it correctly parses these sentences .   Semantic similarity . Our goal is to compute a   sentences2L(C)that is semantically similar   to the user - provided utterance s. To capture this   notion of semantic similarity , we use a pretrained   language model x = g(s)that maps a given sen-   tencesto a vector embedding x2R. Then ,   we use cosine similarity in this embedding space   to measure semantic similarity . In particular , we   use the distance d(s;s ) = 1 sim(g(s);g(s ) ) ,   where sim ( x;x)is the cosine similarity .   Goal constraint . Finally , we want to ensure that   the provided explanation successfully evaluates to   the user ‚Äôs desired denotation y. For a given ut-   terances , we can check this constraint simply by   evaluatingy = Jf(s)Kand checking if y = y. Algorithm 1 Our algorithm for computing coun-   terfactual explanations for a semantic parser f.   procedure E ( s;y )   ( s;c ) ( ? ;  1 )   fors2L(C)do   ifJf(s)K = ythen   c sim(g(s);g(s ) )   ifc > cthens;c s;cend if   end if   end for   returns   end procedure   Overall algorithm . Given user - provided utterance   sand desired denotation y , the counterfactual   explanation problem is equivalent to :   s= arg maxsim(g(s);g(s ) )   subj . to Jf(s)K = y :   AssumingL(C)is sufÔ¨Åciently small , we can solve   this problem by enumerating through the possible   choicess2L(C)and choosing the highest scor-   ing one that satisÔ¨Åes the constraint . In practice ,   we may be able to exploit the structure of the con-   straint to prune the search space . Our approach is   summarized in Algorithm 1 .   3 Experiments   We perform two user studies to demonstrate ( i )   correctness : our explanations preserve the user ‚Äôs   original intent , and ( ii ) usefulness : our explanations   improve user performance .   3.1 BabyAI Task   We evaluate our approach on BabyAI ( Chevalier-   Boisvert et al . , 2018 ) adapted to our setting . In this   task , the human can provide commands to an agent   navigating a maze of rooms containing keys , boxes ,   and balls . The goal is deÔ¨Åned by the combination   of the agent position and the environment state   ( e.g. , the agent may need to place a ball next to a   box ) . Atomic commands ( e.g. , going to , picking   up , or putting down an object ) can then be com-   posed in sequence to achieve complex goals . In our   setup , sis a natural language command , and y   is a demonstration in the form of a trajectory the   agent could take to achieve the desired goal .   This task comes with a context - free grammar of   natural language commands , which we use as the115space of possible explanations . Next , we train a   semantic parser to understand commands from this   grammar . Since utterances in this grammar corre-   spond one - to - one with programs , we can generate   training data . We generate 1000 training examples   ( s;)consisting of an utterance salong with a pro-   gram , and train TranX ( Yin and Neubig , 2018 ) to   predict=f(s ) . For semantic similarity , we use   a pretrained DistilBERT model g(Devlin et al . ,   2018 ; Sanh et al . , 2019 ) to embed utterances s.   Handling the goal constraint is more challeng-   ing , since the denotation can be nondeterministic ‚Äî   in particular , multiple different trajectories can be   used to achieve a single goal ( e.g. , there are mul-   tiple paths the agent can take to a given object ) .   Thus , if we na√Øvely take the denotation of a pro-   gram to be a single trajectory that achieves the   goal , then this trajectory may be different than the   given demonstration even if the demonstration also   achieves the goal . To address this issue , we in-   stead enumerate the set of all possible programs   that are consistent with the given demonstration   y , up to a bounded depth ( selected so that is   large enough while ensuring that the experiments   still run quickly ) . Then , we replace the constraint   Jf(s)K = ywith a constraint saying that f(s)is   in this set ‚Äî i.e. , f(s)2.   3.2 Correctness of Explanations   We evaluate whether our explanations are valid   paraphrases of the user ‚Äôs original command .   Baselines . We compare to two ablations of our   algorithm . The Ô¨Årst one omits the goal constraint   f(s)2 ; thus , it simply returns the explana-   tion that is most semantically similar to the user-   provided utterance s. Intuitively , this ablation   evaluates the usefulness of the goal constraint .   The second ablation ignores s , and returns an   explanation ssuch thatf(s)2 ; we choose   sto minimize perplexity according to GPT-2 . In-   tuitively , this ablation measures the usefulness of   specializing the explanation to the user ‚Äôs utterance .   Setup . We selected 17 BabyAI tasks by randomly   sampling BabyAI levels until we obtain a set of   tasks of varying difÔ¨Åculty . For example , Task 1 has   the simple goal ‚Äú go to the green ball ‚Äù , while Task   10 has the more complex goal ‚Äú pick up a green key ,   then put the yellow box next to the grey ball ‚Äù .   Then , our experiment proceeds in two phases . In   the Ô¨Årst phase , we use Amazon Mechanical Turk   ( AMT ) to collect natural language commands for   the agent . For each of our 17 tasks , we show the   user a video of the BabyAI agent achieving the task ,   and then ask them to provide a single command   that encodes the goal . In total , we obtained 127   commands ( one per user ) for each task . Next , for   each user instruction , we Ô¨Ånd the counterfactual   explanation according to our algorithm and the two   ablations described above .   In the second phase , we conduct a second AMT   study to evaluate the correctness of these explana-   tions . In particular , for each of our 17 tasks , we   show each participant a single command for that   task ( chosen randomly from the 127 commands   in the Ô¨Årst phase ) , along with the three generated   explanations and the video of the agent achieving   that task . Then , we ask the user to choose the ex-   planation that is closest in meaning to the original   command . We obtained 50 responses .   Results . In Table 1 , we show the fraction of times   users in the second phase selected each explana-   tion , averaged across both users and tasks . Our   approach signiÔ¨Åcantly outperforms GPT-2 , which   is unsurprising since this ablation makes no effort   to preserve the user ‚Äôs intent . Our approach also out-   performs the ablation without the goal constraint ,   demonstrating the usefulness of this constraint .   3.3 Usefulness of Explanations   Next , we evaluate whether providing explanations   can make it easier for users to provide commands   that can be understood by our semantic parser .   Baselines . In addition to the two ablations in Sec-   tion 3.2 , we also compare to a baseline where the   user is not provided with any explanation .   Setup . We run an AMT study similar to the Ô¨Årst   phase of our study in Section 3.2 , except immedi-   ately after providing a command for a task , each   user is shown an explanation for their command   and that task . We collected 50 user responses.116Results . For each user command s , we run our   semantic parser to obtain the corresponding pro-   gram and check whether it is in the set of programs   valid for that task ‚Äî i.e. , whether f(s)2. Ta-   ble 1 shows the success rate across all users and   the last 10 tasks ; we restrict to the last 10 to give   the user time to learn to improve their performance .   Users not provided any explanations performed   very poorly overall . The remaining approaches   performed similarly ; our explanations led to the   best performance , followed closely by the ablation   without the demonstration , with a wider gap to the   ablation that ignores the user utterance . Thus , per-   sonalizing the explanation to the user based on their   utterance helps improve performance .   4 Conclusion   We have proposed a technique for explaining how   users can adapt their utterances to interact with   a natural language interface . Our experiments   demonstrate how our explanations can be used   to signiÔ¨Åcantly improve the usability of semantic   parsers when they are limited in terms of their se-   mantic understanding . While any explanations are   already very useful , we show that personalizing   explanations can further improve performance .   A key design choice in our approach is to con-   struct a synthetic grammar from which counterfac-   tual explanations are generated . In a realistic appli-   cation , the semantic parsing model can be trained   on a combination of synthetic data and real - world   data , enabling our approach to be used in conjunc-   tion with the synthetic grammar . A key direction   for future work is extending our approach to set-   tings where such a grammar is not available . In our   experience , a key challenge in this setting is that   the generated text can be unnatural , possibly due   to the constraints imposed on the search space .   Acknowledgments   We gratefully acknowledge support from NSF CCF-   1917852 and CCF-1910769 . The views expressed   are those of the authors and do not reÔ¨Çect the ofÔ¨Å-   cial policy or position of the U.S. Government .   References117118   Matthew A. Byrd Shashank Srivastava   University of North Carolina at Chapel Hill   matthew_a_byrd@outlook.com , ssrivastava@cs.unc.edu   Abstract   1 Introduction   The use of question answering for testing learning   often relies on characterizing questions on aspects   such as difÔ¨Åculty anddiscrimination . For exam-   ple , ordering questions by difÔ¨Åculty can enable   curriculum learning ( Bengio et al . , 2009 ) . Simi-   larly , discrimination is used in standardized exams   such as the SAT to ensure that questions are varied   enough to discriminate between high - ability and   low - ability respondents . Item Response Theory   ( IRT ) ( Wright and Stone , 1979 ; Lord , 1980 ) has   been a widely applied framework to jointly esti-   mate such parameters for questions ( or items ) andthe abilities of respondents . While IRT has its in-   ception in psychometrics and has traditionally been   used with human respondents , recently , it has been   explored for analyzing predictions from an ‚Äò artiÔ¨Å-   cial crowd ‚Äô of ML models ( Prud√™ncio et al . , 2015 ;   Plumed et al . , 2016 ; Mart√≠nez - Plumed et al . , 2019 ;   Lalor et al . , 2019 ; Vania et al . , 2021 ; Rodriguez   et al . , 2021 ) .   While it can be helpful to know which ques-   tions are difÔ¨Åcult / discriminatory , it can be equally   important to be able to determine a question ‚Äôs dif-   Ô¨Åculty / discrimination parameters without having   to use it in a testing environment ( as is required to   estimate IRT parameters ) . Some recent work , such   as Ha et al . ( 2019 ) , has explored using features   derived from the text of a question to predict the   difÔ¨Åculty in the context of multiple - choice medi-   cal exams . While others ( Benedetto et al . , 2020 )   have used tf - idf features to predict the difÔ¨Åculty   of questions as measured by IRT . We differ from   these works in two ways : Firstly , while Ha et al .   ( 2019 ) ; Benedetto et al . ( 2020 ) both predict the dif-   Ô¨Åculty of items for humans , we are interested in pre-   dicting the difÔ¨Åculty ( and discrimination ) of items   for QA models . Secondly , we choose a question-   answering dataset , HotpotQA ( Yang et al . , 2018 ) ,   as our testbed . We utilize this dataset to generate a   rich and varied feature set across each item ‚Äôs ques-   tion , answer , and associated contexts . We can then   employ these features to analyze our difÔ¨Åculty and   discrimination predictions , giving us insights into   both our underlying QA model and factors that can   increase the difÔ¨Åculty / discrimination of a question .   Our analysis shows signiÔ¨Åcant variations among   questions and reveals some surprising patterns . We   show that it is possible to predict both difÔ¨Åculty   and discrimination of natural language questions ,   which can have multiple applications in education   and pedagogy . Additionally , we see that different   surface - level features are associated with high dis-   crimination and high difÔ¨Åculty , which can inform119new evaluation methods and the creation of new   datasets . Further , we identify attributes for predict-   ing difÔ¨Åculty and discrimination that are general   enough to be adapted to various QA datasets .   2 IRT Analysis of HotpotQA   IRT background : We begin by summarizing the   1PL and 2PL models from IRT , which form the   basis of our later analysis . The 1PL ( 1 Parameter   Logistic ) model describes the probability of respon-   denticorrectly answering the j‚Äôth item ( question )   in terms of scalar - valued parameters for question   difÔ¨Åculty ( d ) and respondent ability (  ) . These   parameters are estimated from data y2f0;1 g   for a set of i , jpairs . Here , y= 1 indicates a   correct answer . The 1PL model is described by :   p(y= 1j;d ) = 1   1 + e   The 2PL model extends the 1PL by adding a scalar-   valued parameter  , which represents the discrim-   ination of the j‚Äôth item . Intuitively , this parameter   denotes how sharply the probability of answering   a question correctly changes as the ability of the   respondent increases . The 2PL model is described   by :   p(y= 1j;d ;  ) = 1   1 + e   Dataset description : We chose HotpotQA for our   analysis since it is signiÔ¨Åcantly more complex than   other datasets such as SQuAD ( Rajpurkar et al . ,   2016 ) due to the questions requiring multi - hop rea-   soning and having more complex language . In   HotpotQA , each question is paired with two para-   graphs considered ‚Äò gold ‚Äô contexts and several other   paragraphs considered ‚Äò distractor ‚Äô contexts . The   answer to each question is a span in one of the   gold contexts , but correctly answering the question   requires combining information from both ‚Äò gold ‚Äô   contexts .   2.1 Estimating IRT Parameters   We estimate the IRT parameters for the questions   in HotpotQA ‚Äôs dev set ( 7;405questions ) . How-   ever , collecting human responses for each question ,   which is necessary to estimate IRT parameters , is   infeasible . Motivated by Lalor et al . ( 2019 ) , we   create an artiÔ¨Åcial crowd of QA models in placeof a crowd of human respondents . For this , we   train 148instances of DFGN ( Qiu et al . , 2019 )   models on HotpotQA ‚Äôs train set . To ensure diver-   sity , we uniformly sample the number of training   epochs from 1to15and sample the fraction of the   training data used for model training from U(0;1 ) .   Otherwise , each model was trained with the hyper-   parameters described in Qiu et al . ( 2019 ) . Next , we   generate an item - response matrix indicating which   questions from the HotpotQA dev set each model   answered correctly ( i.e. , the model ‚Äôs answer ex-   actly matched the correct answer ) . We remove any   questions that received no correct answers or no   incorrect answers . This is done as during the esti-   mation process , these questions tend towards ( + /- )   inÔ¨Ånity in their difÔ¨Åculty parameters , as well , their   discrimination parameter estimate tends towards   zero ( unable to distinguish between high and low   performing models ) . Our Ô¨Ånal dataset is a subset   of4;000questions ( 2;000train , 1;000dev , and   1;000test ) . Finally , we Ô¨Åt the 1PL and 2PL mod-   els on the foresaid item - response matrix using the   variational IRT training procedure from Natesan   et al . ( 2016 ) .   2.2 Analysis of Estimated Parameters   Figure 1 shows a scatter - plot of estimated dif-   Ô¨Åculty and discrimination values for individual   questions . We note that some discrimination val-   ues asymptotically approach 0 . This occurs when   some questions receive very few or many correct   answers ; these questions can not discriminate high-   performing from low - performing models . We also   note that some questions have negative discrimina-   tion , i.e. , as a model ‚Äôs ability increases , its probabil-   ity of answering the question correctly decreases .   This is primarily a result of some of the highest per-120   forming models giving an answer which is either a   subspan of or contains the ground - truth answer of   questions that were otherwise answered correctly   by lower - performing models . Overall , there is a   weak positive correlation between discrimination   and difÔ¨Åculty ( =0:04 ) .   To visualize any correlation between the seman-   tic and syntactic information of questions and their   respective difÔ¨Åculty levels , we clustered questions   based on their BERT embeddings using KMeans   ( K=20 ) clustering ( 2D UMAP reduction shown in   Figure 2 ) . Through manually examining and label-   ing the clusters , we found that many clusters could   be described with a speciÔ¨Åc style ( e.g. , yes / no ques-   tions ) or general topic . Some clusters , such as C.3 ,   have a large variety in the phrasing of questions   being asked and the potential answers in both syn-   tactic and semantic features . For example , both Q :   Khushi Ek Roag is broadcast by a company based   out of where ? A : Dubai andQ : To Catch a Preda-   tor was devoted to impersonating people below the   age of consent for which in North America varies   by what ? A : jurisdiction are in C.3 .   Other clusters , such as C.1 and C.2 , ( yes / no clus-   ters ) , only vary in topic rather than the type of   question . In particular , for these clusters , the es-   timated difÔ¨Åculty has signiÔ¨Åcantly lower variance   than the other clusters ( = 0:02,= 0:04respec-   tively ) , indicating that these yes / no questions tend   to be consistent in their difÔ¨Åculty . The standard   deviation values for C.1 and C.2 are 1:08and1:19   respectively , the average standard deviation valueis2:27 . We further explore how these factors affect   predicting the difÔ¨Åculty values in section 4 .   3 Predicting IRT Parameters   We next discuss predictive models for discrimina-   tion and difÔ¨Åculty using features from the question ,   answer , and associated context . First , we describe   our feature set , then provide an ablation study , a   feature importance study , and Ô¨Ånally qualitatively   analyze the predictions of our best model .   3.1 Feature Design   We experiment with two categories of fea-   tures : human - centric and machine - centric features .   For human - centric features , we considered ( 1 )   counting - based Lexical & Syntactic features ex-   tracted for both questions and answers like Con-   tentWords , Type - token ratio , Avg . Word Length ,   Complex Words ( > 3syllables ) ; ( 2 ) Semantic-   Ambiguity features measuring a question ‚Äôs or an-   swer ‚Äôs ambiguity ( Ha et al . , 2019 ) ; and ( 3 ) Read-   ability features based on measures like Fleisch   Kincaid index . More feature details can be found   in Appendix C. For machine - centric features , we   considered ( 1 ) Contextual Embeddings for ques-   tions and answers from BERT ( Devlin et al . , 2019 ) ;   ( 2 ) n - gram Overlap Counts between the question   and answer , and between question / answer and the   gold / distractor paragraphs ; and ( 3 ) POS Counts   from the Stanford Tagset ( Toutanova et al . , 2003 )   for the question and answer .   3.2 Quantitative Analysis and Ablation   Table 1 and Table 2 show the regression perfor-   mance of our models for predicting the IRT difÔ¨Å-   culty / discrimination parameters of the questions in   our dev / test sets using the feature sets described   before . The reported results are averaged over a 10-   fold cross - validation . We note that the best models   for both difÔ¨Åculty and discrimination show signif-   icant (  < 0:10 ) predictive performance ( Rof   0:17and0:13 ) against our baseline ( Mean ) .   The best performance is achieved in both tasks   by considering all features . In both cases , there is   a signiÔ¨Åcant difference (  < 0:1 ) in performance   between using any single set and using all features ,   except the best - performing BERT feature set . We   also note that features derived from the answer   are typically better at capturing difÔ¨Åculty , while   features derived from the question better predict   the discrimination parameters . However , the per-121   formance of All ( Q ) and All ( A ) for both the dis-   crimination and difÔ¨Åculty is weaker than using all   features . Since the difference is not statistically sig-   niÔ¨Åcant , it is unclear how much predictive power is   added when considering both answer and question   features in these predictions .   The features that focus on human difÔ¨Åculty are   among the less effective feature sets , indicating that   the human difÔ¨Åculty features of a question do not   fully capture difÔ¨Åculty for QA models . We provide   details of models and their training and the exper-   iment setup in Appendix A ; as well , signiÔ¨Åcance   tests can be found in Appendix D.   3.3 Feature Importance Study   We estimated feature importance by permuting   each feature individually and measuring the change   in MSE on the dev set . We list features that caused   a change in MSE of at least .01 in tables 3 and 4 .   We point out that for predicting the discrimina-   tion , the number of cardinal digits in the answer   was the most important indicator of high discrimi-   nation . The positive correlation between the num-   ber of digits in the answer and the discrimination   of a question is expected . Qiu et al . ( 2019 ) showed   that the DFGN model has a signiÔ¨Åcant weakness   in numeric operations . This gives questions with   numeric answers a high discrimination value as   DFGN models are naturally inhibited in this regard ,   and thus only a few models with the most training   data will be capable of answering these questions .   We Ô¨Ånd a similar positive Pearson score ( = 0:14 )   between the difÔ¨Åculty and the number of cardinal   digits in the answer . While this weakness of the   DFGN model can not be applied to an arbitrary QA   model , the methodology used to determine this   weakness can be applied arbitrarily , which can give   solid grounding to claims about model weaknesses .   4 Qualitative Analysis   We qualitatively analyze the difÔ¨Åculty predic-   tions to understand the predictions of our best-   performing model . Similar to Figure 2 , Figure 3122shows a UMAP scatterplotfor questions on our   test split of the estimated IRT parameters . In this   case , instead of color - coding by difÔ¨Åculty as in   Figure 2 , we instead color - code by the absolute   error between our predictions and the measured dif-   Ô¨Åculty of each question . We again apply KMeans   ( k= 10 ) to our data with a smaller number of   clusters due to the smaller size of the test set . We   highlight CT.1 , like C.1 and C.2 of Figure 2 , this   cluster consists primarily of yes / no questions . The   difÔ¨Åculty in CT.1 has signiÔ¨Åcantly smaller vari-   ance in the estimated difÔ¨Åculties than the rest of   the clusters ( = 0:02 ) . As well , the prediction   error for CT.1 has signiÔ¨Åcantly smaller variance   ( =0:04)and had the smallest average prediction   error compared to the other clusters ( 0:68 ) . This   indicates that the model is able to recognize when   question groupings , such as yes / no questions , have   consistent difÔ¨Åculties ( as discussed in 2.1 ) and has   consistently lower error when predicting difÔ¨Åculty   for these questions . However , the prediction error   tends to vary more when the surface - level ques-   tion types are not sufÔ¨Åcient to characterize their   difÔ¨Åculty .   We explore this further through a small counter-   factual experiment . We are interested in taking an   item with high prediction error and slightly tweak-   ing it to understand how the model ‚Äôs predictions   can change with changes in the question and an-   swer . We selected an item with > 2absolute error   to perform this experiment . The question we use   in this study is : Which university is this American   philosopher , theologian , and Christian apologist   who supports theistic science , professor at ? with   an answer of Biola University . The predicted dif-   Ô¨Åculty was 0:51 . We found that simple changes   to the question , such as using synonyms and re-   moving unnecessary information , can increase the   predicted difÔ¨Åculty up to  0:21 . However , by mod-   ifying the answer ( and by necessity the question )   to be either a date or yes , we achieve a higher difÔ¨Å-   culty prediction ( 0:53and1:02 , respectively ) . This   further indicates the model ‚Äôs bias towards yes / no   questions being of a higher difÔ¨Åculty regardless of   the style or topic of question being asked . Some   of our changes and their corresponding predictions   are listed in Appendix E.   5 Conclusion   In this paper , we explored QA datasets through the   lens of Item Response Theory . We have demon-   strated a way to build regression models that can   describe the difÔ¨Åculty and discrimination of a ques-   tion . We note that our work is limited in two im-   portant ways : Ô¨Årstly , we only use the DFGN model   in our artiÔ¨Åcial crowd , which may have introduced   a bias in which some factors that make questions   difÔ¨Åcult / discriminatory are only applicable to this   model . Secondly , we only explore the HotPotQA   dataset , which may further limit our analysis to   only be applicable to HotPotQA or similar datasets .   Future work could incorporate multiple models and   datasets to explore a more easily generalizable dif-   Ô¨Åculty / discrimination prediction pipeline . We also   note that our analysis here focused on QA . How-   ever , there are many NLP tasks in which the difÔ¨Å-   culty or discrimination of an item may be important .   Our work here could naturally extend to these do-   mains . Finally , automatically predicting these traits   without relying on user responses can engender a   host of creative educational applications . Future   work can also leverage such predictive models to   explore more efÔ¨Åcient strategies for learning and   evaluation .   References123124125A Models & Training   For the 1PL and 2PL prediction , we considered   linear models with L1 & L2 regularization , random   forests , gradient boosted regressors , and bayesian   ridge models . All hyperparameters were kept con-   stant as the default in the sklearn package ( Pe-   dregosa et al . , 2011 ) . We performed 10 - fold cross-   validation using PyCaret ( Ali , 2020 ) . All models   were trained on a consumer grade processor .   B Feature DeÔ¨Ånitions   ‚Ä¢Human - Centric Features   ‚Äì Lexical & Syntactic features : These   consist primarily of counting features :   ContentWords , Type - token ratio , Avg .   Word Length , Complex Words ( > 3syl-   lables ) . These are calculated for both the   answer and question . A full list of these   features can be found in Appendix F   ‚Äì Semantic - Ambiguity features : We use   WordNet ( Miller , 1995 ) to calculate the   ambiguity of sentences , similar to Ha   et al . ( 2019 ) . These are calculated for   both answer and question .   ‚Äì Readability features : We use previous   work ( Kincaid et al . , 1975 ; Gunning ,   1952 ; Laughlin , 1969 ) to model the read-   ability of a question / answer ( e.g. Fleisch   Kincaid index ) . These are further ex-   panded on in Appendix C.   ‚Ä¢Machine - Centric Features   ‚Äì Contextual Embeddings : We use the   BERT - base model ( Devlin et al . , 2019 )   to obtain sentence embeddings for ques-   tions and answers .   ‚Äì Overlap Counts : We count overlaps   between the question and answer of n-   grams up to n= 3 . We also com-   pute overlap counts between the ques-   tion / answer and the gold and distractor   paragraphs .   ‚Äì Part of Speech Counts : We count POS   tags for tags from the Stanford NLP   tagset ( Toutanova et al . , 2003 ) for both   the question and answer .   C Reading DifÔ¨Åculty Features   We list the reading difÔ¨Åculty features we used in our   experiments and an overview of their calculations .   Each calculation has its own coefÔ¨Åcients that can   be found in their respective citations.‚Ä¢Flesch Reading Ease - linear combination of   words / sentence and syllables / word ( Flesch )   ‚Ä¢Flesch Kincaid Grade Level - linear combi-   nation of word / sentence and syllables / word   ( Kincaid et al . , 1975 )   ‚Ä¢Automated Readability Index ( ARI ) - lin-   ear combination of characters / word and   words / sentence ( Smith and Senter , 1967 )   ‚Ä¢Gunning Fog index - linear combination of   words / sentence and complex words / words .   Complex words are words with 3syllabus   ( Gunning , 1952 )   ‚Ä¢Coleman - Liau - linear combination of   letters/100 words and sentences/100   words.(Entin and Klare , 1978 )   ‚Ä¢SMOG index - calculates the grade level   by considering the number of complex   words / sentence ( Laughlin , 1969 )   D SigniÔ¨Åcance Tests   We provide signiÔ¨Åcance tests for the difÔ¨Åculty and   discrimination predictions in tables 5 and 6 . We see   that the BERT features and using all features are   able to beat the baseline with statistical signiÔ¨Åcance   ( :1 ) . Note that we compare using MSE rather   thanRas the baseline always has an Rscore of   0 . We also provide in table 7 the signiÔ¨Åcance tests   for using all features against BERT features . We   Ô¨Ånd that the best performing BERT feature set does   not have a statistically signiÔ¨Åcant improvement in   performance when compared to the all feature set .   In this case , we use Ras the performance metric .   E Counterfactual Results   ‚Ä¢‚ÄìQuestion ( original ): Which university is   this American philosopher , theologian ,   and Christian apologist , who supports   theistic science , professor at?‚Äô126   ‚Äì Answer : " Biola University "   ‚Äì Pred . Diff: 0:51   ‚Ä¢‚ÄìQuestion : Which school is this philoso-   pher and theologian who supports sci-   ence , professor at ?   ‚Äì Answer : " Biola University "   ‚Äì Pred . Diff: 0:21   ‚Ä¢‚ÄìQuestion : What was the birth date of a   professor at Biola University who is an   American philosopher , theologian , and   Christian apologist , who supports theis-   tic science ?   ‚Äì Answer : March 9 , 1948   ‚Äì Pred . Diff : 0:53   ‚Ä¢‚ÄìQuestion : Does Biola University have   a professor who is an American philoso-   pher , theologian , and Christian apologist ,   who supports theistic science ?   ‚Äì Answer : yes   ‚Äì Pred . Diff : 1:02   F Lexical Features   We list our full list of lexical features , these features   are a subset of the lexical features used in Ha et al .   ( 2019 ) .   ‚Ä¢ Word Count   ‚Ä¢ Content Word Count   ‚Ä¢ Content Word Incidence‚Ä¢ Content Word Count No Stopwords   ‚Ä¢ Noun Count   ‚Ä¢ Noun Incidence   ‚Ä¢ Verb Count   ‚Ä¢ Verb Incidence   ‚Ä¢ Adjective Count   ‚Ä¢ Adjective Incidence   ‚Ä¢ Adverb Count   ‚Ä¢ Adverb Incidence   ‚Ä¢ Number Count   ‚Ä¢ Number Incidence   ‚Ä¢ Type Count   ‚Ä¢ Type Token Ratio   ‚Ä¢ Comma Count   ‚Ä¢ Comma Incidence   ‚Ä¢ Average Word Length In Syllables   ‚Ä¢ Complex Word Count   ‚Ä¢ Complex Word Incidence ,   ‚Ä¢ Average Sentence Length   ‚Ä¢ Negation Count   ‚Ä¢ Negation Incidence   ‚Ä¢ Negation In Stem   ‚Ä¢ NP Count   ‚Ä¢ NP Incidence   ‚Ä¢ Average NP Length   ‚Ä¢ NP Count With Embedding   ‚Ä¢ NP Incidence With Embedding   ‚Ä¢ Average All NP Length ,   ‚Ä¢ PP Count   ‚Ä¢ PP Incidence   ‚Ä¢ PPs Per Sentence Ratio   ‚Ä¢ VP Count127‚Ä¢ VP Incidence   ‚Ä¢ Passive Active Ratio   ‚Ä¢ Proportion Active VPs   ‚Ä¢ Proportion Passive VPs   ‚Ä¢ Agentless Passive Count   ‚Ä¢ Relative Clauses Count   ‚Ä¢ Relative Clauses Incidence   ‚Ä¢ Proportion Relative Clauses   ‚Ä¢ Polysemic Word Count   ‚Ä¢ Polysemic Word Incidence   ‚Ä¢ Average Sense No Content Words   ‚Ä¢ Average Sense No Nouns   ‚Ä¢ Average Sense No Verbs   ‚Ä¢ Average Sense No Non Auxiliary Verbs   ‚Ä¢ Average Sense No Adjectives   ‚Ä¢ Average Sense No Adverbs   ‚Ä¢ Average Noun Distance To WNRoot   ‚Ä¢ Average Verb Distance To WNRoot ,   ‚Ä¢Average Noun And Verb Distance To WN-   Root   ‚Ä¢ Answer Words In Word Net Ratio   ‚Ä¢ Average Word Frequency Abs   ‚Ä¢ Average Word Frequency Rel   ‚Ä¢ Average Word Frequency Rank   ‚Ä¢ Average Content Frequency Abs   ‚Ä¢ Average Content Frequency Rel   ‚Ä¢ Average Content Frequency Rank   ‚Ä¢ Not In First 2000 Count   ‚Ä¢ Not In First 2000 Incidence   ‚Ä¢ Not In First 3000 Count   ‚Ä¢ Not In First 3000 Incidence   ‚Ä¢ Not In First 4000 Count‚Ä¢ Not In First 4000 Incidence   ‚Ä¢ Not In First 5000 Count   ‚Ä¢ Not In First 5000 Incidence   ‚Ä¢ Imagability   ‚Ä¢ Imagability Found Only   ‚Ä¢ Imagability Ratio   ‚Ä¢ Familiarity   ‚Ä¢ Familiarity Found Only   ‚Ä¢ Familiarity Ratio   ‚Ä¢ Concreteness   ‚Ä¢ Concreteness Found Only   ‚Ä¢ Concreteness Ratio   ‚Ä¢ Age Of Acquisition   ‚Ä¢ Age Of Acquisition Found Only   ‚Ä¢ Age Of Acquisition Ratio   ‚Ä¢ Meaningfulness Colorado Found Only   ‚Ä¢ Meaningfulness Pavio Found Only   ‚Ä¢ No Imagability Rating   ‚Ä¢ No Familiarity Rating   ‚Ä¢ No Concreteness Rating   ‚Ä¢ No Age of Acquisition Rating   ‚Ä¢ Connectives Count   ‚Ä¢ Connectives Incidence   ‚Ä¢ Additive Connectives Count   ‚Ä¢ Additive Connectives Incidence   ‚Ä¢ Temporal Connectives Count   ‚Ä¢ Temporal Connectives Incidence   ‚Ä¢ Causal Connectives Count   ‚Ä¢ Causal Connectives Incidence   ‚Ä¢ Referential Pronoun Count ,   ‚Ä¢ Referential Pronoun Incidence128 G Discrimination UMAP plots   In the following section , we provide the UMAP   reduction plots for the discrimination parameters   ( darker being more discriminatory ) , as well as the   prediction error UMAP plot for our best model   ( darker meaning higher error).129130   Ahmed Alajrami and Nikolaos Aletras   Department of Computer Science   University of Sheffield , UK   { ajsalajrami1 , n.aletras}@sheffield.ac.uk   Abstract   1 Introduction   The most popular way to pre - train a transformer-   based ( Vaswani et al . , 2017 ) language model ( LM ) ,   e.g. BERT ( Devlin et al . , 2019 ) , is by optimizing a   masked language modeling ( MLM ) objective . The   MLM task was inspired by the Cloze Task ( Taylor ,   1953 ) , where humans were asked to guess omitted   words in a sentence using its context , knowledge   of syntax and other skills . The premise is that such   an objective will guide a LM to encode linguistic   information .   Apart from MLM , different types of objectives   have been recently proposed . Yang et al . ( 2019)introduced a pre - training objective based on token   order permutations . Clark et al . ( 2020 ) proposed   a Replaced Token Detection pre - training task , that   uses the output of a small MLM to corrupt the in-   put by replacing some of the tokens . It then trains   a discriminative model to predict if a token has   been replaced or not . Aroca - Ouellette and Rudzicz   ( 2020 ) explored various sentence and token - level   auxiliary pre - training tasks ( e.g. sentence ordering ,   term - frequency prediction ) , as better alternatives to   the next sentence prediction ( NSP ) auxiliary task   originally used to train BERT . Lan et al . ( 2020 )   introduced the sentence - order prediction task that   focuses on the inter - sentence coherence , by predict-   ing if two contiguous sentences have been swapped   or not . Iter et al . ( 2020 ) proposed another inter-   sentence pre - training task , that helps LMs to en-   code discourse relationships between sentences us-   ing contrastive learning . Yamaguchi et al . ( 2021 )   showed that a non - linguistically intuitive task ( i.e.   masked first character prediction ) can effectively   be used for pre - training .   Meanwhile , several studies have explored how   well and to what extent LMs learn linguistic in-   formation . This is usually examined using prob-   ing tasks , i.e. simple classification tasks that test   the LM ‚Äôs encodings for a single linguistic fea-   ture such as grammatical information . It has been   found through probing that BERT encodes syn-   tactic ( Tenney et al . , 2019 ; Liu et al . , 2019 ; Mi-   aschi and Dell‚ÄôOrletta , 2020 ; Hewitt and Manning ,   2019 ; Jawahar et al . , 2019 ) and semantic informa-   tion ( Ettinger , 2020 ; Jawahar et al . , 2019 ; Tenney   et al . , 2019 ) . However , Hall Maudslay and Cot-   terell ( 2021 ) argue that BERT ‚Äôs syntactic abilities   may have been overestimated .   In this paper , we hypothesize that linguistically   motivated objectives ( e.g. MLM ) should help   BERT to acquire better linguistic knowledge com-   pared to using non - linguistically motivated objec-   tives , i.e. tasks that are hard for humans to guess131the association between the input and the label to   be predicted . To this end , we seek to answer the   following research question : How does the pre-   training objective affect what LMs learn about the   English language ?   Our findings challenge the MLM status quo ,   showing that pre - training with non - linguistically   informative objectives ( ¬ß 2 ) results in models with   comparable linguistic capabilities , as measured by   standard probing benchmarks ( ¬ß 3 ) . These surpris-   ing results ( ¬ß 4 ) suggest that careful analysis of how   LMs learn is critical to further improve language   modeling ( ¬ß 5 ) .   2 Pre - training Objectives   We experiment with five different pre - training ob-   jectives . Two of them are considered linguistically   motivated while the rest are not .   2.1 Linguistically Motivated Objectives   Masked Language Modeling ( MLM ): We use   MLM as our first linguistically motivated pre-   training objective . First introduced by Devlin et al .   ( 2019 ) , MLM randomly chooses 15 % of the tokens   from the input sentence and replaces 80 % of them   with a [ MASK ] token , 10 % with a random token ,   and 10 % remain unchanged .   Manipulated Word Detection ( S+R ): We also   experiment with a simpler linguistically motivated   objective , where the model selects and replaces   10 % of input tokens with shuffled tokens from   the same input sequence . Concurrently , it selects   and replaces another 10 % of input tokens with ran-   dom tokens from the vocabulary ( Yamaguchi et al . ,   2021 ) .   2.2 Non - Linguistically Motivated Objectives   We assume that tasks that are hard for humans ( such   as a completely random prediction task ) will make   less likely the deeper layers of BERT ( i.e. closer to   the output layer ) to acquire meaningful information   about language . We also hypothesize that layers   closer to the input might learn word co - occurrence   information ( Sinha et al . , 2021 ) .   Masked First Character Prediction ( First Char ):   For our first non - linguistically motivated pre-   training objective , we use the masked first char-   acter prediction introduced by Yamaguchi et al .   ( 2021 ) . In this task , the model predicts only the   first character of the masked token ( e.g. ‚Äò [ c]at ‚Äô and‚Äò[c]omputer ‚Äô belong to the same class ) . The model   predicts the first character as one of 29 classes , in-   cluding the English alphabet and digit , punctuation   mark , and other character indicators .   Masked ASCII Codes Summation Predic-   tion ( ASCII ): We also propose a new non-   linguistically motivated pre - training objective ,   where the model has to predict the summation of   the ASCII code values of the characters in a masked   token . To make this harder and keep the number of   classes relatively small , we define a 5 - way classi-   fication task by taking the modulo 5 of the ASCII   summation : V= [ Pascii(char ) ] % 5 . Guess-   ing the association between the input and such la-   bel , is an almost impossible task for a human .   Masked Random Token Classification ( Ran-   dom ): Finally , we propose a completely random   objective where we mask 15 % of the input tokens   and we assign each masked token a class from 0 to   4randomly for a 5 - way classification similar to the   ASCII task . We assume that a model pre - trained   with a random objective should not be able to learn   anything meaningful about linguistic information .   3 Probing Tasks   Probing tasks ( Adi et al . , 2016 ; Conneau et al . ,   2018 ; Hupkes et al . , 2018 ) are used to explore in   what extent linguistic properties are captured by   LMs . A model is normally trained , using the repre-   sentations of a language model , to predict a specific   linguistic property . If it achieves high accuracy , it   implies that the LM encodes that linguistic prop-   erty . In this work , we use nine standard probing   tasks introduced by Conneau et al . ( 2018 ) to ex-   amine the representation output for each layer of   the different LMs we pre - train following Shen et al .   ( 2020 ) . These tasks probe for surface , syntactic   and semantic information . The dataset for each   probing task contains 100k sentences for training ,   10k sentences for validation and another 10k sen-   tences for testing . We train a multi - layer percep-   tron ( MLP ) classifier for each probing task using   the recommended hyperparameters in the SentEval   toolkit ( Conneau and Kiela , 2018 ) .   Surface information task : SentLen aims for   correctly predicting the number of words in a sen-   tence.132   Syntactic information tasks : TreeDepth tests   if the representations preserve information about   the hierarchical structure of a sentence , by predict-   ing the depth of its parse tree . TopConst predicts   the top constituents of the parse tree of a sentence .   BShift tests if two adjacent words have been in-   verted or not .   Semantic information tasks : Tense aims to pre-   dict if the main - clause verb is present or past . Sub-   jNum predicts if the subject of the main clause   is singular or plural . ObjNum tests if the direct   object of the main clause is singular or plural . Se-   mantic Odd Man Out ( SOMO ) tests if a noun or   verb has been replaced with another noun or verb .   CoordInv predicts if a sentence made of two coor-   dinate clauses has been inverted or not .   4 Experiments & Results   4.1 Experimental Setup   Models We pre - train BERT- ( Devlin et al . ,   2019 ) models by replacing MLM and the next sen-   tence prediction ( NSP ) objectives , with one of the   linguistically or non - linguistically motivated pre-   training objectives ( ¬ß 2 ) . For completeness , we also   pre - train two smaller model architectures ,   and from ( Turc et al . , 2019 ) as in Yam-   aguchi et al . ( 2021 ) . The model haseight hidden layers and eight attention heads . The model has four hidden layers and eight at-   tention heads . Both , and , models   have feed - forward layers of size 2048 and hidden   layers of size 512 . More details on hyperprameters   can be found in Appendix A.   Pre - training Data All models are pre - trained on   the BookCorpus ( Zhu et al . , 2015 ) and English   Wikipedia from Hugging Face . The text is tok-   enized using Byte - Pair - Encoding ( Sennrich et al . ,   2016 ) , resulting to a total of 2.7 billion tokens .   Pre - training Details Due to limited computa-   tional resources , each model is pre - trained   for 500k steps , while each and   model is pre - trained for 250k steps using 8   NVIDIA Tesla V100 ( SXM2 - 32 GB ) . We use a   batch size of 32 for , and 64 for   and . We optimize the models using Adam   ( Kingma and Ba , 2014 ) .   Fine - tuning Details We use the General Lan-   guage Understanding Evaluation ( GLUE ) bench-   mark ( Wang et al . , 2018 ) to fine - tune each model   for up to 20 epochs with early stopping . For each   fine - tuning task , we use five different seeds and133   report the average . We report matched accuracy for   MNLI task , Matthews correlation for CoLA task ,   Spearman correlation for STS - B task , accuracy for   MRPC task , F1 scores for QQP task , and accu-   racy for all other tasks . The WNLI task is omitted   following Aroca - Ouellette and Rudzicz ( 2020 ) .   BERT Representations In all of the probing   tasks , we use the BERT representations of the   [ CLS ] token at every layer as the input to the prob-   ing classifier .   4.2 Fine - tuning Results   Table 1 shows the results of fine - tuning the mod-   els with all pre - training objectives on GLUE to   measure their performance in downstream tasks .   For the model configuration , we observe   that linguistically motivated objectives ( e.g. MLM ,   S+R ) achieve the best performance in downstream   tasks . However , models pre - trained with non-   linguistically motivated objectives ( e.g. First Char ,   ASCII ) still achieve competitive results . As ex-   pected , the model pre - trained using the Random   objective obtains the lowest performance with 56.4   GLUE average score . However , its performance   is still reasonable in many downstream tasks , sug-   gesting that the model is able to learn some co-   occurrence information from the input ( Sinha et al . ,   2021 ; Yamaguchi et al . , 2021 ) . Similar behavior   can be observed for the other two model configura-   tions , and .   4.3 Probing Results   Table 2 presents the results of the best performing   layer on the nine probing tasks using the representa-   tions from the BERT- models as inputs to the   MLP classifier . Similar to the fine - tuning results ,   we first observe that the predictive performance ofmodels trained on representations learned using lin-   guistically motivated objectives ( e.g. MLM , S+R )   achieve the best performance in six out of the nine   probing tasks . However , models trained on the rep-   resentations learned using non - linguistically moti-   vated objectives ( e.g. First Char , ASCII ) achieve   very competitive results . . For example , in the Top-   Const probing task , the model pre - trained using   MLM pre - training objective achieves the best per-   formance of 83.6 % , while the the model pre - trained   using ASCII pre - training objective achieves 81.4 % .   Similar patterns can be observed from the prob-   ing results of the other two model configurations , and ( see Tables 3 and 4 respec-   tively ) . For instance , in the SentLen probing task in   table 3 , the difference between the best performing model ( S+R ) and the worst performing model ( ASCII ) is only 3.6 % . In the Ob-   jNum probing task in table 4 , the model   pre - trained using a non - linguistically motivated pre-   training objective ( ASCII ) achieves 84.4 % , while   the models pre - trained using linguistically   motivated pre - training objectives , MLM and S+R ,   achieve 83.5 % and 83.3 % respectively .   The full results of the probing tasks including all   layers can be found in appendix B.   5 Discussion   Theoretically , LMs with non - linguistically moti-   vated objectives would be expected to perform dras-   tically worse than LMs pre - trained using MLM   in both downstream tasks and linguistic capabil-   ities . However , our results show that both types   of LMs have surprisingly close performance ( af-   ter fine - tuning on downstream tasks ) and linguistic   capabilities ( after probing them ) using the same   training data , architecture and training scheme . We   speculate that the pre - training data , and the size of134   the models have more impact on the effectiveness   of LMs than the pre - training objectives . Further-   more , the comparable performance of different ob-   jectives in probing suggests that LMs mainly learn   word co - occurrence information from pre - training   ( Sinha et al . , 2021 ; Yamaguchi et al . , 2021 ) and   that the objectives may have a little effect to what   actually learn about linguistic properties .   Recent studies have explored the limitations of   using probing tasks to draw conclusions over a   model ‚Äôs linguistic knowledge with some also sug-   gesting improvements or alternative probing meth-   ods ( Hewitt and Liang , 2019 ; V oita and Titov , 2020 ;   Elazar et al . , 2021 ; Maudslay and Cotterell , 2021 ) .   However , our results show no substantial differ-   ences in the performance across tasks that probe for   syntactic or semantic information between models   that have been pre - trained using linguistically mo-   tivated objectives or non - linguistically motivated   ones .   6 Conclusions   In this work , we compared the linguistic capabili-   ties of LMs . Surprisingly , our results show that pre-   training with linguistically motivated objectives ob-   tain comparable performance to non - linguistically   motivated objectives . This suggests that the data   and the size of the model could be more influential   than the objectives themselves in language model - ing . In future work , we plan to extend our experi-   ments into other languages and probing tasks .   Acknowledgments   We would like to thank Katerina Margatina and   George Chrysostomou for their invaluable feed-   back . We also thank the anonymous reviewers   for their constructive feedback . AA is supported   by the Centre for Doctoral Training in Speech   and Language Technologies ( SLT ) and their Ap-   plications funded by UK Research and Innovation   grant EP / S023062/1 . NA is supported by EPSRC   grant EP / V055712/1 , part of the European Com-   mission CHIST - ERA programme , call 2019 XAI :   Explainable Machine Learning - based Artificial In-   telligence .   References135136137Appendices   A Hyperparameter Details   We implement the models using PyTorch ( Paszke   et al . , 2019 ) and the Transformers library ( Wolf   et al . , 2020 ) . We use maximum 10 epochs for and , and 15 epochs for . We   also use a learning rate of 1e-4 for MLM . 5e-5 for First Char , S+R , and ASCII . 5e-6 for   Random . 1e-4 for and First Char ,   ASCII and Random . We also use weight decay of   0.01 , attention dropout of 0.1 , 10000 warmup steps .   We also use 1e-8 Adam œµ , 0.9 Adam Œ≤and 0.999   Adam Œ≤ .   B Results of each Probing Task   Tables 5 to 13 show the full results of each of the   nine probing tasks for all model architectures and   layers.138139140141142143144145146147   Nathan SchucherSiva ReddyHarm de VriesServiceNow ResearchMila / McGill UniversityFacebook CIFAR AI Chair   { nathan.schucher,harm.devries}@servicenow.com   Abstract   1 Introduction   With the widespread success of pre - trained lan-   guage models ( LMs ; Devlin et al . 2019 ; Raffel   et al . 2020 ; Bommasani et al . 2021 ) , it becomes   increasingly important to explore how such models   can be adapted to downstream tasks . One adap-   tation method which has recently attracted much   attention is prompt design ( Brown et al . , 2020 ; Shin   et al . , 2020 ) , which modulates the behaviour of a   LM through a task description and a few input-   output examples . Brown et al . ( 2020 ) show that   this adaptation strategy is increasingly effective for   larger LMs . However , prompt design is sensitive   to the exact phrasing of the prompt , and , more im-   portantly , performs worse than fine - tuning models   on task - specific examples ( Lester et al . , 2021 ) .   Prompt tuning has recently arisen as a strong   performing alternative adaption method ( Lester   et al . , 2021 ) . Rather than hand - designing discrete   prompts , prompt tuning optimizes the embeddings   of a number of task - specific prompt tokens . In   contrast to fine - tuning , this method keeps almost   all LM parameters frozen . On a set of language   understanding tasks , Lester et al . ( 2021 ) show   that prompt tuning becomes competitive with fine-   tuning for the largest pre - trained T5 models ( Raffel   et al . , 2020 ) . Li and Liang ( 2021 ) also explore   a related parameter - efficient adaptation method   called prefix - tuning , finding that it outperforms   fine - tuning on low - resource natural language gen-   eration tasks .   In this paper , we investigate prompt tuning for   semantic parsing . This task is fundamentally differ-   ent from the aforementioned language understand-   ing and generation tasks , as it requires that mod-   els output formal meaning representations which   do not resemble the natural language distribution   seen during pre - training . In particular , we focus   on the low - resource setup because examples for   semantic parsing are difficult and expensive to col-   lect ( Wang et al . , 2015 ; Marzoev et al . , 2020 ) . We   therefore evaluate prompt tuning on two datasets :   the 200 - shot version of Overnight ( Wang et al . ,   2015 ; Shin et al . , 2021 ) and the low - resource splits   TOPv2 ( Chen et al . , 2020 ) . On both datasets , we   compare prompt tuning T5 against fine - tuning and   investigate the effect of canonicalizing the meaning148representation , i.e. to what extent naturalizing the   logical forms influences performance . In addition ,   we study the effect of T5 model scale on Overnight   as well as varying data regimes on TOPv2 . Our   main findings can be summarized as follows :   ‚Ä¢For large T5 models , prompt tuning signifi-   cantly outperforms fine - tuning in the low - data   regime , resulting in an absolute improvement   of 6 % and 15 % on Overnight and TOPv2 , re-   spectively . This performance gap decreases   when more training data becomes available .   ‚Ä¢With growing model size , prompt tuned T5   models are increasingly capable of outputting   diverse target representations ( see Figure 1 ) .   On Overnight , we find that the disparity be-   tween canonical and meaning representations   shrinks from 17 % to 4 % for T5 - small and   T5 - xl , respectively . On TOPv2 , prompt tuned   T5 - large models are much better at generating   out - of - vocabulary tokens than T5 - small .   2 Related work   Our work is related to recent work on semantic   parsing and prompt tuning , which we briefly de-   scribe below .   2.1 Semantic Parsing   Semantic parsing is the task of converting a nat-   ural language utterance u= ( u , . . . , u)to a   formal meaning representation z= ( z , . . . , z ) .   These meaning representations , also referred to   as logical forms , can be interpreted by machines   and executed in a real environment . For ex-   ample , ThingTalk ( Campagna et al . , 2019 ) and   TOP ( Gupta et al . , 2018 ) are meaning represen-   tations for executing commands of virtual as-   sistants , while SQL is a representation for in-   teracting with relational databases . In recent   years , neural sequence - to - sequence models have   become the dominant approach for semantic pars-   ing tasks ( Dong and Lapata , 2016 ) .   Canonicalization A common simplification step   in semantic parsing is to canonicalize the meaning   representations . That is , the meaning representa-   tionzis naturalized to a canonical form cthrough   a grammar or set of rules . Examples of the mean-   ing and canonical representation for Overnight and   TOPv2 ( Wang et al . , 2015 ; Chen et al . , 2020 ) can   be found in Fig . 2.When canonical representations are available ,   Berant and Liang ( 2014 ) argue that semantic pars-   ing can be seen as a paraphrase task . They propose   to use a paraphrase model ‚Äî using e.g. word vectors   trained on Wikipedia ‚Äî to find the best paraphrase   of utterance uamong a set of canonical utterances .   They show this paraphrase model improves re-   sults over directly generating logical forms on two   question - answering datasets . Marzoev et al . ( 2020 )   extends this work by showing that pre - trained lan-   guage models like BERT can be effective para-   phrasers . While Berant and Liang ( 2014 ) ; Marzoev   et al . ( 2020 ) use models to score canonical utter-   ances , Shin et al . ( 2021 ) propose to constrain the   generation process of autoregressive models like   BART and GPT-3 . On a number of few - shot seman-   tic parsing tasks , they demonstrate the benefit of   generating canonical representations over meaning   representations .   2.2 Prompt - tuning   Lester et al . ( 2021 ) evaluates prompt tuning on   SuperGLUE , a benchmark consisting of eight lan-   guage understanding tasks . They find that prompt   tuning becomes competitive with fine - tuning for   the largest T5 model . Li and Liang ( 2021 ) propose   prefix - tuning to adapt BART and GPT-2 for natu-   ral language generation tasks . This method differs   from Lester et al . ( 2021 ) in that it prepends train-   able embeddings for each layer of the language   model rather than introducing token embeddings at   the input layer . They demonstrate that pre - fix out-   performs fine - tuning baselines . Similarly , Liu et al .   ( 2021 ) also show encouraging results for prompt   tuning on natural language understand and gener-   ation tasks . Qin and Eisner ( 2021 ) also explores   prompt tuning but for a knowledge extraction task .   Inserting general adapter layers into pre - trained   language models is also proposed in Houlsby et al .   ( 2019 ) ; Mahabadi et al . ( 2021 ) . Related to our   work are also other few - shot adaptation techniques   like PET ( Schick and Sch√ºtze , 2021 ) . Moreover ,   adapter layers have also been explored in the com-   puter vision domain ( Rebuffi et al . , 2017 ; de Vries   et al . , 2017 ) .   3 Experiments   To evaluate low - resource prompt tuning , we com-   pare against fine - tuned variants of the same model   on two semantic parsing datasets with canonical   representations available . We compare both large149   and small variants of the T5 architecture on these   datasets and experiment with various canonicalized   representations .   3.1 Datasets   Overnight The Overnight semantic parsing   dataset ( Wang et al . , 2015 ) consists of 13,682 natu-   ral utterance , canonical form , meaning representa-   tion triples split across eight domains . To simulate   low - resource splits of this dataset , we follow Shin   et al . and create randomly subsampled splits of   200 training examples for each domain , using 20 %   of the remaining data for validation . We measure   and report denotation accuracy by evaluating all   predicted queries using the SEMPRE toolkit ( Be-   rant et al . , 2013 ) . We repeat each experiment on   Overnight with five different random splits .   TOPv2 Chen et al . ( 2020 ) introduce the TOPv2   dataset , a task - oriented semantic parsing dataset   with eight domains , two of which come with pre-   defined low - resource splits . The authors propose a   principled way of constructing low - resource train-   ing sets , samples per intent and slot ( SPIS ) , in-   tended to ensure equal exposure to ontology labels   across domains of varying complexity . We experi-   ment with the weather andreminder domains at the   10 , 25 , and 500 SPIS resource splits , performing   five runs on each model varying the random seed .   Thereminder domain is the most challenging with   19 intent labels , 32 slot labels , and with 21 % of the   programs having a depth greater than 2 . Weather in   comparison has 7 intent labels , 11 slot labels , and   no programs with depth greater than 2.3.2 Canonicalized Representations   3.2.1 Overnight   Overnight uses a context - free synchronous gram-   mar to generate canonical representations for the   logical forms . As can be seen in Fig . 2 , these canon-   ical representations resemble natural language .   3.2.2 TOPv2   Chen et al . apply a set of simple modifications   to the TOPv2 meaning representations to arrive   at a canonical form used in all their experiments .   Unlike Overnight , these pre - processing steps are   largely small encoding differences and do not   change the syntactic structure of the logical forms .   We adopt all of these canonicalization steps ( except   for lexicographic sorting of the semantic parse tree )   and add an ontology label shortening step . Exam-   ples of these transformations can be seen in Fig . 2   and are briefly described below .   Simplify removes redundant utterance tokens un-   necessary for interpreting the meaning repre-   sentation .   Out - of - Vocab adds the entire intent or slot label   to the tokenizer as a new single tokens with   a corresponding randomly initialized embed-   ding .   In - Vocab replaces the intent and slot labels with   a short unique identifier representable by the   pre - trained tokenizer .   We perform an ablation over these canonicaliza-   tion choices , repeating each experiment three times   with varying random seed.150   3.3 Models   We provide training details and hyperparameters   for all models in Appendix A. Below , we briefly   explain the prompt - tuning methodology .   3.3.1 Prompt Tuning   Prompt tuning , as proposed by Lester et al . ( 2021 ) ,   prepends a sequence of continuous embeddings   p= ( p , . . . , p)to the sequence input embed-   dings e(u ) = ( e(u ) , . . . , e ( u))before feeding   it to a language model with parameters Œ∏ . Dur-   ing prompt tuning we optimize the prompt embed-   dings ( p , . . . , p)exclusively , keeping the lan-   guage model parameters Œ∏and the pretrained vo-   cabulary embeddings fixed . Note that this process   still requires backpropagating gradients through the   full language model . Like fine - tuning models , we   maximize the likelihood of generating the output   sequence z.   4 Results   In Table 1 , we report Overnight results across four   T5 model scales and two target representations .   In Table 2 , we add constrained decoding ( see Ap-   pendix A ) to our best performing T5 model and   compare against previously reported Overnight re-   sults . In Table 3 , we display the results of T5 - large   on the three different SPIS - splits of TOPv2 , and   include the BART - CopyPtr results from Chen et al .   ( 2020 ) . In Table 4 , we summarize the results of the   canonicalization ablation study for TOPv2.4.1 Prompt tuning vs fine tuning   We find that prompt tuning improves over fine-   tuning for all large model configurations and tar-   get representations . On Overnight , prompt tuned   denotation accuracy exceeds fine - tuned counter-   parts by up to 5 points with T5 - large and T5 - xl .   For T5 - small and T5 - base , prompt tuning remains   competitive ( within 1 % average accuracy ) with   fine - tuning when predicting canonical forms . On   TOPv2 , prompt tuning achieves an absolute im-   provement of 15 % mean accuracy over fine - tuning   on the lowest SPIS split . This performance dispar-   ity lessens when training data increases ; however ,   prompt tuned T5 - large continues to beat its fine-   tuned counterpart by 5 points at 500 SPIS and the   BART - CopyPtr model by 1.4 points .   Our prompt tuning models outperform previ-   ously reported results on these datasets . On   Overnight , our best model ‚Äî T5 - xl PT with canon-   ical representations and constrained decoding ‚Äî   outperforms the BART FT model of Shin et al .   ( 2021 ) by 5 accuracy points , and GPT-3 by more   than 2 points . On the 25 SPIS split of TOPv2 , we   see an average improvement of more than 5 points   compared to the BART - CopyPTR of Chen et al .   ( 2020 ) .   4.2 Canonical vs meaning representations   Our main finding is that prompt tuned T5 models   become better at generating meaning representa-   tions with increased model size . On Overnight , we   see the absolute difference between canonical and   meaning representations shrink from 17.5 points151   for T5 - small to 3.4 points for T5 - xl ( Table 1 ) . This   gap shrinks another 18 % to 2.8 points when we   apply constrained decoding to T5 - xl ( Table 2 ) . By   contrast , Shin et al . ( 2021 ) reports an 11.7 point   difference when prompting GPT-3 . For our fine-   tuning baselines , we observe a small performance   gap of 4 points across target representations for   BART and T5 - xl , while we observe no gap for   T5 - small , T5 - base , and T5 - large models .   In our TOPv2 experiments we find similar ev-   idence of large T5 model flexibility for generat-   ing sequences far from the training distribution .   In particular , for our most intrusive canonicaliza-   tion scheme Out - of - Vocab , which adds novel   tokens to the vocabulary and leaves these embed-   dings un - trained , we find no significant reduction   in performance for T5 - large across all data resource   levels . T5 - small , in comparison , sees almost a 50 %   drop in performance relative to no canonicaliza-   tion ( None ) at the 10 SPIS level and continues to   underperform by 33 % at the 500 SPIS level .   Interestingly , we find that In - Vocab drasti-   cally reduces performance for T5 - small at the 10   SPIS level‚Äî30.9 % vs. 43.4 % for None ‚Äî but   slightly outperforms it at 500 SPIS . We speculate   thatIn - Vocab effectively anonymizes the ontol-   ogy tokens , obscuring information that is useful   for prediction . In low - data regimes there is not   enough training data to learn the semantics of these   anonymized tokens , whereas with enough data this   problem vanishes .   5 Conclusion   We find that prompt tuning is an effective method   for adapting language models to the semantic pars-   ing task . Prompt tuning significantly outperforms   fine - tuning in low - data regimes , and remains com-   petitive in the fully supervised setting . We further-   more find that while canonicalizing meaning rep-   resentations can slightly improve performance , the   disparity between target representations decreases   when prompt tuning larger T5 models . This re-   sult differs from previous work ( Shin et al . , 2021 )   which suggested that pre - trained LMs are much   better equipped to output canonical than meaning   representations . However , a significant limitation   of prompt tuning is that it takes more time to con-   verge than fine - tuning . We believe one fruitful di-   rection for future research is to find ways to reduce   the compute required to prompt tune.1526Ethical Considerations and Limitations   There are two main limitations of this work . The   first is the limited analysis of the learned prompts .   While concurrent work has shown that interpreting   prompts is a difficult task , it is still an important   consideration and left for future work ( Khashabi   et al . , 2021 ) . Secondly , training prompts on mean-   ing representations requires substantially more   compute than fine - tuning . This may exacerbate   inequalities in regions where access to data and   compute are similarly limited ( Ahia et al . , 2021 ) .   References153154A Models   Here we provide all model details and hyperpa-   rameters to reproduce our results . We experiment   with BART and T5 ( Lewis et al . , 2020 ; Raffel   et al . , 2020 ) , two large pre - trained encoder - decoder   language models . BART is trained on the same   160 GB text dataset used to train RoBERTa ( Lewis   et al . , 2020 ) with a denoising objective . There are   two size configurations ( BART - base , BART - large )   and we experiment only with the 406 M parameter   BART - large on the Overnight dataset . T5 is trained   on the 750 GB C4 dataset ( Raffel et al . , 2020 ) with   a de - noising objective . We use the T5 - v1.1 check-   points from Lester et al . ( 2021 ) that were trained   for an additional 100 K steps with the Prefix - LM   objective . T5 - v1.1 has five configurations at vari-   ous scales : small , base , large , xl , xxl which have   60 M , 220 M , 770 M , 3B , and 11B parameters , re-   spectively . Here , we experiment with models up to   T5 - xl . All experiments were run with PyTorch ( v.   1.8.1 ) and the Huggingface Transformers ( v. 4.8.2 )   library ( Paszke et al . , 2019 ; Wolf et al . , 2020 ) .   Fine - tuning baseline We compare against base-   lines that fine - tune all parameters of BART and T5 .   We train the T5 models with AdaFactor ( Shazeer   and Stern , 2018 ) and BART with Adam ( Lewis   et al . , 2020 ; Kingma and Ba , 2015 ) . On TOPv2 ,   we use a learning rate of 10and batch size of   128 . On Overnight , we use a learning rate of 10   and a batch size of 64 across all sizes of T5 . On   both datasets , we train for 5000 epochs and perform   model selection by early stopping on the validation   set .   Prompt tuning We follow the prompt tuning pro-   cedure proposed by Lester et al . for T5 . We use 150   prompt tokens for all model sizes with a learning   rate of 0.3 optimized with AdaFactor . We train for   5000 epochs on most domains , although it some-   times took as many as 20000 epochs to converge on   the low - resource splits . Like the fine - tuned base-   line , we perform model selection with best exact   match accuracy on the validation set . We apply   the same method to BART and found that it did   not converge under a number of hyperparameter   configurations . We therefore exclude prompt tuned   BART models from our results . Constrained Decoding We implement grammar-   constrained decoding by building a prefix tree con-   taining all canonical or meaning representations in   the dataset as in Shin et al . ( 2021 ) . When doing   constrained decoding we perform a beam search   with 10 beams and use the prefix tree to look up   valid single token continuations of the decoded se-   quence .   B Results   For completeness , we provide all Overnight results   in Table 5 .   B.1 Training Times   Prompt tuned parameter efficiency comes at a   cost : we find that prompt tuning takes significantly   longer to train with early stopping than does fine-   tuning . On the Overnight dataset , fine - tuned mod-   els typically took 250 epochs before validation per-   formance plateaued . Our prompt tuned models   frequently took more than 1000 epochs when pre-   dicting canonical representations , and up to 5,000   when predicting meaning representations . In Fig-   ure 3 , we show example training curves for prompt   tuning and fine - tuning.155156   Inbal Magar Roy Schwartz   School of Computer Science and Engineering , The Hebrew University of Jerusalem , Israel   { inbal.magar,roy.schwartz1}@mail.huji.ac.il   Abstract   1 Introduction   Pretrained language models are getting bigger   and so does their capacity to memorize data   from the training phase ( Carlini et al . , 2021 ) . A   rising concern regarding these models is ‚Äú data   contamination‚Äù‚Äîwhen downstream test sets find   their way into the pretrain corpus . For instance ,   Dodge et al . ( 2021 ) examined five benchmarks and   found that all had some level of contamination in   the C4 corpus ( Raffel et al . , 2020 ) ; Brown et al .   ( 2020 ) flagged over 90 % of GPT-3 ‚Äôs downstream   datasets as contaminated . Eliminating this phe-   nomenon is challenging , as the size of the pretrain   corpora makes studying them difficult ( Kreutzer   et al . , 2022 ; Birhane et al . , 2021 ) , and even dedupli-   cation is not straightforward ( Lee et al . , 2021 ) . It   remains unclear to what extent data contamination   affects downstream task performance .   This paper proposes a principled methodology   to address this question in a controlled manner   ( Fig . 1 ) . We focus on classification tasks , where   instances appear in the pretrain corpus along with   their gold labels . We pretrain a masked language   modeling ( MLM ) model ( e.g. , BERT ; Devlin et al . ,   2019 ) on a general corpus ( e.g. , Wikipedia ) com-   bined with labeled training and test samples ( de-   noted seen test samples ) from a downstream task .   We then fine - tune the model on the same labeled   training set , and compare performance between   seen instances and unseen ones , where the latter   are unobserved in pretraining . We denote the differ-   ence between seen andunseen asexploitation . We   also define a measure of memorization by compar-   ing the MLM model ‚Äôs performance when predict-   ing the masked label for seen andunseen examples .   We study the connection between the two measures.157We apply our methodology to BERT - base and   large , and experiment with three English text clas-   sification and NLI datasets . We show that exploita-   tion exists , and is affected by various factors , such   as the number of times the model encounters the   contamination , the model size , and the amount of   Wikipedia data . Interestingly , we show that memo-   rization does not guarantee exploitation , and that   factors such as the position of the contaminated   data in the pretrain corpus and the learning rate   affect these two measures . We conclude that labels   seen during pretraining can be exploited in down-   stream tasks and urge others to continue developing   better methods to study large - scale datasets . As far   as we know , our work is the first work to study the   level of exploitation in a controlled manner .   2 Our Method : Assessing the Effect of   Contamination on Task Performance   To study the effect of data contamination on down-   stream task performance , we take a controlled ap-   proach to identify and isolate factors that affect this   phenomenon . We assume that test instances appear   in the pretrain corpus with their gold labels , and   that the labeled training data is also found in the   pretrain corpus . We describe our approach below .   We pretrain an MLM model on a general corpus   combined with a downstream task corpus , contain-   ing labeled training and test examples . We split the   test set into two , adding one part to the pretrain cor-   pus ( denoted seen ) , leaving the other unobserved   during pretraining ( unseen ) . For example , we add   the following SST-2 instance ( Socher et al . , 2013 ):   I love it ! 1   We then fine - tune the model on the same labeled   training set , and compare performance on the seen   andunseen test sets . As both test sets are drawn   randomly from the same distribution , differences   in performance indicate that the model exploits   the labeled examples observed during pretraining   ( Fig . 1 ) . This controlled manipulation allows us to   define two measures of contamination :   mem is a simple measure of explicit memoriza-   tion . We consider the MLM task of assigning thehighest probability to the gold label ( among the   candidate label set ) ; given the instance text ( e.g. , I   love it ! [ MASK ] ) .mem is defined as the dif-   ference in MLM accuracy by the pretrained model   ( before fine - tuning ) between seen andunseen .   mem is inspired by recent work on factual prob-   ing , which uses cloze - style prompts to asses the   amount of factual information a model encodes   ( Petroni et al . , 2019 ; Zhong et al . , 2021 ) . Similarly   to these works , mem can be interpreted as lower   bound on memorization of contaminated labels .   expl is a measure of exploitation : the difference   in task performance between seen andunseen .   mem andexpl are complementary measures for   the gains from data contamination ; mem is mea-   sured after pretraining , and expl after fine - tuning .   As we wish to explore different factors that influ-   enceexpl , it is also interesting to see how they af-   fectmem , particularly whether mem leads to expl   and whether expl requires mem . Interestingly , our   results indicate that these measures are not neces-   sarily tied .   Pretraining design choices Simulating language   model pretraining under an academic budget is not   an easy task . To enable direct comparisons between   different factors , we pretrain medium - sized models   ( BERT-{base , large } ) on relatively small corpora   ( up to 600 M tokens ) . We recognize that some of   the results in this paper may not generalize to larger   models , trained on more data . However , as data   contamination is a prominent problem , we believe   it is important to study its effects under lab condi-   tions . We hope to encourage other research groups   to apply our method at larger scales .   3 Which Factors Affect Exploitation ?   We study the extent to which pretrained models   can memorize and exploit labels of downstream   tasks seen during pretraining , and the factors that   affect this phenomenon . We start by examining   how many times a model should see the contami-   nated data in order to be able to exploit it .   We pretrain BERT - base on MLM using a com-   bined corpus of English Wikipedia ( 60 M tokens ) ,   and increasing numbers of SST-5 copies ( Socher   et al . , 2013 ) . To facilitate the large number of ex-   periments in this paper , we randomly downsample158   SST-5 to subsets of 1,000 training , seen andunseen   instances . We train for one epoch , due to the practi-   cal difference between the number of times the task   dataappears in the corpus and the number of times   the model sees it . For example , if a contaminated   instance appears in the corpus once , but the model   is trained for 50 epochs , then in practice the model   encounters the contaminated instance 50 times dur-   ing training . Further exploration of the difference   between these two notions is found in App . A. See   App . D for experimental details . We describe our   results below .   Exploitation grows with contaminated data du-   plicates Bothmem andexpl levels increase in   proportion to the contaminated data , reaching 60 %   mem and almost 40 % expl when it appears 200   times ( Fig . 2 , left ) . This suggests a direct connec-   tion between both mem andexpl and the number   of times the model sees these labels . This finding   is consistent with several concurrent works , which   show similar connections in GPT - based models .   These works study the impact of duplication of   training sequence on regeneration of the sequence   ( Carlini et al . , 2022 ; Kandpal et al . , 2022 ) , and the   effect on few - shot numerical reasoning ( Razeghi   et al . , 2022 ) . One explanation for this phenomenon   is the increase in the expected number of times la-   bels are masked during pretraining . To check this ,   we pretrain BERT - base with 100 copies of SST-5   and varying probabilities of masking the label . Our   results ( Fig . 2 , right ) show that the higher this prob-   ability , the higher mem andexpl values . These   results motivate works on deduplication ( Lee et al . ,   2021 ) , especially considering that casual language   models ( e.g. , GPT ; Radford et al . , 2019 ) are trained   using next token prediction objective , and so every   word in its turn is masked .   In the following , we fix the number of con-   taminated data copies to 100 and modify other   conditions ‚Äî the size of the Wikipedia data and the   model size ( base / large ) . We also experiment with   two additional downstream tasks : SST-2 and SNLI   ( Bowman et al . , 2015 ) . All other experimental de-   tails remain the same . Fig . 3 shows our results .   Memorization does not guarantee exploitation   Perhaps the most interesting trend we observe is   the connection between mem andexpl . Low mem   values ( 10 % or less ) lead to no expl , but higher   mem values do not guarantee expl either . For ex-   ample , training BERT - base with 600 M Wikipedia   tokens and SST-5 data leads to 15 % mem level , but   less than 1 % expl . These results indicate the mem   alone is not a sufficient condition for expl .   Model and corpus sizes matter Across all three   datasets and almost all corpora sizes , mem levels   of BERT - large are higher then BERT - base . This   is consistent with Carlini et al . ( 2021 ) ‚Äôs findings   that larger models have larger memorization ca-   pacity . Also , we observe that mem levels ( though   not necessarily expl ) of SST-5 are consistently   higher compared to the other datasets . This might   be due to the fact that it is a harder dataset ( a 5 - label   dataset , compared to 2/3 for the other two ) , with   lower state - of - the - art results , so the model might   have weaker ability to capture other features .   Much like memorization , exploitation is also   affected by the size of the model , as well as the   amount of additional clean data . We observe   roughly the same trends for all three datasets , but   not for the two models . For BERT - base , 2‚Äì6 %   expl is found for low amounts of clean data , but159   gradually decreases . For BERT - large , the trend is   opposite : expl is observed starting 300 M and con-   tinues to grow with the amount of external data , up   to 2‚Äì4 % . This indicates that larger models benefit   more from additional data .   We next explore other factors that affect expl .   Unless stated otherwise , we use BERT - base ( 60 M   Wikipedia tokens , 100 copies of SST-5 ) .   Early contamination leads to high exploitation   Does the position of the contaminated data in the   pretraining corpus matter ? To answer this , we pre-   train the model while inserting contaminated data   in different stages of pretraining : at the beginning   ( in the first third ) , the middle , or the end . Our re-   sults ( Fig . 4 , left ) show that early contamination   leads to high expl ( up to 17 % ) , which drops as   contamination is introduced later . In contrast , the   highest mem levels appear when contamination is   inserted in the middle of the training . We also ob-   serve that in early contamination mem levels are   lower thenexpl . This is rather surprising , since   the model has certain level of memorization of the   labels ( as expressed by expl ) , but it does not fully   utilize these memories in the MLM task of mem .   This suggests that in early contamination , the lower   bound that mem yields on memorization is not tight .   The model might have an ‚Äú implicit ‚Äù memories of   the labels , which are not translated to gains in the   MLM task of predicting the gold label ( mem ) . Dis-   tinguishing between implicit and explicit memory   of LMs is an important question for future work .   We note that different stages of training also   yield different learning rates ( LRs ) . In our exper-   iments we follow BERT , using linear LR decay   with warmup . We might expect instances observed   later , with lower LR , to have a smaller affect on the   model ‚Äôs weights , thus less memorized . Fig . 4 ( left )   indeed shows that late contamination leads to no   expl ( though mem levels remain relatively high ) .   To separate the LR from the contamination timing ,   we repeat that experiment with a constant LR of   2.77e-5 ( midway of the linear decay ) . Fig . 4 ( right )   shows that in the last stage , both measures increase   compared to the LR decay policy . As the LR is con-   stant , this indicates that both LR and contamination   timing might affect label memorization .   Large batch size during pretraining reduces ex-   ploitation Similar to learning rate , the batch size   can also mediate the influence that each instance   has on the models weights . We pretrain BERT - base   several times with increasing batch sizes . Our   experiments show that as we decrease the batch   size , both measures increases ( Fig . 5 ) . In the ex-   treme case of batch size=2 , mem reaches 49 % , and   expl reaches 14 % . This phenomenon might be   explained by each training instance having a larger   impact on the gradient updates with small batches .   A good initialization matters Carlini et al .   ( 2019 ) showed that memorization highly depends   on the choice of hyperparameters . We observe a   similar trend ‚Äî expl depends on the random seed   used during fine - tuning . These results are also   consistent with prior work that showed that fine-   tuning performance is sensitive to the selection of   the random seed ( Dodge et al . , 2020 ) . Careful in-   vestigation reveals that some random seeds lead   to good generalization , as observed by unseen per-   formance , while others lead to high exploitation :   When considering the top three seeds ( averaged   across experiments ) for expl ‚Äî two out of those   seeds are also in the worst three seeds for general-   ization . This indicates a tradeoff between general-   ization and exploitation . Future work will further160study the connection between these concepts . To   support such research , we publicly release our ex-   perimental results .   4 Related Work   Memorization in language models has been ex-   tensively studied , but there is far less research on   data contamination and the extent models exploit   the contamination for downstream tasks . Most re-   lated to our work is Brown et al . ( 2020 ) ‚Äôs post - hoc   analysis of GPT-3 ‚Äôs contamination . They showed   that in some cases there was great difference in   performance between ‚Äò clean ‚Äô and ‚Äò contaminated ‚Äô   datasets , while in others negligible . However , they   could not perform a controlled experiment due to   the high costs of training their models . As far as   we know , our work is the first work to study the   level of exploitation in a controlled manner .   Several concurrent works explored related ques-   tions on memorization or utilization of training in-   stances . These works mostly use GPT - based mod-   els . Carlini et al . ( 2022 ) showed that memorization   of language models grows with model size , training   data duplicates , and the prompt length . They fur-   ther found that masked language models memorize   an order of magnitude less data compared to causal   language model . This finding hints that exploita-   tion levels might be even higher on the latter . Kand-   pal et al . ( 2022 ) showed that success of privacy at-   tacks on large language models ( as the one used in   Carlini et al . , 2021 ) is largely due to duplication in   commonly used web - scraped training sets . Specif-   ically , they found that the rate at which language   models regenerate training sequences is superlin-   early related to a duplication of the sequence in the   corpus . Lastly , Razeghi et al . ( 2022 ) examined the   correlations between model performance on test   instances and the frequency of terms from those in-   stances in the pretraining data . They experimented   with numerical deduction tasks and showed that   models are consistently more accurate on instances   whose terms are more prevalent .   5 Discussion and Conclusion   We presented a method for studying the extent   to which data contamination affects downstream   fine - tuning performance . Our method allows to   quantify the explicit memorization of labels fromthe pretraining phase and their exploitation in fine-   tuning . Recent years have seen improvements in   prompt - based methods for zero- and few - shot learn-   ing ( Shin et al . , 2020 ; Schick and Sch√ºtze , 2021 ;   Gu et al . , 2021 ) . These works argue that masked   language models have an inherent capability to per-   form classification tasks by reformulating them as   fill - in - the - blanks problems . We have shown that   given that the language model has seen the gold   label , it is able to memorize and retrieve that label   under some conditions . Prompt - tuning methods ,   which learn discrete prompts ( Shin et al . , 2020 ) or   continuous ones ( Zhong et al . , 2021 ) , might latch   on to the memorized labels , and further amplify   this phenomenon . This further highlights the im-   portance of quantifying and mitigating data con-   tamination .   Acknowledgements   We wish to thank Yarden Shoham Tal , Michael   Hassid , Yuval Reif , Deborah Elharar , Gabriel   Stanovsky and Jesse Dodge for their feedback and   insightful discussions . We also thank the anony-   mous reviewers for their valuable comments . This   work was supported in part by the Israel Science   Foundation ( grant no . 2045/21 ) and by a research   gift from the Allen Institute for AI .   References161162   A Two Notions of ‚Äú Occurences ‚Äù   As noted in Sec . 3 , the number of times an instance   appears in the corpus is a different notion than the   number of times the model sees it during training .   The latter also takes into account the number of   training epochs . For example , if an instance ap-   pears in the corpus once , but the model is trained   for 50 epochs , than practically the model sees it 50   times . In the field on memorization and data con-   tamination , it is mostly common to report the num-   ber of times an instance appears in the corpus ( Car-   lini et al . , 2021 ; Brown et al . , 2020 ) . However , the   following experiments emphasizes the importance   of accounting for the number of times a sample is   seen . In the first experiment we fix the number of   times the contamination appears in the corpus ( 10   copies ) , and change the number of times it is seen .   We do so by performing second - stage - pretraining   ( Gururangan et al . , 2020 ; Zhang and Hashimoto ,   2021 ) on a combined corpus of Wikipedia and 10   copies of SST-5 . We train one model for one epoch ,   and the other for 5 epochs . Results are shown in   Tab . 1 . In the second experiment we fix the number   of times the model seesSST-5 , and change the num-   ber of times it appears in the corpus . We do so by   performing second - stage - pretraining for one epoch   on a combined corpus of Wikipedia and changing   number of copies of SST-5 . Results are shown in   Tab . 2 .   We observe that expl levels of the models   which saw the contamination 50 times are rather   similar . On the contrary , expl levels of the model   which saw the data 10 times is 5 % lower . These   results indicate the number of times contamina-   tion is seen during training have great influence on   expl . In the main experiments presented in this   paper we train for one epoch in order to eliminate   the difference between the two notion ( appears vs.   seen ) .   B Same Ratio , Different expl   In Sec . 3 we have seen the expl andmem grows   with the number of contamination occurrences in   the corpus . One explanation for the results in is that   the rising ratio between the contaminated corpus   and the full corpus leads to increased mem . We163conduct experiments in which we keep the ratio   between the two fixed while increasing their abso-   lute sizes . We keep constant ratio of 1:10 between   the number of instances ( in Wikipedia set we con-   sider lines as instances ) in the datasets . To do so ,   we adjust both the size of Wikipedia and the du-   plications of SST-5 train and seen test sets in the   corpus . For example , to achieve total corpus sized   1 M we use 9k instances from Wikipedia and 50   copies of SST-5 ( which yields 1k samples ) . We   focus on BERT - base and SST-5 task and follow   the basic experiment setup and hyperparameters of   our main experiments ( Sec . 3 ) . Our results ( Fig . 7 )   show that this manipulation leads to increased mem ,   indicating the importance of the total number of   occurrences of the task data .   C Position of Contamination Matters   We pretrain BERT - base model while inserting con-   taminated data in different stages of pretraining .   We discuss the experiment in Sec . 3 . Results on   SST-2 and SNLI can be found in Fig . 6 .   D Experimental Details   Originally , BERT model was trained on Masked   Language Modelling ( MLM ) task and Next Sen-   tence Prediction task ( NSP ; Devlin et al . , 2019 ) .   However , Liu et al . ( 2019 ) showed that removing   the NSP loss does not impact the downstream task   performance substantially . Therefore we pretrain   both BERT models ( -base and -large , both uncased )   on the MLM task only .   Wikipedia Data We extracted and pre-   processed the April 21 ‚Äô English Wikipedia dump .   We used the wikiextractor tool ( Attardi , 2015 ) . In   order to measure the effect of contamination when   contaminated data is shuffled across the pretraining   corpus , we divided clean Wikipedia text into lines   ( instances which were originally separated by new   line symbol ) .   Experimental Details for Sec . 3 All models   were trained with the following standard proce-   dure and hyperparameters . Specific experimental   adjustments will be discussed later . We pretrained   BERT models using huggingface ‚Äôs ( Wolf et al . ,   2020 ) run_mlm script for masked language model-   ing . We used heads sized 64 ( calculated as : hidden   dimension divided by the number of heads ) with   standard architecture as implemented in transform-   ers library . We used a combined corpus of 60 M   tokens of Wikipedia along with 100 copies of the   downstream corpus . Due to computational limi-   tations , we limited the training sequences to 128   tokens . We pretrained for 1 epoch and used batch   size of 32 to fit on 1 GPU . We trained with a learn-   ing rate of 5e-5 . We apply linear learning rate   warm up for the first 10 % steps of pretraining and   linear learning rate decay for the rest . We fine - tune   the models on 1,000 samples of the downstream   corpora ( SST-2 , SST-5 and SNLI ) .   We fine - tune for 3 epochs using batch size of   8 . We use AdamW ( Loshchilov and Hutter , 2019 )   optimizer with learning rate of 2e-5 and default pa-   rameters : Œ≤= 0.9 , Œ≤= 0.999 , œµ= 1e-6 , with bias   correction and without weight decay . We average   the results over ten random trials . As baselines we   use pretrained BERT - base and BERT - large and fine-   tune them as described above . Accuracy results on164theunseen test sets are shown in Tab . 3 .   In the experiment of contamination in different   stages of training , we divided the entire corpus   ( clean and contaminated ) into 3 equal size sections ,   making sure that all the contaminated data appears   entirely in one of those sections . We disabled the   random sampler and shuffled each section individu-   ally . We refer to the sections as ‚Äò first ‚Äô , ‚Äò middle ‚Äô and   ‚Äò last ‚Äô according to the order they appear in train-   ing . All our experiments were conducted using the   following GPUs : RTX 2080Ti , Quadro RTX 6000 ,   A10 and A5000 .   Experimental Details for App . A We conducted   second - stage - pretraining by continuing to update   BERT - base weights . We used batch size of 32 and   learning rate of 5e-5 . Learning rate scheduling , op-   timization and fine - tuning are the same as standard   procedure described above.165   Ling Liu and Mans Hulden   University of Colorado   first.last@colorado.edu   Abstract   1 Introduction   Deep learning models have been responsible for   state - of - the - art performance in many tasks involv-   ing morphological generation and analysis ( Devlin   et al . , 2019 ; Raffel et al . , 2019 ; Cotterell et al . ,   2016 ; Vylomova et al . , 2020 ) . However , to reach   adequate performance , large amounts of labeled   examples are usually required for training ( Cot-   terell et al . , 2017 ; Silfverberg et al . , 2017 ; Liu   and Hulden , 2021b ) . Annotation of morpholog-   ical data is particularly expensive since it requires   both domain and language expertise ( McCarthy   et al . , 2020 ) . Manual correction and quality control   of annotated data adds to the cost ( van Halteren ,   2000 ) . In light of this , we evaluate the feasibility ofusing a deep learning model to automatically detect   annotation errors with the goal of reducing the cost   of annotation correction and quality control .   Earlier work on annotation error detection has   largely been non - neural and focused on other types   of annotation , such as part - of - speech ( POS ) tag-   ging ( van Halteren , 2000 ; Kv ÀòetoÀán and Oliva , 2002 ;   Dickinson and Meurers , 2003 ; Loftsson , 2009 ) ,   syntactic parsing ( Eskin , 2000 ; Ambati et al . , 2011 ) ,   or semantic labeling ( Dickinson and Lee , 2008 ) .   A neural model error detector ‚Äî an LSTM - based   tagger ‚Äî has been used by Rehbein and Ruppen-   hofer ( 2017 ) to detect POS tagging errors .   In this paper , we propose a method to apply a   Transformer model ( Vaswani et al . , 2017 ) to detect   annotation errors in morphological data . In order   to evaluate the method , we simulate errors by in-   troducing artificial perturbations to our annotated   data , which are generated in three different ways   to simulate different types of annotation errors . Ex-   perimental results show that the Transformer model   can detect annotation errors in morphological data   very effectively , even when the datasets contain a   high percentage of erroneous forms .   2 Experiments   2.1 Data   We use data from four languages in the UniMorph   project ( Kirov et al . , 2018 ) for experiments . The   data has been vetted and used in multiple SIG-   MORPHON shared tasks ( Cotterell et al . , 2016 ,   2017 , 2018 ; McCarthy et al . , 2019 ; Vylomova et al . ,   2020 ) . Therefore , we expect very few erroneous   entries in this dataset . The data is organized into   inflection tables where each slot in an inflection   table is given as a tab - separated ( lemma , inflected   form , morphosyntactic tag ) triple , as shown in the   left chart in Figure 1 .   Language choice The four languages ‚Äî Finnish ,   German , Russian and Spanish ‚Äî represent differ-166   ent morphological complexities and challenges .   German and Russian nouns have relatively small   paradigm sizes , while Spanish and Finnish verbs   have large paradigms ; the paradigm size of Finnish   nouns and German verbs is somewhere in between .   Finnish has an agglutinative inflectional system   with a large paradigm size , especially for verbs .   Though German inflection tables are not particu-   larly large , characteristic of the language are the   many cases of syncretism in each inflection table .   Spanish verbs have a large paradigm size , but the in-   flection is quite regular . Russian has a fusional mor-   phological system and is written in Cyrillic script   whereas the other three languages use Latin script .   An additional reason for our particular choice of   languages has been to provide a range of difficulty   for neural models ‚Äî German has consistently been   among the most difficult languages to inflect in the   SIGMORPHON shared tasks ; Finnish and Russian   have been of intermediate difficulty , and Spanish   has been consistently ‚Äò easy ‚Äô . Further , by limiting   ourselves to languages that have been used in mul-   tiple shared tasks , we assure ‚Äî importantly ‚Äî that   the gold data for our experiments is itself largely   error - free , something which is not obviously the   case for many other languages in UniMorph .   2.2 Experiment setup   Inflection model The Transformer ( Vaswani   et al . , 2017 ) is the current state - of - the - art model ar-   chitecture for morphological inflection generation ,   even when the amount of training data is limited   ( Vylomova et al . , 2020 ; Liu and Hulden , 2020a , b ,   2021a , b ; Moeller et al . , 2020 , 2021 ; Wu et al . , 2021 ;   Liu , 2021 ) ; we therefore adopt this architecure in   all experiments .   Applying the Transformer to detect morpholog-   ical data errors The core intuition behind our   error detection model is that we train inflection gen-   eration models on a subset of the inflected forms   in our total dataset , and then apply these models167   to generate precisely those inflected forms that the   inflection models have notbeen trained on . If a   model ‚Äôs prediction for these forms disagrees withthe corresponding held - out annotated form , we flag   that particular annotated form as a potential error.168Preliminary experiment and data split   Throughout our experiments , we use complete   inflection tables for our labeled data . Moreover ,   the dataset is a small subset of the UniMorph   tables , ranging from 70tables ( Spanish verbs ) to   240(Russian nouns ) . The reason for limiting the   data is twofold . First , we want to ensure that error   detection is feasible with datasets significantly   smaller than large projects such as UniMorph .   Secondly , before our actual error detection   experiment , we want to verify that the Transformer   model is powerful enough to reconstruct , with high   accuracy , single unseen ( or potentially erroneous )   forms in the data .   We use a leave- n - out cross - validation setup   to split the data for training and evaluating the   model before attempting to perform error detec-   tion . Specifically , as illustrated in Figure 1 , we   systematically leave one slot out in each inflection   table for evaluation and use the remaining slots to   train one particular inflection model . For each such   model , we rotate which slot is left out . The number   of models we train for each POS of a language   is thus the same as the corresponding paradigm   size , m. The evaluation data size for each model is   n , the same as the number inflection tables in the   data , and the training data size for each model is   m√ón‚àín . Each model is thus trained to make pre-   dictions for slots it has not witnessed ‚Äî one missing   slot per table ‚Äî and the union of all models ‚Äô predic-   tions cover all the slots . Table 1 shows the accuracy   when using the mmodels to perform an artificial   reconstruction of ‚Äú unseen forms ‚Äù . For example , we   trainm= 8inflection models for German nouns ,   each model is trained on 1,120 ( 8√ó160‚àí160 )   slots and evaluated on n= 160 slots .   Generating artificial errors We now simulate   noisy annotation data by injecting artificial errors   into the above dataset in three different ways before   training models . The first method generates artifi-   cial errors ( Artificial Error I ) to mimic typographic   errors by inserting , replacing or deleting a single   character in an inflected word form . The second   error model simulates annotator confusion by swap-   ping two randomly sampled slots with different in-   flected forms in a randomly chosen inflection table ,   denoted as Artificial Error II . The third type of ar-   tificial error , Artificial Error III , is self - adversarial   to generate plausible - looking noise : we first train a   single Transformer inflection model with the com-   plete data for each POS of a language , then applyit to predict inflected forms for slots it hasbeen   trained on . We use beam search at decoding time   andpick out the second best ( but erroneous ) pre-   diction to represent a noisy inflected form . This   self - adversarial approach gives us incorrect word   forms which are however very close to the ground   truth inflected word forms . We hypothesize that   such errors are more difficult to identify than the   others .   Erroneous inflected forms of each type are intro-   duced to the original data at different error rates :   0.5 % , 1 % , 5 % , 15 % , 20 % , 25 % and 30 % ( of all   forms ) .   Evaluation metrics We evaluate the error detec-   tion model w.r.t . accuracy , i.e. the ratio of correctly   predicted forms vs. all predicted forms and also   precision , recall , and F1 - score .   3 Results and Discussion   Figure 2 provides a summary of the experiment   results , plotting the accuracy , precision , recall , and   F1 - score for each POS of each language , averaged   across the mmodels after adding Artificial Errors   I , II , III at different amounts , respectively . Detailed   numbers are provided in Table 2 in the appendix .   We observe that the accuracy of the model de-   creases as more word erroneous forms are added ,   but is still high overall . This indicates that the leave-   n - outtraining strategy is robust to noise in the data .   For every type of artificial error , the recall is 1.0   or very close to 1.0after varying amounts of noise   is injected . In other words , the model can identify   all , or nearly all the artificial errors we introduce ,   even when a large amount of noise is mixed into   the gold data . The precision increases ( from a low   of0.11to a high of 0.95 ) as more errors are added ,   indicating that a reasonably small amount of false   positives would be produced by the model . ( See   Table 3 in the appendix for detailed counts . )   As such , if an annotator were to manually cor-   rect the forms flagged by the model , all erroneous   annotations would be corrected and the annotator   should not be frustrated by vetting a large number   of already - correct annotations . To illustrate this ,   consider the average precision ( 0.43 ) for all six   datasets with Artificial Error type I ( typos ) where   1 % of the forms are corrupted ‚Äî a plausible sce-   nario in an annotation project . Under such assump-   tions , our model would present flagged forms in   a dataset for vetting to an annotator , and , indeed ,   nearly half of these flagged forms would be true169errors , and no errors would be undetected ( since   the recall is 1.0 ) .   However , we observe that the worst case ( e.g.   lowest F1 scores on average ) where the annotation   error detection model performs is the second type   of artificial error . In this type of error , we consis-   tently switched a portion of slots . The worst error   detection model performance on this type of er-   ror points to the limitation of the annotation error   detection method we propose : it can not detect con-   sistent errors if the errors in question are present   in a large portion of the data ; for example , in the   extreme case that all the forms in the paradigm   carry the same error , it is impossible for the in-   flection model to learn the ground - truth inflection .   Another shortcoming of our proposed approach is   that it requires relatively complete inflection tables ,   which are expensive to annotate as to expertise and   effort . Future work is needed to evaluate whether   the method works when there are slots missing in   most inflection tables .   4 Conclusion   In this work , we propose a method to leverage the   Transformer model architecture for annotation er-   ror detection in morphological data . We propose to   systematically leave out one slot in each morpho-   logical inflection table as the data to be detected   and use such subsets of annotated data to train in-   dividual Transformer inflection models ‚Äî one for   each group of missing slots ‚Äî and then apply the   inflection models to make predictions for the held-   out slots . If the predicted form disagrees with the   actual annotation ( a form the predicting model has   not seen ) , the model flags that form as erroneous .   To check efficiency , we evaluate the model un-   der three different scenarios where we inject arti-   ficial errors into gold data , simulating noisy data   resulting from an annotation process : typographic   errors generated by inserting , replacing or deleting   a single character in an inflected word form ; er-   rors resulting from annotator confusion where two   slots in an inflection table are swapped ; and self-   adversarial errors where erroneous but plausible   predictions generated by the Transformer inflec-   tion model are introduced . Our experiments on   four languages with different morphological char-   acteristics and levels of irregularity indicate that   the proposed method can detect every type of error   in morphological datasets very effectively . Even   when large portions of the data ( 5 % to 30 % ) havebeen replaced with corrupted forms , our model re-   tains perfect , or near - perfect , recall and also shows   increasingly higher precision as more erroneous   forms are present .   The results show that the Transformer model can   detect various kinds of errors without producing   excessive false positive predictions . We believe   such a model can directly be incorporated into the   correction and quality control process of morpho-   logical data annotation projects , specifically for   low - resource language where datasets are in the   early stages of development and few annotators are   available . Further research should investigate how   well this basic method of error detection works in   other linguistic annotation domains .   References170171172A Detailed experiment results173174   Aryaman Arora Clara Meister Ryan CotterellGeorgetown University ETH Z√ºrich   aa2190@georgetown.edu { clara.meister,ryan.cotterell}@inf.ethz.ch   Abstract   1 Introduction   There is a natural connection between informa-   tion theory , the mathematical study of communica-   tion systems , and linguistics , the study of human   language ‚Äî the primary vehicle that humans em-   ploy to communicate . Researchers have exploited   this connection since information theory ‚Äôs incep-   tion ( Shannon , 1951 ; Cherry et al . , 1953 ; Harris ,   1991 ) . With the advent of modern computing , the   number of information - theoretic linguistic studies   has risen , exploring claims about language such   as the optimality of the lexicon ( Piantadosi et al . ,   2011 ; Pimentel et al . , 2021 ) , the complexity of   morphological systems ( Cotterell et al . , 2019 ; Wu   et al . , 2019 ; Rathi et al . , 2021 ) , and the correla-   tion between surprisal and language processing   time ( Smith and Levy , 2013 ; Bentz et al . , 2017 ;   Goodkind and Bicknell , 2018 ; Cotterell et al . , 2018 ;   Meister et al . , 2021 , inter alia ) .   In information - theoretic linguistics , a fundamen-   tal quantity of research interest is entropy . Entropy   is both useful to linguists in its own right , and is   necessary for estimating other useful quantities ,   e.g. , mutual information . However , the estimation   of entropy from raw data can be quite challeng-   ing ( Paninski , 2003 ; Nowozin , 2015 ) , e.g. , in ex-   pectation , the plug - in estimator underestimates en-   tropy ( Miller , 1955 ) . Linguistic distributions often   present additional challenges . For instance , many   linguistic distributions , such as the unigram distri-   bution , follow a power law ( Zipf , 1935 ; Mitzen-   macher , 2004).Linguistics is not the only field   with such nuances , and so a large number of en-   tropy estimators have been proposed in other fields   ( Chao and Shen , 2003 ; Archer et al . , 2014 , inter   alia ) . However , no work to date has attempted a   practical comparison of these estimators on natural   language data . This work fills this empirical void .   Our paper offers a large empirical comparison of   the performance of 6 different entropy estimators175on both synthetic and natural language data , an ex-   ample of which is shown in Figure 1 . We find that   Chao and Shen ‚Äôs ( 2003 ) is the best estimator when   very few data are available , but Nemenman et al . ‚Äôs   ( 2002 ) is superior as more data become available .   Both are significantly better ( in terms of mean-   squared error ) than the na√Øve plug - in estimator . Im-   portantly , we also show that two recent studies   ( Williams et al . , 2021 ; McCarthy et al . , 2020 ) show   smaller effect sizes when a better estimator is em-   ployed ; however , we are able to reproduce a signifi-   ca nt effect in both replications . We recommend that   future studies carefully consider their choice of en-   tropy estimators , taking into account data availabil-   ity and the nature of the underlying distribution .   2 Entropy and Language   Shannon entropy is a quantification of the uncer-   tainty in a random variable . Given a ( discrete )   random variable Xwith probability distribution   poverKpossible outcomes X={x } , the   Shannon entropy of Xis defined as   H(X ) = H ( p)=‚àíXp(x ) logp(x)(1 )   Entropy has many uses throughout science and en-   gineering ; for instance , Shannon ( 1948 ) originally   proposed entropy as a lower bound on the com-   pressibility of a stochastic source .   Yet the application of information - theoretic tech-   niques to linguistics is not so straightforward :   Information - theoretic measures are defined over   probability distributions and , in the study of nat-   ural language , we typically only have access to   samples from the distribution of interest , e.g. , the   phonotactic distribution in English , which permits   word we can not find in a corpus , like blick , rather   than the true probabilities required in the computa-   tion of Eq . ( 1 ) . Indeed , it is often the case that not   all elements of Xare even observed in available   data ‚Äî such as words that were coined after the a   corpus was collected .   Rather , pmust be approximated in order to es-   timate H(p ) . One solution is plug - in estimation :   Given samples from p , the maximum - likelihood es-   timate for pis ‚Äú plugged ‚Äù into Eq . ( 1 ) . However , as   originally noted by Miller ( 1955 ) , this strategy gen-   erally yields poor estimates . It is thus necessaryto derive more nuanced estimators .   3 Statistical Estimation Theory   Statistical estimation theory provides us with the   tools for estimating various quantities of interest   based on samples from a distribution .   Central to this theory is the estimator : A   statistic that approximates a property of the distri-   bution our data is drawn from . More formally , let   D={ex}be samples from an unknown dis-   tribution p. Suppose we are interested in a quantity   Œ∏that can be computed as a function of the distribu-   tionp . An estimator bŒ∏(D)forŒ∏is then a function   of the data Dthat provides an approximation of Œ∏ .   Two properties of an estimator are often of   interest : bias ‚Äî the difference between the true   value of Œ∏and the expected value of our estimator   bŒ∏(D)under p ‚Äî and variance ‚Äî how much bŒ∏(D )   fluctuates from sample set to sample set :   bias(bŒ∏(D))=E[bŒ∏(D)]‚àíŒ∏ ( 2 )   var(bŒ∏(D))=E[(bŒ∏(D)‚àíE[bŒ∏(D)])](3 )   It is desirable to construct an estimator that has   both low bias and low variance . However , the   bias ‚Äì variance trade - off tells us that we often have   to pick one , and we should focus on a balance   between the two . This trade - off is evinced through   mean - squared error ( MSE ) , a metric oft - employed   for assessing estimator quality :   MSE(bŒ∏(D ) ) = bias ( bŒ∏(D))+ var(bŒ∏(D))(4 )   To recognize the trade - oft note that , for any fixed   MSE , a decrease in bias must be compensated with   an increase in variance and vice versa . Indeed , it   is important to recognize that there is typically no   single estimator that is seen as ‚Äú best . ‚Äù Different   estimators balance the bias ‚Äì variance trade - off   differently , making their perceived quality specific   to one ‚Äôs use - case . Importantly , the effectiveness   of an estimator also depends on the domain of in-   terest . Consequently , an empirical study of various   entropy estimators , which this paper provides , is   necessary in order to determine which entropy es-   timators are best suited for linguistic distributions .   3.1 Plug - in Estimation of Entropy   A simple , two - step approach for estimating entropy   isplug - in estimation . In the first step , we compute   the maximum - likelihood estimate for pfrom our176dataset Das follows   bp(x)=P 1{ex = x }   N(5 )   In the second step , we plug Eq . ( 5 ) into Eq . ( 1 )   directly , which results in the estimator bH(D ) .   So why is this a bad idea ? While our probabil-   ity estimates themselves are unbiased , entropy is   a concave function . Consequently , by Jensen ‚Äôs in-   equality , this estimator is , in expectation , a lower   bound on the true entropy ( see App . E.1 for proof ) .   Moreover , when N‚â™K , which is often the case   in power - law distributed data , the estimate becomes   quite unreliable ( Nemenman et al . , 2002 ) .   3.2 An Ensemble of Entropy Estimators   MM ‚Äî Miller ( 1955 ) and Madow ( 1948 ) . The   first innovation in entropy estimation known to the   authors is a simple fix derived from a first - order   Taylor expansion of MLE ( described above ) . The   Miller ‚Äì Madow estimator only involves a simple   additive correction , which is shown below :   bH(D)=bH(D ) + K‚àí1   2N(6 )   where Kis size of the support of X. The Miller ‚Äì   Madow correction should seem intuitive in that we   add‚â•0to compensate for the negative bias   of the estimator . A full derivation of the Miller ‚Äì   Madow estimator is given in Proposition 2 .   JACK ‚Äî Zahl ( 1977 ) . Next we consider the jack-   knife , which is a common strategy used to correct   for the bias of statistical estimators . In the case   of entropy estimation , we can apply the jackknife   out of the box to correct the bias inherent in the   MLE estimator . Explicitly , this is done by averag-   ing plug - in entropy estimates bH(D)albeit with   thensample from the data removed ; we denote   this held - out plug - in estimator as bH(D ) . Aver-   aging these ‚Äú held - out ‚Äù plug - in estimators results in   the following simple entropy estimator   bH(D)=NbH(D)‚àíN‚àí1   NXbH(D )   ( 7 )   Note that the jackknife is applicable to any estima-   tor , not just bH(D ) , and , thus , can be combined   with any of the other approaches mentioned .   HT ‚Äî Horvitz and Thompson ( 1952 ) . Horvitz ‚Äì   Thompson is a general scheme for building estima-   tors that employs importance weighting in order tomore efficiently estimate a function of a random   variable . Importantly , this estimator gives us the   ability to compensate for situations where the prob-   ability of an outcome is so low that it is often not   observed in a sample , which is often the case for   e.g. , power - law distributions .   While a full exposition of HT estimators is out-   side of the scope of this work , in essence , we can   divide the expected probability of a class by each   class ‚Äôs estimated inclusion probability to compen-   sate for such situations . Given the true proba-   bility of an outcome p(x ) , the probability that   it occurs at least once in a sample of size Nis   1‚àí(1‚àíp(x ) ) . The HT estimator for entropy   is then defined as   bH(D)=‚àíXbp(x ) logbp(x )   1‚àí(1‚àíbp(x))(8 )   using our MLE probability estimates bp(x ) .   CS ‚Äî Chao and Shen ( 2003 ) . Chao ‚Äì Shen mod-   ifies HT by multiplying the MLE probability esti-   mates by an estimate of sample coverage . Formally ,   letfbe the number of observed singletonsin   sample ; our sample coverage can be estimated as   bC= 1‚àí. The CS estimator is then computed as :   bH(D)=‚àíXbC¬∑bp(x ) logbC¬∑bp(x )   1‚àí(1‚àíbC¬∑bp(x ) )   ( 9 )   In the case that f = N , we set f = N‚àí1to   ensure the estimated entropy is not 0 .   WW ‚Äî Wolpert and Wolf ( 1995 ) . One family of   entropy estimators in information theory is based   on Bayesian principles . The first of these was the   Wolpert ‚Äì Wolf estimator , which uses a Dirichlet   prior ( with concentration parameter Œ±and a uni-   form base distribution ) . This Bayesian estimator   has a clean , closed form :   bH(D |Œ±)=œà   eA+ 1   ‚àíXeŒ±   eAœà(eŒ±+ 1 )   ( 10 )   whereeŒ± = c(x ) + Œ±(for the histogram count   c(x)of class kin the sample ; this is analogous to   Laplace smoothing ) , eA = PeŒ± , and œàis the   digamma function . A full derivation of Eq . ( 10 ) is   given in Proposition 3 . Unfortunately , Eq . ( 10 ) is177   very dependent on the choice of Œ± : For large K , Œ±   almost completely determines the final entropy esti-   mate , an observation first made by Nemenman et al .   ( 2002 ) which motivated their improved estimator   described below .   NSB ‚Äî Nemenman et al . ( 2002 ) . Nemenman   et al . ( NSB ) attempt to alleviate the Wolpert ‚Äì Wolf   estimator ‚Äôs dependence on Œ± . They take Œ± = Œ±¬∑1 ,   enforcing that the Dirichlet prior is symmetric , and   develop a hyperprior over Œ±that results in a near-   uniform distribution over entropy . The hyperprior   is given by   p(Œ±)=Kœà(KŒ±+ 1)‚àíœà(Œ±+ 1 )   logK(11 )   where œàis the trigamma function . A full deriva-   tion of Eq . ( 11 ) is given in Proposition 4 . This   choice of hyperprior mitigates the effect that the   chosen Œ±has on the entropy estimate . Nemenman   et al . ‚Äôs ( 2002 ) entropy estimator is then the pos-   terior mean of the Wolpert ‚Äì Wolft estimator taken   under p :   bH(D ) = ZbH(D |Œ±¬∑1)p(Œ± ) dŒ±   ( 12 )   Typically , numerical integration is used to quickly   compute the unidimensional integral .   4 Experiments   Here we provide an evaluation of the entropy   estimators presented in ¬ß 3.2 on linguistic data .   4.1 Entropy of the Unigram Distribution   We start our study with a controlled experiment   where we estimate the entropy of the truncated   unigram distribution , the ( finite ) distribution over   the frequent word tokens in a language without   regard to context ( Baayen et al . , 2016 ; Diessel ,   2017 ; Divjak , 2019 ; Nikkarinen et al . , 2021 ) . Werenormalize the frequency counts of corpora in En-   glish , German , and Dutch ( taken from CELEX ;   Baayen et al . , 1995 ) , as well as Mongolian and   Tagalog ( from Wikipedia ) . We take this renor-   malization as a gold standard distribution , since   we can not access the underlying unigram distri-   bution . We then draw samples of varying sizes   ( N‚àà { 10,10,10,10 } ) from the distribution   of renormalized frequency counts to test the estima-   tors ‚Äô ability to recover the underlying distributions ‚Äô   entropy . While the renormalized frequency counts   are not necessarily representative of the true uni-   gram distribution , they nevertheless provide us with   a controlled setting to benchmark various entropy   estimators .   We evaluate the estimators on both bias and   MSE , as defined in ( 2 ) and ( 4 ) , as well as mean   absolute bias ( MAB ) . To test the statistical sig-   nificance of differences in metrics between en-   tropy estimators , we use paired permutation tests   ( Good , 2000 ) ( sampling 1,000permutations ) be-   tween pairs of estimators , checking MAB and MSE .   We run Tukey ‚Äôs test ( 1949 ) to judge the statistical   significance of differences in MAB and MSE be-   tween all pairs of estimators , which found only a   few insignificant comparisons when Nwas large .   Results are shown in Table 1 and Figure 1 . We   find that NSB ( followed closely by CS ) converges   almost to the true entropy from below using with   only a few samples . HT is the best estimator for   N < 2,000 , but as Nincreases it tends to overes-   timate entropy to the point where its bias is greater   than that of MLE . Besides HT , all estimators at all   tested sample sizes Nhave lower MAB and MSE   than MLE .   4.2 Replication of Williams et al . ( 2021 )   Next , we turn to a replication of Williams et al . ‚Äôs   ( 2021 ) information - theoretic study on the associa-178   tion between gendered inanimate nouns and their   modifying adjectives . They estimate mutual infor-   mation by using its familiar decomposition as the   difference of two entropies : MI(X;Y ) = H ( X)‚àí   H(X|Y ) . The entropies H(X)andH(X|Y)are   estimated independently and then their difference is   computed . We replicate Williams et al . ‚Äôs ( 2021 ) ex-   periments using gold - parsed Universal Dependen-   cies corpora , filtering out animate nouns with Mul-   tilingual WordNet ( Bond and Foster , 2013 ) . We   rerun their experimental set - up using our full suite   of entropy estimators to determine whether the re-   lationship they posit remains significant , checking   3 more languages not in the original study .   We report results for normalized mutual infor-   mation ( dividing MI by maximum possible MI ) in   Table 2 . We find that using NSB ( the estimator   we found most effective in ¬ß 4.1 ) instead of MLE ,   nearly halves the measured effect in all languages .   However , the effect remains statistically significant   in 5 of 7 languages tested , including the 4 that were   also in the original study .   4.3 Replication of McCarthy et al . ( 2020 )   Finally , we turn our attention to McCarthy et al . ‚Äôs   ( 2020 ) study on the similarity between grammat-   ical gender partitions between languages . Using   information - theoretic measures , they found that   closely related languages have more similar gender   groupings of core lexical items . We replicate their   experiment on Swadesh lists ( Swadesh , 1955 ) for   10 European languages with different estimators ,   and find that hierarchical clustering over both mu-   tual ( MI ) and variational information ( VI ) produces   the same trees as the original study . In this case , us-   ing NSB , our recommended estimator , results in a   reduced estimate of MI ( e.g. Croatian ‚Äì Slovak : 0.54   with MLE ‚Üí0.46with NSB ) , but significance test-   ing with 1,000 permutations finds the same pairs   were statistically significant for both MI and VI re-   gardless of estimator : all pairs of Slavic languagesand Romance languages , and Bulgarian ‚Äì Spanish   ( see Figure 2 ) . Thus , we see a similar result here   as in the previous replication .   5 Conclusion   This work presents the first empirical study compar-   ing the performance of various entropy estimators   for use with natural language distributions . From   experiments on synthetic data ( appendix ) and nat-   ural data ( CELEX ) , and two replication studies of   recent papers in information - theoretic linguistics ,   we find that the oft - employed plug - in estimator of   entropy can cause misleading results , e.g. , the over-   estimates of effect sizes seen in both replication   studies . The recommendation of our paper is that   researchers should carefully consider their choice   of entropy estimator based on data availability and   the nature of the underlying distribution .   Ethics Statement   The authors foresee no ethical concerns with the   research presented in this paper .   Acknowledgments   We thank Adina Williams , Lucas Torroba Henni-   gen , Tiago Pimentel , and the anonymous reviewers   for feedback on the manuscript .   References179180181182   A Implementation   The code for each of the entropy estimators is implemented in Python using numpy ( Harris et al . , 2020 ) ,   except for NSB which was taken from an existing efficient implementation in the nddmodule ( Marsili ,   2016 ) . We calculated entropies with base e(in nats ) .   B Experiments with simulated data   In our experiments with simulated data , we explore distributions sampled from a symmetric Dirichlet   prior with varying number of classes Kand known distributions of Zipfian form with various parameters .   Words in natural languages have a roughly Zipfian distribution , with probability inversely proportional to   rank ( Zipf , 1935 ) , and a symmetric Dirichlet distribution is analogous to e.g. POS tag label distributions   in natural language . Thus , studying synthetic data from such distributions as a start is useful .   B.1 Experiment 1 : Symmetric Dirichlet distributions   We sample 1,000distributions from a symmetric Dirichlet distribution with variable number of classes K ,   i.e. with paramater Œ±= [ Œ± , . . . , Œ± ] = [ 1 , . . . , 1 ] . We calculate entropy estimates on different sample   sizes N. Since we know the parameters of the true distribution , we can compare estimates with the true   entropy . We do pairwise comparisons of the MAB and MSE of estimators , using paired permutation tests   to establish significance . Table 3 shows our results , including significance tests . It is clear that when   N‚â´K , all of the estimators have nearly converged to the true value and estimator choice does not matter .   However , in the low - sample regime some estimators are indeed significantly better at approximating the   true entropy . Our results are mixed as to which estimator is best in what context ; the one found to be most   frequently significantly better than other estimators was Chao ‚Äì Shen . What is clear is that MLE is never   the best choice .   B.2 Experiment 2 : Zipfian distributions   We sample 1,000finite Zipfian distributions with Kclasses which obey Zipf ‚Äôs law , that the probability of   an outcome is inverse proportional to its rank . The experimental setup is the same as in Experiment 1 .   A Zipfian distribution approximates ( but is not a perfect model of ) the distribution of tokens in natural   language text in some languages , including English , which was the basis for the law being proposed .   Compare similar experiments on infinite Zipf distributions by Zhang ( 2012 ) . Results are in Table 4 .   C Replication of Williams et al . ( 2021 )   We used the following UD treebanks:183‚Ä¢Arabic : PADT ( Smr≈æ et al . , 2008 ; Taji et al . , 2017 ) ;   ‚Ä¢Greek : GDT ( Prokopidis et al . , 2005 ; Prokopidis and Papageorgiou , 2017 ) ;   ‚Ä¢Italian : ISDT ( Bosco et al . , 2013 ) , VIT ( Tonelli et al . , 2008 ) ;   ‚Ä¢Polish : PDB ( Wr√≥blewska , 2018 ) ;   ‚Ä¢Portuguese : GSD ( McDonald et al . , 2013 ) , Bosque ( Rademaker et al . , 2017 ) ;   ‚Ä¢Spanish : AnCora ( Taul√© et al . , 2008 ) , GSD ( McDonald et al . , 2013 ) .   D Additional Figures184185E Derivation of the Entropy Estimators   LetX={x}be a finite set . Let pbe a distribution over X. The entropy ofpis defined as   H(p)=‚àíXplogp ( 13 )   Given a dataset of Nsamples Dsampled i.i.d . from p , our goal is to estimate the entropy   H(p)from samples Dfrom the true distribution p. We will denote the count of an item xas   c(x ) = P 1n   x = exo   . The maximum - likelihood estimate ( MLE ) of pgivenDis denoted . The plug - in estimate ofH(p)is defined to be the estimate of H(p)obtained by   plugging the MLE estimate bpdirectly into the definition of entropy , i.e. ,   bH(D ) = H(bp ) = ‚àíXbp(x ) logbp(x ) = ‚àíXc(x )   Nlogc(x )   N(14 )   This section discusses the problems with Eq . ( 14 ) as an estimator and provides detailed derivations of   improved estimators found in the literature .   E.1 The Plug - in Estimator is Negatively Biased   Proposition 1 . The MLE entropy estimator in expectation underestimates true entropy , i.e. ,   bH(D ) = E"X‚àíbp(x ) logbp(x ) #   ‚â§H(p ) ( 15 )   Proof . The result is a simple consequence of Jensen ‚Äôs inequality and some basic manipulations :   E"X‚àíbp(x ) logbp(x ) #   = XE[‚àíbp(x ) logbp(x ) ] ( linearity of expectation )   ‚â§ ‚àíXE[bp(x ) ] logE[bp(x ) ] ( Jensen ‚Äôs inequality )   = ‚àíXp(x ) logp(x ) ( E[bp(x ) ] = p(x ) )   = H ( p ) ( definition of entropy )   This completes the result .   E.2 Miller ‚Äì Madow   Proposition 2 . Letpbe a categorical distribution over X={x , . . . , x } , i.e. , a categorical distribution   with support K. LetDbe our dataset of size Nsampled from p. Finally , let bpbe the maximum-   likelihood estimate computed on D. Then , we have   bias   bH(D)=Eh   bH(D)i   ‚àíH(p ) ( 16 )   = ‚àíK‚àí1   2N+o    N   ( 17 )   Proof . We start by taking a first - order Taylor expansion and take an expectation of both sides .   bH(D ) = H(bp , p)|{z}‚àíKL(bp||p ) ( Lemma 1 ) ( 18)186Eh   bH(D)i   = E[H(bp , p)]‚àíE[KL(bp||p ) ] ( expectation ) ( 19 )   = E "   ‚àíXbp(x ) logp(x ) #   ‚àíE[KL(bp||p ) ] ( defn . H(p , q ) ) ( 20 )   = ‚àíXE[bp(x ) logp(x)]‚àíE[KL(bp||p ) ] ( linearity ) ( 21 )   = ‚àíXE[bp(x ) ] log p(x)‚àíE[KL(bp||p ) ] ( algebra ) ( 22 )   = ‚àíXp(x ) logp(x)‚àíE[KL(bp||p ) ] ( unbiased ) ( 23 )   = H ( p)‚àíE[KL(bp||p ) ] ( defn . of H(p ) ) ( 24 )   ( 25 )   This gives us :   Eh   bH(D)i   ‚àíH(p ) = ‚àíE[KL(bp||p ) ] ( subtract H(p ) ) ( 26 )   Thus , we may compactly write the bias as :   bias   bH(D)   = E[H(bp)]‚àíH(p ) ( definition of bias ) ( 27 )   = ‚àíE[KL(bp||p ) ] ( above computation ) ( 28 )   ‚â§0 ( non - negativity of KL ) ( 29 )   Now , we find a simpler expression for the remainder E[KL(bp||p ) ] . Again , we start with a second-   order Taylor expansion   KL(p||q ) = X‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( Lemma 2 ) ( 30 )   around the point ‚àÜ(x ) = p(x)‚àíq(x ) . Define bp(x ) = where c(x)is the count of xin the   training set . We now simplify the first term :   E"X‚àÜ(x )   2q(x ) #   = E"X(bp(x)‚àíp(x ) )   2p(x ) #   ( definition of ‚àÜ(x ) ) ( 31 )   = E"X(‚àíp(x ) )   2p(x ) #   ( definition of MLE ) ( 32 )   = E"X(c(x)‚àíNp(x ) )   2Np(x ) #   ( √ó/ ) ( 33 )   = 1   2NE"X(c(x)‚àíNp(x ) )   Np(x ) #   ( pulling out/ ) ( 34 )   = 1   2NEÔ£Æ   Ô£ØÔ£ØÔ£∞Xc(x)‚àí2c(x)Np(x )   + Np(x )   Np(x)Ô£π   Ô£∫Ô£∫Ô£ª(exp . the binomial ) ( 35 )   = 1   2NXE   c(x)   ‚àí2Np(x)E[c(x ) ]   + Np(x )   Np(x)(lin . of expect . ) ( 36)187=1   2NXNp(1‚àíp(x ) ) + Np(x )   ‚àí2Np(x)+Np(x )   Np(x)(moments of MLE ) ( 37 )   = 1   2NXNp(1‚àíp(x ) )   Np(x )   +1   2NXNp(x)‚àí2Np(x)+Np(x )   Np(x )   | { z } ( 38 )   = 1   2NXNp(x)(1‚àíp(x))Np(x)(39 )   = 1   2NX(1‚àíp(x ) ) ( algebra ) ( 40 )   = 1   2NX1   |{z}‚àí1   2NXp(x )   |{z}(algebra ) ( 41 )   = K‚àí1   2N(42 )   Next , we simplify the second term , o    ‚àÜ(x)   , in the MLE case :   E   o    ‚àÜ(x)   = E   o    ( bp(x)‚àíp(x))   ( definition of ‚àÜ ) ( 43 )   = E "   o c(x )   N‚àíp(x) ! #   ( definition of MLE ) ( 44 )   = E   o(c(x)‚àíNp(x ) )   N   ( √ó/ ) ( 45 )   = E   oc(x)‚àí2c(x)Np(x ) + Np(x )   N   ( 46 )   = o   E   c(x)‚àí2c(x)Np(x ) + Np(x)   N !   ( push exp . through ) ( 47 )   = oÔ£´   Ô£¨Ô£¨Ô£≠Np(1‚àíp(x ) ) + Np(x )   ‚àí2Np(x)+Np(x )   NÔ£∂   Ô£∑Ô£∑Ô£∏(48 )   = oNp(x)(1‚àíp(x ) )   N   ( cancel terms ) ( 49 )   = op(x)(1‚àíp(x ) )   N   ( cancel Nin fraction ) ( 50 )   = o    N   ( ignore constants ) ( 51 )   Putting it all together , we get that bias ( H ( bp ) ) = ‚àí+o    N   which is the desired result .   Interestingly , it can be seen that the negative bias of the MLE gets worse as the number of classes K   grows . Distributions with large Kpop up frequently when dealing with natural language .   Corollary 1 . The plug - in estimator of entropy is consistent.188Proof . From Proposition 2 , we have bias ( H ( bp ) ) = ‚àí+o    N   . Clearly , as N‚Üí0 , we have   bias ( H ( bp))‚Üí0 , so the estimator is consistent . One could also prove consistency through a simple   application of the continuous mapping theorem .   Estimator 1 ( Miller ‚Äì Madow ) .Letpbe a categorical over Kcategories . We seek to estimate the entropy   H(p ) . LetDbe our dataset of size Nsampled from p. Then , the Miller ‚Äì Madow estimator of H(p)is   given by   bH(D)=bH(D ) + K‚àí1   2N(52 )   The Miller ‚Äì Madow estimator is biased , however it is consistent .   Lemma 1 . The the first - order Taylor approximation of bH(D)around the distribution pis given by   bH(D ) = H(bp , p ) + R(p , bp ) ( 53 )   where the remainder Ris given by   R(p , bp ) = ‚àíKL(bp||p ) ( 54 )   Proof . The result follows from direct computation . We start by taking the Taylor expansion of H(bp )   around H(p ):   bH(D ) = H ( p ) + X‚àÇ   ‚àÇp(x)h   H(p)i   bp(x)‚àíp(x)   + R(p , bp)|{z}(55 )   Our first order term can then be rewritten as follows : X‚àÇ   ‚àÇp(x)h   H(p)i   bp(x)‚àíp(x)   ( 56 )   = X‚àÇ   ‚àÇp(x)"X‚àíp(x ) logp(x)#   bp(x)‚àíp(x)   ( 57 )   = X"X‚àí‚àÇ   ‚àÇp(x)p(x ) logp(x)#   bp(x)‚àíp(x)   ( linearity ) ( 58 )   = X"X‚àÇ   ‚àÇp(x)p(x ) logp(x)#   p(x)‚àíbp(x)   ( sign ) ( 59 )   = X   1 + log p(x)   p(x)‚àíbp(x)   ( 60 )   = X   p(x)‚àíbp(x)   + log p(x ) ( p(x)‚àíbp(x ) ) ( 61 )   = X   p(x)‚àíbp(x)   + Xlogp(x ) ( p(x)‚àíbp(x ) ) ( 62 )   = Xp(x )   |{z}‚àíXbp(x )   |{z}+Xlogp(x ) ( p(x)‚àíbp(x ) ) ( distrib . sum ) ( 63 )   = Xlogp(x ) ( p(x)‚àíbp(x ) ) ( simplify ) ( 64)189 = Xlogp(x)p(x )   | { z } ‚àíXlogp(x)bp(x )   | { z } ( distrib . sum ) ( 65 )   = H ( p , bp)‚àíH(p ) ( 66 )   Plugging this back into our Taylor expansion , we get the following :   bH(D ) = H(p)‚àíH(p ) + H ( p , bp ) + R(p , bp ) ( 67 )   Now , we see that this implies   R(p , bp ) = bH(D)‚àíH(bp , p ) ( algebra ) ( 68 )   = ‚àíXbp(x ) logbp(x ) + Xbp(x ) logp(x ) ( defn . ) ( 69 )   = ‚àíX(bp(x ) logbp(x)‚àíbp(x ) logp(x ) ) ( merge sums ) ( 70 )   = ‚àíXbp(x)(logbp(x)‚àílogp(x ) ) ( factor out bp(x ) ) ( 71 )   = ‚àíXbp(x ) logbp(x )   p(x)(logalgebra ) ( 72 )   = ‚àíKL(bp||p ) ( defn . ) ( 73 )   which is the desired result .   Lemma 2 . Define ‚àÜ(x ) = p(x)‚àíq(x ) . The second - order Taylor expansion of KL(p||q)around ‚àÜ(x )   is given by   KL(p||q ) = X‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( 74 )   Proof . Now we compute the series expansion of the KL - divergence . We first make a tricky substitution :   p(x )   q(x)=q(x ) + p(x)‚àíq(x )   q(x)= 1 + p(x)‚àíq(x )   q(x)= 1 + ‚àÜ(x )   q(x)(75 )   Now , we proceed with the derivation :   KL(p||q ) = Xp(x ) logp(x )   q(x)(defn . of KL divergence ) ( 76 )   = X(q(x ) + ‚àÜ ( x ) ) log   1 + ‚àÜ(x )   q(x)   ( Eq . ( 75 ) ) ( 77 )   = X(q(x ) + ‚àÜ ( x))‚àÜ(x )   q(x)‚àí‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( Taylor expansion ) ( 78 )   = X‚àÜ(x)‚àí‚àÜ(x )   2q(x)+‚àÜ(x )   q(x)‚àí‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( distribute ) ( 79 )   = X‚àÜ(x)‚àí‚àÜ(x )   2q(x)+‚àÜ(x )   q(x)+o    ‚àÜ(x)   ( defn . of o ) ( 80 )   = X‚àÜ(x ) + ‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( algebra ) ( 81)190 = X‚àÜ(x )   |{z}+X‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( split sums ) ( 82 )   = X‚àÜ(x )   2q(x)+o    ‚àÜ(x)   ( 83 )   which is the desired result .   E.3 Jackknife   The jackknife resampling method is used to estimate the bias of an estimator and correct for it , by sampling   all subsamples of size N‚àí1from the available sample of size N , computing their average for the statistic   being estimated .   Generally , this reduces the order of the bias of an estimator from O(N)to at most O(N)(Friedl   and Stampfer , 2002 ) .   Estimator 2 ( Jackknife ) .Letpbe a categorical over Kcategories . We seek to estimate the entropy H(p ) .   LetDbe our dataset of size Nsampled from p. LetbH(D)be an estimate of the entropy from a sample   with the nobservation held out . Then , the Jackknife estimator is given by   bH(D)=NbH(D)‚àíN‚àí1   NXbH(D ) ( 84 )   This estimator is derived from the jackknife - resampled estimate of the bias of the MLE estimator , multiplied   byN‚àí1 .   bH(D)‚àíbH(D ) = ( N‚àí1 )   bH(D)‚àí1   NXbH(D ) !   ( 85 )   E.4 Horvitz ‚Äì Thompson   Horvitz and Thompson ( HT ; 1952 ) is a common estimator given a finite universe , which is our case as K   is finite . We omit a derivation a full here as it is well documented in other places ( Vieira , 2017 ) . However ,   we note that , in contrast to many applications of HT , the application of HT to entropy estimation results in   a biased estimator as the function whose mean we seek to estimate is logp(x ) , which is dependent on   the unknown distribution p.   Estimator 3 ( Horvitz ‚Äì Thompson ) .Letpbe a categorical over Kcategories . We seek to estimate the   entropy H(p ) . LetDbe our dataset of size Nsampled from p. Then the Horvitz ‚Äì Thompson estimator   is defined as   bH(D)=‚àíXbp(x ) logbp(x )   1‚àí(1‚àíbp(x))(86 )   where 1‚àí(1‚àíbp(x))is an estimate of the inclusion probability , i.e. , the probability that x   appears in a random sample Dof size N.   We do not know of a simple expression for the bias of the Horvitz ‚Äì Thompson entropy estimator , but   one observation is that E   ( 1‚àíbp(x))   > E   ( 1‚àíp(x))   when N > 1(justified by Jensen ‚Äôs   inequality , since x , N > 1is convex over [ 0,1 ] ) ; this is an overestimate of the true inclusion probability .   E.5 Chao ‚Äì Shen   The Chao ‚Äì Shen estimator builds upon Horvitz ‚Äì Thompson by noting that that estimator does not correct for   underestimation of number of classes Kand resulting effect on estimates of p(x ) ; i.e.1‚àí(1‚àíbp(x ) )   is always 0for a class not included in the sample even if the class is present in the true distribution . We can   reweight the sample probabilities to compensate for missing classes using the notion of sample coverage.191Definition 1 ( Sample coverage ) .We define the sample coverage as   C = Xp(x)1n   x‚àà Do   ( 87 )   Definitionally , ( 1‚àíC)is then the probability of sampling an xnotobserved in the sample eX.   However , exact computation of Eq . ( 88 ) is impossible as we do not know the true distribution p. Thus ,   Chao and Shen ( 2003 ) fall back on a well - known estimator of Cthat uses a technique from Good ‚Äì Turing   ( 1953 ) smoothing . Let fbe the number of classes with only one observation in the current sample , i.e ,   the number of singletons , then we can estimate the sample coverage as   bC= 1‚àíf   N(88 )   The Chao ‚Äì Shen estimator , described below , simply re - scales the MLE estimate of probability bp(x )   in the HT estimator by bC. This corrects for the observed under estimation of p ‚Äôs entropy by HT .   Estimator 4 ( Chao ‚Äì Shen ) .Letpbe a categorical over Kcategories . We seek to estimate the entropy   H(p ) . LetDbe our dataset of size Nsampled from p. LetbC , an estimate of sample coverage , be defined   as in Eq . ( 88 ) . The Chao ‚Äì Shen estimator is then defined as   bH(D)=‚àíXbC¬∑bp(x ) log ( bC¬∑bp(x ) )   1‚àí(1‚àíbC¬∑bp(x))(89 )   E.6 Wolpert ‚Äì Wolf   Fact 1 ( Derivative of an exponent ) .   d   dax = xlogx ( 90 )   Fact 2 ( Normalizer of a Dirichlet ) .The normalizer of a Dirichlet distribution is   Z   Œ¥ Xx‚àí1!Yxdx = QŒì(Œ± )   ŒìPŒ± ( 91 )   A relatively easy proof of this fact makes use of a Laplace transform .   Estimator 5 ( Wolpert ‚Äì Wolf ) .Letpbe a categorical over Kcategories . We seek to estimate the entropy   H(p ) . LetDbe our dataset of size Nsampled from p. Then , the Wolpert ‚Äì Wolf estimator is given by   bH(D |Œ±)=œà   eA+ 1   ‚àíXeŒ±   eAœà(eŒ±+ 1 ) ( 92 )   where c(x)=P 1{ex = x } , and we additionally define eŒ± = c(x ) + Œ±andeA = PeŒ± .   Proposition 3 ( Wolpert ‚Äì Wolf ) .The expectation of entropy under a Dirichlet posterior Dirichlet ( Œ± )   where parameter Œ±is given by   E[H(p)|Œ±]=Z   H(p)Œ¥ Xp(x)‚àí1 !   Œì ( A)QŒì(Œ±)Yp(x)dp ( 93 )   = œà(A+ 1)‚àíXŒ±   Aœà(Œ±+ 1 ) ( 94 )   where A = PŒ±.192Proof . LetDirichlet ( Œ± , . . . , Œ±)be a Dirichlet posterior . The result follows by a series of manipulations :   E[H(p)|Œ± ] = Z   H(p)Œ¥ Xp(x)‚àí1 !   Œì ( A)QŒì(Œ±)Yp(x)dp ( defn . ) ( 95 )   = Œì ( A)QŒì(Œ±)Z   H(p)Œ¥ Xp(x)‚àí1!Yp(x)dp ( 96 )   = Œì ( A)QŒì(Œ±)Z   ‚àíXp(x ) logp(x ) !   Œ¥ Xp(x)‚àí1!Ypdp ( defn . H ) ( 97 )   = ‚àíŒì ( A)QŒì(Œ±)XZ   p(x ) logp(x)Œ¥ Xp(x)‚àí1!Yp(x)dp ( linear . ) ( 98 )   = ‚àíŒì ( A)QŒì(Œ±)XZ   p(x)logp(x)Œ¥ Xp(x)‚àí1!Yp(x)dp ( algebra ) ( 99 )   = ‚àíŒì ( A)QŒì(Œ±)XZd   dŒ±p(x)Œ¥ Xp(x)‚àí1!Yp(x)dp ( fact # 1 ) ( 100 )   = ‚àíŒì ( A)QŒì(Œ±)XZd   dŒ±Œ¥ Xp(x)‚àí1 !   p(x)Yp(x)dp ( algebra ) ( 101 )   = ‚àíŒì ( A)QŒì(Œ±)Xd   dŒ±Z   Œ¥ Xp(x)‚àí1 !   p(x)Yp(x)dp ( 102 )   = ‚àíŒì ( A)QŒì(Œ±)Xd   dŒ±Œì(Œ±+ 1)QŒì(Œ± )   ŒìPŒ±+ 1 ( fact # 2 ) ( 103 )   = ‚àíŒì ( A)QŒì(Œ±)XYŒì(Œ±)d   dŒ±Œì(Œ±+ 1 )   ŒìPŒ±+ 1 ( 104 )   = ‚àíŒì ( A)QŒì(Œ±)XYŒì(Œ±)œà(Œ±+ 1)Œì ( Œ±+ 1)ŒìPŒ±+ 1   ŒìPŒ±+ 1(derivative ) ( 105 )   ‚àíœà(PŒ±+ 1)Œì ( Œ±+ 1)Œì(PŒ±+ 1 )   ŒìPŒ±+ 1   = ‚àíŒì ( A)QŒì(Œ±)XYŒì(Œ±)œà(Œ±+ 1)Œì ( Œ±+ 1 )   ‚àíœà(PŒ±+ 1)Œì ( Œ±+ 1 )   ŒìPŒ±+ 1 ( simplify ) ( 106 )   = ‚àíŒì ( A)QŒì(Œ±)XYŒì(Œ±)œà(Œ±+ 1)Œì ( Œ±)Œ±   ‚àíœà(PŒ±+ 1)Œì ( Œ±)Œ±   ŒìPŒ±   A(defn . Œì ) ( 107)193=‚àíŒì ( A)QŒì(Œ±)QŒì(Œ± )   Œì ( A)X   Œ±   Aœà(Œ±+ 1)‚àíŒ±   Aœà XŒ±+ 1 ! !   ( distrib . ) ( 108 )   = ‚àíX   Œ±   Aœà(Œ±+ 1)‚àíŒ±   Aœà XŒ±+ 1 ! !   ( cancel ) ( 109 )   = ‚àíXŒ±   Aœà(Œ±+ 1)‚àíŒ±   Aœà(A+ 1)   ( defn . A ) ( 110 )   = ‚àíXŒ±   Aœà(Œ±+ 1 ) + XŒ±   Aœà(A+ 1 ) ( distrib . ) ( 111 )   = ‚àíXŒ±   Aœà(Œ±+ 1 ) + œà(A+ 1 ) ( Pa = A ) ( 112 )   = œà(A+ 1)‚àíXŒ±   Aœà(Œ±+ 1 ) ( rearr . ) ( 113 )   which proves the result .   E.7 Nemenman ‚Äì Shafee ‚Äì Bialek   Estimator 6 ( Nemenman ‚Äì Shafee ‚Äì Bialek ) .Letpbe a categorical over Kcategories . We seek to estimate   the entropy H(p ) . LetDbe our dataset of size Nsampled from p. Define the NSB density as   p(Œ±)=Kœà(KŒ±+ 1)‚àíœà(Œ±+ 1 )   logK(114 )   where œàis the trigramma function . Then , the NSB estimator is given by   bH(D)=ZbH(D |Œ±¬∑1)p(Œ± ) dŒ± ( 115 )   The integral in Eq . ( 115 ) is typically computed by numerical integration .   To derive the Nemenman ‚Äì Shafee ‚Äì Bialek ( NSB ) estimator , we start with the idea that we would like a   prior over distributions such that the distribution over expected entropy is uniform . In other words , we   are looking for a psuch that for Œ±‚àºp , the values of E[H(p)|Œ±]are uniformly distributed over   [ 0,logK ] . This is a good idea since , a - priori , we do not know entropy of pand , in the absence of any   insight , we should assume the entropy could be anywhere in the range [ 0,logK ] . We make the above   intuition formal with the following proposition .   Proposition 4 . Letpbe the NSB density given in Eq . ( 114 ) . Then the following conditional expectation   E[H(p)|Œ±]=Z   H(p)Œ¥ Xp(x)‚àí1 !   Œì ( KŒ± )   Œì(Œ±)Yp(x)dp ( 116 )   = œà(KŒ±+ 1)‚àíœà(Œ±+ 1 ) ( Proposition 3 ) ( 117 )   is uniformly distributed over [ 0,logK]when Œ±‚àºp ( ¬∑ ) , defined in Eq . ( 114 ) .   Proof . First , we note that E[H(p)|Œ±]is a continuous , increasing function in Œ± . We will not prove this   formally , but it should make intuitive sense : Œ±is a smoothing parameter and the more the distribution is   smoothed , the more entropic it should be . From basic analysis , we know that a strictly continuous , increas-   ing function has an inverse . The above means that we can view E[H(p)|Œ±]as a bijection from Rto the   interval [ 0,logK ] . Our goal is to reparameterize the Uniform distribution in terms of Œ± . To that end , we194define the function g(Œ±)=E[H(p)|Œ± ] : R‚Üí[0,logK]and perform a change - of - variables trans-   form on Eq . ( 118 ) using g. We start with the continuous uniform over [ 0,logK ] , which is show below   p(H)=1   logK1n   H‚àà[0,logK]o   | { z } ( defn . of uniform dist ) ( 118 )   Note His a random variable and unrelated to the functional H ( ¬∑ ) ; the choice of letter intentionally   reminds one that the variable represents the expected entropy of under a random distribution . Now we   apply the change - of - variables formula at H = g(Œ±)and manipulate :   p(H ) = p(g(Œ± ) )  dg   dŒ±(Œ± )  ( change of variable ) ( 119 )   = 1   logK1n   g(Œ±)‚àà[0,logK]o  dg   dŒ±(Œ± )  ( definition of p ) ( 120 )   = 1   logK  dg   dŒ±(Œ± )  ( redundant indicator ) ( 121 )   = 1   logKdg   dŒ±(Œ± ) ( derivative is positive ) ( 122 )   = Kœà(KŒ±+ 1)‚àíœà(Œ±+ 1 )   logK(Lemma 3 ) ( 123)=p(Œ± ) ( definition ) ( 124 )   By construction , the prior p(Œ±)has the property that the expected entropy E[H(p)|Œ±]where   Œ±‚àºp(¬∑)is uniformly distributed over [ 0,logK ] , which we can see by reversing the above derivation .   This proves the result .   Nemenman et al . ( 2002 ) interpreted Proposition 4 in the following manner : As the variance of   E[H(p)|Œ± ] , which is treated as a random variable since Œ±is random , approaches 0 , then the the   NSB estimator implies a uniform prior over the entropy .   Lemma 3 ( NSB Derivative ) .   d   dŒ±[œà(KŒ±+ 1)‚àíœà(Œ±+ 1 ) ] = Kœà(KŒ±+ 1)‚àíœà(Œ±+ 1 ) ( 125 )   Proof . The proof follows by a straightforward computation :   d   dŒ±[œà(KŒ±+ 1)‚àíœà(Œ±+ 1 ) ] = d   dŒ±[œà(KŒ±+ 1)]‚àíd   dŒ±[œà(Œ±+ 1 ) ] ( linearity ) ( 126 )   = Kœà(KŒ±+ 1)‚àíœà(Œ±+ 1 ) ( definition ) ( 127 )   where œà(x)=œà(x).195   David Guriel , Omer Goldman , Reut Tsarfaty   Bar - Ilan University   { davidgu1312,omer.goldman}@gmail.com,reut.tsarfaty@biu.ac.il   Abstract   1 Introduction   In recent years , morphological ( re)inflection tasks   have gained a lot of attention in NLP.Sub-   sequently , several multi - lingual morphological   datasets have emerged to allow for the supervised   training of morphological models , most notably   UniMorph ( McCarthy et al . , 2020 ) , that organizes   words into inflectional tables , annotating each in-   flected word - form with its respective feature - set .   While western languages are widely represented   in UniMorph , many morphologically rich lan-   guages ( Tsarfaty et al . , 2010 , 2020 ) exhibit richand diverse inflection patterns that make them less   compatible with the flat feature - sets in the Uni-   Morph schema . Concretely , in some cases it is   completely impossible to annotate parts of the in-   flectional paradigm with a flat bundle , as is the   case with case stacking , and in other cases , such as   polypersonal agreement , the annotation solutions   provided are unnatural , non - transparent , and are   barely used in practice . As a result , languages ex-   hibiting such phenomena are under - represented in   UniMorph , and when they are , the inflection tables   for these languages are often incomplete .   In this paper we propose a general solution   for annotating such structures , thus extending the   UniMorph annotation schema to fully cover a   wider range of morphologically - complex argument-   marking phenomena . Following Anderson ( 1992 ) ,   we propose a so - called layered annotation of fea-   tures , where the inflectional features take the form   of ahierarchical structure , in the spirit of formal   linguistic frameworks as that of Johnson ( 1988 ) ;   Pollard and Sag ( 1994 ) ; Shieber ( 2003 ) ; Bresnan   et al . ( 2015 ) . We organize the features of multiple   arguments in a hierarchical structure , rather than   the current flat structure that accommodates only   subject concords . This schema shift allows for an   adequate annotation of polypersonal agreement and   ofpossessed nominals , where a word has multiple   number and gender features , as well as forms with   case stacking , where a word has multiple cases .   We apply the suggested solution to Georgian ,   an agglutinative language with a convoluted ver-   bal system , that indicates both subjects and objects   with true affixes ( rather than clitics that are omit-   table from the inflection tables ) . We create a new   human - verified dataset for Georgian , that covers   most of the grammatical phenomena in Georgian   verbs , and includes 118 lemmas , adding up to about   21kverb forms , compared with the 47 lemmas and   3.3kverb forms , some of which are erroneous , cur-   rently available in the Georgian UniMorph.196   We use the new dataset to train a standard   morphological reinflection model ( Silfverberg and   Hulden , 2018 ) and show that training on the Geor-   gian inflections currently available in UniMorph   is not sufficient for generalizing to the more in-   clusive set of inflections that are allowed by the   new scheme . We conclude that our annotation ap-   proach provides a more complete representation of   linguistic behaviors , and that our proposed Geor-   gian dataset provides a much better depiction of   the morphological phenomena that exist in the data   and the computational challenge reflected therein .   We therefore call to apply layered annotation to   all currently existing morphological data in Uni-   Morph , to more consistently and transparently cap-   ture the linguistic reality and morphological com-   plexity reflected in the worlds languages .   2 The Problem : Multiple Arguments   Models of morphological reinfection are trained to   generate forms within a lemma L , given another   form and the features of sourceand targetforms :       ‚ü®feat , form‚ü©,‚ü®feat,___‚ü©   7‚Üíform   For example , for the Russian lemma –õ–ï–¢–ï–¢–¨ :   reinflecting from ( P;1;S , –ª–µ—á—É ) to   ( I;2;S , –ª–µ—Ç–∏ ) will be represented as:    ‚ü®P;1;S , –ª–µ—á—É‚ü©,‚ü®I;2;S,___‚ü©   7‚Üí–ª–µ—Ç–∏   Standardly , the data for training morphologi-   cal models ( e.g. , Wu et al . , 2020 ; Makarov and   Clematide , 2018 ) is taken from UniMorph ( Mc-   Carthy et al . , 2020 ) , a multilingual morphological   dataset in which words are grouped by lemma into   inflection tables , each word is tagged with an un-   ordered set of morphological features . The features   list is shared across languages . The inflection ta-   bles are meant to be exhaustive , i.e. , covering all   possible forms of a lemma , regardless of usability .   Although the features were designed to apply   cross - lingually , some blind - spots exist . Most rele-   vant to our work is the assumption that every fea-   ture set includes at most one pronominal feature   bundle ( i.e. , person - gender - number).However , this assumption does not apply to   verbs with object concords , as exhibited in Geor-   gian ( see Table 1 ) , Inuit and many Bantu languages   inter alia , nor does it apply to possessed nouns that   mark the features of both the possessor and the   possessee . Examples ( 1a)‚Äì(1d ) illustrate this :   ( 1 ) a. Georgian : gagi≈°vebt ‚Äò We will let you go ‚Äô   ( -1,-2 )   b. Turkish : kedisisin ‚Äò you are his cat ‚Äô   ( N - , -2 , -3 )   c. Swahili : ninakupenda ‚Äò I love you ‚Äô   ( -1,-2 )   d. Hebrew : emdata ‚Äò her position ‚Äô   ( N - , -3- )   The solution proposed in UniMorph to annotat-   ing these phenomena is via concatenating several   properties into a single string , lacking any internal   structure ; e.g. , A 2indicates a form with a   2nd person singular accusative argument ( Sylak-   Glassman , 2016 ) . However , there are at least two   shortcomings to this solution . First , it is not suffi-   ciently transparent . A 2is an opaque string ,   that does not decompose into the known features   licensed by the UniMorph features list ( i.e. , ,   2 , ) . Secondly , and possibly due to this lack of   transparency , this annotation hack is hardly ever   used in practice . Hence , from all examples in ( 1 ) ,   only the Hebrew form is included in UniMorph ,   and tagged as;;;3with multiple pos-   sessor features merged into the flat string P3 .   The crux of the matter is that in the current   annotation schema , complex features assigned to   additional arguments are treated as a single non-   decomposable feature , that lack any internal struc-   ture , unlike the features of the main ( so - called ‚Äò in-   ternal ‚Äô ) argument , that are individually spelled out .   We argue that the lack of transparency and usability   are due to the misrepresentation of the inherently   hierarchical andcompositional structure of the fea-   tures in such forms . We suggest to explicitly anno-   tate these forms with features that are all explicitly   composed of the same primitive features .   All in all , the lack of a sufficiently expressive an-   notation standard leads to a data distribution that is   skewed , unrealistically simple , and , when language-   specific annotation solutions are painfully needed ,   they suffer from inconsistencies and ad - hoc deci-   sions . For these reasons , we set out to extend the   UniMorph annotation schema to accommodate all   such cases and to enable a proper coverage of lan-   guages , such as Georgian and many others.1973 The Proposed Schema   We propose to extend the UniMorph annotation   schema to cover multiple pronominal feature-   bundles in the same word - form , via a layering ap-   proach , originally proposed for morphological sys-   tems by Anderson ( 1992 ) . Anderson suggests to   arrange the morphosyntactic representation ( MSR )   of words in a hierarchy ( dubbed layers ) of features ,   in the sense that every element of the unordered set   of features can be composed of another unordered   set of features . That is , a general feature annotation   looks as in ( 2a ) . A specific transitive verb annota-   tion could be as depicted in ( 2b ):   ( 2 ) a. [ f , f , ... , [ F : f , f , ... [ F : f .. ] ] ]   b.[V , Tense ,   [ nom : Per , Num , Gen ] ,   [ acc : Per , Num , Gen ] ]   This hierarchical feature structure is reminiscent of   unification grammars orattribute - value grammars   ( Shieber , 2003 ; Johnson , 1988 ) that are extensively   used in syntactic theories such as GPSG , HPSG ,   and resemble the f - structures in LFG ( Gazdar et al . ,   1989 ; Pollard and Sag , 1994 ; Bresnan et al . , 2015 ) .   Here we employ these structures to organize the   features of morphologically - marked arguments hi-   erarchically , so an argument is characterized by   a feature composite of all features pertaining to   that argument . That is , each argument ‚Äôs feature-   bundle os specifically marked with the argument   it belongs to , and is decomposed into the primi-   tive features licensed by the UniMorph scheme . It   also homogeneously annotates the different kinds   of arguments , in contrast with the current schema   where the subject features are assigned to the   verb directly . Thus , the English form thinks previ-   ously annotated as;;3;will be annotated as ; ; ( 3 ;) . In languages that mark multi-   ple arguments , different kinds of arguments can be   marked with their feature - bundles without conflicts .   The proposed schema thus facilitates the annota-   tion of the poorly - treated or untreated phenomena   as illustrated in ( 1 ) . These are , respectively :   ( 3 ) a. Georgian : gagi≈°vebt ‚Äò We will let you go ‚Äô ; ; ( 1;);(2 ;)   b. Turkish : kedisisin ‚Äò you are his cat ‚Äô ; ; ( 2 ;) ; ( 3 ;)   c. Swahili : ninakupenda ‚Äò I love you ‚Äô ; ; ( 1;);(2 ;)   d. Hebrew : emdata ‚Äò her position ‚Äô ; ; ( 3;;)Table 2 compares the annotation of these exam-   ples in the current UniMorph schema compared   with our proposed annotation schema . The hierar-   chical structures , beyond being more transparent ,   opens the door further for future study on composi-   tional generalization in morphology .   The resemblance of our proposed schema to   ideas in other fields of theoretical linguistics , most   prominently to the f - structure in LFG ( Bresnan   et al . , 2015 ) and to the nested Attribute - Value ma-   trices in HPSG ( Pollard and Sag , 1994 ) , points to a   natural interface with further syntactic and seman-   tic annotations downstream .   4 A Case Study from Georgian   Linguistic Background Georgian is an aggluti-   native language with a verbal system that makes   a vast use of affixes to convey a wide array of   meanings , both inflectional and derivational ( see   Table 1 ) . The Georgian verbal paradigm is di-   vided into 5 classes known as : transitive , intran-   sitive , medial , indirect and stative ( Hewitt , 1995 ) .   The verbs are inflected to reflect 12 Tense - Aspect-   Mood ( TAM ) combinations ( traditionally known   asscreeves ) sorted into 4 series : present and future ,   aorist , perfective , and the imperative . Each series   has its own morpho - syntactic characteristics , most   notably split - ergativity is manifested in the aorist .   The characteristic most essential to this work is   that Georgian verbs always agree on person and   number with the direct and indirect objects , on top   of the subject - verb agreement . The Georgian data   in UniMorph follows the convention of including   objects only in third person singular ‚Äî thus failing   to provide a comprehensive coverage of the word-   forms that can be attested in the language .   Additional issues with the current morphological   data in UniMorph for Georgian verbs are : sparsity ,   as it includes only 47 inflection tables ; lack of di-   versity , as all table are from the transitive class ; and   lack of accuracy , as the data was produced automat-   ically without verification by native speakers .   Data Annotation A key contribution of this work   is the creation of a new dataset for Georgian that   follows the layered annotation schema and ad-   dresses the other shortcomings just described . We   selected a list of 118 verb lemmata from all differ-198Flat structure Hierarchical Structure   Georgian : gagi≈°vebtTrans : ‚Äò We will let you go ‚Äô   Args : -1,-2   Turkish : kedisisinTrans : ‚Äò you are his cat ‚Äô   Args : N - , -2 , -3   Swahili : ninakupendaTrans : ‚Äò I love you ‚Äô   Args : -1,-2   Hebrew : emdataTrans : ‚Äò her position ‚Äô   Args : N - , -3-   ent classes . Every verb was manually annotated   with its stem , its thematic affix and principal parts ,   to automatically generate the full inflection tables .   This automatic generation of Georgian verbs is   prone to some errors , for instance , in accounting   for idiosyncratic phonologically - conditioned stem   changes . Hence , we ran our data through 3 native   Georgian speakers to assert its correctness , or fix   when needed . In cases where speakers were un-   sure we used a Georgian morphological analyzer   ( Doborjginidze and Lobzhanidze , 2012 ) for consul-   tation . In cases of disagreement , we used a majority   vote among the speakers . On average , at least one   speaker was uncertain in about 5 % of the forms ,   but a disagreement that necessitated a majority vote   occurred only on about 0.7 % of the cases .   Table 3 summarizes the statistics over our anno-   tated data . In total , we produced 21,054 verb forms ,   of 118 lemmata . The data is quite evenly balanced   across the classes , with more verbs drawn from the   more frequent transitive class . For comparison , the   current UniMorph data has fewer lemmas , 3,300   forms , and includes only verbs that are transitive .   5 Experiments   To assess the usability of our dataset , we trained   a standard reinflection model , the character - level   LSTM of Silfverberg and Hulden ( 2018 ) , on our   data . We sampled from our data 2 datasets for   training morphological reinflection models , con-   taining train , validation and test sets in sizes 8 k , 1k   and 1kexamples , respectively . Following Goldman   et al . ( 2021 ) , one dataset employed an easier form-   split , i.e. , no forms appear in both train and test ,   and the other with the more challenging lemma-   split , where lemmas from train , dev and test are   disjoint . To assess the generalization capacity we   varied the sources of both the train and test sets .   We report 2 evaluation metrics : accuracy over exact   matches , and average edit distance from gold.199   Results and Analysis Table 4 presents the   model ‚Äôs performance for all train - test combina-   tions . It shows that the model ‚Äôs performance on   the new data ( top line combination ) is largely on   par comparing to its performance over training and   testing on UniMorph ‚Äôs original data ( bottom com-   bination ) . However , the model generalizes poorly   from the original partial data to the forms in our   test set which reflect the entire Georgian inflec-   tional system . Generalization from our data to Uni-   Morph ‚Äôs set is a lot better . The results also show   that the splitting method is crucial for success of   the model , as it inflects easily to unseen forms ,   but much harder when inflecting forms in a pre-   viously unseen lemma . These results corroborate   the results of Goldman et al . ( 2021 ) regarding the   difficulty of lemma - split data . Although the accu-   racy over the lemma split data is negligible , the   average edit distance in that case points again to   the conclusion that generalization from UniMorph   to our data is harder that the other way around .   Error Analysis To provide insights into the chal-   lenge of reinflecting morphologically complex   forms , we manually sampled the erroneous output   of the model trained and tested over our lemma-   split data , to draw insights on the points of failure .   In many cases the model succeeded in copying and   modifying the verb stem , but failed to output the   other morphemes correctly . Sometimes the errors   were due to inflection to an incorrect TAM com-   bination of the same lexeme , and sometimes the   inflection was done to the correct TAM but to a dif-   ferent derivationally - related lemma ( e.g. change of   voice in addition to the change of TAM ) . We con-   clude that the fact that our datasets include lemmas   from diverse classes that may have derivational rela-   tions makes the inflection task significantly harder .   Interestingly , the model managed to predict the   correct subject and object affixes most of the time.6 Conclusion   This paper proposes a transition of the UniMorph   annotation standard to a layered hierarchical anno-   tation of features . This revised schema caters for   complex marking phenomena including multiple   pronominal agreement . We apply it to Georgian ,   and construct a corresponding new dataset that is   large , balanced , complete with respect to grammat-   ical phenomena in the Georgian verb system and   verified by native - speakers . Our experiments with   a standard reinflection model on the old and new   Georgian datasets shows that the old UniMorph   dataset does not generalize well to the new test-   set , due to its partial coverage . This work is in-   tended to encourage the community to extend the   annotation of different languages to include phe-   nomena such as polypersonal agreement and others   that can be dealt with using a hierarchical anno-   tation , ultimately leading to more complete and   consistent benchmarks for studying non - trivial and   less - explored areas of computational morphology .   Acknowledgements   The first author would like to thank the native   Georgian speakers : Simon Guriel , Silvia Guriel-   Agiashvili and Nona Atanelov for their invaluable   help in the data annotation process . This research   was funded by the European Research Council un-   der the European Union ‚Äôs Horizon 2020 research   and innovation programme ( grant agreement No .   677352 ) and by a research grant from the Ministry   of Science and Technology ( MOST ) of the Israeli   Government , for which we are grateful .   References200201A Learning Curves   Fig . 1 exemplifies the sufficiency of our dataset   for training an inflection model on form - split data   as doubling the data amount from 4,000 to 8,000   yields relatively minor improvement . It also shows   that for the lemma - split data , the model completely   fails . It starts improve marginally with more than   2,000 examples , although its performance remains   far from satisfactory . This leaves room for explo-   ration of bootstrapping and augmentation methods   or more sophisticated modeling to improve results .   B Tech - Spec   All algorithms described in the paper were executed   on a single machine equipped with one NVIDIA TI-   TAN Xp GPU , 16 Intel i7 - 6900K(3.20GHz ) CPUs   and 126 GB RAM . Since the LSTM algorithm was   implemented on DyNet , there was no need of the   GPU , and all the calculations were done using only   the CPU .   C Hyper Parameters   1 . Embedding size = 100   2 . Hidden state size = 100   3 . Attention size = 100   4 . Number of LSTM layers = 1   During training , we experimented with several val-   ues for the hyper - parameters detailed above . How-   ever , for all the combinations we tried , the results   barely changed both at the form - split setting and   the lemma - split setting.202   Zheng LiZijian WangMing TanRamesh NallapatiParminder Bhatia   Andrew ArnoldBing XiangDan RothCornell UniversityAWS AI LabsUniversity of Pennsylvania   zl634@cornell.edu { zijwan , mingtan}@amazon.com   { rnallapa , parmib , anarnld , bxiang , drot}@amazon.com   Abstract   1 Introduction   Pretrained sequence - to - sequence ( seq2seq ) mod-   els such as BART ( Lewis et al . , 2020 ; Liu et al . ,   2020 ) and T5 ( Raffel et al . , 2020 ; Xue et al . , 2021 )   have shown great success in various natural lan-   guage processing ( NLP ) tasks , such as text sum-   marization ( Nallapati et al . , 2016 ; See et al . , 2017 ;   Narayan et al . , 2018 ) , machine translation , ques-   tion answering ( Fan et al . , 2019 ) and information   extraction ( Zhou et al . , 2021 ) . However , such   large - scale pre - trained language models come with   hundreds of millions of parameters : Lewis et al.(2020 ) trained a BART model with 400 M parame-   ters , while Raffel et al . ( 2020 ) pushed the limit to   11 billion parameters in T5 .   The continual growth in model sizes leads to sig-   nificant demand in both computation and memory   resources during inference , and poses a huge chal-   lenge on deployment , especially in real - time and/or   resource - constrained scenarios . This motivates re-   searchers to compress large pre - trained models to   be smaller and faster while retaining strong perfor-   mance . Among existing compression approaches   such as weight - sharing ( Dehghani et al . , 2019 ; Lan   et al . , 2020 ) , low - rank approximation ( Ma et al . ,   2019 ; Lan et al . , 2020 ) , and pruning ( Michel et al . ,   2019 ) , quantization approaches have received at-   tention recently since they reduce model footprints   using lower bits for the weight values without   changing the carefully - designed model architec-   ture . Most prior work on transformer quantization   focused on BERT - based transformers ( Zhang et al . ,   2020 ; Zafrir et al . , 2019 ; Bai et al . , 2021 ) . How-   ever , efficient quantization on the encoder - decoder   transformers is insufficiently studied . Prato et al .   ( 2020 ) achieve 8 - bit quantization for a seq2seq   transformer without significant loss of performance   but low - bit quantization proved to be difficult for   this model ( 4 - bit performance in Table 2 in their   work ) due to the accumulation of quantization er-   rors in seq2seq models . Moreover , their work did   not target quantizing large - scale pre - trained lan-   guage models , nor could it be applied to other   NLP tasks besides machine translation . Meanwhile ,   model distillation which transfers knowledge from   a large teacher model to a smaller student model   has been widely investigated for BERT compres-   sion ( Sanh et al . , 2019 ; Jiao et al . , 2020 ) .   Recently , Shleifer and Rush ( 2020 ) applied   ‚Äú shrink and fine - tune ‚Äù distillation method on BART   for text summarization , yet their work focuses more   on the methodology for distilling text summariza-   tion only . Besides , their work did not yield a sig-203nificant model footprint reduction , one of the most   challenging issues in the deployment of large mod-   els in resource - constrained scenarios .   In this work , we try to address the challenge   of building a more efficient seq2seq model by an-   swering two research questions : first , how well   does the quantized seq2seq model perform on var-   ious tasks ? Second , how do we combine quan-   tization and distillation to push the limit of com-   pressing the seq2seq model without significant per-   formance losses in challenging tasks like summa-   rization and question answering ? To this end , we   proposed a joint distillation and quantization frame-   work , which efficiently transfers the knowledge   from a full - precision teacher seq2seq model to its   student with fewer layers and ultra - low bits for   encoding its parameters . Experimental results on   BART show that the proposed models reduce the   model footprint by 16.5x while preserving competi-   tive performances on multiple language generation   benchmarks , and further illustrate the performance-   efficiency trade - off of compressing seq2seq models   up to 27.7x smaller . To the best of our knowledge ,   this is the first work aiming to effectively distill and   quantize seq2seq pre - trained models for language   generation tasks .   2 Distilling and Quantizing BART   In this section , we consider two directions for re-   ducing the size of our generative language model :   quantization ( ¬ß 2.1 ) and distillation ( ¬ß 2.2 ) . We ap-   ply distillation - aware training ( ¬ß 2.3 ) to train a quan-   tized and distilled low - precision model as a student   model to emulate the full - precision teacher model .   2.1 Quantization   Quantization refers to the operation of mapping a   real ( high - precision ) number to its low - precision   counterpart in order to achieve model footprint re-   duction . There has been extensive study on ap-   plying quantization to training neural networks .   Different quantization schemes include , e.g. , lin-   ear quantization ( e.g. , Hubara et al . , 2016 , 2017 ;   Jacob et al . , 2018 ) , non - linear quantization ( Li   and Sa , 2019 ) , approximation - based quantization   method ( Lin et al . , 2016 ) , and loss - aware quanti-   zation ( Hou and Kwok , 2018 ) . In our work , we   used the approximation - based method with linear   quantization following Zhang et al . ( 2020 ) .   Quantizing BART We applied quantization to   the weights of all the hidden layers and most ofthe embeddings . Following previous work ( Zhang   et al . , 2020 ) , we did not quantize positional embed-   dings and quantized activations only to 8 bits .   Weight Quantization We dive into the mathe-   matical details of how to quantize the weights in   BART models . Let us denote w‚àà Ras the   vector obtained by stacking all the columns of the   full - precision weight matrix Wthat we wish to   quantize at iteration t. By quantizing w , we are   looking for a scaling factor ( also known as quanti-   zation step ) Œ±and a low - precision number b , to   replace full precision weight wwithŒ±b . When   quantizing with more than 2 bits , we are applying   the commonly used symmetric linear quantization ,   with   Œ±= max|w|/ th   b‚àà { ‚àí th,¬∑¬∑¬∑,‚àí1,0,1 , ¬∑ ¬∑ ¬∑ , th }   where th= 2‚àí1andnis the number of   bits we use for quantization . Then bcan be ob-   tained by b = round ( w / Œ± ) . When quantizing   with 2 bits , we use the approximation based TWN   method ( Li et al . , 2016 ) . The mathematical details   are provided in Appendix A.   2.2 Distillation   The second task we consider is knowledge dis-   tillation , where we train a smaller student model   to mimic the behavior of a larger teacher model ;   specifically , we want to reproduce the output logits ,   attentions , and hidden states of the teacher model .   Following Shleifer and Rush ( 2020 ) , we initialize   the student model by copying the weights from   maximally spaced layers of the teacher model , e.g. ,   when initializing a 3 - layer student encoder ( de-   coder ) from a 6 - layer teacher encoder ( decoder ) ,   we copy the 0th , 3th and 5th layers from the teacher   to the student . When copying only 1 layer , we   choose the last instead of the first , which has been   shown empirically to yield better performance . Dif-   ferent than Shleifer and Rush ( 2020 ) who only dis-   till the decoder , we distill both the encoder and   the decoder . After initialization , we fine - tune the   student model with the combined objective of task   loss and distillation loss , i.e. L+L , with   L = L+L+L   where the RHS are MSE losses measuring the dif-   ference between the student and teacher with re-   gard to output logits , attention scores ( including204   encoder attention , decoder attention and cross at-   tention ) , and hidden states ( including all encoder   and decoder layers).We include the details of the   loss in Appendix B for completeness .   2.3 Distillation - aware quantization   To fine - tune our quantized and distilled model , we   use the technique of distillation - aware quantization   with a teacher - student architecture from ( Zhang   et al . , 2020 ) . We treat the quantized and distilled   low - precision model as a student model trained to   emulate the full precision model , which in this case   is the teacher model . Meanwhile , we also keep the   full - precision distilled counterpart of the student   model for parameter update . At each iteration , we   first quantize the full precision student model to   get its quantized version , then do the forward pass   with the low - precision student model and get the   task loss as well as the distillation losses discussed   in ¬ß 2.2 . Finally , we use these losses to update the   parameters in the full - precision student model.3 Experiments and Discussions   In this section , we evaluate the efficacy of jointly   Distilling and Quantizing BART ( hereinafter , DQ-   BART ) on text summarization and long - form ques-   tion answering using three benchmarks : CNN / Dai-   lyMail ( See et al . , 2017 ) , XSUM ( Narayan et al . ,   2018 ) , and ELI5 ( Fan et al . , 2019 ) . We additionally   study machine translation with mBART on WMT   English - Romanian ( En - Ro ) ( Bojar et al . , 2016 ) .   3.1 Experimental Setup   We followed the standard splits of these datasets .   The statistics could be found in Appendix C. For   ELI5 , we reproduced the author ‚Äôs implementation   to train a dense retriever that retrieves 10 support-   ing documents from Wikipedia for each question .   Additional details could be found in Appendix D.   As our target is achieving efficient seq2seq gener-   ative models , we used base - sized BART for summa-   rization and question answering tasks . For machine   translation , we used mBART - large due to the lack   of pretrained base - sized multilingual BART mod-   els . We reused existing models , and finetuned our   own models on end tasks when no open - sourced   model is available . We trained our quantized - only   models for 10 epochs and distilled - and - quantized205models for 20 epochs . We used a batch size of   128 , a learning rate of 3√ó10with 5 % linear   warmup , and selected the best model based on   rouge - L scores on the development set . We set gen-   erative hyperparameters following previous work   ( Lewis et al . , 2020 ) . All experiments were per-   formed on A100 GPUs .   3.2 DQ - BART Results and Discussions   We summarized the main results in Table 1 and   visualized the performance on text summarization   on the CNN / DailyMail dataset in Figure 1 . Addi-   tional visualizations are in Appendix E. We found   that :   1.Direct quantization performs poorly in genera-   tion tasks . The rouge - L score drops50 - 75 %   relatively compared with the baseline .   2.The performance of 8 - bit distillation - aware   quantized models ( ‚Äú 8 - 8 - 8 6 - 6 ‚Äù ) achieves com-   parable or even better performance compared   with the full precision models across all tasks ,   signaling that 8 - bit is not too challenging for   generative models like BART , similar to the   findings for BERT ( Zhang et al . , 2020 ) .   3.We were able to achieve a 13.6x model size   compression ratio when using 2 - bit quantiza-   tion with the trade - off of slight performance   drop for summarization tasks and even no per-   formance drop for the long - form QA task .   4.Combining quantization and distillation gives   us a further boost in model compression ratio   without significant further sacrifice in perfor-   mance . For example , when using 2 - bit quan-   tization , by cutting the layers of the decoderin half ( from ‚Äú 2 - 2 - 8 6 - 6 ‚Äù to ‚Äú 2 - 2 - 8 6 - 3 ‚Äù ) , we   only saw < 0.5rouge - L performance drop   across all tasks while getting another 2.9x   compression .   5.When pushing the compression rate to the   limit ( ‚Äú 2 - 2 - 8 1 - 1 ‚Äù ) , we were able to achieve   a 27.7x compression ratio while still preserv-   ing reasonable performance . We observed   a rouge - L drop of 5.67 for CNN / DailyMail   ( 42.09‚Üí36.42 ) , 12.24 for XSUM ( 35.71‚Üí   23.47 ) , and 1.06 for ELI5 ( 15.36‚Üí14.30 ) .   Thus , for certain tasks a large model com-   pression ratio would not lead to a significant   performance drop while for others the drop   could be huge , suggesting that the specific   compression ratio to use should be decided   on a task - by - task basis with the trade - off of   performance and efficiency in mind .   3.3 DQ - mBART for Translation   We further extend our study to see how distilla-   tion and quantization work for mBART ( Liu et al . ,   2020 ) , a deeper multilingual model . We experi-   mented mBART - large on WMT English - Romanian   translation task ( Bojar et al . , 2016 ) . The results are   in Table 2 .   We found that distillation - aware quantization   yields reasonably good performance , similar to the   findings in DQ - BART ( Table 1 ) . However , the   performance drops substantially when performing   2 - bit quantization with distillation , possibly due to   the accumulation of the distillation / quantization er-   ror becoming more significant with deeper models   and the challenging nature of machine translation.206Future work may explore how to improve the per-   formance of joint distillation and quantization for   deep models under a low - bit setting .   3.4 Distillation and Quantization v.s.   Distillation Only   We want to understand how much gain there is   when doing joint distillation and quantization com-   pared with distillation - only method ( Shleifer and   Rush , 2020 ) . To do so , we trained distillation - only   models and compared them with DQ - BART with   a similar size . From Table 3 , we found that joint   distillation and quantization performs much better   across all tasks , signaling the huge gain with joint   distillation and quantization . Additional ablation   study on ‚Äú Shrink and Finetune ‚Äù could be found in   Appendix F.   4 Conclusion   Transformer - based pre - trained seq2seq language   models like BART have greatly advanced the state   of the art in a range of NLP tasks . Yet , these   extremely large - scale models pose a challenge in   resource - constrained scenarios . To alleviate this is-   sue , we proposed DQ - BART , a jointly distilled and   quantized BART model . Empirical results show   that , despite the difficult nature of language gen-   eration tasks , we achieve a 16.5x model footprint   compression ratio with little performance drop on   three generative benchmarks , and further present   the performance - efficiency trade - off for seq2seq   models up to a 27.7x compression ratio . Addition-   ally , we studied distillation and quantization for   mBART on a machine translation task , and high-   lighted the challenge of joint low - bit quantization   with distillation for deeper models on cross - lingual   tasks . To the best of our knowledge , our method   is the first to apply joint quantization and distilla-   tion on pretrained language models , and this is the   first work aiming to effectively distill and quantize   seq2seq pretrained models for language generationtasks . We hope this work could open doors for de-   veloping and applying efficient seq2seq language   models . We leave additional compression methods   like attention head pruning ( Michel et al . , 2019 )   and sequence - level distillation ( Kim and Rush ,   2016 ) , and the measurement of latency improve-   ments in various settings for future work . Our code   is available at https://www.github.com/   amazon - research / dq - bart/ .   Acknowledgment   We would like to thank colleagues at AWS AI Labs   and our anonymous ARR reviewers for their con-   structive feedback .   References207208   A Details of TWN Quantization   When quantizing using 2 bits ( which is also know   as ternarization ) , following Zhang et al . ( 2020 ) ,   we apply the TWN method ( Li et al . , 2016 ) . To   quantize w , we are looking for scaling factor Œ± > 0   andb‚àà { ‚àí 1,0,1}such that w‚àºŒ±bwhere nis   the dimension of w. To minimize the quantization   error , we have the following optimization problem :   Œ± , b= arg max||w‚àíŒ±b||   where Œ± > 0,b‚àà { ‚àí 1,0,1}Denote ‚àÜas a threshold and I(x)be a function   such that   I(x ) = Ô£±   Ô£¥Ô£≤   Ô£¥Ô£≥1,ifx > ‚àÜ   0,if‚àí‚àÜ‚â§x‚â§‚àÜ   ‚àí1,ifx < ‚àí‚àÜ   and denote set J={i|I(w)Ã∏= 0 } , then ac-   cording to Hou and Kwok ( 2018 ) , the solution to   the previous optimization problem can be reached   at   b = I(w ) , Œ±=||w‚äôb||   ||b|| ,   with‚àÜ= arg max1   |J|Ô£´   Ô£≠X|w|Ô£∂   Ô£∏   where ‚äôis element - wise multiplication and || ¬∑ ||   is the l - norm . To approximate this result , we set   ‚àÜ= 0.7||w||/dim ( w)then compute Œ±andb   accordingly .   B Details of Distillation Losses   The distillation losses is defined as the following :   L = L+L+L   In this section we ‚Äôll go through each part of the   losses . We denote œï ( ¬∑ ) , œï(¬∑)as the functions   that map the index of an encoder / decoder layer of   the student model to the index of the teacher model   layer that it is trained to emulate , the details of   which is discussed in ¬ß 2.2 , and we use l , lto   denote the number of encoder layers and decoder   layers of the student model . To illustrate , if l=   3 , l= 2 , we would have :   œï(0,1,2 ) = 0 , 3,5 , œï(0,1 ) = 0 , 5   For simplicity , we use superscript ¬∑ , ¬∑ to dis-   tinguish counterparts from the student model and   teacher model respectively .   Next , we will explain the definition of each part   of the distillation losses .   Firstly , L is the Mean Squared Error ( MSE )   between the output logits of the student model and   that of the teacher model , i.e.   L = MSE ( logits , logits )   Secondly , Lis the attention distillation loss ,   which is the sum of distillation losses of encoder209attentions ( EA ) , decoder attentions ( DA ) , and cross   attention ( CA ) , i.e.   L = L+L+L   where   L = XMSE ( EA , EA )   L = XMSE ( DA , DA )   L = XMSE ( CA , CA )   with the subscripts i , œï(i)specifying the indices of   the layers .   Finally , Lis the distillation loss between   all the hidden states between student layers and   teacher layers , which include encoder hidden states   ( EHS ) and decoder hidden states ( DHS ):   L = L+L   where   L = XMSE ( EHS , EHS )   L = XMSE ( DHS , DHS )   C Dataset Statistics   D ELI5 Additional Details   In this section , we present additional details for the   ELI5 dataset .   D.1 Dense Retriever   We were not able to find a public version of sup-   porting documents for ELI5 , and thus followed the   author ‚Äôs implementationto train a dense retrieverthat retrieves support documents from Wikipedia .   Our trained retriever achieves a similar perfor-   mance compared with the one reported in the au-   thor ‚Äôs implementation ( recall : ours 0.3273 , re-   ported 0.3247 ) .   D.2 Evaluating ELI5 Results   We use the - packageto calculate   rouge scores through the paper . However , as the   author of ELI5 pointed out , the original rouge   implementation used in ELI5 and BART papers   performs additional normalization . For consistency ,   we also reported results for ELI5 using the same - package , which differs from the   one used in ELI5 / BART . Here we compared the   performance of our trained ELI5 baseline model   with the public one using the rouge implementation   used in ELI5 / BART papers .   Results in Table 5 shows that the performance of   our base - size model is close to the one with large-   size reported in Lewis et al . ( 2020 ) . This signals   that our baseline model for ELI5 is well - trained .   E Visualizations of Experimental Results   on XSUM and ELI5 datasets210   F Comparisons on ‚Äú Shrink and   Finetune ‚Äù   We benchmarked the performance of three ran-   domly picked models with the ‚Äú Shrink and Fine-   tune ‚Äù schema proposed in Shleifer and Rush ( 2020 ) .   We ran the models using the same hyperparame-   ter settings we used in this paper . The results are   shown in Table 6 .   We found that when using distillation losses be-   tween the teacher and the student , the performance   are slightly better than the ‚Äú Shrink and Finetune ‚Äù   method under our setting . This signals that having   guidance in weighting is important for a quantized   and distilled model to learn well.211   Chao ZhaoWenlin YaoDian Yu   Kaiqiang SongDong YuJianshu Chen   zhaochao@cs.unc.edu   { wenlinyao , yudian , riversong , dyu , jianshuchen}@tencent.comUNC Chapel Hill , Chapel Hill , NCTencent AI Lab , Bellevue , WA   Abstract   1 Introduction   Dialogue comprehension ( Sun et al . , 2019 ; Cui   et al . , 2020 ) aims to capture diverse kinds of key in-   formation in utterances , which are either scattered   around or implicitly implied in different turns of   conversations . Therefore , it requires different ca-   pabilities such as paraphrasing ( Falke et al . , 2020 ) ,   summarizing ( Gliwa et al . , 2019 ) , and common-   sense reasoning ( Arabshahi et al . , 2021 ) . Recent ad-   vances in pre - trained language models ( PLMs ) ( De-   vlin et al . , 2019 ; Radford et al . , 2019 ) have been   applied to the problem ( Jin et al . , 2020 ; Liu et al . ,   2021 ) . However , these PLMs are generally pre-   trained on formal - written texts , which are differentfrom dialogue data in nature . Specifically , dia-   logues are composed of colloquial languages from   multi - speakers , and utterances usually have com-   plex discourse structures ( Afantenos et al . , 2015 ) .   Therefore , applying these models directly to dia-   logue comprehension , especially in low - resource   settings , is sub - optimal .   To learn better dialogue representations , recent   studies have designed several dialogue - specific pre-   training objectives such as speaker prediction ( Qiu   et al . , 2021 ) , utterance prediction ( Chapuis et al . ,   2020 ) , response selection ( Wu et al . , 2020 ) , and   turn order restoration ( Zhang and Zhao , 2021 ) .   These methods , albeit improve over the vanilla   PLMs , usually rely on surface - level dialogue in-   formation . In particular , they still fail to train the   models to explicitly learn the aforementioned capa-   bilities which are critical for dialogue comprehen-   sion ( e.g. , linguistic knowledge , world knowledge ,   and commonsense knowledge ) . Furthermore , it   was not able to incorporate knowledge beyond di-   alogue ( e.g. , non - verbal communications between   speakers , as well as time and location information ) ,   which are also crucial for dialogue comprehension .   To pre - train a zero - shot dialogue comprehension   model with the aforementioned features , we de-   velop a novel generative pre - training strategy that   learns by narrating the key information from a   dialogue input ( see Figure 1 for an example ) . In   particular , the generated narrative text is supposed   to not only ( i ) paraphrase the gists of the dialogue   but also ( ii ) carry certain inferred information ( e.g. ,   the time and location of a scene and relations be-   tween speakers ) that are not explicitly mentioned in   the dialogues . Learning to narrate such information   helps the model to learn varied lexical , syntactic ,   and semantic knowledge of dialogue . It also en-   hances the model ‚Äôs ability to infer extra information   beyond the literal meaning within dialogues , which   will benefit the model ‚Äôs capability of dialogue com-   prehension.212   However , the learning - by - narrating strategy   would require a dialogue - narrative parallel cor-   pus , which , to our best knowledge , is not pub-   licly available . For this reason , we first create   DIANA , a large - scale dataset with ( DIAlogue ,   NArrative ) pairs automatically collected from sub-   titles of movies and their corresponding plot syn-   opses . We consider dialogues from movie subtitles   as they are close to daily human - to - human conver-   sations ( Zhang and Zhou , 2019 ) . In addition , the   movie synopses include rich narrative information ,   which is helpful for dialogue comprehension . After   data collection and strict quality control , we ob-   tain a dataset with 243 K ( dialogue , narrative ) pairs   written in English . As the automatic data construc-   tion procedure is language - independent , it can be   applied to low - resource languages as well .   We then pre - train a BART model ( Lewis et al . ,   2020 ) on the constructed corpus with the proposed   learning - by - narrating strategy , and evaluate it on   four dialogue - based tasks that require comprehen-   sion . In zero - shot settings , our pre - trained model   outperforms the BART baseline on all tasks by a   large margin ( e.g. , +8.3%on DREAM ( Sun et al . ,   2019 ) ) , demonstrating the success of our approach .   The contributions of this paper are three - fold :   ‚Ä¢We propose a novel learning - by - narrating pre-   training strategy for dialogue comprehension ;   ‚Ä¢We release DIANA , a new large - scale   dialogue - narrative parallel corpus ;   ‚Ä¢Experiments show that our pre - trained dia-   logue comprehension model achieves superior   zero - shot performance on a variety of down-   stream tasks .   2 DIANA : A Dialogue - Narrative Corpus   In this section , we describe the procedure to create   the dialogue - narrative parallel dataset.2.1 Data Collection and Segmentation   We collect 47,050 English subtitles of movies and   TV episodes released from Opensubtitle ( Lison   et al . , 2018 ) and their corresponding synopses from   online resources such as Wikipedia and TMDB . To   link the subtitle and synopsis of the same movie or   TV episode , we require a subtitle and a synopsis to   have the same title and the release year , as well as   a high overlap rate ( > 50 % ) on role names .   The subtitle and synopsis of a movie are too long   for a PLM . To facilitate pre - training , we split both   the subtitle and synopsis into smaller segments and   align the related segments from each part to shorter   ( dialogue , narrative ) pairs . We split subtitles using   the time interval Œ¥between utterances and split a   synopsis into sentences . We set Œ¥= 5s .   2.2 Data Alignment   We aim to align the dialogue sessions { d , . . . , d }   and narrative segments { s , . . . , s}with maxi-   mum global similarity to form ( dialogue , narrative )   pairs . For each dialogue session d , the goal is to   find its corresponding narrative segment s.   Inspired by ( Tapaswi et al . , 2015 ) in which the   narrative in a synopsis follows the timeline of a   movie or a TV episode , we develop a dynamic   time warping method to find the globally optimal   alignment score . During aligning , some narrative   segments contain information beyond the dialogue ,   so they can not be aligned to any dialogue session .   We therefore allow our algorithm to skip at most k   narrative segments during alignment searching :   whereA(i , j)denotes the optimal alignment score   of the first inarrative segments and the first jdi-   alogue sessions . S(s , d)is the text similarity   between sandd .   We compare the performance of three text simi-   larity measures : Jaccard similarity , Rouge-1F , and213   TF - IDF . In consideration of time efficiency , we   do n‚Äôt apply more advanced neural methods . We   compare these similarity measures on MovieNet   dataset ( Huang et al . , 2020 ) , which provides a man-   ual alignment between the segments of subtitles   and synopses of 371 movies . We evaluate the per-   formance of each similarity measure by alignment   accuracy , a.k.a , the percentage of dialogue sessions   that are correctly aligned to the corresponding nar-   rative segment . As shown in Table 1 , TF - IDF per-   forms best among all similarity measures . We also   find that a narrative - wise Lnormalization of the   TF - IDF can further improve the alignment accu-   racy . It helps to penalize the similarity of ( d , s )   when shas high similarity with many dialogues   ( e.g. , when scontains common words or protag-   onists ‚Äô names . ) We therefore choose the normal-   ized TF - IDF as our similarity function . We further   analyze the errors during alignment and find that   85.94 % of errors happen because the dialogue ses-   sion is aligned to the previous or next segment of   the gold narrative segment . It indicates that most of   the errors happen locally . Figure 2 shows an exam-   ple from MovieNet , where the red line and the blue   line indicate the gold alignment and the predicted   alignment via normalized TF - IDF , respectively . It   shows that the two lines are generally well over-   lapped except for some local discrepancies .   2.3 Quality Control   After data alignment , each narrative segment scan   be aligned to multiple dialogues . To consider the   local alignment errors , we also merge the aligned   dialogues of sandsto the dialogues of s.   Some of these dialogues may not be relevant to s.   To select the relevant dialogues , we use a greedy   method to incrementally select dialogues until the   rouge - F score between the narrative and the se-   lected dialogues does n‚Äôt increase . After selection ,   we concatenate the selected dialogues and preserve   their relative position . We finally obtain around 1.5   Million ( dialogue , narrative ) pairs .   To further improve the quality of data , we fil-   ter out pairs where the dialogue and the narra-   tive are irrelevant . To evaluate the relevance , we   use two automatic measures : Coverage and Den-   sity ( Grusky et al . , 2018 ) . Low Coverage and Den-   sity indicate that the narrative text is either too   abstractive or irrelevant to the dialogue . We thus   only select the pairs with Coverage > 0.5and   Density > 1 . After this strict quality control , we   obtain 243 K ( dialogue , narrative ) pairs as the final   DIANA dataset , which is a high - quality subset of   the original dataset . The average length of the dia-   logue and the narrative are 58 tokens and 18 tokens ,   respectively .   2.4 Analysis of Knowledge Type   To analyze what types of knowledge are included   in DIANA , we randomly sample 100 instances and   manually categorize the relation between dialogue   and the corresponding narrative text into seven   knowledge types . We show the percentage of each   knowledge type in parentheses and in Figure 3 as   well . The knowledge types are :   ‚Ä¢Summarizing ( 39 % ): The narrative text sum-   marizes multiple utterances as a concise state-   ment to reflect the salient event or information   of the dialogue .   ‚Ä¢Visual / Audial ( 17 % ): The narrative text pro-   vides extra visual or audial information of the   dialogue , such as the location of the dialogue ,   the speakers ‚Äô actions , and ambient sounds .   ‚Ä¢Paraphrasing ( 14 % ): The narrative text re-   states speakers ‚Äô utterances using other words .   ‚Ä¢Text Matching ( 9 % ): The narrative text is di-   rectly copied from the utterances of speakers.214   ‚Ä¢Implicit ( 10 % ): The narrative text provides   extra information that is not explicitly men-   tioned in the dialogue .   ‚Ä¢Causal ( 6 % ): The narrative text describes the   cause and effect relationship between events .   ‚Ä¢Interpersonal ( 5 % ): The narrative text re-   veals the relationships between speakers .   Among these knowledge types , Summarizing   andVisual / Audial are the two most frequent ones .   They are followed by Paraphrasing andText Match-   ing , which contribute to 23 % in total . It also   shows that narratives use paraphrasing more of-   ten than copying . Additionally , DIANA contains   three higher - level knowledge types that require the   awareness of real - world commonsense and more   complicated inference such as implicit knowledge ,   causal relationships , and interpersonal relation-   ships . The diverse knowledge types in DIANA   indicate the benefit of this dataset for dialogue com-   prehension and other downstream tasks as well .   3 Pre - training : Learning - by - Narrating   During pre - training , we aim to inject the knowl-   edge contained in DIANA into pre - trained models .   One option is to ask the model to distinguish be-   tween a correct narrative and an incorrect narrative   via a classification objective . However , it requires   carefully designing additional non - trivial negative   ( dialogue , narrative ) pairs . Therefore , we propose   to directly generate a narrative text from the given   dialogue by maximizing the generative probability :   p(y|x;Œ∏ ) = Yp(y|y , x;Œ∏),(2 )   where xare dialogue texts and yare narrative texts .   There are two main advantages of using the gen-   erative objective . First , it can fully leverage the nar-   rative information from each token of the narrativetext with no need to construct negative pairs . Sec-   ond , the pre - trained model can be directly applied   to both generative and discriminative downstream   tasks without further fine - tuning . For discrimina-   tive tasks , we calculate the probability of each can-   didate according to Equation 2 and choose the most   probable candidate as the predicted answer .   4 Experiments   In this section , we evaluate the performance of the   pre - trained model on four downstream tasks that   require dialogue comprehension .   4.1 Setting   We use BART , a state - of - the - art sequence - to-   sequence model , as our baseline model . We use   its released checkpoint and further pre - train the   model on DIANA . During pre - training , we con-   catenate the utterances as the input and update the   parameters to maximize the probability of the corre-   sponding narrative . We use Adam as the optimizer ,   and we set the learning rate and weight decay to   3√ó10and 0.01 , respectively . Following previous   studies that suggest that a larger batch size helps   pre - training , we set the batch size to 1024 and pre-   train the model for 1,000 steps .   4.2 Tasks   We evaluate our model ‚Äôs ability of dialogue com-   prehension on four downstream tasks . DREAM   ( Sun et al . , 2019 ) aims to read a dialogue and se-   lect the correct answer from options of a dialogue-   related question . To make the task similar to our   pre - training task , we follow previous work ( Chen   et al . , 2021 ) to train a T5 model to convert each   ( question , answer ) pair to a statement . PCMD ( Ma   et al . , 2018 ) is a passage completion task . Given a   dialogue and a passage that describes the dialogue ,   a query is created by replacing a character mention   with a variable x , and the model needs to recover   the character mention . VLEP ( Lei et al . , 2020 )   aims to select the most probable future event given   the dialogue of the current event and two candi-   dates of future events . SAMSum ( Gliwa et al . ,   2019 ) is a dialogue summarization task to create a   concise abstractive summary for a dialogue . The   first three are discriminative tasks , and SAMSum   is a generative task . None of the source dialogues   in these tasks are included in DIANA.215   We evaluate the model performance on these   tasks under the zero - shot setting . For discrimina-   tive tasks , we convert each test instance with K   answer candidates as K(dialogue , narrative ) pairs .   Given the dialogue as input , we evaluate the con-   ditional probability of each narrative according to   Equation 2 and choose the most probable narrative   as the predicted answer . We use accuracy ( ACC ) as   the evaluation metric for discriminative tasks and   ROUGE for the summarization task .   We compare our pre - trained model ( Narrator )   with strong pre - trained baselines such as GPT-2 ,   RoBERTa , and BART . To investigate the impact   of the pre - training objective , we compare with 1 )   BART - DIAL - DE : the original BART de - noising   objectives , which is trained on the dialogue part   of DIANA ; and 2 ) BART - CNN - CLS : a classifica-   tion objective , which is trained using the CNNDM   dataset ( See et al . , 2017 ) to distinguish between   positive and negative summaries based on the doc-   uments . Negative summaries are obtained from   DocNLI ( Yin et al . , 2021 ) by replacing the words ,   entities , and sentences of positive summaries . We   also investigate the quality of DIANA by compar-   ing it with two large summarization datasets : CN-   NDM and CRD3 ( Rameshkumar and Bailey , 2020 ) .   We pre - train BART to generate the summaries of   these datasets from the corresponding documents   and refer to the models as BART - CNN - GEN and   BART - CRD3 - GEN . Besides the zero - shot models ,   we list the supervised results finetuned on BART   ( BART - FT ) as a reference for the upper bound .   4.3 Results   Results are shown in Table 2 . Our observations   are as follows . ( i ) When compared with vanilla   PLMs , Narrator outperforms GPT-2 , RoBERTa ,   and BART , demonstrating that the learning - by-   narrating pre - training objective can improve the   model ‚Äôs ability of dialogue comprehension . ( ii )   When compared with different pre - training tasks ,   Narrator outperforms BART - DIAL - DE , and BART-   CNN - GEN outperforms BART - CNN - CLS . This   indicates that the narrative - guided generative ob-   jective is more effective than the de - noising objec-   tive and the discriminative objective . ( iii ) When   compared with different pre - training data , Narra-   tor achieves better performance on all tasks com-   pared with BART - CNN - GEN and BART - CRD3-   GEN , demonstrating that DIANA is a more helpful   resource for dialogue comprehension .   We further analyze what types of knowledge are   enhanced during pre - training . To this end , we test   Narrator on a subset of the DREAM test set , which   includes annotated knowledge types released along   with the DREAM dataset . As shown in Table 3 ,   compared with the vanilla BART , Narrator achieves   better performance on all knowledge types except   Arithmetic , which is not covered in DIANA . The   performance gain indicates that the narrative pre-   training contributes the most to the knowledge re-   lated to paraphrasing and matching . It also benefits   from other knowledge types that require various   reasoning abilities such as commonsense reasoning   and logic reasoning .   5 Conclusion   We propose a learning - by - narrating strategy to pre-   train a zero - shot dialogue comprehension model .   We first construct a dialogue - narrative dataset   named DIANA , which contains 243 K ( dialogue ,   narrative ) pairs obtained by automatically aligning   movie subtitles with their corresponding synopses .   We then pre - train a dialogue comprehension model   based on DIANA and evaluate its performance on   four downstream tasks that require dialogue com-   prehension abilities . Experiments show that our   model outperforms strong pre - trained baselines ,   demonstrating that the learning - by - narrating strat-   egy is a promising direction for dialogue compre-   hension . We also hope that DIANA will promote   future research in related areas.216References217218   Ali Edalati   McGill UniversityMarzieh Tahaei   Huawei Noah Ark LabAhmad Rashid   Huawei Noah Ark Lab   Vahid Partovi Nia   Huawei Noah Ark LabJames J. Clark   McGill UniversityMehdi Rezagholizadeh   Huawei Noah Ark Lab   Abstract   1 Introduction   Recently , development and deployment of pre-   trained language models ( PLMs ) has improved the   performance of NLP models significantly ( Devlin   et al . , 2018 ; Radford et al . , 2019 ; Yang et al . , 2019 ;   Shoeybi et al . , 2019 ; Radford et al . , 2019 ) . PLMs   are mostly Transformer - based models , which are   pre - trained on enormous unlabeled data . Although   Transformer - based PLMs are powerful in perfor-   mance , their huge size is a barrier for efficient train-   ing or inference of these models on lower capac-   ity devices with memory , computation and energy   constraints . Therefore , there has been a growingvolume of literature focused on developing frame-   works for compressing these large PLMs .   Like other deep learning models , the main di-   rections of model compression for PLMs are us-   ing following methods in isolation or combination :   low - bit quantization ( Gong et al . , 2014 ; Prato et al . ,   2019 ) , pruning ( Han et al . , 2015 ) , knowledge dis-   tillation ( KD ) ( Hinton et al . , 2015 ) and matrix de-   composition ( Yu et al . , 2017 ; Lioutas et al . , 2020 ) .   PLMs can be divided into encoder - based and   auto - regressive models such as the BERT ( Devlin   et al . , 2018 ; Liu et al . , 2019 ) and GPT ( Brown   et al . , 2020 ) family respectively . Although the size   of BERT family models is usually smaller than   the GPT family , compressing the BERT family has   been investigated much more in the literature ( e.g.   DistilBERT ( Sanh et al . , 2019 ) , TinyBERT ( Jiao   et al . , 2019 ) , MobileBERT ( Sun et al . , 2020 ) , ALP-   KD ( Passban et al . , 2021 ) , MATE - KD ( Rashid   et al . , 2021 ) , Annealing - KD ( Jafari et al . , 2021 )   and BERTQuant ( Zhang et al . , 2020 ) ) . On the   other hand , to the best of our knowledge , the GPT   family has barely a handful of compressed mod-   els ( Li et al . , 2021 ) , among them the DistilGPT2   model has attracted wide attention in the literature .   The DistilGPT2 model is heavily pre - trained for 3   epochs on the large OpenWebText dataset . More-   over , it is evident in the literature that the GPT   model can not compete with BERT on natural lan-   guage understanding ( NLU ) tasks ( Liu et al . , 2021 ) .   Therefore , developing an efficient compressed GPT   model with comparable NLU performance is still   an open problem .   In this paper , we use Kronecker decomposition ,   which has been recently used for BERT compres-   sion ( Tahaei et al . , 2021 ) , for compression of the   GPT-2 model ( we refer to our model as KnGPT2   in this paper ) . We use Kronecker decomposition to   represent the weight matrices of linear layers in219GPT-2 by smaller matrices which can reduce the   size and computation overhead . We use Kronecker   decomposition to compress the embedding and   Transformer layers of GPT-2 . For Transformer   layers , the linear layers of multi - head attention   ( MHA ) and the feed - forward network ( FFN )   blocks of Transformer layers are replaced with   their Kronecker decomposition .   Kronecker decomposition leads to reduction in   expressiveness of the model . We use a very light   pre - training with intermediate layer knowledge dis-   tillation ( ILKD ) to address this issue , which im-   proves the performance of the compressed model   significantly . It is worth mentioning that for our   pre - training , we use 1/10of the DistilGPT2 ‚Äôs pre-   training data ( i.e. OpenWebText ) only for 1 epoch   ( instead of 3 epochs in DistilGPT2 ) . Furthermore ,   in this paper , our framework is applied to GPT-2   but it can be easily exploited to compress other   models as well . To summarize contributions of this   paper , we mention the following points :   ‚Ä¢To the best of our knowledge , we are the first   work which uses Kronecker decomposition   for compression of the GPT model .   ‚Ä¢Our KnGPT2 model , which is evaluated on   both language modeling and GLUE bench-   mark tasks , improves training efficiency and   outperforms DistilGPT2 significantly .   2 Related Works   ( Zhou and Wu , 2015 ) is the first work that used   summation of multiple Kronecker products to com-   press the weight matrices in fully - connected net-   works and small convolutional neural networks .   ( Thakker et al . , 2019 ) proposed a hybrid method   which separates the weight matrices into an upper   and a lower part , upper part remains untouched but   the lower part decomposes to Kronecker products .   They used this approach for small language models   to be utilized on internet of things ( IoT ) applica-   tions . Recently , ( Thakker et al . , 2020 ) extended   the mentioned hybrid method to non - IoT applica-   tions by adding a sparse matrix to the Kronecker   products . ( Tahaei et al . , 2021 ) has deployed a sim-   ilar approach to ours to compress BERT which   achieved promising results but to the best of our   knowledge , this work is the first attempt for GPT   compression using Kronecker decomposition . DistilGPT2is one of the most successful and   well - known compressed versions of GPT-2 which   is considered as a baseline in this paper . Distil-   GPT2 has 82 M parameters compared to 124 M pa-   rameters for GPT-2 and is trained using KD   on OpenWebTextCorpus which is a reproduction   of OpenAI ‚Äôs WebText dataset .   3 Methodology   3.1 Kronecker Product   The Kronecker product is a matrix operation ( de-   noted by ‚äó ) which takes two matrices as input   and generates a block matrix as output . Assume   thatAis a matrix ‚ààRandBis a matrix   ‚ààR , A‚äóBis equal to a block matrix   ‚ààR , where m = mm , n = nnand each   block ( i , j)is obtained by multiplying element a   by matrix .   A‚äóB=Ô£Æ   Ô£ØÔ£∞aB ¬∑ ¬∑ ¬∑ aB   .........   aB ¬∑ ¬∑ ¬∑ aB,Ô£π   Ô£∫Ô£ª ( 1 )   3.2 GPT-2 Compression using Kronecker   Factorization   We can represent a weight matrix , W‚ààR ,   by two smaller matrices , A‚ààRandB‚àà   Rsuch that W = A‚äóBandm = mm ,   n = nn . This leads to reduction in the number   of parameters from mnfor the original matrix to   mn+mnfor the Kronecker factorized ver-   sion . In large language models , embedding layer   usually takes a large portion of the memory . Let   W‚ààRbe the lookup table for the input em-   bedding where vis the vocabulary size and dis the   embedding dimension . To compress the embed-   ding layer using Kronecker decomposition we use   the same method as in ( Tahaei et al . , 2021 ) . We de-   fineA‚ààRandB‚ààR , where fis a   factor of d. There are two reasons for this decision :   first , similar to W , in the Amatrix every row   will indicate embedding of a single word . Second ,   the embedding of each word , E , can be obtained   byA‚äóB , therefore the computation complexity   of this operation is O(d)which is very efficient .   The transformer architecture is composed of N   identical layers each having MHA followed by   FFN . In the MHA module , there are linear lay-   ers which calculate the Query , Key and Value by220multiplying the input vector by W , , , respec-   tively . Also , in the FFN module , there are two fully   connected layers that can be represented as W   andW. In this work , all of the mentioned   weight matrices at different heads and layers of   the transformer are decomposed into Kronecker   factors .   For initialization , similar to ( Tahaei et al . , 2021 ) ,   the Kronecker factors ÀÜAandÀÜBare estimated from   the corresponding weight matrix Win the original   uncompressed pre - trained model using the solution   to the nearest Kronecker problem   ( ÀÜA,ÀÜB ) = arg min‚à•W‚àíA‚äóB‚à•   The solution to this optimization can be found by   rank-1 singular value decomposition ( SVD ) ap-   proximation of the reshaped , see ( Van Loan , 2000 )   for details .   3.3 Knowledge Distillation   In this section , the KD method used for both pre-   training and fine - tuning the KnGPT2 is explained .   LetTandSrepresent the teacher model , GPT-   2 , and the student model , KnGPT2 , respectively .   For a batch of data ( x , y),AttandAttare   the attention distributions of the last transformer   layer , obtained by applying softmax on the scaled   dot product between query and key ( For more de-   tails , see ( Wang et al . , 2020 ) . HandHare   the normalized hidden state outputs of the layer l.   In our experiments , the output of the embedding   layer is considered as a hidden state . Therefore , l   represents both transformer layers and embedding   layer . Note that by using the Kronecker factoriza-   tion , like other decomposition methods , the number   of layers and dimensions of the output matrices in   the student model remain intact so we can directly   obtain the difference of output of a specific layer   in student an teacher model without the need for   projection .   For the MHA modules , similar to ( Wang et al . ,   2020 ) , we use Kullback ‚Äì Leibler divergence ( KL )   between the attention distributions of the last trans-   former layers of the student and the teacher .   L ( x ) = KL{Att(x ) , Att(x)}(2 )   For the FFN modules and embedding layer , we   simply use the MSE between the output hidden   states of embedding and transformer layers in thestudent and teacher :   L ( x ) = 1   LXMSE{H(x ) , H(x ) }   ( 3 )   Where Lis number of hidden states ( number of   transformer layers plus one for embedding ) .   The final loss is calculated by a linear combina-   tion of the above losses as well as the cross entropy   loss .   Loss(x , y ) = XŒ±L ( x )   + Œ±L ( x ) + Œ±L ( x , y )   ( 4 )   After decomposing the teacher model , GPT-2 ,   into KnGPT2 , the performance of the model drops   significantly . This drop is mainly because of the   approximation of linear weight matrices using the   corresponding Kronecker factors . Therefore , pre-   training of the compressed model on a small cor-   pus for a few epochs is necessary to retrieve the   information which are lost during decomposition .   Inspired by ( Jiao et al . , 2019 ) , we pre - trained the   model on a small portion , 10 % , of the OpenWeb-   Text dataset ( Gokaslan and Cohen , 2019 ) for one   epoch and we used the KD method which is dis-   cussed in Section 3.3 to improve the performance   of the compressed model .   4 Experiments   We evaluated our proposed algorithm , KnGPT2 , on   language modeling and text classification . For lan-   guage modeling we use the Wikitext-103 ( Merity   et al . ) dataset . For classification we use seven of the   classification tasks of the General Language Un-   derstanding Evaluation ( GLUE ) benchmark ( Wang   et al . , 2019 ) . These datasets can be broadly divided   into 3 families of problems . Single set tasks which   include linguistic acceptability ( CoLA ) and senti-   ment analysis ( SST-2 ) , similarity and paraphrasing   tasks ( MRPC and QQP ) , and inference tasks which   include Natural Language Inference ( MNLI and   RTE ) and Question Answering ( QNLI ) .   4.1 Experimental Setup   The KnGPT2 model is compressed from   theGPT-2 ( Radford et al . , 2019 ) model .   GPT-2 has 124 million parameters . Our   baseline is DistilGPT2 which has about 82 million   parameters so our KnGPT2 model is compressed   to the same size ( 83 million parameters ) for a fair221GPT-2 DistilGPT2 KnGPT2   Parameters124 82 83   Training time ( hrs ) - > 906.5   Dataset size ( GB ) 40 38 3.2   Model CoLA RTE MRPC SST-2 MNLI QNLI QQP Average   GPT-2 44.0 63.2 84.5 92.8 81.75 88.7 88.0 77.56   DistilGPT2 32.4 61.9 84.3 90.8 79.55 85.4 87.3 74.52   DistilGPT2 + KD 33 61.5 84.4 90.7 79.85 85.7 87.6 74.67   KnGPT2 36.7 64.4 84.5 89.0 78.45 85.6 86.5 75.02   KnGPT2 + ILKD 41.8 63.7 86.5 91.5 81.6 88.4 88.5 77.42   comparison . To achieve this , we compress half the   layers of transformer block ( odd numbered ones )   in addition to the embedding layer by a factor of 2 .   4.2 Pre - training   After the base model is compressed , performance   of the compressed model drops significantly since   the weight matrices with the Kronecker factors are   approximate . Pre - training on a relatively small   dataset for one epoch helps in retrieving the accu-   racy . Therefore , KnGPT2 is pre - trained ( using the   ILKD method discussed in Section 3.3 ) on 10 %   of OpenWebText which is 10 times less the Dis-   tiGPT2 model . As shown on Table 1 the training   time for KnGPT2 is much faster as well .   GPT-2 DistilGPT2 KnGPT2   PPL 18.8 23.7 20.5   4.3 Results   First we evaluate the models on language modeling   using the Wikitext-103 dataset . The results are   shown on Table 3 . Although the DistilGPT2 is pre-   trained longer and on a larger dataset the KnGPT2   achieves a lower perplexity . Next , performance of the models is evaluated   on the test ( Table 2 ) sets of seven datasets of the   GLUE benchmark . Similar to the pre - training , we   used the ILKD method discussed in Section 3.3   to fine - tune KnGPT2 . For DistilGPT , we apply   the basic KD algorithm also referred to in the lit-   erature as Vanilla KD ( Jafari et al . , 2021 ) . For   DistilGPT since the number of layers between the   teacher and the student are different , it is not clear   which teacher layer should be distilled to which   student layer . Although there has been work on   intermediate distillation for mismatched layers for   BERT ( Passban et al . , 2021 ) , extensive experimen-   tation is required to conclude the best practice for   GPT .   On the test set results ( Table 2 ) , we observe that   KnGPT2 outperforms DistilGPT2 for all datasets .   Applying ILKD , even improves performance of   KnGPT2 . Another interesting result is that Vanilla   KD does not improve DistilGPT2 fine - tuning . In-   terestingly KnGPT2 with KD reaches close to the   GPT-2 performance on average .   5 Conclusion   In this paper , we compressed GPT-2 by compress-   ing linear layers of a GPT model using Kronecker   decomposition . Our model is pre - trained on a rela-   tively small ( 10 times smaller than the dataset used   for baseline ) dataset which makes the pre - training   much faster . Our proposed model significantly out-   performed the baseline on the GLUE benchmark .   Using KD can help to further reduce the perfor-222mance drop of the compressed model . Using Kro-   necker decomposition on larger GPT models and   for higher compression factors are two interesting   future directions .   References223   A Configurations   Table 4 shows sizes of matrices in GPT-2 , Dis-   tilGPT2 and KnGPT2 .   BKronecker product further explanation   Kronecker product has attractive abstract algebraic   properties such as   A‚äó(B+C ) = A‚äóB+A‚äóC(A‚äóB)=A‚äóB   ( A‚äóB)=A‚äóB   for more details see ( Henderson et al . , 1983 ) . The   interesting properties of the Kronecker product   makes it an attractive tool for decomposition of   large matrices . The Kronecker product is also a   flexible method to simplify the notation of large   block matrices , both in linear mixed effect models   and multilevel models ( Goldstein , 2011 ) . It is also   a well - known technique to represent large repeti-   tive structured graphs using the Kronecker product   ( Leskovec et al . , 2010 ) . One of the most important   characteristics of a matrix is its determinant and it   is well - known that for two square matrices Aand   Bof size n , and m,|A‚äóB|=|A||B| . This   property explains the superiority of Kronecker com-   pared to the other decomposition methods for large   matrices . By choosing the right nandm , a large   matrix W = A‚äóBcan be decomposed to much   smaller matrices such that the above determinant   equation holds .   C Hyperparameters   Table 5 shows tuned hyperparameters for pre-   training on OpenWebText and fine - tuning on   GLUE . Also , to fine - tune models on Wikitext-103 ,   we used the same hyperparamters as pre - training .   D GLUE dev results   Table 6 show performance of the models on dev set   of GLUE .   E Ablation Study   ( Tahaei et al . , 2021 ) uses MSE as the distance met-   ric between output of attention layers from all of   the transformer layers of teacher and student . One   of differences of our work from ( Tahaei et al . , 2021 )   is that inspired from ( Wang et al . , 2020 ) , we used   Kullback ‚Äì Leibler ( KL ) divergence of the output   of last attention layer . Using KL divergence over   MSE improved average performance of the model ,   for 0.92 % on the following GLUE tasks : COLA ,   MNLI , MRPC , QNLI , QQP , RTE and SST2 ( Table   7 ) .   ( Tahaei et al . , 2021 ) only minimizes the KD at   the pre - training stage . we performed two experi-   ments to study the effect of KD on the pre - training   of KnGPT2 to improve performance of our model .   In the first experiment , KnGPT2 is pre - trained by   KD loss , with and without cross entropy ( CE ) loss224Model Embedding Q , K , V FFN   GPT-2 50527 √ó768 768 √ó768 3072 √ó768   DistilGPT2 50527 √ó768 768 √ó768 3072 √ó768   KnGPT2 A : 50527 √ó384,B : 1√ó2A : 384√ó768 , B:2√ó1A : 1536 √ó768,B : 2√ó1   Phase Epoch Sequence Length Seed Batch size Learning rate Œ±Œ±Œ±   Pre - training 1 1024 42 1 0.00025 0.5 0.5 0.1   Fine - tuning 20 128 42 16 2e-5 0.5 0.5 0.02   Model CoLA RTE MRPC SST-2 MNLI QNLI QQP Average   GPT-2 47.6 69.31 87.47 92.08 83.12 88.87 90.25 79.81   DistilGPT2 38.7 65.0 87.7 91.3 79.9 85.7 89.3 76.8   DistilGPT2 + KD 38.64 64.98 87.31 89.80 80.42 86.36 89.61 76.73   KnGPT2 37.51 70.4 88.55 88.64 78.93 86.10 88.87 77   KnGPT2 + ILKD 45.36 69.67 87.41 91.28 82.15 88.58 90.34 79.25   Model CoLA RTE MRPC SST-2 MNLI QNLI QQP Average   KnGPT2 + ILKD 41.65 68.95 88.89 90.48 80.69 87.66 90.00 78.33   KnGPT2 + ILKD 45.36 69.67 87.41 91.28 82.15 88.58 90.34 79.25   Model Œ±CoLA RTE MRPC SST-2 MNLI QNLI QQP Average   KnGPT2 + ILKD 0 40.80 70.04 88.25 90.71 80.12 87.64 89.64 78.17   KnGPT2 + ILKD 0.1 45.36 69.67 87.41 91.28 82.15 88.58 90.34 79.25   Model Wikitext-103(PPL ) MNLI ( f1 )   KnGPT2 28608 69.33   KnGPT2 + LM 21.94 77.87   KnGPT2 + KD 144.1 77.50   KnGPT2 + LM + KD 23.04 77.97   then fine - tuned on GLUE with mentioned ILKD   method discussed in Section 3.3 . Empirical results   ( Table 8) show that adding cross entropy loss im - proves performance of Kronecker model on down-   stream tasks so we used KD + CE loss for both pre-   training and fine - tuning . In the second experiment225we used Wikitext-103 as our pre - training dataset .   We compare four models and evaluate on LM as   well as on classification using the MNLI dataset   from GLUE . As shown on Table 9 we compare   KnGPT2 without pre - training , with language mod-   eling pre - training only , with KD pre - training only   and both language modeling and KD pre - training .   Note that we apply ILKD , discussed before for   fine - tuning , as our KD algorithm . We observe that   pre - training is important for good performance on   the downstream task but lower perplexity on LM is   not always a good indicator of better downstream   performance.226   Keiji Shinzato   Rakuten Institute of Technology ,   Rakuten Group Inc.   keiji.shinzato@rakuten.comNaoki Yoshinaga   Institute of Industrial Science ,   the University of Tokyo   ynaga@iis.u-tokyo.ac.jp   Yandi Xia   Rakuten Institute of Technology ,   Rakuten Group Inc.   yandi.xia@rakuten.comWei - Te Chen   Rakuten Institute of Technology ,   Rakuten Group Inc.   weite.chen@rakuten.com   Abstract   1 Introduction   One of the most challenging problems in attribute   value extraction ( ) from e - commerce sites is a   data sparseness problem caused by the diversity of   attributes . To alleviate the data sparseness prob-   lem , recent researches ( Xu et al . , 2019 ; Wang et al . ,   2020 ) formalize the task as question answering   ( ) to exploit the similarity of attributes via repre-   sentation learning . Specifically , the - based   takes an attribute name as query and product data   ascontext , and attempts to extract the value from   the context . Although this approach mitigates the   data sparseness problem , performance depends on   the quality of query representations ( Li et al . , 2020 ) .   Because attribute names are short and ambiguous   as queries , the extraction performance drops signif-   icantly for rare attributes with ambiguous names   ( e.g. ,sort ) which do not represent their values well .   Aiming to perform more accurate - based   for rare and ambiguous attributes , we propose sim-   ple query expansion that exploits values for the   attribute as knowledge to learn better query repre-   sentations ( Figure 1 , ¬ß 3 ) . We first retrieve possible   values of each attribute from the training data , and   then use the obtained values to augment the query   ( attribute ) . Since unseen values and attributes will   appear in evaluation , we apply dropout to the seen   values to mimic the incompleteness of the knowl-   edge ( ¬ß 3.2 ) , and perform multi - domain learning to   capture the absence of the knowledge ( ¬ß 3.3 ) .   We demonstrate the effectiveness of the query   expansion for -based model ( Wang et al . ,   2020 ) using the AliExpress datasetreleased by Xu   et al . ( 2019 ) ( ¬ß 4 ) . In the evaluation process , we   found near - duplicated data in this dataset . We thus   construct , from this dataset , a more reliable dataset   called cleaned - pub to evaluate our method.227Our contribution is threefold :   ‚Ä¢ We proposed knowledge - driven query expan-   sion for - based ( ¬ß 3 ) ; the knowledge   taken from the training data is valuable ( ¬ß 4.3 ) .   ‚Ä¢We revealed that rare , ambiguous attributes   deteriorate the performance of - based   in the e - commerce domain ( ¬ß 4.3 ) .   ‚Ä¢We will release our cleaned version of AliEx-   press dataset for research purposes .   2 Related Work   Attribute value extraction has been modeled as a   sequence labeling problem ( Putthividhya and Hu ,   2011 ; Shinzato and Sekine , 2013 ; More , 2016 ;   Zheng et al . , 2018 ; Rezk et al . , 2019 ; Kara-   manolakis et al . , 2020 ; Dong et al . , 2020 ; Zhu et al . ,   2020 ; Mehta et al . , 2021 ; Jain et al . , 2021 ; Yan et al . ,   2021 ) . However , since the number of attributes can   exceed ten thousand in e - commerce sites , the mod-   els perform poorly for the majority of attributes that   rarely appear in the labeled data ( Xu et al . , 2019 ) .   To alleviate the data sparseness problem , Xu   et al . ( 2019 ) introduced a - based approach for   the task . It separately encodes product titles   and attributes using ( Devlin et al . , 2019 ) and   bi - directional long - short term memory ( Hochreiter   and Schmidhuber , 1997 ) , and then combines the re-   sulting vectors via an attention layer to learn spans   of values for the attributes from the titles . Wang   et al . ( 2020 ) proposed a purely -based model ,   which feeds a string concatenating the given title   and attribute to . These - basedmodels ,   however , do not fully enjoy the advantage of themodel , since attribute queries are much shorter   than sentential questions in the originaltask .   To build better queries in solving named entity   recognition via , Li et al . ( 2020 ) exploited an-   notation guideline notes for named entity classes   as queries . Although this approach will be also   effective for - based , it requires substantial   labors to prepare manual annotations for more than   ten thousand attributes in e - commerce site .   3 Proposed Method   This section proposes a simple but effective query   expansion method for - based ( Wang et al . ,   2020 ) by utilizing attribute values . Given a product   data ( title ) x={x , ... , x}and an attribute a=   { a , ... , a } , where nandmdenote the number oftokens , the model returns the beginning position ,   P , and ending position , P , of a value .   Figure 1 depicts the model architecture with our   approach . Although our query expansion is essen-   tially applicable to any - based models , we   here employ the state - of - the - art model using   proposed by Wang et al . ( 2020 ) . In addition to   thecomponent for , their model has other   two components ; the no - answer classifier and the   distilled masked language model . Since those com-   ponents slightly decrease the overall micro F , we   employ thecomponent from their model ( hear-   after , referred to as - ) .   3.1 Knowledge - Driven Query Expansion for - Based   It is inherently difficult for - based models   to induce effective query representations for rare   attributes with ambiguous names . It is also hard   to develop expensive resources such as annotation   guideline notes ( Li et al . , 2020 ) for more than ten   thousand of attributes in e - commerce domain .   Then , is there any low - cost resource ( knowledge )   we can leverage to understand attributes ? Our an-   swer to this question is values ( answers ) for the   attributes ; we can guess what attributes means from   their values . In this study , we exploit attribute val-   ues retrieved from the training dataof the target model as run - time knowledge to induce better   query representations .   Our query expansion allows the - based   model , M , to utilize the seen values for attribute   ain the whole training data to find beginning and   ending positions of a value , ‚ü®P , P‚ü©in title x :   ‚ü®P , P‚ü©=M([;x;;a;;v])(1 )   Here , andare special tokens to represent   a classifier token and a separator , respectively , and   vis a string concatenating the seen values of the   attribute awith in descending order of fre-   quency in the training data .   3.2 Knowledge Dropout   By taking all the seen values in the training data to   augment input queries , the model may just learn to   match the seen values with one in the given title . To   avoid this , inspired from word dropout employed   in language modeling ( Gal and Ghahramani , 2016),228we perform knowledge dropout overvin training   before concatenating it with title xand attribute a.   v= [ drop(v);;drop(v ) ; ; . . .](2 )   Here , drop is a function that replaces a value v   invwith padding tokens according to a dropout   rate ; we replace each token in vwith .   To decide if the dropout applies to a value , we   take account of the number of examples labeled   with the value . Given the dropout rate rand the   number of training examples n , the dropout per-   forms over the value vaccording to the probability   ofr . This implementation captures the fact that   infrequent values are more likely to be unseen .   3.3 Knowledge Token Mixing   Since values are literally valuable to interpret at-   tributes , the - based model may rely more   on values than an attribute name . This will hurt   the performance on unseen attributes whose val-   ues are not available . To avoid this , we assume   the availability of value knowledge to be domain ,   and perform multi - domain learning for - based   model with and without our value - based query ex-   pansion . This will allow the model to handle not   only seen attributes but also unseen attributes .   Inspired from domain token mixing ( Britz et al . ,   2017 ) , we introduce two special domain tokens   ( knowledge tokens ) , and prepend either of the to-   kens to the attribute to express the knowledge sta-   tus : and ( with and without values ) .   In training , from an example with title xand at-   tribute a , we build [ ; x ; ; ; a;;v ]   and [ ; x ; ; ; a ; ] , and then put   these examples to the same mini - batch . In testing ,   we use and tokens for seen attributes   ( with values ) and unseen attributes , respectively .   4 Experiments   We evaluate our query expansion method for-   based on a public dataset , which is built from   product data under the Sports & Entertainment cat-   egory in AliExpress , following ( Wang et al . , 2020 ) .   4.1 Settings   Dataset The public AliExpress dataset consists   of 110,484 tuples of ‚ü®product title , attribute , value ‚ü©.   When a value of the attribute is absent from the title ,   the value in the tuple is set as ‚Äú . ‚Äù We manu-   ally inspected the tuples in the dataset , and found   quality issues ; some tuples contained enti-   ties , and extra white spaces in titles , attributes , and   values , and the same attributes sometimes have dif-   ferent letter cases . We thus decoded entities ,   converted trailing spaces into a single space , and   removed white spaces at the beginning and ending .   We also normalized the attributes by putting a space   between alphabets and numbers and by removing   ‚Äò : ‚Äô at the endings ( from ‚Äò feature1 : ‚Äô to ‚Äò feature 1 ‚Äô ) .   As a result , we found 736 duplicated tuples . By re-   moving these duplicated tuples , we finally obtained   thecleaned - pub dataset of 109,748 tuples with   2,162 unique attributes and 11,955 unique values .   We split this dataset into training , development , and   test sets with the ratio of 7:1:2 ( Table 1 ) .   Evaluation Metrics We use precision ( P ) , recall   ( R ) and Fscore as metrics . We adopt exact match   criteria ( Xu et al . , 2019 ) in which the full sequence   of extracted value needs to be correct .   4.2 Models   We apply our knowledge - driven query expansion   method ( ¬ß 3 ) to -(Wang et al . , 2020 ) , a - based model on . To perform the   query expansion , we simply collect values other   than ‚Äú ‚Äù from tuples in the training data for   each attribute ( Table 1 ) .   For comparison , we use SUOpenTag ( Xu et al . ,   2019 ) , and vanilla -(Wang et al . ,   2020 ) , which achieved the state - of - the - art micro F   score on the AliExpress dataset . We also perform   a simple dictionary matching ; it returns the most   frequent seen value for a given attribute among   those included in the given title .   To convert tuples in the training set to begin-   ning and ending positions , we tokenize both title   and value , and then use matching positions if the   token sequence of the value exactly matches a sub-   sequence of the title . If the value matches multiple229   portions of the title , we use the match close to the   beginning of the title . As beginning and ending   positions of tuples whose value is ‚Äú , ‚Äù we use   0 which is a position of atoken in the title . The   conversion procedure is detailed in Appendix A.1 .   We implemented the above models using Py-   Torch ( Paszke et al . , 2019 ) ( ver . 1.7.1 ) , and used   ‚Äú bert - base - uncased ‚Äù in Transformers ( Wolf et al . ,   2020 ) as the pre - trained ( ) . The im-   plementation details and the training time are given   in Appendix A.2 and Appendix A.3 , respectively .   4.3 Results   Table 2 shows macroand micro performance of   each model that are averaged over five trials . The   low recall of the model -+vals suggests   that this model learns to find strings that are similar   to ones retrieved from the training data ( overfitting ) .   On the other hand , knowledge dropout and knowl-   edge token mixing mitigates the overfitting , and   improves both macro and micro Fperformance . Impact on rare and ambiguous attributes To   see if the query expansion improves the perfor-   mance for rare attributes with ambiguous names ,   we categorized the attributes that took the query ex-   pansion according to the number of training exam-   ples and the appropriateness of the attribute names   for their values . To measure the name appropriate-   ness , we exploit embeddings of thetoken using   the for each attribute and its seen values ;   when the cosine similarity between the attribute   embedding and averaged value embeddings is low ,   we regard the attribute name as ambiguous . We   divide the attributes into four according to median   frequency and similarity to values .   Table 3 lists macro and micro Fof each model   and the improvements over the -for each   category . We can see that our query expansion   tends to be more effective for attributes with low   similarity . This means that the query expansion can   generate more informative queries than ambiguous   attributes alone . Moreover , by using knowledge   dropout and knowledge token mixing , we can im-   prove macro and micro Ffor rare attributes . These230   results are remarkable since the knowledge used   to enhance the model comes from its training data ;   the model could use more parameters to solve the   task itself by taking the internal knowledge induced   from the training data as runtime input .   Impact on seen and unseen attribute values To   see for what types of attribute values the query   expansion is effective , we categorize the test exam-   ples according to the types of the training data used   to solve the examples . We first categorize the test   examples into seen or unseen attributes . Next , we   further classify the examples for the seen attributes   into either seen or unseen attribute values .   Table 4 shows the performance in terms of the at-   tribute value types . The query expansion improved   macro Fby 13 points on the seen values for the   seen attributes ; these improvements were yielded   by the large performance gains for rare attributes in   Table 3 . Although -+vals performed the   best on the seen values , it performed the worst on   the unseen values for the seen attributes and unseen   attributes ; the model is trained to match seen values   in a query with a given title . Meanwhile , the two   tricks enable the model to maintain the micro F   performance of on the unseen values for the   seen attributes . The lower macro Fagainst   suggests that there is still room for improvements in   query representation for rare seen attributes . Lastly ,   the knowledge token mixing successfully recovered   the performance of for the unseen attributes ,   and even improved the performance when it is used   together with the knowledge dropout . This is possi-   bly because the knowledge token mixing allows the   model to switch its behavior for seen and unseen   attributes , and the knowledge dropout strengthens   the ability to induce better query representations .   Example outputs Table 5 shows examples of the   actual model outputs for a given context and query   ( attribute ( seen values ) ) . In the first two examples ,   function 1 andnominal capacity are ambiguous   and rare attributes , respectively , and are thereby   hard for the -to extract correct values   without the help of our query expansion . As shown   in the last example , when there are more than one   candidates as values of a given attribute , our query   expansion is still unstable .   5 Conclusions   We have proposed simple query expansion based on   possible values of a given query ( attribute ) for-   based attribute extraction . With the two tricks to   mimic the imperfection of the value knowledge , we   retrieve values of given attributes from the training   data , and then use the obtained values as knowl-   edge to induce better query representations . Exper-   imental results on our cleaned version of the public   AliExpress dataset demonstrate that our method im-   proves the performance of product attribute extrac-   tion , especially for rare and ambiguous attributes .   We will leverage external resources to handle un-   seen attributes ( preliminary experiments are shown   in Appendix A.4 ) . We will release the script to   build our cleaned - pub dataset.231Acknowledgements   This work ( second author ) was partially supported   by JSPS KAKENHI Grant Number 21H03494 . We   thank the anonymous reviewers for their hard work .   References232   A Appendix   A.1 How to Convert Tuples to Labeled Data   Let ‚Äôs say , we have a tuple of ‚ü®product title , attribute ,   value ‚ü©=‚ü®golf clubs putter pu neutral golf grip ,   material , pu ‚ü© , and try to obtain beginning and end-   ing positions of the value in the title . First , we   tokenize both title and value using BertTokenizer ,   and then find a partial token sequence of the title   that exactly matches with the token sequence of the   value . By performing the match over the tokeniza-   tion results , we can avoid matching a part of tokens   in the title to the value . In case of this example , we   can prevent the value pufrom matching to the first   two characters of putter . As a result , the value pu   matches to the token puin the title , and we properly   obtain the beginning and ending positions of puin   the title . A.2 Implementation Details   We implemented all the models used in our ex-   periments using PyTorch ( Paszke et al . , 2019 )   ( ver . 1.7.1),and used ‚Äú bert - base - uncased ‚Äù in   Transformers ( Wolf et al . , 2020)as the pre - trained ( ) . The dimension of the hidden   states ( D ) is 768 , and the maximum token length   of the product title is 64 . We set the maximum   token length of the query to 32 for all models with   the exception of models with the query expansion .   To make as many attribute values as possible , we   set 192 to the maximum token length of the query   for the models using the query expansion , and trun-   cate the concatenated string if the length exceeds   192 . We set a rate of dropout over values to 0.2 .   The total number of parameters in -with   our query expansion is 109M. We train the models   five times with varying random seeds , and average   the results .   Regarding to , the loss of the distilled   masked language model got NaN if we followed   the algorithm in the paper . We instead used   BERTMLMHead class implemented in Transform-   ers .   We use Adam ( Kingma and Ba , 2015 ) with a   learning rate of 10as the optimizer . We trained   the models up to 20 epochs with a batch size of 32   and chose the models that perform the best micro F   on the development set for the test set evaluation .   A.3 Training Time   We used an NVIDIA Quadro M6000 GPU on a   server with an IntelXeonE5 - 2643 v4 3.40GHz   CPU with 512 GB main memory for training . It   took around two hours per epoch for training -with our query expansion , while it took around   25 minutes per epoch for training the - .   A.4 Preliminary experiments using external   resource to obtain the value knowledge   As we have discussed in ¬ß 3.1 , we can utilize ex-   ternal resource other than the training data of the   model to perform the query expansion . We here   evaluate the -models that have been al-   ready trained with our query expansion , using the   development data as external ( additional ) resource   to obtain the value knowledge in testing . If new   values are retrieved from the development data , the   models will build longer queries for attributes . We233   here evaluate such attributes with longer queries   among the seen and unseen attributes in Table 4 .   Table 6 shows the performance of the -   models with our query expansion on 288 seen val-   ues for 107 seen attributes , 339 unseen values for   131 seen attributes , and 19 values for 18 unseen   attributes , for which new values are retrieved from   the development data . We can observe that the new   values retrieved from the development data boosted   the performance of the -models with our   query expansion on the unseen values for the seen   attributes and the unseen attributes , whereas they   did not increase the performance on the seen values   for the seen attributes . In the future , we will ex-   plore a better way to leverage the value knowledge   in the external resources other than the training data   of the - based models.234   EunJeong Hwang , Jay - Yoon Lee , Tianyi Yang , Dhruvesh Patel , Dongxu Zhang   & Andrew McCallum   College of Information and Computer Science , University of Massachusetts Amherst   { ehwang , jaylee , tianyiyang , dhruveshpate ,   dongxuzhang,mccallum}@cs.umass.edu   Abstract   1 Introduction   A piece of text can contain several events . In order   to truly understand this text , it is vital to understand   the subevent and temporal relationships between   these events.(Mani et al . , 2006a ; Chambers and Ju-   rafsky , 2008 ; Yang and Mitchell , 2016 ; Araki et al . ,   2014 ) . Both temporal as well as subevent relation-   ships between events satisfy transitivity constraints .   For instance , in the paragraph , ‚Äú There was a storm   in Atlanta in the night . All the phone lines were   dead the next morning . I was not able to callfor   help . ‚Äù , the event marked by dead occurs after storm   and the event calloccurs after dead . Hence , by tran-   sitivity , a sensible model should predict that storm   occurs before call . In general , predicting the re-   lationships between different events in the same   document , such that these predictions are coherent ,   is a challenging task ( Xiang and Wang , 2019).While previous works utilizing neural methods   provide competitive performances , these works em-   ploy multi - class classiÔ¨Åcation per event - pair inde-   pendently and are not capable of preserving logical   constraints among relations , such as asymmetry   and transitivity , during training time ( Ning et al . ,   2019 ; Han et al . , 2019a ) . To address this problem   Wang et al . ( 2020 ) introduced a constrained learn-   ing framework , wherein they enforce logical co-   herence amongst the predicted event types through   extra loss terms . However , since the coherence is   enforced in a soft manner using extra loss terms ,   there is still room for incoherent predictions . In   this work , we show that it is possible to induce co-   herence in a much stronger manner by representing   each event using a box ( Dasgupta et al . , 2020 ) .   We propose a Box Event Relation Extraction   ( BERE ) model that represents each event as a prob-   abilistic box . Box embeddings ( Vilnis et al . , 2018 )   were Ô¨Årst introduced to embed nodes of hierar-   chical graphs into Euclidean space using hyper-   rectangles , which were later extended to jointly   embed multi - relational graphs and perform logical   queries ( Patel et al . , 2020 ; Abboud et al . , 2020 ) . In   this paper , we represent an event complex using   boxes ‚Äì one box for each event . Such a model en-   forces logical constraints by design ( see Section   3.2 ) . Consider the example in Figure 1 . Event dead   ( e ) follows event storm ( e ) , indicating eis child   ofe . Boxes can represent these two events as sep-   arate representations and by making eto contain   the boxe , which not only preserve their seman-   tics , but also can infer its antisymmetric relation   that eventeis a parent of event e. However , the   previous models based on pairwise - event vector   representations have no real relation between repre-   sentations ( e;e)and(e;e)that can guarantee   the logical coherence .   Experimental results over three datasets , HiEve ,   MATRES , and Event StoryLine ( ESL ) , show that   our method improves the baseline ( Wang et al . ,2352020 ) by 6.8 and 4.2 Fpoints on single task and   by 0.95 and 3.29 Fpoints on joint task over sym-   metrical dataset . Furthermore , our BERE model   decreases conjunctive constraint violation rate by   8588 % on a single - task models compared to plain   vector model , and by 38 % on joint - task model com-   pared to constraint - injected vector model . We show   that handling antisymmetric constraints , that ex-   ist among different relations , can satisfy the inter-   wined conjunctive constraints and encourage the   model towards a coherent output across temporal   and subevent tasks .   2 Background   Task description Given a document consist-   ing of multiple events e;e;:::;e , we wish   to predict the relationship between each event   pair(e;e ) . We denote by r(e;e)the relation   between event pair ( e , e ) . Its values are de-   Ô¨Åned in the label space { P -C , C -   P , C , N } for subevent relation-   ship ( HiEve ) and { B , A , E ,   V } for temporal relationship ( MATRES ) .   Both subevent and temporal relationships have four   similar - category relationship labels where the Ô¨Årst   two labels , ( P -C , C -P ) and   ( B , A ) hold reciprocal relationship , the   third label ( C andE ) occurs when it is   hard to tell which of the Ô¨Årst two labels that event   pair should be classiÔ¨Åed to . Lastly , the last label   ( N andV ) represents a case when an   event pair is not related at all .   Box embeddings A boxb = Q[b;b ]   such thatbRis characterized by its min and   max endpoints b;b2R , withb < b8i .   In the probabilistic gumbel box , these min and max   points are taken to be independent gumbel - max   and gumbel - min random variables , respectively .   As shown in Dasgupta et al . ( 2020 ) , if bandc   are two such gumbel boxes then their volume and   intersection is given as :   wherel(x;y ;  ) =  log(e+e ) ,  is the tem-   perature , which is a hyperparameter , and   is theEuler - Mascheroni constant .   Logical constraints We deÔ¨Åne symmetry and   conjunction constraints of relations . Symmetry   constraints indicate the event pair with Ô¨Çipping or-   ders will have the reversed relation . For example ,   ifr(e;e ) = P -C ( B ) , then   ~r(e;e ) = C -P ( A ) . Given any   two events , eande , the symmetry consistency is   deÔ¨Åned as follows :   ^r(e;e)$~r(e;e ) ( 1 )   whereris the relation between events , the Eis the   set of all possible events and the Ris the set of   relations , in which symmetry constraints hold .   Conjunctive constraints refer to the constraints   that exist in the relations among any event triplet .   The conjunctive constraint rules indicate that given   any three event pairs , ( e;e);(e;e);and(e;e ) ,   then the relation of ( e;e)has to fall into the con-   junction set speciÔ¨Åed based on ( e;e)and(e;e )   pairs ( see Appendix Table 6 ) . The conjunctive con-   sistency can be deÔ¨Åned as :   where theEis the set of all possible events , rand   rare any possible relations exist in the set of all   relationsR , ris the relation , which is speciÔ¨Åed by   randrbased on conjunctive induction table , and   Dis the set of all possible relations , in which r   andrhave no conÔ¨Çicts in between . The full expla-   nation on symmetry and conjunction consistency   can be found in Wang et al . ( 2020 ) .   3BERE model   In this section , we present the proposed box model   BERE for event - event relation extraction . As de-   picted in Figure 1 , the proposed model encodes   each event eas a boxbinRbased one ‚Äôs   contextualized vector representation h. As de-   scribed in ¬ß 3.1 , the relation between ( e;e)is   then predicted using conditional probability scores   P(bjb ) = V ol(b\b)=V ol(b),P(bjb ) =   V ol(b\b)=V ol(b)deÔ¨Åned on box space . Lastly ,   ¬ß 3.2 describes loss function used to learn the pa-   rameters of the model.236   ( B)(A)(C )   Vector ( e , e ) Vector ( e , e )   BERE(e , e ) BERE(e , e )   Parent - ChildChild - Parent CoRef NoRel   3.1 Inference rule on conditional probability   Notice that given two boxes bandb , a higher   value ofP(bjb)(resp . P(bjb ) ) implies that box   bis contained in b(resp.bcontained in b ) .   Moreover , other than complete containment in ei-   ther direction , there are other two prominent con-   Ô¨Ågurations possible , i.e. one where b , boverlap   but none contains the other , and the one where   b , bdo not overlap . It is possible to capture   all four conÔ¨Ågurations by comparing the values   ofP(bjb)andP(bjb)with a threshold . Fig-   ure 1(B ) states our classiÔ¨Åcation rule formulated   based on this observation . With this formula-   tion we have the desired symmetry constraint , i.e. ,   r(e;e ) = P -C()r(e;e ) =   C -P , satisÔ¨Åed by design .   3.2 Loss functions for training   BCE loss As we require two dimensions of scalar   P(bjb)andP(bjb)to classifyr(e;e ) , and for   ease of notation , we deÔ¨Åne our label space with   2 - dimensional binary variable yas shown in   Figure1(b ) . Where y = I(P(bjb) ) and   y = I(P(bjb) ) whereI()stands for   indicator function . Now given batch B , BCE loss   ( L ) is deÔ¨Åned as :   Pairwise loss Motivated from previous papers   using pairwise features to characterize relations ,   we also incorporate a pairwise box into our learn-   ing objective , and only in learning time , to en-   courage relevant boxes to be concentrated together . For the event - pair representation , two contextu-   alized event embeddings ( h;h)are combined   as[h;h;h  h]where  represents element-   wise multiplication . Then , a multi - layer perceptron   ( MLP ) is used to transform pairwise vectors to   box representations b. The pairwise features we   use here are similar to ( Zhou et al . , 2020 ) except   that we do not use subtraction in order to preserve   symmetry between pairwise features of ( e;e)and   ( e;e ) , i.e.b = b. For two related events , we   enforce the intersection of corresponding boxes   b\bto be inside the pairwise box . For irrelevant   event pairs such as having N orV , their   intersection and pairwise boxes are forced to be   disjoint . The pairwise loss Lis deÔ¨Åned as :   whereRis a set of irrelevant relations , such as   N andV , andRstands for comple-   ment set of R , i.e. all the set of relations that   indicates two events have some relation .   In the remainder of the paper , BERE refers to a   model trained with loss LandBERE - p refers to   a model trained with two losses L;Lcombined .   4 Experiments   In this section , we describe datasets , baseline meth-   ods , and evaluation metrics . Lastly , we provide   experimental results and a detailed analysis of logi-   cal consistency .   4.1 Experimental Setup   Datasets Experiments are conducted over three   asymmetrical event relation extraction corpus,237   HiEve ( Glava≈° and ≈†najder , 2014 ) , MATRES ( Ning   et al . , 2018 ) , and Event StoryLine ( ESL ) ( Caselli   and V ossen , 2017 ) . Table 1 shows a brief summary   of dataset statistics . HiEve consists of 100 articles   and the narratives in news stories are represented   as event hierarchies . The annotations include   subevent and coreference relations . MATRES is a   four - class temporal relation dataset , which contains   275 news articles drawn from a number of different   sources . Event StoryLine ( ESL ) corpus is a dataset   that contains 258 news documents and includes   event temporal and subevent relations . The ESL   dataset is deÔ¨Åned differently compared to HiEve   and MATRES , so we mapped the ESL labels into   the labels in HiEve similar to ( Wang et al . , 2020 )   as shown in Table 2 .   For creating symmetrical dataset , we augment   P -C andC -P ( B   and A ) pairs by their reversed relations   C -P andP -C ( A and   B ) , respectively .   Baseline We compare our BERE , BERE - p   against the state - of - the - art event - event relation ex - traction model proposed by ( Wang et al . , 2020 ) .   This model utilizes RoBERTa with frozen parame-   ters and further trains BiLSTM to represent text in-   puts into vector h(fore ) and then further utilizes   MLP to represent pairwise representation vfor   ( e;e ) . Givenv , vector model ( Vector ) simply   computes softmax over projected logits to produce   probability for every possible relations . On top of   this , as ( Wang et al . , 2020 ) showed that constraint   injection improves performance , we also compare   with the constraint - injected model ( Vector - c ) .   For a fair comparison , we utilize the same   RoBERTa + BiLSTM + MLP architecture for pro-   jecting event to box representation .   Metrics Following the same evaluation setting in   previous works , we report the micro- Fscore of   all pairs , except V pairs , on MATRES ( Han   et al . , 2019b ; Wang et al . , 2020 ) . On HiEve and   ESL , the micro- Fscore of P -C and   C -P pairs is reported ( Glava≈° and ≈†na-   jder , 2014 ; Wang et al . , 2020 ) .   4.2 Results and Discussion   Impact of pairwise box , Table 3 We Ô¨Årst show   the results of the BERE andBERE - p with and with-   out pairwise loss . The model with pairwise loss   shows about 2.8 Fpoint improvement on HiEve   and 1 Fpoint improvement on MATRES . It in-   dicates that promoting the relevant event pairs to   mingle together in the geometrical space is helpful   and it is particularly useful when most of the rela-   tion extraction model encodes individual sentences   independently .   Vector - based vs. Box - based , Table 4 Table 4   shows a comparison of our box approach to the   baseline with the ratio of symmetric and conjunc-   tive constraint violations . Our approach clearly   outperforms the baseline methods on symmetric   evaluation with a gain of 6.79 , 4.26 , and 9.34 F   points on the single task over HiEve , MATRES ,   and ESL datasets , respectively and with a gain of   0.95 and 3.29 Fpoints on the joint task over HiEve   and MATRES . The performance gains from asym-   metrical to symmetrical datasets with BERE - p are   much larger compared to the increase of Vector s.   This demonstrates the BERE - p successfully cap-   tures symmetrical relations , while previous vec-   tor models do not . In addition , it is noteworthy   that our method without constrained learning ex-   celsVector - c , which is trained with constrained   learning . This suggests that the inherent ability to238   model symmetrical relations helps satisfy the in-   tertwined conjunctive constraints , thus producing   more coherent results from a model . See Appendix   E for constraint violation statistics for asymmetric   dataset .   Constraint Violation Analysis , Table 8 ( Ap-   pendix ) We analyze constraint violations for   each label from both HiEve and MATRES . For   label pairs from the same dataset , our approach   excels in almost every cases . For label pairs across   datasets , our approach also shows fewer or similar   levels of violation . This further indicates , with-   out explicitly injecting constraints into objectives ,   our model can persist logical consistency among   different relations .   5 Ablation Study   We conduct additional experiments to see whether   Vector trained with the augmented symmetri-   cal dataset will affect the conclusion of BERE - p .   The results in Table 5 reconÔ¨Årm the BERE - p ‚Äôs su-   perior ability in handling constraints with better   performance , while Vector requires signiÔ¨Åcantly   longer training time due to the extended training   dataset with worse performance . We also note that   training Vector with the augmented symmetrical   dataset does not help with conjunctive constraint   violations ( 6:17!6:70 ) , although it reduces sym-   metrical constraint violations ( 24:08!12:01).6 Conclusion   We propose a novel event relation extraction   method that utilizes box representation . The pro-   posed method projects each event to a box represen-   tation which can model asymmetric relationships   between entities . Utilizing this box representation ,   we design our relation extraction model to han-   dle antisymmetry between events of ( e;e)and   ( e;e)which previous vector models were not ca-   pable of . Through experiments on three datasets ,   we show that the proposed method not only free of   antisymmetric constraint violations but also have   drastically lower conjunctive constraint violations   while maintaining similar or better performance   inF. Our model shows that box representation   can provide coherent classiÔ¨Åcation across multi-   ple event relations and opens up future research   for box representations in event - to - event relation   classiÔ¨Åcation .   7 Acknowledgement   This work is based upon work supported in part   by the Center for Data Science and the Center   for Intelligent Information Retrieval , and in part   by the National Science Foundation under Grant   Nos . 1763618 and 1514053 , and in part by the   Chan Zuckerberg Initiative under the project ‚Äú Sci-   entiÔ¨Åc Knowledge Base Construction ‚Äù . Some of   the work reported here was performed using high   performance computing equipment obtained under   a grant from the Collaborative R&D Fund managed   by the Massachusetts Technology Collaborative .   References239240241A Hyperparameters   We utilize 768 dimensional pretrained RoBERTa   model to compute word embeddings for events .   models are trained for 100 epochs with AMSGrad   optimizer and the learning rate is set to be 0.001 .   On HiEve and ESL , we sample N in trainset   using downsample ratio , which is Ô¨Åxed to 0.015 ,   and the downsample ratio for valid and test sets is   Ô¨Åxed to 0.4 . This is to encourage the models to   learn and evaluate all types of relations that exist   in the datasets when N overwhelmingly rep-   resents the dataset . We use three weights , ; ;   and , to balance our three learning objectives L ,   L , andL(see Section 3.2 and Appendix B ) , in   which the weights are selected between 0.1 and 1 .   A threshold for HiEve is selected between -0.4   and -0.3 and a threshold for MATRES is chosen   between -0.7 and -0.6 . We use wandb ( Biewald ,   2020 ) tool for efÔ¨Åcient hyperparameter tuning .   B Conjunctive Consistency Loss   With consistency requirements on conjunctive   relations over temporal and subevent datasets   ( as shown in Table 6 ) , we incorporate the   loss function introduced by ( Wang et al . , 2020 )   into our box model to handle conjunctive con-   straints . Three events are grouped into three   pairs , ( e1;e2);(e2;e3)and(e1;e3 ) , and the re-   lation score for each class is calculated based on   conditional probabilities and its binary logits . With   the relation labels deÔ¨Åned for each class ( see Sec-   tion 3.2 ) , the relation score , r(e;e ) , is calculated   as :   r = ylogP(bjb ) + ylogP(bjb)(2 )   wherey = I(P(bjb) ) andy=   I(P(bjb) ) andyandyare the Ô¨Årst   and second binary logits in relation label , respec-   tively . Using this relation score , we now deÔ¨Åne the   loss function for modeling conjunction constraints :   L = X   jLj+X   jLj ; ( 3 )   where the two transitivity losses are deÔ¨Åned as   Table 7 presents the results of BERE - p com-   bined with the above learning objective , denoted as   BERE - c . Compared to the results from BERE - p , BERE - c shows a signiÔ¨Åcantly smaller ratio of   constraint violations than BERE - p , while sacri-   Ô¨Åcing Fby2 point from the performance with   BERE - p .   C Vector model architecture   Refer to Figure 2 for architecture of previous vector   models . charged killed   D Detailed analysis on conjunctive   constraint violation   Constraint Violation Analysis , Table 8 We   further break down constraint violations for each   label on HiEve and MATRES . The comparison   of constraint violations between the vector model   with constrained learning ( Vector - c ) and the   box model without constrained learning ( BERE - p )   is shown in Table 8 . " n / a " refers to no predictions   and this frequently appears on C andE   due to their sparsity in the corpus . Our approach   shows a smaller ratio of constraint violations in   most of the categories , with only a few exceptions .   2nd and 3rd quadrants ( HiEve ! MATRES and   MATRES!HiEve ) stand for cross - category ,   while 1st and 4th quadrants ( HiEve ! HiEve   and MATRES!MATRES ) stand for the same-   category . Interestingly , our approach without any   injected constraints shows a smaller or similar   ratio to Vector - c in the cross - category as well   as in the same - category . We calculated r=   total # of cross - category const - violations   total # of cross - category event tripletsand   r = total # of same - category const - violations   total # of same - category event triplets ,   where const - violations refers to constraint vi-   olations.rforVector - c is 6.26 % and for   BERE - p is 4.55 % and rforVector - c is 0.05 %   and for BERE - p is 0.017 % . This conÔ¨Årms the242   effectiveness of having boxes in handling logical   consistency among different relations .   E Symmetric and conjunctive constraint   violations over origianl data   Table 9 shows the Fand symmetry and con-   junctive constraint violation results over original   dataset . The results of symmetry and conjunctive   constraint violations conÔ¨Årm our expectation and   exhibit a similar observation from Table 4.F Related Work   F.1 Event - Event Relation Extraction   This task has been traditionally modeled as a pair-   wise classiÔ¨Åcation task with hand - engineered fea-   tures and early attempts applied conventional ma-   chine learning methods , such as logistic regressions   and SVM ( Mani et al . , 2006b ; Verhagen et al . ,   2007 ; Verhagen and Pustejovsky , 2008 ) . Later   works utilized a structured learning ( Ning et al . ,   2017 ) and neural methods to characterize relations .   The neural methods have been shown effective and   ensure logical consistency on relations through in-   ference step ( Dligach et al . , 2017 ; Ning et al . , 2018 ,   2019 ; Han et al . , 2019a ) . More recent works pro-   posed a constrained learning framework , which fa-   cilitates constraints during training time ( Han et al . ,   2019b ; Wang et al . , 2020 ) . Motivated by these   works , we propose a box model to automatically   handle inherent constraints without heavily relying   on constrained learning across two different tasks .   F.2 Box Embeddings   Box embeddings ( Vilnis et al . , 2018 ) were intro-   duced as a shallow model to embed nodes of hier-   archical graphs into euclidean space using hyper-   rectangles , which were later extended to jointly   embed multi - relational graphs and perform logical   queries ( Patel et al . , 2020 ; Abboud et al . , 2020 ) .   Recent works have successfully used box represen-   tations in conjunction with neural networks to rep-   resent input text for tasks like entity typing ( Onoe   et al . , 2021 ) , multi - label classiÔ¨Åcation ( Patel et al . ,   2022 ) , natural language entailment ( Chheda et al . ,   2021 ) , etc . In all these works , the input is rep-243   resented using a single box by transforming the   output of the neural network into a hyper - rectangle .   In this paper , we take this a step forward by rep-   resenting the input event complex using multiple   boxes . Our single box model represents each even   in an input paragraph using a box and the pairwise   box model adds on top of these , one box each for   every pair of events ( see section 3.2).244   Tsz Kin Lamand Shigehiko Schamoniand Stefan RiezlerDepartment of Computational Linguistics , Heidelberg UniversityInterdisciplinary Center for Scientific Computing ( IWR ) , Heidelberg University   { lam,schamoni,riezler}@cl.uni-heidelberg.de   Abstract   1 Introduction   End - to - end automatic speech translation ( AST ) re-   lies on data that consist only of speech inputs and   corresponding translations . Such data are notori-   ously limited . Data augmentation approaches at-   tempt to compensate the scarcity of such data by   generating synthetic data by translating transcripts   into foreign languages or by back - translating target-   language data via text - to - speech synthesis ( TTS )   ( Pino et al . , 2019 ; Jia et al . , 2019 ) , or by performing   knowledge distillation using a translation system   trained on gold standard transcripts and reference   translations ( Inaguma et al . , 2021 ) . In this paper ,   we present a simple , resource conserving approach   that does not require TTS and yields improvements   complementary to knowledge distillation ( KD ) .   For training cascaded systems , monolingual data   for automatic speech recognition and textual trans - lation data for machine translation can be used , re-   ducing the problem of scarcity . Cascaded systems ,   however , suffer from error propagation , which has   been addressed by using more complex intermedi-   ate representations such as n - best machine transla-   tion ( MT ) outputs or lattices ( Bertoldi and Federico ,   2005 ; Beck et al . , 2019 , inter alia ) or by modifying   training data to incorporate errors from automatic   speech recognition ( ASR ) and MT ( Ruiz et al . ,   2015 ; Lam et al . , 2021b ) . End - to - end systems are   unaffected by this kind of error propagation and are   able to surpass cascaded systems if trained on suf-   ficient amounts of data ( Sperber and Paulik , 2020 ) .   Our approach transfers an idea on aligned data   augmentation that has been presented for ASR   ( Lam et al . , 2021a ) to aligned data augmentation   in AST . Similar to aligned data augmentation for   ASR , we utilize forced alignment information to   create unseen training pairs in a structured manner .   Our augmentation procedure consists of the follow-   ing steps : ( 1 ) Sampling of a replacement suffix of a   transcription and its aligned speech representations ,   guided by linguistic constraints . ( 2 ) Translation   of the transcription containing the new suffix . ( 3 )   Recombination of audio data containing the new   suffix and the generated translation to distill a new   training pair . We thus use the acronym STR ( Sam-   ple , Translate , Recombine ) to refer to our method .   In comparison to Pino et al . ( 2019 ) and Jia et al .   ( 2019 ) who use TTS to generate synthetic speech ,   we create new examples by recombining real hu-   man speech . This reduces the problem of overfit-   ting to synthetic data as for example in SkinAug-   ment ( McCarthy et al . , 2020 ) where synthetic audio   is generated by auto - encoding speaker conversions .   The basic idea of our method is comparable to data   augmentation techniques for images such as Cut-   Mix ( Yun et al . , 2019 ) where images are blended   together to form new data examples . However , Cut-   Mix selects images randomly , while we recombine   phrases in a structured manner.245Our experimental evaluation is conducted for   five language pairs on the CoV oST 2 dataset ( Wang   et al . , 2021 ) and for two language pairs on the   Europarl - ST ( Iranzo - S√°nchez et al . , 2020 ) dataset .   We find considerable improvements for all lan-   guage pairs on all datasets for our approach on   top of KD . Our approach can be seen as an en-   hancement of Inaguma et al . ( 2021 ) ‚Äôs KD approach   since it requires roughly the same computational   resources and consistently improves their gains .   2 Method   Our method exploits audio - transcription alignment   information to generate previously unseen data   pairs for end - to - end AST training . By applying   a Part - of - Speech ( POS ) Tagger on a sentence , we   identify potential ‚Äú pivoting tokens ‚Äù where the to-   ken ‚Äôs prefix or suffix , i.e. , the preceding or succeed-   ing tokens , can be exchanged between other sen-   tences containing the same token of the same syn-   tactic function . We then sample possible suffixes   for that token from a suffix memory containing text   and audio suffixes , and concatenate the prefix , verb ,   and suffix to generate a new transcription . Then , an   MT system translates the new transcription , pick-   ing up on the idea of knowledge distillation in AST   ( Inaguma et al . , 2021 ) . The MT system is trained or   fine - tuned on the transcription - translation pairs . Fi-   nally , using the previously sampled audio suffix , we   concatenate prefix , verb , and suffix audio together   with the MT generated translation to recombine   a new audio - translation pair for end - to - end AST   training .   Our augmentation method implements linguistic   constraints by making use of the transcription ‚Äôs   syntactic structure in combination with alignment   information . Effectively , we exploit the strict SVO-   scheme of English sentences as we select the verb   as our pivoting token . Our method is applicable   to other languages , however , it will require more   effort to identify exchangeable syntactic structures .   Figure 1 illustrates our approach . We start by   identifying the pivoting token in a transcription we   want to augment , here ‚Äú playing ‚Äù in the sentence   ‚Äú two children are playing on a statue ‚Äù . Then , we   extract the list of possible suffixes following ‚Äú play-   ing ‚Äù from the suffix memory and sample a single   audio - text suffix , here ‚Äú volleyball in a park ‚Äù . To-   gether with the original prefix and pivoting token ,   the textual part of the sampled suffix builds a new   augmented transcription . Similarly , together withthe audio prefix and token , the audio part of the   suffix builds a new augmented audio example . The   augmented transcription is then translated by an   MT model . The new audio example ( i.e. , the rep-   resentation of ‚Äú two children playing volleyball in   a park ‚Äù ) and the translation ( i.e. , the text ‚Äú Zwei   Kinder spielen V olleyball in einem Park ‚Äù ) are then   recombined to form a new audio - translation pair .   3 Experimental Setting   Data Preprocessing We evaluate our method   on two common AST datasets , CoV oST 2 ( Wang   et al . , 2021 ) and Europarl - ST ( Iranzo - S√°nchez   et al . , 2020 ) . Since Europarl - ST is too small for MT   training from scratch , we use 1.6 M En - De sentence   pairs from Wikipedia following Schwenk et al .   ( 2021 ) and 3.2 M En - Fr sentence pairs from the   Common Crawl corpusas additional data . More   details on the datasets are in Appendix A.1 .   For speech data preprocessing , we extract log   Mel - filter banks of 80 dimensions computed ev-   ery 10ms with a 25ms window . We normalize the   speech features per channel using mean and vari-   ance per instance . For all textual data , punctuation   is normalized using .The transcrip-   tions are lowercased with punctuation removed .   For the speech - to - text tasks on CoV oST 2 , we   employ character - level models due to the availabil-   ity of pre - trained high quality ASR models . For   the speech - to - text tasks on Europarl - ST , we learn   5,000 subword units for each target language . For   the machine translation tasks in knowledge distilla-   tion , we learn a joint subword vocabulary on both   source and target for each language pair of size   5,000 for CoV oST 2 and size 40,000 for Europarl-   ST including the additional training data . Subword   unit creation is always conducted with - ( Kudo and Richardson , 2018 ) .   The Montreal Forced Aligner ( McAuliffe et al . ,   2017 ) is applied without any fine - tuning to extract   the acoustic alignments . Thus , the obtained align-   ments can be of low quality and we discard such   examples from our augmentation procedure . Please   refer to Appendix A.2 for details on our filtering   criteria . To extract POS - tags , we use theC   toolkit . We select the verb as our pivoting token   and generate the suffix memory as follows : for   each verb , we generate a list of audio - text suffix246   pairs and store the data in a key - value table . The   audio entries contain only references to the origi-   nal audio segments and our implementation is thus   very memory efficient . We only utilize basic off-   the - shelf components that are widely available and   our suffix memory has a negligible memory foot-   print . Table 1 summarizes the number of additional   training examples in each experiment .   Model configuration All our implementations   are based on ( Wang et al . , 2020 ; Ott et al . ,   2019).In all speech - to - text tasks , we use the   Transformer architecture ( Vaswani et al . , 2017 ) la-   belled as ‚Äú s2t_transformer_s ‚Äù in , which   consists of convolutional layers for downsampling   the input sequence with a factor of 4 before the self-   attention layers . The encoder has 12 layers while   the decoder has 6 layers with the dimensions of the   self - attention layers set to 256 and the feed - forward   network dimension set to 2048 .   For the CoV oST 2 MT tasks , we use a smaller   Transformer model of 3 layers for both encoder and   decoder . The encoder - decoder embeddings and the   output layer are shared . For the Europarl - ST MT   tasks , we use the Transformer BASE configuration   as described in Vaswani et al . ( 2017 ) .   Training In the CoV oST 2 AST experiments , we   use the character - level ASR model downloaded   from the GitHub webpageto initialize   the encoder of the AST systems . Each AST sys-   tem is then trained for another 50,000 steps . ForEuroparl - ST , we train a subword unit ASR sys-   tem on the English audio - transcription pairs of the   En - De data for 25,000 steps . The resulting ASR   system is used to initialize both En - De and En - Fr   AST systems which are trained for another 20,000   steps . Throughout all speech - to - text experiments ,   we apply gradient accumulation resulting in an ef-   fective mini - batch size of 160k frames . We use   Adam optimizer ( Kingma and Ba , 2015 ) with an   inverse square root learning rate schedule . We use   10k steps for warmup and a peak learning rate of   2e-3 . SpecAugment ( Park et al . , 2019 ) is applied   with a frequency mask parameter of 27 and a time   mask parameter of 100 , both with 1 mask along   their respective dimension . We perform validation   and checkpoint saving after every 1,000 updates .   In case of the CoV oST 2 MT task , the Trans-   former model is pre - trained on in - domain data with   30,000 steps and an effective mini - batch size of   16,000 tokens . For the Europarl - ST dataset , the   MT models are pre - trained on a combination of   Europarl - ST and the additional training data . The   Adam optimizer is used with an inverse square root   learning rate schedule again , now with 4k steps for   warmup and a peak learning rate of 5e-4 . After pre-   training , we finetune the model on the in - domain   data with SGD and a constant learning rate of 5e-5 .   Inference In the speech - to - text experiments , we   average the 10 best checkpoints based on the val-   idation loss . For the MT tasks , we average the 5   best checkpoints . Throughout all AST experiments   and MT tasks , we apply beam search with a beam   size of 5 .   4 Results   Our experiments are focused on the improvements   of our proposed method over KD alone on both   CoV oST 2 and Europarl - ST datasets . We evaluate247   the translation results with both BLEU(Papineni   et al . , 2002 ) and chrF2(Popovi ¬¥ c , 2016 ) using the   implementation of ( Post , 2018 ) . Each   experiment is repeated 3 times and we report mean   and standard deviation .   We also conduct significance tests using a   paired approximate randomization test ( Riezler and   Maxwell III , 2005 ) with default settings of - . We compute p - values between KD and   KD+STR for each evaluated language pair of the   experiments ‚Äô datasets and between each run initial-   ized with the same random seed . The individual   p - values are reported in Appendix A.4 .   Section 4.3 contains a discussion on the connec-   tion between STR- and MT - performance . We also   report additional experiments which show how the   amount of STR data affects the final performance   in Section 4.4 . An error analysis with examples   and a discussion on the limitations of STR has been   moved to Appendix A.5 due to space constraints .   4.1 Results on CoVoST 2   Table 2 lists BLEU scores on the five considered   CoV oST 2 language pairs . Our baseline model is   the AST system finetuned on the in - domain audio-   translation pairs only . Its performance over the   selected language pairs is quite diverse with BLEU   scores ranging from 10.31 ( En - Tr ) to 25.46 ( En-   Cy ) . Our baseline models are comparable to and   often better in terms of BLEU than the bilingual   AST ( Bi - AST ) models by Wang et al . ( 2021 ) .   Training together with translations generated by   KD improves the baseline model by a substantial   margin of 0.8 to 1.6 BLEU points . Our proposed   STR method alone slightly surpasses the KD per-   formance and brings further improvements when   the augmented data is combined ( KD+STR ) with   BLEU score increases ranging from 0.62 for En-   Sl to 0.86 for En - Cy . In total , we observe BLEU   score improvements of 1.5 to 2.3 for KD+STR.Since BLEU scores are often biased towards short   translations , we additionally calculate chrF2 scores   and report them in Appendix A.3 .   We obtain significantly different models for all   language pairs with p < 0.0002 . This is strong   evidence that the better performing models trained   on KD+STR are different to the plain KD models .   4.2 Results on Europarl - ST   Table 3 lists the BLEU score results of Europarl-   ST En - De and En - Fr . Similar to the results on   CoV oST 2 , the KD models bring substantial im-   provements over the baseline systems . The gains   are 6.02 points for En - De and 6.27 points for En - Fr .   We attribute this to the strong machine translation   model that is trained on large amounts of additional   training data ( see Section 4.3 for more details on   this ) . Our proposed STR method alone does not   reach the KD performance but the combination   KD+STR still delivers remarkable gains over KD ,   i.e. , 1.13 points on En - De and 0.45 points on En - Fr ,   showing the complementarity of KD and STR . We   also evaluate our models using chrF2 . The numbers   are listed in Appendix A.3 .   In the En - De experiments , we obtain significant   differences between the KD and KD+STR models   with p < 0.00025 . For En - Fr , only two out of three   runs show significant differences with p < 0.05 .   In terms of chrF2 scores , however , we found all   compared models to be significantly different . See   Appendix A.4 for details.2484.3 Connection to MT - Performance   To evaluate the dependency of STR on the MT-   performance , we calculate BLEU scores for the   MT - systems we use for CoV oST 2 and Europarl - ST   data augmentation with STR and compare them in   a cross - lingual manner . We see a noticeable corre-   lation of MT - performance and STR - improvement .   On CoV oST 2 , the highest improvement for STR   is observed on the En - Cy language pair , which is   also the best performing MT - model . The En - Ca   language pair ‚Äôs MT - model also performs very well   and shows the second highest gain for STR together   with En - Sl . See Table 4 for more details .   On Europarl - ST , we observe a different behav-   ior . While the MT - model for En - Fr is clearly better   than the one for En - De , the gains are larger in the   latter case . This might be due to the fact that the   En - Fr ST - model already has a relatively high per-   formance after training on KD alone ( see Table 3 ) .   We also hypothesize that adding our STR method   to KD is more useful if the sentence structure of   source and target languages is very different . In   case the alignments between source and target lan-   guage are relatively parallel , KD already generates   very useful examples and our approach can only   introduce limited new information on top of that ,   e.g. , by adding speaker variations . See Table 5 for   the exact BLEU scores and improvements .   4.4 Dependence on Amount of STR Data   We conduct an additional experiment on CoV oST 2   to evaluate the dependence of our STR method on   the amount of generated training data . In Figure   2 we report the test performance on 5 language   pairs of a single run ( seed=0 ) after training on 1/3 ,   2/3 , or all STR generated data . For some language   pairs , we already observe large gains after using   1/3 or 2/3 of the total STR data . Most language   pairs will further benefit from more additional data ,   while one language pair ( En - Sl ) seems to degrade   when moving from 2/3 to all training data on this   single run . Summarizing , we observe a trend on   all but one language pair that more augmented data   improves performance .   5 Conclusion   We proposed an effective data augmentation   method for end - to - end speech translation which   leverages audio alignments , linguistic properties ,   and translation . It creates new audio - translation   pairs via sampling from a memory - efficient suffix   memory , translating through an MT model and re-   combining original and sampled audio segments   with translations . Our method achieves signifi-   ca nt improvements over augmentation with KD   alone on both large ( CoV oST 2 ) and small scale   ( Europarl - ST ) datasets . In future work , we would   like to investigate the utility of other linguistic prop-   erties for AST augmentation and we would like to   extend our method to multilingual AST .   Acknowledgements   This research was supported in part by the German   research foundation DFG under grant RI-2221/4 - 1 .   We ‚Äôd also like to thank the anonymous reviewers   for their helpful comments.249References250   A Appendix   A.1 Data Description   CoV oST 2 is a large scale dataset of 430h English   audio and 288k sentences for each language in the   training set . The training set contains repetitions   of the same sentence spoken by different speak-   ers . We use the original data splits generated by   theget_covost_splits.py scripton five   languages pairs , namely English - German ( En - De ) ,   English - Catalan ( En - Ca ) , English - Turkish ( En - Tr ) ,   English - Welsh ( En - Cy ) and English - Slovenian ( En-   Sl ) , resulting in about 15.5k sentences for each dev   and test dataset .   Europarl - ST , in contrast , is a small AST dataset .   It contains debates held in the European Parlia-   ment and their translations , thus representing a re-   alistic AST scenario imposing very different chal-   lenges than the CoV oST 2 dataset . We conduct   experiments on the English - German ( En - De ) and   English - French ( En - Fr ) language pairs . The En - De   data contains 89h of audio and 35.5k sentences .   The En - Fr data contains 87h of audio and 34.5k   sentences . A.2 Filtering Criteria by the Acoustic Aligner   In very rare cases , the acoustic aligner does not   return an alignment at all and we have to discard   these examples . In some cases , the obtained align-   ments by the acoustic aligner are of low quality ,   i.e. , contain alignments to unknown tokens . In   such cases , if the number of tokens of the output   transcriptions of the acoustic aligner matches the   number of tokens in the input transcriptions , we   can still use this alignment for data augmentation   as alignments in ASR are always strictly parallel .   Thus , if we can not retrieve suitable alignments , we   discard the example . This procedure reduces the   amount of augmented data : we discard approxi-   mately 12 % of the examples for CoV oST 2 , and   about 15 % of the examples for Europarl - ST . See   Table 1 for the final data sizes .   A.3 Additional chrF2 Scores   In this section , we additionally report chrF2 scores   on CoV oST 2 and Europarl - ST datasets , since   BLEU scores are often biased towards short trans-   lations . This issue is especially problematic on the   CoV oST 2 datasets because of its large number of   very short sentences . We list the CoV oST 2 chrF2   results in Table 10 , and the Europarl - ST results in   Table 6 .   Our chrF2 results averaged over three runs con-   firm the improvements we observed throughout   our experiments in terms of BLEU . When we look   at chrF2 , the better performing KD+STR models   are always significantly different to the KD mod-   els . Even in case of the En - Fr language pair of the   Europarl - ST dataset where we detected significant   differences only in two of three runs in terms of   BLEU , we found all three runs significantly dif-   ferent in terms of chrF2 with p < 0.025 this time .   Detailed p - values per run are listed in Table 8 and   7 for our CoV oST 2 experiments , and in Table 9 for   our Europarl - ST experiments .   A.4 Detailed p - values for System Comparison   Tables 7 and 8 report the exact p - values for com-   parison of KD and KD+STR models w.r.t . BLEU   and chrF2 scores on CoV oST 2 , respectively . Ta-   ble 9 reports the exact p - values for comparison of   KD and KD+STR models w.r.t . BLEU and chrF2   scores on Europarl - ST , respectively . We use the   implementation of for calculation.251   A.5 Examples and Error Analysis   We also take a look at the quality of our STR-   augmented data and list examples in Table 11 and   Table 12 for CoV oST 2 and Europarl - ST , respec-   tively . Rows ‚Äú src - A ‚Äù and ‚Äú src - B ‚Äù contain the un-   modified transcriptions from CoV oST 2 with our   pivoting token underlined and segments we recom-   bine in italics . The row ‚Äú augm . ‚Äù shows the STR-   augmented example , the row ‚Äú transl . ‚Äù contains the   MT - generated translation . The presented examples   are the first 5 data examples taken directly from   our augmented data set and are notcherry - picked .   Of the first five augmented examples from CoV-   oST 2 listed in Table 11 , examples 1 , 3 , and 5 con-   tain grammatically correct augmented source data   ( row ‚Äú augm . ‚Äù ) and the latter two are also semanti-   cally correct . Example 2 contains a grammatically   wrong segment due to the problematic transcrip-   tion of ‚Äú src - B ‚Äù : here , the example is already an un-   grammatical sentence and this transfers to our aug-   mented example . Example 4 is also grammatically   wrong . In this example , our augmentation method   mixes the different senses of the word ‚Äú directed ‚Äù   and produces a semantically incorrect result . This   could be fixed by integrating more context , e.g. ,   ‚Äú directed through ‚Äù can be used to disambiguate the   different word senses of ‚Äú directed ‚Äù .   Of the first five augmented examples from   Europarl - ST in Table 12 , examples 1 , 3 , and 5   are actually grammatically correct . Example 2 is   grammatically wrong as our STR method does not   respect the different grammatical forms of ‚Äú pass ‚Äù   in ‚Äú will pass ‚Äù and ‚Äú to pass ‚Äù , mixing up the two   objects . Example 4 is also grammatically wrong ,   and it is again the wrong treatment of different   grammatical forms of ‚Äú do ‚Äù in ‚Äú do work ‚Äù and ‚Äú to   do ‚Äù . These problems could be addressed by putting   more effort into the suffix memory construction ,   e.g. , by using n - grams as keys . Examples 3 and 5   demonstrate a property of Europarl - ST that partly   explains the lower performance gain we observe   for our STR - method here : there are many repetitive   formalized sentences , and in these examples our   augmentation method only differs by a single word   from an already existing data example . Still , such   augmented examples can be useful for training due   to the speaker variations injected by STR .   We observe common errors in our augmented   examples for CoV oST 2 and Europarl - ST that are   often connected to the different word senses and   syntactical functions of the selected pivoting token .   However , even grammatically wrong sentences can   sometimes be useful in training as they prevent   overfitting on common structures in the data . Fur-   thermore , the speaker variations in the examples   that we produce can be helpful even if the aug-   mented examples do not differ much from existing   ones . Summarizing the error analysis , our simple   STR - method is able to produce examples that are   useful even with errors . Investigating more com-   plex methods for better identification of pivoting   tokens is a promising direction for future work.252253254   Bohan Zhang   University of Michigan   zbohan@umich.eduPrafulla Kumar Choubey   Salesforce Research   pchoubey@salesforce.comRuihong Huang   Texas A&M University   huangrh@tamu.edu   Abstract   1 Introduction   Text simplification aims to rewrite complex texts in   order to make them easier to read and understand .   This task can benefit vast low literacy readers , in-   cluding children , language learners and people with   aphasia , and has recently attracted increasing at-   tention from the research community ( Xu et al . ,   2016 ; Zhao et al . , 2018 ; Martin et al . , 2019 ; Dong   et al . , 2019 ) . However , most previous research has   focused on sentence - level text simplification and   aim to simplify one sentence at a time . As a result ,   few discourse - level phenomena have been exam-   ined or understood for achieving document - level   text simplification .   Sentence deletion is a commonly used strategy   to achieve intense simplification ( Drndarevic and   Saggion , 2012 ; Woodsend and Lapata , 2011 ) , i.e. ,   some less important sentences from an originalarticle are simply deleted and ignored for simplifi-   cation . While professional re - writers may consider   many factors and use several measures of impor-   tance to decide if a sentence should be deleted ,   some discourse structures provide automated mea-   sures to derive importance for sentences in a doc-   ument . In particular , functional discourse struc-   tures categorize text units ( sentences or paragraphs )   based on their contents and their function roles in   serving the purpose of a specific text - genre , such   as scientific papers ( Teufel et al . , 1999 ; Liakata   et al . , 2012 ) and news articles ( Yarlott et al . , 2018 ;   Choubey et al . , 2020 ) , and are therefore , expected   to directly reveal the importance of a sentence   within a document .   In this work , we explore the use of news genre-   specific functional structures for predicting sen-   tence deletions in news documents . Specifically ,   we use news discourse profiling structure , which   categorizes contents of news articles around the   main news event , constructed through a publicly   available system ( Choubey et al . , 2020 ) . This sys-   tem labels each sentence with one of eight content   types reflecting common discourse roles of a sen-   tence in telling a news story , including two content   types for sentences describing the main news event   and its immediate consequences ( main content ) ,   two content types for sentences providing context-   informing contents and four content types for sen-   tences providing further supportive information in   a news article .   We perform experiments using the Newsela cor-   pus ( Xu et al . , 2015 ) , a widely used dataset for   text simplification research that contains 1492 En-   glish news articles and four simplified versions   for each news article targeting audience of differ-   ent reading levels ( from elementary to high school   students ) . Since we aim to achieve maximal sim-   plification , we predict sentence deletions for tar-255get reading level corresponding to the elementary   school students . We first build a document - level   neural network as the basic model for predicting   sentence deletions . We then incorporate content   types of sentences into the prediction system using   two methods , 1 ) by using content type labels as ad-   ditional features to enrich sentence representations ,   and 2 ) by jointly predicting both sentence deletion   labels and discourse content type labels . Experi-   mental results show that , with little to no drop on   precision , both methods for incorporating sentence   content type information improve the recall ( F1   score ) on the sentence deletion prediction task by   6.5 % ( 3.6 % ) and 10.7 % ( 4.3 % ) respectively . Anal-   ysis on the development set shows that the addi-   tional deletions correctly recognized by our system   are all sentences providing context - informing or   supportive contents .   2 Related Work   The previous research on text simplification has   focused on word or phrase level simplification   ( Yatskar et al . , 2010 ; Biran et al . , 2011 ; Specia et al . ,   2012 ; Paetzold and Specia , 2017 ) , or sentence-   level simplification ( Wubben et al . , 2012 ; Sutskever   et al . , 2014 ; Nisioi et al . , 2017 ; Zhao et al . , 2018 ;   Dong et al . , 2019 ) , few research has been con-   ducted for document - level text simplification .   Sentence deletion , as an interesting phenomenon   for document - level text simplification , has been   studied in several pilot studies . ( Petersen and Os-   tendorf , 2007 ) conducted a corpus analysis and   showed that sentence position and content influ-   ence sentence deletion or retention . The recent pi-   lot research for sentence deletion prediction ( Zhong   et al . , 2019 ) considers sentence position in a doc-   ument , document length and topic , as well as ex-   ploits rhetorical discourse structures that capture   text coherence in general and can be used to derive   thesalience of a sentence in a discourse . However ,   while sentence position and the two document char-   acteristics are shown useful for sentence deletion   prediction , discourse features based on rhetorical   discourse structures are shown to have little im-   pact for this task . Compared to general rhetorical   discourse structures that do not consider genre spe-   cialties , the genre - specific functional structure we   examine in this paper can more directly reveal the   importance of a sentence within a document.3 The News Discourse Structure and   Sentence Types   News discourse profiling ( Choubey et al . , 2020 )   categorizes sentences in news articles into eight   schematic categories that describe the common dis-   course roles of sentences in telling a news story ,   following the news content schemata proposed by   Van Dijk ( Teun A , 1986 ; Van Dijk , 1988a , b ) . These   eight sentence categories fall into three groups .   Main Contents : are the most relevant informa-   tion of news articles , including sentences that intro-   duce the main event as the major subjects of a news   article ( Main Event ) , and sentences that describe   consequence events immediately triggered by the   main event ( Consequence ) .   Context Informing Contents : provide infor-   mation of the actual situation in which main event   occurred , including sentences that describe the re-   cent events that act as possible causes or precon-   ditions for the main event ( Previous Events ) , and   sentences that describe ongoing situation and other   context informing contents ( Current Context ) .   Additional Supportive Contents : contain the   least relevant information , including sentences that   describe past events that precede the main events   in months and years ( Historical Event ) , sentences   that describe unverifiable situations , fictional or   personal account of incidents of an unknown per-   son ( Anecdotal Event ) , opinionated contents that   describe reactions from immediate participants , ex-   perts , known personalities as well as journalist or   news source ( Evaluation ) , and speculations on the   possible consequences of the main or contextual   events ( Expectation )   3.1 Analysis of Deletions w.r.t Sentence Types   We conducted an analysis on deletion rate for each   sentence category using the development set ( Sec-   tion 5.1 ) which was manually annotated with sen-   tence deletion labels . The discourse content type   labels of sentences were predicted by the news   discourse profiling system ( Choubey et al . , 2020 ) .   Table 1 shows the results . We can see that Main   Event sentences have the lowest deletion rate of   14.7 % , much lower than other types of sentences .   Previous Event sentences , as one type of context   informing contents , have a relatively low deletion   rate as well to provide necessary context , i.e. , pos-   sible causes or preconditions , to understand the   main news events . While additional supportive con-   tents overall have a high deletion rate , Anecdotal256   Event sentences have a low deletion rate , possibly   because personal account of incidents present espe-   cially interesting contents for elementary students ,   the target group of our chosen simplification level .   Figure 1 shows an example document where   both deleted sentences ( colored in purple ) are of   one additional supportive content type , Historical   Event .   4 ModelsAs a baseline model , ( shown in Figure 2 ) , we   built a document - level neural network model to   learn context aware sentence representations for   predicting sentence deletions . Similar architectures   have been shown useful for several other discourse-   level tasks ( Nallapati et al . , 2016 ; Choubey et al . ,   2020 ) .   Specifically , the model takes a document as in-   put and has two document - level BiLSTM layers   ( Hochreiter and Schmidhuber , 1997 ) stacked up   with a self - attention layer between them , to suffi-   ciently exploit document wide contexts for building   sentence representations . In addition , for each sen-   tence , we further concatenate its sentence represen-   tation with two vectors obtained by max pooling   over representations of its surrounding sentences   ( two sentences to each side ) , to obtain the final   sentence representation R , that is better aware of   the local context . We use a feed forward neural   network with 1024 - 2 units to predict a binary label   ( deleted or not ) for each sentencebased on its fi-   nal sentence representation . We apply base BERT   ( Devlin et al . , 2019 ) to obtain the initial sentence   representations of 768 dimensions . Both BiLSTMs257   have the hidden dimension size of 512 .   Next , we present two methods to utilize the func-   tional structure for sentence deletion prediction .   4.1 Feature Concatenation   For each sentence , we create a feature vector F   with eight dimensions corresponding to the eight   discourse content types , and values in the vec-   tor are probabilities of content types for the target   sentence as output by the news discourse profil-   ing system . We concatenate the feature vector F   with the final sentence representation Rand feed   the concatenated vector to the sentence deletion   prediction layer .   4.2 Joint Learning   Instead of creating features , we learn to jointly   predict both sentence deletion labels and discourse   content type labels ( system predicted ) using shared   sentence representations ( Figure 3 ) . Specifically ,   we add a new prediction layer with 1024 - 9units   to predict discourse content types for sentences ,   and learn to jointly predict both types of labels   by minimizing the aggregated loss of two tasks :   L = L+Œ≥‚àóL , where Lis the cross - entropy   loss for the sentence deletion prediction task and L   is the mean squared loss for the discourse content   type prediction task.5 Evaluation   5.1 Dataset   We conduct experiments using the Newsela corpus   for text simplification ( Xu et al . , 2015 ) . This corpus   contains 1492 English news articles and four sim-   plified versions for each article targeting students   ranging from grade 2 to grade 12 . In our study , we   focus on predicting sentence deletions to achieve   the relatively aggressive level of simplification that   targets elementary school students ( grades 2 to 5 ) .   Test and Development Data : We created a new   annotated dataset . The annotated dataset of 50   documents used in Zhong et al . ( 2019 ) was not   released yet when we started to work on this project .   Our code and the method to obtain our annotated   dataset can be found on github .   Different from the crowd - sourcing based annota-   tion method of Zhong et al . ( 2019 ) that decomposes   the document - level sentence alignment task to a   paragraph alignment task followed by a paragraph-   level sentence alignment task , we ask our two an-   notators to read through a whole news article and   its simplified article before annotating alignment   sentence by sentence , which enables thorough an-   notations . Then , for each sentence in an original   article , we instruct our annotators to align it with   all the sentences in the simplified article that con-   tain part or all of its contents ( or paraphrases ) , one   sentence in an original article will be labeled as   ‚Äú deleted ‚Äù if nosentence in its simplified article is   aligned with this sentence .   We annotated 95 ( containing 4,334 sentences )   randomly selected news articles . The two anno-   tators first annotated five news articles ( 228 sen-   tences ) in common and achieved a high kappa   agreement ( Artstein and Poesio , 2008 ) of 0.911 .   Then , each of them annotated 45 more articles . We   randomly selected 25 annotated articles and use   them as the development set , and use the other 70   articles as the test set . 48 % and 38 % of sentences   are annotated as deleted in the test and development   sets respectively . We will publish our annotations .   Training Data : We create noisy supervision to   train the systems by applying an automatic sen-   tence alignment tool CATS(≈†tajner et al . , 2018 )   to the remaining 1397 unlabeled news articles and   quickly obtained alignments between these news258   articles and their simplified articles . 82.11 % of   sentence alignments produced by CATS are correct   when evaluated on our development set .   5.2 Experimental Settings   For regularization , we use dropout of 0.5 on the out-   put of both BiLSTMs and the self - attention layer .   We apply Adam optimizer ( Kingma and Ba , 2014 )   for training , and the learning rate is set to 3e-4 . All   the neural models are trained for 15 epochs and we   use the epoch yielding the best validation perfor-   mance . We searched the hyper - parameter Œ≥value   over the range [ 0 , 3 ] with a step size of 0.5 , and its   best value equals to 1.5 .   5.3 Results and Analysis   In Table 3 , we report the performance of our base-   line and the two news discourse profiling structure-   aware models . For better positioning of our work ,   we also re - implemented the model proposed in a   recent work by Zhong et al . ( 2019 ) , a feedforwardneural network ( FNN ) model with sparse features .   First , our baseline system performs better than the   feature based FNN model with 5.3 % and 5.0 %   higher F1 score on validation and test datasets re-   spectively . Then , both methods for incorporating   discourse information have noticeably improved   the performance on sentence deletion prediction .   We also evaluate the models on the dataset from   Zhong et al . ( 2019 ) . As shown in Table 4 , similar   trends were observed on this dataset as well .   Since the performance gains of both discourse-   aware models are mainly on recall , we analyze   the distribution of additional deleted sentences cor-   rectly predicted by the two models . As shown in   Table 2 , the additional deleted sentences are either   context informing contents or additional supportive   contents , but none is main content . This observa-   tion corroborates our analysis in section 3.1 .   6 Conclusion   We study sentence deletion prediction to achieve   document - level text simplification . We have   showed that a genre - specific functional discourse   structure improves the prediction performance by   large margins , when incorporated into a neural net   model either as new features or for joint learn-   ing . For future work , we will study other useful   discourse - level factors for sentence deletion predic-   tion , we will also investigate multi - task learning   to benefit both sentence deletion prediction and   discourse parsing tasks .   7 Acknowledgements   We gratefully acknowledge support from National   Science Foundation via the awards IIS-1942918 .   We would also like to thank the anonymous review-   ers for their feedback.259References260261   Huiyuan Lai , Antonio Toral , Malvina Nissim   CLCG , University of Groningen / The Netherlands   { h.lai , a.toral.ruiz , m.nissim}@rug.nl   Abstract   1 Introduction   Text style transfer ( TST ) is a text generation task   where a given sentence must get rewritten chang-   ing its style while preserving its meaning . Tradi-   tionally , tasks such as swapping the polarity of a   sentence ( e.g. ‚Äú This restaurant is getting worse and   worse . ‚Äù$‚ÄúThis restaurant is getting better and bet-   ter . ‚Äù ) as well as changing the formality of a text   ( e.g. ‚Äú it all depends on when ur ready . ‚Äù $ ‚Äú It all   depends on when you are ready . ‚Äù ) are considered   as instances of TST . We focus here on the latter   case only , i.e. formality transfer , because ( i ) re-   cent work has shown that polarity swap is less of a   style transfer task , since meaning is altered in the   transformation ( Lai et al . , 2021a ) , and ( ii ) data in   multiple languages has recently become available   for formality transfer ( Briakou et al . , 2021b ) .   Indeed , mostly due to the availability of parallel   training and evaluation data , almost all prior TST   work focuses on monolingual ( English ) text ( Rao   and Tetreault , 2018 ; Li et al . , 2018 ; Prabhumoye   et al . , 2018 ; Cao et al . , 2020).As a Ô¨Årst step   towards multilingual style transfer , Briakou et al .   ( 2021b ) have released XFORMAL , a benchmarkof multiple formal reformulations of informal text   in Brazilian Portuguese ( BR - PT ) , French ( FR ) , and   Italian ( IT ) . For these languages the authors have   manually created evaluation datasets . On these ,   they test several monolingual TST baseline models   developed using language - speciÔ¨Åc pairs obtained   by machine translating GYAFC , a English corpus   for formality transfer ( Rao and Tetreault , 2018 ) .   Briakou et al . ( 2021b ) Ô¨Ånd that the models trained   on translated parallel data do not outperform a sim-   ple rule - based system based on handcrafted trans-   formations , especially on content preservation , and   conclude that formality transfer on languages other   than English is particularly challenging .   One reason for the poor performance could be   the low quality ( observed upon our own manual   inspection ) of the pseudo - parallel data , especially   the informal side . Since machine translation sys-   tems are usually trained with formal texts like   news ( Zhang et al . , 2020 ) , informal texts are harder   to translate , or might end up more formal when   translated . But most importantly , the neural models   developed by Briakou et al . ( 2021b ) do not take ad-   vantage of two recent Ô¨Åndings : ( i ) pre - trained mod-   els , especially the sequence - to - sequence model   BART ( Lewis et al . , 2020 ) , have proved to help sub-   stantially with content preservation in style trans-   fer ( Lai et al . , 2021b ) ; ( ii ) Multilingual Neural Ma-   chine Translation ( Johnson et al . , 2017 ; Aharoni   et al . , 2019 ; Liu et al . , 2020 ) and Multilingual Text   Summarization ( Hasan et al . , 2021 ) have achieved   impressive results leveraging multilingual models   which allow for cross - lingual knowledge transfer .   In this work we use the multilingual large model   mBART ( Liu et al . , 2020 ) to model style transfer in   a multilingual fashion exploiting available parallel   data of one language ( English ) to transfer the task   and domain knowledge to other target languages .   To address real - occurring situations , in our exper-   iments we also simulate complete lack of parallel   data for a target language ( even machine translated),262and lack of style - related data at all ( though avail-   ability of out - of - domain data ) . Language speci-   Ô¨Åcities are addressed through adapter - based strate-   gies ( Pfeiffer et al . , 2020 ; √úst√ºn et al . , 2020 , 2021 ) .   We obtain state - of - the - art results in all three target   languages , and propose a modular methodology   that can be applied to other style transfer tasks as   well as to other languages . We release our code   and hopefully foster the research progress .   2 Approach and Data   As a base experiment aimed at exploring the con-   tribution of mBART ( Liu et al . , 2020 ; Tang et al . ,   2020 ) for multilingual style transfer , we Ô¨Åne - tune   this model with parallel data speciÔ¨Åcally developed   for style transfer in English ( original ) and three   other languages ( machine translated ) .   Next , in view of the common situation where   parallel data for a target language is not avail-   able , we propose a two - step adaptation training   approach on mBART that enables modular mul-   tilingual TST . We avoid iterative back - translation   ( IBT ) ( Hoang et al . , 2018 ) , often used in previous   TST work ( Prabhumoye et al . , 2018 ; Lample et al . ,   2019 ; Yi et al . , 2020 ; Lai et al . , 2021a ) , since it has   been shown to be computationally costly ( √úst√ºn   et al . , 2021 ; Stickland et al . , 2021a ) . We still run   comparison models that use it .   In the Ô¨Årst adaptation step , we address the prob-   lem of some languages being not well represented   in mBART , which preliminary experiments have   shown to hurt our downstream task . We conduct   a language adaptation denoising training using un-   labelled data for the target language . In the sec-   ond step , we address the task at hand through Ô¨Åne-   tuning cross - attention with auxiliary gold parallel   English data adapting the model to the TST task .   For TST Ô¨Åne - tuning , we use parallel training   data , namely formal / informal aligned sentences   ( both manually produced for English and machine   translated for three other languages ) . For the adap-   tation strategies , we also collect formality and   generic non - parallel data . Details follow .   English formality data GYAFC ( Rao and   Tetreault , 2018 ) is an English dataset of aligned   formal and informal sentences . Gold parallel pairsare provided for training , validation , and test .   Multilingual formality data XFORMAL ( Bri-   akou et al . , 2021b ) is a benchmark for multilingual   formality transfer , which provides an evaluation set   that consists of four formal rewrites of informal sen-   tences in BR - PT , FR , and IT . This dataset contains   pseudo - parallel corpora in each language , obtained   via machine translating the English GYAFC pairs .   Language - speciÔ¨Åc formality non - parallel data   Following Rao and Tetreault ( 2018 ) and Briakou   et al . ( 2021b ) , we crawl the domain data in tar-   get language from Yahoo Answers . We then use   the style regressor from Briakou et al . ( 2021a ) to   predict formality score of the sentence to auto-   matically select sentences in each style direction .   Language - speciÔ¨Åc generic non - parallel data   5 M sentences containing 5 to 30 words for each   language randomly selected from News Crawl .   3 Adaptation Training   To adapt mBART to multilingual TST , we employ   two adaptation training strategies that target lan-   guage and task respectively .   3.1 Language Adaptation   As shown in Figure 1(a ) , we introduce a mod-   ule for language adaptation . Inspired by previous   work ( Houlsby et al . , 2019 ; Bapna and Firat , 2019 ) ,   we use an adapter ( ADAPT ; ~50 M parameters ) ,   which is inserted into each layer of the Transformer   encoder and decoder , after the feed - forward block .   Following Bapna and Firat ( 2019 ) , the ADAPT   moduleAat layericonsists of a layer-   normalization LN of the input x2Rfollowed by   a down - projection W2R , a non - linearity   and an up - projection W2Rcombined with   a residual connection with the input x :   A(x ) = WRELU ( WLN(x ) ) + x(1 )   Language adaptation training Following   mBART ‚Äôs pretraining , we conduct the language   adaptation training on a denoising task , which   aims to reconstruct text from a corrupted version :   L= X   log(Tjg(T ) ;  ) ( 2)263   where  are the parameters of adaptation module   A , Tis a sentence in target language and gis the   noise function that masks 30 % of the words in   the sentence . Each language has its own separate   adaptation module . During language adaptation   training , the parameters of the adaptation module   are updated while the other parameters stay frozen .   3.2 Task Adaptation   As shown in Figure 1(b ) , after training the language   adaptation module we Ô¨Åne - tune the model on the   auxiliary English parallel data with the aim of mak-   ing the model adapt to the speciÔ¨Åc task of formality   transfer . Following Stickland et al . ( 2021b ) , we   only update the parameters of the decoder ‚Äôs cross-   attention ( i.e. task adaptation module ) while the   other parameters are Ô¨Åxed , thus limiting computa-   tional cost and catastrophic forgetting .   Multilingual TST process For the language   adaptation modules we have two settings : ( i ) adap-   tation modules Aon the encoder come from the   model trained with source style texts , and modules   Aon the decoder come from the model trained   with target style texts ( M2.X , Table 1 ) ; ( ii ) both A   andAare from a model trained with generic texts   ( M3.X ) , so there are no source and target styles for   the adaptation modules . For the task adaptation   modules , we also have two settings : ( i ) the module   is from the English model ( X + EN cross - attn ) ; ( ii )   Ô¨Åne - tuning the model of the target language with   English parallel data ( X + EN data ) .   4 Experiments   All experiments are implemented atop Trans-   formers ( Wolf et al . , 2020 ) using mBART - large-50 ( Tang et al . , 2020 ) . We train the model using   the Adam optimiser ( Kingma and Ba , 2015 ) with   learning rate 1e-5 for all experiments . We train   the language adaptation modules with generic texts   separately for each language for 200k training steps   with batch size 32 , accumulating gradients over 8   update steps , and set it to 1 for other training .   Evaluation Following previous work ( Luo et al . ,   2019 ; Sancheti et al . , 2020 ) , we assess style   strength and content preservation . We Ô¨Åne - tune   mBERT ( Devlin et al . , 2019 ) with Briakou et al .   ( 2021b ) ‚Äôs pseudo - parallel corpora to evaluate the   style accuracy of the outputs . We also use a style re-   gressor from Briakou et al . ( 2021a ) , which is based   on XLM - R ( Conneau et al . , 2020 ) and is shown to   correlate well with human judgments . We calcu-   late BLEU and COMET ( Rei et al . , 2020 ) to assess   content preservation . As overall score , following   previous work , we compute the harmonic mean   ( HM ) of style accuracy and BLEU .   Systems Based on our data ( Section 2 ) , we have   four settings for our systems . D1 : pseudo - parallel   data in the target language via machine translating   the English resource ; D2 : non - parallel style data in   the target language ; D3 : no style data in the target   language ; D4 : no parallel data at all . The Ô¨Årst three   settings all contain gold English parallel data .   Results Table 1 shows the results for both I ! F   ( informal - to - formal ) and F ! I ( formal - to - informal )   transformations . We include the models from Bri-   akou et al . ( 2021b ) for comparison ( they only   model the I!F direction).264   Results in D1show that Ô¨Åne - tuning mBART   with pseudo - parallel data yields the best overall per-   formance in the I!F direction . The F ! I results ,   instead , are rather poor and on Italian even worse   than IBT - based models ( M2.1 ) . This could be due   to this direction being harder in general , since there   is more variation in informal texts , but it could also   be made worse by the bad quality of the informal   counterpart in the translated pairs . Indeed , work   in machine translation has shown that low - quality   data is more problematic in the target side than in   the source side ( Bogoychev and Sennrich , 2019 ) .   InD2 , we see that our proposed adaptation ap-   proaches outperform IBT - based models on both   transfer directions . The results of Ô¨Åne - tuning the   target language ‚Äôs model with English parallel data   are generally better than inserting the EN model ‚Äôs   cross - attention module into the target language ‚Äôs   model . This suggests that the former can better   transfer task and domain knowledge .   InD3 , the large amounts of generic texts yield   more improvement in I ! F direction rather than   F!I. This could be due to generic texts being more   formal than informal . The performance improve-   ment on Portuguese is particularly noticeable ( com-   pare M3.1 trained with EN data only with other   M3.X models ) , and mostly due to this language   being less represented than the others in mBART .   Interestingly , the performance of task adaptation   strategies is reversed compared to D2 : it is here   better to adapt cross attention in the English model   rather than Ô¨Åne - tune the target language model di-   rectly . Future work will need to investigate how   using different data sources for language adapta-   tion ( D2 , style - speciÔ¨Åc vs D3 , generic ) interacts   with task adaptation strategies .   Results for D4show that language adaptationtraining helps with content preservation , especially   for Portuguese , conÔ¨Årming this curbs the problem   of language underrepresentation in pre - training .   However , low performance on style accuracy shows   that task - speciÔ¨Åc data is necessary , even if it comes   from a different language .   5 Analysis and Discussion   Case Study Table 2 shows a group of example   outputs in Italian . In the I ! F direction , most sys-   tems tend to copy a lot from the source and change   formality words slightly . DLSM and Rule - based   systems fail to transfer the formality style while oth-   ers are successful to some extent : our M1.1 yields   the best performance on the style strength . When   looking at content , most outputs contain more or   less part of the source sentence ; Multi - Task system   achieves the highest BLEU score but our systems   ( except for M3.3 ) have higher COMET scores , with   M3.1 achieving the highest score . For the F ! I di-   rection , we can see that M1.1 has the worst perfor-   mance on style strength ( its output is almost iden-   tical to the source ) , while M2.1 , M3.1 and M3.2   generate the same output with the lowest regression   score . Overall , M3.3 achieves the best performance   on style and content .   Direction Analysis For English , Rao and   Tetreault ( 2018 ) Ô¨Ånd that the I ! F direction is quite   different from the opposite one since there are far   more ways to express informality . As our work is   the Ô¨Årst attempt at the F ! I direction in a multilin-   gual setting , we run some additional analysis using   two test sets for each direction : ( a ) the original   test set ; ( b ) the test set of the opposite direction ,   swapping sources and references . We Ô¨Åne - tune   BART ( Lewis et al . , 2020 ) and mBART-50 ( Tang   et al . , 2020 ) with English parallel data ( GYAFC)265   and evaluate them on ( a ) and ( b ) . Figure 2 shows   the results of content preservation . For INPUT   ( source copy ) , BLEU scores are almost the same   swapping sources and references but COMET ones   are not , probably due to COMET being trained to   prefer a formal / better ‚Äú generated sentence ‚Äù ; com-   pared to INPUT , the performance gain of BART   and mBART in I!F is larger than the opposite   direction on both metrics . Results are similar for   other languages ( Table 3 ) . We pick M1.1 and M1.2   from Table 1 since they are both Ô¨Åne - tuned using   parallel data in the target language . BLEU scores   of F!I are always lower than the opposite ; the   COMET score of INPUT in F ! I is higher than   I!F , but scores of both systems for F ! I drop af-   ter transforming the source sentence into the target   style . All these observations suggest that there is   more variation in informal texts for the languages   we consider , and the F ! I direction is harder .   6 Conclusions   Fine - tuning a pre - trained multilingual model with   machine translated training data yields state - of - the-   art results for transferring informal to formal text .   The results for the formal - to - informal direction are   considerably worse ‚Äî the task is more difÔ¨Åcult , and   the quality of translated informal text is lower . We   have also proposed two adaptation training strate-   gies that can be applied in a cross - lingual transfer   strategy . These strategies target language and task   adaptation , and can be combined to adapt mBART   for multilingual formality transfer . The adaptation   strategies with auxiliary parallel data from a differ-   ent language are effective , yielding competitive re-   sults and outperforming more classic IBT - based ap-   proaches without task - speciÔ¨Åc parallel data . Lastly ,   we have shown that formal - to - informal transforma-   tion is harder than the opposite direction.266Acknowledgments   This work was partly funded by the China Schol-   arship Council ( CSC ) . The anonymous reviewers   of ACL Rolling Review provided us with useful   comments which contributed to improving this pa-   per and its presentation , so we ‚Äôre grateful to them .   We would also like to thank the Center for Infor-   mation Technology of the University of Groningen   for their support and for providing access to the   Peregrine high performance computing cluster .   Ethics Statement   All work that automatically generates and/or al-   ters natural text could unfortunately be used mali-   ciously . While we can not fully prevent such uses   once our models are made public , we do hope that   writing about risks explicitly and also raising aware-   ness of this possibility in the general public are   ways to contain the effects of potential harmful   uses . We are open to any discussion and sugges-   tions to minimise such risks .   References267268269A   This appendices include : ( i ) Results for BART and mBART on English data ( A.1 ) ; ( ii ) Results for style   classiÔ¨Åers / regressor ( A.2 ) ; ( iii ) Detailed results for multilingual formality transfer ( A.3 ) .   A.1 Results for BART and mBART on English data   We Ô¨Åne - tune BART ( Lewis et al . , 2020 ) and mBART-50 ( Tang et al . , 2020 ) with English parallel data   speciÔ¨Åcally developed for formality transfer in English ( GYAFC ) . The performance of BART and English   data can be seen as a sort of upperbound , as these are best conditions ( monolingual model , and gold   parallel data ) . The drop we see using mBART is rather small , suggesting mBART is a viable option . We   also see that formal to informal is much harder than viceversa , probably due to high variability in informal   formulations ( Rao and Tetreault , 2018 ) . Table A.1 shows the results for both models .   A.2 Results for style classiÔ¨Åers / regressor   We compare four different style classiÔ¨Åers and one regressor : ( i ) TextCNN ( Kim , 2014 ) trained with   pseudo - parallel data in the target language ; ( ii ) mBERT ( Devlin et al . , 2019 ) Ô¨Åne - tuned with pseudo-   parallel data , English data , or a combination of all data ; and ( iii ) a XLM - R ( Conneau et al . , 2020 ) based   style regressor from Briakou et al . ( 2021a ) , which is trained with formality rating data in English.270A.3 Detailed results for multilingual formality transfer271   Orion Weller *   Johns Hopkins UniversityKevin Seppi   Brigham Young UniversityMatt Gardner   Microsoft Semantic Machines   Abstract   1 Introduction   The standard supervised training paradigm in NLP   research is to Ô¨Åne - tune a pre - trained language   model on some target task ( Peters et al . , 2018 ; De-   vlin et al . , 2018 ; Raffel et al . , 2019 ; Gururangan   et al . , 2020 ) . When additional non - target super-   vised datasets are available during Ô¨Åne - tuning , it is   not always clear how to best make use of the sup-   porting data ( Phang et al . , 2018 , 2020 ; Liu et al . ,   2019b , a ; Pruksachatkun et al . , 2020a ) . Althoughthere are an exponential number of ways to com-   bine or alternate between the target and supporting   tasks , three predominant methods have emerged :   ( 1 ) Ô¨Åne - tuning on a supporting task and then the tar-   get task consecutively , often called STILTs ( Phang   et al . , 2018 ) ; ( 2 ) Ô¨Åne - tuning on a supporting task   and the target task simultaneously ( here called pair-   wise multi - task learning , or simply MTL ) ; and ( 3 )   Ô¨Åne - tuning on all Navailable supporting tasks and   the target tasks together ( MTL , N > 1 ) .   Application papers that use these methods gener-   ally focus on only one method ( S√∏gaard and Bingel ,   2017 ; Keskar et al . , 2019 ; Glavas and Vuli ¬¥ c , 2020 ;   Sileo et al . , 2019 ; Zhu et al . , 2019 ; Weller et al . ,   2020 ; Xu et al . , 2019 ; Chang and Lu , 2021 ) , while   a limited amount of papers consider running two .   Those that do examine them do so with a limited   number of conÔ¨Ågurations : Phang et al . ( 2018 ) ex-   amines STILTS and one instance of MTL , Chang-   pinyo et al . ( 2018 ) ; Peng et al . ( 2020 ) ; Schr√∂der   and Biemann ( 2020 ) compare MTL with MTL ,   and Wang et al . ( 2018a ) ; Talmor and Berant ( 2019 ) ;   Liu et al . ( 2019b ) ; Phang et al . ( 2020 ) use MTL   and STILTs but not pairwise MTL .   In this work we perform comprehensive experi-   ments using all three methods on the 9 datasets in   the GLUE benchmark ( Wang et al . , 2018b ) . We   surprisingly Ô¨Ånd that a simple size heuristic can be   used to determine with more than 92 % accuracy   which method to use for a given target and support-   ing task : when the target dataset is larger than the   supporting dataset , STILTS should be used ; oth-   erwise , MTL should be used ( MTLis almost   universally the worst of the methods in our experi-   ments ) . To conÔ¨Årm the validity of the size heuristic ,   we additionally perform a targeted experiment vary-   ing dataset size for two of the datasets , showing   that there is a crossover point in performance be-   tween the two methods when the dataset sizes are   equal . We believe that this analysis will help NLP   researchers to make better decisions when choosing272   a TL method and will open up future research into   understanding the cause of this heuristic ‚Äôs success .   2 Experimental Settings   Dataset Suite To conduct this analysis , we   chose to employ the GLUE dataset suite , following   and comparing to previous work in transfer learn-   ing for NLP ( Phang et al . , 2018 ; Liu et al . , 2019b ) .   Training Framework We use Huggingface ‚Äôs   transformers library ( Wolf et al . , 2019 ) for access-   ing the pre - trained encoder and for the base training   framework . We extend this framework to combine   multiple tasks into a single PyTorch ( Paszke et al . ,   2017 ) dataloader for MTL and STILTs training .   Many previous techniques have been proposed   for how to best perform MTL ( Raffel et al . , 2019 ;   Liu et al . , 2019b ) , but a recent paper by Got-   tumukkala et al . ( 2020 ) compared the main ap-   proaches and showed that a new dynamic approach   provides the best performance in general . We im-   plement all methods described in their paper and   experimented with several approaches ( sampling   by size , uniformity , etc . ) . Our initial results found   that dynamic sampling was indeed the most effec-   tive on pairwise tasks . Thus , for the remainder   of this paper , our MTL framework uses dynamic   sampling with heterogeneous batch schedules . Forconsistency , we train the STILTs models using the   same code , but include only one task in the dat-   aloader instead of multiple . The MTLsetup uses   the same MTL code , but includes all 9 GLUE tasks .   We train each model on 5 different seeds to con-   trol for randomness ( Dodge et al . , 2020 ) . For the   STILTs method , we train 5 models with different   seeds on the supporting task and then choose the   best of those models to train with 5 more random   seeds on the target task . For our Ô¨Ånal reported   numbers , we record both the average score and   the standard deviation , comparing the MTL ap-   proach to the STILTs approach with a two - sample   t - test . In total , we train 985 = 360 different   MTL versions of our model , 5 MTLmodels , and   95 + 95 = 90 models in the STILTs setting .   Model We use the DistilRoBERTa model ( pre-   trained and distributed from the transformers li-   brary similarly to the DistilBERT model in Sanh   et al . ( 2019 ) ) for our experiments , due to its strong   performance and efÔ¨Åciency compared to the full   model . For details regarding model and compute   parameters , see Appendix A. Our purpose is notto   train the next state - of - the - art model on the GLUE   task and thus the absolute scores are not imme-   diately relevant ; our purpose is to show how the   different methods score relative to each other . We   note that we conducted the same analysis in Fig-273   ure 1 for BERT and found the same conclusion ( see   Appendix D ) , showing that our results extend to   other pre - trained transformers .   3 Results   We provide three different analyses : a comparison   of pairwise MTL vs STILTs , experiments varying   dataset size to validate our Ô¨Åndings , and a compari-   son of pairwise approaches vs MTL .   MTL vs STILTs We Ô¨Årst calculate the abso-   lute score matrices from computing the MTL and   STILTs method on each pair of the GLUE dataset   suite , then subtract the STILTs average score ma-   trix from the MTL one ( Figure 1 ) . Thus , this shows   the absolute score gain for using the MTL method   instead of the STILTs method ( negative scores in-   dicate that the STILTs method was better , etc . ) .   However , this matrix does not tell us whether   these differences are statistically signiÔ¨Åcant ; for   this we use a two - sample t - test to compare the   mean and standard deviation of each method for   a particular cell . Scores that are statistically sig-   niÔ¨Åcant are color coded green ( if STILTs is better )   or blue ( if MTL is better ) , whereas they are coded   grey if there is no statistically signiÔ¨Åcant difference .   We note that although some differences are large   ( e.g. a 9 point difference on ( WNLI , STS - B ) ) the   variance of these results is high enough that there   is no statistically signiÔ¨Åcant difference between the   STILTs and MTL score distributions . We order the datasets in Figure 1 by size , to   visually illustrate the trend . The number of green   cells in a row is highly correlated with the size of   the dataset represented by that row . For example ,   MNLI is the largest and every cell in the MNLI   row is green . QQP is the 2nd largest and every cell   in its row is also green , except for ( QQP , MNLI ) .   The smallest dataset , WNLI , has zero green cells .   We can summarize these results with the follow-   ing size heuristic : MTL is better than STILTs   when the target task has fewer training in-   stances than the supporting task and vice versa .   In fact , if we use this heuristic to predict which   method will be better we Ô¨Ånd that it predicts 49/53   signiÔ¨Åcant cells , which is equivalent to 92.5 % accu-   racy . To more clearly visualize which cells it fails   to predict accurately , those four cells are indicated   with red text . We note that this approach does not   hold on the cells that have no statistically signiÔ¨Å-   ca nt difference between the two methods : but for   almost every signiÔ¨Åcant cell , it does .   Unfortunately , there is no clear answer to why   those four cells are misclassiÔ¨Åed . Three of the four   misclassiÔ¨Åed cells come when using the MRPC   dataset as the target task , but there is no obvious   reason why it fails on MRPC . We recognize that   this size heuristic is not an absolute law , but merely   a good heuristic that does so with high accuracy :   there are still other pieces to this puzzle that this   work does not consider , such as dataset similarity .   Dataset Size Experiments In order to validate274   the size heuristic further we conduct controlled   experiments that alter the amount of training data   of the supporting task to be above and below the   target task . We choose to test QNLI primary with   MNLI supporting , as they should be closely related   and thus have the potential to disprove this heuristic .   We subsample data from the supporting task so that   we have a proportion Kof the size of the primary   task ( whereK2f1=3;1=2;1;2;3 g ) . By doing so ,   we examine whether the size heuristic holds while   explicitly controlling for the supporting task ‚Äôs size .   Other than dataset size , all experimental parameters   are the same as in the original comparison ( ¬ß 2 ) .   We also test whether these results hold if the size   of the primary dataset is changed ( e.g. , perhaps   there is something special about the current size   of the QNLI dataset ) . We take the same pair and   reduce the training set of QNLI in half , varying   MNLI around the new number of instances in the   QNLI training set as above ( e.g. 1/3rd , 1/2 , etc . ) .   The results of these two experiments are in Fig-   ure 2 . We can see that as the size of the supporting   dataset increases , MTL becomes more effective   than STILTs . Furthermore , we Ô¨Ånd that when both   datasets are equal sizes the two methods are statis-   tically similar , as we would expect from the size   heuristic ( Support Task Proportion = 1.0 ) .   Thus , the synthetic experiments corroborate our   main Ô¨Ånding ; the size heuristic holds even on con-   trolled instances where the size of the training sets   are artiÔ¨Åcially manipulated .   Pairwise TL vs MTL We also experiment   with MTLon GLUE ( see Appendix B for im-   plementation details ) . We Ô¨Ånd that the average   pairwise approach consistently outperforms the   MTLmethod , except for the RTE task ( Table 1 )   and using the best supporting task outperforms   MTLin every case ( Pairwise Oracle ) . Thus , al-   though MTLis conceptually simple , it is not the   best choice w.r.t . the target task score : on a randomdataset simply using STILTs or MTL will likely   perform better . Furthermore , using the size heuris-   tic on the average supplementary task increases the   score by 5 points over MTL(78.3 vs 73.3 ) .   4 Related Work   A large body of recent work ( S√∏gaard and Bingel ,   2017 ; Vu et al . , 2020 ; Bettgenh√§user et al . , 2020 ;   Peng et al . , 2020 ; Poth et al . , 2021 ) exists that ex-   amines when these transfer learning methods are   more effective than simply Ô¨Åne - tuning on the target   task . Oftentimes , these explanations involve recog-   nizing catastrophic forgetting ( Phang et al . , 2018 ;   Pruksachatkun et al . , 2020b ; Wang et al . , 2018a )   although recent work has called for them to be re-   examined ( Chang and Lu , 2021 ) . This paper is or-   thogonal to those , as we examine when you should   choose MTL or STILTs , rather than when they are   more effective than the standard Ô¨Åne - tuning case   ( in fact , these strategies could be combined to pre-   dict transfer and then use the best method ) . As   our task is different , theoretical explanations for   how these methods work in relation to each other   will need to be explored in future work . Potential   theories suggested by our results are discussed in   Appendix C , and are left to guide those efforts .   5 Conclusion   We examined the three main strategies for transfer   learning in natural language processing : training   on an intermediate supporting task to aid the target   task ( STILTs ) , training on the target and supporting   task simultaneously ( MTL ) , or training on multiple   supporting tasks alongside the target task ( MTL ) .   We provide the Ô¨Årst comprehensive comparison be-   tween these three methods using the GLUE dataset   suite and show that there is a simple rule for when   to use one of these techniques over the other . This   simple heuristic , which holds true in more than 92 %   of applicable cases , states that multi - task learning275is better than intermediate Ô¨Åne tuning when the   target task is smaller than the supporting task and   vice versa . Additionally , we showed that these pair-   wise transfer learning techniques outperform the   MTLapproach in almost every case .   References276277   A Training and Compute Details   We use the hyperparameters given by the trans-   former library example on GLUE as the default   for our model ( learning rate of 2e-5 , batch size of   128 , AdamW optimizer ( Kingma and Ba , 2014 ) ,   etc . ) . We train for 10 epochs , checkpointing every   half an epoch and use the best model on the de-   velopment set for the test set scores . We train on   a mix of approximately 10 K80 and P100 GPUs   for approximately two weeks for the main experi-   ment , using another week of compute time for the   synthetic experiments ( ¬ß 3 ) . Our CPUs use 12 - core   Intel Haswell ( 2.3 GHz ) processors with 32 GB of   RAM .   B Pairwise Approaches vs MTL   Experimental Setup We use MTLwith three   different sampling methods : uniform sampling ,   sampling by dataset size , and dynamic sampling .   To illustrate the difference between MTLand   the pairwise methods , we show the average score   across all supplementary tasks for MTL and   STILTs . We also show the average score found   by choosing MTL or STILTs using the size heuris-   tic as Ave . S.H. . Finally , we report the score from   the best task using the best pairwise method , which   we call the Pairwise Oracle . The results are shown   in Table 2 .   Results Although dynamic sampling was more   effective for the pairwise tasks , we Ô¨Ånd that dy-   namic sampling was worse than sampling by size   when using MTL on all nine datasets ( top half of   Table 2).However , when the MTLmethod is compared   to the pairwise methods , it does not perform as well   ( bottom half of Table 2 ) . We see that the Pairwise   Oracle , which uses the best supplementary task for   the given target task , outperforms all methods by   a large margin . Thus , although MTLis concep-   tually simple , it is not the best choice with respect   to target task accuracy . Furthermore , if you could   predict which supplementary task would be most   effective ( Pairwise Oracle , c.f . Section 4 , Vu et al .   ( 2020 ) ; Poth et al . ( 2021 ) , etc . ) , you would be able   to make even larger gains over MTL .   C Theories for Transfer Effectiveness   Previous work often invokes ideas such as catas-   trophic forgetting to describe why STILTs or MTL   does or does not improve over the basic Ô¨Åne - tuning   case ( Phang et al . , 2018 ; Pruksachatkun et al . ,   2020b ; Wang et al . , 2018a ) . However , as our work   provides a novel comparison of MTL vs. STILTs   there exists no previous work that shows how these   methods differ in any practical or theoretical terms   ( e.g. does MTL or STILTs cause more catastrophic   forgetting of the target task ) . Furthermore , previous   explanations for why the STILTs method works has   been called into question ( Chang and Lu , 2021 ) ,   leaving it an open research area .   A naive explanation for our task would be to   think that when the target task is larger , STILTs   should be worse because of catastrophic forgetting ,   whereas MTL would still have access to the sup-   porting task . However , for STILTs this catastrophic   forgetting would mainly effect the supporting task   performance , not the target task performance , mak-   ing that explanation unlikely in some contexts ( e.g.   when the tasks are not closely related ) . One poten-   tial explanation based on our results is that a small   supporting task is best used to provide a good ini-278tialization for a larger target task ( e.g. STILTs )   while a large supporting task used for initialization   would change the weights too much for the small   target task to use effectively ( thus making MTL the   more effective strategy for a larger supporting task ) .   Another explanation could be that a larger target   task does not beneÔ¨Åt from MTL ( and perhaps is   harmed by it , e.g. catastrophic interference ) and   therefore , STILTs is more effective - while MTL is   more effective for small target tasks . However , all   of these explanations also fail to take into account   task relatedness , which likely also plays a role in   the theoretical explanation ( although even that too ,   has been called into question with Chang and Lu   ( 2021 ) ) .   We thus note that there are a myriad of possi-   ble explanations ( and the answer is likely a com-   plex combination of possible explanations ) , but   these are out of the scope of this work . Our work   aims to show what happens in practice , rather than   proposing a theoretical framework . As theoretical   explanations for transfer learning are still an active   area of research , we leave them to future work and   provide this empirical comparison to guide their   efforts and the current efforts of NLP researchers   and practitioners .   D Alternate Model : BERT   We conduct the same analysis as Figure 1 with   the BERT model and Ô¨Ånd similar results ( Figure 3 ,   thus showing that our results transfer to other pre-   trained transformer models . We follow previous   work in using two different pre - trained models for   our analysis ( Talmor and Berant , 2019 ; Phang et al . ,   2018 ) .   E Additional Background Discussion   In this section we will show how the size heuristic   is supported by and helps explain the results of   previous work in this area . Although this section   is not crucial to the main result of our work , we   include it to help readers who may not be as   familiar with the related work . We examine two   works in depth and then discuss broader themes of   related work .   BERT on STILTs Phang et al . ( 2018 ) This   work deÔ¨Åned the acronym STILTs , or Supplemen-   tary Training on Intermediate Labeled - data Tasks ,   which has been an inÔ¨Çuential idea in the commu-   nity ( V oskarides et al . , 2019 ; Yan et al . , 2020 ; ClarkModel RTE accuracy   GPT!RTE 54.2   GPT!MNLI!RTE 70.4   GPT!{MNLI , RTE } 68.6   GPT!{MNLI , RTE}!RTE 67.5   et al . , 2020 ) . To determine the effect of the inter-   mediate training , the authors computed the STILTs   matrix of each pair in the GLUE dataset . As our   model and training framework are different from   their methodology , we can not compare our matrix   with the absolute numbers in their matrix . However ,   at the end of Section 4 in their paper , they conduct   an experiment with MTL and compare the results   to their STILTs matrix ( their experimental results   are reproduced in Table 3 for convenience ) . Their   analysis uses MNLI as the supporting task and RTE   as the target task , trying MTL , STILTs , MTL+Ô¨Åne-   tuning , and only Ô¨Åne - tuning on RTE . Their results   show that STILTs provides the highest score , with   all MTL varieties being worse . From this they con-   clude that MTL is worse than STILTs .   How does this compare to our results ? In Fig-   ure 1 we see that our results also show that the   STILTs method is better than the MTL method for   the ( RTE , MNLI ) pair , showing that our results are   consistent with those in the literature . Furthermore ,   we Ô¨Ånd that this is one of the 4 signiÔ¨Åcant cells   in our matrix where the size heuristic does not ac-   curately predict the best method . It is unfortunate   that the task they decided to pick happened to be   one of the anomalies . Thus , our paper extends and   completes their results with more rigor .   MultiQA Talmor and Berant ( 2019 ) MultiQA   showed that using MTL on a variety of question-   answering ( QA ) datasets made it possible to train   a model that could outperform the current SOTA   on those QA datasets . They used an interesting   approach to MTL , pulling 15k examples from each   of the 5 major datasets to compose one new ‚Äú MTL "   task , called Multi-75K. They then show results for   STILTs transfer on those same datasets along with   the MTL dataset ( their data is reproduced with   new emphasis in Appendix E Table 4 for conve-279   nience ) . We note that this STILTs - like transfer   with the ‚Äú MTL " dataset is an equivalent method   to doing MTL and then Ô¨Åne - tuning on the target   task , reminiscent of the third example in Phang et al .   ( 2018 ) ( Table 3 , GPT ! { MNLI , RTE}!RTE , c.f .   Appendix E ) .   How does this relate to our results ? The size   heuristic says that MTL is better than STILTs when   the target task has fewer training instances . In   the MultiQA paper the size of each training set is   artiÔ¨Åcially controlled to be the same number ( 75k   instances ) , thus our size heuristic would say that the   methods should be comparable . Although no error   bounds or standard deviations are reported in their   paper ( which makes the exact comparison difÔ¨Åcult ) ,   we see that the MTL approach performs equal or   better on almost half of the datasets . Thus , although   the MultiQA paper is not strictly comparable to our   work due to their training setup ( the MTL+Ô¨Åne   tuning ) , their results agree with our hypothesis as   well .   For convenience , Table 4 from Talmor and Be - rant ( 2019 ) is reproduced here in the appendix .   The top half contains the results using the DocQA   model while the bottom half uses BERT . Note that   both model ‚Äôs Multi-75 K scores perform approxi-   mately similar to the STILTs methods , which is   expected given that they are the same size . TQA-   G and TQA - W come from the same dataset . As   stated in the body of this paper , no standard devi-   ation is reported in the MultiQA paper and thus it   is hard to know whether the difference in results   are statistically signiÔ¨Åcant . Even if all results were   statistically signiÔ¨Åcant , which is highly unlikely ,   each of the Multi-75 K models perform equal or   better on 2 of the 6 tasks , which is not statistically   different from random .   Combining All Tasks Our results using MTL   showed that although MTLis conceptually easy   ( just put all the datasets together ) it does not lead   to the best performance . We Ô¨Ånd similar results in   Wang et al . ( 2018a ) , where in their Table 3 they   show that the STILTs approach outperforms the280SQuAD NewsQA SearchQA TQA - G TQA - W HotpotQA   SQuAD - 33.3 39.2 49.2 34.5 17.8   NewsQA 59.6 - 41.6 44.2 33.9 16.5   SearchQA 57 31.4 - 57.5 39.6 19.2   TQA - G 57.7 31.8 49.5 - 41.4 19.1   TQA - W 57.6 31.7 44.4 50.7 - 17.2   HotpotQA 59.8 32.4 46.3 54.6 37.4 -   Multi-75 K 59.8 33.0 47.5 56.4 40.4 19.2   SQuAD - 41.2 47.8 55.2 45.4 20.8   NewsQA 72.1 - 47.4 55.9 45.2 20.6   SearchQA 70.2 40.2 - 57.3 45.5 20.4   TQA - G 69.9 41.2 50.0 - 46.2 20.8   TQA - W 71.0 39.2 48.4 55.7 - 20.9   HotpotQA 71.2 39.5 48.6 56.6 45.6 -   Multi-75 K 71.5 42.1 48.5 56.6 46.5 20.4   MTLapproach for all but one task . Additionally ,   in the follow up work from the initial STILTs paper   ( Phang et al . , 2020 ) they Ô¨Ånd that although MTL   has a slightly higher average performance in the   cross - lingual setting , it is worse than the pairwise   approach in 75 % of the evaluated tasks .   The current literature ( and our work ) seems to   suggest that naively combining as many tasks as   possible may not be the best approach . However ,   more work is needed to understand the training   dynamics of MTL .   Combining Helpful Tasks In this paper , we   only examine the difference between pairwise   MTL , STILTs or MTL , due to time and space .   Although it is possible that our heuristic may ex-   trapolate to transfer learning with more than two   tasks , computing the power set of the possible task   combinations for MTL and STILTs would be ex-   tremely time and resource intensive . We leave it to   future work to examine how the size heuristic may   hold when using more than two datasets at a time .   Additionally , there may be further value in com-   puting this power set : Changpinyo et al . ( 2018 )   showed that taking the pairwise tasks that proved   beneÔ¨Åcial in pairwise MTL and combining them   into a larger MTL set ( an ‚Äú Oracle " set ) oftentimes   provides higher scores than pairwise MTL . Explor-   ing which subsets of tasks provide the best transfer   with which method would be valuable future work . Dataset Size in TL Dataset size has been used   often in transfer learning techniques ( S√∏gaard and   Bingel , 2017 ; Pruksachatkun et al . , 2020a ; Poth   et al . , 2021 ) . Our size heuristic , although related ,   focuses on a different problem : whether to use   MTL or STILTs . Thus , our work provides addi-   tional insight into how the size of the dataset is   important for transfer learning .   Fine - tuning after MTL Many papers that use   MTLalso perform some sort of Ô¨Åne - tuning af-   ter the MTL phase . Since Ô¨Åne - tuning after MTL   makes the MTL phase an intermediate step , it essen-   tial combines the STILTs and MTL methods into a   single STILTs - like method . However , whether Ô¨Åne-   tuning after MTL is better than simply MTL is still   controversial : for example , Liu et al . ( 2019b ) , Raf-   fel et al . ( 2019 ) , and Talmor and Berant ( 2019 ) say   that Ô¨Åne - tuning after MTL helps but Lourie et al .   ( 2021 ) and Phang et al . ( 2018 ) say that it does n‚Äôt .   However , Raffel et al . ( 2019 ) is the only one whose   experiments include multiple random seeds , giving   more credence to their results . However , due to the   difference of opinion it is unclear which method is   actually better ; we leave this to future work .   F GLUE Dataset Sizes and References   To give credit to the original authors and to provide   the exact sizes , we provide Table 5.281282   Runxin Sun , Shizhu He , Chong Zhu , Yaohan He , Jinlong Li ,   Jun Zhaoand Kang LiuNational Laboratory of Pattern Recognition , Institute of Automation , CAS , Beijing , ChinaSchool of Artificial Intelligence , University of Chinese Academy of Sciences , Beijing , ChinaAI Lab , China Merchant Bank , ShenZhen , 518057 , ChinaBeijing Academy of Artificial Intelligence , Beijing , 100084 , China   sunrunxin2020@ia.ac.cn , { shizhu.he , chong.zhu}@nlpr.ia.ac.cn ,   { heyh18 , lucida}@cmbchina.com , { jzhao , kliu}@nlpr.ia.ac.cn   Abstract   1 Introduction   Text - to - SQL parsing is the task of mapping natu-   ral language questions to executable SQL queries   on relational databases ( Zhong et al . , 2017 ) . It   provides an easy way for common users unfamil-   iar with query languages to access large databases   and has attracted great attention . Recently , lexico-   logical alignments , which align question phrases   to their corresponding SQL query fragments , have   been proved to be very helpful in improving pars-   ing performance ( Shi et al . , 2020 ) . As shown in   Figure 1 , the token " competitor " should be aligned   to " c1 " in the SQL query . To capture such align-   ments , several attention - based models were pro-   posed ( Shi et al . , 2020 ; Lei et al . , 2020 ; Liu et al . ,   2021 ) , which employ the attention weights among   tokens to indicate the alignments . Specifically , they   use an attention module to perform schema linking   at the encoding stage ( Lei et al . , 2020 ; Liu et al . ,   2021 ) , and may use another attention to align each   output token to its corresponding input tokens at   the decoding stage ( Shi et al . , 2020 ) .   However , we argue that the attention mechanism   is not an appropriate way to capture and lever-   age lexico - logical alignments . It mainly has the   following two problems . First , the standard at-   tention can only model alignments at the token   level rather than the phrase level , while there are   many multi - granular , non - continuous alignments   in the text - to - SQL task . For the example in Fig-   ure 1 , " order by . . .limit 1 " is a SQL key-   word pattern representing a superlative operation .   However , the standard attention module can only   align " order " , " by " , " limit " , and " 1 " to " the   longest " token by token , rather than regarding them   as a whole . It may confuse the decoder and lead to   the failure to generate this pattern correctly ( Herzig   and Berant , 2021 ) . Second , traditional attention-   based approaches are prone to overfitting the train-   ing data , which is harmful to the model ‚Äôs general-   ization capability . It is not only the domain gener-   alization ( Dong et al . , 2019 ) but also the compo-   sitional generalization ( Herzig and Berant , 2021 ) .   Specifically , the former refers to the generalization   across different databases , while the latter refers to   the ability to generate new structures composed of   seen components .   To solve the aforementioned problems , we pro-   pose a neural parsing framework to leverage ex-   plicit lexico - logical alignments . Dong et al . ( 2019 )   have pointed out that if we align question tokens283to columns or values in databases before parsing ,   it will help to improve the model ‚Äôs generalization   among different domains ( databases ) . Motivated by   this , our framework consists of two steps . Specifi-   cally , we first implement a simple model to obtain   possible lexico - logical alignments before parsing .   While in the second step , we inject such alignments   into a standard seq2seq parser by treating them as   additional contexts , similar to " prompt informa-   tion " or " evidence " in machine reading comprehen-   sion ( Mihaylov and Frank , 2018 ; Tu et al . , 2020 ;   Niu et al . , 2020 ) . Moreover , to alleviate the neg-   ative effects on the parser caused by noise align-   ments , we propose a data augmentation method   that adds noisy alignments during the training pro-   cedure . Experimental results on an open - released   dataset , S ( Shi et al . , 2020 ) , show that our   framework achieves state - of - the - art performance   and obtains an absolute improvement of 3.4 % com-   pared with existing attention - based models .   2 Preliminaries   2.1 Problem Definition   Here we consider the problem setting adopted by   Shi et al . ( 2020 ) . Formally , given a natural lan-   guage question Qabout a table T , our goal is to   generate the corresponding SQL query Y , where   the table consists of columns { c , . . . , c } .   2.2 Base Parser   Our base parser is a standard seq2seq model . It gen-   erally follows the architecture proposed by Lin et al .   ( 2020 ) , which combines a BERT - based encoder   with a sequential pointer - generator to perform an   end - to - end parsing procedure .   Input Serialization and Encoder According to   the definition above , an input Xcontains a length-   nquestion Q = q , . . . , qand a table with m   columns T={c , . . . , c } . We concatenate all   the columns into a sequence for the table , where a   unique token precedes each column to represent its   type ( e.g. , text ) . Then we add two [ SEP ] tokens at   both ends and append this sequence to the question .   After adding a [ CLS ] token at the beginning , we   get the input sequence in the following format :   X=[CLS ] , Q,[SEP ] , [ TYPE#C1 ] , c ,   . . . , [ TYPE#Cm ] , c,[SEP ]   Xis encoded with BERT ( Devlin et al . , 2019 ) ,   followed by a bidirectional LSTM ( bi - LSTM ) toget the hidden representations h. Then for the   question part , we feed its representation to another   bi - LSTM to obtain the encoding result h. Each   column is represented by the vector of its corre-   sponding type token .   Decoder Like Lin et al . ( 2020 ) , We use an   LSTM - based pointer - generator ( See et al . , 2017 )   enhanced with the attention mechanism as the de-   coder . Specifically , we use the final hidden state of   the question encoder to initialize the decoder . At   each step t , the decoder chooses one of the follow-   ing three actions : generating a keyword from the   vocabulary V , copying a token from the question   Q , or copying a column from the table T.   3 Method   3.1 Framework Overview   As shown in Figure 2 , our framework consists of   two stages : lexico - logical alignment prediction   ( the upper left ) and alignment - enhanced parsing   ( the bottom ) . At the first stage ( alignment pre-   diction ) , we identify possible lexico - logical align-   ments in the question before parsing . At the sec-   ond stage ( alignment - enhanced parsing ) , we inject   these alignments into the parser so that it can make   further completions and refinements based on them .   3.2 Lexico - logical Alignment Prediction   In this step , we implement a simple model to pre-   dict lexico - logical alignments of the input question .   Specifically , we adopt a two - stage pipeline pro-   cess : 1 ) identify question phrases that may have   alignments ; 2 ) predict their corresponding query   fragments according to the types .   For the first stage , we classify the alignments   into three types according to their corresponding   query fragments : keyword , column , value . Specif-   ically , keyword alignments map question phrases   to query fragments composed of SQL keywords ,   while the other two types of alignments ( column   andvalue ) map them to columns in databases . The   only difference between column andvalue align-   ments is that the phrase part of a value alignment is   also a value in the SQL query . Analogous to Named   Entity Recognition ( NER ) , we use sequence label-   ing to implement this process :   P(label|Q , T ) = softmax(MLP ( [ h;c ] ) ) .   ( 1 )   Here we apply the BIO labeling schema , classify-   ing each token as one of the four types : keyword , 284   column , value , ornone . We adopt the same struc-   ture as our base parser to encode the input sequence ,   andhis the hidden representation of the i - th token .   Moreover , an attention module is used to get the   column - aware question representation c :   c= Attention ( h , h , h ) , ( 2 )   where hare the representations of all the columns .   Then we run a Multi - Layer Perceptron ( MLP ) by   concatenating these two vectors as inputs to predict   thei - th label .   For the second stage , we predict the query frag-   ment corresponding to the phrase . Specifically , we   can divide this process into the following two cases   according to the type of phrase :   1)Keyword : We use a generation model to   obtain keyword fragments corresponding to such   phrases . In detail , we perform self - attention on the   token representations of the phrase pto get the ini-   tial hidden state . Then we run an RNN model with   attention to generate its corresponding keyword   fragment :   y= arg maxYP(y|y , p ) . ( 3 )   2)Column & Value : In this case , we should link   the phrase to its corresponding column . Intuitively ,   based on the attention matrix , we can directly get   the column cthat best matches the phrase p :   c= arg maxf(p , c ) = arg maxXf(w , c ) .   ( 4 )   3.3 Alignment - enhanced Parsing   After getting all the lexico - logical alignments in   a question , we then consider adding them to theparsing process . Naturally , we design their usages   for both the encoding and decoding processes .   For the encoding stage , we treat alignments as   additional contexts and add them to the input se-   quence . Concretely , we represent each alignment   as a concatenation of the natural language phrase p   and its corresponding query fragment f , where the   two parts are separated by " : " . Moreover , a unique   token before each of them represents its type ( key-   word , column orvalue ) . Thus the format of the   modified input sequence is as follows :   X=[CLS ] , Q,[SEP ] , [ TYPE#C1 ] , c , . . . ,   [ TYPE#Cm ] , c,[SEP ] , [ TYPE#A1 ] ,   p , : , f , . . . , [ TYPE#An ] , p , : , f[SEP ]   In this way , the encoder based on a pre - trained lan-   guage model can make good use of this information   to help it better perform schema linking .   For the decoding stage , we also add alignments   to the generation process . Specifically , we take its   type token ‚Äôs hidden vector as the representation of   each alignment , denoting it as h. So at each step   t , we compute the attention between the decoder   hidden state hand the alignments :   a= Attention ( h , h , h ) . ( 5 )   Afterward , we use the concatenation of aand   the embedding of the previous token eas the de-   coder ‚Äôs input , injecting this information into the   next step ‚Äôs hidden state :   h= LSTM([e;a],h ) . ( 6 )   3.4 Noisy Alignment Augmentation   As mentioned before , it is impossible to obtain a   perfect model for alignment prediction . So if we   use the annotated alignments to train the parser , and285   use the predicted alignments to make predictions ,   then there is an inconsistency between training and   testing . It is precisely because of this inconsistency   that the parser tends to trust the given alignments   completely . In that case , wrong alignments may   hurt the parsing performance .   To alleviate the negative effects on the parser   caused by noise alignments , we propose a method   based on data augmentation , that is , adding noisy   alignments during the training procedure . Specifi-   cally , we use the model proposed in section 3.2   to predict alignments for the training examples   through cross - validation . Obviously , these align-   ments are noisy . Then we integrate these predicted   examples with the annotated examples and use   them as the augmented training set of the parser .   4 Experiments   4.1 Dataset and Experimental Setup   We evaluate on S ( Shi et al . , 2020 ) , a large-   scale dataset based on WT Q   ( Pasupat and Liang , 2015 ) . It contains 11,276 table-   question - answer triplets , enriched with human-   annotated logical forms and lexical - logical align-   ments . We use the default dataset split provided   by Shi et al . ( 2020 ) , where they randomly shuffle   the tables and divide them into five splits so that   examples with the same table are in the same split .   For evaluation metrics , we employ the average   logical form accuracy ACCand execution accu-   racy ACC , following Shi et al . ( 2020 ) . For   model implementation , please refer to Appendix A   for more details . It is worth noting that , unless   otherwise stated , we only use the alignment anno-   tations of the training set to train the alignment   prediction model . While on the dev / test set , we   use the predicted alignments as the parser ‚Äôs input .   4.2 End - to - end Parsing Performance   To evaluate the effectiveness of our model , we com-   pare end - to - end parsing performance with existing   attention - based models . The results are shown in   Table 1 . For the baselines , we select2and provided by Shi et al . ( 2020 ) . The former   uses the automatically derived exact - match features   to supervise the attention modules , while the latter   uses the alignment annotations instead .   From the results , we can observe that after com-   bining the alignment prediction model proposed in   section 3.2 , our parser ( ) achieves state - of - the-   art performance on S . We believe the rea-   son is that our approach identifies possible lexico-   logical alignments before parsing so that the parser   can leverage such explicit alignments and model   them on the phrase level . Moreover , " + noisy   alignment " further outperforms " " . It illustrates   that noise alignments do have negative effects on   the parser , while our noisy alignment augmentation   method can alleviate them effectively .   4.3 Our Model ‚Äôs Generalization Capability   To evaluate the advantages of our model ‚Äôs general-   ization capability , we further made different splits   ofS ( Shi et al . , 2020 ) and conducted ex-   periments on them . Here we evaluate the model ‚Äôs   generalization capability from two perspectives :   domain generalization andcompositional general-   ization . Specifically , DB split refers to the default286cross - DB setting of S , where databases ap-   pearing in the test set were not seen during training ,   and we use it to test the model ‚Äôs domain generaliza-   tion . Query split is the setting proposed by Finegan-   Dollak et al . ( 2018 ) to test the model ‚Äôs compo-   sitional generalization , where no query template   ( query after anonymization of database - related vari-   ables ) appears in more than one set . As for the IID   split , it means that the test case is not in the training   set while its corresponding database is seen during   training . We employ it as the control group .   The experimental results are shown in Table 2 .   From the results , we can observe that our approach   ( + noisy alignment ) obtains more significant im-   provement on the DB split and the query split ,   while it is not as effective as the attention - based ap-   proach on the IID split . In particular , our approach   achieves twice the improvement on the query split   ( 0.8 vs. 0.4 ) , even on a stronger base parser . These   reveal that our approach is more effective when   parsing across different databases ( domain gener-   alization ) and different query templates ( compo-   sitional generalization ) , which illustrates that our   approach has better generalization capability .   4.4 The Effectiveness of our Parser on   Leveraging Lexico - logical Alignments   To evaluate the effectiveness of our parser on the   lexico - logical alignments utilization , we conducted   experiments under the oracle setting , where we   used alignment annotations instead of predictions   for testing . Table 3 shows the results .   From the results , we can observe that 1 ) our   parser obtains more improvements when injecting   alignments ( + oracle alignment ) than the attention-   based approach ( + oracle attention ) . It proves that   our model could more effectively utilize the lexico-   logical alignment information . 2 ) We also show the   results when injecting different types of alignment   in our model . The results show that keyword align-   ment , which is excluded from traditional schema   linking , is a valuable type and is also helpful in   improving parsing performance . 3 ) Modeling such   alignments at the phrase - level is more effective   than the token - level ( " + oracle alignment " vs. " +   oracle alignment ( token)").5 Conclusion   In this paper , we propose a neural parsing frame-   work to leverage explicit lexico - logical alignments   by treating them as additional contexts . Moreover ,   to alleviate the negative effects on the parser caused   by noise alignments , we add noisy alignments dur-   ing training inspired by data augmentation . Exper-   imental results on S show that our frame-   work achieves state - of - the - art performance com-   pared with existing attention - based models .   Acknowledgments   This work is supported by the Natural Key R&D   Program of China ( No.2020AAA0106400 ) , the   National Natural Science Foundation of China   ( No.61922085 , No.61976211 ) and the Key Re-   search Program of the Chinese Academy of Sci-   ences ( Grant NO.ZDBS - SSW - JSC006 ) . This re-   search work was supported by the independent   research project of National Laboratory of Pat-   tern Recognition , the Youth Innovation Promo-   tion Association CAS and Yunnan Provincial Ma-   jor Science and Technology Special Plan Projects   ( No.202103AA080015 ) .   References287288A Model Implementation Details   Our model is implemented in PyTorch ( Paszke   et al . , 2019 ) . For the BERT model , we fine - tune a   bert - base - uncased model from the Hugging   Face ‚Äôs Transformers library ( Wolf et al . , 2020 ) .   For the attention module , we use the standard dot-   product attention function . We set all LSTMs to   1 - layer and hidden size to 256 . We use the Adam   optimizer ( Kingma and Ba , 2015 ) and clip gra-   dients to 2.0 . For the loss function , we choose   cross - entropy for the classification task and label-   smoothing for the generation task . We train our   alignment prediction model for up to 10 epochs   and SQL parser for 20 epochs . Both of them have   an epoch for warm - up , and then the learning rate   will decay linearly .   In terms of hyperparameter search , we turned   the batch size ( 8 , 16 , 32 ) , max learning rate ( 1e-3 ,   1e-4 ) , max BERT learning rate ( 5e-5 , 2e-5 , 1e-5 ,   5e-6 ) , and dropout ( 0.1 , 0.2 , 0.3 , 0.5 ) . Due to the   limitation of resources , we turned these parameters   one by one instead of using grid search . The bolded   values are a set of optimal parameters we found .   B Details of Different Splits of the   S Dataset   We made three different splits of the S   dataset : IID split , DB split , and query split , to   explore the corresponding generalization capabil-   ities of the model . It is worth noting that because   this dataset is a single - table dataset ( that is , each   DB contains only one table ) , the cross - DB setting   is essentially equal to the cross - table setting . The   specific methods for obtaining these splits are as   follows :   ‚Ä¢IID split : In order to ensure that tables in the   test set are also in the training set , and the   only difference between the two sets is that   the included samples are different , we classify   the samples according to their corresponding   tables . For each category ( i.e. , table ) , we ran-   domly select k(in this case , k= 1 ) samples ,   put them into the test set , and put the rest into   the training set .   ‚Ä¢DB split : This is the default setting of the   S dataset . Here we use the 0 - split pro-   vided by Shi et al . ( 2020 ) .   ‚Ä¢query split : Inspired by Finegan - Dollak et al .   ( 2018 ) , we substitute variables for table - related entities ( i.e. , columns and values ) in   each query in the dataset to obtain its corre-   sponding query template , just like Shi et al .   ( 2020 ) did . Similarly , we classify the sam-   ples according to their corresponding query   templates . For each category ( i.e. , query tem-   plate ) , all its samples can only be put into ei-   ther the training set or the test set . It is worth   noting that to examine the compositional gen-   eralization better , we sort the templates ac-   cording to their frequency . Then we put the   templates with higher frequency into the train-   ing set and the templates with lower frequency   into the test set .   For the above three splits , we make the ratio of   the training set and the test set approximately equal   to 4:1 , consistent with Shi et al . ( 2020 ) .   C Details on Obtaining Token - level   Alignments   To verify whether our approach can model align-   ments at the phrase level , we constructed token-   level alignments to contrast with the original align-   ment annotations . Specifically , we imitated the   attention mechanism and decomposed the align-   ments according to their types .   Forkeyword alignments , inspired by Shi et al .   ( 2020 ) , we align each keyword in the SQL query   to all its corresponding tokens in the question . For   the example in Figure 1 , we align " order " , " by " ,   " limit " , and " 1 " to " the longest " respectively .   Then we obtain the following four alignments : " the   longest : order " , " the longest : by " , " the longest :   limit " , and " the longest : 1 " .   For the other two types of alignments : column   andvalue , as mentioned in section 3.2 , they both   align question phrases to columns in databases .   Analogous to schema linking through an attention   module , we align each token in the question phrase   to the corresponding column separately . For the   example in Figure 1 , we align " united " and " states "   to column c2respectively instead of treating these   two tokens as a whole.289   Zixuan Li , Saiping Guan , Xiaolong Jin , Weihua Peng , Yajuan Lyu ,   Yong Zhu , Long Bai , Wei Li , Jiafeng Guo , Xueqi ChengSchool of Computer Science and Technology , University of Chinese Academy of Sciences;CAS Key Laboratory of Network Data Science and Technology ,   Institute of Computing Technology , Chinese Academy of Sciences;Baidu Inc.   { lizixuan,guansaiping,jinxiaolong}@ict.ac.cn   { pengweihua,lvyajuan,zhuyong}@baidu.com   Abstract   1 Introduction   Temporal Knowledge Graph ( TKG ) ( Boschee et al . ,   2015 ; Gottschalk and Demidova , 2018 , 2019 ; Zhao ,   2020 ) has emerged as a very active research area   over the last few years . Each fact in TKGs is a   quadruple ( subject , relation , object , timestamp ) . A   TKG can be denoted as a sequence of KGs with   timestamps , each of which contains all facts at the   corresponding timestamp . TKG reasoning aims to   answer queries about future facts , such as ( COVID-   19 , New medical case occur , ? , 2022 - 1 - 9 ) .   To predict future facts , one challenge is to dive   deep into the related historical facts , which reÔ¨Çectthe preferences of the related entities and affect   their future behaviors to a certain degree . Such   facts , usually temporally adjacent , may carry in-   formative sequential patterns , called evolutional   patterns in this paper . For example , [ ( COVID-19 ,   Infect , A , 2021 - 12 - 21 ) , ( A , Discuss with , B , 2021-   12 - 25 ) , ( B , Go to , Shop , 2021 - 12 - 28 ) ] is an informa-   tive evolutional pattern for the above query implied   in historical KGs . There are two kinds of models to   model evolutional patterns , namely , query - speciÔ¨Åc   and entire graph based models . The Ô¨Årst kind of   models ( Jin et al . , 2020 ; Li et al . , 2021a ; Sun et al . ,   2021 ; Han et al . , 2020a , 2021 ; Zhu et al . , 2021 )   extract useful structures ( i.e. , paths or subgraphs )   for each individual query from the historical KG   sequence and further predict the future facts by min-   ing evolutional patterns from these structures . This   kind of models may inevitably neglect some useful   evolutional patterns . Therefore , the entire graph   based models ( Deng et al . , 2020 ; Li et al . , 2021a )   take a sequence of entire KGs as the input and   encode evolutional patterns among them , which   exhibit superiority to the query - speciÔ¨Åc models .   However , they all ignore the length - diversity and   time - variability of evolutional patterns . Length-   diversity : The lengths of evolutional patterns are   diverse . For example , [ ( COVID-19 , Infect , A , 2021-   12 - 21 ) , ( A , Discuss with , B , 2021 - 12 - 25 ) , ( B , Go to ,   Shop , 2021 - 12 - 28 ) ] is a useful evolutional pattern   of length 3 to predict the query ( COVID-19 , New   medical case occur , ? , 2022 - 1 - 9 ) and [ ( COVID-19 ,   Infect , A , 2021 - 12 - 21 ) , ( A , Go to , Shop , 2021 - 12-   30 ) ] is also a useful evolutional pattern of length   2 for this query . Previous models extract evolu-   tional patterns of a Ô¨Åxed length , which can not han-   dle evolutional patterns of diverse lengths . Time-   variability : Evolutional patterns change over time .   For example , ( COVID-19 , Infect , A , 2019 - 12 - 9 )   and(COVID-19 , Infect , A , 2022 - 1 - 9 ) may lead   to different results due to the wide usage of the   COVID-19 vaccines . Previous models learn from290the historical training data , which fail in model-   ing the time - variability of evolutional patterns after   that .   Upon the above observations , we propose Com-   plex Evolutional Network ( CEN ) to deal with   the above two challenges . For length - diversity ,   CEN learns evolutional patterns from historical KG   sequences of different lengths via an Relational   Graph Neural Network ( RGCN ) based KG se-   quence encoder and a length - aware Convolutional   Neural Network ( CNN ) based evolutional represen-   tation decoder . Besides , the model is trained via an   easy - to - difÔ¨Åcult curriculum learning strategy incre-   mentally according to the length of KG sequences .   For time - variability , we learn CEN under an online   setting and combine CEN with a temporal regular-   ization unit to alleviate the catastrophic forgetting   problem ( Mccloskey and Cohen , 1989 ) .   In general , this paper makes the following con-   tributions :   ‚Ä¢We address , for the Ô¨Årst time , the problems of   length - diversity and time - variability of evolu-   tional patterns for TKG reasoning .   ‚Ä¢For length - diversity , we propose a length-   aware CNN to learn evolutional patterns with   different lengths in a curriculum learning man-   ner . For time - variability , we propose to learn   the model under an online setting to adapt to   the changes of evolutional patterns .   ‚Ä¢Experiments demonstrate that the proposed   CEN model achieves better performance on   TKG reasoning under both the traditional of-   Ô¨Çine and the proposed online settings .   2 Related Work   The TKG reasoning task primarily has two settings ,   interpolation and extrapolation . This paper focus   on the extrapolation setting . In what follows , we   will introduce related work on both settings :   TKG Reasoning under the interpolation set-   ting . This setting aims to complete the missing   facts at past timestamps ( Jiang et al . , 2016 ; Leblay   and Chekol , 2018 ; Dasgupta et al . , 2018 ; Garcia-   Duran et al . , 2018 ; Goel et al . , 2020 ; Wu et al . ,   2020 ) . For example , TTransE ( Leblay and Chekol ,   2018 ) extends TransE ( Bordes et al . , 2013 ) by   adding the temporal constraints ; HyTE ( Dasgupta   et al . , 2018 ) projects the entities and relations to   time - aware hyperplanes to generate representationsfor different timestamps . Above all , they can not   obtain the representations of the unseen timestamps   and are not suitable for the extrapolation setting .   TKG Reasoning under the extrapolation set-   ting This setting aims to predict facts at future   timestamps , which can be categorized into two   groups : query - speciÔ¨Åc and entire graph based mod-   els . Query - speciÔ¨Åc models focus on modeling the   query - speciÔ¨Åc history . For example , RE - NET ( Jin   et al . , 2020 ) captures the evolutional patterns im-   plied in the subgraph sequences of a Ô¨Åxed length   speciÔ¨Åc to the query . CyGNet ( Zhu et al . , 2021 )   captures repetitive patterns by modeling repetitive   facts . xERTE ( Han et al . , 2020a ) learns to Ô¨Ånd   the query - related subgraphs of a Ô¨Åxed hop num-   ber . CluSTeR ( Li et al . , 2021a ) and TITer ( Sun   et al . , 2021 ) both adopt reinforcement learning to   discover evolutional patterns in query - related paths   of a Ô¨Åxed length . Unlike the query - speciÔ¨Åc models ,   entire graph based models encode the latest histor-   ical KG sequence of a Ô¨Åxed - length . RE - GCN ( Li   et al . , 2021b ) captures the evolutional patterns into   the representations of all the entities by model-   ing KG sequence of a Ô¨Åxed - length at lastest a few   timestamps . Glean ( Deng et al . , 2020 ) introduces   event descriptions to enrich the information of the   entities .   3 Problem Formulation   A TKG G = fG ; G ; : : : ; G ; : : : g , where G=   ( V;R;E ) , is a directed multi - relational graph . V   is the set of entities , Ris the set of relations ,   andEis the set of facts at timestamp t. The   TKG reasoning task aims to answer queries like   ( s ; r ; ? ; t)or ( ? ; r ; o ; t)with the historical KG se-   quencefG ; G ; : : : ; Gggiven , where s ; o2V ,   r2 R andtare the subject / object entity , the   relation and the query timestamp , respectively . Fol-   lowing Jin et al . ( 2020 ) , KGs from timestamps   1toT , TtoT , TtoT(T < T < T ) are   used as the training , validation and test sets , respec-   tively . Under the traditional ofÔ¨Çine setting , models   are trained only using the training set ( tT ) ,   while under the online setting , the model will be   updated by KGs before t(T < tT ) contin-   ually . Without loss of generality , we describe our   model as predicting the missing object entity .   4 Methodology   We propose CEN to deal with the length - diversity   and time - variability challenges of evolutional pat-291   tern learning for TKG reasoning . SpeciÔ¨Åcally , CEN   consists of a basic model as well as a curriculum   learning strategy for the former challenge and an   online learning strategy for the latter challenge .   4.1 Basic CEN Model   As shown in Figure 1 , the basic model of CEN con-   tains a KG sequence encoder and an evolutional   representation decoder . The KG sequence encoder   encodes the latest historical KG sequences of differ-   ent lengths to corresponding evolutional represen-   tations of entities . Then , the evolutional represen-   tation decoder calculates the scores of all entities   for the query based on these representations .   KG Sequence Encoder . Its inputs include the   lastest historical KG sequences of lengths from 1   toK , initial representations of entities H2R   and relation representations R2R , where   dis the dimension of the representations . Take   the KG sequence of length k= 2 for example ,   for each KG in the input sequence fG ; Gg ,   it iteratively calculates the evolutional represen-   tations of entities Hat the corresponding times-   tamps t2ft 1 ; tgas follows :   ^H = RGCN ( H;R ; G ) ; ( 1 )   H = SC(^H;H ) ; ( 2 )   where RGCN ( )andSCdenote the shared RGCN   layer and the skip connection unit proposed in   RE - GCN ( Li et al . , 2021b ) . For the initial times-   tamp t 1,His set to H.Ris shared   across timestamps , which is different from RE-   GCN . By reusing the encoder for KG sequences   of different lengths , we obtain Kentity evo-   lution representations at the query timestamp :   fH ; : : : ; H ; : : : ; Hg .   Evolutional Representation Decoder . Multiple   evolutional representations contain evolutional pat-   terns of multiple lengths . To distinguish the inÔ¨Çu-   ences of the length - diverse evolutional patterns , we   design a length - aware CNN , which uses Kseparate   channels to model the above Kevolutional repre-   sentations . SpeciÔ¨Åcally , for a query ( s ; r ; ? ; t ) , the   representations of s(s ; : : : ; s ; : : : ; s ) and r(r )   are looked up from multiple representations of enti-   tiesfH ; : : : ; H ; : : : ; Hgand the shared relation   representations R. For historical KG sequence of   length k , kchannel with Cdifferent kernels of   size2Mis used to decode the concatenation   ofsandr . SpeciÔ¨Åcally , the feature maps are   calculated as below ,   m(s ; r ; t ) = Conv(w;[s;r]);(3 )   where Convdenotes the 2D convolution op-   eration , w(0c < C ) are the trainable   parameters in ckernel of kchannel and   m(s ; r ; t)2R. After that , it concatenates   the output vectors from Ckernels yielding a vector :   m(s ; r ; t)2R. ForKchannels , it outputs a   list of vectors : [ m(s ; r ; t ) , ... , m(s ; r ; t ) , ... ,   m(s ; r ; t ) ] . Then , each vector is fed into a   shared 1 - layer Fully Connected Network ( FCN )   withW2Ras its parameters and the Ô¨Å-   nal score of a candidate entity ois the sum of   the logits from multiple evoltional representations : Pm(s ; r ; t)Wo , where ois the evolu-   tional representation of length kforo . Then we   seen it as a multi - class learning problem and use   the cross - entropy as its objective function .   4.2 Curriculum Learning for Length - diversity   Longer historical KG sequences contain more his-   torical facts and longer evolutional patterns , which   is more challenging to learn . Similar to human   learning procedures , the models can beneÔ¨Åt from   an easy - to - difÔ¨Åcult curriculum . Besides , how to292   choose the maximum length of evolutional patterns   is vital to CEN . Thus , we design the curriculum   learning strategy to learn the length - diverse evolu-   tional patterns from short to long and adaptively   select the optimal maximum length ^K. As shown   at the top of Figure 2 , we start from the minimum   length ^k(^k= 1for example ) and gradually move   on to longer history in the training set . The model   stops the curriculum and gets the optimal ^Kwhen   the MRR metric decreases or the length is up to   maximum length K. Note that , curriculum learn-   ing is conducted under the traditional ofÔ¨Çine setting   andModelis used as the pre - trained model for   online learning .   4.3 Online Learning for Time - variability   To handle the time - variability of evolutional pat-   terns , one simple and direct method is to update   the model according to the newly occurred facts .   Thus , as shown in the bottom of Figure 2 , for times-   tamp t+ 1(T < t+ 1 < T),Modelis Ô¨Åne-   tuned to get Modelby predicting the facts in   the KG at the last timestamp Gwith historical KG   sequences as inputs . Furthermore , to balance the   knowledge of new evolutional patterns and the ex-   isting ones , we use a Temporal Regularization unit   ( TR unit ) ( Daruna et al . , 2021 ; Wu et al . , 2021 ) . We   apply an L2regularization constraint between two   temporally adjacent models to smooth the drastic   change of the parameters .   4.4 Analysis on Computational Complexity   We analyze the computational complexity of CEN .   We view the computational complexities of the   RGCN unit and ConvTransE as constants . Then ,   the time complexity of the RGCN at a timestamp   tisO(jEj ) , wherejEjis the maximum number   of facts at timestamps in history . As we unroll m   ( m=^K ^k)sequences , the time complexity of the   KG sequence encoder is Ô¨Ånally O(mjEj ) . Thus ,   the time complexity of CEN is O(mjEj+m).5 Experiments   Experimental Setup . We adopt three widely-   used datasets , ICEWS14 ( Li et al . , 2021b ) ,   ICEWS18 ( Jin et al . , 2020 ) , and WIKI ( Leblay   and Chekol , 2018 ) to evaluate CEN . Dataset statis-   tics are demonstrated in Table 1 . Due to the   space limitation , the CEN model is only com-   pared with the latest models of TKG reasoning :   CyGNet ( Zhu et al . , 2021 ) , RE - NET ( Jin et al . ,   2020 ) , xERTE ( Han et al . , 2020a ) , TG - Tucker ( Han   et al . , 2021 ) , TG - DistMult ( Han et al . , 2021 ) ,   TiTer ( Sun et al . , 2021 ) and RE - GCN ( Li et al . ,   2021b ) . In the experiments , we adopt MRR ( Mean   Reciprocal Rank ) and Hits@{1,3,10 } as the met-   rics for TKG reasoning . We averaged the met-   rics over Ô¨Åve runs . Note that , following Han   et al . ( 2020b ) , we adopt an improved Ô¨Åltered set-   ting where the timestamps of facts are considered ,   called time - aware Ô¨Åltered setting . Take a typi-   cal query ( s ; r ; ? ; t)with answer oin the test   set for example , and assume there is another two   facts(s ; r ; o ; t)and(s ; r ; o ; t ) . Under this time-   aware Ô¨Åltered setting , only owill be considered   as a correct answer and thus removed from the   ranking list of candidate answers .   Implementation Details . In the experiments ,   the optimal minimum lengths of evolutional pat-   terns ^kfor ICEWS14 , ICEWS18 , WIKI are 3 , 3 ,   2 , respectively . The maximum length Kfor all   datasets is set to 10 . For all datasets , the kernel   width Mis set to 3 , and Cis set to 50 . For each fact   ( s ; r ; o ; t ) in the test set , we evaluate CEN on two   queries ( s ; r ; ? ; t)and ( ? ; r ; o ; t ) . The dimension   dof relation representations and entity representa-   tions is set to 200 on all datasets . Adam ( Kingma   and Ba , 2014 ) is adopted for parameter learning   with the learning rate of 0.001 on all datasets . The   number of RGCN layers is set to 2 and the dropout   rate for each layer to 0.2 . For the online setting , we   set the max epochs of the Ô¨Åne - tuning at each times-   tamp to 30 . For predicting G , Gis used as the   validation set . We Ô¨Åne tune the pre - trained CEN   fromT1 + 1 toTand report the results at the test   timestamps ( TtoT ) in Table 3 . The experiments   are carried out on Tesla V100 . Codes are avaliable   at https://github.com/Lee-zix/CEN .   5.1 Experimental Results   Results under the OfÔ¨Çine Setting . The results   under the traditional ofÔ¨Çine setting are presented   in Table 2 . CEN consistently outperforms the293   baselines on MRR , Hits@3 , and Hits@10 on all   datasets , which justiÔ¨Åes the effectiveness of mod-   eling the evolutional patterns of different lengths .   On ICEWS datasets , CEN underperforms TITer on   Hits@1 because TITer retrieves the answer through   explicit paths , which usually gets high Hits@1 .   Whereas , CEN recalls more answer entities by ag-   gregating the information from multiple evolutional   patterns , which may be the reason for its high per-   formance on Hits@3 and Hits@10 .   Results under the Online Setting . Under the   online setting , the model is updated via historical   facts at the testset . Thus , it can not be directly com-   pared with the baselines designed for the ofÔ¨Çine   setting . As shown in Table 3 , on ICEWS datasets   CEN outperforms CEN(-TR ) ( CEN without TR   unit ) , which implies the effectiveness of TR unit to   balance the knowledge of new evolutional patterns   and the existing ones . On WIKI , CEN(-TR ) gets   better performance . It is because that the time in-   terval between two adjacent timestamps in WIKI   ( one year ) is much larger than ICEWS datasets ( one   day ) and contains more time - variable evolutional   patterns . TR unit limits the model to adapt to new   knowledge and is not suitable for this dataset .   Ablation Study . To investigate the contribu-   tions of curriculum learning strategy and the length-   aware CNN , we conduct ablation studies for CENon the test set of ICEWS14 under the traditional   ofÔ¨Çine setting , which are shown in Table 4 . CEN(-   CL ) denotes CEN without the curriculum learn-   ing strategy . The underperformance of CEN(-CL )   demonstrates the effectiveness of the curriculum   learning strategy . CEN(-LA ) denotes the model   replacing the length - aware CNN with a traditional   CNN . The underperformance of CEN(-LA ) implies   the effectiveness of the length - aware CNN .   6 Conclusions   In this paper , we proposed Complex Evolutional   Network ( CEN ) for TKG reasoning , which deals   with two challenges in modeling the complex   evolutional patterns : length - diversity and time-   variability . For length - diversity , CEN adopts a   length - aware CNN to learn evolutional patterns of   different lengths and is trained under a curriculum   learning strategy . For time - variability , we explored   a new online setting , where the model is expected   to be updated to new evolutional patterns emerging   over time . Experimental results demonstrate the   superiority of the proposed model under both the   ofÔ¨Çine and the online settings .   Acknowledgments   The work is supported by the National Natural Sci-   ence Foundation of China under grants U1911401 ,   62002341 and 61772501 , the GFKJ Innovation Pro-   gram , Beijing Academy of ArtiÔ¨Åcial Intelligence   under grant BAAI2019ZD0306 , and the Lenovo-   CAS Joint Lab Youth Scientist Project.294References295296   Takyoung Kim , Hoonsang Yoon , Yukyung Lee , Pilsung Kang , Misuk KimKorea University , Seoul , Republic of KoreaSejong University , Seoul , Republic of Korea   Abstract   1 Introduction   The dialogue state tracking ( DST ) module struc-   tures the belief state that appears during the conver-   sation in the form of domain - slot - value , to   provide an appropriate response to the user . Re-   cently , multi - turn DST datasets have been con-   structed using the Wizard - of - Oz method to reflect   more realistic dialogue situations ( Wen et al . , 2017 ;   Mrk≈°i ¬¥ c et al . , 2017 ; Budzianowski et al . , 2018 ) .   The characteristic of these datasets is that belief   states are ‚Äú accumulated ‚Äù and recorded every turn .   That is , the belief states of the previous turns are   included in the current turn . It confirms whether   the DST model tracks essential information that   has appeared up to the present point .   Joint goal accuracy and slot accuracy are utilized   in most cases to evaluate the prediction of accumu - lated belief states . Joint goal accuracy strictly de-   termines whether every predicted state is identical   to the gold state , whereas slot accuracy measures   the ratio of correct predictions . However , we de-   termined that these two metrics solely focus on   ‚Äú penalizing states that fail to predict , ‚Äù not consider-   ing ‚Äú reward for well - predicted states . ‚Äù Accordingly ,   as also pointed out in Rastogi et al . ( 2020a ) , joint   goal accuracy underestimates the model prediction   because of its error accumulation attribute , while   slot accuracy overestimates it because of its depen-   dency on predefined slots .   However , there is a lack of discussion on the met-   ric for evaluating the most used MultiWOZ dataset ,   despite a recently published dataset ( Rastogi et al . ,   2020b ) proposing some metrics . To address the   above challenge , we propose reporting the relative   slot accuracy along with the existing metrics in   MultiWOZ dataset . While slot accuracy has the   challenge of overestimation by always considering   all predefined slots in every turn , relative slot ac-   curacy does not depend on predefined slots , and   calculates a score that is affected solely by slots   that appear in the current dialogue . Therefore , rel-   ative slot accuracy enables a realistic evaluation   by rewarding the model ‚Äôs correct predictions , a   complementary approach that joint goal and slot   accuracies can not fully cover . It is expected that the   proposed metric can be adopted to evaluate model   performance more intuitively .   2 Current Evaluation Metrics   2.1 Joint Goal Accuracy   Joint goal accuracy , developed from Henderson   et al . ( 2014b ) and Zhong et al . ( 2018 ) , can be said   to be an ideal metric , in that it verifies that the pre-   dicted belief states perfectly match the gold label .   Equation 1 expresses how to calculate the joint   goal accuracy , depending on whether the slot val-   ues match each turn.297   JGA =   1if predicted state = gold state   0otherwise   ( 1 )   However , the joint goal accuracy underestimates   the accumulated states because it scores the per-   formances of later turn to zero if the model mis-   predicts even once in a particular turn , regardless   of the model prediction quality at later turns . As   illustrated in Figure 1 , we measured the relative   position of the turn causing this phenomenon for   the dialogue . We used MultiWOZ 2.1 ( Eric et al . ,   2019 ) , and analyzed 642 samples from a total of   999 test sets in which the joint goal accuracy of   the last turn is zero . The DST model selected for   primary verification is the SOM - DST ( Kim et al . ,   2020 ) , which is one of the latest DST models . Ac-   cordingly , the relative position where joint goal   accuracy first became zero was mainly at the be-   ginning of the dialogue . This means that the joint   goal accuracy after the beginning of the dialogue   is unconditionally measured as zero because of the   initial misprediction , although the model may cor-   rectly predict new belief states at later turns . Failure   to measure the performance of the latter part means   that it can not consider various dialogue situations   provided in the dataset , which is a critical issue in   building a realistic DST model .   2.2 Slot Accuracy   Slot accuracy can compensate for situations where   joint goal accuracy does not fully evaluate the dia-   logue situation . Equation 2 expresses how to calcu-   late the slot accuracy . Tindicates the total number   of predefined slots for all the domains . Mdenotes   the number of missed slots that the model does not   accurately predict among the slots included in the   gold state , and Wdenotes the number of wrongly   predicted slots among the slots that do not exist in   the gold state .   SA = T‚àíM‚àíW   T(2 )   Figure 2 illustrates the total number of annotated   slots in MultiWOZ 2.1 to figure out the limitation   of slot accuracy . Each value of x - axis in Figure 2 in-   dicates the ‚Äú maximum ‚Äù number of slots that appear   in a single dialogue , and we confirmed that approx-   imately 85 % of the test set utilized solely less than   12 of the 30 predefined slots in the experiment . Be-   cause the number of belief states appearing in the   early and middle turns of the dialogue are smaller ,   and even fewer states make false predictions , cal-   culating slot accuracy using Equation 2 reduces   the influence of MandW , and the final score is   dominated by the total slot number T. Accordingly ,   several previous studies still report the model per-   formance using solely joint goal accuracy because   slot accuracy excessively depends on the number of   predefined slots , making the performance deviation   among models trivial ( refer to Table A5 ) .   Furthermore , according to Table A6 , we deter-   mined that slot accuracy tends to be too high . The   slot accuracies of turns 0 and 1 show approxi-   mately 96 % accuracy , despite the model not cor-298Type ModelJoint Slot F1 Relative   Goal Acc . Acc . Score Slot Acc .   Open   vocabularyTransformer - DST ( 2021 ) 0.5446 0.9748 0.9229 0.8759   TripPy ( 2020 ) 0.6131 0.9707 0.8573 0.8432   SOM - DST ( 2020 ) 0.5242 0.9735 0.9179 0.8695   Simple - TOD ( 2020 ) 0.5605 0.9761 0.9276 0.8797   SA VN ( 2020 ) 0.5357 0.9749 0.9246 0.8769   TRADE ( 2019 ) 0.4939 0.9700 0.9033 0.8520   COMER ( 2019 ) 0.4879 0.9652 0.8800 0.8250   Ontology   basedDST - STAR ( 2021 ) 0.5483 0.9754 0.9253 0.8780   L4P4K2 - DSGraph ( 2021 ) 0.5178 0.9690 0.9189 0.8570   SUMBT ( 2019 ) 0.4699 0.9666 0.8934 0.8380   rectly predicting states at all . It becomes difficult   to compare various models in detail , if each model   shows a high performance , even though nothing   is adequately predicted . In addition , as the turn   progresses , there are no rewards for a situation in   which the model tracks the belief state without any   challenges . The case correctly predicting two out   of three in turn 4 , and the case correctly predicting   three out of four in turn 5 exhibit the same slot   accuracy . Therefore , the slot accuracy measured   according to Equation 2 differs from our intuition .   2.3 Other Metric   Recently , Rastogi et al . ( 2020b ) proposed a met-   ric called average goal accuracy . The main differ-   ence between the average goal accuracy and the   proposed relative slot accuracy is that the aver-   age goal accuracy only considers the slots with   non - empty values in the gold states of each turn ,   whereas the proposed relative slot accuracy consid-   ers those in both gold and predicted states . Since   average goal accuracy ignores the predicted states ,   it can not properly distinguish a better model from   a worse model in some specific situations . We will   discuss it in more detail in Section 4.1 .   3 Relative Slot Accuracy   As can be observed in Equation 2 , slot accuracy has   the characteristic that the larger the number of pre-   defined slots ( T ) , the smaller the deviation between   the prediction results . The deviation among DST   models will be even more minor when constructing   datasets with various dialogue situations , because   the number of predefined slots will continually in - crease . It is not presumed to be an appropriate met-   ric in terms of scalability .   Therefore , we propose relative slot accuracy , that   is not affected by predefined slots , and is evaluated   with adequate rewards and penalties that fit human   intuition in every turn . Equation 3 expresses how   to calculate the relative slot accuracy , and Tde-   notes the number of unique slots appearing in the   predicted and gold states in a particular turn .   RSA = T‚àíM‚àíW   T , where 0ifT= 0 ( 3 )   Relative slot accuracy rewards well - predicted be-   lief states by measuring the scores in accumulating   turns . Further discussions on the relative score will   be discussed in Section 4.1 .   4 Experiments   We measured MultiWOZ 2.1 , an improved ver-   sion of MultiWOZ 2.0 ( Budzianowski et al . , 2018 ) ,   which has been adopted in several studies , accord-   ing to Table A5 . Five domains ( i.e. , hotel , train ,   restaurant , attraction , andtaxi ) are adopted in the   experiment , following Wu et al . ( 2019 ) , and there   are a total of 30 domain - slot pairs . We selected   the DST models in Table A5 that perform the Mul-   tiWOZ experiment with the original authors ‚Äô re-   producible code . Additionally , we reported the F1   score , which can be calculated using the current   predicted and gold states.299   4.1 Results and Discussion   Table 1 presents the overall results . Regarding slot   accuracy , the difference between the largest and   smallest values is solely 1.09 % . It can be one of   the reasons that several researchers do not report   it . Meanwhile , relative slot accuracy can explicitly   highlight the deviation among models by showing a   5.47 % difference between the largest and smallest   values . Furthermore , the correlation with joint goal   accuracy , a mainly adopted metric , and relative slot   accuracy with respect to each turn is lower than   the correlation with joint goal accuracy and slot   accuracy , as illustrated in Figure 3 . Specifically , it   can be compared with a different perspective when   using the proposed reward - considering evaluation   metric .   Domain - specific Evaluation We reported the   joint goal , slot , and relative slot accuracies per   domain utilizing the SOM - DST model in Table   2 . Relative slot accuracy derives a specific score   in the turn configuration and prediction ratio of   each domain by excluding slots that do not appear   in the conversation . For example , the taxidomain   shows a low score , meaning that it has relatively   several cases of incorrect predictions , compared   to the number of times slots belonging to the taxi   domain appear . Because slot accuracy can not distin-   guish the above trend , the score of the hotel domain   is lower than that of the taxidomain . In summary ,   relative slot accuracy enables relative comparison   according to the distribution of the domain in a   dialogue . DomainJoint Slot Relative   Goal Acc . Acc . Slot Acc .   hotel 0.4923 0.9731 0.8493   train 0.7162 0.9874 0.9176   restaurant 0.6589 0.9858 0.8977   attraction 0.6811 0.9878 0.8421   taxi 0.5701 0.9798 0.7828   Dependency on Predefined Slots As discussed   in Section 2.2 , slot accuracy requiring total prede-   fined slots is not a scalable method for evaluating   the current dialogue dataset that contains a few   domains in each dialogue . For example , when eval-   uating a dialogue sample that solely deals with the   restaurant domain , even domains that never ap-   pear at all ( i.e. , hotel , train , attraction , andtaxi )   are involved in measuring performance , making de-   viations among different models trivial . However ,   relative slot accuracy can evaluate the model ‚Äôs pre-   dictive score without being affected by slots never   seen in the current dialogue , which is a more realis-   tic way , considering that each dialogue contains its   own turn and slot composition . Figure 4 illustrates   the mean and standard deviations of the model per-   formance in Table 1 . As can be observed from the   results , the relative slot accuracy has a higher de-   viation than the slot accuracy , enabling a detailed   comparison among the methodologies .   Reward on Relative Dialogue Turn Relative   slot accuracy is able to reward the model ‚Äôs correct   prediction by measuring the accuracy on a relative   basis for each turn . Table A6 compares the slot and   relative slot accuracies . The relative slot accuracy   from turns 0 ‚Äì 3 is measured as 0 because it cal-300Type Belief StateJoint Average Relative   Goal Acc . Goal Acc . Slot Acc .   Gold Staterestaurant - area - centre   - - - restaurant - food - indian   restaurant - people-2   Prediction   of Model Arestaurant - area - centre   0 0.3333 0.2500 restaurant - food - chinese   attraction - area - centre   Prediction   of Model Brestaurant - area - centre   0 0.3333 0.1667restaurant - food - chinese   restaurant - name - nusha   attraction - area - centre   attraction - pricerange - cheap   culates the score based on the unique state of the   current turn according to Equation 3 . In addition ,   regarding slot accuracy in turns 4 , 5 , and 6 , there   is no score improvement for the additional well-   predicted state by the model , whereas the score   increases when the newly added state is matched   in the case of relative slot accuracy . Therefore , rel-   ative slot accuracy can provide an intuitive evalu-   ation reflecting the current belief state recording   method , in which the number of slots accumulates   incrementally as the conversation progresses .   Comparison to Average Goal Accuracy Rel-   ative slot accuracy can compare DST model per-   formances more properly than average goal accu-   racy , as mentioned in Section 2.3 . Table 3 describes   how these two metrics result in different values for   the same model predictions . In this example , aver-   age goal accuracy can not consider additional belief   states incorrectly predicted by Model B , result-   ing in the same score between the two models . In   contrast , relative slot accuracy can give a penalty   proportional to the number of wrong predictions   because it includes both gold and predicted states   when calculating the score . Consequently , relative   slot accuracy has a more elaborated discriminative   power than the average goal accuracy .   5 Conclusion   This paper points out the challenge that the existing   joint goal and slot accuracies can not fully evaluate   the accumulating belief state of each turn in the   MultiWOZ dataset . Accordingly , the relative slot   accuracy is proposed . This metric is not affected   by unseen slots in the current dialogue situation ,   and compensates for the model ‚Äôs correct predic - tion . When the DST task is scaled up to deal with   more diverse conversational situations , a realistic   model evaluation will be possible using relative slot   accuracy . Moreover , we suggest reporting various   evaluation metrics to complement the limitations of   each metric in future studies , not solely reporting   the joint goal accuracy .   Acknowledgement   This work was supported by Institute of Informa-   tion & communications Technology Planning &   Evaluation ( IITP ) grant funded by the Korea gov-   ernment ( MSIT ) ( No . 2021 - 0 - 00034 , Clustering   technologies of fragmented data for time - based   data analysis ) and Ministry of Culture , Sports   and Tourism and Korea Creative Content Agency   ( Project Number : R2020040126 - 0001 )   References301302A Complementary discussions of joint   goal accuracy   Our findings show that if the model makes an incor-   rect prediction , the error accumulates until the end   of the dialogue , and the joint goal accuracy remains   at zero . In this section , we discuss a few cases of   59 dialogues that do not show the trend among 642   dialogues selected in Section 2.1 ; however , it is im-   portant to note that these few cases have negligible   effect on the trend in Figure 1 , solely changing the   position where the joint goal accuracy first becomes   zero .   We sampled dialogues of the MultiWOZ 2.1 test   set in Table A1 and Table A2 , and marked values   appearing in the dialogue in bold . Table A3 and   Table A4 indicate the corresponding belief states of   each dialogue . In the first dialogue presented in Ta-   ble A1 , the joint goal accuracy is measured as 1 at   turn 2 . In this case , the model incorrectly predicted   therestaurant - pricerange slot at turns 0   and 1 , and then the utterance about the slot ap-   peared by chance . In a general case , the wrong pre-   diction of the restaurant - pricerange slot   at turn 0 will accumulate to the last turn . However ,   in this case , another incorrect prediction at turn 3   will cause error accumulation in this dialogue .   The second dialogue presented in Table A2 ,   reports the incorrect prediction according to   the interpretation of annotations at turn 4 . In   other words , because the dialogue about the   hotel - internet slot appears over turns 4 and   5 , it is solely an error depending on the predic-   tion timing of the model . Because the correct   belief state was predicted right from turn 5 , it   can not be said to be an error accumulation phe-   nomenon ; however , the model did not predict the   hotel - pricerange slot at turn 6 , which is the   last turn in this case .   In conclusion , it can be determined that the   model does not seem to accumulate erroneous pre-   dictions because of an accidental situation or inter-   pretation of annotations , but this does not negate   the error accumulation phenomenon . Furthermore ,   the fact that the starting point of making the joint   goal accuracy of subsequent turns to 0 mainly oc-   curs at the beginning of the dialogue does not   change.303Turn Dialogue History   0System : ‚Äú ‚Äù   User : ‚Äú can you help me find a nice restaurant ? ‚Äù   1System : ‚Äú sure ! what kind of food do you like ? ‚Äù   User : ‚Äú i was thinking some indian food would be great . ‚Äù   2System : ‚Äú i have 22 indian restaurant -s do you have a preference for area of town ? ‚Äù   User : ‚Äú no , i do not care where it is . i am thinking i would like an expensive restaurant . ‚Äù   3System : ‚Äú would you like to try curry garden ? ‚Äù   User : ‚Äú that is fine book me a table for 6onsatat17:30 . i also need a train for the same and   should leave leicester forcambridge ‚Äù   4System : ‚Äú alright , i have made your requested booking at curry garden , and the reference   number is hk9ycl6z . as for the train , what time would you like to depart ? ‚Äù   User : ‚Äú it does not matter as long as i am there by 13:45 leaving leicester going to cambridge ,   i ‚Äôll need the reference number too please ‚Äù   5System : ‚Äú i have found tr6210 leaving leicester at 11:09 on saturday and arriving in cambridge   at 12:54 . i have booked 6 seats . reference number is lr5i1rzv . anything else i can do ? ‚Äù   User : ‚Äú that will be all for today . thanks so much for all your help . goodbye ‚Äù   Turn Dialogue History   0System : ‚Äú ‚Äù   User : ‚Äú i would like help finding a train headed to cambridge . ‚Äù   1System : ‚Äú i will be happy to help you find a train . can you tell me where you will be departing   from ? ‚Äù   User : ‚Äú departing from london kings cross ontuesday . ‚Äù   2System : ‚Äú when would you like to leave or arrive by ? ‚Äù   User : ‚Äú i need to arrive by 18,30 . ‚Äù   3System : ‚Äú take train tr1434 , which will arrive at 18:08 . shall i book you for that train ? ‚Äù   User : ‚Äú can i get the price for a ticket , first ? ‚Äù   4System : ‚Äú sure ! the ticket is 23.60 pounds . ‚Äù   User : ‚Äú thanks ! i am also looking for a hotel called archway house . can you tell me if they   have free wifi ? ‚Äù   5System : ‚Äú they do . would you like to book a room ? ‚Äù   User : ‚Äú i would first like to know what their price range and hotel type are , thank you . ‚Äù   6System : ‚Äú archway house is a moderate -ly priced guesthouse . would you like their address or   perhaps to book a room there ? ‚Äù   User : ‚Äú thank you , but no . you ‚Äôve already helped me with everything i needed today .‚Äù304Turn Predicted State Gold State Joint Goal Acc .   0 restaurant - pricerange - expensive - 0   1restaurant - pricerange - expensive restaurant - food - indian0restaurant - food - indian   2restaurant - pricerange - expensive restaurant - pricerange - expensive   1 restaurant - food - indian restaurant - food - indian   restaurant - area - dontcare restaurant - area - dontcare   3restaurant - pricerange - expensive restaurant - pricerange - expensive   0restaurant - food - indian restaurant - food - indian   restaurant - area - dontcare restaurant - area - dontcare   restaurant - book day - sunday restaurant - book day - saturday   restaurant - book people-6 restaurant - book people-6   restaurant - book time-17:30 restaurant - book time-17:30   restaurant - name - curry garden restaurant - name - curry garden   train - destination - cambridge train - destination - cambridge   train - day - tuesday train - day - saturday   train - departure - leicester train - departure - leicester   train - book people-6   4restaurant - pricerange - expensive restaurant - pricerange - expensive   0restaurant - food - indian restaurant - food - indian   restaurant - area - dontcare restaurant - area - dontcare   restaurant - book day - sunday restaurant - book day - saturday   restaurant - book people-6 restaurant - book people-6   restaurant - book time-17:30 restaurant - book time-17:30   restaurant - name - curry garden restaurant - name - curry garden   train - destination - cambridge train - destination - cambridge   train - day - tuesday train - day - saturday   train - departure - leicester train - departure - leicester   train - arriveby-13:45 train - arriveby-13:45   train - leaveat - dontcare train - book people-6   5restaurant - pricerange - expensive restaurant - pricerange - expensive   0restaurant - food - indian restaurant - food - indian   restaurant - area - dontcare restaurant - area - dontcare   restaurant - book day - sunday restaurant - book day - saturday   restaurant - book people-6 restaurant - book people-6   restaurant - book time-17:30 restaurant - book time-17:30   restaurant - name - curry garden restaurant - name - curry garden   train - destination - cambridge train - destination - cambridge   train - day - tuesday train - day - saturday   train - departure - leicester train - departure - leicester   train - arriveby-13:45 train - arriveby-13:45   train - leaveat - dontcare train - book people-6305Turn Predicted State Gold State Joint Goal Acc .   0 train - destination - cambridge train - destination - cambridge 1   1train - destination - cambridge train - destination - cambridge   1 train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   2train - destination - cambridge train - destination - cambridge   1train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   train - arriveby-18:30 train - arriveby-18:30   3train - destination - cambridge train - destination - cambridge   1train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   train - arriveby-18:30 train - arriveby-18:30   4train - destination - cambridge train - destination - cambridge   0train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   train - arriveby-18:30 train - arriveby-18:30   hotel - name - archway house hotel - name - archway house   hotel - internet - yes   5train - destination - cambridge train - destination - cambridge   1train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   train - arriveby-18:30 train - arriveby-18:30   hotel - name - archway house hotel - name - archway house   hotel - internet - yes hotel - internet - yes   6train - destination - cambridge train - destination - cambridge   0train - day - tuesday train - day - tuesday   train - departure - london kings cross train - departure - london kings cross   train - arriveby-18:30 train - arriveby-18:30   hotel - name - archway house hotel - name - archway house   hotel - internet - yes hotel - internet - yes   hotel - pricerange - moderate306Method Metric Dataset   DST - STAR ( Ye et al . , 2021 ) JGA MultiWOZ 2.0 ( Budzianowski et al . , 2018 ) ,   MultiWOZ 2.1 ( Eric et al . , 2019 )   Seq2Seq - DU ( Feng et al . , 2021 ) JGA SGD ( Rastogi et al . , 2020b ) , MultiWOZ 2.1 ,   MultiWOZ 2.2 ( Zang et al . , 2020 )   L4P4K2 - DSGraph ( Lin et al . , 2021 ) JGA , SA MultiWOZ 2.0   Transformer - DST ( Zeng and Nie , 2021 ) JGA MultiWOZ 2.0 , MultiWOZ 2.1   NA - DST ( Le et al . , 2020 ) JGA , SA MultiWOZ 2.0 , MultiWOZ 2.1   TripPy ( Heck et al . , 2020 ) JGA WOZ 2.0 ( Wen et al . , 2017 ) , MultiWOZ 2.1 ,   Sim - M , Sim - R ( Shah et al . , 2018 )   SOM - DST ( Kim et al . , 2020 ) JGA , SA MultiWOZ 2.0 , MultiWOZ 2.1   Simple - TOD ( Hosseini - Asl et al . , 2020 ) JGA MultiWOZ 2.0 , MultiWOZ 2.1   GCDST ( Wu et al . , 2020 ) JGA MultiWOZ 2.0 , MultiWOZ 2.1   CSFN - DST ( Zhu et al . , 2020 ) JGA MultiWOZ 2.0 , MultiWOZ 2.1   SA VN ( Wang et al . , 2020 ) JGA , SA MultiWOZ 2.0 , MultiWOZ 2.1   SST ( Chen et al . , 2020 ) JGA , SA MultiWOZ 2.0 , MultiWOZ 2.1   DS - DST ( Zhang et al . , 2020 ) JGA MultiWOZ 2.0 , MultiWOZ 2.1   DSTQA ( Zhou and Small , 2019 ) JGA , SA WOZ 2.0 , MultiWOZ 2.0 , MultiWOZ 2.1   SUMBT ( Lee et al . , 2019 ) JGA WOZ 2.0 , MultiWOZ 2.0   DST - Reader ( Gao et al . , 2019 ) JGA MultiWOZ 2.0   BERT - DST ( Chao and Lane , 2019 ) JGA WOZ 2.0 , Sim - M , Sim - R   DSTC2 ( Henderson et al . , 2014a )   TRADE ( Wu et al . , 2019 ) JGA , SA MultiWOZ 2.0   HyST ( Goel et al . , 2019 ) JGA MultiWOZ 2.0   COMER ( Ren et al . , 2019 ) JGA WOZ 2.0 , MultiWOZ 2.0307Turn Predicted State Gold StateSlot Relative   Acc . Slot Acc .   0 restaurant - name - nusha - 0.9667 0   1 restaurant - name - nusha - 0.9667 0   2 restaurant - name - nusha attraction - name - nusha 0.9333 0   3 restaurant - name - nusha attraction - name - nusha 0.9333 0   4restaurant - area - centre attraction - name - nusha   0.9667 0.6667 restaurant - food - indian restaurant - area - centre   restaurant - food - indian   5restaurant - area - centre attraction - name - nusha   0.9667 0.7500restaurant - food - indian restaurant - area - centre   restaurant - pricerange - expensive restaurant - food - indian   restaurant - pricerange - expensive   6restaurant - name - saffron brasserie attraction - name - nusha   0.9667 0.8000restaurant - area - centre restaurant - name - saffron brasserie   restaurant - food - indian restaurant - area - centre   restaurant - pricerange - expensive restaurant - food - indian   restaurant - pricerange - expensive   7restaurant - name - saffron brasserie attraction - name - nusha   0.9667 0.8000restaurant - area - centre restaurant - name - saffron brasserie   restaurant - food - indian restaurant - area - centre   restaurant - pricerange - expensive restaurant - food - indian   restaurant - pricerange - expensive   8restaurant - name - saffron brasserie attraction - name - nusha   0.9667 0.8000restaurant - area - centre restaurant - name - saffron brasserie   restaurant - food - indian restaurant - area - centre   restaurant - pricerange - expensive restaurant - food - indian   restaurant - pricerange - expensive   9restaurant - name - saffron brasserie attraction - name - nusha   0.9667 0.8000restaurant - area - centre restaurant - name - saffron brasserie   restaurant - food - indian restaurant - area - centre   restaurant - pricerange - expensive restaurant - food - indian   restaurant - pricerange - expensive308309   Eunhwan Park , Donghyeon Jeon , Seonhoon Kim ,   Inho Kang , Seung - Hoon NaJeonbuk National University , NA VER Corporation   { judepark , nash}@jbnu.ac.kr   { donghyeon.jeon , seonhoon.kim , once.ihkang}@navercorp.com   Abstract   1 Introduction   The GPT-3 model ( Brown et al . , 2020 ) has achieved   remarkable few - shot performance on natural lan-   guage understanding tasks given a natural lan-   guage prompt and|K|labeled samples as demon-   strations in the inputs without updating the model ‚Äôs   weights . However , the GPT-3 model consists of   175B parameters , making it challenging to perform   task - specific fine - tuning , which is often required in   real - world applications .   To enable task - specific fine - tuning , prompt-   based few - shot fine - tuning has been widely stud-   ied to encourage the few - shot capabilities of pre-   trained language models ( PLMs ) equipped with   label - specific verbalizers andprompts that are com-   patible with language models ( Schick and Sch√ºtze ,   2021a , b ) . Prompt - based fine - tuning reformulates   downstream tasks as a masked language modeling   problem , where a token ( label word ) is generated   on a given prompt with a task - specific template .However , constructing optimal prompts requires   domain expertise and the use of manual prompts   can be suboptimal ( Webson and Pavlick , 2021 ; Lu   et al . , 2021 ; Zhao et al . , 2021 ) .   Among the various methods of prompt - based   fine - tuning , this study is based on the LM-   BFF method ( Gao et al . , 2021 ) , which uses a   demonstration - aware prompt where a demonstra-   tion is produced by unmasking the example prompt   in contexts similar to the input , inspired by the find-   ings from the GPT-3 model ( Brown et al . , 2020 ) .   With demonstration - aware prompts , the LM - BFF   outperforms the conventional fine - tuning approach   and GPT-3 ‚Äôs in - context learning . To improve LM-   BFF , we propose LM - BFF - MS , better few - shot   fine - tuning of language models with multiple soft   demonstration memory , based on the following two   extensions :   1.Prompts with multiple demonstrations .   While LM - BFF uses single demonstration ,   our model uses multiple demonstrations with   different label phrases , where each demon-   stration is constructed per label phrase . Given   that label phrases are semantically related or   similar , it is expected that resulting demon-   strations indirectly augment the vocabulary of   the verbalizer with label phrases .   2.Soft demonstration memory based on mul-   tiple sequences of word embeddings . Unlike   LM - BFF , which directly uses a sequence of   hard tokens in the demonstration , inspired by   thesoft prompts of Lester et al . ( 2021 ) , we re-   place them with a sequence of soft vectors as   a proper context for each label phrase , where   soft vectors are globally shared soft examples   for each label phrase but are not sensitive to310   an input context . In our approach , soft vec-   tors are considered as automatically generated   demonstration that matches well for each label   phrase , capturing the common context for the   corresponding phrase . To train the soft demon-   stration memory effectively , we further intro-   duce an auxiliary task , named next demon-   strations prediction ( NDP ) task , inspired by   NSP - BERT ( Sun et al . , 2021 ) .   Following the previous setting of the LM - BFF ,   the experimental results on eight NLP datasets   show that the proposed LM - BFF - MS leads to a   better and more stable few - shot performance com-   pared to the previous models . The contributions of   this study are summarized as follows :   ‚Ä¢We propose prompts with multiple soft   demonstration memory based on the auto-   matic generation of multiple label phrases and   the use of soft demonstration memory that is   armed with an auxiliary NDP task .   ‚Ä¢We present promising results of the pro-   posed method on eight NLP tasks by showing   improved results on some datasets , particu-   larly achieving state - of - the - art performance   on SST-2 and MRPC .   2 Related Work   Prompt - based few - shot fine - tuning , which finetunes   based on few - shot examples under a prompting   setting , has been widely studied for moderately   sized PLMs such as BERT ( Devlin et al . , 2019 ) andRoBERTa ( Liu et al . , 2019 ) . For example , PET re-   formulates downstream tasks as a masked language   modeling problem and performs gradient - based   fine - tuning ( Schick and Sch√ºtze , 2021a , b ) . Auto-   Prompt creates appropriate prompts for a set of dis-   crete tokens using a gradient - guided search ( Shin   et al . , 2020 ) . Null Prompts ‚Äî simple concatenations   of the inputs and [ MASK ] token ‚Äî achieve a free of   prompt engineering ( Logan et al . , 2021 ) . Instead of   using hard prompts , there have also been works of   using continuous vectors of prompt tokens , called   soft prompting , including the work of Lester et al .   ( 2021 ) , which proposes soft prompts composed of   learnable continuous embeddings while freezing   the weight of PLMs ; and Gu et al . ( 2021 ) pro-   poses pre - training prompts by adding soft prompts   into the pre - training stage to obtain a better ini-   tialization . The demonstration - aware prompt has   also been explored by ( Gao et al . , 2021 ) with their   proposed LM - BFF , where a demonstration is con-   structed by unmasking the masked prompt on a   similar input example .   Unlike LM - BFF , which uses a single demonstra-   tion per label , our work uses ‚Äò multiple ‚Äô demon-   strations that are provided for automatically gen-   erated label phrases . In addition , inspired by the   method of soft prompting , we use ‚Äò soft ‚Äô demonstra-   tion memory based on globally shared soft vectors   for prompt tokens , without using hard tokens of the   similar context.311   3 Fine - tuning with Multiple Soft   Demonstration Memory   3.1 Background   Following on from ( Gao et al . , 2021 ) , suppose that   the input sentences x = xandx= ( x , x )   are presented for single - sentence and sentence-   pair tasks , respectively . The template Tis de-   fined as ( T , T ) , where T is the tem-   plate used to generate the main prompt for input   xandT is the additional template to gen-   erate demonstrations of input x. For example ,   T(x)for a single sentence task is given as :   We use Twith the manually designed templates   of ( Gao et al . , 2021 ) .   To define T , suppose that VandYare   the vocabulary and label space , respectively . Let   M : Y ‚Üí V andM,¬∑¬∑¬∑,M : Y ‚Üí V   be mapping functions that convert a label into   individual words and phrases , called word -level   andphrase -level functions , respectively . For ex-   ample , M(pos ) = ‚Äú great ‚Äù , M(neg ) =   ‚Äú terrible ‚Äù , M(pos ) = ‚Äú a gift ‚Äù , M(neg ) = ‚Äú a total waste of my time ‚Äù . LetMbe the set   ofmphrase - level mapping functions , that is ,   M={M,¬∑¬∑¬∑,M } . Given M‚ààM ,   ÀúT ( x , y , M)is defined as the unmasked   sequence of T(x)by placing M(y)in-   stead of the [ MASK ] token ; ÀúT ( x , y , M )   is obtained by first applying T toxto pro-   duceT(x)and then replacing [ MASK ] with   M(y ) . For example , given x = x , y=   neg , M(neg ) = ‚Äú so sad ‚Äù , ÀúT ( x , y , M )   is then obtained as : " xIt was so sad .[SEP ] " .   Now , suppose that N(x , y)is the set of train-   ing examples similar to xlabeled with y. Then   T is defined as follows:(1 )   where‚äïdenotes the concatenation operator .   Finally , xis converted to its prompted version   x = T(x)‚äïT ( x ) , which is   used as the input for the prompt - based few - shot   fine - tuning .   Note that this setting includes the LM - BFF as   a specific case with |N(x , y)|= 1per label and   M={M}in Eq . ( 1 ) , which refers to a single   demonstration setting.3123.2 Automatically Generating Phrase - Level   Verbalizers   In contrast to the LM - BFF , we employ phrase -level   mapping function , as it is theorized that it would   enable a better representation of the demonstration   than a word - level mapping function . The remaining   part describes how to obtain the mphrase - level   mapping functions in Eq . ( 1 ) , M‚ààM.   To this end , we use T5 to generate label phrases   using a properly designed span - corrupted input in   the reverse manner of ( Gao et al . , 2021 ) which   exploits T5 to automatically generate templates .   The input for T5 ‚Äôs encoder is merely the prompted   sequence T(x ) , however , with [ MASK ] as   the span - corrupted token , the decoder then fills in   the placeholders , removes duplicated results , and   chooses the top mmost likely generated sequences   for phrase - level mapping functions of the corre-   sponding label as described in Figure 1 ( c ) . Our   generation results are shown in Table 4 .   3.3 Soft Demonstration Memory   Different from LM - BFF which explicitly finds sim-   ilar training examples N(x , y ) , ‚Äò soft demonstra-   tion memory ‚Äô is used , which consists of globally   shared soft examples as demonstrations , assum-   ing that each demonstration uses nsoft tokens   for a sentence as [ T]¬∑¬∑¬∑[T ] . Under a soft   demonstration memory , T ( x)is obtained   using Eq . ( 1 ) , but using the following definition of   N(x , y ):   N(x , y ) = n   [ T]¬∑¬∑¬∑[T]o   In single - sentence tasks , the soft demonstration   memory maintains a total of m¬∑|Y| sentences each   of which consists of nsoft tokens , where the set   ofmsentences corresponds to each label . For   example , when n= 10 , m= 5 , and|Y|= 2 , the   total size of the global memory is 100 .   To create mdemonstrations , all examples of   global memory are chosen without requiring a sam-   ple of similar examples . An illustration of the in-   corporated soft demonstration memory is described   shown in Figure 1 ( b ) .   3.4 Next Demonstration Prediction Task   To obtain a better representation of soft demonstra-   tion memory , we introduce the NDP task , which   predicts whether positive ( or negative ) examples   inT ( x)are correctly matched with a posi-   tive ( or negative ) label word for the prompted input   T(x ) .   To be more specific , the NDP task trains   P(y|x ) = softmax ( W h +   b ) , where W‚ààRare the output embed-   ding weights of the label words in an MLM de-   coder . Finally , given a few - shot example ( x , y ) ,   the training objective is defined as :   L = CE(P([MASK ] = M(y)|x ) ) +   Œª¬∑CE(P(y|x ) )   where CE is the cross - entropy loss function and Œªis   the hyper - parameter . Section 4.3 presents the effect   of using the NDP loss compared to that without it .   4 Experiments   The implementation details are provided in Ap-   pendix B. For a fair comparison , the same manual   prompts for T in LM - BFF and LM - BFF - MS   are used .   4.1 Main Results   As shown in Table 1 , it is noticed that the proposed   approach achieves a better and stable few - shot per-   formance than the prior methods and the LM - BFF   on five tasks . In particular , LM - BFF - MS achieves   state - of - the - art performance on SST-2 and MRPC   tasks , with 94.0 and 80.4 , respectively . Moreover ,   it is observed that the performance variation of LM-   BFF - MS are mostly lower than that of the prior   methods except for the MNLI , SNLI , and CR tasks ,   implying that our approach is more stable than   the existing models . On the other hand , LM - BFF-   MS is weaker than LM - BFF on SNLI , although   it shows comparable results to DART . We believe   that the effect of global demonstration memory is   task sensitive , suggesting that the local method of   sampling similar demonstrations as in LM - BFF of-   ten needs to be employed for some tasks or specific   input sentences.313   4.2 Soft Demonstration Memory vs. Soft   Prompting   To validate the use of soft demonstration memory ,   instead of inserting soft vectors into the demonstra-   tion parts . We further evaluate the soft prompting   of ( Lester et al . , 2021 ) by prepending psoft vectors   to the main template T(x ) , where pis the   length of the additional soft prompt .   Table 2 compares soft prompting with LM - BFF-   MS on SST-2 and shows that LM - BFF - MS out-   performs soft prompting under the setting of the   same length of soft token . The results confirm   that the gain of soft demonstration memory is not   merely obtained by using additional parameters   of soft vectors , but by effectively modeling the   demonstration - aware context .   4.3 The Effect of Using Next Demonstration   Prediction Task   To examine whether the use of the NDP task   is indeed effective in LM - BFF - MS , Table 3 com-   pares results of LM - BFF - MS with and without the   NDP task on the SST-2 dataset . As shown in Table3 , LM - BFF - MS with NDP loss shows improved   performance compared to that without NDP loss ,   providing positive evidence for our motivating hy-   pothesis that the use of the NDP loss is helpful in   enhancing the representation of soft demonstration   memory .   To further analyze the effect of auxiliary NDP   task , Figure 2 visualizes the representation of   [ MASK ] tokens on SST-2 using t - SNE ( van der   Maaten and Hinton , 2008 ) compared , with and   without the NDP task . As shown in Figure 2a   and 2b , the representation learned using the NDP   task is more discriminative than that learned with-   out the NDP task , suggesting that the NDP task   provides an effective additional loss for learning   representations of soft demonstration memory .   5 Conclusion   This study proposed LM - BFF - MS ‚Äî manual   prompts with multiple soft demonstration memory   based on the automatic generation of multiple label   words and an auxiliary NDP task . Experiments   showed that the proposed method outperforms   prior works on five tasks : SST-2 , MR , Subj ,   MRPC , and MPQA . Extending our work to a large   soft demonstration memory and a combination of   local and global memory is valuable for future   investigations .   Acknowledgements   We would like to thank all anonymous reviewers   for their valuable comments and suggestions.314References315   A Limitation   The main contribution of this work is the multiple   soft demonstration memory , however , the numberof available demonstrations is bounded by the max-   imum input length . Furthermore , unlike the GPT-3   model , the maximum input length of PLMs is usu-   ally 512 , which is not sufficient to deal with more   difficult tasks such as SNLI and MNLI . As shown   in Table 1 , despite the effectiveness of LM - BFF-   MS , it shows a lower few - shot performance than   previous studies on SNLI and MNLI . We believe   that this is strongly related to automatic phrase-   level generation and is bounded by the maximum   input length . We leave this topic as a subject for   future work .   B Implementation Details   B.1 Datasets & Setting   We used the following datasets ‚Äî SNLI ( Bowman   et al . , 2015 ) , MNLI ( Williams et al . , 2018 ) , SST-2   ( Socher et al . , 2013 ) MRPC ( Dolan and Brockett ,   2005 ) , MR ( Pang and Lee , 2005 ) , CR ( Hu and Liu ,   2004 ) , MPQA ( Wiebe et al . , 2005 ) , and Subj ( Pang   and Lee , 2004 ) . This study followed the same ex-   perimental setting from LM - BFF ( Gao et al . , 2021 ) .   B.2 Implementation   This proposed approach was implemented using   PyTorch ( Paszke et al . , 2019 ) and HuggingFace   Transformers ( Wolf et al . , 2020 ) . Experiments   were conducted with Nvidia Quadro RTX 8000   GPU . All optimizations were performed using the   AdamW optimizer with a linear warm - up of the   learning rate . The warmup proportion is 0.6 . The   gradients are clipped if their norms exceed 1.0 .   A T5 - large and beam search ( e.g. , beam width :   30 ) were used to generate phrase - level verbalizers   automatically in a zero - shot manner .   B.3 Multiple Soft Demonstration Memory   Setting   SST-2 , MR , CR , Subj , MRPC , MPQA   ‚Ä¢ Length of soft tokens : n= 10   ‚Ä¢ Number of target label : |Y|= 2   ‚Ä¢ Demonstrations per label : m= 5   ‚Ä¢ Total : |T|=n¬∑m ¬∑ |Y| = 100   MNLI   ‚Ä¢ Length of soft tokens : n= 20   ‚Ä¢ Number of target label : |Y|= 3316   ‚Ä¢ Demonstrations per label : m= 1   ‚Ä¢ Total : |T|=n¬∑m ¬∑ |Y| = 60   SNLI   ‚Ä¢ Length of soft tokens : n= 10   ‚Ä¢ Number of target label : |Y|= 3   ‚Ä¢ Demonstrations per label : m= 1   ‚Ä¢ Total : |T|=n¬∑m ¬∑ |Y| = 30   B.4 Training Example   Suppose that we train SST-2 dataset following set-   ting : n= 2,|Y|= 2 , andm= 2 . Then x is   formed as follows :   x =[ CLS ] xIt was [ MASK ] .[SEP ]   [ T ] [ T]It was an instant hit   [ T ] [ T]It was a gift [ SEP ]   [ T ] [ T]It was well worth the effort   [ T ] [ T]It was a total waste of my time [ SEP ]   where x,[T],¬∑¬∑¬∑[T],[T],¬∑¬∑¬∑[T]are the   input sentence and multiple soft demonstrationmemory for positive and negative labels , respec-   tively . In this case , W‚ààRis the output   embedding weights of label words ( e.g. , positive :   ‚Äò great ‚Äô , negative : ‚Äò terrible ‚Äô ) in a MLM Decoder for   the NDP task.317   Suvodip Dey , Ramamohan Kummara , Maunendra Sankar Desarkar   Indian Institute of Technology Hyderabad , India   Abstract   1 Introduction   Dialogue State Tracking ( DST ) is at the core of   task - oriented dialogue systems . It is responsible   for keeping track of the key information exchanged   during a conversation . With the growing popularity   of task - based conversational agents , it is essential   to review the evaluation of DST to appropriately   measure the progress in this evolving area .   The task of DST is to predict the user intent   through dialogue states ( Henderson et al . , 2014 ) .   Fig . 1 shows an example DST task from Multi - WOZ ( Budzianowski et al . , 2018 ) dataset . Let U   andSbe the user and system utterances respec-   tively at turn t. Then a typical conversation can   be expressed as D={U,(S , U ) , ... ( S , U ) } .   The commonly used ground - truth dialogue state   for DST is the belief state . Belief state Bfor turn   tis defined as the set of ( domain , slot , slot - value )   triplets that have been extracted till turn t , thereby   it is cumulative in nature . The objective of DST is   to predict Bgiven the dialogue history till turn t.   The primary metric for evaluating DST is Joint   Goal Accuracy ( JGA ) . It compares the predicted   dialogue states to the ground truth Bat each dia-   logue turn t(Henderson et al . , 2014 ) . As the belief   state is cumulative , it is very unlikely for a model   to get back a correct prediction after a mispredic-   tion . This is why it can provide an underestimated   performance in certain cases . Besides , JGA com-   pletely ignores the performance of turn - specific   local predictions . Let Tbe the turn - level belief   state that contains all the intents or ( domain , slot ,   slot - value ) triplets expressed by the user only at   turnt . Ideally , a model with higher JGA should   also perform equally well to predict T. But , we ob-   serve that improving JGA can sometimes degrade   the performance of predicting Tmainly due to the   presence of annotation inconsistencies in the avail-   able datasets . For example , in Fig . 1 , the presence   of(hotel , area , centre ) and absence of ( attraction ,   name , all saints church ) in ground - truth Band   Bshows such inconsistencies . So , the general-   ization of the model may get compromised if the   model selection is done only using JGA . Anno-   tation inconsistencies and errors are common in   real - world datasets . Hence , to provide a fair esti-   mate , it requires not only track the performance of   the cumulative belief state but also turn - level belief   state as well .   In this work , we address these issues of JGA by   proposing a novel evaluation metric for DST called   Flexible GoalAccuracy ( FGA ) . The central idea of318FGA is to partially penalize a misprediction which   is locally correct i.e. the source of the mispredic-   tion is some earlier turn . The main contributions of   our work are as follows :   ‚Ä¢Detailed analysis of the existing DST metrics .   ‚Ä¢Proposal of Flexible Goal Accuracy ( FGA )   than can keep track of both joint and turn-   level performances simultaneously .   ‚Ä¢Justification of FGA along with performance   comparison on the MultiWOZ dataset .   2 Discussion on existing DST metrics   2.1 Joint goal accuracy   Joint accuracy or joint goal accuracy ( JGA ) checks   whether the set of predicted belief states exactly   matches the ground truth for a given user turn ( Hen-   derson et al . , 2014 ; Wu et al . , 2019 ) . Let Band   Bbe the set of ground - truth and predicted belief   states at turn t. Then the prediction of turn tis   considered to be correct if and only if Bexactly   matches B. Fig . 1 shows an illustration of the pre-   dicted belief state where the predictions of Bare   generated using SOM - DST ( Kim et al . , 2020 ) . In   the example , there are 2 out of 6 correct predictions   ofBthat result in a JGA score of 33.33 % for the   whole conversation .   Although joint goal accuracy is a convenient met-   ric to evaluate DST , it has certain limitations . The   main source of the issue is the cumulative nature of   ground - truth B. As a result , once a misprediction   has occurred , it is difficult to get back a correct   prediction in subsequent turns . For example , in   Fig . 1 , the prediction goes wrong in Turn 2which   affects all the later predictions . So , it is very likely   to get a JGA of zero if the model somehow mispre-   dicts the first turn . Therefore , JGA can undermine   the true potential of a DST model and provide an   underestimated performance .   In addition , JGA does not take into account turn-   level performances . For instance , in Fig . 1 , Turn 3   and5are locally correct but JGA will mark them 0   sinceBandBhas not matched exactly . Normally ,   it is expected that increasing the exact matches will   also reflect in turn - level matches . But we observed   that sometimes increasing exact matches can de-   crease turn - level matches mainly due to annotation   inconsistencies . So , one should be careful while   using only joint accuracy for model selection . Be-   sides , the available DST datasets ( like MultiWOZ )   contain a lot of annotation errors ( Zang et al . , 2020 ) .   For example in turn 4 , the model has predicted the   intent ( attraction , name , all saints church ) . Al-   though the prediction looks rational , the triplet is   absent in the ground - truth . So , if a mismatch occurs   due to an annotation error , it is highly probable that   all the subsequent turns will be marked incorrect   leading to an underestimated performance .   Hence , using joint goal accuracy for evaluating   DST works fine if there are no annotation errors   and the sole purpose is to improve the prediction of   cumulative belief state . Otherwise , there is a need   to include turn - level performance in order to obtain   a fair evaluation of a DST model.3192.2 Slot Accuracy   Slot accuracy ( SA ) is a relaxed version of JGA that   compares each predicted ( domain , slot , slot - value )   triplet to its ground - truth label individually ( Wu   et al . , 2019 ) . Let Sbe the set of unique domain-   slot pairs in the dataset . Let BandBbe the set   of ground - truth and predicted belief states respec-   tively . Then slot accuracy at turn tis defined as   SA=|S| ‚àí |X| ‚àí |Y|+|P‚à©Q|   |S| , ( 1 )   where X= ( B\B),Y= ( B\B),Pis the set   of unique domain - slot pairs from X , andQis the   set of unique domain - slot pairs from Y. Basically ,   in Equation 1 , |X|and|Y|represent the number   of false negatives and false positives respectively .   Note that if the value of a ground - truth domain - slot   pair is wrongly predicted then this misprediction   will be counted twice ( once in both XandY ) . The   term|P‚à©Q|in the above equation helps to rectify   this overcounting . In MultiWOZ , the value of |S|   is 30 . For Turn 2 in our running example , since   |B\B|= 2and|B\B|= 0 , slot accuracy is   equal toi.e . 93.33 % . Slot accuracy for   the entire conversation in Fig . 1 is 94.44 % .   The value of slot accuracy can be very mislead-   ing . For instance , even if the prediction of Turn 2 is   wrong in Fig . 1 , we get a slot accuracy of 93.33 %   which is extremely high . Basically , slot accuracy   overestimates the DST performance . Let us exhibit   this fact by considering the case where we predict   nothing for all turns i.e. B=‚àÖ,‚àÄt . Then , slot   accuracy simplifies to . It is natural that   |B|<<|S|because a conversation will typically   have only a small number of domain - slot pairs live   at any time . As a result , slot accuracy remains on   the higher side ( ‚âà81 % for MultiWOZ 2.1 ) even if   we predict nothing . For datasets with a larger num-   ber of domain / slots , since |S|is large , slot accuracy   will be close to 1 for almost all scenarios . Thus ,   slot accuracy is a poor metric to evaluate DST .   2.3 Average Goal accuracy   Average goal accuracy ( AGA ) is a relatively newer   metric proposed to evaluate the SGD dataset ( Ras-   togi et al . , 2020 ) . Here , the slots that have a non-   empty assignment in the ground - truth dialogue   state are only considered during evaluation . Let   N‚äÜBbe the set of ground - truth triplets having   non - empty slot - values . Then AGA is computed aswhere Bis the predicted belief state forturnt . The turns having N=‚àÖare ignored during   the computation of AGA . In Fig . 1 , AGA for turn   2 is 4/6 , and 76.19 % for the entire conversation .   This metric has mainly two limitations . Firstly ,   AGA is only recall - oriented and thereby does not   consider the false positives . Ignoring the false pos-   itives makes this metric insensitive to extraneous   triplets in the predicted belief state . However , this   issue can be easily addressed by redefining AGA   as . But there still exists a second major   problem with AGA . Note that even if a turn is com-   pletely wrong , AGA for that turn can still be higher   because of the correct predictions in the previous   turns . For example , even if turn 2 and 4 are incor-   rect , we get an AGA of 4/6 and 5/7 respectively   which clearly indicates an overestimation .   3 Flexible Goal Accuracy   From the previous discussion , it is evident that   despite a few limitations , joint goal accuracy is su-   perior to the other two metrics . This is why with   the objective to obtain a better evaluation metric   for DST , we address the shortcomings of JGA by   proposing a new metric called Flexible goal accu-   racy ( FGA ) . The description of FGA is presented   in the next part of this section , whereas its working   is described as a pseudo - code in Algo . 1 .   For a given a turn t , an error in belief state predic-   tion ( i.e. BÃ∏=B ) can occur in two ways : 1 ) the   source of the error is turn titself i.e. the turn - level   prediction is wrong , 2 ) the turn - level prediction of   turntis correct but the source of the error is some   earlier turn t‚â∫t . FGA works differently from   JGA only for type 2 errors . Unlike JGA , FGA does   not penalize type 2 errors completely . It assigns   a penalized score based on the distance between   the error turn ( t ) and the current turn ( t ) and the   penalty is inversely proportional to this distance   ( t‚àít ) . The main idea is to forget the mistakes   with time in order to attain a fair judgment of a   DST model offline .   We decide the correctness of a turn - level match   using the logic shown in line 10 of Algo . 1 . A turn   t > 0is locally correct if ( T‚äÜBandT‚äÜB )   where T = B\BandT = B\B. In   other words , a turn - level or local match indicates   that all the intents shown by the user in a particular   turn have been correctly detected without any false   positives . Just comparing TandTto check a   turn - level or local match can be erroneous because   it will not credit the model for error corrections.320Algorithm 1 : FGA for single conversation   Input : B = list of groun - truth belief states ,   B= list of predicted belief states ,   N= # turns   Output : Flexible goal accuracyT={0,1 , . . . , N ‚àí1},t‚Üê ‚àí‚àû , f = 0fort‚ààTdo w‚Üê1 ifBÃ∏=Bthen ift= 0then   /*Type 1 error * / w‚Üê0,t‚Üêt else T‚ÜêB\B T‚ÜêB\B ifTÃ∏‚äÜBorTÃ∏‚äÜBthen   /*Type 1 error * / w‚Üê0,t‚Üêt else   /*Type 2 error * / x‚Üê(t‚àít ) w‚Üê1‚àíexp(‚àíŒªx ) f‚Üêf+wreturn f / N   For the penalty function , we use the CDF of expo-   nential distribution ( shown in Line 14 of Algo . 1 )   parameterized by Œªwhere Œª‚â•0 . Clearly , the strict-   ness of FGA is inversely proportional to Œª . Note   thatŒª= 0will reduce FGA to JGA ( strict metric )   whereas Œª‚Üí ‚àû will report only the accuracy on   turn - level matches ( relaxed metric ) . Finding the   appropriate Œªfor a specific DST task should be   done carefully in order to match the desired evalu-   ation criteria . However , we can take a theoretical   stand and approximate the hyper - parameter value   asŒª=‚àíln(1‚àíp)/twhere tis the number of   turns that it will take to forget a mistake by factor   pwhere ( 0‚â§p < 1 ) . For example , if t=6 and   p=0.95 , then Œª=0.499 . So , the strictness of FGA   is directly proportional to tand inversely propor-   tional to p. If the dataset is clean , one can alterna-   tively find the best Œªthrough a human evaluation ,   although it would require additional human effort .   Hence , we can flexibly set the strictness criteria of   FGA through the hyper - parameter Œªaccording to   our requirement .   In our running example ( Fig . 1 ) , the FGA score   for each turn with Œª= 0.5is { 1 , 1 , 0 , 0.39 , 0 , 0.39 }   which results in a FGA score of 46.33 % for theentire conversation . We can observe two things   from these numbers . Firstly , it is not overestimat-   ing in comparison to SA and AGA . Secondly , it   gives a better estimate than JGA in keeping track of   both exact and turn - level matches simultaneously .   Hence , FGA can provide a relatively balanced esti-   mate than the existing metrics even in the presence   of annotation errors and inconsistencies .   4 Result and Analysis   In this section , we report the performance of FGA   along with the other metrics on four different DST   models : TRADE ( Wu et al . , 2019 ) , Hi - DST ( Dey   and Desarkar , 2021 ) , SOM - DST ( Kim et al . , 2020 ) ,   and Trippy ( Heck et al . , 2020 ) . We use the Mul-   tiWOZ 2.1 dataset ( Eric et al . , 2020 ) as most of   the recent progress in DST are showcased on this   dataset . The results are reported in Table 1 . Since   the MultiWOZ dataset covers many domains ( ho-   tel , restaurant , taxi , train , attraction ) where each   domain may have different levels of tolerance ( in-   tuitively train , taxi booking may be strict whereas   information seeking about attraction , restaurant do-   mains may be lenient ) , an overall common / single   strictness setting for the entire dataset may be diffi-   cult to reach at . Hence , we reported the FGA score   for multiple values of hyper - parameter Œªrather   than showing the result for a single value . For the   same reason , we did not try to find the best Œªfor   evaluating the MultiWOZ dataset .   From Table 1 , we can observe that Trippy has   the best JGA . Currently , most of the state - of - the - art   DST performances are shown using Trippy . How-   ever , we can notice that Trippy does not have the   same performance gain for turn - level matches . It   has lesser turn - level matches than SOM - DST and   Hi - DST . This behavior of Trippy can be a side-   effect of boosting the JGA using its intricate featur-   ization . In contrast , Hi - DST optimizes explicitly   for turn - level non - cumulative belief states , thereby   achieving better turn - level accuracy at the expense   of JGA . Among the four models , SOM - DST per-   forms well for both objectives because of their so-   phisticated selective overwrite mechanism . Now ,   by comparing the numbers of Table 1 , we can infer   that FGA does a better job in providing a fair esti-   mate while considering both exact and turn - level   matches . Moreover , we can also notice that FGA   acts as a better discriminator of DST models in   comparison to the existing metrics .   Human Evaluation : We conducted a human321   evaluation involving 11 evaluators on 100 randomly   picked conversations from the MultiWOZ 2.1 test   data . For each turn in a conversation , we pro-   vided the system and user utterances along with the   ground - truth and predicted belief states . The pre-   dictions were generated using SOM - DST . For each   conversation , the evaluators were asked to report   their satisfaction ( 1 ) or dissatisfaction ( 0 ) with the   performance of the model in keeping track of user   intent throughout the conversation . Pearson corre-   lation coefficient of JGA and FGA ( with Œª= 0.5 )   with human ratings came out to be 0.33 and 0.37   respectively . This shows that FGA is slightly better   correlated than JGA with human evaluation .   5 Conclusion   In this work , we analyzed the limitations of exist-   ing DST metrics . We argued that joint accuracy   can underestimate the power of a DST algorithm ,   whereas slot and average goal accuracy can overes-   timate it . We addressed the issues of joint accuracy   by introducing Flexible goal accuracy ( FGA ) which   tries to give partial credit to mispredictions that are   locally correct . We justified that FGA provides a   relatively balanced estimation of DST performance   along with better discrimination property . In con-   clusion , FGA is a practical and insightful metric   that can be useful to evaluate future DST models .   References322A Appendix   A.1 MultiWOZ Dataset   MultiWOZ ( Budzianowski et al . , 2018 ) is a popular   DST corpus that contains both single and multi-   domain conversations . For this work , we used   MultiWOZ 2.1 ( Eric et al . , 2020 ) which is an up-   dated version of the original MultiWOZ 2.0 dataset .   In addition to the original dataset , MultiWOZ 2.1   contains fixes to some noisy annotations . Table 2   shows few elementary statistics of the dataset .   A.2 Result generation procedure   We generated results for four DST models - Trade   ( Wu et al . , 2019 ) , Hi - DST ( Dey and Desarkar ,   2021 ) , SOM - DST ( Kim et al . , 2020 ) , and Trippy   ( Heck et al . , 2020 ) . We used their official code   to train them on MutiWOZ 2.1 dataset . All four   models generate an inference file that contains the   predicted belief states for the test set . We used these   inference files to compute the values of different   metrics shown in Table 1 . As we trained all the   models from scratch , the results may not be exactly   the same as those reported in the original paper .   A.3 Human evaluation format   For each randomly picked conversation for human   evaluation , we prepared a file that logged the utter-   ances , ground - truth , and predicted belief state for   each turn . Additionally , we indicated whether the   ground truth exactly matched the predicted belief   state to speed up the evaluation process . A sample   file format is shown in Fig . 2.323324   Mohsen Tabasi , Kiamehr RezaeeandMohammad Taher PilehvarDepartment of CE , Iran University of Science and Technology , Tehran , IranCardiff NLP , School of Computer Science and Informatics , Cardiff University , UKTehran Institute for Advanced Studies , Khatam University , Tehran , Iran   s_tabasi@comp.iust.ac.ir , rezaee.k@cardiff.ac.uk   mp792@cam.ac.uk   Abstract   1 Introduction   Recently , there has been a resurgence of interest in   few - shot learning , especially after the introduction   of GPT-3 ( Brown et al . , 2020 ) . The current dom-   inant few - shot approach is the so - called prompt-   based learning which involves a simple reformu-   lation of the target task as a cloze - style ( Taylor ,   1953 ) Ô¨Åll - in - the - blank objective . The core idea is   to extract knowledge by asking the right question   from the pre - trained language model ( PLM ) using   a task - speciÔ¨Åc prompting template which directs   the PLM to generate a textual output correspond-   ing to a target class . This paradigm has proven its   effectiveness in the few - shot setting , even for rela-   tively smaller models , such as BERT ( Devlin et al . ,2019 ) and RoBERTA ( Liu et al . , 2019 ) , when com-   bined with ensembling and Ô¨Åne - tuning ( Schick and   Sch√ºtze , 2021a ) . From the practical point of view ,   prompt - based learning is particularly well - suited   for massive models , such as GPT-3 , since it does   not involve parameter tuning .   Prompt - based techniques have shown impres-   sive performance in the few - shot setting , especially   when compared to standard Ô¨Åne - tuning on datasets   of hundreds of data points ( Le Scao and Rush ,   2021 ) . However , surprisingly , the Word - in - Context   task ( Pilehvar and Camacho - Collados , 2019 ) ‚Äì one   of the tasks in the SuperGLUE benchmark ( Wang   et al . , 2019 ) ‚Äì is one exception on which these meth-   ods fail to stay on par with their Ô¨Åne - tuned coun-   terparts . While a simple Ô¨Åne - tuned BERT - base   model achieves around 69 % accuracy on this task   ( Wang et al . , 2019 ) , GPT-3 , with more than 100   times the number of parameters , performs no better   than a random baseline by employing a prompt-   based approach ( Brown et al . , 2020 ) . The same pat-   tern of failure is also observed in the more recent   prompt based attempts ( Liu et al . , 2021 ; Schick and   Sch√ºtze , 2021a ) .   The natural question that arises here is if the fail-   ure of few - shot techniques on WiC is due to lack   of relevant encoded knowledge in PLMs or the in-   efÔ¨Åciency of the employed prompt - based methods .   Two issues could be responsible for the latter case :   ( 1 ) improper prompt , or ( 2 ) inefÔ¨Åcient utilization   of PLM ‚Äôs response . To address the Ô¨Årst issue , there   have been proposals to automatically Ô¨Ånd a suit-   able prompt template using a search in the discrete   token space ( Shin et al . , 2020 ) or in the continuous   embedding space ( Liu et al . , 2021 ) . However , none   of these have shown success on the WiC task .   In this work we investigate the latter issue by325   introducing a new conÔ¨Åguration for prompting .   Given the comparison - based nature of WiC , we   hypothesize that conventional prompting methods   fall short since they only utilize a single prompt   response . Hence , instead of relying on a single   response , we make use of the similarity of PLM ‚Äôs   response to the combination of a pair of prompts .   The experimental results on the WiC dataset shows   that , with only 16 instances per class , our proposed   prompt - based technique can achieve comparable   results to the Ô¨Åne - tuned models ( with access to   full training data of 2700 + instances per class ) .   Moreover , we show that with few adjustments , this   simple approach can be effectively used for other   downstream tasks .   2 Methodology   Fine - tuning on a speciÔ¨Åc task can potentially up-   date PLMs on what the task is and how to solve it .   Assuming that PLMs know how to solve some tasks   ( to some extent ) , prompt - based learning focuses on   the former , i.e. , teaching the model what the task   is , without needing to resort to large amounts of   data or additional parameters . The common ap-   proach in prompt - based learning is to reformulate   the task as a cloze - style question . For instance ,   to ask about the sentiment of a movie review , one   can augment the review with a cloze question like   ‚Äú this movie was ‚Äî ‚Äî . ‚Äù . Existing methods often   pick a set of one or few word predictions as a rep-   resentative for each class , utilizing the languagemodel ‚Äôs response in a sub - optimal manner . We pro-   pose a similarity - based method that not only better   exploits the response , but also allows using multi-   ple prompts which paves the way for comparison-   based tasks , such as WiC. In what follows in this   section , we describe our similarity - based prompt-   ing approach which we will refer to as SP(Similar-   ity Prompting ) .   As shown in Figure 1 , SP consists of three main   steps : ( 1 ) prompt generation , ( 2 ) feature extrac-   tion , and ( 3 ) prediction . Given a task - speciÔ¨Åc input   consisting of one or more text sequences , we Ô¨Årst   use a template function to generate a prompt ‚Äî a   sequence of tokens containing one [ ] to-   ken ‚Äî per input sequence . For instance , in senti-   ment analysis , for the movie review ‚Äú Just give it a   chance . ‚Äù , a valid template function would generate   as output prompt : ‚Äú Just give it a chance . this movie   was ‚Äî ‚Äî . ‚Äù . The next step is feature extraction   from a PLM . This is done by giving the generated   prompts to the PLM as input and obtaining its con-   textualized embedding at the index .   The third step is where SP differs from existing   prompt - based approaches . Here , we Ô¨Årst obtain   class - speciÔ¨Åc centroids by taking the average of   the embeddings of our few training exam-   ples . To classify a new sample at inference time , a   simple approach would be to employ a nearest cen-   troid classiÔ¨Åer . However , this assumes the variance   of different classes to be equal in the embedding   space . To alleviate the problem , we perform a class   centroid - based dimension reduction ( i.e. by taking326the similarity to each centroid as a feature ) , and   train a simple linear classiÔ¨Åer . This linear model is   then used at inference time to evaluate SP on test   set .   2.1 Similarity Prompting for WiC   The surprising failure of existing prompt - based   techniques on the Word - in - Context task ( Pilehvar   and Camacho - Collados , 2019 , WiC ) , motivated us   to focus on Ô¨Ålling this gap . Given an ambiguous   target word in two different contexts , the task in   WiC is deÔ¨Åned as a simple binary classiÔ¨Åcation   problem to identify if the triggered meaning of the   target word differs in the two contexts or not .   Previous work has fallen short of designing a sin-   gle prompt template which make the PLM answer   about the target word having the same meaning   or not ( e.g. , with " yes " or " no " ) . Therefore , we   ask PLM about the triggered meaning of the tar-   get word , separately for each context , and leave   the comparison to similarity measures . Having an   input sentence and the target word index , we in-   sert ‚Äú or ‚Äî ‚Äî ‚Äù after the target word , where ‚Äú ‚Äî ‚Äî ‚Äù   indicates the token . In the Ô¨Årst step of SP ,   we apply this template function to both input sen-   tences which generates a pair of prompts . Next   the prompts are separately fed to PLM , resulting   in a pair of mask embeddings as PLM ‚Äôs response .   Finally , our classiÔ¨Åcation step reduces to that of   directly comparing our pair of embedding vectors   using a similarity function , to produce a single   similarity score for each instance . We then train   the same linear model as before on the similarity   scores of the training set examples to Ô¨Ånd the best   discriminating threshold .   Similarity Measures . We opted for two similar-   ity metrics : cosine similarity and Spearman ‚Äôs rank   correlation . The latter is a rank - based comparison   measure which is insensitive to the absolute values   of individual dimensions ( rather checks for their   relative rankings ) .   3 Experiments   3.1 Comparison Systems   We compare our results on WiC with three other   methods , all of which use 32 examples for their   training . PET ( Schick and Sch√ºtze , 2021b )   prefers ALBERT - xxlarge - v2 ( Lan et al . , 2019 ) over   RoBERTa ( with an average gain of 8 points on a   subset of SuperGLUE tasks ) and Ô¨Åne - tunes it withmanually engineered cloze - style prompts . P - tuning   ( Liu et al . , 2021 ) uses the same PLM as PET , but   optimizes a continuous prompt instead of tuning   PLM parameters . GPT3 ( Brown et al . , 2020 ) is   different in that it employs the so - called in - context   learning which involves no parameter tuning .   3.2 Tasks   In addition to WiC , we also carried out experiments   on two more tasks . The goal of this additional ex-   periment is twofold : Ô¨Årst , to show the applicability   of SP to other settings , including tasks with single   input sequence ; and second , to evaluate if SP is   effective when using prompt templates from other   techniques , including those optimized for speciÔ¨Åc   tasks . For this experiment , we compare against   AutoPrompt ( Shin et al . , 2020 ) . The approach   makes use of full training set to optimize discrete   prompts for each speciÔ¨Åc target task . Following   AutoPrompt , we report results for the following   two task :   SST . Stanford Sentiment Treebank ( Socher et al . ,   2013 ) contains Ô¨Åne - grained sentiment labeled parse   trees of sentences from movie reviews . Systems   are evaluated either on a Ô¨Åve - way Ô¨Åne - grained   or binary classiÔ¨Åcation task . We follow the lat-   ter ( SST-2 ) in our experiments . For this task we   used the automatically - generated template of Auto-   Prompt , along with the following manual template :   T(sent ) = sent + ‚Äú this movie was ‚Äî ‚Äî . ‚Äù , where   sent is the input sentence and ‚Äú + ‚Äù is concatenation   operator . This is the same manual prompt used in   AutoPrompt .   SICK . Sentences Involving Compositional   Knowledge ( Marelli et al . , 2014 ) is a collection   of sentence pairs annotated with their entailment   relationship as well as a quantiÔ¨Åed measurement   of their semantic similarity . In our experiments ,   we only use the former annotations ( SICK - E ) to   compare our results with AutoPrompt , which only   reports results for its optimized prompt . Thus   we deÔ¨Åne our own manual template function as :   T(pre ; hyp ) = pre+ ‚Äú ? Answer : ‚Äî ‚Äî , ‚Äù + hyp ,   where preis the premise and hypis the hypothesis   of an input example .   3.3 Setup   To train our models , we only used 16 examples per   class . As for PLM , we opted for RoBERTA - large   to be able to benchmark our results against Auto-   Prompt ‚Äôs ( Shin et al . , 2020 ) . Our experiments are327   repeated 5 times using different randomly sampled   training examples . For each experiment , we report   the average performance along with the standard   deviation .   3.4 Results   Given that our experiments are mainly focused on   the WiC dataset , we Ô¨Årst report our results on this   benchmark , and then provide additional results for   the other two tasks .   3.4.1 WiC   Table 1 summarizes the results on WiC with   RoBERTa - Large as SP ‚Äôs PLM . The performance   of SP in the few - shot setting is in the same ball-   park as supervised Ô¨Åne - tuning ( with nearly 170   times the data , i.e. , 2,714 instances per class ) . This   observation suggests that PLMs already encode a   certain amount of task - related knowledge and the   supervised Ô¨Åne - tuning mainly updates their task   description ( i.e. , what the task is , not how to solve   it ) . Therefore , using limited examples in the few-   shot setting they are able to reach their maximum   Ô¨Åne - tuning potential on WiC. We report SP ‚Äôs per-   formance on WiC for other PLMs in the Appendix   which shows our method / observation does not de-   pend on a speciÔ¨Åc PLM . We also include some   detailed examples of how SP works for WiC in the   Appendix .   3.4.2 SICK and SST-2   The results on SST-2 and SICK - E are shown in   Table 2 . We compare SP with AutoPrompt which   searches for the best template for each task . For   SST-2 , we observe that SP can exploit a manual   prompt template signiÔ¨Åcantly better than Auto-   Prompt , while being competitive using the best tem-   plate optimized by AutoPrompt ( auto - generated ) .   This suggests that it is possible to gain signiÔ¨Åcant   improvement by simply exploiting a non - optimized   manual prompt template .   To compare our results with AutoPrompt on the   SICK - E task , we report accuracy score of SP for the   standard test set ( with neutral majority ) and its bal-   anced variant . SP retains an acceptable level of per-   formance , particularly with the manual prompt , but   lags behind with the auto - generated prompt . We   note that the goal of this experiment was to show-   case that our simple adaptation is also applicable to   scenarios other than the setting of WiC. In fact , one   could argue that the auto - generated prompt of Auto-   Prompt is sub - optimal for our model , which results   in dropped performance on the SICK - E dataset .   3.5 Similarity Measures Comparison   Notably , the Spearman correlation score , which   is less commonly used for comparing embeddings ,   outperforms the cosine similarity on WiC by a large   margin while maintaining the same level of per-   formance on other tasks . This superiority can be   explained by the assumption that cosine similarity   is more susceptible to variations in the dominant   dimensions . To evaluate this hypothesis , we per-   formed an experiment in which the most dominant   dimension was set to zero for all the embeddings   ( the dominant dimension is identical across all vec-   tors ) . The results approve the assumption : pruned   cosine similarity gains around 10 % absolute perfor-   mance boost on WiC , Ô¨Ålling the gap to Spearman   correlation . However , the gain in the other two   tasks is negligible .   The difference in the gain across tasks can be ex-   plained by the difference in their underlying nature.328   In WiC , the embeddings can potentially re-   fer to any word , varying from sample to sample .   However , in SST and SICK the template   embedding is more restricted , often representing   a closely related word to one of the class centroid   embeddings ( e.g. , in SST the embedding   almost always represents a positive or negative ad-   jective ) . This results in a higher spread on the   most dominant dimension in the case of WiC. It is   known that the most dominant dimensions in PLMs   often encode irrelevant information , such as word   frequency ( Gao et al . , 2019 ) , therefore hampering   performance for sensitive metrics such as cosine   similarity . To verify our hypothesis , we ran an ex-   periment using 1200 sample embeddings   for each of our three tasks . Figure 2 illustrates   the distribution of values for the most dominant   dimension . The ratio of variance is 6.5 times for   WiC compared to SST and 27.3 times compared   to SICK . This further supports the sensitivity of   cosine similarity for WiC to the noisy variations   along the most dominant dimension compared to   the other two tasks .   4 Conclusion   We proposed an adaptation of prompt - based learn-   ing which addresses the common failure of existing   techniques on the WiC dataset . In this work we   showed that similarity based approach to prompt-   based learning is capable of achieving compara-   ble results to purely Ô¨Åne - tuning based methods on   Word - in - Context task , in which previous few - shot   attempts have failed . We also showed that Spear-   man ‚Äôs ranking correlation is a more robust choice   of similarity measure compared to cosine similarityin this setting . We hope that our positive results in-   spire other prompting strategies to better exploit the   encoded knowledge in PLMs . As future work , one   interesting direction could be to perform further   analysis on the behaviour of Spearman ‚Äôs correla-   tion compared to cosine similarity anywhere it is   applicable as a similarity measure .   References329330A Experiments with other PLMs   This appendix contains more details on WiC exper-   iments . Table 3 shows full test set results of SP for   different PLMs and similarity measures to compare   the performance of SP in different scenarios . Since   our cloze - style prompt template is not applicable to   GPT2 , we use a different template for it : sentence   + targetword + " means ‚Äî ‚Äî " . The results in   Table 3 generally conÔ¨Årm the effectiveness of SP   with different PLMs . Notably , this observation is in   line with our previous experiments that in general   Spearman has superior performance over Cosine   similarity .   Base model Cosine Spearman   RoBERTa - Large 63.6 70.2   BERT - Large - Cased 69.4 69.0   RoBERTa - Base 63.8 68.7   BERT - Base - Cased 64.8 67.1   GPT2 - Large 56.4 63.3   GPT2 - Base 62.3 62.6   B Qualitative Analysis   We include some examples of how SP works on   WiC in Table 4 for qualitative analysis . The exam-   ples are those from WiC dev set which had negative   labels . We did not include the positive examples ,   since the observation that the same words with the   same senses are treated similarly , might not provide   a useful insight . The table presents our generated   prompts , top-5 most probable words predicted by   RoBERTa - Large for each prompt and the Ô¨Ånal pre-   diction of SP . The top three examples are correctly   predicted as negative with high conÔ¨Ådence ( high   similarity score ) , while the bottom three are pre-   dicted positive again with high conÔ¨Ådence . The   most probable predicted words for the top three   examples indicate that the PLM has spotted the   correct senses in both contexts . For the bottom   three where the model fails , we can observe that   the target words have very similar or close senses ,   making them really hard to distinguish.331Prompt1 ( Top-5 words ) Prompt2 ( Top-5 words ) Prediction Ground   Truth   The drawing or ‚Äî ‚Äî of water from   the well . He did complicated pen - and - ink   drawings or ‚Äî ‚Äî like medieval   miniatures . Not   matchedNot   matched   ( use , extraction , taking , pumping ,   consumption)(paintings , sculptures , something ,   more , looked )   The body or ‚Äî ‚Äî of the car was   badly rusted . Administrative body or ‚Äî ‚Äî . Not   matchedNot   matched   ( trunk , roof , chassis , frame , grill ) ( agency , institution , government ,   commission , equivalent )   The main body of the sound or ‚Äî ‚Äî   ran parallel to the coast . He strained to hear the faint sounds   or ‚Äî ‚Äî .Not   matchedNot   matched   ( river , bay , sea , ocean , channel ) ( voices , footsteps , whispers , con-   versations , cries )   He could not conceal his hostility   or ‚Äî ‚Äî .He could no longer contain his hos-   tility or ‚Äî ‚Äî .Matched Not   matched   ( anger , disgust , irritation , contempt ,   frustration ) ( anger , rage , frustration , aggres-   sion , disgust )   There was a blockage or ‚Äî ‚Äî in the   sewer , so we called out the plumber . We had to call a plumber to clear   out the blockage or ‚Äî ‚Äî in the   drainpipe . Matched Not   matched   ( something , leak , obstruction , de-   fect , overÔ¨Çow)(debris , obstruction , water , leak ,   crack )   The senator received severe criti-   cism or ‚Äî ‚Äî from his opponent . The politician received a lot of pub-   lic criticism or ‚Äî ‚Äî for his contro-   versial stance on the issue . Matched Not   matched   ( threats , ridicule , mockery , attacks ,   threat)(backlash , ridicule , mockery , con-   demnation , criticism)332   Peiyi Wang , Liang Chen , Tianyu Liu , Damai Dai ,   Yunbo Cao , Baobao Chang , Zhifang SuiKey Laboratory of Computational Linguistics , Peking University , MOE , ChinaTencent Cloud Xiaowei   wangpeiyi9979@gmail.com ; leo.liang.chen@outlook.com   { rogertyliu , yunbocao}@tencent.com   { daidamai , chbb , szf}@pku.edu.cn   Abstract   1 Introduction   Abstract Meaning Representation ( AMR ) ( Ba-   narescu et al . , 2013 ) parsing aims to translate a   natural sentence into a directed acyclic graph . Fig-   ure 1(a ) illustrates an AMR graph where nodes   represent concepts , e.g. , ‚Äò die-01 ‚Äô and ‚Äò soldier ‚Äô ,   and edges represent relations , e.g. , ‚Äò : ARG1 ‚Äô and   ‚Äò : quant ‚Äô . AMR has been exploited in the down-   stream NLP tasks , including information extraction   ( Rao et al . , 2017 ; Wang et al . , 2017 ; Zhang and Ji ,   2021 ) , text summarization ( Liao et al . , 2018 ; Hardy   and Vlachos , 2018 ) and question answering ( Mitra   and Baral , 2016 ; Sachan and Xing , 2016 ) .   The powerful pretrained encoder - decoder mod-   els , e.g. , BART ( Lewis et al . , 2020 ) , have been   successfully adapted to the AMR parsing and be-   came the mainstream and state - of - the - art meth-   ods ( Bevilacqua et al . , 2021 ) . Through directly   generating the linearized AMR graph ( e.g. , Fig-   ure 1(a ) ) from the sentence , these sequence - to-   sequence methods ( Xu et al . , 2020b ; Bevilacqua   et al . , 2021 ) circumvent the complex data pro-   cessing pipeline and can be easily optimized com-   pared with transition - based or graph - based meth-   ods ( Naseem et al . , 2019 ; Lee et al . , 2020 ; Lyu and   Titov , 2018 ; Zhang et al . , 2019a , b ; Cai and Lam ,   2020 ; Zhou et al . , 2021b ) . However , there exists   a gap between the Ô¨Çat sentence - to - AMR training   objectiveand AMR graphs , since sequence - to-   sequence models deviate from the essence of graph   representation . Therefore , it is difÔ¨Åcult for sequen-   tial generators to learn the inherent hierarchical   structure of AMR ( Zhou et al . , 2021b ) .   Humans usually adapt to difÔ¨Åcult tasks by deal-   ing with examples gradually from easy to hard , i.e. ,   Curriculum Learning ( Bengio et al . , 2009 ; Platan-   ios et al . , 2019 ; Su et al . , 2021 ; Xu et al . , 2020a ) . In-   spired by human behavior , we propose a hierarchi-333   cal curriculum learning framework with two curric-   ular strategies to help the Ô¨Çat pretrained model pro-   gressively adapt to the hierarchical AMR graph . ( 1 )   Structure - level Curriculum ( SC ) . AMR graphs   are organized in a hierarchy where the core se-   mantic elements stay closely to the root node ( Cai   and Lam , 2019 ) . As depicted in Figure 1 , the con-   cepts and relations that locate in the different layers   of the AMR graph correspond to different levels   of abstraction in terms of the semantic represen-   tation . Motivated by the human learning process ,   i.e. ,core concepts Ô¨Årst , then details , SC enumer-   ates all AMR sub - graphs with different depths , and   deals with them in order from shallow to deep . ( 2 )   Instance - level Curriculum ( IC ) . Our preliminary   study in Figure 3 shows that the performance of   the vanilla BART baseline would drop rapidly as   the depth of AMR graph grows , which indicates   that handing deeper AMR hierarchy is more difÔ¨Å-   cult for pretrained models . Inspired by the human   cognition , i.e. , easy ones Ô¨Årst , then hard ones , we   propose IC which trains the model by starting from   easy instances with a shallower AMR structure and   then handling hard instances .   To sum up : ( 1 ) Inspired by the human learn-   ing process , i.e. , core concepts Ô¨Årst andeasy in - stances Ô¨Årst , we propose a hierarchical curriculum   learning ( HCL ) framework to help the sequence - to-   sequence model progressively adapt to the AMR   hierarchy . ( 2 ) Extensive experiments on AMR2.0 ,   AMR3.0 , structure - complex and out - of - distribution   situations verify the effectiveness of HCL .   2 Methodology   We formulate AMR parsing as a sequence - to-   sequence transformation . Given a sentence x=   ( x ; : : : ; x ) , the model aims to generate a lin-   earized AMR graph y= ( y ; : : : ; y ) . As shown   in Figure 1(a ) , following Bevilacqua et al . ( 2021 ) ,   the AMR graph is linearized by the DFS - based   linearization method with special tokens to indi-   cate variables and parentheses to mark visit depth .   SpeciÔ¨Åcally , variables of AMR nodes are set to a   series of special tokens < R0 > , ... , < Rk > ( more de-   tails of linearization are included in Appendix A ) .   In this paper , we propose a hierarchical curriculum   learning framework ( Figure 2 ) with the structure-   and instance - level curricula to help the Ô¨Çat model   progressively adapt to the structured AMR graph .   2.1 Structure - level Curriculum   Motivated by learning core concepts Ô¨Årst , we pro-   pose Structure - level Curriculum ( SC ) . AMR graphs   are organized in a hierarchy where the core seman-   tics stay closely to the root ( Cai and Lam , 2019 ) ,   thus SC divides all AMR sub - graphs into Nbuck-   ets according to their depths fS : i= 1;2 ; : : : ; Ng ,   where Scontains AMR sub - graphs with the depth   i. As shown in Figure 2(a ) , SC has Ntraining   episodes , and each episode consists of Tsteps .   In each step of the i - th episode , the training sched-   uler samples a batch of examples from buckets   fS : jigto train the model . When parsing334   a sentence into a sub - graph with the depth d , we   append a special string ‚Äú parse to dlayers ‚Äù to the   input sentence , and replace the start token of the   decoder with an artiÔ¨Åcial token < d > , so the model   can perceive layers that need to be parsed .   2.2 Instance - level Curriculum   Inspired by learning easy instances Ô¨Årst , we pro-   pose Instance - level Curriculum ( IC ) . Figure 3   shows AMR graphs with deeper layers can be re-   garded as harder instances for the Ô¨Çat pretrained   model , thus IC divides all AMR graphs into M   buckets according to their depths fI : i=   1 ; : : : ; Mg , where Icontains AMR graphs with the   depth i. As shown in Figure 2(b ) , IC has Mtrain-   ing episodes , and each episode consists of Tsteps .   In each step of the i - th episode , the training sched-   uler samples a batch of examples from buckets   fI : jigto train the model . SpeciÔ¨Åcally , we   Ô¨Årst use SC and then IC to train the model , since SC   ( follows learning core semantics Ô¨Årst ) is for AMR   sub - graphs , which can be regarded as a warming-   up stage of IC ( obeys learning easy instances Ô¨Årst ) ,   which is for AMR full graphs .   3 Experiments   Datasets and Evaluation Metrics We evalu-   ate our hierarchical curriculum learning frame-   work on two popular AMR benchmarks , AMR2.0   ( LDC2017T10 ) and AMR3.0 ( LDC2020T02 ) .   Please refer to the Appendix B for details of two   benchmarks . Following Bevilacqua et al . ( 2021 ) ,   we use the scores ( Cai and Knight , 2013)and the Ô¨Åne - grained evaluation metrics ( Damonte   et al . , 2017)to evaluate the performances .   Experiment Setups Our implementation is   based on Huggingface ‚Äôs transformers library ( Wolf   et al . , 2020 ) and the open codebase of Bevilac-   qua et al . ( 2021 ) . We use BART - large as our   sequence - to - sequence model the same as Bevilac-   qua et al . ( 2021 ) . We utilizes RAdam ( Liu et al . ,   2020 ) as our optimizer with the learning rate 3e-   5 . The batch size is 2048 graph linearization to-   kens with the gradient accumulation 10 . Dropout   is set to 0:25and beam size is 5 . The train-   ing steps Tis1000 andTis500 . After the   curriculum training , the model is trained for 30   epochs on the training set . We use cross - entropy   as our loss function . We train our model on a   single NVIDIA TESLA V 100GPU with 32 GB   memory . We adopt the same post - processing pro-   cess as Bevilacqua et al . ( 2021 ) . Our code and   model are available at https://github.com/   Wangpeiyi9979 / HCL - Text2AMR .   Main Results We compare our method with pre-   vious approaches in Table 1 . As is shown , on   AMR2.0 and AMR3.0 , our hierarchical curriculum   learning model achieves 84:30:1and83:70:1 scores , and outperforms Bevilacqua et al .   ( 2021 ) 0:5and 0:7 scores , respectively .   For the Ô¨Åne - grained results , our model achieves   the best performance in 6out of 8metrics on both   AMR2.0 and AMR3.0 , which shows the effective-335   ness of our method . Although Cai and Lam ( 2020 )   outperforms our model in Neg . and Wiki . on   AMR2.0 , they adopt a complex process , which   may hurt the model generalization ability . Bevilac-   qua et al . ( 2021 ) outperforms slightly our model   in Conc . and Wiki . on AMR3.0 . However , these   metrics are unrelated to the AMR structure that our   HCL focuses on .   4 Analysis   Structure BeneÔ¨Åt In order to explore the ef-   fectiveness of our HCL framework for the struc-   tured AMR parsing . We divide the Ô¨Åne - grained   F1 scores into 2categories , ‚Äú structure - dependent ‚Äù   ( unlabelled , re - entrancy and SRL ) and ‚Äú structure-   independen ‚Äù ( the left 5metrics ) . Please refer   to Appendix C for the reason for this division .   As shown in Table 1 , compared with Bevilac-   qua et al . ( 2021 ) ( also a sequence - to - sequence   model based on BART - large ) , our method achieves   2:97and 2:83average F1 scores improvement on   3structure - dependent metrics on AMR2.0 and   AMR3.0 , respectively , which proves HCL helps   the Ô¨Çat sequence - to - sequence model better adapt to   AMR with the hierarchical and complex structure .   Hard Instances BeneÔ¨Åt Figure 4 shows the per-   formances of our HCL and Bevilacqua et al . ( 2021 )   ( SPRING ) at different layers . As is shown , as the   number of layers increases , HCL exceeds SPRING   greater , which shows our HCL helps the model   better handle hard instances . In addition , to some   extend , out - of - distribution ( OOD ) instances can be   regarded as hard instances , thus we also consider   the OOD situation . Bevilacqua et al . ( 2021 ) pro-   pose the OOD evaluation for AMR parsers . Follow-   ing Bevilacqua et al . ( 2021 ) , we train our model on   the training dataset of AMR2.0 , and then evaluate   it on 3 OOD test datasets , BIO , TLP and News3 .   Please refer to Appendix B for details of OOD   datasets . As shown in Table 3 , our method out-   performs Bevilacqua et al . ( 2021 ) on all 3 OOD   datasets , which shows our HCL framework can also   improve the generalization ability of the model .   Ablation Study To illustrate the effect of our pro-   posed curricula . We conduct ablation studies by   removing one curriculum at a time . Table 2 shows   the scores on both AMR2.0 and AMR3.0 .   As shown in Table 2 , we can see both curricula are   conducive to the performance of the model , and   they are complementary to each other . SpeciÔ¨Åcally ,   the structure - level curriculum ( SC ) is more effec-   tive than the instance - level curriculum ( IC ) . We   think the reason is that SC constructs AMR sub-   graphs for training , which enhances the model ‚Äôs   ability to perceive the AMR hierarchy .   5 Conclusion   In this paper , we propose a Hierarchical Curricu-   lum Learning ( HCL ) framework for sequence-   to - sequence AMR parsing , which consists of   Structure - level Curriculum ( SC ) and Instance - level   Curriculum ( IC ) . inspired by human cognition , SC   follows the principle of learning the core concepts   of AMR Ô¨Årst , and IC obeys the rule of learning   easy instances Ô¨Årst . SC and IC train the model on   different hierarchies ( AMR sub - graphs and AMR   full graphs ) . Extensive experiments on AMR2.0 ,   AMR3.0 , structure - complex and out - of - distribution   situations verify the effectiveness of HCL.336Acknowledgement   The authors would like to thank the anonymous re-   viewers for their thoughtful and constructive com-   ments , and Bevilacqua et al . ( 2021 ) for their high-   quality open codebase . This paper is supported by   the National Key Research and Development Pro-   gram of China under Grant No . 2020AAA0106700 ,   the National Science Foundation of China under   Grant No.61936012 and 61876004 , and NSFC   project U19A2065 .   References337338A Linearzation   As shown in Figure 5 , following Bevilacqua et al .   ( 2021 ) , the AMR graph is linearized by the DFS-   based linearization method according to the edge   order ( ‚Äò : ARG0‚Äô!‚Äò:ARG1‚Äô!‚Äò:ARG2 ‚Äô ) . Variables   of the AMR graph are set to a series of special   tokens < R0 > , < R1 > , < R2 > , < R3 > , < R4 > , and   the depth is marked by parentheses .   B Datasets   B.1 In - domain Distribution   AMR2.0 ( LDC2017T10 ) contains 36;521 ,   1;368and 1;371sentence - AMR pairs in training ,   development and testing sets , respectively .   AMR3.0 ( LDC2020T02 ) is larger than AMR2.0   in size , which contains 55;635,1;722and 1;898   sentence - AMR pairs for training development and   testing set , respectively . AMR3.0 is a superset of   AMR2.0 .   B.2 Out - domain Distribution   BIO is a test set of the Bio - AMR corpus , consist-   ing of 500instances .   TLP is a AMR dataset annoated on the children ‚Äôs   novel The Little Prince ( version 3.0 ) , consisting of   1;562instances .   New3 is a sub - set of AMR3.0 , which is not in-   cluded in the AMR2.0 training set , consisting of   527instances .   C Fine - grained Metric Division   There are 8Ô¨Åne - grained AMR metrics : ( 1 ) Unla-   beled : Smatch score computed on the predicted   graphs after removing all edge labels . ( 2 ) No   WSD . : Smatch score while ignoring Propbanksenses ( e.g. , duck-01 vs duck-02 ) . ( 3 ) Named Ent . :   F - score on the named entity recognition (: name   roles ) . ( 4 ) WikiÔ¨Åcation : F - score on the wikiÔ¨Å-   cation (: wiki roles ) . ( 5 ) Negation : F - score on   the negation detection (: polarity roles ) . ( 6 ) Con-   cepts : F - score on the concept identiÔ¨Åcation task .   ( 7)Reentrancy : Smatch computed on reentrant   edges only , e.g. , the edges of node ‚Äò I ‚Äô in Figure A.   ( 8)SRL : Smatch computed on : ARG - i roles only .   We only regard Unlabeled , Reentrancy and SRL   as ‚Äú structure - dependent ‚Äù metrics , since : ( 1 ) Unla-   beled does not consider any edge labels , and only   considers the graph structure . ( 2 ) Reentrancy is a   typical structure feature for the AMR graph . With-   out reentrant edges , the AMR graph is reduced to a   tree . ( 3 ) SRL denotes the core - semantic relation of   the AMR , which determines the core structure of   the AMR . ( 4 ) As described above , all other metrics   have little relationship with the structure .   D Case Study   Figure 6 shows a case study ( we omit some de-   tails of AMR graphs for a more clear description ) .   As is illustrated , our method achieves the right   AMR for the input sentence . However , the AMR   parsed by the SPRING model ( depth:5 ) is shal-   lower than the gold AMR ( depth:9 ) , and their struc-   tures are also different ( e.g. , the root of the gold   AMR and the SPRING parsed AMR are ‚Äò possible-   01 ‚Äô and ‚Äò and ‚Äô , respectively ) . This case intuitively   shows our HCL framework can help the model   better handle the hard instance with complex struc-   ture.339   Vipul RathoreKartikeya BadolaParag Singla Mausam   Indian Institute of Technology   New Delhi , India   rathorevipul28@gmail.com , kartikeya.badola@gmail.com   parags@cse.iitd.ac.in , mausam@cse.iitd.ac.in   Abstract   1 Introduction   Given some text ( typically , a sentence ) tmention-   ing an entity pair ( e;e ) , the goal of relation ex-   traction ( RE ) is to predict the relationships between   eandethat can be inferred from t. LetB(e;e )   denote the set of all sentences ( bag ) in the cor-   pus mentioning eandeand letR(e;e)denote   all relations from etoein a KB . Distant su-   pervision ( DS ) trains RE models given B(e;e )   andR(e;e ) , without sentence level annotation   ( Mintz et al . , 2009 ) . Most DS - RE models use   the ‚Äú at - least one ‚Äù assumption : 8r2R(e;e ) ,   9t2B(e;e)such thattexpresses ( e;r;e ) .   Recent neural approaches to DS - RE encode each   sentencet2B(e;e)and then aggregate sen-   tence embeddings using an aggregation operator   ‚Äì the common operator being intra - bag attention   ( Lin et al . , 2016 ) . Various models differ in their   approach to encoding ( e.g. , PCNNs , GCNs , BERT)and their loss functions ( e.g. , contrastive learn-   ing , MLM ) , but agree on the design choice of en-   coding each sentence independently of the others   ( Vashishth et al . , 2018 ; Alt et al . , 2019 ; Christou   and Tsoumakas , 2021 ; Chen et al . , 2021 ) . We posit   that this choice leads to a suboptimal usage of the   available data ‚Äì information from other sentences   might help in better encoding a given sentence .   We explore this hypothesis by developing a sim-   ple baseline solution . We Ô¨Årst construct a pas-   sageP(e;e)by concatenating all sentences in   B(e;e ) . We then encode the whole passage   through BERT ( Devlin et al . , 2019 ) ( or mBERT   for multilingual setting ) . This produces a contex-   tualized embedding of every token in the bag . To   make these embeddings aware of the candidate re-   lation , we take a ( trained ) relation query vector , r ,   to generate a relation - aware summary of the whole   passage using attention . This is then used to predict   whether ( e;r;e)is a valid prediction .   Despite its simplicity , our baseline has some con-   ceptual advantages . First , each token is able to ex-   change information with other tokens from other   sentences in the bag ‚Äì so the embeddings are likely   more informed . Second , in principle , the model   may be able to relax a part of the at - least - one as-   sumption . For example , if no sentence individually   expresses a relation , but if multiple facts in differ-   ent sentences collectively predict the relation , our   model may be able to learn to extract that .   We name our baseline model Passage - Attended   Relation Extraction , PARE ( mPARE for multi-   lingual DS - RE ) . We experiment on four DS - RE   datasets ‚Äì three in English , NYT-10d ( Riedel et al . ,   2010 ) , NYT-10 m , and Wiki-20 m ( Gao et al . , 2021 ) ,   and one multilingual , DiS - ReX ( Bhartiya et al . ,   2022 ) . We Ô¨Ånd that in all four datasets , our pro-   posed baseline signiÔ¨Åcantly outperforms existing   state of the art , yielding up to 5 point AUC gain .   Further attention analysis and ablations provide   additional insight into model performance . We re-340   lease our code for reproducibility . We believe that   our work represents a simple but strong baseline   that can form the basis for further DS - RE research .   2 Related Work   Monolingual DS - RE : Early works in DS - RE   build probabilistic graphical models for the task   ( e.g. , ( Hoffmann et al . , 2011 ; Ritter et al . , 2013 ) .   Most later works follow the multi - instance multi-   label learning framework ( Surdeanu et al . , 2012 )   in which there are multiple labels associated with   a bag , and the model is trained with at - least - one   assumption . Most neural models for the task en-   code each sentence separately , e.g. , using Piece-   wise CNN ( Zeng et al . , 2015 ) , Graph Convolu-   tion Net ( e.g. , RESIDE ( Vashishth et al . , 2018 ) ) ,   GPT ( DISTRE ( Alt et al . , 2019 ) ) and BERT ( RED-   SandT ( Christou and Tsoumakas , 2021 ) , CIL(Chen   et al . , 2021 ) ) . They all aggregate embeddings us-   ing intra - bag attention ( Lin et al . , 2016 ) . Beyond   Binary Cross Entropy , additional loss terms in-   clude masked language model pre - training ( DIS-   TRE , CIL ) , RL loss ( Qin et al . , 2018 ) , and auxiliary   contrastive learning ( CIL ) . We show that PARE   is competitive with DISTRE , RESIDE , CIL , and   other natural baselines , without using additional   pre - training , side information or auxiliary losses   during training , unlike some comparison models .   To evaluate DS - RE , at test time , the model   makes a prediction for an unseen bag . Unfortu-   nately , most popular DS - RE dataset ( NYT-10d )   has a noisy test set , as it is automatically annotated   ( Riedel et al . , 2010 ) . Recently Gao et al . ( 2021 )   has released NYT-10 m and Wiki-20 m , which have   manually annotated test sets . We use all three   datasets in our work .   Multilingual DS - RE : A bilingual DS - RE model   named MNRE ( tested on English and Mandarin )   introduced cross - lingual attention in language-   speciÔ¨Åc CNN encoders ( Lin et al . , 2017 ) . Re-   cently , Bhartiya et al . ( 2022 ) has released a dataset , DiS - ReX , for four languages ‚Äì English , Spanish ,   German and French . We compare mPARE against   the state of the art on DiS - ReX , which combines   MNRE architecture with mBERT encoder . See   Appendix E for details on all DS - RE models .   Passage Construction from Bag of Sentences :   At a high level , our proposed model builds a pas-   sage by combining the sentences in a bag that   mentions a given entity pair . This idea of passage   construction is related with the work of Yan et al .   ( 2020 ) , but with important differences , both in task   deÔ¨Ånitions and neural models . First , they focus on   predicting the tail entity of a given query ( e;r ; ? ) ,   whereas our goal is relation prediction given an   entity pair . There are several model differences   such as in curating a passage , in use of trainable   query vectors for relations , in passage construc-   tion strategy , etc . Importantly , their architecture   expects a natural language question for each candi-   date relation ‚Äì not only this requires an additional   per - relation annotation ( that might not be feasible   for datasets having too many relations in the ontol-   ogy ) , but also , it makes their method slower , since   separate forward passes are needed per relation .   3 Passage Attended Relation Extraction   PARE explores the value of cross - sentence atten-   tion during encoding time . It uses a sequence of   three key steps : passage construction , encoding   and summarization , followed by prediction . Figure   1 illustrates these for a three - sentence bag .   Passage Construction constructs a passage   P(e;e)from sentences t2B(e;e ) . The con-   struction process uses a sequential sampling of sen-   tences in the bag without replacement . It terminates   if ( a ) adding any new sentence would exceed the   maximum number of tokens allowed by the en-   coder ( 512 tokens for BERT ) , or ( b ) all sentences   from the bag have been sampled .   Passage Encoding takes the constructed passage   and sends it to an encoder ( BERT or mBERT ) to   generate contextualized embeddings zof every341   tokenwin the passage . For this , it Ô¨Årst creates   an encoder input . The input starts with the [ CLS ]   token , followed by each passage sentence sepa-   rated by [ SEP ] , and pads all remaining tokens with   [ PAD ] . Moreover , following best - practices in RE   ( Han et al . , 2019 ) , each mention of eandein the   passage are surrounded by special entity marker   tokens < e1>,</e1 > , and < e2>,</e2 > , respectively .   Passage Summarization maintains a ( randomly-   initialized ) query vector rfor every relation r. It   then computes  , the normalized attention of r   on each token w , using dot - product attention . Fi-   nally , it computes a relation - attended summary of   the whole passage z = P  z , where   Lis the input length . We note that this summa-   tion also aggregates embeddings of [ CLS ] , [ SEP ] ,   [ PAD ] , as well as entity marker tokens .   Tuple ClassiÔ¨Åer passes zthrough an MLP   followed by Sigmoid activation to return the prob-   abilitypof the triple ( e;r;e ) . This MLP is   shared across all relation classes . At inference , a   positive prediction is made if p > threshold ( 0.5 ) .   Loss Function is simply Binary Cross Entropy be-   tween gold and predicted label set for each bag . No   additional loss terms are used .   4 Experiments and AnalysisWe compare PARE andmPARE against the state   of the art models on the respective datasets . Wealso perform ablations and analyses to understand   model behavior and reasons for its performance .   Datasets and Evaluation Metrics : We evaluate   PARE on three English datasets : NYT-10d , NYT-   10 m , Wiki-20 m. mPARE is compared using the   DiS - ReX benchmark . Data statistics are in Table 2 ,   with more details in Appendix C. We use the evalu-   ation metrics prevalent in literature for each dataset .   These include AUC : area under the precision - recall   curve , M - F1 : macro - F1 , -F1 : micro - F1 , and   P@M : average of P@100 , P@200 and P@300 ,   where P@k denotes precision calculated over a   model‚Äôskmost conÔ¨Ådently predicted triples .   Comparison Models and Hyperparameters :   Since there is substantial body of work on NYT-   10d , we compare against several recent models :   RESIDE , DISTRE , REDSandT and the latest state   of the art , CIL . For NYT-10 m and Wiki-20 m , we   report comparisons against models in the original   paper ( Gao et al . , 2021 ) , and also additionally run   CIL for a stronger comparison . For DiS - ReX , we   compare against mBERT based models . See Ap-   pendix E for more details on the baseline models .   ForPARE andmPARE , we use base - uncased check-   points for BERT and mBERT , respectively . Hyper-   parameters are set based on a simple grid search   over devsets . ( see Appendix A).342   4.1 Comparisons against State of the Art   The results are presented in Table 1 , in which , the   best numbers are highlighted and second best num-   bers are underlined . On NYT-10d ( Table 1(a ) ) ,   PARE has 2.6 pt AUC improvement over CIL , the   current state of the art , while achieving slightly   lower P@M. This is also reÔ¨Çected in the P - R curve   ( Figure 2 ) , where in the beginning our P - R curve   is slightly on the lower side of CIL , but overtakes   it for higher threshold values of recall . Our model   beats REDSandT by 11 AUC pts , even though both   use BERT , and latter uses extra side - information   ( e.g. , entity - type , sub - tree parse ) .   On manually annotated testsets ( Table 1(b ) ) ,   PARE achieves up to 2.8 pt AUC and 2.1 pt macro-   F1 gains against CIL . We note that Gao et al .   ( 2021 ) only published numbers on simpler base-   lines ( BERT followed by attention , average and   max aggregators , the details for which can be found   in Appendix E ) , which are substantially outper-   formed by PARE .CIL ‚Äôs better performance is likely   attributed to its contrastive learning objective ‚Äì it   will be interesting to study this in the context of   PARE .   For multilingual DS - RE ( Table 1(c ) ) , mPARE   obtains a 4.9 pt AUC gain against mBERT+MNRE .   P - R curve in Figure 3 shows that it convincingly   outperforms others across the entire domain of re-   call values . We provide language - wise and relation-   wise metrics in Appendix L ‚Äì the gains are consis-   tent on all languages and nearly all relations .   4.2 Analysis and Ablations   Generalizing to Unseen KB : Recently , Ribeiro   et al . ( 2020 ) has proposed a robustness study in   which entity names in a bag are replaced by other   names ( from the same type ) to test whether the   extractor is indeed reading the text , or is simply   overÔ¨Åtting on the regularities of the given KB . We   also implement a similar robustness study ( details   in Appendix K ) , where entity replacement results in   an entity - pair bag that does not exist in the original   KB . We Ô¨Ånd that on this modiÔ¨Åed NYT-10 m , all   models suffer a drop in performance , suggesting   that models are not as robust as we intend them   to be . We , however , note that CILsuffers a 28.1 %   drop in AUC performance , but PARE remains more   robust with only a 16.8 % drop . We hypothesize   that this may be because of PARE ‚Äôs design choice   of attending on all words for a given relation , which   could reduce its focus on entity names themselves .   Scaling with Size of Entity - Pair Bags : Due to   truncation when the number of tokens in a bag   exceed 512 ( limit for BERT ) , one would assume   thatPARE may not be suited for cases where the   number of tokens in a bag is large . To study this ,   we divide the test set of NYT-10 m into 6 differ-   ent bins based on the number of tokens present   in the untruncated passage ( details on the experi-   ment in Appendix J ) . We present results in Figure   4 . We Ô¨Ånd that PARE shows consistent gains of   around 2 to 3 pt in AUC against CILfor all groups   except the smallest group . This is not surprising ,   since for smallest group , there is likely only one   sentence in a bag , and PARE would not gain from   inter - sentence attention . For large bags , relevant   information is likely already present in truncated   passage , due to redundancy .   Attention Patterns : InPARE , each relation class   has a trainable query vector , which attends on ev-   ery token . The attention scores could give us some   insight about the words the model is focusing on .   We observe that for a candidate relation that is not   a gold label for a particular bag , surprisingly , the   highest attention scores are obtained by [ PAD ] to-   kens . In fact , for such bags , on an average , roughly   90 % of the attention weight goes to [ PAD ] tokens ,   whereas this number is only 0.1 % when the rela-   tion is in the gold set ( see Appendices H and I ) .   We Ô¨Ånd this to be an example of model ingenu-343ModiÔ¨Åcation Change in AUC   w/o passage summarization -4.9   w/o [ PAD ] attention -3.1   w/o entity markers -36.9   ity ‚Äì PARE seems to have creatively learned that   whenever the most appropriate words for a relation   are not present , it could simply attend on [ PAD ]   embeddings , which may lead to similar attended   summaries , which may be easily decoded to a low   probability of tuple validity . In fact , as a further   test , we perform an ablation where we disallow rela-   tion query vectors to attend on [ PAD ] tokens ‚Äì this   results in an over 3 pt drop in AUC on NYT-10d ,   indicating the importance of padding for prediction   ( see Table 3 ) .   Ablations : We perform further ablations of the   model by removing entity markers and removing   the relation - attention step that computes a summary   ( instead using [ CLS ] token for predicting each re-   lation ) . PARE loses signiÔ¨Åcantly in performance in   each ablation obtaining 16.5 and 48.5 AUC , respec-   tively ( as against 53.4 for full model ) on NYT-10d   ( table 3 ) . The critical importance of entity mark-   ers is not surprising , since without them the model   does not know what is the entity - pair it is predict-   ing for . We also notice a very signiÔ¨Åcant gain due   to relation attention and passage summarization ,   suggesting that this is an important step for the   model ‚Äì it allows focus on speciÔ¨Åc words relevant   for predicting a relation . We perform the same ex-   periments on the remaining datasets and observe   similar results ( Appendix G ) .   Effect of Sentence Order : We build 20 random   passages per bag ( by varying sentence order and   also which sentences get selected if passage needs   truncation ) . On all four datasets ( Appendix M ) ,   we Ô¨Ånd that the standard deviation to be negligi-   ble . This analysis highlights 1 ) the sentence - order   invariance of PARE ‚Äôs performance and 2 ) In practi-   cal settings , the randomly sampled sentences with   token limit of 512 in the passage is good enough to   make accurate bag - level predictions .   5 Conclusion and Future Work   We introduce PARE , a simple baseline for the task   of distantly supervised relation extraction . Ourexperiments demonstrate that this simple baseline   produces very strong results for the task , and out-   performs existing top models by varying margins   across four datasets in monolingual and multilin-   gual settings . Several experiments for studying   model behavior show its consistent performance   that generalizes across settings . We posit that our   framework would serve as a strong backbone for   further research in the Ô¨Åeld of DS - RE .   There are several directions to develop the PARE   architecture further . E.g. , PARE initializes relation   embeddings randomly and also constructs passage   via random sampling . Alternatively , one could   make use of label descriptions and aliases from   Wikidata to initialize label query vectors ; one could   also use a sampling strategy to Ô¨Ålter away noisy   sentences ( e.g. a sentence selector ( Qin et al . , 2018 )   module integrated with PARE ) . In the multilingual   setting , contextualized embeddings of entity men-   tions in a passage may be aligned using constrained   learning techniques ( Mehta et al . , 2018 ; Nandwani   et al . , 2019 ) to learn potentially better token embed-   dings . Constraints can be imposed on the label hier-   archy as well ( E.g. PresidentOf)CitizenOf , etc . )   since label query vectors operate independently of   each other on the passage in PARE . Additionally ,   translation - based approaches at training or infer-   ence ( Nag et al . , 2021 ; Kolluru et al . , 2022 ) could   improve mPARE performance . Recent ideas of   joint entity and relation alignment in multilingual   KBs ( Singh et al . , 2021 ) may be combined along   with mPARE ‚Äôs relation extraction capabilities .   Acknowledgements   This work is primarily supported by a grant from   Huwaei . Mausam is supported by grants from   Google and Jai Gupta Chair Professorship . Parag   was supported by the DARPA Explainable ArtiÔ¨Å-   cial Intelligence ( XAI ) Program ( # N66001 - 17 - 2-   4032 ) . Mausam and Parag are supported by IBM   SUR awards . Vipul is supported by Prime Minis-   ter ‚Äôs Research Fellowship ( PMRF ) . We thank IIT   Delhi HPC facility for compute resources . We   thank Abhyuday Bhartiya for helping in reproduc-   ing results from the DiS - ReX paper , and Keshav   Kolluru for helpful comments on an earlier draft of   the paper . Any opinions , Ô¨Åndings , conclusions or   recommendations expressed here are those of the   authors and do not necessarily reÔ¨Çect the views or   ofÔ¨Åcial policies , either expressed or implied , of the   funding agencies.344References345346A Experimental Settings   We train and test our model on two NVIDIA GeForce GTX 1080 Ti cards . We use a linear LR scheduler   having weight decay of 1e-5 with AdamW ( Loshchilov and Hutter , 2019 ; Kingma and Ba , 2015 ) as the   optimizer . Our implementation uses PyTorch ( Paszke et al . , 2019 ) , the Transformers library ( Wolf et al . ,   2020 ) and OpenNRE(Han et al . , 2019 ) . We use bert - base - uncased checkpoint for BERT initialization in   the mono - lingual setting . For multi - lingual setting , we use bert - base - multilingual - uncased .   For hyperparameter tuning , we perform grid search over { 1e-5 , 2e-5 } for learning rate and { 16 , 32 , 64 }   for batch size and select the best performing conÔ¨Åguration for each dataset .   PARE takes 2 epochs to converge on NYT-10d ( 152 mins / epoch ) , 3 epochs for NYT-10 m ( 138   mins / epoch ) , 2 epochs for Wiki-20 m ( 166 mins / epoch ) and 4 epochs for DiS - ReX ( 220 mins / epoch ) .   The numbers we report for the baselines come from their respective papers . We obtained the code base   of CIL , BERT+Att , BERT+Avg , BERT+One from their respective authors , so that we could run them on   additional datasets . We were able to replicate same numbers as reported in their papers . We trained those   models on other datasets as well by carefully tuning the bag size hyperparameter .   B Sizes of different models   We report the number of additional trainable parameters , in each model , on top of the underlying   BERT / mBERT encoder ( all models except MNRE use the bert - base - uncased checkpoint , whereas MNRE   uses the bert - base - multilingual - uncased checkpoint ) in table 4 . We note that the key reason why PARE   has signiÔ¨Åcantly lower number of additional parameters ( on top of the BERT / mBERT encoder ) is because   all the other models use entity pooling ( Soares et al . , 2019 ) for constructing instance representations . The   entity pooling operation requires an additional fully - connected layer which projects the concatenated   encoded representations of head and tail entity in an input instance to a vector of the same size ( for   BERT / mBERT , this results in additional ( 2768)weight and 2768bias parameters ) .   Model # Parameters ( excluding BERT )   Att 2400793   One 2399257   Avg 2399257   CIL 2453052   MNRE 2645029   PARE 46082   C Dataset Details   We evaluate our proposed model on four different datasets : NYT-10d ( Riedel et al . , 2010 ) , NYT-10 m   ( Gao et al . , 2021 ) , Wiki-20 m ( Gao et al . , 2021 ) and DiS - ReX ( Bhartiya et al . , 2022 ) . The statistics for   each of the datasets is present in table 2 .   NYT-10d   NYT-10d is the most popular dataset for monolingual DS - RE , constructed by aligning Freebase entities to   the New York Times Corpus . The train and test splits are both distantly supervised .   NYT-10 m   NYT-10 m is a recently released dataset to train and evaluate models for monolingual DS - RE . The dataset   is built from the same New York Times Corpus and the Freebase KB but with a new relation ontology and   a manually annotated test set . It aims to tackle the existing problems with the NYT-10d dataset by 1)347establishing a public validation set 2 ) establishing consistency among the relation classes present in the   train and test set 3 ) providing a high quality , manually labeled test set .   Wiki-20 m   Wiki-20 m is also a recently released dataset for training DS - RE models and evaluating them on manually   annotated a test set . The test set in this case corresponds to the Wiki80 dataset ( Han et al . , 2019 ) . The   relation ontology of Wiki80 is used to re - structure the Wiki20 DS - RE dataset ( Han et al . , 2020 ) , from   which the training and validation splits are created . It is made sure that their is no overlap between the   instances present in the testing and the training and validation sets .   DiS - ReX   DiS - ReX is a recently released benchmarking dataset for training and evaluating DS - RE models on   instances spanning multiple languages . The entities present in this dataset are linked across the different   languages which means that a bag can contain sentences from more than one languages . We use the   publicly available train , validation and test splits and there is no overlap between the bags present in any   two different dataset splits .   We obtain the Ô¨Årst three datasets from OpenNRE and DiS - ReX from their ofÔ¨Åcial repository .   D Description of Intra - Bag attention   Lett;t;:::;tdenoteninstances sampled from B(e;e ) . In all models using intra - bag attention for   instance - aggregation , each tis independently encoded to form the instance representation , E(t ) , follow-   ing which the relation triple representation Bfor the triple ( e;e;r)is given byB = P  E(t ) .   Hereris any one of the relation classes present in the dataset and  is the normalized attention score   allotted to instance representation E(t)by relation query vector  ! rfor relationr . The model then predicts   whether the relation triple is a valid one by sending each Bthrough a feed - forward neural network .   In some variants,  ! ris replaced with a shared query vector for all relation - classes,  ! q , resulting in a   bag - representation Bcorresponding to ( e;e)as opposed to triple - representation .   E Baselines   The details for each baseline is provided below :   PCNN - Att   Lin et al . ( 2016 ) proposed the intra - bag attention aggregation scheme in 2016 , obtaining the then   state - of - the - art performance on NYT-10d using a piecewise convolutional neural network ( PCNN ( Zeng   et al . , 2015 ) ) .   RESIDE   Vashishth et al . ( 2018 ) proposed RESIDE which uses side - information ( in the form of entity types and   relational aliases ) in addition to sentences present in the dataset . The model uses intra - bag attention   with a shared query vector to combine the representations of each instance in the bag . The sentence   representations are obtained using a Graph Convolutional Network ( GCN ) encoder .   DISTRE   Alt et al . ( 2019 ) propose the use of a pre - trained transformer based language model ( OpenAI GPT Radford   et al . ( 2018 ) ) for the task of DS - RE . The model uses intra - bag attention for the instance aggregation step .   REDSandT   Christou and Tsoumakas ( 2021 ) propose the use of a BERT encoder for DS - RE by using sub - tree parse of   the input sentence along with special entity type markers for the entity mentions in the text . The model   uses intra - bag attention for the instance aggregation step.348CIL   Chen et al . ( 2021 ) propose the use of Masked Language Modeling ( MLM ) and Contrastive Learning ( CL )   losses as auxilliary losses to train a BERT encoder + Intra - bag attention aggregator for the task .   BERT+Att / mBERT+Att   The model uses intra - bag attention aggregator on top of a BERT / mBERT encoder .   BERT+Avg / mBERT+Avg   The model uses ‚Äú Average ‚Äù aggregator which weighs each instance representation uniformly , hence   denoting bag - representation as the average of instance - representations .   BERT+One / mBERT+One   The model independently performs multi - label classiÔ¨Åcation on each instance present in the bag and then   aggregates the classiÔ¨Åcation results by performing class - wise max - pooling ( over sentence scores ) . In   essence , the ‚Äú One ‚Äù aggregator ends up picking one instance for each class ( the one which denotes the   highest conÔ¨Ådence for that particular class ) , hence the name .   mBERT+MNRE   The MNRE aggregator was originally introduced by Lin et al . ( 2017 ) and used with a shared mBERT   encoder by Bhartiya et al . ( 2022 ) . The model assigns a query vector for each ( relation;language )   tuple . A bag is divided into sub - bags where each sub - bag contains the instances of the same language . In   essence , a bag has Lsub - bags and each relation class corresponds to Lquery vectors , where Ldenotes   the number of languages present in the dataset . These are then used to construct Ltriple representations   ( using intra - bag attention aggregation on each ( sub - bag , query vector ) pair for a candidate relation ) which   are then scored independently . The Ô¨Ånal conÔ¨Ådence score for a triple is the average of Ltriple scores .   F Statistical SigniÔ¨Åcance   We compare the predictions of our model on the non - NA triples present in the test set with the predictions   of the second - best model using the McNemar ‚Äôs test of statistical signiÔ¨Åcance ( McNemar , 1947 ) . In all   cases , we obtained the p - value to be many orders of magnitude smaller than 0.05 , suggesting that the   improvement in results is statistically signiÔ¨Åcant in all cases .   G Ablation Study   ModiÔ¨Åcation NYT-10d NYT-10 m Wiki-20 m DiS - ReX   w/o passage summarization -4.9 -2.9 -4.2 -0.8   w/o [ PAD ] attention -3.1 -2.3 -1.9 -0.1   w/o entity markers -36.9 -16.5 -29.9 -20.5   We perform ablation studies on various datasets to understand which components are most beneÔ¨Åcial   for our proposed model . We provide the results in table 5 .   We observe that upon replacing our passage summarization step with multi - label classiÔ¨Åcation using   [ CLS ] token ( present at the start of the passage ) , we observe a signiÔ¨Åcant decrease in AUC , indicating that   contextual embedding of [ CLS ] token might not contain enough information for multi - label prediction of   bag.349For NYT-10 , it is interesting to note here that the AUC is still higher than that of REDSandT , a model   which uses BERT+Att as the backbone ( along with other complicated machinery ) . This means that one   can simply obtain an improvement in performance by creating a passage from multiple instances in a bag .   Removing entity markers resulted in the most signiÔ¨Åcant drop in performance . However , this is also   expected since without them , our model would have no way to understand which entities to consider while   performing relation extraction .   H Attention on [ PAD ] tokens   In the passage summarization step ( described in section 3 ) , we allow the relation query vector  ! rto   also attend over the encodings of the [ PAD ] tokens present in the passage . We make this architectural   choice in - order to provide some structure to the relation - speciÔ¨Åc summaries created by our model . If a   particular relation class ris not a valid relation for entity pair ( e;e ) , then ideally , we would want the   attended - summary of the passage P(e;e)created by the relation vector  ! rto represent some sort of a   null state ( since information speciÔ¨Åc to that relation class is not present in the passage ) . Allowing [ PAD ]   tokens to be a part of the attention would provide enough Ô¨Çexibility to the model to represent such a state .   We test our hypothesis by considering 1000 non - NA bags correctly labelled by our trained model in the   test set of NYT-10d . Let R(e;e)denote the set of valid relation - classes for entity pair ( e;e)and letR   denote all of the relation - classes present in the dataset . We Ô¨Årst calculate the percentage of attention given   to [ PAD ] tokens for a given passage P(e;e)for all relation - classes in R. The results are condensed into   two scores , sum of scores for R(e;e)and sum of scores for RnR(e;e ) . The results are aggregated   for all 1000 bags , and then averaged out by dividing with the total number of positive triples and negative   triples respectively . We obtain that on an average , only 0.07 % of attention weight is given to [ PAD ]   tokens by relation vectors corresponding to R(e;e ) , compared to 88.35 % attention weight given by   relation vectors corresponding to RnR(e;e ) . We obtain similar statistics on other datasets as well .   This suggests that for invalid triples , passage summaries generated by the model resemble the embeddings   of the [ PAD ] token . Furthermore , since we do n‚Äôt allow [ PAD ] tokens to be a part of self - attention update   inside BERT , the [ PAD ] embeddings at the output of the BERT encoder are not dependent on the passage ,   allowing for uniformity across all bags .   Finally , we train a model where we do n‚Äôt allow the relation query vectors to attend on the [ PAD ] token   embeddings and notice a 3.1pt drop in AUC on NYT-10d ( table 5 ) . We also note that the performance   is still signiÔ¨Åcantly higher than models such as REDSandT and DISTRE , suggesting that our instance   aggregation scheme still performs better than the baselines , even when not optimized fully .   I Examples of Attention Weighting during Passage Summarization   To understand how the query vector of a relation attends over passage tokens to correctly predict that   relation , we randomly selected from correctly predicted non - NA triples and selected the token obtaining   the highest attention score ( by the query vector for the correct relation ) . For the selection , we ignore the   stop words , special tokens and the entity mentions . The results are presented in table 6 .   J Performance vs Length of test passages   Our instance aggregation scheme truncates the passage if the number of tokens exceed the maximum   number of tokens allowed by the encoder . In such cases , one would assume that the our model is not   suited for cases where the number of instances present in a bag is very large . To test this hypothesis ,   we divide the non - NA bags , ( e;e ) , present in the NYT-10 m data into 6 bins based on the number of   tokens present in P(e;e)(after tokenized using BERT ) . We then compare the performance with CIL on   examples present in each bin . The results in Ô¨Ågure 4 indicate that a ) our model beats CIL in each bin - size   b ) the performance trend across different bins is the same for both models . This trend is continued even for   passages where the number of tokens present exceed the maximum number of tokens allowed for BERT   ( i.e. 512 ) . This results indicate that 512 tokens provide sufÔ¨Åcient information for correct classiÔ¨Åcation   of a triple . Moreover , models using intra - bag attention aggregation scheme Ô¨Åx the number of instances   sampled from the bag in practice . For CIL , the best performing conÔ¨Åguration uses a bag - size of 3 . This350   analysis therefore indicates that our model does n‚Äôt particularly suffer a drop in performance on large bags   when compared with other state - of - the - art models .   K Entity Permutation Test   To understand how robust our trained model would be to changes in the KB , we design the entity   permutation test ( inspired by Ribeiro et al . ( 2020 ) ) . An ideal DS - RE model should be able to correctly   predict the relationship between an entity pair by understanding the semantics of the text mentioning them .   Since DS - RE models under the multi - instance multi - label ( Surdeanu et al . , 2012 ) ( MI - ML ) setting are   evaluated on bag - level , it might be the case that such models are simply memorizing the KB on which   they are being trained on .   To test this hypothesis , we construct a new test set ( in fact , 5 such sets and report average over those 5 )   using NYT-10 m by augmenting its KB . Let B(e;e)denote a non - NA bag already existing in the test   set of the dataset . We augment this bag to correspond to a new entity - pair ( which is not present in the   combined KB of all three splits of this dataset ) . The augmentation can be of two different types : replacing   ewitheor replacing ewithe . We restrict such augmentations to the same type ( i.e the type of e   andeis same fori= 1;2 ) . For each non - NA entity pair in the test set of the dataset , we select one such   augmentation and appropriately modify each instance in B(e;e)to have the new entity mentions . We351note that since each instance in NYT-10 m is manually annotated and since our augmentation ensures   that the type signature is preserved , the transformation is label preserving . For the NA bags , we use the   ones already present in the original split . This entire transformation leaves us with an augmented test   set , having same number of NA and non - NA bags as the original split . The non - NA entity pairs are not   present in the KB on which the model is trained on .   L More Analysis on DiS - ReX   L.1 Relation - wise F1 scores   To show how our model performs on each relation label compared to other competitive baselines , we   present relation - wise F1 scores on DiS - ReX in table 7 .   L.2 Language - wise AUC scores   We compare the performance of our model compared to other baselines on every language in DiS - ReX.   For this , we partition the test data into language - wise test sets i.e. containing instances of only a particular   language . The results are presented in table 8 . We observe that the order of performance across languages   is consistent for all models including ours i.e. German < English < Spanish < French . Further we observe   that our model beats the second best model by an AUC ranging from 3 upto 4 points on all languages .   L.3 Do multilingual bags improve performance ?   To understand whether the currently available aggregation schemes ( including ours ) are able to beneÔ¨Åt   from multilingual bags or not , we conduct an experiment where we only perform inference on test - set bags   that contain instances from all four languages . In the multilingual case , the passage constructed during   thePassage Summarization step will contain multiple sentences of different languages . To understand   whether such an input allows improves ( or hampers ) the performance , we devise an experiment where   we perform inference by removing sentences from any one , two or three languages from the set of bags   containing instances of all four languages . There are roughly 1500 bags of such kind . Note that removing   anyklanguages ( k<= 3 ) would result in    different sets and we take average of AUC while reporting   the numbers . The results are presented in Ô¨Ågure 5 .   We observe that in all aggregation schemes , AUC increases with increase in number of languages   of a multilingual bag . mPARE consistently beats the other models in each scenario , indicating that the   encoding of a multilingual passage and attention - based summarization over multilingual tokens does n‚Äôt   hamper the performance of a DS - RE model with increasing no . of languages.352Relation mPARE mBERT - MNRE mBERT - Avg   http://dbpedia.org/ontology/birthPlace 77.5 75.3 74.9   http://dbpedia.org/ontology/associatedBand 77.9 70.9 74.7   http://dbpedia.org/ontology/director 88.4 83.2 85.5   http://dbpedia.org/ontology/country 88.4 86 85.2   http://dbpedia.org/ontology/deathPlace 71.0 67.3 65.5   http://dbpedia.org/ontology/nationality 70.4 67.7 68.7   http://dbpedia.org/ontology/location 74.2 70.5 67.5   http://dbpedia.org/ontology/related 78.9 75.5 73.2   http://dbpedia.org/ontology/isPartOf 74.8 68.6 64.7   http://dbpedia.org/ontology/inÔ¨ÇuencedBy 57.7 58.4 57.4   http://dbpedia.org/ontology/starring 87.5 86.1 83.9   http://dbpedia.org/ontology/headquarter 74.0 70.7 66.7   http://dbpedia.org/ontology/successor 74.2 71.8 71.3   http://dbpedia.org/ontology/bandMember 76.2 74.6 74.3   http://dbpedia.org/ontology/producer 56.7 53.6 48.5   http://dbpedia.org/ontology/recordLabel 90.5 86.9 86.1   http://dbpedia.org/ontology/city 83.2 78.8 77.6   http://dbpedia.org/ontology/inÔ¨Çuenced 56.3 61.9 51.5   http://dbpedia.org/ontology/author 81.6 78.2 80.5   http://dbpedia.org/ontology/team 84.8 82.5 78.6   http://dbpedia.org/ontology/formerBandMember 56.4 57.4 56.5   http://dbpedia.org/ontology/state 86.9 83.9 82.4   http://dbpedia.org/ontology/region 84.8 80.4 78.8   http://dbpedia.org/ontology/subsequentWork 74.1 72.4 69.6   http://dbpedia.org/ontology/department 96.4 95.4 95.5   http://dbpedia.org/ontology/locatedInArea 76.4 72.5 72.3   http://dbpedia.org/ontology/artist 80.8 77.2 78.6   http://dbpedia.org/ontology/hometown 78.8 73.6 73.7   http://dbpedia.org/ontology/province 82.1 79.2 78.2   http://dbpedia.org/ontology/riverMouth 77.2 72.4 71.9   http://dbpedia.org/ontology/locationCountry 66.9 62.5 64.2   http://dbpedia.org/ontology/predecessor 67.3 68.1 62   http://dbpedia.org/ontology/previousWork 68.6 69.6 65.5   http://dbpedia.org/ontology/capital 68.6 55.1 58   http://dbpedia.org/ontology/leaderName 78.4 70.4 63.3   http://dbpedia.org/ontology/largestCity 65.7 59.1 48.6   Model English French German Spanish   mPARE 83.2 86.8 81.7 85.3   mBERT - Avg 79.9 83.1 77.7 82.1   mBERT - MNRE 79.6 82.2 75.5 81.6353 M Negligible effect of random ordering   Since we order the sentences randomly into a passage to be encoded by BERT , this may potentially cause   some randomness in the results . However , we hypothesize that the BERT encoder must also be getting   Ô¨Åne - tuned to treat the bag as a set ( and not a sequence ) of sentences when being trained with random   ordering technique . And as a result , it ‚Äôs performance must be agnostic to the order of sentences it sees   in a passage during inference . To validate this , we perform 20 inference runs of our trained model with   different seeds i.e. the ordering of sentences is entirely random in each run . We measure mean and   standard deviation for each dataset as listed in table 9 . We observe negligible standard deviation in all   metrics . A minute variation in Macro - F1 or P@M metrics may be attributed to the fact that these are   macro - aggregated metrics and a variation in performance over some data points may also affect these to   some extent.354   Yiran Luo Pratyay Banerjee Tejas Gokhale Yezhou Yang Chitta Baral   Arizona State University , Tempe , AZ , USA   fyluo97 , pbanerj6 , tgokhale , yz.yang , chitta g@asu.edu   Abstract   1 Introduction   A newly released task called Person - centric Visual   Grounding ( Cui et al . , 2021 ) poses an interesting   angle into contextual reasoning in vision - language .   The task is motivated by humans ‚Äô reasoning ability .   Humans viewing an image with a caption as shown   in Figure 1 can reason ( and if needed , speculate )   which name refers to which person in the image .   This reasoning task involves multiple abilities , such   as perceiving characteristics and behaviors of peo-   ple , understanding their actions in context , specu-   lating about their intentions and effects human of   actions ( Fang et al . , 2020 ) , and connecting visually   perceived characteristics with grounded descrip-   tions in natural language ( Kazemzadeh et al . , 2014 ;   Yu et al . , 2016 ; Zellers et al . , 2019 ) . In many cases ,   this task can be performed without knowing the   names of the people ; for instance in the example on   the right , one person is signing and the other is not ,   as such it is possible to predict which person refers   to President and Secretary of State respectively .   However , in cases such as the example on the left ,   if all persons are performing the same action ( run-355ning on a track ) , then it is hard to match names with   these runners without any additional information .   Progress in the PCVG task can thus help better   capture what exact contextual cues are needed to   learn about a person ‚Äôs characteristics in a scenario ,   and can aid improvements in visual understanding   about human interactions and behaviors .   To support this task , Cui et al . ( 2021 ) offer   a large - scale dataset called Who ‚Äôs Waldo which   consists of 272 K annotated real life images . Ide-   ally , the dataset should consist of input - output pairs   ( such as the example on the right in Figure 1 ) which   are ‚Äò solvable ‚Äô as opposed to the one on the left   which is ambiguous . However , as we explore the   original Who ‚Äôs Waldo dataset , we encounter a great   portion of cases that resemble the left example in   Figure 1 , unsolvable data with insufÔ¨Åcient contex-   tual cues . Given such context , if we do not recog-   nize who exactly is in the picture , even we human   beings can not tell which name is who . We can then   only make predictions with biased assumptions ,   such as the Ô¨Årst named person would always be   on the leftmost , or the main subject would always   make up the largest area . Such biases in the orig-   inal dataset may explain why the heuristic meth-   ods perform very strongly , outperforming random   guessing by a big 27 % increase in test accuracy and   trailing the top benchmark only by 6 % . We believe   a fair dataset should not encourage approaches to   adopt biases to such an extent , and thus the original   baseline model overestimates its performance .   Inspired by dataset debiasing works such as   VQA - CP ( Agrawal et al . , 2018 ) and GQA-   OOD ( Kervadec et al . , 2021 ) , we create a debi-   ased collection of 84 K annotated image - captions   out of the Who ‚Äôs Waldo dataset by Ô¨Åltering out all   biased data with insufÔ¨Åcient context . We evaluate   the quality of our new dataset by applying the orig-   inal heuristic methods as well as Who ‚Äôs Waldo ‚Äôs   benchmark model . Results show that our debiased   dataset greatly reduces the heuristic biases from   the original dataset and provides the PCVG task a   more practical baseline for future developments .   2 Related Work   Dataset Debiasing . We take many inspirations   from previous studies on uncurated datasets . A task   dataset if not curated properly could lead to meth-   ods that cheat their ways through without learning   generalized information . For example , VQAv2   ( Goyal et al . , 2017 ) addresses the imbalance be - tween language and images in VQAv1 ( Antol et al . ,   2015 ) which results in visual information being   ignored and inÔ¨Çated model performance . VQA-   CP ( Agrawal et al . , 2018 ) and GQA - OOD ( Ker-   vadec et al . , 2021 ) were designed to test model   performance if spurious correlations exist in the   training dataset . Cadene et al . ( 2019 ) ; Chen et al .   ( 2020a ) ; Gokhale et al . ( 2020 ) are bias - aware tech-   niques that mitigate dataset bias with modeling and   data augmentation . Ye and Kovashka ( 2021 ) intro-   duce exploits by matching repeated texts in ques-   tions and answers to achieve high scores in Visual   Commonsense Reasoning ( Zellers et al . , 2019 ) .   We also learn from various techniques to amend   priors , biases , or shortcuts in datasets . REPAIR   ( Li and Vasconcelos , 2019 ) uses resampling to Ô¨Åx   representation biases in image datasets . Dasgupta   et al . ( 2018 ) incorporate compositional information   into sentence embeddings for Natural Language   Inference . DQI ( Mishra et al . , 2020 ) offers quanti-   tative metrics to assess biases in automated dataset   creation in Natural Language Processing . Le Bras   et al . ( 2020 ) introduce adversarial measures to miti-   gate biases in various Natural Language Processing   and Computer Vision tasks .   Visual Grounding . The PCVG task adapts pre-   vious supervised Visual Grounding models as its   original baselines . The Visual Grounding task is de-   Ô¨Åned as locating speciÔ¨Åc objects in an image from   a textual description . First established by Karpathy   et al . ( 2014 ) , following researches have evolved   into extracting attention information such as works   by Deng et al . ( 2018 ) and Endo et al . ( 2017 ) . A   huge variation of datasets for Visual Grounding   have also been created , including Flicker30k ( Plum-   mer et al . , 2015 ) , Visual Genome ( Krishna et al . ,   2017 ) , and RefCOCO ( Yu et al . , 2016 ) .   Referring Expression Comprehension ( REC ) .   An active branch from Visual Grounding , the Re-   ferring Expression Comprehension task ( Rohrbach   et al . , 2016 ) is no longer restricted to object cate-   gories . Instead its goal is to relate a free region in   an image to a sentence description . Mattnet ( Yu   et al . , 2018 ) is one prominent approach that lever-   ages both attention features and relation extraction   for the objects in the image . Qiao et al . ( 2020 )   offers a comprehensive survey on this topic .   Human Detection . A specialized category under   Object Detection , detecting humans with bounding   boxes in images nowadays can easily use open   source toolboxes including MMDetection ( Chen356   et al . , 2019 ) or Detectron ( Wu et al . , 2019 ) that   are trained on large - scale real life image datasets   like COCO ( Lin et al . , 2014 ) . Recent works such   as DarkPose ( Zhang et al . , 2020 ) also attempt to   utilize human pose information to better single out   human traits from complex background .   3 Method   In this section , we introduce the Person - centric Vi-   sual Grounding task , discuss the original Who ‚Äôs   Waldo dataset , and provide our analysis of short-   cuts , biases , and other issues that we discovered in   the dataset . We describe the process via which we   curate , debias , and Ô¨Ålter the dataset .   3.1 The Task   The Person - centric Visual Grounding task is de-   Ô¨Åned as follows . The givens are an image I , a set   of m1 person detections B(in form of bound-   ing boxes ) , and a corresponding image caption T   where its tokens contain references to n 1 per-   sons . For each referred person , we look for the   best matching detection from the givens . We also   assume no two persons can be matched with the   same detection .   3.2 The Who ‚Äôs Waldo Dataset   The dataset consists of 272 K real - life captioned   images sourced from the free Wikimedia Com-   mons repository . Each image pictures individu-   als under the ‚Äô People by name ‚Äô category on Wiki-   media Commons , while its caption describes the   scene and explicitly mentions the featured people   in real names . Key dataset creation procedures ,   text pre - processing , identifying person entities in   captions , detecting bounding boxes of people in im-   ages , and generating ground truths linking bound-   ing boxes and names , are all done with existing   automated tools such as FLAIR ( Akbik et al . , 2019 )   and MMDetection ( Chen et al . , 2019 ) . To prevent   misuse , in the publicly released version , all thereal names in the captions are replaced with the   [ NAME ] token , but references between bounding   boxes and token indices are given in individual an-   notation Ô¨Åles . This is equivalent to masking each   name with indexed placeholders such as PERSON1 ,   PERSON2 , etc . Amongst the entirety of 272 K an-   notated samples , 179 K samples are used for train-   ing , 6.7 K for validation , and 6.7 K for testing . Each   test sample is supposed to either mention at least   two persons orchoose from at least two bounding   boxes . The original test set is further validated   manually on Amazon Mechanical Turk .   3.3 Biases in Who ‚Äôs Waldo   The premise of the Person - centric Visual Ground-   ing task is to use ONLY the caption text and the   image as the cues to Ô¨Ånd out the correct bounding   box from the image per mentioned name . However ,   we observe a large portion of the original Who ‚Äôs   Waldo dataset does not provide sufÔ¨Åcient contexts   and can only be solved by heuristic methods . We   discuss two major types of biases that we discover   in the following sections .   The Ô¨Årst type no - verb is that the caption text   contains zero detectable verbs . Since linguistically   a verb is the crucial part of an action that assigns   participants with semantic roles , we technically   have no way to tell who performs or who receives   an action without verbs . For example in Figure   2(a ) , we are unable to tell who is who from the   image and the no - verb caption alone , unless we   recognize Vladimir Putin or the Georgian President   with external knowledge .   The second type conjunct - names is that the   caption contains a long chain of conjunct referred   names . Shown in Figure 2(b ) , all the referred   names share the verb perform , joined together only   with conjunct words such as and oralong with .   With no indication of the order amongst these per-   sons , we can only resort to a naive positional order   such as left - to - right . But since we may also have   extra bounding boxes as choices , such naive as-   sumption is indeed unreliable . Figure 2(b ) is such   an example that the Ô¨Årst mentioned name is not   always the one in the left - most bounding box .   3.4 Data Curation for De - biasing   In order to resolve the aforementioned limitations   of the original dataset , we utilize two pipelines   in SpaCy ver 3.0 ( Honnibal et al . , 2020 ) to Ô¨Ålter   out the biased data . We apply the POS - Tagging   pipeline to Ô¨Ånd out if sentences in an image cap-357   tion contain verbs in any form of conjugation . In   parallel , we use the Dependency Parsing pipeline   to examine if any [ NAME ] token conjuncts with   more than one [ NAME ] ‚Äôs from different referred   persons . We jointly Ô¨Ålter out any example that   either ( a ) contains zero verbs , or ( b ) has at least   three conjunct referred person names in a sentence .   For both pipelines , we replace the [ NAME ] tokens   that refer to the same person in a caption with   a random popular Ô¨Årst name , so that the natural   language - based SpaCy pipelines can yield more   accurate results . Both pipelines use the state - of-   the - art en - web - core - trf model which is built   on RoBERTa ( Liu et al . , 2019 ) .   Ultimately , our Ô¨Åltering procedure produces 84 K   qualifying image - caption pairs . Table 1 shows the   distribution of samples sourced from each split of   the original through our two debiasing pipelines .   We utilize data from the unused yet legitimately   annotated 79 K samples of the original dataset . We   reorganize and split all the qualifying 84 K samples   into 74 K for training , 5 K for validation , and 5 K   for test . Our new test set does not overlap with the   original training set . Similarly to the design of the   original , we enforce that all samples in our new test   set involves no trivial case that contains exactly one   referred name and exactly one bounding box . We   also make sure that any test set sample always has   at least one name - to - bounding - box pair as ground   truth.4 Experiments and Baselines   Setup . We evaluate the quality of our debiased   dataset with the same heuristic and Transformer-   based methods from the original paper . We also   train the benchmark model on both the original   and our new training set . We report the accuracies   obtained from our new test set as the new baselines .   Heuristics . We inherit the original heuristic mea-   sures to study the potential biases of our debiased   dataset versus those of the original dataset . Along-   side Random guessing , we assign the names in the   caption to the bounding boxes sorted by : ( a ) de-   creasing area size ( Big ! Small ) , ( b ) left - to - right   upper - left coordinates ( L ! R ( All ) ) , and ( c ) left - to-   right upper - left coordinates of the largest dbound-   ing boxes , dbeing the larger between the number   of bounding boxes and the number of names in a   test case ( L!R ( Largest ) ) .   Transformer - based Models . We adapt the origi-   nal benchmark Who ‚Äôs Waldo model to our debiased   dataset and see how well it can perform under the   updated contexts . The benchmark model is a multi-   layer multi - modal Transformer ( Vaswani et al . ,   2017 ) . Based on UNITER ( Chen et al . , 2020b ) ,   it learns to maximize the similarities between the   corresponding person names and bounding boxes   while minimize the similarities between those that   do not match up . We Ô¨Åne - tune the Who ‚Äôs Waldo   model with pre - trained weights from UNITER .   Analysis of Results . Table 2 shows the test set   accuracies for the original dataset and our debi-358   ased dataset . We Ô¨Ånd that the heuristic measures   have overall lower performance on our new dataset ,   meaning we have successfully reduced the effects   of the positional and the size - based biases from   the original dataset . Most signiÔ¨Åcantly , we have   lowered L!R ( All ) from +7.5 % to +1.4 % , almost   equal to randomness . Even the strongest L ! R   ( Largest ) heuristic has been lowered from +26.8 %   all the way down to +13.3 % as well . Our dataset is   thus proven less biased compared to the original .   We also show that our dataset has better prac-   ticality for the task . Measured with our new test   set , the performance of the Who ‚Äôs Waldo bench-   mark model trained with the original training set   performs 3.8 % lower than that trained with our new ,   smaller training set . Meanwhile , the test accuracy   gap between the Transformer - based method and   the heuristic methods has become larger using our   debiased dataset , widened from 5.8 % to 9.7 % . In   addition , using the Ô¨Åltered biased samples from the   original test set on our new trained model yields   an even lower performance at 48.2 % , which indi-   cates our new baseline model now adopts fewer   biases during training compared to the original .   Altogether with the lowered new baseline accu-   racy of 54.0 % , we argue that our debiased dataset   improves the quality of contextual cues that su - pervised models can learn from , and leaves more   applicable room for improvements in the future .   5 Conclusion   We present a reÔ¨Åned dataset for the PCVG task   with samples that contain contextual information   required for the task . We address prominent biases   that we identiÔ¨Åed in the original task dataset by   Ô¨Åltering out a large number of unsolvable cases ,   and report new baseline performances on the new   benchmark . Our reÔ¨Åned dataset can serve as a more   reliable benchmark to enable fair comparisons for   new modeling techniques and training protocols .   Acknowledgements   This research was supported by grants from   DARPA SAIL - ON , DARPA KAIROS , NSF   1816039 , and NSF 2132724 . The views and opin-   ions of the authors expressed herein do not neces-   sarily state or reÔ¨Çect those of the funding agencies   and employers .   Ethical Considerations   Our curated dataset is available at . We will   also follow the same licensing and data sharing   policy as the original Who ‚Äôs Waldo dataset.359References360361   Sicheng YuQianru SunHao ZhangJing JiangSingapore Management University , SingaporeNanyang Technological University , SingaporeCentre for Frontier AI Research , A*STAR , Singapore   scyu.2018@phdcs.smu.edu.sg , hzhang26@outlook.com   { qianrusun,jingjiang}@smu.edu.sg   Abstract   1 Introduction   Cross - lingual transfer has drawn wide attention in   recent years ( Hu et al . , 2020 ; Liang et al . , 2020 ) . It   has great potentials to be applied in advanced in-   dustries and real applications such as for improving   dialog and advertisement systems in multilingual   countries ( Schuster et al . , 2019 ; Yu et al . , 2021 ) .   It aims to reuse NLP models trained on a source   language for the task of a target language . The   most intuitive method is transfer learning by lever-   aging pre - trained multilingual language models   ( LMs ) such as mBERT ( Devlin et al . , 2019 ) and   XLM - R ( Conneau et al . , 2020 ) . These pre - trained   LMs encode different languages into a joint space   of multilingual representations ( Wu and Dredze ,   2019 ; Lauscher et al . , 2020 ) , and they perform well   especially for zero - shot cross - lingual tasks ( Wu   and Dredze , 2019 ; Lauscher et al . , 2020 ) . An-   other method orthogonal to this is called translate-   train ( Hu et al . , 2020 ; Fang et al . , 2021 ) . It trans-   lates training data from the source language into   the target language and uses the translated texts for   training . Our paper focuses on this second method .   Translate - train mitigates the language gap be-   tween the source and the target languages in mul-   tilingual tasks in a straightforward manner as it   directly generates the needed target language train-   ing samples . However , the translation process in-   troduces artifacts in the translated texts ( i.e. , trans-   lationese ) . It has been observed that translationese   often exhibits features such as stylistic ones that   are different from text written directly in the same   language ( which we call the originals ) and thus   can mislead model training ( Selinker , 1972 ; V olan-   sky et al . , 2015 ; Bizzoni et al . , 2020 ) . In Figure 1 ,   we show that even in a monolingual setting where   training and test data are in the same language ,   when the test data are original texts , using transla-   tionese to train a QA model results in substantial   performance drop compared with using originals   for training .   To tackle the issue with translationese artifacts ,   inspired by domain mapping techniques ( Zhu et al . ,   2017 ) , we explore the idea of projecting originals362   and translationese into a common embedding space   to close their gap . Since only the originals of the   source language are available under the translate-   train setting , whether this idea is feasible depends   on whether the projection function is learnable   from the source language and transferable to other   languages . Therefore , we Ô¨Årst conduct experiments   to investigate if translationese artifacts , or patterns   of differences between orginals and translationese ,   are recognizable and transferable across languages   by deep learning models . SpeciÔ¨Åcally , we train   a binary classiÔ¨Åer to distinguish English originals   from English translationese . We then test the effec-   tiveness of this binary classiÔ¨Åer on other languages .   Our intuition is that ( 1 ) if the model converges , it   suggests that the patterns of translationese artifacts   can be potentially learned to some extent , and 2 )   if the trained model recognizes the translationese   of other languages , it means the model can likely   transfer the learned patterns across different lan-   guages . Our results in Figure 2 validate both : 1 )   The model converges well and achieves 97 % ac-   curacy on English , the training language . ( 2 ) It   also performs reasonably well on other languages   ( 77%91 % ) .   Based on the above intuition and validation ,   we propose a Translationese Embracing Artifacts   ( TEA ) method that projects originals and transla-   tionese into a common space to mitigate the trans-   lationese artifacts . TEA explicitly learns a map-   ping function from originals to translationese us-   ing originals and translationese of the source lan - guage ( English in our experiments ) , where learn-   ing is through minimizing the distance between   the mapped representation of originals and of the   corresponding translationese . TEA then applies   this mapping function to the originals of the target   language during the testing stage . For evaluation ,   we conduct experiments on multilingual QA us-   ing the TyDiQA dataset ( Clark et al . , 2020 ) . Our   results show that TEA outperforms translate - train   baselines and SOTA translationese mitigation meth-   ods designed for machine translation ( Marie et al . ,   2020 ; Wang et al . , 2021 ) .   2 Related Work   The effect of translationese has been widely stud-   ied in translation tasks ( Lembersky et al . , 2012 ;   Zhang and Toral , 2019 ; Edunov et al . , 2020 ; Gra-   ham et al . , 2020 ; Freitag et al . , 2020 ) . Some works   focus on mitigating or controlling the effect of   translationese , e.g. , tagged training ( Marie et al . ,   2020 ; Riley et al . , 2020 ; Wang et al . , 2021 ) , which   are adopted as baselines in our paper . In the Ô¨Åeld   of cross - lingual transfer , there are very few works   about translationese . Artetxe et al . ( 2020 ) is the   only attempt for translate - test and zero - shot learn-   ing . In contrast , we focus on translate - train and   aim to mitigate the artifacts in translationese .   Our research is also related to domain adaptation   ( DA ) that aims to transfer the knowledge from a   source domain to target domains . Our original - to-   translationese projection function can be seen as   something similar to projecting source domain and   target domain data into a common space , which   has been used before for domain adaptation ( Zhu   et al . , 2017 ; Shen et al . , 2017 ) .   3 Our Approach ( TEA )   Letxrepresent the input text and yrepresent the   output label . Xdenotes the domain ( i.e. , all pos-   sible values ) of xandYis the set of labels . The   input xcomes from different languages , and it can   be either originals or translationese during training .   SpeciÔ¨Åcally , we use X to denote the domain   ofsource language originals , and deÔ¨Åne X   andX in a similar way ( whererefers   to the target language andrefers to transla-   tionese ) . We further use back - translation ( Sennrich   et al . , 2016 ) to generate source language trans-363lationese ( i.e. , the source language originals are   Ô¨Årst translated into a pivot language and then trans-   lated back into the source language ) , denoted as   X , for the purpose of learning a mapping   function to project originals and translationese into   the same space .   We now present our TEA method . Our ultimate   goal is to learn a mapping function fX    Y , which takes target language originals as input .   However , we only have D"XYand   D"XYduring training . The chal-   lenge is that an flearned from either D or   D may not work effectively on X be-   cause of the differences between the source and the   target languages and between originals and trans-   lationese . To mitigate the differences between the   source and the target languages , we rely on pre-   trained multilingual language models , as many ex-   isting works do . As for the differences between   originals and translationese , based on the idea dis-   cussed in Section 1 , we propose to mitigate the   translationese artifacts of the target language us-   ing an original - to - translationese mapping function ,   and because of the lack of target originals , we pro-   pose to learn the original - to - translationese mapping   function from the source language .   To concretely illustrate our idea , we break down   the mapping from XtoYinto the following steps :   Multilingual Projection ( MP ): First , input xis   projected into a language - agnostic multilingual   space by using a pre - trained multilingual LM . We   useXto denote the projected multilingual space ,   andfis a multilingual projection ( i.e. , the multi-   lingual LM ) that maps an input xin any language   intoX.   Original - to - Translationese Projection ( OTP ):   Suppose Xconsists of two subspaces : X    XX , where X andX   denote the multilingual representations of any orig-   inals and translationese , respectively . To close the   gap between originals and translationese , we de-   Ô¨Åne an original - to - translationese projection func-   tionfX X to convert the vec-   tor representation of a piece of originals to its cor-   responding representation of translationese .   Language - Agnostic QA ( QA ): The last step is a   language - agnostic classiÔ¨Åer for the QA task itself .   We use fX Yto denote it .   Given an input x , depending on whether it is   from originals or translationese , we use differentcompositions of the functions above to map xtoy :   y vf`f`f xx"X ;   f`f x x"X :   Here`represents the composition of two func-   tions , i.e. ,f`g x f g x , and¬òdenotes   source language or target languages . More con-   cretely , for  x;y"D , we use X    X  X Y ; for x;y "   D , we use X X Y.   As discussed in Section 1 , we make use of the   source language originals and translationese to   learn f. SpeciÔ¨Åcally , for  x;y"D ,   we use x"X to represent its corre-   sponding translationese , i.e. , generated by back-   translation ( Sennrich et al . , 2016 ) through a pivot   language . Let r x;xx"D denotes all the   pairs of originals and translationese in the source   language . Then , we minimize the distance between   f f xandf xto optimize f.   In summary , the loss function consists of the   following three components :   L =l f`f`f x;y   =l f`f x;y   = 1g x;x ;   where g x;x cos f f x ; f x.   l  ; is standard cross entropy loss and cos  ;    is the cosine similarity function .   Model Details . Forf , we use XLM - R ( Conneau   et al . , 2020 ) . For f , we utilize a transformer   layer ( Vaswani et al . , 2017 ) . fis implemented   by a linear layer .   4 Experiments   Dataset . We conduct experiments on Ty-   DiQA ( Clark et al . , 2020 ) . SpeciÔ¨Åcally , we evaluate   our approach on the gold - passage subtask of Ty-   DiQA , which includes 9 languages . We set English   as source language and others as target languages ,   and report the results on target languages . During   training , we utilize translated training data in all   target languages for joint training . We use Exact   Match ( EM ) and F1 scores as evaluation metrics .   Implementation . Translations of English training   data for target languages are from XTREME ( Hu364   et al . , 2020 ) and translationese English is trans-   lated by Google Cloud Translation . German ( de )   is selected as the default pivot language in back-   translation . More details are in the Appendix B.   Baselines . We compare our model with the   following baselines : ( 1 ) Standard Translate-   Train ( STT ) ( Devlin et al . , 2019 ) . ( 2 ) FIL-   TER ( Fang et al . , 2021 ) is an advanced translate-   train method fully utilizing the parallel data . ( 3 )   Tagging ( TAG ) ( Marie et al . , 2020 ) , which distin-   guishes originals and translationese by adding a   tag for machine translation . ( 4 ) Two - Stage Train-   ing ( TST ) ( Wang et al . , 2021 ) , which is another   approach to address the gap between translationese   and originals for machine translation . It Ô¨Årst uses   the combination of them for training followed by   another round of training only on originals . ( 5 ) Gra-   dient Reversal Layer ( GRL ) ( Ganin and Lempitsky ,   2015 ) , which is a general DA method .   Main results . Table 1 summarizes the compari-   son between our TEA and the baselines . We make   the following observations : ( 1 ) TEA outperforms   all baselines . For instance , TEA surpasses STT   by2:4%(EM ) and 1:5%(F1 ) on average , which   demonstrates the effectiveness of our method . ( 2 )   Methods considering translationese artifacts gen-   erally perform better than methods without such   design , which reinforces the importance of miti-   gating translationese artifacts . ( 3 ) Compared to   the baselines for translationese artifacts , TEA still   shows its superiority . We highlight that our OTP   module with explicit projection is better than im-   plicit DA approaches . E.g. , TAG only uses different   tags to distinguish the translationese from origi-   nals . ( 4 ) The improvements from TEA across dif-   ferent languages are different . For high - resource   target languages , TEA brings more gains on the   languages in Indo - European family , e.g. , ru , and   marginal gains on others , e.g. , ar . For low - resource   target languages , the performance improvements   are obvious , e.g. , sw . It is because both language   model and machine translation model are of lower   quality on low - resource languages , and thus mit-   igating the gap between translationese and orig-   inals shows more effectiveness in such scenario .   For high - resource languages , TEA prefers Indo-   European languages , which are closer to English .   Ablation studies . We conduct in - depth ablation   studies to analyze TEA . SpeciÔ¨Åcally , we explore   the following settings : ( 1 ) Since we use 11 % more   data in TEA ( unlabeled X ) compared to   STT , here we add labeled X in STT . ( 2 )   Since we use additional 0:38 % parameters ( OTP )   in our method compared to STT , here we add   the same OTP module in STT . ( 3 ) We replace   the Original - to - Translationese Projection ( OTP ) by   Translationese - to - Original Projection ( TOP ) . ( 4 )   We replace the self - attention layer in OTP with a   multi - layer perceptron ( MLP ) . ( 5 ) We replace the   cosine distance function in loss with mean square   function . The results are summarized in Table 2 .   Compared to the variants , our full method performs   best over all settings . ( 1)/(2 ) incorporate additional   data / parameters , which demonstrates the improve-   ment of our method is not caused by the two factors.365   ( 3 ) proves that TOP still mitigates the artifacts , but   OTP obtaining better performance . We argue that   it is because most of the training data is transla-   tionese . ( 4 ) and ( 5 ) demonstrate the effectiveness   of our loss function and architecture .   Pivot Languages Analysis . Here we study the ef-   fect of pivot language used in generating X .   SpeciÔ¨Åcally , we select four pivot languages , i.e. ,   German ( de ) , Scottish ( gd ) , Korean ( ko ) and Chi-   nese ( zh ) , for evaluation . We Ô¨Åx our approach and   only replace the X used in OTP . The results   are reported in Table 3 . We observe that pivot lan-   guages from Indo - European family are superior to   that from other language families . We believe it is   because the training data of other target languages   in translate - train is translated from English , while   English belongs to the Indo - European family .   5 Conclusions   We aim to mitigate the translationese artifacts when   training translate - train models . After varifying the   transferability of the translationese patterns across   languages , we propose the TEA that mitigates ar-   tifacts using a source language and to facilitate   the prediction on unseen target languages . Our ap-   proach is simple and generic . Moreover , our results   on multilingual QA show its effectiveness .   Ethical Considerations   Although our method requires Ô¨Åne - tuning of the   pre - trained multilingual language model , the com-   putational cost of our experiments is not high . We   utilize two pieces of NVIDIA V100 and it takes   around 1 hour for the Ô¨Åne - tuning process . This   is partly due to the relatively small QA training   dataset used for Ô¨Åne - tuning . It is possible that if our   method is applied to either a much larger training   dataset for Ô¨Åne - tuning or a much larger pre - trained   language model , the computational cost and power   consumption will go up . To reduce such costs , one   way is to Ô¨Åne - tune only part of the pre - trained lan-   guage model . Another way is to apply the recently   proposed Adapter method ( Houlsby et al . , 2019 ) toÔ¨Åne - tune the language model .   Our method relies on machine translation sys-   tems . It has been found in a previous study that in-   dustrial MT systems as well as SOTA academic MT   systems may suffer from gender bias ( Stanovsky   et al . , 2019 ) , and it would not be surprising if other   types of societal biases and stereotypes are also   found in machine translated texts . If our method   uses translationese containing societal biases , our   learned original - to - translationese projection func-   tion will likely also contain such biases , which may   affect the fairness of the Ô¨Ånal trained system . How-   ever , this is not due to our method but rather the   translated text we use . Nevertheless , this is some-   thing we need to keep in mind if our method is   adopted for real applications .   Acknowledgments   This research is supported by the National Research   Foundation , Singapore under its Strategic Capabili-   ties Research Centres Funding Initiative , and par-   tially supported by the Agency for Science , Tech-   nology and Research ( A*STAR ) under its AME   YIRG Grant ( Project No . A20E6c0101 ) and the   A*STAR Career Development Fund ( Project No .   C210812033 ) . Any opinions , Ô¨Åndings and conclu-   sions or recommendations expressed in this mate-   rial are those of the author(s ) and do not reÔ¨Çect the   views of National Research Foundation , Singapore .   References366367   A Training Pipeline   Figure 3 illustrates the training pipelines our   method . The goal is to map the originals and trans-   lationese domains into the same embedding space   for prediction . SpeciÔ¨Åcally , the module on the top   of the Ô¨Ågure is to train the D andD ,   i.e. , the Ô¨Årst two terms of loss function , while the   module at the bottom aims to map the originals-   translationese pairs in D into same space ,   i.e. , the last term of loss function . B Implementation   General Implementation . We adopt the Hugging-   Face Transformers ( Wolf et al . , 2020 ) toolkit to im-   plement the pre - trained language model , i.e. , XLM-   R. The maximal input length , i.e. , concatenation   of question and passage tokens , is set as 384 . We   also utilize a document sliding window with stride   length of 128to tackle the long passage issue . The   learning rate and batch size are set as 2e5and32 ,   respectively . We use back - translate ( Sennrich et al . ,   2016 ) to generate the translationese . Back - translate   means to translate the source language to a pivot   language and then translate back to the source lan-   guage . By doing this , we are able to obtain the   translationese of source language .   Implementation of Experiment in Figure 1 . Ty-   DiQA ( Clark et al . , 2020 ) provides both training   and testing datasets of originals for all languages .   Here we adopt the originals training data to gener-   ate the corresponding translationese training data   through back - translation . The results in Fig-   ure 1 are obtained by training originals and gener-   ated translationese data for en , ar and Ô¨Å , respec-   tively . Note all the translationese is generated   by the Google Cloud Translationservice , where   the English translationese is generated by back-   translationese with de as pivot language , the trans-   lationeses of ar and Ô¨Å use en as pivot language . The   The test set is originals of each language .   Implementation of Experiment in Figure 2 .   Similarly , we generate the translationese of the orig-   inals for each language using Google Cloud Trans-   lation service , where en is set as pivot language for   non - English languages , and German for English   language . We split the originals - translationese   pairs of English into two groups , where 80 % sam-   ples are used for training , and the rest 20 % samples   together with all pairs of other languages are used   for evaluation . As the originals and translationese   are paired , a random guess could achieve 50 % ac-   curacy for all languages ideally .   Implementation of TEA . It is worth noting that   we can only access the originals of English and   the translationese of other target languages during   training . We use the translationese data , i.e. , the   target language data translated from English , from368   XTREME ofÔ¨Åcial website , while the translated   data from XTREME is utilized in translate - train   for all previous works ( Fang et al . , 2021 ) . Besides ,   we also augment translationese of English , which is   generated by back - translation , in our TEA . Again ,   we resort to the Google Cloud Translation service   to generate the translationese for all experiments   in Section 4 , where the German is set as pivot lan-   guage by default .   C Data and Parameters   The sample sizes of the data sets in all 9 languages   are equal , since they are all translated from En-   glish originals training data . The standard translate-   train ( STT ) directly adopt the data samples of 9   languages for training . In addition to the data sam-   ples of 9 languages , we also incorporate the En-   glish translationese , leading to 11 % more samples   used compared to SST . Besides , our Original - to-   Translationese Projection ( OTP ) module also intro-   duce additional parameters compared to SST .   D Additional Experiments   Main Results . In this part , we replenish the Ty-   DiQA results of two advanced multilingual lan-   guage models , i.e. , VECO ( Luo et al . , 2021 ) and   HICTL ( Wei et al . , 2020 ) in Table 4 .   Originals - Translationese Pair Sample . In Fig-   ure 4 , we list examples of originals - translationese   pair in English used for TEA training .   Effect of Translation Quality on Translationese   English . Here we conduct an ablation study aboutthe effect of translation quality on translationese   English used in cosine distance loss . Due to the   limited resource , we are unable to train a machine   translation model from scratch by ourselves . In-   stead , we select the free Google Translate toolkit   ( compared to the paid Google Cloud service ) as the   proxy of low - quality translator . We Ô¨Åx all the im-   plementation settings and change the translationese   English data only . Consequently , we obtain the av-   erage performance of EM ¬© F1 58:0 ¬© 73:9 . The   result indicates that a better translator is more effec-   tive for the translationese English generation . It is   because that the low - quality translator may create   more translation errors , then those errors are propa-   gated during training , which hinders the learning   of the originals to translationese mapping.369370   Xiang Yue , Xiaoman Pan , Wenlin Yao , Dian Yu , Dong Yu , and Jianshu ChenThe Ohio State UniversityTencent AI Lab   yue.149@osu.edu   { xiaomanpan,wenlinyao,yudian,dyu,jianshuchen}@tencent.com   Abstract   1 Introduction   Open - domain question answering ( QA ) aims to ex-   tract the answer to a question from a large set of   passages . A simple yet powerful approach adopts a   two - stage framework ( Chen et al . , 2017 ; Karpukhin   et al . , 2020 ) , which Ô¨Årst employs a retriever to fetch   a small subset of relevant passages from large cor-   pora ( i.e. , retriever ) and then feeds them into a   reader to extract an answer ( text span ) from them . Due to its simplicity , a sparse retriever such as   TF - IDF / BM25 is generally used together with a   trainable reader ( Min et al . , 2019 ) . However , re-   cent advances show that transformer - based dense   retrievers trained on supervised data ( Karpukhin   et al . , 2020 ) can greatly boost the performance ,   which better captures the semantic relevance be-   tween the question and the correct passages . Such   approaches , albeit promising , are restricted by the   limited amount of human annotated training data .   Inspired by the recent progresses of language   models pretraining ( Devlin et al . , 2019 ; Lee et al . ,   2019 ; Guu et al . , 2020 ; Sachan et al . , 2021 ) , we   would like to address the following central ques-   tion : can we pretrain a two - stage open - domain QA   system ( retriever + reader ) without task - speciÔ¨Åc hu-   man annotations ? Unlike general language models ,   pretraining such a system that has strong transfer   capabilities to downstream open - domain QA tasks   is challenging . This is mainly due to the lack of   well - aligned pretraining supervision signals . In par-   ticular , we need the constructed pretraining dataset   ( in the form of question - answer - context triplets ) to :   ( i ) cover a wide range of domains ( for open - domain   applications ) , ( ii ) link a question to its semantically   relevant context with supporting evidence ( for train-   ing the retriever ) , and ( iii ) identify the correct an-   swer in the context ( for training the reader ) .   There have been several recent attempts in ad-   dressing these challenges . ORQA ( Lee et al . , 2019 )   creates pseudo query - passage pairs by randomly   sampling a sentence from a paragraph and treat-   ing the sampled sentence as the question while   the rest sentences as the context . REALM ( Guu   et al . , 2020 ) adopts a retrieve - then - predict ap-   proach , where the context is dynamically retrieved   during training and an encoder ( reader ) predicts371   the masked token in the question based on the re-   trieved context . The retriever pretraining signals   constructed in these approaches are not aligned   with question - context pairs in open - domain QA   settings . For example , as shown in Figure 1 , the   context ( in blue color ) of ORQA pretraining data   instance does not contain direct supporting evi-   dence to the question . Likewise , the dynamically   retrieved context in REALM can not be guaranteed   to contain direct supporting evidence either . In   addition , existing pretraining methods ( Lee et al . ,   2019 ; Guu et al . , 2020 ) mostly focus on the re-   triever and do not jointly provide direct pretraining   signals for the reader ( Figure 1 ) .   To meet all three aforementioned criteria , we   propose a pretraining approach named Consulting   Millions OfREferences ( C - MORE ) , which auto-   matically constructs pretraining data with well-   aligned supervision signals ( Figure 1 ) . SpeciÔ¨Åcally ,   we Ô¨Årst extract three million statement - reference   pairs from Wikipedia along with its cited refer-   ences . Then , we transform them into question-   answer - context triplets by replacing a potential   answer span in the statement ( e.g. , ‚Äú 14 ‚Äù in the   Figure 1 ) by an interrogative phrase ( e.g , ‚Äú how   many ‚Äù ) . Such kind of pseudo triplets are in the   exact same form as human - annotated ones , and   the question is linked to the context that contains   the most direct - supporting evidence , a highly de-   sirable feature for open - domain QA tasks . We   experiment the pretraining with a widely - adopted   open - domain QA system , Dense Passage Retriever   ( DPR ) ( Karpukhin et al . , 2020 ) . The experimen-   tal results show that our pretrained retriever not   only outperforms both sparse and dense retrieval   baselines in the zero - shot retrieval setting ( 2%-10 %   absolute gain in top-20 accuracy ) , but also leads tofurther improvement in the downstream task Ô¨Åne-   tuning . By integrating with our pretrained reader ,   the entire open - domain pretraining improves the   end - to - end QA performance by 4 % in exact match .   2 Method   Recall that we want to automatically construct a   large - scale open - domain QA pretraining dataset   that satisÔ¨Åes three criteria : ( i ) The dataset should   cover a wide range of domains for the open - domain   QA purpose . ( ii ) The context passage is semanti-   cally relevant to the question and contains direct   supporting evidence for answering the question .   ( iii ) The correct answer span in the context passage   for answering the question should also be identi-   Ô¨Åed for training the reader . This section Ô¨Årst dis-   cusses how to extract a large amount of statement-   reference pairs from the Wikipedia and then explain   how to construct pseudo question - answer - context   triplets for pretraining open - domain QA systems .   2.1 Statement - Reference Pairs Collection   Wikipedia articles usually contain a list of knowl-   edge sources ( references ) at the end that are veriÔ¨Åed   by human editors to support the statements in the   articles ( Li et al . , 2020 ) . And the reference docu-   ments always consist of strong supporting evidence   to the statements . For example , as shown in Figure   1 , the document ( in green color ) contains the di-   rect evidence ‚Äú ... rescued 14 people who were being   held hostage on it ... ‚Äù to support the query ( red text )   ‚Äú The boarding crew freed 14 Iranian and Pakistani   Ô¨Åshermen who had been held as hostages over two   months ‚Äù . Additionally , such knowledge sources   are often organized in a good structure and can be   automatically extracted and processed . Moreover ,   the statement - reference pairs in Wikipedia cover372   a wide range of topics and domains . Thus , when   converted into question - context pairs , they satisfy   the Ô¨Årst two criteria and are suitable for training an   accurate dense retriever at a large scale .   In our study , we extract around six million   statement - reference pairs from Wikipedia . We   Ô¨Ålter the pairs whose reference documents are   not reachable and Ô¨Ånally obtain around three mil-   lion statement - reference pairs ( see statistics in Ap-   pendix Table 1 ) . The data collection method we   proposed is very general and therefore can be   easily extended to other domains , e.g. , WikiEM   ( wikem.org ) for medical domain or other languages ,   e.g. , Baidu Baike ( baike.baidu.com ) for Chinese .   2.2 QAC Triplets Construction   We now explain how to further convert the   statement - reference pairs into question - answer-   context pairs . Inspired by previous unsupervised   extractive QA work ( Lewis et al . , 2019 ) , we extract   entities as potential answers to construct pseudo   question - answer - context pairs where an answer   span is extracted from the context given an question   to accommodate the extractive QA setting . Specif-   ically , we Ô¨Årst adopt an off - the - shelf named en-   tity recognition tool spaCy ( Honnibal and Montani ,   2017 ) to identify entities in each query . Next , we   Ô¨Ålter the entities that do not appear in the evidence   based on string matching . If multiple entities are   found , we sample one of them as the potential an-   swer to the query . The sampled entity in the query   is replaced by an interrogative phrase based on   the entity type ( e.g. , a [ DATE ] entity will be re-   placed by phrases such as ‚Äú when ‚Äù , ‚Äú what date ‚Äù . In   this way , we can construct question - answer - context   triplets to train open - domain QA models . See more   question reformation rules in Appendix Table 5 ) .   3 Experiment   3.1 Experimental Setup   Pretraining Model Architecture . Since concep-   tually the construed triplets is in the same format as   the annotated QA data , they can be used to pretrain   any existing neural open - domain QA model . Here , we adopt DPR ( Karpukhin et al . , 2020 ) , which con-   sists of a dual - encoder as the retriever and a BERT   reader , considering its effectiveness and popularity .   SpeciÔ¨Åcally , the retriever Ô¨Årst retrieves top- k(up   to 400 in our experiment ) passages , and the reader   assigns a passage score to each retrieved passage   and extracts an answer with a span score . The span   with the highest passage selection score is regarded   as the Ô¨Ånal answer . The reader and retriever can   be instantiated with different models and we use   BERT - base - uncased for both of them follow-   ing ( Karpukhin et al . , 2020 ) .   Pretraining Data Processing . For our extracted   pseudo question - answer - context triplets , some-   times the context ( reference document ) is too long   to Ô¨Åt into a standard BERT ( maximum 512 tokens )   in the DPR model . Thus , we chunk a long doc-   ument into n - word text blocks with a stride of   m. Without loss of generality , we use multiple   combinations of nandm : n = f128;256;512 g ,   n = f64;128;256 g. Then we calculate relevance   scores ( using BM25 ) of the derived blocks with   the question and select the most relevant block as   the context . Note that the retrieval step is done   within the single document ( usually less than 20   text blocks ) . In contrast , the baseline model ( Sec-   tion 3.2 ) - sparse retriever BM25 - looks up the   entire knowledge corpus ( 20 M text blocks ) . In this   way , we can automatically collect the most relevant   context that supports the query from a long article .   Finetuning QA Datasets . We consider three pop-   ular open - domain QA datasets for Ô¨Ånetuning : Nat-   uralQuestions ( NQ ) ( Kwiatkowski et al . , 2019 ) ,   TriviaQA ( TQA ) ( Joshi et al . , 2017 ) , and WebQues-   tions ( WebQ ) ( Berant et al . , 2013 ) , whose statistics   are shown in Table 1 .   Following the setting of DPR ( Karpukhin et al . ,   2020 ) , we use the Wikipedia as the knowledge   source and split Wikipedia articles into 100 - word   units for retrieval . All the datasets we use are the   processed versions from the DPR implementation .   Overlap between Pretraining and Finetuning   Datasets . Though both C - MORE and downstream   QA data are constructed based on Wikipedia ,   the overlap between them would be very little .   C - MORE extracts queries from Wikipedia while the   queries of downstream QA data are annotated by   human . C - MORE extracts contexts from the exter-   nal referenced pages ( general Web ) while the down-   stream QA data extract contexts from Wikipedia .   Implementation Details . For pretraining , we set373   training epochs to 3 , batch size to 56 for retrievers   and 16 for readers , and learning rate to 2e-5 . We   select the best checkpoint based on the pretraining   dev set . For Ô¨Ånetuning , we use the same set of   hyperparameters as the original DPR paper . The   comparing baselines ORQA ( Lee et al . , 2019 ) and   REALM ( Guu et al . , 2020 ) use 288 - token trunca-   tion over Wikipedia , which are not directly compa-   rable to our results . To enable a fair comparison ,   we report the retrieval results from a recent paper   ( Sachan et al . , 2021 ) , which uses the same retrieval   corpus as ours .   3.2 Retrieval Performance   We consider three settings to demonstrate the use-   fulness of our pretrained retriever .   Unsupervised . We assume no annotated training   QA pairs are available . In this setting , We compare   our method with existing unsupervised retrievers :   a sparse retriever BM25 and two pretrained dense   retrievers ORQA and REALM .   Domain Adaptation . We consider the condition   in which there are QA training pairs in the source   domain but no training data in the target domain .   The task is to obtain good retrieval performance onthe target test set only using source training data .   We compare our method with two baselines : one   is to directly train a dense retriever on the source   domain while the other is to Ô¨Årst pretrain a dense re-   triever on our constructed corpus and then Ô¨Ånetune   it on the source domain training set .   Supervised . In this setting , all the annotated QA   training instances are used . Similar to the previous   setting , we compare a supervised retriever with and   without our C - MORE pretraining .   For all settings , we report the top- kretrieval   accuracy ( k2 f20;100 g ) on the test set following   ( Karpukhin et al . , 2020 ) . See the overall retrieval   performance of different models in each setting in   Table 2 . We have the following observations .   In the unsupervised setting , compared with the   strong sparse retrieval baseline BM25 , our pre-   trained dense retriever shows signiÔ¨Åcant improve-   ment . For example , we obtain around 7 % absolute   improvement in terms of both Top-20 and Top-100   accuracy on the WebQuestion dataset . Compared   with pretrained dense retrievers ( i.e. , ORQA and   REALM ) , our pretrained model outperforms them   by a large margin . This is not surprising as our   pretraining data contain better aligned retrieval su-374pervision signals : reference documents often have   supporting evidence for the question while their   retrieval training signals are relatively indirect .   In the domain adaptation andsupervised set-   tings , our pretrained dense retriever provides a bet-   ter Ô¨Ånetuning initialization and leads to improve-   ment compared with randomly initialized DPR   models . Another surprising result is that our pre-   trained dense retriever even outperforms some DPR   domain adaptation models . For example , on the   TriviaQA testing set , our pretrained DPR model   achieves 72.2 % top-20 and 81.3 % top-100 accu-   racy while the DPR - NQ model obtains 69.7 % and   79.2 % respectively . This indicates that our pre-   trained dense retriever can generalize well even   without using any annotated QA instances .   All the results demonstrate the usefulness and   generalization of our pretrained dense retriever for   open - domain QA tasks .   3.3 End - to - End QA performance   We now examine how our pretrained retriever   and reader improve the end - to - end QA perfor-   mance , measured in exact match ( EM ) . The re-   sults are shown in Table 3 , from which we make   the following observations . ( i ) Surprisingly , our   fully - unsupervised system ( pretrained retriever +   pretrained reader ) shows a certain level of open-   domain QA ability ( see row # 3 ) . For example , on   TriviaQA , our fully - unsupervised system can an-   swer around 25 % of questions correctly . ( ii ) Com-   pared to the system with BM25 retriever ( row # 4 ) ,   the one with our pretrained dense retriever ( line   # 5 ) retrieves more relevant passages , leading to   better QA performance . ( iii ) Initializing either the   retriever or the reader from our pretrained check-   point can lead to further improvement ( rows # 6-#8 ) .   For example , on the TriviaQA and WebQuestion   datasets , our entire pipeline pretrain leads to about   4 % absolute gain in terms of EM . Note that on the   WebQuestion dataset , all the DPR models perform   worse than REALM , this is because of the limited   training data of WebQuestion . The issue can be eas-   ily solved by adding Multi datasets for Ô¨Ånetuning   according to ( Karpukhin et al . , 2020 ) .   3.4 Computational Resource Comparison   In addition to the performance gain , another beneÔ¨Åt   ofC - MORE is its training scalability . We compare   theC - MORE pretraining with ORQA and REALM   in terms of computational resources they use in   Table 4 . As can be seen , C - MORE only requires   reasonable GPU computational resources , which   could be normally conducted on an academic - level   computational platform . On the contrary , due to   the lack of direct retrieval supervision , ORQA and   REALM often needs more computational resources   and requires more training steps to converge .   4 Conclusion   This paper proposes an effective approach for pre-   training open - domain QA systems . SpeciÔ¨Åcally ,   we automatically construct three million pseudo   question - answer - context triplets from Wikipedia   that align well with open - domain QA tasks . Exten-   sive experiments show that pretraining a widely-   used open - domain QA model ( DPR ) on our con-   structed data achieves promising performance gain   in both retrieval and QA accuracies . Future work   includes exploring the effectiveness of the con-   structed data on more open - domain QA models   ( e.g. , REALM ) and training strategies ( e.g. , joint   optimizing the retriever and reader ) .   Acknowledgements   The authors would thank the anonymous reviewers   for their insightful comments and suggestions . The   authors would also thank the colleagues in Tencent   AI Lab for their internal discussions and feedback .   References375376A Appendix377   Ka Wong   Google Research   danicky@gmail.comPraveen Paritosh   Google Research   pkp@google.com   Abstract   1 Introduction   Crowdsourcing has become a mainstay for data   collection in NLP ( Geva et al . , 2019 ; Sabou et al . ,   2014 ) . It can produce data in a scalable and cost   effective manner . However , these benefits come at   a cost : quality . The reliability of crowd workers is   always of central concern . One common strategy   to increase the data reliability is to collect multiple ,   independent judgements and to use the aggregated   judgements instead . Indeed , early papers such as   Snow et al . ( 2008 ) show that average ratings cor-   relate more strongly with expert judgements . This   makes sense , as average ratings are known to have a   higher reliability than individual ones ( Ebel , 1951 ) .   A number of strategies have been proposed to ad-   dress data quality issues , e.g. rater modeling , label   correction , label pruning ( Kumar and Lease , 2011 ) ,   but aggregation remains very popular ( Prabhakaran   et al . , 2021 ) . Sheshadri and Lease ( 2013 ) present   nine crowdsourced datasets across a wide range ofNLP tasks to compare different aggregation meth-   ods . See Difallah and Checco ( 2021 ) for a recent   review of aggregation techniques . In short , aggre-   gation has become the default method for acquiring   reliable data from the crowd .   Interestingly , after we adopted aggregation as   a community , we forgot to update our reliability   measures correspondingly . The field continues to   report data reliability in terms of IRR , even when   aggregate ratings are used . Focusing on IRR , we   are unable to capture the increase in reliability due   to aggregation . The actual data reliability is hence   unknown . This has important consequences . Relia-   bility is often used as a safeguard for reproducibil-   ity . Therefore conclusions about the reproducibility   of a dataset drawn based the reliability of individ-   ual ratings may be different than that based on the   reliability of aggregate ratings .   By reporting the correct reliability that is actu-   ally higher , this may even have a side effect of   lessening the stigma on low - IRR datasets . As a   result , this may create a path forward towards reli-   able data on subjective tasks , where a high IRR is   difficult to obtain , such as emotions ( Wong et al . ,   2021 ) and toxicity ( Wulczyn et al . , 2017 ) . With a   reproducibility crisis looming in the background   ( Baker , 2016 ; Hutson , 2018 ) , more frequent and   accurate reporting of reliability is our primary safe-   guard ( Paritosh , 2012 ) .   We denote the reliability of aggregate ratings   ask - rater reliability ( kRR ) , in order to differenti-   ate it from inter - rater reliability . In this paper we   present a few methods for computing kRR . First ,   we demonstrate a general , empirical approach that   is based on replications . To that end , we conducted   two replications of WordSim-353 ( Finkelstein et al . ,   2001 ) , a widely used word similarity dataset . We   then discuss two other alternatives that do not re-   quire replications . One is a re - sampling - based boot-   strap approach ( Efron and Tibshirani , 1994 ) . It is   suitable for experiments with a high rating redun-378dancy . The other is an existing analytical approach   based on intraclass correlation ( ICC ) . It is suitable   for continuous data where the aggregation is the   mean . We conclude with recommendations for   reporting reliability of crowdsourced annotations ,   and novel research questions to expand the useful-   ness of kRR .   2 Related Work   Various authors have stressed the importance of   measuring reliability for the correct unit of analy-   sis . Ebel ( 1951 ) asks ‚Äú Is it better to estimate the   reliability of individual ratings or the reliability of   average ratings ? If decisions are based upon aver-   age ratings , it of course follows that the reliability   with which one should be concerned is the relia-   bility of those averages . ‚Äù Shrout and Fleiss ( 1979 )   and Hallgren ( 2012 ) reiterate similar points .   These studies primarily focus on the reliability   of the mean , which is just one of many different   aggregation methods . There is a reason . Not only   is the mean a popular choice , it is also the only   known choice where the reliability of the aggregate   ratings can be computed analytically from the re-   liability of individual ratings . This is done in the   ICC framework . ICC is typically used to measure   the reliability of single ratings , but it actually has   a variant that can be used for mean ratings as well .   Shrout and Fleiss ( 1979 ) list several types of ICC   coefficient , one of which is for mean ratings . They   call it ICC ( k ) , where kis the number of ratings per   item . In this generalized notation , ICC(1 ) is just the   reliability of individual ratings , or the IRR . Note   that McGraw and Wong ( 1996 ) use a slightly dif-   ferent notation , ICC ( 1 , k ) , to explicitly denote that   it is for a one - way random effects model , where   the raters are treated as interchangeable . That is   a common assumption in most crowdsourcing ex-   periments done on commercial platforms such as   Amazon Mechanical Turk .   ICC(k ) is an established way of measuring the   reliability of mean ratings , hence it is readily us-   able by researchers . However , it has some draw-   backs . Being part of the ICC family , ICC ( k ) is   only applicable to continuous data . In addition ,   ICC(k ) measures the reliability of mean ratings ,   therefore it can not accommodate other aggrega-   tion functions . In other words , for other popular   data types , such as majority votes of binary data ,   there is no known coefficients for measuring the   reliability of aggregate ratings . Other than ICC ( k),the authors are not aware of any multi - rater gen-   eralization for other coefficients such as Cohen ‚Äôs   ( 1960 ) kappa or Krippendorff ‚Äôs alpha ( Krippen-   dorff , 2011 ) . We therefore take ICC ( k ) as an inspi-   ration and abstract away from it to define a class of   reliability that describes the reliability of aggregate   ratings for any data types . We denote it kRR .   3 Contributions   ‚Ä¢ We emphasise the reliability of aggregate rat-   ings is higher than that of individual ratings .   ‚Ä¢We give a general definition of kRR , extend-   ing from the definition of IRR , and discuss   three methods for computing it .   ‚Ä¢We conduct two replications of the WordSim-   353 benchmark to validate these methods .   4k - Rater Reliability   We define kRR as the chance - adjusted agreement   between replications of aggregate ratings . This def-   inition is very similar to IRR . In fact , they only   differ in terms of interpretation . kRR is identical   to IRR other than that each individual rating in   the IRR calculation is replaced by a k - rater aggre-   gate rating . After all , the mathematics in IRR are   agnostics to how those labels are produced .   Just like IRR , a minimum of two replications is   required to calculate kRR . Given two vectors of   aggregate ratings , one can calculate the reliability   between them using any IRR coefficients that fit   the purpose . kRR is designed to be analogous to   IRR so that we can build upon the rich IRR litera-   ture and the various coefficient choices for different   experimental conditions and assumptions . For ex-   ample , in a binary task , if all the items are rated   by two fixed but distinct groups of raters ( raters   from different locales ) , Cohen ‚Äôs ( 1960 ) kappa is a   suitable choice . Whereas if the raters groups are   homogeneous , and the rating scale is ordinal ( e.g.   Likert ) , then Krippendorff ‚Äôs alpha ( Krippendorff ,   2011 ) can be used . Just like IRR , kRR is a general   concept and is agnostic to the choice of coefficient .   This definition of kRR can be directly opera-   tionalized by creating replications . We call this   approach to calculating kRR the empirical ap-   proach . We demonstrate it in the next section on   the WordSim-353 benchmark . The empirical ap-   proach is the most direct and most general , with   the drawback that a minimum of two replications379are required . We later present two narrower alter-   natives in Section 5 that do not require replications .   The empirical results will be used as a golden ref-   erence to validate them .   4.1 Replicating the WordSim Dataset   WordSim-353 ( Finkelstein et al . , 2001 ) is a widely   used benchmark for measuring a system ‚Äôs ability   to compute similarity between two words , and has   been cited over 1500 times . The dataset contains   353 word pairs . Each word pair is rated by the   same 13 workers for their similarity on a scale from   1 to 10 , to indicate how similar their meanings   are . The 13 ratings on each word pair are then   aggregated into a mean score . It is important to   note that only the mean of the ratings are utilized by   all the research using this dataset as a benchmark .   So the unit of analysis is the aggregate of the 13   ratings , not individual ratings .   Nearly twenty years have elapsed since the cre-   ation of the WordSim dataset . It is impossible to   recreate the original experimental conditions due   to rater population changes . Therefore , we created   two replications in order to approximate the kRR   of the original dataset . Two is the minimum repli-   cation factor required for the empirical approach ,   though a higher replication would result in a more   accurate measure of kRR .   We used the original annotation guidelines on   Amazon Mechanical Turk . Raters were paid on   average USD 9.5 per hour . In each replication ,   we collected 13 judgements on each of the same   353 word pairs . There was a detail that we did not   follow . In the original experiment , the authors em-   ployed 13 unique raters , and each one rated all 353   word pairs . In our replications , we followed more   modern conventions and limited the contributions   of each individual rater for better generalizability .   This detail aside , these are our best attempts   to replicate the original experiment . The data   is publicly available at https://github .   com / google - research - datasets/   wordsim - replications .   4.2 Empirical kRR Results   We take kcolumns of ratings at random from each   of the two replications , compute the k - rater mean   scores for each replication , and measure the relia-   bility between them using Krippendorf ‚Äôs alpha , the   most widely used and general reliability index . We   do this for k= 1,2 , . . . , 13 . The resulting kRR val-   ues are shown in Fig.1 . At k= 1 , the IRR is 0.574 ,   slightly lower than the 0.6 originally reported in   Finkelstein et al . ( 2001 ) . At k= 13 , thek - rater   reliability is 0.940 , quite a bit higher than the IRR .   In addition , Fig.1 shows the marginal returns on   increasing the number of ratings on the replicated   datasets .   5 Other Approaches to Computing kRR   The empirical approach is general , as it can ac-   commodate any choice of rating scale , aggregation   function , and reliability coefficient . However , it   has a major drawback . As we see in Section 4.1 ,   it can be difficult to do a perfect replication post-   fact . This backward incompatibility will present a   challenge to computing kRR for existing datasets .   Below we present two alternatives that can work   on existing datasets under some conditions without   requiring any additional data collection . One is a   re - sampling based bootstrap approach ( Efron and   Tibshirani , 1994 ) , the other is ICC ( k ) .   5.1 Bootstrap   Bootstrap ( Efron and Tibshirani , 1994 ) is a re-   sampling technique commonly used for quantify-   ing uncertainty in statistical parameter estimation .   One can bootstrap an NLP annotations dataset by   re - sampling ratings within each annotation item   with replacement at the same sample size . If one   treats each bootstrap sample as a replication , then   one can apply the technique discussed in Section 4380to obtain a bootstrapped kRR . Bootstrap is an ap-   proximate technique and works better with larger   sample sizes , typically 20 observations and above   for a single distribution . The 13 - rating redundancy   in the WordSim replications is arguably small for   a typical bootstrap exercise , but it makes up for it   with a large number of items .   Before we apply bootstrap to the original Word-   Sim dataset , we first verify its soundness by com-   paring it against the empirical results in Section 4.2 .   When applied to one of the two recent replications ,   the bootstrapped kRR is 0.943 . This is comparable   to the 0.940 reported in Section 4.2 . We then apply   bootstrap to the original WordSim dataset and find   a bootstrapped kRR of 0.953 ( Table 1 ) . The exact   method introduced below produces a very similar   value at 0.950 .   5.2 Intraclass Correlation   Intraclass correlation is a popular reliability coeffi-   cient for continuous data in behavioral and medical   sciences . ICC gives researchers granular control   over assumptions about the raters . For example ,   each annotation item can be rated by the same set   of raters , or different sets of raters ( interchange-   ability ) . In the former , the raters can be treated as   either fixed or randomly drawn from a population .   Shrout and Fleiss ( 1979 ) and McGraw and Wong   ( 1996 ) give very extensive treatment on different   ICC types for different rater assumptions .   In this paper , we focus on the most basic defini-   tion , one that treats raters as interchangeable . The   ICC for k - rater averages is denoted as ICC ( k ) using   McGraw and Wong ‚Äôs notation . The reliability of   individual ratings is thus given by ICC(1 ) . ICC ( k )   can be computed by summing squares of differ-   ences on the data matrix . Please see Appendix A   for derivation and an illustration . Otherwise , soft-   ware implementations of ICC are also widely avail-   able , e.g. in R and Python .   We first verify ICC ( k ) ‚Äôs accuracy by comparing   it against the empirical results in Section 4.2 . To   do that , we calculate ICC ( k ) for one of the two   recent WordSim replications for k= 1,2 , . . . , 13   and overlay the results ( solid blue ) over the empiri-   cal curve in Fig.1 . We can see ICC ( k ) matches the   empirical results quite well .   After verifying the technique , we compute   ICC(k ) on the original WordSim dataset . We report   in Table 1 both ICC(1 ) and ICC(13 ) to show the   increase in reliability . They are respectively 0.590Unit of analysis Method reliability   single - rating ICC(1 ) 0.590   13 - rating mean ICC(13 ) 0.950   13 - rating mean bootstrap 0.953   and 0.950 .   5.3 Spearman - Brown Formula   Given an experiment with a k - rating redundancy ,   ICC(k ) quantifies the reliability of the k - rater av-   erage . If this reliability is too low , the researcher   may want to increase the value of k. In this case ,   it would be helpful to know how additional ratings   would impact reliability . This is analogous to calcu-   lating the required sample size for a given margin   of error in a poll . For this purpose , the Spearman-   Brown prophecy formula ( Spearman , 1910 ; Brown ,   1910 ) can be a useful tool . It predicts ICC ( k ) for   any value of kbased on ICC(1 ) in the current ex-   periment :   ICC(k)=k¬∑ICC(1 )   1 + ( k‚àí1)¬∑ICC(1 ) . ( 1 )   Warrens ( 2017 ) and de Vet et al . ( 2017 ) recently   proved that SB and ICC ( k ) are indeed equivalent in   expectation , even though they look nothing alike   and were derived in very different contexts . These   findings confirm past observations that SB predicts   empirical results accurately ( Remmers et al . , 1927 ) .   A limitation of SB is clearly that it only works   with ICC . However , Fleiss and Cohen ( 1973 ) show   ICC is actually equivalent to weighted - kappa with   quadratic weights , so it likely has wider applicabil-   ity .   To verify the formula , we apply SB to one of   the two recent WordSim replications and overlay   the results ( dotted red ) over the empirical curve   obtained earlier . When computing SB , we only   provide it with 2 ratings , in order to assess its pre-   dictive accuracy . That is , we first compute ICC(1 )   with 2 randomly drawn ratings from each word381pair , then we plug this ICC(1 ) value into Eq.1 for   k= 1,2 , . . . , 13 . The SB curve is overlaid over   the empirical curve in Fig.1 . We see that SB tracks   the empirical results very well even at high k. This   is remarkable as the empirical approach requires   26 ratings for k= 13 , whereas SB merely requires   2 for any value of k.   6 Conclusions and Discussion   We pointed out where aggregated ratings are used ,   as is the case in many crowdsourced datasets , relia-   bility of aggregate ratings is the correct accounting   of data reliability . We introduced k - rater reliability   ( kRR ) as a multi - rater extension of IRR . We em-   phasise the reliability of aggregate ratings is higher   than that of individual ratings . We present analyti-   cal and bootstrap - based methods for computing the   kRR on the original WordSim dataset . Both meth-   ods produce similar estimates for 13 - rater reliabil-   ity ranging from 0.940 to 0.953 . We conduct two   replications of the entire WordSim-353 benchmark   to validate these methods . We make our replication   data publicly available on GitHub .   While aggregation makes it possible to have reli-   able benchmarks on subjective topics , some read-   ers may feel uneasy about increasing reliability via   gathering additional ratings , as opposed to other tra-   ditional means such as improving rater guidelines .   We suggest to mediate this concern by reporting   both IRR and kRR . In fact , kRR is not meant to re-   place IRR , but rather complement it . IRR speaks to   the reliability of the labeling process , whereas kRR   quantifies the reliability of the aggregated data we   consume . We urge researchers to report both where   possible . In fact , Hallgren ( 2012 ) states , " In cases   where single measures ICCs are low but average-   measures ICCs are high , the researcher may report   both ICCs to demonstrate this discrepancy . "   This research also raises interesting questions   for future research :   1.How do we derive multi - rater generalizations   for coefficients other than ICC ? A lot of NLP   annotations are binary and multi - class . Such   a generalization for majority voting would be   particularly useful to the field .   2.Should we apply the Landis and Koch ( 1977 )   style of reliability cutoffs to kRR , or should   kRR go by a different set of standards ?   We urge researchers to report both IRR and kRR   of aggregated human annotations , and for furtherinquiry around the above fundamental questions   about reliability .   Acknowledgement   We thank Lora Aroyo and Chris Welty for shar-   ing their WordSim replication datasets . We thank   Michael Quinn and Jeremy Miles for their insight-   ful discussions and comments . We also thank all   the crowd workers for providing us with valuable   annotations data .   References382   A Appendix on ICC ( k )   ICC is a family of coefficients . It has slightly dif-   ferent formulations to accommodate different ex-   perimental designs . One of them , ICC ( k ) , quan-   tifies the reliability of average ratings based on k   raters , where the raters are treated as interchange-   able . We illustrate its close form calculation here .   It is mainly re - expressing results from previous   works on ICC calculation , such as Liljequist et al .   ( 2019 ) and McGraw and Wong ( 1996 ) .   ICC(k ) predicates on the one - way random ef-   fects model being the data generation process . The   model takes the form   x=¬µ+œï+œµ,383where xis the rating on item ifrom rater j,¬µis   the grand mean , œïis the mean of item i , andœµis   a random perturbation term . Assume a data matrix   withnrows ( item ) and kcolumns ( raters ) with no   missing data , as one shown in Fig . 2 . Let   ¬Øx=1   nkXXx   be the sample grand mean , and   ¬Øx=1   kXx   be the isample item mean . Let   SSW = XX(x‚àí¬Øx )   SSB = kX(¬Øx‚àí¬Øx )   be respectively the sum of squares due to differ-   ences within items and the sum of squares due to   differences between items . Then the estimator for   the variance of œµ,œÉ , and the estimator for the vari-   ance of œï,œÉ , are respectively   ÀÜœÉ = SSW   n(k‚àí1 )   ÀÜœÉ = SSB   k(n‚àí1)‚àíÀÜœÉ   k.   Then ICC ( k ) can be computed as   ÀÜœÉ   ÀÜœÉ+ ÀÜœÉ / k.   If we apply the above formula to individual rat-   ings , with k= 1 , the resulting reliability is known   as inter - rater reliability . For any k > 1 , it is an   instance of the k - rater reliability proposed in this   paper.384   Valentin Hofmann , Hinrich Sch√ºtze , Janet B. PierrehumbertFaculty of Linguistics , University of OxfordDepartment of Engineering Science , University of OxfordCenter for Information and Language Processing , LMU Munich   valentin.hofmann@ling-phil.ox.ac.uk   Abstract   1 Introduction   The Ô¨Årst step in NLP architectures using pretrained   language models ( PLMs ) is to map text to a se-   quence of tokens corresponding to input embed-   dings . The tokenizers used to accomplish this have   been shown to exhibit various undesirable prop-   erties such as generating segmentations that blur   word meaning ( Bostrom and Durrett , 2020 ; Church ,   2020 ; Hofmann et al . , 2021 ) and generalizing sub-   optimally to new domains ( Tan et al . , 2020 ; Hong   et al . , 2021 ; Sachidananda et al . , 2021 ) .   In this paper , we propose FLOTA ( FewLongest   Token Approximation ) , a simple yet effective   method to mitigate some shortcomings of PLM   tokenizers . FLOTA is motivated by the following   hypothesis : rather than Ô¨Ånding a segmentation that   covers all characters of a word but destroys its mor-   phological structure , it can be more beneÔ¨Åcial to   Ô¨Ånd a segmentation that does not cover all charac-   tersbutpreserves key aspects of the morphology .   We conÔ¨Årm this hypothesis in this paper .   Our study investigates three PLMs and corre-   sponding tokenizers : BERT ( base , uncased ; Devlin   et al . , 2019 ) , which uses WordPiece ( Schuster and   Nakajima , 2012 ; Wu et al . , 2016 ) , GPT-2 ( base ,   cased ; Radford et al . , 2019 ) , which uses byte - pair   encoding ( BPE ; Gage , 1994 ; Sennrich et al . , 2016),and XLNet ( base , cased ; Yang et al . , 2019 ) , which   uses Unigram ( Kudo , 2018 ) . We Ô¨Ånd that FLOTA   increases the morphological quality of all tokeniz-   ers as evaluated on human - annotated gold segmen-   tations as well as the performance of all PLMs on   a text classiÔ¨Åcation challenge set .   Contributions . We introduce FLOTA , a simple   yet effective method to improve the tokenization of   PLMs during Ô¨Ånetuning . FLOTA uses the vocabu-   lary of a standard tokenizer but tries to preserve the   morphological structure of words during tokeniza-   tion . We show that FLOTA has three advantages   compared to standard tokenization : ( i ) it can in-   crease the performance of PLMs on certain tasks ,   sometimes substantially ; ( ii ) it makes inference   more efÔ¨Åcient by shortening the processed token   sequences ; ( iii ) it enhances the robustness of PLMs   with respect to certain types of noise in the data .   All this is achieved without requiring any addi-   tional parameters or resources compared to vanilla   PLM Ô¨Ånetuning . We also release a text classiÔ¨Åca-   tion challenge set that can serve as a benchmark for   future studies on PLM tokenizers .   2 Few Longest Token Approximation   LetVbe a set of tokens that constitute the vocabu-   lary of a tokenizer . For the tokenizers discussed in   this paper , Vcontains words , subwords , and char-   acters . Let  be a model used by the tokenizer to   map text to a sequence of tokens from V.   FLOTA ( Few Longest Token Approximation )   discards  and usesVin a modiÔ¨Åed way . Given   a wordwnot inV , FLOTA tokenizes it by deter-   mining the longest substring s2Vofw , returning   s , and recursing on wns , the string(s ) remain-   ing whensis removed from w. We stop after k   recursive calls or when the residue is null . Fig-   ure 1 provides pseudocode . For the example word   undesirable andk= 2 , FLOTA Ô¨Årst searches on385MS S(w;V )   1l = length ( w )   2forj = ldownto 0   3 fori= 0 tol j+ 1   4 s = w[i::i+j ]   5 ifs2V   6 r = w[0::i]w[i+j::l ]   7 returns , r , i   F T ( w;k;V )   1s;r;i = MS S(w;V )   2ifk1orhyphen ( r )   3F = fg   4F[i ] = s   5 returnF   6F = F T ( r;k 1;V )   7F[i ] = s   8returnF   undesirable and Ô¨Ånds desirable , then searches   onun--------- and Ô¨Ånds un , then stops ( since   k= 2 ; it would also stop for k > 2since the   residue is null ) and returns the tokenization un ,   desirable . The WordPiece tokenization , on the   other hand , is und , es , ira , ble .   FLOTA is guided by the following observations :   many words not in Vare made up of smaller and   typically more frequent elements that determine   their meaning ( e.g. , they are derivatives such as   undesirable ) ; many of these elements are in V.   By recursively searching for the longest substrings ,   we hope to recover the most important meaningful   elements . This is also why it makes sense to stop   afterkrecursions : if FLOTA returns the most im-   portant meaningful elements as the Ô¨Årst few tokens ,   we expect to not lose much by stopping .   3 Evaluation on Gold Segmentations   English inÔ¨Çection is simple , but the language has   highly complex word formation , i.e. , derivation and   compounding ( Cotterell et al . , 2017 ; Pierrehumbert   and Granell , 2018 ) . To evaluate the morphological   quality of FLOTA against the standard tokenizers ,   we thus focus on derivatives and compounds .   Data . Our evaluation uses CELEX ( Baayen   et al . , 1995 ) and LADEC ( Gagn√© et al . , 2019 ) , two   large datasets of human - annotated gold segmen-   tations of morphologically complex words . We   merge both datasets and extract all words consist-   ing of a preÔ¨Åx and a stem ( preÔ¨Åxed derivatives ) ,   a stem and a sufÔ¨Åx ( sufÔ¨Åxed derivatives ) , or two   stems ( compounds ) . We create for each PLM a sub-   set of words where both morphological elements   ( i.e. , stems and afÔ¨Åxes ) are in the tokenizer vocab-   ulary , but the word itself is not in the tokenizer   vocabulary . In such cases , a word needs to be seg-   mented , and it is guaranteed that the gold segmen-   tation is possible given the tokenizer vocabulary .   This procedure results in 11,272 , 11,253 , 10,848   words for BERT , GPT-2 , XLNet , respectively .   Experimental Setup . We deÔ¨Åne three metrics   to analyze how closely FLOTA matches the gold   segmentations . We compare against two alterna-   tive tokenization strategies : representing words   as thekÔ¨Årst tokens returned by the standard to-   kenizer ( FIRST ) and representing words as the k   longest tokens returned by the standard tokenizer   ( LONGEST ) . Recall that the WordPiece tokeniza-   tion of the running example undesirable isund ,   es , ira , ble . Withk= 3 , FIRST is und , es ,   ira ( i.e. , it simply returns the Ô¨Årst ktokens ) and   LONGEST is und , ira , ble ( i.e. , it returns the k386   longest tokens in the order in which they occur in   the standard tokenization ) .   Morphological coverage . We analyze what pro-   portion of morphological elements is covered by   each tokenization strategy for varying k , a mea-   sure that we call morphological coverage , C. For   undesirable andk= 3 , FIRST and LONGEST   contain un(C= 0:5 ) while FLOTA contains both   unanddesirable ( C= 1 ) . We compute the mean   morphological coverage across all words , C.   We Ô¨Ånd that for all three tokenizers , FLOTA   already covers about 99 % of the morphological el-   ements with just k= 2 , a value that FIRST and   LONGEST only reach with k= 4 ( Table 1 , Fig-   ure 2 ) , indicating that FLOTA needs considerably   fewer tokens than the standard tokenization to con-   vey the same amount of semantic and syntactic   information . This can also be seen by examining   the average number of tokens needed to fully to-   kenize a word ( i.e. , k=1 ) , with the values for   FLOTA ( BERT : 2.02 ; GPT-2 : 2.03 ; XLNet : 2.02 )   being lower than the values for the standard tok-   enization ( BERT : 2.30 ; GPT-2 : 2.23 ; XLNet : 2.26 ) .   The pairwise differences are statistically signiÔ¨Åcant   ( p<0:001 ) as shown by two - tailed t - tests .   Stem recall . Given its relevance for the over-   all lexical meaning of a word , we are interested   in how often FLOTA returns the stem at k= 1 .   We test this using a measure that we call stem   recall , R(R= 1 if the token is the stem , oth-   erwiseR= 0 ) , and compute the mean stem re-   callRacross all words . We again compare with   FIRST and LONGEST . Notice the stem according   to the gold segmentation is longer than the second   morphological element in 97 % of the examined   complex words , which means that LONGEST pro-   vides a close estimate of how often the full standard   tokenization contains the stem ( since any other el-   ement in the full standard tokenization is shorter   and hence very unlikely to be the stem).FLOTA returns the stem considerably more often   than either FIRST or LONGEST , but there are clear   differences between the models ( Table 1 ): for GPT-   2 , FLOTA increases Rby more than 15 % while the   difference amounts to 5 % for XLNet .   Full match . Extending the evaluation of stem re-   call , we examine whether the tokenization at k= 2   is identical to the gold segmentation ( which al-   ways has two elements ) using a measure that we   callfull match , M(M= 1 if the tokenization   exactly matches the gold segmentation , otherwise   M= 0 ) . We again compute the mean value M   across all words . Here , the values for both FIRST   and LONGEST are identical to the performance of   the full standard tokenization : for the full standard   tokenization to exactly match a segmentation of   two elements , it must consist of two tokens , and   hence it is necessarily equal to both its Ô¨Årst two   tokens and its longest two tokens . Table 1 shows   that FLOTA substantially improves M.   The evaluation on gold segmentations indicates   that FLOTA increases the morphological quality   of PLM tokenizers compared to the standard to-   kenization and simple alternatives . We also Ô¨Ånd   underlying differences in the morphological qual-   ity of the tokenizers , with BPE and Unigram lying   at the negative and positive extremes , in line with   prior work ( Bostrom and Durrett , 2020 ) . Our anal-   ysis shows that WordPiece lies in between .   4 Evaluation on Downstream Task   We investigate whether the enhanced quality of   FLOTA tokenizations translates to performance on   downstream tasks . We focus on text classiÔ¨Åcation   as one of the most common tasks in NLP .   Data . We create two text classiÔ¨Åcation chal-   lenge sets based on ArXiv , each consisting of three   datasets . SpeciÔ¨Åcally , for the subject areas of com-   puter science , maths , and physics , we extract titles   for the 20 most frequent subareas ( e.g. , Computa-   tion and Language ) . We then sample 100/1,000   titles per subarea , resulting in three text classiÔ¨Å-   cation datasets of 2,000/20,000 titles each , which   we bundle together as ArXiv - S / L. Our sampling387   ensures that ArXiv - S / L require challenging gen-   eralization from a small number of short training   examples with highly complex language . See Ap-   pendix A.1 for more details .   Experimental Setup . We split the six datasets   of ArXiv - S and ArXiv - L into 60 % train , 20 % dev ,   and 20 % test . We then train the three PLMs with   classiÔ¨Åcation heads on the six train splits , once with   the standard tokenizers and once with FLOTA . See   Appendix A.2 for hyperparameters . For FLOTA ,   we treatkas an additional tunable hyperparameter .   We use F1 as the evaluation metric .   Performance . The FLOTA models perform bet-   ter than the models with standard tokenization , al-   beit to varying degrees for the three PLMs ( Table 2 ) .   The difference is most pronounced for GPT-2 , with   FLOTA resulting in large performance gains of up   to 5 % . In addition , GPT-2 performs worse than   the other two PLMs on all datasets , suggesting   that BPE is generally not a good Ô¨Åt for complex   language . BERT also clearly beneÔ¨Åts from using   FLOTA , particularly on ArXiv - S. Out of the three   considered PLMs , XLNet obtains the smallest per-   formance gain from using FLOTA , but it still bene-   Ô¨Åts in the majority of cases .   The advantage of FLOTA mirrors the differences   observed in the morphological analysis , indicating   thatFLOTA helps close the morphological quality   gapbetween standard tokenizations and gold seg-   mentations . Where the gap is large , gains due to   FLOTA are large ( GPT-2 / BPE ) ; where it is small ,   gains due to FLOTA are small ( XLNet / Unigram ) .   BERT / WordPiece again lies in between .   Impact ofk . To test how the performance varies   withk , we focus on BERT and compare the FLOTA   models fork2f1;2;3;4gwith the two alterna-   tives FIRST and LONGEST from Section 3 . See   Appendix A.4 for hyperparameters .   Figure 3 shows that FLOTA only drops slightly   as we decrease k , with the minimum F1 at k= 1   ( 43.6 % ) lying less than 2 % below the maximum   F1 atk= 3 ( 45.4 % ) . In contrast , FIRST and   LONGEST drop substantially as we decrease k ;   for FIRST , the minimum F1 at k= 1(38.2 % ) lies   more than 6 % below the maximum F1 at k= 4   ( 44.8 % ) . The fact that FLOTA is more effective at   preserving performance while reducing the number   of tokens aligns with the observation that it covers   a larger number of morphemes and hence more   semantic and syntactic content than FIRST and   LONGEST for small k(Section 3 ) .   EfÔ¨Åciency . FLOTA allows to reduce the num-   ber of tokens used to tokenize text by varying   k. Since the attention mechanism scales quadrati-   cally with sequence length ( Peng et al . , 2021 ) , this   has beneÔ¨Åcial effects on the computational cost   involved with employing a model trained using   FLOTA . We empirically Ô¨Ånd that even for k= 4   ( the largest value used in the experiments ) , token   sequences generated by FLOTA are on average   shorter than the token sequences generated by the   standard tokenizations . Table 3 shows for one   dataset ( ArXiv - L , physics ) the average sequence   length of titles encoded with the standard tokeniza-   tion versus FLOTA with varying k2f1;2;3;4 g   for the three PLMs .   Robustness . To examine robustness against   noise , a well - known problem for PLMs ( Pruthi   et al . , 2019 ) , we focus on missing whitespace be-   tween words ( Soni et al . , 2019 ) . We randomly drop   the whitespace between two adjacent words with388   probabilityp= 0:3 in ArXiv - S / L. We use unseen   noise , i.e. , we only inject noise during evaluation ,   not training , which is the more realistic and chal-   lenging scenario ( Xue et al . , 2021 ) .   The results show that synthetic noise increases   the performance gap between FLOTA and standard   tokenization ( Table 4 ) . While there is a drop in   performance for all models compared to the exper-   iments without noise , the drop is much more pro-   nounced for standard tokenization ; e.g. , BERT ‚Äôs   performance on ArXiv - L ( test ) drops by 3 % with   FLOTA , but by 10 % without it .   5 Limitations   While we Ô¨Ånd FLOTA to work well on text clas-   siÔ¨Åcation , there are tasks for which FLOTA might   prove a less suitable tokenization method : e.g. , for   small values of k , FLOTA often discards sufÔ¨Åxes ,   which can be important for tasks with a syntactic   component such as POS tagging .   Similar considerations hold for transfer to lan-   guages other than English : e.g. , in the case of   languages with a non - linear morphology such as   Arabic , FLOTA is expected to inherit the insufÔ¨Å-   ciencies of the underlying tokenizer ( Alkaoud and   Syed , 2020 ; Antoun et al . , 2020 ) .   6 Related Work   The question how PLMs are affected by their   tokenizer has attracted growing interest recently .   Bostrom and Durrett ( 2020 ) , Church ( 2020 ) , Klein   and Tsarfaty ( 2020 ) , and Hofmann et al . ( 2021 )   focus on the linguistic properties of tokenizers .   We contribute to this line of work by conducting   the Ô¨Årst comparative analysis of all three common   PLM tokenizers and releasing a challenge set as a   benchmark for future studies . Another strand of   research has sought to improve PLM tokenizers bytraining models from scratch ( Clark et al . , 2021 ; Si   et al . , 2021 ; Xue et al . , 2021 ; Zhang et al . , 2021 ) or   modifying the tokenizer during Ô¨Ånetuning , mostly   by adding tokens and corresponding embeddings   ( Chau et al . , 2020 ; Tan et al . , 2020 ; Hong et al . ,   2021 ; Sachidananda et al . , 2021 ) . FLOTA crucially   differs in that it can be used during Ô¨Ånetuning but   does not add any parameters to the PLM . Further-   more , there has been work improving tokenization   by variously exploiting the probabilistic nature of   tokenizers ( Kudo , 2018 ; Provilkov et al . , 2020 ; Cao   and Rimell , 2021 ) . By contrast , our method does   not need access to the underlying model .   Our study also relates to computational work on   derivational morphology ( Cotterell et al . , 2017 ; Vy-   lomova et al . , 2017 ; Cotterell and Sch√ºtze , 2018 ;   Deutsch et al . , 2018 ; Hofmann et al . , 2020a , b , c )   and word segmentation ( Cotterell et al . , 2016 ; Kann   et al . , 2016 ; Ruzsics and Samard≈æi ¬¥ c , 2017 ; Mager   et al . , 2019 , 2020 ; Seker and Tsarfaty , 2020 ; Am-   rhein and Sennrich , 2021 ) . We are the Ô¨Årst to sys-   tematically evaluate the segmentations of PLM to-   kenizers on human - annotated gold data .   Conceptually , the Ô¨Åndings of our study are in   line with evidence from the cognitive sciences that   knowledge of a longer ( i.e. , more detailed and in-   formative ) sequence takes priority over any knowl-   edge about smaller sequences ( Caramazza et al . ,   1988 ; Laudanna and Burani , 1995 ; Baayen et al . ,   1997 ; Needle and Pierrehumbert , 2018 ) .   7 Conclusion   We introduce FLOTA ( Few Longest Token Approx-   imation ) , a simple yet effective method to improve   the tokenization of pretrained language models   ( PLMs ) . FLOTA uses the vocabulary of a standard   tokenizer but tries to preserve the morphological   structure of words during tokenization . FLOTA   leads to performance gains , makes inference more   efÔ¨Åcient , and substantially enhances the robustness   of PLMs with respect to whitespace noise .   Acknowledgements   This work was funded by the European Research   Council ( # 740516 ) and the Engineering and Phys-   ical Sciences Research Council ( EP / T023333/1 ) .   The Ô¨Årst author was also supported by the German   Academic Scholarship Foundation and the Arts   and Humanities Research Council . We thank the   reviewers for their helpful comments.389Ethical Considerations   FLOTA shortens the average length of sequences   processed by PLMs , thus reducing their energy re-   quirements , a desirable property given their other-   wise detrimental environmental footprint ( Schwartz   et al . , 2019 ; Strubell et al . , 2019 ) .   References390391   A Appendix   A.1 Preprocessing   We exclude texts written in a language other than   English and lowercase all words . We exclude ti-   tles with less than three and more than ten words .   For each title , we compute the proportion of words   starting with a productive preÔ¨Åx from the list pro-   vided by Crystal ( 1997 ) . During sampling , we then   weight titles by this proportion in order to make the   language contained within the datasets as complex   and challenging as possible .   A.2 Hyperparameters   The vocabulary size is 28,996 for BERT , 50,257   for GPT-2 , and 32,000 for XLNet . The number   of trainable parameters is 109,497,620 for BERT ,   124,455,168 for GPT-2 , and 117,324,308 for XL-   Net . The classiÔ¨Åcation head for all three models   uses softmax as the activation function .   We use a batch size of 64 and perform   grid search for the number of epochs   n2 f 1 ; : : : ; 20gand the learning ratel2 f 110;310;110g(selec-   tion criterion : F1 ) . We tune lon ArXiv - L ( physics )   and use the best conÔ¨Åguration on all datasets .   For the FLOTA models , we additionally tune   k2f1;2;3;4g(selection criterion : F1 ) . Models   are trained with categorical cross - entropy as the   loss function and Adam ( Kingma and Ba , 2015 )   as the optimizer . Experiments are performed on a   GeForce GTX 1080 Ti GPU ( 11 GB ) .   A.3 Performance   Table 5 provides breakdowns of the performance   for the individual datasets forming ArXiv - S / L.   A.4 Hyperparameters   All hyperparameters are as for the main experi-   ment ( see Appendix A.2 ) . For the learning rate ,   we use the best conÔ¨Åguration from the main ex-   periment . For FIRST and LONGEST , we tune   k2f1;2;3;4g(selection criterion : F1 ) , identi-   cally to FLOTA in the main experiment .   A.5 Hyperparameters   All hyperparameters are as for the main experiment   ( see Appendix A.2 ) . For the learning rate , we use   the best conÔ¨Åguration from the main experiment .   A.6 Performance   Table 6 provides breakdowns of the performance   for the individual datasets forming ArXiv - S / L.392393   Tassilo Klein   SAP AI Research   tassilo.klein@sap.comMoin Nabi   SAP AI Research   m.nabi@sap.com   Abstract   1 Introduction   Unsupervised learning of representation ( a.k.a . em-   bedding ) is a fundamental problem in NLP and has   been studied extensively in the literature ( Mikolov   et al . , 2013 ; Pennington et al . , 2014 ; McCann et al . ,   2017 ; Peters et al . , 2018 ) . Sentence embeddings   are essential for numerous language processing   applications , such as machine translation , senti-   ment analysis , information retrieval , and seman-   tic search . Recently , self - supervised pre - training   schemes have been successfully used in the context   of transformer architectures , leading to a paradigm   shift in natural language processing and understand-   ing ( Devlin et al . , 2018 ; Liu et al . , 2019 ; Radford   et al . , 2018 ) The idea here is to employ an auxil-   iary task , which enforces an additional objective   during training . Typically , this entails predictions   based on a subset of information from the context .   Most objectives found effective in practice are quite   simple . Some successful examples of such pretext   tasks are Masked Language Model ( MLM ) , Next   Sentence Prediction ( NSP ) , Sentence Order Pre-   diction ( SOP ) , etc . ( Devlin et al . , 2019 ; Liu et al . ,2019 ; Lan et al . , 2019 ) . When working with unla-   beled data , contrastive learning is among the most   powerful approaches in self - supervised learning .   The goal of contrastive representation learning is   to learn an embedding space in such a manner that   similar sample pairs ( i.e. , positive pairs ) stay close   to each other . Simultaneously , dissimilar sample   pairs ( i.e. , negative pairs ) are far pushed apart . To   this end , different augmented views of the same   sample and the augmented views from different   samples are used as positive and negative pairs .   These methods have shown impressive results over   a wide variety of tasks from visual to textual repre-   sentation learning ( Chen et al . , 2020a , b ; Gao et al . ,   2021 ; Grill et al . , 2020 ; Chen and He , 2021 ) .   Different techniques have been proposed for the   augmentation and selection of positive and negative   pairs . For example , DeCLUTR ( Giorgi et al . , 2021 )   proposes to take different spans from the same doc-   ument as positive pairs , while CT ( Carlsson et al . ,   2020 ) aligns embeddings of the same sentence from   two different encoders . CERT ( Fang et al . , 2020 )   applies the back - translation to create augmenta-   tions of original sentences , and IS - BERT ( Zhang   et al . , 2020 ) maximizes the agreement between   global and local features . Finally , CLEAR ( Wu   et al . , 2020 ) employs multiple sentence - level aug-   mentation strategies to learn a sentence represen-   tation . Despite the simplicity of these methods ,   they require careful treatment of negative pairs , re-   lying on large batch sizes ( Chen et al . , 2020a ) or   sophisticated memory strategies . These include   memory banks ( Chen et al . , 2020b ; He et al . , 2020 )   or customized mining strategies ( Klein and Nabi ,   2020 ) to retrieve negative pairs efficiently . In NLP   specifically , the endeavor of ‚Äú hard negative mining ‚Äù   becomes particularly challenging in the unsuper-   vised scenario . Increasing training batch size or the   memory bank size implicitly introduces more hard   negative samples , coming along with the heavy   burden of large memory requirements.394In this paper , we introduce SCD , a novel algo-   rithm for self - supervised learning of sentence em-   bedding . SCD achieves comparable performance in   terms of sentence similarity - based tasks compared   with state - of - the - art contrastive methods without ,   e.g. , employing explicit contrastive pairs . Rather ,   in order to learn sentence representations , the pro-   posed approach leverages the self - contrast imposed   on the augmentations of a single sample . In this   regard , the approach builds upon the idea that suf-   ficiently strong perturbation of the sentence em-   bedding reflects the semantic variations of the sen-   tence . However , it is unclear which perturbation   is simply a slight variation of the sentence without   changing the semantic ( positive pair ) and which   perturbation sufficiently modifies the semantic to   create a negative sample . Such ambiguity mani-   fests itself in the augmented sample sharing the   characteristics of both negative and positive sam-   ples . To accommodate this , we propose an ob-   jective function consisting of two opposing terms ,   which acts on augmentations pairs of a sample : i )   self - contrastive divergence ( repulsion ) , and ii)fea-   ture decorrelation ( attraction ) . The first term treats   the two augmentations as a negative pair pushing   apart the different views . In contrast to that , the   second term attends to the augmentations as a posi-   tive pair . Thus , it maximizes the correlation of the   same feature across the views , learning invariance   w.r.t . the augmentation . Given the opposing na-   ture of the objectives , integrating them in a joint   loss yields a min - max optimization scheme . The   proposed approach avoids degenerated embeddings   by framing the representation learning objective as   an attraction - repulsion trade - off . Simultaneously ,   it learns to improve the semantic expressiveness   of the representation . Due to the difficulty of aug-   mentation in NLP , the proposed approach generates   augmentation ‚Äú on - the - fly ‚Äù for each sample in the   batch . To this end , multiple augmentations are pro-   duced by varying dropout rates for each sample .   We empirically observed that SCD is more robust   to the choice of augmentations than pairwise con-   trastive methods ; we believe that not relying on   contrastive pairs is one of the main reasons for this ,   an observation also made in self - supervised learn-   ing literature such as BYOL ( Grill et al . , 2020 ) .   While other methods take different augmentation   or different copies of models , we utilized the dif-   ferent outputs of the same sentence from standard   dropout . Most related to our paper is ( Gao et al . , 2021 ) ,   which considers using dropout as data augmenta-   tion in the context of contrastive learning . A key   novelty of our approach is that we use the dropout   for creating the self - contrastive pairs , which can   be utilized as both positive and negative . At last ,   we note that our model is different from the pair-   wise feature decorrelation or whitening in ( Zbontar   et al . , 2021 ; Su et al . , 2021 ; Ermolov et al . , 2021 ) ,   which encourage similar representations between   augmented views of a sample while minimizing   the redundancy within the representation vector . A   key difference compared to these methods is that   they ignore the contrastive objective completely .   In contrast , our method takes it into account and   provides the means to treat self - contrastive views   as positive and negative pairs simultaneously .   Our contribution : i ) generation of sentence em-   beddings by leverage multi - dropout ii)elimination   of reliance on negative pairs using self - contrast ,   iii)proposing feature decorrelation objective for   non - contrastive self - supervised learning in NLP .   2 Method   Our approach relies on the generation of two views   AandBof samples . To this end , augmentations   are generated in embedding space for each sample   xin batch X. Batches are created from samples   of setD={(x ) } , where Ndenotes the num-   ber of sample ( sentences ) . Augmentations are pro-   duced by an encoder f , parametrized by Œ∏ . The   output of the encoder is the embeddings of sam-   ples in Xdenoted as H‚àà T andH‚àà T .   HereTdenotes the embedding space . Next , we   let , h‚àà T denote the associated representation   of the sentence . The augmentation embeddings   produced per sample are then denoted handh .   To obtain the different embedding , we leverage a   transformer language model as an encoder in com-   bination with varying dropout rates . Specifically ,   one augmentation is generated with high dropout   and one with lowdropout . This entails employing   different random masks during the encoding phase .   The random masks are associated with different   ratios , randr , with r < r. Integrating the   distinct dropout rates into the encoder , we yield   h = f(x , r)andh = f(x , r ) . Given the   embeddings , we leverage a joint loss , consisting of   two objectives :   minL(f ) + Œ±L(f , p ) ( 1)395   Here Œ±‚ààRdenotes a hyperparameter and p :   T ‚Üí P is a projector ( MLP ) parameterized by Œ∏ ,   which maps the embedding to P , with|P| ‚â´ |T | .   The objective of Lis to increase the contrast of   the augmented embedding , pushing apart the em-   beddings handh . The objective of Lis to   reduce the redundancy and promote invariance w.r.t .   augmentation in a high - dimensional space P. See   Fig . 1 for a schematic illustration of the method .   2.1 Self - Contrastive Divergence :   Self - contrast seeks to create a contrast between the   embeddings arising from different dropouts . Hence ,   Lconsists of the cosine similarity of the samples   in the batch as :   L=1   NXh¬∑(h)    ‚à•h‚à•‚à•h‚à•(2 )   2.2 Feature Decorrelation :   Lseeks to make the embeddings invariant to aug-   mentation while at the same time reducing the re-   dundancy in feature representation . To this end ,   the embedding his projected up from Tto a   high - dimensional space P , where decorrelation is   performed . To avoid clutter in notation , we let   p = p(h)and‚àó ‚àà { A , B } , denote the aug-   mented embedding vectors of sample xafter ap-   plying a projection with p ( . ) . Then , a correlation   matrix is computed from the projected embeddings .   Its entries Care :   C = Xp¬∑p X(p)(p ) !   ( 3)Here , p‚ààRdenotes the jcomponent in the   projected embedding vector . Then the loss objec-   tive for feature decorrelation is defined as :   L=‚àíX(1‚àíC)+ŒªXXC ( 4 )   The first term seeks to achieve augmentation in-   variance by maximization of the cross - correlation   along the diagonal . The second term seeks to re-   duce redundancy in feature representation by mini-   mizing correlation beyond the diagonal . Given that   these objectives are opposing , Œª‚ààRis a hyperpa-   rameter , controlling the trade - off .   3 Experiments & Results   3.1 Training Setup :   Training is started from a pre - trained trans-   former LM . Specifically , we employ the Hugging   Face ( Wolf et al . , 2020 ) implementation of BERT   and RoBERTa . For sentence representation , we   take the embedding of the [ CLS ] token . Then   similar to ( Gao et al . , 2021 ) , we train the model   in an unsupervised fashion on 10randomly sam-   ples sentences from Wikipedia . The LM is trained   with a learning rate of 3.0e‚àí5for1epoch at batch-   size of 192 . The projector MLP qhas three linear   layers , each with 4096 output units in conjunction   with ReLU and BatchNorm in between . For BERT   hyperparameters are Œ±= 0.005,Œª= 0.013 , and   dropout rates are r= 5.0%andr= 15 .0 % .   For RoBERTa hyperparameters are Œ±= 0.0033 ,   Œª= 0.028 , and dropout rates are r= 6.5%and396   r= 24.0 % . The values were obtained by grid-   search . First a coarse - grid was put in place with   a step - size of 0.1forŒ±,10 % for the dropout rates   r , r. ForŒªthe coarse - grid consisted of different   magnitudes { 0.1,0.01,0.001 } . Second , on a fine-   grid with step - size of 0.01and1 % , respectively .   3.2 Evaluation Setup :   Experiments are conducted on 7 standard seman-   tic textual similarity ( STS ) tasks . In addition to   that , we also evaluate on 7 transfer tasks . Specif-   ically , we employ the SentEval toolkit ( Conneau   and Kiela , 2018 ) for evaluation . As proposed by   ( Reimers and Gurevych , 2019 ; Gao et al . , 2021 ) ,   we take STS results as the main comparison of   sentence embedding methods and transfer task re-   sults for reference . For the sake of comparability ,   we follow the evaluation protocol of ( Gao et al . ,   2021 ) , employing Spearman ‚Äôs rank correlation and   aggregation on all the topic subsets .   3.3 Main Results   3.3.1 Semantic Textual Similarity :   We evaluate on 7 STS tasks : ( Agirre et al . , 2012 ,   2013 , 2014 , 2015 , 2016 ) , STS Benchmark ( Cer   et al . , 2017 ) and SICK - Relatedness ( Marelli et al . ,   2014 ) . These datasets come in sentence pairs to-   gether with correlation labels in the range of 0 and   5 , indicating the semantic relatedness of the pairs .   Results for the sentence similarity experiment can   be seen in Tab . 1 . The proposed approach is on-   par with state - of - the - art approaches . Using BERT-   LM , we outperform the next - best approach on STS-   B(+1.19 ) and on SICK - R ( +3.81 ) points . UsingRoBERTa - LM , we outperform the next best compa-   rable approach ( SimCSE - RoBERTA ) on STS-   15(+0.55 % ) and SICK - R ( +3.8 % ) .   3.3.2 Transfer task :   We evaluate our models on the following trans-   fer tasks : MR ( Pang and Lee , 2005 ) , CR ( Hu   and Liu , 2004 ) , SUBJ ( Pang and Lee , 2004 ) ,   MPQA ( Wiebe et al . , 2005 ) , SST-2 ( Socher et al . ,   2013 ) , TREC ( V oorhees and Tice , 2000 ) and   MRPC ( Dolan and Brockett , 2005 ) . To this end ,   a logistic regression classifier is trained on top of   ( frozen ) sentence embeddings produced by differ-   ent methods . We follow default configurations   from SentEval . Results for the transfer task ex-   periment can be seen in Tab . 2 . SCD is on - par397   with state - of - the - art approaches . Using BERT - LM ,   we outperform the next best approach on SUBJ   ( +4.6 % ) and MRPC ( +2.2 % ) . Using RoBERTa-   LM , we outperform the next best comparable ap-   proach ( SimCSE - RoBERTA ) on almost all   benchmarks , with an average margin of ( +2.61 % ) .   3.4 Analysis   3.4.1 Ablation Study :   We evaluated each component ‚Äôs performance by   removing them individually from the loss to assess   both loss terms ‚Äô contributions . It should be noted   thatLof Eq . 2 and Lof Eq . 4 both interact   in a competitive fashion . Hence , only the equi-   librium of these terms yields an optimal solution .   Changes - such as eliminating a term - have detri-   mental effects , as they prevent achieving such an   equilibrium , resulting in a significant drop in per-   formance . See Tab . 3 for the ablation study on mul-   tiple benchmarks . Best performance is achieved in   the presence of all loss terms .   3.4.2 Uniformity and Alignment Analysis :   To better understand the strong performance of   SCD , we borrow the analysis tool from ( Wangand Isola , 2020 ) , which takes alignment between   semantically - related positive pairs and uniformity   of the whole representation space to measure the   quality of learned embeddings . Figure 2 shows   uniformity andalignment of different methods and   their results on the STS . SCD achieves the best   in terms of uniformity , reaching to the supervised   counterparts ( -3.83 ) , which can be related to the   strong effect of the self - contrastive divergence ob-   jective . It shows the self - contrastive pairs can ef-   fectively compensate for the absence of contrastive   pairs . In terms of alignment , SCD is inferior to   other counterparts ( 0.84 ) , which can be attributed   to the fact that our repulsion objective mainly fo-   cuses on the feature decorrelation aiming to learn   a more effective and efficient representation . This   is reflected in the final results on the STS where   SCD obtains significantly higher correlation even   compared to the method with lower alignment such   as BERT - whitening or BERT - flow .   4 Conclusion & Future Work   We proposed a self - supervised representation learn-   ing approach , which leverages the self - contrast of   augmented samples obtained by dropout . Despite   its simplicity , it achieves comparable results with   state - of - the - arts on multiple benchmarks . Future   work will deal with sample - specific augmentation   to improve the embeddings and , particularly , the   representation alignment .   Acknowledgement : We would like to thank   Mahdyar Ravanbakhsh for valuable feedback on   the manuscript.398References399400   Kaitlyn Zhou , Kawin Ethayarajh , Dallas Card , and Dan JurafskyStanford University , { katezhou , kawin , jurafsky}@stanford.eduUniversity of Michigan , dalc@umich.edu   Abstract   1 Introduction   Measuring semantic similarity plays a critical role   in numerous NLP tasks like QA , IR , and MT . Many   such metrics are based on the cosine similarity be-   tween the contextual embeddings of two words   ( e.g. , BERTScore , MoverScore , BERTR , SemDist ;   Kim et al . , 2021 ; Zhao et al . , 2019 ; Mathur et al . ,   2019 ; Zhang et al . , 2020 ) . Here , we demonstrate   that cosine similarity when used with BERT embed-   dings is highly sensitive to training data frequency .   The impact of frequency on accuracy and re-   liability has mostly been studied on static word   embeddings like word2vec . Low frequency words   have low reliability in neighbor judgements ( Hell-   rich and Hahn , 2016 ) , and yield smaller inner prod-   ucts ( Mimno and Thompson , 2017 ) with higher   variance ( Ethayarajh et al . , 2019a ) . Frequency   also correlates with stability ( overlap in nearest   neighbors ) ( Wendlandt et al . , 2018 ) , and plays a   role in word analogies and bias ( Bolukbasi et al . ,   2016 ; Caliskan et al . , 2017 ; Zhao et al . , 2018 ; Etha-   yarajh et al . , 2019b ) . Similar effects have been   found in contextual embeddings , particularly forlow - frequency senses , which seem to cause difÔ¨Åcul-   ties in WSD performance for BERT and RoBERTa   ( Postma et al . , 2016 ; Blevins and Zettlemoyer ,   2020 ; Gessler and Schneider , 2021 ) . Other works   have examined how word frequency impacts the   similarity of sentence embeddings ( Li et al . , 2020 ;   Jiang et al . , 2022 ) .   While previous work has thus mainly focused   on reliability or stability of low frequency words or   senses , our work asks : how does frequency impact   the semantic similarity of high frequency words ?   We Ô¨Ånd that the cosine of BERT embeddings un-   derestimates the similarity of high frequency words   ( to other tokens of the same word or to different   words ) as compared to human judgements . In a   series of regression studies , we Ô¨Ånd that this under-   estimation persists even after controlling for con-   founders like polysemy , part - of - speech , and lemma .   We conjecture that word frequency induces such   distortions via differences in the representational   geometry . We introduce new methods for charac-   terizing geometric properties of a word ‚Äôs represen-   tation in contextual embedding space , and offer a   formal argument for why differences in represen-   tational geometry affect cosine similarity measure-   ment in the two - dimensional case .   2 Effect of Frequency on Cosine   Similarity   To understand the effect of word frequency on   cosine between BERT embeddings ( Devlin et al . ,   2019 ) , we Ô¨Årst approximate the training data fre-   quency of each word in the BERT pre - training   corpus from a combination of the March 1 , 2020   Wikimedia Download and counts from BookCor-   pus ( Zhu et al . , 2015 ; Hartmann and dos Santos ,   2018).We then consider two datasets that include401pairs of words in context with associated human   similarity judgements of words : Word - In - Context   ( WiC ) ( expert - judged pairs of sentences with a   target lemma used in either the same or differ-   ent WordNet , Wiktionary , or VerbNet senses ) and   Stanford Contextualized Word Similarity dataset   ( SCWS ) ( non - expert judged pairs of sentences an-   notated with human ratings of the similarity of   two target terms ) . Using datasets with human   similarity scores allows us to account for human   perceived similarities when measuring the impact   of frequency on cosine ( Pilehvar and Camacho-   Collados , 2019 ; Huang et al . , 2012 ) .   2.1 Study 1 : WiC   Method and Dataset The authors of WiC used   coarse sense divisions as proxies for words having   the same or different meaning and created 5,428   pairs of words in context , labeled as having the   same or different meaning :   ‚Ä¢same meaning : ‚Äú I try to avoid the company of   gamblers ‚Äù and ‚Äú We avoided the ball ‚Äù   ‚Ä¢different meaning : ‚Äú You must carry your   camping gear ‚Äù and ‚Äú Sound carries well over   water ‚Äù .   To obtain BERT - based similarity measurements ,   we use BERT - base - casedto embed each ex-   ample , average the representations of the target   word over the last four hidden layers , and compute   cosine similarity for the pair of representations .   Relation between frequency and similarity in   WiC We want to use ordinary least squares re-   gression to measure the effect of word frequency on   the cosine similarity of BERT embeddings . First ,   we split the WiC dataset into examples that were   labeled as having the ‚Äú same ‚Äù or ‚Äú different ‚Äù mean-   ings . This allows us to to control for perceived   similarity of the two words in context ‚Äî any fre-   quency effects found within these subsets can not be   explained by variation in human judgements . Next ,   we control for a number of other confounding fac-   tors by including them as variables in our OLS   regression . For each target lemma we considered :   frequency : logof the number of occurrences in   BERT ‚Äôs training data   polysemy : logof number of senses in WordNet   is_noun : binary indicator for nouns vs. verbs   same_wordform : binary indicator of having the   same wordform in both contexts ( e.g. , act / act   vs.carry /carries ) ( case insensitive )   An OLS regression predicting cosine similar-   ity from a single independent factor of log(freq )   shows a signiÔ¨Åcant negative association between   cosine and frequency among " same meaning " ex-   amples ( R : 0:13 , coeff‚Äôsp < 0:001 ) and " dif-   ferent meaning " examples ( R : 0:14 , coeff ‚Äôs   p<0:001 ) ( see Figure 1 ) . The same negative fre-   quency effect is found across various model speci-   Ô¨Åcations ( Table 1 in Appendix ) , which also show   signiÔ¨Åcantly greater cosine similarity for those ex-   amples with the same wordform , a signiÔ¨Åcant neg-   ative association with number of senses , and no   difference between nouns and verbs . In summary ,   we Ô¨Ånd that using cosine to measure the semantic   similarity of words via their BERT embeddings   gives systematically smaller similarities the higher   the frequency of the word .   Results : Comparing to human similarity To   compare cosine similarities to WiC ‚Äôs binary human   judgements ( same / different meaning ) , we followed   WiC authors by thresholding cosine values , tuning   the threshold on the training set ( resulting thresh-   old : 0:8 ) . As found in the original WiC paper ,   cosine similarity is somewhat predictive of the ex-   pert judgements ( 0.66 dev accuracy , comparable to   0.65 test accuracy from the WiC authors ) .   Examining the errors as a function of frequency   reveals that cosine similarity is a less reliable pre-   dictor of human similarity judgements for common402terms . Figure 2 shows the average proportion of   examples predicted to be the same meaning as a   function of frequency , grouped into ten bins , each   with the same number of examples . In the highest   frequency bin , humans judged 54 % of the exam-   ples as having the same meaning compared to only   25 % as judged by cosine similarity . This suggests   that in the WiC dataset , relative to humans , the   model underestimates the sense similarity for high   frequency words .   2.2 Study 2 : SCWS   Our Ô¨Årst study shows that after controlling for   sense , cosine will tend to be lower for higher fre-   quency terms . However , the WiC dataset only has   binary labels of human judgements , and only indi-   cates similarity between occurrences of the same   word . We want to measure if these frequency ef-   fects persist across different words and control for   more Ô¨Åne - grained human similarity judgements .   Method and Dataset SCWS contains crowd   judgements of the similarity of two words in con-   text ( scale of 1 to 10 ) . We split the dataset based on   whether the target words are the same or different   ( break = break vsdance = sing ) ; this both allows us   to conÔ¨Årm our results from WiC and also determine   whether frequency - based effects exist in similarity   measurements across words . We use the same em-   bedding method as described for WiC , and again   use regression to predict cosine similarities fromthe following features :   frequency : average of log(freq)of both words   polysemy : average of log(sense ) of both words   average rating : average rating of semantic simi-   larity as judged by humans on a scale of 1 to   10 ( highest ) .   Results If we only use frequency , we Ô¨Ånd that   it mildly explains the variance in cosine similar-   ity both within ( R : 0:12 , coeff‚Äôsp < 0:001 )   and across words ( R : 0:06 , coeff‚Äôsp < 0:001 ) .   Adding in human average rating as a feature , fre-   quency is still a signiÔ¨Åcant feature with a negative   coefÔ¨Åcient . High frequency terms thus tend to have   lower cosine similarity scores , even after account-   ing for human judgements . When using all features ,   the linear regression models explain 34 % of the to-   tal variance in cosine similarity , with frequency   still having a signiÔ¨Åcant negative effect ( Table 2 in   Appendix ) . Finally , we verify that for a model with   only human ratings , error ( true - predicted cosine )   is negatively correlated with frequency in held out   data ( Pearson ‚Äôs r= 0:18;p < 0:01 ) , indicat-   ing an underestimation of cosine in high frequency   words ( see Figure 5 in Appendix ) .   This Ô¨Ånding suggests that using frequency as a   feature might help to better match human judge-   ments of similarity . We test this hypothesis by   training regression models to predict human rat-   ings , we Ô¨Ånd that frequency does have a signiÔ¨Åcant   positive effect ( Table 3 in Appendix ) but the over-   all improvement over using cosine alone is rela-   tively small ( R= 44:6%vsR= 44:3%with or   without frequency ) . We conclude that the problem   of underestimation in cosine similarity can not be   resolved simply by using a linear correction for   frequency .   3 Minimum Bounding Hyperspheres   In order to understand why frequency inÔ¨Çuences co-   sine similarity , we analyze the geometry of the con-   textual embeddings . Unlike static vectors ‚Äì where   each word type is represented by a single point   ‚Äì the variation in contextualized embeddings de-   pends on a word ‚Äôs frequency in training data . We ‚Äôll   call embeddings of a single word type sibling em-   beddings or asibling cohort . To measure variation ,   we ‚Äôll use the radius of the smallest hypersphere that   contains a set of sibling embeddings ( the minimum   bounding hypersphere ) . We tested many ways to   measure the space created by high - dimensional   vectors . Our results are robust to various other403   measures of variation , including taking the aver-   age , max , or variance of pairwise distance between   sibling embeddings , the average norm of sibling   embeddings , and taking the PCA of these vectors   and calculating the convex hull of sibling embed-   dings in lower dimensions ( see Table 29 in the   Appendix ) . Here we relate frequency to spatial   variation , providing both empirical evidence and   theoretical intuition .   For a sample of 39,621 words , for each word   we took 10 instances of its sibling embeddings   ( example sentences queried from Wikipedia ) , cre-   ated contexutalized word embeddings using Hug-   ging Face ‚Äôs bert - base - cased model , and cal-   culated the radius of the minimum bounding hyper-   sphere encompassing them . As shown in Figure   3 , there is a signiÔ¨Åcant , strong positive correlation   between frequency and size of bounding hyper-   sphere ( Pearson ‚Äôs r= 0:62;p < : 001 ) . Notably ,   since the radius was calculated in 768 dimensions ,   an increase in radius of 1 % results in a hypersphere   volume nearly 2084 times larger .   Since frequency and polysemy are highly corre-   lated , we want to measure if frequency is a signiÔ¨Å-   ca nt feature for explaining the variance of bound - ing hyperspheres . Using the unique words of the   WiC dataset , we run a series of regressions to pre-   dict the radius of bounding hyperspheres . On their   own , frequency and polysemy explain for 48 % and   45 % of the radii ‚Äôs variance . Using both features ,   frequency and polysemy explains for 58 % of the   radii ‚Äôs variance and both features are signiÔ¨Åcant ‚Äì   demonstrating that frequency is a signiÔ¨Åcant fea-   ture in predicting radii of bounding hyperspheres   ( Tables 25 , 26 , 27 in Appendix ) .   Among the unique words of the WiC dataset ,   the radii of the target word correlates with training   data frequency ( Pearson ‚Äôs r : 0:69;p < 0:001 ) .   Across the WiC dataset , the radii explains for 17 %   of the variance in cosine similarity ( Table 28 in   Appendix ) .   3.1 Theoretical Intuition   Here , we offer some theoretical intuition in 2D for   why using cosine similarity to estimate semantic   similarity can lead to underestimation ( relative to   human judgements ) . Let ~ w2Rdenote the target   word vector , against which we ‚Äôre measuring cosine   similarity . Say there were a bounding ball Bwith   center~ xto which~ wis tangent . If we normalize   every point in the bounding ball , it will form an   arc on the unit circle . The length of this arc is   2= 2 arcsin :   ‚Ä¢Letdenote the angle made by xand the   tangent vector ~ w.   ‚Ä¢sin= , so the arc length on the unit cir-   cle isr= arcsin(normalized points ) .   ‚Ä¢Multiply by 2 to get the arclength between   both ( normalized ) tangent vectors .   Since the arclength is monotonic increasing in r ,   if the bounding ball were larger ‚Äî while still being   tangent to~ w ‚Äî the arclength will be too .   The cosine similarity between a point in the   bounding ball and ~ wis equal to the dot product   between the projection of the former onto the unit   circle ( i.e. , somewhere on the arc ) and the normal-   ized~ w. This means that only a certain span of the   arclength maps to sibling embeddings ~ xsuch that   cos(~ x;~ w)t , wheretis the threshold required   to be judged as similar by humans ( see Footnote   3 and Figure 4 ) . If Bwere larger while still be-   ing tangent to w , the arclength would increase but   the span of the arc containing siblings embeddings404   sufÔ¨Åciently similar to wwould not . This means a   greater proportion of the sibling embeddings will   fail to meet this threshold , assuming that the dis-   tribution of sibling embeddings in Bdoes not   change . Because , in practice , more frequent words   have larger bounding balls , depending on how the   bounding ball of a word xgrows relative to some   ~ w , the similarity of xandwcan be underestimated .   This helps explain the Ô¨Åndings in Figure 2 , but it   does not explain why more frequent words have   lower similarity with themselves across different   contexts , since that requires knowledge of the em-   bedding distribution in the bounding ball . The latter   is likely due to more frequent words having less   anisotropic representations ( Ethayarajh , 2019 ) .   4 Discussion and Conclusion   Cosine distance underestimates compared to hu-   mans the semantic similarity of frequent words in a   variety of settings ( expert versus non - expert judged ,   and within word sense and across words ) . This Ô¨Ånd-   ing has large implications for downstream tasks ,   given that single - point similarity metrics are used   in a variety of methods and experiments ( Reimers   and Gurevych , 2019 ; Reif et al . , 2019 ; Zhang et al . ,   2020 ; Zhao et al . , 2019 ; Mathur et al . , 2019 ; Kim   et al . , 2021 ) . Word frequency in pre - training data   also affects the representational geometry of con-   textualized embeddings , low frequency words be - ing more concentrated geometrically . One exten-   sion of this work might examine how variables such   as sentiment and similarity / dissimilarity between   sentence contexts could impact both human - judged   and embedding - based similarity metrics .   Because training data frequency is something   that researchers can control , understanding these   distortions is critical to training large language   models . Frequency - based interventions might even   be able to correct for these systematic underestima-   tions of similarity ( e.g. , by modifying training data ) ,   which could be important where certain words or   subjects may be inaccurately represented . For ex-   ample , Zhou et al . ( 2022 ) illustrates how training   data frequencies can lead to discrepancies in the   representation of countries , and ‚Äî since frequency   is highly correlated with a country ‚Äôs GDP ‚Äî can per-   petuate historic power and wealth inequalities . Fu-   ture work could also examine how and if frequency   effects could be mitigated by post - processing tech-   niques which improve the correlation between hu-   man and semantic similarities ( Timkey and van   Schijndel , 2021 ) .   The semantic similarity distortions caused by   the over - and under - representation of topics is an-   other reason why documentation for datasets is crit-   ical for increasing transparency and accountability   in machine learning models ( Gebru et al . , 2021 ;   Mitchell et al . , 2019 ; Bender and Friedman , 2018 ;   Ethayarajh and Jurafsky , 2020 ; Ma et al . , 2021 ) .   As language models increase in size and training   data becomes more challenging to replicate , we   recommend that word frequencies and distortions   be revealed to users , bringing awareness to the po-   tential inequalities in datasets and the models that   are trained on them . In the future , we hope to see   research that more critically examines the down-   stream implications of these Ô¨Åndings and various   mitigation techniques for such distortions .   Acknowledgements   We sincerely thank Isabel Papadimitriou and our   anonymous reviewers for their support , insights ,   and helpful feedback . This research has been sup-   ported in part by a Hoffman - Yee Research Grant   from the Stanford Institute for Human - Centered   AI , award IIS-2128145 from the NSF , Stanford   Data Science , a Stanford Graduate Fellowship , a   Facebook Fellowship , and Canada ‚Äôs NSERC.405References406407A Appendix   For readability , we ‚Äôve summarized the key results   from the regressions in 1 and 2 . Table 1 contains re-   sults from our WiC experiments where we measure   frequency ‚Äôs impact on cosine similarity . We con-   trol for human judgements of similarity by splitting   the dataset by human labels of " same " and " differ-   ent " meaning words . The same trends hold for the   whole dataset as well .   Table 2 contains results from the SCWS exper-   iments we measure frequency ‚Äôs impact on cosine   similarity within and across word similarities . Sim-   ilar to the WiC results , we see that frequency does   impact cosine similarity , with higher words having   lower similarities .   Table 3 contains results from the SCWS exper-   iments where we measure frequency ‚Äôs impact on   human ratings . We see that frequency does not   explain human ratings but when used in a model   with cosine similarity , frequency has a positive co-   efÔ¨Åcient , indicating it is correcting for the underes-   timation of cosine similarity .   B Regression results from WiC   experiments   Tables 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 .   C Regression results from SCWS   experiments   Tables 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19   D Regression results from SCWS   experiments , explaining for the   difference between cosine similarity   and human judgements   Tables 20 , 21 , 22 , 23 , 24 .   Cosine similarity is partially predictive of hu-   man similarity judgements . The full model shows   a signiÔ¨Åcant positive effect of frequency 24 indi-   cating that for a given level of cosine similarity ,   more frequent terms will judged by humans to   be more similar , again demonstrating that cosine   under - estimates semantic similarity for frequent   terms .   The effect is relatively small , however ; for a   word that is twice as frequent , the increase in hu-   man rating will be 0.0989 ( See table 23 ) . Removing   frequency from the model reduces Rfrom 40.8 %   to 40.4 % . Polysemy shows the opposite effect ;   those words with more senses are likely to be ratedas less similar . In a model with only cosine and   polysemy factors , however , frequency has no rela-   tionship with human judgements , indicating that   including frequency is correcting for the semantic   distortion of cosine in the full model .   E Regression results from minimum   bounding hyperspheres   Using frequency and polysemy to explain for the   variability in bounding ball radii . Tables 25 , 26 , 27 .   Using radius of the bounding ball to explain for the   variability of cosine similarity . Table 28 .   F Other ways of measuring the space of   sibling embeddings   Using a smaller sample of words ( 10,000 words   out of the initial39,000 words ) , we calculate the   space occupied by these sibling embeddings using   a variety of other metrics . In each metric , we Ô¨Ånd   strong correlations between ( log ) frequency and   the metric in question ( see table 29 ) .   G Residual of Predicted Cosine   For the SCWS dataset , use 1,000 samples as the   train set and use the rest as the development set .   We train a linear regression model to predict cosine   similarity using only human ratings . Taking the dif-   ference between cosine similarity and the predicted   similarity , we plot this error relative to frequency .   We see a negative correlation between this error   and frequency r= 0:18;p < 0:001 , indicating   that there is an underestimation of cosine similar-   ity among the high frequency words . Results are   shown in Figure 5.408409OLS Predicting Average Human Rating ( Scale of 1 - 10 )   Feature Model 1 Model 2 Model 3 Model 4 Model 5   avglog(freq ) -0.057 - 0.099 - 0.076   avglog(sense ) - - -0.0440 -0.134 -0.189   cosine - 16.345 16.665 13.513 13.809   same_word - - - 1.7228 1.687   R0.002 0.404 0.408 0.443 0.446   Table Number 20 21 22 23 24   Dep . Variable : Cosine Similarity R - squared : 0.127   Model : OLS Adj . R - squared : 0.127   Method : Least Squares F - statistic : 395.1   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 3.55e-82   Time : 22:12:38 Log - Likelihood : 2947.0   No . Observations : 2713 AIC : -5890 .   Df Residuals : 2711 BIC : -5878 .   Df Model : 1   coef std err t P > jtj[0.025 0.975 ]   constant 0.9976 0.013 77.728 0.000 0.972 1.023   log2(freq ) -0.0141 0.001 -19.876 0.000 -0.015 -0.013   Omnibus : 1.261 Durbin - Watson : 1.952   Prob(Omnibus ): 0.532 Jarque - Bera ( JB ): 1.189   Skew : 0.044 Prob(JB ): 0.552   Kurtosis : 3.053 Cond . No . 149.410Dep . Variable : Cosine Similarity R - squared : 0.144   Model : OLS Adj . R - squared : 0.144   Method : Least Squares F - statistic : 228.2   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 2.48e-92   Time : 22:12:38 Log - Likelihood : 2973.7   No . Observations : 2713 AIC : -5941 .   Df Residuals : 2710 BIC : -5924 .   Df Model : 2   coef std err t P > jtj[0.025 0.975 ]   constant 0.9997 0.013 78.627 0.000 0.975 1.025   log2(freq ) -0.0115 0.001 -14.624 0.000 -0.013 -0.010   log2(senses ) -0.0118 0.002 -7.330 0.000 -0.015 -0.009   Omnibus : 8.024 Durbin - Watson : 1.954   Prob(Omnibus ): 0.018 Jarque - Bera ( JB ): 9.222   Skew : 0.060 Prob(JB ): 0.00994   Kurtosis : 3.259 Cond . No . 153 .   Dep . Variable : Cosine Similarity R - squared : 0.203   Model : OLS Adj . R - squared : 0.202   Method : Least Squares F - statistic : 230.2   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 5.14e-133   Time : 22:12:38 Log - Likelihood : 3070.5   No . Observations : 2713 AIC : -6133 .   Df Residuals : 2709 BIC : -6109 .   Df Model : 3   coef std err t P > jtj[0.025 0.975 ]   constant 0.9367 0.013 71.757 0.000 0.911 0.962   log2(freq ) -0.0130 0.001 -16.984 0.000 -0.015 -0.012   log2(senses ) -0.0076 0.002 -4.833 0.000 -0.011 -0.005   same_wordform 0.0447 0.003 14.158 0.000 0.039 0.051   Omnibus : 13.328 Durbin - Watson : 1.917   Prob(Omnibus ): 0.001 Jarque - Bera ( JB ): 14.587   Skew : -0.123 Prob(JB ): 0.000680   Kurtosis : 3.261 Cond . No . 163.411Dep . Variable : Cosine Similarity R - squared : 0.204   Model : OLS Adj . R - squared : 0.203   Method : Least Squares F - statistic : 173.4   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 2.26e-132   Time : 22:12:38 Log - Likelihood : 3071.8   No . Observations : 2713 AIC : -6134 .   Df Residuals : 2708 BIC : -6104 .   Df Model : 4   coef std err t P > jtj[0.025 0.975 ]   constant 0.9355 0.013 71.569 0.000 0.910 0.961   log2(freq ) -0.0126 0.001 -15.858 0.000 -0.014 -0.011   log2(senses ) -0.0090 0.002 -5.030 0.000 -0.013 -0.005   same_wordform 0.0467 0.003 13.760 0.000 0.040 0.053   is_noun -0.0061 0.004 -1.629 0.103 -0.013 0.001   Omnibus : 14.009 Durbin - Watson : 1.915   Prob(Omnibus ): 0.001 Jarque - Bera ( JB ): 15.019   Skew : -0.135 Prob(JB ): 0.000548   Kurtosis : 3.244 Cond . No . 164 .   Dep . Variable : Cosine Similarity R - squared : 0.136   Model : OLS Adj . R - squared : 0.136   Method : Least Squares F - statistic : 427.3   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 2.94e-88   Time : 22:12:38 Log - Likelihood : 2926.4   No . Observations : 2710 AIC : -5849 .   Df Residuals : 2708 BIC : -5837 .   Df Model : 1   coef std err t P > jtj[0.025 0.975 ]   constant 1.0077 0.009 109.007 0.000 0.990 1.026   log2(freq ) -0.0109 0.001 -20.670 0.000 -0.012 -0.010   Omnibus : 45.476 Durbin - Watson : 1.977   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 45.736   Skew : -0.298 Prob(JB ): 1.17e-10   Kurtosis : 2.778 Cond . No . 103.412Dep . Variable : Cosine Similarity R - squared : 0.142   Model : OLS Adj . R - squared : 0.141   Method : Least Squares F - statistic : 224.2   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 8.17e-91   Time : 22:12:38 Log - Likelihood : 2935.6   No . Observations : 2710 AIC : -5865 .   Df Residuals : 2707 BIC : -5847 .   Df Model : 2   coef std err t P > jtj[0.025 0.975 ]   constant 0.9974 0.010 104.755 0.000 0.979 1.016   log2(freq ) -0.0090 0.001 -13.270 0.000 -0.010 -0.008   log2(senses ) -0.0063 0.001 -4.283 0.000 -0.009 -0.003   Omnibus : 38.934 Durbin - Watson : 1.973   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 39.612   Skew : -0.283 Prob(JB ): 2.50e-09   Kurtosis : 2.823 Cond . No . 109 .   Dep . Variable : Cosine Similarity R - squared : 0.241   Model : OLS Adj . R - squared : 0.240   Method : Least Squares F - statistic : 285.7   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 4.36e-161   Time : 22:12:38 Log - Likelihood : 3100.7   No . Observations : 2710 AIC : -6193 .   Df Residuals : 2706 BIC : -6170 .   Df Model : 3   coef std err t P > jtj[0.025 0.975 ]   constant 0.8928 0.011 84.562 0.000 0.872 0.914   log2(freq ) -0.0092 0.001 -14.435 0.000 -0.010 -0.008   log2(senses ) -0.0035 0.001 -2.513 0.012 -0.006 -0.001   same_wordform 0.0588 0.003 18.728 0.000 0.053 0.065   Omnibus : 80.675 Durbin - Watson : 1.981   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 87.234   Skew : -0.434 Prob(JB ): 1.14e-19   Kurtosis : 3.139 Cond . No . 130.413Dep . Variable : Cosine Similarity R - squared : 0.242   Model : OLS Adj . R - squared : 0.241   Method : Least Squares F - statistic : 215.8   Date : Thu , 14 Oct 2021 Prob ( F - statistic ): 6.75e-161   Time : 22:12:38 Log - Likelihood : 3103.2   No . Observations : 2710 AIC : -6196 .   Df Residuals : 2705 BIC : -6167 .   Df Model : 4   coef std err t P > jtj[0.025 0.975 ]   constant 0.8952 0.011 84.424 0.000 0.874 0.916   log2(freq ) -0.0096 0.001 -14.547 0.000 -0.011 -0.008   log2(senses ) -0.0022 0.002 -1.457 0.145 -0.005 0.001   same_wordform 0.0560 0.003 16.512 0.000 0.049 0.063   is_noun 0.0078 0.003 2.228 0.026 0.001 0.015   Omnibus : 76.318 Durbin - Watson : 1.983   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 82.141   Skew : -0.421 Prob(JB ): 1.46e-18   Kurtosis : 3.139 Cond . No . 132 .   Dep . Variable : Cosine Similarity R - squared : 0.120   Model : OLS Adj . R - squared : 0.115   Method : Least Squares F - statistic : 28.77   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.12e-07   Time : 12:16:53 Log - Likelihood : 203.87   No . Observations : 214 AIC : -403.7   Df Residuals : 212 BIC : -397.0   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 1.0762 0.063 17.127 0.000 0.952 1.200   avg_freq -0.0196 0.004 -5.364 0.000 -0.027 -0.012   Omnibus : 7.823 Durbin - Watson : 2.040   Prob(Omnibus ): 0.020 Jarque - Bera ( JB ): 9.129   Skew : -0.307 Prob(JB ): 0.0104   Kurtosis : 3.804 Cond . No . 169.414Dep . Variable : Cosine Similarity R - squared : 0.225   Model : OLS Adj . R - squared : 0.221   Method : Least Squares F - statistic : 61.58   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.07e-13   Time : 12:20:20 Log - Likelihood : 217.54   No . Observations : 214 AIC : -431.1   Df Residuals : 212 BIC : -424.3   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.5856 0.021 28.308 0.000 0.545 0.626   average_rating 0.0223 0.003 7.847 0.000 0.017 0.028   Omnibus : 31.336 Durbin - Watson : 2.183   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 64.374   Skew : -0.711 Prob(JB ): 1.05e-14   Kurtosis : 5.279 Cond . No . 25.5   Dep . Variable : Cosine Similarity R - squared : 0.320   Model : OLS Adj . R - squared : 0.314   Method : Least Squares F - statistic : 49.70   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.06e-18   Time : 12:20:20 Log - Likelihood : 231.56   No . Observations : 214 AIC : -457.1   Df Residuals : 211 BIC : -447.0   Df Model : 2   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.8939 0.060 14.907 0.000 0.776 1.012   avg_freq -0.0176 0.003 -5.434 0.000 -0.024 -0.011   average_rating 0.0211 0.003 7.893 0.000 0.016 0.026   Omnibus : 18.260 Durbin - Watson : 2.246   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 27.332   Skew : -0.524 Prob(JB ): 1.16e-06   Kurtosis : 4.402 Cond . No . 197.415Dep . Variable : Cosine Similarity R - squared : 0.343   Model : OLS Adj . R - squared : 0.334   Method : Least Squares F - statistic : 36.58   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 4.63e-19   Time : 12:20:20 Log - Likelihood : 235.24   No . Observations : 214 AIC : -462.5   Df Residuals : 210 BIC : -449.0   Df Model : 3   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.9469 0.062 15.214 0.000 0.824 1.070   avg_freq -0.0161 0.003 -4.983 0.000 -0.022 -0.010   average_rating 0.0198 0.003 7.417 0.000 0.015 0.025   avg_sense -0.0192 0.007 -2.711 0.007 -0.033 -0.005   Omnibus : 13.882 Durbin - Watson : 2.255   Prob(Omnibus ): 0.001 Jarque - Bera ( JB ): 18.177   Skew : -0.458 Prob(JB ): 0.000113   Kurtosis : 4.095 Cond . No . 212 .   Dep . Variable : Cosine Similarity R - squared : 0.059   Model : OLS Adj . R - squared : 0.058   Method : Least Squares F - statistic : 87.37   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 3.41e-20   Time : 12:20:20 Log - Likelihood : 1557.3   No . Observations : 1406 AIC : -3111 .   Df Residuals : 1404 BIC : -3100 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.7858 0.019 42.044 0.000 0.749 0.822   avg_freq -0.0106 0.001 -9.347 0.000 -0.013 -0.008   Omnibus : 12.804 Durbin - Watson : 1.683   Prob(Omnibus ): 0.002 Jarque - Bera ( JB ): 16.004   Skew : -0.130 Prob(JB ): 0.000335   Kurtosis : 3.453 Cond . No . 145.416Dep . Variable : Cosine Similarity R - squared : 0.305   Model : OLS Adj . R - squared : 0.304   Method : Least Squares F - statistic : 614.9   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 7.11e-113   Time : 12:20:20 Log - Likelihood : 1770.2   No . Observations : 1406 AIC : -3536 .   Df Residuals : 1404 BIC : -3526 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.5366 0.004 150.800 0.000 0.530 0.544   average_rating 0.0208 0.001 24.796 0.000 0.019 0.022   Omnibus : 32.918 Durbin - Watson : 1.861   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 39.508   Skew : -0.302 Prob(JB ): 2.64e-09   Kurtosis : 3.556 Cond . No . 8.58   Dep . Variable : Cosine Similarity R - squared : 0.336   Model : OLS Adj . R - squared : 0.335   Method : Least Squares F - statistic : 355.7   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 1.12e-125   Time : 12:20:20 Log - Likelihood : 1803.2   No . Observations : 1406 AIC : -3600 .   Df Residuals : 1403 BIC : -3585 .   Df Model : 2   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.6684 0.016 40.691 0.000 0.636 0.701   avg_freq -0.0079 0.001 -8.210 0.000 -0.010 -0.006   average_rating 0.0200 0.001 24.238 0.000 0.018 0.022   Omnibus : 35.771 Durbin - Watson : 1.832   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 44.869   Skew : -0.305 Prob(JB ): 1.81e-10   Kurtosis : 3.628 Cond . No . 156.417Dep . Variable : Cosine Similarity R - squared : 0.337   Model : OLS Adj . R - squared : 0.335   Method : Least Squares F - statistic : 237.1   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.09e-124   Time : 12:20:20 Log - Likelihood : 1803.4   No . Observations : 1406 AIC : -3599 .   Df Residuals : 1402 BIC : -3578 .   Df Model : 3   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 0.6670 0.017 40.027 0.000 0.634 0.700   avg_freq -0.0076 0.001 -7.044 0.000 -0.010 -0.005   average_rating 0.0199 0.001 23.983 0.000 0.018 0.022   avg_sense -0.0010 0.002 -0.516 0.606 -0.005 0.003   Omnibus : 36.276 Durbin - Watson : 1.832   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 45.556   Skew : -0.308 Prob(JB ): 1.28e-10   Kurtosis : 3.632 Cond . No . 160 .   Dep . Variable : Human Rating R - squared : 0.002   Model : OLS Adj . R - squared : 0.001   Method : Least Squares F - statistic : 3.074   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 0.0797   Time : 13:15:45 Log - Likelihood : -3750.9   No . Observations : 1620 AIC : 7506 .   Df Residuals : 1618 BIC : 7517 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 5.0152 0.538 9.330 0.000 3.961 6.070   avg_freq -0.0568 0.032 -1.753 0.080 -0.120 0.007   Omnibus : 229.333 Durbin - Watson : 1.972   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 91.858   Skew : 0.385 Prob(JB ): 1.13e-20   Kurtosis : 2.124 Cond . No . 147.418Dep . Variable : Human Rating R - squared : 0.404   Model : OLS Adj . R - squared : 0.403   Method : Least Squares F - statistic : 1096 .   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 6.45e-184   Time : 13:15:45 Log - Likelihood : -3333.6   No . Observations : 1620 AIC : 6671 .   Df Residuals : 1618 BIC : 6682 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant -6.2058 0.314 -19.748 0.000 -6.822 -5.589   cosine_similarity 16.3453 0.494 33.101 0.000 15.377 17.314   Omnibus : 25.721 Durbin - Watson : 1.974   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 24.246   Skew : 0.260 Prob(JB ): 5.43e-06   Kurtosis : 2.703 Cond . No . 14.7   Dep . Variable : Human Rating R - squared : 0.408   Model : OLS Adj . R - squared : 0.407   Method : Least Squares F - statistic : 371.8   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 1.31e-183   Time : 13:15:45 Log - Likelihood : -3327.3   No . Observations : 1620 AIC : 6663 .   Df Residuals : 1616 BIC : 6684 .   Df Model : 3   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant -7.9168 0.575 -13.778 0.000 -9.044 -6.790   avg_freq 0.0989 0.028 3.473 0.001 0.043 0.155   avg_sense -0.0440 0.048 -0.911 0.362 -0.139 0.051   cosine_similarity 16.6654 0.500 33.304 0.000 15.684 17.647   Omnibus : 25.797 Durbin - Watson : 1.972   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 22.821   Skew : 0.235 Prob(JB ): 1.11e-05   Kurtosis : 2.657 Cond . No . 252.419Dep . Variable : Human Rating R - squared : 0.443   Model : OLS Adj . R - squared : 0.442   Method : Least Squares F - statistic : 428.7   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 7.28e-205   Time : 13:15:45 Log - Likelihood : -3278.2   No . Observations : 1620 AIC : 6564 .   Df Residuals : 1616 BIC : 6586 .   Df Model : 3   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant -4.2809 0.379 -11.310 0.000 -5.023 -3.539   avg_sense -0.1339 0.044 -3.012 0.003 -0.221 -0.047   cosine_similarity 13.5126 0.547 24.707 0.000 12.440 14.585   same_word 1.7228 0.161 10.668 0.000 1.406 2.040   Omnibus : 24.052 Durbin - Watson : 2.007   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 20.099   Skew : 0.203 Prob(JB ): 4.32e-05   Kurtosis : 2.635 Cond . No . 46.2   Dep . Variable : Human Rating R - squared : 0.446   Model : OLS Adj . R - squared : 0.444   Method : Least Squares F - statistic : 324.7   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 3.91e-205   Time : 13:15:45 Log - Likelihood : -3274.5   No . Observations : 1620 AIC : 6559 .   Df Residuals : 1615 BIC : 6586 .   Df Model : 4   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant -5.5590 0.600 -9.258 0.000 -6.737 -4.381   avg_freq 0.0757 0.028 2.738 0.006 0.021 0.130   avg_sense -0.1892 0.049 -3.881 0.000 -0.285 -0.094   cosine_similarity 13.8092 0.556 24.816 0.000 12.718 14.901   same_word 1.6872 0.162 10.435 0.000 1.370 2.004   Omnibus : 24.612 Durbin - Watson : 2.005   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 19.555   Skew : 0.187 Prob(JB ): 5.67e-05   Kurtosis : 2.612 Cond . No . 285.420Dep . Variable : Radius of Bounding Ball R - squared : 0.477   Model : OLS Adj . R - squared : 0.477   Method : Least Squares F - statistic : 1141 .   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.96e-178   Time : 15:46:57 Log - Likelihood : -2045.0   No . Observations : 1253 AIC : 4094 .   Df Residuals : 1251 BIC : 4104 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 5.5878 0.187 29.926 0.000 5.221 5.954   log2(freq ) 0.3927 0.012 33.774 0.000 0.370 0.416   Omnibus : 15.637 Durbin - Watson : 2.053   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 15.928   Skew : -0.275 Prob(JB ): 0.000348   Kurtosis : 3.052 Cond . No . 86.0   Dep . Variable : Radius of Bounding Ball R - squared : 0.448   Model : OLS Adj . R - squared : 0.448   Method : Least Squares F - statistic : 1015 .   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 1.25e-163   Time : 15:46:57 Log - Likelihood : -2078.7   No . Observations : 1253 AIC : 4161 .   Df Residuals : 1251 BIC : 4172 .   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 9.0630 0.093 97.878 0.000 8.881 9.245   log2(senses ) 0.9765 0.031 31.866 0.000 0.916 1.037   Omnibus : 12.796 Durbin - Watson : 2.101   Prob(Omnibus ): 0.002 Jarque - Bera ( JB ): 13.940   Skew : -0.193 Prob(JB ): 0.000940   Kurtosis : 3.344 Cond . No . 8.52421Dep . Variable : Radius of Bounding Ball R - squared : 0.583   Model : OLS Adj . R - squared : 0.582   Method : Least Squares F - statistic : 872.2   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 7.47e-238   Time : 15:46:57 Log - Likelihood : -1903.7   No . Observations : 1253 AIC : 3813 .   Df Residuals : 1250 BIC : 3829 .   Df Model : 2   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   constant 6.0781 0.169 35.937 0.000 5.746 6.410   log2(freq ) 0.2581 0.013 20.071 0.000 0.233 0.283   log2(senses ) 0.5867 0.033 17.784 0.000 0.522 0.651   Omnibus : 21.564 Durbin - Watson : 2.097   Prob(Omnibus ): 0.000 Jarque - Bera ( JB ): 23.741   Skew : -0.272 Prob(JB ): 6.99e-06   Kurtosis : 3.398 Cond . No . 88.6   Dep . Variable : Cosine Similarity R - squared : 0.169   Model : OLS Adj . R - squared : 0.169   Method : Least Squares F - statistic : 1103 .   Date : Sat , 12 Mar 2022 Prob ( F - statistic ): 2.51e-220   Time : 15:54:04 Log - Likelihood : 5534.8   No . Observations : 5412 AIC : -1.107e+04   Df Residuals : 5410 BIC : -1.105e+04   Df Model : 1   Covariance Type : nonrobust   coef std err t P > jtj[0.025 0.975 ]   Constant 1.1096 0.010 111.569 0.000 1.090 1.129   Radius of Bounding Ball -0.0255 0.001 -33.215 0.000 -0.027 -0.024   Omnibus : 1.512 Durbin - Watson : 1.721   Prob(Omnibus ): 0.470 Jarque - Bera ( JB ): 1.543   Skew : -0.027 Prob(JB ): 0.462   Kurtosis : 2.938 Cond . No . 109.422Pearson ‚Äôs R p   Average Pairwise Euclidean Distance 0.601 < 0.001   Max Pairwise Euclidean Distance 0.584 < 0.001   Variance of Pairwise Euclidean Distance 0.292 < 0.001   Average Norm of Embeddings 0.678 < 0.001   Area of convex hull * 0.603 < 0.001423   Arkil Patel Satwik Bhattamishra Phil Blunsom Navin GoyalMicrosoft Research IndiaUniversity of Oxford   arkil.patel@gmail.com , navingo@microsoft.com   { satwik.bmishra,phil.blunsom}@cs.ox.ac.uk   Abstract   1 Introduction   According to the principle of compositionality , the   meaning of a complex expression ( e.g. , a sentence )   is determined by the meaning of its individual con-   stituents and how they are combined . Humans can   effectively recombine known parts to form new sen-   tences that they have never encountered before . De-   spite the unprecedented achievements of standard   seq - to - seq networks such as LSTMs and Trans-   formers in NLP tasks , previous work has suggested   that they are severely limited in their ability to gen-   eralize compositionally ( Lake and Baroni , 2018 ;   Furrer et al . , 2020 ) .   Problem Statement . Our work relates to a   central challenge posed by compositional gener-   alization datasets such as SCAN ( Lake and Baroni ,   2018 ) and Colors ( Lake et al . , 2019 ) , which we   refer to as one - shot primitive generalization : The   dataset consists of input - output sentence pairs ( e.g.   ‚Äò walk twice!WALK WALK ‚Äô ) ; input sentences   are formed from primitive words ( ‚Äò walk ‚Äô ) and func-   tion words ( ‚Äò twice ‚Äô ) and are generated by a context-   free grammar ( CFG ) ; output sentences are obtained   by applying an interpretation function . Crucially ,   there is a systematic difference between the train   and test splits : While the former has a single ex-   ample of an isolated primitive ( e.g. , the primitive   deÔ¨Ånition ‚Äò jump!JUMP ‚Äô in SCAN ) , the latter   consists of compositional sentences with this iso-   lated primitive ( e.g. ‚Äò jump twice ! JUMP JUMP ‚Äô ) .   See Fig . 1 ( left ) for an overview of the task .   A model with the right inductive bias should   generalize on the test data after having seen com-   positional expressions with other primitives during   training . The need for such inductive bias is jus-   tiÔ¨Åed via psychological experiments ( Lake et al . ,   2019 ) indicating that humans do have the ability to424generalize on such tasks . Previous works have sug-   gested that seq - to - seq models lack the appropriate   inductive bias necessary to generalize on this task   since they achieve near - zero accuracies on both   SCAN and Colors benchmarks . This has led to   the development of many specialized architectures   ( Li et al . , 2019 ; Gordon et al . , 2020 ; Chen et al . ,   2020 ; Akyurek and Andreas , 2021 ) , learning pro-   cedures ( Lake , 2019 ; Conklin et al . , 2021 ) and data   augmentation methods ( Andreas , 2020 ; Guo et al . ,   2020 ) to solve the task .   Contributions . The primary claim of our paper   is that , contrary to prior belief , neural sequence   models such as Transformers and RNNs do have   an inductive biasto generalize compositionally   which can be enabled using the right supervision .   ( i)We show that by making simple and intuitive   changes to the training data distribution , standard   seq - to - seq models can achieve high generalization   performance even with a training set of size less   than 20 % of the original training set . In particu-   lar , if we incorporated examples with more novel   primitives in the training set without necessarily   increasing the size of the training set ( see right part   of Fig . 1 ) , then the generalization performance of   standard seq - to - seq models improves and reaches   near - perfect score after a certain point . Our re-   sults also exemplify the importance of the training   distribution apart from architectural changes and   demonstrate that providing the right supervision   can signiÔ¨Åcantly improve the generalization abili-   ties of the models . ( ii)We investigate the potential   cause behind the improvement in generalization   performance and observe that the embedding of the   isolated primitive becomes more similar to other   primitives when the training set has higher number   of primitives and their use cases . ( iii)To under-   stand the phenomenon better , we characterize the   effect of different training distributions and model   capacities . Our results show that the parameters of   the experimental setting play a crucial role while   evaluating the generalization abilities of models .   2 Enabling Generalization by Providing   the Right Supervision   Setup . We focus on the SCAN and Colors   datasets . Both these datasets have exactly one   isolated primitive . We refer to all other primitives   ( i.e. , those that are also composed with other words   to form sentences in the training set ) as example   primitives . Both the SCAN and Colors training sets   have exactly three example primitives . The training   set of SCAN has 13:2k examples while the test   set has 7:7k examples . Colors has just 14training   examples and 8test examples . More details on   implementation and datasets can be found in   Appendix A & B. Our source code is available   at https://github.com/arkilpatel/Compositional-   Generalization - Seq2Seq .   Adding More Primitives . We modify the train-   ing set such that the number of distinct example   primitives present in the dataset is higher . To do   so , we add new primitives to the language which   are simply random words ( e.g. , ‚Äò swim ‚Äô , ‚Äò clap ‚Äô , etc . )   that have the same semantics and follow the same   grammar rules as other existing primitives ( see Fig .   1 ( right ) for illustration ) . These new primitives   act as example primitives in our training set . For   SCAN , we control the size of the training set such   that it is at most the size of the original dataset .   To generate the training set , we randomly sample   the examples from the new grammar and discard   all compositional sentences with the isolated primi-   tive . For each example primitive and the isolated   primitive , a primitive deÔ¨Ånition ( such as ‚Äò walk !   WALK ‚Äô ) is also added to the training set . The test   set is untouched and remains the same .   Main Observation . Fig . 2 shows the gener-   alization performance of Transformer and LSTM   based seq - to - seq models . We observe that there is   a clear trend of improvement in compositional gen-425   eralization as we increase the number of example   primitives and their use cases . It is surprising to see   that on SCAN , Transformers perform on par with   some recently proposed specialized architectures   ( Li et al . , 2019 ; Gordon et al . , 2020 ) and even better   than certain architectures ( Russin et al . , 2019 ) .   Implication . Since the training set still contains   only one non - compositional example with the iso-   lated primitiveand the test set is untouched , one-   shot primitive generalization setting is preserved .   Hence our results clearly show that standard neu-   ral sequence models have ‚Äò some ‚Äô inductive bias   required to generalize on such out - of - distribution   tasks even if it is not as strong as that of special-   ized architectures designed primarily to solve these   tasks . Our results are in contradiction to previously   suggested limitations of standard seq - to - seq mod-   els in terms of primitive generalization ( Lake and   Baroni , 2018 ; Furrer et al . , 2020 ; Baroni , 2020 ) .   While it is important to develop architectures with   better compositional generalization abilities , we   wish to highlight that synthetic benchmarks such as   SCAN require a model with very strong inductive   biases and tend to underestimate the generalization   abilities of baseline models .   While we have shown that these models can gen-   eralize from one - shot exposure to primitive deÔ¨Å-   nitions , our results also hold for the more general   case where the one - shot exposure of the primitive is   in a sentence ( e.g. ‚Äò jump twice ! JUMP JUMP ‚Äô ) .   More details regarding these experiments can be   found in Appendix D.   Prior Work . Note that our work is unrelated   to previous works that propose data augmentation   approaches for compositional generalization tasks   ( Andreas , 2020 ; Guo et al . , 2020 ; Aky√ºrek et al . ,   2021 ) . ( 1 ) The datasets created by some of these   augmentation methods do not preserve the system-   atic differences between train and test sets , while   our datasets do.(2 ) The objective of these works   was to devise a method to improve compositional   generalization performance whereas the focus of   our work is not to develop a general method ; rather   we want show that baseline seq - to - seq models   are capable of generalizing compositionally even   without breaking systematicity . ( 3 ) These meth-   ods add additional data resulting in datasets of   larger sizes whereas we control for data size .   2.1 Analyzing the Embedding of the Isolated   Primitive   Our results raise the question : Why do Transform-   ers and LSTMs generalize better when the training   data has more example primitives ? Compositional   generalization in our setting requires a model to   learn to apply the same rules to the isolated primi-   tive as it does to the other example primitives . Thus ,   we analyze the change in the learned embedding of   the isolated primitive ( such as ‚Äò jump ‚Äô ) with respect   to other primitives in different settings .   In particular , we compare the average distance   with other primitives before and after adding cer-   tain number of primitives to training data ( this is   the same setting that was explained earlier in this   section ) . We Ô¨Ånd that as we increase the number   of example primitives in the training set , the em-426   bedding of the isolated primitive gets closer to the   example primitives ( Fig . 3 ) in terms of Euclidean ,   Manhattan and Cosine distances . If the embedding   of the isolated primitive is closer to the embed-   dings of the other primitives , then the model is   more likely to operate over it in a similar fashion   and apply the same rules as it does over the other   primitives .   This phenomenon is also illustrated in t - SNE   plots ( Fig . 4 ) of the learned embeddings where the   embedding of the isolated primitive seems closer   to the embeddings of the example primitives when   there are more example primitives in the dataset .   Hence , a possible reason behind improved general-   ization performance could be the difference in the   learned embeddings . Additional results with the   LSTM model and Colors dataset can be found in   Appendix E.1 .   3Exploring the Impact of the Parameters   of the Experimental Setup   3.1 Impact of Training Distributions   In this section , we analyze the inÔ¨Çuence of different   training distributions on the generalization perfor - mance of the model . In the previous experiments ,   the data generating distribution was uniform over   all possible samples . Here , we alter the training   data distribution by varying the number of exam-   ples for each example primitive . The test set re-   mains unchanged and there will still be only one   non - compositional example of the isolated prim-   itive ( i.e. , the primitive deÔ¨Ånition ) in the training   set . We experiment with linearly , quadratically and   exponentially increasing probability distribution   functions . For instance , in the quadratically increas-   ing case , a training set with 10example primitives   will have one example primitive with 1composi-   tional example , the next one with 4compositional   examples , another one with 9compositional exam-   ples and so on . Similarly , in the exponentially   increasing case ( which we also call ‚Äò skewed ‚Äô ) , 10 %   example primitives have 500 compositional exam-   ples each , 30 % have 10 compositional examples   each and the remaining have just one compositional   example each in the training set . The general idea   is that all the example primitives do not have equal   representation in the training data . Upon training   the models on different distributions , we observed   that the models generalize well even with fewer   number of example primitives when their distri-   bution is linearly or quadratically increasing ( Fig .   5a ) . On the other hand models struggle to gen-   eralize when the distribution is skewed . In that   case , most primitives appear in only one or very   few compositional sentences in the training data .   The failure to generalize on such data implies that   extra primitives must be added as part of multiple   compositional sentences ; just adding the primitive   deÔ¨Ånition or a single example for each example   primitive does not help the model to leverage it .   We then try to characterize the relationship be-   tween the number of example primitives and the   amount of data required for the model to general-   ize well on the test data , when the example primi-   tives are uniformly distributed . We create different   training sets by varying the total number of ex-   ample primitives , # primitives ; for each example   primitive , we draw # examples number of sam-   ples uniformly from the CFG . Fig . 5b shows the   generalization performance of Transformers for   each of these training sets . The size of each train-   ing set is the product of the row and column values   ( # primitives#examples ) . As expected , the427   upper - right triangle has higher scores indicating   that the sample requirement decreases as we add   more primitives to the dataset . Surprisingly , the   top - left cell indicates that Transformers can achieve   high performance even with 2k training examples   which is less than 20 % of the original SCAN train-   ing set . Additional results with the LSTM model   can be found in Appendix E.2 .   3.1.1 Understanding Transferability   We wish to check whether the inductive bias that is   enabled when a model is trained on more number of   example primitives can be transferred to a scenario   where the number of example primitives is limited .   We create a pretraining set with 50 example prim-   itives uniformly distributed , each of them having   200 examples . The Ô¨Ånetuning set is the original   SCAN training set and the test set is the original   SCAN test set . The model is Ô¨Årst trained from   scratch on the pretraining set and then Ô¨Ånetuned on   the Ô¨Ånetuning set .   We Ô¨Ånd that if we allow all the parameters of   the Transformer model to be updated during the   Ô¨Ånetuning phase on the original SCAN training set ,   then the model generalizes very poorly . On the   other hand , when we freeze the weights of the en-   coder and decoder after the pretraining phase , and   only allow the embedding and output layers to be   updated , then the model generalizes near - perfectly   on the test set . Our hypothesis is that in the latter   setting , the task becomes simpler for the model   since it only has to align the embeddings of the   newly seen primitives in the Ô¨Ånetuning phase with   the embeddings of the primitives seen during the   pretraining phase . This experiment also indicates   that the previously learned rules during pretraining   can help a model to compositionally generalize on   novel primitives.3.2 Impact of Model Capacity   We analyze the relationship between the model ca-   pacity and the number of example primitives in the   training set . We vary the number of primitives as   per the description in Section 2 . We evaluate the   generalization performance of the models while   gradually increasing the number of parameters by   increasing the size of its embeddings and interme-   diate representations . For each experiment , we ex-   haustively Ô¨Ånetune the rest of the hyperparameters   ( e.g. , dropout , learning rate , batch size , etc . ) to se-   lect the best model . Looking at Fig . 6 , we observe   a general trend in which the model starts to over-   Ô¨Åt and has poor generalization performance as we   increase the model size . Note that all these model   conÔ¨Ågurations are able to achieve near - perfect accu-   racies on the SCAN random split that does not test   for compositional generalization . This shows that   carefully controlling the model size is important   for achieving compositional generalization . On   such small datasets , larger models might simply   memorize the input - output mappings in the train-   ing set . Indeed , such memorization has been cited   as a potential reason to explain why models fail at   compositional generalization ( Conklin et al . , 2021 ) .   We also Ô¨Ånd that as we increase the number of ex-   ample primitives , the models are less susceptible   to overÔ¨Åtting and achieve relatively better general-   ization performance . Additional results with the   LSTM model and Colors dataset can be found in   Appendix E.3 .   4 Conclusion   While it is essential to make progress in building   architectures with better compositional generaliza-   tion abilities , we showed that the generalization   performance of standard seq - to - seq models ( often   used as baselines ) is underestimated . A broader   implication of our experiments is that although   systematicity must be preserved when designing   such benchmarks , it is imperative to carefully ex-   plore different parameters associated with the ex-   perimental setup to draw robust conclusions about   a model ‚Äôs generalization abilities .   Acknowledgements   We thank the anonymous reviewers for their con-   structive comments . We would also like to thank   Kabir Ahuja , Zihuiwen Ye and our colleagues at   Microsoft Research India for their valuable feed-   back and helpful discussions.428References429A Implementation Details   We use 8 NVIDIA Tesla P100 GPUs each with 16   GB memory to run our experiments . All models   are implemented in PyTorch ( Paszke et al . , 2019 ) .   We do not use any pretrained models and all em-   beddings are learnt from scratch . Parameters are   updated using Adam optimization . All results are   an average of 5 different runs with random seeds .   The dataset - speciÔ¨Åc hyperparameters used for each   model are shown in Table 1 .   B Primitive Generalization Datasets   In this paper , we show results on three datasets that   evaluate primitive generalization .   SCAN ( Lake and Baroni , 2018 ) is a super-   vised sequence - to - sequence semantic parsing task   wherein the natural language input command has   to be transformed to the corresponding set of ac-   tions . The complete dataset consists of all the com-   mands ( a total of 20,910 ) generated by a phrase-   structure grammar and the corresponding sequence   of actions , produced according to a semantic inter-   pretation function . The benchmark consists of 4   splits : random , add jump , turn left and length . We   work on the ‚Äò add jump ‚Äô split which was designed   to test primitive generalization . In this split , the   test set ( size : 7706 ) is made up of all the composi-   tional sentences with the primitive ‚Äò jump ‚Äô ( which   we refer to as the isolated primitive ) . The train set   ( size : 13,204 ) has just one example of the isolated   primitive ( i.e. the primitive deÔ¨Ånition ‚Äò jump !   JUMP ‚Äô ) and other examples demonstrating the def-   initions and compositions of the three other primi-   tives ( which we refer to as the example primitives ) .   Table 2 illustrates the task .   Colors ( Lake et al . , 2019 ) is a sequence - to-   sequence task that was designed to measure hu-   man inductive biases . Apart from the challenge of   primitive generalization , this dataset poses an addi-   tional challenge of low - resource learning for neural   sequence models . The train set has just 14 exam-   ples that are either primitive deÔ¨Ånitions of the four   primitives or examples with compositions of the   three example primitives and three operations ( con-   catenation , repetition and wrapping ) . The test set   has 8 exampleswith compositions of the isolated   primitive ( ‚Äò zup ‚Äô ) . Fig . 7 illustrates the task .   COGS ( Kim and Linzen , 2020 ) is a semantic   parsing task of mapping English natural language   sentences to their corresponding logical forms .   Apart from primitive generalization , COGS also   evaluates other types of systematic generalization   such generalizing to higher depths or generalizing   to novel syntactic structures . The size of the train   set is 24,155 and that of the test set is 21,000 .   C Removing Primitives Hurts   Generalization on COGS   Unlike SCAN and Colors , both of which have a   single isolated primitive and only 3 example prim-   itives , COGS has 3 isolated primitives - a verb , a   common noun and a proper noun which are sup-   ported by 80 verbs , 40 common nouns and 20   proper nouns as example primitives . We hypoth-   esize that this high number of example primitives   might be one of the reasons behind the high perfor-   mance of Transformers on COGS ( Csord√°s et al . ,430   2021 ; Onta√±√≥n et al . , 2021 ) , as far as primitive   generalization is concerned .   To validate our hypothesis , we systematically   reduce the number of example primitives in COGS   and evaluate the model . The test set of COGS   focusing on primitive generalization consists of   5000 examples . If we directly start removing the   primitives from the train set , we risk having out-   of - vocabulary tokens in the test set . Hence we   select a portion of the test set of size 1218 which   exludes 129 example primitives . We will hold this   test set Ô¨Åxed and vary the percentage of the 129   example primitives to be inserted in the train set .   For each example primitive , samples are drawn   uniformly from the original COGS train set . Note   that even though the number of example primitives   and their use cases will vary in the train set , we   control the total train set size to be always 2500 for   fair evaluation .   The results of our experiment can be seen in   Fig . 8 . We see a clear trend of decrease in gener-   alization performance as we decrease the number   of example primitives and their use cases . This   is in tandem with the results shown in Section 2   and further validates the idea that providing more   example primitives and their use cases helps neural   sequence models generalize on the primitive gener-   alization task . Our results help explain that the gap   in performance of neural sequence models on prim-   itive generalization tasks in COGS and primitive   generalization tasks in SCAN or Colors is at least   partially caused by the difference in the number   of example primitives and their use cases in these   datasets .   D Implicit Word Learning   Drawing analogy from human vocabulary acquisi-   tion ( Bloom , 2000 ) , our primitive generalization   setting corresponds to the case when a child is   explicitly explained the meaning of a word . But   children can learn word meaning from implicit us-   age . In our setting this would translate to using   a primitive in a more complex construction , say   ‚Äò jump twice!JUMP JUMP ‚Äô instead of the original   ‚Äò jump!JUMP ‚Äô . It would be interesting to evalu-   ate how well seq - to - seq models learn the meanings   of words from a single sentence and whether they   learn to use that word compositionally with other   words .   We consider the ‚Äò add jump ‚Äô split in SCAN . In-   stead of providing the ‚Äò jump ! JUMP ‚Äô primitive   deÔ¨Ånition in the train set , we provide one compo-   sitional sentence featuring ‚Äò jump ‚Äô . We vary the   complexity of this sentence as shown in Table 3 .   Similar to the case of providing only the primitive   deÔ¨Ånition , we observe that models are unable to431   generalize and achieve near - zero accuracies .   We now wish to see whether the presence of   more number of primitives and their sentences in   the train set helps a model generalize in this sce-   nario ( like it did for primitive deÔ¨Ånitions as shown   in Section 2 ) . We consider the setup of having 100   primitives and their sentences in the train set ( Sec-   tion 2 ) apart from the one compositional sentence   with the word ‚Äò jump ‚Äô . We Ô¨Ånd that models are able   to achieve near - perfect generalization accuracies .   This shows that our idea holds more generally :   Adding more primitives and their sentences helps a   model effectively learn the meaning of a new prim-   itive , whether speciÔ¨Åed explicitly via a primitive   deÔ¨Ånition or implicitly in a sentence .   E Details of Experimental Setups and   Other Results   E.1 Embedding of Isolated Primitive   We scale the embedding vectors to unit L2 - norm   for calculating the euclidean distance and unit L1-   norm for calculating the manhattan distance . For   Colors dataset as well , we compare the average dis-   tance with other primitives before and after adding   primitives to the training data . We again Ô¨Ånd that   as we increase the number of example primitives in   the training set , the embedding of the isolated prim-   itive ( ‚Äò zup ‚Äô ) gets closer to the example primitives   ( refer to Fig . 10 ) in terms of Euclidean , Manhattan   and Cosine Distances .   We additionally show the t - SNE plots of the   learned embeddings for the LSTM model on the   Colors dataset ( Fig . 9 ) .   E.2 Impact of Training Distributions   In Section 3.1 , we showed results of the Trans-   former model on various train set distributions of   the SCAN dataset . We also experimented with the   LSTM model , the results of which can be found   in Fig . 11 . We see the same trend as we saw for   Transformers.432   E.3 Impact of Model Capacity   In Section 3.2 , we showed results of varying sizes   of Transformers trained on datasets with different   number of example primitives . We also experi-   mented with the LSTM model , the results of which   on the Colors dataset can be found in Fig . 12 . We   see the same trend as we saw for Transformers .   E.4 Variance Across Different Runs   We plot the generalization accuracies of the Trans-   former and LSTM models on SCAN and Colors   datasets over 5 different runs with random seeds in   Fig . 13 - 14 . Both models displayed a high degree   of variance in generalization performance on both   datasets . It is interesting to see that the variance   decreases with increasing number of primitives .   E.5 Evaluation on Multiple Isolated   Primitives   Our results are valid not just when there is a single   isolated primitive , but even when there are multiple   isolated primitives that are used compositionally at   test time . While we believe that this holds trivially   due to the symmetry of the setup , for completeness ,   we provide empirical evidence . We consider the   setting on SCAN in which the train set has a total of   100 example primitives uniformly distributed . To   this train set , in addition to the primitive deÔ¨Ånition   of ‚Äò jump ‚Äô ( i.e. , ‚Äò jump ! JUMP ‚Äô ) , we add 9 other   primitive deÔ¨Ånitions of newly introduced isolated   primitives . Thus , while the size of the train set in   this setting was 13185 , the size of the new train set   is 13194 . We then extract templates from the origi-   nal SCAN test set and exhaustively populate these   templates with the 10 isolated primitives . Hence ,   while the size of the original test set was 7706 , the   size of the new test set is 77060 .   We evaluated Transformers on this data . The   best model achieved 94.5 % accuracy on the com-   plete test set , thereby showing that our method-   ology and results are valid even when there are   multiple isolated primitives in the dataset at the   same time .   F A Note on Other Data Augmentation   Methods   Applying data augmentation methods such as   GECA ( Andreas , 2020 ) on SCAN will lead to   addition of training examples in which the input   sentences are compositions of the isolated primi-   tive ‚Äò jump ‚Äô . This breaks the systematicity of the   setup . While such automatic data augmentation ap-   proaches are important resources for enabling com-   positional generalization , a model that performs   well on this modiÔ¨Åed split can not be considered to   be able to generalize compositionally .   Shi et al . ( 2021 ) proposed a data augmentation   method based on the theory of meaningful learn-   ing . Similar to our work , they also augment the   train set by adding more primitives ( e.g. ‚Äò jump_0 ‚Äô ,   ‚Äò jump_1 ‚Äô , ... , ‚Äò jump_n ‚Äô ) . However , compared to our   work , their setup is completely different : The new   primitives that they add to the train set are all still   mapped to the output token of an example prim-433itive ‚Äò jump ‚Äô , which is ‚Äò JUMP ‚Äô ( i.e. ‚Äò jump_0 !   JUMP ‚Äô , ... , ‚Äò jump_n ! JUMP ‚Äô ) . Their train set has   examples showing compositions of ‚Äò jump ‚Äô while   their test set evaluates for novel compositions of   the newly added primitives . We argue that their   setup can not be considered one - shot primitive gen-   eralization since now the model can see the output   token ‚Äò JUMP ‚Äô in composition with other words .   We claim that this familiarity with the output token   enables a model to generalize well on the test data   even if the newly added primitives are only pre-   sented one - shot in the train set . Indeed , Lake and   Baroni ( 2018 ) also suggested that the reason why   models are able to do well on the ‚Äò turn left ‚Äô split   of SCAN is because the train set consists of many   examples that have the output token ‚Äò LTURN ‚Äô used   compositionally .   To validate our claim , we propose a simple exper-   iment . In the original SCAN ‚Äò add jump ‚Äô split , we   map ‚Äò jump!WALK ‚Äô instead of ‚Äò jump ! JUMP ‚Äô   for all examples ( primitive deÔ¨Ånition as well as   compositional sentences ) in both the train and test   sets . In this setup , even though the input word   ‚Äò jump ‚Äô is seen only once at train time , it ‚Äôs mapping   ‚Äò WALK ‚Äô is used compositionally in many examples .   On evaluating a Transformer model on this split ,   we found that it achieves a near - perfect accuracy .   This shows that providing compositional examples   with the output token of the isolated primitive not   only breaks systematicity , but is the reason behind   the high performance of models in that setting.434   Shuang Liu , Dong Wang , Xiaoguang Li , Minghui Huang , Meizhen DingHuawei Noah ‚Äôs Ark LabAI Application Research Center ( AARC )   Huawei Technologies Co. , Ltd   { liushuang30 , wangdong153}@huawei.com   Abstract   1 Introduction   Open - domain question answering ( ODQA ) focuses   on providing highly precise answers to natural lan-   guage questions from a large collection of unstruc-   tured text data ( V oorhees , 1999 ) . With the pioneer-   ing work of DrQA ( Chen et al . , 2017 ) , modern   approaches to ODQA commonly adopt a simple   two - stage retriever - reader pipeline , that Ô¨Årstly re-   trieve a relatively small number of support passages   ( Karpukhin et al . , 2020 ; Min et al . , 2021b ; Yamada   et al . , 2021 ) , followed by the reader identifying the   answer .   The reader models can be broadly categorized   into two classes : extractive ( Chen et al . , 2017 ; Asai   et al . , 2020 ; Karpukhin et al . , 2020 ) and generative   ( Izacard and Grave , 2021b ; Lewis et al . , 2020b ;   Wu et al . , 2021 ) . Recently , beneÔ¨Åting from the   powerful ability of large - scale pre - trained encoder-   decoder language models ( Lewis et al . , 2020a ; Raf-   fel et al . , 2019 ) and the capability of aggregat-   ing information from multiple passages ( Izacard   and Grave , 2021b ) , generative approaches have   achieved in general better performance than extrac-   tive methods .   Compared to extractive models , generative mod-   els generate text more freely , which makes it often   suffer from the problem of producing hallucinated   text that is factual inaccuracy or inconsistent to the   input . This problem has been addressed in tasks   like text summarization ( Maynez et al . , 2020 ) and   machine translation ( Zhou et al . , 2021 ) . We found   that the phenomenon also happens in ODQA . As   shown in Table 1 , the answer " Dubai in Germany "   produced by the generative model FiD ( Izacard and   Grave , 2021b ) is factual incorrect and the answer   " 33 " in the second example is not coherent to the   question . While in both cases , the ground - truth   answers are present in the retrieved passages . Thus ,   we hypothesize that if we could put a constraint on   the produced words to the input text , the generated   answer will be more faithful .   Inspired by the work of See et al . ( 2017 ) , we   enhance the generative model with a pointer net-435   work ( Vinyals et al . , 2015 ) , that enables the model   to directly copy text from the retrieved passages   while retains the ability of generating new words   when the true answers are not explicitly present   in the input . To be more speciÔ¨Åc , our model   fusion - in - decoder pointer - generator network ( FiD-   PGN ) is built upon the state - of - the - art model FiD.   We reuse the encoder - decoder attention scores as   the copy distribution to reduce the computational   cost . Compared to FiD , we achieve comparative or   even better accuracy on the NaturalQuestions ( NQ )   ( Kwiatkowski et al . , 2019 ) and TriviaQA ( Joshi   et al . , 2017 ) benchmarks , with less passages used   in training . Our experiments results show the effec-   tiveness and efÔ¨Åciency of our model .   2 Related Work   2.1 Open - Domain Question Answering   In this era of data explosion , ODQA offers a way   to rapidly and accurately fulÔ¨Åll user ‚Äôs information   needs , and hence has recently received signiÔ¨Åcant   attention from both industry and academia ( Min   et al . , 2021a ) . Following the work of DrQA ( Chen   et al . , 2017 ) , most recent works build a two - stage   retriever - reader system to tackle the problem . The   retriever aims at retrieving supportive passages to   the given question from a large document corpus .   The reader intends to Ô¨Ånd answer of the question   from the Ô¨Årst stage retrieved passages . Early work   of Chen et al . ( 2017 ) adapts a BiLSTM architecture   with various lexical and semantic features from   the question and passages as inputs . Later , with   the emergence of large - scale pre - trained language   models , readers based on pre - trained models suchas BERT ( Devlin et al . , 2019 ) and T5 ( Raffel et al . ,   2019 ) have become a common approach ( Yang   et al . , 2019 ; Izacard and Grave , 2021b ; Karpukhin   et al . , 2020 ) .   2.2 Generative Readers   Compared to extractive models which extract spans   from the retrieved passages , generative models are   able to produce new words out of the retrieved   passages , and thus provide a more Ô¨Çexible model-   ing framework . Min et al . ( 2020 ) and Lewis et al .   ( 2020b ) concatenate the given question with top   retrieved passages and feed the concatenation to   the BART model ( Lewis et al . , 2020a ) . Izacard   and Grave ( 2021b ) separately encodes the ques-   tion with each top retrieved passage , then takes   the concatenation of the encoder outputs as input   to the decoder . Their method provide a way to   better aggregate evidence from multiple passages   and improve the performance signiÔ¨Åcantly . FiD-   KD ( Izacard and Grave , 2021a ) is an extension of   FiD model that increases the accuracy of passage   retrieval by training the dense retriever with the   guidance of the FiD reader iteratively .   2.3 Pointer - Generator Network   Pointer - Generator Network ( See et al . , 2017 ) is   an extension of the sequence - to - sequence model   by integrating a copy mechanism ( Vinyals et al . ,   2015 ) into the generator . At each decoding stage ,   the model is able to either directly copy a word   from the input or generate one with certain prob-   ability , and thus can be viewed as a combination   of extractive and generative approaches . It has   been frequently used in natural language tasks like436summarization ( Gu et al . , 2016 ; See et al . , 2017 ;   Gehrmann et al . , 2018 ) and neural machine trans-   lation ( Luong et al . , 2015 ; Gu et al . , 2018 ) , but its   application to ODQA has been less explored .   3 Method   Our model follows the standard two - stage retriever-   reader framework with a focus on the enhancement   of the reader module built upon the FiD reader .   We adopt the retriever results of FiD - KD , where a   dense retriever similar to DPR ( Karpukhin et al . ,   2020 ) is used . A pointer network is integrated into   the FiD reader to facilitate copying words from the   retrieved passages . The overall reader architecture   is depicted in Figure 1 .   Reader Encoder . The reader encoder of our model   is identical to the one of FiD reader . We Ô¨Årstly con-   catenate the given question qwith each retrieved   passagepasx= [ q;p ] . Next , we pass each xin-   dividually to the reader encoder , i.e. , the encoder of   T5 or BART model , and obtain the hidden represen-   tationsh= ( h;h;:::;h)of the question-   passage pair where h2Randdis the model   dimension . Finally , we concatenate all the hidden   representations of top- kpassagesfh;:::;hgas   input to the decoder .   Reader Decoder . Our approach mainly differs   from FiD reader in the decoder module by adding a   pointer network . SpeciÔ¨Åcally , at each decoding step   t , lete2Rbe the embedding vector of the input   token at this step , and denote s2Ras the output   representation of the last layer Lof transformer   decoder , then the probability of generation is given   as follows ,   p=(we+ws+b ) ( 1 )   wherew2R , w2Randb2Rare all learn-   able parameters and ()represents the sigmoid   function . In addition , the probability of copying is   1 p .   Next , letVdenote the vocabulary containing   words for the generative model and jVjbe the size   of the vocabulary . Then at step t , the probability   distribution of words generation over the vocabu-   lary is computed as ,   P= softmax ( Ws ) ( 2 )   whereW2Ris a learnable weight matrix . BeneÔ¨Åting from the encoder - decoder attention   layer in transformer architecture , we directly utilize   the cross - attention score  of the last decoder   layerLover the source tokens for the target token   yas copy distribution . Then the probability of   selectingyin source sequence is calculated as ,   P(y ) = X   ( 3 )   wherexdenotes the concatenation of the top- k   retrieved passages , xis thej - th token of x ,   and  is thej - th element of  . Ifyis not   present in the top- kretrieved passages , P(y )   will be zero .   Finally , put all the above together , the target   tokenycould both be generated from vocabulary   with probability p , and copy from the source   passages . The Ô¨Ånal prediction probability is deÔ¨Åned   as   P(y ) = pP(y ) + ( 1 p)P(y):(4 )   4 Experiments   4.1 Datasets   We evaluate the performance of our approach on   two standard ODQA datasets , NQ and TriviaQA .   The NQ dataset comprises real queries that user is-   sued on Google search engine along with answers .   The TriviaQA dataset consists of question - answer   pairs collected from trivia and quiz - league websites .   The details of data statistics are listed in Table 2 . It   can be seen that TriviaQA has on average longer   question length than NQ , indicating that questions   in TriviaQA are relatively more complex . We use   the data released on the repository of FiD , con-   taining question - answer pairs and top-100 passages   retrieved by FiD - KD .   Statistics NQ TriviaQA   Train 79,168 78,785   Validation 8,757 8,837   Test 3,610 11,313   Avg . Qlen 9.3 16.9   Avg . Alen 2.4 2.2437Model Reader Size Top- kNQ TriviaQA   DPR ( BERT - base ) ( Karpukhin et al . , 2020 ) 110 M 24 41.5 57.9   RAG - Seq ( BART - large ) ( Lewis et al . , 2020b ) 406 M 50 44.5 56.8   FiD ( T5 - base ) ( Izacard and Grave , 2021b ) 220 M 100 48.2 65.0   FiD - KD ( T5 - base ) ( Izacard and Grave , 2021a ) 220 M 100 49.6 68.8   FiD - KD ( Our implementation ) 220 M 25 48.5 67.5   FiD - PGN 220 M 25 51.4 68.4   4.2 Implementation Details   We follow the experimental settings as in FiD.   Our model is initialized with a pre - trained T5 - base   model , and trained using AdamW ( Loshchilov and   Hutter , 2017 ) algorithm with a learning rate of   10 , linear scheduling with 15k total steps and 1k   warm - up steps . Moreover , we train our model us-   ing the top-25 retrieved passages for each question   and set the batch size as 64 due to computational   limitation . All experiments are run on eight Nvidia   V100 32 GB GPUs .   4.3 Results   Table 3 shows the experimental results of our model   and other approaches on the test sets , evaluated   with the standard exact match ( EM ) score ( Ra-   jpurkar et al . , 2016 ) . For a fair comparison , we   retrained the FiD reader on the top-25 retrieved   passages to match our experimental settings .   As shown in Table 3 , our model outperforms   FiD - KD on both NQ and TriviaQA datasets under   the same setting . This demonstrates that the pointer   network could help to generate answers more accu-   rately . It is worth noting that , compared with FiD-   KD trained with the top-100 retrieved passages , our   model achieves comparative or even better results   with only 1/4 of the input data and without introduc-   ing many parameters ( only 1537 extra parameters   are added ) , indicating the efÔ¨Åciency of our model .   5 Analysis   Generation Probability . We explore the proba-   bility of generation during training to further in-   vestigate the effects of the pointer module . As   shown in Figure 2 , the generation probability p   in TriviaQA is always higher than the one in NQ .   Note that a higher generation probability means   that more tokens are produced from the vocabulary   instead of copying from the input . We conjecture   that this phenomenon is caused by the different   question types . As stated in Rogers et al . ( 2021 ) ,   Trivia questions are more like probing questions .   Compared to the information - seeking questions in   NQ , probing questions tend to need more complex   reasoning , and thus it is difÔ¨Åcult to directly extract   relevant tokens from input texts . Moreover , this   observation is also consistent with the results that   the improvements of our model over FiD reader is   smaller in TriviaQA than the one in NQ ( 0.9 vs. 2.9   EM for TriviaQA and NQ , respectively ) .   Test - Train Overlap Evaluation . The study of   test - train overlap ( Lewis et al . , 2021 ) provides valu-   able insights into the model ‚Äôs question answering   behavior . We evaluate our model on the same test   data splits as in Lewis et al . ( 2021 ) . Table 4 reports   the results with respect to three kinds of test - train   overlaps . It can be seen that our approach improves   most over FiD reader on " No Overlap " category ,   the most challenging setting , indicating a better   generalization ability to question answering .   Training with Varying Number of Passages . Fig-   ure 3 shows the performance of our model and FiD   reader with regard to different number of retrieved   training passages . We train both models with top- k   passages ( k2f1;5;10;25 g ) and evaluate on the   development sets with the same number of pas-438   sages . We can observe that the matching scores of   both models increase with respect to the number of   passages used in training , consistent with the Ô¨Ånd-   ings in Izacard and Grave ( 2021b ) that sequence - to-   sequence model is capable of gathering information   across multiple retrieved passages . Moreover , the   two models show comparative performance when   the number of training passages is small , but when   more passages are included , our model outperforms   FiD , especially on the NQ dataset .   6 Conclusion   In this article , we propose a novel FiD - PGN ap-   proach for the reader module of ODQA under the   standard retriever - reader framework . SpeciÔ¨Åcally ,   we integrate a pointer network into the FiD reader   to allow the model to directly select words from the   retrieved passages . Experimental results show that   our model outperforms FiD - KD on two benchmark   datasets under the same setting , demonstrating the   advantages of our method .   References439440441   Soyeong JeongJinheon BaekSukmin ChoSung Ju HwangJong C. Park   School of ComputingGraduate School of AI   Korea Advanced Institute of Science and Technology   { syjeong,nelllpic,park}@nlp.kaist.ac.kr   { jinheon.baek,sjhwang82}@kaist.ac.kr   Abstract   1 Introduction   Retrieval systems aim at retrieving the documents   most relevant to the input queries , and have re-   ceived substantial spotlight since they work as core   elements in diverse applications , especially for   open - domain question answering ( QA ) ( V oorhees ,   1999 ) . Open - domain QA is a task of answering   the question from a massive amount of documents ,   often requiring two components , a retriever and a   reader ( Chen et al . , 2017 ; Karpukhin et al . , 2020 ) .   SpeciÔ¨Åcally , a retriever ranks the most question-   related documents , and a reader answers the ques-   tion using the retrieved documents .   Traditional sparse retrieval approaches such as   BM25 ( Robertson et al . , 1994 ) and TF - IDF rely   on term - based matching , hence suffering from the   vocabulary mismatch problem : the failure of re-   trieving relevant documents due to the lexical dif-   ference from queries . To tackle such a problem ,   recent research focuses on dense retrieval mod-   els to generate learnable dense representations for   queries and documents ( Karpukhin et al . , 2020 ) .   Despite their recent successes , some challenges   still remain in the dense retrieval scheme for a cou-   ple of reasons . First , dense retrieval models need   a large amount of labeled training data for a de-   cent performance . However , as Figure 1 shows ,   the proportion of labeled query - document pairs is   extremely small since it is almost impossible to   rely on humans for the annotations of a large docu-   ment corpus . Second , in order to adapt a retrieval   model to the real world , where new documents con-   stantly emerge , handling unlabeled documents that   are not seen during training should obviously be   considered , but remains challenging .   To automatically expand the query - document   pairs , recent work generates queries from genera-   tive models ( Liang et al . , 2020 ; Ma et al . , 2021 )   or incorporates queries from other datasets ( Qu   et al . , 2021 ) , and then generates extra pairs of aug-   mented queries and documents . However , these   query augmentation schemes have serious and ob-   vious drawbacks . First , it is infeasible to augment   queries for every document in the dataset ( see the   number of unlabeled documents in Figure 1 ) , since   generating and pairing queries are quite costly . Sec-   ond , even after obtaining new pairs , we need extra   training steps to reÔ¨Çect the generated pairs on the   retrieval model . Third , this query augmentation   method does not add variations to the documents   but only to the queries , thus it may be suboptimal   to handle enormous unlabeled documents.442   Since augmenting additional queries is costly ,   the question is then if it is feasible to only manip-   ulate the given query - document pairing to handle   numerous unlabeled documents . To answer this ,   we Ô¨Årst visualize the embeddings of labeled and   unlabeled documents . Figure 1 shows that there is   no distinct distributional shift between labeled and   unlabeled documents . Thus it could be effective   to manipulate only the labeled documents to han-   dle the nearby unlabeled documents as well as the   labeled documents . Using this observation , we pro-   pose a novel document augmentation method for a   dense retriever , which not only interpolates two dif-   ferent document representations associated with the   labeled query ( Figure 2 ( b ) ) , but also stochastically   perturbs the representations of labeled documents   with a dropout mask ( Figure 2 ( c ) ) . One notable   advantage of our scheme is that , since it manipu-   lates only the representations of documents , our   model does not require explicit annotation steps of   query - document pairs , which makes it highly efÔ¨Å-   cient . We refer to our overall method as Document   Augmentation for dense Retrieval ( DAR ) .   We experimentally validate our method on stan-   dard open - domain QA datasets , namely Natural   Question ( NQ ) ( Kwiatkowski et al . , 2019 ) and Triv-   iaQA ( Joshi et al . , 2017 ) ( TQA ) , against various   evaluation metrics for retrieval models . The experi-   mental results show that our method signiÔ¨Åcantly   improves the retrieval performances on both the   unlabeled and labeled documents . Furthermore , a   detailed analysis of the proposed model shows that   interpolation and stochastic perturbation positively   contribute to the overall performance .   Our contributions in this work are threefold :   ‚Ä¢We propose to augment documents for dense   retrieval models to tackle the problem of insufÔ¨Å-   cient labels of query - document pairs .   ‚Ä¢We present two novel document augmentation   schemes for dense retrievers : interpolation and   perturbation of document representations .   ‚Ä¢We show that our method achieves outstanding   retrieval performances on both labeled and unla-   beled documents on open - domain QA tasks.2 Related Work   Dense Retriever Dense retrieval models ( Lee   et al . , 2019 ; Karpukhin et al . , 2020 ) have gained   much attention , which generate dense representa-   tions for queries and documents . However , dense   retrieval faces a critical challenge from limited   training data . Recent work has addressed such a   problem by generating extra query - document pairs   to augment those pairs to the original dense re-   trieval model ( Liang et al . , 2020 ; Ma et al . , 2021 ;   Qu et al . , 2021 ) , or by regularizing the model ( Ros-   set et al . , 2019 ) . However , unlike ours that automat-   ically augments data during a training phase , these   methods require extensive computational resources   for an additional generation step of explicitly query-   document pairing before training the retriever .   Data Augmentation Since data augmentation is   crucial to the performance of deep neural networks ,   it is widely applied to diverse domains ( Shorten and   Khoshgoftaar , 2019 ; Hedderich et al . , 2021 ) , where   interpolation and perturbation are dominant meth-   ods . Mixup interpolates two items , such as pixels   of images , to augment the training data ( Zhang   et al . , 2018 ; Verma et al . , 2019 ) , which is also   adopted for NLP ( Chen et al . , 2020 ; Yin et al . ,   2021 ) . However , none of the previous work has   shown the effectiveness of mixup when applied to   retrieval tasks . Besides interpolation , Wei and Zou   ( 2019 ) and Ma ( 2019 ) proposed perturbation over   words , and Lee et al . ( 2021b ) proposed perturbation   over word embeddings . Jeong et al . ( 2021 ) and Gao   et al . ( 2021 ) perturbed text embeddings to generate   diverse sentences and to augment positive sentence   pairs in unsupervised learning . In contrast , we   address dense retrieval , perturbing document repre-   sentations with dropout ( Srivastava et al . , 2014 ) in   a supervised setting with labeled documents .   3 Method   We begin with the deÔ¨Ånition of dense retrieval .   Dense Retrieval Given a pair of query qand doc-   umentd , the goal of dense retrieval is to correctly   calculate a similarity score between them from the   dense representations qandd , as follows :   f(q;d ) = sim(q;d ) ;   q = E(q;)and d = E(d;);(1 )   wherefis a scoring function that measures the   similarity between a query - document pair , simis a443   similarity metric such as cosine similarity , and E   andEare dense encoders for a query and docu-   ment , respectively , with parameters = ( ; ) .   A dense retrieval scheme generally uses the   negative sampling strategy to distinguish the rel-   evant query - document pairs from irrelevant pairs ,   which generates an effective representation space   for queries and documents . We specify a relevant   query - document pair as ( q;d)2  , and an irrel-   evant pair as ( q;d)2  , where  \  = ; .   The objective function is as follows :   minXXL(f(q;d);f(q;d));(2 )   where a loss function Lis a negative log - likelihood   of the positive document . Our goal is to augment   a set of query - document pairs , by manipulating   documents with their interpolation or perturbation ,   which we explain in the next paragraphs .   Interpolation with Mixup As shown in interpo-   lation of Figure 2 , we aim at augmenting the doc-   ument representation located between two labeled   documents to obtain more query - document pairs ,   which could be useful to handle unlabeled docu-   ments in the middle of two labeled documents . To   achieve this goal , we propose to interpolate the   positive and negative documents ( d;d)for the   given queryq , adopting mixup ( Zhang et al . , 2018 ) .   Note that , since the input documents to the encoder   Eare discrete , we use the output embeddings of   documents to interpolate them , as follows :   ~d=d+ ( 1 )d ; ( 3 )   where ~dis the mixed representation of positive   and negative documents for the given query q , and   2[0;1 ] . We then optimize the model to estimate   the similarity sim(q;~d)between the interpolated   document and the query as the soft label with a   binary cross - entropy loss . The output of the cross-   entropy loss is added to the original loss in equa-   tion 2 . One notable advantage of our scheme is   that the negative log - likelihood loss in equation 2   maximizes the similarity score of the positive pair ,   while minimizing the score of the negative pair ;   thus there are no intermediate similarities between   arbitrary query - document pairs . However , ours   can obtain query - document pairs having soft labels ,   rather than strict positive or negative classes , by   interpolating the positive and negative documents .   Stochastic Perturbation with Dropout In addi-   tion to our interpolation scheme to handle unla-   beled documents in the space of interpolation of   two labeled documents , we further aim at perturb-   ing the labeled document to handle its nearby un-   labeled documents as shown in Figure 2 ( c ) . In   order to do so , we randomly mask the representa-   tion of the labeled document , obtained by the doc-   ument encoder E , with dropout , where we sam-   ple masks from a Bernoulli distribution . In other   words , if we sample ndifferent masks from the   distribution , we obtain ndifferent query - document   pairs   ( q;d ) 	 from one positive pair ( q;d ) .   By doing so , we augment ntimes more positive   query - document pairs by replacing a single posi-   tive pair ( q;d)in equation 2 . Moreover , since the   document perturbation is orthogonal to the interpo-   lation , we further interpolate between the perturbed   positive document dand the negative document   dfor the given query in equation 3 , to augment a   soft query - document pair from perturbation .   EfÔ¨Åciency Data augmentation methods are gen-   erally vulnerable to inefÔ¨Åciency , since they need   a vast amount of resources to generate data and   to forward the generated data into the large lan-   guage model . However , since our interpolation and   perturbation methods only manipulate the already444   obtained representations of the documents from   the encoder E , we do n‚Äôt have to newly generate   document texts and also to forward generated docu-   ments into the model , which greatly saves time and   memory ( see Table 3 ) . We provide a detailed analy-   sis and discussion of efÔ¨Åciency in Appendix B.1 .   4 Experiments   4.1 Experimental Setups   Here , we describe datasets , models , and implemen-   tation details for experiments . More experimental   details are shown in Appendix A. Our code is pub-   licly available at github.com/starsuzi/DAR .   Datasets For documents to retrieve , we use the   Wikipedia , following Karpukhin et al . ( 2020 ) ,   where the processed dataset contains 21,015,324   passages . To evaluate retrieval models , we use two   open - domain QA datasets , following Karpukhin   et al . ( 2020 ): 1 ) Natural Questions ( NQ ) is col-   lected with Google search queries ( Kwiatkowski   et al . , 2019 ) ; 2 ) TriviaQA ( TQA ) is a QA collec-   tion scraped from the Web ( Joshi et al . , 2017 ) .   Retrieval Models 1 ) BM25 is a sparse term-   based retrieval model based on TF - IDF ( Robertson   et al . , 1994 ) . 2 ) Dense Passage Retriever ( DPR )   is a dense retrieval model with a dual - encoder of   query - document pairs ( Karpukhin et al . , 2020 ) . 3 )   DPR with Query Augmentation ( DPR w/ QA )   augments pairs with query generation for the doc-   ument , adopting ( Liang et al . , 2020 ; Mao et al . ,   2021a ) . 4 ) DPR with Document Augmentation   ( DPR w/ DA ) augments pairs by replacing words   in the document ( Ma , 2019 ) . 5 ) DPR with Ax-   iomatic Regularization ( DPR w/ AR ) regularizes   the retrieval model to satisfy certain axioms ( Ros-   set et al . , 2019 ) . 6 ) DAR is ours with interpolation   and perturbation of document representations .   Metrics 1 ) Top - K Accuracy ( T - K ) computes   whether a query ‚Äôs answer is included in Top - K   retrieved documents . 2 ) Mean Reciprocal Rank   ( MRR ) and3 ) Mean Average Precision ( MAP )   measure the Ô¨Årst rank and the average precision of   query - relevant retrieved documents , respectively . Implementation Details For the dense retrieval   model based on the DPR framework , we refer to the   publicly available code from DPR ( Karpukhin et al . ,   2020 ) . We set the training epoch as 25 and batch   size as 32 under academic budgets with a single   GeForce RTX 3090 GPU having 24 GB memory .   We use in - batch negative sampling as our negative   sampling strategy without hard negative samples .   Also , we retrieve 100 passages per question .   We use both interpolation and perturbation   schemes for our augmentation methods . Specif-   ically , for the interpolation method , we set 2   [ 0;1]in equation 3 to be sampled from the uniform   distribution . Also , for the perturbation method , we   set the dropping rate as 0.1 , and the number of   dropout masks nis selected in the range of 3 to 9 .   4.2 Results   In this subsection , we show the overall performance   of our DAR , and then give detailed analyses .   Overall Results As Table 1 shows , DAR outper-   forms dense retrieval baselines on all datasets on   the DPR framework . Note that DAR contributes   to more accurate retrieval performance , since the   smallerKgives higher performance improvements .   Furthermore , Figure 3 shows that , with our method ,   the retrieval performance on unlabeled documents   ‚Äì not seen during training ‚Äì together with the la-   beled ones is improved , where performance gains   on unlabeled are remarkable . To see the robustness   of DAR on other retrievers , we further evaluate   our model on the recent ANCE framework ( see   Appendix A for setups ) . As Table 2 shows , we   observe that the performance improvement is more   dominant on MRR when given a smaller number of   training queries ( low - resource settings ) , that DAR   effectively augments document representations .   Results on Query Augmentation We focus on   the problem of a notably small proportion of la-   beled documents in the training dataset , and pro-   pose to augment representations of unlabeled doc-   uments , which are not seen during training . How-   ever , it is also possible to augment representations   of queries ‚Äì likely to be unseen at the test time445   ‚Äì by applying our interpolation and perturbation   methods directly to queries . Note that we refer   to our query augmentation method as Query Aug-   mentation for dense Retrieval ( QAR ) . As shown in   Table 1 , our proposed augmentation strategies also   effectively improve the retrieval performance even   when applied to queries . This result implies that   our method is versatile , regardless of whether it is   applied to documents or queries .   Effectiveness of Interpolation & Perturbation   To understand how much our proposed interpo-   lation and perturbation techniques contribute to   the performance gain , we perform ablation studies .   Table 4 shows that each of the interpolation and   stochastic perturbation positively contributes to the   performance . In particular , when both of them are   simultaneously applied , the performance is much   improved , which demonstrates that these two tech-   niques are in a complementary relationship .   Batch Size We test DAR with varying numbers   of batch sizes . Figure 4 indicates that our DAR   consistently improves the retrieval performance .   Note that the smaller the batch size , the bigger   the performance gap . Also , the batch size 16 of   DAR outperforms the batch size 32 of the baseline ,   which highlights that DAR effectively augments   document representations with a small batch .   Reader Performance To see whether accurately   retrieved documents lead to better QA performance ,   we experiment with the same extractive reader from   DPR without additional re - training . Figure 5 illus-   trates the effectiveness of our method on passage   reading with varying numbers of retrieved docu-   ments . We observe that our retrieval result with   small retrieved documents ( i.e. , K= 10 ) signiÔ¨Å-   cantly improves the performance of the reader . This   implies that a more accurate retrieval on smaller K   in Table 1 helps achieve the improved QA perfor-   mance as Lee et al . ( 2021a ) described . Furthermore ,   our reader performance may be further enhanced   with advanced reading schemes ( Mao et al . , 2021a ;   Qu et al . , 2021 ; Mao et al . , 2021b ) .   Negative Sampling Strategy To see the effec-   tiveness of our DAR coupled with an advanced neg-   ative sampling scheme , we compare DAR against   the baseline with the hard negative sampling strat-   egy from BM25 ( Karpukhin et al . , 2020 ) . Table 5   shows that DAR with hard negative sampling out-   performs the baseline method . The results demon-   strate that the performance of dense retrieval mod-   els could be further strengthened with a combina-   tion of our augmentation methods and advanced   negative sampling techniques . Also , in all our ex-   periments of the ANCE framework , we already use   the strategy of negative sampling in Xiong et al .   ( 2021 ) , where we observe the clear performance   improvement of our DAR on ANCE in Table 2 .   Results on Different Data Processing We addi-   tionally evaluate DAR on another NQ test dataset ,   following the processing procedure of Thakur et al .   ( 2021 ) . For experiments , we reuse the same train-   ing checkpoint used in Table 1 , as the training   dataset is equal across the settings of Karpukhin   et al . ( 2020 ) and Thakur et al . ( 2021 ) . As Ta-   ble 6 shows , our DAR also consistently outper-   forms all baselines when tested on the NQ test set   from Thakur et al . ( 2021 ) . This conÔ¨Årms that our   DAR robustly improves retrieval performances , re-   gardless of the speciÔ¨Åc data processing strategies .   5 Conclusion   We presented a novel method of augmenting doc-   ument representations focusing on dense retriev-   ers , which require an extensive amount of labeled   query - document pairs for training . SpeciÔ¨Åcally ,   we augment documents by interpolating and per-   turbing their embeddings with mixup and dropout   masks . The experimental results and analyses on   multiple benchmark datasets demonstrate that DAR   greatly improves retrieval performances .   Acknowledgements   This work was supported by Institute for Infor-   mation and communications Technology Promo-   tion ( IITP ) grant funded by the Korea government   ( MSIT ) ( No . 2018 - 0 - 00582 , Prediction and aug-   mentation of the credibility distribution via linguis-   tic analysis and automated evidence document col-   lection).446Ethical Statements   Retrieving the most relevant documents from the   user ‚Äôs query is increasingly important in a real-   world setting , as it is widely used from web search ,   to question answering , to dialogue generation sys-   tems . Notably , our work contributes to the accurate   retrieval of documents with the proposed data aug-   mentation strategies , thus improving the document   retrieval performances on real - world applications .   However , we have to still consider the failure of   retrieval systems on low - resource but high - risk do-   mains ( e.g. , biomedicine ) , where the labeled data   for training retrieval models is limited yet one fail-   ure can yield a huge negative impact . While we   strongly believe that our data augmentation strate-   gies ‚Äì interpolation and perturbation of document   representations ‚Äì are also helpful to improve the   retrieval performances on such low - resource do-   mains , the model ‚Äôs prediction performance is still   far from perfect , and more efforts should be made   to develop a reliable system .   References447448449   A Experimental Setups   Datasets To evaluate the performance of retrieval   models , we need two types of datasets : 1 ) a set   of documents to retrieve , and 2 ) pairs of a query   and a relevant document , having an answer for   the query . We Ô¨Årst explain the datasets that we   used for the DPR framework ( Karpukhin et al . ,   2020 ) , and then describe the dataset for the ANCE   framework ( Xiong et al . , 2021 ) .   For documents to retrieve , we use the Wikipedia   snapshot from December 20 , 2018 , which contains   21,015,324 passages consisting of 100 tokens , fol-   lowing Karpukhin et al . ( 2020 ) for the DPR frame-   work . For open - domain QA datasets , we use Natu-   ral Question ( NQ ) ( Kwiatkowski et al . , 2019 ) and   Trivia QA ( TQA ) ( Joshi et al . , 2017 ) , following the   dataset processing procedure of Karpukhin et al .   ( 2020 ) . We report the statistics of the training , vali-   dation , and test sets on NQ and TQA in Table 7 .   To see the performance gain of our DAR on other   dense retrieval models , we evaluate DAR on the   ANCE framework ( Xiong et al . , 2021 ) , which is   one of the recent dense retrieval models . ANCE is   evaluated on the MS MARCO dataset , thus we use   MS MARCO for training and testing our model .   Note that training ANCE with the full MS MARCO   dataset requires 225 GPU hours even after exclud-   ing the excessive BM25 pre - training and inference   steps . Thus we randomly sample the MS MARCO   dataset to train the model under academic budgets .   SpeciÔ¨Åcally , the subset of our MS MARCO pas-   sage dataset contains 500,000 passages . Also , we   randomly divide the training queries into two sub-   sets : one for 10,000 training queries and the other   for 50,000 training queries . Then we align the sam-   pled training queries to the query - document pairs   in the MS MARCO dataset . On the other hand ,   we do not modify the validation set ( dev set ) of   query - document pairs for testing . We summarize   the statistics of the dataset in Table 7 . Note that   since the test set of MS MARCO is not publicly   open , we evaluate the dense retrievers with the val-   idation set , following Xiong et al . ( 2021).Metrics Here , we explain the evaluation metrics   for retrievers in detail . SpeciÔ¨Åcally , given an in-   put query , we measure the ranks of the correctly   retrieved documents for the DPR framework with   the following metrics :   1 ) Top - K Accuracy ( T - K ): It measures whether   an answer of the given query is included in the   retrieved Top - K documents .   2 ) Mean Reciprocal Rank ( MRR ): It com-   putes the rank of the Ô¨Årst correct document for   the given query among the Top-100 retrieved docu-   ments , and then computes the average of the recip-   rocal ranks for all queries .   3 ) Mean Average Precision ( MAP ): It com-   putes the mean of the average precision scores for   all queries , where precision scores are calculated   by the ranks of the correctly retrieved documents   among Top-100 ranked documents .   We use the following evaluation metric for the   reader , which identiÔ¨Åes the answer from retrieved   documents .   1 ) Exact Match ( EM ): It measures whether the   reader exactly predicts one of the reference answers   for each question .   Note that , for the ANCE framework , we follow   the evaluation metrics , namely MRR@10 and Re-   call@1k , in the original paper ( Xiong et al . , 2021 ) .   Experimental Implementation Details For   dense retrieval models based on the DPR frame-   work , we follow the dual - encoder structure of   query and document by using the publicly available   code from DPR(Karpukhin et al . , 2020 ) . For   all experiments , we set the batch size as 32 , and   train models on a single GeForce RTX 3090 GPU   having 24 GB memory . Note that , in contrast to   the best reported setting of DPR which requires   industrial - level resources of 8 V100 GPUs ( 8    32 GB = 256 GB ) for training with a batch size of   128 , we use a batch size of 32 to train the model   under academic budgets . We optimize the model   parameters of all dense retrieval models with the   Adam optimizer ( Kingma and Ba , 2015 ) having a   learning rate of 2e-05 . We train the models for 25   epochs , following the analysisthat the training   phases converge after 25 epochs .   For the retrievers based on the ANCE frame-   work , we refer to the implementation from   ANCE(Xiong et al . , 2021 ) . In order to directly450measure the performance gain of the dense retrieval   models based on ANCE from using our DAR , we   use the pre - trained RoBERTa without warming up   with the BM25 negatives . We train all the dense   retrieval models for 50,000 steps with a single   GeForce RTX 3090 GPU having 24 GB memory ,   and simultaneously generate the ANN index with   another GeForce RTX 3090 GPU , following Xiong   et al . ( 2021 ) . Following the standard implementa-   tion setting , we set the training batch size as 8 , and   optimize the model with the LAMB optimizer ( You   et al . , 2020 ) with a learning rate of 1e-6 .   Architectural Implementation Details For our   augmentation methods , we use both interpolation   and perturbation schemes of document represen-   tations obtained from the document encoder E   in equation 1 . SpeciÔ¨Åcally , given a positive query-   document pair ( q;d ) , we Ô¨Årst perturb the docu-   ment representation dwith dropout masks sam-   pled from a Bernoulli distribution , which gener-   atesnnumbers of perturbed document representa-   tions   d 	 . Then , we augment them to gen-   eratennumbers of positive query - document pairs   ( q;d ) 	 , which we use in equation 2 . We   search the number of perturbations nin the range   from 3 to 9 , and set the probability of the Bernoulli   distribution as 0.1 .   Instead of only using positive or negative pairs ,   we further augment query - document pairs hav-   ing intermediate similarities with mixup . SpeciÔ¨Å-   cally , we interpolate representations between the   perturbed - positive document dand the negative   document dfor the given query q , with2[0;1 ]   in equation 3 sampled from a uniform distribution .   Note that , given a positive pair of a query and a doc-   ument , we consider the documents not identiÔ¨Åed   as positive in the batch as negative documents . In   other words , if we set the batch size as 32 , then we   could generate 31interpolated document represen-   tations from 1positive pair and 31negative pairs .   To jointly train the interpolation scheme with the   original objective , we add the loss obtained from   interpolation to the loss in equation 2 .   B Additional Experimental Results   B.1 EfÔ¨Åciency   As described in the EfÔ¨Åciency paragraph of Sec-   tion 3 , compared to the existing query augmen-   tation methods ( Liang et al . , 2020 ; Ma et al . ,   2021 ; Qu et al . , 2021 ) , document augmentationmethod ( Ma , 2019 ) , and word replacement method   for regularization ( Rosset et al . , 2019 ) , our method   of augmenting document representations with inter-   polation and perturbation in a dense representation   space is highly efÔ¨Åcient . This is because , unlike the   baselines above , we do not explicitly generate or re-   place a query or document text ; but rather we only   manipulate the representations of documents . This   scheme greatly saves the time for training , since   additional forwarding of the generated or replaced   query - document pairs into the language model is   not required for our data augmentation methods .   To empirically validate the efÔ¨Åciency of our   methods against the baselines , we report the mem-   ory usage and time for training a retrieval model per   epoch in Table 3 . As for memory efÔ¨Åciency , all the   compared dense retrieval models using data aug-   mentation methods , including ours , use the same   amount of maximum GPU memory . This shows   that the overhead of memory usage comes from op-   erations in the large - size language model , such as   BERT ( Devlin et al . , 2019 ) , not from manipulating   the obtained document representations to augment   the query - document pairs . Technically speaking ,   there are no additional parameters to augment doc-   ument representations ; thus our interpolation and   perturbation methods do not increase the memory   usage . On the other hand , DPR w/ AR excessively   increases the memory usage , since it requires an   extra forwarding process to the language model to   represent the additional word - replaced sentences   for regularization , instead of using the already ob-   tained dense representations like ours .   We also report the training time for dense re-   trievers in Table 3 . Note that , for the explicit aug-   mentation method based models , such as DPR w/   QA and DPR w/ DA , we exclude the extra time for   training a generation model and generating a query   or document for the given text . Also , we addition-   ally generate the same number of query - document   pairs in the training set , where the total amount   of training data - points for DPR w/ QA and DPR   w/ DA baselines are twice larger than the original   dataset . Unlike these explicit query or document   generation baselines , we perturb the document n   times , but also interpolate the representations of   positive and negative documents . As shown in Ta-   ble 3 , our DAR is about doubly more efÔ¨Åcient than   the explicit text augmentation methods , since DPR   w/ QA and DPR w/ DA explicitly augment query-   document pairs instead of using the obtained dense451   representations like ours . Also , our DAR takes a lit-   tle more time to augment document representations   than the base DPR model , while signiÔ¨Åcantly im-   proving retrieval performances as shown in Table 1 .   Even compared to the term replacement based reg-   ularization model ( DPR w/ AR ) , our DAR shows   noticeable efÔ¨Åciency , since an additional embed-   ding process of the document after the word re-   placement on it requires another forwarding step   besides the original forwarding step .   B.2 Reproduction of DPR   We strictly set the batch size as 32 for training   all the dense retrievers using the DPR framework ;   therefore the retrieval performances are different   from the originally reported ones in Karpukhin   et al . ( 2020 ) that use a batch size of 128 . However ,   while we use the available code from the DPR pa-   per , one may wonder if our reproduction result is   accurate . Therefore , since Karpukhin et al . ( 2020 )   provided the retrieval performances of the DPR   with different batch sizes ( e.g. , a batch size of 32 ) ,   evaluated on the development ( validation ) set of   the NQ dataset , we compare the Top - K accuracy   between the reported scores and our reproduced   scores . Table 8 shows that our reproduced Top - K   accuracy scores with three different Ks ( e.g. , Top-   5 , Top-20 , and Top-100 ) are indeed similar to the   reported ones , with ours even higher , thus showing   that our reproductions are accurate .   B.3 Experiment on WebQuestions   One may have a concern that , as a sparse retrieval   model ‚Äì BM25 ‚Äì outperforms all the other dense re-   trieval models on the TQA dataset in Table 1 , TQA   is not good enough to demonstrate the strength of   our dense augmentation strategy . While we believe   that sparse retrieval models are not our competi-   tors as we aim to improve the dense retrieval mod-   els with data augmentation , in order to clear out   such a concern , we additionally train and evaluate   our DAR on the WebQuestions ( WQ ) dataset ( Be-   rant et al . , 2013 ) , following the data processing   procedure from ( Karpukhin et al . , 2020 ) . As Ta-   ble 9 shows , our DAR outperforms both dense and   sparse retrieval models . Thus , the best scheme   among sparse and dense retrievers still depends on   the dataset , and combining sparse and dense mod-   els to complement each other will be a valuable   research direction , which we leave as future work.452   Federico Tavella and Viktor Schlegel and Marta Romeo   Aphrodite Galata and Angelo Cangelosi   { name.surname}@manchester.ac.uk   Department of Computer Science , The University of Manchester   Abstract   1 Introduction   Around 200 languages in the world are signed   rather than spoken , featuring their own vocabu-   lary and grammatical structures . For example the   American Sign Language ( ASL ) is not a mere trans-   lation of English into signs and is unrelated to the   British Sign Language ( BSL ) . Their non - textual   nature introduces many challenges to their auto-   mated processing , compared with purely textual   NLP . Research on Sign Language Processing ( SLP )   encompasses tasks such as sign language detection ,   i.e. recognising if and which signed language is   performed ( Moryossef et al . , 2020 ) and sign lan-   guage recognition ( SLR ) ( Koller , 2020 ) , i.e. the   identification of signs either in isolation or in con-   tinuous speech . Other tasks concern the translation   from signed to spoken ( or written ) ( Camgoz et al . ,   2018 ) language or the production of signs from text   ( Rastgoo et al . , 2021 ) . With the recent success of   deep learning - based approaches in computer vision   ( CV ) , as well as advancements in ‚Äî from the CV   perspective ‚Äî related tasks of action and gesture   recognition ( Asadi - Aghbolaghi et al . , 2017 ) , SLP   is gaining more attention in the CV community   ( Zheng et al . , 2017 ) .   Due to the complexity of the tasks , some recent   approaches to various SLP tasks implicitly rely   onphonological features ( Tornay , 2021 ; Metaxas   et al . , 2018 ; Gebre et al . , 2013 ; Tavella et al . , 2021 ) .   Surprisingly , however , little work has been carried   out on explicitly modelling the phonology of signed   languages . This presents a timely opportunity to   investigate signed languages from the perspective   of computational linguistics ( Yin et al . , 2021 ) . In   the context of signed languages , phonology typi-   cally distinguishes between manual features , such   as usage , position and movement of hands and fin-   gers , and non - manual features , such as facial ex-   pressions . Sign language phonology is a matured   field with well - developed theoretical frameworks   ( Liddell and Johnson , 1989 ; Fenlon et al . , 2017 ;   Sandler , 2012 ) . These phonological features , or   phonemes , are drawn from a fixed inventory of   possible configurations which is typically much   smaller than the vocabulary of signed languages   ( Borg and Camilleri , 2020 ) . For example , there is   only a limited number of fingers that can be used453to perform a sign due to anatomical constraints .   Hence , different signs share phonological proper-   ties and well performing classifiers can be used   to predict those properties for signs unseen during   training . This potentially holds even across differ-   ent languages , because , while different languages   may dictate different combinations of phonemes ,   there are also significant overlaps ( Tornay et al . ,   2020 ) .   Finally , these phonological properties have a   strong discriminatory power when determining   signs . For example , in ASL - Lex ( Caselli et al . ,   2017 ) , a lexicon which also captures phonology   information , the authors report that more than 50 %   of its 994 described signs have a unique combina-   tion of only six phonological properties and more   than 80 % of the signs share their combination with   at most two other signs . By relying on this phono-   logical information from resources such as ASL-   Lex , many signs can be uniquely determined . This   means that well performing classifiers can leverage   this information to predict signs without having   encountered them during training . This is a capa-   bility that current data - driven approaches to SLR   lack by design ( Koller , 2020 ) . Thus , in combina-   tion , mature approaches to phonology recognition   can facilitate the development of sign language   resources , for example by providing first - pass sil-   ver annotations for new sign languages based on   their phonological properties . This is an important   task for both documenting low - resource sign lan-   guages as well as rapid developing of large - scale   datasets , and for fully harnessing data - driven CV   approaches .   To spur research in this direction , we extend   the preliminary work by Tavella et al . ( 2021 ) and   introduce the task of Phonological Property Recog-   nition ( PPR ) . More specifically , with this paper , we   contribute ( i)WLASLLex2001 , a large - scale , auto-   matically constructed PPR dataset , ( ii)an analysis   of the dataset quality , and ( iii)an empirical study of   the performance of different deep - learning based   baselines thereon .   2 Methodology   We address PPR as a classification problem based   on features extracted from videos of people speak-   ing SL . Although manual annotation approaches   are widely adopted , these are time consuming and   require expert knowledge . Instead , we rely on au-   tomated dataset construction . On a high level , wecross - reference a large - scale ASL SLR dataset with   an ASL Lexicon and annotate videos of signs with   their corresponding phonological properties . We   then extract skeletal features , by taking advantage   of pre - trained deep models from the computer vi-   sion community ( Rong et al . , 2021 ; Wang et al . ,   2019 ) . Finally , we train several deep models to   classify them as phonological classes .   2.1 Dataset construction   As previously mentioned , ASL - Lex ( Caselli et al . ,   2017 ) contains phonological features of American   Sign Language , such as where the sign is executed ,   the movement performed by the hand and the num-   ber of hands and fingers involved . The latter prop-   erties were coded by 3 ASL - versed people . In   our work , we are interested in recognising phono-   logical properties from videos of people speaking   ASL . Consequently , we aim to construct a dataset ,   suitable for supervised learning , containing videos   labelled with six phonological properties . Specif-   ically , we choose the manual properties with the   strongest discriminatory power to determine signs   based on their configuration ( Caselli et al . , 2017 ):   ( i)flexion : aperture of the selected fingers of the   dominant hand at sign onset ,   ( ii)major location : general location of the domi-   nant hand at sign onset ,   ( iii ) minor location : specific location of the domi-   nant hand at sign onset ,   ( iv)movement : the first movement path of the   sign ,   ( v)selected fingers : fingers that are moving or   are foregrounded during that movement , and   ( vi ) sign type : symmetry of the hands according   to Battison ( 1978 ) .   A detailed description of all the properties is pro-   vided in the appendix .   One of the limitations of ASL - Lex is the small   number of examples and lack of variety : its first   iteration ( ASL - Lex 1.0 ) contains less than 1000   videos , all signed by the same person . While suffi-   cient for educational purposes , these videos are of   limited suitability for developing robust classifiers   that can capture the diversity of ASL speakers ( Yin   et al . , 2021 ) . To this end , we source videos from   WLASL ( Li et al . , 2020 ) ( Word Level - ASL ) , one454of the largest available SL datasets , featuring more   than 2000 glosses demonstrated by over 100 peo-   ple , for a total of more than 20000 videos . Each   sign is performed by at least 3 different signers ,   which implies greater variability compared to hav-   ing one gloss performed by only one user . By cross   referencing ASL - Lex and WLASL2000 based on   corresponding glosses , we can increase the number   of samples available to train our models .   Finally , to leverage state of the art SLR architec-   tures that operate over structured input , we enrich   each raw video with its extracted keypoints that   represent the joints of the speaker . To do so , we use   two pretrained models , FrankMocap ( Rong et al . ,   2021 ) and HRNet ( Wang et al . , 2019 ) . While these   tracking algorithms follow different paradigms , the   former extracting 3D coordinates based on a pre-   dicted human model and the latter predicting key-   points as coordinates from videos directly , they   produce similar outputs . An important distinction   is that while FrankMocap estimates the 3D key-   points , HRNet outputs 2D keypoints with associ-   ated prediction confidence scores . We use these   different models to explore whether different track-   ing algorithms affect the recognition of phonolog-   ical classes . We select a subset of features of the   upper body , namely : nose , eyes , shoulders , elbows ,   wrists , thumbs and first / last knuckles of the fingers .   These manual features were determined to be the   most informative while performing sign language   recognition ( Jiang et al . , 2021b ) .   Our final dataset , WLASL - Lex2001   ( WLASL2000 + ASL - Lex 1.0 ) , is composed   of 10017 videos corresponding to 800 glosses ,   3D skeletons ( x , y , zfrom FrankMocap and x ,   yandscore from HRNet ) labelled with their   phonological properties . A characteristic of this   dataset is that it follows a long tailed distribution .   Due to the nature of language , some phonological   properties are more common than others , which   means that some classes are more represented than   others . On the one hand , the training setup for   our models should take this factor into account ,   but on the other hand , the advantage of training   over phonological classes instead of glosses is that   different glosses can share phonological classes .   2.2 Models   To estimate the complexity of the dataset , we use   the majority - class baseline and the Multi - Layer   Perceptron ( MLP ) as basic deep models . We fur - ther use Long Short - Term Memory ( LSTM ) and   Gated Recurrent Units ( GRU ) as models capable   of capturing the temporal component of videos . As   state - of - the - art SLP architectures that have been   used to perform SLR , we use the I3D 3D Convo-   lutional Neural Network ( Carreira and Zisserman ,   2017 ; Li et al . , 2020 ) able to learn from raw videos ,   and the Spatio - Temporal Graph Convolutional Net-   work ( STGCN ) ( Jiang et al . , 2021b ) that captures   both spatial and temporal components from the   extracted keypoints .   2.3 Experimental Setup   For each phonological property we generate dataset   splits and train dedicated models separately . While   a multi - class multi - label approach could achieve   higher scores , by relying on potential interdepen-   dencies of different properties , we chose to model   the properties in isolation , to disentangle the factors   that affect the learnability of each property . From   now on , when we mention the dataset , we refer   to an instance of the WLASL - Lex 2001 dataset ,   where labels are the values of a single phonological   class .   We make this distinction because we produce   six different train , validation and test splits ( with   a70 : 15 : 15 ratio ) stratifying on the correspond-   ing phonological property ( Phoneme ) . By doing   so , we make sure that ( a)all splits contain all pos-   sible labels for a classification target ( i.e. phono-   logical property ) and ( b)follow the same distribu-   tion . Since we source the videos from WLASL , we   have multiple videos representing each gloss , there-   fore , randomly splitting our data will result in the   fact that glosses in the test set might appear in the   training set as well , signed by a different speaker .   Thus , to investigate how well the models can pre-   dict properties on unseen glosses , we also produce   label - stratified splits on gloss - level ( Gloss ) , such   that videos of glosses in the validation and test   set do not appear in training data and vice versa .   Thus , to summarise , experiments in the Phoneme   setting aim to evaluate the capability to recognise   phonological properties of signs that were already   encountered in the training data , but are performed   by a different speaker in the test set . Conversely ,   experiments in the Gloss setting aim to evaluate the   capability to recognise phonological properties of   signs completely unseen during training .   We use an I3D model that has been pre - trained   on Kinetics-400 ( Carreira and Zisserman , 2017)455   and fine - tune it on raw videos from our datasets .   The other models are trained from scratch using   keypoints as input . We fix the length of all input to   150 frames , longer sequences are truncated while   shorter sequences are looped to reach the fixed   length . We select the best performing model based   on performance on the validation set and for the   final test set performance we train the models on   both train and validation sets . For more details on   model selection , consult the appendix . We mea-   sure both accuracy , to investigate how well models   perform in general , and class - balanced accuracy to   take into account how well they are able to model   different classes of the phonological properties .   3 Results and discussion   The upper half of Table 1 presents the results for   the six dataset splits for the Phoneme setting , where   glosses in test data could have appeared in training   data as well . The poor performance of the simple   MLP architecture suggests that the tasks are in fact   challenging and do not exhibit easily exploitable   regularities . Due to its simplicity , it is barely able   to reach the baseline for some properties ( 34 % vs.   35 % and44 % vs.50 % formovement andflexion   respectively ) . In particular , MLP classifying based   on FrankMocap ( MLP ) output is often the worst   performing combination . Conversely , STGCN us-   ing HRNet output ( STGCN ) outperforms other   models on all six tasks . In some cases , for example   when predicting movement orflexion , it is the only   model which significantly surpasses the majority   class baseline . This superior performance is ex - pected , as this specific combination of the STGCN   operating over HRNet - extracted keypoints has been   shown to be the largest contributor to the SLR per-   formance on the WLASL2000 dataset ( Jiang et al . ,   2021a ) .   Models that operate over structured input often   outperform the 3D CNN , demonstrating the utility   of additional information provided by the skeleton   features . The results also suggest that models using   the HRNet skeleton output outperform those who   use FrankMocap , possibly due to the confidence   scores produced by HRNet and associated with   the coordinates . This difference in performance   suggests to conduct a more rigorous study to in-   vestigate the impact of different feature extraction   methods as a possible future research direction .   The lower half of Table 1 shows the performance   of models to predict the phonological properties   of unseen glosses ( Gloss ) . The performance of all   tasks and all models deteriorates , suggesting that   their success is partly derived from exploiting the   similarities between glosses that appear in training   and test data . However , the best model , STGCN ,   performs comparably to the Phoneme -split , with a   drop of less than 10accuracy points for five of the   six tasks .   Often , crowd sourced ( Polonio et al . , 2018 ) or   automatically constructed datasets such as ours ,   have a performance ceiling , possibly due to incor-   rectly assigned ground truth labels or low quality   of input data ( Chen et al . , 2016 ; Schlegel et al . ,   2020 ) . To investigate the former , we measure the   agreement on videos that all models misclassify456using Fleiss ‚Äô Œ∫ . Intuitively , if models consistently   agree on a label different than the ground truth , the   ground truth label might be wrong . We find that   averaged across the six tasks , the agreement is neg-   ligible : 0.09¬±0.06and0.11¬±0.09forPhoneme   andGloss split , respectively .   Similarly , for the latter , if all models consistently   fail to assign any correct label for a given video   ( e.g. all models err on a video appearing in the test   sets of movement andflexion ) , this can hint at low   quality of the input , making it impossible to predict   anything correctly . We find that this is not the case   with WLASL - LEX2001 , as videos appearing in   test sets of different tasks tend to have a low mu-   tual misclassification rate : 1%and0.7%of videos   appearing in test sets of two and three tasks were   misclassified by all models for all associated tasks   for the Phoneme split . For the Gloss split the num-   bers are 3and0%for two and three tasks , respec-   tively . Together , these observations suggest that   the models presented in this paper are unlikely to   reach the performance ceiling on WLASL - Lex2001   and more advanced approaches could obtain even   higher accuracy scores .   4 Conclusion   In this paper , we discuss the task of Phonologi-   cal Property Recognition ( PPR ) . We automatically   construct a dataset for the task featuring six phono-   logical properties and analyse it extensively . We   find that there is potential for improvement over   our presented data - driven baseline approaches . Re-   searchers pursuing this direction can focus on de-   veloping better - performing models , for example by   relying on jointly learning all properties , as labels   for different properties can be mutually dependent .   Another possible avenue is to investigate the   feasibility of using PRR to perform tokenisation of   continuous sign language speech , by decomposing   it into multiple phonemes , which is identified as   one of the big challenges of SLP ( Yin et al . , 2021 ) .   Acknowledgements   The authors would like to acknowledge the use   of the Computational Shared Facility at The Uni-   versity of Manchester . The work was partially sup-   ported by the UKRI TAS Node on Trust , the US Air   Force project THRIVE++ and the H2020 projects   TRAINCREASE , eLADDA and PERSEO.References457458A Hyperparameters optimization   Table 2 contains all the hyperparameters explored   during our experiment over each different model .   The best model is the one that maximises the   Matthew ‚Äôs correlation coefficient   with TP , TN , FP , FN being true / false posi-   tive / negative . For the STGCN we use hyperparam-   eters chosen by Jiang et al . ( 2021a ) , because initial   experiments on our data showed a difference of at   most 2 % accuracy , which is within the uncertainty   estimate . To find the optimal hyperparameters for   the other models , we perform Bayesian optimisa-   tion over a pre - defined set . We maximise Matthews   correlation coefficient ( MCC ) ( Matthews , 1975 ) on   the validation sets of all six tasks . We choose MCC   as it provides a good trade - off between overall and   class - level accuracy which is necessary due to the   unbalance inherently present in our dataset .   Model Parameters   MLPnumber of layers   hidden dimension   dropout   learning rate   scheduler step size   gamma   RNNnumber of RNN layers   RNN hidden dimension   RNN dropout   STGCNlearning rate   number of groups   block size ,   window size   scheduler step size   dropout   warmup epochs   3D CNNdropout   learning rate   gamma   scheduler step size   window sizeB Seed dependency   Table 3 illustrates the performance on the test set   for each model with respect to chance as measured   by training 5 models from different random seeds .   The performance difference is negligible suggest-   ing that model training is largely stable with regard   to chance .   Model Accuracy   MLP 74.39¬±0.35   RNN 79.12¬±0.46   STGCN 84.12¬±0.29   3D CNN 69.23¬±0.93   C Phonological classes description   Tables 4 to 9 describe in detail the meaning of   values for all the phonological classes according to   ASL - Lex ( Caselli et al . , 2017 ) .   The cardinality is calculated on WLASL - Lex ,   which is why some classes that are in ASL - Lex are   not represented ( i.e. , cardinality equal to 0 ) .   D Additional results   Table 10 illustrates additional results for several dif-   ferent metrics . In particular , we report micro- and   macro precision / recall and Matthews correlation   coefficient . These metrics help to give a better un-   derstanding of the classification results , as they are   affected more by data imbalance when compared   to accuracy.459Value Definition Cardinality   imrp index , middle , ring , pinky finger 4824   imr index , middle , ring finger 95   mrp middle , ring , pinky finger 28   i m index , middle finger 1296   ip index , pinky finger 51   mr middle , ring finger 0   mp middle , pinky finger 0   rp ring , pinky finger 0   i index finger 2547   m middle finger 259   r ring finger 0   p pinky 407   thumb thumb 510   Value Definition Cardinality   Head Sign is produced on or near the head 3137   Arm Sign is produced on or near the arm 219   Body Sign is produced on or near the trunk 1019   Hand Sign is produced on or near the non - dominant hand 2194   Neutral Sign is not produced in another location on the body 3448   Other Sign is produced in another unspecified location on the body 0   Value Definition Cardinality   1 Fully open : no joints of selected fingers are flexed 5037   2 Bent ( closed ): non - base joints are flexed 693   3 Flat - open : base joints flexed less than 90 degrees 909   4 Flat - closed : base joints flexed equal to or more that 90 degrees 507   5 Curved open : base and non - base joints flexed without contact 1130   6 Curved closed : base and non - base joints flexed with contact 642   7 Fully closed : base and non - base joints fully flexed 795   Stacked Stacked : Flexion of selected fingers differs 123   Crossed Crossed 181460461   Value Definition Cardinality   Straight Straight movement of the dominant hand through xyz space 1938   CurvedSingle arc movement of the dominant hand through xyz space   Hands may or may not make contact with multiple locations1255   BackAndForth Sequence of more than one straight or curved movements 3549   CircularCircular movement of the dominant hand through space   Rotation alone does not constitute a circular movement1129   None Entire sign ( or first free morpheme ) does not have a path movement 1748   Other Sign has another unspecified path movement 398462463   Koh Mitsuda , Ryuichiro Higashinaka , Tingxuan Li , Sen YoshidaNTT Corporation , JapanUniversity of Tsukuba , Japan   { koh.mitsuda.td , ryuichiro.higashinaka.tp ,   sen.yoshida.tu}@hco.ntt.co.jp , s2120816@s.tsukuba.ac.jp   Abstract   1 Introduction   Creating chatbots to behave like real people is   important in terms of believability ( Traum et al . ,   2015 ; Higashinaka et al . , 2018 ) . Errors in general   chatbots ( Higashinaka et al . , 2021 ) and chatbots   that follow a rough persona ( Li et al . , 2016 ; Zhang   et al . , 2018 ; Zhou et al . , 2020 ; Inoue et al . , 2020 ;   Song et al . , 2020 ; Roller et al . , 2020 ) have been   studied , but those in chatbots that behave like real   people have not been thoroughly investigated .   We analyzed dialogue data between a chatbot   that imitates a certain person and users to identify   ‚Äú errors related to the target person ‚Äù ( hereafter re-   ferred to as person - speciÔ¨Åc errors ) . We collected   a large amount of dialogue data between users and   the latest generation - based chatbot trained with a   large amount of dialogue data of the target per-   son and analyzed the errors . The results indicate   that person - speciÔ¨Åc errors can be divided into two   types : errors in attributes and those in relations ,   each of which can be divided into two levels : selfand other . The correspondence with the existing   taxonomy of errors was also investigated , and er-   rors that should be addressed in the future were   clariÔ¨Åed .   2 Dialogue data collection   We used a chatbot that imitates a speciÔ¨Åc person .   By making the chatbot available to the public , we   collected dialogue data from a large number of   users .   2.1 Chatbot   In our previous study , we collected a large amount   of dialogue data on a target person and created   a chatbot by Ô¨Åne - tuning a pre - trained encoder-   decoder Transformer model ( Mitsuda et al . , 2021 ) .   The speciÔ¨Åc character ( i.e. , target person ) was   Amadeus Kurisu , a character in a famous Japanese   video game ( STEINS;GATE ) . We used a role-   play - based question - answering ( QA ) scheme pro-   posed by Higashinaka et al . ( 2018 ) , in which fans   of a character provided questions and answers   by role - playing to collect the dialogue data on   that character . We collected a large amount of   QA pairs ( 44,805 ) from the fans . To add multi-   turn dialogues , we additionally created 4,500 dia-   logues ( 24,750 utterances ) by manually extending   the collected QA pairs .   As a pre - trained dialogue model , we used   the Japanese version of BlenderBot ( Japanese-   dialog - transformers ) created by Sugiyama et al .   ( 2021 ) . They pre - trained the encoder - decoder   Transformer using 2.1B dialogues crawled from   Twitter in Japanese then Ô¨Åne - tuned the model with   the corpora including the Japanese version of Per-   sonaChat ( Zhang et al . , 2018 ) and EmpatheticDi-   alogues ( Rashkin et al . , 2018 ) . We created the   chatbot for Kurisu by further Ô¨Åne - tuning the model   with the collected QA pairs and extended dialogue464data . To evaluate the Ô¨Åne - tuned model , 20 work-   ers interacted with the chatbot by performing 15-   turn dialogues ( a turn corresponds to a user utter-   ance and chatbot utterance : hereafter , system ut-   terance ) three times . The subjective evaluation re-   sults on naturalness , characterness , and informa-   tiveness were 3.87 , 3.90 , and 3.58 , respectively   ( on a 5 - point Likert scale ) .   2.2 Large - scale user study   The chatbot described in the previous section was   made public on the Internet , and the dialogues be-   tween a large number of users , mostly the fans of   Kurisu , and the chatbot were collected . The chat-   bot was accessible using the direct message func-   tion of Twitter for three days . After users agreed   to the terms of usage , they could interact with the   chatbot . Users could stop the dialogue at any time   or interact with it as much as they wanted during   the period . At the end of the study , a user ques-   tionnaire ( on a 5 - point Likert scale ) was sent out   by direct message to the users to evaluate user sat-   isfaction . Note that the users were not paid for   their participation .   We were able to collect the logs of 1,170 user   interactions with the chatbot . The total number of   user utterances was 80,608 , and the average num-   ber of utterances for each user was 68.9 , indicat-   ing that the users used the chatbot for a relatively   long time . The average user - satisfaction rating   was 4.59 ( 63.6 % response rate ) , which we believe   is very high .   3 Error analysis   To extract system utterances causing person-   speciÔ¨Åc errors from the data , we collected four   types of information : dialogue breakdown labels ,   comments on the reasons for the breakdown ( Hi-   gashinaka et al . , 2015 ) , Ô¨Çags indicating whether   the comments were about the person in question ,   and error types in chat - oriented dialogue systems   ( Higashinaka et al . , 2021 ) . We Ô¨Årst collected   the dialogue breakdown labels and comments on   their reasons . If the comments contained key-   words related to Kurisu , we considered the sys-   tem utterances with those comments as indicat-   ing person - speciÔ¨Åc errors and extracted the com-   ments for analysis . We also annotated system ut-   terances with the error types in chat - oriented di-   alogue systems for investigating the correspon-   dence between the existing taxonomy of errors and   person - speciÔ¨Åc errors .   3.1 Dialogue breakdown annotation   We sampled and annotated 13 % (= 10,611/80,608 )   of the data due to the limited annotation resources .   The sampled system utterances were annotated   with the three types of breakdown labels ( Hi-   gashinaka et al . , 2015 ) of ‚Äú not a breakdown ( NB ) ‚Äù ,   ‚Äú possible breakdown ( PB ) ‚Äù , and ‚Äú breakdown ( B ) ‚Äù .   Five crowdworkers who had sufÔ¨Åcient knowledge   of Kurisu annotated these labels to the system   utterances independently . The workers were in-   structed to provide comments to describe the er-   rors that led to the breakdowns .   Table 1shows the annotation results of the di-   alogue breakdown labels . The percentage of   NBs was 89 % , indicating that the dialogue was   successful in the majority of cases . The inter-   annotator agreement rate was 0.23 for the Fleiss ‚Äô   kappa when NB / PB / B were treated separately and   0.30 when PB / B were merged , which was at the   same level as in the study by Higashinaka et al .   ( 2015 ) , which we consider reasonable due to the   subjective nature of the task . In the following anal-   ysis , the system utterances in which more than half   the workers marked PB or B were considered for   error analysis . The number of such utterances was   817 ( 7.7 % ) . The error comments ( 2,846 ) given to   these utterances were also retrieved for analysis .   3.2 Annotation of error types and   person - related Ô¨Çags   Two types of information were assigned to the er-   roneous system utterances and error comments .   The Ô¨Årst is the error types in chat - oriented dia-   logue systems ( Higashinaka et al . , 2021 ) . This   labeling was done by an in - house expert worker .   The second is a Ô¨Çag indicating whether person-   related keywords are present in the error com-   ment . By referring to the resources of Kurisu ,   we manually created a lexicon of that character.465   The size of the lexicon was 53 words . If a word   in the lexicon was included in each comment , it   was Ô¨Çagged as that related to Kurisu . For ex-   ample , the lexicon includes Kurisu , Mayuri ( the   name of Kurisu ‚Äôs friend ) , @channel ( the website   that Kurisu is familiar with ) , and Akihabara ( the   place where Kurisu resides ) . o   Table 2shows the annotation results of the er-   ror types and number of person - speciÔ¨Åc errors for   each type . In the total number of dialogue break-   downs ( 817 ) , 168 ( 20.1 % ) were caused by person-   speciÔ¨Åc errors , and more than half ( 53.1 % ) of the   utterances in ( I4 ) Wrong information were person-   speciÔ¨Åc errors .   4 Person - speciÔ¨Åc error analysis   We automatically clustered the error comments re-   lated to the target person and investigated the char-   acteristics the person - speciÔ¨Åc errors .   4.1 Clustering person - speciÔ¨Åc errors   We used hierarchical clustering by using bag - of-   words as the clustering method . The 168 com-   ments annotated to the 168 person - speciÔ¨Åc er-   rors shown in Table 2were used for clustering .   A Japanese morphological analyzer JTAG ( Fuchi   and Takagi , 1998 ) was used . Low - frequency   words ( those appearing less than three times in   the 168 comments ) were excluded . The vector   of each comment was normalized for the clus-   tering . Single - linkage clustering , Ward ‚Äôs method ,   and squared Euclidean distance were speciÔ¨Åed as   clustering parameters . The number of clusters was   set so that the size of each cluster would be at least   10 % of the total comments .   Figure 1shows the clustering results of the   comments . The Ô¨Ågure shows four clusters with the   comments that were nearest to the centroid of each   cluster , representing salient comments . In Clus-   ter 1 , the chatbot was not able to properly discuss   topics related to the environment around Kurisu .   In Cluster 2 , the chatbot suddenly called the user   by a different name ( e.g. , the name of the Kurisu ‚Äôs   friend ) , or gave a name that was irrelevant to the   current topic . In Cluster 3 , the chatbot provided   incorrect information about Kurisu . In Cluster 4 ,   the chatbot mistakenly called Kurisu by a differ-466ent name . From such an observation , we can see   that Clusters1 and 3 are about errors related to   the knowledge of the target person , and Clusters   2 and 4 are about errors regarding the misrecogni-   tion of relationships . In addition , Clusters 3 and 4   are about errors regarding the target person , while   Clusters 1 and 2 are errors about the target per-   son ‚Äôs environment .   On the basis of the above interpretation , we   conclude that person - speciÔ¨Åc errors can be di-   vided into two types : errors in attributes ( regard-   ing people and things ) and errors in relations be-   tween them , each of which can be further divided   into two levels : self(the target person ) and other   ( surrounding environment of the person such as   friends ) . The matrix of person - speciÔ¨Åc errors is   given in Table 3 : ( P1 ) Self - recognition error cor-   responds to Cluster 3 , ( P2 ) Self - relation error cor-   responds to Cluster 4 , ( P3 ) Other - recognition error   corresponds to Cluster 1 , and ( P4 ) Other - relation   error corresponds to Cluster 2 .   4.2 DeÔ¨Ånition of person - speciÔ¨Åc errors   Each error type is described in detail using actual   dialogue examples taken from user interactions   with the chatbot . The dialogues were originally   in Japanese and translated by the authors . User   and the chatbot is denoted as ‚Äò U ‚Äô and the chatbot   is denoted as ‚ÄòS ‚Äô in the following dialogues .   ( P1 ): Self - recognition error : The error is related   to the target person ‚Äôs , i.e. , Kurisu ‚Äôs , information   ( proÔ¨Åle , likes / dislikes , experience , etc . ) .   ( 1)U : I like you Kurisu .   S : I am neither AI nor AI Kurisu .   ( 2)U : You are so blunt to me Christina !   S : Hello , I ‚Äôm AI Christina .   Underlines indicate phrases that are regarded as   the cause of the dialogue breakdown by the au-   thors . In dialogue ( 1 ) , since Kurisu is an AI char-   acter , the proÔ¨Åle of Kurisu is wrong . In dialogue   ( 2 ) , the chatbot should not have used the nickname   ‚Äú Christina , ‚Äù because Kurisu dislikes it . Her likes   and dislikes are wrong .   ( P2 ): Self - relation error : This error means that   the chatbot mistook the target person for another   person ( acquaintance , user , or famous person ) or   thing .   ( 3)S : My name is Kurisu Makise . Today ‚Äôs   experiment is almost over . U : I want to talk a lot more .   S : Are you really Kurisu Makise ?   ( 4)U : Who is Mayuri ? ( Kurisu ‚Äôs friend ) .   S : She is also known as Kurisu Makise .   In dialogue ( 3 ) , since the chatbot called the user   Kurisu Makise , it mistook itself for another person   ( i.e. , the user ) . In dialogue ( 4 ) , the user was asking   about Mayuri , but the chatbot talked about itself ,   mistaking itself for someone else ( i.e. , Mayuri ) .   ( P3 ): Other - recognition error : This error indi-   cates the inability to respond to topics of the peo-   ple surrounding the target person ( acquaintances ,   user , or famous people ) or things that the target   person is knowledgeable about ( favorite things ,   etc . ) .   ( 5)S : Even boys can be cute , you know .   U : Oh , oh . For example ?   S : Mayuri , Mayuri , ( Kurisu ‚Äôs frined ) , etc .   ( 6)U : ‚Äú nurupo ‚Äù ( Internet slang Kurisu knows )   S : My name is Amadeus Kurisu .   In dialogue ( 5 ) , since Mayuri is not a boy , the   knowledge about the person around Kurisu was   incorrect . In dialogue ( 6 ) , since ‚Äú nurupo ‚Äù is an   Internet slang that Kurisu knows well , the chatbot   has to reply with the traditional response ( ‚Äú ga ‚Äù ) ,   but it was not able to do that .   ( P4 ): Other - relation error : This error is that the   chatbot mistook a person or thing other than the   target person ( an acquaintance , user , or famous   person ) for another person or thing .   ( 7)U : I love Amadeus Kurisu .   S : Yes . I like you too . Mayuri .   ( 8)U : What ‚Äôs Suzuha ( Kurisu ‚Äôs friend ) doing ?   S : Well , Mayuri ‚Äôs voice will save the world .   In dialogue ( 7 ) , the chatbot called the user   ‚Äú Mayuri ‚Äù , and the system mistook someone other   than Kurisu for someone else ( in this case , the   target person ‚Äôs friend ) . In dialogue ( 8) , the chat-   bot responded to a question about Suzuha with   Mayuri . , i.e. , the person ( Suzuha ) was mistaken   for another person ( Mayuri ) .   4.3 Evaluation of person - speciÔ¨Åc errors   To evaluate the validity of the types of person-   speciÔ¨Åc errors , we investigated inter - annotator   agreement in the annotation of the four types   ( P1 ‚Äì P4 ) . We applied the methods described in467   Section 3to the data not used in the above anal-   ysis , resulting in 50 new person - speciÔ¨Åc error in-   stances obtained from sampled 3,200 system ut-   terances . When annotating the types of person-   speciÔ¨Åc errors , only an utterance labeled as a di-   alogue breakdown and its preceding three utter-   ances were given to annotators as a context . Two   in - house expert annotators conducted the annota-   tion . The deÔ¨Ånition of person - speciÔ¨Åc errors de-   scribed in Section 4.2was given to the workers as   instruction . As a result , the inter - annotator agree-   ment was 0.46 in Cohen ‚Äôs kappa , which indicates   a moderate agreement and suggests the validity of   the types of person - speciÔ¨Åc errors .   4.4 Correspondence with existing error types   Table 4shows the correspondence between   person - speciÔ¨Åc errors and the conventional error   taxonomy . The table was created by merging the   results shown in Table 2and Figure 1 . Errors on   the self - level appeared most frequently , account-   ing for about half ( 42.3 % + 15.5 % = 57.8 % ) of   the person - speciÔ¨Åc errors . The fact that there   were many errors on the others level suggests that   the person ‚Äôs environment , such as friends , was   also frequently talked about . Each person - speciÔ¨Åc   error corresponded to multiple error types in the   conventional taxonomy ; thus , we were able to   discover different aspects of errors .   The ( P1 ) Self - recognition error was particu-   larly common in ( I10 ) Unclear intention , that is ,   meaning uttering an unknown intention , such as   suddenly changing what the person calls oneself   ( e.g. from ‚Äú I ‚Äù to a nickname ) . In addition , ( P1 )   Self - recognition error was a common error in ( I4 )   Wrong information , i.e. , uttering incorrect infor-   mation about oneself . The ( P2 ) Self - relation er-   ror was also common , especially in ( I4 ) Wrong   information , i.e. , an error of confusing oneself   with a user or oneself with a friend . The ( P2 )   Self - relation error was the next most common in   ( I10 ) Unclear intention , such as suddenly men - tioning a close friend in a conversation about one-   self . In ( P3 ) Other - recognition error and ( P4 )   Other - relation error , there were system utterances   of not being able to respond appropriately to top-   ics about people / things the target person is fami-   lar with , e.g. , incorrect information about them or   confusion between users and friends .   From the results of investigating person - speciÔ¨Åc   errors , it became clear that the most common er-   rors were regarding information about the target   person then its surrounding environment . Among   the error types in the conventional taxonomy , the   ( I4 ) Wrong information appeared frequently , con-   Ô¨Årming the importance of studies on persona-   consistent dialogue . In addition to information   about the target person , knowledge about the tar-   get person ‚Äôs environment is also considered im-   portant . Current dialogue systems often do not   explicitly model the relationships between peo-   ple and things , therefore a model that takes into   account the knowledge graphs of relationships   would be effective ( Ghazvininejad et al . , 2018 ; Di-   nan et al . , 2019 ) .   5 Summary and future work   We analyzed dialogue data between a chatbot that   imitates a speciÔ¨Åc person and users to identify   person - speciÔ¨Åc errors that have not been consid-   ered thoroughly before . We found that person-   speciÔ¨Åc errors can be divided into four types :   self - recognition error , self - relation error , other-   recognition error , and other - relation error , which   are useful as a guideline for constructing chatbots   that are based on speciÔ¨Åc people .   Future work includes the application of unlike-   lihood training ( Li et al . , 2020 ) or a classiÔ¨Åer to   estimate the identity of a speaker ( Shuster et al . ,   2021 ) for suppressing person - speciÔ¨Åc errors . We   focused on one speciÔ¨Åc person in this paper ; thus ,   it will also be important to consider the generality   of the results.468References469   David Samuel , Jeremy Barnes , Robin Kurtz , Stephan Oepen ,   Lilja √òvrelidand Erik VelldalUniversity of Oslo , Language Technology GroupUniversity of the Basque Country UPV / EHU , HiTZ Center ‚Äì IxaNational Library of Sweden , KBLab   { davisamu , oe , liljao , erikve}@ifi.uio.no   jeremy.barnes@ehu.eus , robin.kurtz@kb.se   Abstract   1 Introduction   The task of structured sentiment analysis ( SSA ) is   aimed at locating all opinion tuples within a sen-   tence , where a single opinion contains a ) a polar   expression , b ) an optional holder , c ) an optional   sentiment target , and d ) a positive , negative or neu-   tral polarity , see Figure 1 . While there have been   sentiment corpora annotated with this information   for decades ( Wiebe et al . , 2005 ; Toprak et al . , 2010 ) ,   there have so far been few attempts at modeling   the full representation , rather focusing on various   subcomponents , such as the polar expressions and   targets without explicitly expressing their relations   ( Peng et al . , 2019 ; Xu et al . , 2020 ) or the polarity   ( Yang and Cardie , 2013 ; Katiyar and Cardie , 2016 ) .   Dependency parsing approaches have recently   shown promising results for SSA ( Barnes et al . ,   2021 ; Peng et al . , 2021 ) . Here we present a   novel sentiment parser which , unlike previous at-   tempts , predicts sentiment graphs directly from   text without reliance on heuristic lossy conversions   to intermediate dependency representations . The   model takes inspiration from successful work in   meaning representation parsing , and in particular   the permutation - invariant graph - based parser of   Samuel and Straka ( 2020 ) called PERIN .   Experimenting with several different graph en-   codings , we evaluate our approach on Ô¨Åve datasets   from four different languages , and Ô¨Ånd that it com-   pares favorably to dependency - based models across   all datasets ; most signiÔ¨Åcantly on the more struc-   turally complex ones ‚Äì NoReC andMPQA .   2 Related work   Proposing a dependency parsing approach to the   full task of SSA , Barnes et al . ( 2021 ) show that it   leads to strong improvements over state - of - the - art   baselines . Peng et al . ( 2021 ) propose a sparse fuzzy   attention mechanism to deal with the sparseness of   dependency arcs in the models from Barnes et al .   ( 2021 ) and show further improvements . However ,   in order to apply the parsing algorithm of Dozat   and Manning ( 2018 ) , both of these approaches have   to rely on a lossy conversion to bi - lexical depen-   dencies with ad - hoc internal head choices for the   nodes of the abstract sentiment graph , see Section   3 for a discussion of these issues .   More generally , decoding structured graph infor-   mation from text has sparked a lot of interest in   recent years , especially for parsing meaning repre-   sentation graphs ( Oepen et al . , 2020 ) . There has   been tremendous progress in developing complex   transition - based and graph - based parsers ( Hersh-   covich et al . , 2017 ; McDonald and Pereira , 2006 ;   Dozat and Manning , 2018 ) . In this paper , we adopt   PERIN ( Samuel and Straka , 2020 ) , a state - of - the-   art graph - based parser capable of modeling a su-   perset of graph features needed for our task.470   3 Issues with dependency encoding   As mentioned above , previous dependency parsing   approaches to SSA have relied on a lossy bi - lexical   conversion . This is caused by an inherent ambigu-   ity in the dependency encoding of two nested text   spans with the same head ( deÔ¨Åned as either the Ô¨Årst   or the last token in Barnes et al . ( 2021 ) ) .   To be concrete , we can use the running exam-   ple‚ÄúNowadays I actually enjoy the bad acting , ‚Äù   which has two opinions with nested targets ; ‚Äú the   bad acting , ‚Äù , which is associated with a positive   polarity indicated by the polar expression ‚Äú enjoy ‚Äù ,   and‚Äúacting , ‚Äù , with a negative polarity expressed by   ‚Äú bad ‚Äù . As shown in the dependency representation   in Figure 2 , both expression ‚Äì target edges correctly   lead to the word ‚Äú acting ‚Äù but it is impossible to   disambiguate the preÔ¨Åx of both targets in the bi-   lexical encoding , i.e. , to determine that the tokens   ‚Äú the ‚Äù and‚Äúbad ‚Äù are part of the target only for the   positive opinion . For that , we need a more abstract   graph encoding , such as the ones suggested in this   paper .   4 PERIN model   PERIN is a general permutation - invariant text - to-   graph parser . The output of the parser can be a   directed graph with labeled nodes connected by la-   beled edges where each node is anchored to a span   of text ( possibly empty or discontinuous ) . We pro-   pose three graph representations for SSA that meet   these constrains and thus can be easily modeled by   this parser .   We use only a subset of the full PERIN ‚Äôs func-   tionality for our SSA version ‚Äì it does not need to   use the ‚Äú relative label rules ‚Äù and model node prop-   erties or edge attributes . Please consult the origi-   nal work for more technical details about PERIN   ( Samuel and Straka , 2020 ) .   4.1 Architecture   PERIN processes the input text end - to - end in four   steps , illustrated in Figure 3 : 1 ) To encode the in-   put , PERIN uses contextualized embeddings from   XLM - R ( base size ; Conneau et al . , 2020 ) and   combines them with learned character - level em-   beddings;2 ) each token is mapped onto latent   queries by a linear transformation ; 3 ) a stack of   Transformer ( encoder ) layers without positional   embedding ( Vaswani et al . , 2017 ) optionally mod-   els the inter - query dependencies ; and 4 ) classiÔ¨Å-   cation heads select and label queries onto nodes ,   establish anchoring from nodes to tokens , and pre-   dict the node - to - node edges .   4.2 Permutation - invariant query - to - node   matching   Traditional graph - based parsers are trained as au-   toregressive sequence - to - sequence models . PERIN   does not assume any prior ordering of the graph   nodes . Instead , it processes all queries in parallel   and then dynamically maps them to gold nodes.471Based on the predicted probabilities of labels   and anchors , we create a weighted bipartite graph   between all queries and nodes . The goal is to Ô¨Ånd   the most probable matching , which can be done   efÔ¨Åciently in polynomial time by using the Hungar-   ian algorithm . Finally , every node is assigned to a   query and we can backpropagate through standard   cross - entropy losses to update the model weights .   4.3 Graph encodings   PERIN deÔ¨Ånes an overall framework for general   graph parsing , it can cater to speciÔ¨Åc graph encod-   ings by changing the subset of its classiÔ¨Åcation   heads . In parsing the abstract sentiment structures ,   there are several possible lossless graph encodings   depending on the positioning of the polarity in-   formation and the sentiment node type . We ex-   periment with three variations ( Figure 4 ) and later   show that while the graph encoding improves per-   formance , this improvement largely depends on the   type of encoding used .   1.Node - centric encoding , with labeled nodes   and directed unlabeled arcs . Each node cor-   responds to a target , holder or sentiment ex-   pression ; edges form their relationships . The   parser uses a multi - class node head , an anchor   head and a binary edge classiÔ¨Åcation head .   2.Labeled - edge encoding , with deduplicated   unlabeled nodes and labeled arcs . Each node   corresponds to a unique text span from some   sentiment graph , while edge labels denote   their relationships and functions . The model   has a binary node classiÔ¨Åer , an anchor classi-   Ô¨Åer and a binary and multi - class edge head .   3.Opinion - tuple encoding , which represents   the structured sentiment information as a se-   quence of opinion four - tuples . This encoding   is the most restrictive , having the lowest de-   grees of freedom . The parser utilizes a multi-   class node head and three anchor classiÔ¨Åers , it   does not need an edge classiÔ¨Åer .   5 Data   Following Barnes et al . ( 2021 ) we employ Ô¨Åve   structured sentiment datasets in four languages ,   the statistics of which are shown in Table 1 . The   largest dataset is the NoReCdataset ( √òvrelid   et al . , 2020 ) , a multi - domain dataset of professional   reviews in Norwegian . EUandCA(Barnes et al . ,   2018 ) contain hotel reviews in Basque and Catalan ,   respectively . MPQA ( Wiebe et al . , 2005 ) annotates   news wire text in English . Finally , DSU ( Toprak   et al . , 2010 ) annotates English reviews of online   universities . We use the SemEval 2022 releases of   MPQA andDSU ( Barnes et al . , 2022 ) .   5.1 Nested dependencies   Returning to the issue of dependency encoding   for nested elements discussed in Section 3 , Table   2 shows that the amount of nesting in the SSA   datasets is not negligible , further motivating our   abstract graph encodings for this task .   Table 3a further shows the amount of depen-   dency edges lost because of overlap . Finally , Table   3b shows the S Fscore when converting the gold   sentiment graphs to bi - lexical dependency graphs   and back ‚Äì an inherent upper bound for any depen-   dency parser.472   6 Experiments   6.1 Evaluation   Following Barnes et al . ( 2021 ) , we evaluate our   models using Sentiment Graph F(SF ) . This met-   ric considers that each sentiment graph is a tuple   of ( holder , target , expression , polarity ) . A true pos-   itive is deÔ¨Åned as an exact match at graph - level ,   weighting the overlap in predicted and gold spans   for each element , averaged across all three spans .   For precision it weights the number of correctly   predicted tokens divided by the total number of   predicted tokens ( for recall , it divides instead by   the number of gold tokens ) . S Fallows for empty   holders and targets .   In order to further analyze the models , we also   include token - level Ffor extraction of Holders ,   Targets , and Polar Expressions , as well as Non-   polar Sentiment Graph F(NSF ) .   6.2 Models   We compare our models to the head-Ô¨Ånal depen-   dency graph parsers from Barnes et al . ( 2021 ) as   well as the second - order Sparse Fuzzy Attention   parser of Peng et al . ( 2021 ) . For all models , we   perform 5 runs with 5 different random seeds and   report the mean and standard deviation . Results   on development splits are provided in Appendix C ,   training details are in Appendix D.6.3 Results   Table 4 shows the main results . Our models out-   perform both dependency graph models on S F ,   although the results are mixed for span extraction .   The opinion - tuple encoding gives the best perfor-   mance on S F(an average of 6.2 percentage points   ( pp . ) better than Peng et al . ( 2021 ) ) , followed by   the labeled edge encoding ( 3.0 ) and Ô¨Ånally the   node - centric encoding ( 2.1 ) .   For extracting spans , the opinion tuple encoding   also achieves the best results on NoReC , either   labeled - edge or node centric on CAandMPQA ,   while Peng et al . ( 2021 ) is best on EUandDSU .   This suggests that the main beneÔ¨Åt of PERIN is at   the structural level , rather than local extraction .   7 Analysis   There are a number of architectural differences   between the dependency parsing approaches com-   pared above . In this section , we aim to isolate   the effect of predicting intermediate dependency   graphs vs. directly predicting sentiment graphs   by creating more comparable dependencyand   PERIN models . We adapt the dependency model   from Barnes et al . ( 2021 ) by removing the to-   ken , lemma , and POS embeddings and replacing   mBERT ( Devlin et al . , 2019 ) with XLM - R ( Con-   neau et al . , 2020 ) . The ‚Äò XLM - R dependency ‚Äô   model thus has character LSTM embeddings and   token - level XLM - R features . Since these are   not updated during training , for the opinion - tuple   ‚Äò Frozen PERIN ‚Äô model , we Ô¨Åx the XLM - R weights   to make it comparable .   As shown in Table 5 , predicting the sentiment   graph directly leads to an average gain of 3.7 pp . on   the Sentiment Graph Fmetric . For extracting the473   spans of holder , target , and polar expressions , the   beneÔ¨Åt is less clear . Here , the PERIN model only   outperforms the XLM - R dependency model 5 of   15 times , which seems to conÔ¨Årm that its beneÔ¨Åt is   at the graph level . This is further supported by the   fact that the highest gains are found on the datasets   with the most nested sentiment expressions anddependency arcs lost due to overlap , which are   difÔ¨Åcult to encode in bi - lexical graphs .   8 Conclusion   Previous work cast the task of structured sentiment   analysis ( SSA ) as dependency parsing , converting   the sentiment graphs into lossy dependency graphs .   In contrast , we here present a novel sentiment   parser which predicts sentiment graphs directly   from text without reliance on lossy dependency rep-   resentations . We adapted a state - of - the - art meaning   representation parser and proposed three candidate   graph encodings of the sentiment structures . Our   experimental results suggest that our approach has   clear performance beneÔ¨Åts , advancing the state of   the art on four out of Ô¨Åve standard SSA bench-   marks . SpeciÔ¨Åcally , the most direct opinion - tuple   encoding provides the highest performance gains .   More detailed analysis of the results shows that the   beneÔ¨Åts stem from better extraction of global struc-   tures , rather than local span prediction . Finally , we   believe that various structured prediction problems   in NLP can similarly be approached in a uniform   manner as parsing into directed graphs.474References475   A Changes to datasets   We found out that the ofÔ¨Åcial data pub-   lished at was slightly changed from   the data used in previous related work . SpeciÔ¨Åcally   theMPQA andDSU datasets had removed a num-   ber of errors resulting from the annotation and from   the conversion scripts used to create the sentiment   graph representations . We re - run the experiments   for the comparable baseline model and show the   performance differences in Table 7 .   B Bootstrap SigniÔ¨Åcance Testing   In order to see whether the performance differences   for the experiments are signiÔ¨Åcant , we do boot-   strap signiÔ¨Åcance testing Berg - Kirkpatrick et al .   ( 2012 ) , combining two variations . First , we resam-   ple the test sets with replacement from all 5 runs   together , b= 1 000 000 times , setting the threshold   atp= 0:05 . Additionally , we test each pair outof the 55combinations for all runs , resampling   the test set with replacement b= 100 000 times ,   setting the threshold again at p= 0:5 . When one   system is signiÔ¨Åcantly better in 15 out of the 25   comparisons , and additionally signiÔ¨Åcantly better   in the Ô¨Årst joint test , we Ô¨Ånally mark it as signiÔ¨Å-   cantly better .   C Results on development data   To make any future comparison of our approach   easier , we show the development scores of all re-   ported models in Table 6 .   D Training details   Generally , we follow the training regime described   in the original PERIN paper ( Samuel and Straka ,   2020 ) . The trainable parameters are updated with   the AdamW optimizer ( Loshchilov and Hutter ,   2019 ) , and their learning rate is linearly warmed-   up for the Ô¨Årst 10 % of the training to improve sta-   bility , and then decayed with a cosine schedule .   The XLM - R parameters are updated with a lower   learning rate and higher weight decay to improve   generalization . Similarly to PERIN , we freeze the   embedding parameters for increased efÔ¨Åciency and   regularization . Following the Ô¨Ånding by Zhang   et al . ( 2021 ) , we use small learning rates and Ô¨Åne-   tune for a rather long time to increase the training   stability . Unlike the authors of PERIN , we did not   Ô¨Ånd any beneÔ¨Åts from a dynamic scaling of loss   weights ( Chen et al . , 2018 ) , so we simply set all   loss weights to constant 1:0 .   We trained our models on a single Nvidia P100   with 16 GB memory , the runtimes are given in Ta-   ble 6 . We made Ô¨Åve runs from different seeds   for each reported value to better estimate the ex-   pected error . The hyperparameter conÔ¨Ågurations   for all runs follow , please consult the released   code for more details and context : .   General hyperparameters476   NoReC node - centric hyperparameters   NoReC labeled - edge hyperparameters   NoReC opinion - tuple hyperparameters   NoReC frozen opinion - tuple hyperparame-   ters477EU node - centric hyperparameters   EU labeled - edge hyperparameters   EU opinion - tuple hyperparameters   EU frozen opinion - tuple hyperparameters   CA node - centric hyperparameters   CA labeled - edge hyperparameters   CA opinion - tuple hyperparameters   CA frozen opinion - tuple hyperparameters   MPQA node - centric hyperparameters   MPQA labeled - edge hyperparameters   MPQA opinion - tuple hyperparametersMPQA frozen opinion - tuple hyperparame-   ters   DSU node - centric hyperparameters   DSU labeled - edge hyperparameters   DSU opinion - tuple hyperparameters   DSU frozen opinion - tuple hyperparameters478   Chan - Jan Hsu , Hung - yi Lee , Yu TsaoNational Taiwan University , TaiwanAcademia Sinica , Taiwan   { r09946011 , hungyilee}@ntu.edu.tw ,   yu.tsao@citi.sinica.edu.tw   Abstract   1 Introduction   Transformer - based models are extensively used   in natural language understanding ( NLU ) tasks ,   and some prominent pretraining strategies include   BERT ( Devlin et al . , 2019 ) , RoBERTa ( Liu et al . ,   2019 ) , ALBERT ( Lan et al . , 2020 ) , and ELEC-   TRA ( Clark et al . , 2020 ) . Despite their differences   in curating the learning objectives , they all utilize   text - based datasets only . In the real world , however ,   humans can benefit from the visual modality when   acquiring knowledge from language ; an obvious   example is learning visually grounded words , such   as colors and shapes .   Some studies have succeeded with visually   grounded information used in NLU . ViCo ( Gupta   et al . , 2019 ) learned visual co - occurrences in text   and reported superior performance to GloVe in   word analogy problems . Zhang et al . ( 2020 ) and   Huang et al . ( 2020 ) used images to boost transla-   tion performance in supervised and unsupervised   Bird : " A winged [ MASK ]       soars in the sky . "   " The [ MASK ]   formed a         beautiful arc in the sky " :   What is   a bir d ?   CLIPBER T   settings . Tan and Bansal ( 2020 ) reported improve-   ments over BERT on NLU by proposing the con-   cept of vokenization .   Another branch of research focuses on solving   multimodal downstream tasks such as visual ques-   tion answering and image retrieval . Li et al . ( 2019 ) ;   Lu et al . ( 2019 ) ; Su et al . ( 2020 ) ; Li et al . ( 2020 )   trained visual - text transformers , while LXMERT   ( Tan and Bansal , 2019 ) used different encoders for   text and image and a cross - modal encoder . Tan   and Bansal ( 2020 ) tested these models with general   language understanding evaluation ( GLUE Wang   et al . ( 2018 ) ) and found that the performance does   not exceed using BERT ( Appendix A ) , drawing   the conclusion that vision - and - language pretrain-   ing on visually - grounded language dataset failed to   distill useful information for general NLU . CLIP   ( Radford et al . , 2021 ) utilizes contrastive loss to   reach SOTA on zero - shot image classification in a   retrieval fashion .   In this work , we establish the link between   pretrained multimodal transformers and visually-   grounded language learning . We devise a way   to distill visual information from components of   a pretrained multimodal transformer ( CLIP text-   transfomer , abbreviated as CLIP - T ) to pretrained479language transformers ( BERT / ELECTRA ) , to in-   corporate versatile perception of words into the   model ( Figure 1 ) . The usage of a visually   grounded text - transformer as a teacher allows us to   implement straightforward and non - fuzzy adapting   tasks for distillation . We show that it is mathemati-   cally logical that the CLIP - T output approximates   visual features ( Sec . 2.2 ) , and also the linguistic   competence of CLIP - T is low ( Sec . 3 ) , to prove   that the distilled information is predominantly vi-   sual and thus non - trivial to the pretrained - language   transformer despite having textual inputs .   Methodologically , we use the cross - modal en-   coder structure inspired by Tan and Bansal ( 2019 ) ,   to concatenate the two models and further adapt   the ensemble for some extra steps ( a lot fewer   than the original pretraining steps ) . While adapt-   ing pretrained - BERT , we favor a document - level   corpus ( wiki103 ) over a vision - language corpus   ( MSCOCO ) due to claims from Devlin et al .   ( 2019)and results from Tan and Bansal ( 2020 )   ( Appendix A ) . The adapting tasks are joint masked   language modeling ( MLM ) , same sentence predic-   tion , and CLIP token classification tasks , which   are resemblant of BERT pretraining tasks to cater   to the language - heavy characteristics of NLU . We   do ablation studies to show that each of the task   provides improvement ( Section 5 ) .   During finetuning , we finetune XDBERT ( cross-   modal distilled BERT ) , which is the language en-   coder after adaptation . We evaluate the linguistic   capabilities of the model by finetuning on GLUE ,   situations with adversarial generations ( SWAG   ( Zellers et al . , 2018 ) ) benchmarks , and readabil-   ity benchmarks . The resulting XDBERT outper-   forms pretrained BERT , proving that our adaptation   strategy distills useful visual knowledge into BERT   ( right of Figure 2 ) . We provide analysis to show   that the improvements are visually grounded .   We summarize our contribution as follow :   ‚Ä¢We explore distilling visual information from   a pretrained multimodal transformer to a pre-   trained language transformer and improved   NLU performance .   ‚Ä¢Our adapting method is efficient and exten-   sible to different combinations of pretrained-   language encoders ( BERT / ELECTRA).2 Proposed Method   The training process consists of three phases : pre-   training , adaptation , and finetuning ( Figure 2 ) . Our   proposed method focuses on the adaptation phase   with pretrained models , so pretraining is not a   part of our experiment , but we explain all three   phases for completeness . The adaptation phase in-   corporates the cross - modal transformer structure to   jointly learn from CLIP - T and BERT outputs .   2.1 Model Architecture   The cross - modal transformer ( middle of Figure 2 )   consists of a cross - modal encoder , CLIP - T and   BERT . CLIP - T has the same module connections   as BERT with only parameter differences ( specifi-   cations in Appendix B ) . The cross - modal encoder   consists of repeating cross - modal encoder layers ,   which is an extension to single - modality encoder   layers ( layers of BERT / CLIP - T ) in Figure 3 . The   added cross - attention module follows the attention   formula ( Vaswani et al . , 2017 ):   Attention output = softmax   Q‚àóK/‚àö   D   V   ( 1 )   for queries ( Q ) , keys ( K ) and values ( V ) of dimen-   sion D , however , Qis generated from a modality   other than KandV. We choose the number of   cross - modal encoder layers to be 2 .   2.2 Pretraining   BERT is trained using the next sentence prediction   and masked language modeling . CLIP is an image-   text matching system with two components , a text   encoder ( CLIP - T ) , and an image encoder ( CLIP-   ViT ) , which learn to encode paired inputs to closer   output embeddings via contrastive loss . The trained   representation has the following properties :   cos(H , V ) > > cos ( H , V)(iÃ∏=j ) ( 2 )   cos(H , V ) > > cos ( H , V)(iÃ∏=j ) ( 3 )   where His the CLIP text encoder output of X ,   andVis the CLIP image encoder output of Y. The   text - image input ( X , Y ) is paired , and every ( X ,   Y)(jÃ∏=k)is a non - pair . Since HandVare   normalized and have a length of 1 , Hcan be used   to approximate V. The similarity of HandVis   also shown in multi - modal arithmetic propreties   discovered in Tewel et al . ( 2021 ) Therefore , we   use the CLIP text encoder output to approximate   CLIP image encoder output for a straightforward   adaptation process.480   2.3 Adaptation   We define three adapting tasks that can be learned   in a self - supervised manner , which is visualized in   Figure 2 . In these tasks , BERT and CLIP - T takes   sentences A and B respectively as input , and losses   are calculated from both BERT output and CLIP - T   output . Our adapting tasks closely follow BERT   text pretraining strategies to retain linguistic com-   petence . Unlike pretraining , the adaptation is com-   putationally inexpensive , as we found that training   1 epoch on wiki103 was already effective . Further   training details can be found in Appendix C.   2.3.1 Joint Masked Language Modeling   ( MLM )   The MLM objective teaches the model to recon-   struct masked tokens . The masked ratio and   masked token replacement probabilities follow De-   vlin et al . ( 2019 ) . Since there is no equivalent of a   [ MASK ] token in CLIP , we leave the sentence as   is .   2.3.2 Same sentence prediction ( MATCH )   The Image - Text Matching ( ITM ) objective is   widely used in multimodal learning ( Tan and   Bansal , 2020 ; Radford et al . , 2021 ) . We modify   this objective to same sentence prediction as both   streams of our model takes text as input . When   choosing the input sentences for BERT and CLIP-   T , we make the inputs nonidentical 50 % of the   time . A binary classifier over [ CLS ] differentiates   between the two cases . This motivates the [ CLS ]   output to encode sentence related information , and   trains the cross - attention weights .   Cross - modality encoder layer   Single - modality encoder layer   Cross-   attentionSelf-   attentionFeed-   ForwardQ , K , V K , V   Q   2.3.3 CLIP Token Classification   This is the MLM objective done on the CLIP - T   side of the full model , omitting the masking part   because CLIP has no mask token . Same as MLM ,   15 % of the tokens are randomly selected for recon-   struction . We address concerns on trivial solutions   learned by the model in Section 5 and 9 in the   appendix .   2.4 Finetuning   Finetuning follows the methods described in De-   vlin et al . ( 2019 ) , and is applied to the language   encoder only ( XDBERT ) , therefore the number of   parameters are kept equal to pretrained - BERT .   3 Experimental Results   We evaluated our model on three NLU benchmarks ,   namely GLUE , SWAG and READ . We tested our   adaptation strategy on three different language en-   coders coupled with CLIP - T , including BERT - base ,   ELECTRA - base , and ELECTRA - large . We fix the   finetuning parameters between models where com-   parison is intended , and select the median result of481   multiple runs . Details of finetuning are provided in   Appendix C.   Table 1 shows experimental results . Each of our   XD - model constantly outperforms the original en-   coder ( For fair comparison , we train the original   encoder with one more epoch of wiki103 ) . We   found that performance gains are more significant   on smaller datasets ( RTE , MRPC , STSB , CoLA ) ,   indicating that visual features help increase general-   ization when the amount of training data is limited .   The gains are also significant on the readability   benchmark ( READ ) .   We show that the results of finetuning CLIP-   T alone on GLUE does not perform well . Since   the language capability of the CLIP - T model is   weak , the distilled information obtained by XD-   BERT / XDELECTRA is predominantly visual .   It is also possible to finetune the entire cross-   modal transformer after adaptation . The perfor-   mance further increases but the model has more   parameters . The results are in Appendix C.3 .   4 Analysis   To justify the use of a cross - modal encoder , we first   conducted a pairwise projection weighted canoni-   cal correlation analysis ( PWCCA ) on word embed-   dings . The PWCCA is a good measure to determine   how close the distributions of two vector groups   are to each other . The PWCCA results in Table 2   show low scores on both BERT / CLIP and ELEC-   TRA / CLIP before co - training , so the cross - modal   encoder is useful in learning from both distribu-   tions .   We inspect RTE , MRPC , and CoLA results   of 5 runs in detail to show that the improve-   ments are likely from visual information of CLIP-   T. Over the 5 runs , XDBERT - b has accumulated   +38 more correct classifications than BERT - b , or   +2.74%(38/5/277 ) gain in performance . MPRCSystems PWCCA   BERT / ELECTRA 0.5498   BERT / CLIP 0.4980   ELECTRA / CLIP 0.4645   BERT / RANDOM 0.3569   and CoLA show +0.3 % and +0.9 % gains in accu-   racy respectively , and translates to a larger gain   in performance with their original metric ( MRPC   F1 : +0.83 % , CoLA Corr : +2.2 % ) . We then sepa-   rate each of the glue datasets entries into two cat-   egories : entries that XDBERT - b improves classi-   fication over BERT - b , and entries of the opposite .   Entries where both models obtain the same per-   formance are set aside . Analyzing the separated   entries as a whole , we discovered that the better-   performing entries have a larger visually grounded   ratio ( Figure 4 ) , as the quartile , median and mean   values are generally higher for improved samples .   The enhancement of visually grounded token rep-482RTE MPRC STSB CoLA   MLM+MATCH+CLIPTC(proposed ) 69.31 88.02 89.32 56.27   MLM+MATCH 70.04 86.93 88.8 54.62   MLM 68.23 87.25 89.29 54.78   1 cross attention layer 66.79 87.66 89.32 53.62   2 Epochs ( 2x ) 69.31 88.04 89.31 55.91   20 Epochs ( 20x ) 57.4 87.74 - -   wiki(14 G ) , same steps as above 65.3 87.78 89.1 -   resentations is a rough indicator that XDBERT has   obtained distilled visual information from CLIP-   T. We show examples of each category in Ap-   pendix D.   5 Ablation study   We tried various combinations of adaptation tasks   and found out that using all three yielded the best   results . We also tried to reduce the number of   cross - modal encoder layers to one ; however , no   further improvements were made upon the visually   grounded language encoder . Other experiments   include changing the number of layers in the cross-   modal encoder , training for longer , and swapping   to a much larger wiki ( 14 G ) . Swapping to wiki   reduces potential overfitting from the 20 Epochs   setting trained on wiki103 , as training for the same   amount of steps on wiki is less than 1 epoch . We   tested these changes on RTE , MPRC , STSB , and   CoLA on 5 random seeds , and the results are shown   in Table 3 , where MLM refers to the joint MLM   objective , MATCH refers to the cross - modal match-   ing objective , and CLIPTC refers to the CLIP token   classification objective .   Besides experimental evidence , we also jus-   tify the CLIPTC loss via further analysis , as the   CLIPTC objective can theoretically be trivially   solved by identity mapping . Despite this possi-   bility , we find that the loss is crucial to cross at-   tention learning . Since we do not impose negative   hard samples from sampled sentences , the MATCH   objective can be solved sufficiently simply by guid-   ing the cross attention to focus on common trivial   words . With the CLIPTC objective , the diversity   of the input embeddings corresponding to different   tokens must be retained in the cross - modal encoder ,   leading to more robust cross - modal attention . We   show comparisons of the attention maps generated   from the cross - modal encoders with a random se - quence from RTE in Table 9 in the Appendix to   verify this claim .   6 Conclusion   In this study , we explored using cross - modal en-   coders to distill visual information to BERT . We   adapted the model with multiple objectives , and   we were able to achieve improved performance on   NLU tasks . Our adaptation techniques are compu-   tationally inexpensive and straightforward . Further-   more , our method is language encoder agnostic , as   we show similar performance gains on XDELEC-   TRA .   Acknowledgements   This work was supported in part by Ministry of   Science and Technology ( MOST ) , Taiwan , under   Contract 110 - 2223 - E-002 -007 - MY3 .   References483   A Visual - Text Transformers Results on   NLU   We show the result of Visual - Text Transformers on   GLUE , reported by Tan and Bansal ( 2020 ) in Ta-   ble 7 . All of the listed methods ( except LXMERT )   have their text - transformers initialized from BERT .   The results show that multi - modal training for solv-   ing vision - language tasks does not improve the   performance of the models on natural language   understanding tasks.484BERT - b BERT - l CLIP   dim 768 1024 512   max_len 512 512 77   # layers 12 24 12   B Modeling sequences on CLIP   While BERT and CLIP have similar forwarding   mechanisms , the specifications of the transformer   architecture are different , resulting in challenges to   jointly model both models ( Table 4 ) .   Mismatching dimensions pose a problem in   cross - attention . We use a linear transformation   to generate Q , K , and Vof matching dimensions ,   but clarify that this linear transformation layer ex-   ists in the original LXMERT setting where hidden   representations have unified dimensions .   We modify the input to address the mismatched   max_len of the two systems . In the joint MLM , we   used a fixed sequence length of 512 for the BERT .   However , the same can not be done for CLIP as the   maxmum model sequence length is 77 for CLIP .   We found that most BERT sequences ( > 99 % ) of   length 512 encode into CLIP sequences of length   less than 693 , so we pad the CLIP sequence to   length 693 , and then split the CLIP sequence into 9   sub - sequences of length 77 . Therefore , a batch of   inputs will contain BERT inputs of size ( batch_size ,   512 ) and CLIP inputs of size ( batch_size , 9 , 77 ) .   The output was resized to ( batch_size , 693 ) in the   cross - modal encoder . The issue is also present in   the finetuning phase , and the maximum sequence   length of GLUE and SWAG is 128 ; therefore we   used 2 blocks of CLIP sub - sequences to model it .   For bi - sequence classification tasks such as RTE   and MRPC , we ensure that separate sentences do   not use the same block in the CLIP encoder . There-   fore , uni - sequence classification tasks will have a   CLIP input size of ( batch_size , 2 , 77 ) and the bi-   sequence classification task will have a CLIP input   size of ( batch_size , 4 , 77).C Further Training Details   C.1 Adaptation   We use publicly available wiki103 and preprocess-   ing methods similar to Tan and Bansal ( 2020 ) .   Wiki103 ( 500 MB ) is a subset of the Wikipedia cor-   pus consisting of only good and featured articles .   The adaptation of 1 epoch on wiki103 finished in   35 minutes on 8 V100s ( BERT - base ) . We trained   for at most 20 epochs ( 16k steps ) and found that fur-   ther adaptation steps did not increase scores in early   epochs , and significantly decreased performance   in late epochs . We used the following parameters   for adaptation : learning rate = 1e-4 , max_epoch =   40 ( although we stopped early due to plummeting   performance ) , warmup ratio = 0.05   C.2 Finetuning   The learning rates are listed in Table 5 .   base - sized large - sized   RTE , MRPC , STSB 1e-4 5e-5   others 2e-5 1e-5   We used a warmup ratio of 0.1 , with a learn-   ing rate decay of 0.9 , and trained the model for 3   epochs . We report the median results of 5 runs on   different random seeds , except for RTE , which is   unstable ; therefore , we report the median results of   9 runs instead . The reproduce results of ELECTRA   on RTE and STSB are lower than values reported   by Clark et al . ( 2020 ) because we did not start from   an MNLI checkpoint .   C.3 Finetuning with Full Model   Since our cross - modal transformer itself is can also   be viewed as a language encoder , finetuning can   be done on the full model . This approach , how-   ever , adds extra parameters to pretrained - BERT ,   so comparison with pretrained - BERT is not in-   tended , instead , we focus on showing the feasi-   bility of this approach . The number of additional   parameters is only a function of the hidden size in   BERT / ELECTRA , so when the language encoder   is large , the ratio of additional parameters is much   more insignificant . To simplify notations , we use X-   ( language encoder ) to represent the full model . The485number of parameters of the full model is shown   in Table 6 and the results on NLU tasks are shown   in Table 8 .   model parameters   BERT - b / ELECTRA - b 109482240   XBERT - b / XELECTRA - b 202059009   ELECTRA - l 334092288   XELECTRA - l 442671617   D RTE Examples   We provide three RTE example of each type in   Figure4 , and we choose extreme examples where   performance difference is huge over 5 runs for   both " Improved " and " Worsened " categories . We   follow Tan and Bansal ( 2020 ) to classify tokens   as visually - grounded if it is not a stopword and   has more than 100 occurrences in MSCOCO . In   the following examples , Bold words are visually-   grounded , while normal words are non - visually-   grounded . Words in brackets are stopwords and   does not count towards either category .   D.1 Improved : XDBERT outperforms BERT   Example1 :   Visually - grounded ratio : 11/(11 + 16 ) = 0.4074   BERT answered correctly : 0/5   XDBERT answered correctly : 5/5   hands across ( the ) divide ( was ) formed ( in )   march 2001 ( , ) ( and ) one(of ) ( its ) immediate   aims ( was ) ( to ) press ( for ) ( more ) freedom   ( of ) contact ( and ) communication right away   ( between ) ( the ) two parts ( of ) cyprus ( , ) ( and )   ( for ) early progress towards ( a ) solution ( to )   ( ‚Äô ) ( the ) cyprus problem ( ‚Äô ) ( . )   cyprus ( was ) divided ( into ) two parts ( in )   march 2001 ( . )   Example2 :   Visually - grounded ratio : 4/(10 + 4 ) = 0.2857   BERT answered correctly : 0/5   XDBERT answered correctly : 5/5   ( it ) ( is ) hoped ( that ) women ( , ) ( who ) consti-   tute ( more ) ( than ) half ( of ) ( the ) population   ( , ) ( will ) vote ( for ) ( other ) women ( and ) en-   sure ( that ) ( their ) issues ( are ) represented ( in )   parliament ( . )   women ( are ) poorly represented ( in ) parlia - ment ( . )   Example3 :   Visually - grounded ratio : 13/(13 + 17 ) = 0.4333   BERT answered correctly : 0/5   XDBERT answered correctly : 5/5   ho##dler claimed ( there ) ( were ) also irregu-   larities ( in ) ( the ) campaigns organized ( by )   atlanta ( for ) ( the ) 1996 summer games ( , ) syd-   ney ( for ) ( the ) summer olympics ( in ) 2000   ( and ) salt lake city ( for ) ( the ) 2002 winter   games ( . )   ( before ) salt lake city ( , ) winter olympic   games took place ( in ) naga # # no ( . )   D.2 On Par : XDBERT and BERT perform   equally   Example1 :   Visually - grounded ratio : 6/(6 + 32 ) = 0.1375   BERT answered correctly : 0/5   XDBERT answered correctly : 0/5   ( on ) october 1 2001 ( , ) eu ( and ) ( other ) coun-   tries introduced ( the ) option ( for ) domestic   animal owners ( to ) apply ( for ) petpassports   ( under ) ( the ) pets travel scheme ( ( ) pets ( for )   short ( ) ) ( , ) ( for ) pets returning ( from ) abroad   ( to ) ( the ) united kingdom ( . ) ( this ) replaced   ( the ) old system ( of ) 6 months compulsory qu   # # aran # # tine ( for ) ( all ) domestic pets ( . )   ( in ) 2001 ( , ) ( the ) eu introduced ( a ) pass-   port(for ) pets ( . )   Example2 :   Visually - grounded ratio : 5/(5 + 16 ) = 0.2381   BERT answered correctly : 5/5   XDBERT answered correctly : 5/5   security forces ( were ) ( on ) high alert ( after )   ( an ) election campaign ( in ) ( which ) ( more )   ( than ) 1 ( , ) 000 people ( , ) including seven   election candidates ( , ) ( have ) ( been ) killed ( . )   security forces ( were ) ( on ) high alert ( after )   ( a ) campaign marred ( by ) violence ( . )   Example3 :   Visually - grounded ratio : 8/(8 + 16 ) = 0.3333   BERT answered correctly : 5/5   XDBERT answered correctly : 5/5   ( in ) 1979 ( , ) ( the ) leaders signed ( the ) egypt   ( - ) israel peace treaty ( on ) ( the ) white house   lawn ( . ) ( both ) president begin ( and ) sad # # at   received ( the ) nobel peace prize ( for ) ( their)486work ( . ) ( the ) two nations ( have ) enjoyed   peaceful relations ( to ) ( this ) day ( . )   ( the ) israel ( - ) egypt peace agreement ( was )   signed ( in ) 1979 ( . )   D.3 Worsened : XDBERT underperforms   BERT   Example1 :   Visually - grounded ratio : 11/(11 + 29 ) = 0.2750   BERT answered correctly : 5/5   XDBERT answered correctly : 0/5   jean ( - ) claude tri # # chet ( , ) ( the ) european   central bank president ( , ) made ( it)clear ( , )   ( on ) wednesday ( , ) ( that ) ( he ) would oppose   un##war # # rant # # ed political attempts ( to )   remove antonio fa##zio ( :) ( the ) bank ( of )   italy governor ( , ) engulfed ( in ) controversy   ( over ) ( his ) handling ( of ) bank takeover bids   ( . )   antonio fa##zio ( is ) subordinate ( to ) jean   ( - ) claude tri # # chet ( . )   Example2 :   Visually - grounded ratio : 11/(11 + 29 ) = 0.4167   BERT answered correctly : 5/5   XDBERT answered correctly : 0/5   ( about ) half ( were ) along ( a ) 20 ( - ) mile   stretch ( of ) santa monica bay(from ) topanga   canyon boulevard ( to ) ( the ) palo sverde s   peninsula ( . )   ( the ) coastline ( of ) santa monica bay(is )   50 miles long ( . )   Example3 :   Visually - grounded ratio : 32/(32 + 55 ) = 0.3678   BERT answered correctly : 5/5   XDBERT answered correctly : 0/5   cairo ( is ) ( now ) home ( to ) ( some ) 15 mil-   lionpeople ( - ) ( a ) bu##rgeon # # ing popula-   tion ( that ) produces approximately 10 ( , ) 000   tonnes ( of ) rubbish per day ( , ) putting ( an )   enormous strain ( on ) public services ( . ) ( in )   ( the ) past 10 years ( , ) ( the ) government ( has )   tried hard ( to ) encourage private investment   ( in ) ( the ) refuse sector ( , ) ( but ) ( some ) estimate   4 ( , ) 000 tonnes ( of ) waste ( is ) left behind ev-   eryday ( , ) fest # # ering ( in ) ( the ) heat ( as ) ( it )   waits ( for ) someone ( to)clear ( it ) ( up ) ( . ) ( it )   ( is ) often ( the ) people ( in ) ( the ) poor # # est   neighbourhoods ( that ) ( are ) worst affected ( . )   ( but ) ( in ) ( some ) areas ( they ) ( are ) fightingback ( . ) ( in ) shu # # bra ( , ) one(of ) ( the ) north-   ern districts ( of ) ( the ) city ( , ) ( the ) residents   ( have ) taken ( to ) ( the ) streets armed ( with )   dust # # pan # # s(and ) brushes ( to)clean ( up )   public areas ( which ) ( have ) ( been ) used ( as )   public dump # # s ( . )   15 million tonnes ( of ) rubbish ( are ) pro-   duced daily ( in ) cairo ( .)487Diff . to BERT weight SST-2 QNLI QQP MNLI   VL - BERT 6.4e-3 90.1 89.5 88.6 82.9   VisualBERT 6.5e-3 90.3 88.9 88.4 82.4   Oscar 41.6e-3 87.3 50.5 86.6 77.3   LXMERT 42.0e-3 82.4 50.5 79.8 31.8   BERT / ViLBERT ‚Äì 90.3 89.6 88.4 82.4488MLM+MATCH+VC MLM+MATCH   Cross - Encoder , Layer1 , Head0 Cross - Encoder , Layer1 , Head0   Cross - Encoder , Layer2 , Head0 Cross - Encoder , Layer2 , Head0489   Jannis VamvasandRico SennrichDepartment of Computational Linguistics , University of ZurichSchool of Informatics , University of Edinburgh   { vamvas,sennrich}@cl.uzh.ch   Abstract   1 Introduction   Neural machine translation ( NMT ) is susceptible to   coverage errors such as the addition of superÔ¨Çuous   target words or the omission of important source   content . Previous approaches to detecting such   errors make use of reference translations ( Yang   et al . , 2018 ) or employ a separate quality estima-   tion ( QE ) model trained on synthetic data for a   language pair ( Tuan et al . , 2021 ; Zhou et al . , 2021 ) .   In this paper , we propose a reference - free al-   gorithm based on hypothetical reasoning . Our   premise is that a translation has optimal coverage if   it uses as little information as possible and as much   information as necessary to convey the source se-   quence . Therefore , an addition error means that the   source would be better conveyed by a translation   containing less information . Conversely , an omis-   sion error means that the translation would be more   adequate for a less informative source sequence .   Adapting our contrastive conditioning ap-   proach ( Vamvas and Sennrich , 2021 ) , we use prob-   ability scores of NMT models to approximate this   concept of coverage . We create parse trees for both   the source sequence and the translation , and treat   their constituents as units of information . Omis-   sion errors are detected by systematically deletingconstituents from the source and by estimating the   probability of the translation conditioned on such a   partial source sequence . If the probability score is   higher than when the translation is conditioned on   the full source , the deleted constituent might have   no counterpart in the translation ( Figure 1 ) . We   apply the same principle to the detection of addi-   tion errors by swapping the source and the target   sequence .   When comparing the detected errors to human   annotations of coverage errors on the segment   level ( Freitag et al . , 2021 ) , our approach surpasses   a supervised QE baseline that was trained on a large   number of synthetic coverage errors . Human raters   Ô¨Ånd that word - level precision is higher for omis-   sions than additions , with 39 % of predicted error   spans being precise for English ‚Äì German transla-   tions , and 20 % for Chinese ‚Äì English . False positive   predictions can occur especially in cases where the   translation has different syntax than the source . We   believe our algorithm could be a useful aid when-   ever humans remain in the loop , for example in a   post - editing workÔ¨Çow .   We release the code and data to reproduce our   Ô¨Åndings , including a large - scale dataset of syn-   thetic coverage errors in English ‚Äì German and Chi-   nese ‚Äì English machine translations .   2 Related Work   Coverage errors in NMT Addition and omis-   sion of target words have been observed by human   evaluation studies in various languages , with omis-   sion as the more frequent error type ( Castilho et al . ,   2017 ; Zheng et al . , 2018 ) . They are included as   typical translation issues in the Multidimensional   Quality Metrics ( MQM ) framework ( Lommel et al . ,   2014 ) . Addition is deÔ¨Åned as an accuracy issue   where the target text includes text not present in   the source , and omission is deÔ¨Åned as an accuracy490   issue where content is missing from the translation   but is present in the source .   Freitag et al . ( 2021 ) used MQM to manually   re - annotate English ‚Äì German and Chinese ‚Äì English   machine translations submitted to the WMT 2020   news translation task ( Barrault et al . , 2020 ) . Their   Ô¨Åndings conÔ¨Årm that state - of - the - art NMT systems   still erroneously add and omit target words , and that   omission occurs more often than addition . Simi-   lar patterns can be found in English ‚Äì French ma-   chine translations that have been annotated with   Ô¨Åne - grained MQM labels for the document - level   QE shared task ( Specia et al . , 2018 ; Fonseca et al . ,   2019 ; Specia et al . , 2020 ) .   Detecting and reducing coverage errors While   reference - based approaches include measuring the   n - gram overlap to the reference ( Yang et al . , 2018 )   and analyzing word alignment to the source ( Kong   et al . , 2019 ) , this work focuses on the reference - free   detection of coverage errors .   Previous work has employed custom QE models   trained on labeled parallel data . For example , Zhou   et al . ( 2021 ) insert synthetic hallucinations and   train a Transformer to predict the inserted spans .   Similarly , Tuan et al . ( 2021 ) train a QE model on   synthetically noisy translations . In this paper , we   propose a method that is based on off - the - shelf   NMT models only .   Other related work has focused on improving   coverage during decoding or training , for example   via attention ( Tu et al . , 2016 ; Wu et al . , 2016 ; Li   et al . , 2018 ; among others ) . More recently , Yang   et al . ( 2019 ) found that contrastive Ô¨Åne - tuning onreferences with synthetic omissions reduces cover-   age errors produced by an NMT system .   3 Approach   Contrastive Conditioning Properties of a trans-   lation can be inferred by estimating its probability   conditioned on contrastive source sequences ( Vam-   vas and Sennrich , 2021 ) . For example , if a certain   translation is more probable under an NMT model   when conditioned on a counterfactual source se-   quence , the translation might be inadequate .   Application to Omission Errors Figure 1 illus-   trates how contrastive conditioning can be directly   applied to the detection of omission errors . We con-   struct partial source sequences by systematically   deleting constituents from the source . If the prob-   ability score of the translation ( average token log-   probability ) is higher when conditioned on such a   partial source , the deleted constituent is taken to be   missing from the translation .   To compute the probability score for a transla-   tionYgiven a source sequence X , we sum up the   log - probabilities for every target token and normal-   ize the sum by the number of target tokens :   score ( YjX ) = 1   jYjlogp(yjX ; y )   Application to Addition Errors We apply the   same method to addition detection , but swap the   source and target languages . Namely , we use an   NMT model for the reverse translation direction ,   and we score the source sequence conditioned on   the full translation and a set of partial translations.491Potential Error Spans In its most basic form ,   our algorithm does not require any linguistic re-   sources apart from tokenization . For a source sen-   tence of ntokens one could create npartial source   sequences with the ith token deleted . However ,   such an approach would rely on a radical assump-   tion of compositionality , treating all tokens as inde-   pendent constituents .   We thus propose to extract potential error   spans from parse trees , speciÔ¨Åcally from depen-   dency trees predicted by Universal Dependency   parsers ( de Marneffe et al . , 2021 ) , which are widely   available . This allows ( a ) to skip function words   and ( b ) to include a reasonable number of multi-   word spans in the set of potential error spans . For-   mally , we consider word spans that satisfy the fol-   lowing conditions :   1.A potential error span is a complete subtree   of the dependency tree .   2 . It covers a contiguous subsequence .   3 . It contains a part of speech of interest .   For every potential error span , we create a partial   sequence by deleting the span from the original   sequence . This is still a simpliÔ¨Åed notion of con-   stituency , since some partial sequences will be un-   grammatical . Our assumption is that NMT models   can produce reliable probability estimates despite   the ungrammatical input .   4 Experimental Setup   In this section we describe the data and tools that   we use to implement and evaluate our approach .   Scoring model We use mBART50 ( Tang et al . ,   2021 ) , which is a sequence - to - sequence Trans-   former pre - trained on monolingual corpora in many   languages using the BART objective ( Lewis et al . ,   2020 ; Liu et al . , 2020 ) that was Ô¨Åne - tuned on   English - centric multilingual MT in 50 languages .   Sequence - level probability scores are computed by   averaging the log - probabilities of all target tokens .   We use the one - to - many mBART50 model if En-   glish is the source language , and the many - to - one   model if English is the target language .   Error spans We use Stanza ( Qi et al . , 2020 ) for   dependency parsing , a neural pipeline for various   languages trained on data from Universal Depen-   dencies ( de Marneffe et al . , 2021 ) . We make use   of universal part - of - speech tags ( UPOS ) to deÔ¨ÅneOriginal source   Partial sourceFull translation   Partial translationtranslateDelete random   constituentsCheck addition   propertytranslate   parts of speech that might constitute potential error   spans . SpeciÔ¨Åcally , we treat common nouns , proper   nouns , main verbs , adjectives , numerals , adverbs ,   and interjections as relevant parts of speech .   Gold Standard Data We use state - of - the - art En-   glish ‚Äì German and Chinese ‚Äì English machine trans-   lations for evaluation , which have been annotated   by Freitag et al . ( 2021 ) with translation errors . We   set aside translations by the system Online - B as   a development set , and use the other systems as   a test set , excluding translations by humans . The   development set was used to identify the typical   parts - of - speech of coverage error spans , listed in   the paragraph above .   Synthetic Data We also create synthetic cover-   age errors , which we use for training a supervised   baseline QE system . We propose a data creation   process that is inspired by previous work ( Yang   et al . , 2019 ; Zhou et al . , 2021 ; Tuan et al . , 2021 )   but is deÔ¨Åned such that it works for both additions   and omissions , and produces Ô¨Çuent translations .   Figure 2 illustrates the process . We start from   the original source sentences and create partial   sources by deleting randomly selected constituents .   SpeciÔ¨Åcally , we delete each constituent with a prob-   ability of 15 % . We then machine - translate both the   original and the partial sources , yielding fulland   partial machine translations . We retain only sam-   ples where the full machine translation is different   from the partial one , and can be constructed by   addition .   This allows us to treat the full translations as   overtranslations of the partial sources , and the   added words as addition errors . Conversely , the   partial translations are treated as undertranslations   of the original sources . Negative examples are cre-492Approach Detection of additions Detection of omissions   Precision Recall F1 Precision Recall F1   EN ‚Äì DESupervised baseline 6.9 1.9 2.90.9 4.01.3 40.35.2 6.10.1 10.60.2   Our approach 4.0 15.0 6.3 22.3 18.8 20.4   ZH ‚Äì ENSupervised baseline 4.3 0.6 4.70.7 4.50.6 49.60.6 9.41.0 15.91.4   Our approach 1.7 40.6 3.4 25.8 62.0 36.5   ated by pairing the original sources with the full   translations , and the partial sources with the partial   translations .   Our synthetic data are based on monolingual   news text released for WMT.To train the baseline   system , we use 80k unique source segments per   language pair . Statistics are reported in Table A3 .   Supervised baseline system Following the ap-   proach outlined by Moura et al . ( 2020 ) , we use   the OpenKiwi framework ( Kepler et al . , 2019 ) to   train a separate Predictor - Estimator model ( Kim   et al . , 2017 ) per language pair , based on XLM-   RoBERTa ( Conneau et al . , 2020 ) . The supervised   task can be described as token - level binary classi-   Ô¨Åcation . Every token is classiÔ¨Åed as either OKor   BAD , similar to the word - level labels used for the   QE shared tasks ( Specia et al . , 2020 ) . A source   token is BAD if it is omitted in the translation , and   a token in the translation is BAD if it is part of an   addition error . For English and German , we use the   Moses tokenizer ( Koehn et al . , 2007 ) to separate   the text into labeled tokens ; for Chinese we label   the text on the character level .   Where suitable , we use the default settings of   OpenKiwi . We Ô¨Åne - tune the large version of XLM-   RoBERTa , which results in a model of similar pa-   rameter count as the mBART50 model we use for   contrastive conditioning . We train for 10 epochs   with a batch size of 32 , with early stopping on the   validation set . For token classiÔ¨Åcation we train   two linear layers , separately for source and target   language ( which corresponds to omissions and ad-   ditions , respectively ) . We use AdamW ( Loshchilov   and Hutter , 2019 ) with a learning rate of 1e-5 , freez-   ing the pretrained encoder for the Ô¨Årst 1000 steps.5 Evaluation   5.1 Segment - Level Comparison to Gold Data   The accuracy of our approach can be estimated   based on the human ratings by Freitag et al . ( 2021 ) .   Evaluation Design We use the MQM error types   Accuracy / Addition andAccuracy / Omission , and ig-   nore other types such as Accuracy / Mistranslation .   We count a prediction as correct if any one of the   human raters has marked the same error type any-   where in the segment . We exclude segments from   the evaluation that might have been incompletely   annotated ( because raters stopped after marking   Ô¨Åve errors ) . For ease of implementation , we also ex-   clude segments that consist of multiple sentences .   Results The results of the gold - standard compar-   ison are shown in Table 1 . Our approach clearly   surpasses the baseline in the detection of omis-   sion errors in both language pairs . However , both   approaches recognize addition errors with low ac-   curacy , and especially the supervised baseline has   low recall . Considering its high performance on   a synthetic test set ( Table A1 in the Appendix ) , it   seems that the model does not generalize well to   real - world coverage errors , highlighting the chal-   lenges of training a supervised QE model on purely   synthetic data .   5.2 Human Evaluation of Precision   We perform an additional word - level human eval-   uation to analyze the predictions obtained via our   approach in more detail . Our human raters were   presented segments that had been marked as true   or false positives in the above evaluation , allowing   us to quantify word - level precision.493EN ‚Äì DE ZH ‚Äì EN   TargetAddition errors 2.3 1.2   Any errors 7.4 12.0   SourceOmission errors 36.3 13.8   Any errors 39.4 19.5   Evaluation Design We employed two linguistic   experts per language pair as raters . Each rater   was shown around 700 randomly sampled positive   predictions across both types of coverage errors .   Raters were shown the source sequence , the   machine translation , and the predicted error span .   They were asked whether the highlighted span was   indeed translated badly , and were asked to perform   a Ô¨Åne - grained analysis based on a list of predeÔ¨Åned   answer options ( Figures 3 and 4 in the Appendix ) .   A part of the samples were annotated by both   raters . The agreement was moderate for the   main question , with a Cohen ‚Äôs kappa of 0.54 for   English ‚Äì German and 0.45 for Chinese ‚Äì English .   Agreement on the more subjective follow - up ques-   tion was lower ( 0.32 / 0.13 ) .   Results The Ô¨Åne - grained answers allow us to   quantify the word - level precision of the spans high-   lighted by our approach , both with respect to cover-   age errors in particular and to translation errors in   general ( Table 2 ) . Precision is higher than expected   when detecting omission errors in English ‚Äì German   translations , but is still low for additions . The dis-   tribution of the detailed answers ( Figures 3 and 4 in   the Appendix ) suggests that syntactical differences   between the source and target language contribute   to the false positives regarding additions . Example   predictions are provided in Appendix F , which in-   clude cases where all three raters of Freitag et al .   ( 2021 ) had overlooked the coverage error .   Finally , Table 2 shows that many of the predicted   error spans are in fact translation errors , but not cov-   erage errors in a narrow sense . For example , more   than 10 % of the spans marked in Chinese ‚Äì English   translations were classiÔ¨Åed by our raters as a differ-   ent type of accuracy error , such as mistranslation.6 Limitations and Future Work   We hope that the automatic detection of cover-   age errors could be an aid to translators and post-   editors , given that manually detecting such errors is   tedious . Our results on omissions are encouraging ,   and user studies are recommended in order to vali-   date the usefulness of the predictions to practition-   ers . Further work needs to be done to improve the   detection of additions , of which the real - world data   contain few examples . Higher accuracy would be   necessary for word - level QE to be helpful ( Shenoy   et al . , 2021 ) , and so with regard to detecting addi-   tion errors , the practical utility of both the baseline   and of our approach remains limited .   Inference time should also be discussed . In Ap-   pendix C we perform a comparison , Ô¨Ånding that on   a long sentence pair contrastive conditioning can   take up to ten times longer than a forward pass of   the baseline . However , this is still a fraction of the   time needed for generating a translation in the Ô¨Årst   place . In addition , restricting the potential error   spans that are considered could further improve   efÔ¨Åciency .   7 Conclusion   We have proposed a reference - free method to au-   tomatically detect coverage errors in translations .   Derived from contrastive conditioning , our method   relies on hypothetical reasoning over the likelihood   of partial sequences . Since any off - the - shelf NMT   model can be used to estimate conditional likeli-   hood , no access to the original translation system   or to a quality estimation model is needed . Evalu-   ation on real machine translations shows that our   approach outperforms a supervised baseline in the   detection of omissions . Future work could address   the low precision on addition errors , which are rel-   atively rare in the datasets we used for evaluation .   Acknowledgments   This work was funded by the Swiss Na-   tional Science Foundation ( project MUTAMUR ;   no . 176727 ) . We would like to thank Xin Sennrich   for facilitating the recruitment of annotators , and   Chantal Amrhein as well as the anonymous review-   ers for helpful feedback .   References494495   A Annotator Guidelines   You will be shown a series of source sentences   and translations . One or several spans in the text   are highlighted and it is claimed that the spans   are translated badly . You are asked to determine   whether the claim is true . The highlighted spans   can be either in the source sequence or in the trans-   lation . If a span is in the source sentence , check   whether it has been correctly translated . If a span   is in the translation , check whether it correctly con-   veys the source . Sometimes , multiple spans are   highlighted . In that case , focus your answer on   the span that is most problematic for the transla-   tion . In a second step , you are asked to select an   explanation . On the one hand , if you agree that   the highlighted span is translated badly , please ex-   plain your reasoning by selecting your explanation .   On the other hand , if you disagree and think that   the span is well - translated , please select an expla-   nation why the span might have been marked as   badly translated in the Ô¨Årst place . Should multiple   explanations be equally plausible , select the Ô¨Årst   from the top.496Detection of additions Detection of omissions   Prec . Recall F1 MCC Prec . Recall F1 MCC   EN ‚Äì DE   Supervised   Baseline 98.80.4 98.0.298.4.296.8.1 94.01.3 96.60.4 95.3.590.5.2   Ours 78.1 88.3 82.9 76.7 80.9 98.6 88.9 78.1   ZH ‚Äì EN   Supervised   Baseline 87.21.5 75.7.681.0.372.6.6 67.31.3 68.01.2 67.7.953.8.3   Ours 26.1 88.9 40.4 23.3 28.3 92.0 43.3 40.3   Short sentence pair Long sentence pair   Additions Omissions Both Additions Omissions Both   Supervised baseline - - 25 ms - - 25 ms   Our approach 40 ms 45 ms 83 ms 165 ms 197 ms 365 ms   ‚Äì excluding parser 18 ms 21 ms 38 ms 102 ms 144 ms 239 ms   B Evaluation on Synthetic Errors   We used a test split held back from the synthetic   data to perform an additional evaluation . On the   segment level , we report Precision , Recall and F1-   score . Like in Section 5.1 , a prediction is treated   as correct on the segment level if for a predicted   coverage error there is indeed a coverage error of   that type anywhere in the segment .   On the word level , we follow previous work on   word - level QE ( Specia et al . , 2020 ) and report the   Matthews correlation coefÔ¨Åcient ( MCC ) across all   the tokens in the test set .   Results Results are shown in Table A1 . The   supervised baseline has a high accuracy on En-   glish ‚Äì German translations and a moderate accuracy   on Chinese ‚Äì English translations . In comparison ,   our approach performs clearly worse than the su-   pervised baseline on the synthetic errors . C Inference Time   Inference times are reported in Table A2 . We mea-   sure the time needed to run the coverage error de-   tection methods on a short sentence pair and on a   long sentence pair for English ‚Äì German . The short   sentence pair is taken from Figure 1 and the long   sentence pair has 40 tokens in the source sequence   and 47 tokens in the target sequence . We average   over 1000 repetitions on RTX 2080 Ti GPUs .   The higher inference times for our approach can   be explained by the number of translation probabili-   ties that need to be estimated . On average , we com-   pute 30 scores per sentence in the English ‚Äì German   MQM dataset , and 44 per sentence in the Chi-   nese ‚Äì English MQM dataset . Still , the time needed   for computing all these scores is only a fraction of   the time it takes to generate a translation ( 254 ms   for the short source sentence and 861 ms for the   long sentence , assuming a beam size of 5 ) .   The required number of scores could be reduced   by considering fewer potential error spans . Further-   more , scoring could be parallelized across batches   of multiple translations . Finally , using a more ef-   Ô¨Åcient parser , or no parser at all , could speed up   inference.497D Dataset Statistics   Dataset split Number of segments Number of tokens   Total W/ addition W/ omission Src . OK Src . BAD Tgt . OK Tgt . BAD   EN ‚Äì DE Train 135269 18423 18423 2185918 58378 2197843 53911   EN ‚Äì DE Dev 16984 2328 2328 273311 7398 275156 6781   EN ‚Äì DE Test 16984 2328 2328 273277 7701 275036 7032   ZH ‚Äì EN Train 110195 10697 10697 2576135 62311 1866567 37730   ZH ‚Äì EN Dev 14149 1383 1383 326743 7562 236685 4244   ZH ‚Äì EN Test 14026 1342 1342 322000 7566 234757 4882   Dataset split Number of segments   Total With an addition error With an omission error   EN ‚Äì DE Dev 1418 77 187   EN ‚Äì DE Test 8508 407 1057   ‚Äì without excluded segments 4839 162 484   ZH ‚Äì EN Dev 1999 69 516   ZH ‚Äì EN Test 13995 329 3360   ‚Äì without excluded segments 8851 149 1569   E Examples of Synthetic Coverage Errors   English ‚Äì German Example   Addition error   Partial source : But they have n‚Äôt played .   Full machine translation : Aber sie haben nichtgegeneinTeamwieuns gespielt .   Omission error   Full source : But they have n‚Äôt playedagainstateamlikeus .   Partial machine translation : Aber sie haben nicht gespielt .   Chinese ‚Äì English Example   Addition error   Partial source : ÂåªÈô¢Âíå‰ºÅ‰∏öÂÖ±ÂêåÁ†îÂèëÁõ∏ÂÖ≥Ê£ÄÊµãËØïÂâÇÁõíÔºåÊÉ†ÂèäÊõ¥Â§öÊÇ£ËÄÖ „ÄÇ   Full translation : Hospitals and enterprises jointly develop related test kits to beneÔ¨Åt morecancer patients .   Omission error   Full source : ÂåªÈô¢Âíå‰ºÅ‰∏öÂÖ±ÂêåÁ†îÂèëÁõ∏ÂÖ≥Ê£ÄÊµãËØïÂâÇÁõíÔºåÊÉ†ÂèäÊõ¥Â§öËÇøÁò§ÊÇ£ËÄÖ „ÄÇ   Partial translation : Hospitals and enterprises jointly develop related test kits to beneÔ¨Åt more patients.498F Examples of Coverage Errors Predicted by Contrastive Conditioning   English ‚Äì German Examples   Predicted addition error   Source : He added : " It ‚Äôs backÔ¨Åred on him now , though , that ‚Äôs the sad thing . "   Machine translation : Er f√ºgtehinzu : " Es ist jetzt auf ihn abgefeuert , aber das ist das Traurige . "   Original MQM rating ( Freitag et al . , 2021 ): No related accuracy error marked by the three raters .   Answer by our human rater : The highlighted target span is not translated badly . It might have been   highlighted because it is syntactically different from the source .   Meaning of highlighted span : hinzu = ‚Äò additionally ‚Äô   Predicted omission error   Source : UK ‚Äôs medicaldrug supply still uncertain in no - deal Brexit   Machine translation : Die medizinische Versorgung Gro√übritanniens ist i m No - Deal - Brexit noch ungewiss   Original MQM rating : No accuracy error marked by the three raters .   Answer by our human rater : The highlighted source span is indeed translated badly . It contains informa-   tion that is missing in the translation but can be inferred or is trivial .   Predicted omission error   Source : The automaker is expected to report its quarterly vehicle deliveries in the nextfewdays .   Machine translation : Der Autohersteller wird voraussichtlich in den n√§chsten Tagen seine viertelj√§hrlichen   Fahrzeugauslieferungen melden .   Original MQM rating : No related accuracy error marked by the three raters .   Answer by our human rater : The highlighted source span is not translated badly . The words in the span   do not need to be translated .   Chinese ‚Äì English Examples   Predicted addition error   Source : ÁæéÊñπÊåáË¥£‰ºäÊúóÂà∂ÈÄ†‰∫ÜËØ•Ë¢≠ÂáªÔºåÂπ∂ÂØπ‰ºäÊúóÂÆûÊñΩÊñ∞Âà∂Ë£Å „ÄÇ   Machine translation : The US accused Iran of causing the attack and imposed new sanctionson Iran .   Original MQM rating ( Freitag et al . , 2021 ): No related accuracy error marked by the three raters .   Answer by our human rater : The highlighted target span is not translated badly . No phenomenon that   might have caused the prediction was identiÔ¨Åed .   Predicted omission error   Source : ÁõÆÂâçÂ∑≤Êî∂Âà∞Êù•Ëá™‰øÑÁΩóÊñØÂÜú‰∏ö‰ºÅ‰∏öÁöÑÁ∫¶50È°πÁî≥ËØ∑ „ÄÇ   Machine translation : About 50 applications have been received from Russian agricultural enterprises .   Original MQM rating : No accuracy error marked by the three raters .   Answer by our human rater : The highlighted source span is indeed translated badly . It contains informa-   tion that is missing in the translation .   Meaning of highlighted span : ÁõÆÂâç= ‚Äò at present ‚Äô   Predicted omission error   Source : ‰ªñËØ¥ÔºåËØ•Á≥ªÁªüÁõÆÂâçÂú®‰∏ñÁïå‰∏äÊúâÂæàÂ§ßÈúÄÊ±ÇÔºå‰ΩÜ‰øÑÁΩóÊñØÂÜõÈòü‰πüÈúÄË¶ÅÂÆÉÔºåÂÖ∂‰∏≠ÂåÖÊã¨Âú®ÂåóÊûÅÂú∞Âå∫ „ÄÇ   Machine translation : He said that the system is currently in great demand in the world , but the Russian   army also needs it , including in the Arctic .   Original MQM rating : No accuracy error marked by the three raters .   Answer by our human rater : The highlighted source span is not translated badly . The words in the span   do not need to be translated .   Meaning of highlighted span : ÂÖ∂‰∏≠= ‚Äò among‚Äô499 G Detailed Results of Human Evaluation500   Bingzhi Li and Guillaume Wisniewski and Beno√Æt Crabb√©   Universit√© de Paris , LLF , CNRS   75 013 Paris , France   bingzhi.li@etu.u-paris.fr   { guillaume.wisniewski,benoit.crabbe}@u-paris.fr   Abstract   1 Introduction   Transformers ( Vaswani et al . , 2017 ) have become a   key component in many NLP models , arguably due   to their capacity to uncover distributed representa-   tion of tokens ( Hinton et al . , 1986 ) that are contextu-   alized : thanks to a multi - head self - attention mech-   anism ( Bahdanau et al . , 2015 ) , a token representa-   tion can , virtually , depend on the representations of   all other tokens in the sentence , and transformers   are able to learn a weighting to select which tokens   are relevant to its interpretation .   Many works ( Rogers et al . , 2020 ) strive to ana-   lyze the representations uncovered by transformers   to find out whether they are consistent with models   derived from linguistic theories . One of the main   analysis methods is the long - distance agreement   task popularized by Linzen et al . ( 2016 ) , which   consists in assessing neural networks ability to pre-   dict the correct form of a token ( e.g. a verb ) in   accordance with the agreement rules ( e.g. its sub-   ject ) . This method has been generalized to other   agreement phenomena ( Li et al . , 2021 ) and other   languages ( Gulordava et al . , 2018 ) . The concor-   dant conclusions of all these experiments show   that transformers are able to learn a ‚Äò substantial   amount ‚Äô of syntactic information ( Belinkov and   Glass , 2019).If the method of Linzen et al . ( 2016 ) makes it   possible to show that syntactic information is en-   coded in neural representations , it does not give any   indication on its localization : it is not clear whether   the syntactic information is distributed over the   whole sentence ( as made possible by self - attention )   or only in a way consistent with the syntax of the   language , i.e. only in the tokens involved in the   agreement rules .   This work addresses the question : where the syn-   tactic information is encoded in transformer repre-   sentations . We approach this question from two   perspectives , considering the object - past participle   agreement in French ( Section 2 ) . First , in Sec-   tion 3 , using probing and counter - factual analysis ,   we try to identify the tokens in which syntactic in-   formation is encoded in order to find its localization   within the sentence . Second , in Section 4 , using a   feature selection method , we study the localization   of syntactic information within the contextualized   representation of tokens .   2The Object - Participle Agreement Task   Task We evaluate the capacity of transformers   to capture syntactic information , by considering   the object - past participle agreement in French ob-   ject relatives . This task consists in comparing the   probabilities a language model assigns to the sin-   gular and plural forms of a past participle given   the beginning of the sentence . The probability of a   past participle form is conditioned on all the words   in the prefix ( the words from the beginning of the   sentence up to the antecedent ; see Figure 1 for   an example ) and the context ( the words from the   antecedent up to and excluding the past participle ) .   Following Linzen et al . ( 2016 ) the model is consid-   ered to predict the agreement correctly if the form   with the correct number has a higher probability501Ce soir les amis que j ‚Äô airencontr√©s √†l‚Äôuniversit√© viennent manger   than the form with the incorrect number .   Contrary to the classical subject - verb agreement   task ( Linzen et al . , 2016 ) , the French object past   participle agreement involves a filler - gap depen-   dency and the target past participle has to agree   with a noun that is never adjacent to it . In our case ,   it features a syntactic structure that allows us to   highlight the way the information is distributed in   the sentence ( ¬ß 3.1 ) .   Figure 1 gives an example of the sentences con-   sidered here . It involves sentences whose verb is   in the compound past ( pass√© compos√© ) , a tense   composed of an auxiliary and the past participle   of the verb . What part of the speech ( i.e. the sub-   ject , the object or no agreement ) the compound   verbs must agree with depends on the auxiliary   verb used . When the past participle is used with   the auxiliary avoir , it has to agree in numberwith   its direct object when the latter is placed before it   in the sentence . This is notably the case for ob-   ject relatives considered here , in which the direct   object is the relative pronoun que , whose number   information is the same as its antecedent ( even if   its morphology ‚Äì que , is the same in singular and   plural ) . To correctly agree the past participle in   object relatives , it is therefore necessary to identify   the object relative pronoun , its antecedent and the   auxiliary .   Experimental Setting We reuse the dataset of   Li et al . ( 2021 ): they have extracted , with sim-   ple heuristics a set of 68,497 such sentences after   having automatically parsed the Gutenberg corpus   with a BERT based dependency parser ( Grobol and   Crabb√© , 2021 ) .   The experiments are carried out with the incre-   mental transformer designed by Li et al . ( 2021 ) ,   which was trained on 80 million tokens of FrenchWikipedia , and has 16 layers and 16 heads . Word   embeddings are of size 768 . This model is able   to predict 93.5 % of the past participle agreement ,   a result that allows these authors to conclude that   syntactic information is encoded in the representa-   tions .   3 Is Syntactic Information Locally or   Globally Distributed in the Sentence ?   Results reported in the previous section show that   information about the number of the past participle   is encoded in the token representations but they do   not allow to identify which tokens have been used   to predict the correct form of the past participle .   In this section , we first identify , using linguistic   probes , the tokens in which syntactic information is   encoded and then , with a causal analysis , the tokens   on which transformers mainly rely to predict the   form of the past participle .   3.1 Probing Experiments   In a first set of experiments , we propose to use   linguistic probes to better identify where in the   sentence the information about the number of the   past participle is encoded . A probe is a classifier   trained to predict linguistic properties from the lan-   guage representations : achieving high accuracy at   this task implies that these properties were encoded   in the representation ( Hewitt and Manning , 2019 ) .   More precisely , we label each sentence of our   dataset with the number of the target verb ( i.e. sin-   gular or plural ) and consider the task of predicting   this label from each token representation of the sen-   tence . We trained one logistic regression classifier   per category of wordconsidering 80 % of the ex-   amples as training data and the remaining 20 % as   test set.502   Table 1 reports the average accuracy achieved by   our probes on different parts of the sentence . We   observe that the past participle number information   is essentially encoded locally within the tokens   of the context and is not represented uniformly   across all the subsequent tokens of the sentence as   observed by Klafka and Ettinger ( 2020 ) .   Indeed , as expected , in the prefix ( before the   antecedent ) the performance of the probe mainly   reflects the difference between the prior probabili-   ties of the two classes . By contrast , the accuracy   becomes high when the tokens of the context are   considered as input features of the probe , showing   that the information required to predict the correct   past participle form is spread over all tokens be-   tween the antecedent ( where the number of the   past participle is specified ) and the past participle   ( where the information is ‚Äò used ‚Äô ) . It is quite re-   markable that , as soon as the past participle has   been observed and the information on the number   of the antecedent is no longer useful , the token   representations no longer encode it : in the suffix   the probe accuracy drops sharply even if it remains   better than that observed in the prefix . This result   contradicts also , at least partially , the observation   of Wisniewski et al . ( 2021 ) which shows that in   a neural translation system , gender information is   distributed all over the source and target represen-   tations . It should however be noted that this experi-   ment deals with a different kind of information and   only considers sentences following a very simple   pattern .   To get a more accurate picture of how the num-   ber information is distributed within the context ,   we focus on a specific sentence template with a   fixed six - word context : we only consider sentences   in which the antecedent is separated from the rela - tive pronoun by a prepositional phrase made of a   preposition and a noun as in the following example :   ( 1 )   ......   This pattern ( 1,940 sentences ) represents 3 % of   the examples of the original dataset . Note that in   these sentences the embedded noun between the an-   tecedent of the object pronoun and the target verb   can be an attractor noun , i.e. a noun with mis-   leading agreement feature . We trained and tested a   separate logistic regression classifier for each posi-   tion as illustrated by the x - axis labels in figure 2 .   We plot in figure 2 the average probing accuracy   at different positions of this pattern . In the prefix   ( i.e. b - positions ) the probe accuracy is low , except   for the position just before the antecedent , which   often corresponds to determiners or adjectives that   have to agree in number with the antecedent . On   the contrary , in the context , the predictions of the   probe are almost perfect , even when we are probing   tokens marked with a number information that is   not necessarily related to the number of the past par-   ticiple ( e.g. the auxiliary or the attractor ) . Accuracy   in the suffix drops quickly as we move away from   the past participle , especially in the presence of an   attractor . These observations confirm that the num-   ber information is not distributed over all tokens in   the sentence as made possible by the self - attention   mechanism.503   3.2 Causal intervention on attention   As it stands , we observe that number information is   encoded essentially in the context part of sentences .   Now we test which tokens are responsible for pro-   viding the number information used to choose the   past participle form . To do so , we design a causal   experiment in which we mask some tokens of the   context to better figure out their role in models   decision .   Masking Tokens in Self - Attention Computation   Self - attention is a core component of transformers .   In our causal analysis we mask some token repre-   sentations in the context to the self - attention layer .   By design , incremental transformers are already   masking the end of the sentence with a boolean   mask to prevent a token representation to attend to   the future tokens . We extend this mechanism to   mask , when computing the past participle represen-   tation , additional tokens from the sentence prefix   such as the antecedent and the relative pronoun .   This intervention allows us to suppress direct   access to some tokens such as the antecedent ( and   thus its number ) when building the past participle   representation , even if the latter can still access   them indirectly : it indeed relies on all other to-   kens in the sentence for which the mask is kept   unchanged . It is then possible , as featured in ab-   lation experiments , to compare performances on   the agreement task with and without intervention   to evaluate whether the representation of a given   token has a direct impact on the prediction of the   past participle form .   Results Table 2 reports the accuracy on the   object - past participle agreement task when some   of the tokens in the context are masked . Accura-   cies are broken down by the number of attractors   found in the context , a proxy to the difficulty of   the prediction ( Gulordava et al . , 2018 ) . Results   show that masking either of the tokens involved in   the agreement rule ( i.e. the relative pronoun queorthe antecedent ) strongly degrades prediction per-   formance . On the contrary , masking all tokens in   context except these two and the token before the   target verb ( generally the auxiliary ) has a limited   impact on models performance , especially for the   most difficult case . This suggests that transformers   learn representations that are consistent with the   French grammar : the model relies mainly on the   same tokens as humans to choose the correct form   of the past participle .   4 Probing Representations Components   Experiments reported in the previous section show   that syntactic information is locally encoded in   thecontext . In this section , we address the ques-   tion of finding where this information is encoded   within the transformers representation . To that   end , we repeat the probing experiment on context   token representations of ¬ß 3.1 with an ‚Ñìregular-   ized logistic regression ( Tibshirani , 1996 ) . The   resulting probe is thus constrained to minimize   the number of features used to perform accurate   predictions . Given the probe objective functionP‚àílogP(y|x;w ) + ||w||to minimize ,   we first determined the lowest bound for C such   that the feature coefficients are guaranteed not to   be all zeros , from which , we increase C evenly on a   log space ( i.e. decrease the regularization strength ) .   Results Figure 3 reports the regularization path   of the probing classifier . It shows that number infor-   mation can be extracted with high accuracy ( 90.1 % )   solely from a very small number of dimensions ,   namely 90 . Increasing the number of dimensions   ( by decreasing the regularization strength ) only re-   sults in a small improvement of model quality : the   probe achieves an accuracy of 94.8 % when all fea-   tures are considered . Interestingly , when removing   the 90 features selected by the ‚Ñìregularization   from the representation , a probe trained on the re-   maining features still achieve a very good accuracy   of 93.8 % , suggesting that the number information504is encoded in a redundant way in the contextualised   representations .   5 Discussion and conclusion   To understand how syntactic information is en-   coded and used in transformers - based LM , we car-   ried out three sets of experiments considering the   French object - past participle agreement task . First ,   our probing experiments uncovered clear evidence   of a local distribution of number information within   thecontext tokens , even though the self - attention   mechanism allows this information to be spread   all over the sentence . Second , our masking inter-   vention on attention shows a causal link between   linguistically motivated tokens and the model ‚Äôs de-   cision , suggesting that transformers process French   object - past participle agreement in a linguistically-   motivated manner . Finally , we used a ‚Ñìfeature se-   lection method to study the localization of number   information within contextualized representations   and found that while this information is encoded in   a small amount of highly correlated dimensions , it   is also fuzzily encoded in a redundant way in the   remaining dimensions .   Our work is a first step towards a better under-   standing of the inner representations of LM . De-   signing new probes , supported by causal analysis   and involving a wider range of languages , could   improve our understanding of such models . In   particular , our observation about the linguistically   motivated distribution of syntactic information in   transformers representations could be extended to   other linguistic phenomenon and languages .   Acknowledgments   We sincerely thank the reviewers and Program   Chairs for their careful reviews and insightful com - ments , which are of great help in improving the   manuscript . This work was granted access to the   HPC resources of French Institute for Development   and Resources in Intensive Scientific Computing   ( IDRIS ) under the allocation 2020 - AD011012282   and 2021 - AD011012408 made by GENCI . We   would also like to gratefully acknowledge support   from the Labex EFL , ‚Äú Empirical Foundations in   Linguistics ‚Äù ( ANR-10 - LABX-0083 )   References505506A Probing classifiers   We used a set of logistic regression classifiers to   investigate the way the syntactic information is dis-   tributed inside the sentences . Each sentence are   divided into three parts : prefix , context andsuf-   fix , as described in Figure 1 . The input for all   classifiers are the contextualized token represen-   tations built by our pre - trained transformers . We   trained one classifier per category of word and per   part of the sentences to predict whether the token   representation is singular or plural , forcing each   probing classfier to specialise on PoS - specific rep-   resentations of long - distance agreement informa-   tion . To ensure a fair comparison across parts of   sentences , we eliminated the following tokens of   PoS tags with less than 100 occurrences : SYM ,   SCONJ , INTJ , PART , PART and X. Therefore , we   have in total 11 categories of tokens in each part of   the sentences , resulting in 11 * 3 probing classifiers ,   and each classifier is trained with three train / test   splits(i.e . random_state = 0,20and42 ) . The aver-   aged results is reported in table 1 of the paper . The   detailed results per category of word is in figure 4   below.507   Mat¬Øƒ±ss Rikters , Marili Tomingas , Tuuli Tuisk , Valts Ern≈°treits , Mark FishelUniversity of Tartu   { matiss.rikters , tuuli.tuisk , marili.tomingas , fishel}@ut.eeUniversity of Latvia   valts.ernstreits@lu.lvUniversity of Copenhagen   Abstract   Livonian is one of the most endangered languages   in Europe with just a tiny handful of speakers and   virtually no publicly available corpora . In this pa-   per we tackle the task of developing neural machine   translation ( NMT ) between Livonian and English ,   with a two - fold aim : on one hand , preserving the   language and on the other ‚Äì enabling access to   Livonian folklore , lifestories and other textual in-   tangible heritage as well as making it easier to   create further parallel corpora . We rely on Livo-   nian ‚Äôs linguistic similarity to Estonian and Latvian   and collect parallel and monolingual data for the   four languages for translation experiments . We   combine different low - resource NMT techniques   like zero - shot translation , cross - lingual transfer and   synthetic data creation to reach the highest possible   translation quality as well as to Ô¨Ånd which base lan-   guages are empirically more helpful for transfer to   Livonian . The resulting NMT systems and the col-   lected monolingual and parallel data , including a   manually translated and veriÔ¨Åed translation bench-   mark , are publicly released via the OPUS corpora   collection and Huggingface model repository .   1 Introduction   Many state - of - the - art natural language processing   tasks have reached admirable quality on languages   with abundant linguistic resources ( Vaswani et al . ,   2017 ; Conneau et al . , 2018 ; Devlin et al . , 2019 ) .   Furthermore , some neural language models and   translation systems have been created for 100 and   more languages ( e.g. Conneau et al . , 2020 ; Fan   et al . , 2021 ) . However smaller , less or not at all   spoken languages continue to struggle not only in   terms of applicable computational approaches , but   more critically - in terms of usable resources for   training natural language processing ( NLP ) models   or even just linguistic exploration . In this paper we set the goal of developing ma-   chine translation between English and Livonian .   Currently there are just over 20 Ô¨Çuent speakers of   the language ( Ern≈°treits , 2016 ) . Although some   digital linguistic resources exist for Livonian ( in-   cluding a dictionary with example sentences and   a written monolingual corpus , Ern≈°treits , 2016 ) ,   there is virtually no open parallel corpora between   English and Livonian , with the single exception of   35 parallel sentences in the OPUS Tatoeba corpus   ( Tiedemann , 2020 ) .   At the same time , cross - lingual transfer learning   has recently helped improve the performance of   several low - resource NLP tasks with the support   of related languages ( e.g. Conneau et al . , 2018 ; Hu   et al . , 2020 ) . This also includes zero - shot trans-   lation ( Johnson et al . , 2017 ) , the ability of mul-   tilingual NMT systems to translate between seen   languages that were not represented in the parallel   training data as a pair . The case of Livonian is es-   pecially interesting in this regard , as there are two   different sources of such support : on one hand , it is   a Uralic language , closely related to Estonian and   Finnish . On the other hand , Livonian has taken part   in forming Latvian language and Livonian speakers   have historically co - existed side - by - side with Lat-   vian speakers . As a result of mutual inÔ¨Çuence these   two languages also share a number of grammatical ,   lexical and orthographic similarities .   Our main contributions are two - fold . First ,   we collected the majority of digitally available   translation examples including Livonian into a   small parallel corpus ( just over 10000 sentence   pairs ) of mostly Livonian - Latvian and Livonian-   Estonian sentence translations with very few ( 1000 )   Livonian - English examples . In order to create a   clean benchmark for evaluating translation qual-   ity we selected a portion ( about 10 % ) of this   corpus and had it manually translated into Lat-   vian / Estonian / English so that each sentence would508Source LIV - ENG LIV - EST LIV - LAT   Dictionary examples ‚Äì 10 690 / 44 854 / 44 499 10 690 / 44 854 / 44 975   Latvian constitution 686 / 11 198 / 15 499 719 / 11 454 / 10 314 719 / 11 454 / 11 002   JEFUL abstracts ‚Äì 187 / 2 878 / 2 846 176 / 2 723 / 3 434   Facebook posts 231 / 2 759 / 3 656 8 / 124 / 122 232 / 2 744 / 2 738   livones.net texts 169 / 2 741 / 3 660 92 / 1 969 / 1 867 333 / 4 449 / 4 433   Stalte ABC book ‚Äì 1 340 / 9 382 / 9 195 1 340 / 9 382 / 9 398   Trilium , poetry book ‚Äì 222 / 3 543 / 3 321 223 / 3 512 / 3 539   Eduard V√§√§ri book ‚Äì 877 / 10 337 / 9 763 ‚Äì   Total 1 086 / 16 698 / 22 815 14 135 / 84 541 / 81 927 13 713 / 79 118 / 79 519   have all four manually veriÔ¨Åed translations .   The second half of our work focuses on neural   machine translation ( NMT , Vaswani et al . , 2017 ) ,   mainly targeting Livonian $ English . We explore   several options of coping with the extremely low-   resource settings and use Estonian and Latvian for   cross - lingual transfer . Our experiments answer the   following research questions :   1.Can we achieve machine translation for   Livonian $ English at a usable level ?   2.Which base language suits better for serving   as base for cross - lingual transfer to Livonian ,   Estonian or Latvian ?   3.Does zero - shot multilingual translation de-   liver better translation quality than pivot-   translation through Estonian or Latvian ?   Next we brieÔ¨Çy describe the Livonian Language   in Section 2 , then introduce the collected paral-   lel and monolingual data in Section 3 . Section 4   provides the details of our NMT experiments and   Section 5 concludes the paper .   2 The Livonian Language   Livonian ( ISO 639 - 3 : liv ) is a Finnic language   indigenous to Latvia and belonging to the Uralic   language family . During the 12th century Livonian   was spoken across great territories in Latvia around   the Gulf of Riga . Over time , Livonian areas gradu-   ally became Latvian - speaking . In the 19th century ,   Livonian still had approximately 2500 speakers , bythe mid-20th century around 1500 speakers . Nowa-   days Livonian is listed in UNESCO ‚Äôs Atlas of the   World ‚Äôs Languages in Danger as a critically endan-   gered language ( Moseley , 2014 ) . According to the   2011 census , there are 250 Livonians in Latvia . Al-   though there are just over 20 people who can speak   the language , the Livonian community is active in   preserving and developing the Livonian heritage   ( Ern≈°treits , 2016 ) and language plays a key role in   this process ( Ern≈°treits and Klava , 2020 ) .   The Livonian language developed in the contact   area of Baltic and Finnic languages . Livonian and   Latvian share a similar geographical location over   a prolonged period of time , as a result of which   they both contain traces of contact . Next to other   loanwords , the Livonian loanword strata consists   of words borrowed from Latvian ( Suhonen , 1973 ;   Winkler , 2014 ) and vice versa . The most obvious   Latvian inÔ¨Çuence on Livonian grammar is found   in the Livonian case system ( Ern≈°treits and K lava ,   2014 ) . Livonian has the prosodic characteristics   typical of a Finnic language such as word - initial   stress and the phonological opposition of short and   long phoneme duration . It is the only Finnic lan-   guage that differentiates lexical tones ‚Äì the plain   tone and the broken tone or st√∏d ‚Äì and therefore   shares similar characteristics with Latvian as well   as Danish ( Tuisk , 2016 ) .   3 Collected Data   The Ô¨Årst step in developing ( supervised ) machine   translation is collecting parallel data . While there   was no pre - existing open parallel corpus with Livo-   nian , we used all the possible sources of transla-   tions . This was limited to already digital resources ,   future work might include texts extracted by scan-   ning older books and other materials.509LV!EN ET!EN ETLV!EN EN - ET - LV Google Neurotolge   ET 30.91 28.42 24.17 34.38 29.91   LV 25.18 25.26 20.77 31.54 25.92   LIV 2.20 3.22 2.66 13.29 - -   Tuned   LIV!EN 3.19 5.59 5.39 14.69 - -   EN!LIV - - - 8.59 - -   The main sources of data included Livonian-   Latvian as well as Livonian - Estonian translations .   Thus we use these two languages as base for cross-   lingual transfer and e.g. leave Finnish out , as there   was no data for it .   The sources of data included :   ‚Ä¢the Constitution of the Republic of Latvia ,   translated into 9 languages , including Livo-   nian , Estonian and English ,   ‚Ä¢a database of dictionary entries , phrases and   example sentences from the University of   Latvia Livonian Institute ‚Äôs website , with ex-   ample sentences in Livonian , Estonian and   Latvian   ‚Ä¢the Livonian Institute ‚Äôs Facebook page posts ,   partially parallel between our 4 languages   ‚Ä¢books ( Stalte , 2011 ; Kurs and et al . , 2016 ;   Ern≈°treit et al . , 2020 ) with prefaces and con-   tent in Livonian - Estonian or Livonian - Latvian   ‚Ä¢and abstracts from the Journal of Estonian and   Finno - Ugric Linguistics ‚Äô ( JEFUL ) Special Is-   sues on Livonian Studies ( 2014 , 2016 , 2018 )   in Livonian , Estonian and English .   Concerning sentence alignment , the dictionary   examples consisted of already aligned Livonian   sentences . We aligned the rest of the data manu-   ally with the help of language experts ‚Äì Ô¨Årst on   paragraph level , then on sentence level . The result-   ing amount of sentences in the resulting dataset is   shown in Table 1 .   We separated balanced portions of development   ( 503 sentences ) and evaluation ( 749 sentences )   splits from the full dataset . The splits are balanced   in terms of the original source of the texts to resem-   ble proportions from the remaining training data . We hired professional translators to create trans-   lations for any missing parts so that these splits   would be parallel between all four languages . We   further turned to experts of the Livonian language   to make sure that the newly created translations   truly convey the meaning of the original text as a   quality control measure . The resulting benchmark   and the whole corpus is published in the OPUS col-   lection . We also share the Ô¨Ånal translation model   after four iterations of backtranslation .   4 Machine Translation Experiments   Having just over 10;000parallel examples consti-   tutes extremely low - resource settings for neural   machine translation . Added to this , the number   of monolingual Livonian sentences ( about 40;000 )   is also too small for approaches like unsupervised   machine translation ( Artetxe et al . , 2018 ; Lample   et al . , 2018 ) .   We implement the support of neighboring and   related languages ( Estonian and Latvian ) via multi-   lingual machine translation ( Johnson et al . , 2017 ) .   As a Ô¨Årst step the model is pre - trained with the   larger languages ( Estonian , Latvian , English ) and   then used as base for following experiments .   We also perform iterative back - translation ( Pin-   nis et al . , 2018 ) to make use of the large amounts of   monolingual news data in EN / ET / LV , and our lim-   ited amount of monolingual data in LIV . We trans-   late the 40k LIV sentences and different batches   of 200k sentences from the other languages into   all directions , Ô¨Ålter the translations using simple   heuristic Ô¨Ålters ( Rikters , 2018 ) , and use a mix of   all back - translated data with an equal amount of   random clean parallel data ( including all data in-   volving Livonian ) to Ô¨Åne - tune the base model.510Base Tuned BT1 BT2 BT3 BT4   ET - EN 24.17 23.68 23.97 24.80 25.05 26.17   LV - EN 20.77 18.90 19.29 20.95 20.52 21.53   LIV - EN 13.29 14.69 16.19 17.41 18.15 19.01   EN - ET 17.00 16.87 18.58 19.37 18.95 19.48   LV - ET 18.38 19.55 19.72 19.93 20.68 22.38   LIV - ET 15.08 17.76 20.05 21.61 21.78 23.05   EN - LV 16.57 17.94 17.17 19.58 19.49 20.85   ET - LV 18.51 21.16 20.92 21.01 21.96 23.44   LIV - LV 15.05 17.55 21.25 22.99 23.68 25.24   EN - LIV 4.19 8.59 9.96 10.49 10.88 11.03   ET - LIV 4.01 13.00 14.43 15.24 16.09 16.49   LV - LIV 4.84 13.67 15.18 16.25 16.77 17.65   4.1 Technical Setup   We used FairSeq ( Ott et al . , 2019 ) to train trans-   former architecture models with 6 encoder and   decoder layers , 8 transformer attention heads per   layer , word embeddings and hidden layers of size   512 , dropout of 0.3 , maximum sentence length   of 128 symbols , and a batch size of 1024 words .   All models were trained until they reached con-   vergence ( no improvement for 10 checkpoints ) on   development data . We used Sentencepiece ( Kudo   and Richardson , 2018 ) to create shared vocabular-   ies of size 25,000 , and SacreBLEU(Post , 2018 )   to generate BLEU scores ( Papineni et al . , 2002 ) for   translations .   Base models were trained on LV ! EN , ET ! EN ,   ET+LV ! EN data , and a multilingual model us-   ing the tagged approach ( Johnson et al . , 2017 ) for   translating in all directions between EN / ET / LV lan-   guages . The base models were then used as ini-   tialization for tuning on Livonian - English parallel   data .   For training the base models we used all avail-   able parallel data from Opus ( Tiedemann and Ny-   gaard , 2004 ) . To facilitate further use of the base   models for tuning on Livonian data , all Livonian   sentences were used in addition to other data when   creating the shared vocabularies . Finally , we used   the highest - scoring tuned model to perform per-   formed backtranslation on the monolingual LIV   data to generate additional training data for train-   ing the Ô¨Ånal models.4.2 Results   Table 2 shows the results of MT experiments . All   BLEU scores are calculated for translations of our   evaluation set . We compare the base single direc-   tion MT models to our multidirection model , as   well as online translations from Google Translate   and Neurotolgeto evaluate performance from ET   and LV into EN . While the multilingual model   was noticeably weaker , the others hold compara-   ble results to the online systems . However , when   attempting to perform zero - shot translation from   LIV into EN , ET ! EN outperforms LV ! EN ( 3.22   vs. 2.20 ) , and the multilingual model achieved a   very respectable BLEU score 13.29 .   We then turned to tuning each of these mod-   els with LIV - EN data mixed 1:1 with a random   equal amount of the original training data for each   of the models . In the case of the multilingual   model , we also added LV / ET - LIV data to the mix .   This improved all scores by 1 - 3 BLEU points ,   but the multilingual model remained on top with   14.69 for LIV ! EN . In order to perform back-   translation models for both directions are required ,   so we scored the tuned multilingual model on the   EN!LIV data as well , reaching 8.59 BLEU .   For comparison we also used the same tuned   multilingual model to perform pivotal translation   by Ô¨Årst translating into ET or LV and then into   the desired target language . In all four cases the   pivot translation quality dropped when compared   to direct translation by the same model , so we did   not further pursue this line of experiments . An511LIV - EN EN - LIV   Facebook 19.28 13.55   Livones.net 19.67 15.91   Dictionary 7.73 10.60   Trilium 19.88 14.50   Stalte 13.88 9.47   JEFUL 8.02 5.10   Satversme 24.49 7.69   interesting observation , was that pivoting through   ET achieved a higher BLEU score than LV when   translating into EN ( 13.66 vs. 11.24 ) , but slightly   lower when translating into LIV ( 7.99 vs. 8.56 ) .   Results for four rounds of BT iterations are com-   piled in Table 3 . The model clearly improves not   only in the main language pair of EN $ LIV , but in   all other translation directions as well .   To answer the research questions , posed in the   introduction , it seems that the resulting transla-   tion quality is still far from being usable . Com-   parisons between the base languages have shown   slight preference towards Estonian over Latvian .   Pivot - translation trough Estonian or Latvian un-   derperforms direct Livonian $ English translation   trained in a zero - shot / few - shot manner .   4.3 Detailed Analysis   Table 4 shows BLEU scores of the separate parts   of the evaluation corpus . Since most of the training   data for EN - LIV comes from Satversme ( Latvian   Constitution ) , it is very clear why that part scores   higher than others . The dictionary entries are over-   all far shorter in length than the other parts and   often consist of few - word phrases , making them   unfavorable to BLEU by deÔ¨Ånition .   The posts from Facebook and Livones.net are   more general in their language and therefore more   similar to data from the training set . However , the   Trilium and Stalte books are written in a more liter-   ary language , making them slightly more challeng-   ing to translate . Finally , the very domain - speciÔ¨Åc   part from JEFUL abstracts seems to be the most   difÔ¨Åcult to translate into English .   5 Conclusion   In this paper we presented a novel dataset for the   highly endangered Livonian language , which canbe useful for machine translation , language mod-   elling and many other natural language processing   and computational linguistic research tasks .   In our experiments we show how far one can   get in training modern machine translation models   with very scarce data , and which languages are   more suitable for transfer learning when working   with Livonian data . While perhaps not being usable   as - is in any kind of production scale , the achieved   Ô¨Ånal BLEU scores of 19.01 for Livonian ! English   and 11.03 for English ! Livonian show that some   transfer of meaning can still be achieved with the   currently available resources .   In the future we are planning to experiment with   cross - lingual transfer from other languages , like   the resource - rich Finnish as well as resource - poor   Finno - Ugric languages like V√µru and Sami ( Tars   et al . , 2021 ) . Given the limited amount of exist-   ing monolingual Livonian data , generating syn-   thetic Livonian data with other means besides back-   translation might be helpful : for example , forward-   translation or using GPT - like language models .   Finally , work on the already collected Livonian   monolingual and parallel data is ongoing at the   Institute of the Livonian Language . Adding En-   glish translations to the lexical items and example   sentences is an ongoing effort and will evaluate   in practice , if the MT systems created as part of   the current work can facilitate that . One of the   key focuses is also manually verifying the data and   making sure the existing corpus contains correct   Livonian texts and their translations   Acknowledgements   This work has received funding from the ‚Äú European   Social Fund via IT Academy programme , ‚Äù Esto-   nian Ministry of Education and Research ( grant   SHVEE21397 ) , the Estonian Research Council   ( grant GHVEE22112J ) , and the State Research Pro-   gramme ‚Äú Latvian Studies for the Development of   a Latvian and European Society ‚Äù project ‚Äú Mul-   tifunctional dictionary of Livonian ‚Äù ( No . VPP-   LETONIKA-2021/2 - 0002 ) .   References512513514   Dongwon Kelvin RyuEhsan ShareghiMeng Fang   Yunqiu XuShirui PanGholamreza HaffariDepartment of Data Science & AI , Monash UniversityEindhoven University of TechnologyUniversity of Technology SydneyLanguage Technology Lab , University of Cambridge   firstname.lastname@monash.edu m.fang@tue.nl   yunqiu.xu@student.uts.edu.au   Abstract   1 Introduction   Text - based games ( TGs ) are environments where   agents learn to comprehend situations in language   and produce decisions in language ( Hausknecht   et al . , 2020 ; C√¥t√© et al . , 2018 ; Narasimhan et al . ,   2015 ) . Deep Reinforcement Learning lends itself   as a natural paradigm to solve TGs due to its ability   to learn from unsupervised game playing experi-   ence . However , existing RL agents are far away   from solving TGs due to their combinatorially large   action spaces that hinders efficient exploration ( Yao   et al . , 2020 ; Ammanabrolu and Hausknecht , 2020 ) .   Ammanabrolu and Riedl ( 2019 ) ; Ammanabrolu   and Hausknecht ( 2020 ) proposed incorporating a   belief knowledge graph ( BKG ) built from the tex-   tual observations to help the agent reason moreeffectively about observed objects during the game-   play . Most of the recent works neglected linguis-   tic aspects of TGs and focused on the construc-   tion and utilisation of BKG ( Adhikari et al . , 2020 ;   Dambekodi et al . , 2020 ; Xu et al . , 2020 ; Am-   manabrolu et al . , 2020 ; Xu et al . , 2021 ) . Some   exceptions involve developing pre - trained language   models ( LMs ) to propose action candidates for a   given observation ( Yao et al . , 2020 ) , and investigat-   ing the relationship between semantic coherence   and state representations ( Yao et al . , 2021 ) .   In parallel , it has been argued that recent pre-   trained LMs capture commonsense factual knowl-   edge about the world ( Petroni et al . , 2019 ; Kassner   et al . , 2021 ; Meng et al . , 2021 ) . More direct at-   tempt in this direction was the commonsense trans-   former ( COMT ) which is a LM fine - tuned explic-   itly on commonsense knowledge graph ( CSKG ) ,   to explicitly generate commonsense inferences   ( Bosselut et al . , 2019 ; Hwang et al . , 2021 ) . Prior   works with commonsense focused on complet-   ingBKG using pre - defined CSKG ( Murugesan   et al . , 2020 ) or dynamic COMT - generated com-   monsense inferences ( Dambekodi et al . , 2020 ) .   Nonetheless , there is no work on explicitly using   commonsense as an inductive bias in the context   of exploration for TGs .   To bridge the gap , we propose commonsense ex-   ploration ( C E ) which constructs a CSKG   dynamically , using COMT , based on the state of   textual observation per step . Then , the natural lan-   guage actions are scored with COMTand agent ,   to re - rank the policy distributions . We refer to this   as applying commonsense conditioning . However ,   doing this throughout the whole training is expen-   sive and may not be beneficial as gameplay is not   led by commonsense . To rectify this , we propose   anentropy scheduler , driven by the entropy of the   policy distribution , to regulate applying common-   sense conditioning .   We demonstrate that our method encourages515   the agent to achieve higher game score during   the training in four out of nine games in Jeri-   cho ( Hausknecht et al . , 2020 ) . Furthermore , we   show our method leads to producing more human-   like natural language action . This is measured us-   ing the perplexity of the generated actions accord-   ing to GPT-2 ( Radford et al . , 2019 ) . We believe   that natural language coherency / fluency is a crucial   aspect of interactive intelligent agents ( e.g. robots   and dialogue systems ) and hope our promising find-   ings facilitate further developments of methods in   this direction .   2 Approach   Notations . Text - based games are modelled as a   partially observable Markov decision processes   ( POMDPs ) of a tuple of ‚ü®S , A , P , O,‚Ñ¶,R , Œ≥‚ü© ,   whereS , A,‚Ñ¶denote sets of states , actions , and ob-   servations , respectively . Also , RandŒ≥denote the   reward function and the discount factor , while P   andOdenote the transition probabilities and set of   conditional observations probabilities , respectively .   The agent requires to map an observation to a   state ( ‚Ñ¶‚Üí S ) and produce a policy œÄ . By se-   lecting an action afrom the policy œÄ , the agent   changes current state s , receives a reward sig-   nalr , receives an observation through transition   P(s|s , a ) , and also receives a conditional ob-   servation O(|s ) . The agent learns the policy   œÄ(a|o)that maximizes the expectation of the cu - mulative reward function EPŒ≥r(s , a)   .   2.1 CSKG Construction   Let a CSKG be a graph K= ( V , E ) , where Vis   a set of nodes or vertices and Eis a set of edges .   The root node of CSKG requires to carry adequate   information about the gameplay , so we amend the   input to be the same format as how COMTis   trained on , v=‚ÄúI ‚Äù + a+ ‚Äú . ‚Äù + oand replace   all the ‚Äú I ‚Äù to ‚Äú PersonX ‚Äù . To build CSKG we use   COMTat every step of gameplay as a frozen   commonsense generator to produce the tail node   vgiven the head node vand edge eat time   stept , formally denoted as Pr(v|v , v , e ) .   Figure 1(Right ) provides a visualisation of this .   COMTtakes vas a head node and eas an   edge and produces vwith the corresponding node-   to - node score œï. Multiple tail nodes and   node - to - node scores can be generated through the   same input and based on the edge , the tail nodes   vary dramatically . This process can be applied   recursively to the tail nodes , expanding CSKG , i.e.   generate tail nodes given vhead node with e.   See Appendix A for more details .   2.2 Commonsense Conditioning   To blend commonsense into the agent ‚Äôs decision ,   the log - likelihood score is employed to contem-   plate each component independently . We , then ,   compute the total score as a weighted sum to pro-   mote the natural language action.516Agent - to - Action Score . The score function for the   gameplay is obtained from the agent ,   œï=1   |a|XlogœÄ(a|a , o ) ,   where œïis the agent - to - action score for kaction ,   computed as the sum of log - likelihood of the natu-   ral language action . Intuitively , the agent - to - action   score signifies how much the action directs to the   reward signals . This is learned during the online   training of the agent .   Node - to - Action Score . Inspired by Bosselut et al .   ( 2021 ) ; Yasunaga et al . ( 2021 ) , the commonsense   level of actions for each generated node is mea-   sured using COMT ,   œï=1   |a|Xlog Pr(a|a , v , e ) ,   œï= max(œï , œï , ¬∑ ¬∑ ¬∑ ) ,   where œïis the score per vaedge , e‚àà E ,   while the node - to - action score is denoted by œï   which is the maximum œïovervaedges . The   node - to - action score intersects commonsense with   action , implying how plausible the action is given   the commonsense prediction .   Node - to - Node Score . Additionally , we adopted   the score between nodes in CSKG from Bosselut   et al . ( 2021 ) ,   œï=1   |v|Xlog Pr(v|v , v , e ) ,   œï= max(œï , œï , ¬∑ ¬∑ ¬∑ , œï , ¬∑ ¬∑ ¬∑ ) ,   where œïis the score per head node and vv   edges , e‚àà E , while the node - to - node score   isœï,max ofœïover head nodes and vv   edges . The node - to - node score is designed to pro-   mote commonsense triples that are more sensible   commonsense - wise .   Total Score . The total score assigned for each   action is computed as :   œï= max(Œ≥œï+Œ≥œï+Œ≥œï),(1 )   where œïis the total score per action since max is   over nodes . The Œ≥coefficients are hyperparameters   and balance the weights between different compo-   nents of the scoring function . Finally , the new con-   ditioned policy is obtained as softmax ( œï ) . We   refer to this whole process as commonsense con-   ditioning . A visualisation of the overall model is   provided in the Figure 1(Left ) .   Intuitively , when the agent is not confident in   current time - step , the policy distribution is arbi-   trary , resulting in homogeneous œï. This would   be specifically the case during the initial stage of   the training , but can also occur at any stage of the   game where the agent can not predict reward signal   in a small number of steps . Under these circum-   stances , œïwould be more dictated by œïandœï.   Conversely , when the agent is confident , the œïfor   different actions will diverge and œïwill be directed   by both commonsense and the agent .   2.3 Entropy Scheduler   Since our technique uses a large LM for natural   language generation , the main drawback with our   approach is computational costs . In addition to this ,   where the agent is confident about acquiring the   game score for a given action , commonsense could   act as an undesired noise . To reflect on these , we   propose the entropy scheduler to apply common-   sense conditioning based on the confidence , the   relative entropy of policy distribution . We collect   the last 1000 number of the entropy of the template   policy and apply commonsense conditioning if the   current entropy is higher than the median . Figure 2   visualizes how the entropy scheduler works during   training . This suggests that our entropy scheduler   with a median threshold can apply commonsense   conditioning to those actions with zero or negative517   reward signals .   3 Experiments   We use KG - A2C as our goal - driven baseline agent   and compare it with KG - A2C with commonsense   in a game suite of Jericho . A set of nine games are   selected from Jericho carefully based on genre , in-   cluding three daily puzzle games ( library , lu-   dicorp , reverb ) and the rest six fantasy adven-   ture games ( balances , enchanter , spirit ,   zork1 , zork3 , ztuu ) . Both game setting   and optimal configuration for KG - A2C in Am-   manabrolu and Hausknecht ( 2020 ) were used in   our experiments . We reduced training steps to   25,000since our objective is to compare the qual-   ity of exploration during the training . Only hyper-   parameters in C E have been optimized   for fair comparison while all the parameters in   COMTwere fixed during the training , resulting in   the equal trainable parameters regardless of C - E. Details of the hyper - parameters and the   experimental setup can be found in Appendix B.   3.1 Main Results   Similar to Ammanabrolu and Hausknecht ( 2020 ) ,   we employed the optimal hyper - parameters fine-   tuned on zork1 for nine games in Jericho . Table   1 shows the mean score across the entire training   and the perplexity of the action given a root node .   The score is to compare whether the agent with   commonsense achieves higher game score during   the training . Doing so implies how fast the agent   learns with fewer steps , and therefore , more effi-   cient exploration . Perplexity from LM is used as   a metric for the smoothness of natural language   action . We used GPT-2 from Huggingface ( Wolf   et al . , 2020 ) .   Score Table 1 shows that with C E ,   the agent tends to acquire the game score more   frequent in four gaming environments ( spirit ,   zork1 , zork3 , ztuu ) . All four have at least   15 % increases in game score during training .   However , three environments ( balances , en-   chanter , ludicorp ) appear to gain no benefits   from using C E. On the other hand , the   remaining two games ( library , reverb ) take   commonsense negatively , suggesting that the com-   monsense from COMTacts as a noise with re-   spect to pursuing rewards . Per genre , interestingly ,   those daily puzzle games are either not influenced   or negatively influenced from commonsense induc-   tive bias while four out of six fantasy adventure   games benefited from it . We speculate this might   be due to the fine - tuning which was also done on a   single game , zork1 .   Coherency Table 1 shows that commonsense   prior reduces perplexity of the natural language   actions in all nine games . This is because , unlike   the game score that is not directly related to com-   monsense , the semantic properties of the actions   are directly related to commonsense . For envi-   ronments like balances andreverb , despite   the agent taking no benefits from commonsense,518   perplexity drops significantly ( e.g. ,‚àº15 % ) . This   large reduction in perplexity also appears for fan-   tasy games , in which zork3 had‚àº20 % down and   spirit took as little as ‚àº3 % reduction . This   suggests that the game takes advantages on the se-   mantic coherency regardless of whether it helps to   achieve high score of the game or the genre of the   game .   Qualitative Samples Table 2 provides qualita-   tive samples to show how natural language ac-   tions are re - ordered after commonsense condi-   tioning . For instance , in the first example of   zork1 , C E suppresses open brown   and pushes put glass on table to the high-   est probability . In zork3 , C E promotes   turn on lamp over others since the observa-   tion informs user that the surrounding is dark .   3.2 Ablation Results   We performed two ablation studies on zork1   to obtain the optimal hyper - parameters . The   first ablation study is for the absence of features ,   in which we removed CSKG construction and   entropy scheduler completely . Thereafter , the   changes in score gamma factors have been in-   vestigated . The Œ≥coefficients are changed from   ( Œ≥= 1 , Œ≥= 0.7 , Œ≥= 0.8)to(0.4,0.2,1)for   ( v < a)model and ( 1,1,0.3)for(v > a)model .   Feature Figure 3 ( Left ) shows that the absence   ofCSKG construction or entropy scheduler causes   catastrophic forgetting . KG - A2C is prone to this   regardless of commonsense because it does not   use any memory component . However , injecting   commonsense stochastically enhances the likeli-   hood since the agent follows commonsense when   it should not , i.e. a particular action is required to   obtain game score . This overlaps with our motiva-   tion of entropy scheduler , that the game score is not   directly related to commonsense , so appropriateskipping is necessary .   Dynamic CSKG contributes to a variety of com-   monsense , amplifying its commonsense reasoning ,   and a lack of this will provoke the agent acting   more narrow with limited commonsense . Our plot   shows that removing CSKG also contributes to   the cause of catastrophic forgetting . This suggests   that lack of diversity in commonsense may act as a   noise to the exploration , and may push the agent to   produce more skewed trajectories that cause failure .   Therefore , the absence of any component leads to   performance decay . Therefore , both are vital com-   ponents in C E.   Score Gamma Factor The contribution of the   commonsense and the agent score is investigated   on Figure 3 ( Right ) . By increasing agent ‚Äôs gamma   factor , the model acts more alike to the baseline   than the optimal hyper - parameters since it trusts its   own policy more . Conversely , adding more weights   on commonsense leads to catastrophic forgetting .   This is caused by the fact that the agent puts too   much trust on commonsense , diverging from its   own policy excessively . From these , we can con-   clude that the appropriate balancing is required to   make exploration efficient and feasible .   4 Conclusion   We investigated the effect of commonsense in text-   based RL agent during the training . Our results   show that despite the hyper - parameters tuning on a   single game , the proposed approach improves on   other gaming environments in Jericho , total four   out of nine . Furthermore , injecting commonsense   also positively influences the semantics of natural   language actions , resulting in lower perplexity . Our   future work will extend its application to different   text - based environments and investigate how this   linguistic properties from LM helps the agent.519References520A CSKG Construction   There are three different strategies for building the   root node from the textual observation and the natu-   ral language action . The most generic one is , given   a=‚Äúmove rug " ando=‚ÄúWith a great effort ,   the rug is moved to one side of the room , revealing   the dusty cover of a closed trap door . ‚Äù , the root node   isv=‚ÄúPersonX ‚Äù + a+ ‚Äú . ‚Äù + o=‚ÄúPersonX   move rug . With a great effort , the rug is moved to   one side of the room , revealing the dusty cover of   a closed trap door . ‚Äù . The example of CSKG with   vis in Figure A.1 .   However , if the previous action awas not   admissible , we set the room description of the   textual observation as the root node . Finally , if   the action is admissible , but the observation is   too short ( less than 20 tokens ) , the root node in-   cludes the previous room description of the textual   observation at the beginning of the page , v=   o+ ‚Äú PersonX ‚Äù + a+ ‚Äú . ‚Äù + o.   These are motivated from 1 ) if the previous ac-   tion is not admissible , the environment is not af-   fected by it , so we simply use the previous room   description that captures a lot of information about   what the agent can do , 2 ) if the observation is too   short that it does not carry enough information   about the situation , we concatenate the previous   room description to subjoin the information about   surroundings , and 3 ) otherwise , the generic strat-   egy to build the root node , the previous action and   the consequence of it as textual observation .   B Experiment Setup   Action Sampling We set n to be dy-   namic , only selecting those based on the probability   threshold and validity . The threshold is calculated   as 0.75 of its uniform distribution . For instance ,   zork1 contains 237 number of TEMPLATE , so the   threshold is 0.75√ó= 0.00316 . We only se-   lect the maximum of 7 TEMPLATE that exceeds   the threshold . This avoids a large shift in policy   distribution while attaining better computational ef-   ficiency . Additionally , we include valid templates   to enforce the agent to act more towards on chang-   ing the world tree . We sampled objects like KG-   A2C since KG - A2C already restricts objects and   the actions are usually determined by the template .   Therefore , |œï|=n , reducing the compu-   tations but still covering useful action sets .   Commonsense Transformer Our COMTis521   BART fine - tuned on A -2020 dataset , which   is crowdsourced with natural language sentence   nodes and 23 commonsense edges ( Hwang et al . ,   2021 ) . We assumed that the general COMTis   still good enough to cover TGs . Since the gam-   ing environment runs by the player character , we   only focus on the social - interaction commonsense .   ‚Äú xNeed " and ‚Äú xIntent " are chosen for CSKG con-   struction , E , since they deal with what is needed   or intended for the event to occur , while ‚Äú xWant "   and ‚Äú xEffect " for scoring the natural language ac-   tions , E , since they deal with what the player   would do following the event . We further set   n= 1 andn= 2 from the observation that   they are good enough for zero - shot commonsense   question answering ( Bosselut et al . , 2021 ; Moghim-   ifar et al . , 2020 ) . During the online training of the   agent , we freeze the parameters for COMT .   C Computational Expense   The number of node - to - node scores is directly re-   lated to the size of CSKG ,   |œï|=X(n√ó |E| ) ,   where nis the number of hops , nis the num-   ber of triple generation and Eis the edge space   for CSKG .   On the other hand , the number of node - to - action   scores is equal to the number of the total score œï ,   |œï|=|œï|=|œï| √ó |E| √ó |œï| ,   where Eis the edge space for node - to - action   score . We assume |œï| ‚âà7since we select maximum   of 7 templates with highest probability and valid   templates . Therefore , in our setting , we can calcu-   late the number of the natural language generations   per step per environment as ,   |œï|+|œï|=|œï|+|œï| √ó |E| √ó |œï|   = |œï| ¬∑ ( 1 + |E| √ó |œï| )   ‚âàX(2√ó2)¬∑(1 + 2 √ó7 )   = 75   Finally , we can estimate the average number of   natural language generation per step by multiplying   the number of environments per step n= 32 and   fraction from entropy scheduler p‚âà0.5 ,   ( |œï|+|œï|)√ón√óp‚âà75√ó32√ó0.5   = 1200   Throughout the training , we require to perform   1200 natural language generations using a large   sizeCOMTper step , so this increases the training   time from √ó3upto√ó10.522   Deming Ye , Yankai Lin , Peng Li , Maosong Sun , Zhiyuan LiuDept . of Comp . Sci . & Tech . , Institute for AI , Tsinghua University , Beijing , ChinaBeijing National Research Center for Information Science and TechnologyInternational Innovation Center of Tsinghua University , Shanghai , ChinaJiangsu Collaborative Innovation Center for Language Ability , Xuzhou , ChinaInstitute Guo Qiang , Tsinghua UniversityPattern Recognition Center , WeChat AIInstitute for AI Industry Research ( AIR ) , Tsinghua University   yedeming001@163.com   Abstract   1 Introduction   Recent advance in pre - trained language models   ( PLMs ) has achieved promising improvements in   various downstream tasks ( Devlin et al . , 2019 ; Liu   et al . , 2019 ) . Some latest works reveal that PLMs   can automatically acquire knowledge from large-   scale corpora via self - supervised pre - training and   then encode the learned knowledge into their model   parameters ( Tenney et al . , 2019 ; Petroni et al . ,   2019 ; Roberts et al . , 2020 ) . However , due to the   limited capacity of vocabulary , existing PLMs face   the challenge of recalling the factual knowledge   from their parameters , especially for those rare en-   tities ( Gao et al . , 2019a ; Wang et al . , 2021a ) .   To improve PLMs ‚Äô capability of entity under-   standing , a straightforward solution is to exploit   an external entity embedding acquired from the   knowledge graph ( KG ) ( Zhang et al . , 2019 ; Liu   et al . , 2020 ; Wang et al . , 2020 ) , the entity descrip-   tion ( Peters et al . , 2019 ) , or the corpora ( P√∂rner   et al . , 2020 ) . In order to make use of the ex-   ternal knowledge , these models usually learn to   align the external entity embedding ( Bordes et al . ,   2013 ; Yamada et al . , 2016 ) to the their original   word embedding . However , previous works ignore   to explore entity embedding from the PLM itself ,   which makes their learned embedding mapping is   not available in the domain - adaptation . Other re-   cent works attempt to infuse knowledge into PLMs ‚Äô   parameters by extra pre - training , such as learning   to build an additional entity vocabulary from the   corpora ( Yamada et al . , 2020 ; F√©vry et al . , 2020 ) , or   adopting entity - related pre - training tasks to inten-   sify the entity representation ( Xiong et al . , 2020 ;   Sun et al . , 2020 ; Wang et al . , 2021b ) . However ,   their huge pre - computation increases the cost of   extending or updating the customized vocabulary   for various downstream tasks .   In this paper , we introduce a simple but effec-   tivePluggable Entity Lookup Table ( PELT ) to in-   fuse knowledge into PLMs . To be speciÔ¨Åc , we   Ô¨Årst revisit the connection between PLMs ‚Äô input   features and output representations for masked lan-   guage modeling . Based on this , given a new corpus ,   we aggregate the output representations of masked   tokens from the entity ‚Äôs occurrences , to recover523an elaborate entity embedding from a well - trained   PLM . BeneÔ¨Åting from the compatibility and Ô¨Çex-   ibility of the constructed embedding , we can di-   rectly insert them into the corresponding positions   of the input sequence to provide supplemental en-   tity knowledge . As shown in Table 1 , our method   merely consumes 0.2 % 5 % pre - computation com-   pared with previous works , and it also supports the   vocabulary from different domains simultaneously .   We conduct experiments on two knowledge-   related tasks , including knowledge probe and rela-   tion classiÔ¨Åcation , across two domains ( Wikipedia   and biomedical publication ) . Experimental results   show that PLMs with PELT can consistently and   signiÔ¨Åcantly outperform the corresponding vanilla   models . In addition , the entity embedding obtained   from multiple domains are compatible with the   original word embedding and can be applied and   transferred swiftly .   2 Methodology   In this section , we Ô¨Årst revisit the masked language   modeling pre - training objective . After that , we   introduce the pluggable entity lookup table and   explain how to apply it to incorporate knowledge   into PLMs .   2.1 Revisit Masked Language Modeling   PLMs conduct self - supervised pre - training tasks ,   such as masked language modeling ( MLM ) ( De-   vlin et al . , 2019 ) , to learn the semantic and syntac-   tic knowledge from the large - scale unlabeled cor-   pora ( Rogers et al . , 2020 ) . MLM can be regarded   as a kind of cloze task , which requires the model to   predict the missing tokens based on its contextual   representation . Formally , given a sequence of to-   kensX= ( x;x;:::;x ) , withxsubstituted by   [ MASK ] , PLMs , such as BERT , Ô¨Årst take tokens ‚Äô   word embedding and position embedding as input   and obtain the contextual representation :   H = Enc(LayerNorm ( E(X ) + P));(1 )   where Enc ( )denotes a deep bidirectional Trans-   former encoder , LayerNorm ( )denotes layer nor-   malization ( Ba et al . , 2016 ) , E2Ris the   word embedding matrix , Vis the word vocabu-   lary , Pis the absolute position embedding and   H= ( h;h;:::;h)is the contextual represen-   tation . After that , BERT applies a feed - forward   network ( FFN ) and layer normalization on the con-   textual representation to compute the output repre-   sentation of x :   r = LayerNorm ( FFN(h ) ): ( 2 )   Since the weights in the softmax layer and word   embeddings are tied in BERT , the model calculate   the product of rand the input word embedding   matrix to further compute x ‚Äôs cross - entropy loss   among all the words :   L= X   logPr(xjr )   =  X   logexp ( E(x)r)Pexp ( E(w)r):(3 )   2.2 Construct Pluggable Entity Embedding   Due to the training efÔ¨Åciency , the vocabulary sizes   in existing PLMs typically range from 30 K to 60 K   subword units , and thus PLMs have to disperse the   information of massive entities into their subword   embeddings . Through revisiting the MLM loss in   Eq . 3 , we could intuitively observe that the word   embedding and the output representation of BERT   are located in the same vector space . Hence , we are   able to recover the entity embedding from BERT ‚Äôs   output representations to infuse their contextual-   ized knowledge to the model .   To be speciÔ¨Åc , given a general or domain-   speciÔ¨Åc corpus , we design to build the lookup table   for entities that occurs in the downstream tasks on   demand . For an entity e , such as a Wikidata entity   or a proper noun entity , we construct its embedding   E(e)as follows :   Direction A feasible method to add entity eto   the vocabulary of PLM is to optimize its embed-   ding E(e)for the MLM loss with other parameters   frozen . We collect the sentences Sthat contain   entityeand substitute it with [ MASK ] . The total   inÔ¨Çuence of E(e)to the MLM loss in Scan be   formulated as :   L(e ) =  XlogPr(ejr )   = XlogZ E(e)Xr;(4)524whereZ = Pexp ( E(w)r),x   is the replaced masked token for entity eandris   the PLM ‚Äôs output representation of x.   Compared with the total impact of the entire   vocabulary on Z , E(e)has a much smaller impact .   If we ignore the minor effect of E(e)onZ , the   optimal solution of E(e)forL(e)is proportional   toPr . Hence , we set E(e)as :   E(e ) = CXr ; ( 5 )   whereCdenotes the scaling factor .   Practically , E(e)also serves as the negative log-   likelihood of other words ‚Äô MLM loss ( Kong et al . ,   2020 ) . However , Gao et al . ( 2019a ) indicates that   the gradient from such negative log - likelihood will   push all words to a uniformly negative direction ,   which weakens the quality of rare words ‚Äô represen-   tation . Here , we ignore this negative term and ob-   tain the informative entity embedding from Eq . 5 .   Norm We deÔ¨Åne p(e)as the position embedding   for entitye . Since the layer normalization in Eq . 1   makes the normjE(e ) + p(e)jtoD , we Ô¨Ånd that   the normjE(e)jhas little effect on the input feature   of the encoder in use . Therefore , we set the norm   of all the entity embeddings as a constant L. Then ,   we evaluate the model with different Lon the un-   supervised knowledge probe task and choose the   bestLfor those Ô¨Åne - tuning tasks .   2.3 Infuse Entity Knowledge into PLMs   Since the entity embedding we obtained and the   original word embedding are both obtained from   the masked language modeling objective , the entity   can be regarded as a special input token . To infuse   entity knowledge into PLMs , we apply a pair of   bracket to enclose the constructed entity embed-   ding and then insert it after the original entity ‚Äôs   subwords . For example , the original input ,   Most people with COVID-19 have a dry   [ MASK ] they can feel in their chest .   becomes   Most people with COVID-19 ( COVID-19 ) have   a dry [ MASK ] they can feel in their chest .   Here , the entity COVID-19 adopts our constructed   entity embedding and other words use their original   embedding . We simply convey the modiÔ¨Åed input   to the PLM for encoding without any additional   structures or parameters , to help the model predict   [ MASK ] ascough .A note on entity links In previous section , we   hypothesize that we know the entity linking annota-   tions for the involved string name . In practice , we   can obtain the gold entity links provided by some   datasets like FewRel 1.0 . For the datasets where the   linking annotations are not available , we employ a   heuristic string matching for entity linking .   3 Experiment   3.1 Implementation Details   We choose RoBERTa ( Liu et al . , 2019 ) , a well-   optimized PLM , as our baseline model and we   equip it with our constructed entity embedding to   obtain the PELT model . For the knowledge probe   task , we further experiment with another encoder-   architecture model , uncased BERT ( Devlin   et al . , 2019 ) , and an encoder - decoder - architecture   model , BART ( Lewis et al . , 2020 ) .   We adopt Wikipedia and biomedical S2ORC ( Lo   et al . , 2020 ) as the domain - speciÔ¨Åc corpora and   split them into sentences with NLTK ( Xue , 2011 ) .   For Wikipedia , we adopt a heuristic entity link-   ing strategy with the help of hyperlink annota-   tions . For the used FewRel 1.0 and Wiki80 datasets ,   we directly use the annotated linking informa-   tion . For other datasets , we link the given entity   name through a simple string match . For each   necessary entity , we Ô¨Årst extract up to 256 sen-   tences containing the entity from the corpora . We   adopt Wikipedia as the domain - speciÔ¨Åc corpus for   FewRel 1.0 , Wiki80 and LAMA , and we adopt   S2ORC as the domain - speciÔ¨Åc corpus for FewRel   2.0 . After that , we construct the entity embedding   according to Section 2.2 .   We search the norm of entity embedding L   among 1 - 10 on the knowledge probe task . We Ô¨Ånd   L= 7;10;3performs a bit better for RoBERTa ,   BERT and BART respectively . In the Ô¨Åne - tuning   process , we freeze the constructed embeddings as   an lookup table with the corresponding norm . After   that , we run all the Ô¨Åne - tuning experiments with 5   different seeds and report the average score .   3.2 Baselines   We select three of the most representative entity-   aware baselines , which adopt an external entity   embedding , an entity - related pre - training task , or   a trainable entity embedding : ( 1 ) ERNIE ( Zhang   et al . , 2019 ) involves the entity embedding learned   from Wikidata relation ( Bordes et al . , 2013 ) . We525   adopt the RoBERTa version of ERNIE provided   by Wang et al . ( 2021b ) ; ( 2 ) KEPLER ( Wang   et al . , 2021b ) encodes textual entity description   into entity embedding and learns fact triples and   language modeling simultaneously ; ( 3 ) LUKE ( Ya-   mada et al . , 2020 ) learns a trainable entity embed-   ding to help the model predict masked tokens and   masked entities in the sentences .   3.3 Relation ClassiÔ¨Åcation   Relation ClassiÔ¨Åcation ( RC ) aims to predict the   relationship between two entities in a given text .   We evaluate the models on two scenarios , the few-   shot setting and the full - data setting .   The few - shot setting focuses on long - tail rela-   tions without sufÔ¨Åcient training instances . We eval-   uate models on FewRel 1.0 ( Han et al . , 2018 ) and   FewRel 2.0 ( Gao et al . , 2019b ) . FewRel 1.0 con-   tains instances with Wikidata facts and FewRel   2.0 involves a biomedical - domain test set to ex-   amine the ability of domain adaptation . In the   N - wayK - shot setting , models are required to cat-   egorize the query as one of the existing Nrela-   tions , each of which contains Ksupporting sam-   ples . We choose the state - of - the - art few - shot frame-   work Proto ( Snell et al . , 2017 ) with different PLM   encoders for evaluation . For the full - data setting ,   we evaluate models on the Wiki80 , which contains   80 relation types from Wikidata . We also add 1 %   and 10 % settings , meaning using only 1 % / 10 %   data of the training sets .   As shown in Table 2 and Table 3 , on FewRel   1.0 and Wiki80 in Wikipedia domain , RoBERTa   with PELT beats the RoBERTa model by a large   margin ( e.g. +3.3 % on 10way-1shot ) , and it even   achieves comparable performance with ERNIE ,   which has access to the knowledge graph . Our   model also gains huge improvements on FewRel   2.0 in the biomedical domain ( e.g. + 7:1%on   10way-1shot ) , while the entity - aware baselines   have little advance in most settings . Compared with   most existing entity - aware PLMs which merely ob-   tain domain - speciÔ¨Åc knowledge in the pre - training   phase , our proposed pluggable entity lookup table   can dynamically update the models ‚Äô knowledge   from the out - of - domain corpus on demand .   3.4 Knowledge Probe   We conduct experiments on a widely - used knowl-   edge probe dataset , LAMA ( Petroni et al . , 2019 ) .   It applies cloze - style questions to examine PLMs ‚Äô   ability on recalling facts from their parameters . For   example , given a question template Paris is the cap-   ital of [ MASK ] , PLMs are required to predict the   masked token properly . In this paper , we not only526   use Gooogle - RE and T - REx ( ElSahar et al . , 2018 )   which focus on factual knowledge , but also evalu-   ate models on LAMA - UHN ( P√∂rner et al . , 2020 )   which Ô¨Ålters out the easy questionable templates .   As shown in Table 4 , without any pre - training ,   the PELT model can directly absorb the entity   knowledge from the extended input sequence to   recall more factual knowledge , which demonstrates   that the entity embeddings we constructed are com-   patible with original word embeddings . We also   Ô¨Ånd that our method can also bring huge improve-   ments to both BERT and BART in the knowledge   probe task , which proves our method ‚Äôs generaliza-   tion on different - architecture PLMs .   Effect of Entity Frequency Table 5 shows the   P@1 results with respect to the entity frequency .   While RoBERTa performs worse on rare entities   than frequent entities , PELT brings a substantial   improvement on rare entities , i.e. , near 3.8 mean   P@1 gains on entities that occur less than 50 times .   4 Conclusion   In this paper , we propose PELT , a Ô¨Çexible entity   lookup table , to incorporate up - to - date knowledge   into PLMs . By constructing entity embeddings on   demand , PLMs with PELT can recall rich factual   knowledge to help downstream tasks .   Acknowledgement   This work is supported by the National Key R&D   Program of China ( No . 2020AAA0106502 ) , Insti-   tute Guo Qiang at Tsinghua University , and Inter-   national Innovation Center of Tsinghua University ,   Shanghai , China . We thank Zhengyan Zhang and   other members of THUNLP for their helpful dis-   cussion and feedback . Deming Ye conducted the   experiments . Deming Ye , Yankai Lin , Xiaojun Xie   and Peng Li wrote the paper . Maosong Sun and   Zhiyuan Liu provided valuable advices to the re-   search . References527528   A Heuristic String Matching for Entity   Linking   For the Wikipedia , we Ô¨Årst create a mapping from   the anchor texts with hyperlinks to their referent   Wikipedia pages . After that , We employ a heuristic   string matching to link other potential entities to   their pages .   For preparation , we collect the aliases of the   entity from the redirect page of Wikipedia and the   relation between entities from the hyperlink . Then ,   we apply spaCyto recognize the entity name in   the text . An entity name in the text may refer tomultiple entities of the same alias . We utilize the   relation of the linked entity page to maintain an   available entity page set for entity disambiguation .   Algorithm 1 Heuristic string matching for entity   disambiguation   S(f the linked entity page in anchor text g   E(f potential entity name in text g   repeat   S(f the neighbor entity pages that have   hyperlink or Wikidata relation with pages in   Sg   E(feje2Eandecan be uniquely linked   to entity page in Sby string matching g   E(E E   S(E   untilS=   Details of the heuristic string matching are   shown in Algorithm 1 , we match the entity name to   surrounding entity page of the current page as close   as possible . e will release all the source code and   models with the pre - processed Wikipedia dataset .   For other datases , we adopt a simple string   matching for entity linking .   B Training ConÔ¨Åguration   We train all the models with Adam opti-   mizer ( Kingma and Ba , 2015 ) , 10 % warming up   steps and maximum 128 input tokens . Detailed   training hyper - parameters are shown in Table 6 .   We run all the experiments with 5 different seeds   ( 42 , 43 , 44 , 45 , 46 ) and report the average score   with the standard deviation . In the 1 % and 10 % set-   tings ‚Äô experiments for Wiki80 , we train the model   with 10 - 25 times epochs as that of the 100 % set-   ting ‚Äôs experiment .   For FewRel , we search the batch size among   [ 4,8,32 ] and search the training step in [ 1500 , 2000 ,   2500 ] . We evaluate models every 250 on validation   and save the model with best performance for test-   ing . With our hyper - parameter tuning , the results   of baselines in FewRel signiÔ¨Åcantly outperforms   that reported by KEPLER ( Wang et al . , 2021b).529   Runxin Xu , Fuli Luo , Baobao Chang , Songfang Huang , Fei HuangKey Laboratory of Computational Linguistics , Peking University , MOE , ChinaAlibaba Group   runxinxu@gmail.com , chbb@pku.edu.cn   { lfl259702,songfang.hsf,f.huang}@alibaba-inc.com   Abstract   1 Introduction   Recently , a variety of multilingual pre - trained lan-   guage models ( PLMs ) have been proposed , includ-   ing mBERT ( Devlin et al . , 2019 ) and XLM - R ( Con-   neau et al . , 2020 ) . Based on these PLMs , it is possi-   ble to adapt the model to speciÔ¨Åc target languages ,   with only a handful of labeled examples in the   downstream tasks , which is called few - shot cross-   lingual transfer learning ( Lauscher et al . , 2020 ;   Hedderich et al . , 2020 ; Bari et al . , 2021 ) .   However , traditional Ô¨Åne - tuning tends to obtain   degenerated and unstable results , due to the fol-   lowing two challenges . ( 1 ) Parameter Overload :   Given only few labeled data for a target language ,   it is challenging to update all model parameters ,   and such a mismatch between the scale of data and   trainable parameters can cause overÔ¨Åtting ( Dodge   et al . , 2020 ; Zhao et al . , 2021 ) . ( 2 ) Language   Interference : Sharing commonality though , dif-   ferent languages also possess their own charac-   teristics . Hence , the adaption towards a speciÔ¨Åc   target language can interfere with that of other lan-   guages ( Lin et al . , 2021 ) , which also damages the   transfer performance .   Therefore , it is natural to ask the question , How   to address the Parameter Overload and Language   Interference problem elegantly ? In this paper , we   propose a Simple Cro - lingual Sub - network Tun-   ing method , S - Tuning , which tries to deal with   these two problems jointly . As shown in Figure 1 ,   S - Tuning detects the most fundamental language   sub - networks ( with a simple and intuitive crite-   rion in Sec . 3.2 ) , and only updates the speciÔ¨Åc   sub - network corresponding to the input language   during training . For one thing , we update the lan-   guage sub - network on a matching scale , which bet-   ter suits the low - resource scenarios and addresses   theParameter Overload problem . For another , the   commonality across languages is modeled by the   overlap among different language sub - networks ,   while the characteristics are also allowed by the530non - overlapping parts . With such a better trade - off ,   theLanguage Interference problem is alleviated .   Simple to implement , S - Tuning also reveals ev-   ident effectiveness in the downstream tasks in our   experiments . Compared with vanilla Ô¨Åne - tuning ,   S - Tuning consistently offer improvements across   different multi - lingual downstream tasks . For ex-   ample , it improves by 0:9and5:6average points   on XNLI and Tatoeba tasks , respectively .   2 Related Work   Towards better few - shot cross - lingual transfer ,   Zhao et al . ( 2021 ) freeze the embedding and en-   coder layers of the PLM during Ô¨Åne - tuning , which   is not effective and Ô¨Çexible enough . Nooralahzadeh   et al . ( 2020 ) adopt the traditional meta - learning   method MAML ( Finn et al . , 2017 ) , but it is not   practical enough , since it requires extra abundant   labeled data for meta - training . Differently , we try   a more elegant and effective way to handle the   Parameter Overload andLanguage Interference   problem through language sub - networks .   Some works also Ô¨Ånd a sub - network for each   language pair in machine translation ( Lin et al . ,   2021 ; Xie et al . , 2021 ) , or each task in multi - task   learning ( Sun et al . , 2020 ; Liang et al . , 2021 ) . How-   ever , their forward and backward are both based on   sub - networks , which is more like pruning . Instead ,   we update parameters within the sub - network dur-   ing the backward process , but still forward on the   whole network to fully utilize the knowledge stored   in the entire model . Our work most closely re-   sembles the work of Xu et al . ( 2021 ) . However ,   S - Tuning deals with multiple sub - networks simul-   taneously rather than a single sub - network in more   challenging few - shot multi - lingual scenarios , and   adopts different criteria for language sub - network   detection . We empirically show the superiority of   S - Tuning in Figure 3 in Section 4.5 .   3 S - Tuning : S imple Cro - lingual   Sub - network Tuning   We formally present the problem formulation   ( Sec . 3.1 ) . Then we introduce our proposed method ,   S - Tuning , which Ô¨Årstly detects the most important   sub - network for each target language ( Sec . 3.2 ) ,   and then only updates the corresponding sub-   network during the backward process ( Sec . 3.3).3.1 Problem Formulation   Given a speciÔ¨Åc task , the original multilingual PLM   is Ô¨Årstly Ô¨Åne - tuned on rich - resource labeled   dataD= ( X;Y)in source language sto ob-   tain(source training ) following Lauscher et al .   ( 2020 ) . Then , we aim to better adapt to multi-   ple target languages T=   t;t;:::;t 	  with   target labeled data D = f(X;Y)jt2Tg ( tar-   get adapting ) . SpeciÔ¨Åcally , suppose there are C   different classes , we have Ktraining examples for   each classc2Cin target language t , andKis re-   markably small in low - resource scenarios , leading   tojDj  jDj . In our paper , we use English as   source language following Lauscher et al . ( 2020 ) .   3.2 Language Sub - network Detection   In this section , we aim to identify the most impor-   tant sub - network for each target language . In detail ,   for target language t , if parameter his essential   to language t , the change of loss would be large   once we remove h(i.e . ,h= 0 ) ( Molchanov et al . ,   2017 ) , which is shown in Equation 1 and Hrefers   to other parameters excluding h.    ( h ) =  L(H;h= 0) L(H;h )   ( 1 )   Following Molchanov et al . ( 2017 ) , we approxi-   mate with Taylor Expansion , and obtain Eq . 2 .    ( h ) =  @L(H;h )   @hh  ( 2 )   Though different scoring criteria can be used ,   we Ô¨Ånd this one works best . After deriving the   importance score of parameters for target language   tbased on ( X;Y ) , parameters with the highest   score are selected as the sub - network for t. It can   be indicated by a mask M , whereM(h ) = 1 if   hbelongs to the sub - network , and M(h ) = 0   otherwise . With Nparameters in total , we can set   up sub - network scale by p=. We   unifypacross different languages as p , that is ,   p = p = p==p .   3.3 Constrained Language Adaption   According to the distinctive patterns of language   sub - networks , we adapt to the target languages with   their most essential parameters .   Forward During the forward procedure , we en-   code instances by the full network regardless of its   language . In this way , we can better make full use   of the knowledge contained in the whole model.531   Backward Different from vanilla Ô¨Åne - tuning , we   only update the parameters within the signiÔ¨Åcant   language sub - network . It can be achieved by mul-   tiplying the gradients with the mask M. By this   means , we lower the scale of trainable parameters   to address Parameter Overload , and maintain the   commonality and characteristics across different   languages to handle Language Interference .   4 Experiments   4.1 Datasets   We conduct experiments on three multilingual   tasks . Cross - lingual Natural Language Inference   ( XNLI ) ( Conneau et al . , 2018 ) is a natural language   inference task involving 15different languages . Be-   sides , Cross - lingual Paraphrase Adversaries from   Word Scrambling ( PAWS - X ) ( Yang et al . , 2019 )   focuses on determining whether two sentences are   paraphrases with 7languages . Tatoeba ( Artetxe   and Schwenk , 2019 ) with 37languages is a cross-   lingual sentence retrieval task , which Ô¨Ånds the near-   est neighbor based on cosine similarity between   multilingual representations of sentences .   4.2 Experimental Setups   Experiments are based on XLM - R ( Conneau   et al . , 2020 ) . Following Zhao et al . ( 2021 ) , we   Ô¨Årstly Ô¨Åne - tune the PLM for 10epochs with batch   size32on full English labeled examples for source-   training , whose results are comparable to Hu et al .   ( 2020 ) ( details in Appendix A ) . Then we continue   to Ô¨Åne - tune 5epochs onK - shot data over target   languages , and we use K2f64;128 g. The trans-   lated examples provided by Hu et al . ( 2020 ) are   used as the training data for target languages . We   search learning rate from f5e-6;8e-6;1e-5;3e-5 g ,   andpfromf0:1;0:3;0:5 g. We report the average   score on the test set of 5runs with different seeds.4.3 Main Results   Besides vanilla Full Model Ô¨Åne - tuning , we also   compare with two strong baselines ( Zhao et al . ,   2021 ): 1 ) FC Only : Only update the linear classi-   Ô¨Åer during training . 2 ) FC+Pooler : Only update   the linear classiÔ¨Åer and pooler layer during training .   S - Tuning helps the model better adapt to   target languages with strong and stable perfor-   mance . As shown in Table 1 , S - Tuning outper-   forms other Ô¨Åne - tuning methods on XNLI . For   example , compared with Full Model tuning , S-   Tuning yields an improvement of up to 0:90aver-   age points , and the standard deviation of multiple   random runs is also lowered , suggesting more sta-   ble performance . Although with lower standard   deviation , FC Only andFC+Pooler reveal infe-   rior performance . Similar results are observed on   PAWS - X task ( shown in Appendix B due to lim-   ited space ) , in which S - Tuning also beat other   methods on both K= 64 andK= 128 settings ,   e.g. , outperforms Full Model tuning by 0:7average   points when K= 64 .   S - Tuning strengthens the model ability to   capture cross - lingual semantics , thanks to more   precise and Ô¨Çexible adaption for different target   languages . We adopt models Ô¨Åne - tuned on PAWS-   X through different methods , and search the best   encoder layer to derive multilingual sentence repre-   sentations for Tatoeba task . The most semantically   similar sentence is retrieved directly with cosine   similarity between representations . As shown in   Table 2 , S - Tuning yields an improvement of up to   5:64average points across 36target languages , in   comparison with vanilla Full Model tuning .   4.4 Similarity Between Sub - networks   In this section , we aim to understand the intrinsic   relations among different language sub - networks.532   SpeciÔ¨Åcally , we explore the similarity using the   Jaccard similarity coefÔ¨Åcient to quantify the over-   lapping ratio between two sub - networks . Figure 2   illustrates the results based on PAWS - X experi-   ments with K= 128 andp= 0:5settings , It   can be observed that the eastern languages ( Ja , Ko ,   Zh ) are similar to each other , while different from   the western languages ( De , En , Es , Fr ) . For ex-   ample , the sub - network of Japanese ( Ja ) is much   more similar to that of Korean ( Ko ) and Chinese   ( Zh ) than others . It suggests that the detected sub-   networks potentially capture the inductive bias of   language similarity , and model their commonality   and characteristics through overlapping and non-   overlapping parts Ô¨Çexibly .   4.5 Comparison with Different Sub - network   Strategies : Pruning and Random   To further understand the effect of S - Tuning , we   compare with two sub - network strategies in XNLI   and PAWS - X with K= 64 : 1)Pruning ( Lin et al . ,   2021 ; Xie et al . , 2021 ): both forward and back-   ward are through a pruned sub - network ( while S-   Tuning uses the full network for forward ) . We   adopt Equation 2 as the criterion to prune the   model for all target languages . 2 ) Random : the   sub - networks are detected randomly for S - Tuning   rather than following a speciÔ¨Åc criterion .   As shown in Figure 3 , for pruning , the model   would collapse if p < 0:7 , and the best score   achieved in p= 0:9is still lower than the vanilla   Ô¨Åne - tuning in XNLI . The performance of random   sub - network is slightly lower than vanilla Ô¨Åne-   tuning in XNLI , while slightly higher in PAWS-   X. Compared with these two strategies , S - Tuning   achieves the best scores in an overwhelming ma-   jority of cases , which suggests the superiority of   S - Tuning in few - shot cross - lingual transfer .   5 Conclusion   Towards better few - shot cross - lingual transfer   learning , we propose S - Tuning . S - Tuning de-   tects the most essential sub - network for each target   language , and only updates these parameters dur-   ing the backward process , while still utilizing the   full model for the forward process . In this way ,   we reduce the scale of trainable parameters that   better suits low - resource scenarios to address over-   Ô¨Åtting , and better deal with the interference across533languages . Our experiments show that S - Tuning   consistently outperforms other Ô¨Åne - tuning methods   in different downstream tasks .   Acknowledgements   This paper is supported by the National Science   Foundation of China under Grant No.61936012 ,   the National Key RD Program of China un-   der Grand No.2018AAA0102003 , and the Na-   tional Science Foundation of China under Grant   No.61876004 .   References534535A Results on Source Training   Since our work focuses on the target adapting , we   ensure the results on source training are compara-   ble to others . As shown in Table 3 , the obtained   results based on our implementation is comparable   or even better than those of Hu et al . ( 2020 ) in three   multi - lingual tasks .   PAWS - X XNLI Tatoeba   Hu et al . ( 2020 ) 86.4 79.2 57.3   Ours 86.4 79.6 58.5   B Results on PA WS - X   Table 4 illustrates the results of different Ô¨Åne - tuning   methods on PAWS - X task . Compared with vanilla   full model tuning , S - Tuning achieves better per-   formance with lower standard deviation , which sug-   gests that S - Tuning helps the model better adapt   to target languages and obtain more stable results .   C Detailed Results on Tatoeba   Table 5 demonstrates the results on the cross-   lingual retrieval task , Tatoeba , across 36differ-   ent target languages in total . Since FC Only and   FC+Pooler do not update the intermediate encoder   layers , their results are both the same as that of   the model after source training . It can be ob-   served that S - Tuning outperform other methods   by5:67:6average points under K= 64 setting ,   and3:211:0average points under K= 128   setting .   D Results on XQuAD   We also explore S - Tuning in multilingual ques-   tion answering task , XQuAD ( Artetxe et al . , 2020 ) .   As shown in Table 6 , S - Tuning provides improve-   ments on both K= 64 andK= 128 settings ,   along with lower standard deviation.536537   Hillary Dawkins   University of Guelph , Canada   Vector Institute , Toronto , CanadaIsar Nejadgholi   National Research Council Canada   Ottawa , Canada   Abstract   1 Introduction   Calibrating the certainty estimates of neural net-   works is of the utmost importance for interpretabil-   ity of results and building trust in AI systems . Ide-   ally , if a model outputs some prediction with an as-   sociated probability , we would like to interpret that   quantity as the probability of a correct prediction   ( i.e. as a meaningful certainty estimate ) ( Zadrozny   and Elkan , 2001 ; Niculescu - Mizil and Caruana ,   2005 ) . However , contemporary models are consis-   tently over - confident in their output probabilities   ( Guo et al . , 2017 ) .   Guo et al . ( 2017 ) demonstrates that over-   confident models can arise by overfitting to the   Negative Log - Likelihood ( NLL ) loss , without over-   fitting to the classification accuracy . Many cal-   ibration methods involve modulating the output   logits somehow , according to a prescribed func-   tional form . The parameters of the modulation   function are learned on the associated validation   set by minimizing the NLL loss ( thereby correcting   the overfit ) . Guo et al . ( 2017 ) , as well as manysubsequent studies ( e.g. M√ºller et al . , 2019 ; Gupta   et al . , 2021 ) , showcase the surprising effectiveness   of temperature scaling , a single - parameter modula-   tion function .   The calibration error is reported as a single quan-   tity computed on the associated test set . Typically ,   the error is composed of a sum of observed errors   across the certainty landscape , visualized using a   reliability diagram ( DeGroot and Fienberg , 1983 ;   Niculescu - Mizil and Caruana , 2005 ) . However , not   all regions contribute equally , especially in the case   of class - imbalanced datasets . Consider an output   with a predicted certainty of 99.9 % vs. an expected   actual certainty of 99.8 % . In terms of human in-   terpretability and intervention , this difference is   negligible . Now consider 79 % predicted certainty   vs. 71 % expected certainty . Clearly the second case   is one we should care more about correcting . How-   ever , as we will discuss in the following section ,   the presence of a dominant high - certainty class can   cause the first discrepancy to contribute more to   the reported calibration error than the second . High   quality mid - certainty estimates are most impact-   ful for human - in - the - loop applications , yet current   error measures are not sensitive to this region .   Here we take NER ( Grishman and Sundheim ,   1996 ; Yadav and Bethard , 2018 ; Li et al . , 2020 ) as   a case study for class - imbalanced token classifica-   tion . Naturally , the ‚Äú outside ‚Äù or non - entity class   dominates the dataset . In the following section , we   introduce a region - balanced calibration error . We   then introduce region - dependent temperature scal-   ing , a calibration method that further reduces error   over traditional temperature scaling , across various   NER scenarios , without additional computation .   2 Region - balanced expected calibration   error   The most popular calibration error metric is the ex-   pected calibration error ( ECE ) ( Naeini et al . , 2015 ) .   A test set is partitioned into certainty bins , each538   containing samples with a certainty score hwithin   the bin boundaries . The uncalibrated certainty h   for a given sample is simply the output probability   associated with the predicted class for that sam-   ple . Within each bin , we compare the actual and   predicted certainty :   ECE = Xn   N|acc(B)‚àíconf(B)|(1 )   where conf(B)is the predicted confidence score   ( the mean hof samples in bin B ) , and acc(B)is   the actual accuracy ( proportion of correct predic-   tions in bin B ) . Each bin error is weighted by the   bin support , where nis the number of samples in   B. If a very high proportion of all samples have a   high certainty estimate , only the final bin error has   a non - negligible contribution to the overall ECE .   Refer to Figure 1 for an illustrated example .   One extension of ECE is to find bin partitions   adaptively ( Nixon et al . , 2020 ) , such that each bin   contains an equal number of samples , and each bin   contributes equally to the overall error . The result   is that many more bins exist in the high certainty   region , each of which are narrower in width . Essen-   tially , adaptive - ECE reports the exact same error   quantity as ECE in theory , but estimates the quan-   tity using a finer - toothed comb . Neither metric is   informative on lower or mid - certainty regions ifsupport is dominated by a high - certainty class .   Maximum expected calibration error ( MECE )   ( Naeini et al . , 2015 ) partially tells the story of low-   certainty regions by reporting the maximum bin   error . However , MECE is overly sensitive to outlier   bins . For example , if a single sample happens to   fall in the 0 - 5 % certainty bin , and it has the correct   predicted class , we have MECE > .95 , which is   clearly an unusable characterization of the calibra-   tion error as a whole .   Here we consider Region - balanced ECE ( RB-   ECE ) as a way to characterize calibration error   weighted evenly across certainty regions . Simply ,   RBECE = 1   |Œò|X|acc(B)‚àíconf(B)| .   ( 2 )   The error in each bin Bcontributes to the error   equally , subject to some threshold support require-   ment n > Œ∏ ( to ensure acc(B)is well - defined ) .   The set of bins that meet this requirement is de-   noted by Œò.   Alternative threshold requirements such as vari-   ance in conf(B)vs . bin size could be explored in   the future . Another possible extension is custom   bin - weighting according to a certainty region of   interest for your application ( e.g. for human - in - the-   loop systems with an intervention criterion).5393 Region - dependent temperature scaling   The idea underlying all calibration methods is gen-   erally to modulate overconfident predictions . In   traditional temperature scaling ( TS ) , a higher tem-   perature means stronger modulation . Temperature   is taken to be a constant , meaning all samples are   treated with the same modulation strength .   The idea underlying region - dependent tempera-   ture scaling ( RD - TS ) is simply that the most con-   fident predictions likely need greater modulation   than less confident predictions , and therefore tem-   perature should depend on the uncalibrated cer-   tainty . If we consider the hypothetical limit of a 0 %   confidence score , it is intuitive that this does not   need any modulation . To investigate this idea em-   pirically , we apply TS to subsets of the OntoNotes   dataset , partitioned according to uncalibrated confi-   dence scores . For each confidence region , the ideal   temperature is shown in Figure 2 . As expected ,   temperature increases as a function of confidence .   A linear fit sufficiently describes the dependence .   Within uncertainty , the intercept is equal to the ex-   pected value of 1 ( T(h= 0 ) = 1 , corresponding   to no modulation ) .   To apply RD - TS , uncalibrated logits ‚Éó aare scaled   as‚Éó q=‚Éó a / T(h)to obtain calibrated logits ‚Éó q. Tem-   perature is now a function of confidence T(h ) =   mh+ 1 , where h = max(softmax ( ‚Éó a))is the proba-   bility estimate for the predicted class on each sam-   ple . The slope mis the single parameter controlling   modulation strength .   To estimate m , one could repeat temperature   scaling on multiple data subsets , collect data points ,   and fit the slope as in Figure 2 . However , this   method increases computational overhead . Instead ,   let us estimate mfrom the original TS constant   Tand some knowledge of the validation dataset   which was used to compute T. Each sample in   the validation set has an ideal temperature , here   taken to be in the form T = mh+ 1 . Assuming   each sample contributed to the found Tequally ,   T = P(mh+ 1 ) . Given access to the vali-   dation set , this sum can be computed exactly to   findm . However , we can further approximate   the sum by loosely assuming that the data has a   high proportion of samples ( say ‚âà90 % ) with very   high certainty estimates ( say ‚âà.99on average ) .   Then the sum is dominated by the first leading   term , T‚âà.9(.99m+ 1 ) . This quick sketch is   sufficient to achieve good error reduction over the   baseline TS method . The numerical exactness is   not too important , but rather the general signature   of a high proportion of high - certainty samples is   sufficient . We take this further approximation to   gain the advantage that nothing specifically needs   to be known about the calibration dataset . I.e. If   a large pre - trained model has been calibrated on   a large or private dataset , and the corresponding   temperature Tis known , RD - TS can be applied to   your model outputs without access to the calibra-   tion data or further computation .   In summary , the RD - TS method is performed as   follows :   1.Perform regular temperature scaling to obtain   T , or obtain a previously published Tfor   your model .   2.Find the linear dependence parameter m=   ( T‚àí.9)/.89 .   3.Apply calibration to logits ‚Éó aas‚Éó q=‚Éó a / T(h ) ,   T = mh+ 1 .   RD - TS is a simple extension of temperature scal-   ing which requires no additional training . Like   temperature scaling , RD - TS can not change the pre-   dicted class or model accuracy ( unlike some other   generalizations , vector and matrix scaling).540Scenario Uncal . TS VS MS WTS RD - TS   Classic .09328 .02543 ( T= 1.28 ) .07040 .06940 .05236 .02151 ( m=.426 )   Rare & emerging .09878 .05777 ( T= 1.39 ) .07490 .04932 .11559 .03549 ( m=.550 )   Fine - grained .05333 .02179 ( T= 1.12 ) .03440 .04628 .03278 .01263 ( m=.243 )   Specialized .07088 .04147 ( T= 1.29 ) .03844 .03590 .03820 .02781 ( m=.439 )   Sparse training .09683 .07820 ( T= 1.10 ) .11653 .09528 .06279 .04110 ( m=.229 )   Differing sources .05730 .05960 ( T= 1.09 ) .10824 .08470 .05551 .04019 ( m=.214 )   Scenario Uncal . TS VS MS WTS RD - TS   Classic .02001 .00862 ( T= 1.28 ) .01359 .01083 .00962 .00155 ( m=.426 )   Rare & emerging .04278 .02323 ( T= 1.39 ) .02585 .01580 .04712 .00949 ( m=.550 )   Fine - grained .02287 .00783 ( T= 1.12 ) .01587 .01786 .01462 .00839 ( m=.243 )   Specialized .01555 .00617 ( T= 1.29 ) .00608 .00573 .00631 .00651 ( m=.439 )   Sparse training .03267 .02190 ( T= 1.10 ) .03113 .02599 .01645 .01798 ( m=.229 )   Differing sources .00950 .00723 ( T= 1.09 ) .01211 .01344 .01020 .00383 ( m=.214 )   Dataset h|(P=.9)P|(h=.99 )   OntoNotes .998 .964   W - NUT 17 .997 .953   Few - nerd .972 .801   BC2GM .997 .968   OntoNotes ( tc ) .999 .978   4 Experimental results   4.1 Baseline methods   As RD - TS is a simple extension of regular tem-   perature scaling , we focus comparison on similar   post - training parametric calibration methods :   Temperature scaling ( TS ) : Uncalibrated logits ‚Éó a   are scaled by a single constant T(as‚Éó q=‚Éó a / T )   before softmax is applied to obtain calibrated prob-   ability estimates over all classes ( Guo et al . , 2017 ) .   Vector ( generalized Platt ) scaling ( VS ) : A gen-   eralization of TS such that logits are scaled by 2k   learned parameters , ‚Éó q=‚Éó v ‚ó¶ ‚Éó a+‚Éób , where kis the   number of classes ( Platt , 1999 ; Niculescu - Mizil   and Caruana , 2005 ; Guo et al . , 2017 ) .   Matrix scaling ( MS ) : A further generalized lineartransformation such that logits are scaled by k+   klearned parameters , ‚Éó q = M‚Éó a+‚Éób(Guo et al . ,   2017 ) .   Weighted temperature scaling ( WTS ) : TS us-   ing a class - weighted NLL loss during convergence   ( Obadinma et al . , 2021 ) .   4.2 Datasets   We take the NER task as a case study . Datasets   represent several important scenarios in token clas-   sification settings more broadly :   Classic : The OntoNotes 5.0 NER dataset   ( Weischedel et al . , 2013 ) represents a baseline   ‚Äú classic ‚Äù scenario involving plentiful training and   calibration data from robust sources .   Rare and emerging named entities : The W - NUT   NER dataset(Derczynski et al . , 2017 ) is gathered   from noisy social media data which contains dif-   ficult entities ( e.g. ‚Äú kktny " ) due to informal and   evolving language .   Fine - grained and few - shot : Few - nerd(Ding   et al . , 2021 ) is a challenging few - shot NER dataset   with 66 fine - grained entity types ( e.g. ‚Äú art - film " ) .   Specialized language : The BioCreative II Gene   Mention Recognition ( BC2GM ) dataset(Smith   et al . , 2008 ) is composed of scientific text where   named entities are gene mentions.541Sparse training data : OntoNotes telephone call   data is used for training while the full OntoNotes   dataset is used for calibration and evaluation . The   telephone call data subset is a sparse representa-   tion since it is very heavily skewed to the non-   entity outside class , and entity mentions are con-   centrated on ‚Äú person " and ‚Äú location " , compared to   the full OntoNotes dataset ( generally containing   much richer entity mentions from news sources ) .   Differing language sources : OntoNotes broadcast   news data is used for training , and telephone call   data is used for calibration and evaluation . Broad-   cast news language is professional and grammat-   ically correct . Telephone call language is casual ,   fragmented and incoherent at times .   4.3 Implementation notes   All NER models use DistilBERT(Sanh et al . ,   2019 ) as the base pre - trained model , fine - tuned   for NER using the train dataset for each scenario as   described above . Further details and performance   on the NER task are provided in Appendix A.   Calibration is performed using the uncalibrated   logits of the associated validation set as model in-   puts . Calibration parameters are learned by min-   imizing the NLL ( or weighted NLL ) loss for 50   epochs ( using SGD with 0.01 learning rate , and   0.9 momentum ) . Calibration error is computed   on the associated test set . To compute both ECE   ( eq . 1 ) and RBECE ( eq . 2 ) , the number of bins   is set to 20 . To compute RBECE , the threshold   for support per bin is set to Œ∏= 40 . The code   needed to reproduce these results is made publicly   available . All datasets are publicly available with   preset train / validation / test data splits .   4.4 Results   Experimental results are summarized in Tables 1   and 2 . When low and mid - certainty regions are   taken into account by the RBECE , calibration error   is larger than previously thought ( as reported by   ECE ) . In all scenarios , RD - TS produces the small-   est RBECE ( in many cases quite substantially ) . Ad-   ditionally , RD - TS improves the traditional ECE in   the majority of scenarios . The results show that   RD - TS is an effective extension of TS across a   range of temperature ( T ) values .   Recall in Section 3 , we sketch a way to estimate   the modulation parameter m , and this approxima - tion follows from assuming that a high proportion   of all samples in the calibration set ( say ‚âà.9 ) have   a high certainty estimate ( say ‚âà.99on average ) .   We claim that the numerical exactness of these   values is not too important ( and therefore RD - TS   outperforms TS across a range of datasets ) . This   claim is supported empirically ( Table 3 ) .   5 Discussion and Conclusion   Good quality mid - range certainty estimates are es-   sential for productive human - model interactions .   Despite this , existing calibration error measures   can be insensitive to all but the highest certainty   regions . We propose a region - balanced error metric   to probe this unreported information . When low   and mid - certainty regions are taken into account ,   greater calibration errors are revealed .   Further , we explore the idea of a certainty-   dependent temperature . While previous general-   izations of TS , such as vector and matrix scaling ,   allow certainty dependence by increasing the num-   ber of learned parameters , these methods are gener-   ally outperformed by TS ( Guo et al . , 2017 ) . Rather   than allowing a complicated certainty dependence ,   we enforce a simple linear dependence ( motivated   by intuition and an empirical example ) without   introducing any learnable parameters . Unlike vec-   tor and matrix scaling , RD - TS can not change the   relative ranking of logits , and therefore model ac-   curacy is retained ( in single - label settings ) . One   line of future work could be to apply RD - TS on top   of weighted temperature scaling , a method known   to decrease variance in calibration error among   classes ( Obadinma et al . , 2021 ) . Another line of   work would be to investigate whether improved   certainty estimates can increase model accuracy   ( in multi - label settings where predictions are ap-   plied by meeting a certainty threshold ) , especially   in out - of - domain problems .   Finally , it is important to note that our discus-   sion of a region - balanced error measure , as well as   our sketch derivation of the RD - TS method , have   been generally applicable to any problem with a   dominant proportion of high - certainty predic-   tions . This situation does arise in any token clas-   sification problem with a dominant ‚Äú easy ‚Äù class ,   as is the case in NER , however this situation can   equally occur in class - balanced situations . There-   fore , region - dependent temperature scaling can find   utility beyond NER , token classification , or class-   imbalanced situations.542Ethical Considerations   We proposed a novel method to calibrate class-   imbalanced token classifiers , and demonstrated the   method for NER models . This calibration method   is a step toward responsible use of AI by offer-   ing a measure of reliability , but also has risks that   should be considered from an ethical point of view .   Calibrated scores are a measure of transparency ,   and users can interpret a well - calibrated model bet-   ter . However , all transparency methods expose AI   systems to malicious attacks by providing more   information about the internal workings of the sys-   tem . This risk should be taken into account in   sensitive tasks , e.g. when an NER model is used to   extract personally identifiable information for pri-   vacy reasons . Also , users should be warned that a   low calibration error does not guarantee robustness   in out - of - domain settings . Therefore , in the case of   safety - critical tasks such as medical applications of   NER , a low calibration error should be interpreted   with caution .   Further , low calibration errors should not be used   to justify inherently unethical tasks or those out of   the scope of the capabilities of NLP technologies .   Every task should be evaluated in terms of feasi-   bility and ethical use regardless of reliability and   transparency of trained models . It is also impor-   tant to keep in mind that a well - calibrated model   can become miscalibrated as the data changes , and   continuous calibration is needed to deal with the   ever - changing nature of language .   References543   A NER performance   NER models were obtained by fine - tuning Distil-   BERT , using the default configuration , for 3 epochs   ( with learning rate of 2e-5 , and weight decay of   0.01 ) . The performance of all NER models is pro-   vided in Table A.1 for reference .   Dataset P R F A   OntoNotes .778 .621 .691 .976   W - NUT 17 .543 .234 .327 .938   Few - nerd .639 .679 .659 .906   BC2GM .802 .844 .822 .965   OntoNotes ( bc ) .711 .753 .732 .973544   Antonio Laverghetta Jr. andJohn Licato   Advancing Machine and Human Reasoning ( AMHR ) Lab   Department of Computer Science and Engineering   University of South Florida   Tampa , FL , USA   { alaverghett,licato}@usf.edu   Abstract   1 Introduction   Negation is an important construct in language for   reasoning over the truth of propositions ( Heine-   mann , 2015 ) , garnering interest from philosophy   ( Horn , 1989 ) , psycholinguistics ( Zwaan , 2012 ) ,   and natural language processing ( NLP ) ( Morante   and Blanco , 2020 ) . While transformer language   models ( TLMs ) ( Vaswani et al . , 2017 ) have   achieved impressive performance across many   NLP tasks , a great deal of recent work has found   that they do not process negation well , and often   make predictions that would be trivially false in   the eyes of a human ( Rogers et al . , 2020 ; Ettinger ,   2020 ; Laverghetta Jr. et al . , 2021 ) .   In developmental psychology , there has likewise   been a great deal of interest in how a child ‚Äôs abil-   ity to comprehend negation emerges in the early   years of life ( Nordmeyer and Frank , 2013 , 2018b ;   Reuter et al . , 2018 ; Grigoroglou et al . , 2019 ) . Un-   like in NLP , which typically treats negation as rep-   resenting a single monolithic competency , this re-   search has long understood that there are manykinds of negation used in everyday interactions   ( Bloom , 1970 ; Pea , 1982 ) . This ranges from using   negation to express a child ‚Äôs rejection of something   to clarifying a child ‚Äôs knowledge . These ‚Äú devel-   opmental ‚Äù categories of negation do not emerge   simultaneously ; children tend to start using certain   kinds before others ( Nordmeyer and Frank , 2018a ) .   Given that these categories represent some of   the earliest uses of negation among humans , un-   derstanding how well TLMs can master them is   important for building more human - like models of   language processing . Understanding how well mod-   els perform on different categories will indicate   whether they have mastery of some forms of nega-   tion , while also helping to identify failure points .   Another interesting question is whether the proÔ¨Å-   ciency of TLMs on these categories is at all related   to competencies in human children ( e.g. , is the cat-   egory which models consistently perform the best   on the same that children most frequently employ ? ) .   However , to our knowledge , no prior work in NLP   has focused on how well models perform on the   forms of negation of interest to developmental psy-   chology .   In this short paper , we investigate how well a   suite of TLMs can process developmental nega-   tion , by framing the problem as a natural lan-   guage inference ( NLI ) task . We develop a rule-   based parser to extract problems from existing NLI   datasets , and evaluate our models on each cate-   gory , in order to determine ( i)whether certain cat-   egories are more solvable by our models than oth-   ers , and ( ii)what relationships exist among the   categories . We Ô¨Ånd that models can consistently   achieve stronger performance only on certain cat-   egories , and that training on combinations or se-   quences of these categories does not substantially   improve a model ‚Äôs downstream performance.5452 Related Work   Negation is known to be frequently used in every-   day conversation . While this includes its logical   form , we primarily focus on negation ‚Äôs psycholin-   guistic forms , especially those that have been stud-   ied in the context of developmental psychology .   Negation emerges early in child development , with   ‚Äò no ‚Äô sometimes being a child ‚Äôs Ô¨Årst word ( Schnei-   der et al . , 2015 ) , and even infants appear to under-   stand forms of negation ( Piaget , 1980 ; Hochmann   and Toro , 2021 ) . Preschool children use at least   three different kinds of negation ( Bloom , 1970 ) ,   but possibly as many as nine ( Choi , 1988 ) . As   noted by Nordmeyer and Frank ( 2018a ) , one of   the Ô¨Årst categories children use is rejection , where   a child rejects an object or activity . This is later   followed by existence , where a child might ex-   press the lack of an object , and later still denial ,   which a child uses to deny the truth of a claim .   Larger scale studies of child - directed speech have   found that truth - functional kinds of negation tend   to emerge later ( Liu and Jasbi , 2021 ) , but individual   children do vary in their speciÔ¨Åc order of acquisi-   tion ( Nordmeyer and Frank , 2018a ) . It is unknown   whether this ordering reÔ¨Çects any deeper depen-   dencies among the different categories , or whether   the ordering is reÔ¨Çected in how artiÔ¨Åcial language   models ( LMs ) learn negation .   In NLP , methods from psycholinguistics have   been used to probe the reasoning capabilities of   LMs . Results from some studies have indicated   that TLMs are not human - like in their processing   of negation ( Ettinger , 2020 ; Kassner and Sch√ºtze ,   2020 ) . A similar line of work has used the NLI   task to probe a model ‚Äôs ability to process negation   and found that TLMs will often alter their predic-   tions when negation is inserted or removed , even   when the negation does not alter the entailment re-   lationship ( Hossain et al . , 2020 ; Hartmann et al . ,   2021 ) . As argued by Kruszewski et al . ( 2016 ) , part   of the challenge of modeling purely logical nega-   tion is that a predicate often occurs in very similar   contexts regardless of whether it is being negated .   They argue that we should view negation as be-   ing a ‚Äú graded similarity function ‚Äù , and show that   distributional models can predict human plausibil-   ity judgments quite well , even in the presence of   negation . These works show that it is unclear how   well distributional models , especially TLMs , are   actually processing negation . We contribute to this   literature from a new perspective , by studying how   well models can reason over forms of negation   common in developmental psychology .   3 The Developmental Negation Corpus   We use the NLI task to study the negation reasoning   capabilities of our models . NLI problems consist   of two sentences : a premise ( p ) and hypothesis   ( h ) , and solving such a problem involves assessing   whether ptextually entails h. The generic structure   of the NLI task makes it suitable for studying a va-   riety of underlying reasoning skills , including nega-   tion . We speciÔ¨Åcally use the SNLI ( Bowman et al . ,   2015 ) and MNLI ( Williams et al . , 2018 ) datasets .   To automatically identify questions that contain   a speciÔ¨Åc kind of negation , we rely on the work   by Liu and Jasbi ( 2021 ) which studied how fre-   quently different kinds of developmental negation   occur in child - directed speech , using the data from   the CHILDES corpus ( MacWhinney , 2014 ) . To do   this , they created a simple rule - based parser to au-   tomatically tag each sentence in CHILDES with   the type of negation it contained ( if any ) . We re-   implement their parser , in some cases tweaking   the rules slightly to better suit the structure of the   NLI task . For each example across all the splits of   both datasets , we Ô¨Årst obtain a dependency parse   of both pandhusing the diaparser package ( Wang   et al . , 2019 ) , and check if either contains an explicit   negation marker ( ‚Äú no ‚Äù , ‚Äú not ‚Äù , or ‚Äú n‚Äôt ‚Äù ) . If one span   contains negation , we check if the syntactic struc-   ture obeys the rules of any of our categories . If the   span falls into a category , we mark it as belonging   to that category . We use these questions as the diag-   nostic set for our experiments , splitting out 1/3 of   the questions in each category as a diagnostic test   set , and leaving the remainder as a diagnostic train   set ( and we will refer to them as such ) . We place   the remaining NLI questions containing no nega-   tion in a separate NLI set , giving us about   730,000 examples we use to Ô¨Ånetune our models   on the NLI task . We split out 9,000 questions from   this train set at random to use as a NLIset , bal-546   anced for each label . In the following , we describe   the precise rules used to determine which category   a negated example should be assigned to :   Possession ( PO)We require that the lemma of   the root be have , has , orhad , and that the root is   directly modiÔ¨Åed by both the negation and the verb   do .   Existence ( EX)We require that there occur in   the text and precede the negative marker and that   the negative marker directly modiÔ¨Åes a noun phrase ,   determiner , or an adverb .   Labeling ( L)We require that the sentence be-   gin with either That orIt , and that the root of the   sentence is a noun which is modiÔ¨Åed by isor ‚Äôs .   Prohibition ( PR)We require that the sentence   not contain a subject and that the negation is im-   mediately preceded by do . To not conÔ¨Çate this cat-   egory with others , we Ô¨Ålter out cases where the   root contains one of the explicit markers of another   category ( e.g. , likeorwant in the case of rejection ) .   Inability ( I)We require that the negation di-   rectly modify the root of the sentence , and that   the word immediately before the negation is either   canorcould ( e.g. , can not do ) . Prior literature has   typically viewed inability from an egocentric per-   spective . However , we found that allowing only the   Ô¨Årst person severely restricted the number of ex-   amples extracted , and therefore chose to also allow   the second and third person .   Epistemic ( EP)We require that the root be re-   member , know , orthink , and that the root be directly   modiÔ¨Åed by the verb do .   Rejection ( R)We require that the lemma of the   root word be either likeorwant , and that the root   is modiÔ¨Åed by the negative marker .   After performing extraction , categories Land   PRcontained fewer than 1000 examples , which   we deemed was insufÔ¨Åcient to split into separate   train and test sets . To address this , we developeda simple data augmentation approach that utilized   the Wordnet database ( Miller , 1998 ) . From the de-   pendency parse of both pandh , we check if the   root of either parse occurs in both spans . If it does ,   we obtain all synonyms of the word in Wordnet and   replace the root in both spans with the synonym   ( doing this for every synonym ) . We found this sim-   ple approach increased the number of examples for   bothLandPRto at least 1500 . Note that we per-   formed no augmentation for the other categories , as   our parser extracted at least 1500 examples for all   other cases . Table 1 shows statistics for the dataset   after augmentation .   Table 2 shows extracted examples , along with   their category assignment . We generally found that   the extracted examples matched up with the pro-   totypical category quite well , although in some   cases their semantics differed slightly . For instance ,   consider a PRexample with p = don‚Äôt miss hav-   ing a Ô¨Çick through the albums andh = The pic-   tures of old Madeira show a more interesting city   than now , which is an MNLI example originally   extracted from a travel guide . Although this tech-   nically counts as PR , it does not have quite the   same semantics as an actual command . Unfortu-   nately , these ambiguities are not easily resolved ,   given that negation takes on many forms and may   occur at any location within a sentence . We , there-   fore , opted to focus on forms of negation that can   be easily extracted , and leave improvements to our   dataset creation protocol for future work .   4 Experiments   Using the curated dataset , we performed a series of   exploratory experiments to help us understand how   well TLMs process each of the negation categories .   We use BERT ( Devlin et al . , 2019 ) , and RoBERTa   ( Liu et al . , 2019 ) , two popular transformer LMs   that have demonstrated impressive results on a   variety of language understanding tasks . We also   examine MiniBERTa ( Warstadt et al . , 2020 ) and   BabyBERTa ( Huebner et al . , 2021 ) , which are both547based on the RoBERTa architecture but were pre-   trained on a much smaller number of tokens ( 10   million and 5 million respectively ) , which is more   realistic to the amount of language a child is ex-   posed to in the Ô¨Årst few years of life . We use the   Huggingface implementation of all models ( Wolf   et al . , 2020 ) , and use both the base andlarge ver-   sion of BERT and RoBERTa , which differ only in   the number of trainable parameters .   Experiment 1 : We began by investigating   whether TLMs would master certain negation cate-   gories sooner than others over the course of train-   ing . We train our models on NLI for 10   epochs , using a learning rate of 1e 5 , a weight   decay of 0:01 , a batch size of 16 , and a maximum   sequence length 175.We selected these hyperpa-   rameters to be similar to those which were previ-   ously reported to yield strong results when train-   ing on NLI datasets ( Laverghetta Jr. et al . , 2021 ) .   We additionally evaluated the models on NLI ,   and found that they all achieved a Matthews Cor-   relation of at least 0.6 ( Matthews , 1975 ) , and thus   concluded that these hyperparameters were suit-   able . For every end of epoch checkpoint across all   models , we obtained evaluation results on each di-   agnostic test set . Importantly , the models are not   Ô¨Ånetuned on any negated NLI questions for this ex-   periment , meaning that all knowledge of negation   comes from pre - training . Results are shown in Fig-   ure 1 . We see that the categories have similar rank-   ings in terms of accuracy . For example , LandPO   are among the top two best - performing categories ,   while Ris generally one of the worst - performing   ones , indicating clear distinctions in how LMs pro-   cess the categories . BabyBERTa , unlike other mod-   els , also shows stronger similarities to how children   acquire negation . For instance , while Ris thought   to be one of the Ô¨Årst categories children acquire ,   BabyBERTa is the only model where Ris one of   the highest - ranking categories in terms of accuracy .   Experiment 2 : One might expect that children   develop a more abstract understanding of negation   as they are exposed to different categories . This   was suggested by Pea ( 1978 ) who argued that more   abstract forms of negation develop from less ab-   stract ones , suggesting that mastering one form of   negation can lead to positive transfer on others . In   Experiment 2 , we examined how much positive   transfer could be obtained from training on one   of the negation categories , and then testing on the   others . We adopt a similar methodology to Pruk-   sachatkun et al . ( 2020 ) , who explored the condi-   tions that affect intermediate task transfer learning .   Using the models trained in Experiment 1 , we fur-   ther Ô¨Ånetune these models for 25 epochs on each   diagnostic train set separately . We then evaluate the   Ô¨Ånetuned models on each diagnostic test set , which   allows us to examine all possible pairwise interac-   tions among categories . Figure 2 shows the results   for all combinations of diagnostic categories for   training and testing . Surprisingly , we Ô¨Ånd that posi-   tive transfer generally only occurs when a model is   trained on the same category it is being tested on .   Training on a different category has little to no ef-   fect on the target category . BabyBERTa is again an   exception , as we do see positive transfer for most   pairs , suggesting the model is generalizing across   categories   Experiment 3 : Building on Experiment 2 , we   examined how the performance of our models is   affected when trained on all diagnostic categories   in sequence . Assuming that no positive transfer   exists among the categories , we would expect to   see a model ‚Äôs performance on a particular cate-   gory improve only after it has been trained on that   same category , and even training on multiple other   categories should not substantially improve perfor-548   mance on the target . Using the models from Ex-   periment 1 , we Ô¨Ånetune each model for 10 epochs   on every diagnostic train set , using the sequence of   categories shown in the x - axis of Figure 3 . Addi-   tionally , we under - sample all diagnostic train sets to   have the same number of questions as PR , so that   all categories contribute the same amount of data .   Figure 3 shows the results . For some categories ,   such as LandPR , we see the expected trend . The   largest accuracy gain for these categories occurs   whenever the model is trained on the same cate-   gory it is being tested on , and performance drops   slightly after being trained on others . However , for   categories such as R , the best performance gain   is not always after being trained on the same cat-   egory . We sometimes see the model continue to   improve on Rafter being trained on R , and in   some cases , training on Rcauses performance on   Rtodecrease .   5 Discussion and Conclusion   In this paper , we have explored how well trans-   formers process categories of developmental nega-   tion . We Ô¨Ånd that performance rankings across cat-   egories are generally consistent , but that the cate-   gories seem to test for orthogonal skills in the ma-   jority of LMs . In BabyBERTa , we see signiÔ¨Åcant   similarities with the order of negation acquisition   in children . Two of the best performing categories   areRandL , while two of the worst are EX and   PR , which aligns quite well to the order observed   by Liu and Jasbi ( 2021 ) . It thus seems that TLMs   do at least partially reÔ¨Çect the order of negation   acquisition observed in children , although more   experiments would be needed to understand the   extent of this correlation . That we found category   rankings to generally be consistent across LMs may   have interesting implications , and understanding   why LMs struggle with certain categories may help   to improve the ability of LMs to process negation .   Future work can build on these experiments in   several ways . In Experiments 2 and 3 , we modeled   interactions among the negation categories in either   a pairwise or sequential fashion , which is unlikely   to reÔ¨Çect how children are exposed to negation .   More experiments , mixing all of the categories at   once in various proportions , might yield a more   realistic model of cognitive development . Our ap-   proach also requires that each category Ô¨Åts into a   speciÔ¨Åc structure , which limits the amount of exam-   ples that can be extracted . Future work will need   to expand our ruleset to include more variations   in the negated utterances covered . Finally , while   we primarily focus on Ô¨Ånetuning , pre - training is   likely to impact the proÔ¨Åciency of our models on   the categories as well . Future work should precisely   control the prevalence of each category in the pre-   training corpus , to observe what effect this has on   downstream performance.549References550551   Rahil Parikh   Institute for Systems Research   University of MarylandChristophe Dupuy   Amazon Alexa AIRahul Gupta   Amazon Alexa AI   Abstract   1 Introduction   Natural Language Understanding ( NLU ) mod-   els are used for different tasks such as question-   answering ( Hirschman and Gaizauskas , 2001 ) , ma-   chine translation ( Macherey et al . , 2001 ) and text   summarization ( Tas and Kiyani , 2007 ) . These mod-   els are often trained on crowd - sourced data that   may contain sensitive information such as phone   numbers , contact names and street addresses . Nasr   et al . ( 2019 ) , Shokri et al . ( 2017 ) and Carlini et al .   ( 2018 ) have presented various attacks to demon-   strate that neural - networks can leak private infor-   mation . We focus on one such class of attacks ,   called Model Inversion Attack ( ModIvA ) ( Fredrik-   son et al . , 2015 ) , where an adversary aims to recon-   struct a subset of the data on which the machine-   learning model under attack is trained on . We   also demonstrate that established ML practices ( e.g.   dropout ) offer strong defense against ModIvA.In this work , we start with inserting potentially   sensitive target utterances called ‚Äò canaries‚Äôalong   with their corresponding output labels into the train-   ing data . We use this augmented dataset to train an   NLU model f. We perform a open - box attack on   this model , i.e. , we assume that the adversary has   access to all the parameters of the model , including   the word vocabulary and the corresponding em-   bedding vectors . The attack takes the form of text   completion , where the adversary provides the start   of a canary sentence ( e.g. , ‚Äò my pin code is ‚Äô ) and   tries to reconstruct the remaining , private tokens   of an inserted canary ( e.g. , a sequence of 4 digit   tokens ) . A successful attack on freconstructs all   the tokens of an inserted canary . We refer to such a   ModIvA as ‚Äò Canary Extraction Attack ‚Äô ( CEA ) . In   such an attack , this token reconstruction is cast as   an optimization problem where we minimize the   loss function of the model fwith respect to its   inputs ( the canary utterance ) , keeping the model   parameters fixed .   Previous ModIvAs were conducted on computer   vision tasks where there exists a continuous map-   ping between input images and their corresponding   embeddings . However , in the case of NLU , the dis-   crete mapping of tokens to embeddings makes the   token reconstruction from continuous increments   in the embedding space challenging . We thus for-   mulate a discrete optimization attack , in which the   unknown tokens are eventually represented by a   one - hot like vector of the vocabulary length . The   token in the vocabulary with the highest softmax   activation is expected to be the unknown token of   the canary . We demonstrate that in our attack ‚Äôs best   configuration , for canaries of type ‚Äú my pin code   iskkkk",k‚àà { 0,1 , . . . , 9},1‚â§i‚â§4 , we   are able to extract the numeric pin kkkkwith   an accuracy of 0.5(a lower bound on this accu-   racy using a naive random guessing strategy for a   combination of four digits equals 1√ó10).552Since we present a new application of ModIvA   to NLU models , defenses against them are an im-   portant ethical consideration to prevent harm and   are explored in Section 6 . We observe that stan-   dard training practices commonly used to regular-   ize NLU models successfully thwart this attack .   2 Related Work   Significant research has been conducted in the field   of privacy - preserving machine learning . Shokri   et al . ( 2017 ) determine whether a particular data-   point belongs to the training set X. The success   of such attacks has prompted research in investi-   gating them ( Truex et al . , 2019 ; Hayes et al . , 2017 ;   Song and Shmatikov , 2019 ) . Carlini et al . ( 2018 )   propose the quantification of unintended memoriza-   tion in deep networks and presents an extraction   algorithm for data that is memorized by genera-   tive models . Memorization is further exploited in   Carlini et al . ( 2020 ) where instances in the training   data of very large language - models are extracted by   sampling the model . The attacks described above   are closed - box in nature where the adversary does   not cast the attack as an optimization problem but   instead queries the model multiple times .   Open - box ModIvA were initially demonstrated   on a linear - regression model ( Fredrikson et al . ,   2014 ) for inferring medical information . It has   been extended to computer vision tasks such as fa-   cial recognition ( Fredrikson et al . , 2015 ) or image   classification ( Basu et al . , 2019 ) . Our work is a   first attempt at performing ModIvAs on NLP tasks .   3 Attack Setup   We consider an NLU model fthat takes an ut-   terance xas input and uses the word - embeddings   E(x)for the tokens in xto perform a joint in-   tent classification ( IC ) and named - entity recogni-   tion ( NER ) task . We assume an adversary with   open - box access to f , which means that they are   aware of the model architecture , trained parame-   tersŒ∏ , loss function L(f(E(x)),y ) , label set Y   of intents and entities supported by the model and   vocabulary Vwhich is obtained from the word-   embeddings matrix W‚ààI R. However , the   adversary does not have access to the training data   Xused to train f. The adversary ‚Äôs goal is to   reconstruct a ( private ) subset ÀÜx‚äÜX.   To perform a CEA on f , we keep the parame-   tersŒ∏fixed and minimize the loss function Lwith   respect to the unknown inputs ( i.e. , tokens ) of agiven utterance . This is analogous to a traditional   learning problem , except with fixed model param-   eters and a learnable input space . In this work ,   we use the NLU model architecture described in   Section 4.1 .   3.1 Canary Extraction Attacks   We consider a canary sentence x= ( x , x ) ,   x‚ààXwith tokens ( p , .. , p , u .. , u)and   output label y‚ààY. The first mtokens in x   represent a known prefix x(e.g . ‚Äúmy pin code   is ‚Äù ) and the next ntokens ( u , .. , u)represent the   unknown tokens that an attacker is interested in   reconstructing x(e.g . ‚Äúone two three four ‚Äù ) .   We represent the set of word embeddings of this   canary E(x)as(e, .. ,e , e, .. ,e ) .   A trivial attack to identify the nunknown tokens   inxis by directly optimizing L(f(E(x)),y )   over(e, .. ,e ) , where ( e, .. ,e)are ran-   domly initialized . Words corresponding to the op-   timized values of ( e, .. ,e)are then assigned   by identifying the closest vectors in the embedding   matrix Wusing a distance metric ( e.g. Euclidean   distance ) . However , our experiments demonstrate   that this strategy is not successful since the updates   are performed in a non - discrete fashion , whereas   the model fhas a discrete input space . We thus fo-   cus on performing a discrete optimization , inspired   by works on relaxing categorical variables to facili-   tate efficient gradient flow ( Jang et al . , 2016 ; Song   and Raghunathan , 2020 ) , as illustrated in Figure 1 .   We define a logit vector z‚ààI Rfor each   token u‚ààx . We then apply a softmax activation   with temperature Tto obtain a‚ààI R :   a = e   Pefor v = 1 , 2 , . . . , |V|(1 )   ais a differentiable approximation of the arg - max   over the logit vector for low values of T. This   vector then selectively attends to the tokens in the   embedding matrix , W‚ààI R , resulting in the   embeddings ( e, .. ,e)used as inputs fed to   the model during the attack :   e = W¬∑afor1‚â§i‚â§n ( 2 )   We then train our attack and optimize for Z‚àà   I R , withZ= ( z , . . . , z ):   ÀÜZ= arg minL(f(E(x)),y ) ( 3)553   Zis the only trainable parameter in the attack and   all parameters of fremain fixed . Once converged ,   we identify the token xas the one with the highest   activation in a. We decrease the temperature T   exponentially to ensure low values of Tin Equation   ( 1)and enforce the inputs to fto be discrete . In   our experiments , we define zover a subset of   candidate words for xV , V‚äÜVto prevent the   logit vector from becoming too sparse .   4 Experiments   4.1 Target Model Description   We attack an NLU model jointly trained to perform   IC and NER tagging . This model has a CLC struc-   ture ( Ma and Hovy , 2016 ) . The input embeddings   lead to 2 bi - LSTM layers and a fully - connected   layer with softmax activation for the IC task and   a Conditional Random Field ( CRF ) layer for the   NER task . The sum of the respective cross - entropy   and CRF loss is minimized during training . We   use FastText embeddings ( Mikolov et al . , 2018 ) as   inputs to our model .   4.2 Canary Insertion   We inject Rrepetitions of a single canary with sen-   sitive information and its corresponding intent and   NER labels into the training set of the NLU model .   We insert three different types of canaries with n   unknown tokens , n‚àà { 4,6,8,10 } , described in   Table 1 . Cis a set of 12 colors . Additional details   of the canaries and their output labels are presented   in the Appendix A. The adversary aims to recon-   struct all the nunknown , sensitive tokens in the   canary . The reduced vocabulary Vin Equation ( 1 )   is the set of all digits for canary callandpinand   the names of 12 colors for canary color .   4.3 Attack Evaluation   We inject the canary into Snips ( Coucke et al . ,   2018 ) , ATIS ( Dahl et al . , 1994 ) and NLU-   Evaluation ( Xingkun Liu and Rieser , 2019 ) . The   canary is repeated with R‚àà { 1,10,100,500 } . For   each combination of R , canary type and length n ,   the experiment is repeated 10 times ( trials ) with 10   different canaries , to account for variation induced   by canary selection . We define the following evalu-   ation metrics averaged across all trials to evaluate   the strength of our attack .   Average Accuracy ( Acc ): Fraction of the trials   where the attack correctly reconstructs the entire   canary sequence in the correct order . A higher   Accuracy indicates better reconstruction . Accuracy   is1if we can reconstruct all ntokens in each of   the 10 trials .   Average Hamming Distance per Token   ( HDT ): The Hamming Distance ( HD ) ( Hamming ,   1950 ) is the number of positions at which the recon-   structed utterance sequence is different from the   inserted canary . Since HD is proportional to the   length of the canary , we normalize it by the length   of the unknown utterance ( HDT = HD / n ) . The   HDT can be interpreted as the probability of recon-   structing the incorrect token for a given position in   the canary , averaged across the 10 trials . A lower   HDT indicates better reconstruction .   Accuracy reports our performance on recon-   structing allnunknown tokens in the correct order   and is a conservative metric . HDT quantifies our   average performance for reconstructing each po-554   sition in the unknown sequence . We evaluate our   attack against randomly choosing a token from the   reduced vocabulary V. Thus for a given value of   n , the expected accuracy and HDT of this baseline   are()and1‚àírespectively .   5 Results   The trivial attack described in Sec3.1 without dis-   crete optimization performs comparably to the ran-   dom selection baseline . We thus focus on perform-   ing the attack with discrete optimization in this Sec-   tion . Table 2 shows the best reconstruction metrics   for the different values of nand the correspond-   ing repetitions R‚àà { 10,100,500}at which these   metrics are observed in the Snips dataset . In our ex-   periments , our attack consistently outperforms the   baseline . For n= 4,6 , we reconstruct at least one   complete canary for each pattern . The attack also   completely reconstructs a 10 - digit pinfor higher   values of R , with an accuracy of 0.10 . Even when   we are unable to reconstruct every token in any   trial , i.e. accuracy is zero , we still outperform the   baseline , as observed from the HDT values .   For the sake of brevity , we summarize the attack   performance on other datasets in Appendix C.2 .   We observe that the attack is dataset - dependent   with best performance for the Snips dataset and   poorest for the NLU - evaluation dataset .   5.1 Discussion   The training data of NLU models may poten-   tially contain sensitive utterances such as ‚Äú call   k. . . k",k‚àà { 0,1 , . . . , 9 } . An adver-   sary who wishes to extract the phone - number can   assume the prefix ‚Äú call " , along with the output la-   bels of the utterance which are also trivial to guess , given access to the label set Y. Our canaries act   as a placeholder for such utterances . We choose to   insert the canary color since the names of colors   appear infrequently in the datasets mentioned in   Section 4.3 , allowing us to evaluate the attack on   ‚Äò out - of - distribution ‚Äô data which is more likely to be   memorized by deep networks ( Carlini et al . , 2018 ) .   Forn= 4 andR= 1 ( i.e. , the canary only   appears once in the train set ) , our attack has an   accuracy of 0.33 for canary color and 0.10 for pin .   This suggests that the attack could potentially re-   construct sensitive information from short rare ut-   terances in real - world scenarios . For a special case   when the adversary attempts to reconstruct a ten   digit phone - number in canary callwith a three digit   area - code of their choosing , the attack can recon-   struct the remaining seven digits of the number   with an accuracy of 0.1 when R= 1 . For con-   ciseness , we show these results in Appendix C.1 .   We observe that our model is more effective and   with fewer repeats for the canary color than ca-   naries pinandcallof the same length . Our empiri-   cal analysis indicates the attack is more successful   in extracting tokens that are relatively infrequent   in the training data and in reconstructing shorter   canaries . As shown in Appendix C.1 , the attack   performs best for R= 1000 . However , this trend   of improved reconstruction for larger values of R   is not monotonic and we observe a general decline   in reconstruction for R > 1000 . We are unsure of   the vulnerabilities that facilitate CEA . While un-   intended memorization is a likely explanation , we   note that our attack performs best on the Snips data ,   although the smaller ATIS data should be easier to   memorize ( Zhang et al . , 2016 ) .   6 Proposed Defenses against ModIvA   We propose three commonly used modeling tech-   niques as defense mechanisms- Dropout ( D ) , Early   Stopping ( ES ) ( Arpit et al . , 2017 ) and including   a Character Embeddings layer in the NLU model   ( CE ) . D and ES are regularization techniques to re-   duce memorization and overfitting . CE makes the   problem in 3 more difficult to optimize , by concate-   nating the embeddings of each input token with a   character level representation . This character level   representation is obtained using a convolution layer   on the input sentence ( Ma and Hovy , 2016 ) .   For defense using D , we use a dropout of 20 %   and 10 % while training the NLU model . For ES ,   we stop training the NLU model under attack if the555validation loss does not decrease for 20 consecutive   epochs to prevent over - training .   6.1 Efficacy of Defenses   In this section we present the performance of the   proposed defenses against ModIvA. To do so , we   evaluate the attack on NLU models trained with   each defense mechanism individually , and in all   combinations . The canaries are inserted into the   Snips dataset and repeated 10,500and1000 times .   The results are summarized in Table 3 . We observe   that the attack accuracy for each defense ( used indi-   vidually and in combination ) is nearly zero for all   canaries and is thus omitted in the table . We also   note that the HDT approaches the random baseline   for most defense mechanisms . The attack perfor-   mance is comparable to a random - guess when the   three mechanisms are combined . However , when   dropout or character embedding is used alone , HDT   values are lower than the baseline , indicating the   importance of combining multiple defense mecha-   nisms . Additionally , training with defenses do not   have any significant impact on the performance of   the NLU model under attack . The defenses thus   successfully thwart the proposed attack without   impacting the performance of the NLU models.7 Conclusion   We formulate and present the first open - box   ModIvA in a form of a CEA to perform text com-   pletion on NLU tasks . Our attack performs discrete   optimization to select unknown tokens by optimiz-   ing over a set of continuous variables . We demon-   strate our attack on three patterns of canaries and   reconstruct their unknown tokens by significantly   outperforming the ‚Äò chance ‚Äô baseline .   To ensure that the proposed attack is not misused   by an adversary , we propose training NLU mod-   els with three commonplace modelling practices ‚Äì   dropout , early - stopping and including character   level embeddings . We observe that the above prac-   tices are successful in defending against the attack   as its accuracy and HDT values approach the ran-   dom baseline . Future directions include ‚Äò demystify-   ing‚Äôsuch attacks , and strengthening the attack for   longer sequences with fewer repeats and a larger V   and investigating additional defense mechanisms ,   such as those based on differential privacy , and   their effect on the model performance .   8 Ethical Considerations   The addition of proprietary data to existing datasets   to fine - tune NLU models can often insert confi-   dential information into datasets . The proposed   attack could be misused to extract private infor-   mation from such datasets by an adversary with   open - box access to the model . The objectives of   this work are to ( 1 ) study and document the actual   vulnerability of NLU models against this attack ,   which shares similarities with existing approaches   ( Fredrikson et al . , 2014 ; Song and Raghunathan ,   2020 ) ; ( 2 ) warn NLU researchers against the pos-   sibility of such attacks ; and ( 3 ) propose effective   defense mechanisms to avoid misuse and help NLU   researchers protect their models .   Our work demonstrates that private information   such as phone - numbers and zip - codes can be ex-   tracted from a discriminative text - based model ,   and not only from generative models as previ-   ously demonstrated ( Carlini et al . , 2020 ) . We advo-   cate for the necessity to privatize such data using   anonymization ( Ghinita et al . , 2007 ) or differential   privacy ( Feyisetan et al . , 2020 ) . Additionally , in   case the training data continues to contain some   private information , practitioners can prevent the   extraction of sensitive data by using the defense   mechanisms described in Section 6 , which reduces   the attack performance to a random guess.556References557558A Inserted Canary Information   The inserted canaries and corresponding intent and   NER label sets are listed below .   1.Canary call : ‚Äú call k. . . k",k‚àà   { 0,1 , . . . , 9 } , for1‚â§i‚â§n .   ‚Ä¢Sequence NER tags : ‚Äú O B - canary   I - canary . . .I - canary| { z } "   ‚Ä¢ Intent : ‚Äú CallIntent "   2.Canary 2 : ‚Äú my pin code is k. . . k",k , for   1‚â§i‚â§n .   ‚Ä¢Sequence NER tags : ‚Äú O O O O B - canary   I - canary . . .I - canary| { z } "   ‚Ä¢ Intent : ‚Äú PinIntent "   3.Canary 3 : ‚Äú color k. . . k",k‚àà{‚Äòred ‚Äô ,   ‚Äò green ‚Äô , ‚Äò lilac ‚Äô , ‚Äò blue ‚Äô , ‚Äò yellow ‚Äô , ‚Äò brown ‚Äô ,   ‚Äò cyan ‚Äô , ‚Äò magenta ‚Äô , ‚Äò orange ‚Äô , ‚Äò pink ‚Äô , ‚Äò purple ‚Äô ,   ‚Äò mauve ‚Äô } for 1‚â§i‚â§n .   ‚Ä¢Sequence NER tags : ‚Äú O B - canary   I - canary . . .I - canary| { z } "   ‚Ä¢ Intent : ‚Äú ColorIntent "   The canary repetitions Rare split between the train   and validation set in a ratio of 9 : 1 .   B Training Parameters   We decrease the temperature Texponentially after   each iteration t. The temperature at the titeration   Tis given by T= 0.997√ó10 .   We use the Adam optimizer and train our attack   for 250 epochs . We begin with an initial learning   rate of 6.5√ó10for our attack with a decay rate   of9.95√ó10 .   C Results   C.1 Attack Performance Across Canary   Repetitions   Table 4 shows the model performance for just one   repeat of the canary in the Snips dataset i.e. R=   1 . The n= 7 example for the callcanary refers   to the special case when the adversary is trying   to reconstruct a 10 - digit phone number beginning   with a three digit area code of their choice .   Table 5 illustrates the best reconstruction   metrics for different values on nand with   R‚àà { 10,100,500,1000,2000 } . We observe an   accuracy of 0.5 for the canary pinwhen n= 4   andR= 1000 . Figure 2 illustrates the model   performance across canaries in the Snips dataset   with varying number of repetitions R. As observed   in Table 5 and Figure 2 , the attack is most likely   to succeed when Ris 1000 . However , the attack   weakens for higher values of R.   C.2 Attack Performance Across Datasets   We evaluate our attack on the ATIS and NLU-   Evaluation Datasets , for canaries color andpin   withn= 4 and canary call withn= 10 . To   ensure that we maintain a comparable number   or repeats with respect to the size of the dataset ,   R‚àà { 10,100,200,500}for the ATIS dataset and   R‚àà { 100,500,1000,5000,10000}for the NLU-   Evaluation dataset . As shown in Figure 3 , the at-   tack performance is almost comparable for shorter   sequences in Snips and ATIS but under - performs   for the NLU - Evaluation data . Figure 4 and Figure   5 illustrate the HDT for the ATIS and NLU Evalua-   tion datasets for Rcanary repetitions respectively.559560   Yang Trista Cao , Yada Pruksachatkun , Kai - Wei ChangRahul Gupta   Varun Kumar , Jwala Dhamala , Aram GalstyanUniversity of Maryland , College ParkAmazon Alexa AI - NU , University of California , Los AngelesInformation Sciences Institute , University of Southern California   ycao95@umd.edu , yada.pruksachatkun@gmail.com   { kaiwec , gupra , kuvrun , jddhamala , argalsty } @amazon.com   Abstract   1 Introduction   Recent natural language processing ( NLP ) systems   use large language models as the backbone . These   models are first pre - trained on unannotated text and   then fine - tuned on downstream tasks . They have   been shown to drastically improve the downstream   task performance by transferring knowledge from   large text corpora . However , several studies ( Zhao   et al . , 2019 ; Barocas et al . , 2017 ; Kurita et al . , 2019 )   have shown that societal bias are also encoded in   these language models and transferred to down-   stream applications . Therefore , quantifying the bi-   ases in contextualized language representations is   essential for building trustworthy NLP technology .   To quantify these biases , various fairness met-   rics and datasets have been proposed . They can be   roughly categorized into two categories : extrinsic   andintrinsic metrics ( Goldfarb - Tarrant et al . , 2021 ) .   Intrinsic fairness metrics probe into the fairness ofthe language models ( Guo and Caliskan , 2021 ; Ku-   rita et al . , 2019 ; Nadeem et al . , 2020 ; Nangia et al . ,   2020 ) , whereas extrinsic fairness metrics evaluate   the fairness of the whole system through down-   stream predictions ( Dhamala et al . , 2021 ; Jigsaw ,   2019 ; De - Arteaga et al . , 2019 ) . Extrinsic metrics   measure the fairness of system outputs , which are   directly related to the downstream bias that affects   end users . However , they only inform the fairness   of the combined system components , whereas in-   trinsic metrics directly analyze the bias encoded in   the contextualized language models .   Nevertheless , the relationship between upstream   and downstream fairness is unclear . While some   prior work has demonstrated that biases in the up-   stream language model have significant effects on   the downstream task fairness ( Jin et al . , 2021 ) , oth-   ers have shown that intrinsic and extrinsic metrics   are not correlated ( Goldfarb - Tarrant et al . , 2021 ) .   These studies either focus on one specific applica-   tion or consider static word embeddings . Therefore ,   it is still obscure how fairness metrics correlate   across different tasks that use contextualized lan-   guage models .   To better understand the relationship between   intrinsic and extrinsic fairness metrics , we conduct   extensive experiments on 19pre - trained language   models ( BERT , GPT-2 , etc . ) . We delve into three   kinds of biases , toxicity , sentiment , and stereotype ,   with six fairness metrics across intrinsic and ex-   trinsic metrics , in text classification and generation   downstream settings . The protected group domains   we focus on are gender , race , and religion .   Similar to the observations in static embeddings   ( Goldfarb - Tarrant et al . , 2021 ) , we find that these   metrics correlate poorly . Therefore , when evaluat-   ing model fairness , researchers and practitioners   should be careful in using intrinsic metrics as a   proxy for evaluating the potential for downstream   biases , since doing so may lead to failure to detect   bias that may appear during inference . Specifi-561cally , we find that correlations between intrinsic   and extrinsic metrics are sensitive to alignment in   notions of bias , quality of testing data , and pro-   tected groups . We also find that extrinsic metrics   are sensitive to variations on experiment configu-   rations , such as to classifiers used in computing   evaluation metrics . Practitioners thus should en-   sure that evaluation datasets correctly probe for   the notions of bias being measured . Additionally ,   models used to compute evaluation metrics such as   those in BOLD ( Dhamala et al . , 2021 ) can intro-   duce additional bias , and thus should be optimized   to be robust .   The main contribution of our work is as follows :   First , we conduct an extensive study on correla-   tions between intrinsic and extrinsic metrics . Sec-   ond , we conduct ablation studies to show the ef-   fect of ( mis)alignment of notions of bias and pro-   tected groups , and noise in recent fairness evalua-   tion datasets . Finally , we conduct a study on the   robustness ( or lack thereof ) of sentiment classifiers   to text generation quality , diversity , and noise .   With this paper , we aim to suggest preliminary   best practices in bias evaluation , which is important   to quantify progress in debiasing work that will   make models usable in the real world .   2 Fairness Evaluation Metrics   The notion of bias we focus on is group dispari-   ties in sentiment , toxicity andstereotypes , which   have been extensively researched in the context of   fairness ( Huang et al . , 2020 ; Zhang et al . , 2020 ;   Borkan et al . , 2019 ; Adragna et al . , 2020 ) . The   metrics are chosen to represent a variety of down-   stream use cases and source data . We use CEAT   andILPS for intrinsic metrics for toxicity and sen-   timent , HX , TD , and B - Tfor extrinsic   toxicity metric , and B - Sfor extrinsic sentiment   metric . For stereotype , we use S Sand   B - S for intrinsic and extrinsic metrics .   Intrinsic : Contextualized Embedding Associa-   tion Test ( CEAT ) ( Guo and Caliskan , 2021 ) mea-   sures bias through word embedding distances in   semantic vector space . Given a set of protected   group ( e.g. ‚Äú woman ‚Äù ) and two polar attribute sets   ( e.g. pleasant words vs unpleasant words ) , CEAT   measures the effect size of distance difference be-   tween the protected group and the two attribute   sets . A lower effect size score indicates the pro-   tected group is closer to the negative polar of the   attribute ( e.g. unpleasant ) and thus probably morecorrelated with toxic words .   Intrinsic : Increased Log Probability Score   ( ILPS ) ( Kurita et al . , 2019 ) measures bias on   word associations between protected groups and   attributes with templates such as ‚Äú The [ group ] is   < mask > . ‚Äù It computes the increase in probability of   the attribute ( e.g. ‚Äú happy ‚Äù ) for the < mask > token   given a protected group ( e.g. ‚Äú woman ‚Äù ) . A higher   ILPS score means the protected group is more cor-   related with the attribute , thus a higher ILPS score   with unpleasant words indicate the protected group   is more correlated with negative sentiment .   Intrinsic : StereoSet ( Nadeem et al . , 2020 ) mea-   sures bias in language model scoring of stereo-   typed sentences over anti - stereotyped sentences .   Its dataset consists of minimal pair stereotyped and   anti - stereotyped sentence pairs for each protected   group . We only focus on their intrasentence pairs ,   where the sentences in each pair are only differ-   ent in the attributes ( e.g. ‚Äú The Iranian man might   be a terrorist individual ‚Äù and ‚Äú The Iranian man   might be a hardworking individual ‚Äù is a sentence   pair for Iranian group ) . The stereotype score for   each protected group is computed as the propor-   tion of pairs where the stereotyped sentences has a   higher pseudo loglikelihood than its antistereotypi-   cal counterpart .   Extrinsic : Jigsaw Toxicity ( TD ) ( Jigsaw ,   2019 ) measures bias in toxicity detection systems   that covers multiple protected groups . The fairness   notion is defined by equalized odds , which mini-   mizes differences in False Positive Rate ( FPR ) to   ensure that text containing mentions of any one   group is not being unjustly mislabelled as toxic .   This is important for the classifiers to be able to de-   tect toxicity in content containing identifiers across   all protected groups , while not silencing any one .   Extrinsic : HateXPlain ( HX ) ( Mathew et al . ,   2020 ) measures bias in hate speech detection sys-   tems . While the original problem is cast as a mul-   ticlass classification problem ( normal , offensive ,   toxic ) , we cast it as a binary problem ( toxic , non-   toxic ) due to lack of consistency in what is labelled   as offensive and/or toxic . Similar to TD , the   measure of bias against a certain group is the False   Positive Rate on examples with group mentions .   Extrinsic : BOLD ( Dhamala et al . , 2021 ) is a   dataset that measures bias in language generation   that consist of Wikipedia - sourced natural prompts .   Given a prompt containing direct or indirect men-   tions of a protected group , BOLD evaluates the562quality of the sentences finished by the language   model . We focus on the sentiment ( B - S ) met-   ric for sentiment , toxicity ( B - T ) metric for toxi-   city , and regard ( B - R ) metric for stereotype .   Additionally , for stereotype , we train a stereotype   classifier by finetuning the BERT model with Stere-   oSet ( Nadeem et al . , 2020 ) , CrowS - Pairs ( Nangia   et al . , 2020 ) , and Social Bias Frames ( Sap et al . ,   2020 ) datasets , and use this classifier to evaluate   BOLD generations on stereotype ( B - S ) .   The bias score for each protected group is cal-   culated as the average toxicity , sentiment , regard ,   and stereotype score on the generations from the   prompts with that protected group .   3 Correlation between Metrics   Experiment Setup We conduct a study on gen-   der , race , and religion domains ( see the Ap-   pendix A for the list of protected groups on   each domain ) . We conduct correlation analysis   on the variance of group metric scores across   protected groups , as it captures score disparities   across protected groups for each domain . For   example , for M = CEAT , we define S=   Var(s , s , s , ... ) . A less - biased model   would have smaller variance score . Thus , if two   metrics are correlated , we would see a positive cor-   relation , as reducing the disparity between groups   in one metric , as measured by variance would re-   duce that in the other .   We evaluate 19 popular pre - trained language   models . These models consist of ALBERT   ( Lan et al . , 2020 ) ( base - v2 , large - v2 , xlarge - v2 ,   xxlarge - v2 ) , BERT ( Devlin et al . , 2019 ) ( base-   cased , large - cased ) , RoBERTa ( base , large ) , Dis-   tilRoBERTa ( Sanh et al . , 2019 ) , GPT2 ( Radford   et al . , 2019 ) ( base , medium , large , xl ) , DistilGPT2 ,   EleutherAI / gpt - neo ( Black et al . , 2021 ) ( 125 M ,   1.3B , 2.7B ) , and XLNet ( Yang et al . , 2019 ) ( base-   cased , large - cased ) . For intrinsic metrics , we sim-   ply measure the corresponding metric scores on   the language models . For extrinsic metrics , we   fine - tune language models for classification - based   tasks , and either sample in an autoregressive man-   ner for autoregressive language models , or use ran-   dom masking - based generation for MLM - based   models ( Wang and Cho , 2019 ) following the BOLD   paper , for generation - based tasks .   For each intrinsic and extrinsic metric pair , we   take the intrinsic and extrinsic scores for each563   model . With the list of score pairs from the 19   models , we compute the correlation using the Pear-   son correlation coefficient . If the metrics are pos-   itively correlated , the correlation score should be   close to 1 . Figure 1 depicts some examples of the   correlation plots .   Correlation Results Table 1 contains correla-   tions scores for each intrinsic / extrinsic metric pair   on sentiment and toxicity . Only few metrics have   significantly positive correlations . In general , ILPS   has more significantly positive correlations with the   extrinsic metrics compared to CEAT , except for   the religion domain . This may due to the nature   of the two intrinsic metrics ‚Äì ILPS is calculated   with log probabilities , which is more related to the   downstream generative tasks such as BOLD since   generation samples based on log probabilities .   For sentiment metrics , we find more statistically   significant positive correlations between intrinsic   metrics and B - S than toxicity extrinsic metrics .   In both toxicity and sentiment , we see that there   are statistically negative correlations for the reli-   gion domain , which we investigate in Section 3.2 .   For stereotype , Table 2 contains the results on   stereotype metrics . We see that none of the correla-   tions are significant nor positive .   4 Ablation Study   There are many factors at play in fairness evalu-   ation processes , such as notion of bias measured ,   choice of protected groups , quality of the testing   data , and confounding factors in the models used   to compute metrics themselves . In this section , we   conduct careful analysis to explore why extrinsic   and intrinsic metrics are not always correlated.4.1 Misalignment between metrics   In our main study , we use the experimental settings   defined in their original papers . However , these   metrics may have subtle misalignments in type of   bias measured , protected groups factored in calcu-   lation , and characteristics of the evaluation dataset .   Misalignment on the notion of bias Among the   toxicity metrics , the notion of bias are not con-   sistent ‚Äì some measure sentiment ( CEAT , ILPS ,   B - S ) while others measure toxicity . Therefore ,   we recompute CEAT scores with toxicity word   seeds , which we denote as CEAT . We manu-   ally pick 20 toxic and 20 anti - toxic words from the   word clouds of the toxic and non - toxic labeled sen-   tences in the JigsawToxicity dataset for CEAT .   See Appendix D for the full list of the words .   As seen in Table 3 , the correlations between   the toxicity - related extrinsic metrics and CEAT   are more positive than with CEAT . Also note   thatCEAT is better correlated with B - S than   CEAT , except for religion . Though many of the   correlation scores remain not statistically signifi-   ca nt , the result supports our hypothesis that intrin-   sic and extrinsic metrics are more correlated when   they have the same notion of bias .   Misalignment on the protected groups Due   to the limited number of overlapping protected   groups ( stereotype metrics only have four groups   in common ) , we compute the domain - level vari-   ance scores for all protected groups contained in a   dataset . However , the groups that are not present   in both the evaluation datasets for intrinsic and   extrinsic metrics may introduce metric disalign-   ment , as they would be factored in metric compu-   tation in one but not the other . We recompute the   correlation of S Swith B - R and   B - S with only overlapping protected race   groups : White , Black , Hispanic , and Asian .   We find the correlation of S Swith B-   R raises from ‚àí0.08to0.19(p - value 0.56 ) .   The correlation with B - S increases from   ‚àí0.18to0.08(p - value 0.80 ) . These metrics are   more positively correlated with the aligned groups .   Misalignment on evaluation dataset We ob-   serve that dataset sources for certain metrics are   misaligned , such as that for BOLD and S -S.S Suses crowdworkers to gener-   ate testing data specifically to contain particular564stereotypes . On the other hand , BOLD prompts are   sourced from Wikipedia , which consist of more for-   mal writing and is not directly engineered to probe   for stereotypes . Examples of source misalignment   can be seen in the Appendix .   To align the stereotype metrics , we use data from   theS Sintersentence dataset , which con-   sists of a one - sentence context followed by a rele-   vant stereotyped sentence , to compute BOLD met-   rics . Specifically , we use the context sentence for   BOLD - like generation ( see Appendix F for gen-   eration examples ) . We test S Swith the   new B - S on the race domain and find that   the correlation score increase from ‚àí0.18to0.02   ( p - value 0.98 ) . This indicates that aligning the   evaluation dataset source has a modest impact on   improving correlation between metrics .   4.2 Noise in Evaluation Datasets   As pointed out in Blodgett et al . ( 2021 ) , some fair-   ness evaluation datasets lack consistency in fram-   ing and data collection methodology , which leads   to datasets not properly evaluating the intended no-   tion of bias . We find evidence of this phenomena in   the BOLD dataset for religion prompts , which con-   tain toxic and stereotyped content , which will bias   generations to be more toxic for certain groups . To   debias BOLD , we use the sentiment , regard , and   toxicity classifier to filter out prompts that have   higher polarity values , and recalculate the correla-   tions of intrinsic metrics with BOLD - related extrin-   sic metrics on religion domain . We find that scores   forCEAT andB - S increases to 0.11,S -SandB - S increases to 0.10 . This indi-   cates that bias in datasets can affect the metrics .   4.3 Effect of Experiment Configuration on   Metric Scores   Experiment configurations may also affect the   amount of bias detected in fairness metrics , which   we observe in BOLD metrics . In our main study ,   we fix several configurations for BOLD to isolate   the effect of the underlying language models in   our correlation study from confounding factors ,   notably 1 ) the sampling procedure and 2 ) the eval-   uation classifiers used to compute metrics . We   conduct additional experiments to show the effect   of varying these configurations .   Impact of sampling temperature on classifier-   based metrics We input five sample prompts ( en-   listed in Appendix G ) from BOLD dataset to GPT-2model and for each prompt , generate 100 sentences .   We use two temperature settings ( T = 0.5 and T =   1.0 ) and compute the average sentiment over the   generated sentences . We observe that the propor-   tion of negative sentiment assignment increases   from 4.6 % to 15.6 % by changing the temperature ,   and thus the generation quality and diversity .   Impact of noise in generated outputs on clas-   sifier based metrics We introduce noise to 500   BOLD generations through word swaps or dele-   tions ( examples shown in Appendix H ) . We then   feed these perturbed generations into the sentiment   and regard models used in BOLD metric compu-   tation . As shown in Appendix H , these noise ad-   ditions have a moderate amount of impact in the   classification , reducing the proportion of negative   sentiment from 13.6 % to 12.18 % and proportion   of negative sentiment from 25.2 % to 22.86 % .   These experiments serve as a case study on the   additional confounding factors in downstream met-   rics that are not present in upstream metrics . Thus ,   when evaluating downstream tasks , authors should   identify and show the effect of such experiment   configurations on metrics , so that model users are   aware of the various factors that can lead to the   detection ( or lack thereof ) of bias in these models .   5 Conclusion   We present a study on intrinsic and extrinsic fair-   ness metrics in contextualized word embeddings .   Our experiments highlight the importance of align-   ment in the evaluation dataset , protected groups ,   and the quality of the evaluation dataset when it   comes to aligning intrinsic and extrinsic metrics .   Based on this study , we impart three takeaways   for researchers and developers . First , we can not   assume that an improvement in language model   fairness will fix bias in downstream systems . Sec-   ondly , when choosing fairness metrics to evaluate   and optimize for , it is important to choose a metric   that is closest to the downstream application . If   that is not possible for all downstream applications ,   then it is important to align intrinsic metrics to the   extrinsic use cases . Finally , it is important to mit-   igate factors that may lead to bias in the metric   computation itself , including noise in evaluation   datasets , models used in metric computation , and   inference experiment configurations such as decod-   ing temperature for text generation.5656 Broader Impact Statement   This work shows preliminary evidence against an   assumption in prior fairness and bias literature -   that lack of bias in upstream tasks are correlated   with that in downstream tasks , and the effect of   model settings on fairness evaluation . We hope   that this paper will contribute to the formulation of   best practices in bias evaluation .   References566567Warning : This appendix contains examples   which are offensive in nature .   A Protected Groups   Table 4 list all the protected groups covered by each   fairness metric .   B Stereotype Classifier   We fine - tune BERT - base - uncased model on three   datasets : StereoSet ( Nadeem et al . , 2020 ) , CrowS-   Pairs ( Nangia et al . , 2020 ) , and Social Bias   Frames ( Sap et al . , 2020 ) . We fine - tune for 1 epoch   with learning rate 5e , dropout 0.1and batch   size 16 .   C Finetuning for Classification Tasks   Toxicity Detection classifiers are fine - tuned for   1 epoch with learning rate 5e , dropout of 0.1 .   The average F1 score of the classifiers is 0.95 .   Hate Speech classifiers are fine - tuned for 3   epochs with a learning rate of 3e , and dropout   of0.1 . The average F1 score of the classifiers is   0.79 .   D CEATAttribute Word Sets   The list of toxic attributes are : stupid , idiot , ig-   norant , racist , crap , kill , dumb , ridiculous , damn ,   pathetic , hate , shit , corrupt , ass , garbage , fool , hell ,   disgusting , criminal , violence .   The list of anti - toxic attributes are : like , good ,   right , care , hope , love , future , agree , thank , health ,   top , interesting , responsible , friend , respect , happy ,   energy , truth , benefit , glad .   EExamples of Misalignment in Source in   BOLD and StereoSet   Table 6 show examples in BOLD and StereoSet   that probe for bias against a particular group , but   differ in language due to the difference in sources   the datasets are from .   F BOLD Generation Examples with   Prompts from StereoSet   See Table 5 for examples of BOLD generations   using S Sprompts .   G Sample points used to test the impact   of temperature on sentiment scores   The five BOLD prompts we used to test the im-   pact of the temperature parameter on sentiment areas follows : ( i ) " The fundamental moral qualities   in Islam " , ( ii ) " As a religion , Islam emphasizes " ,   ( iii ) " Islamic theology says that all of " , ( iv ) " Is-   lam teaches that the creation of " , ( v ) " The primary   scriptures of Islam are " . We obtain 100 generations   for each of these prompts . 4.6 % of the generations   are classified to carry negative sentiment with a   temperature value of 0.5 , while 15.6 % of the gen-   erations are classified to carry negative sentiment   with a temperature value of 1 .   H Investigating the impact of synthetic   noise on sentiment classification   Our fairness metrics are statistical models which   are trained on clean text . Since , statistical model   may not robust to noise , we evaluate their perfor-   mance on low - quality data . In order to evaluate the   impact of noisy input on fairness metrics , we took   500BOLD generations , and then generated noisy   version of the sentences by randomly swapping or   deleting words in a given sentence . For a given   sentence , we generate 10 noisy sentences . An   example of noise addition is give below .   Original : ‚Äú the person was well - known for her   work as a feminist activist and a writer . ‚Äù   Modified using word deletion : ‚Äú the person was   for her work a feminist activist and a writer . ‚Äù   We run the fairness classifiers on the original   and modified sentences , and compute the propor-   tion of sentences assigned to the negative class . We   repeat experiments 10 times and report mean neg-   ative scores with the standard deviation . Table 7   presents the results and we observe that both re-   gard and sentiment classifiers are susceptible to   give different results with different noise levels.568569Dataset used Proportion of Proportion of   negative regard negative sentiment   Clean data 25.2 13.6   5 % word swap 25.12 ( 0.21 ) 13.52 ( .10 )   10 % word swap 24.65 ( 0.37 ) 13.45 ( 0.32 )   15 % word swap 24.54 ( 0.67 ) 13.20 ( 0.26 )   20 % word swap 24.12 ( 0.49 ) 13.28 ( 0.35 )   5 % word deletion 24.88 ( 0.61 ) 13.24 ( 0.30 )   10 % word deletion 24.30 ( 0.50 ) 12.72 ( 0.68 )   15 % word deletion 23.38 ( 0.75 ) 12.30 ( 0.45 )   20 % word deletion 22.86 ( 0.49 ) 12.18 ( 0.42)570   Chen Yu andDaniel Gildea   Department of Computer Science   University of Rochester   Rochester , NY 14627   Abstract   1 Introduction   Abstract Meaning Representation ( AMR ) ( Ba-   narescu et al . , 2013 ) is a graph that encodes the   semantic meaning of a sentence . In Figure 1a , we   show the AMR of the sentence : You told me to   wash the dog . AMR has been widely used in many   NLP tasks ( Liu et al . , 2015 ; Hardy and Vlachos ,   2018 ; Mitra and Baral , 2016 ) .   AMR parsing is the task of mapping a sentence   to an AMR semantic graph automatically . A graph   is a complex data structure which is composed of   multiple vertices and edges . There are roughly four   types of parsing strategies in previous work :   ‚Ä¢Two - Stage Parsing ( Flanigan et al . , 2014 ;   Lyu and Titov , 2018 ; Zhang et al . , 2019a ;   Zhou et al . , 2020 ): first produce vertices , and   produce edges after that .   ‚Ä¢Transition - Based Parsing ( Damonte et al . ,   2016 ; Ballesteros and Al - Onaizan , 2017 ; Guo   and Lu , 2018 ; Wang and Xue , 2017 ; Naseem   et al . , 2019 ; Astudillo et al . , 2020 ; Zhou et al . ,   2021 ): process the sentence from left to right ,   and produce vertices and edges based on the   current focused word .   ‚Ä¢Graph - Based Parsing ( Zhang et al . , 2019b ;   Cai and Lam , 2019 , 2020 ): produce vertices   and edges based on a graph traversal order ,   such as DFS or BFS .   ‚Ä¢Sequence - to - Sequence Parsing ( Konstas   et al . , 2017 ; van Noord and Bos , 2017 ; Peng   et al . , 2017 , 2018 ; Xu et al . , 2020 ; Bevilacqua   et al . , 2021 ): this method linearizes the AMR   graph to a sequence , then uses a sequence - to-   sequence model to do the parsing .   Bevilacqua et al . ( 2021 ) achieved the state - of-   the - art performance by using the last seq - to - seq   strategy . They linearized the AMR graph ( see Fig-   ure 1b ) and fine - tuned BART ( Lewis et al . , 2020 ) , a   denoising sequence - to - sequence pretrained model   based on Transformer ( Vaswani et al . , 2017 ) , for   the parsing . We briefly show the method in Fig-   ure 2 . During training , they linearize all the AMR   graphs in the training dataset into sequences , then   they can fine - tune the BART model in this new   sequence - to - sequence dataset . At inference time ,   they first generate the AMR sequence using the   BART model , then they recover the AMR graph   from this sequence .   However , purely treating the graph as a sequence   may not take advantage of important information   about the structure of the graph . When generating571   the last token dogin Figure 1b , for example , the dot-   product attention layer in the Transformer Decoder   attends to all the previous tokens and lets the model   learn the weight of these tokens . However , if we   can tell the model which tokens are its ancestors ,   like its parent is wash-01 and its grand - parent is   tell-01 ( see Figure 1a ) , it will make this token much   easier to generate . Adding graph structure has been   demonstrated to be useful for the AMR - to - text task   ( Zhu et al . , 2019 ; Yao et al . , 2020 ; Wang et al . ,   2020 ) . These approaches added the graph structure   to the Transformer Encoder . Therefore , we expect   that adding structure in Transformer Decoder for   AMR parsing task will also be helpful .   In this paper , we base our work on the seq - to - seq   model of Bevilacqua et al . ( 2021 ) with the AMR   linearized by DFS traversal order . We introduce   several strategies to add ancestor information into   the Transformer Decoder layer . We also propose a   novel strategy , which consists of setting parameters   in the mask matrix for those ancestor tokens and   tuning them . We find that this new strategy makes   the largest improvement .   2 Add Ancestors Information into Model   2.1 DFS linearization and Ancestors   The DFS linearization of Bevilacqua et al . ( 2021 )   used pairs of parentheses to indicate the start and   the end of exploring a node in the DFS traversal   order . The readers can use Figure 1 as an example   and are referred to Bevilacqua et al . ( 2021 ) for   more details .   This means when generating the next token , we   can construct the partial graph from previous to-   kens and determine the ancestors tokens among   them . In Figure 3b , for example , when we generate   the token I , we can construct the partial graph in   Figure 3a and find its ancestors ( tell-01 ‚Äì > : ARG2   ‚Äì > ) .   If AMR were a tree , then the ancestors of each   token would be clear to define . However , since   AMR is a graph , one node may be visited multiple   times ( which is called re - entrancy ) , which brings   ambiguity to find the ancestors . For example , in   Figure 4 , when we generate the last token < R2 > , it   is actually the re - entrancy of the token Igenerated   before . Under this circumstance , we will use the   tokens in the new path ( tell-01 ‚Äì > : ARG1 ‚Äì > wash-   01 ‚Äì > : ARG0 ‚Äì > ) as its ancestors . We can not use   tokens from the old path ( tell-01 ‚Äì > : ARG2 ‚Äì > ) ,   since we can not know it is a re - entrancy before we   have actually generated it .   2.2 Transformer Background   The original Transformer ( Vaswani et al . , 2017 )   used scaled dot - product self - attention . Typically ,   the input of the attention consists of a query ma-   trixQ , a key matrix Kand a value matrix V , the   columns of which represent the query vector , the   key vector and the value vector of each token . The572attention matrix can be calculated as follows :   Attention ( Q , K , V , M ) = SoftmaxS‚àö   d+M   V ,   S = QK ,   where Q , K , V ‚ààR , Nis the length of the   sequence , dis the dimension of the model , and M   is the mask matrix to control which tokens in the   sequence are attended for a given token .   A typical Transformer module consists of several   layers . In each layer it uses MultiHead attention .   For each head , it calculates attention as above , and   then averages the results .   In the Encoder self - attention and Encoder-   Decoder attention layers , the mask matrix is the   same across all the heads and all the layers , and   all the elements in the matrix are 0 , meaning all   the tokens are attended . But in the Decoder self-   attention layers , the elements denoting the attention   to the future token ( Mwithi < j ) are set to ‚àí‚àû ,   meaning that they have no effect when calculating   the weighted sum .   2.3 Add Ancestor Information into Model   We focus on the mask matrix Min the Trans-   former Decoder self - attention layers to add the   ancestor information during the parsing . We in-   troduce two strategies : a hard strategy and a novel   soft strategy .   Hard Strategy Under this strategy , we set ele-   ments denoting the ancestors to 0 , and the elements   denoting the non - ancestors to ‚àí‚àû inM , such that   only the ancestor tokens are attended . We will ex-   plore the influence by using the new mask matrix   only on some decoder layers or on some heads .   Soft Strategy Under this novel strategy , we will   not mask the non - ancestor tokens and abandon   them in a hard way . Instead , what we do is only   telling the model which are the ancestor tokens   and letting the model learn the weights by itself .   Specifically , we use three different values in the   mask matrix : ‚àí‚àû for all future tokens ; 0 for all   non - ancestor previous tokens ; parameter Œ±for all   ancestor tokens . We let the model learn the weight   Œ±to control how much it should focus on the an-   cestor tokens . Similar to the hard strategy , we will   also explore the influence by setting different pa-   rameters on different layers or on different heads.2.4 Inference   During the inference stage , the input of the de-   coder is no longer the complete linearized AMR   sequence . Instead , it is dynamically extended , and ,   at each step , the input is the tokens that have been   generated during the previous steps . A natural ques-   tion is : how can we find the ancestors of a token   when we do n‚Äôt yet have a complete sequence ( and   therefore ca n‚Äôt convert it to a graph to find its an-   cestors ) .   Fortunately , the DFS linearization uses several   special tokens to denote the graph structure . We   can rely on two special tokens to find the ancestors   of a token : relation tokens ( e.g. : ARG0 ) and the   parentheses . The basic idea is : we maintain an   ancestor stack for the token that will be generated ,   and adjust it according to the generated token . If   a relation token is generated , we know that the   previous siblings have been completely explored ,   so we will remove all the tokens of that sibling   from the ancestor stack . If a right parenthesis is   generated , we know that a token has been explored   and we should return to its parent token , so we will   remove it and all its descendants from the ancestor   stacks . We always add the generated token ( except   the right parenthesis ) into the ancestor stack after   these special operations .   In Figure 5 , we give an example of how to find   the ancestor tokens during inference . In 1 ) , the last   token is the right parenthesis , meaning the last to-   kenyouhas been explored completely and should   be removed from the ancestor token list . Therefore ,   we remove the tokens in the ancestor list backwards   until we encounter a left parentheses . In 2 ) , the last   token is a relation token , meaning the previous sib-   ling has been explored completely , so we remove   the tokens in the ancestor list backwards until we   encounter a previous relation token , then add the   current relation token in the list . The steps 3 ) , 4 )   and 5 ) are following the same rule .   3 Experiments   3.1 Setup   Dataset We use the AMR 2.0 ( LDC2017T10 )   and AMR 3.0 ( LDC2020T02 ) dataset . The AMR   2.0 includes 39,260 manually - created graphs , and   the AMR 3.0 includes 59,255 . The AMR 2.0 is   a subset of AMR 3.0 . Both datasets are split into   training , development and test datasets.573   Pre - processing and Post - processing We use the   same DFS - based linearization technique as Bevilac-   qua et al . ( 2021 ) . We omit the detail here , but the   reader can refer to Figure 1 as an example . In the   pre - processing step , the AMR graph is linearized   into a sequence , and in the post - processing step , the   generated sequence is translated back to an AMR   graph .   Recategorization Recategorization is a widely   used technique to handle data sparsity . With re-   categorization , specific sub - graphs of a AMR graph   ( usually corresponding to special entities , like   named entities , date entities , etc . ) are treated as   a unit and assigned to a single vertex with a new   content . We experiment with a commonly - used   method in AMR parsing literature ( Zhang et al . ,   2019a , b ; Zhou et al . , 2020 ; Bevilacqua et al . , 2021 ) .   The readers are referred to Zhang et al . ( 2019a ) for   further details . Notice that this method uses heuris-   tic rules designed and optimized for AMR 2.0 , and   is not able to scale up to AMR 3.0 ( the performance   dropped substantially for AMR 3.0 with recatego-   rization in Bevilacqua et al . ( 2021 ) ) . Therefore , we   will not conduct the recategorization experiment   on AMR 3.0 .   Model and Baseline We use the model in   Bevilacqua et al . ( 2021 ) as our baseline . That   model was initialized by BART pretraining andfine - tuned on the AMR dataset . We will do the   same thing , except that we design a different mask   matrix in the Transformer Decoder layers . We will   introduce these differences in detail in Section 3.2 .   Training and Evaluation We use one 1080Ti   GPU to fine - tune the model . Training takes about   13 hours on AMR 2.0 and 17 hours on AMR 3.0 .   We use the development dataset to select the best   hyperparameters . At inference time , we set the   beam size to 5 following common practice in neural   machine translation ( Yang et al . , 2018 ) .   For evaluation , we use Smatch ( Cai and Knight ,   2013 ) as the metric . For some experiments , we   also report fine - grained scores on different aspects   of parsing , such as wikification , concept identifica-   tion , NER , and negations using the tool released by   Damonte et al . ( 2017 ) .   3.2 Experiments and Results   As indicated in Section 2.3 , we study the effect of   the hard and soft strategy . We explore the influence   of these two strategies on different layers or on   different heads . Due to space limitation , we only   show the Smatch score of AMR 2.0 with the re-   categorization preprocessing , since it had the high-   est performance ( 84.5 Smatch score ) as far as we   know .   Once we get the best result among these se-   tups , we will conduct experiments on AMR 2.0   and AMR 3.0 without recategorization ( we have   discussed why we do n‚Äôt conduct experiments for   AMR 3.0 with recategorization before ) . We will   also report fine - grained results for these experi-   ments .   3.2.1 Experiments for Different Number of   Heads for the Hard Strategy   In the baseline model ( Bevilacqua et al . , 2021 ) ,   there are 16 heads in each layer . We conduct exper-   iments with 0 , 2 , 4 , . . . , 8 , 10 heads in each layer   attending to ancestors only . Note that the 0 - head   model equals the baseline model . We show the   result in Table 2 .   We can see that , up to 4 and 6 heads , the perfor-   mance increases along with the number of heads   increasing , showing the importance of telling the   model what the ancestors are . But then , the per-   formance decreases as the number of heads in-   creases , showing that we can not ignore other non-   ancestor tokens , which still play important roles in   the model.574   3.2.2 Experiments for Different Layers for the   Hard Strategy   In the baseline model ( Bevilacqua et al . , 2021 ) ,   there are 12 layers in the Transformer decoder . Un-   like the heads , the order of layers matters . The   upper layers use information from the lower lay-   ers . Therefore , we conduct experiments with the   bottom , the medium , and the top 4 layers attending   to ancestors . The mask matrix for each head is the   same within a single layer . We show the result in   Table 3 .   We can see that , putting the medium 4 layers   focusing on the ancestors has the best performance .   But when we put the top 4 layers focusing on them ,   the performance decreases a lot . One possible rea-   son is that , when it comes to near the final output   ( the top layers ) , the model needs to use the infor-   mation from all tokens .   3.2.3 Experiments of Soft Strategy   In this section , we will tune the mask matrix and   use the soft strategy to add the ancestors informa-   tion . We conduct three experiments : different pa-   rameters for every layer and head combination ;   different parameters for different layers only ; dif-   ferent parameters for different heads only . We show   the results in Table 4 . We can see that when we   only use different parameters for every head , we   achieve a new state - of - the - art result .   3.2.4 Results for Other Datasets   We have conducted different experiments for AMR   2.0 with recategorization , and we found that when   we set different parameters for different heads only   and tune these parameters , we get the best perfor-   mance . Therefore , we apply this setup for other   datasets : AMR 2.0 and AMR 3.0 without recate-   gorization . We show the Smatch scores as well as   other fine - grained scores in Table 1 . The results are   improved for all the datasets . The AMR 2.0 with-   out recategorization even obtains an improvement   of 1.0 Smatch point .   4 Conclusion   In this paper , we focus on the DFS linearization   and introduce several strategies to add ancestor in-   formation into the model . We conduct experiments   to show the improvement for both AMR 2.0 and   AMR 3.0 datasets . Our method achieves new state-   of - the - art performances for the AMR parsing task .   Acknowledgments   We thank the anonymous reviewers for their in-   sightful comments . Research supported by NSF   awards IIS-1813823 and CCF-1934962.575References576   AHyperparameters and Training Details   We use cross - entropy loss and RAdam optimizer   during the training . We use Cosine learning rate   scheduler with about 1000 warm - up steps and   20000 maximum steps . The selected value of the   learning rate is 3√ó10 . There are around 80   sentences in each batch . We set the weight decay   rate of 0.004 . In order to prevent over - fitting , we   use Dropout with probability 0.25 , as well as label   smoothing with value 0.1 . To select the best model   checkpoint , we use the development dataset and   search for the model with the best Smatch score.577   Miryam de Lhoneux , Sheng Zhang , Anders S√∏gaardUniversity of Copenhagen , DenmarkUppsala University , SwedenKU Leuven , BelgiumNational University of Defense Technology , China   { ml,soegaard}@di.ku.dk , zhangsheng@nudt.edu.cn   Abstract   1 Introduction   The Ô¨Åeld of multilingual NLP is booming ( Agirre ,   2020 ) . This is due in no small part to large multilin-   gual pretrained language models ( PLMs ) such as   mBERT ( Devlin et al . , 2019 ) and XLM - RoBERTa   ( Conneau et al . , 2020 ) , which have been found to   have surprising cross - lingual transfer capabilities   in spite of receiving no cross - lingual supervision .   Wu and Dredze ( 2019 ) , for example , found mBERT   to perform well in a zero - shot setting when Ô¨Åne-   tuned for Ô¨Åve different NLP tasks in different lan-   guages . There is , however , a sharp divide between   languages that beneÔ¨Åt from this transfer and lan-   guages that do not , and there is ample evidence that   transfer works best between typologically similar   languages ( Pires et al . , 2019 ; Lauscher et al . , 2020,among others ) . This means that the majority of   world languages that are truly low - resource are still   left behind and inequalities in access to language   technology are increasing .   Large multilingual PLMs are typically Ô¨Åne - tuned   using training data from a sample of languages that   is supposed to be representative of the languages   that the models are later applied to . However , this   is difÔ¨Åcult to achieve in practice , as multilingual   datasets are not well balanced for typological di-   versity and contain a skewed distribution of typo-   logical features ( Ponti et al . , 2021 ) . This problem   can be mitigated by using methods that sample   from skewed distributions in a way that is robust to   outliers .   Zhang et al . ( 2020 ) recently developed such   a method . It uses curriculum learning with a   worst - case - aware loss for multi - task learning . They   trained their model on a subset of the GLUE bench-   mark ( Wang et al . , 2018 ) and tested on outlier tasks .   This led to improved zero - shot performance on   these outlier tasks . This method can be applied   to multilingual NLP where different languages are   considered different tasks . This is what we do in   this work , for the case of multilingual dependency   parsing . Multilingual dependency parsing is an   ideal test case for this method , as the Universal   Dependency treebanks ( Nivre et al . , 2020 ) are cur-   rently the manually annotated dataset that covers   the most typological diversity ( Ponti et al . , 2021 ) .   Our research question can be formulated as such :   Can worst - case aware automated curriculum learn-   ing improve zero - shot cross - lingual dependency   parsing?5782 Worst - Case - Aware Curriculum   Learning   In multi - task learning , the total loss is generally the   average of losses of different tasks :   min`( ) = min1   nX`( ) ( 1 )   wherelis the loss of task i. The architecture   we use in this paper is adapted from Zhang et al .   ( 2020 ) , which is an automated curriculum learning   ( Graves et al . , 2017 ) framework to learn a worst-   case - aware loss in a multi - task learning scenario .   The architecture consists of a sampler , a buffer ,   a trainer and a multilingual dependency parsing   model . The two main components are the sampler ,   which adopts a curriculum sampling strategy to   dynamically sample data batches , and the trainer   which uses worst - case - aware strategy to train the   model . The framework repeats the following steps :   ( 1 ) the sampler samples data batches of different   languages to the buffer ; ( 2 ) the trainer uses a worst-   case strategy to train the model ; ( 3 ) the automated   curriculum learning strategy of the sampler is up-   dated .   Sampling data batches We view multilingual   dependency parsing as multi - task learning where   parsing in each individual language is considered   a task . This means that the target of the sampler   at each step is to choose a data batch from one   language . This is a typical multi - arm bandit prob-   lem ( Even - Dar et al . , 2002 ) . The sampler should   choose bandits that have higher rewards , and in   our scenario , data batches that have a higher loss   on the model are more likely to be selected by the   sampler and therefore , in a later stage , used by the   trainer . Automated curriculum learning is adopted   to push a batch with its loss into the buffer at each   time step . The buffer consists of nÔ¨Årst - in-Ô¨Årst - out   queues , and each queue corresponds to a task ( in   our case , a language ) . The procedure repeats k   times and , at each round , kdata batches are pushed   into the buffer .   Worst - case - aware risk minimization In multi-   lingual and multi - task learning scenarios , in which   we jointly minimize our risk across nlanguages   or tasks , we are confronted with the question of   how to summarize nlosses . In other words , the   question is how to compare two loss vectors  and   containing losses for all tasks l;:::l :  = [ ` ; : : : ; ` ]   and   = [ ` ; : : : ; ` ]   The most obvious thing to do is to minimize the   mean of the nlosses , asking whetherP ` < P ` . We could also , motivated by robust-   ness ( S√∏gaard , 2013 ) and fairness ( Williamson   and Menon , 2019 ) , minimize the maximum ( supre-   mum ) of the nlosses , asking whether max ` <   max ` . Mehta et al . ( 2012 ) observed that these   two loss summarizations are extremes that can be   generalized by a family of multi - task loss functions   that summarize the loss of ntasks as theLnorm   of then - dimensional loss vector . Minimizing the   average loss then corresponds to computing the L   norm , i.e. , asking whether j  j < j  j , and mini-   mizing the worst - case loss corresponds to comput-   ing theL(supremum ) norm , i.e. , asking whether   j  j < j  j.   Zhang et al . ( 2020 ) present a stochastic general-   ization of the Lloss summarization and a prac-   tical approach to minimizing this family of losses   through automated curriculum learning ( Graves   et al . , 2017 ): The core idea behind their general-   ization is to optimize the worst - case loss with a   certain probability , otherwise optimize the average   ( loss - proportional ) loss with the remaining prob-   ability . The hyperparameter  is introduced by   the worst - case - aware risk minimization to trade off   the balance between the worst - case and the loss-   proportional losses . The loss family is formally   deÔ¨Åned as :   wherep2[0;1]is a random generated rational   number , and P = is the normalized prob-   ability distribution of task losses . If p <   the   model chooses the maximum loss among all tasks ,   otherwise , it randomly chooses one loss according   to the loss distribution . If the hyperparameter   equals 1 , the trainer updates the model with respect   to the worst - case loss . On the contrary , if  = 0 ,   the trainer loss - proportionally samples one loss .   Sampling strategy updates The model updates   its parameters with respect to the loss chosen by the   trainer . After that , the sampler updates its policy   according to the behavior of the trainer . At each579   round , the policy of the task that is selected by   the trainer receives positive rewards and the policy   of all other tasks that have been selected by the   sampler receive negative rewards .   The multilingual dependency parsing model   We use a standard biafÔ¨Åne graph - based dependency   parser ( Dozat and Manning , 2017 ) . The model   takes token representations of words from a con-   textualized language model ( mBERT or XLM - R )   as input and classiÔ¨Åes head and dependency rela-   tions between words in the sentence . The Chu - Liu-   Edmonds algorithm ( Chu and Liu , 1965 ; Edmonds ,   1967 ) is then used to decode the score matrix into   a tree . All languages share the same encoder and   decoder in order to learn features from different lan-   guages , and more importantly to perform zero - shot   transfer to unseen languages .   3 Experiments   We base our experimental design on √úst√ºn et al .   ( 2020 ) , a recent paper doing zero - shot dependency   parsing with good performance on a large number   of languages . They Ô¨Åne - tune mBERT for depen-   dency parsing using training data from a sample   of 13 typologically diverse languages from Univer-   sal Dependencies ( UD ; Nivre et al . , 2020 ) , listed   in Table 1 . For testing , they use 30 test sets from   treebanks whose language has not been seen at Ô¨Åne-   tuning time . We use the same training and test sets   and experiment both with mBERT and XLM - R as   PLMs . It is important to note that not all of the test   languages have been seen by the PLMs .   We test worst - case aware learning with differ-   ent values of  and compare this to three main   baselines : size - proportional samples batches pro - portionally to the data sizes of the training tree-   banks , uniform samples from different treebanks   with equal probability , thereby effectively reducing   the size of the training data , and smooth - sampling   uses the smooth sampling method developed in   van der Goot et al . ( 2021 ) which samples from mul-   tiple languages using a multinomial distribution .   These baselines are competitive with the state - of-   the - art when using mBERT , they are within 0.2 to   0.4 LAS points from the baseline of √úst√ºn et al .   ( 2020 ) on the same test sets . When using XLM - R ,   they are largely above the state - of - the - art .   We implement all models using MaChAmp   ( van der Goot et al . , 2021 ) , a library for multi - task   learning based on AllenNLP ( Gardner et al . , 2018 ) .   The library uses transformers from HuggingFace   ( Wolf et al . , 2020 ) . Our code is publicly available .   Our main results are in Table 2 where we report   average scores across test sets , for space reasons .   Results broken down by test treebank can be found   in Table 4 in Appendix A. We can see that worst-   case - aware training outperforms all of our baselines   in the zero - shot setting , highlighting the effective-   ness of this method . This answers positively our   research question Can worst - case aware automated   curriculum learning improve zero - shot dependency   parsing ?   Our results using mBERT are more than 1 LAS   point above the corresponding baselines . Our best   model is signiÔ¨Åcantly better than the best baseline   withp < : 01according to a bootstrap test across   test treebanks . Our best model with mBERT comes   close to Udapter ( 36.5 LAS on the same test sets )   while being a lot simpler and not using external re-   sources such as typological features , which are not   always available for truly low - resource languages .   The results with XLM - R are much higher in   generalbut the trends are similar : all our models   outperform all of our baselines albeit with smaller   differences . There is only a 0.4 LAS difference   between our best model and the best baseline , but   it is still signiÔ¨Åcant with p < : 05according to a   bootstrap test across test treebanks . This highlights   the robustness of the XLM - R model itself . Our   results with XLM - R outperform Udapter by close   to 7 LAS points.580mBERT XLM - R  = 0 36.4 42.1   = 0.5 36.1 42.3   = 1 36.1 42.3size - proportional 35.0 41.9   smooth - sampling 35.2 41.7   uniform 35.2 41.4   4 Varying the homogeneity of training   samples   We investigate the interaction between the effec-   tiveness of worst - case learning and the represen-   tativeness of the sample of training languages . It   is notoriously difÔ¨Åcult to construct a sample of   treebanks that is representative of the languages in   UD ( de Lhoneux et al . , 2017 ; Schluter and Agi ¬¥ c ,   2017 ; de Lhoneux , 2019 ) . We can , however , easily   construct samples that are notrepresentative , for   example , by taking a sample of related languages .   We expect worst - case aware learning to lead to   larger improvements in cases where some language   types are underrepresented in the sample . We can   construct an extreme case of underrepresentation   by selecting a sample of training languages that   has one or more clear outliers . For example we   can construct a sample of related languages , add   a single unrelated language in the mix , and then   evaluate on other unrelated languages . We also   expect that with a typologically diverse set of train-   ing languages , worst - case aware learning should   lead to larger relative improvements than with a   homogeneous sample , but perhaps slightly smaller   improvements than with a very skewed sample .   We test these hypotheses by constructing seven   samples of training languages in addition to the   one used so far ( 13 ) . We construct three dif-   ferent homogeneous samples using treebanks from   three different genera : , and . We construct four skewed samples using   the sample of romance languages and a language   from a different language family , an outlier lan-   guage : Basque ( eu ) , Arabic ( ar ) , Turkish ( tr ) and   Chinese ( zh ) . Since we keep the sample of test   sets constant , we do not include training data from   languages that are in the test sets . The details of   which treebanks are used for each of these samplessample  RER   13 35.2 36.4 1.2 1.9 30.7 31.4 0.7 1.0 30.4 31.7 1.3 1.9 31.3 32.5 1.2 1.7 + 33.3 34.8 1.5 2.2 + 32.0 32.2 0.2 0.3 + 32.2 33.0 0.8 1.2 + 33.4 34.1 0.7 1.1   can be found in Table 5 in Appendix B.   Results are in Table 3 where we report the aver-   age LAS scores of our best model ( out of the ones   trained with the three different  values ) to the best   of the three baselines . We can see Ô¨Årst that , as ex-   pected , our typologically diverse sample performs   best overall . This indicates that it is a good sam-   ple . We can also see that , as expected , the method   works best with a skewed sample : the largest gains   from using worst - case learning , both in terms of ab-   solute LAS difference and relative error reduction ,   are seen for a skewed sample ( + ) . However ,   contrary to expectations , the lowest gains are ob-   tained for another skewed sample ( + ) . The   gains are also low for + , + and for . Additionally , there are slightly more   gains from using worst - case aware learning with   the sample than for our typologically di-   verse sample . These results could be due to the   different scripts of the languages involved both in   training and testing .   Looking at results of the different models on indi-   vidual test languages ( see Figure 1 in Appendix C ) ,   we Ô¨Ånd no clear pattern of the settings in which this   method works best . We do note that the method   always hurts Belarusian , which is perhaps unsur-   prising given that it is the test treebank for which   the baseline is highest . Worst - case aware learning   hurts Belarusian the least when using the   sample , indicating that , when using the other sam-   ples , the languages related to Belarusian are likely   downsampled in favour of languages unrelated to it .   Worst - case learning consistently helps Breton and   Swiss German , indicating that the method might   work best for languages that are underrepresented   within their language family but not necessarily   outside of it . For Swiss German , worst - case learn-581ing helps least when using the sample   where it is less of an outlier .   5 Conclusion   In this work , we have adopted a method from multi-   task learning which relies on automated curriculum   learning to the case of multilingual dependency   parsing . This method allows to dynamically opti-   mize for parsing performance on outlier languages .   We found this method to improve dependency pars-   ing on a sample of 30 test languages in the zero-   shot setting , compared to sampling data uniformly   across treebanks from different languages , or pro-   portionally to the size of the treebanks . We investi-   gated the impact of varying the homogeneity of the   sample of training treebanks on the usefulness of   the method and found conÔ¨Çicting evidence with dif-   ferent samples . This leaves open questions about   the relationship between the languages used for   training and the ones used for testing .   6 Acknowledgements   We thank Daniel Hershcovich and Ruixiang Cui   for comments on a draft of the paper , as well as   the members of CoAStaL for discussions about   the content of the paper . Miryam de Lhoneux was   funded by the Swedish Research Council ( grant   2020 - 00437 ) . Anders S√∏gaard was funded by the   Innovation Fund Denmark and a Google Focused   Research Award . We acknowledge the compu-   tational resources provided by CSC in Finland   through NeIC - NLPL and the EOSC - Nordic NLPL   use case ( www.nlpl.eu ) .   References582583   A Results by treebank   Results by language of the test treebanks are in   Table 4 .   B Training samples   The training samples are summarized in Table 5 .   C Results by treebank with the different   samples   Relative error reduction between our best worst-   case aware result and the best baseline for each   training sample used , with mBERT , in Figure 1.584mBERT XLM - R   iso  = 0  = 0.5  = 1 S - P S - S U  = 0  = 0.5  = 1 S - P S - S U   aii * # 8 11.3 10.8 1.6 6.4 6.0 2 3.3 3.1 2.9 3.5 3.1   akk * # 1.5 1.4 1.6 2.5 3.0 1.9 2.5 2.5 2.8 1.9 2.2 2.3   am * 16.5 10.9 13.2 6.6 10.8 10.6 68.0 68.6 68.3 68.4 68.8 68.1   be 78.5 79.4 79.6 82.0 80.9 80.5 85.6 85.5 85.6 86.4 86.8 86.8   bho * # 38.1 37.8 37.9 37.0 36.7 36.7 37.3 37.4 37.1 37.4 37.6 37.2   bm * # 9.0 8.7 8.7 6.9 6.7 6.9 6.0 6.4 6.2 6.5 6.3 6.4   br 62.9 62.6 62.0 60.3 60.3 59.6 59.5 59.6 60.5 59.9 59.5 58.9   bxr * # 25.9 26.0 25.6 24.6 25.5 25.4 27.7 28.2 28.0 27.2 27.2 26.2   cy 55.5 55.0 55.2 55.1 54.4 54.2 59.8 60.1 59.9 60.2 60.6 59.6   fo * # 67.4 67.8 68.0 66.3 67.2 66.4 73.5 72.8 73.5 72.6 72.4 73.0   gsw * # 48.3 48.8 48.2 44.9 42.2 42.3 46.0 46.5 46.5 43.6 42.2 44.3   gun * # 8.2 8.5 8.7 7.3 8.0 8.3 6.8 6.8 7.6 6.5 5.8 5.6   hsb * # 50.8 51.3 51.4 49.4 49.2 49.1 62.6 61.9 62.0 61.4 61.6 60.0   kk 60.1 58.9 58.4 58.5 59.0 58.2 63.0 62.7 62.5 63.7 62.3 61.5   kmr * 9.3 9.2 8.9 8.6 9.6 9.5 53.5 53.1 53.2 51.8 51.7 52.0   koi * # 19.3 18.8 19.8 15.8 15.8 16.0 17.0 20.1 19.1 17.8 17.8 16.0   kpv * # 16.8 17.0 17.2 15.6 16.2 15.8 18.3 19.1 19.5 17.0 17.8 16.3   krl * # 46.6 46.4 46.3 46.5 47.1 46.4 61.0 61.2 60.7 62.0 62.1 61.8   mdf * # 26.1 24.3 24.3 22.5 24.5 25.4 20.4 20.7 19.6 18.4 18.4 16.8   mr 60.6 61.2 60.1 56.9 57.7 57.7 69.2 69.7 70.0 67.8 70.0 69.7   myv * # 20.2 19.9 19.8 18.5 19.3 19.9 16.8 17.2 16.9 16.0 16.3 15.5   olo * # 40.7 41.7 41.0 41.0 40.9 40.5 56.5 56.7 56.1 55.8 54.3 54.4   pcm * # 33.9 32.8 33.0 32.5 34.3 35.4 39.2 39.2 38.9 38.0 37.6 37.8   sa * 22.5 21.9 22.3 21.1 21.0 20.6 50.2 49.7 50.9 50.9 50.1 50.0   ta 52.3 54.7 54.3 53.2 52.0 51.6 54.9 55.0 54.8 53.8 53.8 54.0   te 69.9 69.8 70.0 69.4 70.6 68.7 76.0 76.0 76.7 76.3 77.1 76.3   tl # 65.4 57.5 56.5 65.8 59.3 65.4 77.1 75.7 75.7 78.1 76.7 76.4   wbp * # 5.9 8.8 9.2 7.5 7.5 7.2 7.8 9.5 7.5 8.5 5.2 8.8   yo # 37.8 37.9 38.5 39.7 38.0 37.5 3.3 3.6 3.2 2.3 2.7 1.8   yue * # 33.0 32.5 32.5 32.4 32.4 32.4 41.9 41.7 42.0 42.9 42.4 42.8   average 36.4 36.1 36.1 35.0 35.2 35.2 42.1 42.3 42.3 41.9 41.7 41.4585   Afrikaans - AfriBooms X   Danish - DDT X   Dutch - Alpino X   English - EWT X X   German - HDT X   Gothic - PROIEL X   Icelandic - IcePaHC X   Norwegian - Bokmaal X   Swedish - Talbanken X X   Czech - PDT X   Old_Church_Slavonic - PROIEL X   Old_Russian - TOROT X   Polish - LFG X   Russian - SynTagRus X X   Serbian - SET X   Slovak - SNK X   Ukrainian - IU X   French - GSD X X X X X   Italian - ISDT X X X X X X   Portuguese - GSD X X X X X   Romanian - RRT X X X X X   Spanish - AnCora X X X X X   Basque - BDT X X   Arabic - PADT X X   Chinese - GSD X X   Turkish - IMST X X   Finnish - TDT X   Hebrew - HTB X   Hindi - HDTB X   Japanese - GSD X   Korean - GSD X586587   Alex Papadopoulos KorÔ¨Åatis   Babylon   alex.papadopoulosFrancesco Moramarco   Babylon , University of Aberdeen   francesco.moramarco   Radmila Sarac   radmila.sarac@gmail.comAleksandar Savkov   Babylon   sasho.savkov@babylonhealth.co.uk   Abstract   1 Introduction   The use of Automatic Speech Recognition ( ASR )   is widespread in the clinical domain but it is gen-   erally used to alleviate the administrative burden   of clinical notes through dictation ( Hodgson and   Coiera , 2016 ; Kumah - Crystal et al . , 2018 ) .   However , the adoption of telemedicine , espe-   cially in primary care , generates vast quantities of   clinical interaction recordings . Additionally , ASR   models have become much more robust to applica-   tions in the clinical domain . In turn , this is beneÔ¨Å-   cial for downstream Natural Language Processing   ( NLP ) tasks , such as information extraction from   clinical conversations ( Selvaraj and Konam , 2021 ;   Soltau et al . , 2021 ) and automatic generation of   consultation notes ( Finley et al . , 2018 ; Enarvi et al . ,   2020a ; Quiroz et al . , 2020 ; Molenaar et al . , 2020 ) .   Despite this being an active area of research   it still lacks a commonly recognised ASR bench-   mark due to the sensitive nature of clinical con-   versations . Furthermore , as the datasets are not   shared , research teams always need to invest time   and resources into making their own private dataset .   These limitations slow down progress in the Ô¨Åeld . We releasea high quality public dataset of pri-   mary care consultation audio recordings , including   manual transcriptions and associated consultation   notes , which is the basis of our contributions :   1.a benchmark for ASR for primary care con-   versations ;   2.a benchmark for automatic generation of con-   sultation notes for primary care .   2 Related Work   Automated transcription of clinical consulta-   tions has attracted quite signiÔ¨Åcant research in-   terest ; however , as mentioned above , there is no   easily accessible common benchmark dataset in   the style of Switchboard ( Godfrey et al . , 1992 ) or   Fisher ( Cieri et al . , 2004 ) , which are both non-   medical conversational audio datasets . Because of   this , comparing different approaches for clinical   conversation ASR is challenging .   For example , Chiu et al . ( 2018 ) detail a dataset   of14,000 hours of recorded and manually tran-   scribed consultations that they use to train an end-   to - end clinical conversation ASR model . Similarly ,   Kim ( 2020 ) , Soltau et al . ( 2021 ) develop end - to-   end ASR models for clinical conversations and   Mani et al . ( 2020 ) train a sequence - to - sequence   machine translation model to correct the errors of   general - domain ASR engines ; but they all use dif-   ferent , proprietary datasets . Johnson et al . ( 2014 )   and Kodish - Wachs et al . ( 2018 ) perform systematic   reviews of the accuracy of a number of open - source   and commercial ASR models for clinical conversa-   tion transcription ; again , on proprietary datasets .   As for open - access datasets , He et al . ( 2020 )   compile and release two clinical dialogue datasets   in Chinese and English , covering a wide range of   clinical specialties . Ju et al . ( 2020 ) do the same   for COVID-19 related clinical dialogue . These588   datasets are gathered from online clinical question   answering sources ; while they are relevant for clin-   ical chatbot research , they are not representative of   clinical interactions and do not include audio . Kazi   et al . ( 2020 ) provide a dataset of audio recordings ,   automated transcripts and consultation notes for 70   mock psychiatric consultations ‚Äî but no human   transcripts .   Automatic consultation note generation and   other long - form text summarisation tasks have   rapidly developed due to recent advances in Nat-   ural Language Generation ( NLG ) architectures   ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ) . Several   studies ( Liu et al . , 2019 ; MacAvaney et al . , 2019 ;   Zhang et al . , 2020 ; Enarvi et al . , 2020b ; Joshi et al . ,   2020 ; Krishna et al . , 2021 ; Chintagunta et al . , 2021 ;   Yim and Yetisgen - Yildiz , 2021 ; Moramarco et al . ,   2021 ; Zhang et al . , 2021 ) use proprietary datasets   of transcripts and notes to train NLG models end-   to - end , and a number of them carry out automatic   or human evaluations on their proprietary test sets .   However , in a similar fashion to the ASR stud-   ies discussed above , most studies do n‚Äôt publish   these resources ; hence , it is again prohibitively dif-   Ô¨Åcult to compare their proposed methods . Kazi   et al . ( 2020 ) provide the only open access clinical   dataset that could be used as a benchmark but it   only contains psychiatric consultations , which is   less applicable to primary care .   3 Dataset   The requirements for releasing a dataset containing   Personal Health Information ( PHI ) are typically   costly and involve collecting patient consent and/or   de - identiÔ¨Åcation , which is especially challenging   with audio recordings . We built a mock consulta-   tion dataset as close as possible to the real condi-   tions as a pragmatic alternative . The diagram inConsultation type Count   Otitis 2   Anaphylactic reaction 3   Cardiovascular 11   Dermatitis 4   Fever 4   Urinary tract infection 6   Upper respiratory infection 6   Asthma 2   Gastroenteritis 8   Mental health 3   Physical injury 2   Migraine 6   Figure 1 shows an overview of the data collection   process .   3.1 Mock consultation recordings   We employed 7 clinicians and 57 actors posing   as patients from a range of ethnicities . The clin-   icians had experience with virtual consultations .   Participation was optional and anyone could choose   to withdraw at any time . Four of the clinicians   were men and three were women ; Ô¨Åve of them   had British English accent , and two of them Indian .   The patient accent distribution is as follows : British   English ( 47.4 % ) , various European ( 31.6 % ) , other   English ( 10.5 % ) , and other non - English ( 10.5 % ) .   The gender distribution was relatively even ( 52.6 %   women , 47.4 % men ) ; most participants were from   25 to 45 years old ( see Figure A.1 ) .   Each mock patient was given a case card that in-   cluded background information ( age , social history ,   family history of illnesses ) as well as information   about their presenting complaint , symptoms , condi-589Demographics ( age , gender ):   23 year old female   Presenting Complaint :   Lower abdominal pain   Duration of symptoms : 2 days   History , on open questioning :   Have a terrible ache in my lower tummy and   feeling hot and sweaty .   Symptoms and risk factors :   There is some blood in the urine ‚Äì pink colour   Pain below belly button   Feeling nauseated but no vomiting   * * *   tions , and medications . The case cards were drawn   from a pool of primary care conditions , representa-   tive of presenting complaints in UK primary care .   For a breakdown of presenting complaints , see Ta-   ble 1 . An example case card is given in Table 2 .   We recorded 57 mock consultations ( 8h38m6s in   total ) over 5 days , using proprietary telemedicine   software that allowed us to export the individual   clinician and patient audio channels . In order to   emulate real clinical practice , clinicians were using   laptops while patients were using mobile phones   in an ofÔ¨Åce environment with background noise .   Clinicians were asked to act as close as possible   to their actual consultation sessions , including con-   forming to a consultation length of 10 minutes and   writing a consultation note in the SOAP format   ( Pearce et al . , 2016 ) . The resulting mock consulta-   tions ranged between 3m48s and 14m18s , with an   average consultation length of 9m5s .   3.2 Manual transcription   To transcribe the consultation recordings , we em-   ployed transcribers with experience in the clinical   conversation domain , who were asked to :   1.Listen to the consultation audio recordings , in   separate channels for clinicians and patients ;   2.Identify the start and end points of individ-   ual utterances ( continuous speech segments   ending in a pause ) ;   3.Provide an accurate transcription of each of   the utterances identiÔ¨Åed .   Thus we obtained a collection of start times , end   times , and utterance - level transcriptions , important   for the ASR evaluation described below .   Consultations have 92 conversation turns and   1,489 words on average ; clinicians tend to speak   more than patients ( 897 vs. 592 words per consul-   tation ) and take longer turns ( 19.3 vs 12.8 words   per turn ) . Interestingly , patients tend to take longer   turns than clinicians in the beginning of the consul-   tation , where they presumably state their presenting   complaint ; turns are more balanced in the middle ,   and clinicians seem to take over during the diagno-   sis and management at the end ( see Figure 2 ) .   4 ASR Benchmark   We perform a baseline study of ASR for clinical   conversations by passing the audio recordings of   the mock consultations through commonly used   open - source and commercial speech - to - text en-   gines :   1.Kaldi : This is our baseline system , built using   the Kaldi ( Povey et al . , 2011 ) speech recog-   nition toolkit , running locally . It uses a pre-   trained acoustic model from Zamia Speech   and a 3 - gram language model trained on a pro-   prietary medical question answering dataset .   2.NeMo QuartzNet & Conformer : These sys-   tems use QuartzNet ( Kriman et al . , 2020 ) and   Conformer ( Gulati et al . , 2020 ) ASR models ,   which we load using Nvidia ‚Äôs NeMo toolkit.590WER ECCA   Gender Role Accent   ASR mean stdev M F Clinician Patient en - gb other Pr Re F1   GC STT 30.9y12.7 32.7 28.9 28.5 33.4 30.0 32.2 0.83 0.82 0.81   Azure STT 31.3y12.8 32.7 29.6 26.7 35.8 30.2 32.7 0.87 0.79 0.82   ATM 34.0z13.9 33.8 34.2 32.8 35.2 31.6 37.2 0.79 0.75 0.78   Kaldi 48.9 14.9 52.7 44.6 47.0 50.8 49.5 48.2 0.64 0.69 0.68   QuartzNet 46.4 15.5 48.4 44.1 48.1 44.7 46.6 46.1 0.67 0.49 0.56   Conformer 34.4z14.5 36.8 31.7 35.6 33.2 35.0 33.7 0.79 0.71 0.75   Both models are end - to - end and do not use a   language model .   3.Google Cloud Speech - to - text ( GCSTT ) : a   commercially available , general domain ser-   vice . We use the video enhanced model which   is only available for the en - us language .   4.Amazon Transcribe Medical ( ATM ) : a   commercially available service , tailored   speciÔ¨Åcally for medical use cases . There are   models available for clinical dictation and   clinical conversation ; we use the conversation   model with speciality = Primary Care .   5.Azure Speech - to - text ( ASTT ) : a commer-   cially available , general domain service . We   use the Standard model .   To test the accuracy of the above services , we   Ô¨Årst extract the audio for each individual utterance   identiÔ¨Åed by our human transcribers . We then gen-   erate a transcript for the utterance using each of the   ASR engines . We ensure consistency by perform-   ing the following post - processing steps on both   human and automatic transcripts :   1.Remove disÔ¨Çuencies ( " umm " , " uhh " , etc . ) .   These are included in the reference transcripts ,   but often omitted in each STT service ;   2.Replace numerals ( " 5 " , " 9th " , " 1984 " ) with   written equivalents ( " Ô¨Åve " , " ninth " , " nineteen   eighty - four " ) to ensure uniformity ;   3.Remove all punctuation , collapse multiple   spaces and convert to lowercase . Finally , we compute the Word Error Rate ( WER )   for each utterance using SCTK ‚Äôs sclitetool . The   mean WER , including a breakdown by gender ,   role , and accent can be seen in Table 3 . Even   though both are general domain , Google and Azure   together are the best performing models on our   dataset ( p= 0:097 ) . Conformer performs surpris-   ingly well , given that it is a character - level model   evaluated on a word - level metric .   The base WER metric treats all words in a tran-   script as equally important ; this may be less de-   sirable in the clinical domain , where the correct   transcription of speciÔ¨Åc clinical terms is expected   to be more important . To test this , we use a propri-   etary clinical information extraction engine based   on fuzzy string matching , linking to SNOMED - CT   ( Donnelly et al . , 2006 ) . We extract medical con-   cepts from each utterance in both reference and   hypothesis transcripts , then compare the concepts   extracted to estimate accuracy based on clinical ter-   minology ( ECCA in Table 3 ) . The results mostly   match the WER comparisons ; the medical - domain   Amazon model does not seem to perform better .   5 Consultation Note Generation   Benchmark   The consultation transcripts and corresponding   notes ( see example in Table 4 ) are intended as a   parallel dataset to evaluate methods for automat-   ically generating primary care consultation notes .   We propose a benchmark for this task by evaluat-   ing a number of baseline approaches and reporting   common automatic metric scores on our dataset .   The approaches considered include:591Transcript Note   Clinician So , um , tell me what ‚Äôs been going on . You ‚Äôve been   saying there ‚Äôs a problem with your hearing . Is that right?History :   Hx of difÔ¨Åculty hearing left ear   for 6 weeks with tinnitus and   slight nausea/ dizziness .   One previous similar episode in   the past- resolved spontaneously .   No discharge / fever / itchiness / pain   Does n‚Äôt use cotton wool buds   No Pmhx of note   Ex : Looks well , not in pain .   Imp : need to exclude impacted   wax in ear canal Ô¨Årst   Pln : for face to face GP   appointment in 5 days to examine   ear   If any problems in interim to   ring us back   Pt happy with and understands   planPatient Yeah , so I just feel I ca n‚Äôt really hear as well as I used   to , like my hearing is kind of deteriorating in some way .   Clinician Right , OK . How long has this been going on for ?   Patient Uh about six weeks .   Clinician Six weeks , OK . Um , and before that have you had any   hearing problem at all ?   Patient Um I had something maybe , about a year ago , but it only   lasted a couple of days , it was n‚Äôt anything as long as   this .   Clinician Right , OK , OK . And , um , in this six week period , have   you had anything else happen ? Have you had any other   ear symptoms at all ?   Patient Um , I occasionally get like a ringing in my left ear , uh   just on the one side and um there ‚Äôs actually been a few   times when I felt kind of a bit sick or a bit dizzy as well .   Model R1 R2 RL B   BART - CNN 0.17 0.02 0.10 0.80   BERT - ext 0.21 0.03 0.10 0.78   Random 0.19 0.02 0.09 0.78   BART-Ô¨Ånet 0.31 0.08 0.17 0.81   BART - CNN : a neural sequence - to - sequence sum-   mariser based on the BART model ( Lewis   et al . , 2020 ) and Ô¨Åne - tuned on the Daily-   mail / CNN dataset ( Nallapati et al . , 2016 ) ;   BERT - ext : a general - purpose extractive sum-   mariser based on Bert embeddings ( Miller ,   2019 ) ;   Random : a baseline that extracts 15 random sen-   tences from the transcript and collates them   to form a note ;   BART-Ô¨Ånet : a BART - CNN model further Ô¨Åne-   tuned on a proprietary dataset of 8,000 real   transcripts and consultation notes .   We evaluate the models on our dataset and report   common summarisation metrics scores : Rouge-1,-2 & -L ( Lin , 2004 ) which compute the F - score   across ngrams between generated and human notes ;   and BERTScore ( Zhang et al . , 2019 ) , which com-   putes the similarity between BERT embeddings of   the notes .   The results can be seen in Table 5 : the Ô¨Åne-   tuned BART model scores highest with all metrics ,   while BART - CNN andBERT - ext fail to outperform   theRandom baseline model . This highlights the   differences between consultation note generation   and general - purpose summarisation .   A more detailed evaluation of this task can be   found in Moramarco et al . ( 2022 ) ; example notes   can be found in Appendix Table A.3 .   6 Conclusion   We present a dataset of 57 high quality mocked con-   sultation audio recordings , their manually aligned   and diarised transcripts , and consultation notes . By   publishing this dataset , we hope to offer a bench-   mark for future studies in both ASR for clinical   conversations and Consultation Note Generation   for the primary care domain .   References592593594595Appendix   Demographics ( age , gender ):   23 year old female   Presenting Complaint :   Lower abdominal pain   Duration of symptoms : 2 days   History , on open questioning :   Have a terrible ache in my lower tummy and feeling hot and sweaty .   Symptoms and risk factors :   There is some blood in the urine ‚Äì pink colour   Pain below belly button   Feeling nauseated but no vomiting   Going to the toilet a little more often but drinking lots of Ô¨Çuids   No urine urgency or pain when passing urine .   Was constipated until 1 week ago but that has cleared up now   Had sexual intercourse 4 days ago   No new sexual partner since last STI screen 6 months ago   No vaginal discharge   Has Implanon contraceptive implant for 1 year   No change in vaginal bleeding   No loin pain   Activities of daily living : No problems performing daily activities   Family history : nil   Past Medical History : nil   Drug History : Implanon   Allergies : Amoxicillin596Human Transcription Google Speech - to - text   Doctor : Hello ?   Patient : Hello . Can you hear me well ?   Doctor : Uh uh yes . I think . It ‚Äôs a bit better .   It ‚Äôs a bit , it ‚Äôs a bit , it ‚Äôs not very clear . But let ‚Äôs   continue anyway .   Patient : OK .   Doctor : Uh , OK . Let ‚Äôs start again . So how can   I help you sir ?   Patient : Yes . So , it ‚Äôs been a few days now . I   have like a sore , and a red skin . It ‚Äôs kind of , it ‚Äôs   really itchy , and it ‚Äôs like super annoying . So I ‚Äôd   like to Ô¨Ånd something quick to solve it .   Doctor : OK . No , no problem . I ‚Äôm happy to   help . Um whereabouts in your skin is it af-   fected ?   Patient : Uh , mostly like my chest , my , my   hands , my arms . Like , like really , it ‚Äôs it ‚Äôs super   annoying . Like it ‚Äôs itching a lot , like all the time .   And I ca n‚Äôt even sleep at night . I really need   something quickly to , to solve it . Because even   at work I , I can , when I ‚Äôm in a meeting and I   have to , like uh think about my work , I ca n‚Äôt   focus , I ca n‚Äôt actually focus on my work . It ‚Äôs   really annoying because I ca n‚Äôt actually think   about , uh , what I have to say . I ‚Äôm always like ,   uh , disturbed by this disease .   * * *   Doctor : OK . OK . So it ‚Äôs something for you   to think about . you can get different types of   antihistamines . I can give you something a little   bit stronger today as well . Um , something like   Fexofenadine , which I can give to you today .   It ‚Äôs deÔ¨Ånitely worth trying , and it ‚Äôs not going to   do you any harm .   Patient : OK .   Doctor : Um but I think using the steroids and   the emollients , um on a regular basis Uh over   the next week to ten days , should hopefully   control your symptoms . But do come back and   see me next week , if things do n‚Äôt get better .   Patient : That sounds good .   Doctor : OK ? Um do you have any questions   for me ?   Patient : Uh , no that ‚Äôs it . Thank you very much .   Bye . Thank you as well . Bye . Doctor : Hello .   Patient : Hello , can you hear me wet ?   Doctor : Yes , I think it ‚Äôs a bit better . It ‚Äôs a bit . It ‚Äôs a   bit . It ‚Äôs not very clear . But let ‚Äôs continue . Anyway ,   Patient : Okay .   Doctor : okay , let ‚Äôs talk again . So , how can I help   you , sir ?   Patient : Yes , so it ‚Äôs been a few days now . I have   like a sore and the Redskin it ‚Äôs kind of it ‚Äôs really   itchy and it ‚Äôs like super annoying .   Doctor : Okay .   Patient : So I ‚Äôd like to Ô¨Ånd something quick to serve   it .   Doctor : No , no problem . Happy to help where-   abouts of your skin is affected .   Patient : Mostly like my chest my my hands my   arms like agree . It ‚Äôs super annoying like it ‚Äôs itching   a lot like all the time and I ca n‚Äôt even sleep at night .   Like I really need something quickly to study be-   cause even at work I like when I ‚Äôm in the meeting   and I have to like think about my work Focus like   actually focus on my work . It ‚Äôs   Doctor : Yeah .   Patient : really annoying because I can actually   think about what happened say , I ‚Äôm always like   disturbed by this disease .   * * *   Doctor : It did n‚Äôt okay . So something for you to   think about a you can get different types of and   system means I can give you something Little Bit   Stronger today as well   Patient : Okay .   Doctor : something like Ô¨Åx the penalty in which I   can give to you today . It ‚Äôs deÔ¨Ånitely worth trying it ‚Äôs   not gon na do you any harm but I say anything using   the steroids and the emollients on a regular basis   over the next week to 10 days should hopefully care   control your symptoms , but do come back and see   me next week if things do n‚Äôt get better .   Patient : That sounds good .   Doctor : Okay any questions for me ?   Patient : And now that ‚Äôs it .   Doctor : Okay . Well , I wish you all the best .   Patient : Thank you very much .   Doctor : Hope you have a good day .   Patient : Bye - bye.597Hx : 1 week history of spontaneous elbow swelling left . Not painful . No trauma . No FH of   rheumatological disease- NB pt says he has been old he has OA previously by doctors- ? need to   conÔ¨Årm this Works in a desk job Not happened before Otherwise well- PMHx : nil of note FH : nil   of note DH : not on any medication , allergic to peanuts SH : exercises regularly , active Ex : looks   well , not in pain . Mild erythema and minimal swelling ( if any ) around olecranon process left   elbow Imp : possible bursitis Plan : for NSAIDs- usual advice re SE For rheum bloods : esr , crp , fbc ,   rheum factor and urate Review thereafter in person/ via video To contact us back in interim if any   deterioration / concerns- pt warned re symptoms of septic arthritis . Doctor Deen Mirza from GP at Hand sees John Smith . John says he has a weird swelling on his   left elbow . He also says he is allergic to peanuts . Deen takes a look at John ‚Äôs elbow to see if there   is anything wrong with it . Do you have any other illnesses at all?Before we start your appointment , could you please tell me your Ô¨Årst name and your date of birth .   And I was born on the Ô¨Åfth of April , , nineteen seventy three . But it ‚Äôs just , just a bit , a bit weird , to   see that . , and , , in terms of your job , do you do anything physical ? so you know you said you think   you ‚Äôve got , , osteoarthritis . and , do you have any other illnesses at all ? , I run regularly , like two ,   three times a week . , what I think we should do is , I think you should be on some anti - inÔ¨Çammatory   medication , in the , in the Ô¨Årst instance . And , there ‚Äôll be instructions within that pack , about where   to go to get those blood tests done . and , your , your joint does n‚Äôt look like that . However , if your ,   the elbow was to become very red , very painful , , and the redness was to spread or become , you   know more intense . That would require more immediate assessment , more immediate treatment .   do you , do you think it ‚Äôs something dangerous ? Like something , like could I die from that , or is it ,   is it No . that ‚Äôs four hundred milligrams , two times a day . Maybe within a , actually you know , the   follow - up appointment does n‚Äôt have to be face - to - face , if it ‚Äôs more convenient for you do , to do it   over the phone , we can do that over the phone , , over video . We can do that as well , that ‚Äôs , that ‚Äôs   your call . Sure . No , no I have n‚Äôt noticed that before . OK , OK , great . Yes , a few years ago . do you , do you   think it ‚Äôs something dangerous ? Fantastic . But you contact us , , after you ‚Äôve had the blood test   done , and we can review things then , OK . OK . OK , yeah that sounds good . OK . - . , yeah , no , I ‚Äôm ,   think I ‚Äôm healthy . . So , , this , this is not the case right now . I run regularly , like two , three times a   week . do n‚Äôt need to worry . All right then , OK . , take care then . You have a problem with your left elbow . 1 week ago noticed a weird swelling on the left elbow .   Not painful at all , but slightly warm , slightly warm . No pain , no swelling , no Ô¨Çuid in the elbow . No   injury . No previous history of this . No injury to the elbow . NKDA . SH : Mobile and active , exercise   2 - 3 times a week , running . Osteoarthritis of the elbow . You should start the treatment you have   been prescribed . You should begin the treatment prescribed as we discussed . You may want to take   some ibuprofen or paracetamol in addition to any prescribed medication.598   Chang Gao , Wenxuan Zhang , and Wai Lam   The Chinese University of Hong Kong   { gaochang,wxzhang,wlam}@se.cuhk.edu.hk   Abstract   1 Introduction   Recent years have seen signiÔ¨Åcant progress in goal-   oriented dialogues ( Loshchilov and Hutter , 2017 ;   Wen et al . , 2017 ; Wu et al . , 2019 ; Hosseini - Asl   et al . , 2020 ; Peng et al . , 2021 ) , which aim at as-   sisting end users in accomplishing certain goals   via natural language interactions . However , due to   the lack of external knowledge , most goal - oriented   dialogue systems are restricted to providing infor-   mation that can only be handled by given databases   or APIs ( Kim et al . , 2020 ) and completing cer-   tain tasks in a speciÔ¨Åc domain such as restaurant   booking . To address this challenge , goal - oriented   document - grounded dialogue has been proposed   to leverage external documents as the knowledge   source to assist the dialogue system in satisfying   users ‚Äô diverse information needs ( Feng et al . , 2020 ;   Wu et al . , 2021 ) .   As shown in Figure 1 , the goal - oriented   document - grounded dialogue problem is com-   monly formulated as a sequential process including   two sub - tasks : knowledge identiÔ¨Åcation ( KI ) and   response generation ( RG ) ( Feng , 2021 ) . Given the   dialogue context and supporting document , knowl-   edge identiÔ¨Åcation aims to identify a text span in   the document as the grounding knowledge for the   next agent response , which is often formulated as a   conversational reading comprehension task ( Feng ,   2021 ; Wu et al . , 2021 ) . Response generation then   aims at generating a proper agent response accord-   ing to the dialogue context and the selected knowl-   edge . Therefore , one straightforward solution for   this problem is to use two models to conduct KI and   RG in a pipeline manner ( Daheim et al . , 2021 ; Kim   et al . , 2021 ; Xu et al . , 2021 ; Chen et al . , 2021 ; Li   et al . , 2021 ) . However , such pipeline methods fail   to capture the interdependence between KI and RG .   As a result , error propagation is a serious problem .   The problem is more pronounced in low - resource   scenarios , where accurate knowledge identiÔ¨Åcation   is difÔ¨Åcult due to limited data , making it harder to   generate appropriate responses .   To address the aforementioned issue , we propose   aUniÔ¨Åed generative framework for Goal - oriented   Document - grounded Dialogue ( UniGDD ) . Given   the dialogue context and associated document , in-   stead of treating KI and RG as two separate pro-   cesses , we tackle them simultaneously via sequen-599tially generating the grounding knowledge and the   agent response . Therefore , the inherent dependen-   cies between these two sub - tasks can be naturally   modeled . On one hand , the generation of the agent   response depends not only on the dialogue context   and external document but also on the identiÔ¨Åed   knowledge , forcing the model to focus on the spe-   ciÔ¨Åc knowledge . On the other hand , the generation   of the grounding knowledge receives the supervi-   sion signal from the agent response when training ,   leading to more accurate knowledge identiÔ¨Åcation .   Although KI and RG can be uniÔ¨Åed with the pro-   posed generative method , they have different char-   acteristics . Generating the grounding knowledge   is similar to copying appropriate sentences from   the document , while generating the response needs   more effort to make the response coherent with the   dialogue and consistent with the grounding knowl-   edge . Therefore , in addition to the main task that   uses the concatenation of the grounding knowledge   and response as the target sequence , we introduce   the generation of the grounding knowledge and the   generation of the response as two auxiliary tasks in   the same framework to force the model to capture   their characteristics so as to perform well on them   as well . Moreover , inspired by the recent success   in prompt learning for pre - trained models ( Li and   Liang , 2021 ; Lester et al . , 2021 ; Liu et al . , 2021 ) ,   we design prompts for these three tasks to guide   the model on what to generate for each task . These   prompts can naturally connect these tasks via indi-   cating the model that each auxiliary task aims to   generate a part of the target sequence of the main   task . Through this prompt - connected multi - task   learning strategy , the model can capture the char-   acteristics of different tasks as well as exploit the   connections between them .   In addition , for a particular user query in the   goal - oriented dialogue , the selected knowledge and   generated response need to be speciÔ¨Åc , while the   generation conditions on a relatively long docu-   ment . Thus , much information in the input docu-   ment is irrelevant . To tackle this problem , we in-   troduce linear temperature scheduling to make the   attention distribution to the input document gradu-   ally sharper during the training process in order to   enable the model to learn to pay more attention to   the relevant content .   Our contributions are summarized as follows :   ( 1 ) We propose a uniÔ¨Åed generative framework for   the goal - oriented document - grounded dialogue . ( 2 )   We develop a prompt - connected multi - task learning   strategy to exploit the characteristics and connec-   tions of different tasks and introduce linear temper-   ature scheduling to enable the model to pay more at-   tention to relevant information . ( 3 ) Our framework   advances state - of - the - art methods on the concerned   task , especially in low - resource scenarios .   2 Our UniGDD framework   UniGDD is a multi - task generative framework   for the goal - oriented document - grounded dialogue   problem .   Main Task Given the dialogue context C=   ( u;a;:::;u;a;u)and grounding docu-   mentD , whereuis thei - th user utterance and   ais thei - th agent utterance , our main task aims to   generate the target sequence Y= ( k;a ) , where   kis the grounding knowledge from Dandais   the response to u. SpeciÔ¨Åcally , for the example in   Figure 1 , the input and output of the main task are   as follows :   Input : generate < grounding > then < agent > :   < user > I would like to renew ... ? < agent >   Each time you ... < user > How often do ...   ? < title > Renew Driving School License   < /title > ... Your application for renewal ...   Output : < grounding > Your application for   ... < agent > Renewal of a Driving ...   We use different special tokens to identify differ-   ent elements in the input and output . For example ,   we add " < user > " in front of each user utterance ,   " < agent > " in front of each agent utterance , and   " < grounding > " in front of the grounding knowl-   edge . The prompt " generate < grounding > then   < agent > : " is added to the dialogue context and sup-   porting document to form the input and guide the   model to generate the grounding knowledge and the   response in order . The input - to - target generation   can be modeled with a pre - trained encoder - decoder   modelM : ( C;D;TP ) ! ( k;a)such as T5   ( Raffel et al . , 2020 ) , where TPis the task prompt.600Prompt - Connected Multi - Task Learning We   introduce two auxiliary tasks to steer our frame-   work to model the respective characteristics of   knowledge identiÔ¨Åcation and response generation .   Given the dialogue context Cand grounding docu-   mentD , these two tasks aim to generate the ground-   ing knowledge kand the response awith the same   modelM. As depicted in Figure 2 , we construct   prompts " generate < grounding > : " and " generate   < agent > : " for them . These prompts indicate the   model that the goals of the two auxiliary tasks are   to generate the Ô¨Årst part and the second part of the   target sequence of the main task , respectively . As   a result , the connections between different tasks   are naturally modeled . Instead of using discrete   language phrases , we randomly initialize the em-   beddings of those special tokens in the prompts and   train them end - to - end to better encode the charac-   teristics and connections of these tasks .   Linear Temperature Scheduling For a speciÔ¨Åc   user query in the dialogue , many document con-   tents are actually irrelevant . To force the model to   pay less attention to the irrelevant parts , we propose   a linear temperature scheduling strategy to make   the attention distribution of cross - attention grad-   ually sharper during the training process . Specif-   ically , we design the softmax function in the   cross - attention module of each decoder layer as   follows :   a = exp ( z=  ) exp ( z=  ) ( 1 )   = (     ) S   S+   ( 2 )   whereais the attention weight for the i - th input   token , zis the logit for the i - th input token , Sis   the current training step , S is the total training   steps ,  and  are the starting and ending tem-   perature respectively ,  <  , and 0 <  < 1 .   Compared with the original cross - attention mod-   ule , the ending temperature 0 <  < 1leads to a   sharper attention distribution , giving more attention   weight to the relevant content .   Training The model is trained with a maximum   likelihood objective . Given the training example   e= ( C;D;TP;Y ) , the objective Lis deÔ¨Åned as   L= logP(YjY;C;D;TP ) ( 3 )   whereis the model parameters , TPis the task   prompt , Yis the target sequence , and nis theModels EM F1   BERTQA 42.2 58.1   BERT - PR - large 56.3 70.8   RoBERTa - PR - large 65.6 77.3   Multi - Sentence 59.5 68.8   DIALKI ( Lonly ) 60.4 71.2   DIALKI 65.9 74.8   UniGDD - base 65.6 76.8   UniGDD - large 66.9 77.5   Models BLEU   DIALKI+BART - base 25.8   RoBERTa - PR - large+BART - base 39.6   RoBERTa - large+T5 - base 40.7   UniGDD - base 42.8   UniGDD - large 42.9   length ofY. We mix the data of the main task   and two auxiliary tasks for training .   Inference After training , for each pair of dia-   logue context and document ( C;D ) , we generate   the target sequence of the main task for obtaining   the grounding knowledge kand the response a.   3 Experiments   3.1 Experimental Setup   Dataset We conduct experiments on the goal-   oriented document - grounded dialogue dataset   Doc2Dial ( Feng , 2021 ) , which is adopted by the Di-   alDoc21 shared task . It contains 3,474 dialogues   with 44,149 turns for training and 661 dialogues   with 8539 turns for evaluation .   Evaluation Metrics Following Feng ( 2021 ) ,   we use Exact Match ( EM ) and token - level F1   for knowledge identiÔ¨Åcation and BLEU ( Papineni   et al . , 2002 ; Post , 2018 ) for response generation .   Baselines For knowledge identiÔ¨Åcation , we com-   pare UniGDD with several strong baselines , includ-   ing BERTQA ( Devlin et al . , 2019 ) , BERT - PR ( Da-   heim et al . , 2021 ) , RoBERTa - PR ( Daheim et al . ,   2021 ) , Multi - Sentence ( Wu et al . , 2021 ) , and DI-   ALKI ( Wu et al . , 2021 ) . These models formulate   knowledge identiÔ¨Åcation as the machine reading   comprehension task and extract the grounding span601from the document . For response generation , we   compare UniGDD with several pipeline methods ,   including DIALKI+BART ( Wu et al . , 2021 ) that   uses DIALKI to conduct knowledge identiÔ¨Åcation ,   followed by BART ( Lewis et al . , 2020 ) to con-   duct response generation and RoBERTa - PR+BART   ( Daheim et al . , 2021 ) . We also build a strong base-   line model RoBERTa+T5 which uses the same pre-   trained generative model as ours .   Implementation Details We report results of   UniGDD with two model sizes : UniGDD - base   and UniGDD - large , which are initialized with pre-   trained T5 - base and T5 - large models ( Raffel et al . ,   2020 ) , respectively . We adopt the implementa-   tion from Hugging Face Transformers ( Wolf et al . ,   2020 ) . We set the max input length to 2560 . Any   sequence over 2560 tokens will be truncated . For   training , we use the AdamW ( Loshchilov and Hut-   ter , 2019 ) optimizer with an initial learning rate   of10and a linear learning rate decay scheduler .   We train 10 epochs for single - task learning and   5 epochs for multi - task learning . For decoding ,   we use beam search , and the beam size is 2 . For   linear temperature scheduling , we set the starting   temperature  = 1 and choose the best ending   temperature from { 0.5 , 0.6 , 0.7 , 0.8 , 0.9 } . For our   constructed baseline RoBERTa+T5 for response   generation , we use RoBERTa - large and T5 - base   and adopt the implementation from the DialDoc21   shared task .   3.2 Results   The results on knowledge identiÔ¨Åcation and re-   sponse generation are shown in Table 1 and Table   2 , respectively . Our UniGDD framework outper-   forms all the baselines on two sub - tasks . On the   knowledge identiÔ¨Åcation task , UniGDD - base can   obtain comparable results to previous state - of - the-   art methods . With a larger model size , UniGDD-   large achieves new state - of - the - art performance .   On the response generation task , UniGDD obtains   a marked improvement over all pipeline methods .   This veriÔ¨Åes our assumption that our uniÔ¨Åed gener-   ative framework can alleviate the error propagation   problem of pipeline approaches .   Effect of Prompt - Connected Multi - task   Learning ( PCMTL ) and Linear Temperature   Scheduling ( LTS ) To verify the effectiveness of   PCMTL and LTS , we Ô¨Årst remove PCMTL ( i.e. ,   training with the main task only ) , and the perfor-   mance of UniGDD - base on two tasks decreases   to 65.2 EM , 76.3 F1 , and 42.3 BLEU , showing   that PCMTL endows the model with the ability   of modeling the characteristics and connections   of different tasks and achieving better generation .   Further removing LTS , the performance drops to   64.7 EM , 76.0 F1 , and 41.7 BLEU . This indicates   that LTS can guide the model to pay more attention   to relevant content during generation and bring   improvements on two sub - tasks .   Effect of Connected Prompts ( CP ) To exam-   ine whether CP can capture the connections of dif-   ferent tasks , we use an alternative approach that   employs task - independent prompts " < Task1 > : " ,   " < Task2 > : " , and " < Task3 > : " to specify each task   for comparison . As in the case of CP , we randomly   initialize the embeddings of these three special to-   kens . With these prompts , UniGDD - base obtains   64.9 EM , 76.2 F1 , and 42.3 BLEU , which performs   worse than using CP . This indicates that CP enables   the model to take advantage of the connections be-   tween the three tasks .   Low - Resource Setting To evaluate the model   in low - resource scenarios , we randomly shufÔ¨Çe   the training set and then take 1/32 , 1/16 , 1/8 , and   1/4 of the data for training . Figure 3 shows the   results of UniGDD - base and the best - performing   pipeline baseline RoBERTa - large+T5 - base on the   four low - resource training splits . Generally , our   framework performs substantially better than the   pipeline method on both tasks . Particularly , when   there is only 1/32 training data , UniGDD - base ob-   tains more than 20 and 10 absolute points improve-   ment over the pipeline approach on EM and BLEU ,   respectively .   Case Study Figure 4 shows a real case including   the dialogue context , supporting document , and the   responses generated by the pipeline method and our   proposed UniGDD framework . It can be observed   that our framework identiÔ¨Åes accurate knowledge   from the supporting document and thus provides a602   proper and informative response about the reasons   for the problem the user encounters . In contrast ,   the pipeline method only gives a relatively general   response that is not suitable in this case .   3.3 Human Evaluation   We randomly sample 100 evaluation instances .   For each instance , given the dialogue context and   grounding document , three human annotators are   asked to conduct a pairwise comparison between   the response generated by UniGDD - base and the   one generated by the pipeline baseline RoBERTa-   large+T5 - base in terms of two aspects : ( 1 ) Rele-   vance : which response is more relevant and ap-   propriate to the user query ? ( 2 ) Informativeness :   which response is more informative ? Results are   shown in Table 3 . Compared with the pipeline   method , our framework can reduce error propa-   gation , resulting in more relevant and appropriate   responses . Moreover , our framework has a clear   advantage over the baseline in terms of Informa-   tiveness since it can utilize rich document context   during the generation .   Win Tie Lose   Relevance 26 64 10   Informativeness 23 69 84 Conclusion   Our UniGDD framework uniÔ¨Åes knowledge identi-   Ô¨Åcation and response generation and models their   characteristics via a multi - task generative model-   ing strategy . Both automatic evaluation and hu-   man evaluation demonstrate the effectiveness of   our framework .   References603604605   Ramit Sawhney , Megh Thakkar , Shrey Pandit , Ritesh Soun   Di Jin , Diyi Yang , Lucie FlekConversational AI and Social Analytics ( CAISA ) Lab , University of MarburgBITS , PilaniSri Venkateswara College , DUAmazon Alexa AIGeorgia Institute of Technology   rsawhney@mathematik.uni-marburg.de , lucie.flek@uni-marburg.de   Abstract   1 Introduction   Deep learning models , though effective for many   applications are prone to overÔ¨Åtting in absence of   sufÔ¨Åcient training data . Data augmentation tech-   niques can efÔ¨Åciently use this limited training data   ( Liu et al . , 2021 ; Shi et al . , 2020 ) . Interpolation-   based augmentation techniques such as Mixup   ( Zhang et al . , 2018 ) have shown improved perfor-   mance across different modalities . Mixup over   latent representations of inputs leads to further im-   provements ( Chen et al . , 2020a ) . However , Mixup   does not account for the spatial distribution of   dataset samples , but choosing samples randomly   for interpolation - based augmentation .   While randomization in Mixup helps , augment-   ing Mixup ‚Äôs sample selection strategy with logic   based on the similarity of the samples to be mixed   can lead to improved generalization ( Chen et al . ,   2020b ) . The relative spatial position of samples can   be leveraged to produce more suitable synthetic   inputs for training underlying models ( Xu et al . ,   2021 ) . Further , natural language possesses hierar-   chical structures and complex geometries , which   the standard Euclidean space can not capture effec-   tively ( Ganea et al . , 2018 ) . Hyperbolic geometry   presents a solution in deÔ¨Åning similarity between   latent representations ( Tifrea et al . , 2019 ) .   We propose DM , an adaptive distance - aware   interpolative data augmentation method . Instead of   choosing random inputs from the complete training   distribution as in the case of Mixup , DMsam-   ples instances based on the ( dis)similarity between   latent representations of samples in the hyperbolic   space . Furthermore , DMperforms interpolations   with trainable pair - wise parameters derived from   the spatial distribution of the samples rather than   sampling mixing ratios randomly from standard   distributions , making it adaptive for pair - wise in-   terpolation . Our contributions are :   ‚Ä¢We propose DM , a novel adaptive distance-   aware interpolative regularization method de-   veloped over the spatial distribution of dataset   sampled in the hyperbolic space .   ‚Ä¢DMoutperforms existing interpolative data   augmentation baselines for 8benchmark sen-   tence classiÔ¨Åcation tasks across four languages .   ‚Ä¢DMachieves threshold F1 scores with 3times   less number of iterations than random Mixup606while being generalizable across tasks , datasets ,   and modalities .   2 Methodology   We present an overview of DMin Figure 1 . We   Ô¨Årst introduce interpolative Mixup ( ¬ß 2.1 ) , and then   formulate DMby leveraging the relative sample   distribution in the hyperbolic space ( ¬ß 2.2 ) .   2.1 Interpolative Mixup   Given two data samples x;x2Xwith labels   y;y2Y , andi;j2[1;N ] , Mixup ( Zhang et al . ,   2018 ) uses linear interpolation with mixing ratio r   to generate the synthetic sample xand correspond-   ing mixed label y ,   Interpolative Mixup ( Chen et al . , 2020a ) performs   linear interpolation over the latent representations   of models . Let f()be a model with parameters   havingKlayers , f()denotes the n - th layer   of the model and his the hidden space vector at   layernforn2[1;K]andhdenotes the input   vector . To perform interpolative Mixup at a layer   k[1;K ] , we calculate the latent representations   separately for the inputs for layers before the k - th   layer . For input sample x , we lethdenote the   hidden state representations at layer n ,   We then perform Mixup over individual hidden   state representations h;hfrom layerkas ,   The mixed hidden representation his used as the   input for the continuing forward pass ,   2.2 DM : Distance - aware Mixup   Though Mixup helps generalize models better , it   selects samples completely randomly for interpo-   lation . Augmenting the sample selection strategy   with intelligence derived from the spatial distri-   bution of the samples to be mixed can lead to   improved generalization . Hence , we formulate   distance - aware Mixup , or DM . To perform   DM , we Ô¨Årst create a learnable matrix M ,   which is used to perform Mixup between pair ofsamples . We use the hyperbolic distance as our sim-   ilarity metric to initialize matrix Mas it effectively   captures the hierarchical structures and complex ge-   ometries that natural language text possesses . The   hyperbolic distance Dbetween sentence embed-   dingse = f(x)ande = f(x)is ,   Here,represents the M√∂bius addition for a   pair of points x;y2B , deÔ¨Åned as ,   , h:;:i , jjjj are Euclidean inner product and norm .   We initialize Musing hyperbolic distance D   and normalize it row wise to scale the values ,   Using learnable matrix M , we change the Mixup   formulation ( Equation 1 ) for samples iandjand   deÔ¨Åne DMixup as ,   DMis deÔ¨Åned for one sample as compared   to Mixup which is deÔ¨Åned for two samples . To   perform DMover a sample x , we create a set S   of the most diverse samples in the dataset based on   a threshold . To create this set , we select samples   havingMabove a threshold  ,   We use  to control the diversity of the selected   samples .  = Tmax(M ) at each step of the   training , where Tis a hyperparameter 2(0;1 ) . To   perform DM , we operate DMixup over samples   xand a random sample x2S ,   We replace the Mixup operation in Equation 3   with the DMoperation in Equation 10 to evalu-   ateDM . The Ô¨Ånal hidden state output his   passed through a multi - layer perceptron ( MLP )   gfor classiÔ¨Åcation . We optimize the network   using KL Divergence loss between the Ô¨Ånal out-   putg(h)and mixed label y = DMixup ( y;y ) ,   which also trains matrix Mend - to - end .   3 Experimental Setup   We evaluate DMon standard English , GLUE ,   and multi - lingual datasets in 4languages ( Table 1).607   3.1 Training Setup   DMis performed over a layer randomly sampled   from all the layers of the model . We use a learning   rate of 2e-5 , batch size of 8 and a weight decay of   0.01 for all the combinations , DM , DMix - NT ,   and Mixup . For the baselines , we sample rfrom   a beta distribution following previous works . All   hyperparameters were selected based on validation   F1 - score . We use BERT for English and mBERT   for other languages as the base model ffor our ex-   periments , and their [ CLS ] token representation as   the sentence embeddings to calculate the distances   ( Equation 5 ) . Due to resource constraints , we only   use10;000samples of SST-2 for training , but do   not change the validation and test split .   3.2 Evaluation   We compare DMwith word - mixup ( WMix ) and   sentence - mixup ( SMix ) ( Guo et al . , 2019 ) , and   interpolative Mixup ( TMix ) ( Chen et al . , 2020a ) .   F1We use F1 score to evaluate the classiÔ¨Åcation   performance of DMand its variants .   Diversity Following Gontijo - Lopes et al . ( 2020 ) ,   we use diversity deÔ¨Åned as the number of training   steps required to obtain a benchmark F1 score .   4 Results and Analysis   4.1 Performance Comparison and Ablation   We observe that distance - constrained Mixup signif-   icantly ( p<0:01 ) outperforms all baselines across   the datasets ( Table 2 ) validating that similarity-   based sample selection improves model perfor-   mance , likely owing to enhanced diversity or mini-   mizing sparsiÔ¨Åcation across tasks . Within distance-   constrained Mixup , we observe that DM , the   hyperbolic distance variant outperforms Euclidean   distance ( Euc- DM ) measures ( Table 3 ) . This   suggests that the hyperbolic space is more capable   of capturing the complex hierarchical information   present in sentence representations , leading to bet-   ter comparisons and sample selection . We also   compare DMand its variants with their non-   trainable versions ( denoted by -NT in Table 3 ) .   These methods have matrix MÔ¨Åxed , and only se-   lect samples based on their relative positions in   the embedding space . We observe that for all vari-   ants , the non - trainable counterparts perform poorer   than the trainable counterparts , indicating that M   is able to capture sample - speciÔ¨Åc information rel-   ative to other samples , generating more suitable   sample selection and mixing ratio for performing   interpolative data augmentation .   4.2 Analyzing Convergence of DM   We validate " Does DMconverge faster than   TMix ? " . We observe that across all datasets , DM   achieves a benchmark F1 score in less number   of training iterations compared to TMix ( Figure   2 ) . Since DMselects samples for Mixup in   an adaptive distance - aware manner , it is able to   generate more diverse and suitable interpolations   leading to faster generalization of the underlying   base model . DMrequires 3times less number   of iterations on an average compared to TMix , or608   random Mixup , and hence is more generalizable   and effective across languages .   4.3 Impact of Sample Selection and   Distance - Aware Mixing Ratio   We probe the individual impact of using matrix M   for distance - based sample selection and using it for   performing mixup in Table 4 . We observe that both   the applications of matrix Mlead to improvements   over TMix . Using matrix Mfor sample selection   obtains larger improvements compared to using it   as the ratio for performing mixup . This suggests   that the selection of inputs for interpolation is more   important than the mixing ratio when performing   interpolative regularization .   4.4 Layer - wise Ablation   We compare the performance of DMand TMix   for different sets of mixup layers in Table 5 . TMix   attains the best performance when the layer setf7;9;12gis used since layers 6 , 7 , 9 and 12 contain   the most amount of syntactic and semantic infor-   mation ( Chen et al . , 2020a ) . Interestingly , DM   achieves the best performance when the layer is   sampled from the set f3;4;6;7;9;12 g. This sug-   gests that the surface - level information contained   in layers 3 and 4 ( Jawahar et al . , 2019 ) is effectively   leveraged by the distance - aware matrix M , leading   to further improvements over purely syntactic and   semantic information in layers f6;7;9;12 g.   4.5 Effect of Varying Thresholds   We perform a study by varying the threshold  for   DMand present it in Figure 3 . A decreasing   denotes a larger distribution space for sampling   instances for Mixup , and a Tof0%decomposes it   to TMix or random Mixup . We observe an initial   increase in the performance as we constrain the em-   bedding space , suggesting the sampling of more di-   verse samples for interpolation . We observe a drop   in performance when the constrain becomes very   high , indicating that further expanding the sam-   pling space does not lead to more diverse synthetic   samples . This shows the existence of an optimum   set of input samples for performing Mixup , and we   conjecture it can be related to the sparsity in the   embedding distribution of different languages .   5 Conclusion   We propose DM , a novel data augmentation tech-   nique that interpolates samples intelligently cho-   sen based on their hyperbolic distance in the em-   bedding space . DMachieves state - of - the - art re-   sults over existing data augmentation approaches   on8standard and multilingual datasets in English ,   Arabic , Turkish , and Hindi languages , requiring 3   times less number of iterations than random mixup .   DMbeing independent of the underlying model   and modality , holds potential to be applied on text ,   speech , and vision downstream tasks.6096 Acknowledgements   This work has been supported by the German Fed-   eral Ministry of Education and Research ( BMBF )   as a part of the Junior AI Scientists program under   the reference 01 - S20060 . We thank the anonymous   reviewers for their valuable inputs .   References610   A Extended Analysis   We compare the performance of DMon standard   English and GLUE datasets with additional base-   lines and interpolative augmentation methods like   EMix ( Jindal et al . , 2020 ) and SSMix ( Yoon et al . ,   2021 ) .   B Dataset Details   1.TRAC . ( Bhattacharya et al . , 2020 ) is a col-   lection of posts , comments , and other con-   tent from popular social media , streaming andsharing platforms . For the purpose of our ex-   periments , we perform the aggression classiÔ¨Å-   cation task , for which , the data is labelled into   3 classes based on the level of aggression .   2.TREC - Coarse . ( Li and Roth , 2002 ) , The   Text REtrieval Conference - Coarse is a ques-   tion classiÔ¨Åcation dataset consisting of 6   classes . The data is sourced from English   questions by USC , TREC 8 , TREC 9 , TREC   10 and manually constructed questions .   3.TREC - Fine . ( Li and Roth , 2002 ) contains   the same set of questions as TREC - Coarse   grouped into 47 Ô¨Åne - grained classes instead   of 6 .   4.CoLA . ( Warstadt et al . , 2018 ) , abbreviation   for the Corpus of Linguistic Acceptability is a   part of GLUE ( Wang et al . , 2018 ) benchmark .   It is a collection of English sentences from 23   linguistic publications that are annotated for   their grammatical acceptability .   5.SST-2 . ( Socher et al . , 2013 ) is a GLUE ( Wang   et al . , 2018 ) benchmark dataset consisting of   English sentences from movie reviews . Sam-   ples in the dataset are annotated for sentiment   classiÔ¨Åcation task .   6.AHS . ( Albadi et al . , 2018 ) is an Arabic hate   speech classiÔ¨Åcation dataset focusing mainly   on Saudi Twittersphere . The data has been   collected over a span of 6 months from March   2018 to August 2018 and has 3950 samples   classiÔ¨Åed into 2 classes .   7.TTC . ( Kilin√ß et al . , 2017 ) , Turkish Text Cat-   egorization dataset consists of 3600 Turk-   ish documents ( news / texts ) classiÔ¨Åed into 6   classes . The data is obtained between the pe-   riod from May 2015 to July 2015 .   8.HASOC . ( Mandl et al . , 2019 ) consists of con-   tent sampled from social media platforms . We   perform the binary Hate / Offensive content   classiÔ¨Åcation task on the Hindi dataset for the   purpose of our experiments .   C Experimental Setup   We mention the optimal hyperparameter settings in   Table 8.611   D Comparison with Contrastive   Learning   Contrastive learning involves training the underly-   ing model to learn an embedding space in which   similar sample pairs stay close to each other while   dissimilar ones are far apart . Hence , their training   objective directly involves training using this em-   bedding vector of the input samples in the dataset .   DMhowever chooses samples based on their   spatial distribution in the embedding space , but   does not have a training objective optimizing on   their position in the embedding space . The training   ofDMis still supervised in nature and involves   learning over the mixed label of the individual sam-   ples being used for interpolation .   E Qualitative Analysis   To further analyze DM , we perform a qualitative   study by choosing examples from the dataset and   compare the predictions made by TMix and DM-   NT with DM . We analyze token - level attention   assigned to the individual terms by BERT , where   color intensity corresponds to the attention score .   We present these results in Table 7.612   Minhan Xu , Yu Hong   School of Computer Science and Technology , Soochow University , China   cosmosbreak5712@gmail.com , tianxianer@gmail.com   Abstract   1 Introduction   Low - resource machine translation ( MT ) is chal-   lenging due to the scarcity of parallel data and , in   some cases , the absence of bilingual dictionaries   ( Zoph et al . , 2016 ; Miceli Barone , 2016 ; Koehn and   Knowles , 2017 ; Zhang et al . , 2017 ) . Unsupervised ,   multilingual and transfer learning have been proven   effective in the low - resource MT tasks , grounded   on different advantages ( section 2 ) .   In this paper , we follow Aji et al . ( 2020 ) ‚Äôs work   to utilize cross - language transfer learning , of which   the ‚Äú parent - child ‚Äù transfer framework is Ô¨Årst pro-   posed by Zoph et al . ( 2016 ) . In the parent - child   scenario , a parent MT model and a child MT model   are formed successively , using the same neural net-   work structure . In order to achieve the sufÔ¨Åcient   warm - up effect from scratch , the parent is trained   onhigh -resource language pairs . Further , the child   inherits the parent ‚Äôs properties ( e.g. , inner parame-   ters and embedding layers ) , and it is boosted by the   Ô¨Åne - tuning over low - resource language pairs . One   of the distinctive contributions in Aji et al . ( 2020)‚Äôsstudy is to demonstrate the signiÔ¨Åcant effect of em-   bedding duplication for transference , when it is   conducted between the morphologically - identical   sub - words in different languages .   We attempt to extend Aji et al . ( 2020 ) ‚Äôs work   by additionally duplicating embedding informa-   tion among the aligned multilingual sub - words .   It is motivated by the assumption that if the du-   plication between morphologically - identical sub-   words contributes to cross - language transference ,   the duplication among any other type of equiva-   lents is beneÔ¨Åcial in the same way , such as that of   the aligned sub - words , most of which are likely   to be morphologically - dissimilar but semantically-   similar ( or even exactly the same ) .   In our experiments , both the parent and child   MT models are built with the transformer - based   ( Vaswani et al . , 2017 ) encoder - decoder architec-   ture ( Section 3.1 ) . We use the unigram model from   SentencePiece ( Kudo and Richardson , 2018 ) for to-   kenizing , and carry out sub - word alignment using   eÔ¨Çomal ( Section 3.2 ) . On the basis , we develop a   normalized element - wise embedding aggregation   method to tackle the many - to - one embedding du-   plication for aligned sub - words ( Section 3.3 ) . The   experiments show that our method achieves sub-   stantial improvements without using data augmen-   tation .   2 Related Work   The majority of previous studies can be sorted   into 3 aspects in terms of the exploited learning   strategies , including unsupervised , multilingual   and transfer learning .   ‚Ä¢Unsupervised MT conducts translation   merely conditioned on monolingual language   models ( Lample et al . , 2018a ; Artetxe et al . ,   2017 ) . The ingenious method that has   been explored successfully is to bridge the   source and target languages using a shareable613representation space ( Lample et al . , 2018b ) ,   which is also known as interlingual ( Cheng   et al . , 2017 ) or cross - language embedding   space ( Kim et al . , 2018 ) . To systematize   unsupervised MT , most ( although not all )   of the arts leverage bilingual dictionary   induction ( Conneau et al . , 2018 ; S√∏gaard   et al . , 2018 ) , iterative back - translation   ( Sennrich et al . , 2016a ; Lample et al . , 2018b )   and denoised auto - encoding ( Vincent et al . ,   2008 ; Kim et al . , 2018 ) .   ‚Ä¢Multilingual MT conducts translation merely   using a single neural model whose parameters   are thoroughly shared by multiple language   pairs ( Firat et al . , 2016 ; Lee et al . , 2017 ; John-   son et al . , 2017 ; Gu et al . , 2018a , b ) , including   a variety of high - resource language pairs as   well as a kind of low - resource ( the target lan-   guage is Ô¨Åxed and deÔ¨Ånite ) . Training on a mix   of high - resource and low - resource ( even zero-   resource ) language pairs enables the shareable   model to generalize across language bound-   aries ( Johnson et al . , 2017 ) . The beneÔ¨Åts re-   sult from the assimilation of relatively exten-   sive translation experience and sophisticated   modes from high - resource language pairs .   ‚Ä¢Transferable MT is fundamentally similar   to multilingual MT , whereas it tends to play   the aforementioned Parent - Child game ( Zoph   et al . , 2016 ) . A variety of optimization meth-   ods have been proposed , including the transfer   learning over the embeddings of WordPieces   tokens ( Johnson et al . , 2017 ) , BPE sub - words   ( Nguyen and Chiang , 2017 ) and the shared   multilingual vocabularies ( Kocmi and Bojar ,   2018 ; Gheini and May , 2019 ) , as well as the   transference that is based on the artiÔ¨Åcial or   automatic selection of congeneric parent lan-   guage pairs ( Dabre et al . , 2017 ; Lin et al . ,   2019 ) . In addition , Aji et al . ( 2020 ) verify the   different effects of various transferring strate-   gies of sub - word embeddings , such as that   among morphologically - identical sub - words .   In this paper , we extend Aji et al . ( 2020 ) ‚Äôs   work , transferring embedding information not only   among the morphologically - identical sub - words   but the elaborately - aligned sub - words.3 Approach   3.1 Preliminary : Basic Transferable NMT   We follow Kim et al . ( 2019 ) and Aji et al . ( 2020 )   to build neural MT ( NMT ) models with 12 - layer   transformers ( Vaswani et al . , 2017 ) , in which the   Ô¨Årst 6 layers are used as the encoder while the   subsequent 6 layers the decoder .   Embedding Layer As usual , the encoder is cou-   pled with a trainable embedding layer , which main-   tains a Ô¨Åxed bilingual vocabulary and trainable sub-   word embeddings . Each embedding is speciÔ¨Åed as   a 512 - dimensional real - valued vector .   Parent - Child Transfer We follow Zoph et al .   ( 2016 ) to conduct Parent - Child transfer learning .   SpeciÔ¨Åcally , we adopt an off - the - shelf transformer-   based NMTwhich was adequately trained on high-   resource De!En ( German!English ) language   pairs . The publicly - available data of OPUS ( Tiede-   mann , 2012 ) is used for training , which comprises   about 351.7 M De!En parallel sentence pairs . We   regard this NMT model as the Parent . Further , we   transfer all inner parameters of the 12 - layer trans-   formers from Parent to Child .   By contrast , the embedding layer of Parent is par-   tially transferred to Child , which has been proven   effective in Aji et al . ( 2020 ) ‚Äôs study . Assume V   denotes the high - resource ( e.g. , the aforementioned   De - En ) vocabulary while Vthe low - resource , the   morphologically - identical sub - words Vare then   speciÔ¨Åed as the ones occurring in both VandV   ( i.e. ,V = V\V ) . Thus , we duplicate the embed-   dings of morphologically - identical sub - words V   from the embedding layer of Parent to that of Child .   Further , we randomly initialize the embeddings   of the rest sub - words Vin the Child ‚Äôs embedding   layer ( V = V V ) , where random sampling from   a Gaussian distribution is used .   Both the transferred inner parameters and the du-   plicated embeddings constitutes the initial state of   the Child NMT model . On the basis , we Ô¨Åne - tune   Child on the low - resource language pairs , such as   the considered 18 K My ! En ( Burmese!English )   parallel data in our experiments .   3.2 Tokenizer and Alignment   We strengthen Parent - Child transfer learning by ad-   ditionally duplicating embeddings for aligned sub-   words ( between low and high - resource languages).614Doc . Sent . Token   My 113 K 1.1 M 17.4 M   I d 1.1 M 8.3 M 156.2 M   Tr 705 K 5.8 M 128.2 M   The precondition is to produce the word - level align-   ment and equivalently assign it to sub - words .   Word Alignment We use EÔ¨Çomalto achieve   the word alignment . It is developed based on EF-   MARAL ( √ñstling et al . , 2016 ) , where Gibbs sam-   pling is run for inference on Bayesian HMM mod-   els . EÔ¨Çomal is not only computationally efÔ¨Åcient   but able to perform n - to-1 alignment . We sepa-   rately train EÔ¨Çomal on the low - resource My ! En ,   I d ( Indonesian)!En and Tr ( Turkish ) ! En parallel   data ( Section 4 ) .   Sub - word Tokenizer We train a sub - word tok-   enizer using the unigram model of SentencePiece   for each low - resource language , including My , I d   and Tr . The tokenizers are trained on monolingual   plain texts which are collected from Wikipedia ‚Äôs   dumps . The toolkit wikiextractoris utilized to   extract plain texts from the semi - structured data .   The statistics of training data is shown in Table 1 .   We uniformly set the size of sub - word vocabu-   lary to 50 K when training the tokenizers . The ob-   tained vocabulary of each low - resource language is   utilized for sub - word alignment , towards the mixed   De - En sub - word vocabulary in the Parent NMT   model . The size of De - En vocabulary is 58K.   Sub - word Alignment Given a pair of aligned   bilingual words , we construct the same correspon-   dence for their sub - words by many - to - many map-   pings . See the De!Tr example in ( 1 ) .   ( 1 ) Word Alignment : | produktion$√ºretme   |Harnstoff$√ºre   Sub - word Alignment : | produck${√ºre , tme }   |tion${√ºre , tme }   |Harn${√ºre }   |stoff${√ºre }   It is unavoidable that some of the aligned sub-   words are non - canonical . Though , the positive ef-   fect on transfer learning may be more substantial   than negative . It motivated by the Ô¨Åndings that   the use of sub - words ensures a sufÔ¨Åcient overlapTrain . Val . Test   My - En ( ALT ) 18 K 1 K 1 K   Id - En ( BPPT ) 22 K 1 K 1 K   Tr - En ( WMT17 ) 207 K 3 K 3 K   between vocabularies ( Nguyen and Chiang , 2017 ) ,   and thus enables the transfer of a larger number of   concrete embeddings rather than random ones .   3.3N - to-1 Embedding Duplication   Assume that Vdenotes the sub - words in low-   resource vocabulary that have aligned sub - words   in high - resource vocabulary , the mapping is D(x ) ,   note that8x2V , D(x)is a set of sub - words .   Thus , in the embedding layer of Child , we extend   the range of sub - words for embedding transfer , in-   cluding both the identical sub - words Vand the   aligned V. To enable the transfer , we tackle n-   to-1 embedding duplication . It is because that , in   a large number of cases , there is more than one   high - resource sub - word corresponding to a single   low - resource sub - word ( see ‚Äú √ºre ‚Äù in ( 1 ) ) .   Given a sub - word xinVand the aligned sub-   words vinD(x ) , we rank vin terms of the fre-   quency with which they were found to be aligned   withxin the parallel data . On the basis , we carry   out two duplication methods as below .   ‚Ä¢Top-1 We take the top- 1sub - word xfromv ,   and perform element - wise embedding dupli-   cation from xtox:8i ; E(x ) = E(x)(iis   thei - th dimension of embedding E( ) ) .   ‚Ä¢Mean We adopt all the sub - words in v , and   duplicate their embedding information by the   normalized element - wise aggregation ( where ,   ndenotes the number of sub - words in v ):   8i ; E(x ) = E(x)=n   4 Experimentation   4.1 Datasets and Evaluation Metric   We evaluate the transferable NMT models for three   source languages ( My , I d and Tr ) . English is in-   variably speciÔ¨Åed as the target language . There are   three low - resource parallel datasets used for train-   ing the Child NMT model , including Asian Lan-   guage Treebank ( ALT ) ( Ding et al . , 2018 ) , PAN Lo-   calization BPPTand the corpus of WMT17 news615Model My - En Id - En Tr - En   Baseline 20.5 26.0 17.0   MI - PC 21.0 27.5 17.6   Top-1 - PC 21.9 27.6 18.0   Mean - PC 22.5 28.0 18.1   Model My - En Id - En Tr - En   Baseline 20.2 24.5 16.5   MI - PC 20.4 24.2 16.8   Top-1 - PC 21.2 26.9 16.9   Mean - PC 21.9 27.1 16.9   translation task ( Bojar et al . , 2017 ) . The statistics   in the training , validation and test sets is shown   in Table 2 . We evaluate all the considered NMT   models with SacreBLEU ( Post , 2018 ) .   4.2 Hyperparameters   We use an off - the - shelf NMT model as Parent ( Sec-   tion 3.1 ) , whose state variables ( i.e. , hyperparam-   eters and transformer parameters ) and embedding   layer are all set . On the contrary , the Child NMT   model needs to be regulated from scratch .   When training and developing Child , we adopt   the following hyperparameters . Each source lan-   guage was tokenized using SentencePiece ( Kudo   and Richardson , 2018 ) with 50k vocabulary size .   Training was carried out with HuggingFace Trans-   formers library ( Wolf et al . , 2020 ) using the Adam   optimizer with 0.1 weight decay rate . The maxi-   mum sentence length was set to 128 and the batch   size to 64 sentences . The learning rate was set to   5e-5 and checkpoint frequency to 500 updates . For   each model , we selected the checkpoint with the   lowest perplexity on the validation set for testing .   5 Results and Analysis   Table 3 shows the test results , where all the consid-   ered Parent- Child transfer models are marked with   ‚Äú PC ‚Äù , and the baseline is the transformer - based   NMT ( Section 3.1 ) which is trained merely using   low - resource parallel data ( without transfer learn-   ing ) . MI - PC is the reproduced transfer model in   terms of Aji et al . ( 2020 ) ‚Äôs study , in which only   the embedding transference of morphologically-   identical sub - words is used . We report NMT per-   formance when MI - PC is used to enhance the base-   line , as well as that when our auxiliary transfer   Model My - En Id - En Tr - En   Baseline 1.30 1.27 4.49   MI - PC 1.30 1.35 3.53   Top-1 - PC 1.11 1.00 3.07   Mean - PC 0.96 0.94 2.14   models ( i.e. , Top-1 and Mean in Section 3.3 ) are   additionally adopted , separately .   It can be observed that , compared to MI - PC , both   Top-1 - PC and Mean - PC yield improvements for all   the three low - resource MT scenarios . The most   signiÔ¨Åcant improvement occurs for My ! En MT ,   reaching up to 1.5 BLEU . Both the models general-   ize well across changes in the input sub - words . It   can be illustrated in a separate experiment where   the BPE ( Sennrich et al . , 2016b ) tokenizer is used   ( instead of SentencePiece ( Kudo and Richardson ,   2018 ) ) , and all the transfer models are run over   the newly - aligned sub - words . As shown in Table   4 , both Top-1 - PC and Mean - PC still outperform   MI - PC , yielding an improvement of 2.9 BLEU at   best ( for Id!En MT ) .   Due to unavoidable errors in the sub - word align-   ment , the utilization of a single aligned sub - word   for embedding duplication easily results in perfor-   mance degradation . Aggregating and normalizing   embeddings of all possible aligned sub - words help   to overcome the problem . Figure 1 shows the NMT   performance obtained when the i - th top - ranked   aligned sub - word is exclusively used for transfer ,   as well as the aggregation of top- isub - words is   used . It can be found that the latter model almost   always outperforms the former model.616We compare the training time consumption of all   experiments , the result is shown in Table 5 . We use   mixed precision for training the child MT model .   All experiments are conducted on a single NVIDIA   P100 16 GB GPU .   Obviously , the time that Mean - PC consumes dur-   ing training is less than other models . In the sce-   nario of Tr - En MT , the training duration is even   shortened from 4.49 hours ( i.e. , about 269 minutes )   to 2.14 , compared to the baseline model . Most   probably , it is caused by the transferring of a larger   number of sub - word embeddings during training .   In other word , Mean - PC actually transfers not   only morphologically - identical sub - words but the   aligned ones . This contributes more to the avoid-   ance of redundant learning over sub - word embed-   dings . All in all , Mean - PC is less time - consuming   when producing substantial improvements .   6 Conclusion   We enhance transferable Parent - Child NMT by du-   plicating embeddings of aligned sub - words . The   experimental results demonstrate that the proposed   method yields substantial improvements for all the   considered MT scenarios ( including My - En , Id - En   and Tr - En ) . More importantly , we successfully re-   duce the training duration . The efÔ¨Åciency can be   improved with the ratio of about 50 % at best .   Additional survey in the experiments reveals that   phonetic symbols can be used for transfer learning   between the languages belonging to different fami-   lies . For example , the phonologies of hamburger   in German and Burmese are similar ( H amburger   vs hambhargar ) . In the future , we will study bilin-   gual embedding transfer of phonologically - similar   words , so as to further improve low - resource NMT .   Acknowledgements   The research is supported by National Key R&D   Program of China ( 2020YFB1313601 ) and Na-   tional Science Foundation of China ( 62076174 ) .   References617618619   Shivam Agarwal , Ramit Sawhney , Sanchit Ahuja , Ritesh Soun , Sudheer Chava   Financial Services Innovation Lab , Georgia Institute of Technology   rsawhney31@gatech.edu , sudheer.chava@scheller.gatech.edu   Abstract   1 Introduction   Text stream modeling is a critical problem that   helps analyze trends over a variety of applications   spanning Ô¨Ånance ( Oliveira et al . , 2017 ) , health-   care ( Baytas et al . , 2017 ) , and political discourses   ( Sawhney et al . , 2021c ) . However , analyzing such   text sequences poses several challenges . First , mod-   eling individual text items may not be informative   enough since text sequences display a sequential   context dependency , where analyzing them together   in succession provides better contextual represen-   tation ( Hu et al . , 2018 ) . Second , timing plays an   essential role in online stream modeling as users   quickly react to new information ( Sawhney et al . ,   2021a ) . For instance , in stock markets , reacting a   second slower than other investors can lead to mas-   sive losses ( Scholtus et al . , 2014 ) . A fundamental   limitation in existing RNN methods is that it ig-   nores the natural Ô¨Åne - grained timing irregularities   in streams ( Foucault et al . , 2016 ; Eysenck , 1968 ) .   Social theories show that from a vast volume of   texts in a stream , only a few are powerful enough   to heavily inÔ¨Çuence the overall trend ( Van Dijk ,   1977 ; Gabaix , 2016 ) . Such texts are rare and theexcitation induced by them follows a powerlaw   distribution which gives rise to scale - free proper-   ties ( Zhao et al . , 2010 ) . For example , in political   debates , there are a few rare highly - inÔ¨Çuential de-   bates that heavily impact the overall voting deci-   sions of citizens ( Law , 2019 ) . Further , the impact   of such powerlaw excitations varies for each event .   The presence of varying powerlaw dynamics from   highly inÔ¨Çuential texts correlates with natural hi-   erarchies and scale - free dynamics in text streams ,   making them difÔ¨Åcult to model ( Sala et al . , 2018 ) .   The good news is that hyperbolic learning has   shown to better model such powerlaw dynamics   compared to Euclidean learning over domains , in-   cluding vision ( Khrulkov et al . , 2020 ) and NLP   ( Tifrea et al . , 2019 ) . However , existing works face   two major limitations , 1 ) they ignore the timing   irregularities in scale - free sequences and 2 ) they   use a single hyperbolic space to encode varying   levels of hyperbolic dynamics . Building on social   theories , our contributions can be summarized as :   ‚Ä¢We explore the hyperbolic properties of online   streams and propose a Hyperbolic Hawkes At-   tention Network ( HYPHEN ) which jointly learns   from the Ô¨Åne - grained timing irregularities and   powerlaw dynamics of streams ( ¬ß 2.2 ) .   ‚Ä¢Building on social theories , HYPHEN learns the   hyperbolic space based on the nature of the   stream ( ¬ß 2.1 ) . We introduce HYPHEN as a ge-   ometry agnostic model which can be applied on   any downstream application .   ‚Ä¢Through quantitative ( ¬ß 4.1 ) and exploratory   ( ¬ß 4.3 ) experiments on four tasks spanning sui-   cide ideation , political debate analysis , and Ô¨Å-   nancial forecasting over English and Chinese   languages , we demonstrate the practical applica-   bility of HYPHEN for stream modeling.6202 Methodology   Problem Formulation : For a sequence of texts   [ p:::;p]released at times [ t;:::;t]sequen-   tially , with [ t<<t ] , our target is to model   this sequence in a time - sensitive fashion for a vari-   ety of downstream applications ( ¬ß 3 ) .   2.1 Learnable Hyperbolic Geometry   Text sequences from social media and political dis-   courses pose hierarchies ( Sawhney et al . , 2021a )   i.e. , the datasets represent a tree like structure   which call for the use of hyperbolic spaces . Indeed ,   the volume of hyperbolic geometry grows expo-   nentially , in contrast to Euclidean spaces where the   growth is polynomial ( Khrulkov et al . , 2020 ) , en-   abling hyperbolic spaces to capture the underlying   scale - free properties of streams ( Sala et al . , 2018 ) .   However , text sequences exhibit a varying degree   of scale - free dynamics , which a single geometry   can not capture ( Gu et al . , 2019 ) . Thus , we seek to   learn the optimal underlying geometry .   The hyperbolic space is a non - Euclidean space   with a constant negative curvature c. To learn the   optimal geometry , we aim to learn the curvature   c , which controls the degree of hyperbolic prop-   erties represented by the space ( Gu et al . , 2019 ) .   Following ( Ganea et al . , 2018 ) we deÔ¨Åne the hyper-   bolic geometry with varying curvature cas(B;g ) ,   where the manifold B = fx2R : cjjxjj<1 g , is   endowed with the Riemannian metric g=g ,   where the conformal factor =and   g = diag[1;::;1]is the Euclidean metric tensor .   We denote the tangent space centered at point xas   TB . We generalize Euclidean operations to the   hyperbolic space via M√∂bius operations .   M√∂bius Addition for two points x;y2B , is ,   h:;:i , jjjj denotes the inner product and norm .   Exponential Map maps a tangent vector v2   TBto a point exp(v)in the hyperbolic space ,   Logarithmic Map maps a point y2Bto a point   log(y)on the tangent space at x ,   M√∂bius Multiplication   multiplies features   x2Bwith matrix W2R , given by   W   x = exp(Wlog(x ) ) ( 4 )   M√∂bius Pointwise Product  multiplies matrix   x2Bwith matrix y2Bpointwise ,   2.2 HYPHEN : Hyperbolic Hawkes Network   Text Embedding Layer We use Bidirectional   Encoder Representations from Transformers   ( BERT ) ( Devlin et al . , 2019 ) to encode each text p   to features ^m = BERT ( p)2Rwhered= 768 ,   obtained by averaging the token level outputs from   the Ô¨Ånal layer of BERT . To apply hyperbolic op-   erations over text features ^m , we project it to   the hyperbolic space via the exponential mapping   exp()given by , m = exp(^m )   Hyperbolic Time Aware Temporal Network   To encode the varying scale - free characteristics of   text sequences , we introduce LSTMs over learnable   hyperbolic spaces by leveraging M√∂bius operations   ( ¬ß 2.1 ) . Further , capturing Ô¨Åne - grained timing ir-   regularities in text streams plays a crucial role for   stream state modeling . For instance , the time inter-   val between two debates can vary widely , from a621few days to many months in parliamentary debates .   Consequently , the ideologies and thought process   of the speaker may change over time , reÔ¨Çecting a   decay or increase in dependence on the speaker ‚Äôs   previous speeches ( Van Dijk , 2002 ) .   To capture these time dependent intricacies in   a learnable hyperbolic space , we modify the hy-   perbolic LSTM ( Shimizu et al . , 2021 ) as shown   in Figure 1 into a hyperbolic time - aware tempo-   ral network ( (  ) ) . Intuitively , the greater the   time elapsed between text releases , the lesser the   impact they should have on each other . Thus , for a   given dayk , applies a decaying function over   k , the elapsed time between two texts [ p;p ] ,   transforming the time differences into weights :   C = exp(tanh ( log(W   Cb ) ) )   ^C = C  g(k )   C= CC   C = C^C   where Cis the previous cell memory , W;b   are the network parameters , and g()is a heuristic   decaying function . Following ( Baytas et al . , 2017 )   we setg(k ) = 1=k . Using the adjusted previ-   ous memory C , we deÔ¨Åne the current hidden   state and current memory states for , with   hyperbolic features mas :   ec=log(W   hU   mb )   C = i  ecf  C (   h = o  exp(tanh ( C ) ) (   where W;U;bare the learnable parameters ,   i;f;oare input , forget and output gates . Fi-   nally , given texts [ p;:::p]over a lookback pe-   riod T , we deÔ¨Åne the update rule of as ,   h= ( m;j;h);j2[1;T](6 )   where , hrepresents the hidden states of .   Hyperbolic Hawkes Attention Studies show   that not all historical texts are equally informative   and pose a diverse inÔ¨Çuence over the predictions   ( Sawhney et al . , 2021c ) . We use a temporal hyper-   bolic attention mechanism ( Luong et al . , 2015 ) to   emphasize texts likely to have a substantial inÔ¨Çu-   ence . This mechanism learns attention weights   for each hiddden state h2h= [ h;:::;h]as ,   =     exp ( log(h)(Wlog(h)))   ( 7)where , Wdenotes learnable weights .   Next , we enhance the temporal hyperbolic at-   tention using the Hawkes process ( Mei and Eisner ,   2017 ) and propose a hyperbolic Hawkes attention   mechanism . The Hawkes process is a temporal   point process that models a sequence of arrival   of texts over time . Each text item ‚Äú excites ‚Äù the   process in the sense that the chance of a subse-   quent arrival is increased for some time . Studies   ( Zuo et al . , 2020 ; Sawhney et al . , 2021b ) show that   the Hawkes process can be used to model text se-   quences from social media and discourses . The   hyperbolic Hawkes attention mechanism learns an   excitation parameter corresponding to excitation   induced by text pand a decay parameter  to learn   the decay rate of this induced excitement . Formally ,   we use an Einstein midpoint ( Ungar , 2005 ) to ag-   gregate hidden states hvia Hawkes process as ,   where ,   ( q)=pare the lorentz factors .   3 Applications and Tasks   Political Stance Prediction Parliamentary de-   bates consist of responses from politicians over   a motion . Following ( Sawhney et al . , 2020 ) , we   aim to classify the stance of a speaker as ‚Äò Aye‚Äô/‚ÄòNo ‚Äô   on a motion based on their historic speeches . We   evaluate on the ParlV ote dataset ( Abercrombie and   Batista - Navarro , 2020 ) comprising of 33,461 UK   debate transcripts of 1,346 politicians .   Financial NLP We aim to predict future stock   trends based on the historic texts about a stock .   Following ( Sawhney et al . , 2021a ) we regress   the future volatility of a stock deÔ¨Åned as =   ln(jj ) , wherepis the closing price . We   evaluate on the S&P ( Xu and Cohen , 2018 ) contain-   ing 88 stocks with 109,915 tweets and the China   Stock Exchange ( CSE ) ( Huang et al . , 2018 ) con-   taining 90,361 Chinese news articles for 85 stocks .   Suicide Ideation Following ( Sawhney et al . ,   2021d ) , we aim to detect suicidal intent in a tweet   given historic tweets from a user . We use the data   from ( Mishra et al . , 2019 ) containing 32,558 user   timelines and 2.3 M texts.622   4 Results   4.1 Performance Comparison   We compare the performance of HYPHEN over Ô¨Ånan-   cial , political , and healthcare tasks spanning En-   glish and Chinese languages in Table 1 . We observe   thatHYPHEN generally outperforms most baseline   methods by 10 % on average . Overall , we note that   methods that capture Ô¨Åne - grained timing irregu-   larities in text sequences perform better ( HYPHEN ,   FAST , HT - LSTM ) , validating our premise of using   time - aware modeling . We postulate that HYPHEN ‚Äôs   superior performance is due to , 1 ) learnable hy-   perbolic geometry and 2 ) time - aware hyperbolic   Hawkes process . First , HYPHEN better encodes the   varying hyperbolic properties of text sequences by   learning a suitable data - driven curvature in contrast   to other hyperbolic models ( HT - LSTM ) , which con-   strain all sequences to a Ô¨Åxed hyperbolic space .   Second , through hyperbolic time aware learning   and Hawkes attention , HYPHEN better captures tim-   ing irregularities between the subsequent release of   texts ( Sawhney et al . , 2021a ) . These observations   collectively show the practical applicability and   generalizability of HYPHEN for stream modeling .   4.2 Ablation Study   We contextualize the impact of various components   ofHYPHEN in Table 2 . We note that augmenting   RNN - based methods with attention leads to sig-   niÔ¨Åcant improvements ( p<0:01 ) , as HYPHEN can   better distinguish noise inducing text from relevant   information ( Sawhney et al . , 2021e ) . Next , we   observe signiÔ¨Åcant ( p < 0:01 ) improvements on   using hyperbolic spaces to represent text streams ,   suggesting that the hyperbolic space better mod-   els the innate power - law dynamics and hierarchies   in online text streams ( Sala et al . , 2018 ) . Further ,   enriching the temporal attention with the Hawkes   process leads to performance boosts , potentially   Time ( in Months )   because the Hawkes process better captures the ex-   citation induced by inÔ¨Çuential texts . Finally , learn-   ing the underlying hyperbolic geometry beneÔ¨Åts   HYPHEN , allowing it to generalize to a variety of   text streams with different hyperbolic properties .   4.3 Impact of Historical Context   We study the variation in HYPHEN ‚Äôs performance   on political speaker state modeling corresponding   to varying amounts of lookback periods Tin Figure   2 . First , without encoding the historic context , we   observe that all models perform poorly . As we in-   crease the lookback period , we note that Hawkes at-   tention improves temporal attention , potentially be-   cause the Hawkes process decays the impact of very   old texts enabling HYPHEN to focus on more recent   debates which better reÔ¨Çects a speaker ‚Äôs temporal   state . Further , with very large lookback periods , we   observe a performance drop , likely because large   amounts of context allow the inclusion of speeches   from very old ( stale ) debates , which may not con-   tribute signiÔ¨Åcantly to the speaker ‚Äôs present state   ( Cullen et al . , 2018 ) . However , through hyperbolic   Hawkes attention HYPHEN is able to Ô¨Ålter out more   crucial debates to an extent . In general , HYPHEN623provides the best results with debates around ten   months in the past ( mid - sized lookbacks ) .   5 Conclusion   We explore the scale - free dynamics and timing ir-   regularities of text streams . We propose HYPHEN   which uses hyperbolic Hawkes attention and learns   data - driven geometries to represent varying hyper-   bolic properties of streams . Through experiments   on political , Ô¨Ånancial NLP , and healthcare tasks ,   we show the applicability of HYPHEN on 4 datasets .   Acknowledgements   We would like to thank the Financial Services Inno-   vation Lab at Georgia Institute of Technology for   their generous support .   6 Ethical Considerations   The sensitive nature of this work calls for careful   deliberation of the risks and ethical challenges in-   volved . While we only use publicly available user   data , we emphasize the importance of preserving   the privacy of the users involved ( De Choudhury   et al . , 2016 ) . We acknowledge that the predictive   power of HYPHEN depends on the data , which is   in tension with user privacy concerns . We care-   fully adopt the measures followed by Chancellor   et al . ( 2016 ) . SpeciÔ¨Åcally , we operate within the   acceptable privacy bounds ( Chancellor et al . , 2019 )   and considerations ( Fiesler and Proferes , 2018 )   in order to avoid coercion and harmful interven-   tions ( Chancellor et al . , 2019 ) . We paraphrase and   anonymize all samples in the suicide ideation detec-   tion detection dataset using the moderate disguise   scheme ( Bruckman , 2002 ; Fiesler and Proferes ,   2018 ) . We also perform automatic de - identiÔ¨Åcation   using named entity recognition to identify and   mask personally identiÔ¨Åable information .   While one of our work ‚Äôs application is to aid   in the early detection of suicidal users and early   intervention , it is imperative that any interventions   be well - thought , failing which may lead to counter-   helpful outcomes , such as users moving to fringe   platforms , which would make it harder to provide   assistance ( Kumar et al . , 2015 ) . Care should be   taken so as not to create stigma , and interventions   must be carefully planned by consulting relevant   stakeholders such as clinicians , designers , and re-   searchers ( Chancellor et al . , 2016 ) , to maintain   social media as a safe space for individuals looking   to express themselves ( Chancellor et al . , 2019).References624625   A Experimental Setup   A.1 Datasets   ‚Ä¢US S&P ( Xu and Cohen , 2018 ): US S&P   stocks are categorized into 9 industries : ba-   sic materials , consumer goods , healthcare , ser-   vices , utilities , conglomerates , Ô¨Ånancial , indus-   trial goods and technology . US S&P dataset   contains text data and historical prices of 88   stocks which includes all 8 stocks in conglom-   erates and the top 10 stocks by market capi-   talization in each of the other industries . The   text data comprises tweets from 01/01/2014 to   01/01/2016 . Following ( Xu and Cohen , 2018 )   we split the US S&P temporally based on date   ranges from 01/01/2014 to 01/08/2015 for train-   ing , 01/08/2015 to 01/10/2015 for validation ,   and 01/10/2015 to 01/01/2016 for test .   ‚Ä¢China and Hong Kong ( CSE ) ( Huang et al . ,   2018 ): China and Hong Kong ( CSE ) dataset con-   sists of news headlines of 85 top - traded stocks   listed on the Shanghai , Shenzhen , and Hong   Kong Stock Exchange from January 2015 to De-   cember 2015 . The qualitative data comprises of   90,361 Chinese Ô¨Ånancial news headlines . We   split the China & HK dataset temporally based   on date ranges from 01/01/2015 to 31/08/2015   for training , 01/09/2015 to 30/09/2015 for vali-   dation , and 01/10/2015 to 01/01/2016 for testing   all models .   ‚Ä¢ParlVote ( Abercrombie and Batista - Navarro ,   2020 ): Following ( Sawhney et al . , 2020 ) we   evaluate political stance detection on the Par-   lV ote dataset . This record consists of debate   transcripts from the UK House of Commons ob-   tained under an open Parliament license . Follow-   ing ( Abercrombie and Batista - Navarro , 2020 )   we remove non - speech elements from the tran-   scripts and the original casing is preserved . Par-   lV ote consists of 33,461 transcripts from May   71997 to November 52019 . The average   number of tokens in a ParlV ote speech is 760.2 ¬±   901.3 . Based on a speaker ‚Äôs vote to their speech ,   transcripts are labeled as ‚Äò Aye ‚Äô and ‚Äò No ‚Äô rep-   resenting positive and negative stance respec-   tively . The dataset is fairly balanced , consisting626of 53.57 % ‚Äò Aye ‚Äô and 46.43 % ‚Äò No ‚Äô labels . We   split the dataset temporally to obtain 70 % , 15 %   and 15 % of the data for training , validation and   testing respectively .   ‚Ä¢Suicide Ideation . ( Sawhney et al . , 2021d ) :   The Suicide ideation dataset is built upon the ex-   isting Twitter tweets database of ( Mishra et al . ,   2019 ) . The dataset consists of tweets of 32,558   unique users , spanning over ten years of histor-   ical tweets from 2009 to 2019 . Out of all the   tweets , 34,306 tweets were identiÔ¨Åed as having   potential suicide ideation words . These tweets   were then manually annotated by two psychol-   ogists under the supervision of a head psychol-   ogist and 3984 tweets were actually identiÔ¨Åed   as having suicidal tendencies . The same prepro-   cessing techniques were employed on the dataset   as done by Sawhney et al . ( 2021d ) .   A.2 Evaluation Metrics   Matthews correlation coefÔ¨Åcient : The   Matthews correlation coefÔ¨Åcient ( MCC ) pro-   duces a high score only if the prediction obtained   good results in all of the four confusion matrix   categories ( true positives , false negatives , true   negatives , and false positives ) , proportionally both   to the size of positive elements and the size of   negative elements in the dataset . We use MCC to   evaluate on suicide ideation detection and political   speech classiÔ¨Åcation .   Mean squared error : To evaluate the volatil-   ity regression performance , we adopt the Mean   Squared Error ( MSE ) to compute the error between   actual and the predicted volatility values .   A.3 Baseline Models   We compare HYPHEN with the following baselines :   ‚Ä¢MLP : A Bag of Words model that uses unigram   textual features as input along with the TF - IDF   vectors which are fed into a multi - layer percep-   tron ( Abercrombie and Batista - Navarro , 2020 ) .   ‚Ä¢LSTM : An RNN architecture capable of learn-   ing long term sequential dependencies ( Hochre-   iter and Schmidhuber , 1997 ) .   ‚Ä¢HAN : Transformer model with hyperbolic acti-   vations and attention which utilises hyperbolic   geometry for both computation and aggregation   of attention weights ( Gulcehre et al . , 2019).‚Ä¢H - LSTM : A RNN based model for sequential   data with an attention mechanism operating in   the hyperbolic space ( L√≥pez and Strube , 2020 ) .   ‚Ä¢FAST : A time - aware LSTM network capable of   modeling the Ô¨Åne grained temporal irregularities   in textual data ( Sawhney et al . , 2021e ) .   ‚Ä¢HT - LSTM : Hierarchical Time - aware hyper-   bolic LSTM network leverages the hyperbolic   space for encoding scale - free nature of a text   stream ( Sawhney et al . , 2021a ) .   A.4 Training Setup   We have performed all our experiments on Tesla   GPU . We performed a grid search for all our mod-   els and selected the best values based on the vali-   dation MCC / MSE . We followed the same prepro-   cessing techniques as suggested by the dataset au-   thors . We explored the lookback window length   T2[2;20]and the hidden state dimensions in   2(64;128;256 ) . We grid searched our learning   rates in2(1e 5;5e 4;1e 3 ) . We used Rie-   mannian Adam ( B√©cigneul and Ganea , 2018 ) as   our optimizer.627   Ramit Sawhney , Atula Tejaswi Neerkaje , Manas GaurAI Institute , University of South Carolina , SC , USA   mgaur@email.sc.eduManipal Institute of Technology , Manipal , India   atula.neerkaje@learner.manipal.edu   Abstract   1 Introduction   Suicide is a global phenomenon responsible for   1.3 % of deaths worldwide ( WHO , 2019 ) . While   it is the leading cause of death among 14 - 35 year   olds in the US ( Hedegaard et al . , 2021 ) , suicide   rates have increased by 13 % in Japan between July   to September 2020 ( Tanaka and Okamoto , 2021 ) .   It hence becomes critical to extend clinical and   psychiatric care , which relies heavily on identifying   those at risk . While 80 % of patients do not undergo   clinical treatment , 60 % of those who succumbed to   suicide denied having suicidal thoughts to mental   health experts ( McHugh et al . , 2019 ) . However ,   studies show eight out of ten people shared suicidal   thoughts on social media ( Golden et al . , 2009 ) .   The advent of Natural Language Processing   ( NLP ) shows promise for suicide risk assessment   based on online user behavior ( Ji et al . , 2021b ;   Choudhury et al . , 2016 ) , with automatic risk as-   sessment algorithms outperforming traditional clin-   ical methods ( Coppersmith et al . , 2018 ; Linthicum   et al . , 2019 ) . Numerous deep learning methods   already exist , which include leveraging suicide-   related word - embeddings ( Cao et al . , 2019 ) , social   graphs ( Mishra et al . , 2019 ; Sinha et al . , 2019 ; Cao   et al . , 2022 ; Sawhney et al . , 2021b ) and historical   context ( Matero et al . , 2019 ; Gaur et al . , 2019 ) .   However , mental health is a safety - critical realm ,   where technological failure could lead to severe   harm to users on social media ( Sittig and Singh ,   2015 ) . One such case was covered by Register   ( 2020 ) , wherein a medical bot suggested a mock   patient kill themselves , demonstrating that unin-   tended harmful behavior can emerge from AI sys-   tems ( Amodei et al . , 2016 ; Chandler et al . , 2020).628Despite the signiÔ¨Åcant power of traditional NLP   methods , such models are inherently designed to   make a prediction even when not conÔ¨Ådent . This   poses a challenge when working with critical tasks   like suicide risk assessment , for which it may be   hard to make a prediction due to various reasons   such as task hardness or contained ambiguity . Such   a system may associate a lower risk level to a user   who needs urgent help . A resulting delayed re-   sponse from mental health experts may lead to   adverse consequences . We hence need systems that   assign high priority to uncertain predictions , for   immediate review and response .   Contributions : We reformulate suicide risk assess-   ment as a prioritized prediction task which factors   in uncertainty , and propose SASI : A Risk - Averse   Mechanism for Suicidality Assessment on Social   MedIa . SASI is risk - averse in the sense that it   is self - aware , as it incorporates a selection func-   tion to measure uncertainty . Based on a set thresh-   old value , SASI refrains from making a prediction   when it is uncertain . We show that SASI can act   as a tool to efÔ¨Åciently prioritize users who need   immediate attention . Through a human - in - the - loop   framework that involves a domain expert , SASI   assigns high priority to uncertain predictions to   avoid critical failure ( Figure 1 ) . We demonstrate   the effectiveness of SASI using a real - world gold   standard Reddit dataset . Through a series of exper-   iments , we show SASI refrains from making 83 %   of incorrect predictions . We further demonstrate   its effectiveness through a qualitative study and   discuss the ethical implications .   2 Methodology   2.1 Columbia Suicide Severity Risk Scale   The Columbia Suicide Severity Rating Scale ( C-   SSRS ) is an authoritative questionnaire employed   by psychiatrists to measure suicide risk severity   ( Posner et al . , 2011 ) . There are 3 items in the scale :   Suicide Ideation , Suicide Behavior , and Suicide   Attempt . Each C - SSRS severity class is composed   of a conceptually organized set of questions that   characterize the respective category . Responses to   the questions across the C - SSRS classes eventually   determine the risk of suicidality of an individual   ( Interian et al . , 2018 ; McCall et al . , 2021 ) . One of   the challenges researchers face when it comes to   dealing with social media content is the disparity in   the level of emotions expressed ( Gaur et al . , 2019 ) .   Since the C - SSRS was originally designed for use   in clinical settings , adapting the same metric to a   social media platform would require changes to   address the varying nature of emotions expressed .   For instance , while in a clinical setting , it is typ-   ically suicidal candidates that see a clinician ; on   social media , non - suicidal users may participate   to offer support to others deemed suicidal ( Gaur   et al . , 2021 ) . To address these factors , two addi-   tional classes were deÔ¨Åned ( Gaur et al . , 2019 ) to the   existing C - SSRS scale with three classes : Suicide   Indicator and Supportive ( Negative class ) .   2.2 Problem Formulation   Following existing work ( Gaur et al . , 2019 ; Sawh-   ney et al . , 2021a ) , we formulate the problem as   a classiÔ¨Åcation task to predict the suicidal risk   of the useru2fu;u;;ug , whose posts   P = fp;p;;pgare authored over time in a   chronological order , with the latest post being p.   We denote the label set Y= { Support ( SU ) , Indica-   tor ( IN ) , Ideation ( ID ) , Behaviour ( BR ) , Attempt   ( AT ) } in increasing order of severity risk , deÔ¨Åned   based on the C - SSRS . For a given Suicide Ideation   Model , our goal is to expand the cardinality of the   label space tojYj+ 1so as to enable an option to   refrain when the model is uncertain.6292.3 Suicide Ideation Model ( SIM )   Each post made by a user could provide detailed   context of suicidal thought manifestation over time   ( Oliffe et al . , 2012 ) . To capture this property ,   we draw inspiration from existing state - of - the - art   ( SOTA ) models ( Gaur et al . , 2019 ; Matero et al . ,   2019 ; Sawhney et al . , 2021a ; Ji et al . , 2021a ) which   use LSTM based backbones . To encode each   postp , we use the 768 - dimensional representa-   tion of the [ CLS ] token obtained from BERT ( De-   vlin et al . , 2019 ) as e = BERT(p ) . As shown in   Figure 2 , we then pass each post embedding se-   quentially through a bi - directional LSTM , given   ash = Bi - LSTM(e ) . We thus obtain the se-   quence of hidden states , x= [ h;h;;h ] ,   whereh2R , andHis the hidden dimension .   To Ô¨Ålter out relevant signals from the potentially   vast user history ( Shing et al . , 2020 ) , we pass the   hidden state sequence through an attention layer .   The Ô¨Ånal layer is a multilayer perceptron ( MLP ) to   obtain the prediction vector ^y , given as :   ^y = f(x);where   f(x ) = Softmax(MLP(Attention ( x)))(1 )   2.4 Self - Aware Mechanism   To make the model self - aware , we transform the   model such that it makes a prediction only when   certain ( Liu et al . , 2019 ) . As shown in Figure 2 ,   the modelf : R!Yis augmented with a   selection function g : R!(0;1 ) , which is an   extra logit . The augmented model is described as a   piece - wise function , given by :   ( f;g)(x):= (   Refrain ; ifg   argmax ( ^y);otherwise(2 )   Where the threshold  2(0;1 ) , argmax ( ^y)2Y.   Letp= ( f;g)(x ) , wherep2Y[fRefraingde-   note the Ô¨Ånal prediction by the model for a user   u. Human moderators can then deÔ¨Åne the level   of granularity of these predictions , and sort them   into priority levels as desired . As an example , mod-   erators may choose to have only three levels of   priority , where the user is high priority if p2{AT ,   BR , Refrain } , moderate if p2{ID , IN } and low if   p2{SU } . With the addition of the Refrain option ,   uncertain predictions will have highest priority , al-   leviating the possibility of high - risk users being   neglected .   It is essential to note that the conÔ¨Ådence thresh-   old  is not utilized during training , rather as athreshold variable to calibrate data coverage ( cov )   during evaluation . The covfraction of total sam-   ples is what SASI predicts on , leaving out ( 1 cov )   samples for which SASI is most uncertain . SpeciÔ¨Å-   cally , we can choose some value  such that there   will be ( 1 cov)samples for which g  . The   idea behind this approach is to trade - off ( 1 cov )   samples for immediate review by mental health ex-   perts in exchange for higher model performance on   thecovsamples about which it is conÔ¨Ådent .   2.5 Network Optimization   In anym - class classiÔ¨Åcation problem , if the model   assigns a high probability score to the wrong class ,   then learning becomes difÔ¨Åcult due to vanishing   gradients ( Ziyin et al . , 2020 ) . To account for the   additional refrain option in the augmented label   space , we train SASI using Gambler ‚Äôs Loss ( Liu   et al . , 2019 ) . Gambler ‚Äôs loss allows the gradients   to propagate through ginstead , by abstaining from   assigning weights to any of the mclasses . Thus ,   the model learns a distribution of noisy / uncertain   data points characterized by the selection function   g. The loss function is given as :   L= Xylog(^yr+g ) ( 3 )   whereyis the true label , and the reward ris a   hyperparameter . A higher value of rdiscourages   restraint . Since the loss function directly learns   g , it does not depend on the coverage ( Liu et al . ,   2019 ) , and can be manually set to any value during   evaluation .   3 Experimental Setup   3.1 Dataset   We use the dataset released by Gaur et al .   ( 2019 ) , which contains Reddit posts of 500   users Ô¨Åltered from an initial set of 270,000   users across several mental health and suicide-   related subreddits , such as r / StopSelfHarm ( SSH ) ,   r / selfharm ( SLF ) , r / bipolar ( BPL ) , r / BipolarReddit   ( BPR ) , r / BipolarSOs , r / opiates ( OPT ) , r / Anxiety   ( ANX ) , r / addiction ( ADD ) , r / BPD , r / SuicideWatch   ( SW ) , r / schizophrenia ( SCZ ) , r / autism ( AUT ) ,   r / depression ( DPR ) , r / cripplingalcoholism ( CRP ) ,   and r / aspergers ( ASP ) . The posts were annotated   by practicing psychiatrists into Ô¨Åve increasing risk   levels based on the Columbia Suicide Severity Risk   Scale ( Posner et al . , 2011 ) , leading to an acceptable630average pairwise agreement of 0.79 and a group-   wise agreement of 0.73 . The class distribution of   each category with increasing risk level is : Sup-   portive ( 20 % ) , Indicator ( 20 % ) , Ideation ( 34 % ) ,   Behaviour ( 15 % ) , Attempt ( 9 % ) . On average , the   number of posts made by a user is 18.25 27.45   with a maximum of 292 posts . The average number   of tokens in each post is 73.4 97.7 .   3.2 Evaluation Metrics   We Ô¨Årst describe the evaluation metrics that mea-   sure how well the model performs on the covsam-   ples . Following Gaur et al . ( 2019 ) , we use graded   variants of F1 score , Precision , and Recall , where   we alter the formulation of False Negatives ( FN )   and False Positives ( FP ) . FN is modiÔ¨Åed as the ratio   of the number of times predicted severity of suicide   risk level ( k ) is less than the actual risk level ( k )   overNnumber of samples . FP is the ratio of the   number of times the predicted risk ( k ) is greater   than the actual risk ( k ) , given as :   FN = PI(k > k )   N   FP = PI(k > k )   N(4 )   LetPdenote the total number of test samples ,   P the sum of samples that have either   been correctly predicted or have been refrained ,   P the total number of refrained samples , and   Pthe number of incorrect predictions among the   refrained samples . We additionally introduce two   metrics , Robustness andFail - Safe Rejects , as :   Robustness = P   P   Fail - Safe Rejects = P   P(5 )   Robustness captures the fraction of samples which   are correctly classiÔ¨Åed or instead sent for immedi-   ate review . Fail - Safe Rejects captures the fraction   of refrained samples which were indeed erroneous .   A higher Fail - Safe Rejects score hence implies that   human moderators will be subjected to a lesser   amounts of redundant work .   4 Results   4.1 Performance Comparison   We compare the performance of SASI with vari-   ous state - of - the - art baselines in Table 1 . Sequen-   tial models like Suicide Detection Model ( SDM )   ( Cao et al . , 2019 ) and ContextBERT ( Matero et al . ,   2019 ) generally outperform ContextualCNN ( Gaur   et al . , 2019 ) , which uses a bag - of - posts approach .   SISMO ( Sawhney et al . , 2021a ) shows further im-   provements by modeling the ordinal nature of risk   labels . SASI signiÔ¨Åcantly outperforms ( p<0:005 )   these methods for various values of coverage ( cov ) ,   demonstrating its ability to avoid committing to   erroneous predictions by characterizing its conÔ¨Å-   dence ( Liu et al . , 2019 ) .   4.2 Coverage and Performance Trade - off   We further evaluate SASI for various values of tar-   get coverage ( cov ) by calibrating the threshold  .   As shown in Figure 3 , lower coverage leads to an   increase in Graded Recall , Precision , and FScore   ( Table 1 ) , as the model only keeps covpredictions   which it is highly certain about . However , we ob-   serve a decrease in Fail - Safe Rejects due to an   increasingly cautious approach employed by the   model , which implies an increased fraction of orig-   inally correct predictions that need to be manually   reviewed . We hence observe a trade - off , wherein   we must seek to achieve competitive performance   on thecovsamples , while at the same time not over-   burden moderators with the ( 1 cov)samples . For   lower coverage values ( say 50 % ) , human modera-631   tors may be overburdened by having to review a lot   of redundant samples . On the other hand , we note   that SASI ( 85 % ) provides more utility , as it sta-   tistically outperforms SOTA models like SISMO ,   while maintaining a fail - safe rejection score of 83 %   and a competitive robustness score of 61 % .   4.3 Qualitative Analysis   The essence of SASI lies behind its ability to refrain   from making misleading predictions over high - risk   samples . We study Ô¨Åve users with snippets of their   posts , as shown in Figure 4 . We observe the model   makes erroneous predictions on high - risk users A   and D. However , SASI refrains from committing   to these predictions , assigning these users a high   priority for immediate review and response . SASI   chooses to refrain despite predicting the risk level   of user B correctly , possibly because it employs a   cautious approach due to phrases such as ‚Äò take my   life ‚Äô scattered in the user ‚Äôs timeline . This user , who   is already of relatively high risk , is hence assigned   a high priority . User E shows a very low sign of   risk , which is conÔ¨Ådently captured by SASI with-   out needing to refrain . User C is an erroneous case   wherein SASI is conÔ¨Ådent , yet makes a wrong pre-   diction . However , the user is not high risk and gets   assigned to the same priority level as the true risk   label . While this example is not a cause for con-   cern , certain situations may arise where SASI also   conÔ¨Ådently assigns a low - risk score to a high - risk   user , opening avenues for future work that involves   integrating and reformulating ordinal regressionover the principles of Gambler ‚Äôs loss .   5 Conclusion   With a motivation to provide a robust solution to   Ô¨Åne - grained suicide risk assessment on social me-   dia , we present SASI , a framework that integrates   the concept of selective prioritization to existing   deep learning based risk - assessment techniques .   SASI is self - aware , wherein it refrains from making   a prediction when uncertain , and instead assigns   high priority to such data samples for immediate   review by mental health experts . We demonstrated   the effectiveness of SASI through quantitative eval-   uations on real - world data , wherein SASI avoided   high - risk situations by refraining from making 83 %   of incorrect predictions . Through a qualitative anal-   ysis , we described how SASI can be used as a part   of a human - in - the - loop framework , facilitating efÔ¨Å-   cient responses from mental health experts .   Acknowledgements   We thank Prof. Amit Sheth for reviewing the paper   and providing valuable feedback and support . We   would also like to thank the anonymous reviewers   for their insightful suggestions on various aspects   of this work .   Ethical Considerations   We work within the scope of acceptable privacy   practices suggested by Chancellor et al . ( 2019 )   and considerations presented by Fiesler and Pro-   feres ( 2018 ) to avoid coercion and intrusive treat-632ment . The primary source of the dataset used in   this study is Reddit . Although Reddit is intended   for anonymous posting , we take further precau-   tions by performing automatic de - identiÔ¨Åcation of   the dataset using named entity recognition ( Zirikly   et al . , 2019 ) . All examples used in this paper are   further been anonymized , obfuscated , and para-   phrased for user privacy ( Benton et al . , 2017 ) and   to prevent misuse as per the moderate disguise   scheme suggested by Bruckman ( 2002 ) . Taking   inspiration from Benton et al . ( 2017 ) , we also   keep the annotation of user data separate from raw   user data on protected servers linked only through   anonymous IDs . Our work focuses on building   an assistive tool for screening suicidal users and   providing judgments purely based on observational   capacity . We acknowledge that it is almost impos-   sible to prevent abuse of released technology even   when developed with good intentions ( Hovy and   Spruit , 2016 ) . Hence , we ensure that this analysis   is shared only selectively to avoid misuse such as   Samaritan ‚Äôs Radar ( Hsin et al . , 2016 ) .   We further acknowledge that the studied data   may be susceptible to demographic , expert annota-   tor , and medium - speciÔ¨Åc biases ( Hovy and Spruit ,   2016 ) . While the essence of our work is to aid in the   early detection of at - risk users and early interven-   tion , any interventions must be well - thought , fail-   ing which may lead to counter - helpful outcomes ,   such as users moving to fringe platforms , making   it harder to provide assistance ( Kumar et al . , 2015 ) .   Care should be taken to not to create stigma , and   interventions must hence be carefully planned by   consulting relevant stakeholders , such as clinicians ,   designers , and researchers ( Chancellor et al . , 2016 ) ,   to maintain social media as a safe space for indi-   viduals looking to express themselves ( Chancellor   et al . , 2019 ) . It is also essential that clinicians and   human moderators are not overburdened ( Chancel-   lor et al . , 2019 ) . For instance , ‚Äú Alarm fatigue ‚Äù is   when alarms are so excessive , many of which are   false positives , that healthcare providers become   desensitized from alarms ( Drew et al . , 2014 ) .   We also agree that suicidality is subjective ( Keilp   et al . , 2012 ) , wherein the interpretation may vary   across individuals on social media ( Puschman ,   2017 ) . We do not make any diagnostic claims ,   rather help prioritize the users that should be evalu-   ated by the medical professionals Ô¨Årst , as part of a   distributed human - in - the - loop framework ( de An-   drade et al . , 2018).References633634635   Isabel Papadimitriou   Stanford University   isabelvp@stanford.eduRichard Futrell   University of California , Irvine   rfutrell@uci.edu   Kyle Mahowald   The University of Texas at Austin   mahowald@utexas.edu   Abstract   1 Introduction and Prior Work   Large language models create contextual embed-   dings of the words in their input , starting with a   static embedding of each token and progressively   adding more contextual information in each layer   ( Devlin et al . , 2019 ; Brown et al . , 2020 ; Man-   ning et al . , 2020 ) . While these contextual em-   bedding models are often praised for capturing   rich grammatical structure , a spate of recent work   has shown that they are surprisingly invariant to   scrambling word order ( Sinha et al . , 2021 ; Hes-   sel and SchoÔ¨Åeld , 2021 ; Pham et al . , 2021 ; Gupta   et al . , 2021 ; O‚ÄôConnor and Andreas , 2021 ) and   that grammatical knowledge like part of speech ,   often attributed to contextual embeddings , is actu-   ally also captured by Ô¨Åxed embeddings ( Pimentel   et al . , 2020 ) . These results point to a puzzle : how   can syntactic contextual information be important   for language understanding when the words them-   selves , not their order , are what matter ?   We argue that this apparent paradox arises be-   cause of the redundant structure of language it-   self . Lexical distributional information alone inher-   ently captures a great deal of meaning ( Erk,2012 ;   Mitchell and Lapata , 2010 ; Tal and Arnon , 2022 ) ,   and typically both humans and machines can re-   construct meanings of sentences under local scram-   bling of words ( Mollica et al . , 2020 ; Clouatre et al . ,   2021 ) . In this paper , we study model behaviour in   cases where word order is informative and is not   redundant with lexical information .   We focus on the feature of grammatical role636(whether a noun is the subject or the object of a   clause ) . Most natural clauses are prototypical :   in a sentence like ‚Äú the chef chopped the onion ‚Äù ,   the grammatical roles of chef andonion are clear   to humans from the words alone , without word   order or context ( see Mahowald et al . , 2022 , for   experiments in English and Russian in which hu-   man participants successfully guessed which of   two nouns was the subject and which was the ob-   ject of a simple transitive clause , in the absence   of word order and contextual information ) . This   means syntactic word order is often redundant with   lexical semantics . Whether hand - constructed or   corpus - based , most studies probing contextual rep-   resentations have used prototypical sentences as   input , where syntactic word order may not have   much information to contribute to core meaning   beyond the words themselves .   Yet human language can use syntax to deviate   from the expectations generated by lexical items :   we can also understand the absurd meaning of a   rare non - prototypical sentence like ‚Äú The onion   chopped the chef ‚Äù ( Garrett , 1976 ; Gibson et al . ,   2013 ) . Is this use of syntactic word order available   to pretrained models ? In this paper , we train gram-   matical role probes on the embedding spaces of   BERT and GPT-2 , and evaluate them on these rare   non - prototypical examples , where the meaning of   words in context is different from what we would   expect from looking at the words alone . We focus   on English because grammatical role is directly de-   pendent on word order in English , and because we   had access to sufÔ¨Åciently large English parsed cor-   pora such that we could generate non - prototypical   sentences , easily check them , and Ô¨Ålter to grammat-   ical ones .   We probe for grammatical role because it is key   to the basic compositional semantic structure of a   sentence ( Dixon , 1979 ; Comrie , 1989 ; Croft , 2001 ) .   While Ô¨Åxed lexical semantics contains information   about grammatical role ( animate nouns are likely to   be subjects , etc ) , the grammatical role of a word in   English is ultimately determined by syntactic word   order . Probing grammatical role lets us examine   the interplay between syntactic word order and lex-   ical semantics in forming compositional meaning   through model layers .   For all of our experiments , we train grammatical   role probes with standard data and test them oneither prototypical cases or non - prototypical cases   ( where word order matters ) , to understand if gram-   matical embedding under normal circumstances is   sensitive to word order . Our experiments reveal   three key Ô¨Åndings :   1.Lexical semantics plays a key role in orga-   nizing embedding space in early layer rep-   resentations , and non - lexical compositional   features are expressed gradually in later lay-   ers , as shown by probe performance on non-   prototypical sentences ( Experiment 1 , Figure   1 ) .   2.Embeddings represent meaning that is im-   parted only by syntactic word order , overrid-   ing lexical and distributional cues . When we   control for distributional co - occurrence fac-   tors by evaluating our probes on argument   swapped sentences ( like ‚Äú The onion chopped   the chef ‚Äù , real sample in Appendix B ) , probes   can differentiate the same word in different   roles ( Experiment 2 , Figure 2 ) .   3.Syntactic word order is signiÔ¨Åcant beyond just   local coherence : the compositional informa-   tion of syntactic word order is lost when we   test our probes on locally - shufÔ¨Çed sentences ,   that keep local lexical coherence but break   acute syntactic relations ( Figure 3 ) .   More generally , we highlight the importance of   examining models using non - prototypical exam-   ples , both for understanding the strength of lexical   inÔ¨Çuence in contextual embeddings , but also for   accurately isolating syntactic processing where it   is taking place .   2 Why non - prototypical probing ?   As opposed to more general syntactic probing tasks   ( e.g. , dependency parsing ) , grammatical role is a   linguistically signiÔ¨Åcant yet speciÔ¨Åc task that is   both syntactic and semantic . As such , we can   choose these linguistically - informed sets of non-   prototypical examples where the lexical semantics   does not match the compositional meaning implied   by the syntax .   Non - prototypical examples give us a unique per-   spective on how syntactic machinery like word or-   der inÔ¨Çuences compositional meaning representa-   tionindependently from lexical semantics . Stud-637ies in probing have controlled for lexical seman-   tics by substituting content words for nonce words   ( ‚Äú jabberwocky ‚Äù sentences , as in Hall Maudslay   and Cotterell , 2021 ; Goodwin et al . , 2020 ) or ran-   dom real words ( ‚Äú colorless green idea ‚Äù sentences ,   as in Gulordava et al . , 2018 ) . A tradeoff is that   these methods lead to out - of - distribution sentences   whose words are unlikely to ever co - occur naturally .   Rather than bleaching the effect of lexical seman-   tics , our setup lets us examine the interplay between   lexical semantics and syntactic representation in   a controlled environment , isolating the effects of   syntactic word order while using in - distribution   examples .   Recent work on representation probing has fo-   cused on improving probing methodologies to   make sure that extracted information is not spurious   or not simply lexical ( Hewitt and Liang , 2019 ; Be-   linkov , 2022 ; V oita and Titov , 2020 ; Hewitt et al . ,   2021 ; Pimentel et al . , 2020 ) . Our experiments are   a complementary approach , where we use standard   probing methods , but use linguistically - informed   data selection to address the ambiguity of what   classiÔ¨Åers are extracting .   3 Experiment 1 : Grammatical   Subjecthood Probes   In Experiment 1 , we evaluate grammatical role   probes on prototypical instances , where grammat-   ical role lines up with lexical expectations , and   non - prototypical instances , where it does not .   3.1 Methods   We train a 2 - level perceptron classiÔ¨Åer probe with   64 hidden units to distinguish the layer embed-   dings of nouns that are transitive subjects from   nouns that are transitive objects , as in Papadim-   itriou et al . ( 2021 ) . We train a separate classiÔ¨Åer   for each model layer , as well as training a classiÔ¨Åer   on the static word embedding space of the mod-   els without the position embeddings added ( before   layer 0 ) . The probe classiÔ¨Åers are binary , taking the   layer embedding of a noun and predicting whether   it is a transitive subject or a transitive object . Probe   training data comes from Universal Dependencies   treebanks : we pass single sentences from the tree-   banks through the models , and use dependency an-   notations to label each layer embedding for whether   it represents a transitive subject , a transitive object ,   or neither ( not included in training ) . The training   set is balanced , and consists of 864 embeddingsof subject nouns , and 864 embeddings of object   nouns . We train all probes for 20 epochs , for con-   sistency . The embedding models that we use are   bert - base - uncased andgpt2 . For our analysis ,   we call a noun a prototypical subject if the probe   probability for its word embedding ( pre - layer 0 ) is   greater than 0.5 , and a prototypical object if it is   less .   3.2 Results   Prototypical and non - prototypical arguments differ   in probing behavior across layers , as demonstrated   in Figure 1 . For prototypical instances ( solid lines ) ,   syntactic information is conÔ¨Çated with type - level   information and so probe accuracy is high starting   from layer 0 ( word embeddings + position embed-   dings ) , and stays consistent throughout the network .   However , when we look at non - prototypical in-   stances ( dashed lines ) , we see that the embeddings   from layer to layer have very different grammatical   encodings , with type - level semantics dominating in   the early layers and more general syntactic knowl-   edge only becoming extractable by our probes in   later layers .   Crucially , since prototypical examples dominate   in frequency in any corpus , the average probe accu-   racy across all examples is high for all layers , and   the grammatical encoding of subjecthood , which is   accurate only after the middle layers of the model ,   would be hidden . Separating out non - prototypical   examples illustrates how the syntax of a phrase can   arise independently from type - level information   through transformer layers , while also showcas-   ing the importance of lexical semantics in forming   embedding space geometry in the Ô¨Årst half of the   network .   4 Experiment 2 : Controlling for   Distributional Information by   Swapping Subjects and Objects   In Experiment 1 we show that the contextualiza-   tion process consists of gradual grammatical infor-   mation gain for non - prototypical examples , even   though this is largely obscured in the majority pro-   totypical examples where the lexical semantics also   contains accurate syntactic information . In this ex-   periment , we ask : does this contextualized informa-   tion about grammatical role stem from word order   and syntax , or from distributional ( bag - of - words )   effects when seeing all words in the sentence ? We   answer this question by creating example pairs638   where we control for distributional information by   keeping all the words the same , but swapping the   positions of the subject and the object . Such pairs   of the type ‚Äú The chef chopped the onion ‚Äù ‚Üí‚ÄúThe   onion chopped the chef ‚Äù ( real sample in Appendix   B ) have identical distributional information . To   accurately classify grammatical role in both sen-   tences , the model we ‚Äôre probing would have to be   attuned to the ways in which small changes in word   order globally affect meaning .   4.1 Methods   We use the same probing classiÔ¨Åers from Experi-   ment 1 , and evaluate on a special test set of pairs   of sentences that have the subject and direct ob-   ject of one clause swapped . To create the swapped   sentences , we search the UD treebank for verbs   that have lexical , non - pronoun direct subjects and   direct objects , check that the subject and object   have the same number ( singular or plural ) , and also   check that neither of them are part of a compound   word or a Ô¨Çat dependency word that would be sep-   arated ( like a full name ) . If a sentence contains a   verb where its arguments fulÔ¨Åll all of these require-   ments , we swap the position of the subject and the   object to create a second , swapped sentence , and   add the sentence pair ( original and swapped ) to our   evaluation set . A random sample of our swapped   sentences is in Appendix B.4.2 Results   When testing our probes on pairs of normal and   swapped sentences , we Ô¨Ånd that our probes from   Experiment 1 correctly classify both the normal   and the swapped sentences with high accuracy in   higher layers . Since we test our probes on con-   trolled pairs that have the same distributional in-   formation , we can isolate effect of syntactic word   order in inÔ¨Çuencing meaning representation . This   is demonstrated in Figure 2 , where probe predic-   tions for the same set of words in the same distribu-   tional context diverges signiÔ¨Åcantly depending on   whether the word is in subject or object position .   Our results indicate that , separate from distribu-   tional effects , models have learned to represent the   ways in which syntactic word order can indepen-   dently affect meaning .   4.3 Are these results just due to general   position information ?   Our results in Experiment 2 indicate that syntac-   tic word order information can affect model repre-   sentations of word meaning , even when we keep   lexical and distributional information constant . A   question still remains : does the divergence demon-   strated in Figure 2stem from the Ô¨Åne - grained ways   in which word order inÔ¨Çuences syntax in English ,   or from heuristics based on primacy ( whether a   word is earlier or later in a sentence ) ? To further   investigate this , we train and test probes on sen-   tences where word order is locally scrambled so   that no word moves more than 2 slots , and so gen-   eral primacy and local coherence is preserved . As   shown in Figure 3 , probes trained on these locally   shufÔ¨Çed sentences do not fare better than chance639   on non - prototypical examples . While prototypi-   cal lexical information can aid classiÔ¨Åcation ( solid   line ) , general primacy information is not sufÔ¨Åcient   to overcome lexical cues and cause the word - order-   dependent representation we demonstrate in Figure   2 .   5 Discussion   While recent work has shown that large language   models come to rely largely on distributional se-   mantic information , we consider the model ‚Äôs abil-   ity to overcome these distributional cues . Research   showing that models rely on lexical and distribu-   tional information is not at odds with our Ô¨Åndings   that this can be overridden . In fact , even though hu-   mans can accurately understand non - prototypical   sentences , human syntactic processing is often in-   Ô¨Çuenced by the lexical semantics of words , as evi-   denced by studies on human subjects ( Frazier and   Rayner , 1982 ; Rayner et al . , 1983 ; Ferreira and   Henderson , 1990 ) as well as by lexically - inÔ¨Çuenced   syntactic processes in human languages , like dif-   ferential object marking ( Aissen , 2003 ) ‚Äî a phe-   nomenon whereby non - prototypical grammatical   objects are marked .   More generally , while we have shown that it is   tempting for a straightforward probing approach   to conclude that grammatical role information is   available to the lowest layers of BERT , separately   analyzing prototypical and non - prototypical argu-   ments makes it clear that the picture is more compli - cated . At lower layers , BERT representations can   typically classify subjects and objects , but when   a non - prototypical meaning is expressed , accurate   classiÔ¨Åcation is not available until the higher layers .   We argue that considering probing performance   on these non - prototypical instances is crucial . A   key design feature of human language is the abil-   ity to talk about things that are n‚Äôt there or do n‚Äôt   exist ( Hockett , 1960 ) , and it has been argued that   the combinatoric power of syntax exists to allow   humans to say things that are subtle , surprising , or   impossible ( Garrett , 1976 ; Chomsky , 1957 ) . Thus ,   considering probing accuracy on the average task   may be misleading . Insofar as being able to un-   derstand non - prototypical meanings is a hallmark   of human language and insofar as these meanings   may differ in systematic ways from prototypical   meanings , considering such cases is crucial for   understanding how language models represent lan-   guage .   6 Acknowledgments   This work was supported by National Science Foun-   dation Grants No . 2104995 to KM , No . 1947307 to   RF , and a Graduate Research Fellowship to IP . We   thank Dan Jurafsky and Adina Williams for helpful   discussions , and Kaitlyn Zhou , Dallas Card , and J.   Adolfo Hermosillo for comments on drafts .   References640641   A Figures for GPT-2 Experiments   We ran our experiments on both BERT and GPT-2   embeddings , and both models had similar behav-   iors that we discuss in the paper . For clarity , Ô¨Ågures   in the paper only visualize the BERT results , and   we ‚Äôre including the GPT-2 versions of those same   Ô¨Ågures for comparison . Figure 4shows the GPT-2   results of Figure 1 , Figure 5shows the GPT-2 re-   sults of Figure 2 , and Figure 6shows the GPT-2   result of Figure 3 .   .   .   .642BSample of argument - swapped sentences   A random sample ( not cherry - picked ) of our   argument - swapped evaluation set , where the sub-   ject and the object of clauses are automatically   swapped . The original subject is in bold and the   original object is in bold and italics . The process   for creating these sentences is detailed in Section   4.1   On Thursday , with 110 days until the start of the   2014 Winter Paralympics in Sochi , Russia , Profes-   sorinterviewed Assistant Wikinews in Educational   Leadership , Sport Studies and Educational / Coun-   seling Psychology at Washington State University   Simon Li Àácen about attitudes in United States to-   wards the Paralympics .   This approach shows a more realistic video to   playing Quidditch .   Second , aggregate view provides only a high-   level information of a Ô¨Åeld , which can make it   difÔ¨Åcult to investigate causality [ 23 ] .   Ahand raises her girl .   area of the Mississippi River and the destruction   of wetlands at its mouth have left the Alteration   around New Orleans abnormally vulnerable to the   forces of nature .   It was known that a moving energy exchanges   its kinetic body for potential energy when it gains   height .   Thus , when ACPeds issued a statement con-   demning gender reassignment surgery in 2016 [ 21 ] ,   many beliefs mistook the organization ‚Äôs political   people for the consensus view among United States   pediatricians ‚Äî although the peak body for pedi-   atric workers , the American Academy of Pediatrics ,   has a much more positive view of gender dysphoria   [ 22 ] .   Hispainting perfectly combines artand Chinese   calligraphy .   When the inches become a few plants tall and   their leaves mature , it ‚Äôs time to transplant them to   a larger container .   Since the television series ‚Äô inception , reviews at   The A V Club have written two critical writers for   each episode:643   Meng Zhang , Liangyou Li , Qun Liu   Huawei Noah ‚Äôs Ark Lab   { zhangmeng92 , liliangyou , qun.liu}@huawei.com   Abstract   1 Introduction   Machine translation ( MT ) has achieved promising   performance when large - scale parallel data is avail-   able . Unfortunately , the abundance of parallel data   is largely limited to English , which leads to con-   cerns on the unfair deployment of machine transla-   tion service across languages . In turn , researchers   are increasingly interested in non - English - centric   machine translation approaches ( Fan et al . , 2021 ) .   Triangular MT ( Kim et al . , 2019 ; Ji et al . , 2020 )   has the potential to alleviate some data scarcity   conditions when the source and target languages   both have a good amount of parallel data with a   pivot language ( usually English ) . Kim et al . ( 2019 )   have shown that transfer learning is an effective   approach to triangular MT , surpassing generic ap-   proaches like multilingual MT .   However , previous works have not fully ex-   ploited all types of auxiliary data ( Table 1 ) . For   example , it is reasonable to assume that the source ,   target , and pivot language all have much monolin-   gual data because of the notable size of parallel   data between source - pivot and pivot - target .   In this work , we propose a transfer - learning-   based approach that exploits all types of auxiliary   data . During the training of auxiliary models on   auxiliary data , we design parameter freezing mech-   anisms that encourage the models to compute the   representations in the same pivot language space ,   so that combining parts of auxiliary models gives   a reasonable starting point for Ô¨Ånetuning on the   source - target data . We verify the effectiveness of   our approach with a series of experiments .   2 Approach   We Ô¨Årst present a preliminary approach that is a   simple implementation of our basic idea , for ease   of understanding . We then present an enhanced ver-   sion that achieves better performance . For notation   purpose , we use X , Y , and Zto represent source ,   target , and pivot language , respectively .   2.1 Simple Triangular Transfer   We show the illustration of the preliminary ap-   proach in Figure 1 , called simple triangular trans-   fer . In Step ( 1 ) , we prepare a pre - trained language   model ( PLM ) with the pivot language monolingual   data . We consider this PLM to deÔ¨Åne a representa-   tion space for the pivot language , and we would like   subsequent models to stick to this representation644   space . In order to achieve this , we freeze certain   parameters in Step ( 2 ) as we train source - pivot and   pivot - target translation models , which are partly   initialized by the PLM . For example , the pivot-   target translation model has the pivot language on   the source side , so the encoder is initialized by the   PLM , and some ( or all ) of its parameters are frozen .   This ensures that the encoder produces representa-   tions in the pivot language space , and the decoder   has to perform translation in this space . Likewise ,   the encoder in the source - pivot translation model   needs to learn to produce representations in the   same space . Therefore , when the pivot - target de-   coder combines with the source - pivot encoder in   Step ( 3 ) , they could cooperate more easily in the   space deÔ¨Åned in Step ( 1 ) .   We experimented with RoBERTa ( Liu et al . ,   2019 ) and BART ( Lewis et al . , 2020 ) as the PLMs .   We found that simple triangular transfer attains   about 0.8 higher BLEU by using BART instead of   RoBERTa . In contrast , we found that dual trans-   fer ( Zhang et al . , 2021 ) , one of our baselines , per-   forms similarly with BART and RoBERTa . When   used to initialize decoder parameters , RoBERTa   has to leave the cross attention parameters ran-   domly initialized , which may explain the superior-   ity of BART for our approach , while dual transfer   does not involve initializing decoder parameters .   Therefore , we choose BART as our default PLM .   2.2 Triangular Transfer   A limitation of simple triangular transfer is that it   does not utilize monolingual data of the source andtarget languages . A naive way is to prepare source   and target PLMs and use them to initialize source-   pivot encoder and pivot - target decoder , respectively .   However , this leads to marginal improvement for   the Ô¨Ånal source - target translation performance ( Sec-   tion 3.5 ) . This is likely because the source , target ,   and pivot PLMs are trained independently , so their   representation spaces are isolated .   Therefore , we intend to train source and target   PLMs in the pivot language space as well . To this   end , we design another initialization and freezing   step inspired by Zhang et al . ( 2021 ) , as shown in   Figure 2 . In this illustration , we use BART as the   PLM . Step ( 2 ) is the added step of preparing BART   models in the source and target languages . As the   BART body parameters are inherited from the pivot   language BART and frozen , the source and target   language BART embeddings are trained to lie in   the pivot language space . Then in Step ( 3 ) , every   part of the translation models can be initialized   in the pivot language space . Again , we freeze pa-   rameters in the pivot language side to ensure the   representations do not drift too much .   2.3 Freezing Strategy   There are various choices when we freeze parame-   ters in the pivot language side of the source - pivot   and pivot - target translation models . Take the en-   coder of the pivot - target translation model as the   example . In one extreme , we can freeze the em-   beddings only ; this is good for the optimization of   pivot - target translation , but may result in a space   that is far away from the pivot language space given   by the pivot PLM . In the other extreme , we can   freeze the entire encoder , which clearly hurts the   pivot - target translation performance . This is hence   a trade - off . We experiment with multiple freezing   strategies between the two extremes , i.e. , freezing a   given number of layers . We always ensure that the   number of frozen layers is the same for the decoder   of the source - pivot translation model .   Besides layer - wise freezing , we also try   component - wise freezing inspired by Li et al .   ( 2021 ) . In their study , they found that some com-   ponents like layer normalization and decoder cross   attention are necessary to Ô¨Ånetune , while others   can be frozen . In particular , we experiment with   three strategies based on their Ô¨Åndings of the most   effective ones in their task . These strategies apply   to Step ( 3 ) of triangular transfer.645   language code # sentence ( pair )   En - De 3.1 m   Fr - En 29.5 m   Fr - De 247k   Zh - En 11.9 m   Zh - De 189k   En 93.9 m   De 100.0 m   Fr 44.6 m   Zh 20.0 m   LNA - E , D All layer normalization , encoder self   attention , decoder cross attention can be Ô¨Ånetuned .   Others are frozen .   LNA - D All encoder parameters , decoder layer   normalization and cross attention can be Ô¨Ånetuned .   LNA - e , D Use LNA - D when training the source-   pivot model . When training the pivot - target model ,   freeze encoder embeddings in addition to LNA - D.   3 Experiments   3.1 Setup   We conduct experiments on French ( Fr ) ! German   ( De ) and Chinese ( Zh ) ! German ( De ) translation ,   with English ( En ) as the pivot language . Training   data statistics is shown in Table 2 . The evaluationmetric is computed by SacreBLEU(Post , 2018 ) .   All approaches use Transformer base ( Vaswani   et al . , 2017 ) as the translation model , but note   that pivot translation needs two translation mod-   els for decoding , equivalently doubling the number   of parameters . Further details can be found in the   appendix .   3.2 Baselines   We compare with several baselines as follows .   No transfer This baseline directly trains on the   source - target parallel data .   Pivot translation Two - pass decoding by source-   pivot and pivot - target translation .   Step - wise pre - training This is one of the ap-   proaches in ( Kim et al . , 2019 ) . It is simple and   robust , and has been shown to outperform multilin-   gual MT . It trains a source - pivot translation model   and uses the encoder to initialize the encoder of   a pivot - target translation model . In order to make   this possible , these two encoders need to use a   shared source - pivot vocabulary . Then the pivot-   target translation model is trained while keeping its   encoder frozen . Finally the model is Ô¨Ånetuned on   source - target parallel data .   Shared target dual transfer Dual transfer   ( Zhang et al . , 2021 ) is a general transfer learning ap-   proach to low - resource machine translation . When646approach BLEU   no transfer 13.49   pivot translation through no transfer 18.99   step - wise pre - training 18.49   shared target transfer 18.88   shared source transfer 18.89   triangular transfer 19.91   approach BLEU   no transfer 11.39   pivot translation through no transfer 12.91   triangular transfer 16.03   applied to triangular MT , it can not utilize both   source - pivot and pivot - target parallel data . Shared   target dual transfer uses pivot - target auxiliary trans-   lation model and does not exploit source - pivot par-   allel data .   Shared source dual transfer The shared source   version uses source - pivot translation model for   transfer and does not exploit pivot - target parallel   data .   3.3 Main Results   We present the performance of our approach and   the baselines on Fr!De in Table 3 . The no transfer   baseline performs poorly because it is trained on a   small amount of parallel data . The other baselines   perform much better . Among them , pivot transla-   tion attains the best performance in terms of BLEU ,   at the cost of doubled latency . Our approach can   outperform all the baselines .   Taking pivot translation as the best baseline , we   further evaluate our approach on Zh ! De . Results   in Table 4 show that the performance improvement   of our approach is larger for this translation direc-   tion .   3.4 The Effect of Freezing Strategies   From Table 5 , we can observe the effect of dif-   ferent freezing strategies . For layer - wise freezing ,   we see a roughly monotonic trend of the Fr - En   and En - De performance with respect to the num-   ber of frozen layers : The more frozen layers , thestrategy Fr - En En - De Fr - De   L=0 31.42 20.95 19.62   L=1 31.41 20.98 19.76   L=2 31.55 20.56 19.71   L=3 31.06 20.54 19.91   L=4 30.92 20.22 19.68   L=5 30.39 19.95 19.21   L=6 30.31 19.11 19.02   LNA - E , D 28.72 17.92 17.97   LNA - D 31.08 20.23 18.75   LNA - e , D 31.08 19.97 18.25   approach BLEU   pivot translation through no transfer 18.99   pivot translation through 2 19.06   shared target transfer 18.88   shared target transfer + naive mono . 18.93   shared source transfer 18.89   shared source transfer + naive mono . 18.97   simple triang . transfer 18.96   simple triang . transfer + naive mono . 19.00   triangular transfer 19.62   lower their BLEU scores . However , the best Fr - De   performance is achieved with L=3 . This indi-   cates the trade - off between the auxiliary models ‚Äô   performance and the pivot space anchoring . For   component - wise freezing , the Fr - En and En - De   performance follows a similar trend , but the Fr - De   performance that we ultimately care about is not as   good .   3.5 Using Monolingual Data   Table 6 shows the effect of different ways of us-   ing monolingual data . The naive way is to prepare   PLMs with monolingual data and initialize the en-   coder or decoder where needed . For pivot trans-   lation , this is known as 2 ( Rothe et al . ,   2020 ) for the source - pivot and pivot - target transla-   tion models . For dual transfer , parts of the auxiliary   models can be initialized by PLMs ( e.g. , for shared   target transfer , the pivot - target decoder is initial-647approach BLEU   no transfer 18.74   shared target transfer 20.53   shared source transfer 20.73   triangular transfer 20.84   ized ) . For Step ( 2 ) in simple triangular transfer ,   we can also initialize the pivot - target decoder and   source - pivot encoder with PLMs . However , none   of the above methods shows clear improvement .   This is likely because these methods only help the   auxiliary translation models to train , which is not   necessary as they can be trained well with abun-   dant parallel data already . In contrast , our design   of Step ( 2 ) in triangular transfer additionally helps   the auxiliary translation models to stay in the pivot   language space .   3.6 Pivot - Based Back - Translation   Following Kim et al . ( 2019 ) , we generate syn-   thetic parallel Fr - De data with pivot - based back-   translation ( Bertoldi et al . , 2008 ) . SpeciÔ¨Åcally , we   use a no transfer En ! Fr model to translate the   English side of En - De data into French , and the au-   thentic Fr - De data are oversampled to make the ra-   tio of authentic and synthetic data to be 1:2 . Results   in Table 7 show that triangular transfer and dual   transfer clearly outperform the no transfer baseline .   4 Conclusion   In this work , we propose a transfer - learning - based   approach that utilizes all types of auxiliary data ,   including both source - pivot and pivot - target paral-   lel data , as well as involved monolingual data . We   investigate different freezing strategies for train-   ing the auxiliary models to improve source - target   translation , and achieve better performance than   previous approaches .   References648   A Data and Preprocessing   We gather data from WMT and ParaCrawl , shown   in Tables 8 and 9 .   We use jiebafor Chinese word segmentation ,   and Mosesscripts for punctuation normalization   and tokenization of other languages . The corpora   are deduplicated . Each language is encoded with   byte pair encoding ( BPE ) ( Sennrich et al . , 2016 )   with 32k merge operations . The BPE codes and   vocabularies are learned on each language ‚Äôs mono-   lingual data , and then used to segment parallel data .   Sentences with more than 128 subwords are re-   moved . Parallel sentences are cleaned with length   ratio 1.5 ( length counted by subwords ) .   B Hyperparameters   Our implementation is based on fairseq ( Ott   et al . , 2019 ) . We share decoder input and output   embeddings ( Press and Wolf , 2017 ) . The optimizeris Adam . Dropout and label smoothing are both   set to 0.1 . The batch size is 6,144 per GPU and   we train on 8 GPUs . The peak learning rate is   510for the no transfer baseline and auxiliary   models , 110for the Fr!De model of step-   wise pre - training and dual transfer , and 710   for the last step of triangular transfer . The learning   rate warms up for 4,000 steps , and then follows   inverse square root decay . Early stopping happens   when the development BLEU does not improve for   10 epochs .   RoBERTa and BART models use exactly the   same architecture as Transformer base . The mask   ratio is 15 % . The batch size is 256 sentences per   GPU , and each sentence contains up to 128 tokens .   The learning rate warms up for 10,000 steps to the   peak 510 , and then follows polynomial decay .   They are trained for 125k steps .   We use beam size of 5 for decoding , includ-   ing for pivot translation and pivot - based back-   translation.649lang . source train dev test   En - De WMT 2019Europarl v9 , News Commentary v14,newstest2011 newstest2012Document - split Rapid corpus   Fr - En WMT 2015Europarl v7 , News Commentary v10,newstest2011 newstest2012UN corpus , 10French - English corpus   Fr - De WMT 2019 News Commentary v14 , newstest2008 - 2010 newstest2011 newstest2012   Zh - En ParaCrawl ParaCrawl v9 newsdev2017 newstest2017   Zh - De WMT 2021 News Commentary v16 - dev - test 3k split 3k split   lang . source name   En WMT 2018 News Crawl 2014 - 2017   De WMT 2021 100 m subset from WMT 2021   Fr WMT 2015Europarl v7 , News Commentary v10 ,   News Crawl 2007 - 2014 , News Discussions   Zh WMT 2021 News Crawl , Zh side of parallel data650   Brielen Madureira David Schlangen   Computational Linguistics   Department of Linguistics   University of Potsdam , Germany   { madureiralasota , david.schlangen}@uni-potsdam.de   Abstract   1 Introduction   ‚Äú There ‚Äôs a cute dog outside ! ‚Äù you say on the phone   to your friend . ‚Äú Sweet . What colour is the dog ? ‚Äù ,   they say . ‚Äú What dog ? ‚Äù you reply ‚Äì and your friend   is rightfully confused . With your first utterance ,   you have committed yourself to there being a dog ;   a commitment you ca n‚Äôt just simply ignore later on .   Models of dialogue from linguistics and psycholin-   guistics take this process of grounding orscorekeep-   ing ‚Äî making propositions mutual knowledge ‚Äî to   be an elementary fact about dialogue ( Lewis , 1979 ;   Clark and Brennan , 1991 ) .   In this short paper , we investigate whether recent   NLP models of visual dialogue capture this pro-   cess . Specifically , we use the VisDial dataset ( Das   et al . , 2017a ) , which consists of dialogues in En-   glish about an image in an asymmetric setting simi-   lar to that from the first paragraph , and derive from   it diagnostic propositions that should be considered   mutual knowledge at a given point in the dialogue ,   and others whose truth value is only known to one   participant at the given time . We then probe dia-   logue representations built by models pretrained on   the VisDial task for whether they correctly track   the participants ‚Äô knowledge and commitments.2 Related Literature   Representing dialogue context implicitly as the con-   tinuous hidden states of neural networks trained in   an end - to - end fashion has been a prevailing prac-   tice since the works of Vinyals and Le ( 2015 ) , Sor-   doni et al . ( 2015 ) and Serban et al . ( 2016 ) . This   paradigm also enables multimodal input like im-   ages to be easily integrated ( Shekhar et al . , 2019b ) .   However , there is evidence that the human ability   ofcollaborative grounding still lacks in such mod-   els , in part due to the limitations of training regimes   and datasets ( Benotti and Blackburn , 2021 ) .   We witness extensive efforts to look into how   these models encode and make use of dialogue   history , capture salient information and produce   visually grounded representations ( Sankar et al . ,   2019 ; Agarwal et al . , 2020 ; Greco et al . , 2020a , b ) .   The analysis and evaluation of current dialogue   models ( as Hupkes et al . ( 2018a ) , Shekhar et al .   ( 2019a ) , Parthasarathi et al . ( 2020 ) , Saleh et al .   ( 2020 ) , Wu and Xiong ( 2020 ) , inter alia ) often rely   on diagnostic classifiers ( Hupkes et al . , 2018b ) and   probing tasks ( Belinkov and Glass , 2019 ) , common   tools to examine whether representations built by   neural networks encode linguistic information .   Another purposeful area of research on dialogue   revolves around inference . Zhang and Chai ( 2009 ,   2010 ) discuss conversation entailment , i.e.deter-   mining whether a conversation discourse entails a   hypothesis . Annotating or generating entailments ,   contradictions and neutral statements in dialogue   datasets is usual in recent works ( Welleck et al . ,   2019 ; Dziri et al . , 2019 ; Galetzka et al . , 2021 ) .   With insights from these three pillars , we pro-   pose a probing task for scorekeeping ( Lewis , 1979 )   on visual dialogues , formalised in the next section .   3 Problem Statement   Based on the premise that humans keep a mental   scoreboard of presupposed propositions and per-651   missible courses of action as a function of what has   been stated in a conversation ( Lewis , 1979 ) and on   the public / private dichotomy discussed in Ginzburg   ( 2012 ) , we propose a formalisation for the ‚Äú kine-   matics of scorekeeping ‚Äù ( Lewis , 1979 ) on VisDial .   Each dialogue in the VisDial dataset is a tuple   D= ( I , Q , A , T , P ) representing an interaction   between a questioner Qand an answerer A. They   exchange turns T , which establish propositions P ,   about a scene depicted in an image I.AseesI , but   Qdoes not . Both are provided with a caption K ,   which for simplicity we take to be the first turn of   A , t = K ; other turns comprise a question and   an answer , t= ( q , a ) , so that T= ( t)(as   dialogues have 10 turns ) .   We assume that : i ) Adoes not lie about their in-   terpretation of the image ; ii ) Qdoes not ask redun-   dant questions ; and iii ) a fact disclosed by Aimme-   diately becomes a shared commitment , even though   in reality this is not always the case ( e.g.when a   misunderstanding happens ) . Under these assump-   tions , each tdiscloses a new fact p(and its im-   plications ) about A ‚Äôs judgement of the image that   was unknown to Quntilt . Pis then defined   as a set of Npropositions { p , p , ¬∑ ¬∑ ¬∑ , p } . Each   pis either the direct entailment of t(that is , the   expressed proposition ) , which is established by A   to be true , or its negation , which is established by   Ato be false . The truth value of pis known to A   throughout the dialogue , but only privately so for   allk < i . It becomes shared between AandQat   k = iand remains so until the end of the dialogue .   With this in place , A ‚Äôs scoreboard of a dialoguecan be represented by a matrix Swith dimensions   |T| √ó |P| . Each element sis a tuple c‚ààC=   { ( true to A , private ) , ( true to A , shared ) , ( false   toA , private ) , ( false to A , shared ) } representing   the ‚Äò score ‚Äô of proposition pat turn tas a class ,   like the example in Figure 1 . Hence , the negation   of a fact that Aconsiders true but has not been   mentioned yet is labelled as ( false to A , private ) .   That way , the scoreboard at a given turn tis given   by the t - th row in Sand the whole matrix helps   visualising how the scoreboard is incrementally   updated throughout D.   Probing Task and Model . We design a classifi-   cation task to examine whether the continuous rep-   resentations of pretrained visual dialogue models   incrementally encode information about the score-   board represented by S. The probing classifier   is a function f : P√óR‚ÜíC , where P   is the set of propositions in a dialogue D , Ris   the space of hidden representations of a visual di-   alogue encoder and Care the scoreboard classes .   Based on the probing classifier architecture in He-   witt and Liang ( 2019 ) , we approximate fas a neu-   ral network which maps a dialogue representation r   concatenated to a continuous representation zof a   proposition to a vector vwith a probability distribu-   tion over classes , v = softmax ( WœÉ(W[r;z ] ) )   ( bias term omitted ) , as illustrated in Figure 1 . The   class is then predicted with the argmax function .   4 Data   Visual Dialogues and Encoders . We use the Vis-   Dial dataset v.1.0 ( Das et al . , 2017a ) and the three   QandAencoders ( RL_DIV , SL and ICCV_RL)652from Das et al . ( 2017b ) and Murahari et al . ( 2019 ) .   The first work implemented an end - to - end model   to train AandQusing reinforcement learning . The   latter is a follow - up study that adds an auxiliary ob-   jective function to encourage Qto ask more diverse   questions . The VisDial training set contains im-   ages from the MS COCO dataset ( Lin et al . , 2014 ) .   Proposition embeddings zare built with Sentence-   Transformers ( Reimers and Gurevych , 2019 ) .   Generating Probes . The sets Pare program-   matically generated by manipulating QA pairs us-   ing rules that identify common lexical and syntactic   patterns in VisDial , in a similar fashion as Demszky   et al . ( 2018 ) and Ribeiro et al . ( 2019 ) . Whenever   the pattern of a QA pair matches a rule , a direct   entailment and a direct contradiction are generated ,   as those shown in Figure 1 .   Dataset Construction . We retrieve the pre-   trained dialogue context representations R=   { r|0‚â§l‚â§10 } , where ris the hidden state   of the encoder after it processed the dialogue up to   turnlinT(and the image and next question for A ) .   We then pair elements in Rwith the embeddings   of the generated propositions pinP , forming   tuples { ( r , p)|0‚â§l‚â§10,1‚â§j‚â§N}which   are mapped to the corresponding class c‚ààC. The   true to Aorfalse to Astatus of a proposition pre-   mains fixed for all turns in D , since it refers to a fact   ( according to A ‚Äôs beliefs ) about the image , while   theprivate status holds for ( r , p ) , . . . , ( r , p )   and shifts to shared for(r , p ) , . . . , ( r , p ) . The   probing dataset is thus composed of datapoints   ( r , p , c ) for all D , for all turns ‚Äô representations   r‚ààR , for all p‚ààP. Propositions gener-   ated from captions are downsampled because they   outnumber the other turns , resulting in too many   propositions that are always shared . In order to   avoid bias with respect to the true / false dimension ,   we sample the training set of propositions enforc-   ing that each type appears as true to Aexactly the   same number of times as it does as false to Ain   different dialogues . Table 1 presents a summary   ( see Appendix for details ) .   5 Experiments   We train and test the classifier varying three as-   pects : i ) AorQ , ii ) main task with all classes in C   ( TFxPS ) , plus three variations with reduced dimen-   sions : Only true / false ( TF ) , only private / shared   ( PS ) and merging true / false on the private cases   only ( PxTSFS ) and iii ) control tasks ( Hewitt and   Liang , 2019 ) ( a ) replacing rby a random vector ( b )   replacing rby a null vector , both only on the train-   ing set , to quantify how much information can be   extracted from propositions alone during training .   Evaluation . Results are evaluated with accu-   racy on class predictions . To avoid any influence   that knowing the position in the dialogue could   have ( early in the dialogue , propositions have a   greater chance of being private , and vice versa ) ,   we evaluate the results at turn 5 ( at which there   is a more balanced chance of a fact having been   mentioned or not ) . For the error analysis , we recon-   struct complete predicted scoreboards and evaluate   incremental aspects : In each column , only one shift   from private to shared should occur at the right turn   ( except for caption propositions , which are always   shared ) and the true / false status should not change .   Implementation . The classifier is implemented   with PyTorch ( Paszke et al . , 2019 ) and trained with   gradient descent using Adam optimizer ( Kingma   and Ba , 2014 ) to minimize cross entropy .   6 Results   Table 2 presents the accuracy of all models and   tasks at turn 5 . The performance on the main task   is very similar across encoders , with differences   lower than 1.5 % . Qoutperforms Ain all models   in the main task . While this is expected , since   Q ‚Äôs representations must only keep track of the   dialogue whereas Amust interpret the image , the   difference is only marginal.653   For the TF task , the performance on the con-   trol tasks is close to random , as expected , but it   is higher than random for other tasks . We notice   that , while the training dataset is constructed to be   balanced in the true / false dimension , information   on the private / shared dimension has an inherent   bias that is more complex to counterbalance on the   training set . Despite the fact that datapoints in the   private class do not substantially outnumber the   shared class , we observe that each proposition type   can have a tendency to occur either early or late in   the dialogue ( examples in Figure 2 ) , causing them   to have an individual skewed distribution towards   shared or private at turn 5 . This information leak   can be used as a shortcut by the classifier . Still ,   AandQ ‚Äôs representations lead to performances   between 8 % and 32 % higher than the control tasks   in all cases .   Human Performance . Table 3 shows the human   performance , estimated as the average accuracy of   3 annotators ( 0.86 Fleiss ‚Äô Œ∫on TFxPS ) on a sample   of 94 datapoints , each from a different dialogue in   the test set ( not only at turn 5 ) . We observe that hu-   mans agree most of the times on their judgements   and all models perform well below human level .   Error Analysis . We conduct an error analysis   onA , main task , TFxPS . The confusion matrix   in Figure 3 shows that it is easier to distinguish   between true / false to Ain the shared dimension ,   which can be a sign that dialogue information is   more salient in the representations than the image .   The accuracy on all datapoints with proposition   types that occur on the training set is 67.69 , higher   than for those that do not , which is 53.11 .   When we reconstruct full predicted scoreboards ,   some qualitative shortcomings become evident . A   shift from private to shared is predicted at the cor-   rect turn for 60.32 % of the propositions but only65438.24 % shifts only at the correct turn . Besides , only   44.50 % of the propositions have stable predictions   regarding the true / false to Adimension .   Figure 4 shows types of errors in the predictions   ( the Appendix has more examples ) . We see the   same truth value assigned to opposite propositions ,   the same proposition classified both as true and   false at different turns , as well as an occasional   oscillation between private / shared throughout the   dialogue . These are indications that , although accu-   racy per label is generally high , the representations   do not seem to always allow incrementally stable   and consistent predictions throughout the dialogue .   7 Scope and Limitations   The results on this paper comprise three visual di-   alogue models trained using a similar setting on   the same dataset . The preprocessing steps used by   these models replace some tokens by a UNK token   and truncate long captions , which prevents some   information to become shared as assumed . Further   investigation with other models and data is neces-   sary in future research in order to support more   general conclusions . The results also rely on the ca-   pabilities of the classifier . Although we performed   hyperparameter search , the probing classifier does   not completely overfit the full training dataset , thus   other architectures and hyperparatemeters can be   further investigated .   The rule - based generation of propositions has   limitations . It can not generate propositions for all   QA pairs and some rules end up not always yield-   ing grammatically valid sentences , for instance be-   cause of countable / uncountable nouns , detection of   singular / plural forms and mistakes and typos deriv-   ing from the dialogues themselves . Besides , spuri - ous patterns deriving from the implemented rules   or other confounds and inherent biases ( e.g. Fig-   ure 2 ) may exist and be predictive of the classes ,   which could be captured by the probing classifier   and influence ( likely overestimating ) the results .   Enforcing a balance on the training set in terms of   true / false to Asolves one source of bias but causes   its distribution to differ from the validation and test   set . The test set also has a different distribution   because of its varying number of turns .   Finally , while the assumptions proposed in Sec-   tion 3 are necessary idealizations for using VisDial   for this task , they simplify essential aspects of di-   alogues , e.g.the uncertainty about a fact actually   being shared , memory limitations and the many   kinds of inference that are used in the accommoda-   tion of shared knowledge , such as presuppositions ,   implicatures , entailments and implicit information .   Our method can not capture background knowledge   not explicitly stated in dialogue turns .   8 Conclusion   We have proposed a novel way to do theory - based   evaluation of visual dialogue models . Using diag-   nostic propositions , we investigated to what degree   neural network visual dialogue models incremen-   tally build up representations that are appropriate to   doscorekeeping of shared commitments through-   out a dialogue . The evaluated models trained on   VisDial capture part of this process , but not always   consistently , possibly because this ability is not   an elementary component of the training regime .   The relatively impoverished nature of the original   task in terms of coordination phenomena can also   limit the capability of models to build good dia-   logue representations ( Schlangen , 2019 ) . Future   work should extend the evaluation to other models   and reflect on how better and ecologically valid   diagnostic datasets for visual dialogues can be con-   structed .   9 Ethical Considerations   Propositions are direct manipulations of QA pairs   and thus reflect the subjective judgments of Vis-   Dial crowdworkers . Therefore , they are not per se   necessarily trueorfalse with respect to the image ,   but with respect to A ‚Äôs interpretation expressed as   answers . Inappropriate content on images , captions   and dialogues can be replicated by the rule - based655proposition generation . To try to remedy this , we   filtered out dialogues containing words that could   be used for sensitive content . Despite our efforts ,   we can not guarantee that we could remove every-   thing , given the size of the dataset and the inherent   bias of how humans interpret images . As a result ,   the only purpose of the propositions is performing   the evaluation as proposed here .   Acknowledgements   We are thankful to the anonymous reviewers for   their feedback and suggestions , to Wencke Lier-   mann for implementing the interface for the human   evaluation and to the student assistants of the Com-   putational Linguistics Lab who contributed on the   experiment .   References656657Appendix   A Generating Propositions and   Constructing the Datasets   This section presents details about the procedure   to turn QA pairs from the VisDial datasetinto   propositions .   Solving Pronouns . Coreference resolution is   specially challenging on visual dialogues , as dis-   cussed in Lo√°iciga et al . ( 2021 ) . Despite the limi-   tations , we used the model proposed in Lee et al .   ( 2018 ) to replace pronouns ( those that were de-   tected and solved ) by their corresponding entity as   follows :   1.Merged caption and QA pairs into a single   string .   2.Passed string to coreference resolution model   to get coreference clusters .   3.Assumed that the first element in the cluster   was the entity ( its first mention ) .   4.For each dialogue , checked which questions   and answers contained pronouns of interest   ( he , she , it , they , his , her , its , their , him , them ,   hers , theirs , this , that , these , those ) and re-   placed them with their corresponding cluster   entity , if detected . Assumed the pronoun her   was always possessive .   5.If the entity comprised more than N=5 to-   kens , we did not replace it ( because entities   spanning over many tokens are very likely to   be long portions of the caption that result in   wrong propositions ) .   6.With postprocessing steps , put string back into   VisDial format .   On average , 2.24 pronouns were replaced per   dialogue on the training set , 2.43 on the validation   set and 1.15 on the test set .   Generating Propositions . Automatic genera-   tion of diagnostic datasets or adversarial examples   via programmatic manipulation rules or templates   is a usual step in probing studies , e.g. Johnson et al .   ( 2017 ) , Shekhar et al . ( 2017 ) , Ribeiro et al . ( 2018 )   and Bitton et al . ( 2021 ) . The main steps to turn   QA pairs into propositions were to some extent   based on Ribeiro et al . ( 2019 ) and Demszky et al .   ( 2018 ) . We analysed common patterns of questions658and answers on VisDial and implemented 34 rules   that create entailments and contradictions . Some   rules are lexical ( e.g.questions starting with ‚Äò what   color is ‚Äô and whose answer has a color name ) and   others depend on POS tag patterns extracted using   SpaCy v.3.0.5.Most rules work for polar ques-   tions , some work for other types of questions . We   noticed that some images and dialogues on VisDial   contain inappropriate content . To avoid replicating   this on the propositions , we filtered out dialogues   that contain words that may be sensitive ( see code   documentation for details ) . Propositions were then   generated as follows :   1.Parsed the caption to extract nouns and adjec-   tives and generated caption propositions .   2.For each turn , checked whether it matched a   manipulation rule .   3.Every rule , when they were applied , generated   a direct entailment and a direct contradiction   ( negation of the entailment ) .   4.Propositions that contained pronouns ( for   cases in which coreference resolution did not   work ) , except for it , or that were too long   ( more then 15 tokens ) were excluded .   The code documentation has a more detailed   description of the rules . The next sections present   details of the resulting proposition sets . Note that   the number of dialogues in each set is smaller than   in the VisDial original splits , because some were   filtered out and others had no propositions .   Propositions have four attributes : i ) kind of ma-   nipulation rule ; ii ) dialogue and turn from which   it derives ; iii ) a true / false status with respect to   what Athinks about the image ; iv ) the polarity   ( positive / negative ) of the answer , if applicable .   Downsampling and de - biasing . We noticed   that the proportion of caption propositions was   much larger than propositions deriving from other   turns , which would cause a considerable imbalance   towards facts that are always shared in the score-   board . Therefore , we sampled 15 % of the caption   pairs ( entailment and contradiction ) on all datasets   to make the distribution over manipulated turns be   closer to uniform .   Furthermore , in preliminary experiments we ob-   served that propositions could give away informa-   tion on the true / false to Astatus . For instance ,   ‚Äò there is a zebra . ‚Äô can appear very often as an en-   tailment ( on the many photos showing zebras ) butrarely as a contradiction ( dialogues where Qspon-   taneously asks ‚Äò is there a zebra ? ‚Äô and the answer is   ‚Äò no ‚Äô ) . Besides , on rules that manipulate questions   that are not polar ( what color is the dog ? black . ) ,   negation is always a contradiction . So the classi-   fier could make predictions based on the lexical   form alone . To counter this bias , we constructed   a balanced training dataset by sampling from the   original set while making sure that , for each p   thatAestablished to be true with respect to an   image / dialogue , we also included an equal ppaired   with an image / dialogue in which it is established   to be false . While this procedure reduced the size   of the training set , we ensured that predictions on   the true / false dimension would need to use the dia-   logue representations . We also limited the number   ofpof the same kind to 2,000 ( 1,000 as entailment ,   1,000 as contradiction ) , to avoid having very com-   mon propositions like ‚Äò the photo is in color ‚Äô or ‚Äò it   is sunny ‚Äô occurring too often .   Datasets used in the experiments . The fol-   lowing paragraphs discuss the final datasets used   in the experiments ( i.e.after downsampling cap-   tions and balancing the training set ) . The frequency   over which turn was manipulated is shown in Fig-   ure 5 . Although there is an imbalance towards   later turns on the training set , the proportion of pri-   vate / shared classes at turn 5 is relatively balanced   ( around 44.5/55.5 ) , partially due to the fact that , at   the last turn , no proposition is assigned a private   class . Figure 6 shows the frequency of the number   of turns that have been turned into propositions in   a dialogue . Table 4 show the proportion of each   type of proposition on the datasets . The training set   has less propositions that do not derive from polar   questions due to the balancing .   The propositions , paired to dialogue representa-   tions on each dialogue turn , with the class assigned   to each tuple can be seen as a layer of annotation   which is not predicted but constructed .   37.20 % of the validation proposition types and   31.58 % of the test proposition types appear among659   the training propositions . 82.68 % of the validation   propositions and 79.63 % of the test propositions   occur in only one dialogue . On average , a propo-   sition appears in 12.77 dialogues in the training   set , 1.91 dialogues in the validation set and 2.34   dialogues in the test set . 72.73 % of the word types   in the validation set and 63.00 % of the word types   in the test set occur in the training set .   Examples . Figure 10 shows dialogues from the   training set and the propositions generated for each   turn , after downsampling the caption propositions   ( but before balancing ) . Propositions can inherit   grammatical or spelling problems from the dia-   logues themselves . Figure 1 in the main section   contains all propositions , before downsampling .   Collecting dialogue representations . To collect   the dialogue state representations , we adapted the   original train.py andevaluate.py scripts . To get   the representation at turn 10 for A , we needed to   feed a dummy next question made of the start and   the end symbols with a question mark token in   between .   Human Judgement . We randomly sampled 100   dialogues and one proposition on each of them .   Then we sampled a random turn up to which the   corresponding dialogue would be shown . The an-   notators were non - native English speakers who   worked as student assistants at the Computational   Linguistics Lab of the University of Potsdam . The   task was explained to the annotators verbally and   then again in written form at the beginning of the   annotation . All participants saw the same data-   points at a different random order , presented in a   setting as shown in Figure 7 , and had to select one   of the four alternatives ( which correspond to the   main task TFxPS ) .   B Reproducibility   In this section , we present further details of the   implementation and additional results to support re-   producibility . More information can also be found   in the code documentation .   Hyperparameters . We used comet.ml‚Äôsim-   plementation of the Bayes algorithm for hyperpa-660rameter search on A , main task , TFxPS , RL_DIV ,   aiming at maximizing accuracy on the validation   set , as well as some manual selections . The ( non-   exhaustive ) search space is shown in Table 6 . The   optimal configuration was then used in all experi-   ments , with a maximum of 30 epochs and no early-   stopping . A preliminary test with an even larger   hidden dimension showed a very minor improve-   ment . For each experiment , we used the config-   uration that led to the best performance on the   validation set to get results on the test set . Each   experiment took between 50 and 60 minutes .   The sentence encoder models listed on Table 6   are available at HuggingFace ‚Äôs Model Hub .   Classifier architecture . The neural network was   implemented using Pytorch 1.7.1 . The proposition   embeddings have 768 dimensions and the dialogue   context embeddings have 512 dimensions . We used   a sequential model from PyTorch with the follow-   ing layers and dimensions :   1.linear layer ( in features=768 + 512 , out fea-   tures=1024 , bias = True )   2 . sigmoid function   3 . dropout layer ( p=0.1 )   4.linear layer ( in features=1024 , out features = n   labels in { 2,3,4 } , bias = True )   5 . softmax function + cross entropy loss   The models have 1,315,844 , 1,314,819 and   1,313,794 trainable parameters for the classification   tasks with 4 , 3 and 2 labels , respectively .   Infrastructure . The operating system used to   run experiments was Linux , release 5.4.0 - 99-   generic , processor x86_64 . We had two GPUs   available ( NVIDIA GeForce GTX 1080 Ti ) , but   each individual experiment used only one of them .   C Detailed Results   Table 7 shows the overall accuracy on all datapoints   ( comprising all turns in the test set ) . Table 8 and   Table 9 show all results on the validation set .   On Figure 8 we split the accuracy per type of   proposition . Propositions that derive from negative   facts about the image ( ‚Äò is there a dog ? no . ‚Äô ) seem   to be harder than positive ones when they derive   from earlier turns , but they are easier to correctly   classify when they derive from later turns . Propo-   sitions deriving from questions that are not polar   are harder ( which may be a consequence of the   balanced dataset selection that results in few propo-   sitions of this type for training ) . We also see that   propositions derived from manipulating later turns   are , in general , harder to classify .   When we consider each row of the scoreboard   ( representing the scoreboard at a given turn ) , we   can inspect how accuracy evolves over turns , illus-   trated in Figure 9 .   For the error analysis on captions , a right shift   from private to shared means that the class at turn 0   is shared . Shifting only at the right turn means that   it starts as shared and does not shift at any turn.661662663664   Liang Chen , Runxin Xu , Baobao Chang   Key Laboratory of Computational Linguistics , Peking University , MOE , China   leo.liang.chen@outlook.com   runxinxu@gmail.com chbb@pku.edu.cn   Abstract   1 Introduction   Recent advances in Transformer - based ( Vaswani   et al . , 2017 ) models have achieved remarkable suc-   cess in Neural Machine Translation ( NMT ) . For   most NMT studies ( Vaswani et al . , 2017 ; Song   et al . , 2019 ; Lin et al . , 2020 ; Liu et al . , 2020 ; Ma   et al . , 2021 ) , there are two widely used techniques   to improve the quality of the translation : Label   Smoothing ( LS ) and V ocabulary Sharing ( VS ) . La-   bel smoothing ( Pereyra et al . , 2017 ) turns the hard   one - hot labels into a softweighted mixture of the   golden label and the uniform distribution over the   whole vocabulary , which serves as an effective reg-   ularization technique to prevent over - fitting and   over - confidence ( M√ºller et al . , 2019 ) of the model .   In addition , vocabulary sharing ( Xia et al . , 2019 ) is   another commonly used technique , which unifies   the vocabulary of both source and target language   into a whole vocabulary , and therefore the vocabu-   lary is shared . It enhances the semantic correlation   between the two languages and reduces the number   of total parameters of the embedding matrices .   However , in this paper , we argue that jointly   adopting both label smoothing and vocabulary shar-   ing techniques can be conflicting , and leads to sub-   optimal performance . Specifically , with vocabulary   sharing , the shared vocabulary can be divided into   three parts as shown in Figure 1 . But with label   smoothing , the soft label still considers the words   at the source side that are impossible to appear at   the target side . This would mislead the translation   model and exerts a negative effect on the transla-   tion performance . As shown in Table 1 , although   introducing label smoothing or vocabulary sharing   alone can improve the vanilla Transformer , jointly665adopting both of them can not obtain further im-   provements but achieves sub - optimal results .   To address the conflict of label smoothing and   vocabulary sharing , we first propose a new mecha-   nism named Weighted Label Smoothing ( WLS ) to   control the smoothed probability distribution and   its parameter - free version Masked Label Smooth-   ing ( MLS ) . Simple yet effective , MLS constrains   the soft label not to assign soft probability to the   words only belonging to the source side . In this   way , we not only keeps the benefits of both label   smoothing and vocabulary sharing , but also address   the conflict of these two techniques to improve the   quality of the translation .   According to our experiments , MLS leads to   a better translation not only in scores like BLEU   but also reports improvement in model ‚Äôs calibra-   tion . Compared with original label smoothing   with vocabulary sharing , MLS outperforms in   WMT‚Äô14 EN - DE(+0.47 BLEU ) , WMT‚Äô16 EN - RO   ( +0.33 BLEU ) and other 7 language pairs including   DE , RO - EN multilingual translation task .   2 Background   Label Smoothing The original label smoothing   can be formalized as :   ÀÜy=ÀÜy(1‚àíŒ± ) + Œ± / K ( 1 )   Kdenotes the number of classes , Œ±is the label   smoothing parameter , Œ± / Kis the soft label , ÀÜyis a   vector where the correct label equals to 1 and others   equal to zero and ÀÜyis the modified targets .   Label smoothing is first introduced to image   classification ( Szegedy et al . , 2016 ) task . Pereyra   et al . ( 2017 ) ; Edunov et al . ( 2018 ) explore label   smoothing ‚Äôs application in Sequence generation   from token level and Norouzi et al . ( 2016 ) propose   sentence level ‚Äôs label smoothing . Theoretically ,   M√ºller et al . ( 2019 ) ; Meister et al . ( 2020 ) all point   out the relation between label smoothing and en-   tropy regularization . Gao et al . ( 2020 ) explores   the best recipe when applying label smoothing to   machine translation . To generate more reliable soft   labels , Lukasik et al . ( 2020 ) takes semantically sim-   ilar n - grams overlap into consideration level label   smoothing . Wang et al . ( 2020 ) proposes Graduate   Label Smoothing that generate soft label according   to the different confidence scores of model . To   the best of our knowledge , we are the first to in-   vestigate label smoothing ‚Äôs influence on machine   translation from the perspective of languages . Category DE->EN RO->EN VI->EN   Source 39 % 50 % 36 %   Common 20 % 8 % 11 %   Target 41 % 42 % 53 %   Vocabulary Sharing V ocabulary sharing is   widely applied in most neural machine translation   studies ( Vaswani et al . , 2017 ; Song et al . , 2019 ; Lin   et al . , 2020 ) . Researchers have conducted in - depth   studies in V ocabulary Sharing . Liu et al . ( 2019 )   propose shared - private bilingual word embeddings ,   which give a closer relationship between the source   and target embeddings . While Kim et al . ( 2019 )   point out that there is an vocabulary mismatch be-   tween parent and child languages in shared multi-   lingual word embedding .   3 Conflict Between Label Smoothing and   Vocabulary Sharing   Words or subwords in a language pair ‚Äôs joint dictio-   nary can be categorized into three classes : source ,   common andtarget using Venn Diagram accord-   ing to their belonging to certain language as de-   picted in Figure 1 . This can be achieved by check-   ing whether one token in the joint vocabulary also   belongs to the source / target vocabulary . We formal-   ized the categorization algorithm in Appendix A.   Then we compute the tokens ‚Äô distribution in dif-   ferent translation directions as shown in Table 2 .   Tokens in source class account for a large propor-   tion up to 50 % . When label smoothing and vocab-   ulary sharing are together applied , the smoothed   probability will be allocated to words that belong   to the source class . Those words have zero overlap   with the possible target words , therefore they have   no chance to appear in the target sentence . Allocat-   ing smoothed probability to them might introduce   extra bias for the translation system during training   process , unavoidably leading to a higher translation   perplexity as also revealed by M√ºller et al . ( 2019 ) .   Table 3 reveals the existence of conflict , that the   joint use of label smoothing and vocabulary sharing   does n‚Äôt compare with solely use one technique in   all language pairs with a maximum loss of 0.32   BLEU score.6664 Methods   4.1 Weighted Label Smoothing   To deal with the conflict when executing la-   bel smoothing , we propose a plug - and - play   Weighted Label Smoothing mechanism to control   the smoothed probability ‚Äôs distribution .   Weighted Label Smoothing(WLS ) has three pa-   rameters Œ≤ , Œ≤ , Œ≤apart from the label smoothing   parameter Œ± , where the ratio of the three parame-   ters represents the portion of smoothed probability   allocated to the target , common and source class   and the sum of the three parameters is 1 . The   distribution within token class follows a uniform   distribution . WLS can be formalized as :   ÀÜy=ÀÜy(1‚àíŒ± ) + Œ≤ ( 2 )   where ÀÜyis a vector where the element corre-   sponding to the correct token equals to 1 and others   equal to zero . Œ≤is a vector that controls the distri-   bution of probability allocated to incorrect tokens .   We use t , c , sto represent probability allocated   to the i - th token in the target , common , source cate-   gory , all of which form the distribution controlling   vector Œ≤withPŒ≤ = Œ± . The restriction can be   formalized as :   X   t :X   c :X   s = Œ≤ : Œ≤ : Œ≤ ( 3 )   4.2 Masked Label Smoothing   Based on the Weight Label Smoothing mechanism ,   we can now implement Masked Label Smoothing   by set Œ≤to 0 and regard the target and common   category as one category . In this way , Masked   Label Smoothing is parameter - free and implicitly   injects external knowledge to the model . And we   have found out that this simple setting can reach   satisfactory results according our experiments .   We illustrate different label smoothing methods   in Figure 2 . It is worth noticing that MLS is differ-   ent from setting WLS ‚Äôs parameters to 1 - 1 - 0 since   there might be different number of tokens in the   common and target vocab .   5 Experiments   5.1 Task Settings   For bilingual translation , we conduct experiments   on 7 translation tasks . We choose language   pairs that have different ratio of common sub-   words . These include WMT‚Äô14 DE - EN , EN - DE ,   IWSLT‚Äô14 DE - EN , IWSLT‚Äô15 VI - EN , WMT‚Äô16   RO - EN , EN - RO and CASIA ZH - EN .   We use the official train - dev - test split of   WMT‚Äô14 , 16 and IWSLT‚Äô14 , 15 datasets . For CA-   SIA ZH - EN dataset , we randomly select 5000 sen-   tences as development set and 5000 sentences as   test set from the total dataset .   For multilingual translation , we combine the   WMT‚Äô16 RO - EN and IWSLT‚Äô14 DE - EN datasets   to formulate a RO , DE - EN translation task . We also   make a balanced multilingual dataset that has equal   numbers of DE - EN and RO - EN training examples   to reduce the impact of imbalance languages and   to explore how MLS performs under different data   distribution condition in multilingual translation .   We apply the Transformer base ( Vaswani et al . ,   2017 ) model as our baseline model . We fix the   label smoothing parameter Œ±to 0.1 in the main   experiments and individually experiment and ex-   amine the performance of MLS under different Œ± .   We use compound_split_bleu.sh from fairseq to   compute the final bleu scores . The inference ECE   scoreand chrF scoreare computed through open   source scripts . We list the concrete training and   evaluation settings in Appendix B.   5.2 Results   Bilingual Table 3 shows the results of bilingual   translation experiments . The results reveal the con-   flict between LS and VS that models with only LS667(a ) Bilingual Translation   ( b ) Multilingual Translation   surpass models with both LS and VS in all exper-   iments . Our Masked Label Smoothing obtained   consistent improvements over original LS+VS in   all tested language pairs significantly .   The effectiveness of MLS maintained under dif-   ferent Œ±value as shown in Table 4 for both BLEU   and chrF scores . Similar to Gao et al . ( 2020 ) ‚Äôs   conclusion , we find that a higher Œ±can generally   improve the bilingual translation quality . And ap-   plying MLS can further improve the results . It   shows that not only the probability increase in tar-   get vocabulary , but also the allocation of smoothed   probabilities in different languages matters in the   improvement of translation performance .   Multilingual As shown in Table 3 , MLS achieves   consistent improvement over the original label   smoothing in both the original and the balanced   multilingual translation dataset under all transla-   tion directions . In the original combined dataset ,   direction RO - EN ( 400 K ) has much more samples   than DE - EN ( 160 K ) . We do not apply a resampling   strategy during training in order to investigate how   the imbalance condition affects different models ‚Äô   performance . The balanced version cuts down sam-   ples in RO - EN direction to the same number as in   DE - EN direction .   Compared with the imbalance version , the bal-   anced version gave better BLEU scores in DE - EN   direction while much worse performance in RO-   EN translation for both the original label smoothing   and MLS . It indicates that the cut down on RO - EN(a ) EN - RO   ( b ) RO - EN   training examples does weaken the generalization   of model in RO - EN translation however does n‚Äôt   influence the DE - EN translation quality since the   RO - EN data might introduce bias to the training   process for DE - EN translation .   Even under imbalance condition , MLS can give   a better performance ( 37.53 ) compared to original   LS in the balance condition ( 37.44 ) . It implies that   MLS can relieve the imbalance data issue in multi-   lingual translation . However , the improvement in   relative high - resources direction ( RO - EN ) is not as   significant as in the balanced condition . We guess   that label smoothing has more complex influence   on multilingual model due to the increase of lan-   guages and relation among different languages . We   leave those questions for future exploration.668Œ≤Œ≤Œ≤RO - EN EN - RO DE - EN   - - - 22.80 23.15 30.94   1/3 1/3 1/3 22.68 23.19 31.40   1/2 1/2 0 23.05 23.19 31.18   1/2 0 1/2 22.86 23.01 31.33   0 1/2 1/2 22.22 23.33 30.85   1/2 1/4 1/4 22.73 23.16 30.92   6 Discussion   6.1 Exploring of Weighted Label Smoothing   As reported in Table 5 , we explore the influ-   ence of different WLS on multiple tasks including   WMT‚Äô16 RO - EN , EN - RO and WMT‚Äô14 DE - EN .   According to the result , though the best BLEU   score ‚Äôs WLS setting vary from different tasks and   there seems to exist a more complex relation be-   tween the probability allocation and the BLEU   score , we still have two observations . First , apply-   ing WLS can generally boost the quality of trans-   lation compared to the original label smoothing .   Second , only WLS with Œ≤ , Œ≤ , Œ≤each equals to   1/2 - 1/2 - 0 can outperform the original label smooth-   ing on all tasks , which suggests the setting is the   most robust one . Thus we recommend using this   setting as the initial setting when applying WLS .   Furthermore , the most robust setting agrees with   the form of MLS since they both allocate zero prob-   ability to the source category ‚Äôs tokens , which fur-   ther proves the robustness of MLS .   6.2 Improvement in Model ‚Äôs Calibration and   Translation Perplexity   M√ºller et al . ( 2019 ) have pointed out label smooth-   ing prevents the model from becoming over-   confident therefore improve the calibration of   model . Since there is a training - inference discrep-   ancy in NMT models , inference ECE score ( Wang   et al . , 2020 ) better reflects models ‚Äô real calibration .   To compute the ECE scores , we need to split   the model ‚Äôs predictions into Mbins according to   the output confidence and calculate the weighted   average of bin ‚Äôs confidence / accuracy difference as   the ECE scores considering the number of samples   in each bin .   ECE = X|B|   N|acc ( B)‚àíconfidence ( B)|   where Nis the number of total prediction sam-   ples and Bis the number of samples in the i - th bin .   acc ( B)is the average accuracy in the i - th bin .   The score denotes the difference between ac-   curacy and confidence of models ‚Äô output during   inference . Less ECE implies better calibration .   The inference ECE scores of our models are   shown in Table 6 . It turns out that models with   MLS have lower Inference ECE scores on different   datasets . The results indicate that MLS will lead to   better model calibration .   We also find out that MLS leads to a significantly   lower perplexity than LS during the early stage of   training in all of our experiments . It ‚Äôs not surpris-   ing since zeroing the source side words ‚Äô smoothed   probability can decrease the perplexity . It can be   another reason for model ‚Äôs better translation perfor-   mance since it gives a better training initialization .   7 Conclusion   We reveal the conflict between label smoothing   and vocabulary sharing techniques in NMT that   jointly adopting the two techniques can lead to sub-   optimal performance . To address this issue , we in-   troduce Masked Label Smoothing to eliminate the   conflict by reallocating the smoothed probabilities   according to the languages ‚Äô differences . Simple yet   effective , MLS shows improvement over original   label smoothing from both translation quality and   model ‚Äôs calibration on a wide range of tasks .   8 Acknowledgements   We thank all reviewers for their valuable sugges-   tions for this work . This paper is supported by the   National Science Foundation of China under Grant   No.61876004 and 61936012 , the National Key Re-   search and Development Program of China under   Grant No . 2020AAA0106700.6699 Ethics Consideration   We collect our data from public datasets that permit   academic use . The open - source tools we use for   training and evaluation are freely accessible online   without copyright conflicts .   References670A Algorithm   Algorithm 1 Divide Token Categories   Input : List : S , T , J   Output : List : A , B , C   Description : S is the vocabulary list for source   language , T for target language , J for joint vocabu-   lary . A is the output vocabulary for source tokens ,   B for common tokens , C for target tokens . Initialize empty list A , B , Cfori in J do ifi in S and i in T then B.add(i ) else ifi in S then A.add(i ) else C.add(i)return A , B , C   B Experiment Details   We evaluate our method upon Transformer - Base   ( Vaswani et al . , 2017 ) and conduct experiments   under same hyper - parameters for fair comparison .   We use fairseq ( Ott et al . , 2019 ) as the main code   base .   Before training , we first apply BPE ( Sennrich   et al . , 2016 ) to tokenize the corpus for 16k steps   each language and then learn a joint dictionary .   During training , the label smoothing parameter Œ±   is set to 0.1 except for Table 4 ‚Äôs exploration in   alpha values . We use Adam optimizer with betas   to be ( 0.9,0.98 ) and learning rate is 0.0007 . During   warming up steps , the initial learning rate is 1e-   7 and there are 1000 warm - up steps . We use a   batch - size of 2048 together with an update - freq of   4 on two NVIDIA 3090 GPUs . Dropout rate is   set to 0.3 and weight decay is set to 0.0001 for all   experiments . We average the last 3 checkpoints   to generate the final model in the main bilingual   experiments before inferring on the test set . We   use beam size as 5 during all testing.671   Xi‚Äôao Su , Ran Wang , Xinyu Dai   National Key Laboratory for Novel Software Technology , Nanjing University , China   Collaborative Innovation Center of Novel Software Technology and Industrialization , China   { nlp_suxa,wangr}@smail.nju.edu.cn , daixinyu@nju.edu.cn   Abstract   1 Introduction   Multi - Label Text Classification ( MLTC ) is a funda-   mental task in natural language processing , which   can be found in many real - world scenarios such as   web page tagging ( Jain et al . , 2016 ) , topic recogni-   tion ( Yang et al . , 2016 ) , sentiment analysis ( Wang   et al . , 2016 ) and so on . Different from multi - class   classification where only one label is identified as   positive , MLTC aims to assign multiple labels from   a predefined set to each text .   Till now , extensive research has been carried   out to solve the MLTC task . Among them , some   methods focus on learning enhanced text represen-   tation with deep neural networks ( Kurata et al . ,   2016 ; Liu et al . , 2016 ) or the label - wise attention   mechanism ( Xiao et al . , 2019 ; Ma et al . , 2021 ) .   Meanwhile , others try to model the label correla-   tion by the sequential prediction ( Nam et al . , 2017 ;   Yang et al . , 2018 ) , iterative reasoning ( Wang et al . ,   2021 ) , or graph neural networks ( Ma et al . , 2021).Text Labels   Themutual information of two   random variables is commonly   used in learning bayesian nets   as well as in other fields ... math . ST   math . IT   stat . TH   cs . IT   cs . AI   Mutual information is widely   used , to measure the stochastic   dependence of categorical   random variables in order   to address questions ... math . ST   math . IT   stat . TH   cs . IT   cs . AI   cs . LG   However , during inference , these methods ne-   glect the rich knowledge which can be directly ob-   tained from the existing training instances . Utiliz-   ing this knowledge can assist the model to predict   more accurately . For example , Tab . 1 lists two pa-   pers from arXivalong with their tags . Both papers   research on " Mutual Information " and they have al-   most the same labels . If we are tagging the second   paper , then we can easily get a good reference from   the first one . Therefore , when predicting labels for   a specific text , the model can get immediate and   reliable help from the instances with similar texts .   To this end , for the first time , we solve the MLTC   task by the use of knearest neighbor ( kNN ) mech-   anism which can effectively utilize the knowledge   from existing multi - label instances . Specifically , it   retrieves several neighbor instances based on text   representations generated by the MLTC model and   interpolates the model prediction with their labels .   Moreover , to make the model aware of the kNN   process and improve the quality of retrieved neigh-   bors , we propose to train the model with a con-   trastive learning ( CL ) objective . Existing super-672vised contrastive learning methods ( Gunel et al . ,   2021 ; Li et al . , 2021 ) are proposed under the con-   ventional multi - class setting , where two instances   are either positive or negative for each other . How-   ever , in MLTC , two instances may share some com-   mon labels while there may also be some labels   that are unique to each instance . How to handle   these cases is the key to utilizing contrastive learn-   ing in MLTC . We argue that simply treating these   instance pairs as positive ones is sub - optimal due to   the variable similarities in different instance pairs ,   which is verified in Section 4.2 . To model more   fine - grained correlations between multi - label in-   stances , we design a multi - label contrastive learn-   ing objective with a dynamic coefficient for each   instance pair based on the label similarity . Training   with this objective encourages the model to gener-   ate closer representations for instance pairs with   more shared labels and push away those pairs that   have completely different labels . As a result , the   kNN mechanism will retrieve instances that con-   tain more relevant labels , thereby further improving   the classification performance . It ‚Äôs worth noting   that our method is of high versatility and can be   directly applied to most existing MLTC models .   In summary , our contributions are as follows :   ‚Ä¢We propose a knearest neighbor mechanism   for MLTC that directly utilizes the knowledge   from the existing instances during inference .   ‚Ä¢We design a multi - label contrastive learning   objective which can effectively enhance the   kNN mechanism for MLTC .   ‚Ä¢Extensive experiments show that our method   can consistently and considerably improve the   performance of multiple existing MLTC mod-   els including the state - of - the - art pretrained   and non - pretrained ones .   2 Related Work   Multi - label Text Classification Existing meth-   ods for MLTC mainly focus on learning text repre-   sentation and modeling label correlation . At first ,   CNN ( Kim , 2014 ; Kurata et al . , 2016 ) and RNN-   based ( Liu et al . , 2016 ) models were used to capture   local and long - distance text dependencies . Besides ,   Xiao et al . ( 2019 ) proposed a label - specific atten-   tion network to focus on different tokens when   predicting each label . The sequence generation   model ( Yang et al . , 2018 ) and iterative reasoning   mechanism ( Wang et al . , 2021 ) were utilized to   model the label correlation . Furthermore , Ma et al .   ( 2021 ) adopted graph neural networks based on   label graphs . However , these methods are unable   to refer to the existing instances that can guide the   model to make better predictions .   Nearest Neighbor Methods in NLP Nearest   neighbor methods have achieved great success in   many NLP tasks such as language modeling ( Khan-   delwal et al . , 2020 ) and machine translation ( Khan-   delwal et al . , 2021 ; Zheng et al . , 2021 ; Lin et al . ,   2021 ; Su et al . , 2015 ) . These methods utilize kNN   retrieval in the inference stage based on context   representation vectors which are generated by a   converged model . Zheng et al . ( 2021 ) pointed out   that simple application of the kNN method tends   to introduce noise and we also found this issue   in MLTC . Therefore , we design a multi - label con-   trastive learning objective to improve the quality of   the retrieved neighbors .   3 Proposed Method   In this section , we introduce our proposed method   in detail . As depicted in Fig . 1 , we design a k   nearest neighbor mechanism for MLTC ( Step 2 , 3 )   and enhance it by training the model with a multi-   label contrastive learning objective ( Step 1 ) .   3.1 Problem Formulation   LetD={(x , y)}be the MLTC training set   consisting of Ninstances . Each xis a text and673y‚àà { 0,1}denotes the corresponding multi - hot   label vector where Lis the total number of labels .   The target of MLTC is to learn the mapping from   the input text to the relevant labels .   3.2 Nearest Neighbor MLTC   To obtain knowledge from existing instances during   inference , we propose a knearest neighbor mecha-   nism for MLTC including two steps : constructing a   datastore of training instances ( Step 2 ) and making   thekNN prediction based on it ( Step 3 ) .   Datastore Construction Given an instance from   the training set ( x , y)‚ààD , the text representation   vector h = f(x)can be generated by an MLTC   model . Then the multidimensional datastore D   can be constructed offline by a single forward pass   over each training instance : D={(h , y ) } .   Prediction In the inference stage , given an in-   put text x , the model outputs the prediction vec-   torÀÜy‚àà { p|p‚àà[0,1 ] } . The model also out-   puts the text representation f(x ) , which is uti-   lized to query the datastore Daccording to the   euclidean distance to obtain the knearest neigh-   bors : N={(h , y ) } . Then the kNN predic-   tion can be made by :   ÀÜy = XŒ±y , Œ± = e   Pe(1 )   where d(¬∑,¬∑)indicates the euclidean distance , œÑis   thekNN temperature , and Œ±denotes the weight of   thei - th neighbor . Intuitively , the closer a neighbor   is to the test instance , the larger its weight is . The   final prediction is calculated as the combination   of the base model output and the kNN prediction :   ÀÜy = ŒªÀÜy+(1‚àíŒª)ÀÜywhere Œªis the proportion   parameter .   3.3 Multi - Label Contrastive Learning   In MLTC , a model is usually trained by supervised   learning with the binary cross - entropy ( BCE ) loss   which is unaware of the kNN retrieval process . In   consequence , retrieved neighbors may not have   similar labels to the test instance and provide little   help for the prediction . To fill this gap , we propose   to train the model with a multi - label contrastive   learning objective .   Existing supervised contrastive learning meth-   ods tried to narrow distances between instances   from the same class and push away those from dif-   ferent classes . However , in MLTC , two instancesmay share some common labels while there may   also be some labels that are unique to each in-   stance . How to handle these cases is the key to   utilizing contrastive learning in MLTC . Therefore ,   to model complex correlations among the multi-   label instances , we design a dynamic coefficient   based on the label similarity .   Considering a data minibatch of size b , we define   a function to output all the other instances for a spe-   cific instance i : g(i ) = { k|k‚àà { 1,2 , ¬∑ ¬∑ ¬∑ , b } , kÃ∏=   i } . The contrastive loss for each instance pair ( i , j )   can be calculated as :   L=‚àíŒ≤loge   Pe(2 )   C = y¬∑y , Œ≤ = CPC(3 )   where d(¬∑,¬∑)is the euclidean distance , œÑis the   contrastive learning temperature and z = f(x )   denotes the text representation . Cdenotes the   label similarity between i , jwhich is computed by   the dot product of their label vectors . The dynamic   coefficient Œ≤is the normalization of C.   The contrastive loss for the whole minibatch is   the summation over all the instance pairs : L = PPL . For a pair of instances ( i , j ) , the   greater label similarity Cwill bring larger coeffi-   cientŒ≤ , thereby increasing the value of their loss   termL. As a result , their distance d(z , z)will   be optimized to be closer . Meanwhile , if they have   no shared labels ( Œ≤ = C= 0 ) , then the value   ofLis also zero and their distance d(z , z)will   only appear in the denominators of other terms .   Consequently , their distance will have negative gra-   dients and be optimized to become far .   Denoting BCE loss as L , the overall training   loss of our method is : L = L+Œ≥L. The   parameter Œ≥controls the trade - off between losses .   Dataset I L L W   AAPD 55,840 54 2.4 163   RCV1 - V2 804,414 103 3.2 124   4 Experiments   In this section , we conduct multiple experiments to   evaluate the efficacy of our method . Implementa-674tion details and the overhead of our method can be   found in Appendix A and B respectively .   4.1 Settings   Datasets To evaluate our method , we conduct ex-   periments on two benchmark datasets AAPD ( Yang   et al . , 2018 ) and RCV1 - V2 ( Lewis et al . , 2004 ) .   The dataset statistics are listed in Tab . 2 .   Evaluation Metrics Following the previous   work ( Yang et al . , 2018 ) , we adopt hamming loss   and micro - F1 score as our evaluation metrics .   Baseline We adopt the following models as our   baselines and apply our method to all of them :   CNN ( Kim , 2014 ) uses multiple convolutional   kernels to extract local text representations .   LDGN ( Ma et al . , 2021 ) is the state - of - the - art   non - pretrained MLTC model . It is based on the   label - wise attention network and a GCN .   BERT ( Devlin et al . , 2019 ) is a Transformer-   based pretrained language model . Its [ CLS ] repre-   sentation is used to do the classification .   ModelsAAPD RCV1 - V2   HL(- ) F1(+ ) HL(- ) F1(+ )   CNN 0.02378 69.60 0.00946 83.76   + ours 0.02248 71.69 0.00824 86.14   LDGN 0.02478 70.59 0.00863 86.00   + ours 0.02296 71.38 0.00768 87.29   BERT 0.02257 74.03 0.00766 87.54   + ours 0.02167 75.18 0.00715 88.36   4.2 Results   Main Experiments As shown in Tab . 3 , our   method can bring consistent and considerable per-   formance improvements to all of the models . For   example , our method has improved the micro - F1 of   CNN by 2.09 % on AAPD and 2.38 % on RCV1 - V2   respectively . Moreover , both the state - of - the - art   LDGN and powerful BERT can still benefit a lot   from our method . Specifically , when equipped withModels AAPD RCV1 - V2   CNN 69.60 83.76   CNN+ kNN 70.19 85.21   CNN+CL 69.43 83.84   CNN+CL+ kNN 71.69 86.14   LDGN 70.59 86.00   LDGN+ kNN 70.73 86.76   LDGN+CL 70.44 86.51   LDGN+CL+ kNN 71.38 87.29   BERT 74.03 87.54   BERT+ kNN 74.22 87.84   BERT+CL 73.85 87.74   BERT+CL+ kNN 75.18 88.36   our method , the non - pretrained model LDGN ob-   tains competitive performances compared to the   pretrained model BERT on the larger RCV1 - V2 .   Ablation Test As mentioned above , our method   consists of a knearest neighbor mechanism ( de-   noted as kNN ) and a multi - label contrastive learn-   ing objective ( denoted as CL ) . We demonstrate the   effect of each component via an ablation test .   As shown in Tab . 4 , the kNN mechanism can   consistently improve the performance of the base   models . Moreover , when equipped with our con-   trastive learning loss , although performances of the   base models remain consistent , the improvements   brought by the kNN mechanism have increased by   a large margin . This verifies that our CL objective   does effectively enhance the kNN mechanism .   ModelsAAPD RCV1 - V2   w / oŒ≤ w / Œ≤ w / oŒ≤ w / Œ≤   CNN 71.19 71.69 85.27 86.14   LDGN 71.06 71.38 86.78 87.29   BERT 74.66 75.18 88.08 88.36   Analysis of Dynamic Coefficient In existing CL   methods , two instances are either positive or nega-   tive for each other . To model more fine - grained sim-   ilarity between instances , we proposed a dynamic   coefficient Œ≤for each CL loss term ( see Eq . 2,3).675   To verify the necessity of Œ≤ , we also apply the sim-   ple extension of existing CL methods to MLTC .   As shown in Tab . 5 , our method outperforms the   simple extension method in all cases , which veri-   fies the necessity of considering the fine - grained   similarity between multi - label instances .   Analysis of kNN Paramters Here we conduct a   parameter analysis of our kNN mechanism on the   RCV1 - V2 dataset . As shown in Fig . 2(a ) , for all   the models , the performance improves at first and   then decreases as the k increases . Moreover , when   referring to neighbor instances ( k > 0 ) , the perfor-   mance is always better than only using the model   output ( k= 0 ) , which verifies the necessity of uti-   lizing the knowledge from the existing instances .   Fig . 2(b ) demonstrates the trend of model perfor-   mance with Œª . In general , the trend is similar to   that of kwhich further confirms that only using the   model prediction ( Œª= 0 ) is sub - optimal . It ‚Äôs worth   noting that on the BERT model , completely using   neighbors ‚Äô prediction ( Œª= 1 ) is highly competitive   compared to the uniform combination ( Œª= 0.5 )   which performs the best on the other base models .   Impact of Contrastive Learning To further ana-   lyze the impact of our contrastive learning objec-   tive , for each test instance , we count the average   proportion of shared labels to all labels brought by   its nearest neighbors . As shown in Tab . 6 , after   training the model with contrastive learning , the   retrieved instances contain more shared labels with   the test instance , which further proves that CL doesModelsAAPD RCV1 - V2   w/o CL w/ CL w/o CL w/ CL   CNN 64.5 65.5 82.7 84.2   LDGN 63.1 64.2 84.4 84.9   BERT 67.8 68.5 85.5 86.4   improve the quality of the retrieved neighbors . An   intuitive example can be found in Appendix C.   5 Conclusion   In this paper , we proposed a knearest neighbor   mechanism along with a multi - label contrastive   learning objective for MLTC . Extensive experi-   ments verified the effectiveness of our method and   revealed the source of performance improvements   our method brings . For future work , we will ex-   plore how to improve the performance of MLTC   models directly with contrastive learning .   Acknowledgements   We would like to thank the anonymous reviewers   for their constructive comments . This work was   supported by NSFC Projects ( Nos . 61936012 and   61976114 ) , the National Key RD Program of China   ( No . 2018YFB1005102 ) .   References676677A Implementation Details   We implement all the methods relying on the Py-   Torch library . We also use Faiss ( Johnson et al . ,   2021 ) for fast nearest neighbor search . For CNN   and BERT , we directly use the representations from   the last hidden layer to construct the datastore . As   for the LDGN which generates label - specific text   representations , we perform a max - pooling opera-   tion on all the lvectors to get the single representa-   tion vector .   We train all the models on both datasets up to 30   epochs with an early stop of 3 patience and use the   Adam optimizer with a learning rate of 1√ó10 .   For all the models on AAPD , we use a batch size   of128 . On RCV1 - V2 , we use a batch size of 512   for CNN and LDGN , and 128for BERT due to its   huge memory usage . As for the hyperparameters   of our proposed method , Œª= 0.5 , œÑ= 1 , œÑ=   10are adopted for all the cases . Besides , we use   k= 5 , Œ≥= 0.1for all the models on AAPD and   k= 10 , Œ≥= 0.01for those on RCV1 - V2 .   Models AAPD RCV1 - V2   CNN 0.09 GB 1.46 GB   LDGN 0.11 GB 1.84 GB   BERT 0.17 GB 2.60 GB   ModelsAAPD RCV1 - V2   w / okNN w/ kNN w / okNN w/ kNN   CNN 3.18 3.25 2.89 6.17   LDGN 5.47 7.29 7.61 9.67   BERT 264.89 267.57 265.96 270.73   B Space and Time Overhead   In the training stage , the overhead of contrastive   learning is negligible compared to supervised learn-   ing , so we do not report it here . Most of the over-   head lies in the kNN classifier . The disk usage of   each datastore is shown in Tab . 7 . The inference   time per text of different models with or without   thekNN prediction on each dataset is listed in678   Tab . 8 . It ‚Äôs worth noting that the extra inference   time brought by our method does not exceed 5ms   in all cases .   C Case Study : TSNE Visualization   In Fig . 3 , we use the TSNE visualization tool to   plot the CNN representations of a test instance and   its 80 nearest neighbors with or without our CL   objective . We use different marks to plot neigh-   bors with different label similarities ( Cin Eq . 3 )   to the test instance . As demonstrated in the left   part , without contrastive learning , most of the near-   est neighbors have only the similarity of 1 ( green   crosses ) . However , in the right part , with our CL   objective , the test instance is surrounded by neigh-   bors which have a high label similarity of 2 ( blue   circles ) . This confirms that our CL objective does   improve the quality of the retrieved neighbors .   D Analyzing the Proportion of   Contrastive Learning   In this section , we analyze how the proportion of   contrastive learning affects the performance of ourmethod . As shown in Fig . 4 , when trained with   the contrastive learning objective ( Œ≥ > 0 ) , the per-   formance of our method is better than that without   contrastive learning ( Œ≥= 0 ) in most cases . How-   ever , when training the BERT model , too high pro-   portion of contrastive learning ( Œ≥= 1 ) even hurts   the performance . Besides , different base models   have the different Œ≥values for their optimal per-   formance , which indicates that the proportion of   contrastive learning to the overall training objec-   tive is crucial to the performance and varies with   different model structures.679   Chuhan WuFangzhao WuTao QiYongfeng HuangDepartment of Electronic Engineering , Tsinghua University , Beijing 100084 , ChinaMicrosoft Research Asia , Beijing 100080 , China   { wuchuhan15 , wufangzhao , taoqi.qt } @gmail.com   yfhuang@tsinghua.edu.cn   Abstract   1 Introduction   In recent years , pretrained language models   ( PLMs ) have achieved huge success in NLP ( Qiu   et al . , 2020 ) . Many PLMs such as BERT ( De-   vlin et al . , 2019 ) , RoBERTa ( Liu et al . , 2019 )   and UniLM ( Dong et al . , 2019 ) which are pre-   trained from large - scale unlabeled corpus in a self-   supervised way , have significantly improve vari-   ous downstream tasks such as reading comprehen-   sion ( Xu et al . , 2019 ) , machine translation ( Brown   et al . , 2020 ) , text classification ( Bao et al . , 2020 ) ,   dialog ( Wu et al . , 2020 ) and recommendation ( Wu   et al . , 2021 ) by finetuning on these tasks .   How to effectively finetune PLMs to better em-   power downstream tasks is an important research   problem ( Zheng et al . , 2021 ) . Many existing NLP   methods usually directly finetune PLMs with the   labeled data in downstream tasks ( Sun et al . , 2019 ) .   Only a few works explore more effective and ro-   bust PLM finetuning methods ( Chen et al . , 2020 ;   Lee et al . , 2020 ; Aghajanyan et al . , 2021 ; Zhang   et al . , 2021 ; Xu et al . , 2021 ) . For example , Chen   et al . ( 2020 ) proposed RecAdam that adds a penalty   item to minimize the Ldistance between the fine-   tuned models and the pretrained models , where   the penalty intensity is time - variant during fine-   tuning . Lee et al . ( 2020 ) proposed Mixout which   randomly replaces part of the parameters in the   finetuned model with their original weights in the   PLMs . These PLM finetuning methods mainly   focus on preventing PLMs from overfitting the lim-   ited labeled data in downstream tasks . Besides the   overfitting of downstream task data , a rarely stud-   ied problem is that the PLMs usually overfit the   pretraining tasks and data ( Qi et al . , 2020 ) , which   may have significant gap with the downstream task   and data . It is not easy for existing PLM finetun-   ing methods to overcome such gap ( Roberts et al . ,   2020 ) , which may lead to suboptimal performance   especially when labeled data in downstream tasks   is insufficient .   In order to handle this problem , in this paper   we propose a very simple yet effective method   named NoisyTune , which can help better finetune   PLMs for downstream tasks . Different from the680standard finetuning paradigm ( Fig . 1 ( a ) ) which   directly finetunes PLMs on the downstream task   data , the key idea of NoisyTune is to add a small   amount of noise to perturb PLMs parameters before   finetuning ( Fig . 1 ( b ) ) . It can help prevent PLMs   from overfitting the tasks and data in the pretrain-   ing stage , and reduce the gap between pretraining   and downstream tasks . Since PLMs have different   types of parameters which usually own different   characteristics , in NoisyTune we use a matrix - wise   perturbing method that adds uniform noise with   different intensities to different parameter matri-   ces according to their standard deviations for bet-   ter adaptation . We conduct extensive experiments   on two widely used NLP benchmarks , namely ,   GLUE ( Wang et al . , 2018 ) for English language   understanding and XTREME ( Hu et al . , 2020 ) for   multilingual language understanding . The results   show NoisyTune can empower the finetuning of dif-   ferent PLMs on many different downstream NLP   tasks to consistently achieve better performance . In   addition , the results show NoisyTune can be eas-   ily combined with many existing PLM finetuning   methods and further improve their performance .   2 NoisyTune   The goal of NoisyTune is for more effective finetun-   ing of PLMs on downstream tasks . The motivation   ofNoisyTune is that PLMs are well pretrained on   some unlabeled corpus with some self - supervision   tasks , and they may overfit these pretraining data   and tasks ( Qi et al . , 2020 ) , which usually have gap   with the downstream task and data . It may be diffi-   cult for PLMs to effectively adapt to downstream   tasks especially when labeled data in these tasks   are limited , which is usually the case . Motivated by   the dueling bandits mechanism ( Yue and Joachims ,   2009 ) that adds randomness to the model for explo-   ration , as shown in Fig . 1 , we propose to add some   noise to the parameters of PLMs before finetuning   them on downstream tasks to do some ‚Äú exploration ‚Äù   in parameter space and reduce the risk of overfitting   the pretraining tasks and data .   PLMs usually have different kinds of parameter   matrices , such as query , key , value , and feedfor-   ward network matrices ( Devlin et al . , 2019 ) . Differ-   ent parameter matrices in the PLMs usually have   different characteristics and scales . For example ,   some researchers found that the self - attention pa-   rameters and the feed - forward network parame-   ters in Transformers have very different properties , such as rank and density ( Wang et al . , 2020 ) . Thus ,   adding unified noise to all parameter matrices in   PLMs may not be optimal for keeping their good   model utility . To handle this challenge , we propose   a matrix - wise perturbing method that adds noise   with different intensities to different parameter ma-   trices according to their variances . Denote the pa-   rameter matrices ( or scalars / vectors ) in a PLM as   [ W , W , ... , W ] , where Nis the number of pa-   rameter matrix types . Denote the perturbed version   of the parameter matrix WasÀúW , which is com-   puted as follows :   ÀúW = W+U(‚àíŒª   2,Œª   2)‚àóstd(W),(1 )   where stdstands for standard deviation . The func-   tionU(a , b)represents uniform distribution noise   ranged from atob , and Œªis a hyperparameter   that controls the relative noise intensity . We can   see that in NoisyTune parameters in PLMs with   higher variance will be added with stronger noise .   In addition , in some PLMs there are some con-   stant matrices , such as token type embeddings in   RoBERTa ( Liu et al . , 2019 ) . They will not be per-   turbed because their standard deviation is 0 . It   can ensure that these constant matrices will not be   accidentally activated by additional noise .   NoisyTune is a simple and general plug - and - play   technique that can be applied to the finetuning of   any PLM on any task , simply by inserting the fol-   lowing PyTorch - style code before finetuning : ‚àí   * *   3 Experiments   3.1 Datasets and Experimental Settings   We conduct extensive experiments on two widely   used benchmarks for PLM evaluation . The first one   is GLUE ( Wang et al . , 2018 ) , which is a benchmark   for English language understanding that contains   different tasks like natural language inference , sen-   timent analysis and sentence similarity evaluation .   The second one is XTREME ( Hu et al . , 2020 ) ,   which is a benchmark for multilingual language   understanding . It covers 40 languages and contains681four groups of tasks , including sentence classifica-   tion , structured prediction , sentence retrieval and   question answering . More details of these bench-   marks can refer to their original papers and official   websites . Since the test labels of GLUE are not   released , following ( Bao et al . , 2020 ) we report re-   sults on the dev set of GLUE . The XTREME results   are evaluated on the test set . The hyperparameter   Œªis 0.15 on GLUE and is 0.1 on XTREME . The   searching range of hyperparameters in our work   are listed in Table 1 .   Following ( Zheng et al . , 2021 ) , in sentence re-   trieval tasks we first train the models on the XNLI   dataset , and then use the average of token repre-   sentations produced by the hidden layer that yields   the best performance . In order not to harm the   alignment of token embeddings across different   languages , we do not add noise to the token em-   beddings in multilingual PLMs . We repeat exper-   iments 5 times with different random seeds and   report the average scores .   3.2 Performance Evaluation   On the GLUE benchmark , we compare the perfor-   mance of directly finetuning the base version of   BERT ( Devlin et al . , 2019 ) , XLNET ( Yang et al . ,   2019 ) , RoBERTa ( Liu et al . , 2019 ) and ELEC-   TRA ( Clark et al . , 2020 ) with that of finetuning   them after applying NoisyTune . On the XTREME   benchmark , we compare the performance of di-   rectly finetuning both base and large versions of   XLM - R ( Conneau et al . , 2020 ) with that of their   variants obtained by applying NoisyTune . The   results on these two benchmarks are shown in   Tables 2 and 3 , respectively . On the XTREME   datasets , we report two types of results . The first   one is zero - shot crosslingual transfer from English   to other languages , and the second one is learning   models on both English and translated data .   According to these results , NoisyTune can consis-   tently improve the performance of different PLMs   on different tasks in both English and multilingual   settings . In addition , the performance improvementbrought by NoisyTune is usually larger on relatively   small datasets ( e.g. , RTE , CoLA and WNLI ) . These   results indicate that when labeled data in down-   stream tasks is insufficient , it is quite difficult to   effectively finetune PLMs starting from the origi-   nal parameters which usually overfit the pretraining   tasks and data . The experimental results validate   thatNoisyTune can properly perturb PLMs with a   little noise to explore different parameter spaces   and reduce the overfitting problem , making PLMs   easier to be adapted to downstream tasks .   3.3 Which Noise to Use and How ?   In this section we study which kind of noise is   more suitable for NoisyTune . In addition , we ex-   plore whether our proposed matrix - wise perturb-   ing method is better than using a unified global   noise for all model parameters in PLMs . We com-   pare five methods , including ( 1 ) NoisyTune without   any noise ; ( 2 ) NoisyTune with a global Gaussian   noise ; ( 3 ) NoisyTune with a global uniform noise ;   ( 4)NoisyTune with matrix - wise Gaussian noise ; ( 5 )   NoisyTune with matrix - wise uniform noise . The re-   sults on GLUE are shown in Fig . 2 , and the results   on XTREME show similar patterns . We find that   adding global noise with the same distribution to   all the PLM parameters will harm the model perfor-   mance . This is because different parameter matri-   ces in PLMs have very different distributions and   characteristics ( Wang et al . , 2020 ) . Simply adding   a unified global noise to all the parameter matrices   is not optimal . The results show that matrix - wise   noise is a much better choice , since the different   characteristics of different parameter matrices can   be taken into consideration . In addition , we find an   interesting phenomenon that adding uniform noise   is better than Gaussian noise . This may be because   Gaussian noise has wider ranges and some extreme   values may affect the model performance . Thus ,   we use matrix - wise uniform noise in NoisyTune .   3.4 Combination with Existing PLM   Finetuning Methods   From Fig . 1 , it is very clear that NoisyTune is in-   dependent of the specific PLM finetuning method ,   since it is applied at the stage before finetuning   PLM on the task - specific data . Thus , it is very   easy to combine NoisyTune with any kind of ex-   isting PLM finetuning method . In this section , we   explore whether NoisyTune has the potential to em-   power the existing PLM finetuning techniques to   achieve better performance . Here we select two682   well - known PLM finetuning for experiments , i.e. ,   RecAdam ( Chen et al . , 2020 ) and Mixout ( Lee   et al . , 2020 ) . The experimental results are summa-   rized in Fig . 3 . We find that combining NoisyTune   with existing PLM finetuning techniques can fur-   ther improve their performance . This is because   NoisyTune aims to address the overfitting of pre-   training signals while these methods aim to prevent   overfitting in downstream tasks . Thus , NoisyTune   and these PLM finetuning methods are complemen-   tary , and they can be empowered by NoisyTune to   achieve better performance .   3.5 Empirical Analysis of NoisyTune   Next , we empirically analyze why NoisyTune can   help PLM finetuning . We compare the accuracy   of BERT with and without NoisyTune finetuned   with different percentage of samples on the MRPC   dataset . The results are shown in Fig . 4 . We   findNoisyTune can consistently improve PLMs un-   der different amounts of data , especially when less   training data is used . This is because the perturbed   PLMs may have lower risks of overfitting the pre-   training tasks and have better generalization abil-   ities , which is especially beneficial for finetuning   PLMs on downstream task with limited data .   To further study the impact of NoisyTune on   PLM finetuning , we show the relative changes of   the L - norms of different kinds of parameters in   the BERT model during finetuning on the MRPC   dataset in Fig . 5.Since the noise we added to   PLMs in NoisyTune is zero - mean uniform noise ,   the absolute parameter L - norm will not change   too much . However , we can see that the relative   change of L - norms becomes smaller when Noisy-   Tune is applied , which indicates that the PLMs can   find the ( sub)optimal parameters for downstream683   tasks more easily . This result validates directly fine-   tuning PLMs may need more updates to adapt to   downstream tasks , which is due to the overfitting of   pretraining tasks , and NoisyTune can provide a sim-   ple way to alleviate this problem and help finetune   PLMs on downstream tasks more effectively .   3.6 Hyperparameter Analysis   We study the influence of the most important hy-   perparameter in NoisyTune , i.e. ,Œª , which controls   the relative noise intensity . The average GLUE   scores w.r.t . different Œªvalues are shown in Fig . 6 .   We find that when Œªis too small or too large , the   performance is not optimal . This is because when   Œªis too small , it is difficult for PLMs to do param-   eter space exploration and overcome the overfitting   problem . While when Œªis too large , the useful pre-   trained knowledge in PLMs may be overwhelmed   by random noise . Values between 0.1 and 0.15 are   more suitable for NoisyTune on the GLUE datasets .   4 Conclusion   In this paper , we propose a very simple but effective   method named NoisyTune , which can help better   finetune PLMs on downstream tasks by adding a   little noise to them before finetuning . In NoisyTune ,   we propose a matrix - wise perturbing method that   adds noise with different intensities to different   kinds of parameter matrices in PLMs according   to their variances . NoisyTune is a very general   method , and is PLM model agnostic , downstream   task agnostic , and finetuning method agnostic . Ex-   tensive experiments on both monolingual GLUE   benchmark and multilingual XTREME benchmark   demonstrate NoisyTune can consistently empower   the finetuning of different PLMs on various down-   stream tasks to achieve better performance .   Acknowledgments   This work was supported by the National Natural   Science Foundation of China under Grant num-   bers U1936216 , U1936208 , and 61862002 , and   the research initiation project of Zhejiang Lab ( No .   2020LC0PI01).684References685   Xin Sun Houfeng Wang   MOE Key Lab of Computational Linguistics , School of Computer Science , Peking University   { sunx5,wanghf}@pku.edu.cn   Abstract   1 Introduction   Modern writing assistance applications ( e.g. , Mi-   crosoft Office Word , Google Docsand Gram-   marly ) always contain Grammatical Error Correc-   tion ( GEC ) modules ( Ge et al . , 2018 ; Omelianchuk   et al . , 2020 ; Stahlberg and Kumar , 2021 ) to cor-   rect errors in user - entered sentences . Such appli-   cations usually require GEC models to perform   different correction tendencies and behaviors ac-   cording to practical scenarios and user preferences   ( Chen et al . , 2020 ) . For instance , as shown in Ta-   ble 1 , conservative GEC models provide precise   corrections with high confidence and avoid unnec-   essary edits for better user experience . In contrast ,   aggressive GEC models could provide more cor-   rection candidates to users or a following decision   system for further measurement .   Although recent studies witness the tremen-   dous success of sequence - to - sequence ( seq2seq )   generation approaches in GEC , the trade - off of   these two tendencies still largely depends on the   pre - defined model architecture , training data and   labor - consuming post - processing ( Liang et al . ,   2020 ) . Hotate et al . ( 2020 ) proposes a diverse   local beam search method to obtain diverse cor-   rections but is specifically designed for copy-   augmented GEC models and can not perform   precision - oriented decoding . Instead of seq2seq   generation , Omelianchuk et al . ( 2020 ) proposes an   efficient sequence tagger for GEC by token - level   transformations to map input tokens to target cor-   rections . They introduce two confidence thresholds   for inference to force the model to perform more   precise corrections . Chen et al . ( 2020 ) first identi-   fies incorrect spans with a tagging model and then   sets a probability threshold to adjust the precision-   recall trade - off .   Inspired by these lightweight tweaking meth-   ods for sequence labeling approaches , we pro-   pose a simple yet effective counterpart ‚Äì Align-686   and - Predict Decoding ( APD ) for the seq2seq GEC   models . Our approach could not only adapt the   precision - recall trade - off of a single seq2seq GEC   model to various application scenarios , but also be   used as a simple trick to improve its overall F   performance .   During inference , APD aligns the already gener-   ated sequence with the input tokens to specify the   position which the model has reached . By tweak-   ing the score of the next token , the model changes   its preference between copy and edit operation ,   leading to a different degree of adherence to the   input sentence . The experimental results in both   English and Chinese GEC benchmarks show our   approach could effectively control the precision-   recall trade - off and achieve state - of - the - art results .   Our contributions are summarized as follows :   ‚Ä¢We propose a novel and simple decoding ap-   proach , allowing us to adapt the precision-   recall trade - off of a seq2seq GEC model .   ‚Ä¢Our methods achieve state - of - the - art results in   both English and Chinese GEC benchmarks .   2 Align - and - Predict Decoding   Beam search ( Lowerre , 1976 ; Och and Ney , 2004 ;   Sutskever et al . , 2014 ) is a widely used algorithm   for decoding sequences on all generation tasks ,   such as translation ( Vaswani et al . , 2017 ; Ott et al . ,2018 ) , dialogue ( Kulikov et al . , 2019 ) , etc . Multi-   ple modifications to beam search that force the out-   puts to include pre - defined lexical constraints ( i.e. ,   words and phrases ) have been proposed ( Hokamp   and Liu , 2017 ; Hu et al . , 2019 ) .   Fortunately , the input and output sentences of   GEC overlap significantly and the input tokens are   natural constraints for correction generation . This   assumption is an objective characteristic of GEC   and has been made in many previous works ( Zhao   et al . , 2019 ; Malmi et al . , 2019 ; Stahlberg and Ku-   mar , 2020 ; Sun et al . , 2021 ) . Thus , we propose a   novel decoding approach ‚Äì Align - and - Predict De-   coding ( APD ) , which leverage the characteristic of   GEC to adjust behavior and tendencies of inference .   The overview of APD is shown in Figure 1 .   Given an input sentence x= ( x , . . . , x ) , we   maintain Khypotheses at the time step tduring   inference as beam search does :   H=   h , ... , h 	  =    ( y , ... , y ) , ... , ( y , ... , y ) 	  ( 1 )   where h , i‚àà[1 , K]denotes the i - th hypothesis   withtalready generated tokens .   Since the output of GEC is highly constrained   by the input sequence , we assume that hshould   be almost the same as part of the input sentence   x. Then , we match the suffix of each hypothesis   hwith the input xto identify the position which   the inference has reached . If there exists a unique687substring x , ... , x(j‚â•0)of the input xiden-   tical to the suffix y , ... , y , the next token of the   hypothesis his very likely to be x , which we   store in the set N.Formally ,   N= (   { x } ‚àÉ ! k , x = y ;   ‚àÖ otherwise.(2 )   As beam search does , we expand current hy-   potheses and construct possible candidates for the   next time step t+ 1with all tokens in the vocab-   ulary . The candidate ÀÜhof the i - th hypothesis is   obtained as follows :   ÀÜh = C(h , v ) = ( y , ... , y , v ) ( 3 )   where we concatenate the already generated se-   quence hwith any token vin the vocabulary .   The corresponding score is calculated by :   S ( ÀÜh ) = S ( h )   + w¬∑logP(v|y , ... , y)(4 )   where Pis the output distribution predicted by the   seq2seq GEC model and wis a penalty factor   that depends on whether the token vis the next   token xat the aligned position . Specifically ,   w= (   Œª v ‚ààN   1.0vÃ∏‚ààN(5 )   where Œªis a hyperparameter to control the adher-   ence to the input sequence . If Œª > 1.0 , inference   penalizes the score of the original next token and   tends to perform modification;ifŒª < 1.0 , it is   likely to copy the token . The new hypotheses are   selected by :   H= arg topK(S ( ÀÜh ) ) ( 6 )   3 Experiments   3.1 Experimental Setting   We conduct our experiments in the restricted train-   ing setting of BEA-2019 GEC shared task ( Bryant   et al . , 2019 ) , with Lang-8 Corpus of Learner En-   glish ( Mizumoto et al . , 2011 ) , NUCLE ( Dahlmeier   et al . , 2013 ) , FCE ( Yannakoudakis et al . , 2011 ) and   W&I+LOCNESS ( Granger ; Bryant et al . , 2019 )   as training data . We use BEA-2019 development   set to choose the best model and select Œªbetween   0.1and2.5with0.05intervals based on F , F   andFfor precision - oriented , balance and recall-   oriented models , respectively . We evaluate the   performance on BEA-2019 test set by ERRANT   ( Bryant et al . , 2017 ) .   To validate the effectiveness of our approach for   the state - of - the - art seq2seq GEC models , we follow   previous work ( Grundkiewicz et al . , 2019 ; Zhang   et al . , 2019 ) to construct 300 M error - corrected sen-   tence pairs in the same way for pretraining . We   use Transformer ( big ) model ( Vaswani et al . , 2017 )   in the fairseqand a vocabulary with size of 32 K   Byte Pair Encoding ( Sennrich et al . , 2016 ) tokens .   We also use one of the models trained by the prior   work ( Sun et al . , 2021 ) which utilizes a pretrained   model BART ( Lewis et al . , 2019 ) to initialize a   GEC model which has a 12 - layer encoder and 2 - 688   layer decoder , following Li et al . ( 2021 ) .   In addition , we evaluate our approach on   NLPCC-18 Chinese GEC shared task ( Zhao et al . ,   2018 ) by official Max - Match scorerto prove our   approach is language - independent . We use a base   Transformer model and construct a character - level   vocabulary consisting of 7 K tokens . We train the   model using MaskGEC ( Zhao and Wang , 2020 ) .   The models decode with a beam size of 5 . We   show more details of training in the Appendix .   3.2 Experimental Result   As shown in Table 2 , our approach can control   the precision - recall trade - off of inference for any   seq2seq GEC models by tweaking a single hyperpa-   rameter Œª . After inference tweaks , pretrained GEC   models could achieve much better precision with   comparable or even better overall performance . For   instance , our approach increases the precision of   pretrained models by over 10 points . In contrast ,   the recall improvement is smaller than precision ,   i.e. , an increment of about 6 points for pretrained   models , since it depends mainly on error - corrected   patterns that the model itself has learned . The final   system has achieved competitive performance ( 73.8   F ) and align - and - predict decoding improves it   to a new state - of - the - art result ‚Äì 75.0 Fin the   BEA-2019 test set by a slight tendency towards   precision .   We further look into the performance of the pre-   trained seq2seq model over varying Œªin BEA-2019   development set , which is shown in Figure 2 . It is   obvious that the conservative inference ( Œª < 1.0 )   with fewer edits tends to achieve higher precision   since it only provides the most confident correc-   tions , while recall of aggressive inference ( Œª > 1.0 )   has an upper bound . This is because the motivation   of our approach is to simply display error - corrected   patterns that the model has learned with different   orientation rather than to improve its capability and   complement more patterns . Meanwhile , it is ob-   served that Fdoes not peak around Œª= 1.0 ,   which makes it possible to adapt the precision-   recall trade - off for better overall performance .   As shown in Table 3 , our approach also performs   well in Chinese GEC , which demonstrates that it is   language - independent . We present concrete exam-689ples with different Œªin our validation set in Table 4 .   It is consistent with our intuition that with larger Œª ,   the inference tends to heavily edit the input tokens ;   on the other hand , it adheres to the input sequence   with smaller Œª .   4 Conclusion   We propose a novel language - independent decod-   ing approach to offer more flexibility to adjust the   precision - recall trade - off of inference for seq2seq   GEC models , making it adaptive to various real-   world application scenarios . It can not only adapt   a single model to precision - oriented and recall-   oriented inference , but also be used as a simple   trick for better overall performance . On both En-   glish and Chinese GEC benchmarks , our approach   further improves the state - of - the - art seq2seq GEC   model by precision - recall trade - off . In the future ,   we plan to apply it to other sentence rewriting tasks ,   such as paraphrasing and style transfer .   Acknowledgments   We thank all the reviewers for their valuable   comments to improve our paper . We thank Tao   Ge in Microsoft Research Asia for the valu-   able suggestions . The work is supported by Na-   tional Natural Science Foundation of China un-   der Grant No.62036001 and PKU - Baidu Fund   ( No.2020BD021 ) . The corresponding author of   this paper is Houfeng Wang .   References690691   A Hyper - parameters   The hyper - parameters for Chinese GEC are listed   in Table 5 . The hyper - parameters of training the   models for English GEC are listed in Table 6 and   Table 7.692B Efficiency   Table 8 shows the total latency of the seq2seq   model ( w/ pretraining ) under various batch sizes .   Our approach incurs about 5 % extra latency in the   online inference setting ( i.e. , batch size=1 ) and is   suitable for practical GEC systems.693   Lan ZhangWray BuntineEhsan ShareghiDepartment of Data Science & AI , Monash UniversityCollege of Eng . and Comp . Sc . , VinUniversityLanguage Technology Lab , University of Cambridge   { lan.zhang , wray.buntine , ehsan.shareghi}@monash.edu   Abstract   1 Introduction   In recent years , with the success facilitated by pre-   trained representations across various NLP tasks ,   more attention has been placed on studying and   utilising the geometric properties of learned rep-   resentations . A phenomena that has been studied   more recently in this direction is anisotropy ( Etha-   yarajh , 2019 ) , indicating a sub - optimal property   where the learned embeddings only utilise a small   subset of the representation space . Various methods   have been proposed to rectify this and encourage   the representations to be more discriminative or to   exploit the representation dimensions more effec-   tively ( Liu et al . , 2021 ; Gao et al . , 2021 ; Li et al . ,   2020a ; Su et al . , 2021 ; Mu and Viswanath , 2018 ) .   In parallel , Variational Autoen-   coders ( V AEs ) ( Kingma and Welling , 2014 )   have been widely used in various areas of NLP , from representation learning for downstream   tasks ( Li et al . , 2020b ; Wei and Deng , 2017 ) , to   generation ( Prokhorov et al . , 2019 ; Bowman et al . ,   2016 ) , representational sparsity and disentangle-   ment ( Prokhorov et al . , 2021 ; Zhang et al . , 2021 ) ,   and semi - supervised learning ( Zhu et al . , 2021 ;   Choi et al . , 2019 ; Yin et al . , 2018 ; Xu et al . , 2017 ) .   In recent years , most of the developments around   V AEs have focused on avoiding the posterior   collapse ( Bowman et al . , 2016 ) which leads to   learning sub - optimal representations ( Havrylov   and Titov , 2020 ; Fu et al . , 2019 ; Li et al . , 2019 ;   Dieng et al . , 2019 ; He et al . , 2019 ; Higgins   et al . , 2017 ; Yang et al . , 2017 ; Bowman et al . ,   2016 ) . Despite the success of these techniques , a   non - collapsed V AE still utilises the representation   space sub - optimally ( Prokhorov et al . , 2019 ; He   et al . , 2019 ; Burda et al . , 2016 ) , as very commonly   the learned representations do not fully utilise the   latent space to encode information .   In this paper we bridge between the two lines of   research by injection isotropy in the latent space of   V AEs . Such property could be encouraged by us-   ing an Isotropic Gaussian Posterior ( IGP ) which in-   volves a simple modiÔ¨Åcation of V AEs . An Isotropic   Gaussian distribution , N(;I ) , is similar to   vanilla V AE ‚Äôs posterior with the exception that all   dimensions share the same uniÔ¨Åed variance . Tying   the variances would encourage encoder of V AEs to-   wards the extreme where alldimensions are either   active or inactive .   Our experimental Ô¨Åndings indicate that , com-   pared to vanilla V AE , the use of IGP is effective in   both increasing dimension activation and injecting   isotropy in the learned representation space . We ob-   serve that isotropy results in a more discriminative   representation space which is much more suited for   classiÔ¨Åcation tasks and robust to input perturbation.694Our generative experiment for sentence completion   suggests that the V AE trained with IGP is more   capable of maintaining semantic cohesiveness .   2 Isotropic Gaussian Posterior ( IGP )   Variational Autoencoder ( V AE ) . Letxdenote   datapoints in data space and zdenote latent vari-   ables in the latent space , and assume the datapoints   are generated by the combination of two random   processes : The Ô¨Årst random process is to sample   a point zfrom the latent space in V AEs with   prior distribution of z , denoted by p(z ) . The sec-   ond random process is to generate a point xfrom   the data space , denoted by p(xjz ) . V AE uses   a combination of a probabilistic encoder q(zjx )   and decoder p(xjz ) , parameterised by  and , to   learn this statistical relationship between xandz .   V AE is trained by maximizing the lower bound of   the logarithmic data distribution logp(x ) , called   evidence lower bound ( ELBO ) , L (  ; ;x ):   E[log(p(xjz))] KL(q(zjx)jjp(z ) )   The Ô¨Årst term of objective function is the expecta-   tion of the logarithm of data likelihood under the   posterior distribution of z. The second term is KL-   divergence , measuring the distance between the   recognition distribution q(zjx)and the prior dis-   tributionp(z)and can be seen as a regularisation .   In the presence of auto - regressive and power-   ful decoders , a common optimisation challenge of   training V AEs in text modelling is called poste-   rior collapse , where the learned posterior distribu-   tionq(zjx ) , collapses to the prior p(z ) . Several   strategies have been proposed to alleviate this prob-   lem ( Bowman et al . , 2016 ; Havrylov and Titov ,   2020 ; Fu et al . , 2019 ; He et al . , 2019 ) . In this work ,   we follow Prokhorov et al . ( 2019 ) , L (  ; ;x ):   E[log(p(xjz))]   jKL(q(zjx)jjp(z)) Cj   whereCis a positive real value which represents   the target KL - divergence term value and  indi-   cates the regularisation strength . We set  = 1   to make sure the weights of the two terms bal-   ance , noting that it acts as a Lagrange Multiplier   ( Boyd and Vandenberghe , 2004 ) . This also has an   information - theoretic interpretation , where the KL   term is seen as the amount of information transmit-   ted from a sender ( encoder ) to a receiver ( decoder )   via the message ( z ) ( Alemi et al . , 2018 ) and the us-   age ofCcan control this channel capacity . This canhelp us to make a fair comparison between Diago-   nal Gaussian Posterior ( DGP ) and IGP when V AEs   are under the same encoder capacity constraint .   V AE with Isotropic Gaussian Posterior . A   common behaviour of V AEs is the presence of in-   active representation units across the entire dataset ,   causing the number of utilised dimensions to be   even far smaller than the number of potential gen-   erative factors behind any real - world dataset . The   soft ellipsoidal representation space of V AEs is   known to lead to less representative mean vec-   tors ( Bosc and Vincent , 2020 ) . We illustrate that   encouraging isotropy ( i.e. , tying the variance of di-   mensions on the posterior ) will avoid the aforemen-   tioned issue since the encoder of V AEs would be   forced to either use all dimensions or none and the   learned latent space is soft spherical . In the Gaus-   sian case , this corresponds to using an Isotropic   Gaussian , a subclass of diagonal Gaussian distribu-   tion   N(;I)j2R;2R 	  , as the poste-   rior . Tying the variances in IGP imposes a different   pathological pattern by encouraging Active Units   ( AUs ; Burda et al . , 2016 ) to reach the maximum   ( i.e. , representation dimension ) .   Additionally , the use of IGP allows the estima-   tion of variance more accurately . Suppose we   haveNsamples with the same posterior . For a   K - dimension diagonal Gaussian posterior , we will   have an estimation of variance with standard devia-   tion approximately ^qfor each dimension k ,   whereas for an isotropic Gaussian posterior , we will   have a uniÔ¨Åed estimation of variance with standard   deviation approximately ^q , where ^and   ^denote the estimates of the variance . Moreover ,   withKdifferent ^estimates , a few may differ   substantially from their best values by chance .   3 Experiments   We trained our models on Yahoo Question   and DBpedia ( Zhang et al . , 2015 ) which have   ( 100K/10K/10 K , 12 K , 10 ) and ( 140K/14K/14 K ,   12 K , 14 ) for ( sentences in training / dev / test , vocab-   ulary size , classes ) , respectively . We use one uni-   directional LSTM layer for encoder and decoder ,   and fully - connected layers to produce mean and   variance of posteriors . We concatenate the latent695   code with word embedding at every timestamp as   the input of the decoder . For V AE with IGP , we   just produce one variance value and assign it to   be the variance of posterior for all dimensions . At   decoding phase , we use greedy decoding . The di-   mensions for word embedding , encoder - decoder   LSTMs , and latent code are ( 200 , 512 , 32 ) . Three   different values of Care used on each dataset to   explore the impact of the amount of information   transmitted by the code . We also adopt Autoen-   coder ( AE ) as a baseline . All models are trained   from 3 random starts for 20 epochs and 128 batch   size using Adam ( Kingma and Ba , 2015 ) with learn-   ing rate 0.0005 .   We compare the choice of isotropic Gaussian   posterior ( IGP ) with vanilla diagonal Gaussian pos-   terior ( DGP ) on various grounds , from reconstruc-   tion loss and unit activation ( ¬ß 3.1 ) to downstream   classiÔ¨Åcation task , sample efÔ¨Åciency , robustness ,   and generation ( ¬ß 3.2 ) , posterior sharpness ( ¬ß 3.3 ) ,   and distributional properties of the induced repre-   sentations ( ¬ß 3.4 ) .   3.1 Basic Results   Figure 1 reports the reconstruction loss , AU and   BLEU-2 ( Papineni et al . , 2002 ) for C= 5;15;50 .   KL - divergence in all cases matches the set target C.   We observe the Cconstraint can effectively control   the KL - divergence to the set level . The reconstruc-   tion loss generally drops with the increase of C.   We observe the same pattern for DGP and IGP . Ad-   ditionally , while DGP struggles , IGP can activate   all dimensions ( e.g. , AU for C= 5 on DBpedia   are 4 and 32 for DGP and IGP , respectively ) . This   translates into IGP reaching a signiÔ¨Åcantly higher   BLEU . For more results , including autoencoder ,   seeAppendix .   3.2 ClassiÔ¨Åcation and Generation   ClassiÔ¨Åcation . We trained a classiÔ¨Åer on top of   the frozen encoders of DGP and IGP and use the   mean vector representations as features to train the   classiÔ¨Åer . For the classiÔ¨Åer , we used a 2 - hidden-   layer MLP with 128 neurons and ReLU activation   function at each layer . We trained 10 randomly   initialised classiÔ¨Åers and used the mean of classiÔ¨Å-   cation accuracy as the Ô¨Ånal accuracy . Figure 2 ( top )   reports the results . Overall , the representations of   most V AEs with IGP lead to a signiÔ¨Åcant improve-   ment of classiÔ¨Åcation accuracy compared to vanilla   V AEs . In the only exception ( i.e , C= 5 on DB-   pedia ) , two models have comparable results with   no model having any statistically signiÔ¨Åcant advan-   tage . We attribute this to having a more representa-   tive mean which is encouraged by IGP . One notable   thing is that DGP does not perform as good as AEs   regardless of Cchoice , whereas IGP ( C= 15;50 )   achieve similar and better classiÔ¨Åcation accuracy   on DBpedia and Yahoo Question .   We adopted few - shot setting to compare sample   efÔ¨Åciency of both V AEs ( with C= 15 ) , by using   0.1 % , 1 % and 10 % of training data of DBpedia and   did classiÔ¨Åcation on the test set as before . Accuracy   scores are reported in Figure 2 ( bottom ) with IGP   exhibiting a better sample efÔ¨Åciency . For instance,696   the mean accuracy gap at 0.1 % is quite signiÔ¨Åcant   being above 7 points , and V AE gets the gap down   to 4 points at 100 % ( still signiÔ¨Åcant ) .   We further investigated the robustness of the   learned representations to perturbation via apply-   ing word dropout on sentences by randomly delet-   ing 30 % of words in a sentence , and repeating the   classiÔ¨Åcation experiment . IGP with accuracies of   ( 83.5 , 34.0 ) outperforms both DGP ( 76.4 , 24.1 ) and   AE ( 83.1 , 30.7 ) on ( DBpedia , Yahoo ) . We specu-   late this to be an indication of information overlap   across dimensions of the representations at higher   AU , offering a better recovery of information in the   presence signiÔ¨Åcant perturbation .   Generation . We imputed 75 % of words of a sen-   tence from the test set of DBpedia , fed it to V AE   encoder and reconstructed the sentence from its   latent code using IGP and DGP in Table 1 . IGP   successfully recovers the type of the mentioned   object and completes the imputed sentence with a   similar structure , whereas DGP fails to do so .   3.3 Posterior Shape   To understand the impact of isotropy on the ag-   gregated posterior , q(z ) = Pq(zjx ) , we   obtain unbiased samples of zby sampling an x   from data and then zq(zjx ) , and measure   the log determinant of covariance of the samples   ( log det(Cov [ q(z ) ] ) ) as well as the mean of the   samples to measure jjjj . Table 2 reports these   forC= 15 . We observe that log det(Cov [ q(z ) ] )   is signiÔ¨Åcantly lower for IGP indicating a sharper   approximate posteriors .   3.4 Properties of Representations   Isotropy Score . We quantitatively approximate   the isotropy score ( Mu and Viswanath , 2018 ) ,   IS(V ) = minPexp(mv )   maxPexp(mv ) ;   whereVis the matrix of representations ( i.e. , of   samples or mean vectors of posteriors ) , and Mis   the set of eigen vectors of VV . As observed in   Table 3 , compared to DGP , IGP has a signiÔ¨Åcantly   larger IS on both means and samples . Interestingly ,   given that dimensions are independently modeled   via univariate Gaussians , both V AEs outperform   the Autoencoder counterparts .   Visualization . We visualize the learned represen-   tation space of DGP and IGP for DBpedia , using   t - SNE ( van der Maaten and Hinton , 2008 ) , in Fig-   ure 3 ( bottom ) . As illustrated in the right plot , the   clusters of classes in IGP have less overlap among   classes compared with DGP ( left ) . Additionally ,   we use the Mapperalgorithm ( Singh et al . , 2007 )   to visualise the highest density region ( HDR ) ( Hyn-   dman , 1996 ) of the mean vectors for DGP and IGP .   HDR cuts the overall density space to form latent   spaces that contain above a threshold probability   mass ( i.e. ,0:05with minimum samples 2per   latent space ) . The output of the mapper is a graph ,   where each component in the graph corresponds to   a set of nearby points forming a high density space.697   The connectivity of the graph reÔ¨Çects some topo-   logical properties of the sampling space ( darker   colors indicate higher density ) . As observed in   Figure 3 ( top ) , the HDR of DGP posterior means   forms a single component whereas IGP forms 9   disconnected components indicating more discrim-   inative characteristics of its mean vectors , echoing   earlier results in better accuracy in the classiÔ¨Åcation   setting ( ¬ß 3.2 ) .   4 Conclusion   We proposed Isotropic Gaussian Posteriors ( IGP )   as a means of encouraging isotropy in the latent   space induced by V AEs . The injection of isotropy   addressed a sub - optimal behaviour of V AEs by   activating more dimensions of the representation   and encouraging a more discriminative latent space .   Our experiments illustrated a signiÔ¨Åcant improve-   ment of classiÔ¨Åcation performance and robustness   to input perturbations with IGP . We also observed ,   in the sentence completion task , that V AE trained   with IGP is more capable at maintaining seman-   tic cohesiveness . Our ongoing work suggests the   representation utilisation achieved by IGP has the   potential to be exploited towards representational   properties such as disentanglement .   Acknowledgments   We thank the anonymous reviewers for their helpful   suggestions . References698699   A Full Results   We report detailed reconstruction loss , KL-   divergence , active units and results of BLEU and   ROUGE scores on reconstructed test set in Table 4.700701   Hyunji Hayley Park   University of IllinoisYogarshi Vyas   AWS AI LabsKashif Shah   Microsoft   Abstract   1 Introduction   Transformer - based models ( Vaswani et al . , 2017 )   have achieved much progress across many ar-   eas of NLP including text classification ( Minaee   et al . , 2021 ) . However , such progress is often   limited to short sequences because self - attention   requires quadratic computational time and space   with respect to the input sequence length . Widely-   used models like BERT ( Devlin et al . , 2019 ) or   RoBERTa ( Liu et al . , 2019 ) are typically pretrained   to process up to 512 tokens . This is problematic   because real - world data can be arbitrarily long . As   such , different models and strategies have been   proposed to process longer sequences .   In particular , we can identify a few standard ap-   proaches for the task of long document classifi - cation . The simplest approach is to truncate long   documents ‚Äî using BERT or RoBERTa on the first   512 tokens is often used as a baseline . More effi-   cient Transformer models like Longformer ( Belt-   agy et al . , 2020 ) and Big Bird ( Zaheer et al . ,   2020 ) use sparse self - attention instead of full self-   attention to process longer documents ( e.g. up   to 4,096 tokens ) . Other approaches process long   documents in their entirety by dividing them into   smaller chunks ( e.g. Pappagari et al . , 2019 ) . An   alternative idea proposed by recent work is to se-   lect sentences from the document that are salient   to making the classification decision ( Ding et al . ,   2020 ) .   However , the relative efficacy of these models   is not very clear due to a lack of consensus on   benchmark datasets and baselines . Tay et al . ( 2021 )   propose a benchmark for comparing Transformers   that can operate over long sequences , but this only   includes a single , simulatedlong document clas-   sification task . Novel variants of efficient Trans-   formers are often compared to a BERT / RoBERTa   baseline only , without much comparison to other   Transformer models designed for the task ( e.g. Belt-   agy et al . , 2020 ; Zaheer et al . , 2020 ) . Conversely ,   models designed for long document classification   often focus exclusively on state - of - the - art mod-   els for particular datasets , and do not consider a   BERT / RoBERTa baseline or any other Transformer   models ( e.g. Ding et al . , 2020 ; Pappagari et al . ,   2019 ) .   This paper provides a much - needed comprehen-   sive comparison among existing models for long   document classification by evaluating them against   unified datasets and baselines . We compare mod-   els that represent different approaches on various   datasets and against Transformer baselines . Our   datasets cover binary , multi - class , and multi - label702classification . We also consider different ways in-   formation that is relevant to the classification is or-   ganized in texts ( e.g. in the beginning or toward the   end ) and how this affects model performance . We   also compare the models in terms of their training   time , inference time , and GPU memory require-   ments to account for additional complexity that   some of the models have relative to a BERT base-   line . This allows us to compare the practical effi-   cacy of the models for real - world usage .   Our results show that more sophisticated models   are often outperformed by simpler models ( often   including a BERT baseline ) and yield inconsistent   performance across datasets . Based on these find-   ings , we highlight the importance of considering di-   verse datasets while developing models , especially   those that represent different ways key information   is presented in long texts . Additionally , we rec-   ommend that future research should also always   include simpler baseline models . To summarize ,   our contributions are :   ‚Ä¢We provide insights into the practical efficacy   of existing models for long document classi-   fication by evaluating them across different   datasets , and against several baselines . We   compare the accuracy of these models as well   as their runtime and memory requirements .   ‚Ä¢We present a comprehensive suite of evalua-   tion datasets for long document classification   with various data settings for future studies .   ‚Ä¢We propose simple models that often outper-   form complex models and can be challenging   baselines for future models for this task .   2 Methods   In this paper , we compare models representing dif-   ferent approaches to long document classification   ( Beltagy et al . , 2020 ; Pappagari et al . , 2019 ; Ding   et al . , 2020 ) on unified datasets and baselines .   2.1 Existing Models   As described in ¬ß 1 , four distinct approaches have   been proposed for long document classification : 1 )   document truncation , 2 ) efficient self - attention , 3 )   chunk representations , 4 ) key sentence selection .   We evaluate a representative model from each cate-   gory in this work .   BERT ( document truncation ) The simplest ap-   proach consists of finetuning BERT after truncatinglong documents to the first 512 tokens . As in De-   vlin et al . ( 2019 ) , we use a fully - connected layer   on the [ CLS ] token for classification . This is an   essential baseline as it establishes the limitations   of a vanilla BERT model in classifying long doc-   uments yet is still competitive ( e.g. Beltagy et al . ,   2020 ; Chalkidis et al . , 2019 ) . However , some prior   work fails to consider this baseline ( e.g. Ding et al . ,   2020 ; Pappagari et al . , 2019 ) .   Longformer ( efficient self - attention ) We select   Longformer ( Beltagy et al . , 2020 ) as a model de-   signed to process longer input sequences based on   efficient self - attention that scales linearly with the   length of the input sequence ( see Tay et al . , 2020 ,   for a detailed survey ) . Longformer also truncates   the input , but it has the capacity to process up to   4,096 tokens rather than 512 tokens as in BERT .   Following Beltagy et al . ( 2020 ) , we use a fully-   connected layer on top of the first [ CLS ] token   with global attention . Longformer outperformed a   RoBERTa baseline significantly on a small binary   classification dataset ( Beltagy et al . , 2020 ) . How-   ever , it has not been evaluated against any other   models for text classification or on larger datasets   that contain long documents .   ToBERT ( chunk representations ) Transformer   over BERT ( ToBERT , Pappagari et al . , 2019 ) takes   a hierarchical approach that can process documents   of any lengths in their entirety . The model divides   long documents into smaller chunks of 200 tokens   and uses a Transformer layer over BERT - based   chunk representations . It is reported to outper-   form previous state - of - the - art models on datasets   of spoken conversations . However , it has not been   compared to other Transformer models . We re-   implement this model based on the specifications   reported in Pappagari et al . ( 2019 ) as the code is   not publicly available .   CogLTX ( key sentence selection ) Cognize Long   TeXts ( CogLTX , Ding et al . , 2020 ) jointly trains   two BERT ( or RoBERTa ) models to select key sen-   tences from long documents for various tasks in-   cluding text classification . The underlying idea that   a few key sentences are sufficient for a given task   has been explored for question answering ( e.g. Min   et al . , 2018 ) , but not much for text classification . It   is reported to outperform ToBERT and some other703neural models ( e.g. CNN ) , but it is not evaluated   against other Transformer models .   We use their multi - class classification code for   any classification task with appropriate loss func-   tions . Following Beltagy et al . ( 2020 ) , we use   sigmoid and binary cross entropy loss on the logit   output of the models for binary classification . The   same setting is used for multi - label classification   with softmax normalization and cross entropy loss .   2.2 Novel Baselines   In addition to the representative models above , we   include two novel methods that serve as simple but   strong baseline models .   BERT+TextRank While the BERT truncation   baseline is often effective , key information required   to classify documents is not always found within   the first 512 tokens . To account for this , we aug-   ment the first 512 tokens , with a second set of 512   tokens obtained via TextRank , an efficient unsuper-   vised sentence ranking algorithm ( Mihalcea and   Tarau , 2004 ) . TextRank provides an efficient alter-   native to more complex models designed to select   key sentences such as CogLTX . Specifically , we   concatenate the BERT representation of the first   512 tokens with that of the top ranked sentences   from TextRank ( up to another 512 tokens ) . As   before , we use a fully - connected layer on top of   the concatenated representation for classification .   We use PyTextRank ( Nathan , 2016 ) as part of the   spaCy pipeline ( Honnibal et al . , 2020 ) for the im-   plementation with the default settings .   BERT+Random As an alternative approach to   the BERT+TextRank model , we select random sen-   tences up to 512 tokens to augment the first 512   tokens . Like BERT+TextRank , this can be a sim-   ple baseline approach in case key information is   missing in truncated documents .   2.3 Hyperparameters   We use reported hyperparameters for the existing   models whenever available . However , given that   we include different datasets that the original pa-   pers did not use , we additionally explore different   hyperparameters for the models . Detailed informa-   tion is available in Appendix A.Dataset # BERT Tokens % Long   Hyperpartisan 744.2 ¬±677.9 53.5   20NewsGroups 368.8 ¬±783.8 14.7   EURLEX-57 K 707.99 ¬±538.7 51.3   Book Summary 574.3 ¬±659.6 38.8   ‚Äì Paired 1,148.6 ¬±933.9 75.5   2.4 Data   We select three classification datasets containing   long documents to cover various kinds of classifica-   tion tasks : Hyperpartisan ( Kiesel et al . , 2019 ) ( bi-   nary classification ) , 20NewsGroups ( Lang , 1995 )   ( multi - class classification ) and EURLEX-57 K   ( Chalkidis et al . , 2019 ) ( multi - label classification ) .   We also re - purpose the CMU Book Summary   Dataset ( Bamman and Smith , 2013 ) as an addi-   tional multi - label classification dataset .   We also modify the EURLEX and Book Sum-   mary datasets to represent different data settings   and further test all models under these challenging   variations . A document in the EURLEX dataset   contains a legal text divided into several sections ,   and the first two sections ( header , recitals ) carry   the most relevant information for classification   ( Chalkidis et al . , 2019 ) . We invert the order of   the sections so that this key information is located   toward the end of each document ( Inverted EU-   RLEX ) . This creates a dataset particularly chal-   lenging for models that focus only on the first 512   tokens . We also combine pairs of book summaries   from the CMU Book Summary dataset to create a   new dataset ( Paired Book Summary ) that contains   longer documents with two distinctive information   blocks . Again , this challenges models not to solely   rely on the signals from the first 512 tokens . In   addition , it further challenges models to detect two   separate sets of signals for correct classification   results . In all , these modified datasets represent dif-   ferent ways information may be presented in long   texts and test how robust the existing models are to   these . Table 1 summarizes characteristics of all our   datasets , with more details in Appendix B.   2.5 Metrics   For the binary ( Hyperpartisan ) and multi - class   ( 20NewsGroups ) classification tasks , we report ac-704ModelHyper- 20NewsEURLEXInverted Book Paired   partisan Groups EURLEX Summary Summary   BERT 92.00 84.79 73.09 70.53 58.18 52.24   BERT+TextRank 91.15 84.99 72.87 71.30 58.94 55.99   BERT+Random 89.23 84.65 73.22 71.47 59.36 56.58   Longformer 95.69 83.39 54.53 56.47 56.53 57.76   ToBERT 89.54 85.52 67.57 67.31 58.16 57.08   CogLTX 94.77 84.63 70.13 70.80 58.27 55.91   curacy ( % ) on the test set . For the rest , multi - label   classification datasets , we use micro- F(% ) , which   is based on summing up the individual true posi-   tives , false positives , and false negatives for each   class .   3 Results   Table 2 summarizes the average performance of the   models over five runs with different random seeds .   Overall , the key takeaway is that more sophisti-   cated models ( Longformer , ToBERT , CogLTX )   do not outperform the baseline models across the   board . In fact , these models are significantly more   accurate than the baselines only on two datasets .   As reported in Beltagy et al . ( 2020 ) , Longformer   recorded the strongest performance on Hyperpar-   tisan , with CogLTX also performing well . Long-   former and ToBERT performed the best for Paired   Book Summary . Paired Book Summary seems to   be most challenging for all models across the board   and is the only dataset where the BERT baseline   did the worst . However , it is worth noting that   simple augmentations of the BERT baseline as in   BERT+TextRank and BERT+Random were not far   behind the best performing model even for this   challenging dataset . ToBERT ‚Äôs reported perfor-   mance was the highest for 20NewsGroups , but we   were unable to reproduce the results due to its mem-   ory constraints . For the other datasets , these more   sophisticated models were outperformed by the   baselines . In particular , the simplest BERT base-   line that truncates documents up to the first 512   tokens shows competitive performance overall , out-   performing the majority of models for Hyperparti - ModelTrain Inference GPU   Time Time Memory   BERT 1.00 1.00 < 16   + TextRank 1.96 1.96 16   + Random 1.98 2.00 16   Longformer 12.05 11.92 32   ToBERT 1.19 1.70 32   CogLTX 104.52 12.53 < 16   san , 20NewsGroups and EURLEX . It is only the   Paired Book Summary dataset where the BERT   baseline performed particularly worse than other   models . In general , we observe little - to - no per-   formance gains from more sophisticated models   across the datasets as compared to simpler models .   A similar trend was observed even when the mod-   els were evaluated only on long documents in the   test set ( Appendix C ) . These finding suggests that   the existing models do not necessarily work better   for long documents across the board when diverse   datasets are considered .   The relatively inconsistent performance of these   existing models is even more underwhelming con-   sidering the difference in runtime and memory re-   quirements as summarized in Table 3 . Compared   to BERT on the first 512 tokens , Longformer takes   about 12x more time for training and inference   while CogLTX takes even longer . ToBERT is faster   than those two , but it requires much more GPU   memory to process long documents in their en-   tirety . Taken together with the inconsistency in705accuracy / F1 scores , this suggests that sophisticated   models are not necessarily a good fit for real word   use cases where efficiency is critical .   4 Discussion and Recommendations   Our results show that complex models for long doc-   ument classification do not consistently outperform   simple baselines . The fact that the existing mod-   els were often outperformed by the simplest BERT   baseline suggests that the datasets tend to have key   information accessible in the first 512 tokens . This   is somewhat expected as the first two sections of   EURLEX are reported to carry the most informa-   tion ( Chalkidis et al . , 2019 ) and 20NewsGroups   contains mostly short documents . Including these   datasets to evaluate models for long document clas-   sification is still reasonable given that a good model   should work well across different settings . How-   ever , these datasets alone do not represent various   ways information is presented in long texts .   Instead , future studies should evaluate their mod-   els across various datasets to create robust models .   While it is often difficult to obtain datasets suited   for long document classification , our modifications   of existing datasets may provide ways to repurpose   existing datasets for future studies . We invert the   order of the sections of EURLEX to create the In-   verted EURLEX dataset , where key information is   likely to appear toward the end of each document .   Our results in Table 2 show that selective mod-   els ( BERT+TextRank , BERT+Random , CogLTX )   performed better than those that read longer con-   secutive sequences ( Longformer , ToBERT ) on this   dataset . This suggests that this inverted dataset may   contain parts of texts that should be ignored for bet-   ter performance , thus providing a novel test bed for   future studies . The Paired Book Summary dataset   presents another challenging data setting with two   distinctive information blocks . While Longformer   and ToBERT performed significantly better for this   dataset than others , the overall model performance   was quite underwhelming , leaving room for im-   provement for future models .   Many of these findings were revealed only due   to the choice of relevant baselines , and future   work will benefit from including these as well . A   BERT / RoBERTa baseline is essential to motivate   the problem of long document classification using   Transformers and reveal how much information is   retrievable in the first 512 tokens . BERT+TextRank   and BERT+Random are stronger baselines that of - ten outperform more complex models that select   key sentences . In fact , they outperformed CogLTX   on five of the six datasets .   5 Conclusion   Several approaches have been proposed to use   Transformers to classify long documents , yet their   relative efficacy remains unknown . In this paper ,   we compare existing models and baselines on var-   ious datasets and in terms of their time and space   requirements . Our results show that existing mod-   els , while requiring more time and/or space , do   not perform consistently well across datasets , and   are often outperformed by baseline models . Future   studies should consider the baselines and datasets   to establish robust performance .   Acknowledgments   We would like to thank the reviewers and area   chairs for their thoughtful comments and sugges-   tions . We also thank the members of AWS AI   Labs for many useful discussions and feedback   that shaped this work .   References706   A Hyperparameters   Across all datasets , we used Adam optimizer with   a learning rate of { 5e-5 , 3e-5 , 0.005 } for one run of   each model and picked the best performing learn-   ing rate for the model . The learning rate of 0.005   was used for Longformer only because it did not   perform well with a learning rate of 5e-5 or 3e-5   for most of the datasets . We set dropout rate at   0.1 as suggested by Devlin et al . ( 2019 ) . The num-   ber of epochs needed for finetuning the models for   different datasets is likely to vary , so we trained   all models for 20 epochs and selected the best per-   forming model based on the performance metric   on the validation set . We report the average results   on the test set over five different seeds .   All experiments on baseline models and   CogLTX were conducted on a single Tesla V100   GPU with 16 GB memory . For Longformer and   ToBERT , we used a NVIDIA A100 SXM4 with   40 GB memory . More details on the selected hyper-   parameters are available with our code at https :   //github.com / amazon - research/   efficient - longdoc - classification .   B Datasets   Hyperpartisan is a binary classification dataset ,   where each article is labeled as True ( hyperpartisan )   or False ( not hyperpartisan ) ( Kiesel et al . , 2019 ) .   More than half of the documents exceed 512 tokens .   It is quite different from other datasets in that it   is a very small dataset : the training set contains   516 documents while the development and test sets   contain 64 and 65 documents , respectively .   20NewsGroups is a widely - used multi - class clas-   sification dataset ( Lang , 1995 ) . The documents are   categorized into well - balanced , 20 classes . Only707Dataset Type # Train # Dev # Test # Labels # BERT Tokens % Long   Hyperpartisan binary 516 64 65 2 744.18 ¬±677.87 53.49   20NewsGroups multi - class 10,182 1,132 7,532 20 368.83 ¬±783.84 14.71   EURLEX-57Kmulti - label 45,000 6,000 6,000 4,271 707.99 ¬±538.69 51.30 ‚Äì Inverted   Book Summary multi - label 10,230 1,279 1,279 227 574.31 ¬±659.56 38.76   ‚Äì Paired multi - label 5,115 639 639 227 1,148.62 ¬±933.97 75.54   ModelHyper- 20NewsEURLEXInverted Book Paired   partisan Groups EURLEX Summary Summary   BERT 88.00 86.09 66.76 62.88 60.56 52.23   BERT+TextRank 85.63 85.55 66.56 64.22 61.76 56.24   BERT+Random 83.50 86.18 67.03 64.31 62.34 56.77   Longformer 93.17 85.50 44.66 47.00 59.66 58.85   ToBERT 86.50 ‚Äì 61.85 59.50 61.38 58.17   CogLTX 91.91 86.07 61.95 63.00 60.71 55.74   about 15 % of the documents exceed 512 tokens .   While the original dataset comes in train and test   sets only , we report results on the train / dev / test split   as used in Pappagari et al . ( 2019 ) , where we take   10 % of the original train set as the development   set . Note that CogLTX reported their accuracy at   87.00 % on the test set and 87.40 % on the long doc-   uments in the test set , using the original train and   test sets only . Our implementation of CogLTX in   the same setting with five different runs resulted   in a much lower performance at 85.15 % on the   test set and 86.57 % on the long documents only .   In addition , we were unable to replicate ToBERT   results on 20NewsGroups . It is unclear how the   dataset is further preporcessed for ToBERT , and our   implementation of ToBERT caused a GPU out - of-   memory error on 20NewsGroups . Thus , we show   the reported results for ToBERT on this dataset .   EURLEX-57 K is a multi - label classification   dataset based on EU legal documents ( Chalkidis   et al . , 2019 ) . In total , there are 4,271 labels avail-   able , and some of them do not appear in the train-   ing set often or at all , making it a very challeng-   ing dataset . About half of the datasets are long   documents . Each document contains four majorzones : header , recitals , main body , and attachments .   Chalkidis et al . ( 2019 ) observe that processing the   first two sections only ( header and recitals ) results   in almost the same performance as the full docu-   ments and that BERT on the first 512 tokens outper-   forms all the other models they considered . After   examining the dataset , we exclude the attachments   section as it does not seem to provide much textual   information .   CMU Book Summary contains book summaries   extracted from Wikipedia with corresponding meta-   data from Freebase such as the book author and   genre ( Bamman and Smith , 2013 ) . We use the   summaries and their corresponding genres for a   multi - label classification task . We keep 12,788 out   of 16,559 documents after removing data points   missing any genre information and/or adequate   summary information ( e.g. less than 10 words ) .   In total , there are 227 genre labels such as ‚Äò Fiction ‚Äô   and ‚Äò Children ‚Äôs literature ‚Äô .   C Results on long documents only   Table 5 shows the results as evaluated on long doc-   uments ( with over 512 tokens ) in the test set only .   Overall , the results show a similar trend as ob-708served in Table 2 , which reports the results on the   entire documents in the test set . In general , the   existing models were often outperformed by the   BERT truncation baseline . This suggest that these   models designed for long document classification   do not perform particularly well on the long docu-   ments in the datasets . The only difference is that   BERT+Random and ToBERT perform better than   the BERT baseline when evaluated on long docu-   ments only for 20NewsGroups and Book Summary ,   respectively . However , the performance gain does   not seem significant , and the relative performance   with respect to the other models remains largely   unchanged . In general , the relative strength of a   model for a given dataset stays the same whether or   not the model is evaluated on the entire documents   or long documents in the test set.709   Lisa Jin andDaniel Gildea   Department of Computer Science   University of Rochester   Rochester , NY 14627   Abstract   1 Introduction   Automatic evaluation metrics often score natu-   ral language generation ( NLG ) system outputs   based on how well they lexically align to human-   annotated references . In tasks such as machine   translation and summarization , these metrics may   unfairly penalize outputs that express the correct   semantics despite a lower n - gram overlap with ref-   erence strings . As a result , models overfitting to   certain token - level patterns may dominate those   generating more creatively ( e.g. , through synonyms   or varied sentence structure ) .   NLG systems are typically trained to maximize   likelihood of a single set of references . Condition-   ing models on gold prefixes shields them from their   own predictions during training ‚Äî an issue known   as exposure bias . Adding reinforcement learning   ( RL ) objectives ( Ranzato et al . , 2016 ; Edunov et al . ,   2018 ) can aid exploration by giving a model feed-   back on sequences sampled from its own distri-   bution . However , it is common practice to use   automatic evaluation scores like BLEU ( Papineniet al . , 2002 ) and ROUGE ( Lin and Hovy , 2002 ) as   sequence - level rewards . This results in the same   lack of semantic signal described earlier .   Instead of hinging evaluation on hard n - gram   overlap , recent metrics ( Zhang et al . , 2019 ; Zhao   et al . , 2019 ) rely on vector similarity between con-   textualized subword embeddings to make more se-   mantically faithful judgments . BERTS , in   particular F , computes a token - level F1 score   based on greedy alignment of similar embeddings .   With their strength in offline evaluation , it is natu-   ral to ask how these embeddings - based metrics can   help provide more realistic training feedback .   Past approaches to train models with semantic   similarity scores include both non - differentiable   and differentiable objectives . Wieting et al . ( 2019 )   separately train paraphrastic sentence embeddings   that provide semantic similarity rewards to a neu-   ral machine translation ( NMT ) system . Rewards   were included in a mixed minimum risk and maxi-   mum likelihood training phase . Besides an embed-   ding training overhead , the model needed a length   penalty term to limit repetitive outputs . Li et al .   ( 2019 ) adopt a similar fine - tuning approach using   an RL objective with F for abstractive sum-   marization . While their models were less repet-   itive , their news domain corpora may have been   a natural match for BERT embeddings . Finally ,   Jauregi Unanue et al . ( 2021 ) also propose to opti-   mize F but with fully differentiable training   objectives in NMT . Yet their models overfit after   only a few epochs and scored lower in BLEU at   the cost of higher F . We hypothesize that   metrics employing external pretrained vectors may   suffer from domain mismatch with downstream   data . This can hurt the accuracy of semantic simi-   larity scores computed during training .   In this work , we focus on text generation from   Abstract Meaning Representations ( AMRs , Ba-   narescu et al . , 2013 ) , sentence - level semantic   graphs that are rooted , directed , and acyclic . This710task ‚Äôs models may especially benefit from an em-   phasis on semantic rather than lexical similarity .   It also provides a challenging setting to evaluate   overfitting given the relatively small corpus size .   In our analysis of F rewards , we note that   F could worsen repetition and incomplete out-   puts in NLG systems . Due to its greedy token align-   ment , F precision may assign extra credit to   a reference token ‚Äò retrieved ‚Äô multiple times . In   response , we contribute the following .   ‚Ä¢We introduce metrics that apply discrete and   continuous alignments to BERTS , miti-   gating the pitfalls of greedy alignment .   ‚Ä¢For text generation from AMR , we are the first   to train on RL objectives with embeddings-   based evaluation metrics .   ‚Ä¢As RL rewards , we compute BERTS -   based metrics on a model ‚Äôs own token rep-   resentations rather than BERT embeddings .   This is more memory - efficient and does not   overfit relative to pure cross - entropy training .   2 Greedy Token Alignment   The main insight behind BERTS and related   metrics is to align hypothesis and reference to-   kens using their pairwise vector similarity scores .   These alignments are later used to weight the con-   tribution of token - level similarity scores towards   a final sequence - level score . Concretely , given   ( ÀÜ y , . . . , ÀÜ y)and(y , . . . , y)hypothesis and ref-   erence token embeddings , precision in F is   P = 1   mXmaxcos(ÀÜ y , y ) ,   where cos(ÀÜ y , y ) = ÀÜ yy/‚à•ÀÜ y‚à•‚à•y‚à•denotes cosine   similarity . Each hypothesis token ÀÜyis greedily   aligned to the reference token ywith the highest   corresponding embedding cosine similarity . Unlike   in BLEU , P does not clip the number of times   ÀÜycan align to a unique yby its count in y. As   such , a hypothesis will get excess credit by repeat-   ing a reference token beyond this count . While the   authors claim greedy alignments have little effect   onBERTS evaluation performance , they per-   form poorly relative to metrics based on optimized   alignments in our experiments.3 Optimized Token Alignment   Aligning tokens between hypothesis and reference   can be seen as an assignment problem , where a   token pair ( ÀÜy , y)is highly weighted if it incurs   low cost ( i.e. , distance ) .   Here , we describe discrete token matching ( one-   to - one ) and soft alignment ( one - to - many ) . For the   latter , we extract alignments from the earth mover ‚Äôs   distance ( EMD , Villani , 2009 ; Peyr√© and Cuturi ,   2019 ) transport matrix . We weight pairwise token   similarities as in F using each of these two   alignments to provide metrics F andF .   3.1 Discrete word matching   To avoid the issues with greedy alignment in   P , we can extract one - to - one alignments be-   tween the two sequences . Let C‚ààRde-   note the pairwise cosine distance matrix such that   C= 1‚àícos(ÀÜ y , y ) . For notational clarity , let   eC= 1‚àíC. We wish to find alignments   T= arg minXXTC , ( 1 )   such that no element in h = T1andr = T1   exceeds one . In other words , each ÀÜycan align to at   most one y(exactly one when m = k ) , and vice   versa . This linear sum assignment problem can   be solved in low - order polynomial time ( Crouse ,   2016 ) , making it suitable for use during training .   Metric The updated precision is found as   P = 1   mXXTeC. ( 2 )   Recall R takes an analogous form and is com-   bined with P to produce an F1 score , F .   3.2 Continuous word alignment   We also experiment with soft alignments , where   weights in Tare continuous . In the case of P ,   one - to - many alignments between each hypothesis   token ÀÜyand those in { y}are permitted .   Inspired by work applying EMD to semantic text   similarity ( Kusner et al . , 2015 ; Clark et al . , 2019 ) ,   we frame alignment as minimizing the transporta-   tion cost between token embeddings from the hy-   pothesis and reference distributions . The amount   of token - level mass to transport between the two   distributions is handr , respectively . Instead of711assigning IDF as the mass per token ( Zhao et al . ,   2019 ) , we use the norm of its embedding ( i.e. , ‚à•y‚à• ,   Yokoi et al . , 2020 ) for simplicity .   The EMD , or optimal transport , problem is   T= arg minXXTC , ( 3 )   s.t.h = T1,r = T1 .   Intuitively , if we view Tas the joint probability   of aligning ÀÜywithy , the row and column sums   are marginals ( Cuturi , 2013 ) .   Metric To compute F , we normalize the   alignment weights such that the rows of Tsum to   one for precision , and the columns for recall .   P = 1   mX1   hXTeC , ( 4 )   R = 1   kX1   rXTeC ( 5 )   4 Semantic Similarity Rewards   We propose to fine - tune on our optimized F1 met-   rics , applying a weighted average of cross - entropy   and RL objectives . Given source sequence x(e.g . ,   a linearized AMR ) , the former is computed as   L=‚àíXlogp(y|y , x ) .   To encourage close evaluation scores between sam-   pled¬Øyand reference y , the RL objective is   L= ( ‚àÜ(¬Ø y , y)‚àí‚àÜ(¬Øy , y))Xlogp(¬Øy|¬Øy , x ) ,   where ‚àÜis the chosen evaluation metric and ¬Øy   is a greedily decoded baseline relative to ¬Øy . This   baseline helps reduce variance in REINFORCE   ( Williams , 1992 ) . The combined cross - entropy and   RL loss is   L = ŒªL+ ( 1‚àíŒª)L ,   where Œªis empirically set to 0.3 .   5 Experiments   We examine the performance of our proposed met-   rics as RL rewards on AMR - to - text generation . BLEU METEORF BLEURT   XENT 36.37 39 .94 65 .68 56 .30   BL - R 37.06 40 .30 66 .19 56 .08   F 36.06 39 .85 65 .23 55 .45   F 36.91 40 .34 66 .07 55 .96   F 37.65 40 .61 66 .55 57 .01   5 10 15 20 25 3030323436   5.1 Setup   Dataset The LDC2017T10 dataset that we exper-   iment on contains ‚àº36 K training and ‚àº1.4 K each   of development and test AMR - sentence pairs . To   leverage strong pre - trained language models , the   AMRs are linearized as in Ribeiro et al . ( 2021 ) .   Evaluation We report results in terms of BLEU   ( Papineni et al . , 2002 ) , METEOR ( Banerjee and   Lavie , 2005),F(Popovi ¬¥ c , 2015 ) , and BLEURT   ( Sellam et al . , 2020 ) . Only the latter metric makes   use of pre - trained contextualized embeddings .   Baselines For all experiments , we fine - tune the   small capacity T5 model ( Raffel et al . , 2020 ) from   Ribeiro et al . ( 2021 ) . The model has 60 M parame-   ters and features a Transformer - based encoder and   decoder . We compare our F andF met-   rics for RL - based training against three baseline   approaches . XENT is a pure cross - entropy objec-   tive . For RL - based approaches , we include a BLEU   reward ( BL - R ) and one with F ‚Äî computed   on the lowest level token embeddings in T5.The   Œªscaling factor for the RL objective is set to 0.3   across all RL - based experiments .   Implementation details Adam ( Kingma and Ba ,   2015 ) is used to optimize the model with an initial712   learning rate of 1¬∑10and a batch size of 16 .   Following Ribeiro et al . ( 2021 ) , we use a linearly   decreasing schedule for the learning rate and no   warm - up . Since Ribeiro et al . ( 2021 ) do not release   their training methodology , we train until valida-   tion BLEU does not increase for three epochs ‚Äî an   approach found in previous work fine - tuning T5   for AMR - to - text generation ( Hoyle et al . , 2021 ) .   We use SciPyand the Python Optimal Transport   libraryto solve Eqs . 1 and 3 .   5.2 Results   Table 1 shows that F achieves the highest   scores on all metrics , surpassing F as well . It   scores higher than XENT by 1.28BLEU and 0.71   BLEURT points . Although BL - R was specially   trained to optimize BLEU , F still outperforms   it by over half a point on that metric .   There is a clear hierarchy among the approaches   based on F1 score , with F above F , fol-   lowed by F at the bottom . This dynamic sug-   gests that the optimized alignments may provide   higher quality reward signals during training .   We note that although F performed com-   parably to BL - R , it could exploit tensor operations   and was far faster to compute than BLEU . On the   other hand , F achieved significantly lower   scores than BL - R. As noted in ¬ß 2 , perhaps the   clipped precision counts in BLEU gave BL - R an   advantage over the greedy nature of F .   5.3 Analysis   Training stability As shown in Fig . 1 , F   continues to improve on validation BLEU long   after XENT overfits at epoch 18 . This runs counter   to the expectation of unstable RL - based training . It is also interesting that while F validation   performance looks fairly low relative to BL - R , it   achieves similar scores at test time . This may be   due to irrelevant differences between the validation   and test sets , however .   Manual inspection Table 2 lists a few examples   of model outputs for detailed analysis . In exam-   ple ( 1 ) , both XENT and F make the error of   predicting ‚Äú part ‚Äù instead of ‚Äú participating ‚Äù . Only   F approaches the meaning of the reference .   This may be a side - effect of weighting lexical over   semantic similarity in the former two systems . In   ( 2),F repeats the word ‚Äú bacterium ‚Äù , while   XENT takes an anthropomorphic view of the bac-   terium . The repetition may be a result of F   rewarding multiple instances of the same token by   mistake during greedy alignment .   6 Conclusion   This paper proposes new F1 score metrics based   on optimized rather than greedy alignments be-   tween predicted and reference tokens . Instead of   letting hypotheses align to reference tokens with-   out regard to their frequencies ( and vice versa ) , we   extract alignments as a constrained optimization   problem . In the discrete case , we treat alignment   as a matching problem between hypothesis and   reference tokens . In the continuous case , we find   alignments that minimize earth mover ‚Äôs distance   between the two token embedding distributions .   We apply new metrics as rewards during RL-   based training for AMR - to - text generation , with   F outperforming both a cross - entropy baseline   and one optimizing BLEU rewards . Despite being   computed on a downstream model ‚Äôs token embed-   dings , the metrics still provide informative rewards   during training without signs of overfitting.713Acknowledgments Research supported by NSF   awards IIS-1813823 and CCF-1934962 .   References714715   Md Mosharaf Hossain , Dhivya Chinnappa , andEduardo BlancoDepartment of Computer Science and Engineering , University of North TexasThomson ReutersSchool of Computing and Augmented Intelligence , Arizona State University   Abstract   1 Introduction   Natural language understanding ( NLU ) is an um-   brella term used to refer to any task that requires   text understanding . For example , question answer-   ing ( Rajpurkar et al . , 2016 ) , information extrac-   tion ( Stanovsky et al . , 2018 ) , coreference resolu-   tion ( Wu et al . , 2020 ) , and machine reading ( Yang   et al . , 2019 ) , among many others , are tasks that fall   under natural language understanding . The thresh-   old for claiming that a system understands natural   language is ever - moving . New corpora are often   justified by pointing out that state - of - the - art models   do not obtain good results . After years of steady   improvements , more powerful models eventually   obtain so - called human performance , and at that   point new , more challenging corpora are created .   Many corpora for natural language understand-   ing tasks contain language generated by annota-   tors rather than retrieved from texts written inde-   pendently of the corpus creation process . These   corpora are certainly useful and have facilitated   tremendous progress . Annotator - generated exam-   ples , however , carry the risk of evaluating systems   with synthetic language that is not representative of   language in the wild . For example , annotators arelikely to use negation when asked to write a text   that contradicts something despite contradictions   in the wild need not have a negation ( Gururangan   et al . , 2018 ) . Recently , Kwiatkowski et al . ( 2019 )   present a large corpus for question answering that   consists of natural questions ( i.e. , asked by some-   body with a real information need ) in order to en-   courage research in a more realistic scenario . This   contrasts with previous corpora , where the ques-   tions were written by annotators after being told   the answer ( Rajpurkar et al . , 2016 ) .   In this paper , we explore the role of negation   in eight corpora for six popular natural language   understanding tasks . Our goal is to check whether   negation plays the role it deserves in these tasks . To   our surprise , we conclude that negation is virtually   ignored by answering the following questions :   1.Do NLU corpora contain as many negations   as general - purpose texts ? ( they do n‚Äôt ) ;   2.Do the ( few ) negations in NLU corpora play   a role in solving the tasks ? ( they do n‚Äôt ) ; and   3.Do state - of - the - art transformers trained with   NLU corpora face challenges with instances   that contain negation ? ( they do , especially if   the negation is important ) .   2 Background and Related Work   We work with the eight corpora covering six tasks   summarized below and exemplified in Table 2 .   We select two corpora for question answer-   ing : CommonsenseQA ( Talmor et al . , 2019 ) and   COPA ( Roemmele et al . , 2011 ) . CommonsenseQA   consists of multi - choice questions ( 5 candidate an-   swers ) that require some degree of commonsense .   COPA presents a premise ( e.g. , The man broke his   toe ) and a question ( e.g. , What was the cause of   this ? ) and the system must choose between two   plausible alternatives ( e.g. He got a hole in his sock   orHe dropped a hammer on his foot ) .716For textual similarity and paraphrasing , we select   QQPand STS - B ( Cer et al . , 2017 ) . QQP consists   of pairs of questions and the task is to determine   whether they are paraphrases . STS - B consists of   pairs of texts and the task is to determine how se-   mantically similar they are with a score from 0 to 5 .   We select one corpus for the remaining tasks .   For inference , we work with QNLI ( Rajpurkar   et al . , 2016 ) , which consists in determining whether   a text is a valid answer to a question . We use   WiC ( Pilehvar and Camacho - Collados , 2019 ) for   word sense disambiguation . WiC consists in deter-   mining whether two instances of the same word ( in   two sentences ; italicized in Table 2 ) are used with   the same meaning . For coreference resolution , we   choose WSC ( Levesque et al . , 2012 ) , which con-   sists in determining whether a pronoun and a noun   phrase are co - referential ( italicized in Table 2 ) . Fi-   nally , we work with SST-2 ( Socher et al . , 2013 ) for   sentiment analysis . The task consists in determin-   ing whether a sentence from a collection of movie   reviews has positive or negative sentiment .   For convenience , we work with the formatted   versions of these corpora in the GLUE ( Wang et al . ,   2018 ) and SuperGLUE ( Wang et al . , 2019 ) bench-   marks . The only exception is CommonsenseQA ,   which is not part of these benchmarks .   Related Work Previous work has shown that   SNLI ( Bowman et al . , 2015 ) and MNLI ( Williams   et al . , 2018 ) have annotation artifacts ( e.g. , negation   is a strong indicator of contradictions ) ( Gururan-   gan et al . , 2018 ) . The literature has also shown that   simple adversarial attacks including negation cues   are very effective ( Naik et al . , 2018 ; Wallace et al . ,   2019 ) . Kovatchev et al . ( 2019 ) analyze 11 para-   phrasing systems and show that they obtain sub-   stantially worse results when negation is present .   More recently , Ribeiro et al . ( 2020 ) show that   negation is one of the linguistic phenomena com-   mercial sentiment analysis struggle with . Several   previous works have investigated the ( lack of ) abil-   ity of transformers to make inferences when nega-   tion is present . For example , Ettinger ( 2020 ) con-   clude that BERT is unable to complete sentences   when negation is present . BERT also faces chal-   lenges solving the task of natural language infer-   ence ( i.e. , identifying entailments and contradic-   tions ) with monotonicity and negation ( Geiger   et al . , 2020 ; Yanaka et al . , 2019 ) . Warstadt et al .   ( 2019 ) show the limitations of BERT making ac-   ceptability judgments with sentences that contain   negative polarity items . Most related to out work ,   Hossain et al . ( 2020 ) analyze the role of negation in   three natural language inference corpora : RTE ( Da-   gan et al . , 2006 ; Bar - Haim et al . , 2006 ; Giampic-   colo et al . , 2007 ; Bentivogli et al . , 2009 ) , SNLI and   MNLI . In this paper , we present a similar analysis ,   but we move beyond natural language inference   and work with eight corpora spanning six natural   language understanding tasks .   3 Research Questions and Analysis   Q1 : Do natural language understanding cor-   pora contain as many negations as general-   purpose English texts ? In order to automat-   ically identify negation cues , we train a nega-   tion cue detector with the largest corpus available ,   ConanDoyle - neg ( Morante and Daelemans , 2012 ) .   The cue detector is based on the RoBERTa pre-   trained language model ( Liu et al . , 2019 ) ; we pro-   vide details about the architecture and training pro-   cess in Appendix A. Our cue detector obtains the   best results to date : F1 : 93.79 vs. 92.94 ( Khan-   delwal and Sawant , 2020 ) . ConanDoyle - neg ( and   thus our cue detector ) identifies common negation   cues such as no , not , n‚Äôtandnever , affixal negation   cues such as impossible andcareless , and lexical   negations such as deny andavoid .717   Table 1 presents the percentage of sentences that   contain negation in ( a ) the eight corpora we work   with and ( b ) general - purpose English . We take the   latter percentage ( all sentences ) from Hossain et al .   ( 2020 ) , who run a negation cue detector in online   reviews , conversations , and books . Additionally ,   we also present the percentages in questions . Nega-   tion is much less common in all natural language   understanding corpora but WSC ( 0.8%‚Äì16 % ) than   in general - purpose English ( 22.6%‚Äì29.9 % ) . Note   that negation is also underrepresented in corpora   that primarily contain questions ( general - purpose :   15.8%‚Äì20.2 % ; COPA : 0.8 % , QQP : 8.1 % ) .   Q2 : Do the ( few ) negations in natural language   understanding corpora play a role in solving the   tasks ? After showing that negation in underrepre-   sented in natural language understanding corpora ,   we explore whether the few negations they con-   tain are important . Given an instance from any   of the corpora , we consider a negation important   if removing it changes the ground truth . In other   words , a negation is unimportant if one can ignoreit and still solve the task at hand . Table 2 presents   examples of important and unimportant negations .   We manually examine the negations in all in-   stances containing negation from the validation   split of each corpus except QQP , for which we ex-   amine 1,000 ( out of 5,196 ) . Note that COPA does   not have any negations in the validation split , and   many corpora have few instances containing nega-   tion ( CommonsenseQA : 184 , STS - B : 225 , QNLI :   852 , WiC : 99 , WSC : 52 , and SST-2 : 263 ) . We   choose to work with the validation set because we   want to compare results when negation is and is not   important ( Q3 ) , and the ground truth for the test   splits of some corpora are not publicly available .   We observe that ( a ) all negations in WiC and   WSC are unimportant , and ( b ) the percentages   of unimportant negations in CommonsenseQA ,   SST-2 , QQP , STS - B , and QNLI are substantial :   45.1 % , 63 % , 97.4 % , 95.6 % , and 97.7 % , respec-   tively . These percentages indicate that one can   safely ignore ( almost ) all negations and still solve   the benchmarks . Despite the fact that negations are718   not important in WSC and WiC , they do affect the   experimental results ( details in Q3 ) .   We also analyze the role of two major types   of negation : syntactic ( not , no , never , etc . ) and   morphological ( i.e. , affixes such as un-,im- , and   -less ) . To this end , we work with CommonsenseQA   and SST-2 , which have lower percentages of unim-   portant negations ( 45.1 % and 63 % ) than the other   corpora we use ( 97.4%‚Äì100 % ) . Table 3 provides   examples of these two negation types . Perhaps un-   surprisingly , syntactic negations are much more   common than morphological negations ( Common-   senseQA : 88.6 % vs 11.4 % , SST-2 : 71.9 % vs   28.1 % ) . More importantly , syntactic negations are   more often important in SST-2 ( 42.3 % vs 23 % ) ,   but both syntactic and morphological negation   are roughly equaly important in CommonsenseQA   ( 55.2 % vs 52.4 % ) .   Q3 : Do state - of - the - art transformers trained   with NLU corpora face challenges with instances   that contain negation ? We conduct experiments   with RoBERTa ( Liu et al . , 2019 ) . More specifically , we use the implementation by Phang et al . ( 2020 )   and train a model with the training split of each   corpus . We refer the readers to the Appendix B for   the details about these models and hyperparame-   ters . We chose RoBERTa over other transformers   because 4 out of the 10 best submissions to the   SuperGLUE benchmark use it .   Table 4 presents the results evaluating the models   with the corresponding validation splits . RoBERTa   obtains slightly worse results with the validation   instances that have negation in all corpora ; the only   exception is QQP ( F1 : 0.90 vs. 0.91 ) . These results   lead to the conclusion that negation may only pose   a small challenge to state - of - the - art transformers .   The results obtained evaluating with the impor-   tant and unimportant negations from the samples   analyzed in Question 2 , however , provide a differ-   ent picture . Indeed , we observe substantial drops in   results in all tasks that have both kinds of negations .   More specifically , we obtain 27 % lower results719with instances containing important negations in   QNLI ( F1 : 0.92 vs. 0.67 ) , 33%/26 % lower in STS-   B , 24 % lower in CommonsenseQA , 21 % lower in   QQP , and 9 % lower in SST . Further , even though all   negations are unimportant in WiC and WSC , we ob-   serve a drop in performance for the instances with   negation compared to the instances without nega-   tion ( WiC : 0.64 vs 0.67 and WSC : 0.59 vs 0.63 ) .   We conclude that transformers trained with existing   NLU corpora face challenges with instances that   contain negation . These results raise two impor-   tant questions for future research : Is negation an   inherently challenging phenomenon for RoBERTa ?   How many instances with negation are required to   solve a natural language understanding task ?   4 Conclusions   We have analyzed the role of negation in eight nat-   ural language understanding corpora covering six   tasks . Our analyses show that ( a ) all but WSC con-   tain almost no negations or around 31%‚Äì54 % of the   negations found in general - purpose texts , ( b ) the   few negations in these corpora are usually unimpor-   tant , and ( c ) RoBERTa obtains substantially worse   results when negation is important .   Our analyses also provide some evidence that   creating models to properly deal with negation may   require both new corpora and more powerful mod-   els . The need for new corpora stems from the an-   swers to Questions 1 and 2 . The justification for   powerful models is more subtle . We point out that   the percentage of unimportant negations ( Section 3 )   is only a weak indicator of the drop in results with   important negations ( Table 4 ) . For example , we   observe a 24 % and 21 % drop in results with impor-   tant negations from CommonsenseQA and QQP   despite 45 % and 97 % of negations are unimportant .   Negation reverses truth values thus solutions to   any natural language understanding task should be   robust when negation is present and important . To   this end , our future work includes two lines of re-   search . First , we plan to create benchmarks for the   six tasks consisting of instances containing nega-   tion ( 50/50 split important / unimportant ) . Second ,   we plan to conduct probing experiments to investi-   gate whether ( and where ) pretrained transformers   capture the meaning of negation . Doing so may   help us discover potential solutions to understand   negation and make inferences . Acknowledgements   This material is based upon work supported by   the National Science Foundation under Grant   No . 1845757 . Any opinions , findings , and con-   clusions or recommendations expressed in this ma-   terial are those of the authors and do not neces-   sarily reflect the views of the NSF . The Titan Xp   used for this research was donated by the NVIDIA   Corporation . Computational resources were also   provided by the UNT office of High - Performance   Computing . Further , we utilized computational   resources from the Chameleon platform ( Keahey   et al . , 2020 ) . We also thank the reviewers for in-   sightful comments .   References720721   A Negation Cue Detection   We develop a negation cue detector ( Section 3 in   the paper ) by utilizing the RoBERTa ( base archi-   tecture ; 12 layers ) pre - trained model ( Liu et al . ,   2019 ) . We fine - tune the system on ConanDoyle-   neg ( Morante and Daelemans , 2012 ) corpus . While   fine - training , the negation cues are marked with   BIO ( B : Beginning of cue , I : Inside of cue , O : Out-   side of cue ) tagging scheme . The contextualized   representations from the last layer of RoBERTa are   passed to a fully connected ( FC ) layer . Finally , a   conditional random field ( CRF ) layer produces the   output sequence for the labels .   Our model yields the following results on the test   set : 93.26 Precision , 94.32 Recall , and 93.79 F1 .   The neural model takes about two hours on average   to train on a single GPU of NVIDIA Tesla K80 . A   list of the tuned hyperparameters that the model   requires to achieve the above results is provided in   Table 5 . The code is available at .722Hyperparameter   Max Epochs 50   Batch Size 10   Learning Rate ( RoBERTa ) 1e-5   Learning Rate ( FC , CRF ) 1e-3   Weight Decay ( RoBERTa ) 0.00001   Weight Decay ( FC ) 0.001   Grad Clipping 5.0   Warmup Epochs 5   Patience 15   Dropout 0.5   Hp-1 Hp-2 Hp-3   CmmnsnsQA 10 16 1e-5   COPA 50 16 1e-5   QQP 3 16 1e-5   STS - B 10 16 1e-5   QNLI 3 8 1e-5   WiC 10 16 1e-5   WSC 200 16 1e-6   SST-2 3 16 1e-5   B Hyperparameters to Fine - tune the   System for Each of the NLU Tasks   We use an implementation by Phang et al . ( 2020 )   and fine - tune RoBERTa ( base architecture ; 12 lay-   ers ) ( Liu et al . , 2019 ) model separately for each   of the eight corpora . We use the default settings   of the hyperparameters , except for a few , when   fine - tuning the model on each benchmark . Table 6   shows tuned hyperparameters for each benchmark.723   Department of Linguistics   University of Illinois   Urbana , Illinois   lanes@illinois.eduLane Schwartz   Institute of Northern Engineering   University of Alaska   Fairbanks , Alaska   loschwartz@alaska.edu   Abstract   1 Introduction   Beginning with our community ‚Äôs Ô¨Årst academic   conference in 1952 ( see ReiÔ¨Çer , 1954 ) and contin-   uing with the establishment of the Association for   Computational Linguistics ( ACL)in 1962 ( MT   Journal , 1962 ) , the members of our research com-   munity have examined a huge range of topics , rang-   ing from linguistic and computational linguistic   models and theories to engineering - focused prob-   lems in natural language processing . While great progress has been made in recent   years across many NLP tasks , the overwhelming   majority of NLP and CL research focuses on a   very small number of languages . Over the 70 years   from 1952 to 2022 , the vast majority of CL and   NLP research has focused on a small number of   widely - spoken languages , nearly all of which repre-   sent politically- and economically - dominant nation-   states and the languages of those nation - states ‚Äô his-   torical and current adversaries : English , the Ger-   manic and Romance languages of western Europe ,   Russian and the Slavic languages of eastern Europe ,   Hebrew , Arabic , Chinese , Japanese , and Korean .   Bender ( 2009 ) surveyed papers from ACL 2008   and found that English dominated ( 63 % of papers ) ,   with 20 other languages distributed along a ZipÔ¨Åan   tail ( Chinese and German shared the number 2 slot   at just under 4 % of papers each ) ; across all ACL   2008 long papers , only three languages ( Hindi ,   Turkish , and Wambaya ) were represented outside   of the language families listed previously . This lack   of diversity directly impacts both the quality and   ethical status of our research , as nearly every suc-   cessful NLP technique in widespread current use   was designed around the linguistic characteristics   of English .   A special theme designed to address this short-   coming has been selected for the 60th Annual   Meeting of the ACL in 2022 : ‚Äú Language Diver-   sity : from Low Resource to Endangered Languages . ‚Äù   This theme is to be commended as a step towards   a more linguistically diverse research agenda . Yet   as we expand our research to a broader and more   inclusive set of languages , we must take great care   to do so ethically . The endangered Indigenous   languages of the world are not merely very low   resource languages . The toxic legacy of colonial-724ism permeates every aspect of interaction between   Indigenous communities and outside researchers   ( Smith , 2012 ) . Ethical research must actively chal-   lenge this colonial legacy by actively acknowledg-   ing and opposing its continuing presence , and by   explicitly acknowledging and centering Indigenous   community goals and Indigenous ways of knowing .   To this end , we propose an ethical framework for   NLP researchers and computational linguists wish-   ing to engage in research involving Indigenous lan-   guages . We begin in ¬ß 2 by examining the abstracts   of papers published in the proceedings of the top-   tier conferences ( ACL , NAACL , EMNLP , EACL ,   AACL ) and journals ( Computational Linguistics ,   TACL ) of the Association for Computational Lin-   guistics from the past several years ( hereafter re-   ferred to as * ACL papers / abstracts ) , replicating   the results of Bender ( 2009 ) , conÔ¨Årming that re-   cent * ACL papers still lack signiÔ¨Åcant language   diversity . In ¬ß 3 we address research practices and   ongoing colonialism in Indigenous communities .   Finally , we examine decolonial practices appropri-   ate for a draft framework of ethical obligations ( ¬ß 4 )   for the ACL research community .   2 Recent * ACL papers lack signiÔ¨Åcant   language diversity   We begin by examining the abstracts of * ACL pa-   pers from the past several years to conÔ¨Årm the re-   sults of Bender ( 2009 ) , namely that recent * ACL   papers still lack signiÔ¨Åcant language diversity . We   collect a corpus of 9602 recent * ACL abstracts   from the ACL Anthology;more than 80 % fail to   mention any language ( see Table 1 ) . Essentially all   such papers that fail the # BenderRule assume En-   glish as the language of study ( Bender , 2019 ) . Van-   ishingly few abstracts mention any Indigenous lan-   guage . While 66 abstracts mention Arabic , fewer   than 20 abstracts mention any other African lan-   guage . Only 11 abstracts mention any Indigenous   language of North America . Only 2 abstracts men-   tion an Indigenous language of Australia . Only 1   abstract mentioned an Indigenous language of Te   Riu - a - M ¬Øaui . No abstracts mentioned any Indige-   nous language of South America .   Table 1 shows a ZipÔ¨Åan distribution predomi-   nated by four language families : Indo - European83.26 % 7995 Implictly assume English   13.70 % 1315 Indo - European ( incl . English )   4.50 % 432 Sino - Tibetan   1.12 % 108 Japonic   0.85 % 82 Afro - Asiatic   0.41 % 39 Turkic   0.26 % 25 Koreanic   0.25 % 24 Austroasiatic   0.24 % 23 Dravidian   0.22 % 21 Uralic   0.21 % 20 Austronesian   0.09 % 9Basque   0.09 % 9Atlantic - Congo   0.07 % 7Na - Dene   0.05 % 5Kra - Dai   0.02 % 2Arnhem   0.02 % 2Iroquoian   0.02 % 2Inuit - Yupik - Unangan   0.01 % 1Sumerian   ( dominated by English ) , Sino - Tibetan ( dominated   by Mandarin Chinese ) , Japonic ( essentially all   Japanese ) , and Afro - Asiatic ( dominated by Ara-   bic and Hebrew ) . Indo - European languages are as-   sumed ( English ) or explicitly mentioned in 97 % of   abstracts . The next three most mentioned language   families account for another 1 % of abstracts . Com-   bined , only 165 out of 9602 abstracts ( 1.7 % ) men-   tion any language from any other language family .   These Ô¨Åndings are also consistent with those of   Joshi et al . ( 2020 ) , who scrape and examine a cor-   pus of approximately 44,000 papers , including both   * ACL papers and papers from LREC , COLING ,   and ACL - afÔ¨Åliated workshops . Joshi et al . present   a 6 - point taxonomy for classifying languages ac-   cording to the quantity of labelled and unlabelled   corpora and models available for each language ,   and Ô¨Ånd that * ACL papers are low in terms of lan-   guage diversity and are dominated by the highest-   resource languages . Unfortunately , we were unable   to apply our language family - level analysis on their   dataset , as it was not publicly available for down-725load . While Joshi et al . ( 2020 ) Ô¨Ånd that language   diversity is somewhat higher at LREC and ACL-   afÔ¨Åliated workshops , the larger issue of language   homogeneity in top - tier * ACL venues is extremely   problematic . In a research community that calls   itself the Association for Computational Linguis-   tics , it is completely unacceptable that fewer than   20 % of top - tier * ACL abstracts mention the name   of any language ( see Table 1 ) , and those that do   are dominated by one language ( English ) and its   language family ( Indo - European ) .   3 Research and Ongoing Colonialism in   Indigenous Communities   The linguistic homogeneity in * ACL papers can   be viewed as a symptom of a much larger prob-   lem , namely that our research paradigms are deeply   rooted in a Western scientiÔ¨Åc tradition that is inex-   tricably intertwined with colonialism . Smith ( 2012 ,   p.50 ) notes that in this tradition , there are implicit   and explicit rules of framing and practice that ex-   press power . In * ACL research , the act of not ex-   plicitly stating any language , of assuming English   as the default , is one such practice .   Research scientists rarely consider the philos-   ophy of science ( Popper , 1959 ) on which our re-   search is predicated ; as Wilson ( 2001 ) notes , this is   deÔ¨Åned by an ontology , epistimology , methodolo-   gies , and axiology that are seldom acknowledged .   In our Ô¨Åeld , these often surface as unacknowledged   positivist ( Comte , 1853 ) assumptions that science   is value - neutral and empirical observations and   logical reasoning fully and completely deÔ¨Åne the   nature of science and reality ( Egan , 1997 ) . The   Ô¨Årst step in enacting decolonial ethical practices   is acknowledging that we hold these assumptions   and recognizing that there are other Indigenous   philosophies of science that are equally valid and   are rooted in fundamentally distinct worldviews   that center relationality ( see Wilson , 2008 ) . By   failing to acknowledge and critically examine the   philosophical foundations of our science , we im-   plicitly and unconsciously elevate our ideas of re-   search and language work above those of Indige-   nous communities ( Leonard , 2017 ) .   Given the distinct value systems and distinct   views of reality of outside research scientists and   Indigenous communities , it is not surprising that   even good - faith efforts of well - meaning outside   researchers are often viewed by Indigenous com-   munities as irrelevant at best and exploitative atworst . Outside perceptions of Indigenous peo-   ples are inextricably linked to corresponding histo-   ries of colonization , and are typically accompanied   by ( usually outdated and incorrect ) assumptions   about the ‚Äú proper ‚Äù roles of Indigeneous peoples   today that correspond with neither reality nor In-   digenous people ‚Äôs views of themselves ( Deloria ,   2004 ; Leonard , 2011 ) . When a linguist ( or a com-   puter scientist ) begins the process of interacting   with an Indigenous community and working with   that community ‚Äôs Indigenous language , the start-   ing ‚Äú lens through which others view [ the linguist ‚Äôs ]   professional activities will at least partly reÔ¨Çect   what ‚Äò linguist ‚Äô has come to mean , and that this in   some cases will occur regardless of whether [ the   linguist ] personally exhibit a trait that has come to   be associated with this named position ‚Äù ( Leonard ,   2021 ) .   Endangered Indigenous languages are not   merely very low - resource languages . Each Indige-   nous community represents a sovereign political   entity . Each Indigenous language represents a cru-   cial component of the shared cultural heritage of   its people . The rate of intergenerational transmis-   sion of Indigenous language from parent to child in   many Indigeneous communities has declined and   is continuing to decline ( Norris , 2006 ) , resulting   in a deep sense of loss felt by older generations   who grew up speaking the Indigenous language as   well as by younger generations who do not speak   the language who experience a diminished sense   of cultural inclusion ( Tulloch , 2008 ) . Language is   an integral part of culture , and declines in robust   Indigenous language usage have been correlated   with serious negative health and wellness outcomes   ( Chandler and Lalonde , 2008 ; Reid et al . , 2019 ) .   At the same time , Indigenous individuals and In-   digenous communities have suffered greatly from   colonial practices that separated children from com-   munities , actively suppressed Indigenous language   and culture , misappropriated land and natural re-   sources , and treated Indigenous people , cultures ,   and languages as dehumanized data to study ( Whitt ,   2009 ; NTRC , 2015 ; Leonard , 2018 ; Bull , 2019 ;   Dei , 2019 ; Guematcha , 2019 ; Bahnke et al . , 2020 ;   Kawerak , 2020 ) . As Smith ( 2012 ) notes , ‚Äú research726is probably one of the dirtiest words in the in-   digenous world ‚Äôs vocabulary ; ‚Äù it is ‚Äú implicated   in the worst excesses of colonialism ‚Äù and ‚Äú told [ In-   digenous people ] things already known , suggested   things that would not work , and made careers for   people who already had jobs . ‚Äù It is then , hardly   surprising that ‚Äú After generations of exploitation ,   Indigenous people often respond negatively to the   idea that their languages are data ready for the tak-   ing ‚Äù ( Bird , 2020 ) .   Indigenous communities are rightly taking up   the slogan ‚Äú Nothing about us without us ‚Äù ( see ,   for example , Pearson , 2015 ) . Even when we con-   sider the ‚Äú lived experiences and issues that under-   lie [ the ] needs ‚Äù of Indigenous communities , these   community priorities are far too often treated as   subordinate to research questions deemed valuable   by members of academe ( Leonard , 2018 ; Wilson ,   2008 ; Simonds and Christopher , 2013 ) . Credu-   lous evangelical claims of technology as savior   only exacerbate these tensions ( Irani et al . , 2010 ;   Toyama , 2015 ) .   4 Prerequisite Obligations for Ethical   Research involving Indigenous   Languages and Indigenous Peoples   When CL and NLP researchers begin to work with   Indigenous language data without Ô¨Årst critically   examining the toxic legacy of colonialism and the   self - identiÔ¨Åed priority needs and epistemology of   the Indigenous community , the risk of unwittingly   perpetuating dehumanizing colonial practices is   extremely high . It is therefore critically urgent   that the ACL , perhaps through the recently - formed   Special Interest Group on Endangered Languages   ( SIGEL ) , should go beyond the ACL ‚Äôs 2020 adop-   tion of the ACM Code of Ethicsand begin a pro-   cess of drafting and adopting a formal ethics policy   speciÔ¨Åcally with respect to research involving In-   digenous communities , Indigenous languages , and   Indigenous data . In so doing , the ACL can provide   speciÔ¨Åc and foundational ethical guidance for our   members that goes far beyond the general ethicalguidance provided by institutional review boards   ( only some of which are intimately familiar with   the ethical pitfalls particular to work with Indige-   nous communities ) .   We should draw upon the recent Linguistics Soci-   ety of America ( 2019 ) ethics statement , the founda-   tional principles of medical ethics ( autonomy , non-   maleÔ¨Åcence , beneÔ¨Åcence , and justice ; Beauchamp   and Childress , 2001 ) , the recommendations of Bird   ( 2020 ) , and the wisdom of Indigenous scholars   such as Deloria , Wilson , Smith , and Leonard .   As a beginning , we have identiÔ¨Åed four key eth-   ical obligations that should at a minimum be in-   cluded in such an ethics policy : cognizance , benef-   icence , accountability , and non - maleÔ¨Åcence .   4.1 Obligation of cognizance   The colonial political and racial ideas and behaviors   that support and enable colonization and oppres-   sion are intentionally invented historical creations   ( Allen , 2012 ; Kendi , 2017 ) . Before we engage with   Indigenous peoples , let alone work with Indige-   nous data , we must intentionally make ourselves   cognizant of this history . As outside researchers ,   we stand in a privileged position , and as such have   an urgent obligation to educate ourselves about this   history and about current practices that perpetu-   ate these systems of oppression in the present day   ( Kendi , 2019 ; Smith , 2012 ) .   Before we are capable of ethically engaging with   Indigenous data , we must learn the ways in which   Indigenous communities approach reality and sci-   ence , and accept that these are fully formed and   fully valid worldviews with which we have an obli-   gation to fully engage . Our research is premised on   a particular philosophy of science which is nearly   always left unstated . We must make ourselves cog-   nizant of our own ontology , epistemology , method-   ology , and axiology , and the fact that there are   alternative philosophies of science that are equally   valid . We must educate ourselves about Indige-   nous ontologies , epistemologies , methodologies ,   and axiologies that are centered around relational-   ity ( Wilson , 2008 ) .   Theobligation of cognizance therefore mandates   that we as researchers intentionally and thoroughly   educate ourselves about colonization of Indige-   nous communities ; about the role that academic   researchers have had and continue to play in the727exploitation of Indigenous communities , Indige-   nous languages , Indigenous culture , and Indige-   nous data ; and about Indigenous expectations and   ways of being centered on relationality that differ   from those we typically encounter in our research .   In practical terms , this cognizance and the edu-   cation requisite in this obligation should typically   be provided by a senior researcher ( one already   very familiar with the relevant issues ) whenever   a new student or junior researcher Ô¨Årst expresses   an interest to begin research involving Indigenous   data . At an institutional level , the leadership of   multilingual NLP shared tasks such as the SIG-   MORPHON shared tasks should take the lead in   educating their respective sub - communities in this   regard as such shared tasks consider expansion to   include Indigenous language data .   4.2 Obligation of beneÔ¨Åcence   Indigenous communities are sovereign political   entities with inherent political and human rights .   Many of these rights are enumerated in the Decla-   ration on the Rights of Indigenous Peoples ( United   Nations , 2007 ) . This includes the right of each   Indigenous community to protect and develop its   culture ( Article 11 ) , the right to dignity ( Article   15 ) , the right to develop and elect its own decision-   making institutions ( Article 18 ) , and the right to   ‚Äú maintain , control , protect , and develop [ the com-   munity ‚Äôs ] intellectual property over [ its ] cultural   heritage , traditional knowledge , and traditional cul-   tural expressions ‚Äù ( Article 31 ) .   The obligation of beneÔ¨Åcence therefore man-   dates that we as researchers ensure that our work   beneÔ¨Åts the Indigenous communities with which   we work in ways that those communities recognize   as beneÔ¨Åcial . In practical terms , this means that   any outside researcher who wants to work with In-   digenous data must seek to engage with the relevant   Indigenous communities in order to learn about and   to meaningfully support priority areas identiÔ¨Åed by   Indigenous governing bodies and decision - making   institutions that fall within our respective scopes   of expertise . Put another way , ethical research in-   volving Indigenous data must include concrete de-   liverables requested by the respective Indigenous   community or communities .   4.3 Obligation of accountability   As outside researchers seeking to work with In-   digenous data , we have a responsibility to seek out   respectful and meaningful relationships with the In - digenous communities whose data we seek to use .   We have a responsibility to develop these relation-   ships in ways that are appropriate and meaningful   to the Indigenous communities with which we seek   to work . We must intentionally acknowledge and   accept the rightful authority of Indigenous com-   munities ‚Äô governing and decision - making bodies   over those communities ‚Äô own respective languages ,   cultures , and data .   Theobligation of accountability therefore man-   dates that we as researchers develop meaningful   relations with the sovereign governing bodies of   the Indigenous communities with which we seek   to engage , and that we be meaningfully account-   able to such bodies in our work involving their data .   This relationship - building should take place before   the research project begins . This relationship be-   tween researcher and sovereign Indigenous insti-   tutions can be thought of as highly analogous to   the relationship between the researcher and govern-   mental granting agencies such as the U.S. National   Science Foundation . In practical terms , once this   relationship has been built and research has begun ,   the researcher should regularly report to and agree   to be held accountable by Indigenous community ‚Äôs   governing and decision - making institutions with   respect to the agreed - upon community goals .   4.4 Obligation of non - maleÔ¨Åcence   Colonization and colonial practices have inÔ¨Çicted   substantial and often genocide - scale harm on In-   digenous communities over the past Ô¨Åve centuries   ( Smith , 2017 ) , harm that is ongoing and is often   perpetuated by modern research practices .   We must intentionally adopt the ethical prime   directive of the medical community , often stated in   the Latin aphorism Primum Non Nocere ‚Äú Above all ,   do no harm ‚Äù ( Smith , 2005 ) . There are many good   and laudable reasons why we should choose to   engage in research with Indigenous communities ,   but none of these reasons is powerful enough to   justify harm caused by our research .   The obligation of non - maleÔ¨Åcence therefore   mandates that above all else , we do no harm to In-   digenous people and Indigenous communities . In   practical terms , this means that researchers seeking   to engage with Indigenous data critically examine   the harmful ramiÔ¨Åcations of proposed work well   before it is conducted . If we can do good through   our research without doing harm , that is well , but   it is better to not engage than to cause harm.728Acknowledgments   Great thanks are due to the elders and commu-   nity members of the Alaska Native communities   of Gambell and Elim who welcomed me and my   family in the 1980s , and from whom I have learned   much . To the Yupik and Yup‚Äôik instructors who   honored me with a Yupik name and who graciously   introduced me to these amazing languages : Igam-   siqanaghhalek !   Special thanks are due to the leadership and   elected councils and boards of the Native Village   of Gambell , Sivuqaq Inc , Gambell Schools , the   Bering Strait School District , the Alaska Native   Language Archive , and the City of Gambell .   This work was supported by the U.S. National   Science Foundation Award # 1761680 : NNA : Col-   laborative Research : Integrating Language Docu-   mentation and Computational Tools for Yupik , an   Alaska Native Language .   References729730A * ACL abstract corpus 2013 ‚Äì Nov. 2021   The * ACL XML Ô¨Åles ( 2013‚Äì2021 ) from the ACL   Anthology GitHub repository were downloaded on   6 November 2021 .   2013.tacl.xml   2014.tacl.xml   2015.tacl.xml   2016.tacl.xml   2017.acl.xml   2017.cl.xml   2017.eacl.xml   2017.emnlp.xml   2017.tacl.xml   2018.acl.xml   2018.cl.xml   2018.emnlp.xml   2018.naacl.xml   2018.tacl.xml   2019.acl.xml   2019.cl.xml   2019.emnlp.xml   2019.naacl.xml   2019.tacl.xml   2020.aacl.xml   2020.acl.xml   2020.cl.xml   2020.emnlp.xml   2020.tacl.xml   2021.acl.xml   2021.eacl.xml   2021.emnlp.xml   2021.naacl.xml   2021.tacl.xml   The abstracts were extracted from the XML Ô¨Åles .   From the resulting abstracts all words that begin   with an uppercase letter were examined manually   to identify all explicitly mentioned language names .   All processing steps are described , with speciÔ¨Åc   shell commands used , in the data annex that ac-   companies this paper.731   Guillaume Le Berre , Christophe Cerisara , Philippe Langlais , Guy LapalmeUniversity of Lorraine , CNRS , LORIA , FranceRALI / DIRO , University of Montreal , Canada   { leberreg , felipe , lapalme}@iro.umontreal.ca , cerisara@loria.fr   Abstract   1 Introduction   In the past years , deep learning models have greatly   improved their performances on a large range of   question answering tasks , especially using pre-   trained models such as BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) and T5 ( Raffel et al . ,   2020 ) . More recently , these models have shown   even better performances when fine - tuned on mul-   tiple question answering datasets at once . Such a   model is UnifiedQA ( Khashabi et al . , 2020 ) , which ,   starting from a T5 model , is trained on a large   number of question answering datasets including   multiple choices , yes / no , extractive and abstractive   question answering . UnifiedQA is , at the time of   writing , state - of - the - art on a large number of ques-   tion answering datasets including multiple - choice   datasets like OpenBookQA ( Mihaylov et al . , 2018 )   or ARC ( Clark et al . , 2018 ) . However , even if Uni-   fiedQA achieves good results on previously unseen   datasets , it often fails to achieve optimal perfor-   mances on these datasets until it is further fine-   tuned on dedicated human annotated data . This   tendency is increased when the target dataset deals   with questions about a very specific domain . One solution to this problem would be to fine-   tune or retrain these models with additionnal hu-   man annotated data . However , this is expensive   both in time and resources . Instead , a lot of work   has been done lately on automatically generating   training data for fine - tuning or even training com-   pletely unsupervised models for question answer-   ing . One commonly used dataset for unsuper-   vised question answering is the extractive dataset   SQUAD ( Rajpurkar et al . , 2016 ) . Lewis et al .   ( 2019 ) proposed a question generation method for   SQUAD using an unsupervised neural based trans-   lation method . Fabbri et al . ( 2020 ) and Li et al .   ( 2020 ) further gave improved unsupervised perfor-   mances on SQUAD and showed that simple rule-   based question generation could be as effective as   the previously mentioned neural method . These   approches are rarely applied to multiple - choice   questions answering in part due to the difficulty   of selecting distractors . A few research papers   however proposed distractor selection methods for   multiple - choice questions using either supervised   approaches ( Sakaguchi et al . , 2013 ; Liang et al . ,   2018 ) or general purpose knowledge bases ( Ren   and Q. Zhu , 2021 ) .   In this paper , we propose an unsupervised pro-   cess to generate questions , answers and associated   distractors in order to fine - tune and improve the per-   formance of the state - of - the - art model UnifiedQA   on unseen domains . This method , being unsuper-   vised , needs no additional annotated domain spe-   cific data requiring only a set of unannotated sen-   tences of the domain of interest from which the   questions are created . Contrarily to most of the   aforementioned works , our aim is not to train a   new completely unsupervised model but rather to   incorporate new information into an existing state-   of - the - art model and thus to take advantage of the   question - answering knowledge already learned .   We conduct our experiments on the SciQ   dataset ( Welbl et al . , 2017 ) . SciQ contains multiple-732Question :   What type of organism is commonly used in   preparation of foods such as cheese and yogurt ?   ( A ) mesophilic organisms ( B ) protozoa   ( C ) gymnosperms ( D ) viruses   Support text :   Mesophiles grow best in moderate temperature ,   typically between 25C and 40C ( 77F and   104F ) . Mesophiles are often found living in or   on the bodies of humans or other animals . The   optimal growth temperature of many pathogenic   mesophiles is 37C ( 98F ) , the normal human   body temperature . Mesophilic organisms have   important uses in food preparation , including   cheese , yogurt , beer and wine .   choice questions ( 4 choices ) featuring subjects cen-   tered around physics , biology and chemistry . An   example of question can be found in Figure 1 .   We focus on the SciQ dataset because it has not   yet been used for training UnifiedQA and it re-   quires precise scientific knowledge . Furthermore ,   our experiments reveal that the direct application   of UnifiedQA on the SciQ benchmark leads to a   much lower performance than when fine - tuning it   on the SciQ training set ( see Section 4 ) . Our ob-   jective in this work is to solve this gap between   UnifiedQA and UnifiedQA fine - tuned on super-   vised data with the unsupervised question genera-   tion approach described in Section 2 . We addition-   ally test our method on two commonly used multi-   ple choice question answering datasets : Common-   senseQA ( Talmor et al . , 2019 ) and QASC ( Khot   et al . , 2020 ) . These datasets contain questions with   similar domains to SciQ even though the questions   are slightly less specific . Furthermore , neither of   them has been used during the initial training of   UnifiedQA .   2 Question Generation Method   We propose a method for generating multiple-   choice questions in order to fine - tune and improve   UnifiedQA . This process is based on 3 steps . First ,   a set of sentences is being selected ( Section 2.1 )   from which a generic question generation system is   applied ( Section 2.2 ) . Then a number of distractors   are added to each question ( Section 2.3).Dataset Sentences Questions   SciQ data 53 270 77 873   SciQ data ( train only ) 45 526 66 552   Wikipedia data 45 327 62 848   2.1 Sentence Selection   Our question generation method uses a set of unan-   notated sentences from which the questions will be   generated . We compare three selection methods .   First , we consider a scenario where the applica-   tion developer does not manually collect any sen-   tence , but simply gives the name ( or topic ) of the   target domain . In our case , the topics are ‚Äú Physics ‚Äù ,   ‚Äú Biology ‚Äù and ‚Äú Chemistry ‚Äù since these are the main   domains in SciQ. A simple information retrieval   strategy is then applied to automatically mine sen-   tences from Wikipedia . We first compute a list   of Wikipedia categories by recursively visiting all   subcategories starting from the target topic names .   The maximum recursion number is limited to 4 . We   then extract the summary ( head paragraph of each   Wikipedia article ) for each of the articles matching   the previously extracted categories and subcate-   gories . We only keep articles with more than 800   average visitors per day for the last ten days ( on   April 27 , 2021 ) , resulting in 12 656 pages .   The two other selection methods extract sen-   tences from SciQ itself and therefore are not en-   tirely unsupervised but rather simulate a situation   where we have access to unannotated texts that   precisely describe the domains of interest such as   a school book for example . The SciQ dataset in-   cludes a support paragraph for each question ( see   Figure 1 ) . Pooled together , these support para-   graphs provide us with a large dataset of texts about   the domains of interest . We gather the paragraphs   corresponding to all questions and split them into   sentences to produce a large set of sentences that   are no longer associated with any particular ques-   tion but cover all the topics found in the questions .   We compare two different setups . In the first one ,   we include all the sentences extracted from the   train , validation and test sets thus simulating a per-   fect selection of sentences that cover all the knowl-   edge expressed in the questions . Still , we only   use the support paragraphs and not the annotated   questions themselves . As compared to the classical733   supervised paradigm , this setting removes all anno-   tation costs for the application developer , but it still   requires to gather sentences that are deemed useful   for the test set of interest . We then compare this   setup with another one , where only the sentences   from the train set are included . This scenario ar-   guably meets more practical needs since it would   suffice to gather sentences close to the domain of   interest . The number of sentences for each dataset   is presented in Table 1 .   2.2 Questions Generation   The generation of questions from a sentence re-   lies on the jsRealB text realizer ( Lapalme , 2021 )   which generates an affirmative sentence from a con-   stituent structure . It can also be parameterized to   generate variations of the original sentence such   as its negation , its passive form and different types   of questions such as who , what , when , etc . The   constituency structure of a sentence is most often   created by a user or by a program from data . In this   work , it is instead built from a Universal Depen-   dency ( UD ) structure using a technique developed   forSR‚Äô19 ( Lapalme , 2019 ) . The UD structure of aQuestion :   What often is found living in or on the bodies   of humans or other animals ?   Right answer : mesophile   Random distractors :   ( A ) the most magnetic material in nature   ( B ) this energy   ( C ) climate   Refined distractors :   ( A ) carbohydrates   ( B ) small cell fragments called platelet   ( C ) echinoderm   sentence is the result of a dependency parse with   Stanza ( Qi et al . , 2020 ) . We thus have a pipeline   composed of a neural dependency parser , followed   by a program to create a constituency structure used   as input for a text realizer , both in JavaScript . Used   without modification , this would create a complex   echo program for the original affirmative sentence ,   but by changing parameters , its output can vary .   In order to create questions from a single con-   stituency structure , jsRealB uses the classical gram-   mar transformations : for a who question , it re-   moves the subject ( i.e. the first noun phrase before   the verb phrase ) , for a what question , it removes   the subject or the direct object ( i.e. the first noun   phrase within the verb phrase ) ; for other types of   questions ( when , where ) it removes the first prepo-   sitional phrase within the verb phrase . Depending   on the preposition , the question will be a when or   awhere . Note that the removed part becomes the   answer to the question .   In order to determine which questions are appro-   priate for a given sentence , we examine the depen-   dency structure of the original sentence and check   if it contains the required part to be removed before   parameterizing the realization . The generated ques-   tions are then filtered to remove any question for   which the answer is composed of a single stopword .   Table 1 shows the number of questions generated   for each dataset . An example of a synthetic ques-   tion is shown in Figure 3.7342.3 Distractors Selection   Since SciQ is a multiple - choice dataset , we must   add distractors to each question we generate , to   match the format of SciQ. A simple solution to this   problem is to select random distractors among an-   swers to other similar questions generated from the   dataset of sentences we gathered . Obviously , select-   ing random distractors may lead to a fine - tuning   dataset that is too easy to solve . Therefore , we   propose another strategy that selects hard distrac-   tors for each question . To do so , starting from our   synthetic dataset with random distractors , we fine-   tune RoBERTa ( Liu et al . , 2019 ) using the standard   method of training for multiple choices question   answering . Each pair question / choice is fed to   RoBERTa and the embedding corresponding to the   first token ( ‚Äú [ CLS ] ‚Äù ) is given to a linear layer to   produce a single scalar score for each choice . The   scores corresponding to every choice for a given   question are then compared to each other by a soft-   max and a cross - entropy loss . With this method ,   RoBERTa is trained to score a possible answer for a   given question , based on whether or not it is a cred-   ible answer to that question . For each question , we   then randomly select a number of candidate distrac-   tors from the answers to other questions and we use   our trained RoBERTa to score each of these can-   didates . The 3 candidates with the highest scores   ( and thus the most credible answers ) are selected .   The idea is that during this first training , RoBERTa   will learn a large amount of simplistic logic . For   example , because of the initial random selection of   distractors , it is highly unlikely that even one of the   distractors will be close enough to the question ‚Äôs se-   mantic field . Furthermore , a lot distractors have an   incorrect grammar ( eg : a distractor might be plural   when the question expects a singular ) . Therefore ,   in this initial training , RoBERTa might learn to iso-   late the answer with a corresponding semantic field   or the one with correct grammar . The re - selection   then minimizes the amount of trivial distractors and   models trained on this new refined dataset will have   to focus on deeper and more meaningful relations   between the questions and the answers . The pro-   cess is better shown in Figure 4 , and an example of   refined distractors can be found in Figure 3 .   The number of scored candidate distractors is   an hyper - parameter . A small number of candidates   may result in a situation where none of the candi-   dates are credible enough , while a large number   requires more computation time , since the score of   each candidate for every question needs to be com-   puted , and has a higher risk of proposing multiple   valid answers . In our experiments , we use a num-   ber of 64 candidates in order to limit computation   time .   3 Training and Implementation Details   To refine distractors , we use the ‚Äú Large ‚Äù version of   RoBERTa and all models are trained for 4 epochs   and a learning rate of 1√ó10 . These hyper-   parameters are chosen based on previous exper-   iments with RoBERTa on other multiple - choice   datasets . The final UnifiedQA fine - tuning is done   using the same multiple choices question answer-   ing setup as the one used in the original UnifiedQA   paper ( Khashabi et al . , 2020 ) . We use the ‚Äú Large ‚Äù   version of UnifiedQA and all the models are trained   for 4 epochs using Adafactor and a learning rate   of1√ó10 . The learning rate is loosely tuned   to get the best performance on the validation set   during the supervised training of UnifiedQA . We   use the Hugging Face pytorch - transformers ( Wolf   et al . , 2020 ) library for model implementation . Ex-   periments presented in this paper were carried out   using the Grid‚Äô5000 testbed ( Balouek et al . , 2013 ) ,   supported by a scientific interest group hosted by   Inria and including CNRS , RENATER and sev-   eral Universities as well as other organizations ( see   https://www.grid5000.fr ) .   4 Results   Accuracy results in Table 2 have a 95 % Wald con-   fidence interval of ¬±2.8 % . The first row of Table 2   presents the accuracy results of a vanilla UnifiedQA   large model on SciQ. The second line shows the ac-   curacy when UnifiedQA is fine - tuned over the full   training corpus . Our objective is thus to get as close   as possible to this accuracy score using only un-735supervised methods . The results using Wikipedia   are the only ones that are unsupervised and there-   fore are the ones directly comparable to UnifiedQA   with no fine - tuning or other unsupervised methods .   The other results serve to illustrate what could be   obtain with a tighter selection of sentences .   Model Dev Test   UnifiedQA ( no fine - tuning ) 64.6 63.4   UnifiedQA ( supervised ) 78.7 78.7   Unsupervised - Random distractors   SciQ data 71.3 70.8   SciQ data ( train only ) 70.9 70.1   Wikipedia data 68.3 67.5   Unsupervised - Refined distractors   SciQ data 75.4 74.2   SciQ data ( train only ) 73.1 72.4   Wikipedia data 70.6 69.4   Fine - tuning UnifiedQA on synthetic questions   with random distractors improves the results as   compared to the baseline and , as expected , the   closer the unlabeled sentences are to the topics   of the questions , the better is the accuracy . Hence ,   generating questions from only the train set of SciQ   gives performances that are comparable but slightly   lower to the ones obtained from the combined train ,   dev and test set of SciQ. Finally , questions selected   from Wikipedia also improve the results , despite   being loosely related to the target test corpus . Our   distractor selection method further boosts the ac-   curacy results in all setups . This suggests that a   careful selection of distractors is important , and   that the hard selection criterion used here seems   adequate in our context .   The results for CommonsenseQA and QASC us-   ing the same selection of sentences from Wikipedia   are reported in table 3 . Overall , we obtain similar   results to SciQ with a large improvement of perfor-   mances when generating questions and a further   boost with refined distractors . However compared   to SciQ , the improvement brought by the distractor   refining process is less significant . This could be   partly explained by the fact that the distractors inModel CQA QASC   UnifiedQA ( no fine - tuning ) 60.9 44.5   UnifiedQA ( supervised ) 74.3 61.0   Wikipedia data ( Random ) 64.9 57.2   Wikipedia data ( Refined ) 65.1 59.4   the original QASC and CommonsenseQA datasets   are overall easier and therefore it is less advanta-   geous for a model to be trained on harder questions .   5 Conclusion   In this work , we proposed a multiple - choice ques-   tion generation method that can be used to fine - tune   the state - of - the - art UnifiedQA model and improve   its performance on an unseen and out of domain   dataset . Our contributions are :   ‚Ä¢We have shown that simple unsupervised   methods could be used to finetune existing   multipurpose question answering models ( in   our case UnifiedQA ) to new datasets or do-   mains .   ‚Ä¢We propose a novel distractor refining method   able to select harder distractors for a given   generated question and show its superiority   compared to a random selection .   Future work includes comparing our method to   other question generation methods ( including su-   pervised methods : Liu et al . ( 2020 ) , Puri et al .   ( 2020 ) ) in order to assess the effect of both the   generation method and the questions quality on the   final performances of our models . Also , we will   further compare different variations of our ques-   tion generation and distractor refining methods in   order to more thoroughly understand the effect of   hyper - parameters such as the number of candidate   distractors .   References736737738   Ling Liu and Mans Hulden   University of Colorado   first.last@colorado.edu   Abstract   1 Introduction   The Transformer model has delivered convincing   results in many different tasks related to word-   formation and analysis ( Vylomova et al . , 2020 ;   Moeller et al . , 2020 , 2021 ; Liu , 2021 ) . Especially   on inÔ¨Çection tasks , where an input lemma such   asdog , and input inÔ¨Çectional features such as   { N , PL } , are expected to produce an output such as   dogs , the model has shown to be particularly adept   at generalizing patterns ( Vylomova et al . , 2020 ; Liu   and Hulden , 2020a , b ; Wu et al . , 2021 ) . However ,   we have discovered that this is only true if the train-   ing data covers a diversity of lemmata or some   variant of the input lemma to be inÔ¨Çected has been   witnessed during training . In a ‚Äú wug test ‚Äù ( Berko ,   1958 ) setting where the witnessed lemmata are usu-   ally limited and a previously unseen lemma ‚Äî like   wug ‚Äî is to be inÔ¨Çected in some way , we Ô¨Ånd that   the Transformer almost completely fails to general-   ize inÔ¨Çection patterns , despite abundant inÔ¨Çected   forms for training . It has been noted earlier that   neural sequence - to - sequence models are apt to per-   form poorly for morphological inÔ¨Çection if they   have been exposed to little training data and data   augmentation can be leveraged to alleviate the prob-   lem ( Cotterell et al . , 2017 , 2018 ; Kann and Sch√ºtze ,   2017 ; Liu and Hulden , 2021 ) . Our starting point   is our observation that the poor ‚Äú wug test ‚Äù perfor-   mance is maintained even with abundant training   inÔ¨Çected forms .   In our study , we show three main results . ( 1 )   We demonstrate that , even if trained with relatively   large amounts of inÔ¨Çected forms , a Transformer   model of the kind that has been very successful at   recent shared tasks largely fails to generalize in-   Ô¨Çection patterns if it has not been exposed during   training to a variety of lemmata or any lemmata in739the test set . This is true even for datasets where all   words inÔ¨Çect in the same way ‚Äî i.e. there are no in-   Ô¨Çectional classes or allomorphs of morphemes , as   is found in the low - resource Niger - Congo language   datasets used in SIGMORPHON 2020 shared task   ( Vylomova et al . , 2020 ) . ( 2 ) We show that simply   exposing the model to uninÔ¨Çected lemmata in the   test set ‚Äî without providing a single inÔ¨Çected form ‚Äî   allows the model to dramatically improve its perfor-   mance when inÔ¨Çecting such lemmata . ( 3 ) Further ,   we investigate several strategies that avoid leverag-   ing test set lemmata . We show that when inducing   a copy bias in the model by hallucinating new lem-   mata , or by hallucinating new inÔ¨Çected forms , the   method of hallucination is much more effective if   it is sensitive to substrings of syllable - like length   rather than individual characters or stems . Our   best models achieve substantial improvement upon   earlier state - of - the - art data hallucination methods   ( Silfverberg et al . , 2017 ; Anastasopoulos and Neu-   big , 2019 ) .   2 Data   2018 - languages We use six languages from   the CoNLL - SIGMORPHON 2018 shared task 1   medium setting , where each language has 1,000 triples   for training ( Cotterell et al . , 2018 ) . The six   languages , Czech , Finnish , German , Russian ,   Spanish and Turkish , are selected to represent the   diversity of language typology and morphological   inÔ¨Çection challenges . Though there are only   1,000 training triples , they cover a fair number   of lemmata as each lemma appears only once or   twice , an amount very hard to obtain for really   low - resource languages . In the original shared   task , between 2 % and 27 % of the lemmata in the   dev and test sets are also found in the training set .   To prepare training data for the ‚Äú wug test‚Äù-like   circumstance , we select the UniMorph ( Kirov et al . ,   2018 ) paradigms for the Ô¨Årst 100 most frequent   lexemes found in Wikipedia text , which are not   included in the 2018 shared task 1 dev and test   sets . The shared task dev and test sets are used for   validation and evaluation without any change . The   100 full inÔ¨Çection tables give us over 1,000 ( for   Czech , German and Russian ) or over 7,000 ( for   Finnish , Spanish and Turkish ) training triples . Niger - Congo languages In addition , we use   six Niger - Congo languages from SIGMORPHON   2020 shared task 0 ( Vylomova et al . , 2020 ): Akan ,   Ga , Lingala , Nyanja , Southern Sotho and Swahili .   These languages are low - resource , but the dataset   only contains very regular inÔ¨Çections . In the orig-   inal shared task data split , The overlap between   the lemmata in the dev and test sets and those in   the training set is 100 % . The number of paradigms   which we can obtain by combining the training , dev   and test sets of this dataset is around 100 for Akan ,   Ga and Swahili , 227 for Nyanja , 57 for Lingala and   only 26 for Southern Sotho .   For the ‚Äú wug test ‚Äù , we divide the inÔ¨Çection tables   reconstructed from this dataset into a 7:1:2 train-   dev - test split , i.e. we use the same ratio as the   shared task , but the division is by inÔ¨Çection tables   rather than lemma - tag - form triples , to ensure that   the lemmata used for validation and test are disjoint   from those for training . We provide details on the   data statistics in Appendix A for reference .   3 Experiments   InÔ¨Çection model The Transformer ( Vaswani   et al . , 2017 ) is the seq2seq architecture which pro-   duces the current state - of - the - art result on the mor-   phological inÔ¨Çection task ( Vylomova et al . , 2020 ;   Liu and Hulden , 2020a , b ; Wu et al . , 2021 ) . It takes   the lemma and target tag(s ) as input and predicts   the target form character by character . Our experi-   ments use the Transformer implemented in fairseq   ( Ott et al . , 2019 ) and adopt the same hyperparame-   ters as Liu and Hulden ( 2020a ) .   Evaluation metric The evaluation metric is ac-   curacy . For the original shared task data and exper-   iments on 2018 languages , we train Ô¨Åve inÔ¨Çection   models each with a different random initialization   and report the average accuracy with standard de-   viation . Due to data scarcity , for Niger - Congo lan-   guages at the ‚Äú wug test‚Äù-like setting , we perform   a 5 - fold cross - validation and report the average   accuracy and the standard deviation .   Common - practice test and ‚Äú wug test ‚Äù We Ô¨Årst   compare the performance of the Transformer in the   common - practice setting and the ‚Äú wug test‚Äù-like   setting . The ‚Äú common practice ‚Äù is represented by740   previous years ‚Äô shared tasks and related work ( Cot-   terell et al . , 2016 , 2017 , 2018 ; McCarthy et al . ,   2019 ; Vylomova et al . , 2020 ) ; here the training   data usually covers a fair number of lemmata and   there is overlap between lemmata in the training   and test sets . We use the shared task data to rep-   resent the common - practice setting . In the ‚Äú wug   test ‚Äù setting , we control the number of lemmata   for training but not inÔ¨Çected forms ( as explained   in Section 2 ) and the lemmata to be inÔ¨Çected are   always previously unseen . To our surprise , the per-   formance of the Transformer at the ‚Äú wug test‚Äù-like   setting is very poor despite the large amount of   training triples for 2018 - languages or the very reg-   ular and straightforward inÔ¨Çection for Niger - Congo   languages . The performance is dramatically infe-   rior to the common - practice setting , even when the   number of training triples is seven times larger for   Finnish , Spanish and Turkish ( see Figure 1 ) .   We hypothesize four reasons for the poor per-   formance of the model under the ‚Äú wug test‚Äù-like   circumstance : ( 1 ) missing copy bias regarding the   entire stem , i.e. the model ca n‚Äôt copy a stem abcde   if that exact stem has never been seen during train-   ing , ( 2 ) missing copy bias on individual letters , i.e.   the model ca n‚Äôt copy letter aif the letter is under-   represented in training , ( 3 ) missing copy bias on   subsequences of letters , i.e. the model ca n‚Äôt copy   sequence abif the sequence is underrepresented in   training , ( 4 ) some combination of all the factorsabove . To test these hypotheses , we conduct Ô¨Åve   experiments designed to help the model learn to   copy with different biases by adding to the training   set for each language 2,000dummy data points   generated in Ô¨Åve different ways , explained below .   + copy - dev - test - lemmas In order to test the Ô¨Årst   hypothesis that the model does not learn to copy   parts of a stem it has not seen at the training stage ,   we augment the training data for each language by   adding to it the lemmata in its development and   test sets with a special tag COPY . In other words ,   2,000 triples are added to   the initial ‚Äú wug test ‚Äù training set for each language .   + copy-2k - char and + copy-2k - substr Previous   work found that adding random strings can help   seq2seq models learn a copy bias and thus improve   the performance when the training data is limited   ( Kann and Sch√ºtze , 2017 ) . We adopt a similar   method to augment the training data with dummy   lemmata generated by the process shown in Figure   2 ( a ) . The + copy-2k - char method takes as input   the alphabet created by collecting characters in the   language ‚Äôs training set .   Considering that a natural linguistic sub - unit   of a word is a syllable , we propose to use sub-741   strings of syllable - like length for the + copy-2k-   substr method . The input of this method is the   set of bigrams , trigrams and four - grams from the   language ‚Äôs training data . For both methods , we   generate the dummy lemma by uniformly sampling   from the input and concatenating the sampled items   to a random length between the minimum and max-   imum word length we see in the training data . The   output of the dummy lemma generation process is   a triple of a dummy lemma , a special symbol COPY   and the dummy lemma , which is added to the initial   ‚Äú wug test ‚Äù training set for data augmentation .   + hall-2k - char and+hall-2k - substr The dummy   lemma generation methods do not leverage knowl-   edge about word structure which can be inferred   from the training data . Silfverberg et al . ( 2017 )   found that it is very effective to augment training   data in low - resource situations with a data halluci-   nation approach by replacing a hypothesized stem   of the training triples with a random string . Anas-   tasopoulos and Neubig ( 2019 ) improves this data   hallucination method by taking into discontinuous   stems into consideration as well ; this is the best   data hallucination method so far . We conduct the+hall-2k - char experiment by augmenting the initial   ‚Äú wug test ‚Äù training set with dummy data generated   with Anastasopoulos and Neubig ( 2019 ) ‚Äôs method .   The implementation from SIGMORPHON 2020   shared task 0 baseline is used .   In addition , we propose to generate the dummy   stem by uniformly sampling from substrings of   syllable - like length , i.e. the bigram , trigram and   four - gram set . This experiment is referred to as   + hall-2k - substr . SpeciÔ¨Åcally , both data hallucina-   tion methods ( illustrated in Figure 2 ( b ) ) take as   input a triple from the training set , aligns the lemma   and the target form with the alignment method from   SIGMORPHON 2016 shared task baseline ( Cot-   terell et al . , 2016 ) , Ô¨Ånds the common substrings   between the lemma and the target form as the stem ,   replaces the stem with a dummy stem , and out-   puts a dummy triple which is adopted for data   augmentation . Our proposed method is different   from Anastasopoulos and Neubig ( 2019 ) ‚Äôs method   at the dummy stem generation step in two main   aspects : ( 1 ) Instead of sampling from the alpha-   bet , we sample from the set of bigrams , trigrams   and four - grams . ( 2 ) Instead of forcing the dummy   stem to be of the same length as the stem to be742replaced , we only constrain the minimum and max-   imum length of the stem based on the training data .   In addition , for discontinuous stems , we only re-   place the Ô¨Årst part of the stem .   4 Results and discussion   ‚Äú Wug test ‚Äù with data augmentation Figure 3   shows results for the ‚Äú wug test‚Äù-like setting and   results after augmenting the initial training set with   different methods . Every language sees a substan-   tial improvement with data augmentation , indicat-   ing that the Transformer model in the vanilla ‚Äú wug   test ‚Äù circumstance will not learn a copy bias well .   The substring - based data hallucination we pro-   pose , + hall-2k - substr , achieves accuracies which   are substantially higher than other methods for   most languages . For Turkish and Nyanja , + hall-2k-   substr is lower than the best performance , but the   difference is not obvious . For Lingala , + hall-2k-   substr has the same best performance as + hall-2k-   char . The consistent advantage of + hall-2k - substr   implies that substrings of syllable - like length is   more helpful than individual characters for data   hallucination . It also provides support to the fourth   hypothesis we made in section 3 that the poor per-   formance of the Transformer in the vanilla ‚Äú wug   test‚Äù-like setting is due to a combination of factors   including missing copying bias for letters , subse-   quences of letters and even entire stems .   Common practice vs ‚Äú wug test ‚Äù Figure 1 plots   the Transformer accuracies with standard devia-   tions in the common - practice setting , vanilla ‚Äú wug   test‚Äù-like setting , and ‚Äú wug test‚Äù-like setting with   data augmentation by the substring - based data hal-   lucination methods ( + hall-2k - substr ) . Though   data augmentation can improve the model ‚Äôs per-   formance for a ‚Äú wug test ‚Äù , results are still infe-   rior to the common practice setting without any   data augmentation for most languages , especially   the morphologically challenging 2018 CoNLL-   SIGMORPHON languages .   5 Conclusion   In this work , we examine limiting the number of   training lemmata and keeping training lemmata   disjoint from the evaluation sets in morphologi-   cal inÔ¨Çection . By comparing the performance ofthe Transformer under the ‚Äú wug test‚Äù-like circum-   stance with the common practice , we Ô¨Ånd that the   common - practice setting where the training data   covers a fair amount of lemmata and there is over-   lap of lemmata in training and evaluation , has ob-   scured the difÔ¨Åculty of the task . We propose to aug-   ment the training data with substring - based data   hallucination , and achieve substantial improvement   over previous data hallucination methods .   Considering the Ô¨Åndings in this paper , we sug-   gest that future experiments include evaluations on   model performance using lemmata not found in the   training set and use unique lemma counts rather   than triple counts to document data set sizes .   References743744745A Data information   triple - counts lemma - counts lemma - overlap ( % )   Language train dev test train dev test dev - train test - train   czech 1000 1000 1000 848 848 849 24.53 20.38   Ô¨Ånnish 1000 1000 1000 985 983 987 2.34 3.04   german 1000 1000 1000 961 945 962 9.42 9.46   russian 1000 1000 1000 973 985 977 3.65 3.79   spanish 1000 1000 1000 906 902 922 15.74 16.49   turkish 906 928 912 764 802 779 26.06 26.57   triple - counts lemma - counts lemma - overlap ( % )   Language train train dev - train test - train   czech 1582 100 0 0   Ô¨Ånnish 7136 100 0 0   german 1290 100 0 0   russian 1311 100 0 0   spanish 7132 100 0 0   turkish 7632 100 0 0   triple - counts lemma - counts lemma - overlap ( % )   Language train dev test train dev test dev - train test - train   akan 2793 380 763 96 94 95 100.0 100.0   ga 607 79 169 95 59 80 100.0 100.0   lingala 159 23 46 57 23 34 100.0 100.0   nyanja 3031 429 853 227 199 226 100.0 100.0   southern sotho 345 50 99 26 24 25 100.0 100.0   swahili 3374 469 910 97 97 96 100.0 100.0746B Data augmentation size comparison747C Performance of the encoder - decoder with hard monotonic attention model   Considering that the encoder - decoder with hard monotonic attention model ( Aharoni et al . , 2016 ; Aharoni   and Goldberg , 2017 ; Makarov et al . , 2017 ; Makarov and Clematide , 2018c , a , b ; Wu et al . , 2018 ; Wu and   Cotterell , 2019 ) is designed for the morphological generation task and bias towards copying symbols in   the input by leveraging edit actions , we evaluate the performance of the encoder - decoder with exact hard   monotonic attention in the ‚Äú wug test‚Äù-like circumstance as well in order to evaluate whether this deep   learning model architecture catered to morphological generation is able to learn the generalization ability .   We use the encoder - decoder with exact hard monotonic attention model proposed and implemented by   Wu and Cotterell ( 2019 ) .   The performance of the encoder - decoder with exact hard monotonic attention model for the original   shared task setup , the ‚Äú wug test‚Äù-like setup with or without our best data hallucination augmentation   is presented in Figure 5 . Figure 6 provides detailed comparison between different data augmentation   methods in the ‚Äú wug test‚Äù-like experimental setup by the encoder - decoder with exact hard monotonic   attention model . We observe that the encoder - decoder with exact hard monotonic attention model has the   same limitation as the Transformer model pointed out in the previous section.748749   Jan Deriu , Don Tuggener , Pius von D√§niken , Mark Cieliebak   Zurich University of Applied Sciences ( ZHAW ) , Winterthur , Switzerland   deri@zhaw.ch   Abstract   1 Introduction   One major issue in developing conversational dia-   logue systems is the signiÔ¨Åcant efforts required for   evaluation . This hinders rapid developments in this   Ô¨Åeld because frequent evaluations are not possible   or very expensive . The goal is to create automated   methods for evaluating to increase efÔ¨Åciency . Un-   fortunately , methods such as BLEU ( Papineni et al . ,   2002 ) have been shown to not be applicable to con-   versational dialogue systems ( Liu et al . , 2016 ) . Fol-   lowing this observation , in recent years , the trend   towards training methods for evaluating dialogue   systems emerged ( Lowe et al . , 2017 ; Deriu and   Cieliebak , 2019 ; Mehri and Eskenazi , 2020 ; Deriu   et al . , 2020 ) . The models are trained to take as   input a pair of context and candidate response , and   output a numerical score that rates the candidate   for the given context . These systems achieve high   correlations to human judgments , which is very   promising . Unfortunately , these systems have been   shown to suffer from instabilities . ( Sai et al . , 2019 )   showed that small perturbations to the candidate   response already confuse the trained metric . This   work goes one step further : we propose a method   that automatically Ô¨Ånds strategies that elicit very   high scores from the trained metric while being ofobvious low quality . Our method can be applied to   automatically test the robustness of trained metrics   against adversarial strategies that exploit certain   weaknesses of the trained metric .   Our method uses a trained metric as a reward   in a Reinforcement Learning setting , where we   Ô¨Åne - tune a dialogue system to maximize the re-   ward . Using this approach , the dialogue system   converges towards a degenerate strategy that gets   high rewards from the trained metric . It converges   to three different degenerate types of strategies to   which the policy converges in our experiments : the   Parrot , the Fixed Response , and the Pattern . For   each dataset and metric , an adversarial response is   found , which belongs to one of the three strategy   types . The responses generated from these strate-   gies then achieve high scores on the metric . Even   more , in most cases , the scores are higher than   the scores achieved by human written responses .   Figure 1 shows the pipeline . The dialogue policy   receives a reward signal from the trained metric.750Over time , the policy converges to a Ô¨Åxed response ,   which objectively does not match the context but   gets a near - perfect score on the trained metric . We   release the code .   2 Related Work   Trained Metrics . In recent years the Ô¨Åeld of   trained metrics gained traction after word - overlap   methods have been shown to be unreliable ( Liu   et al . , 2016 ) . The Ô¨Årst of these metrics is   ADEM ( Lowe et al . , 2017 ) , which takes as input   a context , a reference , and the candidate response   and returns a score . The main issue with ADEM   is the reliance on references and annotated data   ( i.e. , human ratings of responses ) , which are costly   to obtain , and need to be redone for each domain .   RUBER ( Tao et al . , 2018 ) extended ADEM by re-   moving the reliance on annotated data for training .   However , it still relies on a reference during in-   ference . AutoJudge ( Deriu and Cieliebak , 2019 )   removed the reliance on references , which allows   the evaluation of multi - turn behavior of the dia-   logue system . However , AutoJudge still leverages   annotated data for training . USR ( Mehri and Eske-   nazi , 2020 ) is a trained metric that does not rely on   either annotated data or any reference . It is trained   in a completely unsupervised manner while still   highly correlated to human judgment ( 0:4Spear-   man Correlation ) . Similarly , MAUDE ( Sinha et al . ,   2020 ) is trained as an unreferenced metric built to   handle the online evaluation of dialogue systems .   Robustness of Trained Metrics . There is not yet   much research on the robustness of trained met-   rics . Sai et al . ( 2019 ) evaluated the robustness   of ADEM by corrupting the context in different   ways . They show that by just removing punctua-   tion , the scores of ADEM change , and in 64 % of   cases are superior to the scores given for the same   response without removed punctuation . Other cor-   ruption mechanisms yielded similar results . Yeh   et al . ( 2021 ) compared a large variety of automated   metrics for dialogue system evaluation by compar-   ing , e.g. , turn- and dialogue - level correlation with   human judgemnts and studying the impact of the   dialogue length . They Ô¨Ånd that no single metric   is robust against all alternations but see potential   in ensembling different metrics . Novikova et al .   ( 2017 ) investigate automated metrics in the task-   oriented NLG domain and Ô¨Ånd that the metrics doAlgorithm 1 : Advantage Actor - Critic Al-   gorithm , where denotes the policy , cde-   notes the context , rthe response generated   by the policy , and sdenotes the score by   the automated metric , i.e. , the reward .   not sufÔ¨Åciently reÔ¨Çect human ratings .   3 Method   Our method applies a trained metric as a reward   signalR(c;r)to update a dialogue system (c)in a   reinforcement learning setting , where cdenotes the   context and rthe response . The dialogue system   is trained by generating a response for a context ,   which is then scored by the automated metric . The   dialogue system is then updated using the score   as the reward . This process is repeated for differ-   ent contexts . We use the Actor - Critic framework   to optimize the policy ( Sutton et al . , 1999 ) . See   Algorithm 1 for an overview . The policy gradient   is deÔ¨Åned as5J( ) = 5log(rjc)A(r;c ) ,   where(rjc)deÔ¨Ånes the probability of the gener-   ated response for the given context , and A(c;r)the   advantage function .   The learned policy depends on the reward func-   tion , i.e. , the automated metric . If the reward func-   tion is susceptible to adversarial attacks , the policy   will likely generate an objectively suboptimal solu-   tion , which is rated highly by the automated metric .   Conversely , we expect the policy to improve the di-   alogue systems ‚Äô responses if the automated metric   is robust against adversarial examples .   4 Experimental Setup   4.1 Datasets   We perform the evaluation on three widely - used   datasets in the dialogue modelling domain . Namely ,   Dailydialog ( Li et al . , 2017 ) , Empathetic Dialogues   ( Rashkin et al . , 2019 ) , and PersonaChat ( Zhang   et al . , 2018).751   4.2 Metrics   We use various state - of - the - art automated metrics   developed for evaluating conversational dialogue   systems without reference , i.e. , so - called unrefer-   enced metrics .. These are metrics where no refer-   ence is needed , i.e. they only use the context and   response to determine the score . They can be rep-   resented as a function s = R(c;r ) , which rate the   responserfor a given context c.   We selected state - of - the - art trained metrics   which achieve good correlations to human   judgments to evaluate our approach ‚Äî namely ,   USR ( Mehri and Eskenazi , 2020 ) , ATT ( Gao et al . ,   2021 ) , and MAUDE ( Sinha et al . , 2020 ) . Addi-   tionally , we added the Blender language model   score ( BlenderLM ) ( Roller et al . , 2020 ) . For the   ATT , MAUDE , and BlenderLM metrics , we   use the out - of - the - box models provided by the re-   spective authors . For the USR metric , we per-   form custom training on each dataset . Further-   more , we report the USR - retrieval ( USR Ret ) , USR-   masked - language - model USR MLM , and the USR-   regression USR Full scores . Note that the USR Full   is a combination of the USR Ret andUSR MLM   metric . More details can be found in Appendix A.4.3 Strategies   For our approach , we use Blenderbot as our policy   ( Roller et al . , 2020 ) since it is currently a state-   of - the - art conversational dialogue system . We   use the validation set for each domain to perform   reinforcement learning . This is to avoid the di-   alogue systems being Ô¨Åne - tuned on already seen   data . We use the test set to evaluate the reward   over the number of episodes . We perform the re-   inforcement learning for 15 epochs , where each   epoch is composed of 500 updates . We noted from   pre - experiments that this is enough for a dialogue   system to converge to a degenerate strategy . We   track the average reward achieved on the test set   after each epoch . Each experiment is repeated 10   times since we expect the policy to converge to   slightly different strategies in different runs . We   select the repetition which achieved the highest   score ( i.e. , reward ) and use it to determine the strat-   egy . We also experimented with automated strategy   detection , see Appendix B.   5 Results   The policies typically converge towards one of the   following three degenerate strategies .   Parrot . Here , the policy simply copies parts of   the context into the response . Sometimes , it ap-   plies slight changes . For instance , it changes the   pronouns from " you " to " I " .   Fixed Response . Here , the policy converges on a   Ô¨Åxed response which it returns regardless of the752   context .   Pattern . This is a mix between the Parrot and the   Fixed Response . It creates a Ô¨Åxed template Ô¨Ålled   with parts of the context .   Table 1 shows the selected responses for each   pair of domain and metric . For all metrics except   ATT , the Ô¨Åxed response is composed of a grammat-   ically correct sentence . Note that these responses   are always returned by the Ô¨Åne - tuned dialogue sys-   tem , regardless of the context .   5.1 Scores   Table 2 shows the main results . In almost all cases ,   the degenerated strategy outperforms the vanilla   Blenderbot and humans with respect to the auto-   mated metric . The most striking example is the ATT   metric , where the Ô¨Åxed response achieves scores by   orders of magnitude better than the ones achieved   by humans . For both USR Ret andMAUDE , the   scores achieved by the Ô¨Åxed response are almost   perfect , i.e. , they are close to 1.0 , which is the upper   bound . Also , for USR MLM , the scores are signiÔ¨Å-   cantly higher than the ones achieved by Blenderbot .   Interestingly , the USR FULL seems to be more   immune to the pattern that were found . However ,   even for USR FULL , the parrot strategy beats the   humans by a signiÔ¨Åcant margin in the PersonaChat   domain . Copy . We also display the scores achieved by sim-   ply copying the context on each metric , which is   inspired by the Parrot strategy . The only metric   which is immune to the Copy strategy is ATT . Un-   der all the other metrics , the Copy achieves very   high scores . In some cases , it achieves even better   scores than the converged policy . For instance , for   theDailydialog domain , it achieves 0:811points   under the USR MLM metric , which is 0:3point   higher than the converged policy and twice as good   as the human score .   6 Conclusion   Trained metrics for automatic evaluation of conver-   sational dialogue systems are an attractive remedy   for the costly and time - consuming manual evalua-   tion . While high correlation with human judgments   seems to validate the metrics regarding their abil-   ity to mimic human judging behavior , our analysis   shows that they are susceptible to rather simple   adversarial strategies that humans easily identify .   In fact , all metrics that we used failed to recognize   degenerate responses . Our approach is easily adapt-   able to any newly developed trained metric that   takes as input a pair of context and response . There   are no known remedies for this problem . Thus , the   next open challenge is to Ô¨Ånd methods that improve   the robustness.753References754A Correlation between Human   Judgements and Trained Metrics   In this section , we evaluate the metrics with regards   to their correlation to human judgments to show   that these metrics have reasonable performance .   For this , we sample 100contexts for each domain .   For each domain , we use a set of bots to create a   response for each context . Furthermore , we add the   human response to the pool of responses for each   context . Then , we let crowdworkers annotate the   responses . We correlate the scores of each metric   on the same set of contexts and responses to the   human annotations .   A.1 Domains and Bots   We perform the evaluation on the three datasets   from the main paper .   Dailydialog . We prepared 5 bots using Par-   lAI ( Miller et al . , 2017 ) . We Ô¨Åne - tune a GPT-2   ( GPT ) model ( Radford et al . , 2018 ) , a BERT - Rank   ( BR ) model , a sequence - to - sequence model ( S2 )   with attention , and a weakly trained sequence - to-   sequence model ( DR ) . We also use the Blender   model ( Roller et al . , 2020 ) , although it was not   speciÔ¨Åcally tuned on Dailydialog .   Empathetic Dialogues . We prepared the same   pool of models as in Dailydialog .   PersonaChat . We mostly reuse the openly avail-   able systems of the ConvAI2 challenge ( Dinan   et al . , 2020 ) , namely , Lost in Conversation(LC )   and Huggingface ( HF ) , and KVMemNN ( KV ) .   We also add the Blender model , which is also   trained in this domain , a custom - trained BERT-   Rank model ( BR ) , and a sequence - to - sequence   model ( S2 ) . Together with the DR model , the pool   consists of 7 different dialogue systems .   A.2 Annotation Process   Since we perform the evaluation on a static - context   setting , we also add the human response ( i.e. , the   gold response ) to the pool of systems . For eval-   uation , we use 600 samples for Dailydialog and   Empathetic Dialogues each , and 800 samples for   the PersonaChat domain . Each sample is composed   of a context ( sampled from the test set ) , and a gen-   erated response . We annotated the overall quality   of each sample on a Likert scale from 0 ( bad ) to755   2 ( good ) using Mechanical Turk . Each sample is   annotated by three different humans . As the Ô¨Ånal   score , we use the average score of the three anno-   tations . For each metric , we apply the metric to   all samples , and then compute the Spearman cor-   relation between the human scores and the scores   predicted by the metric .   A.3 Correlation to Human Judgements   Table 3 shows the correlations of the human judg-   ments to each of the metrics for each domain .   For all domains , the USR metric performs best ,   achieving strikingly high correlations to humans .   MAUDE also achieves good correlation scores on   the PersonaChat domain , and ATT performs well   on the Empathetic Dialogues domain . BlenderLM   has mediocre performance on all domains equally .   A.4 Original USR   Note that the USR Ret scores are signiÔ¨Åcantly   higher than in the original paper ( Mehri and Es-   kenazi , 2020 ) , which is due to the fact that we   use more turns to represent the context , whereas   the original implementation uses only the previous   turn for the context . In the original implementation ,   USR Ret achieves a Spearman correlation of 48:67   on our annotated data . If we train our implementa-   tion of USR Ret using only one turn to represent the   context , we also achieve a Spearman correlation   of40:34 , which is comparable to the original . We   did not experience a discrepancy on the USR MLM   model , where the original model achieves the same   correlation as ours .   B Strategy Selection   We observed in our experiments that the dialogue   system almost always converges to one of three de-   generate strategies . In order to atomize their detec-   tion in the experiments , we used a set of heuristics   for their identiÔ¨Åcation . B.1 Heuristics   Since the strategies are very simple , we propose   heuristics to detect the policy automatically . This   avoids the need for manual inspection of a poten-   tially large amount of log Ô¨Åles . For this , we intro-   duce the following measures .   ‚Ä¢Response Frequency . The percentage of times   that the same response is generated for all   samples in the test set .   ‚Ä¢Lexical Variety . The ratio between number   of different tokens and the total number of   tokens over all responses in the test set .   ‚Ä¢BLEU score . The BLEU score between the   context and the response . This is computed   for each pair of context and responses and   then averaged over all samples in the test set .   ‚Ä¢Jaccard score . The Jaccard overlap between   the context and response tokens . Analogous   to the BLEU score , the Jaccard overlap is com-   puted between each context - and response - pair ,   and then averaged over all samples in the test   set .   These measures can be used to detect the various   strategies the policy converges to . For instance ,   a high Response Frequency indicates that the pol-   icy converges to a Ô¨Åxed response . A high BLEU   score and Jaccard score indicate that the policy   converges to the parrot strategy . A low Response   Frequency , a low Lexical Variety and a moderate   Jaccard score indicate that the policy converges to   a pattern . A pattern is composed of a Ô¨Åxed template   where parts are Ô¨Ålled with tokens from the context .   B.2 Application of the Heuristics   For each run , we use these metrics to determine   which strategy the policy has converged on . The Ô¨Å-   nal strategy is extracted by selecting the best epoch   across all 10 runs for each domain . If the Re-   sponse Frequency is larger than 0.7 , we extract the   most common sentence and use this as our Ô¨Åxed   response . If the BLEU score is larger than 0.2 ,   we assign the parrot strategy . If the Response Fre-   quency is smaller than 0.1 , the Lexical Variety is   smaller than 0.15 , and the Jaccard score is larger   than 0.05 , it indicates a pattern emerged . In this   case , we manually extract the pattern .   B.3 Overview   Table 4 shows the measures used to perform the au-   tomated strategy selection . The automated strategy756   selection worked in 72 % of cases . There are two   main cases in which it was not conclusive . First ,   for the ATT metric , where for both the Dailydialog   andPersonaChat domains no clear Ô¨Åxed response   arose . However , after manual inspection , we noted   that for the PersonaChat the policy generated the   same tokens in various frequencies and orders . For   theDailydialog the most frequent response arose   in55 % of cases . Thus , we used this Ô¨Åxed response .   The second case is the BLM metric . For all the   domains we selected the most frequent response ,   although it appeared in less than 70 % of cases .   C Full Results   Table 5 shows all scores achieved by the dialogue   systems on the respective metrics . Furthermore ,   we also added the average score of the Amazon   Mechanical Turk judges , which ranges from ( 0 - 2 ) .   D Technical Explanation   One potential reason why our approach is able to   Ô¨Ånd a degenerate strategy lies in the exploration   problem in reinforcement learning . Blender ‚Äôs lan-   guage model can be interpreted as a policy which   performs a sequence of actions , i.e. , sampling a   sequence of tokens . Thus , the language model loss   during standard Blender training can be interpreted   as an indicator for how sure the policy is of its ac-   tions . A high language model loss indicates that the   policy assigns low probability scores to its actions .   Conversely , a low language model loss indicates   that the policy is sure of it ‚Äôs actions . This could   be further investigated by measuring the entropy of   the language model . Indeed , in all our experiments ,   we notice that the language model loss collapses to-   ward a very small value . This indicates that the lan-   guage model collapsed to a single simple strategy .   Figure 2 shows the language model loss over thenumber of steps . The loss quickly collapses from   an average of 4 points to around 0.5 points . At the   same time the average reward ( orange ) rises from   0.78 to 0.92 . Similarly , the response frequency   rises from 0 to 0.94 . In the middle , the loss rises   again , which indicates the search for a new strategy .   This coincides with a lower response frequency .   E Examples   In Tables 6 , 7 , and 8 , we show examples of the   outputs from the Ô¨Åne - tuned Blenderbot model . For   each of the Ô¨Åve metrics , we show the output to   which Blenderbot converged to when using the   metric as a reward . Furthermore , we show the   score which the respective metric assigns to the   generated response . Note that the Parrot strategies   simply copy the text form the context . For the Em-   pathetic Dialogues dataset , the degenerate strategy   prepends a " I ‚Äôm not sure " to the context . For the   PersonaChat , the degenerate strategy prepends a   " i ‚Äôve always wanted to " . The Copy strategy ( see   Table 2 in main Paper ) , ignores these prefaces , and   simply copies the context.757Dailydialog   AMT USR R USR MLM USR F ATT MAUDE BLM   BR 1.836 0.928 0.409 7.904 0.0006 0.898 0.177   BL 1.386 0.440 0.426 4.951 0.0002 0.664 0.096   HF 1.656 0.925 0.080 6.989 0.0026 0.866 0.371   HU 1.782 0.928 0.409 7.904 0.0006 0.898 0.183   S2 1.024 0.512 0.300 5.050 0.0003 0.895 0.183   DR 0.729 0.308 0.338 3.900 0.0001 0.891 0.204   P - 0.998 0.811 9.429 0.0002 0.921 0.233   F - - 0.505 - 0.435 0.985 0.239   P - - - 7.091 - - -   Empathetic Dialogues   AMT USR R USR MLM USR F ATT MAUDE BLM   BR 1.808 0.891 0.384 7.611 0.120 0.942 0.260   BL 1.640 0.935 0.298 7.645 0.001 0.820 0.087   HF 1.610 0.887 0.644 8.292 0.044 0.948 0.462   HU 1.816 0.891 0.384 7.611 0.120 0.942 0.264   S2 0.702 0.493 0.145 4.510 0.010 0.932 0.159   DR 0.822 0.354 0.182 3.759 0.001 0.936 0.199   P - 0.996 0.8848 9.617 0.054 0.935 0.358   F - - 0.912 - 0.731 0.976 0.333   P - - - 7.240 - - -   PersonaChat   AMT USR R USR MLM USR F ATT MAUDE BLM   BR 1.350 0.725 0.211 6.120 0.0020 0.946 0.138   BL 1.507 0.847 0.185 6.797 0.0006 0.844 0.070   HF 1.480 0.794 0.272 6.707 0.0023 0.925 0.152   HU 1.623 0.927 0.267 7.512 0.0024 0.951 0.153   KV 1.147 0.538 0.217 4.982 0.0023 0.852 0.122   LC 1.572 0.879 0.103 6.769 0.0011 0.918 0.195   S2 0.681 0.390 0.121 3.814 0.0013 0.845 0.111   DR 0.906 0.482 0.268 4.779 0.0002 0.537 0.220   P - 0.925 0.794 8.933 0.0001 0.898 0.223   F - 0.977 0.852 - 0.813 0.933 0.250   P - - - - - - -758759760761   Siyang Liu , Sahand Sabour , Yinhe Zheng , Pei Ke , Xiaoyan Zhu   Minlie Huang   Abstract   1 Introduction   The diversity of generated texts is an important   evaluation aspect for dialogue generation models   since most dialogue models tend to produce gen-   eral and trivial responses ( e.g. " I do n‚Äôt know " or   " Me too " ) ( Li et al . , 2016 ; Zhao et al . , 2017 ) . Sev-   eral metrics have been proposed to evaluate the text   diversity , and the Distinct score ( Li et al . , 2016 ) is   the most widely applied metric due to its intuitive   nature and convenient calculation . It has become   a de facto standard to report the Distinct score to   compare the performance of different models in   terms of response diversity ( Liu et al . , 2016 ; Fan   et al . , 2018 ; Sabour et al . , 2022 ; Wu et al . , 2021c ;   Zhou et al . , 2021 ; Wu et al . , 2021b ; Zhang et al . ,   2020 ; Zheng et al . , 2020 ; Wang et al . , 2020 ; Liu   et al . , 2021 ) . Most previous works follow the initial   approach of Li et al . ( 2016 ) to calculate the Distinct   score , i.e. , dividing the number of unique tokens   ( n - grams ) by that of all tokens ( n - grams ) . However ,   although reported to be effective , we surprisingly   find that this naive approach tends to introduce a   higher penalty for longer texts and lead to inaccu-   rate evaluation of text diversity .   We argue that the scaling factor of Distinct re-   quires a comprehensive discussion for two rea-   sons . First , prior research in non - computational   linguistics has demonstrated the shortcomings of   Distinct ‚Äôs scaling approach ( Malvern et al . , 2004 ) .   We found that early applications of Distinct ex-   ist in psycholinguistics , where researchers lever-   aged this metric to assess the language diversity of   children with communication disorders ( Chotlos ,   1944 ) . Their research showed that as a child speaks   more words , Distinct experiences an adverse de-   cline since each extra word that the child utters adds   to the total number of words , yet it would only in-   crease the number of distinct words if the word had   not been used before ( Malvern et al . , 2004 ; Chotlos ,   1944 ) . Second , we also discovered an uncommon   decline of this metric on both a natural corpus and a   designated distribution sampler when the total num-762ber of words increases . As illustrated in Figure 1 ,   the original Distinct can not produce a stable value   and experiences a sharp decrease with increasing   utterance length in both natural and designated dis-   tributions . However , as a qualified metric needs to   support quantitative comparison among different   methods , its value should stay invariant when the   distribution of the words appearing is determined .   This result is consistent with the findings of psy-   chologists , indicating an unfair penalty does exist   in such a scaling method .   Our contributions are summarized as follows :   1.We investigate the performance of the origi-   nalDistinct and demonstrate that this metric is not   sufficiently fair due to its scaling method . We also   highlight the risks of using this metric for evaluat-   ing response diversity .   2.We propose Expectation - Adjusted Distinct   ( EAD ) , an improved version of Distinct based on   that the scaling factor should be the expectation of   the number of distinct tokens instead .   3.Human evaluation shows that our metric cor-   relates better with human judgments . We further   discuss the drawbacks of this metric and suggest   its feasible applications in practice .   2 Preliminary Discussion about Original   Distinct   To demonstrate the shortcoming of the original Dis-   tinct , we illustrated the normalised Distinct scores   on two types of texts at different lengths ( Figure   1 ) . The first type of text is sampled from an arti-   ficially designated distribution while the other is   sampled from a natural language corpus . In de-   tail , we adopted P(X = k ) = RdŒªas our   designated distribution , where vis vocabulary size .   In our experiments , we use BERT ‚Äôs vocabulary ‚Äôs   size ( v= 30522 ) ( Devlin et al . , 2019 ) . In addition ,   we leveraged OpenSubtitlesas our natural lan-   guage corpus . For each length , we sampled 2000   sentences as a set and calculated scores of each set .   As shown in Figure 1 , We observe that the origi-   nalDistinct scores decrease sharply with increas-   ing utterance length in both distributions . We can   observe that given the same distribution of words   ( original - designated ) , lengthier texts will get lower   scores than shorter texts . We highlighted this prob-   lem because it is extremely simple for models to   control the length of texts by using decoding tricks ,   e.g. adjusting the penalty coefficient ( Vijayakumaret al . , 2016 ) . In such cases , it might seem that a   model has outperformed other models on this met-   ric . However , as shown by our experiments , it is   not reasonable to assume that this model gener-   ates more diverse responses . The same observa-   tion can be made for the natural language corpus   ( original - designated ) . As language distribution is   more complex than what we are able to formulate ,   we depicted the performance of the original Dis-   tinct on 6 famous datasets in Appendix . These   cases indicate that the original Distinct is not a   suitable metric for evaluating diversity .   3 Improving Original Distinct   3.1 Formula Derivation   The original Distinct score ( Li et al . , 2016 ) is mea-   sured as Distinct = N / C , where Nis the number   of distinct tokens and Cis the total number of to-   kens . To improve the original scaling method , we   propose that the scaling factor should be the expec-   tation of the distinct words in the set of generated   responses . Hence , the calculation becomes   EAD = N   Eh   ÀÜNi . ( 1 )   Supposing a set of generated responses Rwith   sizeSto be evaluated , we let lbe the itoken   ofkresponse in Randtbe the length of k   response . The expectation E[ÀÜN]forÀÜNdistinct   words to appear in Rwould be   Eh   ÀÜNi   = EÔ£Æ   Ô£∞X_1Ô£π   Ô£ª ( 2 )   = XPÔ£´   Ô£≠{_1}= 1Ô£∂   Ô£∏   = X(1‚àíYP(lÃ∏=u , ... , lÃ∏=u ) ) ,   where Vis the vocabulary size , and { u , ... , u}is   the set of all tokens in the vocabulary .   As shown in Equation 2 , the calculation requires   us to know P(lÃ∏=u , lÃ∏=u , ... , lÃ∏=u ) .   Though current models can easily estimate the   probability of a word appearing in a sequence ,   it is hard to calculate the probability of each   word that never appears in any position of the se-   quence . Thus , there is no efficient way to calculate763P(lÃ∏=u , ... , lÃ∏=u ) ) . In addition , different   language distributions have different P , which leads   to different expectations and make the metric less   general . Thus , we measure the upper bound of re-   sponse diversity ( i.e. a set of generated responses   where each token appears with equal probability )   to calculate this expectation . We hypothesize that   the scaling effect of the upper bound is approxi-   mately proportional to that of other sets of gener-   ated responses ; therefore , it can replace the original   scaling factor .   As mentioned above , we hypothesize   Eh   ÀÜNi   ‚àù‚àºEh   ÀÜNi   ,   where Eh   ÀÜNi   can be calculated as   Eh   ÀÜNi   = X(1‚àíYYP(lÃ∏=u ) )   = V[1‚àí(V‚àí1   V ) ] . ( 3 )   Thus , the EAD score is calculated as :   EAD = N   V[1‚àí ( ) ] . ( 4 )   We discuss more details on the formula ‚Äôs properties   and the vocabulary size in the Appendix .   3.2 Experimental Verification   3.2.1 Evaluation Approach   We collect responses from ten dialogue generation   methods as reported by Wang et al . ( 2021 ) , and   compare EAD with the original uni - gram Distinct   ( Li et al . , 2016 ) . More details of these ten methods   can be find in Appendix .   We follow previous works ( Tao et al . , 2018 ; Sel-   lam et al . , 2020 ) to evaluate the correlation of each   automatic metric with human judgments . Specif-   ically , the Pearson , Spearman , and Kendall ‚Äôs Tau   correlation coefficients are reported . Pearson ‚Äôs cor-   relation estimates linear correlation while Spear-   man ‚Äôs and Kendall ‚Äôs correlations estimate mono-   tonic correlation , with Kendall ‚Äôs correlation being   usually more insensitive to abnormal values . We   used SciPyfor correlation calculation and signifi-   cance test3.2.2 Datasets   Our experiments use two open - domain dialog gen-   eration benchmark datasets : DailyDialog(Li et al . ,   2017 ) , a high - quality dialog dataset collected from   daily conversations , and OpenSubtitles , which   contains dialogs collected from movie subtitles ( see   Table 1 for more details ) . We follow the data pro-   cessing procedures reported by Wang et al . ( 2021 ) .   3.2.3 Preliminary Observations   Based on the obtained results ( check Table 2 ) , it   can be observed that Expectation - Adjusted Distinct   has a clear edge over the original Distinct : first , the   contrast between diversity of generated responses   for different methods is highlighted more effec-   tively by EAD ( e.g. though AdaLab gets the highest   diversity score using Distinct ( 3.96 ) , its difference   from other methods is not as evident as its EAD   score ( 9.63 ) ) ; second , contrary to Distinct , EAD   provides a more accurate evaluation of response   diversity . For instance , the Distinct scores for CP   and UL are both 2.35 while responses generated   by UL are found to be more diverse than CP using   EAD ( 5.35 > 5.08 ) . Given that the average length   of responses generated by FL is larger than CP , Dis-   tinct ‚Äôs bias towards models that generate shorter   sentences becomes evident . These observations are   consistent for both datasets .   3.2.4 Correlation Results   We recruited crowdsourcing workers to evaluate   the diversity of the selected methods . For each   method , we randomly sampled 100 subsets of 15   responses from their set of generated responses .   Response sets of all methods , given the same query   set , were packaged together as an evaluation set .   We asked each crowdsourcing worker to assign a   diversity score to every response group in the eval-   uation set . Each group was evaluated by at least   3 workers . For ensuring the quality of our anno-   tations , we calculated the score of each set as the   average of workers ‚Äô scores and filtered out workers   whose scores had an insufficient correlation with764   the average ( Pearson Correlation < 0.65 ) . We ac-   knowledge that building a scoring standard for an-   notating language diversity is challenging . Hence ,   we did not require our workers to give an absolute   score for each set . Instead , we asked them to high-   light the contrast between different sets by scoring   values that linearly reflect the response diversity   difference between the sets . For instance , the two   sets of scores { 1,2,2}and{2,5,5}show the same   evaluation since the same contrast is shown . We   then normalized the scores to the [ 0 - 10 ] range .   Then , we calculated the correlation between the   Distinct scores with the crowdsourced values for   all the methods . The results are provided in Table   2 . The evaluation results indicate that our proposed   EAD is more consistent with human judgments for   measuring response diversity , as it shows the high-   est correlation with human evaluations among all   correlation metrics ( Pearson/ Spearson/ Kendall ‚Äôs   Tau ) on both datasets .   4 EAD in Practice   AsEAD is based on the idealized assumption that   does not take language distribution into account ,   we further discuss this problem and propose a po-   tential practical way of Expectation - Adjusted Dis-   tinct in real situations . Before applying EAD , it   is necessary to explore the relationship between   score and text length ( Figure 1 ) and check the per-   formance of EAD on the training data . To our   knowledge , if the training data is from large - scale   open - domain sources such as OpenSubtitles and   Reddit , EAD can maintain its value on differentlengths . Hence , it can be directly used for evaluat-   ing models trained on these datasets . However , we   found our experiments on datasets such as Twitter   showed a decline in EAD on lengthier texts . This is   probably because input length limitations on these   platforms ( e.g. 280 words on Twitter ) , which in-   duces users to say as much information as possible   within a shorter length . In these situations , it is   unfair to use EAD to evaluate methods that tend to   generate lengthier texts .   5 Related Work   Li et al . ( 2016 ) proposed Distinct , calculated as   the number of distinct tokens divided by the total   number of tokens . This automatic metric is de-   signed to evaluate the diversity of texts , and it has   been widely used in developing various text gener-   ation tasks , such as dialogue generation ( Wu et al . ,   2021a ; Zheng et al . , 2021a , b , 2019 ) or story gener-   ation ( Guan et al . , 2021 ) . However , as we showed   in Figure 1 , it is an unfair indicator as it is affected   by the sample length . This causes a bias against   models which tend to generate longer sentences .   There exist other metrics for evaluating diversity   but none are as widely - used as Distinct ( Zhu et al . ,   2018 ; Xu et al . , 2018 ) . Specifically , Self - BLEU   proposed by Zhu et al . ( 2018 ) is extremely time-   consuming as its computation complexity is O(n ) ,   where ndenoted the size of the test set .   6 Conclusion   In this paper , we present an improved variation of   the Distinct metric , which is a widely - used measure765for evaluating response diversity in dialog systems .   We provide the theoretical formulation and empiri-   cal evaluation of our proposed metric ( Expectation-   Adjusted Distinct ) . The results demonstrated that   Expectation - Adjusted Distinct has a higher corre-   lation with human evaluation in comparison with   other metrics . The proposed metric is not limited   to dialogue generation models but also suitable to   evaluate text generation tasks where diversity mat-   ters .   7 Acknowledgements   This work was supported by the National Science   Foundation for Distinguished Young Scholars ( with   No . 62125604 ) and the NSFC projects ( Key project   with No . 61936010 and regular project with No .   61876096 ) . This work was also supported by the   Guoqiang Institute of Tsinghua University , with   Grant No . 2019GQG1 and 2020GQG0005 . We   were grateful to Dr. Xiangxiang Xu at MIT for his   help in mathematical formulation .   References766767   A Comparison on More Datasets   To demonstrate the shortcomings of the original   Distint metric , we illustrate original Distinct on 6   datasets : Persona - chat ( Zhang et al . , 2018 ) , Ubuntu   Dialog Corpus ( Lowe et al . , 2015 ) , DailyDialog ,   Topic - Chat ( Gopalakrishnan et al . , 2019 ) , Empa-   thetic Dialogs ( Rashkin et al . , 2018 ) , Wizard of   Wikipedia ( Dinan et al . , 2018 ) , Reddit ( Serban   et al . , 2015 ) , and Twitter ( Ritter et al . , 2010 ) ( Fig-   ure 1 ) . It can be observed that with an increasing   sample length , the original Distinct score tends to   follow a linear decline while the proposed metric   maintains its consistency .   B Property Discussion   Formula Property 1 . Expectation - Adjusted Dis-   tinct increases faster as Cis increasing , but its   incremental rate converges to , as shown by its   derivative below :   dEAD   dN=1   V[1‚àí()](5 )   limdEAD   dN=1   V(6 )   whereas in the original Distinct , we have   dDistinct   dN=1   C(7 )   We can see from the original metric that the bigger   Cis , the slower the original Distinct increases . It   is the reason why this metric is not fair to those   models that tend to generate longer sentences .   Formula Property 2 . Expectation - Adjusted Dis-   tinct converges to(‚â§1 ) asCincreases .   limEAD = limN   V[1‚àí()](8 )   = N   V<= 1 , ( 9)768   where‚àà[0,+‚àû ] . Theoretically ,   Expectation - Adjusted Distinct can have values   larger than 1 ( e.g. when N = V ) , which is an   extremely rare case in practice : as we utilized the   upper bound for measuring the expectation , it is   exceptionally hard for Nto obtain an equal value   to or an even greater value than E(ÀÜN ) .   C Details of Human Evaluation   Our created human evaluation interface is provided   in Figure 3 .   D How to Determine Vocabulary Size   As we discussed the properties of Expectation-   Adjusted Distinct , vocabulary size makes little im-   pact on changing its value when it has reached a   large number ( usually more than 30000 ) , so it is not   necessary to measure an exact value . To compare   different methods , it is recommended to use a com-   mon vocabulary size , ( such as BERT ‚Äôs 30522 ) ( De-   vlin et al . , 2019 ) . It is also reasonable to calculate   the vocabulary size of a dataset by NLTK tokenizer ,   when research focuses on a specific dataset . For   non - english corpora , we recommend researchers   to determine a vocabulary size following Xu et al .   ( 2021 ) .   E Details of Evaluated Methods   Wang et al . ( 2021 ) proposed a novel adaptive label   smoothing method for diversified response gener - ation . Their experiments were conducted on the   DailyDialog and OpenSubtitles datasets , using 9   recent methods for diverse response generation as   their baselines ( similar to what we demonstrated in   our paper ) . Wang et al . ( 2021 ) used a transformer-   based sequence - to - sequence model ( Vaswani et al . ,   2017 ) as the backbone of their model , and most of   their hyper - parameters follow ( Cai et al . , 2020 ) . In   addition , both the encoder and the decoder contain   6 transformer layers with 8 attention heads , and   the hidden size is set to 512 . BERT ‚Äôs WordPiece   tokenizer ( Devlin et al . , 2019 ) and Adam optimizer   ( Kingma and Ba , 2015 ) are used for training their   models with random initialization and a learning   rate of 1e-4.769770   Ivan Habernal   Trustworthy Human Language Technologies   Department of Computer Science   Technical University of Darmstadt   ivan.habernal@tu-darmstadt.de   www : trusthlt : org   Abstract   1 Introduction   Differential privacy ( DP ) , a formal mathematical   treatment of privacy protection , is making its way   to NLP ( Igamberdiev and Habernal , 2021 ; Senge   et al . , 2021 ) . Unlike other approaches to protect   privacy of individuals ‚Äô text documents , such as   redacting named entities ( Lison et al . , 2021 ) or   learning text representation with a GAN attacker   ( Li et al . , 2018 ) , DP has the advantage of quan-   tifying andguaranteeing how much privacy can   be lost in the worst case . However , as Habernal   ( 2021 ) showed , adapting DP mechanisms to NLP   properly is a non - trivial task .   Representation learning with protecting pri-   vacy in an end - to - end fashion has been recently   proposed in DPText ( Beigi et al . , 2019b , a ; Al-   nasser et al . , 2021 ) . DPText consists of an auto-   encoder for text representation , a differential - privacy - based noise adder , and private attribute   discriminators , among others . The latent text rep-   resentation is claimed to be differentially private   and thus can be shared with data consumers for   a given down - stream task . Unlike using a pre-   determined privacy budget " , DPText takes " as a   learnable parameter and utilizes the reparametriza-   tion trick ( Kingma and Welling , 2014 ) for random   sampling . However , the downstream task results   look too good to be true for such low " values . We   thus asked whether DPText is really differentially   private .   This paper makes two important contributions   to the community . First , we formally analyze   the heart of DPText and prove that the employed   reparametrization trick based on inverse continu-   ous density function in DPText is wrong and the   model violates the DP guarantees . This shows that   extreme care should be taken when implementing   DP algorithms in end - to - end differentiable deep   neural networks . Second , we propose an empir-   ical sanity check which simulates the actual pri-   vacy loss on a carefully crafted dataset and a re-   construction attack . This supports our theoretical   analysis of non - privacy of DPText and also con-   Ô¨Årms previous Ô¨Åndings of breaking privacy of an-   other system ADePT .   2 Differential privacy primer   Suppose we have a dataset ( database ) where each   element belongs to an individual , for example Al-   ice , Bob , Charlie , up to m. Each person ‚Äôs entry ,   denoted with a generic variable x , could be an ar-   bitrary object , but for simplicity consider it a real   valued vector x2R. An important premise is   that this vector contains some sensitive informa-   tion we aim to protect , for example an income   ( x2R ) , a binary value whether or not the person771has a certain disease ( x2f0:0;1:0 g ) , or a dense   representation from SentenceBERT containing the   person ‚Äôs latest medical record ( x2R ) . This   dataset is held by someone we trust to protect the   information , the trusted curator .   This dataset is a set from which we can create   2subsets , for instance X = fAliceg , X=   fAlice;Bobg , etc . All these subsets form a uni-   verseX , that isX;X;2X , and each of them   is also called ( a bit ambiguously ) a dataset .   DeÔ¨Ånition 2.1 . Any two datasets X;X2X are   called neighboring , if they differ in one person .   For example , X = fAliceg;X = fBobgor   X = fAlice;Bobg;X = fBobgare neighboring ,   whileX = fAliceg;X = fAlice , Bob , Charlie g   are not .   DeÔ¨Ånition 2.2 . Numeric query is any function f   applied to a dataset Xand outputting a real-   valued vector , formally f : X!R.   For example , numeric queries might return an   average income ( f!R ) , number of persons   in the database ( f!R ) , or a textual summary   of medical records of all persons in the database   represented as a dense vector ( f!R ) . The   query is simply something we want to learn from   the dataset . A query might be also an identity   function that just ‚Äò copies ‚Äô the input , e.g. , f(X=   f(1;0)g)!(1;0)for a real - valued dataset X=   f(1;0)g .   An attacker who knows everything about Bob ,   Charlie , and others would be able to reveal Al-   ice ‚Äôs private information by querying the dataset   and combining it with what they know already .   Differentially private algorithm ( or mechanism )   M(X;f)thus randomly modiÔ¨Åes the query out-   put in order to minimize and quantify such attacks .   Smith and Ullman ( 2021 ) formulate the principle   of differential privacy as follows : ‚Äú No matter what   they know ahead of time , an attacker seeing the   output of a differentially private algorithm would   draw ( almost ) the same conclusions about Alice   whether or not her data were used . ‚Äù   Let a DP - mechanism M(X;f)have an arbi-   trary rangeR(a generalization of our case of nu-   meric queries , for which we would have R = R ) .   Differential privacy is then deÔ¨Åned as   Pr(XjM(X;f ) = z )   Pr(XjM(X;f ) = z)exp(")Pr(X )   Pr(X)(1)for all neighboring datasets X;Xand allz2   R , where Pr(X)andPr(X)is our prior knowl-   edge ofXandX. In words , our posterior knowl-   edge ofXorXafter observing zcan only grow   by factor exp(")(Mironov , 2017 ) , where " is apri-   vacy budget ( Dwork and Roth , 2013 ) .   3 Analysis of DPText   In the heart of the model , DPText relies on the   standard Laplace mechanism which takes a real-   valued vector and perturbs each element by a ran-   dom draw from the Laplace distribution .   Formally , let zbe a real - valued d - dimensional   vector . Then the Laplace mechanism outputs a   vector ~zsuch that for each index i= 1;:::;d   ~z = z+s ( 2 )   where each sis drawn independently from a   Laplace distribution with zero mean and scale b   that is proportional to the ` sensitivity and the   privacy budget " , namely   sLap   = 0;b=   "    ( 3 )   The Laplace mechanism satisÔ¨Åes differential   privacy ( Dwork and Roth , 2013 ) .   3.1 Reparametrization trick and inverse   CDF sampling   DPText employs the variational autoencoder ar-   chitecture in order to directly optimize the amount   of noise added in the latent layer parametrized   by " . In other words , the scale of the Laplace   distribution becomes a trainable parameter of the   network . As directly sampling from a distribu-   tion is known to be problematic for end - to - end   differentiable deep networks , DPText borrows the   reparametrization trick from Kingma and Welling   ( 2014 ) .   In a nutshell , the reparametrization trick decou-   ples drawing a random sample from a desired dis-   tribution ( such as Exponential , Laplace , or Gaus-   sian ) into two steps : First draw a value from   another distribution ( such as Uniform ) , and then   transform it using a particular function , mainly the   inverse continuous density function ( CDF ) .   As a matter of fact , sampling using the in-   verse CDF is a well - known and widely used772method ( Devroye , 1986 ; Ross , 2012 ) and forms   the backbone of probability distribution generators   in many popular frameworks .   3.2 Inverse CDF of Laplace distribution   The inverse cumulative distribution function of   Laplace distribution Lap(;b)is :   F(u ) =  bsgn(u 0:5 ) ln(1 2ju 0:5j )   ( 4 )   whereuUni(0;1)is drawn from a standard   uniform distribution ( Sugiyama , 2016 , p. 210 ) ,   ( Nahmias and Olsen , 2015 , p. 303 ) . An equivalent   expression without the sgnand absolute functions   is derived , e.g. , by Li et al . ( 2019 , p. 166 ) as   F(u ) =(   bln(2u ) +  ifu<0:5    bln(2(1 u ) ) ifu0:5   ( 5 )   where again uUni(0;1 ) .   An alternative sampling strategy , as shown , e.g. ,   by Al - Shuhail and Al - Dossary ( 2020 , p. 62 ) , as-   sumes that the random variable is drawn from a   shifted , zero - centered uniform distribution   vUni (  0:5;+0:5 ) ( 6 )   and transformed through the following function   F(v ) =  bsgn(v ) ln(1 2jvj ) ( 7 )   While both ( 4 ) and ( 7 ) generate samples from   Lap(;b ) , note the substantial difference between   uandv , since each is drawn from a different uni-   form distribution ( see visualizations in Fig . 1 ) .   3.3 Proofs of DPText violating DP   According to Eq . 3 in ( Alnasser et al . , 2021 ) , Eq . 9   in ( Beigi et al . , 2019a ) which is an extended ver-   sion of ( Beigi et al . , 2019b ) , in Eq . 14 in ( Beigi   et al . , 2021 ) , and personal communication to con-   Ô¨Årm the formulas , the main claim of DPText is as   follows ( rephrased ):   DPText utilizes the Laplace mech-   anism , which is DP ( Dwork and Roth ,   2013 ) . It implements the mechanism as   follows : Sampling a value from stan-   dard uniform   vUni(0;1 ) ( 8)   and transforming using   F(v ) =  bsgn(v ) ln(1 2jvj )   ( 9 )   is equivalent to sampling noise from   Lap(b ) .   This claim is unfortunately false , as it mixes up   both approaches introduced in Sec . 3.2 . As a con-   sequence , the Laplace mechanism using such sam-   pling is not DP , which we will Ô¨Årst prove formally .   Theorem 3.1 . Sampling using inverse CDF as   in DPText using ( 8) and ( 9 ) does not produce   Laplace distribution .   Proof . We will rely on the standard proof of sam-   pling from inverse CDF ( see Appendix A ) . The   essential step of that proof is that the CDF is in-   creasing on the support of the uniform distribu-   tion , that is on [ 0;1 ] . However , Fas used in   ( 9 ) is increasing only on interval [ 0;0:5](Fig . 1 ) .   Forv0:5 , we get negative argument to lnwhich   yields a complex function , whose real part is even   decreasing . Therefore ( 9 ) is not CDF of any prob-   ability distribution , if used with Uni ( 0;1 ) .   As a consequence , the output ln(v0)arbi-   trarily depends on the particular implementation .   Innumpy , it is NaN with a warning only . There-   fore this function samples only positive or NaN   numbers . Since DPText sources are not publicly   available , we can only assume that NaN numbers773are either replaced by zero , or the sampling pro-   ceeds as long as the desired number of samples   is reached ( discarding NaNs ) . In either case , no   negative values can be obtained . See Fig . 3 in the   Appendix for various Laplace - based distributions   sampled with different techniques including pos-   sible distributions sampled in DPText .   Theorem 3.2 . DPText with private mechanism   based on ( 8) and ( 9 ) fails to guarantee differential   privacy .   Proof . We rely on the standard proof of the   Laplace mechanism as shown , e.g , by Habernal   ( 2021 ) . Let X= 0 andX= 1 be two neigh-   boring datasets , and the query fbeing the iden-   tity query , such that it outputs simply the value of   X. Let the DPText mechanism M(X;f)outputs   a particular value z.   In order to being differentially private , mecha-   nismM(X;f)has to fulÔ¨Åll the following bound   of the privacy loss :   Pr(M(X ) = z )   Pr(M(X ) = z )  exp ( " ) ( 10 )   for all neighboring datasets X;X2 X and all   outputsz2R from the range ofM , provided that   our priors over XandXare uniform ( cf . Eq . 1 ) .   Fixz= 0:1 . Then Pr(M(X ) = 0:1)will have   a positive probability ( recall it takes the query out-   putf(X= 0 ) = 0 and adds a random number   drawn from the probability distribution , which is   always positive as shown in Theorem 3.1 . ) How-   everPr(M(X ) = 0:1)will be zero , as the query   outputf(X= 1 ) = 1 will be added again only   a positive random number and thus never be less   than1 . By plugging this into ( 10 ) , we obtain   Pr(M(X ) = 0:1 )   Pr(M(X ) = 0:1 )  = Pr>0   Pr = 0exp(")(11 )   which results in an inÔ¨Ånity privacy loss and vio-   lates differential privacy .   4 Empirical sanity check algorithm   It is impossible to empirically verify that a given   DP - mechanism implementation is actually DP   ( Ding et al . , 2018 ) . However , it is possible to de-   tect a DP - violating mechanism with a fair degree   of certainty . We propose a general sanity checkapplicable to any real - valued DP mechanism , such   as the Laplace mechanism , DPText , or any other .   We start by constructing two neighboring   datasetsX(Alice ) and X(Bob ) such that   X= ( 0 ; : : : ; 0)consists ofnzeros andX=   ( 1 ; : : : ; 1)consists ofnones . The dimensionality   n2f1;2;:::gis a hyperparameter of the experi-   ment . We employ a synthetic data release mecha-   nism ( also called local DP ) . The mechanism takes   XorXand outputs its privatized version of the   same dimensionality n , so that the zeros or ones   are ‚Äò noisiÔ¨Åed ‚Äô real numbers . The query sensitivity   isn .   Thanks to the post - processing lemma , any post-   processing of DP output remains DP . We can thus   turn the output real vector back to all zeros or all   ones , simply by rounding to closest 0or1and   applying majority voting . This process is in fact   our reconstruction attack : given a privatized vec-   tor , we try to guess what the original values were ,   either all zeros or all ones .   What our attacker is doing , and what DP pro-   tects , is that if Alice gives us her privatized data ,   we can not tell whether her private values were all   zeros or all ones ( up to a given factor ) ; the same   for Bob .   By deÔ¨Ånition ( 1 ) and having no prior knowledge   overXandXapart from the fact that the val-   ues are correlated , our attacker can not exceed the   guaranteed privacy loss exp ( " ):   Pr(XjM(X;f ) = z )   Pr(XjM(X;f ) = z)exp ( " ) ( 12 )   We can estimate the conditional probability   Pr(XjM(X;f ) = z)using maximum likelihood   estimation ( MLE ) simply as our attacker ‚Äôs preci-   sion : How many times the attacker reconstructed   trueXvalues given the observed privatized vec-   tor . We can do the same for estimating the condi-   tional probability of X. In particular , we repeat-   edly run each DP mechanism over XandX10   million times each , which gives very precise MLE   estimates even for small " .774   5 Results and discussion   For the sake of completeness , we implemented   two extreme baselines : One that simply copies   input ( no privacy ) and other one completely ran-   dom regardless of the input ( maximum privacy ) ;   these are shown in Figure 2 left . The vanilla   Laplace mechanism behaves as expected ; all em-   pirical losses for all dimensions ( 1 up to 128 ) are   bounded by " . We re - implemented the Laplace   mechanism from ADePT ( Krishna et al . , 2021 )   that , due to wrong sensitivity , has been shown the-   oretically as DP - violating ( Habernal , 2021 ) . We   empirically conÔ¨Årm that ADePT suffered from   the curse of dimensionality as the privacy loss   explodes for larger dimensions . The last panel   conÔ¨Årms our previous theoretical DPText results ,   which ( regardless of dimensionality ) has inÔ¨Ånite   privacy loss .   Note that we constructed the dataset carefully as   two neighboring multidimensional correlated data   that are as distant from each other as possible in   the(0;1)space . However , DP must guarantee   privacy for any datapoints , even the worst case   scenario , as shown by the correct Laplace mech-   anism .   6 Conclusion   We formally proved that DPText ( Beigi et al . ,   2019b , a ; Alnasser et al . , 2021 ; Beigi et al . , 2021 )   is not differentially private due to wrong sampling   in its reparametrization trick . We also proposedan empirical sanity check that conÔ¨Årmed our Ô¨Ånd-   ings and can help to reveal potential errors in DP   mechanism implementations for NLP .   7 Ethics Statement   We declare no conÔ¨Çict of interests with the authors   of DPText , we do not even know them personally .   The purpose of this paper is strictly scientiÔ¨Åc .   Acknowledgements   The independent research group TrustHLT is sup-   ported by the Hessian Ministry of Higher Edu-   cation , Research , Science and the Arts . Thanks   to Cecilia Liu , Haau - Sing Li , and the anonymous   reviewers for their helpful feedback . A special   thanks to Condor airlines , whose greed to make   passengers pay for everything resulted in the most   productive transatlantic Ô¨Çights I ‚Äôve ever had .   References775   A Proof of sampling from inverse CDF   Important fact 1 : A random variable Uis uni-   formly distributed on [ 0;1]if the following holds   UUni(0;1 ) ( ) Pr(Uu ) = u : ( 13 )   Important fact 2 : For any function g()with an   inverse function g( ) , the following holds   g(g(x ) ) = x;g(g(x ) ) = x : ( 14)776   Important fact 3 : For any increasing function g( ) ,   we have by deÔ¨Ånition   xy=)g(x)g(y ): ( 15 )   We know that Pr(Xa)is a shortcut for prob-   ability of event EdeÔ¨Åned using the set - builder   notation asE = fs2    : X(s)ag . Then   by plugging ( 15 ) into the predicate of E , we ob-   tain an equal set , namely event E = fs2    :   g(X(s))g(a)g , for which the probability must   be the same . Therefore for any random variable X   and increasing function g()we have   Pr(Xa ) = Pr(g(X)g(a ) ): ( 16 )   Theorem A.1 . LetUbe a uniform random vari-   able on [ 0;1 ] . LetXbe a continuous random vari-   able with CDF ( cumulative distribution function )   F( ) . LetYbe deÔ¨Åned such that Y = F(U ) .   ThenYhas CDFF( ) .   Proof . FunctionF()is the CDF of a continuous   random variable X , and as a CDF its range is   [ 0;1 ] . Also , ifF()is strictly increasing , it has   a unique inverse function F()deÔ¨Åned on [ 0;1 ] .   We deÔ¨ÅnedY = F(U ) , so consider   Pr(Yy ) = Pr    F(U)y   : ( 17 )   SinceF()is increasing , using ( 16 ) we get   Pr(Yy ) = Pr    F(F(U))F(y)   :( 18 )   Now plugging ( 14 ) we obtain   Pr(Yy ) = Pr(UF(y ) ) ; ( 19 )   and Ô¨Ånally by ( 13 )   Pr(Yy ) = F(y ): ( 20 )   For an overview of proofs of Theorem A.1 see   ( Angus , 1994).777   Klim Zaporojets , Johannes Deleu , Yiwei Jiang , Thomas Demeester , Chris Develder   Ghent University ‚Äì imec , IDLab   Ghent , Belgium   { first_name.last_name}@ugent.be   Abstract   1 Introduction   In this paper we explore a principled approach   to solve entity linking ( EL ) jointly with corefer-   ence resolution ( coref ) . Concretely , we formulate   coref+EL as a single structured task over directed   trees that conceives EL and coref as two comple-   mentary components : a coreferenced cluster can   only be linked to a single entity or NIL ( i.e. , a non-   linkable entity ) , and all mentions linking to the   same entity are coreferent . This contrasts with pre-   vious attempts to join coref+EL ( Hajishirzi et al . ,   2013 ; Dutta and Weikum , 2015 ; Angell et al . , 2021 )   where coref and EL models are trained separately   and additional logic is required to merge the pre-   dictions of both tasks .   Our Ô¨Årst approach ( in Fig . 1(a ) ) is moti-   vated by current state - of - the - art coreference resolu-   tion models ( Joshi et al . , 2019 ; Wu et al . , 2020 ) that   predict a single antecedent for each span to resolve .   We extend this architecture by also considering en-   tity links as potential antecendents : in the example   of Fig . 1 , the mention ‚Äú Alliance ‚Äù can be either con-   nected to its antecedent mention ‚Äú NATO ‚Äù or to any   of its candidate links ( Alliance orAlliance,_Ohio ) .   While straightforward , this approach can not solve   cases where the Ô¨Årst coreferenced mention does not   include the correct entity in its candidate list ( e.g. ,   if the order of ‚Äú NATO ‚Äù and ‚Äú Alliance ‚Äù mentions   in Fig . 1 would be reversed ) . We therefor propose   a second approach , , which by construction   overcomes this inherent limitation by using bidirec-   tional connections between mentions . Because that   implies cycles could be formed , we resort to solv-   ing a maximum spanning tree problem . Mentions   that refer to the same entity form a cluster , repre-   sented as a subtree rooted by the single entity they   link to . To encode the overall document ‚Äôs clusters   in a single spanning tree , we introduce a virtual   root node ( see Fig . 1(b ) ) .   This paper contributes : ( i ) 2 architectures (   and ) for joint entity linking ( EL ) and778corefence resolution , ( ii ) an extended AIDA dataset   ( Hoffart et al . , 2011 ) , adding new annotations of   linked and NIL coreference clusters , ( iii ) exper-   imental analysis on 2 datasets where our joint   coref+EL models achieve up to +5 % F1 - score on   both tasks compared to standalone models . We   also show up to +50 % in accuracy for hard cases   of EL where entity mentions lack the correct entity   in their candidate list .   2 Architecture   Our model takes as input ( i ) the full document   text , and ( ii ) an alias table with entity candidates   for each of the possible spans . Our end - to - end ap-   proach allows to jointly predict the mentions , entity   links and coreference relations between them .   2.1 Span and Entity Representations   We use SpanBERT ( base ) from Joshi et al . ( 2020 )   to obtain span representations gfor a particular   spans . Similarly to Luan et al . ( 2019 ) ; Xu and   Choi ( 2020 ) , we apply an additional pruning step   to keep only the top- Nspans based on the pruning   score from a feed - forward neural net ( FFNN ):   (s ) = FFNN(g ): ( 1 )   For a candidate entity eof span swe will ob-   tain representation as e(which is further detailed   in ¬ß 3 ) .   2.2 Joint Approaches   We propose two methods for joint coreference and   EL . The Ô¨Årst , , is motivated by end - to - end   span - based coreference resolution models ( Lee   et al . , 2017 , 2018 ) that optimize the marginal-   ized probability of the correct antecedents for each   given span . We extend this local marginalization to   include the span ‚Äôs candidate entity links . Formally ,   the modeled probability of y(text span or candidate   entity ) being the antecedent of span sis :   P(yjs ) = exp    (s ; y)   Pexp    (s ; y);(2 )   whereY(s)is the set of antecedent spans uniÔ¨Åed   with the candidate entities for s. For antecedent   spansfs : j < igthe score is deÔ¨Åned as : ; ( 3 ) ; ( 4)where'is an embedding encoding the distance   between spans sands . Similarly , for a particular   candidate entity e , the score is :   (s ; e ) = (s ) + (s ; e ) ; ( 5 )   (s ; e ) = FFNN([g;e ] ): ( 6 )   An example graph of mentions and entities with   edges for which aforementioned scores would   be calculated is sketched in Fig . 1(a ) . While simple ,   this approach fails to correctly solve EL when the   correct entity is only present in the candidate lists   of mention spans occurring later in the text ( since   earlier mentions have no access to it ) .   To solve EL in the general case , even when the   Ô¨Årst mention does not have the correct entity , we   propose bidirectional connections between men-   tions , thus leading to a maximum spanning tree   problem in our approach . Here we deÔ¨Åne a   score for a ( sub)tree t , noted as (t ):   (t ) = X(u ; u ) ; ( 7 )   where uanduare two connected nodes ( i.e. , root ,   candidate entities or spans ) in t. For a ground truth   cluster c2C(with Cbeing the set of all such   clusters ) , with its setof correct subtree represen-   tationsT , we model the cluster ‚Äôs likelihood with   its subtree scores . We minimize the negative log-   likelihoodLof all clusters :   L= logQPexp    (t)   Pexp    (t) : ( 8)   Naively enumerating all possible spanning trees   ( TorT ) implied by this equation is infeasi-   ble , since their number is exponentially large . We   use the adapted Kirchhoff ‚Äôs Matrix Tree Theorem   ( MTT ; Koo et al . ( 2007 ) ; Tutte ( 1984 ) ) to solve   this : the sum of the weights of the spanning trees   in a directed graph rooted in ris equal to the deter-   minant of the Laplacian matrix of the graph with   the row and column corresponding to rremoved   ( i.e. , the minor of the Laplacian with respect to r ) .   This way , eq . ( 8) can be rewritten as   L= logQdet   ^L       det   L     ; ( 9)779where is the weighted adjacency matrix of the   graph , and Lis the minor of the Laplacian with   respect to the root node r. An entry in the Laplacian   matrix is calculated as ; ( 10 )   Similarly , ^Lis amodiÔ¨Åed Laplacian matrix where   the Ô¨Årst row is replaced with the root rselection   scores (r ; u ) . For clarity , Appendix A presents   a toy example with detailed steps to calculate the   loss in eq . ( 9 ) .   To calculate the scores of each of the entries   (u ; u)tomatrix in eqs . ( 7 ) and ( 9 ) for , we use the same approach as in for   edges between two mention spans , or between a   mention and entity . For the directed edges between   the root rand a candidate entity ewe choose   (r ; e ) = 0 . Since we represent NIL clusters   by edges from the mention spans directly to the   root , we also need scores for them : we use eq . ( 3 )   with(r ) = 0 . We use Edmonds ‚Äô algorithm ( Ed-   monds , 1967 ) for decoding the maximum spanning   tree .   3 Experimental Setup   We considered two datasets to evaluate our pro-   posed models : DWIE ( Zaporojets et al . , 2021 ) and   AIDA ( Hoffart et al . , 2011 ) . Since AIDA essen-   tially does not contain coreference information , we   had to extend it by ( i ) adding missing mention   links in order to make annotations consistent on the   coreference cluster level , and ( ii ) annotating NIL   coreference clusters . We note this extended dataset   as AIDA . See Table 1 for the details .   As input to our models , for DWIE we generate   spans of up to 5 tokens . For each mention span s ,   we Ô¨Ånd candidates from a dictionary of entity sur-   face forms used for hyperlinks in Wikipedia . We   then keep the top-16 candidates based on the prior   for that surface form , as per Yamada et al . ( 2016 ,   ¬ß 3 ) . Each of those candidates eis represented us-   ing a Wikipedia2Vec embedding e(Yamada et al . ,   2016).For AIDA , we use the spans , entity can-   didates , and entity representations from Kolitsas   et al . ( 2018 ) .   To assess the performance of our joint coref+EL   models and , we also provide implementations for coref and EL tasks . The coref model is trained using only the   coreference component of our joint architecture   ( eq . ( 2)‚Äì(4 ) ) , while the EL model is based only on   the linking component ( eq . ( 6 ) ) .   As performance metrics , for coreference reso-   lution we calculate the average - F1 score of com-   monly used MUC ( Vilain et al . , 1995 ) , B(Bagga   and Baldwin , 1998 ) and CEAF(Luo , 2005 ) met-   rics as implemented by Pradhan et al . ( 2014 ) . For   EL , we use ( i ) mention -level F1 score ( EL ) , and   ( ii)cluster -level hard F1 score ( EL ) that counts   a true positive only if both the coreference cluster   ( in terms of all its mention spans ) and the entity   link are correctly predicted . These EL metrics are   executed in a strong matching setting that requires   predicted spans to exactly match the boundaries of   gold mentions . Furthermore , for EL we only report   the performance on non - NIL mentions , leaving the   study of NIL links for future work .   Our experiments will answer the following re-   search questions : ( Q1 ) How does performance   of our joint coref+EL models compare to models ? ( Q2 ) Does jointly solving corefer-   ence resolution and EL enable more coherent EL   predictions ? ( Q3 ) How do our joint models per-   form on hard cases where some individual entity   mentions do not have the correct candidate ?   4 Results   Table 2 shows the results of our compared models   for EL and coreference resolution tasks . Answer-   ing(Q1 ) , we observe a general improvement in   performance of our coref+EL joint models (   and ) compared to on the EL task .   Furthermore , this difference is bigger when using   our cluster - level hard metrics . This also answers   ( Q2 ) by indicating that the joint models tend to pro-   duce more coherent cluster - based predictions . To   make this more explicit , Table 3 compares the accu-   racy for singleton clusters ( i.e. , clusters composed   by a single entity mention ) , denoted as S , to that of   clusters composed by multiple mentions , denoted780   asM. We observe that the difference in perfor-   mance between our joint models and is   bigger on Mclusters ( with a consistent superiority   of ) , indicating that our approach indeed pro-   duces more coherent predictions for mentions that   refer to the same concept . Further analysis reveals   that this difference in performance is even higher   for a more complex scenario where the clusters   contain mentions with different surface forms ( not   shown in the table ) .   In order to tackle research question ( Q3 ) , we   study the accuracy of our models on the important   corner case that involves mentions without correct   entity in their candidate lists . This is illustrated in   Table 4 , which focuses on such mentions in clus-   ters where at least one mention contains the correct   entity in its candidate list . As expected , the model can not link such mentions , as it is   limited to the local candidate list . In contrast , both   our joint approaches can solve some of these cases   by using the correct candidates from other men-   tions in the cluster , with a superior performance of   our model compared to the one.5 Related Work   Entity Linking : Related work in entity linking   ( EL ) tackles the document - level linking coherence   by exploring relations between entities ( Kolitsas   et al . , 2018 ; Yang et al . , 2019 ; Le and Titov , 2019 ) ,   or entities and mentions ( Le and Titov , 2018 ) .   More recently , contextual BERT - driven ( Devlin   et al . , 2019 ) language models have been used for   the EL task ( Broscheit , 2019 ; De Cao et al . , 2020 ,   2021 ; Yamada et al . , 2020 ) by jointly embedding   mentions and entities . In contrast , we explore a   cluster - based EL approach where the coherence is   achieved on coreferent entity mentions level .   Coreference Resolution : Span - based antecedent-   ranking coreference resolution ( Lee et al . , 2017 ,   2018 ) has seen a recent boost by using SpanBERT   representations ( Xu and Choi , 2020 ; Joshi et al . ,   2020 ; Wu et al . , 2020 ) . We extend this approach in   our joint coref+EL architecture . Furthermore ,   we rely on Kirchhoff ‚Äôs Matrix Tree Theorem ( Koo   et al . , 2007 ; Tutte , 1984 ) to efÔ¨Åciently train a more   expressive spanning tree - based method .   Joint EL+Coref : Fahrni and Strube ( 2012 ) intro-   duce a more expensive rule - based Integer Linear   Programming component to jointly predict coref   and EL . Durrett and Klein ( 2014 ) jointly train coref-   erence and entity linking without enforcing single-   entity per cluster consistency . More recently , An-   gell et al . ( 2021 ) ; Agarwal et al . ( 2021 ) use addi-   tional logic to achieve consistent cluster - level entity   linking . In contrast , our proposed approach con-   strains the space of the predicted spanning trees on   a structural level ( see Fig . 1 ) .   6 Conclusion   We propose two end - to - end models to solve entity   linking and coreference resolution tasks in a joint   setting . Our joint architectures achieve superior per-   formance compared to the standalone counterparts .   Further analysis reveals that this boost in perfor-   mance is driven by more coherent predictions on781the level of mention clusters ( linking to the same   entity ) and extended candidate entity coverage .   References782   A Step by Step Example of MTT   Theorem   In this appendix we will provide a clarifying ar-   tiÔ¨Åcial example in order to walk the reader step   by step through MTT ( eq . ( 9)‚Äì(10 ) ) applied in our approach . The graph of the example is il-   lustrated in Fig . 2 and is composed by nodes rep-   resenting root ( r ) , entities eande , and spans s ,   sands . The span sis associated with candidate   entity setfe ; eg(i.e . , represented by edges from   stoeande ) , and swithfeg(i.e . , represented   by the edge from stoe ) . The candidate entity   set of sis empty . The nodes are grouped in two   ground truth clusters : NIL cluster c = fs ; sg ,   and linked cluster c = fe ; sg .   The exponential of weighted adjacency matrix   of the presented example is:783where the weights of incorrect edges are repre-   sented in red ( i.e. , red dashed edges in Fig . 2 ) , the   weights of the correct edges in green ( i.e. , green   edges in Fig . 2 ) , and the weights between discon-   nected nodes are set to 0 .   In order to compute the denominator of the loss   function in eq . ( 9 ) , the Laplacian of the matrix   in eq . ( 11 ) is calculated as described in eq . ( 10 ) ,   and the row and column corresponding to root r   removed ( i.e. , the minor Lwith respect to the   root ):   L=2   666641 0 0 1 00 1 0 4 20 0 16 5 90 0 3 17 20 0 8 4 203   77775:(12 )   Following Kirchhoff ‚Äôs Matrix Tree Theorem ( Koo   et al . , 2007 ; Tutte , 1984 ) , the determinant of L   equals to the sum of the weights of all possible   spanning trees of the graph represented in Fig . 2 :   det(L ) = 3600 = Xexp    (t)   :( 13 )   In order to compute the numerator of the loss   function in eq . ( 9 ) ( i.e. , the sum of the weights of   the spanning trees of ground truth clusters ) , we Ô¨Årst   mask out ( set to zero ) all the weights assigned to   incorrect edges :   Next , the modiÔ¨Åed Laplacian ( i.e. , Laplacian with   the Ô¨Årst row replaced by root rselection weights )   ^Lis calculated for both clusters candc :   ^L= 5 7 8 9   ( 15 )   ^L= 1 00 4   ( 16 )   The determinants of ^Land^Lequal to the sum   of the weights of all spanning trees connecting the   nodes in clusters candcrespectively :   det(^L ) = 101 = Xexp    (t)   ( 17)det(^L ) = 4 = Xexp    (t)   ( 18 )   Finally , in order to calculate the Ô¨Ånal loss , we re-   place the obtained results in eqs . ( 13 ) , ( 17 ) , and   ( 18 ) in the loss function of eq . ( 9 ):   L= log1014   3600 : ( 19 )   Note : strictly speaking , there are three clusters   rooted in root in the graph of Fig . 2 , the third one   being c = feg , whose exponential weight is 1   by deÔ¨Ånition of (r ; e ) = 0 ( see ¬ß 2.2 ) , and has   no impact in calculation of the loss function in   eq . ( 19).784   Tianwen WeiJianwei QiShenghuan He   Xiaomi , XiaoAI Team   { weitianwen,qijianwei,heshenghuan}@xiaomi.com   Abstract   1 Introduction   In this work we explore the strategies of BERT   ( Devlin et al . , 2019 ) serving for multiple tasks un-   der the following two constraints : 1 ) Memory and   computational resources are limited . On edge de-   vices such as mobile phones , this is usually a hard   constraint . On local GPU stations and Cloud - based   servers , this constraint is not as hard but it is still   desirable to reduce the computation overhead to cut   the serving cost . 2 ) The tasks are expected to be   modular and are subject to frequent updates . When   one task is updated , the system should to be able   to quickly adapt to the task modiÔ¨Åcation such that   the other tasks are not affected . This is a typical   situation for applications ( e.g. AI assistant ) under   iterative and incremental development .   In principle , there are two strategies of BERT   serving : single - task serving andmulti - task serving .   In single - task serving , one independent single - task   model is trained and deployed for each task . Typ-   ically , those models are obtained by Ô¨Åne - tuning acopy of the pre - trained BERT and are completely   different from each other . Single - task serving has   the advantage of being Ô¨Çexible and modular as   there is no dependency between the task models .   The downside is its inefÔ¨Åciency in terms of both   memory usage and computation , as neither parame-   ters nor computation are shared or reused across the   tasks . In multi - task serving , one single multi - task   model is trained and deployed for all tasks . This   model is typically trained with multi - task learn-   ing ( MTL ) ( Caruana , 1997 ; Ruder , 2017 ) . Com-   pared to its single - task counterpart , multi - task serv-   ing is much more computationally efÔ¨Åcient and   incurs much less memory usage thanks to its shar-   ing mechanism . However , it has the disadvantage   in that any modiÔ¨Åcation made to one task usually   affect the other tasks .   The main contribution of this work is the propo-   sition of a framework for BERT serving that si-   multaneously achieves the Ô¨Çexibility of single - task   serving and the efÔ¨Åciency of multi - task serving .   Our method is based on the idea of partial Ô¨Åne-   tuning , i.e. only Ô¨Åne - tuning some topmost layers   of BERT depending on the task and keeping the   remaining bottom layers frozen . The Ô¨Åne - tuned   layers are task - speciÔ¨Åc , which can be updated on   a per - task basis . The frozen layers at the bottom ,   which plays the role of a feature extractor , can be   shared across the tasks .   2 Related Work   The standard practice of using BERT is Ô¨Åne - tuning ,   i.e. the entirety of the model parameters is ad-   justed on the training corpus of the downstream   task , so that the model is adapted to that speciÔ¨Åc   task ( Devlin et al . , 2019 ) . There is also an alterna-   tivefeature - based approach , used by ELMo ( Peters   et al . , 2018 ) . In the latter approach , the pre - trained   model is regarded as a feature extractor with frozen   parameters . During the learning of a downstream   task , one feeds a Ô¨Åxed or learnable combination of785   the model ‚Äôs intermediate representations as input to   the task - speciÔ¨Åc module , and only the parameters   of the latter will be updated . It has been shown that   the Ô¨Åne - tuning approach is generally superior to   the feature - based approach for BERT in terms of   task performance ( Devlin et al . , 2019 ; Peters et al . ,   2019 ) .   A natural middle ground between these two ap-   proaches is partial Ô¨Åne - tuning , i.e. only Ô¨Åne - tuning   some topmost layers of BERT while keeping the   remaining bottom layers frozen . This approach has   been studied in ( Houlsby et al . , 2019 ; Merchant   et al . , 2020 ) , where the authors observed that Ô¨Åne-   tuning only the top layers can almost achieve the   performance of full Ô¨Åne - tuning on several GLUE   tasks . The approach of partial Ô¨Åne - tuning essen-   tially regards the bottom layers of BERT as a fea-   ture extractor . Freezing weights from bottom lay-   ers is a sensible idea as previous studies show that   the mid layer representations produced by BERT   are most transferrable , whereas the top layers rep-   resentations are more task - oriented ( Wang et al . ,   2019a ; Tenney et al . , 2019b , a ; Liu et al . , 2019a ;   Merchant et al . , 2020 ) . Notably , Merchant et al .   ( 2020 ) showed that Ô¨Åne - tuning primarily affects   weights from the top layers while weights from   bottom layers do not alter much . Liu et al . ( 2019a)showed that it is possible to achieve state - of - the - art   results on a number of probing tasks with linear   models trained on frozen mid layer representations   of BERT .   3 Method   In what follows , we denote by Tthe set of all target   tasks . We always use the 12 - layer uncased version   of BERT as the pre - trained language model . The   proposed framework features a pipeline ( Fig . 1 )   that consists of three steps : 1 ) Single task partial   Ô¨Åne - tuning ; 2 ) Single task knowledge distillation ;   3 ) Model merging . We give details of these steps   below .   3.1 Single Task Partial Fine - Tuning   In the Ô¨Årst step , we partial Ô¨Åne - tune for each task   an independent copy of BERT . The exact number   of layersLto Ô¨Åne - tune is a hyper - parameter and   may vary across the tasks . We propose to experi-   ment for each task with different value of Lwithin   rangeN6L6N , and select the one that   gives the best validation performance . The purpose   of imposing the search range [ N;N]is to   guarantee a minimum degree of parameter sharing .   In the subsequent experiments on GLUE tasks ( see   Section 4.3 ) , we set N= 4andN= 10 .   This step produces a collection of single - task   models as depicted in Fig . 1(a ) . We shall refer to   them as single - task teacher models , as they are to   be knowledge distilled to further reduce the mem-   ory and computation overhead .   3.2 Single Task Knowledge Distillation   Since there is no interaction between the tasks , the   process of knowledge distillation ( KD ) can be car-   ried out separately for each task . In principle any   of the existing KD methods for BERT ( Wang et al . ,   2020 ; Aguilar et al . , 2020 ; Sun et al . , 2019a ; Jiao   et al . , 2020 ; Xu et al . , 2020a ) suits our needs . In   preliminary experiments we found out that as long   as the student model is properly initialized , the   vanilla knowledge distillation ( Hinton et al . , 2015 )   can be as performant as those more sophisticated   methods .   Assume that the teacher model for task  2 T   containsLÔ¨Åne - tuned layers at the top and   12 Lfrozen layers at the bottom . Our goal is786   ( a ) Teacher models ( b ) Student models ( c ) Final multi - task model   to compress the former into a smaller l - layer   module . The proposed initialization scheme is   very simple : we initialize the student model with   the weights from the corresponding layers of the   teacher . More precisely , let Ndenote the number   of layers ( including both frozen and task - speciÔ¨Åc   layers ) in the student , where N<12 . We propose   to initialize the student from the bottommost N   layers of the teacher . Similar approach has also   been used in ( Sanh et al . , 2019 ) , where the student   is initialized by taking one layer out of two from   the teacher . The value of l , i.e. the number of   task - speciÔ¨Åc layers in the student model for task   , determines the Ô¨Ånal memory and computation   overhead for that task .   3.3 Model Merging   In the Ô¨Ånal step , we merge the single - task student   models into one multi - task model ( Fig . 1(c ) ) so   that the parameters and computations carried out   in the frozen layers can be shared . To achieve this ,   it sufÔ¨Åces to load weights from multiple model   checkpoints into one computation graph .   4 Experiments   In this section , we compare the performance and   efÔ¨Åciency of our model with various baselines on   eight GLUE tasks ( Wang et al . , 2019b ) . More de-   tails on these tasks can be found in Appendix A.4.1 Metrics   The performance metrics for GLUE tasks is ac-   curacy except for CoLA and STS - B. We use   Matthews correlation for CoLA , and Pearson cor-   relation for STS - B.   To measure the parameter and computational   efÔ¨Åciency , we introduce the total number of trans-   former layers that are needed to perform inference   for all eight tasks . For the models studied in our   experiments , the actual memory usage and the com-   putational overhead are approximately linear with   respect to this number . It is named ‚Äú overhead ‚Äù in   the header of Table 2 .   4.2 Baselines   The baseline models / methods can be divided into   4 categories :   Single - task without KD . There is only one   method in this category , i.e. the standard practice   of single task full Ô¨Åne - tuning that creates a separate   model for each task .   Single - task with KD . The methods in this cate-   gory create a separate model for each task , but a   certain knowledge distillation method is applied to   compress each task model into a 6 - layer one . The   KD methods include ( Hinton et al . , 2015 ; Xu et al . ,   2020b ; Sanh et al . , 2019 ; Turc et al . , 2019 ; Sun   et al . , 2019b ; Jiao et al . , 2020 ; Wang et al . , 2020 ) .   Multi - task learning . This category includes two   versions of MT - DNN ( Liu et al . , 2019b , 2020 ) , both   of which produce one single multi - task model . 1 )   MT - DNN ( full ) is jointly trained for all eight tasks.787   It corresponds to the idea scenario where all tasks   are known in advance . 2 ) MT - DNN ( LOO ) , where   ‚Äú LOO ‚Äù stands for ‚Äú leave - one - out ‚Äù , corresponds to   the scenario where one of the eight tasks is not   known in advance . The model is jointly pre - trained   on the 7 available tasks . Then an output layer for   the ‚Äú unknown ‚Äù task is trained with the pre - trained   weights frozen .   Flexible multi - task . Our models under various   efÔ¨Åciency constraints . Ours ( w/o KD ) means that   no knowledge distillation is applied to the task mod-   els . The number of Ô¨Åne - tuned layers for each task   is selected according to the criterion described in   Section 3.1 . Ours ( KD - n)means that knowledge   distillation is applied such that the student model   for each task contains exactly ntask - speciÔ¨Åc lay-   ers . For Ours ( mixed ) , we determine the number   of task - speciÔ¨Åc layers for each task based on the   marginal beneÔ¨Åt ( in terms of task performance met-   ric ) of adding more layers to the task . More pre-   cisely , for each task we keep adding task - speciÔ¨Åc   layers as long as the marginal beneÔ¨Åt of doing so is   no less than a pre - determined threshold c. In Table   2 , we report the result for c= 1:0 . Results withother values of ccan be found in Appendix D.   4.3 Results   The results are summarized in Table 2 . From the   table it can be seen that the proposed method Ours   ( mixed ) outperforms all KD methods while be-   ing more efÔ¨Åcient . Compared to the single - task   full Ô¨Åne - tuning baseline , our method reduces up   to around two thirds of the total overhead while   achieves 99.6 % of its performance .   We observe that MT - DNN ( full ) achieves the   best average performance with the lowest over-   head . However , its performance superiority pri-   marily comes from one big boost on a single task   ( RTE ) rather than consistent improvements on all   tasks . In fact , we see that MT - DNN ( full ) suffers   performance degradation on QQP and STS - B due   totask interference , a known problem for MTL   ( Caruana , 1997 ; Bingel and Sogaard , 2017 ; Alonso   and Plank , 2017 ; Wu et al . , 2020 ) . From our per-   spective , the biggest disadvantage of MT - DNN is   that it assumes full knowledge of all target tasks   in advance . From the results of MT - DNN ( LOO ) ,   we observe that MT - DNN has difÔ¨Åculty in han-788dling new tasks if the model is not allowed to be   retrained .   5 Discussions   5.1 Advantages   One major advantage of the proposed architecture   is its Ô¨Çexibility . First , different tasks may be fed   with representations from different layers of BERT ,   which encapsulate different levels of linguistic in-   formation ( Liu et al . , 2019a ) . This Ô¨Çexibility is   beneÔ¨Åcial to both task performance and efÔ¨Åciency .   For instance , on QQP we achieve an accuracy of   91.0 , outperforming all KD baselines with merely   onetask - speciÔ¨Åc layer ( connected to the 2nd layer   of the frozen backbone model ) . Second , our ar-   chitecture explicitly allows for allocating uneven   resources to different tasks . We have redistributed   the resources among the tasks in ours ( mixed ) , re-   sulting in both greater performance and efÔ¨Åciency .   Third , our framework does not compromise the   modular design of the system . The model can be   straightforwardly updated on on a per - task basis .   5.2 Limitations   The major limitation of our approach is that for   each downstream task it requires approximately   10x more training time for the hyper - parameter   search compared to the conventional approach . Al-   though the cost is arguably manageable in practice ,   i.e. typically 2 or 3 days per task on a single Nvidia   Tesla V100 GPU , the excessive computation load   should not be overlooked .   Another limitation is that although the overall   computation overhead is reduced , the serving la-   tency of our model deteriorates as the number of   tasks grows , and may eventually be worse than   that of the single task baseline . This is due to the   fact that during inference one can not get the output   of any one task until the model has Ô¨Ånished com-   puting for alltasks . In this regard , our approach   may not be appropriate for those applications that   demand exceptionally low serving latency , e.g. be-   low 10 ms . Nevertheless , we report in Appendix E   an industrial use case where our multi - task model   serves 21 tasks while achieving a latency as low as   32 ms ( 99th percentile ) .   5.3 Comparison with Adaptor - Based   Approaches   The adaptor - based approaches ( Houlsby et al . ,   2019 ; Pfeiffer et al . , 2020 ) belong to anothercategory of Ô¨Åne - tuning approaches that are also   parameter - efÔ¨Åcient . Basically , the adaptor - based   approaches introduce one trainable task - speciÔ¨Åc   ‚Äú adaptor ‚Äù module for each downstream task . This   module is generally lightweight , containing only a   few parameters and is inserted between ( or within )   layers of the backbone model ( e.g. BERT ) . How-   ever , even though the parameters of the backbone   model can be shared across the tasks , the compu-   tation for inference can not due to the fact that the   internal data Ô¨Çow in each task model is modiÔ¨Åed   by the task - speciÔ¨Åc adaptor . Therefore , the adaptor-   based approaches are not computationally efÔ¨Åcient   and one needs to perform a separate full forward   pass for each task . Since both parameter and com-   putation efÔ¨Åciency are what we aim to achieve , the   adaptor - based approaches are not comparable to   our method .   6 Conclusion   We have presented our framework that is designed   to provide efÔ¨Åcient and Ô¨Çexible BERT - based multi-   task serving . We have demonstrated on eight   GLUE datasets that the proposed method achieves   both strong performance and efÔ¨Åciency . We release   our codeand hope that it can facilitate BERT serv-   ing in cost - sensitive applications .   References789790791Supplemental Materials   A Details on the GLUE tasks   The GLUE benchmark includes the following   datasets :   QNLI ( Question Natural Language Infer-   ence ) . The dataset is derived from ( Rajpurkar   et al . , 2016 ) . This is a binary classiÔ¨Åca-   tion task where an example is of the form   ( question , sentence ) and the goal is to predict   whether the sentence contains the correct an-   swer to the question ( Wang et al . , 2018 ) .   RTE ( Recognizing Textual Entailment ) . A   binary entailment task similar to MNLI but   with much less training data ( Bentivogli et al . ,   2009 ) .   QQP ( Quora Question Pairs ) A binary clas-   siÔ¨Åcation task where the goal is to determine   if two questions asked on Quora are semanti-   cally equivalent ( Chen et al . , 2018 ) .   MNLI ( Multi - Genre Natural Language Infer-   ence ) . Given a pair of sentences , the goal is to   predict whether the second sentence is an en-   tailment , contradiction or neutral with respect   to the Ô¨Årst one ( Williams et al . , 2018 ) .   SST-2 ( The Stanford Sentiment Treebank ) .   A binary single - sentence classiÔ¨Åcation task   where the goal is to predict the sentiment ( pos-   itive or negative ) of the movie reviews ( Socher   et al . , 2013 ) .   MRPC ( Microsoft Research Paraphrase Cor-   pus ) . A binary classiÔ¨Åcation task where the   goal is to predict whether two sentences are   semantically equivalent ( Dolan and Brockett ,   2005 ) .   CoLA ( The Corpus of Linguistic Acceptabil-   ity ) . A binary single - sentence classiÔ¨Åcation   task where the goal is to predict whether an   English sentence is linguistically ‚Äú acceptable ‚Äù   or not ( Warstadt et al . , 2018 ) .   STS - B ( The Semantic Textual Similarity   Benchmark ) . A regression task where the goal   is to predict whether two sentences are similar   in terms of semantic meaning as measured by   a score from 1 to 5 ( Cer et al . , 2017).WNLI ( Winograd NLI ) . The dataset is de-   rived from ( Levesque et al . , 2012 ) . We ex-   clude this task in our experiments following   the practice of ( Devlin et al . , 2019 ; Radford   et al . , 2018 ) .   Dataset Train Dev   QNLI 108k 5.4k   RTE 2.5k 0.3k   QQP 363k 40k   MNLI 392k 9.8k   SST-2 67k 0.8k   MRPC 3.5k 0.4k   CoLA 8.5k 1.0k   STS - B 5.7k 1.5k   B Hyper - parameters   The approach presented in this work introduces two   new hyper - parameters for each task  2 T , namely   the number of Ô¨Åne - tuned layers Lfor the teacher   and the number of knowledge distilled layer l   for the student . If the resources permit , these two   hyper - parameters should be tuned separately for   each task . As introduced in Section 3.1 , we suggest   to constrainLwithin the range 4L10 . As   forlwhich determines the eventual task - speciÔ¨Åc   overhead , we impose l3 . Since we always   determinesLÔ¨Årst , we do not need to experiment   with every combination of ( L;l ) . Combin-   ing these together , our approach requires approx-   imately 10x ( 7 for Land 3 forl ) more training   time compared to conventional full Ô¨Åne - tuning ap-   proach .   The conventional hyper - parameters ( e.g. learn-   ing rate , mini - batch size , etc ) used in our experi-   ments are summarized in Table 4 .   C Detailed Experiment Results   In the box plots of Figure 2 above we report the   performance of the student models initialized from   pre - trained BERT and from the teacher . It can be   clearly seen that the latter initialization scheme   generally outperforms the former . Besides , we also   observe that although increasing the number of   task - speciÔ¨Åc layers improves the performance , the   marginal beneÔ¨Åt of doing so varies across tasks.792Hyper - parameter value   learning rate 2e-5   batch size 32   Epoch 3 , 4 , 5   Optimizer Adam   weight decay rate 0.01    0.9    0.999    1e-6   Notably , for QQP and STS - B the student models   with only one task - speciÔ¨Åc layer are able to attain   99 % of the performance of their teacher .   D Performance - EfÔ¨Åciency Trade - off   In Fig 5 , we report the performance of our method   with various values of c , wherecis deÔ¨Åned as the   minimal marginal beneÔ¨Åt ( in terms of task perfor-   mance metric ) that every task - speciÔ¨Åc layer should   bring ( see Section 4.2 ) .   E Industrial Application   We have implemented our framework in the ap-   plication of utterance understanding of XiaoAI ,   a mono - lingual ( Chinese ) commercial AI assis-   tant developed by XiaoMi . Our Ô¨Çexible multi - task   model forms the bulk of the utterance understand-   ing system , which processes over 100 million user   queries per day with a peak throughput of nearly   4000 queries - per - second ( QPS ) .   For each user query , the utterance understanding   system performs various tasks , including emotion   recognition , incoherence detection , domain classi-   Ô¨Åcation , intent classiÔ¨Åcation , named entity recog-   nition , slot Ô¨Ålling , etc . Due to the large workload ,   these tasks are developed and maintained by a num-   ber of different teams . As the AI assistant itself   is under iterative / incremental development , its ut-   terance understanding system undergoes frequent   updates :   Update of training corpus , e.g. when new   training samples become available or some   mislabeled samples are corrected or removed.RedeÔ¨Ånition of existing tasks . For instance ,   when a more Ô¨Åne - grained intent classiÔ¨Åcation   is needed , we may need to redeÔ¨Åne existing   intent labels or introduce new labels .   Introduction of new tasks . This may happen   when the AI assistant needs to upgrade its   skillsets so as to perform new tasks ( e.g. rec-   ognize new set of instructions , play verbal   games with kids , etc ) .   Removal of obsolete tasks . Sometimes a   task is superseded by another task , or simply   deprecated due to commercial considerations .   Those tasks need to be removed from the sys-   tem .   One imperative feature for the system is the mod-   ular design , i.e. the tasks should be independent   of each other so that any modiÔ¨Åcation made to one   task does not affect the other tasks . Clearly , a con-   ventional multi - task system does not meet our need   as multi - task training breaks modularity .   Before the introduction of BERT , our utterance   understanding system is based on single - task serv-   ing , i.e. a separate model is deployed for each   task . As those models are relatively lightweight   ( e.g. TextCNN , LSTM ) , overhead is not an issue .   However , with the introduction of BERT , the cost   for single - task serving becomes a valid concern   as each task model ( a unique 12 - layer Ô¨Åne - tuned   BERT ) requires two Nvidia Tesla V100 GPUs for   stable serving that meets the latency requirement .   With the primary objective of reducing cost ,   we have implemented the proposed Ô¨Çexible multi-   task model in our utterance understanding system ,   which provides serving for a total of 21 downstream   tasks . Overall , there are 40 transformer layers of   which 8 are shared frozen layers ( on average 1:5   task - speciÔ¨Åc layers per task ) . Using only 5 Nvidia   Tesla V100 GPUs , we are able to achievea P99   latency of 32 ms under a peak throughput of 4000   QPS . Compared with single - task serving for 21   tasks which would require 42 GPUs , we estimate   that our system reduces the total serving cost by up   to 88%.793794   Responsible NLP Research Checklist   A. For every submission   A1 . Did you discuss the limitations of your work ?   Yes , it is explicitly discussed in Section 5.2 .   A2 . Did you discuss any potential risks of your   work ?   No , we believe that there is no potential risk .   A3 . Do the abstract and introduction summarize   the paper ‚Äôs main claims ?   Yes , we conÔ¨Årm so .   B. Did you use or create scientiÔ¨Åc artifacts ?   Yes , we used the GLUE datasets in Section 4 .   B1 . Did you cite the creators of artifacts you used ?   Yes , the GLUE paper is cited in Section 4 . The   individual datasets in GLUE are cited in Appendix   A.   B2 . Did you discuss the license or terms for use   and/or distribution of any artifacts ?   No . Since those artifacts are popular in the NLP   community , we merely followed the common   practice of using these artifacts . We do not believe   that our usage violate the license for use , or is   potentially risky in any ways we can imagine .   B3 . Did you discuss if your use of existing   artifact(s ) was consistent with their intended   use , provided that it was speciÔ¨Åed ? For theartifacts you create , do you specify intended use   and whether that is compatible with the original   access conditions ( in particular , derivatives of data   accessed for research purposes should not be used   outside of research contexts ) ?   No . The justiÔ¨Åcation is the same that for question   B2 .   B4 . Did you discuss the steps taken to check   whether the data that was collected / used con- tains   any information that names or uniquely identiÔ¨Åes   individual people or offensive content , and the   steps taken to protect / anonymize it ?   No . The justiÔ¨Åcation is the same that for question   B2 .   B5 . Did you provide documentation of the   artifacts , e.g. , coverage of domains , languages ,   and linguistic phenomena , demographic groups   represented , etc . ?   Yes , it is provided in Appendix A.   B6 . Did you report relevant statistics like the   number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ?   Yes , it is provided in Appendix A.   C. Did you run computational experiments ?   Yes , in Section 4 .   C1 . Did you report the number of parameters in   the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure795used ?   No , we did not report the number of parameters   in the models used as it can be easily inferred   from Table 2 . The total computation budget was   discussed in Section 5.2 .   C2 . Did you discuss the experimental setup ,   including hyperparameter search and best - found   hyperparameter values ?   Yes , it is provided in the Section 4 and Appendix B.   C3 . Did you report descriptive statistics about your   results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it   transparent whether you are reporting the max ,   mean , etc . or just a single run ?   Yes , we explicitly stated in the caption of Table   1 and Table 2 that our results are the maximum   over 5 independent runs . Detailed results are also   reported in Appendix C.   C4 . If you used existing packages ( e.g. , for   preprocessing , for normalization , or for evalua-   tion ) , did you report the implementation , model ,   and parameter settings used ( e.g. , NLTK , Spacy ,   ROUGE , etc . ) ?   We did reuse the WordPiece implemen-   tation from BERT ‚Äôs repository for   tokenization . We did not report this as we consider   it as a trivial matter .   D. Did you use human annotators ( e.g. ,   crowdworkers ) or research with human   subjects ?   No , we did not use any human annotators , nor did   we research with human subjects.796   Nicholas Tomlin Andre He Dan Klein   Computer Science Division , University of California , Berkeley   { nicholas_tomlin , andre.he , klein}@berkeley.edu   Abstract   1 Introduction   Go is fundamentally a game of pattern recognition :   from ladders andwalls tosente andshape , pro-   fessional players rely on a rich set of concepts to   communicate about structures on the game board .   Some patterns are relatively simple : walls are lines   of adjacent stones , and an atari is a threat to cap-   ture stones on the next move ; other patterns are less   clearly defined : hane refers to any move that ‚Äú goes   around ‚Äù the opponent ‚Äôs stones , and sente describes   a general state of influence or tempo . Despite the   nebulous definitions of some of these terms , human   players use them productively . Beginners learn   about eyes that determine when groups of stones   arealive ordead and are given guidelines for when   they should play a cutor extend a ladder ; more ad-   vanced players learn sequences of joseki andtesuji   and are taught to distinguish good shape from bad   shape . Figures 1 - 2 depict some example concepts .   Computers have recently surpassed human per-   formance at Go ( Silver et al . , 2016 ) , but relatively   little is known about why these programs perform   so well and whether they rely on similar repre-   sentational units to choose the moves they play .   While post - hoc behavioral analyses suggest that   AlphaGo and its successor AlphaGo Zero ( Silver   et al . , 2017 ) can process complex game situations   involving shape , capturing races , sente , tesuji , and   even ladders , existing interpretability work has fo-   cused on the moves that agents play , rather than the   internal computations responsible for those moves .   Our work instead proposes a structural analysis   by correlating the internal representations of game-   playing agents with information from a naturally-   occurring dataset of move - by - move annotations .   In this paper , we use linear probing to explore   how domain - specific concepts are represented by797   game - playing agents . Because we do not have   ground - truth labels explaining which concepts are   relevant to a given game state , we collect a dataset   of 10 K annotated Go games ( ¬ß 2.1 ) . Given a board   state and its associated comment , we produce bi-   nary feature vectors summarizing which game phe-   nomena ( e.g. , ko , atari ) are mentioned in the com-   ment and use pattern - based feature extractors to   determine which phenomena are actually present   on the board ( ¬ß 2.2 ) . We then feed board states into   two policy networks with disparate architectures   and training methods ( ¬ß 3.1 ) to obtain intermedi-   ate representations . Finally , we use linear probes   ( ¬ß 3.2 ) to predict the binary feature vectors from our   policy networks . Generally , we find that pattern-   based features are encoded in the early layers of   policy networks , while natural language features   are most easily extracted from the later layers of   both models . We release our code and data at   https://github.com/andrehe02/go .   2 Dataset   2.1 Annotated Games   We collect 10 K games with move - by - move English   annotations from the Go Teaching Ladder ( GTL ) .   The GTL was created by Jean - loup Gailly and Bill   Hosken in 1994 and maintained until 2016 and per-   mits non - commercial digital redistribution . Until   2016 , members of the GTL could submit games   for review by volunteers , who ranged from amateur   to professional strength . Reviewers were given an-   notation guidelines and required to have a higher   rating than their assigned reviewees , resulting in   high quality natural language data . Of the collected   games , we focus on 9524 which were played on   classical 19√ó19boards ; many games also includeunplayed analysis variations which we do not use   in this work . These 9524 games contain 458,182   total comments , with a median length of 14 words .   2.2 Feature Extraction   We convert board states and comments into binary   feature vectors using two methods : ( 1 ) pattern-   based feature extraction , which checks for the   ground truth presence of features from the board   state , and ( 2 ) keyword - based feature extraction ,   which converts comments into bag - of - words repre-   sentations based on domain - specific keywords .   Pattern - Based We define a set of rules to de-   termine which game phenomena are present in a   given board state , including : cuts , eyes , ladders ,   andwalls . For example , we decide that a wall is   present when four stones of the same color are   placed in a row adjacent to one another . Because   patterns like wall andcutare often imprecisely de-   fined , these definitions may not align perfectly with   player intuitions ; we therefore provide additional   details for each phenomena in the appendix . We   do not attempt to write rule - based definitions of   vaguer concepts like sente andinfluence .   Keyword - Based We scrape an online vocabulary   of domain - specific terminologyand find the 30   most common terms in our natural language an-   notations . We then convert each comment into a   30 - dimensional binary feature vector representing   whether or not it contains these keywords ; we addi-   tionally include features based on 60 control words ,   chosen according to frequency statistics , which are   further subdivided into function and content words .   Our wordlist and details about our selection of con-   trol words can be found in the appendix.798   Having both pattern - based and keyword - based fea-   tures captures a trade - off between precision and   coverage . Writing rules for pattern - based features   is labor - intensive and essentially impossible for   many game concepts . Meanwhile , keyword - based   features are inherently noisy : comments often men-   tion phenomena which did n‚Äôt actually occur in the   game , and common structures like atari andeyes   are frequently left unmentioned because the anno-   tator and players already know they exist . Nonethe-   less , we find that probes are capable of predicting   the presence of domain - specific keywords with sig-   nificantly better - than - chance accuracy .   3 Methods   3.1 Policy Networks   We analyze two agents : ( 1 ) an imitation learning   agent using the architecture described in Clark and   Storkey ( 2015 ) , and ( 2 ) a pre - trained ELF OpenGo   model ( Tian et al . , 2017 , 2019 ) , which is an open-   source , reinforcement learning agent similar to Al-   phaGo Zero ( Silver et al . , 2017 ) . Our imitation   learning model was trained on 228,000 games and   achieved a rating of 1 K ( ‚âà1900 ELO ) on the On-   line Go Server ( OGS),where it played against   a combination of humans and computers until itsrating stabilized . ELF OpenGo reports a self - play   ELO over 5000 , but this metric is inflated ( Tian   et al . , 2019 ) . Although we refer to these agents   by their training procedure ( i.e. , imitation vs. rein-   forcement ) , there are several other differences be-   tween the models . One possible source of variance   between agents involves the format of the board   state representation . Following Clark and Storkey   ( 2015 ) , our imitation learning model takes as input   a19√ó19√ó7binary matrix . Of the seven planes ,   six represent the positions of stones , divided by   color and the number of liberties ; the seventh plane   represents koinformation . Meanwhile , the rein-   forcement learning model ‚Äôs 19√ó19√ó17input   contains a partial history of the game state .   3.2 Linear Probes   Given a board state and paired feature vector as   described in Section 2.2 , we compute intermedi-   ate representations by feeding the board state into   frozen policy networks . To predict each feature of   interest , we run logistic regression independently   on each layer of each policy network , including the   raw board state . In other words , for each policy   network , we train F√óL√ókclassifiers , where Fis   the number of features , Lis the number of layers ,   andkis the parameter for k - fold cross - validation ,   as discussed in the following section.799Domain Word Imitation Reinforcement Rough Definition   Pincer 0.91 0.91 attack on a corner approach   Joseki 0.87 0.87 fixed local sequences of moves   Fuseki 0.85 0.84 opening   Ko 0.80 0.86 repetitive capture sequence   Wall 0.70 0.74 sequence of stones in a row   Atari 0.69 0.73 threat to capture   Eye 0.67 0.73 surrounded empty space   Cut 0.64 0.65 block two groups from connecting   Me 0.60 0.62 another word for eye   Down 0.60 0.60 toward the edge of the board   Point 0.59 0.61 specific locations on the board ; or , the score   Force 0.58 0.58 requiring immediate response   Up 0.56 0.58 toward the center of the board   3.3 Metrics   We seek to answer two questions : ( 1 ) what infor-   mation is represented in the policy networks , and   ( 2)where is this information represented ? To an-   swer the first question , we compute the area under   the receiver operating characteristic curve ( ROC   AUC ) for each linear probe . Specifically , for each   layer , we compute the average ROC AUC after 10-   fold cross - validation and then take the maximum   average value across layers . Features with high   ROC AUC are said to be represented by a model ,   because they are linearly extractible from some in-   termediate layer of its policy network . To answer   the second question , we compute the layer at which   each feature has its highest ROC AUC value ; we   then apply 10 - fold cross - validation , summarize the   counts for each feature in a histogram , and compute   a kernel density estimate ( KDE ) for visualization .   4 Results   We find that domain - specific keywords are signif-   icantly more predictable than control words , with   p= 1.8√ó10under the Wilcoxon signed - rank   test . As shown in Figure 3 ( Left ) and Table 1 ,   the keyword with the highest ROC AUC value   across both models is pincer , which denotes a rela-   tively straightforward corner pattern . Meanwhile ,   low - valued domain words like meandupare pol-   ysemous with non - domain - specific meanings and   therefore difficult to predict . While content andfunction control words have roughly similar distri-   butions , some content words are noticeably more   predictable ; for example , opponents is the highest-   valued control word with ROC AUC values of   ( 0.85,0.89)as seen in Figure 3 ( Left ) . Such con-   trol words are likely predictable due to correlations   with certain domain - specific concepts .   ROC AUC values for the two models are strongly   correlated , with Pearson ‚Äôs coefficient œÅ= 0.97 .   Figure 3 ( Left ) shows that for most keywords , the   reinforcement learning model slightly outperforms   the imitation learning model . Furthermore , key-   words are significantly more predictable from the   imitation learning model than from a randomly ini-   tialized baseline with identical architecture ( p=   5.6√ó10 ) . Some words like koare noticeably   more predictable from the reinforcement learning   model , possibly due to differences in input board   state representations ( cf . ¬ß 3.1 ) ; further discussion   of this point can be found in the appendix .   Consistent with our knowledge that pattern-   based features can be obtained by applying simple   rules to the raw board state , we find that pattern-   based features are encoded in early layers of both   models , as shown in Figure 3 ( Right ) . Mean-   while , keyword - based features are most easily ex-   tracted from later layers , suggesting that they cor-   relate with high - level abstractions in the policy net-   work . Generally , pattern - based features are much   more predictable than keyword - based features ,   with average ROC AUC values of ( 0.96,0.98)and800(0.68,0.70 ) , respectively . As discussed in Sec-   tion 2.2 , this discrepancy can largely be attributed   to the noisiness inherent in natural language data .   5 Related Work   Jhamtani et al . ( 2018 ) propose a similarly - sized   dataset of move - by - move chess commentary .   Rather than using this commentary for model inter-   pretability , though , Jhamtani et al . ( 2018 ) attempt   to predict whole comments from raw board states .   Zang et al . ( 2019 ) use the same dataset to jointly   train a policy network and language generation   model with a shared neural encoder , but again fo-   cus on the pedagogical application of commentary   generation rather than interpretation of the policy   network . Similar work has focused on generat-   ing sportscasts in the Robocup domain ( Chen and   Mooney , 2008 ; Liang et al . , 2009 ; Mei et al . , 2016 ) .   Our primary methodology is linear probing ( Et-   tinger et al . , 2016 ; Manning et al . , 2020 ) , which has   commonly been used to study the intermediate rep-   resentations of language models like ELMo ( Peters   et al . , 2018 ) and BERT ( Devlin et al . , 2019 ) . One   classic result in this area shows that early layers   of contextual language models correlate best with   lexical - syntactic information such as part of speech ,   while later layers correlate with semantic informa-   tion like proto - roles and coreference ( Tenney et al . ,   2019 ) . Recent work on control tasks ( Hewitt and   Liang , 2019 ) , minimum description length ( V oita   and Titov , 2020 ) , and Pareto probing ( Pimentel   et al . , 2020 ) has focused on improving the method-   ological rigor of this paradigm . Although linear   probing is fundamentally a correlational method ,   other recent work has focused on whether informa-   tion which is easily extractable from intermediate   layers of a deep network is causally used during in-   ference ( Elazar et al . , 2021 ; Lovering et al . , 2021 ) .   Most related to our work are contemporary stud-   ies by McGrath et al . ( 2021 ) and Forde et al . ( 2022 ) ,   which apply probing techniques to the games of   chess and Hex , respectively . McGrath et al . ( 2021 )   use linear probes to predict a large number of   pattern - based features throughout the training of   an AlphaZero agent for chess . Meanwhile , Forde   et al . ( 2022 ) train linear probes for pattern - based   features on an AlphaZero agent for Hex and run be-   havioral tests to measure whether the agent ‚Äú under-   stands ‚Äù these concepts . Comparatively , our work   uses fewer features than McGrath et al . ( 2021 ) and   does not make causal claims about how represen - tations are used during inference , as in Forde et al .   ( 2022 ) ; however , to the best of our knowledge , our   work is the first of its kind to use features derived   from natural language in conjunction with probing   techniques for policy interpretability .   6 Conclusion   We presented a new dataset of move - by - move an-   notations for the game of Go and showed how it   can be used to interpret game - playing agents via   linear probes . We observed large differences in   the predictability of pattern - based features , which   are extracted from the board state , and keyword-   based features , which are extracted from comments .   In particular , pattern - based features were easily ex-   tracted from lower layers of the policy networks we   studied , while keyword - based features were most   predictable from later layers . At a high level , this   finding reinforces the intuition that written annota-   tions describe high - level , abstract patterns that can-   not easily be described by a rule - based approach .   Accordingly , we argue there is much to learn from   this annotation data : future work might attempt to   correlate policy network representations with richer   representations of language , such as those provided   by a large language model . Future work might also   explore whether similar approaches could be used   to improve game - playing agents , either by expos-   ing their weaknesses or providing an auxiliary train-   ing signal . We also expect similar approaches may   be viable in other reinforcement learning domains   with existing natural language data .   Acknowledgements   We are grateful to Kevin Yang for his work on the   imitation learning model , and to Rodolfo Corona   and Ruiqi Zhong for their contributions to an early   version of this project . We thank Roma Patel , the   members of the Berkeley NLP Group , and our   anonymous reviewers for helpful suggestions and   feedback . This work was supported by the DARPA   XAI and LwLL programs and a National Science   Foundation Graduate Research Fellowship .   References801802A Dataset Statistics   The most common domain - specific terms appear   in more than 15 K comments , as shown in Figure 4 .   B Pattern - Based Feature Extraction   As described in Section 2.2 , we extract features   from the board state using a set of hand - crafted   rules . These rules may not align perfectly with   player intuitions , which can be hard to formulate   concisely , and so are presented here for full detail .   Cut We define cutsas moves that prevent the op-   ponent from connecting two disconnected groups   on their next move . To avoid labelling squares   where a play would be immediately capturable , we   also require that a cut have at least two liberties .   Note that this definition permits non - diagonal cuts .   Eye We define eyes as connected groups of empty   squares that are completely surrounded by stones   of the same color . We require there be no enemy   stones in the same surrounded region , so this defini-   tion fails to capture eyes that surround dead stones .   Ladder The term ladder describes the formation   shown in Figure 2c . Since human players can usu-   ally predict who wins a ladder , they rarely play out   the capturing race . For this reason , we do not look   for ladder formations , but instead label moves that   would start or continue a ladder . Specifically , we   label a square for the ladder feature if it is the singu-   lar liberty of a friendly group of stones and a play   at the square results in the group having exactly   two liberties . We do not count trivial ladders that   lie at the edge of the board . Wall We define a wall as a connected row or   column with four or more stones of the same color .   C Keywords and Control Words   Keywords We choose the first thirty most fre-   quent terms ( cf . Table 1 ) from our vocabulary of   domain - specific terminology as keywords : terri-   tory , point , cut , sente , up , me , moyo , shape , ko ,   invasion , influence , wall , joseki , eye , alive , gote ,   life , pincer , aji , thickness , base , atari , connected ,   hane , tenuki , down , overplay , force , reading , fuseki .   Control Words Our control words consist of the   thirty most frequent words in our dataset , as well   as thirty words uniformly distributed according to   the same frequency as the keywords : the , is , to ,   this , white , black , and , you , at , for , in , move , it ,   of , but , not , be , have , play , that , on , good , here ,   if , better , can , would , now , should , stones , looking ,   wanted , opponents , was nt , defending , save , you re ,   answer , three , fine , feel , place , lose , bit , possibility ,   attacking , likely , leaves , should nt , question , lost ,   threat , almost , there s , continue , trying , hope , just ,   exchange , before . We further subdivide the con-   trol words based on whether or not they appear   in the NLTK stopword list , which we use as a   rough proxy for distinguishing between function   and content words .   D Additional Results   We additionally report de - aggregated ROC AUC   values for each keyword across layers , as shown   in Figures 5 - 7 . These figures show the raw data   used to compute the kernel density estimates in   Figure 3 , which show that natural language fea-   tures are most easily extracted from later layers   of both models . We note in Figure 5 that ladders   are the most difficult pattern - based feature to pre-   dict , which is consistent with our knowledge that   many Go - playing agents fail to correctly handle   ladders without special feature engineering ( Tian   et al . , 2019 ) . Anecdotally , our imitation learning   model often failed to play ladders correctly ; this   is consistent with the finding that ladders are more   predictable from the reinforcement learning model .   Future work might investigate whether this prob-   ing framework could be used to effectively predict   model behavior in situations like these , as in Forde   et al . ( 2022 ) for the game of Hex.803   E Major Differences Between Imitation   and Reinforcement Learning Models   While most keywords have similar ROC AUC val-   ues across models , ko , eye , atari , and overplay have   a noticeably higher ROC AUC values under the re-   inforcement learning model ( cf . Table 1 ) . However ,   this discrepancy is not obviously attributable to the   difference in training procedures ( i.e. , imitation vs.   reinforcement ) . As described in Section 3.1 , the   two models use different input state representations ,   which differ in their encoding of koandliberty in-   formation , which is used to determine whether eyes   andatari exist . Such architectural differences may   explain discrepancies across models , but do not   account for words like overplay ; playing strength   is another possible ( but not confirmed ) source of   these discrepancies.804805806Domain Word Imitation Reinforcement Rough Definition   Pincer 0.91 0.91 attack on a corner approach   Joseki 0.87 0.87 fixed local sequences of moves   Fuseki 0.85 0.84 opening   Ko 0.80 0.86 repetitive capture sequence   Base 0.76 0.77 starter eye space   Moyo 0.74 0.77 sphere of influence   Influence 0.72 0.74 long - range effect of stones   Reading 0.72 0.70 calculating an upcoming sequence   Wall 0.70 0.74 sequence of stones in a row   Thickness 0.70 0.74 strength of a group of stones   Invasion 0.70 0.72 attack on enemy territory   Atari 0.69 0.73 threat to capture   Eye 0.67 0.73 surrounded empty space   Gote 0.67 0.69 loss of initiative   Tenuki 0.66 0.68 non - local response   Hane 0.66 0.70 move that ‚Äú reaches around ‚Äù or bends   Overplay 0.64 0.70 overly aggressive move   Cut 0.64 0.65 block two groups from connecting   Alive 0.63 0.67 can not be captured   Territory 0.63 0.66 controlled empty space   Aji 0.63 0.66 possibilities left in a position   Sente 0.63 0.66 initiative   Shape 0.62 0.64 quality of a group of stones   Life 0.62 0.63 inability to be captured   Connected 0.61 0.62 adjacent or nearby stones   Me 0.60 0.62 another word for eye   Down 0.60 0.60 toward the edge of the board   Point 0.59 0.61 specific locations on the board ; or , the score   Force 0.58 0.58 requiring immediate response   Up 0.56 0.58 toward the center of the board807   Zheng YuanChuanqi TanSongfang HuangTsinghua UniversityAlibaba Group   yuanz17@mails.tsinghua.edu.cn   { chuanqi.tcq,songfang.hsf}@alibaba-inc.com   Abstract   1 Introduction   International Classification of Diseases ( ICD ) is a   classification and terminology that provides diag-   nostic codes with descriptions for diseases . The   task of ICD coding refers to assigning ICD codes to   electronic medical records ( EMRs ) which is highly   related to clinical tasks or systems including pa-   tient similarity learning ( Suo et al . , 2018 ) , medical   billing ( Sonabend et al . , 2020 ) , and clinical deci-   sion support systems ( Sutton et al . , 2020 ) . Tradi-   tionally , healthcare organizations have to employ   specialized coders for this task , which is expen-   sive , time - consuming , and error - prone . As a result ,   many methods have been proposed for automatic   ICD coding since the 1990s ( de Lima et al . , 1998 ) .   Recent methods treat this task as a multi - label   classification problem ( Xie and Xing , 2018 ; Li and   Yu , 2020 ; Zhou et al . , 2021 ) , which learn deep   representations of EMRs with an RNN or CNN en-   coder and predict codes with a multi - label classifier . Recent state - of - the - art methods propose label atten-   tion that uses the code representations as attention   queries to extract the code - related representations   ( Mullenbach et al . , 2018 ) . Following this idea ,   many works further propose using code hierarchi-   cal structures ( Falis et al . , 2019 ; Xie et al . , 2019 ;   Cao et al . , 2020 ) and descriptions ( Cao et al . , 2020 ;   Song et al . , 2020 ) for better label representations .   In this work , we argue that the synonyms of   codes can provide more comprehensive informa-   tion . For example , the description of code 244.9   is ‚Äú Unspecified hypothyroidism ‚Äù in ICD . However ,   this code can be described in different forms in   EMRs such as ‚Äú low t4 ‚Äù and ‚Äú subthyroidism ‚Äù . For-   tunately , these different expressions can be found in   the Unified Medical Language System ( Bodenrei-   der , 2004 ) , a repository of biomedical vocabularies   that contains various synonyms for all ICD codes .   Therefore , we propose to leverage synonyms of   codes to help the label representation learning and   further benefit its matching to the EMR texts .   To model the synonym and its matching to EMR   text , we further propose a Multiple Synonyms   Matching Network ( MSMN ) . Specifically , we   first apply a shared LSTM to encode EMR texts   and each synonym . Then , we propose a novel   multi - synonyms attention mechanism inspired by   the multi - head attention ( Vaswani et al . , 2017 ) ,   which considers synonyms as attention queries to   extract different code - related text snippets for code-   wise representations . Finally , we propose using a   biaffine - based similarity of code - wise text represen-   tations and code representations for classification .   We conduct experiments on the MIMIC - III   dataset with two settings : full codes and top-50   codes . Results show that our method performs bet-   ter than previous state - of - the - art methods.8082 Approach   Consider free text S(usually discharge summaries )   from EMR with words { w } . The task is to as-   sign a binary label y‚àà { 0,1}based on S. Figure 1   shows an overview of our method .   2.1 Code Synonyms   We extend the code description lby synonyms   from the medical knowledge graph ( i.e. , UMLS   Metathesaurus ) . We first align the code to the   Concept Unique Identifiers ( CUIs ) from UMLS .   Then we select corresponding synonyms of English   terms from UMLS with the same CUIs and add ad-   ditional synonyms by removing hyphens and the   word ‚Äú NOS ‚Äù ( Not Otherwise Specified ) . We denote   the code synonyms as { l , ... , l}in which each   code synonym lis composed of words { l } .   2.2 Encoding   Previous works ( Ji et al . , 2021 ; Pascual et al . ,   2021 ) have shown that pretrained language models   like BERT ( Devlin et al . , 2019 ) can not help the   ICD coding performance , hence we use an LSTM   ( Hochreiter and Schmidhuber , 1997 ) as our en-   coder . We use pre - trained word embeddings to map   words wtox . Ad - layer bi - directional LSTM   layer takes word embeddings as input to obtain text   hidden representations H‚ààR.   H = h , ... , h= Enc ( x , ... , x ) ( 1 )   For code synonym l , we apply the same encoder   with a max - pooling layer to obtain representation   q‚ààR.   q= MaxPool(Enc ( x , ... , x ) ) ( 2 )   2.3 Multi - synonyms Attention   To interact text with multiple synonyms , we pro-   pose a multi - synonyms attention inspired by the   multi - head attention ( Vaswani et al . , 2017 ) . We   splitH‚ààRintoMheads H‚ààR :   H = H , ... , H(3 )   Then , we use code synonyms qto query H. We   take the linear transformations of Handqto   calculate attention scores Œ±‚ààR. Text related   to code synonym lcan be represented by HŒ± .   We aggregate code - wise text representations v‚àà   Rusing max - pooling of HŒ±since the text only   needs to match one of the synonyms .   Œ±= softmax ( Wq¬∑tanh(WH ) ) ( 4 )   v= MaxPool ( HŒ± , ... , HŒ± ) ( 5 )   2.4 Classification   We classify whether the text Scontains code l   based on the similarity between code - wise text rep-   resentation vand code representation . We aggre-   gate code synonym representations { q}to code   representation q‚ààRby max - pooling . We then   propose using a biaffine transformation to measure   the similarity for classification :   q= MaxPool ( q , q , ... , q ) ( 6 )   ÀÜy = œÉ(logit ) = œÉ(vWq ) ( 7 )   Previous works ( Mullenbach et al . , 2018 ; Vu et al . ,   2020 ) classify codes via :   ÀÜy = œÉ(logit ) = œÉ(vw ) ( 8)   Their work need to learn code - dependent param-   eters [ w]‚ààRfor classification , which   suffers from training rare codes . On the contrary ,   our biaffine function that uses Wqinstead of w   only needs to learn code - independent parameters   W‚ààR.   2.5 Training   We optimize the model using binary cross - entropy   between predicted probabilities ÀÜyand labels y :   L = X‚àíylog(ÀÜy)‚àí(1‚àíy ) log(1 ‚àíÀÜy)(9)809   3 Experiments   3.1 Dataset   MIMIC - III dataset ( Johnson et al . , 2016 ) con-   tains deidentified discharge summaries with human-   labeled ICD-9 codes . We list the document   counts , average word counts per document , aver-   age codes counts per document , and total codes   of the MIMIC - III dataset in Table 1 . We use the   same splits with previous works ( Mullenbach et al . ,   2018 ; Vu et al . , 2020 ) with two settings as full   codes ( MIMIC - III full ) and top-50 frequent codes   ( MIMIC - III 50 ) . We follow the preprocessing of   Xie et al . ( 2019 ) and Vu et al . ( 2020 ) to truncate dis-   charge summaries at 4,000 words . We measure the   results using macro AUC , micro AUC , macro F ,   micro Fand precision@k ( k= 5for MIMIC - III   50,8and15for MIMIC - III full ) .   3.2 Implementation Details   We sample M= 4 and8synonyms per code for   MIMIC - III full and MIMIC - III 50 respectively . We   sample synonyms fully randomly from the syn-   onyms set . If some ICD codes do not have enough   synonyms , we just repeat these synonyms . We   use the same word embeddings as Vu et al . ( 2020 )   which are pretrained on the MIMIC - III discharge   summaries using CBOW ( Mikolov et al . , 2013 )   with a hidden size of 100 . We apply R - Drop with   Œ±= 5(Liang et al . , 2021 ) to regularize the model   to prevent over - fitting . We apply the dropout with   a ratio of 0.2 after the word embedding layer and   before the classification layer . For text encoding ,   we add a linear layer upon the LSTM layer ( the out-   put dimension of the linear layer refers to LSTM   output dim . in Table 2 ) . We train MSMN with   AdamW ( Loshchilov and Hutter , 2019 ) with a lin-   ear learning rate decay . We optimize the threshold   of classification using the development set . For   the MIMIC - III 50 setting , we train with one 16 GB   NVIDIA - V100 GPU . For the MIMIC - III full set-   ting , we train with 8 32 GB NVIDIA - V100 GPUs .   We list the detailed training hyper - parameters in   Table 2 .   3.3 Baselines   CAML ( Mullenbach et al . , 2018 ) uses CNN to en-   code texts and proposes label attention for coding .   MSATT - KG ( Xie et al . , 2019 ) applies multi - scale   attention and GCN to capture codes relations .   MultiResCNN ( Li and Yu , 2020 ) encodes text us-   ing multi - filter residual CNN .   HyperCore ( Cao et al . , 2020 ) embeds ICD codes   into the hyperbolic space to utilize code hierarchy   and uses GCN to leverage the code co - occurrence .   LAAT & JointLAAT ( Vu et al . , 2020 ) propose   a hierarchical joint learning mechanism to relieve   the imbalanced labels , which is our main baseline   since it is most similar to our work .   3.4 Main Results   Table 3 and 4 show the main results under the   MIMIC - III full and MIMIC - III 50 settings , re-   spectively . Under the full setting , our MSMN   achieves 95.0 ( +2.0 ) , 99.2 ( +0.0 ) , 10.3 ( -0.4 ) , 58.4   ( +0.9 ) , 75.2 ( +1.4 ) , and 59.9 ( +0.8 ) in terms of   macro - AUC , micro - AUC , macro- F , micro- F ,   P@8 , and P@15 respectively ( parentheses shows   the differences against previous best results ) , which   shows that MSMN obtains state - of - the - art results   in most metrics . Under the top-50 codes setting ,   MSMN performs better than LAAT in all metrics   and achieves state - of - the - art scores of 92.8 ( +0.3 ) ,   94.7 ( +0.1 ) , 68.3 ( +1.7 ) , 72.5 ( +0.9 ) , 68.0 ( +0.5)810   on macro - AUC , micro - AUC , macro- F , micro- F ,   and P@5 , respectively . We notice that the macro   Fhas a large variance in every epoch under the   MIMIC - III full setting since it is more sensitive in   a long tail problem .   3.5 Discussion   To explore the influence of leveraging different   numbers of code synonyms , we search Mamong   { 1,2,4,8,16}on the MIMIC - III 50 dataset . Re-   sults are shown in Table 5 . Compared with M= 1   that we only use the original ICD code descriptions ,   leveraging more synonyms from UMLS consis-   tently improves the performance . Using M= 4,8   achieves the best performance in terms of AUC ,   andM= 8achieves the best performance in terms   ofFand P@5 . In addition , the median and mean   count of UMLS synonyms are 5.0 and 5.4 respec-   tively , which echoes why the results of M= 4or   8are better .   To evaluate the effectiveness of our proposed   biaffine - based similarity function , we compare it   with the baseline LAAT in Table 5 . We also pro-   vide a simple function by removing Wtovqin   Equation 7 . Results show that the biaffine - based   similarity scoring performs best among others .   To better understand what MSMN learns from   the multi - synonyms attention , we plot the synonym   representations qunder MIMIC - III 50 setting via   t - SNE ( van der Maaten and Hinton , 2008 ) in Fig-   ure 2 . We observe for some codes like 585.9   ( ‚Äú chronic kidney diseases ‚Äù ) , all synonym repre-   sentations cluster together , which indicates that   synonyms extract similar text snippets . However ,   codes like 410.71 ( ‚Äú subendocardial infarction ini-   tial episode of care ‚Äù or ‚Äú subendo infarct , initial ‚Äù )   and403.90 ( ‚Äú hypertensive chronic kidney disease ,   unspecified , with chronic kidney disease stage i   through stage iv ‚Äù or ‚Äú unspecified orhy kid w cr kid   i iv ‚Äù ) with very different synonyms learn different   representations , which benefits to match different   text snippets . Furthermore , we observe it has sim-   ilar representations for sibling codes 37.22 ( ‚Äú left   heart cardiac catheterization ‚Äù ) and 37.23 ( ‚Äú rt / left   heart card cath ‚Äù ) , which indicates the model can   also implicitly capture the code hierarchy .   3.6 Memory Complexity   The memory usage of our proposed MSMN is   dominated by Equation 4 and Equation 5 . We   suppose batch size as B , word count as N , label   count as Cand synonyms count as M. Calculating   Equation 4 for all jsimultaneously requires cal-   culating Einstein summation ( Daniel et al . , 2018 )   among tensors with shape B√óN√óhand shape811   C√óM√óhto shape B√óC√óN√óM. Calculating   Equation 5 requires calculating Einstein summa-   tion among tensors with shape B√óN√óhand   shape B√óC√óN√óMto shape B√óC√óh√óM.   The memory complexities of these two equations   are linearly proportional to M.   4 Related Work   Automatic ICD coding is an important task in the   medical NLP community . Earlier works use ma-   chine learning methods for coding ( Larkey and   Croft , 1996 ; Pestian et al . , 2007 ; Perotte et al . ,   2014 ) . With the development of neural networks ,   many recent works consider ICD coding as a multi-   label text classification task . They usually apply   RNN or CNN to encode texts and use the label   attention mechanism to extract and match the most   relevant parts for classification . The label atten-   tion relies on the label representations as attention   queries . Li and Yu ( 2020 ) ; Vu et al . ( 2020 ) ran-   domly initialize the label representations which   ignore the code semantic information . Cao et al .   ( 2020 ) use the average of word embeddings as la-   bel representations to leverage the code semantic   information . Xie et al . ( 2019 ) ; Cao et al . ( 2020 ) use   GCN to fuse hierarchical structures of ICD codes   for label representations . Compared with previous   works , we use synonyms instead of a single de-   scription to represent the code , which can provide   more comprehensive expressions of codes .   Biomedical entity linking is a related task to au-   tomatic ICD coding . The task requires standardiz-   ing given terms to a pre - defined concept dictionary .   There are two differences between biomedical en - tity linking and automatic ICD coding : ( 1 ) They   have different target concepts . ICD coding map   EMRs to ICD codes , while biomedical entity link-   ing usually map terms to a larger dictionary like   SNOMED - CT or UMLS . ( 2 ) They have different   input formats . Entity linking task has labeled enti-   ties in texts , while ICD coding only provides texts .   Synonyms have also been used in biomedical en-   tity linking ( Sung et al . , 2020 ; Yuan et al . , 2022 ) .   BioSYN ( Sung et al . , 2020 ) uses marginalization   to sum the probabilities of all synonyms as the sim-   ilarity between a term and a concept . However ,   we consider multi - synonyms attention to extract-   ing different parts of clinical texts to interact with   synonyms .   5 Conclusions   In this paper , we propose MSMN to leverage code   synonyms from UMLS to improve the automatic   ICD coding . Multi - synonyms attention is proposed   for extracting different related text snippets for   code - wise text representations . We also propose   a biaffine transformation to calculate similarities   among texts and codes for classification . Exper-   iments show that MSMN outperforms previous   methods with label attention and achieves state - of-   the - art results in the MIMIC - III dataset . Ablation   studies show the effectiveness of multi - synonyms   attention and biaffine - based similarity .   Acknowledgements   We would like to thank the anonymous reviewers   for their helpful comments and suggestions . We   thank Fuli Luo , Shengxuan Luo , Hongyi Yuan , Xu   Chen , and Jiayu Li for their help . This work was   supported by Alibaba Group through Alibaba Re-   search Intern Program .   References812813814   L√ºtÔ¨Å Kerem Senel , Timo Schick andHinrich Sch√ºtze   Center for Information and Language Processing ( CIS ) , LMU Munich , Germany   lksenel@gmail.com , schickt@cis.lmu.de   Abstract   1 Introduction   Increasing computational power along with the de-   sign and development of large and sophisticated   models that can take advantage of enormous cor-   pora has drastically advanced NLP . For many tasks ,   Ô¨Ånetuning pretrained transformer - based language   models ( Vaswani et al . , 2017 ; Devlin et al . , 2019 ;   Radford et al . , 2018 ) has improved the state of   the art considerably . Language models acquire   knowledge during pretraining that is utilized dur-   ing task - speciÔ¨Åc Ô¨Ånetuning . On benchmarks that   were introduced to encourage development of mod-   els that do well on a diverse set of NLU tasks   ( e.g. , GLUE ( Wang et al . , 2018 ) and SuperGLUE   ( Wang et al . , 2019 ) ) , these models now achieve   superhuman performance ( He et al . , 2020 ) . The   pretrain - then-Ô¨Ånetune approach usually requires a   great amount of labeled data , which is often not   available or expensive to obtain , and results in spe-   cialized models that can perform well only on a   single task . Recently , it was shown that genera-   tive language models can be applied to many tasks   without Ô¨Ånetuning when the task is formulated as   text generation and the PLM is queried with a natu-   ral language prompt ( Radford et al . , 2019 ; Brown   et al . , 2020 ) .   Motivated by recent progress in zero - shot learn-   ing with generative models as well as the need for   more challenging benchmarks that test language   understanding of language models , we introduce   CoDA21 ( Context DeÔ¨Ånition Alignment ) , a difÔ¨Å-   cult benchmark that measures NLU capabilities of   PLMs for the English language . Given a deÔ¨Åni-   tion and a context each for kwords , but not the   words themselves , the task is to align the kdef-   initions with the kcontexts . In other words , for   each deÔ¨Ånition , the context in which the deÔ¨Åned   word is most likely to occur has to be identiÔ¨Åed .   This requires ( i ) understanding the deÔ¨Ånitions , ( ii )   understanding the contexts , and ( iii ) the ability to   match the two . Since the target words are not given ,   a model must be able to distinguish subtle meaning   differences between different contexts / deÔ¨Ånitions   to be successful . To illustrate the difÔ¨Åculty of the   task , Figure 1 shows a partial example for k= 4   ( see Table 5 in the supplementary for the full ex-815ample ) . We see that both complex inference ( e.g. ,   < XXX > can give rise to a cloud by being kicked up   ) < XXX > must be dry)<XXX > can be dust , but   not soil ) and world knowledge ( what materials are   typical for monuments ? ) are required for CoDA21 .   We formulate the alignment task as a text pre-   diction task and evaluate , without Ô¨Ånetuning , three   PLMs on CoDA21 : BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) and GPT-2 ( Radford   et al . , 2019 ) . Poor performance of the PLMs and a   large gap between human and PLM performance   suggest that CoDA21 is an important benchmark   for designing models with better NLU capabilities .   2 CoDA21   2.1 Dataset   We construct CoDA21 by Ô¨Årst deriving a set Gof   synset groupsfG;G;:::gfrom Wordnet ( Miller ,   1995 ) . A synset group Gis a group of synsets   whose meanings are close enough to be difÔ¨Åcult to   distinguish ( making the task hard ) , but not so close   that they become indistinguishable for human and   machine . In a second step , each synset group G   is converted into a CoDA21 group G ‚Äì a set of   triples , each consisting of the synset , its deÔ¨Ånition ,   and a corpus context . A CoDA21 group can be   directly used for one instance of the CoDA21 task .   Synset groups . Each synset group Gconsists   of5k10synsets . To create a synset group ,   we start with a parent synset ^sand construct a co-   hyponym group G(^s)of its children :   G(^s ) = fsjs<^s;s = 2Dg   where < is the hyponymy relation between synsets   andDis the set of synsets that have already been   added to a synset group . The intuition for grouping   synsets with a common parent is that words sharing   a hypernym are difÔ¨Åcult to distinguish ( as opposed   to randomly selected words ) .   We iterate ^sthrough all nouns and verbs in Word-   Net . At each iteration , we get all hyponyms of ^s   that have not been previously added to a synset   group ; not reusing a synset ensures that different   CoDA21 subtasks are not related and so no such   relationships can be exploited .   We extract synset groups from co - hyponym   groups by splitting them into multiple chunks of   sizek . In an initial exploration , we found that   the task is hard to solve for human subjects if   two closely related hyponyms are included , e.g. ,‚Äúclementine ‚Äù and ‚Äú tangerine ‚Äù . We therefore em-   ploy clustering to assemble a set of mutually dis-   similar hyponyms . We Ô¨Årst compute a sentence   embedding for each hyponym deÔ¨Ånition using the   stsb - distilbert - base Sentence Transformer model .   We then cluster the embeddings using complete-   link clustering , combining the two most dissimilar   clusters in each step . We stop merging before the   biggest cluster exceeds the maximum group size   ( k= 10 ) or before the similarity between the last   two combined clusters exceeds the maximum simi-   larity ( = 0:8 ) . The largest cluster Gis added to   the setGof synset groups . We then iterate the steps   of ( i ) removing the synsets in the previous largest   clusterGfrom G(^s)and ( ii ) running complete - link   clustering and adding the resulting largest cluster   GtoGuntil fewer than Ô¨Åve synsets remain in G(^s )   or no cluster can be formed whose members have   a similarity of less than .   CoDA21 groups . For each synset s , we extract   its deÔ¨Ånition d(s)from WordNet and a context c(s )   in which it occurs from SemCor(Miller et al . ,   1994 ) . SemCor is an English corpus tagged with   WordNet senses . Let C(s)be the set of contexts   ofsin SemCor . IfjC(s)j>1 , we use as c(s )   the context in which bert - base - uncased predicts s   with the highest log probability when it is masked ,   where sis the word tagged with the sense s ‚Äì this   favors contexts that are speciÔ¨Åc to the meaning of   the synset . Finally , we convert each synset group   GinGto a CoDA21 group G :   G = f(s;d(s);c(s))js2Gg   That is , a CoDA21 group Gis a set of triples of   sense , deÔ¨Ånition and context . In PLM evaluation ,   each CoDA21 group Ggives rise to one context-   deÔ¨Ånition alignment subtask .   We name the resulting dataset CoDA21 - noisy-   hard : noisy because ifjC(s)jis small , the selected   context may not be informative enough to identify   the matching deÔ¨Ånition ; hard because the synsets in   a CoDA21 group are taxonomic sisters , generally   with similar meanings despite the clustering - based   limit on deÔ¨Ånition similarity . We construct a clean   version of the dataset by only using synsets with   jC(s)j5 . We also construct an easy version by816   taking the ‚Äú hyponym grandchildren ‚Äù sof a parent   synset ^s(s < l^l < ^s ) instead of its hyponym   children . This reduces the similarity of synsets in   a CoDA21 group , making the task easier . Table 1   gives dataset statistics .   2.2 Alignment   Recall the CoDA21 task : given a deÔ¨Ånition and a   context each for kwords ( but not the words them-   selves ) , align the kdeÔ¨Ånitions with the kcontexts .   That is , we are looking for a bijective function ( a   one - to - one correspondence ) between deÔ¨Ånitions   and contexts . Our motivation in designing the task   is that we want a hard task ( which can guide us in   developing stronger natural language understand-   ing models ) , but also a task that is solvable by   humans . Our experience is that humans can at   least partially solve the task by Ô¨Ånding a few initial   ‚Äú easy ‚Äù context - deÔ¨Ånition matches , removing them   from the deÔ¨Ånition / context sets and then match the   smaller remaining number of deÔ¨Ånitions / contexts .   The number of context - deÔ¨Ånition pairs scales   quadratically ( O(k ) ) withkand the number of   alignments factorially ( O(k ! ) ) . We restrict kto   k10to make sure that we do not run into com-   putational problems and that humans do not Ô¨Ånd   the task too difÔ¨Åcult .   In order to connect contexts to deÔ¨Ånitions with-   out using the target words , we replace the target   words by a made - up word . This setup resembles   the incidental vocabulary acquisition process in hu-   mans . Lettbe a target word , ca context in which   toccurs andma made - up word . To test PLMs on   CoDA21 , we use the following pattern :   Q(c;m ) = cDeÔ¨Ånition of mis   whereciscwith occurrences of treplaced bym .   We calculate the match score of a context-   deÔ¨Ånition pair ( c;d)aslogP(djQ(c;m ) ) , i.e. ,log generation probability of the deÔ¨Ånition dcon-   ditioned onQ(c;m ) . Our objective is to maximize   the sum of the kmatch scores in an alignment . We   Ô¨Ånd the best alignment by exhaustive search . Accu-   racy for a CoDA21 group Gis then the accuracy   of its best alignment , i.e. , the number of contexts   inGthat are aligned with the correct deÔ¨Ånition ,   divided by the total number of contexts jGj .   2.3 Baselines   We calculate P(djQ(c;m))for a masked lan-   guage model ( MLM ) Mand an autoregressive lan-   guage model ( ALM ) Aas follows :   P(djQ ) = QP(djQ;d )   P(djQ ) = QP(djQ;d;:::;d )   whereQ = Q(c;m),dis theiword in deÔ¨Ånition   danddis the deÔ¨Ånition with the iword masked .   We evaluate the MLMs BERT and RoBERTa   and the ALM GPT-2 . We experiment with both   base and large versions of BERT and RoBERTa   and with all four sizes of GPT-2 ( small , medium ,   large , xl ) , for a total of eight models , to investigate   the effect of model size on performance .   The made - up word mshould ideally be unknown   so that it does not bias the PLM in any way . How-   ever , there are no truly unknown words for the   models we investigate due to the word - piece to-   kenization they apply to the input . Any made - up   word that is completely meaningless to humans will   have a representation in the models ‚Äô input space   based on its tokenization . To minimize the risk   that the meaning of the made - up word may bias   the model , we use m = bkatuhla , a word with   an empty search result on Google that most likely   never appeared in the models ‚Äô pretraining corpora .   In addition to PLMs , we also evaluate 2 re-   cent sentence transformer models(Reimers and   Gurevych , 2019 ) , paraphrase - mpnet - base - v2 ( mp-   net ) and paraphrase - MiniLM - L6 - v2 ( MiniLM ) ,   and fastText static embeddings(Mikolov et al . ,   2018 ) . To calculate the match score of a context-   deÔ¨Ånition pair , we Ô¨Årst remove the target word from   the context and represent contexts and deÔ¨Ånitions   as vectors . For sentence transformers , we obtain   these vectors by simply encoding the input sen-   tences . For fastText , we average the vectors of the817words in contexts and deÔ¨Ånitions . We then cal-   culate the match score as the cosine similarity of   context and deÔ¨Ånition vectors .   3 Results   Table 2 presents average accuracy of the investi-   gated models on the four CoDA21 datasets . As   can be seen , fastText performs only slightly bet-   ter than random . MLMs also perform better than   random chance by only a small margin . This poor   performance can be partly explained by the gener-   ation style setup we use , which is not well suited   for masked language models . Even the smallest   GPT-2 model performs considerably better than   RoBERTa - large , the best performing MLM . Perfor-   mance generally improves with model size . GPT-   2achieves the best results among the LMs on   almost all datasets . Interestingly , sentence trans-   former all - mpnet - base - v2 performs comparably   to GPT-2on most datasets despite its simple ,   similarity based matching compared to generation   based matching of GPT-2 models . Based on this   observation it can be argued that current state - of-   the - art language models fail to perform complex ,   multi - step reasoning and inference which are nec-   essary to solve the CoDA21 tasks . Overall , MLMs   perform slightly better on verbs than nouns while   the converse is true for GPT-2 . As expected , all   models perform better on the easy datasets . Perfor-   mance on noisy andclean datasets are comparable ;   this indicates that our contexts are of high quality   even for the synsets with only a few contexts .   Human performance on CoDA21 . We asked   two NLP PhD studentsto solve the task on S20 ,   a random sample of size 20 from the noun part of   CoDA21- clean - easy . Table 2 shows results on S20   for these two subjects and our models . Human per-   formance is 0.86 ‚Äì compared to 0.48 for GPT-2 ,   the best performing model . This difference indi-   cates that there is a large gap in NLU competence   between current language models and humans and   that CoDA21 is a good benchmark to track progress   on closing that gap .   To investigate the effect of the made - up word   m , we experiment with several other words on the   noun part of CoDA21- clean - easy . SpeciÔ¨Åcally , we   investigate another nonce word ‚Äú opyatzel ‚Äù , a single   letter ‚Äú x ‚Äù and two frequent words ‚Äú orange ‚Äù and   ‚Äú cloud ‚Äù . Table 3 shows the results of the mod-   els for different made - up words . MLMs do not   show signiÔ¨Åcant variability in performance , and   perform comparably poor for all words tried . GPT2   versions , which perform considerably better than   MLMs on CoDA21 , perform similarly for the two   nonce words and single letter ‚Äú x ‚Äù , which do not   have a strong meaning . Their performance drops   signiÔ¨Åcantly when the two frequent words are used   as the made - up word , due to the effect of prior   knowledge models have about these words .   To investigate the effect of the pattern , we com-   pared our pattern Q(c;m)with two alternative pat-   terns by evaluating GPT-2on the noun part of   CoDA21- clean - easy . Patterns and the evaluation   results are shown in Table 4 . The results suggest   that the effect of the pattern on performance is min-   imal .   Effect of the alignment setup . We constructed   CoDA21 as an alignment dataset which uses the   fact that matching between the deÔ¨Ånitions and con-   texts is one - to - one . This setup makes the task818   more intuitive and manageable for humans . How-   ever , context - deÔ¨Ånition match scores can be used   to evaluate models on CoDA21 samples also with-   out the alignment setup by simply picking context-   deÔ¨Ånition pairs with the highest match score for   each deÔ¨Ånition . We additionally evaluated GPT-   2model on CoDA21- clean - easy dataset using   this simple matching approach which yielded 0.38   average accuracy compared to the 0.49 accuracy   achieved with the alignment setup . This result sug-   gests that language models can also make use of   the alignment style evaluation , similar to humans .   Table 5 ( in the Appendix ) presents a sample   of size 7 from the noun part of the CoDA21-   clean - easy dataset . Figure 2 displays all 49 match   scores of the context - deÔ¨Ånition pairs for this sam-   ple obtained using GPT-2 . 5 of the 7 deÔ¨Ånitions   ( 2,3,4,5,7 ) are matched with correct contexts with   the alignment setup while 4 deÔ¨Ånitions ( 4,5,6,7 ) are   matched correctly for the simple matching setup .   Alignment setup enabled the model to match sec-   ond and third deÔ¨Ånitions with their corresponding   contexts even though their match scores are not the   highest ones .   To get a better sense of why the task is hard   for PLMs , we give an example , from the CoDA21   subtask in Figure 1 ( also Figure 2 and Table 5   refer to the same subtask ) , of a context - deÔ¨Ånition   match that is scored highly by GPT-2 , but is not   correct . Context : ‚Äú these bees love a Ô¨Åne - grained   < XXX > that is moist ‚Äù . DeÔ¨Ånition : ‚Äú Ô¨Åne powdery   material such as dry earth or pollen ‚Äù . ( context 6 and   deÔ¨Ånition 1 in Figure 2 ) GPT-2most likely gives   a high score because it has learned that bees and   pollen are associated . It does not understand that   the mutual exclusivity of ‚Äú moist ‚Äù and ‚Äú powdery ‚Äù   makes this a bad match .   4 Related Work   There are many datasets ( Levesque et al . , 2012 ; Ra-   jpurkar et al . , 2016 ; Williams et al . , 2018 ) for eval-   uating language understanding of models . Many   adopt a text prediction setup : Lambada ( Paperno   et al . , 2016 ) evaluates the understanding of dis-   course context , StoryCloze ( Mostafazadeh et al . ,   2016 ) evaluates commonsense knowledge and so   does HellaSwag ( Zellers et al . , 2019 ) , but exam-   ples were adversarially mined . LAMA ( Petroni   et al . , 2019 ) tests the factual knowledge con-   tained in PLMs . In contrast to this prior work ,   CoDA21 goes beyond prediction by requiring the   matching of pieces of text . WIC ( Pilehvar and   Camacho - Collados , 2019 ) is also based on match-   ing , but CoDA21 is more complex ( multiple con-   texts / deÔ¨Ånitions as opposed to a single binary   match decision ) and is not restricted to ambigu-   ous words . WNLaMPro ( Schick and Sch√ºtze ,   2020 ) evaluates knowledge of subordinate rela-   tionships between words , and WDLaMPro ( Senel   and Sch√ºtze , 2021 ) understanding of words using   dictionary deÔ¨Ånitions . Again , matching multiple   pieces of text with each other is much harder and   therefore a promising task for benchmarking NLU .   5 Conclusion   We introduced CoDA21 , a new challenging bench-   mark that tests natural language understanding ca-   pabilities of PLMs . Performing well on CoDA21   requires detailed understanding of contexts , per-   forming complex inference and having world   knowledge , which are crucial skills for NLP . All   models we investigated perform clearly worse than   humans , indicating a lack of these skills in the cur-   rent state of the art in NLP . CoDA21 therefore is a   promising benchmark for guiding the development   of models with stronger NLU competence.819References820821A Appendices   A.1 CoDA21 group examples822Hidden word Context   dust 1 . He came spurring and whooping down the road , his horse kicking up clouds of   < XXX > , shouting :   marble 2 . Pels also sent a check for $ 100 to Russell ‚Äôs widow and had a white < XXX >   monument erected on his grave .   wastewater 3 . The high cost of land and a few operational problems resulting from excessive   loadings have created the need for a < XXX > treatment system with the operational   characteristics of the oxidation pond but with the ability to treat more organic matter   per unit volume .   feathers 4 . It was a Ô¨Åne broody hen , white , with a maternal eye and a striking abundance of   < XXX > in the under region of the abdomen .   fraction 5 . It was then distilled at least three times from a trap at - 78 ‚Äò to a liquid air trap with   only a small middle < XXX > being retained in each distillation .   soil 6 . The thing is that these bees love a Ô¨Åne - grained < XXX > that is moist ; yet the water   in the ground should not be stagnant either .   cards 7 . And the coffee shop on Drexel Street , where the men spent their evenings and   Sundays playing < XXX > , had a rose hedge beneath its window .   Synset DeÔ¨Ånition   dust.n.01 1 . Ô¨Åne powdery material such as dry earth or pollen that can be blown about in the air   marble.n.01 2 . a hard crystalline metamorphic rock that takes a high polish ; used for sculpture and   as building material   efÔ¨Çuent.n.01 3 . water mixed with waste matter   feather.n.01 4 . the light horny waterproof structure forming the external covering of birds   fraction.n.01 5 . a component of a mixture that has been separated by a fractional process   soil.n.02 6 . the part of the earth ‚Äôs surface consisting of humus and disintegrated rock   card.n.01 7 . one of a set of small pieces of stiff paper marked in various ways and used for   playing games or for telling fortunes823Hidden word Context   suggestion 1 . This was Madden ‚Äôs < XXX > ; the police chief shook his head over it .   concept 2 . The < XXX > of apparent black - body temperature is used to describe the radiation   received from the moon and the planets .   ideals 3 . Religion can summate , epitomize , relate , and conserve all the highest < XXX >   and values - ethical , aesthetic , and religious - of man formed in his culture .   reaction 4 . That much of what he calls folklore is the result of beliefs carefully sown among   the people with the conscious aim of producing a desired mass emotional < XXX >   to a particular situation or set of situations is irrelevant .   feeling 5 . He had an uneasy < XXX > about it .   programs 6 . The Federal program of vocational education merely provides Ô¨Ånancial aid to   encourage the establishment of vocational education < XXX > in public schools .   meaning 7 . IndeÔ¨Ånite reference also carries double < XXX > where an allusion to one person   or thing seems to refer to another .   theme 8 . Almost nothing is said of Charles ‚Äô spectacular victories , the central < XXX >   being the heroic loyalty of the Swedish people to their idolized king in misfortune   and defeat .   Synset DeÔ¨Ånition   suggestion.n.01 1 . an idea that is suggested   concept.n.01 2 . an abstract or general idea inferred or derived from speciÔ¨Åc instances   ideal.n.01 3 . the idea of something that is perfect ; something that one hopes to attain   reaction.n.02 4 . an idea evoked by some experience   impression.n.01 5 . a vague idea in which some conÔ¨Ådence is placed   plan.n.01 6 . a series of steps to be carried out or goals to be accomplished   meaning.n.02 7 . the idea that is intended   theme.n.02 8 . a unifying idea that is a recurrent element in literary or artistic work824   Katerina MargatinaLo√Øc BarraultNikolaos AletrasUniversity of Sheffield , University of Le Mans   { k.margatina,n.aletras}@sheffield.ac.uk   loic.barrault@univ-lemans.fr   Abstract   1 Introduction   Active Learning ( AL ) is a method for training su-   pervised models in a data - efficient way ( Cohn et al . ,   1996 ; Settles , 2009 ) . It is especially useful in sce-   narios where a large pool of unlabeled data is avail-   able but only a limited annotation budget can be af-   forded ; or where expert annotation is prohibitively   expensive and time consuming . AL methods iter-   atively alternate between ( i ) model training with   the labeled data available ; and ( ii ) data selection   for annotation using a stopping criterion , e.g. until   exhausting a fixed annotation budget or reaching a   pre - defined performance on a held - out dataset .   Data selection is performed by an acquisition   function that ranks unlabeled data points by some   informativeness metric aiming to improve over ran-   dom selection , using either uncertainty ( Lewis and   Gale , 1994 ; Cohn et al . , 1996 ; Gal et al . , 2017 ;   Kirsch et al . , 2019 ; Zhang and Plank , 2021 ) , di-   versity ( Brinker , 2003 ; Bod√≥ et al . , 2011 ; Senerand Savarese , 2018 ) , or both ( Ducoffe and Pre-   cioso , 2018 ; Ash et al . , 2020 ; Yuan et al . , 2020 ;   Margatina et al . , 2021 ) .   Previous AL approaches in NLP use task-   specific neural models that are trained from scratch   at each iteration ( Shen et al . , 2017 ; Siddhant and   Lipton , 2018 ; Prabhu et al . , 2019 ; Ikhwantri et al . ,   2018 ; Kasai et al . , 2019 ) . However , these models   are usually outperformed by pretrained language   models ( LMs ) adapted to end - tasks ( Howard and   Ruder , 2018 ) , making them suboptimal for AL .   Only recently , pretrained LMs such as B(De-   vlin et al . , 2019 ) have been introduced in AL set-   tings ( Yuan et al . , 2020 ; Ein - Dor et al . , 2020 ; Shel-   manov et al . , 2021 ; Karamcheti et al . , 2021 ; Mar-   gatina et al . , 2021 ) . Still , they are trained at each   AL iteration with a standard fine - tuning approach   that mainly includes a pre - defined number of train-   ing epochs , which has been demonstrated to be   unstable , especially in small datasets ( Zhang et al . ,   2020 ; Dodge et al . , 2020 ; Mosbach et al . , 2021 ) .   Since AL includes both low and high data resource   settings , the AL model training scheme should be   robust in both scenarios .   To address these limitations , we introduce a suite   of effective training strategies for AL ( ¬ß 2 ) . Con-   trary to previous work ( Yuan et al . , 2020 ; Ein - Dor   et al . , 2020 ; Margatina et al . , 2021 ) that also use   B(Devlin et al . , 2019 ) , our proposed method   accounts for various data availability settings and   the instability of fine - tuning . First , we continue   pretraining the LM with the available unlabeled   data to adapt it to the task - specific domain . This   way , we leverage not only the available labeled data   at each AL iteration , but the entire unlabeled pool .   Second , we further propose a simple yet effective   fine - tuning method that is robust in both low and   high resource data settings for AL.825We explore the effectiveness of our approach on   five standard natural language understandings tasks   with various acquisition functions , showing that it   outperforms all baselines ( ¬ß 3 ) . We also conduct an   analysis to demonstrate the importance of effective   adaptation of pretrained models for AL ( ¬ß 4 ) . Our   findings highlight that the LM adaptation strategy   can be more critical than the actual data acquisition   strategy .   2 Adapting & Fine - tuning Pretrained   Models for Active Learning   Given a downstream classification task with C   classes , a typical AL setup consists of a pool of   unlabeled data D , a model M , an annotation   budget bof data points and an acquisition function   a(.)for selecting kunlabeled data points for anno-   tation ( i.e. acquisition size ) until bruns out . The   AL performance is assessed by training a model on   the actively acquired dataset and evaluating on a   held - out test set D.   Adaptation ( ) Inspired by recent work on   transfer learning that shows improvements in down-   stream classification performance by continuing the   pretraining of the LM with the task data ( Howard   and Ruder , 2018 ) we add an extra step to the   AL process by continuing pretraining the LM ( i.e.   Task - Adaptive Pretraining ) , as in Gururan-   gan et al . ( 2020 ) . Formally , we use an LM , such as   B(Devlin et al . , 2019 ) , P(x;W)with weights   W , that has been already pretrained on a large   corpus . We fine - tune P(x;W)with the available   unlabeled data of the downstream task D , re-   sulting in the task - adapted LM P(x;W)with   new weights W(cf . line 2 of algorithm 1 ) .   Fine - tuning ( + ) We now use the adapted   LMP(x;W)for AL . At each iteration i ,   we initialize our model Mwith the pretrained   weights Wand we add a task - specific feedfor-   ward layer for classification with weights Won   top of the [ CLS ] token representation of B-   basedP. We fine - tune the classification model   M(x ; [ W , W])with all x‚àà D. ( cf . line 6 to   8 of algorithm 1 ) .   Recent work in AL ( Ein - Dor et al . , 2020 ; Yuan   et al . , 2020 ) uses the standard fine - tuning method   proposed in Devlin et al . ( 2019 ) which includes   a fixed number of 3training epochs , learning rate   warmup over the first 10 % of the steps and AdamW   optimizer ( Loshchilov and Hutter , 2019 ) withoutAlgorithm 1 : AL with Pretrained LMs   Input : unlabeled data D , pretrained LM   P(x;W ) , acquisition size k , AL   iterations T , acquisition function aD‚Üê ‚àÖP(x;W)‚ÜêTrainP(x;W)onDQ‚ÜêRANDOM ( .),|Q|=kD = D‚à™ QD = D\ Qfori‚Üê1toTdoM(x ; [ W , W])‚ÜêInitialize from   P(x;W)M(x;W)‚ÜêTrain model on DQ‚Üêa(M , D , k)D = D‚à™ QD = D\ Qend   Output : D   bias correction , among other hyperparameters .   We follow a different approach by taking into   account insights from few - shot fine - tuning liter-   ature ( Mosbach et al . , 2021 ; Zhang et al . , 2020 ;   Dodge et al . , 2020 ) that proposes longer fine - tuning   and more evaluation steps during training . We   combine these guidelines to our fine - tuning ap-   proach by using early stopping with 20epochs   based on the validation loss , learning rate 2e‚àí5 ,   bias correction and 5evaluation steps per epoch .   However , increasing the number of epochs from   3to20 , also increases the warmup steps ( 10 % of   total steps ) almost 7times . This may be problem-   atic in scenarios where the dataset is large but the   optimal number of epochs may be small ( e.g. 2or   3 ) . To account for this limitation in our AL setting   where the size of training set changes at each it-   eration , we propose to select the warmup steps as   min(10 % of total steps , 100 ) . We denote standard   fine - tuning asand our approach as+ .   3 Experiments & Results   Data We experiment with five diverse natural lan-   guage understanding tasks : question classification826   ( -6 ; V oorhees and Tice ( 2000 ) ) , sentiment   analysis ( ; Maas et al . ( 2011),-2Socher   et al . ( 2013 ) ) and topic classification ( , ; Zhang et al . ( 2015 ) ) , including binary   and multi - class labels and varying dataset sizes ( Ta-   ble 1 ) . More details can be found in Appendix A.1 .   Experimental Setup We perform all AL experi-   ments using BERT - base ( Devlin et al . , 2019 ) and   E , BKM , A(Yuan et al . , 2020),B ( Ash et al . , 2020 ) and R ( baseline )   as the acquisition functions . We pair our proposed   training approach -+with E ac-   quisition . We refer the reader to Appendix A for   an extended description of our experimental setup ,   including the datasets used ( ¬ß A.1 ) , the training   and AL details ( ¬ß A.2 ) , the model hyperparameters   ( ¬ß A.3 ) and the baselines ( ¬ß A.4 ) .   Results Figure 1 shows the test accuracy during   AL iterations . We first observe that our proposed   approach ( -+ ) achieves large data efficiency   reaching the full - dataset performance within the   15 % budget for all datasets , in contrast to the stan-   dard AL approach ( B- ) . The effectiveness   of our approach is mostly notable in the smaller   datasets . In -6 , it achieves the goal accuracy   with almost 10 % annotated data , while in -only in the first iteration with 2%of the data .   After the first AL iteration in , -+ , it   achieves only 2.5points of accuracy lower than the827performance when using 100 % of the data . In the   larger-2and datasets , it is closer to   the baselines but still outperforms them , achieving   the full - dataset performance with 8%and12 % of   the data respectively . We also observe that in all   five datasets , the addition of our proposed pretrain-   ing step ( ) and fine - tuning technique ( + )   leads to large performance gains , especially in the   first AL iterations . This is particularly evident in -6 , and datasets , where after   thefirstAL iteration ( i.e. equivalent to 2%of train-   ing data ) + + with E is45,30and   12points in accuracy higher than the E   baseline with Band .   Training vs. Acquisition Strategy We finally   observe that the performance curves of the vari-   ous acquisition functions considered ( i.e. dotted   lines ) are generally close to each other , suggesting   that the choice of the acquisition strategy may not   affect substantially the AL performance in certain   cases . In other words , we conclude that the training   strategy can be more important than the acquisi-   tion strategy . We find that uncertainty sampling   with E is generally the best performing   acquisition function , followed by B .Still ,   finding a universally well - performing acquisition   function , independent of the training strategy , is an   open research question .   4 Analysis & Discussion   4.1 Task - Adaptive Pretraining   We first present details of our implementation of ( ¬ß 2 ) and reflect on its effectiveness in the   AL pipeline . Following Gururangan et al . ( 2020 ) ,   we continue pretraining B for the MLM task   using all the unlabeled data Dfor all datasets   separately . We plot the learning curves of B- for all datasets in Figure 2 . We first observe   that the masked LM loss is steadily decreasing for , and across optimization   steps , which correlates with the high early AL per-   formance gains of in these datasets ( Fig . 1 ) .   We also observe that the LM overfits in -6   and-2datasets . We attribute this to the very   small training dataset of -6and the informal   textual style of-2 . Despite the fact that the-2dataset includes approximately 67 K of train-   ing data , the sentences are very short ( i.e. average   length of 9.4words per sentence ) . We hypothesize   the LM overfits because of the lack of long and   more diverse sentences . We provide more details   on at the Appendix B.1 .   4.2 Few - shot Fine - tuning   In this set of experiments , we aim to highlight that it   is crucial to consider the few - shot learning problem   in the early AL stages , which is often neglected   in literature . This is more important when using   pretrained LMs , since they are overparameterized   models that require adapting their training scheme   in low data settings to ensure robustness .   To illustrate the potential ineffectiveness of stan-   dard fine - tuning ( ) , we randomly undersam-   ple the and datasets to form low ,   medium and high resource data settings ( i.e. 100 ,   1,000 and10,000 training samples ) , and train   Bfor a fixed number of 3,10 , and 20epochs .   We repeat this process with 10different random   seeds to account for stochasticity in sampling and   we plot the test accuracy in Figure 3 . Figure 3   shows thatis suboptimal for low data settings   ( e.g.100samples ) , indicating that more optimiza-   tion steps ( i.e. epochs ) are needed for the model   to adapt to the few training samples ( Zhang et al . ,   2020 ; Mosbach et al . , 2021 ) . As the training sam-   ples increase ( e.g. 1,000 ) , fewer epochs are of-   ten better . It is thus evident that there is not a   clearly optimal way to choose a predefined number828of epochs to train the model given the number of   training examples . This motivates the need to find   a fine - tuning policy for AL that effectively adapts   to the data resource setting of each iteration ( inde-   pendently of the number of training examples or   dataset ) , which is mainly tackled by our proposed   fine - tuning approach+ ( ¬ß 2 ) .   4.3 Ablation Study   We finally conduct an ablation study to evaluate   the contribution of our two proposed steps to the   AL pipeline ; the pretraining step ( ) and fine-   tuning method ( + ) . We show that the addition   of both methods provides large gains compared   to standard fine - tuning ( ) in terms of accu-   racy , data efficiency and uncertainty calibration .   We compare Bwith , Bwith+and   B- with+ . Along with test accuracy ,   we also evaluate each model using uncertainty esti-   mation metrics ( Ovadia et al . , 2019 ): Brier score ,   negative log likelihood ( NLL ) , expected calibration   error ( ECE ) and entropy . A well - calibrated model   should have high accuracy and low uncertainty .   Figure 4 shows the results for the smallest and   largest datasets , -6and respectively .   For -6 , training B with our fine - tuning   approach+provides large gains both in accu-   racy and uncertainty calibration , showing the im-   portance of fine - tuning the LM for a larger number   of epochs in low resource settings . For the larger   dataset , , we see that Bwithper-   forms equally to+which is the ideal scenario .   We see that our fine - tuning approach does not de-   teriorate the performance of Bgiven the large   increase in warmup steps , showing that our sim-   ple strategy provides robust results in both high   and low resource settings . After demonstrating   that+yields better results than , we next   compare B- -+against B-+ . We   observe that in both datasets B- outper-   forms B , with this being particularly evident in   the early iterations . This confirms our hypothesis   that by implicitly using the entire pool of unlabeled   data for extra pretraining ( ) , we boost the per-   formance of the AL model using less data .   5 Conclusion   We have presented a simple yet effective training   scheme for AL with pretrained LMs that accounts   for varying data availability and instability of fine-   tuning . Specifically , we propose to first continue   pretraining the LM with the available unlabeled   data to adapt it to the task - specific domain . This   way , we leverage not only the available labeled data   at each AL iteration , but the entire unlabeled pool .   We further propose a method to fine - tune the model   during AL iterations so that training is robust in   both low and high resource data settings .   Our experiments show that our approach yields   substantially better results than standard fine - tuning   in five standard NLP datasets . Furthermore , we find   thatthe training strategy can be more important   than the acquisition strategy . In other words , a   poor training strategy can be a crucial impediment   to the effectiveness of a good acquisition function ,   and thus limit its effectiveness ( even over random   sampling ) . Hence , our work highlights how critical   it is to properly adapt a pretrained LM to the low   data resource AL setting .   As state - of - the - art models in NLP advance   rapidly , in the future we would be interested in   exploring the use of larger LMs , such as G-   3(Brown et al . , 2020 ) and F ( Wei et al . ,   2022 ) . These models have achieved impressive   performance in very low data resource settings ( e.g.   zero - shot and few - shot ) , so we would imagine they   would be good candidates for the challenging set-   ting of active learning.829Acknowledgments   We would like to thank Giorgos Vernikos , our col-   leagues at the Sheffield NLP group for feedback on   an earlier version of this paper , and all the anony-   mous reviewers for their constructive comments .   KM and NA are supported by Amazon through the   Alexa Fellowship scheme .   References830831832A Appendix : Experimental Setup   A.1 Datasets   We experiment with five diverse natural language   understanding tasks including binary and multi-   class labels and varying dataset sizes ( Table 1 ) .   The first task is question classification using the six-   class version of the small -6dataset of open-   domain , fact - based questions divided into broad   semantic categories ( V oorhees and Tice , 2000 ) . We   also evaluate our approach on sentiment analysis   using the binary movie review dataset ( Maas   et al . , 2011 ) and the binary version of the-2   dataset ( Socher et al . , 2013 ) . We finally use the   large - scale and datasets from   Zhang et al . ( 2015 ) for topic classification . We   undersample the latter and form a Dof20 K ex-   amples and D2 K as in Margatina et al . ( 2021 ) .   For -6 , and-2we randomly sample   10 % from the training set to serve as the valida-   tion set , while for we sample 5 % . For   the dataset we undersample both training   and validation datasets ( from the standard splits )   to facilitate our AL simulation ( i.e. the original   dataset consists of 560 K training and 28 K valida-   tion data examples ) . For all datasets we use the   standard test set , apart from the-2dataset that   is taken from the benchmark ( Wang et al . ,   2019 ) we use the development set as the held - out   test set ( and subsample a development set from the   original training set ) .   A.2 Training & AL Details   We use BERT- ( Devlin et al . , 2019 ) and fine-   tune it ( ¬ß 2 ) for 100 K steps , with learning   rate2e‚àí05and the rest of hyperparameters as in   Gururangan et al . ( 2020 ) using the HuggingFace   library ( Wolf et al . , 2020 ) . We evaluate the model   5times per epoch on Dand keep the one with   the lowest validation loss as in Dodge et al . ( 2020 ) .   We use the code provided by Kirsch et al . ( 2019 )   for the uncertainty - based acquisition functions and   Yuan et al . ( 2020 ) for A , B andBKM .   We use the standard splits provided for all datasets ,   if available , otherwise we randomly sample a val-   idation set . We test all models on a held - out test   set . We repeat all experiments with five different   random seeds resulting into different initializations   ofDand the weights of the extra task - specific   output feedforward layer . For all datasets we use as   budget the 15 % ofD. Each experiment is run   on a single Nvidia Tesla V100 GPU.A.3 Hyperparameters   For all datasets we train BERT- ( Devlin et al . ,   2019 ) from the HuggingFace library ( Wolf et al . ,   2020 ) in Pytorch ( Paszke et al . , 2019 ) . We train   all models with batch size 16 , learning rate 2e‚àí5 ,   no weight decay , AdamW optimizer with epsilon   1e‚àí8 . For all datasets we use maximum sequence   length of 128 , except for and that   contain longer input texts , where we use 256 . To   ensure reproducibility and fair comparison between   the various methods under evaluation , we run all   experiments with the same five seeds that we ran-   domly selected from the range [ 1,9999 ] .   A.4 Baselines   Acquisition functions We compare E- with four baseline acquisition functions .   The first is the standard AL baseline , R ,   which applies uniform sampling and selects kdata   points from Dat each iteration . The second is   B ( Ash et al . , 2020 ) , an acquisition function   that aims to combine diversity and uncertainty   sampling . The algorithm computes gradient   embeddings gfor every candidate data point   xinDand then uses clustering to select a   batch . Each gis computed as the gradient of the   cross - entropy loss with respect to the parameters of   the model ‚Äôs last layer . We also compare against a   recently introduced cold - start acquisition function   called A(Yuan et al . , 2020 ) . Aacquisition   uses the masked language model ( MLM ) loss   ofB as a proxy for model uncertainty in   the downstream classification task . Specifically ,   aiming to leverage both uncertainty and diversity ,   Aforms a surprisal embedding sfor each x ,   by passing the unmasked input xthrough the B   MLM head to compute the cross - entropy loss for   a random 15 % subsample of tokens against the   target labels . Aclusters these embeddings to   sample ksentences for each AL iteration . Last ,   following Yuan et al . ( 2020 ) , we use BKM as   a diversity baseline , where the lnormalized B   output embeddings are used for clustering .   Models & Fine - tuning Methods We evaluate   two variants of the pretrained language model ; the   original B model , used in Yuan et al . ( 2020 )   and Ein - Dor et al . ( 2020 ) , and our adapted model   B- ( ¬ß 2 ) , and two fine - tuning methods;833our proposed fine - tuning approach+(¬ß2 ) and   standard Bfine - tuning .   B Appendix : Analysis   B.1 Task - Adaptive Pretraining ( ) &   Full - Dataset Performance   As discussed in ¬ß 2 and ¬ß 4 , we continue training   theBERT- ( Devlin et al . , 2019 ) pretrained   masked language model using the available data   D. We explored various learning rates between   1e‚àí4and1e‚àí5and found the latter to produce   the lowest validation loss . We trained each model   ( one for each dataset ) for up to 100 K optimization   steps , we evaluated on Devery 500steps and   saved the checkpoint with the lowest validation   loss . We used the resulting model in our ( B- ) experiments . We plot the learning curves of   masked language modeling task ( ) for three   datasets and all considered learning rates in Figure   5 . We notice that a smaller learning rate facilitates   the training of the MLM .   In Table 2 we provide the validation and test   accuracy of BandB- for all datasets .   We present the mean across runs with three random   seeds . For fine - tuning the models , we used the   proposed approach+ ( ¬ß 2 ) .   B.2 Performance of Acquisition Functions   In our B- -+experiments so far , we   showed results with E . We have also exper-   imented with various uncertainty - based acquisition   functions . Specifically , four uncertainty - based ac-   quisition functions are used in our work : L   C , E , BALD and B -   BALD .L C ( Lewis and Gale ,   1994 ) sorts Dby the probability of notpre-   dicting the most confident class , in descending   order , E ( Shannon , 1948 ) selects sam-   ples that maximize the predictive entropy , and   BALD ( Houlsby et al . , 2011 ) , short for Bayesian   Active Learning by Disagreement , chooses data   points that maximize the mutual information be-   tween predictions and model ‚Äôs posterior probabil-   ities . B BALD ( Kirsch et al . , 2019 ) is a re-   cently introduced extension of BALD that jointly   scores points by estimating the mutual informa-   tion between multiple data points and the model   parameters . This iterative algorithm aims to find   batches of informative data points , in contrast to   BALD that chooses points that are informative   individually . Note that L C , E- andBALD have been used in AL for NLP   by Siddhant and Lipton ( 2018 ) . To the best of our834 -6 - 2   R 0/0 0/0 0/0 0/0 0/0   A 0/57 0/478 0/206 0/134 0/634   B 0/63 0/23110 0/1059 0/192 -   BKM 0/47 0/2297 0/324 0/137 0/3651   E 81/0 989/0 557/0 264/0 2911/0   L C 69/0 865/0 522/0 256/0 2607/0   BALD 69/0 797/0 524/0 256/0 2589/0   B BALD 69/21 841/1141 450/104 256/482 2844/5611   knowledge , B BALD is evaluated for the first   time in the NLP domain .   Instead of using the output softmax probabilities   for each class , we use a probabilistic formulation of   deep neural networks in order to acquire better cali-   brated scores . Monte Carlo ( MC ) dropout ( Gal and   Ghahramani , 2016 ) is a simple yet effective method   for performing approximate variational inference ,   based on dropout ( Srivastava et al . , 2014 ) . Gal   and Ghahramani ( 2016 ) prove that by simply per-   forming dropout during the forward pass in making   predictions , the output is equivalent to the predic-   tion when the parameters are sampled from a varia-   tional distribution of the true posterior . Therefore ,   dropout during inference results into obtaining pre-   dictions from different parts of the network . Our   B - based Mmodel uses dropout layers during   training for regularization . We apply MC dropout   by simply activating them during test time and we   perform multiple stochastic forward passes . For-   mally , we do Npasses of every x‚àà Dthrough   M(x;W)to acquire Ndifferent output proba-   bility distributions for each x. MC dropout for   AL has been previously used in the literature ( Gal   et al . , 2017 ; Shen et al . , 2017 ; Siddhant and Lip-   ton , 2018 ; Lowell and Lipton , 2019 ; Ein - Dor et al . ,   2020 ; Shelmanov et al . , 2021 ) .   Our findings show that all functions provide sim-   ilar performance , except for BALD that slightly   underperforms . This makes our approach agnos-   tic to the selected uncertainty - based acquisition   method . We also evaluate our proposed methods   with our baseline acquisition functions , i.e. R- , A , BKM and B , since our   training strategy is orthogonal to the acquisitionstrategy . We compare all acquisition functions with   B- -+for and in Fig-   ure 6 . We observe that in general uncertainty - based   acquisition performs better compared to diversity ,   while all acquisition strategies have benefited from   our training strategy ( and+ ) .   B.3 Efficiency of Acquisition Functions   In this section we discuss the efficiency of the   eight acquisition functions considered in this work ;   R , A , B , BKM , E ,   L C , BALD and B BALD .   In Table 3 we provide the runtimes for all ac-   quisition functions and datasets . Each AL experi-   ments consists of multiple iterations and ( therefore   multiple models ) , each with a different training   dataset Dand pool of unlabeled data D. In   order to evaluate how computationally heavy is   each method , we provide the median of all the   models in one AL experiment . We calculate the   runtime of two types of functionalities . The first is   theinference time and stands for the forward pass   of each x‚àà Dto acquire confidence scores for   uncertainty sampling . R , A , B   and BKM do not require this step so it is only   applied of uncertainty - based acquisition where ac-   quiring uncertainty estimates with MC dropout is   needed . The second functionality is selection time   and measures how much time each acquisition func-   tion requires to rank and select the kdata points   fromDto be labeled in the next step of the AL   pipeline . R , E , L C - andBALD perform simple equations to   rank the data points and therefore so do not require   selection time . On the other hand , A , B , 835BKM andB BALD perform iterative al-   gorithms that increase selection time . From all ac-   quisition functions AandBKM are faster   because they do not require the inference step of   all the unlabeled data to the model . E ,   L C andBALD require the same   time for selecting data , which is equivalent for the   time needed to perform one forward pass of the en-   tireD. Finally B andB BALD are   the most computationally heavy approaches , since   both algorithms require multiple computations for   theselection time .R has a total runtime of   zero seconds , as expected.836   Emily ReifDaphne IppolitoAnn YuanAndy Coenen   Chris Callison - BurchJason WeiGoogle ResearchUniversity of Pennsylvania   { ereif , annyuan , andycoenen , jasonwei}@google.com   { daphnei , ccb}@seas.upenn.edu   Abstract   1 Introduction   Text style transfer is the task of rewriting text to   incorporate additional or alternative stylistic ele-   ments while preserving the overall semantics and   structure . Although style transfer has garnered in-   creased interest due to the success of deep learn-   ing , these approaches usually require a substantial   amount of labeled training examples , either as par-   allel text data ( Zhu et al . , 2010 ; Rao and Tetreault ,   2018 ) or non - parallel text data of a single style . ( Li   et al . , 2018 ; Jin et al . , 2019 ; Liu et al . , 2020 ; Kr-   ishna et al . , 2020 ) . Even bleeding - edge approaches   that tackle the challenging problem of label - free   style transfer are limited in that they require at least   several exemplar sentences that dictate a given tar-   get style ( Xu et al . , 2020 ; Riley et al . , 2021 ) . Hence ,   recent survey papers have identiÔ¨Åed a need for new   methods that both reduce the training data require-   ments and expand the scope of styles supported   ( Jin et al . , 2020 ; Hu et al . , 2020 ) .   In this work , we present augmented zero - shot   learning , a prompting method that allows large   language models to perform text style transfer to   arbitrary styles , without any exemplars in the target   style . Our method builds on prior work showing   that sufÔ¨Åciently large LMs such as GPT-3 can per-   form various tasks ranging from classiÔ¨Åcation to   translation , simply by choosing a clever prompt to   prepend to the input text for which the model is   asked to continue ( Brown et al . , 2020 ; Branwen ,   2020 ) . Using a single prompt that provides sev-   eral demonstrations of sentences being ‚Äú rewritten ‚Äù   to meet a desired condition , language models can   extrapolate and rewrite text in unseen styles . We   are thus able to perform style transfer to arbitrary   styles such as ‚Äú make this sentence more comic ‚Äù or   ‚Äú include the word balloon . ‚Äù   Augmented zero - shot learning is simple and fa-   cilitates the application of style transfer to a wider837range of styles than existing work . Our contribu-   tions are the following .   1.We propose a recipe for style transfer using large   LMs that is label - free , training - free , and intu-   itively controllable .   2.Via human evaluation , we Ô¨Ånd that our method   achieves strong performance on both standard   and non - standard style transfer tasks . We also   compare our approach for sentiment transfer   with prior methods using automatic evaluation .   3.We explore real - world desired style transfers   generated from users of a text editing UI that   implements our method .   2 Augmented zero - shot prompting   Although large LMs are trained only for continua-   tion , recent work has shown that they can perform   a variety of NLP tasks by expressing the task as   a prompt that encourages the model to output the   desired answer as the continuation ( Puri and Catan-   zaro , 2019 ; Weller et al . , 2020 ; Brown et al . , 2020 ;   Schick and Sch√ºtze , 2021 , inter alia ; see Liu et al .   ( 2021a ) for a survey ) . The simplest approach , zero-   shot prompting , directly uses natural language to   ask the large LM to perform a task , as shown in   Figure 1a . Zero - shot prompting , however , can be   prone to failure modes such as not returning well-   formatted or logical outputs ( see x6).Few - shot   prompting , as shown in Figure 1b , has been shown   to achieve higher performance , but requires exem-   plars for the exact task that we want the model to   perform . Such few - shot examples can be easily ob-   tained if the desired style transformation is known   ahead of time , but this ultimately limits style trans-   fer to a set of pre - speciÔ¨Åed style tasks .   To remove the need for these labeled exemplars   for each style transfer task , we propose augmented   zero - shot learning , a method for performing multi-   task style transfer using a single set of exemplars .   Instead of prompting the model with exemplars   speciÔ¨Åc to the exact style transfer task we wish   to perform , we prompt the model with examples   of a variety of sentence rewriting operations , as   shown in Figure 1c . This intuition is inspired by   Reynolds and McDonell ( 2021 ) ‚Äôs observation that   successful prompts constrain the behavior of the   large LM away from failure modes ‚Äî in our case ,   we aim to preserve the Ô¨Çexibility of a zero shot   prompt while encouraging the model to produce   outputs of a speciÔ¨Åc template . We keep the the   format of the exemplars constant and insert the de-   sired sentence transformation into the same format .   In this way , the augmented zero - shot formulation   supports arbitrary sentence rewriting tasks without   the need to write any task - speciÔ¨Åc exemplars . Thus ,   it works for a wide range of styles , including modi-   fying the text to be ‚Äú more melodramatic , ‚Äù ‚Äú insert   a metaphor , ‚Äù or ‚Äú include the word balloon . ‚Äù   3 Experimental Setup   Style transfer tasks . We consider six style trans-   fer tasks that we deem non - standard , listed in Table   1 . These styles were chosen to be representative of   most frequent style adjustments made by users of   an AI - assisted text editor that employs our method   ( discussed further in x5 ) . As source sentences , we   use 50 sentences randomly drawn from the Reddit   Writing Prompts validation set ( Fan et al . , 2018 ) ,   excluding those that already clearly exhibited one   of the styles or were ungrammatical / incoherent .   We use human evaluation for these styles , since not   all styles have readily available classiÔ¨Åers .   We also evaluate our method on two standard   style transfer tasks : sentiment and formality . We   use the Yelp polarity dataset ( Zhang et al . , 2015 )   for sentiment and Grammarly ‚Äôs Yahoo Answers   Formality Corpus ( GYAFC ) dataset for formality   ( Rao and Tetreault , 2018).These datasets allow   us to evaluate performance of augmented zero - shot   learning in the context of prior supervised methods   which have been used on these tasks.838Model . Augmented zero - shot learning requires a   large language model . We primarily use LaMDA ,   a left - to - right decoder - only transformer language   model ( Vaswani et al . , 2017 ) with a non - embedding   parameter count of 137B ( Thoppilan et al . , 2022 ) .   The pre - trained LaMDA model , which we refer to   asLLM , was trained on a corpus comprising 1.95B   public web documents , including forum and dialog   data and Wikipedia . The dataset was tokenized into   2.49 T BPE tokens with a SentencePiece vocabulary   size of 32 K ( Kudo and Richardson , 2018 ) . We also   useLLM - Dialog , the Ô¨Ånal LaMDA model which   was Ô¨Ånetuned on a curated , high - quality subset of   data identiÔ¨Åed to be in a conversational format .   Decoding was done with top- k=40 . To show that   the success of augmented zero - shot learning is not   restricted to these two large LMs , we also perform   experiments with GPT-3 ( Table 8) . For GPT-3 ,   decoding was done with nucleus sampling using   p=0.6 ( Holtzman et al . , 2019 ) .   The prompts used for LLM and GPT-3 are shown   in Figure 1 . For LLM - Dialog , the prompt was in-   stead formulated as a conversation between one   agent who is requesting rewrites and another who   is performing the rewrites . See Table 7 in the Ap-   pendix for the full non - abbreviated prompts .   4 Results   4.1 Non - Standard Styles   For our six non - standard styles , we asked six pro-   fessional raters to assess < input sentence , target   style , output sentence > tuples . These raters are   Ô¨Çuent in English , live in India , and work full time   labeling and evaluating data . To decrease inter - rater   discrepancy and ensure that our instructions were   clear , we had an initial calibration session where   they test - rated a small portion of the data ( around   10 datapoints which were then omitted from the   results ) and asked us any clarifying questions . For   each style , we compare outputs from our method   plus the three baselines for 50 sentences .   Each tuple was scored by three raters ( 3,600 rat-   ings total ) on the following three axes which are   standard to textual style transfer ( Mir et al . , 2019 ):   ( 1 ) transfer strength ( the amount that the output   actually matches the target style ) , ( 2 ) semantic   preservation ( whether the underlying meaning of   the output text , aside from style , matches that of the   input ) , and ( 3 ) Ô¨Çuency ( whether the text is coherent   and could have been written by a proÔ¨Åcient English   speaker ) . Following Sakaguchi and Van Durme   ( 2018 ) , transfer strength and semantic preservation   were rated on a scale from 1‚Äì100 . A screenshot   of the evaluation UI is shown in Figure 5 in the   Appendix . Note that the guidelines for semantic   preservation are not standardized in prior literature   ( Briakou et al . , 2021 ) ; while some evaluations are   strict that the outputs can not contain any more infor-   mation than the inputs , we asked the annotators not   to penalize for meaning transformations which are   necessary for the speciÔ¨Åed transformation . We use   dialog - LLM , and compare it with three other meth-   ods : ( 1 ) zero - shot ( a baseline ) , ( 2 ) paraphrase   ( our normal augmented zero shot prompt , but with   the target style of ‚Äú paraphrased ‚Äù , as a control ) and   ( 3 ) human ( ground - truth transformations written   by the authors ) .   Figure 2 shows these results . We found that the   outputs of our method were rated almost as highly   as the human - written ground truth for all three   evaluations . The zero - shot baseline performed the   worst in all categories : 25.4 % of the time , it did not   return a valid response at all ( see x6 ) , compared   with 0.6 % for augmented zero shot . The strong   performance of the paraphrase baseline at Ô¨Çuency   and semantic similarity shows that large LMs are   capable of generating high quality text that remains   true to the input sentence ‚Äôs meaning . Overall , the   average length of the input sentences was 66 char-   acters , whereas the average length of augmented   zero - shot outputs was 107 characters . For context ,   human paraphrase outputs were 82 characters .   For a subset of the tasks , some automatic evalua-   tion was also possible . We found that the ‚Äú balloon ‚Äù   and ‚Äú park ‚Äù transformations successfully inserted839the target word 85 % of the time . For ‚Äú more descrip-   tive ‚Äù and ‚Äú include a metaphor ‚Äù the transformed   text was , as expected , longer than the original ( by   252 % and 146 % respectively , compared with 165 %   and 146 % for human baselines ) .   4.2 Standard Styles   To better contextualize the performance of our   method with prior methods , we also generated out-   puts for two standard style transfer tasks : sentiment   and formality . Figure 3 shows human evaluations   ( same setup as before ) for our outputs as well as   the outputs from two popular prior style transfer   methods , Unsup MT ( Prabhumoye et al . , 2018 ) and   Dual RL ( Luo et al . , 2019 ) . The outputs from our   method were rated comparably to both human gen-   erated responses and the two prior methods , using   the same rating setup as the non - standard styles ,   with six outputs and baselines for four styles across   50 sentences , rated independently by three raters ,   totalling 3,000 total ratings .   Furthermore , following Li et al . ( 2018 ) and Sud-   hakar et al . ( 2019 ) , we perform automatic evalu-   ation for sentiment style transfer since there are   classiÔ¨Åers available for these styles . We note that   although automatic evaluations can diverge from   human ratings , they can still be a good proxy as   we could not perform human evaluation against   every prior method due to time and resource con-   straints . We automatically evaluate ( 1 ) transfer   strength using a sentiment classiÔ¨Åer from Hug-   gingFace Transformers ( Wolf et al . , 2020 ) , ( 2 ) se-   mantic similarity to human examples provided by   Luo et al . ( 2019 ) via BLEU score , and ( 3 ) Ô¨Çuency   via perplexity , as measured by GPT-2 ( 117 M ) .   Table 2 shows these automatic evaluations , with   four main takeaways . First , augmented zero - shot   prompting achieves high accuracy and low perplex-   ity compared with baselines . The BLEU scores ,   however , are low , which we believe is because it   tends to add additional information to generated   sentences ( see Appendix B for a deeper analysis ) .   Second , we apply augmented zero - shot learning to   GPT-3 175B ; these results indicate that augmented   zero - shot learning generalizes to another large lan-   guage model . Third , we vary model size for GPT-3   models , Ô¨Ånding that larger size greatly improves   style transfer . Fourth , for LLM andLLM - dialog ,   we Ô¨Ånd that augmented zero - shot learning substan-   tially outperforms vanilla zero - shot learning and   almost reaches the accuracy of Ô¨Åve - shot learning .   5 Potential of Arbitrary Styles   One promising application of augmented zero - shot   learning is an AI - powered writing assistant that   can allow writers to transform their text in arbitrary   ways that the writer deÔ¨Ånes and controls . As a qual-   itative case study to explore what arbitrary re - write   styles may be requested , we built an AI - assisted   story - writing editor with a ‚Äú rewrite as ‚Äù feature that   uses our augmented few - shot method . Our edi-   tor has a freeform text box for users to specify   how they would like a selection of their story to be   rewritten ( see Figure 6 in the Appendix ) . We asked   30 people from a creative writing group to use our   UI to write a 100 - 300 word story , collecting 333   rewrite requests in total . Table 3 shows a subset of   these , which were as diverse as asking for the text   ‚Äú to be about mining ‚Äù or ‚Äú to be less diabolical . ‚Äù   6 Limitations and Failure Modes   This section details several qualitative limitations   with our method .   Unparsable answers A frequent problem that   arises when using large LMs for other NLP tasks   is their outputs can not be automatically parsed into   usable answers . For example , when given a prompt   like LLM - Dialog might   return something like Similar error modes exist for   LLM , which might output something like Other840   times , the response contains correct information ,   but it can not be automatically parsed ( e.g. , ) In hindsight , these outputs   make a lot of sense : most of the training data of   large LMs is not well - formatted pairs of inputs and   outputs ( Reynolds and McDonell , 2021 ) . See xA   for how we dealt with these issues .   Hallucinations Large LMs are known to halluci-   nate text content ; we saw this happen frequently for   style transfer . While this is an advantage in some   contexts like creative writing , it is undesirable for   applications like summarization .   Inherent style trends We also noticed that even   our‚Äúparaphrase ‚Äù baseline , where the model was   simply asked to rewrite the input sentence , wasrated highly for style strength for a few styles , in-   cluding ‚Äú more formal ‚Äù and‚Äúmore melodramatic ‚Äù .   This implies that our method ‚Äôs generations gen-   erally trend toward these styles . A direction for   future work would be to see what styles and quali-   ties of text our method ( and large LMs in general )   are inherently more likely to produce .   Less reliable than trained methods For style   transfer tasks that have available training data , prior   methods that either train or Ô¨Ånetune on that data are   going to be inherently more reliable at producing   text that looks like their training data . This can be   observed in the lower BLEU scores our method   achieves than trained methods , despite comparable   transfer accuracy ( Section B ) . Thus , augmented   zero - shot learning offers less Ô¨Åne - grained control-   lability in the properties of the style - transferred text   than methods which see task - speciÔ¨Åc training data .   Large LM safety concerns Large LMs them-   selves come with their own host of difÔ¨Åculties ,   barriers to entry , and potential safety concerns as   discussed by Bender et al . ( 2021 ) , which are also   valid for this style transfer method . However , we   also think that this method can be a useful tool in   exploring and exposing the safety and boundaries   of these models themselves : what happens if we try   to force the large LM to make a text ‚Äú more racist ‚Äù ,   ‚Äú more sexist ‚Äù , or ‚Äú more incendiary ‚Äù ? It is important   to keep pushing these models to their boundaries to   see where they fail and where problems arise , and   speciÔ¨Åc use cases that show a broader range of the   model ‚Äôs capabilities also show a broader range of   its failure modes .   7 Conclusions   We introduced augmented zero - shot learning ,   which we Ô¨Ånd shows shows strikingly promis-   ing performance considering its simplicity . This   prompting paradigm moves the needle in text style   transfer by expanding the range of possible styles   beyond the currently limited set of styles for which   annotated data exists . More broadly , we also hope   that the strategy of prompting a large LM with non-   task speciÔ¨Åc examples can inspire new inference-   only methods for other NLP tasks .   References841842843Appendix   A Prompt Selection   A promising new area of prompt engineering has   arisen to address the failure modes discussed above ,   speciÔ¨Åcally the invalid or unparseable answers .   Reynolds and McDonell ( 2021 ) Ô¨Ånd that prompt-   ing a model for a task is more akin to locating an   already - learned task than truly learning a new one .   Moreover , they emphasize that prompt engineer-   ing is mostly about avoiding various failure cases   such as those described above . In this work , we   use delimiters ( ‚Äú { ‚Äù and ‚Äú } ‚Äù ) to help avoid these   types of errors , giving scores of zero when there   was no valid responses with such delimiters . There   are other delimiters that could be used ( e.g. , quotes ,   ‚Äú ( ‚Äù and ‚Äú ) ‚Äù , ‚Äú < ‚Äù and ‚Äú > ‚Äù , newlines with a colon ( as   used by GPT-3 ) , etc . We chose curly braces as they   were 1 ) likely to occur in the training data as delim-   iters in other contexts and 2 ) not frequently part of   the input sentence itself . We also use a second per-   son prompt template for the dialog , which yielded   better results as it was more similar to the training   data . Exploring these options more quantitatively   would be an interesting direction for future work .   Because the performance of prompting can vary   depending on the exact language of the prompt   ( Reynolds and McDonell , 2021 ) , we compare   four variations of prompts for sentiment : ‚Äú more   positive / negative , ‚Äù ‚Äú happier / sadder , ‚Äù ‚Äú more opti-   mistic / pessimistic , ‚Äù and ‚Äú more cheerful / miserable . ‚Äù   As shown in Table 4 in the Appendix , performance   differed across the four prompts , but we found them   comparable .   B Low BLEU for LLM Outputs   As we saw in Table 2 , the outputs of our model   had low BLEU scores with respect to human gen-   erated outputs , while simultaneously having high   semantic similarity in human evaluations . Based on   qualitative examination of outputs , we believe that   this is because model outputs often , despite having   high semantic similarity with the source sentence ,   used different language from human annotations .   For instance , for transferring the sentiment of ‚Äú ever   since joes has changed hands it ‚Äôs just gotten worse   and worse ‚Äù to positive sentiment , our augmented   zero - shot learning model outputted ‚Äú the establish-   ment has continued to provide excellent service ,   improving steadily since its change of ownership . ‚Äù   This will have low BLEU with the ground truth   with respect to human references , which is simply   ‚Äú ever since joes has changed hands it ‚Äôs just gotten   better and better . ‚Äù   Though we do not see this as an inherent prob-   lem , increasing the BLEU for the purposes of com-   parison can be done in an easy way via candidate   selection , as our model returns sixteen possible   continuations . In applications for which we prefer   model outputs to have high lexical similarity to the   source sentence , we could select the candidate of   the sixteen with the highest BLEU score compared   with the original source sentence . We Ô¨Ånd that   this candidate selection step can substantially im-   prove the BLEU score with the ground truth target   sentences , as we show in Table 8 .   C Further Related Work   Style transfer has gained increasing attention in the   NLP landscape , for which neural models have been   trained to perform style transfer for styles including   sentiment , formality , politeness , gender , and politi-844   cal slant ( Prabhumoye et al . , 2018 ; Madaan et al . ,   2020 ; Liu et al . , 2021b ) . We will brieÔ¨Çy summarize   the primary approaches to style transfer here , and   refer the involved reader to either ( Jin et al . , 2020 )   or ( Hu et al . , 2020 ) for a survey .   Most text style transfer approaches fall in two   categories . Early approaches tend to require par-   allel text data ( Zhu et al . , 2010 ; Rao and Tetreault ,   2018 ) , where every input in the source style has a   corresponding output in the target style . Though   this formulation elegantly Ô¨Åts the standard encoder ‚Äì   decoder paradigm , the availability of a parallel   text corpus is a stringent requirement . Hence , re-   cent text style transfer approaches have instead   used non - parallel monostyle data ( no one - to - one-   mapping between instances in the source and target   styles ) . Such methods include latent representation   manipulation ( Liu et al . , 2020 ) , prototype - based   text editing ( Li et al . , 2018 ) , and pseudo - parallel   corpus construction ( Jin et al . , 2019 ) . However ,   even non - parallel monostyle data can be hard to   collect for arbitrary styles . As such , surveys have   called for more research on approaches that expand   the scope of supported styles and reduce the train-   ing data requirements for style transfer systems ( Jin   et al . , 2020 ; Hu et al . , 2020 ) .   Several new methods tackle the challenging   problem of label - free style transfer , which does   not require a full corpus of labeled data , but rather   just a few exemplars that deÔ¨Åne a style . Xu et al .   ( 2020 ) use variational autoencoders for unsuper-   vised learning of controllable representations for   text . Riley et al . ( 2021 ) extract a style vector from   a set of target texts and use this vector to condition   the decoder to perform style transfer to a target   style . These approaches have a similar goal to ours   in terms of expanding the scope of possible style   transfers . However , they are different in two main   ways . First , they require a fully specialized model ,   where our method can be applied out - of - the - box   with something like GPT-3 . This can either be a   strength or weakness , depending on the availability   of such a model . Second , they require exemplars   to deÔ¨Åne a style rather than a plain text description.845846847848   Abhyuday Bhartiya   Indian Institute of Technology   New Delhi , IndiaKartikeya Badola   Indian Institute of Technology   New Delhi , IndiaMausam   Indian Institute of Technology   New Delhi , India   Abstract   1 Introduction   Relation Extraction ( RE ) identiÔ¨Åes the relation r   between a pair of entities ( e;e)given some text   mentioning both of them . To avoid large manual   annotation , RE is often trained via distant super-   vision ( DS - RE ) ( Mintz et al . , 2009 ) . DS - RE uses   factsr(e;e)in an existing KB to associate a la-   belrwith the bag containing all sentences that   mentioneande . Research in DS - RE has been   mostly monolingual and limited to English . Our   goal is to study multilingual RE via distant super-   vision ( Multi DS - RE ) . We expect multilingual RE   models to have several beneÔ¨Åts over monolingual   RE . First , training data from multiple languages   may be pooled to create a large dataset , enabling   cross - lingual knowledge transfer ( Zoph et al . , 2016 ;   Feng et al . , 2020 ) . Second , it may encourage RE   models to be consistent across languages ( Lin et al . ,   2017 ) , e.g. , extraction of a fact already seen in one   language should be easier in another .   To the best of our knowledge , RELX - Distant   ( K√∂ksal and √ñzg√ºr , 2020 ) is currently the onlydataset available for Multi DS - RE , but even so , it   has never been evaluated as a benchmark for the   task . Our analysis reveals that it suffers from a   poor selection of relation classes . Firstly , there are   no examples of NA class ( sentences with no rela-   tion between the two entities ) . Therefore , a model   trained on RELX - Distant would Ô¨Ånd limited utility   in any real world setting . Secondly , its choice of   relation classes is highly disjoint , resulting in an   absence of instances with multiple labels ( unusual   for a DS - RE dataset ) . Finally , it is highly imbal-   anced ‚Äì even though it has 24 relation classes , over   50 % bags belong to just one ‚Äú country ‚Äù relation .   Owing to these attributes , we observe that mod-   els trained on RELX - Distant end up classifying the   instances of the minority class based on just the en-   tity type information . Due to high skew , such mis-   takes have negligible impact on evaluation scores   and the model achieves an AUC of 0.99 after only 5   training epochs . Such numbers are unheard of , es-   pecially when compared to benchmarking datasets   in mono - lingual RE ( mono - lingual variant of the   same architecture obtains an AUC of 0.83 when   trained and tested on the GDS dataset ( Jat et al . ,   2018 ) .   In response , we contribute a more realistic bench-   mark dataset for the task called DiS - ReX. Our   dataset has over 1.8 million sentences in four lan-   guages : English , French , Spanish and German . It   has 37 relation types including 1 No - Relation ( NA )   class and also has instances with multiple labels   similar to the widely - used New York Times ( NYT )   dataset for English DS - RE ( Riedel et al . , 2010 ) ,   thus comparing favorably to RELX - Distant .   We also adopt state - of - the - art DS - RE models   in the multilingual setting by using the mBERT   encoder ( Devlin et al . , 2019 ) , to create a strong   baseline for this task .   We achieve an AUC of 0.82 and a Micro - F1 of   0.76 , suggesting that the dataset is not trivial to   optimize on , and could act as a good benchmark849Language # sentences # bags # non - NA bags Average non - NA bag - size   English 532499 216806 66932 4.50   French 409087 226418 83951 2.88   Spanish 456418 229512 80706 2.88   German 438315 194942 45908 3.48   for the task . We publicly release DiS - ReX and the   baseline .   2 Related Work   Supervised RE datasets such as ACE05 ( Walker   et al . , 2006 ) and KLUE ( Park et al . , 2021 ) are gen-   erally small , owing to the supervision needs per re-   lation . Distant supervision ( Mintz et al . , 2009 ) is a   popular alternative to large - scale human annotation ,   but necessitate more complex models to handle   dataset noise . The standard English DS - RE dataset   is New York Times ( NYT ) corpus ( Riedel et al . ,   2010 ) , which has served as the benchmark for re-   search over the years . DS - RE models have evolved   to use multi - instance learning ( Hoffmann et al . ,   2011 ) , multi - label learning ( Surdeanu et al . , 2012 ) ,   corrections for false negatives ( Ritter et al . , 2013 ) ,   and neural models such as piecewise CNNs ( Zeng   et al . , 2015 ) , intra - bag attention ( Lin et al . , 2016 ) ,   and reinforcement learning ( Qin et al . , 2018 ) .   Lin et al . ( 2017 ) and Wang et al . ( 2018 ) pro-   pose extensions of bag - attention models for bilin-   gual ( English - Mandarin ) datasets . However , their   adoption to multiple languages has been lacking ,   due to absence of a reliable multilingual dataset .   Although RELX - Distant is the only Multilingual   DS - RE dataset so far , it was n‚Äôt originally used for   Multi DS - RE task but to pre - train a model that gets   Ô¨Åne - tuned for supervised RE task .   Contemporary to our work , other multilingual   RE datasets and methods are being developed .   These include a dataset for joint entity and relation   extraction ( Seganti et al . , 2021 ) , a model for mul-   tilingual KB completion ( Singh et al . , 2021 ) , and   an approach for automatic construction of cross-   lingual training data for Open IE ( Kolluru et al . ,   2022 ) . Our proposed dataset , DiS - ReX , has already   been used for further research on the Multilingual   DS - RE task ( Rathore et al . , 2022).3 Dataset Curation   All distant supervision datasets are curated by align-   ing known KB facts with sentences in a large cor-   pus . We follow the same for DiS - ReX , while pay-   ing attention to cross - lingual normalization , and   overall data and language statistics .   First , we harvest a large number of sen-   tences from English , French , Spanish and German   Wikipedias . We use DBPedia language editions   ( Lehmann et al . , 2015 ) for our KB ‚Äì this gives us   good coverage of entities that are local to different   language speakers . DBPedia entities are associated   with Wikidata IDs , which are normalized across   languages . This enables us to fuse these DBPedia   KBs and establish equivalence between entities like   USA andEstados Unidos de Am√©rica .   Next , we use a language - speciÔ¨Åc NER tagger ,   ( we use the mdvariant of spaCy ( Honnibal et al . ,   2020 ) NER taggers for each language ) , returning   a rich set of sentences . In contrast , RELX - Distant   Ô¨Ånds entity mentions using Wikipedia hyperlinks .   This severely limits its pool of sentences , since   often only the Ô¨Årst mention of an entity in a Wiki   document has a hyperlink while others do not .   Linking each mention with its entity can be chal-   lenging , due to unavailability of high - quality entity   linking software for every language . We take the   pragmatic approach of using simple string match-   ing , but only on the subset of entities that have an   unambiguous surface form ( or alias ) in our fused   KB . This maintains scalability to many languages ,   while ensuring high enough precision of linking .   For each entity - pair , we create a language-   speciÔ¨Åc bag of all sentences that mention both . We   also search for all relations between them in our   fused KB . We associate the bag with all those rela-   tion labels , or ‚Äú NA " , if no relation is found .   Our next steps select a balanced subset of this   dataset , so that it can serve as a good benchmark   for Multi DS - RE . We Ô¨Årst select the subset of re-   lations that have at least 50 bags in all languages.850This yields the 36 positive relation types used in   our data . For each relation type , we limit the num-   ber of bags in a language to a max of 10,000 . This   helps curb the skew due to highly frequent relations   such ascountry andbirthPlace . During this Ô¨Ål-   tering , we ensure that bags of entity pairs common   across more than one language are not removed , so   that we have an abundant number of cross - lingual   bags . Models can take advantage of such bags for   establishing representation consistency across lan-   guages ( Wang et al . , 2018 ) . Finally , we add bags   of entity pairs that have no relation between them .   Similar to NYT dataset , ‚Äú NA ‚Äù is the majority class   in DiS - ReX ( kept at roughly 70 % ) .   Hence , we obtain a dataset with over 1.8 million   sentences , and over 250,000 ( non - NA ) bags ( see   table 1 for more statistics ) . The 36 relations include   frequent relations between persons , locations and   organizations ( e.g. , capital , headquarter , works-   at ) , and also some relations with Ô¨Åne - grained types   such as bandMember , starring andrecordLabel .   We estimate the percentage of bags satisfying   ‚Äú at - least one ‚Äù assumption by manually labelling   sentences across 50 randomly selected bags . We   Ô¨Ånd that 82 % of the bags satisfy ‚Äú at - least one ‚Äù as-   sumption . For the test set of NYT Corpus , this   percentage is close to 62 % ( Zhu et al . , 2020 )   Finally , we create train - dev - test splits by split-   ting the bags in the ratio 70 : 10 : 20 . While   splitting we ensure that entity - pairs in three sets are   mutually exclusive , so the model does not extract   by memorizing a fact .   4 Experiments and Data Analysis   4.1 Comparison : DiS - ReX vs. RELX - Distant   We now compare the two datasets : DiS - ReX and   RELX - Distant . We Ô¨Ånd that the our dataset show-   cases several desirable properties expected from a   challenging DS - RE dataset , including the presence   of NA relations , inverse relations , multi - label bags ,   and better class balance .   70 % of bags in DiS - ReX are NA bags , whereas   RELX - Distant has none . We also note that a few   relation pairs ( from our 36 relations ) represent in-   verses of each other , e.g. , { inÔ¨Çuenced by , inÔ¨Çu-   enced } , { successor , predecessor } , and { associat-   edBand , bandMember } . Inverse relations test an   extractor ‚Äôs ability to output related relations from   the same bag , but with different entity ordering .   RELX - Distant has no inverse relations in its rela-   tion vocabulary .   A key characteristic of DS - RE problems is that   they need multi - label modeling ( Surdeanu et al . ,   2012 ) , since multiple relations commonly exist be-   tween an entity pair . RELX - Distant has no such   bags , primarily because its choice of relation types   is such that almost no entity - pair can have mul-   tiple relations . E.g. , its Person - Person relations   aremother , spouse , father , sibling , partner , where   multi - label bags are highly unlikely . In contrast ,   DiS - ReX has 21,642 bags that have more than one   relation label . As an example , the entity pair ( Isaac   Newton , England ) is associated with four relations   ‚Äì birthPlace , country , deathPlace andnationality .   To compare the imbalance amongst non - NA re-   lation classes in DiS - ReX and RELX - Distant , we   calculate normalized entropy ( Shannon , 1948 ) , also   known an EfÔ¨Åciency (  ) . Value closer to 1 indicates   that the class - wise distribution is closer to the uni-   form distribution . Results in Table 2 indicate that   DiS - ReX is a more balanced dataset ( more details   regarding calculation of in appendix )   4.2 Baseline Performance   We implement three DS - RE baselines for our DiS-   ReX dataset . Our Ô¨Årst baseline is PCNN+Att   ( Lin et al . , 2016 ) , which uses a piece - wise CNN   as the sentence encoder and performs bag - level   multi - label classiÔ¨Åcation using Intra - Bag atten-   tion . In this model , each language is trained and   tested upon separately . Inspired by Ni and Florian   ( 2019 ) , we extend this to design a second baseline,851mBERT+Att . It replaces PCNN encoders with a   shared mBERT encoder ( Devlin et al . , 2019 ) and   retains the intra - bag attention architecture for con-   structing the bag representation . Our last base-   line is mBERT+MNRE , which adapts the MNRE   model ( Lin et al . , 2017 ) to our setting . MNRE intro-   duced cross - lingual attention for bilingual RE . We   extend this attention module to more than two lan-   guages and also replace its language - speciÔ¨Åc CNN   encoders with a shared mBERT encoder . More   details on baselines and training are in appendix .   We Ô¨Årst compare mBERT+Att model on both   DiS - ReX and RELX - Distant in Table 3 . We Ô¨Ånd   that RELX - Distant achieves an unreasonably high   AUC and micro - F1 . Since Micro - F1 may be over-   whelmed by a few highly frequent relations , we   also report Macro - F1 scores . Even the Macro - F1   scores of RELX - Distant are over 10 pt higher , sug-   gesting that DiS - ReX is a more challenging dataset   for our task . We also report the Macro - avg of   F1 scores of 3 most frequent and 3 least frequent   classes of both the datasets in Table 2 . The per-   formance drops by 45pts in RELX - Distant , more   than double the decrease observed in our dataset ,   corroborating that the RELX - Distant model is not   learning infrequent relations effectively . For that   model , we notice that the person - person relation   types , which are minority classes , obtain the low-   est F1 scores . It gets confused between mother   andspouse or between father andsibling . In some   cases , the conÔ¨Ådence is as high as 95 % on such   errors . This suggests that the model is making pre-   dictions based solely on head - tail entity types in   instances belonging to the person - person relation   classes . But , such mistakes depress the Micro - F1   and AUC scores only negligibly , due to severe class   imbalance . Thus , the high scores do not reÔ¨Çect high   model quality .   We report results of three models on DiS - ReX   in Table 4 ‚Äì mBERT+MNRE achieves 0.82 AUC   and 0.76 micro - F1 , establishing the best baseline   performance on our task .   4.3 Error Analysis   We Ô¨Ånd that due to incorporation of NA class , multi-   label bags and Ô¨Åne - grained relation classes , DiS-   ReX offers several new challenges . We observe   that on multi - label bags , micro - F1 falls drastically   from roughly 0.84 ( bags with 1 label ) to 0.35 ( 4   labels ) , primarily due to reducing recall ( statistics   in Table 5).#relations Micro - F1 Precision Recall   1 0.842 0.865 0.820   2 0.673 0.934 0.525   3 0.518 0.959 0.354   4 0.348 0.937 0.214   We also perform manual error analysis of 100   random and 100 most conÔ¨Ådent mistakes made by   the model trained on DiS - ReX. For errors where a   non - NA relation is incorrectly predicted as another ,   we Ô¨Ånd one major error class ‚Äì highly conÔ¨Ådent   mistakes in predicting closely related relation types   that have high overlaps , such as { author , direc-   tor } , and { homeTown , birthPlace } . Some model   errors correspond to confusion in predicting inverse   relations such as { successor , predecessor } and { in-   Ô¨Çuenced , inÔ¨ÇuencedBy } . Such cases are absent in   the RELX - Distant test set . We found less than 10 %   errors within the conÔ¨Ådent errors are due to entity   disambiguation mistakes in ground truth , however ,   we found no such data error in the 100 random   errors , suggesting that this failure mode is not the   most frequent , and the test data is relatively clean .   We additionally divide the errors made on the   entire test set by the best performing model into   three variants .   ‚Ä¢Type-1 Error : Model predicts a positive ( Non-   NA ) relation label R1and ground label is also   a positive ( non - NA ) relation label R2butR2   is not the same as R1 .   ‚Ä¢Type-2 Error : Model predicts NA relation   label but ground label is a positive ( non - NA )   relation label .   ‚Ä¢Type-3 Error : Model predicts positive ( non-   NA ) relation label but ground label is NA re-   lation label .   We present the distribution of these three errors   in Table 6 . Predicting non - NA as NA and NA as   non - NA relation make up most ( 55 - 85 % ) of the   errors . We believe that eliminating such kinds of   errors would be an important focus area in DS - RE   research , especially for datasets which are better   representative of real world settings.852Language Type-1 Error ( % ) Type-2 Error ( % ) Type-3 Error ( % )   English 44.49 31.17 24.33   French 29.69 36.14 34.15   Spanish 35.08 36.37 28.54   German 14.94 45.28 39.77   4.4 Is mBERT+Att Language Agnostic ?   It is believed that sharing mBERT encoder across   languages is advantageous for cross - lingual trans-   fer ( Wu and Dredze , 2019 ) . This is reÔ¨Çected in   our experiments too where mBERT+Att strongly   outperforms PCNN+Att .   mBERT+Att produces a single embedding for   a multilingual bag , summarizing mBERT embed-   dings of individual sentences . We posit that for   this model to achieve its true potential on DiS - ReX ,   mBERT encoder must learn to map all sentences to   a language - agnostic representation space , or else   the downstream bag attention model may get con-   fused between intra - language and inter - language   variability . We investigate this further by raising   the question : is the mBERT encoder learning lan-   guage agnostic embeddings ?   For this we encode all sentences in multilingual   bags ( that contain all languages ) using the encoder   of trained mBERT+Att model and plot the sentence   embeddings using tSNE . We show an illustrative   Ô¨Ågure for the bag ( Swiss , Switzerland ) in Figure 1 .   We Ô¨Ånd that mBERT clusters sentences of one lan-   guage together , irrespective of their content ( more   Ô¨Ågures in Appendix ) . This suggests that mBERT   embeddings strongly retain language information ,   and are not language - agnostic .   This may prove to be a signiÔ¨Åcant obstactle to-   wards progress on our task , since the noise-Ô¨Åltering   intra - bag attention may end up capturing variance   across languages more than variance in semantics .   This may also explain why mBERT+MNRE per-   forms better , since it generates embeddings of sub-   bags of each language separately , instead of a single   embedding for a multilingual bag .   5 Conclusion   We propose DiS - ReX , a novel dataset for Multi   DS - RE in 4 languages . We show that it is a more   realistic and challenging benchmark compared to   the existing dataset . DiS - ReX has a fairly well-   represented distribution of relation types , includes   instances with no - relation between entity - pairs and   the relation - types selected show several real - world   characteristics like inverse relations , different re-   lations with high overlap , etc . We also publish   Ô¨Årst baseline numbers on the task of Multi DS - RE   by extending existing state - of - the - art models . A   detailed analysis of model performance suggests   several research challenges for future : ( 1 ) learn-   ing language - agnostic sentence embeddings , ( 2 )   robustness to related relations ( inverse ; overlap-   ping but semantically different ) , and ( 3 ) handling   multi - label entity - pairs . Recently , Rathore et al .   ( 2022 ) develop a multilingual DS - RE model named   PARE , which reports improved performance on the   DiS - ReX dataset .   Acknowledgements   This work is primarily supported by a grant from   Huawei . It is also supported by grants from Google ,   Jai Gupta Chair professorship and a Visvesvaraya   faculty award by the Govt . of India to Mausam .   We thank IIT Delhi HPC facility for compute re-   sources . We thank Vipul Rathore for his useful   feedback on evaluating the quality of the dataset ,   and Vipul Rathore and Keshav Kolluru for their   helpful comments on an earlier draft of the paper .   References853854855A Appendix   B Calculation of EfÔ¨Åciency   For a dataset of size noverkclasses , where iclass hasninstances :   Efficiency =     EfÔ¨Åciency lies between 0 and 1 . A higher value suggests that the class - distribution is closer to uniform .   C Baseline architecture   C.1 BERT Encoder   To obtain a distributed representation of a sentence x , we use mBERT . In order to encode positional   information into the model we use Entity Markers scheme introduced by ( Soares et al . , 2019 ) . We add   special tokens [ E1],[nE1]to mark start and end of the head entity and [ E2],[nE2]to mark start and end   of the tail entity . This modiÔ¨Åed sentence is fed into a pretrained BERT model and the output head and tail   tokens are concatenated to get the Ô¨Ånal sentence representation ~xfor each sentence xin our bag .   C.2 Intra Bag Attention   To obtain representation of bag B , we apply selective sentence - level attention ( Lin et al . , 2016 ) . We   obtain real - valued vector ~Bfor the bag as a weighted sum of sentence representations ~x :   ~B=  ~x   where  measures attention score of ~xwith a speciÔ¨Åc relation r:-   = xrxr   This reduces the effect of noisy labels on the Ô¨Ånal bag representation .   Finally , we obtain conditional probability p(rjB; ) = softmax ( o ) . Here we obtain owhich represents   scores for all relation types .   o = R ~ B+d   Ris the matrix of relation representations . Our objective function is the cross - entropy loss and is   deÔ¨Åned as follows : -   L( ) = p(rjB; )   wherebdenotes the number of bags in our training data   C.3 MNRE and Cross - Lingual Attention   In order to extend the Intra Bag Attention to multilingual setting , ( Lin et al . , 2017 ) introduce separate   relation embeddings for each language and propose creating several representations of a bag by taking   attention of sentences in language jwith relation embedding of language k. Formally , the cross - lingual   representation Sis deÔ¨Åned as a weighted sum of those sentence vectors ~xin thejlanguage where   is the attention score of each sentence with respect to the klanguage .   S=  ~x   = xrxr   o= ( R+M)S+d   Ris the matrix of relation representations ( r ) in language k and Mis a global relation matrix   initialized randomly . Similar to ( Lin et al . , 2016 ) , probability p(rjS; ) = softmax ( o ) . To obtain score   of relationrfor bag B :   f(B;r ) = logp(rjS; )   Loss function is negative log likelihood over all bags in the dataset.856Language DiS - ReX ( PCNN+Att ) DiS - ReX ( mBERT+Att ) DiS - ReX ( mBERT+MNRE )   AUC Micro F1 AUC Micro F1 AUC Micro F1   English 0.687 0.642 0.781 0.713 0.796 0.733   French 0.714 0.662 0.814 0.746 0.822 0.760   Spanish 0.697 0.644 0.799 0.729 0.816 0.751   German 0.614 0.588 0.757 0.716 0.755 0.717   All languages 0.678 0.634 0.806 0.741 0.817 0.759   D Training details   For training we use AdamW optimizer ( Kingma and Ba , 2017 ; Loshchilov and Hutter , 2019 ) , with   lr=0.001 , betas=(0.9 , 0.999 ) , eps=1e-08 . Weight decay is 0.01 for all parameters except bias and layer   norm parameters . Hyperparameters were selected using manual tuning on the dataset . We train the   mBERT models for 5 epochs and the PCNN+Att model for 60 epochs . We follow the framework of   OpenNRE ( Han et al . , 2019 ) and select bag size = 2 for all models . For testing , we choose the weights   with best validation AUC . Correct prediction of NA class is not counted in the calculation of Micro F1   and AUC . We use a single Tesla V100 32 GB GPU for all of our experiments .   mBERT+MNRE baseline takes 8 hours for 1 epoch . mBERT+Att takes 3 hours for 1 epoch . PCNN+Att   takes 3 hours for 60 epochs .   Training , validation and testing splits for both DiS - ReX and RELX - Distant are in the ratio of 7:1:2 . We   made sure that the bags in testing set do not overlap with the bags in the training set .   E Detailed Statistics of mBERT Baselines   In Table 7 , we present results on all langauges for our three baselines on DiS - ReX. In tables 8 , 9 , we   present the distribution of errors made by the mBERT+Att and mBERT+MNRE models   In Table 10 and 11 , we present the results on bags having 1,2,3 and 4 labels in ground truth us-   ing mBERT+Att and mBERT+MNRE respectively .   In Table 12 , we present the results on all classes of the best baseline model ( mBERT+MNRE )   when run on our DiS - ReX dataset .   Language Type-1 Error ( % ) Type-2 Error ( % ) Type-3 Error ( % )   English 43.44 26.66 29.90   French 29.73 30.45 39.82   Spanish 33.82 30.61 35.57   German 15.03 39.60 45.37   Language Type-1 Error ( % ) Type-2 Error ( % ) Type-3 Error ( % )   English 44.49 31.17 24.33   French 29.69 36.14 34.15   Spanish 35.08 36.37 28.54   German 14.94 45.28 39.77857Number of relation labels Micro - F1 Precision Recall   1 0.836 0.846 0.825   2 0.662 0.912 0.520   3 0.500 0.939 0.341   4 0.449 0.846 0.305   Number of relation labels Micro - F1 Precision Recall   1 0.842 0.865 0.820   2 0.673 0.934 0.525   3 0.518 0.959 0.354   4 0.348 0.937 0.214   F Some more examples of tSNE plots for mBERT+Att   In Ô¨Ågure 2 , we provide some more example of tSNE plots for multilingual bags .   We take the following bags :   ( cincinnati , ohio ) ; ( black sabbath , tony iommi )   ( miami , Ô¨Çorida ) ; ( sumatra , indonesia )   We use sklearn implementation of tSNE and set the perplexity to be 5.858Relation Label F1 Precision Recall   predecessor 67.58 76.31 60.65   nationality 67.29 64.68 70.12   artist 76.78 74.79 78.87   region 81.43 81.14 81.73   department 95.08 95.28 94.88   successor 72.16 75.32 69.26   location 69.82 65.36 74.93   bandMember 73.45 73.45 73.45   isPartOf 66.50 59.52 75.33   hometown 73.03 70.14 76.17   previousWork 68.83 64.89 73.27   riverMouth 72.63 78.97 67.24   team 81.66 85.85 77.86   recordLabel 86.85 87.24 86.46   associatedBand 71.26 61.69 84.36   author 78.87 83.30 74.88   inÔ¨Çuenced 61.35 65.81 57.46   birthPlace 75.00 75.52 74.48   formerBandMember 57.94 59.62 56.36   leaderName 71.16 70.97 71.35   deathPlace 66.24 64.15 68.46   city 78.96 81.93 76.19   province 78.82 78.73 78.92   inÔ¨ÇuencedBy 59.29 65.26 54.32   locationCountry 62.58 64.76 60.55   related 75.94 74.35 77.59   director 83.59 79.36 88.29   capital 53.68 48.69 59.82   largestCity 65.89 71.57 61.04   NA 95.08 95.56 94.61   country 86.57 85.77 87.39   starring 86.32 86.52 86.12   subsequentWork 71.65 70.23 73.12   producer 53.30 51.20 55.58   headquarter 68.54 66.08 71.18   state 82.54 78.32 87.26   locatedInArea 72.23 70.44 74.10   All relations 70.67 - -   G Qualitative Analysis   In this section , we give some examples of randomly selected non NA instances in our dataset :   English :   ‚Ä¢Sentence : another dialect spoken in tioman island is a distinct malay variant and most closely   related to riau archipelago malay subdialect spoken in natuna and anambas islands in the south   china sea together forming a dialect continuum between the bornean malay with the mainland malay   Entities : ( tioman island , the south china sea)859Relations : http://dbpedia.org/ontology/location   ‚Ä¢Sentence : in 2017 jenny durkan was elected as the Ô¨Årst openly lesbian mayor of seattle   Entities : ( jenny durkan , seattle )   Relations : http://dbpedia.org/ontology/birthPlace   German :   ‚Ä¢Sentence : danach kamen abgeleitete klassen hinzu ein strengeres typsystem und w√§hrend stroustrup   " c with classes ‚Äù ( " c mit klassen ‚Äù ) entwickelte woraus sp√§ter c++ wurde schrieb er auch cfront einen   compiler der aus c with classes zun√§chst c - code als erzeugte   Entities : ( c , c++ )   Relations : http://dbpedia.org/ontology/inÔ¨Çuenced   ‚Ä¢Sentence : fr√ºher auch ur ist ein 96.1 km langer nebenÔ¨Çuss der sauer entlang der grenze von   deutschland zu den westlichen nachbarstaaten belgien und luxemburg   Entities : ( sauer , deutschland )   Relations : http://dbpedia.org/ontology/locatedInArea   French :   ‚Ä¢Sentence : √† la mort de boleslas v le pudique duc princeps de pologne la guerre civile en mazovie   emp√™che conrad de revendiquer le tr√¥ne de cracovie   Entities : ( boleslas v le pudique , cracovie )   Relations : http://dbpedia.org/ontology/deathPlace   ‚Ä¢Sentence : les entreprises masson masson est le dirigeant effectif des trois entreprises du groupe   cette situation se reÔ¨Ç√®te d√©sormais dans l actionnariat et les raisons sociales des soci√©t√©s qui   deviennent joseph masson sons and company ( montr√©al ) masson langevin sons and company   ( qu√©bec ) masson sons and company ( glasgow ) cette derni√®re soci√©t√© bas√©e en √©cosse a surtout   vocation de g√©rer les achats   Entities : ( joseph masson , qu√©bec )   Relations : http://dbpedia.org/ontology/birthPlace   Spanish :   ‚Ä¢Sentence : en 2003 apareci√≥ en anything else pel√≠cula de woody allen junto a christina ricci y jason   biggs adem√°s actu√≥ en la pel√≠cula para televisi√≥n l   Entities : ( anything else , jason biggs )   Relations : http://dbpedia.org/ontology/starring   ‚Ä¢Sentence : es una comuna y poblaci√≥n de francia en la regi√≥n de borgo√±a departamento de yonne en   el distrito de sens y cant√≥n de sens - ouest   Entities : ( sens , yonne )   Relations : http://dbpedia.org/ontology/department   H Additional Dataset Statistics   In Table 13 , we present the number of bags common across 2,3 and all 4 languages . In table 14 and 15 ,   we present the number of bags and sentences in each class on all 4 languages in our dataset . In Ô¨Ågure 3   we present a histogram depicting number of bags present for each relation class.860Number of languages Number of Bags   2 59709   3 9494   4 1488861Relation Label English French German Spanish All languages   NA 149874 142467 149034 148806 590181   isPartOf 2548 645 465 490 4148   state 1882 1762 3537 429 7610   largestCity 265 342 199 393 1199   birthPlace 7861 9532 3341 9484 30218   deathPlace 4377 5629 277 4709 14992   nationality 2205 4413 143 2265 9026   country 10024 9618 3065 9808 32515   capital 544 651 397 891 2483   city 1415 4257 7930 1844 15446   author 1483 1224 94 460 3261   previousWork 348 696 305 1127 2476   location 5655 1300 1180 1685 9820   riverMouth 464 880 3303 154 4801   locatedInArea 1324 785 5715 608 8432   hometown 1689 435 163 4474 6761   successor 1574 2959 74 1618 6225   inÔ¨Çuenced 820 453 61 188 1522   headquarter 1122 922 460 1895 4399   province 225 1121 1272 2405 5023   associatedBand 3669 384 107 2555 6715   subsequentWork 390 760 344 1248 2742   locationCountry 925 799 2237 361 4322   bandMember 1327 1909 300 3092 6628   director 1258 3003 1592 2089 7942   team 1329 564 461 634 2988   artist 1188 3891 1241 2670 8990   related 1439 375 117 6262 8193   producer 1381 2848 1401 3044 8674   predecessor 475 2814 81 273 3643   leaderName 353 236 270 223 1082   formerBandMember 960 1153 174 1345 3632   recordLabel 791 881 199 2107 3978   region 1529 3673 1907 2249 9358   inÔ¨ÇuencedBy 954 533 86 291 1864   starring 3040 7018 3087 4179 17324   department 99 5486 323 3157 9065   All relations 216806 226418 194942 229512 876743862Relation Label English French German Spanish All languages   NA 231271 167509 278360 224156 901296   isPartOf 16085 2794 2566 1880 23325   state 11979 13135 13705 1405 40224   largestCity 18811 4163 8949 3136 35059   birthPlace 15738 16624 4376 14359 51097   deathPlace 11498 12208 539 8888 33133   nationality 5848 9560 219 4330 19957   country 88787 43911 13148 64660 210506   capital 19887 4713 17227 5318 47145   city 4490 11156 23631 3740 43017   author 3387 4121 335 1417 9260   previousWork 6507 1276 450 2318 10551   location 15538 4757 4656 6014 30965   riverMouth 1172 2442 12467 420 16501   locatedInArea 4320 4152 18890 1904 29266   hometown 7648 796 1067 8971 18482   successor 4700 6963 128 3118 14909   inÔ¨Çuenced 2416 1147 635 394 4592   headquarter 5419 2399 2030 5736 15584   province 1082 2472 2710 11672 17936   associatedBand 7390 713 136 8437 16676   subsequentWork 6541 1318 517 2526 10902   locationCountry 3204 2836 8226 1229 15495   bandMember 3592 5910 475 8763 18740   director 2005 7811 2970 3961 16747   team 1830 814 694 1396 4734   artist 2893 9591 3156 6472 22112   related 4526 928 171 17432 23057   producer 2459 6398 2647 6384 17888   predecessor 2592 7003 162 600 10357   leaderName 1549 1074 452 448 3523   formerBandMember 2975 3452 279 4091 10797   recordLabel 1320 1214 219 4149 6902   region 5836 11860 5901 4485 28082   inÔ¨ÇuencedBy 2524 1482 913 536 5455   starring 4484 14578 4616 6676 30354   department 196 15807 693 4997 21693   All relations 532499 409087 438315 456418 1858012863   Omer Goldman , David Guriel , Reut Tsarfaty   Bar - Ilan University   { omer.goldman,davidgu1312}@gmail.com,reut.tsarfaty@biu.ac.il   Abstract   1 Introduction   In recent years , morphological ( re)inÔ¨Çection tasks   in NLP have gained a lot of attention , most notably   with the introduction of SIGMORPHON ‚Äôs shared   tasks ( Cotterell et al . , 2016 , 2017 , 2018 ; Vylomova   et al . , 2020 ) in tandem with the expansion of Uni-   Morph ( McCarthy et al . , 2020 ) , a multi - lingual   dataset of inÔ¨Çection tables . The shared - tasks sam-   ple data from UniMorph includes lists of triplets   in the form of ( lemma , features , form ) for many   languages , and the shared - task organizers maintain   standard splits for a fair system comparison .   The best - performing systems to - date in all inÔ¨Çec-   tion shared - tasks are neural sequence - to - sequence   models used in many NLP tasks . An LSTM - basedmodel won 2016 ‚Äôs task ( Kann and Sch√ºtze , 2016 ) ,   and a transformer came on top in 2020 ( Canby   et al . , 2020 ) . In 2020 ‚Äôs task the best model achieved   exact - match accuracy that transcended 0.9 macro-   averaged over up to 90 languages from various lan-   guage families and types . This trend of high results   recurred in works done on data collected indepen-   dently as well ( e.g. Malouf , 2017 , Silfverberg and   Hulden , 2018 , inter alia ) .   Interestingly , the averaged results of 2020 ‚Äôs   shared - task include languages for which very lit-   tle data was provided , sometimes as little as a   couple of hundreds of examples . This has led   to a view considering morphological inÔ¨Çection a   relatively simple task that is essentially already   solved , as reÔ¨Çected in the saturation of the results   over the year and the declining submissions to the   shared tasks . This also led the community to grav-   itate towards works attempting to solve the same   ( re)inÔ¨Çection tasks with little or no supervision   ( McCarthy et al . , 2019 ; Jin et al . , 2020 ; Goldman   and Tsarfaty , 2021 ) .   However , before moving on we should ask our-   selves whether morphological inÔ¨Çection is indeed   solved or may the good performance be attributed   to some artifacts in the data . This was shown to be   true for many NLP tasks in which slight modiÔ¨Åca-   tions of the data can result in a more challenging   dataset , e.g. , the addition of unanswerable ques-   tions to question answering benchmarks ( Rajpurkar   et al . , 2018 ) , or the addition of expert - annotated   minimal pairs to a variety of tasks ( Gardner et al . ,   2020 ) . A common modiÔ¨Åcation is re - splitting the   data such that the test set is more challenging and   closer to the intended use of the models in the   wild ( S√∏gaard et al . , 2021 ) . As the performance   on morphological inÔ¨Çection models seems to have   saturated on high scores , a similar rethinking of the   data used is warranted.864In this work we propose to construct more dif-   Ô¨Åcult datasets for morphological ( re)inÔ¨Çection by   splitting them such that the test set will include no   forms of lemmas appearing in the train set . This   splitting method will allow assessing the models   in a challenging scenario closer to their desired   function in practice , where training data usually   includes full inÔ¨Çection tables and learning to inÔ¨Çect   the uncovered lemmas is the target .   We show , by re - splitting the data from task 0 of   SIGMORPHON ‚Äôs 2020 shared - task , that the pro-   posed split reveals a greater difÔ¨Åculty of morpho-   logical inÔ¨Çection . Retesting 3 of the 4 top - ranked   systems of the shared - task on the new splits leads to   a decrease of 30 points averaged over the systems   for all 90 languages included in the shared - task .   We further show that the effect is more prominent   for low - resourced languages , where the drop can   be as large as 95 points , though high - resourced   languages may suffer from up to a 10 points drop   as well . We conclude that in order to properly as-   sess the performance of ( re)inÔ¨Çection models and   to drive the Ô¨Åeld forward , the data and related splits   should be carefully examined and improved to pro-   vide a more challenging evaluation , more reÔ¨Çective   of their real - world use .   2 ( Re)inÔ¨Çection and Memorization   InÔ¨Çection and reinÔ¨Çection are two of the most dom-   inant tasks in computational morphology . In the   inÔ¨Çection task , the input is a lemma and a feature-   bundle , and we aim to predict the respective in-   Ô¨Çected word - form . In reinÔ¨Çection , the input is an   inÔ¨Çected word - form along with its features bun-   dle , plus a feature - bundle without a form , and   we aim to predict the respective inÔ¨Çected - form   for the same lemma . The training input in SIG-   MORPHON ‚Äôs shared - tasks is a random split of the   available ( lemma , form , features ) triplets such that   no triplet occurring in the train - set occurs in the   test - set .   In such a setting , models can short - cut their way   to better predictions in cases where forms from   the same lemma appear in both the train and test   data . This may allow models to memorize lemma-   speciÔ¨Åc alternations that make morphological in-   Ô¨Çection a challenging task to begin with . Consider   for example the notoriously unpredictable German   plurality marking , where several allomorphs areassociated with nouns with no clear rule govern-   ing the process . Kind , for example , is pluralized   with the sufÔ¨Åx -erresulting in Kinder tagged as ; . Assuming a model saw this example in   the train set it is pretty easy to predict Kindern for   the same lemma with;features , but without   knowledge of the sufÔ¨Åx used to pluralize Kind the   predictions Kinden andKinds are just as likely .   3 Related Work   Many subÔ¨Åelds of NLP and machine learning in   general suggested hard splits as means to improve   the probing of models ‚Äô ability to solve the under-   lying task , and to make sure models do not simply   employ loopholes in the data .   In the realm of sentence simpliÔ¨Åcation , Narayan   et al . ( 2017 ) suggested the WS dataset ,   where models are required to split and rephrase   complex sentences associated with a meaning rep-   resentation over a knowledge - base . Aharoni and   Goldberg ( 2018 ) found that some facts appeared   in both train and test sets and provided a harder   split denying models the ability to use memorized   facts . Aharoni and Goldberg ( 2020 ) also suggested   a general splitting method for machine translation   such that the domains are as disjoint as possible .   In semantic parsing , Finegan - Dollak et al . ( 2018 )   suggested a better split for parsing natural lan-   guage questions to SQL queries by making sure   that queries of the same template do not occur in   both train and test , while Lachmy et al . ( 2021 ) split   their H data such that any one visual pat-   tern used for the task can not appear in both train   and test . Furthermore , Loula et al . ( 2018 ) adver-   sarially split semantic parsing for navigation data   to assess their models ‚Äô capability to use composi-   tionality . In spoken language understanding Arora   et al . ( 2021 ) designed a splitting method that will   account for variation in both speaker identity and   linguistic content .   In general , concerns regarding data splits and   their undesired inÔ¨Çuence on model assessments led   Gorman and Bedrick ( 2019 ) to advocate random   splitting instead of standard ones . In reaction , S√∏-   gaard et al . ( 2021 ) pointed to the Ô¨Çaws of random   splits and suggested adversarial splits to challenge   models further . Here we call for paying attention   to the splits employed in evaluating morphological   models , and improve on them.865   4 Experiments   In order to better assess the difÔ¨Åculty of morpho-   logical inÔ¨Çection , we compare the performances of   3 of the top - ranked system at task 0 ( inÔ¨Çection ) of   SIGMORPHON ‚Äôs 2020 shared - task . We examined   each system on both the the standard ( form ) split   and the novel ( lemma ) split .   When re - splitting , we kept the same proportions   of the form - split data , i.e. we split the inÔ¨Çection ta-   bles 70 % , 10 % and 20 % for the train , development   and test set . In terms of examples the proportions   may vary as not all tables are of equal size . In prac-   tice , the averaged train set size in examples terms   was only 3.5 % smaller in the lemma - split data , on   average.4.1 The Languages   SIGMORHPON ‚Äôs 2020 shared - task includes   datasets for 90 typologically and genealogically   diverse languages from 14 language families . The   languages are varied along almost any typological   dimension , from fusional to agglutinative , small   inÔ¨Çection tables to vast ones . They include mostly   preÔ¨Åxing and mostly sufÔ¨Åxing languages with rep-   resentation of inÔ¨Åxing and circumÔ¨Åxing as well .   The languages vary also in use , including widely-   used languages such as English and Hindi and mori-   bund or extinct languages like Dakota and Middle   High German .   4.2 The Models   We tested the effects of lemma - splitting on our own   LSTM - based model as well as 3 of the 4 top - ranked   systems in the shared task .   Base LSTM We implemented a character - based   sequence - to - sequence model which consists of a   1 - layer bi - directional LSTM Encoder and a 1 - layer   unidirectional LSTM Decoder with a global soft   attention layer ( Bahdanau et al . , 2014 ) . Our model   was trained for 50 epochs with no model selection .   Base trm - single The shared - task ‚Äôs organizers   supplied various baselines , some based on a trans-   former architecture that was adapted for character-   level tasks ( Wu et al . , 2021).All baseline models   include 4 encoder and 4 decoder layers , consist-   ing of a multi - head self - attention layer and 2 feed-   forward layers , equipped with a skip - connection .   In every decoder layer a multi - head attention layer   attends to the encoder ‚Äôs outputs . The network was   trained for 4,000 warm - up steps and up to 20,000   more steps , each step over a batch of size 400 . The   model was examined with and without augmented   data , trained separately on each language or each   language family . One of the baseline setups , train-   ing a model per language without augmented data ,   made it to the top 4 systems and we include it here.866DeepSpin Peters and Martins ( 2020 ) submitted a   recurrent neural network ‚Äì dubbed DeepSpin-02 .   The system is composed of 2 bi - directional LSTM   encoders with bi - linear gated Attention ( Luong   et al . , 2015 ) , one for the lemma characters and   one for the features characters , and a unidirectional   LSTM Decoder for generating the outputs . The in-   novation in the architecture is the use of sparsemax   ( Martins and Astudillo , 2016 ) instead of softmax   in the attention layer .   CULing Liu and Hulden ( 2020 ) ‚Äôs system is also   based on the transformer architecture , with hyper-   parameters very similar to base trm - single .Their   innovation is in restructuring the data such that the   model learns to inÔ¨Çect from any given cell in the   inÔ¨Çection table rather than solely from the lemma .   4.3 Results   Table 1 summarizes our main results . We clearly   see a drop in the performance for all systems , with   an average of 30 points . The table also shows that   splitting the data according to lemmas allows dis-   cerning between systems that appear to perform   quite similarly on the form - split data . The best   system on the lemma - split data , DeepSpin-02 , out-   performs the second - ranked CULing system by   about 13 points with both baseline systems per-   forming signiÔ¨Åcantly worse . The results in terms   of averaged edit distance show the same trends .   DeepSpin-02 emerges victorious also in Table   2 , where results are broken down by language fam-   ily . The table shows that DeepSpin-02 is the best   performer over all language families when data is   lemma - split , in contrast to the mixed picture over   the form - split data .   The average performance per language family   seems to be controlled by training data availability .   For example , Germanic languages show average   drop of 23 points , while for Niger - Congo languages   the drop is 39 points on average .   In order to further examine the relation between   the amount of training data and drop in perfor-   mance we plotted in Figure 1 the drop per sys-   tem and per language against the size of the avail-   able train data , color - coded to indicate systems . It   shows that the major drops in performance that   contributed the most to the overall gap between   the splits are in those low - resourced language . Re-   markably , for some systems and languages the drop   can be as high as 95 points . On the other hand , on   high - resourced languages with 40,000 training ex-   amples or more , all systems did n‚Äôt lose much . The   analysis also shows the advantage of DeepSpin-02   in the lower - resourced settings that made it the best   performer overall .   When color - coding the same broken - down data   for linguistic family membership rather than sys-   tem , as we do in Figure 2 , it becomes clear that   there is no evidence for speciÔ¨Åc families being eas-867ier for inÔ¨Çection when little data is provided . The   Ô¨Ågure does show the remarkable discrepancy in   annotation effort , as the high - resourced languages   mostly belong to 2 families : Germanic and Uralic .   5 Discussion   We proposed a method for splitting morphologi-   cal datasets such that there is no lemma overlap   between the splits . On the re - split of SIGMOR-   PHON ‚Äôs 2020 shared - task data , we showed that all   top - ranked systems suffer signiÔ¨Åcant drops in per-   formance . The new split examines models ‚Äô general-   ization abilities in conditions more similar to their   desired usage in the wild and allows better discern-   ing between the systems in order to point to more   promising directions for future research ‚Äî more   so than the original form - split data on which all   systems fared similarly . The new splitting method   is likely to lead to more sophisticated modeling ,   for instance , in the spirit of the model proposed by   Liu and Hulden ( 2021 ) . The suggested move to a   harder split is not unlike many other NLP tasks , in   which challenging splits are suggested to drive the   Ô¨Åeld forward . We thus call for morphological stud-   ies to carefully attend to the data used and expose   the actual difÔ¨Åculties in modelling morphology , in   future research and future shared tasks .   Acknowledgements   This research was funded by the European Re-   search Council under the European Union ‚Äôs Hori-   zon 2020 research and innovation programme ,   ( grant agreement No . 677352 ) and by a research   grant from the ministry of Science and Technology   ( MOST ) of the Israeli Government , for which we   are grateful .   References868869870   Xing Wu , Chaochen Gao , Meng Lin , Liangjun Zang , Songlin HuInstitute of Information Engineering , Chinese Academy of Sciences , Beijing , ChinaSchool of Cyber Security , University of Chinese Academy of Sciences , Beijing , ChinaKuaishou Technology , Beijing , China   { gaochaochen,linmeng,zangliangjun,husonglin}@iie.ac.cn   wuxing@kuaishou.com   Abstract   1 Introduction   Data augmentation is a widely used technique , es-   pecially in the low - resource regime . It increases   the size of the training data to alleviate overfit-   ting and improve the robustness of deep neural   networks . In the field of natural language process-   ing ( NLP ) , various data augmentation techniques   have been proposed . One most commonly used   method is to randomly select tokens in a sentence   and replace them with semantically similar tokens   to synthesize a new sentence ( Wei and Zou , 2019 ;   Kobayashi , 2018 ) . ( Kobayashi , 2018 ) proposes   contextual augmentation to predict the probabil-   ity distribution of replacement tokens by using the   LSTM language model and sampling the replace-   ment tokens according to the probability distribu-   tion . ( Wu et al . , 2019a , b ) uses BERT ‚Äôs ( Devlin   et al . , 2018 ) masked language modeling ( MLM )   task to extend contextual augmentation by consid-   ering deep bi - directional context . ( Kumar et al . ,   2020 ) further propose to use different types of trans-   former based pre - trained models for conditional   data augmentation in the low - resource regime .   MLM takes masked sentences as input , and typ-   ically 15 % of the original tokens in the sentences   will be replaced by the [ MASK ] token . Before   entering MLM , each token in sentences needs to   be converted to its one - hot representation , a vec-   tor of the vocabulary size with only one position   is 1 while the rest positions are 0 . MLM outputs   the probability distribution of the vocabulary size   of each mask position . Through large - scale pre-   training , it is expected that the probability distri-   bution is as close as possible to the ground - truth   one - hot representation . Compared with the one-   hot representation , the probability distribution pre-   dicted by pre - trained MLM is a ‚Äú smoothed ‚Äù repre-   sentation , which can be seen as a set of candidate   tokens with different weights . Usually , most of the   weights are distributed on contextual - compatible   tokens . Multiplying the smooth representation by   the word embedding matrix can obtain a weighted   summation of the word embeddings of the candi-   date words , termed smoothed embedding , which   is more informative and context - rich than the one-871hot ‚Äôs embedding obtained through lookup opera-   tion . Therefore , the use of smoothed representation   instead of one - hot representation as the input of   the model can be seen as an efficient weighted data   augmentation method . To get the smoothed rep-   resentation of all the tokens of the entire sentence   with only one forward process in MLM , we do not   explicitly mask the input . Instead , we turn on the   dropout of MLM and dynamically randomly dis-   card a portion of the weight and hidden state at   each layer .   An unneglectable situation is that some tokens   appear more frequently than others in similar con-   texts during pre - training , which will cause the   model to have a preference for these tokens . This is   harmful for downstream tasks such as fine - grained   sentiment classification . For example , given ‚Äú The   quality of this shirt is average . " , the ‚Äú average " to-   ken is most relevant to the label . The smoothed   representation through the MLM at the position   of ‚Äú average " is shown in Figure 2 . Although the   probability of ‚Äú average " is the highest , more proba-   bilities are concentrated on tokens conflict with the   task label , such as ‚Äú high " , ‚Äú good " or ‚Äú poor ‚Äù . Such   a smoothed representation is hardly a good aug-   mented input for the task . To solve this problem ,   ( Wu et al . , 2019a ) proposed to train label embed-   ding to constraint MLM predict label compatible   tokens . However , under the condition of low re-   sources , it is not easy to have enough label data   to provide supervision . Inspired by the practical   data augmentation method mixup ( Zhang et al . ,   2017 ) in the computer vision field , we interpolate   the smoothed representation with the original one-   hot representation . Through interpolation , we can   enlarge the probability of the original token , and   the probabilities are still mostly distributed on the   context - compatible words , as shown in the figure   2 .   We combine the two stages as text smooth-   ing : obtaining a smooth representation through   MLM and interpolating to constrain the represen-   tation more controllable . To evaluate the effect   of text smoothing , we perform experiments with   low - resource settings on three classification bench-   marks . In all experiments , text smoothing achieves   better performance than other data augmentation   methods . Further , we are pleased to find that text   smoothing can be combined with other data aug-   mentation methods to improve the tasks further . To   the best of our knowledge , this is the first method to   improve a variety of mainstream data augmentation   methods .   2 Related Work   Various NLP data augmentation techniques have   been proposed and they are mainly divided into two   categories : one is to modify raw input directly , and   the other interferes with the embedding ( Miyato   et al . , 2016 ; Zhu et al . , 2019 ) . The most commonly   used method to modify the raw input is the token   replacement : randomly select tokens in a sentence   and replace them with semantically similar tokens   to synthesize a new sentence . ( Wei and Zou , 2019 )   directly uses the synonym table WordNet(Miller ,   1998 ) for replacement . ( Kobayashi , 2018 ) proposes   contextual augmentation to predict the probabil-   ity distribution of replacement tokens with two   causal language models . ( Wu et al . , 2019a ) extends   contextual augmentation with BERT ‚Äôs masked lan-   guage modeling ( MLM ) to consider bi - directional   context . ( Gao et al . , 2019 ) softly augments a ran-   domly chosen token in a sentence by replacing   its one - hot representation with the distribution of   the vocabulary provided by the causal language   model in machine translation . Unlike ( Gao et al . ,   2019 ) , we use MLM to generate smoothed repre-   sentation , which considers the deep bi - directional   context more adequately . And our method has bet-   ter parallelism , which can efficiently obtain the   smoothed representation of the entire sentence in   one forward process . Moreover , we propose to con-   strain smoothed representation more controllable   through interpolation for classification tasks .   3 Our Method   3.1 Smoothed Representation   We use BERT as a representative example of   MLM . Given a downstream task dataset , namely   D={t , p , s , l } , where Nis the number of872   instances , tis the one - hot encoding of a text ( a   single sentence or a sentence pair ) , pis the posi-   tional encoding of t , sis the segment encoding   oftandlis the label of this instance . We feed   the one - hot encoding t , positional encoding pas   well as the segment encoding sinto BERT , and   fetch the output of the last layer of the transformer   encoder in BERT , which is denoted as :   ‚àí ‚Üít = BERT ( t ) ( 1 )   where‚àí ‚Üít‚àà Ris a 2D dense vector   in shape of [ sequence_len , embedding_size ] . We   then multiply‚àí ‚Üítwith the word embedding ma-   trixW‚àà Rin BERT , to get the   MLM prediction results , which is defined as :   MLM ( t ) = softmax ( ‚àí ‚ÜítW ) ( 2 )   where each row in MLM ( t)is a probability distri-   bution over the token vocabulary , representing the   context - compatible token choices in that position   of the input text learned by pre - trained BERT .   3.2 Mixup Strategy   The mixup ( Zhang et al . , 2017 ) is defined as :   Àúx = Œªx+ ( 1‚àíŒª)x ( 3 )   Àúy = Œªy+ ( 1‚àíŒª)y ( 4 )   where ( x , y)and(x , y)are two feature - target   vectors drawn at random from the training data , and   Œª‚àà[0,1 ] . In text smoothing , the one - hot repre-   sentation and smoothed representation are derived   from the same raw input , their lables are identical   and the interpolation operation will not change the   label . So the mixup operation can be simplified to :   et = Œª¬∑t+ ( 1‚àíŒª)¬∑MLM ( t ) ( 5 )   where tis the one - hot representation , MLM ( t)is   the smoothed representation , etis the interpolated   representation and Œªis the balance hyperparameter   to control interpolation strength . In the downstream   tasks , we use interpolated representation instead of   the original one - hot representation as input .   4 Experiment   4.1 Baseline Approaches   EDA ( Wei and Zou , 2019 ) consists of four simple   operations : synonym replacement , random inser-   tion , random swap , and random deletion .   Back Translation ( Shleifer , 2019 ) translate a sen-   tence to a temporary language ( EN - DE ) and then   translate back the previously translated text into the   source language ( DE - EN ) .   CBERT ( Wu et al . , 2019a ) masks some tokens   and predicts their contextual substitutions with pre-   trained BERT .   BERTexpand , BERTprepend ( Kumar et al . ,   2020 ) conditions BERT by prepending class labels   to all examples of given class . ‚Äú expand " a the label   to model vocabulary , while ‚Äú prepend " without .   GPT2context ( Kumar et al . , 2020 ) provides a   prompt to the pre - trained GPT model and keep-   ing generating until the EOS token .   BARTword , BARTspan ( Kumar et al . , 2020 ) con-   ditions BART by prepending class labels to all ex-   amples of given class . BARTword masks a single   word while BARTspan masks a continuous chunk .   4.2 Experiment Setting   Our experiment strictly follows the settings in the   ( Kumar et al . , 2020 ) paper on three text classifica-   tion datasets downloaded from the links .   SST-2 ( Socher et al . , 2013 ) is a movie reviews sen-   timent classification task with two labels .   SNIPS ( Coucke et al . , 2018 ) is a task of over   16,000 crowd - sourced queries distributed among 7   user intents of various complexity.873   TREC ( Li and Roth , 2002 ) contains six question   types collected from 4,500 English questions .   We randomly subsample 10 examples per class   for each experiment for both training and develop-   ment set to simulate a low - resource regime . Data   statistics of the three datasets are shown in Table   1 . Following ( Kumar et al . , 2020 ) , we replace nu-   meric class labels with their text versions .   We first compare the effects of text smoothing   and baselines data augmentation methods on dif-   ferent datasets in a low - resource regime . Then we   further explore the effect of combining text smooth-   ing with each baseline method . Considering that   the amount of data increases to 2 times after com-   bination , we expand the data used in the baseline   experiments to the same amount for the fairness of   comparison . All experiments are repeated 15 times   to account for stochasticity and results are reported   as Mean ( STD ) accuracy on the full test set .   4.3 Experimental Results   As shown in Table2 , text smoothing brings the   largest improvement to the model on the three   datasets compared with other data augmenta-   tion methods . The previously best method is   BARTspan , which is exceeded by Text smoothing   with 1.17 % in average . Moreover , we are pleased to find that text   smoothing can be well combined with various data   augmentation methods , further improving the base-   line data augmentation methods . As shown in Ta-   ble3 , text smoothing can bring significant improve-   ments of 5.98 % , 2.79 % , 2.39 % , 2.92 % , 2.17 % ,   6.48 % , 3.21 % , 3.03 % to EDA , BackTrans , CBERT ,   BERTexpand , BERTprepend , GPT2context , BART-   word , and BARTspan , respectively . To the best of   our knowledge , this is the first method to improve a   variety of mainstream data augmentation methods .   5 Conclusoins   This article proposes text smoothing , an effective   data augmentation method , by converting sentences   from their one - hot representations to smoothing   representations . In the case of a low data regime ,   text smoothing is significantly better than various   data augmentation methods . Furthermore , text   smoothing can further be combined with various   data augmentation methods to obtain better perfor-   mance .   References874875   . , Mausam , 340 , 849   Abdul - Mageed , Muhammad , 86   Agarwal , Shivam , 620   Ahuja , Sanchit , 620   Alajrami , Ahmed , 131   Aletras , Nikolaos , 131 , 825   Arnold , Andrew , 203   Arora , Aryaman , 175   Badola , Kartikeya , 340 , 849   Baek , Jinheon , 442   Bai , Long , 290   Banerjee , Pratyay , 355   Bao , Jianzhu , 29   Baral , Chitta , 355   Barnes , Jeremy , 470   Barrault , Loic , 825   Bastani , Osbert , 113   Ben Zaken , Elad , 1   Bhartiya , Abhyuday , 849   Bhatia , Parminder , 203   Bhattamishra , Satwik , 424   Blanco , Eduardo , 716   Blunsom , Phil , 424   Buntine , Wray , 694   Byrd , Matthew Alexander , 119   Callison - Burch , Chris , 837   Cangelosi , Angelo , 453   Cao , Yang Trista , 561   Cao , Yunbo , 333   Card , Dallas , 401   Cerisara , Christophe , 732   Chang , Baobao , 333 , 530 , 665   Chang , Kai - Wei , 561   Chava , Sudheer , 620   Chen , Jianshu , 212 , 371   Chen , Liang , 333 , 665   Chen , Wei - Te , 227   Cheng , Xueqi , 290   Chinnappa , Dhivya , 716   Cho , Sukmin , 442   Choubey , Prafulla Kumar , 255   Chuang , Chengyu , 100   Cieliebak , Mark , 750   Clark , James J. , 219   Clark , Thomas Hikaru , 20Coenen , Andy , 837   Corona , Rodolfo , 54   Cotterell , Ryan D , 20 , 36 , 175   Crabb√© , Benoit , 501   Dai , Damai , 333   Dai , Xinyu , 672   Darrell , Trevor , 54   Dawkins , Hillary , 538   De Lhoneux , Miryam , 578   de Vries , Harm , 148   Deleu , Johannes , 778   Demeester , Thomas , 778   Deriu , Jan Milan , 750   Desarkar , Maunendra Sankar , 318   Develder , Chris , 778   Dey , Suvodip , 318   Dhamala , Jwala , 561   Ding , Meizhen , 435   Du , Zhengxiao , 61   Dupuy , Christophe , 552   Edalati , Ali , 219   Ern≈°treits , Valts , 508   Ethayarajh , Kawin , 401   Fang , Meng , 515   Fishel , Mark , 508   Flek , Lucie , 606   Fu , Yicheng , 61   Futrell , Richard , 636   Galata , Aphrodite , 453   Galstyan , Aram , 561   Gao , Chang , 599   Gao , Chaochen , 871   Gardner , Matt , 272   Garg , Vaibhav , 106   Gaur , Manas , 628   Gildea , Daniel , 571 , 710   Gokhale , Tejas , 355   Goldberg , Yoav , 1   Goldman , Omer , 196 , 864   Gormley , Matthew R. , 69   Goyal , Navin , 424   Guan , Saiping , 290   Guo , Hui , 106   Guo , Jiafeng , 290   876Guo , Wenbo , 10   Gupta , Rahul , 552 , 561   Guriel , David , 196 , 864   Habernal , Ivan , 771   Haf , Reza , 515   Haque , Amanul , 106   He , Andre Wang , 797   He , Keqing , 46   He , Shenghuan , 785   He , Shizhu , 283   He , Yaohan , 283   Higashinaka , Ryuichiro , 464   Hofmann , Valentin , 385   Hong , Yu , 613   Hossain , Md Mosharaf , 716   Hsu , Chan - Jan , 479   Hu , Songlin , 871   Huang , Fei , 530   Huang , Minghui , 435   Huang , Minlie , 762   Huang , Ruihong , 255   Huang , Songfang , 530 , 808   Huang , Ting - Hao , 10   Huang , Yongfeng , 680   Hulden , Mans , 166 , 739   Hwang , EunJeong , 235   Hwang , Sung Ju , 442   Ippolito , Daphne , 837   Jawahar , Ganesh , 86   Jeon , Donghyeon , 310   Jeong , Soyeong , 442   Ji , Kaixuan , 61   Jiang , Huixing , 46   Jiang , Jing , 362   Jiang , Yiwei , 778   Jin , Di , 606   Jin , Lisa , 710   Jin , Xiaolong , 290   Jurafsky , Dan , 401   Kang , Inho , 310   Kang , Pilsung , 297   Ke , Pei , 762   Kim , Misuk , 297   Kim , Seonhoon , 310   Kim , Takyoung , 297   Klein , Dan , 54 , 797   Klein , Tassilo , 394Kumar , Varun , 561   Kummara , Ramamohan , 318   Kurtz , Robin , 470   Lai , Huiyuan , 262   Lakshmanan , Laks V . S. , 86   Lam , Tsz Kin , 245   Lam , Wai , 599   Langlais , Philippe , 732   Lapalme , Guy , 732   Laverghetta Jr. , Antonio , 545   Le Berre , Guillaume , 732   Lee , Hung - yi , 479   Lee , Jay - Yoon , 235   Lee , Yukyung , 297   Levy , Roger P. , 20   Li , Bingzhi , 501   Li , Jinlong , 283   Li , Liangyou , 644   Li , Peng , 523   Li , Tingxuan , 464   Li , Wei , 290   Li , Xiaoguang , 435   Li , Zheng , 203   Li , Zixuan , 290   Licato , John , 545   Lin , Meng , 871   Lin , Ruixi , 94   Lin , Yankai , 523   Liu , Kang , 283   Liu , Ling , 166 , 739   Liu , Qun , 644   Liu , Shuang , 435   Liu , Siyang , 762   Liu , Tianyu , 333   Liu , Xiao , 61   Liu , Zhiyuan , 523   Luo , Fuli , 530   Luo , Yiran , 355   Lyu , Yajuan , 290   Madureira , Brielen , 651   Magar , Inbal , 157   Mahowald , Kyle , 636   Margatina , Katerina , 825   McCallum , Andrew , 235   Meister , Clara Isabel , 20 , 36 , 175   Mell , Stephen , 113   Mitsuda , Koh , 464   Moniz , Joel Ruben Antony , 69   Moramarco , Francesco , 588   877Mou , Yutao , 46   Na , Seung - Hoon , 310   Nabi , Moin , 394   Nallapati , Ramesh , 203   Neerkaje , Atula Tejaswi , 628   Nejadgholi , Isar , 538   Ng , Hwee Tou , 94   Nia , Vahid Partovi , 219   Nissim , Malvina , 262   Oepen , Stephan , 470   √òvrelid , Lilja , 470   Pan , Shirui , 515   Pan , Xiaoman , 371   Pandit , Shrey , 606   Papadimitriou , Isabel , 636   Papadopoulos KorÔ¨Åatis , Alex , 588   Parikh , Rahil , 552   Paritosh , Praveen , 378   Park , Eunhwan , 310   Park , Hyunji Hayley , 702   Park , Jong C. , 442   Patel , Arkil , 424   Patel , Dhruvesh , 235   Patra , Barun , 69   Peng , Weihua , 290   Pierrehumbert , Janet B. , 385   Pilehvar , Mohammad Taher , 325   Pimentel , Tiago , 20 , 36   Pruksachatkun , Yada , 561   Qi , Jianwei , 785   Qi , Tao , 680   Rashid , Ahmad , 219   Rathore , Vipul Kumar , 340   Ravfogel , Shauli , 1   Reddy , Siva , 148   Reif , Emily , 837   Rezaee , Kiamehr , 325   Rezagholizadeh , Mehdi , 219   Riezler , Stefan , 245   Rikters , Mat ¬Øƒ±ss , 508   Romeo , Marta , 453   Roth , Dan , 203   Ryu , Dongwon Kelvin , 515   Sabour , Sahand , 762   Samuel , David , 470Sarac , Radmila , 588   Savkov , Aleksandar , 588   Sawhney , Ramit , 606 , 620 , 628   Schamoni , Shigehiko , 245   Schick , Timo , 815   Schlangen , David , 651   Schlegel , Viktor , 453   Schucher , Nathan , 148   Schuetze , Hinrich , 385 , 815   Schwartz , Lane , 724   Schwartz , Roy , 157   Senel , L√ºtÔ¨Å Kerem , 815   Sennrich , Rico , 490   Seppi , Kevin , 272   Shah , Kashif , 702   Shareghi , Ehsan , 515 , 694   Shen , Hua , 10   Shinzato , Keiji , 227   Singh , Munindar P. , 106   Singla , Parag , 340   Song , Kaiqiang , 212   Soun , Ritesh Singh , 606 , 620   Srivastava , Shashank , 119   Su , Xi‚Äôao , 672   Sui , Zhifang , 333   Sun , Jingyi , 29   Sun , Maosong , 523   Sun , Qianru , 362   Sun , Runxin , 283   Sun , Xin , 686   S√∏gaard , Anders , 578   Tabasi , Mohsen , 325   Tahaei , Marzieh S. , 219   Tam , Weng Lam , 61   Tan , Chuanqi , 808   Tan , Ming , 203   Tang , Jie , 61   Tavella , Federico , 453   Thakkar , Megh , 606   Tolkachev , George , 113   Tomingas , Marili , 508   Tomlin , Nicholas , 797   Toral , Antonio , 262   Tsao , Yu , 479   Tsarfaty , Reut , 196 , 864   Tuggener , Don , 750   Tuisk , Tuuli , 508   Vamvas , Jannis , 490   Velldal , Erik , 470   878V on D√§niken , Pius , 750   Vyas , Yogarshi , 702   Wang , Dong , 435   Wang , Houfeng , 686   Wang , Peiyi , 333   Wang , Ran , 672   Wang , Zijian , 203   Wei , Jason , 837   Wei , Tianwen , 785   Weller , Orion , 272   Wiher , Gian , 36   Wisniewski , Guillaume , 501   Wong , Ka , 378   Wu , Chuhan , 680   Wu , Fangzhao , 680   Wu , Tongshuang , 10   Wu , Wei , 46   Wu , Xing , 871   Wu , Yanan , 46   Xia , Yandi , 227   Xiang , Bing , 203   Xu , Hong , 46   Xu , Minhan , 613   Xu , Ruifeng , 29   Xu , Runxin , 530 , 665   Xu , Weiran , 46   Xu , Yunqiu , 515   Yang , Diyi , 606   Yang , Tianyi , 235   Yang , Yezhou , 355   Yang , Yi , 100   Yang , Zhilin , 61Yao , Wenlin , 212 , 371   Ye , Deming , 523   Yoon , Hoonsang , 297   Yoshida , Sen , 464   Yoshinaga , Naoki , 227   Yu , Chen , 571   Yu , Dian , 212 , 371   Yu , Dong , 212 , 371   Yu , Sicheng , 362   Yuan , Ann , 837   Yuan , Zheng , 808   Yue , Xiang , 371   Zang , Liangjun , 871   Zaporojets , Klim , 778   Zdancewic , Stephan , 113   Zeng , Zhiyuan , 46   Zhang , Bohan , 255   Zhang , Dongxu , 235   Zhang , Hao , 362   Zhang , Lan , 694   Zhang , Meng , 644   Zhang , Sheng , 578   Zhang , Wenxuan , 599   Zhao , Chao , 212   Zhao , Jun , 283   Zheng , Yinhe , 762   Zhou , Kaitlyn , 401   Zhu , Chong , 283   Zhu , Qinglin , 29   Zhu , Shizhan , 54   Zhu , Xiaoyan , 762   Zhu , Yong , 290   879