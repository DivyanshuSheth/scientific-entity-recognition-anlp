  Kelly Marchisio   Johns Hopkins University   kmarc@jhu.eduMarkus Freitag David Grangier   Google Research   { freitag , grangier}@google.com   Abstract   Modern unsupervised machine translation   ( MT ) systems reach reasonable translation qual-   ity under clean and controlled data conditions .   As the performance gap between supervised   and unsupervised MT narrows , it is interesting   to ask whether the different training methods   result in systematically different output beyond   what is visible via quality metrics like adequacy   or BLEU . We compare translations from super-   vised and unsupervised MT systems of simi-   lar quality , finding that unsupervised output is   more fluent and more structurally different in   comparison to human translation than is super-   vised MT . We then demonstrate a way to com-   bine the benefits of both methods into a sin-   gle system which results in improved adequacy   and fluency as rated by human evaluators . Our   results open the door to interesting discussions   about how supervised and unsupervised MT   might be different yet mutually - beneficial .   1 Introduction   Supervised machine translation ( MT ) utilizes paral-   lel bitext to learn to translate . Ideally , this data con-   sists of natural texts and their human translations .   In a way , the goal of supervised MT training is to   produce a machine that mimicks human translators   in their craft . Unsupervised MT , on the other hand ,   uses monolingual data alone to learn to translate .   Critically , unsupervised MT never sees an exam-   ple of human translation , and therefore must create   its own style of translation . Unlike supervised MT   where one side of each training sentence pair must   be a translation , unsupervised MT can be trained   with natural text alone .   In this work , we investigate the output of su-   pervised and unsupervised MT systems of similar   quality to assess whether systematic differences in   translation exist . Our exploration of this research   area focuses on English →German for which abun-   dant bilingual training examples exist , allowing usto train high - quality systems with both supervised   and unsupervised training .   Our main contributions are :   •We observe systematic differences between   the output of supervised and unsupervised MT   systems of similar quality . High - quality un-   supervised output appears more natural , and   more structurally diverse when compared to   human translation .   •We show a way to incorporate unsupervised   back - translation into a standard supervised   MT system , improving adequacy , naturalness ,   and fluency as measured by human evaluation .   Our results provoke interesting questions about   what unsupervised methods might contribute be-   yond the traditional context of low - resource lan-   guages which lack bilingual training data , and sug-   gest that unsupervised MT might have contribu-   tions to make for high - resource scenarios as well .   It is worth exploring how combining supervised   and unsupervised setups might contribute to a sys-   tem better than either creates alone .   We discuss related work in § 2 . In § 3 , we in-   troduce the dataset , model details , and evaluation   setups . In § 4 , we characterize the differences be-   tween the output of unsupervised and supervised   neural MT systems of similar quality . In § 5 , we   demonstrate a combined system which benefits   from the complementary strengths of the two meth-   ods . We summarize the paper in § 6 .   2 Related Work   Unsupervised MT Two paradigms for unsuper-   vised MT are finding a linear transformation to   align two monolingual embedding spaces ( Lample   et al . , 2018a , b ; Conneau et al . , 2018 ; Artetxe et al . ,   2018 , 2019 ) , and pretraining a bi-/multilingual   language model then finetuning on a translation   task ( Conneau and Lample , 2019 ; Song et al . , 2019 ;   Liu et al . , 2020 ) . We study the Masked Sequence-2214to - Sequence Pretraining ( MASS ) language model   pretraining paradigm of Song et al . ( 2019 ) . MASS   is an encoder - decoder trained jointly with a masked   language modeling objective on monolingual data .   Iterative back - translation ( BT ) follows pretraining .   Monolingual Data in MT BT is widely - used to   exploit monolingual data ( Sennrich et al . , 2016 ) .   “ Semi - supervised ” systems use monolingual and   parallel data to improve performance ( e.g. Artetxe   et al . ( 2018 ) ) . Siddhant et al . ( 2020 ) combine mul-   tilingual supervised training with MASS for many   languages and zero - shot translation .   Source Artifacts in Translated Text Because su-   pervised MT is trained ideally on human - generated   translation , characteristics of human translation af-   fects the style of machine translation from such   systems . Dubbed “ translationese , ” human transla-   tion includes source language artifacts ( Koppel and   Ordan , 2011 ) and source - independent artifacts —   Translation Universals ( Mauranen and Kujamäki ,   2004 ) . There are systematic biases inherent to   translated texts ( Baker , 1993 ; Selinker , 1972 ) ,   and biases coming from interference from source   text ( Toury , 1995 ) . In MT , Freitag et al . ( 2019 ,   2020 ) attribute these patterns as a source of mis-   match between BLEU ( Papineni et al . , 2002 ) and   human evaluation measures of quality , raising con-   cerns that overlap - based metrics reward hypotheses   with the characteristics of translated text more than   those with natural language . Vanmassenhove et al .   ( 2019 , 2021 ) note loss of linguistic diversity and   richness from MT , and Toral ( 2019 ) see related ef-   fects even after human post - editing . The impact of   translated text on human evaluation has also been   studied ( Toral et al . , 2018 ; Zhang and Toral , 2019 ;   Graham et al . , 2019 ; Fomicheva and Specia , 2016 ;   Ma et al . , 2017 ) , as has the impact in training data   ( Kurokawa et al . , 2009 ; Lembersky et al . , 2012 ;   Bogoychev and Sennrich , 2019 ; Riley et al . , 2020 ) .   Measuring Word Reordering Word reordering   models are well - studied because they formed a crit-   ical part of statistical MT ( see Bisazza and Federico   ( 2016 ) for a review ) . Others examined metrics   for measuring reordering in translation ( e.g. Birch   et al . , 2008 , 2009 , 2010 ) . Wellington et al . ( 2006 )   and Fox ( 2002 ) use part - of - speech ( POS ) tags in   the context of parse trees , and Fox ( 2002 ) measure   the similarity of French and English with respect   to phrasal cohesion by calculating alignment cross-   ings using parse trees . Most similar to us , Birch(2011 ) view simplified word alignments as permu-   tations and compare distance metrics over these to   quantify the amount of reordering done . They use   TER computed over the alignments as a baseline .   Birch and Osborne ( 2011 ) ’s LRScore interpolates a   reordering metric with a lexical translation metric .   3 Experimental Setup   3.1 Data   Training Experiments are in English →German .   For the main study comparing supervised and un-   supervised MT , we use News Commentary v14   ( 329,000 sentences ) as parallel bitext for the super-   vised system , and News Crawl 2007 - 17 as mono-   lingual data for the unsupervised system . Dedu-   plicated News Crawl 2007 - 17 has 165 million En-   glish sentences and 226 million German sentences .   The combined system demonstration at the end   of our work utilizes a BT selection method . We use   the bilingual training data from WMT2018 ( Bojar   et al . , 2018 ) ( News Commentary v13 , Europarl v7 ,   Common Crawl , EU Press Release ) so that our   model can be compared with well - known work   using BT ( e.g. Edunov et al . , 2018 ; Caswell et al . ,   2019 ) . We deduplicate and filter out pairs with   > 250tokens in either language or length ratio   over 1.5 , resulting in 5.2 million paired sentences .   Development and Test Sets For the main ex-   periments , we use newstest2017 as the dev set   with newstest2018 and newstest2019 for test . new-   stest2018 was originally created by translating one   half of the test data from English →German ( orig-   en ) and the other half from German →English ( orig-   de ) . Since 2019 , WMT produces newstest sets with   only source - original text and human translations   on the target side to mitigate known issues when   translating and evaluating on target - original data   ( e.g. Koppel and Ordan , 2011 ; Freitag et al . , 2019 ) .   For most experiments , we evaluate on orig - en   sentences only to reflect the real use - case for trans-   lation and modern evaluation practice . We exam-   ine orig - de only for BLEU score as an additional   data point of difference between supervised and un-   supervised MT . Zhang and Toral ( 2019 ) show that   target - language - original text should not be used for   human evaluation ( orig - de , in our case ) .   We use the newstest2018 “ paraphrased ” test ref-   erences from Freitag et al . ( 2020),which are made2215for orig - en sentences only . These additional refer-   ences have different structure than the source sen-   tence but maintain semantics , and provide a way   to measure system quality without favoring trans-   lations with the same structure as the source . Ob-   serving work that uses these references , BLEU is   typically much lower than on original test sets , and   score differences tend to be small but reflect tangi-   ble quality difference ( Freitag et al . , 2020 ) .   For the system combination demonstration ,   we use newstest2018 for development and new-   stest2019 for test . We also use newstest2019   German →English and swap source and target to   make an orig - de English →German test set , and use   paraphrase references for newtest2019 ( orig - en ) .   Testing on the official newstest2018 in the main   experiments allows us to see interesting differences   between unsupervised and supervised MT that are   hidden with newstest2019 because it is orig - en only .   Using newstest2018 for development in the system   combination demonstration aligns with similar liter-   ature ( e.g. Edunov et al . , 2018 ; Caswell et al . , 2019 ) .   We use SacreBLEU throughout ( Post , 2018 ) .   3.2 Part - of - Speech Tagging   We use part - of - speech taggers for some experi-   ments : universal dependencies ( UD ) implemented   in spaCyand spaCy ’s language - specific fine-   grained POS tags for German from the TIGER Cor-   pus ( Albert et al . , 2003 ; Brants et al . , 2004 ) .   3.3 Models   Ourunsupervised MT model is a MASS trans-   former with the hyperparameters of Song et al .   ( 2019 ) . We train MASS on the News Crawl cor-   pora , hereafter called “ Unsup . ” Our supervised   MT systems use the transformer - big ( Vaswani   et al . , 2017 ) as implemented in Lingvo ( Shen et al . ,   2019 ) with a vocabulary of 32k subword units .   To investigate differences between approaches ,   we train two language models ( LMs ) on differ-   ent types of data and calculate the perplexity of   translations generated by the supervised and un-   supervised MT systems . We train one LM on the   monolingual German News Crawl dataset with a   decoder - only transformer , hereafter called the “ nat-   ural text LM ” ( nLM ) . We train another on machine   translated sentences which we call the “ translated   text LM ” ( tLM ) . We generate the training corpusby translating the English News Crawl dataset into   German with a English →German transformer - big   model trained on the WMT18 bitext .   3.4 Human Evaluations   Human evaluation complements automatic evalua-   tion and abstracts away from comparison to a hu-   man reference which favors the characteristics of   translated text ( Freitag et al . , 2020 ) . We score ad-   equacy using direct assessment and run side - by-   side evaluations measuring fluency and adequacy   preference between systems . Each campaign has   1,000 test items . For side - by - side eval , a test item   includes a pair of translations of the same source   sentence : one from the supervised system and one   from the unsupervised . We hire 12 professional   translators , who are more reliable than crowd work-   ers ( Toral , 2020 ; Freitag et al . , 2021 ) .   Direct Assessment Adequacy We use the tem-   plate from the WMT 2019 evaluation campaign .   Human translators assess a translation by how ade-   quately it expresses the meaning of the source sen-   tence on a 0 - 100 scale . Unlike WMT , we report the   average rating and do not normalize the scores .   Side - by - side Adequacy Raters see a source sen-   tence with two translations ( one supervised , one   unsupervised ) and rate each on a 6 - point scale .   Side - by - side Fluency Raters assess the alterna-   tive translations ( one supervised , one unsupervised )   without the source , and rate each on a 6 - point scale .   4 Unsupervised vs. Supervised MT   The goal of this section is to analyse supervised and   unsupervised systems of similar overall translation   quality so that differences in quality do not con-   found analyses . As unsupervised systems underper-   form supervised systems , we use a smaller parallel   corpus ( news commentary ) to train systems of sim-   ilar quality . Table 1 summarizes the BLEU scores   and human side - by - side adequacy results for both   systems . Although the supervised system is below   state - of - the - art , these experiments help elucidate   how unsupervised and supervised output is differ-   ent . Overall BLEU and human ratings suggest simi-   lar translation quality . Nevertheless , we observe no-   table differences between orig - de and orig - en sides   of the test set when comparing both systems . Re-   call that orig - de has natural German text on the tar-   get side . Unsup scores higher than Sup on orig - de ,   suggesting that its output is more natural - sounding2216as it better matches text originally written in Ger-   man . Performance discrepancies on orig - en and   orig - de indicate that differences in system output   may exist and prompt further investigation .   4.1 Selecting Translations of Same Adequacy   To assess the translation style and compare linguis-   tic aspects of supervised and unsupervised MT ,   we further must compare translations that have   the same accuracy level on the segment level , so   that neither confounds analysis . We use the ade-   quacy evaluation from Table 1 and retain sentences   for which both approaches yield similar adequacy   scores . We divide the rating scale into bins of low   ( 0–2 ) , medium ( 3–4 ) , and high ( 5–6 ) adequacy . Ta-   ble 2 shows the percentage of sentences in each bin .   For each source sentence , there is one translation by   Unsup and one by Sup . If human judges assert that   both translations belong in the same adequacy bin ,   that sentence also appears in “ Both . ” There are 86 ,   255 , and 218 sentences in “ Both ” for low , medium ,   and high bins , respectively . For subsequent analy-   ses , we examine sentences falling into “ Both . ”   4.2 Comparing Translation Style   Measuring Structural Similarity We develop a   metric to ascertain the degree of structural similar-   ity between two sentences , regardless of language .   When evaluated on a source - translation pair , it mea-   sures the influence of the source structure on the   structure of the output without penalizing for dif-   fering word choice ; thus it is a measure of “ mono-   tonicity ” – the degree to which words are translated   in - order . Given alternative translations in the same   language , it assesses the degree of structural simi-   larity between the two . Thus given a machine trans-   lation and a human translation of the same source   sentence , it can measure the structural similarity   between the machine and human translations .   Word alignment seems well - suited here . Like   Birch ( 2011 ) , we calculate Kendall ’s tau ( Kendall,1938 ) over alignments of source - translation pairs ,   but do not simplify alignments to permutations . We   use fast_align ( Dyer et al . , 2013 ) but observe that   it struggles to align words not on the diagonal , so   sometimes skipped alignments . Because of this   issue , we instead estimate monotonicity / structural   similarity using the new metric , introduced next .   We propose measuring translation edit rate ( TER ,   Snover et al . ( 2006 ) ) over POS tag sequences . TER   is a well - known word - level translation quality met-   ric which measures the number of edits required   to transform a “ hypothesis ” sentence into the ref-   erence , outputting a “ rate ” by normalizing by sen-   tence length . Between languages , we compute   TER between POS tag sequences of the source   text ( considered the reference ) and the translation   ( considered the hypothesis ) , so that TER now mea-   sures changes in structure independent of word   choice . Source - target POS sequences which can   be mapped onto each other with few edits are con-   sidered similar — a sign of a monotonic translation .   Given a machine translation ( hypothesis ) and a hu-   man reference in the same language , TER over   POS tags measures structural similarity between   the machine and human translations . Outputs with   identical POS patterns score 0 , increasing to 1 + as   sequences diverge . Lower TER for ( source , transla-   tion ) pairs indicates monotonic translation ; Lower   TER for ( machine translation , human translation )   pairs indicates structural similarity to human trans-   lation . We call the metric “ posTER ” .   Monotonicity POS sequences are comparable   across languages thanks to universal POS tags . Ta-   ble 3 has a toy example with two possible German   translations of an English source . Next to each sen-   tence is its universal dependencies POS sequence .   In the third column , TER is calculated with the   POS sequence of the English source as reference   and the sequence of the translation as hypothesis .   Table 4 shows posTER over universal depen-   dencies of German translations versus the new-   stest2018 ( orig - en ) source sentences . While   the standard newstest2018 references ( Ref ) score   0.410 , newstest2018p ’s ( RefP ) higher score of   0.546 reflects the fact that the paraphrase references   are designed to have different structure than the   source . Difference in overall monotonicity between   Sup and Unsup is unapparent at this granularity .   Because universal dependencies are designed to2217   suit many languages , the 17 UD categories may be   too broad to adequately distinguish moderate struc-   tural difference . Whereas UD has a single class for   “ VERB , ” the finer - grained German TIGER tags dis-   tinguish between 8 sub - verb types including infini-   tive , modal , and imperative . We use these language-   specific categories next to uncover differences be-   tween systems that broad categories conceal .   Similarity to Human Translation Recall that su-   pervised MT essentially mimics human translators ,   while unsupervised MT learns to translate without   examples . Intuitively , supervised MT output might   be stylistically more like human translation , even   when controlling for quality . The first indication is   Sup ’s lower BLEU score on nt18p — the paraphrase   test set designed to have structure different than the   original human translation .   We compare the structure of MT output with   the human reference using German TIGER tags .   Lower posTER indicates more structural similar-   ity , while higher posTER indicates stylistic devia-   tion from human translation . Comparison with the   newstest2018 orig - en human reference is in Table   5 . Sup and Unsup show negligible difference over-   all , but binning by adequacy shows Unsup output   asless structurally similar to the human reference   on the high - end of adequacy , and more similar on   the low - end . This suggests systematic difference   between systems , and that unsupervised MT might   have more structural diversity as quality improves . Naturalness The first hint that Unsup might pro-   duce more natural output than Sup is its markedly   higher BLEU on the orig - de test set : 27.1 , versus   21.1 from Sup . Recall that orig - de has natural Ger-   man on the target side , so higher BLEU here means   higher n - gram overlap with natural German .   Edunov et al . ( 2020 ) recommend augmenting   BLEU - based evaluation with perplexity from a lan-   guage model ( LM ) to assess fluency or natural-   ness of MT output . Perplexity ( Jelinek et al . , 1977 )   measures similarity of a text sample to a model ’s   training data . We contrast the likelihood of output   according to two LMs : one trained on machine-   translated text ( tLM ) and another trained on non-   translated natural text ( nLM ) . While machine-   translated and human - translated text differ , the   LMs are nonetheless a valuable heuristic and con-   tribute insights on whether systematic differences   between MT system outputs exist . Low perplex-   ity from the nLM indicates natural language . Low   perplexity from the tLM ( trained on English News   Crawl that has been machine - translated into Ger-   man ) shows proximity to training data composed   of translated text , indicating simplified language .   Sup perplexity is lower than Unsup across ade-   quacy bins for the tLM , seen in Table 6 . Conversely ,   Sup generally has higher perplexity from the nLM .   All adequacy levels for Unsup have similar nLM   perplexity , suggesting it is particularly skilled at   generating fluent output . Together , these findings   suggest that unsupervised MT output is more natu-   ralthan supervised MT output .   Stronger Supervised MT Though analyzing sys-   tems of similar quality is important for head - to-   head comparison , we evaluate a stronger super-   vised system for context . We do not have human   evaluation scores , but automatic results give in-   sight : see Table 7 . The model has overall BLEU =   40.9 and a similarly large discrepancy on orig - en vs.   orig - de as did the Sup system used throughout this2218   work : 44.6 for orig - en and 34.9 for orig - de . As for   structural similarity , this stronger system has lower   overall posTER vs. the human reference—0.238   vs. 0.280/0.287 from Sup / Unsup — indicating even   more structural similarity with the reference . For   naturalness , the stronger system has lower perplex-   ity from the nLM . As a higher - quality system , this   is expected . At the same time , it scores much lower   than Sup and Unsup by the tLM , where higher in-   dicates more natural - sounding output : 29.23 vs.   41.06/58.17 for Sup / Unsup .   Ablation : Architecture vs. Data One reason   Unsup might produce more natural - sounding out-   put could be simply that it develops language-   modeling capabilities from natural German alone ,   whereas Sup must see some translated data ( being   trained on bitext of human translations ) . Next , we   ask whether the improved naturalness and struc-   tural diversity is due to the unsupervised NMT ar-   chitecture , or simply the natural training data .   We build a supervised en - de MT system with   329,000 paired lines of translated English source   and natural German , where the source is back-   translated German News Crawl from a supervised   system . In other words , we train on backtranslated   data only on the source side and natural German as   the target . The model thus develops its language-   modeling capabilities on natural sentences alone . If   more natural output is simply a response to training   on natural data , then this supervised system should   perform as well in naturalness as Unsup , or better .   We train another unsupervised system on trans-   lated text only . Source - side training data is syn-   thetic English from translating German News   Crawl with a supervised system . Target - side is syn-   thetic German which was machine - translated from   English News Crawl . If naturalness solely resultsfrom data , this system should perform worst , being   trained only on translated ( unnatural ) text .   Table 8 shows the results . The original unsuper-   vised system ( Unsup ) performs best according to   both LMs , having output that is more natural and   less like translated text . When given only natural   German to build a language model , the supervised   system ( Sup En - Trns / De - Orig ) stillproduces more   unnatural output than Unsup . Even when the unsu-   pervised system uses translated data only ( Unsup-   Trns ) , its output is stillmore natural than the origi-   nal supervised system ( Sup ) according to both LMs .   This is a surprising result , and is interesting for fu-   ture study . Together , these findings suggest that   both German - original data andthe unsupervised ar-   chitecture encourage output to sound more natural .   5 Application : Leveraging Unsupervised   Back - translation   Our results indicate that high - adequacy unsuper-   vised MT output is more natural and more struc-   turally diverse in comparison to human translation ,   than is supervised MT output . We are thus moti-   vated to use these advantages to improve transla-   tion . We explore how to incorporate unsupervised   MT into a supervised system via back - translation .   We train for ∼500,000 updates for each experiment ,   and select models based on validation performance   on newstest2018 . We test on newstest2019(p ) .   5.1 Baselines   The first row of Table 9 is the supervised baseline   trained on the WMT18 bitext . The second row is   Unsup , used throughout this work .   We back - translate 24 million randomly - selected   sentences of German News Crawl twice : once us-   ing a supervised German - English system trained   on WMT18 bitext with a transformer - big architec-   ture , and once using Unsup . Both use greedy de-   coding for efficiency . We augment the WMT18 bi-   text with either the supervised or unsupervised BT .   Seen in Table 9 , adding supervised BT ( + SupBT )   performs as expected ; minorly declining on the   source - original test set ( orig - en ) , improving on   the target - original set ( orig - de ) , and improving on2219   the paraphrase set ( nt19p ) . Conversely , adding   unsupervised BT ( + UnsupBT ) severely lowers   BLEU on source - original and paraphrase test sets .   Randomly - partitioning the BT sentences such that   50 % are supervised BT and 50 % are unsupervised   also lowers performance on orig - en ( +50 - 50BT ) .   5.2 Tagged BT   Following Caswell et al . ( 2019 ) , we tag BT on   the source - side . Tagging aids supervised BT   ( + SupBT_Tag ) and greatly improves unsupervised   BT ( + UnsupBT_Tag ) , which outperforms the base-   line and is nearly on - par with + SupBT_Tag . Com-   bining supervised and unsupervised BT using the   same tag for both ( +50 - 50BT_Tag ) shows no bene-   fit over + SupBT_Tag . +50 - 50BT_TagDiff uses dif-   ferent tags for supervised vs. unsupervised BT .   5.3 Probability - Based BT Selection   We design a BT selection method based on transla-   tion probability to exclude unsupervised BT of low   quality . We assume that supervised BT is “ good   enough . ” Given translations of the same source sen-   tence ( one supervised , one unsupervised ) we assert   that an unsupervised translation is “ good enough ”   if its translation probability is similar or better than   that of the supervised translation . If much lower ,   the unsupervised output may be low - quality .   •Score each supervised and unsupervised BT   with a supervised de - en system .   •Normalize the translation probabilities to con - trol for translation difficulty and output length .   •Compare probability of the supervised and   unsupervised BT of the same source sentence :   ∆P = Pnorm(unsup )   Pnorm(sup )   • Sort translation pairs by ∆P.   •Select the unsupervised BT for pairs scoring   highest ∆P and the supervised BT for the rest .   This filters out unsupervised outputs less than a hy-   perparameter T% as likely as the corresponding   supervised sentence and swaps them with the cor-   responding supervised sentence . Importantly , the   same 24 M source sentences are used in all experi-   ments . The procedure is shown in Figure 1 .   Full results varying T are in the Appendix for   brevity , but we show two example systems in Table   9 . The model we call “ + MediumMix_Tag ” uses   the top∼40 % of ranked unsupervised BT with the   rest supervised ( 9.4 M unsupervised , 14.6 M super-   vised ) . “ + SmallMix_Tag ” uses the top ∼13 % of   unsupervised BT ( 3.1 M unsupervised , 20.9 M su-   pervised).We use the same tag for all BTs . Im-   provements are modest , but our goal was to demon-   strate how one might use unsupervised MT output   rather than build a state - of - the - art system .   + SmallMix_Tag performs better than the previ-   ous best on newstest2018p and + MediumMix_Tag   performs highest overall on nt19p . We recall2220   that small differences on paraphrase test sets can   signal tangible quality differences ( Freitag et al . ,   2020 ) . Trusting BLEU on nt19p , we use + Medi-   umMix_Tag as our model for human evaluation .   One might inquire whether improved perfor-   mance is due to the simple addition of noise in light   of Edunov et al . ( 2018 ) , who conclude that noising   BT improves MT quality . Subsequent work , how-   ever , found that benefit is not from the noise itself   but rather that noise helps the system distinguish   between parallel and synthetic data ( Caswell et al . ,   2019 ; Marie et al . , 2020 ) . Yang et al . ( 2019 ) also   propose tagging to distinguish synthetic data . With   tagging instead of noising , Caswell et al . ( 2019 )   outperform Edunov et al . ( 2018 ) in 4 of 6 test sets   for En - De , furthermore find that noising on top of   tagging does not help . They conclude that “ tagging   and noising are not orthogonal signals but rather   different means to the same end . ” In light of this ,   our improved results are likely not due to increased   noise but rather to systematic differences between   supervised and unsupervised BT .   5.4 Human Evaluation   We run human evaluation with professional trans-   lators for + MediumMix_Tag , comparing its out-   put translation of the newstest2019 test set with   two baseline models . Table 10 shows that humans   prefer the combined system over the baseline out-   puts . Table 11 shows the percentage of sentences   judged as “ worse than , ” “ about the same as , ” or   “ better than ” the corresponding + SupBT_Tag out-   put , based on fluency . Raters again prefer the com-   bined system . The improvements are modest , but   encouragingly indicate that unsupervised MT may   have something to contribute to machine transla-   tion , even in high - resource settings .   6 Conclusion   Recent unsupervised MT systems can reach reason-   able translation quality under clean and controlled   data conditions , and could bring alternative transla-   tions to language pairs with ample parallel data . We   perform the first systematic comparison of super-   vised and unsupervised MT output from systems   of similar quality . We find that systematic differ-   ences do exist , and that high - quality unsupervised   MT output appears more natural andmore struc-   turally diverse when compared to human transla-   tion , than does supervised MT output . Our find-   ings indicate that there may be useful differences   between supervised and unsupervised MT systems   that could contribute to a system better than either   achieves alone . As a first step , we demonstrate an   unsupervised back - translation augmented model   that takes advantage of the differences between the   translation methodologies to outperform a tradi-   tional supervised system on human - evaluated mea-   sures of adequacy and fluency.2221References2222222322242225