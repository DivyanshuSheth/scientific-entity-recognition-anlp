  Fahim Faisal , Yinkai Wang , Antonios Anastasopoulos   Department of Computer Science , George Mason University , USA   Abstract   As language technologies become more ubiq-   uitous , there are increasing efforts towards ex-   panding the language diversity and coverage   of natural language processing ( NLP ) systems .   Arguably , the most important factor inﬂuenc-   ing the quality of modern NLP systems is data   availability . In this work , we study the geo-   graphical representativeness of NLP datasets ,   aiming to quantify if and by how much do   NLP datasets match the expected needs of the   language speakers . In doing so , we use en-   tity recognition and linking systems , present-   ing an approach for good - enough entity link-   ing without entity recognition ﬁrst . Last , we   explore some geographical and economic fac-   tors that may explain the observed dataset dis-   tributions .   1 Introduction   The lack of linguistic , typological , and geographi-   cal diversity in NLP research , authorship , and pub-   lications is by now widely acknowledged and doc-   umented ( Caines , 2019 ; Ponti et al . , 2019 ; Ben-   der , 2011 ; Adelani et al . , 2021 ) . Nevertheless , the   advent of massively multilingual models presents   opportunity and hope for the millions of speakers   of under - represented languages that are currently   under - served by language technologies .   Broadening up the NLP community ’s research   efforts and scaling from a handful up to the al-   most 7000 languages of the world is no easy feat .   In order for this effort to be efﬁcient and success-   ful , the community needs some necessary founda-   tions to build upon . In seminal work , Joshi et al .   ( 2020 ) provide a clear overview of where we cur-   rently stand with respect to data availability for the   world ’s languages and relate them to the languages ’   representation in NLP conferences . Choudhury andFigure 1 : Example of the dataset map our method pro-   duces for the Swahili section of MasakhaNER . The   dataset is only somewhat representative of Swahili   speakers , with only about 17 % of entity mentions re-   lated to Tanzania , Kenya , Uganda , DR . Congo , or   Rwanda and neighboring countries , with the USA and   western Europe over - represented .   Deshpande ( 2021 ) study how linguistically fair are   multilingual language models , and provide a nu-   anced framework for evaluating multilingual mod-   els based on the principles of fairness in economics   and social choice theory . Last , Blasi et al . ( 2022 )   provide a framework for relating NLP systems ’   performance on benchmark datasets to their down-   stream utility for users at a global scale , which   can provide insights into development priorities ;   they also discuss academic incentives and socioeco-   nomic factors that correlate with the current status   of systematic cross - lingual inequalities they ob-   serve in language technologies performance .   These works provide insights into current data   availability and estimated utility that are paramount   for making progress , as well as an evaluation frame-   work for future work . However , there is one miss-   ing building block necessary for real progress : a   way to estimate how representative of the underly-   ing language speakers is the content of our datasets .   Any evaluation framework and any utility estimates3381we build can only be trustworthy as long as the   evaluation data are representative . Gebru et al .   ( 2021 ) and Bender and Friedman ( 2018 ) recognize   the importance of this information , including them   in their proposed guidelines for “ datasheets ” and   “ data statements ” respectively ; but most datasets   unfortunately lack such meta - information . To the   best of our knowledge , MaRVL ( Liu et al . , 2021 ) is   the only dataset that is culturally - aware by design   in terms of its content .   We propose a method to estimate a dataset ’s   cultural representativeness by mapping it onto the   physical space that language speakers occupy , pro-   ducing visualizations such as Figure 1 . Our contri-   butions are summarized below :   •We present a method to map NLP datasets unto   geographical areas ( in our case , countries ) and   use it to evaluate how well the data represent the   underlying users of the language . We perform an   analysis of the socio - economic correlates of the   dataset maps we create . We ﬁnd that dataset rep-   resentativeness largely correlates with economic   measures ( GDP ) , with geographical proximity   and population being secondary .   •We test a simple strategy for performing entity   linking by - passing the need for named entity   recognition . We evaluate its efﬁcacy on 19 lan-   guages , showing that we can get within up to   85 % of a NER - informed harder - to - obtain model .   We also show that encouragingly , using either   model largely leads to similar dataset maps .   2 Mapping Datasets to Countries   Assumptions This work makes two assumptions :   that ( a ) data locality matters , i.e. , speakers of a   language are more likely to talk about or refer to   local news , events , entities , etc as opposed to ones   from a different side of the world , and ( b ) that we   can capture this locality by only focusing on en-   tities . Kumar et al . ( 2019 ) discuss these topical   correlations that are present in datasets , noting   that they exist and that L1 language identiﬁcation   models tend to pick up on them , i.e. if a text men-   tions Finland , a L1 langid model is probably go-   ing to predict that the speaker is Finnish , because   p(Finland /divides.alt0L1 = Finnish ) is generally high . In   that work Kumar et al . ( 2019 ) make explicit effortto avoid learning such correlations because they   are interested in building models for p(L1 / divides.alt0text )   ( i.e. p(L1 = Finnish /divides.alt0Finland ) ) that are not con-   founded by the reverse conditional . The mere fact   they need to do this , though , conﬁrms that real-   world text has such topical confounds .   As for our second assumption that we can cap-   ture these topical correlations by only looking at   entities , one need only to take a look at Table 2   of Kumar et al . ( 2019 ) , which lists the top topi-   cal confounding words based on log - odds scores   for each L1 language in their dataset : all lists in-   clude either entities related to a country where that   language is spoken ( e.g. ‘ Merkel ’ , the name of a   former chancellor , for German ) or topical adjec-   tives ( e.g. ‘ romanian ’ for Romanian ) .   Approach For a given dataset , our method fol-   lows a simple recipe :   1 . Identify named entities present in the dataset .   2 . Perform entity linking to wikidata IDs .   3 . Use Wikidata to link entities to countries .   We discuss each step below .   Entity Recognition Step Standard entity linking   is treated as the sequence of two main tasks : entity   recognition and entity disambiguation . One ap-   proach is to ﬁrst process the text to extract entities   and then disambiguate these entities to the correct   entries of a given knowledge base ( eg . Wikipedia ) .   This approach relies on NER model quality .   However , to perform analysis on several datasets   spanning several low - resource languages , one   needs good - quality NER models in all these lan-   guages . The interested reader will ﬁnd a discussion   on the cross - lingual consistency of NER models in   Appendix F.As we show in Section § 4 , we can   bypass this NER step if we tolerate a small penalty   in accuracy .   Entity Linking Step In this step we map named   entities to their respective Wikidata IDs . We further   discuss this step in Section § 4 .   From Entities to Countries We produce maps   to visualize the geographical coverage of the   datasets we study , discussing their properties and   our ﬁndings in Section § 3.3382To link entities to countries , we rely on Wiki-   data entries , depending on the type of entity :   •for persons , we log their places of birth ( P19 ) and   death ( P20 ) , and country of citizenship ( P27 ) ;   •for locations , we search for their associated coun-   try ( P17 ) ; and   •for organizations , we use the links of the ‘ lo-   cated_at ’ ( P276 ) and ‘ headquartered_at ’ ( P159 )   relations .   Since places of birth / death and headquarters are   not necessarily at the country level , we perform   a second step of associating these locations with   countries . In cases where the result does not cor-   respond to a modern - day country ( as can often be   the case with historical ﬁgures ) , we do not make   any attempts to link it to any modern day countries ,   excluding them from the analysis .   For example , the entry for Nicolaus Copernicus   ( Q619 ) lists him as born in Toru ´ n ( Q47554 ) which   is then mapped to Poland ; as having died in From-   bork ( Q497115 ) that also maps to Poland ; and as   a citizen of the Kingdom of Poland ( Q1649871 )   which is not mapped to any modern - day country ;   so he is only linked to Poland . Albert Einstein is   similarly mapped to both Germany and the United   States , due to his places of birth ( Ulm ) and death   ( Princeton ) .   3 Dataset - Country Maps   Before delving into our case studies , we ﬁrst list   a set of statistics of interest that one could extract   from our produced dataset - country maps , in order   to gauge a dataset ’s representativeness .   Representativeness Measures We will avoid   providing a single metric , largely because the ideal   metric to use will be very dataset - speciﬁc and re-   lated to the goals of the creators of the dataset and   the socioeconomic correlates they are interested in   ( see discussion in Section § 3.3 ) .   As a ﬁrst straightforward representativeness mea-   sure , we will compute the percentage of entities   associated with countries where the language   is largely spoken . For example , according to   Ethnologue ( Eberhard et al . , 2021 ) , most Swahili   speakersreside in Tanzania , Kenya , Uganda , DR .   Congo , and Rwanda . For a Swahili dataset , then ,   we compute the percentage of all entities associated   with this set of countries ( “ in - country ” ) .Notions of equity or fairness across countries   could be measured by various fairness metrics ,   given the distribution of entities over countries in a   dataset : from simply computing the standard devia-   tion of the observations , to treating countries as a   population and computing fairness indices like the   popular Gini index ( Gini , 1912 ; Gastwirth , 1972 ) or   the indices proposed by Speicher et al . ( 2018 ) . We   will opt for a simpler , much more interpretable mea-   sure , the number of countries not represented   in the dataset i.e. countries with associated entity   count below a given threshold ( we use zero for sim-   plicity but higher values would also be reasonable   for large datasets ) .   Last , especially for languages with signiﬁcant   amounts of speakers in more than one country , it   is important to go deeper and measure the repre-   sentativeness of this in - country portion . For a sim-   ple example , an English dataset with entities only   from the UK is probably not representative of Nige-   rian or Jamaican English speakers . Hence , we will   create two distributions over the countries where   the language is largely spoken : the distribution of   speaker populations ( as available from Ethnologue   and other public data ) , and the distribution of enti-   ties observed in the dataset . Discrepancies between   these two distributions will reveal potential issues .   While one could easily compute some measure of   distance between the two distributions ( e.g. the   Bhattacharyya coefﬁcient ( Bhattacharyya , 1943 ) ) ,   in this work we will rely on the interpretable advan-   tages of the visualizations . Measures of fairness   could be computed for this portion of the dataset ,   similarly as discussed above .   In the example dataset of the Swahili portion of   MasakhaNER in Figure 1 , the utility of our method   is apparent . Through the visualization , a researcher   can quickly conﬁrm that the dataset seems to not   reﬂect the users of the language to a large extent :   only about 17 % of the entities indeed correspond to   Tanzania , Kenya , Uganda , DR . Congo , or Rwanda   ( where Swahili and its varieties are treated as a   lingua franca , at least in portions of these coun-   tries ) . Wealthy or populous countries like USA ,   France , and China , are well - represented , as one   would expect , while 156 countries and territories   have no representation . At the same time , the vi-   sualization allows a researcher to identify gaps:3383Natural Questions MLQASQuAD TyDi - QA ( English )   beyond the neighboring African countries and per-   haps the Middle East , north - west African countries   as well as central America or central / south - east   Asia are clearly under - represented in this dataset .   Between the main Swahili - speaking countries , Tan-   zania , Kenya , and Uganda are well - represented   ( DR Congo and Rwanda less so , but they have less   Swahili speakers ) , with the former two perhaps   slightly over - represented and the latter ( as well as   Rwanda ) being under - represented relative to the   speakers population , c.f . red ( dataset entities ) and   green ( proportional to population ) bars in Figure 1 .   3.1 Datasets and Settings   We apply the process described above on several   datasets , chosen mostly for their language and ty-   pological diversity . Our process is not dataset- orlanguage - dependent , and could easily be applied   on any NL dataset . We brieﬂy describe the datasets   we include in our study below , with detailed statis-   tics in Appendix C.   NER Datasets We study the WikiANN   dataset ( Pan et al . , 2017 ) that is commonly used   in the evaluation of multilingual models . We   additionally study the MasakhaNER dataset ( Ade-   lani et al . , 2021 ) , which was created through   participatory design ( ∀et al . , 2020 ) in order to   focus on African languages . Since these datasets   are already annotated with named entities , we only   need to perform entity linking .   Question Answering We study four question   answering datasets ( focusing on the questions3384rather than contexts ) , namely SQuAD ( Rajpurkar   et al . , 2016 ) , MLQA ( Lewis et al . , 2020 ) , TyDi-   QA ( Clark et al . , 2020 ) , and Natural Ques-   tions ( Kwiatkowski et al . , 2019 , NQ ;) , which have   unique characteristics that lend themselves to inter-   esting comparisons . SQuAD is a large English - only   dataset ( although it has been translated through ef-   forts like XQuAD ( Artetxe et al . , 2020 ) ) . MLQA is   an - way parallel multilingual dataset covering 7 lan-   guages , created by translating an English dataset .   TyDi - QA is another multilingual dataset covering   11 languages , but each language portion is derived   separately , without translation involved . Last , NQ   is an English QA dataset created based on real-   world queries on the Google search engine for   which annotators found relevant Wikipedia con-   text , unlike the other datasets that were created by   annotators forming questions given a context .   Additional Datasets While not further dis-   cussed in this paper , additional visualizations   for more datasets ( e.g. for the X - FACTR   benchmark ( Jiang et al . , 2020 ) , and several ma-   chine translation benchmarks ) are available in   the project ’s webpage : .   3.2 Discussion   Beyond Figure 1 , we also show example maps in   Figure 2 for NQ , MLQA , SQuAD , and the English   portion of TyDi - QA . We provide additional maps   for all other datasets in Appendix G.   Comparing datasets The comparison of   MasakhaNER to the WikiANN dataset ( see   Appendix G ) reveals that the former is rather more   localized ( e.g. more than 80 % of the identiﬁed   entities in the Dholuo dataset are related to Kenya )   while the latter includes a smaller portion from   the countries where most native speakers reside   ( between 10%-20 % ) and almost always also   includes several entries that are very European- or   western - centric .   The effect of the participatory design ( ∀et al . ,   2020 ) approach on creating the MasakhaNER   dataset , where data are curated from local sources ,   is clear in all language portions of the dataset , with   data being highly representative of the speakers . In   Figures 8–9 ( App . G ) the majority of entities in the   Wolof portion are from Senegal and neighboring   countries ( as well as France , the former colonial   power of the area ) , and the Yoruba and Igbo ones   are centered on Nigeria . Figure 2 allows for a direct comparison of dif-   ferent QA datasets ( also see maps for other TyDi-   QA languages in Appendix G ) . The ﬁrst notable   point has to do with NQ , which was built based on   real - world English - language queries to the Google   search engine . Since such queries happen all over   the world , this is reﬂected in the dataset , which   includes entities from almost all countries in the   world . Two types of countries are particularly repre-   sented : ones where English is an ofﬁcial language   ( USA , UK , Australia , but also , to a lesser extent ,   India , Nigeria , South Africa , and the Philippines ) ;   and wealthy ones ( European , Japan , China , etc ) . In   our view , NQ is an exemplar of a representative   dataset , because it not only includes representation   of most countries where the language is spoken   ( with the sum of these entities being in their large   majority in - country : 80 % ) but due to its size it also   includes entities from almost all countries .   SQuAD also has a large percentage in - country   ( 63 % ) but it is less representative of different En-   glishes than NQ . India , for instance , is relatively   under - represented in all datasets ; in SQuAD it   ranks 7 , but it ranks 3 in NQ ( see red bars in   bottom left of ﬁgures ) . On the other hand , the ge-   ographical representativeness of both MLQA and   TyDi - QA ( their English portion ) is lacking . Since   these datasets rely on Wikipedia articles for their   creation , and Wikipedia has a signiﬁcant western-   country bias ( Greenstein and Zhu , 2012 ; Hube and   Fetahu , 2018 ) , most entities come from Europe , the   US , and the Middle East . All these datasets under-   represent English speakers from English - speaking   countries of the Global South like Kenya , South   Africa , or Nigeria , since there are practically al-   most no entities from these countries . MLQA fur-   ther under - represents the speakers of all other lan-   guages it includes beyond English , since all data   are translations of the English one . Contrast this to   TyDi - QA and its visualized Swahili portion which ,   even though still quite western - centric , does have a   higher representation from countries where Swahili   is spoken than the TyDi - QA English portion .   This discussion brings forth the importance of   being cautious with claims regarding systems ’ util-   ity , when evaluated on these datasets . One could ar-   gue that a QA system that is evaluated on NQ does   indeed give a good estimation of real - world utility ;   a system evaluated on TyDi - QA gives a distorted   notion of utility ( biased towards western - based   speakers and against speakers from the Global3385   South ) ; a system evaluated on MLQA will give an   estimation as good as one evaluated on TyDi - QA ,   but only on the English portion . We clarify that this   does not diminish the utility of the datasets them-   selves as tools for comparing models and making   progress in NLP : MLQA is extremely useful for   comparing models across languages on the exact   same data , thus facilitating easy comparisons of   the cross - lingual abilities of QA systems , without   the need for approximations or additional statisti-   cal tests . But we argue that MLQA should not be   used to asses the potential utility of QA systems   for German or Telugu speakers .   Similar observations can be made about com-   paring two similar projects that aim at testing the   memorization abilities of large language models ,   namely X - FACTR and multi - LAMA ( mLAMA ;   Kassner et al . , 2021 ) – see corresponding Figures   in Appendix G. Both of these build on top of Wiki-   data and the mTREx dataset . However , mLAMA   translates English prompts and uses entity - relation   triples mined from the English portion of Wikidata ,   unlike X - FACTR which uses different data for each   language , mined from their respective portion of   Wikidata . Both are still western - biased , since they   rely on Wikipedia , but one ( X - FACTR ) is better at   giving an indication of potential downstream utility   to users .   3.3 Socioeconomic Correlates   In this section we attempt to explain our ﬁndings   from the previous section , tying them to socioeco-   nomic factors .   Empirical Comparison of Factors We identify   socioeconomic factors fthat could be used to ex-   plain the observed geographic distribution of the   entities in the datasets we study . These are :   • a country ’s population f   • a country ’s gross domestic product ( GDP ) f   • a country ’s GDP per capita f• a country ’s landmass f   •a country ’s geographical distance from coun-   try / ies where the language is spoken f   The ﬁrst four factors are global and ﬁxed . The   ﬁfth one is relative to the language of the dataset   we are currently studying . For example , when we   focus on the Yoruba portion of the mTREx dataset ,   we use Nigeria ( where Yoruba is spoken ) as the   focal point and compute distances to all other coun-   tries . The assumption here is that a Yoruba speaker   is more likely to use or be interested in entities   ﬁrst from their home country ( Nigeria ) , then from   its neighboring countries ( Cameroon , Chad , Niger ,   Benin ) and less likely of distant countries ( e.g. Ar-   gentina , Canada , or New Zealand ) . Hence , we   assume the probability to be inversely correlated   with the country ’s distance . For macro - languages   or ones used extensively in more than one country ,   we use a population - weighted combination of the   factors of all relevant countries .   To measure the effect of such factors it is com-   mon to perform a correlational analysis , where   one measures Spearman ’s rank correlation coef-   ﬁcient rbetween the dataset ’s observed geograph-   ical distribution and the factors f. It is impor-   tant , though , that the factors are potentially co-   variate , particularly population and GDP . Hence ,   we instead compute the variance explained by a   linear regression model with factors fas input , i.e. ,   af+bf+cf+df+ewith a – elearned   parameters , trained to predict the log of observed   entity count of a country . We report explained   variance and mean absolute error from ﬁve - fold   cross - validation experiments to avoid overﬁtting .   Socioeconomic Correlates and Discussion The   results with different combination of factors for the   QA datasets are listed in Table 1.The best sin-3386glepredictor is , perhaps unsurprisingly , the GDP   of the countries where the language is spoken : all   datasets essentially over - represent wealthy coun-   tries ( e.g. USA , China , or European ones ) . Note   that GDP per capita is not as good a predictor , nei-   ther is landmass . A combination of geographical   distance with GDP explains most of the variance   we observe for all datasets , an observation that   conﬁrms the intuitions we discussed before based   solely on the visualizations . Importantly , the fact   that including population statistics into the model   deteriorates its performance is further proof that our   datasets are not representative of or proportional to   the underlying populations . The only dataset that   is indeed better explained by including population   ( and GDP per capita ) is NQ , which we already ar-   gued presents an exemplar of representativeness   due to its construction protocol .   Limitations It is important to note that our as-   sumptions are also limiting factors in our analyses .   Mapping languages to countries is inherently lossy .   It ignores , for instance , the millions of immigrants   scattered throughout the world whose L1 language   could be different than the dominant language(s ) in   the region where they reside . Another issue is that   for many languages the necessary granularity level   is certainly more ﬁne than country ; if a dataset does   not include any entities related to the Basque coun-   try but does include a lot of entities from Spain and   France , our analysis will incorrectly deem it repre-   sentative , even though the dataset could have been   a lot more culturally - relevant for Basque speakers   by actually including Basque - related entities .   Another limitation lies in the current state of the   methods and data resources on which our approach   relies . Beyond discrepancies in NER / EL across lan-   guages ( addressing which is beyond the scope of   this work ) , we suspect that Wikidata suffers from   the same western - centric biases that Wikipedia is   known for ( Greenstein and Zhu , 2012 ) . As a result ,   we might be underestimating the cultural represen-   tativeness of datasets in low - resource languages .   An additional hurdle , and why we avoid pro-   viding a single concrete representativeness score   or something similar , is that the ideal combina-   tion of socioeconomic factors can be subjective .   It could be argued , for instance , either that geo-   graphic proximity by itself should be enough , or   that it should not matter at all . Even further , other   factors that we did not consider ( e.g. literacy rate   or web access ) might inﬂuence dataset constructiondecisions . In any case , we share the coefﬁcients of   the NQ model , since it is the most representative   dataset we studied , at least for English : a=0:1:46   ( forf),b=0:87(f),c=25:4(f),d=0:41   ( f ) . We believe that ideally GDP should not   matter ( b→0 ) and that a combination of speaker   population and geographic proximity is ideal .   3.4 Geographical Breakdown of Models ’   Performance   Beyond the analysis of the datasets themselves , we   can also break down the performance of models by   geographical regions , by associating test ( or dev )   set samples containing entities with the geographi-   cal location of said entities . Since most test sets are   rather small ( a few hundred to a couple thousand   instances ) we have to coarsen our analysis : we map   each country to a broader region ( Africa , Americas ,   Asia , Europe , Oceania ) , keeping historical entities   in a separate category ( History ) .   We perform such a case study on TyDi - QA ,   comparing the performance on the TyDi - QA de-   velopment sets of two models : one trained mono-   lingually on the training set of each language of   TyDi - QA ( gold task ) , and another model trained   by Debnath et al . ( 2021 ) on English SQuAD and   automatically generated translations in the target   languages . Example results on Telugu shown in   Figure 3 reveal some notable trends . First , train-   ing set representation ( green bars in the Figures )   is not a necessary condition for good test set per-   formance ( red bars ) . Some test set instances ( e.g.   with historical and African entities ) receive simi-   lar test F1 score from both models . Perhaps the   most interesting though , is the comparison of the   Asian and European portions of the test set : the   Telugu monolingual model achieves similar perfor-   mance in these two subsets ; but the SQuAD - trained   model is almost 20 percentage points worse on the   Asian subset , showing the potential unfairness of   translation - based models ( Debnath et al . , 2021 ) .   For most TyDi - QA languages ( Indonesian being an   exception , see Table 2 ) the macro - standard devia-   tion ( computed over the averages of the 6 region   subsets ) is larger for the SQuAD - trained model   ( which is , hence , less fair than models trained on3387(a ) Train : TyDiQA , Test : TyDiQA dev set ( b ) Train : ( English ) SQuAD , Test : TyDiQA dev set   TyDi - QA ) .   4 Bypassing NER for Entity Linking   We use mGENRE ( Cao et al . , 2021 ) for the task of   multilingual entity linking , a sequence to sequence   system that predicts entities in an auto - regressive   manner . It works particularly well in a zero - shot   setting as it considers 100 + target languages as   latent variables to marginalize over .   Typically , the input to mGENRE can be in-   formed by a NER model that provides the named   entity span over the source . For instance , in the Ital-   ian sentence ( Einstein was a German physicist . ) the   word is enclosed within the entity span .   mGENRE is trained to use this information to re-   turn the most relevant Wikidata entries .   Due to the plasticity of neural models and mGE-   BRE ’s auto - regressive token generation fashion ,   we ﬁnd that by simply enclosing the whole sentence   in a span also yields meaningful results . In partic-   ular , for the previously discussed Italian sentence   now the input to mGENRE is .   The advantage of this approach is two - fold . First ,   one does not need a NER component . Second , ex-   actly because of bypassing the NER component ,   the EL model is now less constrained in its output ;   in cases where the NER component made errors ,   there ’s a higher chance that the EL model will re-   turn the correct result .   Experiments and Results We conduct experi-   ments to quantify how different a model unin-   formed by a NER model ( NER - Relaxed ) will   perform compared to one following the typical   pipeline ( NER - Informed ) .   Given the outputs of the two models over the   same set of sentences , we will compare their aver-   age agreement@ k , as in the size of the intersection   of the outputs of the two models divided by the   number of outputs of the NER - Informed model ,   when focusing only on their top- koutputs . We ag-   gregate these statistics at the sentence level over the   whole corpus . We focus on two datasets , namely   WikiANN and MasakhaNER , summarizing the re-   sults in Figure 4 .   Comparing the general performance between   these two datasets , it is clear that general agree-   ment is decent . In 7 Out of 9 typologically diverse   languages from WikiANN , more than 60 % top-1   entities are linked by both models . The African lan-   guages from MasakhaNER are low - resource ones   yielding less than 40 % EL agreement to English   in all cases . Given that most of these languages   have not been included in the pre - training of BART   ( the model mGENRE is based on ) , we expect that   using AfriBERTa ( Ogueji et al . ) or similar models3388 in future work would yield improvements .   Effect on downstream maps We compare the   dataset maps we obtain using NER - Relaxed and   NER - Informed ( using gold annotations ) models in   our pipeline for the MasakhaNER dataset . Overall ,   the maps are very similar . An example visualiza-   tion of the two maps obtained for Swahili is in   Figure 5 in Appendix E.1 .   The NER - Informed model produces slightly   fewer entities overall ( likely exhibiting higher pre-   cision for lower link recall ) but there are mini-   mal differences on the representativeness measures   e.g. , the in - country percentage changes from 15.3 %   ( NER - Informed ) to 16.9 % ( NER - Relaxed ) . We   can compare the distributions of the top- kcountries   obtained with the two models using Ranked Bi-   ased Overlap ( RBO ; higher is better ; Webber et al . ,   2010).The results for varying values for k(top- k   countries ) are presented in Table 6 in Appendix E.1 .   We overall obtain very high RBO values ( > : 8for   k=10 ) for all language portions and all values of   k. For example for 8 of the 10 MasakhNER lan-   guages the two models almost completely agree on   the top-10 countries with only slight variations in   their ranking . Dholuo and Amharic are the ones   exhibiting the worse overlap ( but still > : 5 RBO ) .   5 Conclusion   We present a recipe for visualizing how representa-   tive NLP datasets are with respect to the underlying   language speakers . We plan to further improve our   toolby making NER / EL models more robustly   handle low - resource languages . We will also ex-   pand our dataset and task coverage , to get a broader   overview of the current utility of NLP systems .   Acknowledgements   This work is generously supported by NSF Awards   2040926 and 2125466 .   References3389339033913392A Responsible NLP Notes   We use this section to expand on potential limita-   tions and risks of this work .   An inherent limitation of this work is that many   datasets are constructed with the goal of answering   scientiﬁc questions – not necessarily to be used to   build NLP systems that serve language users . If   our tool is applied without the assumptions behind   dataset construction in mind , it might lead to undue   criticisms of existing datasets . It us also important   to reiterate that no tool , including ours , will ever   be 100 % accurate , so our tool should be used as   an indicator of the cultural representativeness of   language datasets , not as a tool that can provide   deﬁnitive answers .   All scientiﬁc artifacts used in this paper are pub-   licly available under permissive licenses for fair   use . We are not re - distributing any data or code ,   beyond the code that we wrote ourselves ( which   will be released under a CC-0 license ) and the ad-   ditional annotations on top of the existing datasets   which map the datasets to Wikidata entries ( Wiki-   data data are also available under a CC-0 license ) .   Our use of our data is consistent with their intended   use .   B Related Work   Effective measurement of dataset quality is an as-   pect of fast - growing signiﬁcance . Training large   language models require huge amount of data and   as a result , the inference generated by these pre-   trained language model as well as the ﬁne - tuned   models often show inherent data bias . In a re-   cent work ( Swayamdipta et al . , 2020 ) , the authors   present how data - quality aware design - decision can   improve the overall model performance . They for-   mulated categorization of data - regions based on   characteristics such as out - of - distribution feature ,   class - probability ﬂuctuation and annotation - level   discrepancy .   Usually , multilingual datasets are collected from   diverse places . So it is important to assess whether   the utility of these datasets are representative   enough to reﬂect upon the native speakers . We   ﬁnd the MasakhaNER ( Adelani et al . , 2021 ) is one   such dataset that was collected from local sources   and the data characteristics can be mapped to lo-   cal users as a result . In addition , language models   often requires to be truly language - agnostic de-   pending on the tasks , but one recent work shows   that , the current state - of - the - art language applica - tions are far from achieving this goal ( Joshi et al . ,   2020 ) . The authors present quantitative assessment   of available applications and language - resource tra-   jectories which turns out not uniformly distributed   over the usefulness of targeted users and speakers   from all parts of the world .   Linking dataset entities to geospatial concept is   one integral part of our proposed methodology . On-   going geospatial semantics research mostly focuses   on extracting spatial and temporal entities ( Kokla   and Guilbert , 2020 ; Purves et al . , 2018 ) . The usual   approach is to ﬁrst extract geo - location concepts   ( i.e. geotagging ) from semi - structured as well as   unstructured data and then linking those entities to   location based knowledge ontology ( i.e. geocod-   ing ) . In ( Gritta et al . , 2019 ) , the authors propose a   task - metric - evaluation framework to evaluate exist-   ing NER based geoparsing methods . The primary   ﬁndings suggest that NER based geo - tagger models   in general rely on instant word - sense while avoid-   ing contextual information .   One important aspect of our study is the evalua-   tion of cross - lingual consistency while performing   multilingual NER or El tasks . In ( Bianchi et al . ,   2021 ) , the authors focus on the consistency evalu-   ation of language - invariant properties . In an ideal   scenario , the properties should not be changed via   the language transformation models but commer-   cially available models are not prone to avoid do-   main dependency .   C Dataset Statistics   See details in Table 3 .   D Geographical Breakdown of Models   Performance   See details in Table 4 .   E NER - Informed vs NER - Relaxed   Models   In this section , we report the detailed results ( see   Table 5 ) from our experiment with using intermedi-   ate NER model vs skipping this step .   E.1 Comparison of NER - Informed and   NER - Relaxed Maps   This experiment was performed on MasakhaNER   data . See Figure 5 for example maps in Swahili .   The distributions of the top- kcountries we obtain   with the two models ( one using the gold NER3393   annotations for NEL and one using our NER-   relaxed approach ) are compared using Ranked   Biased Overlap ( RBO ; higher is better ) ( Webber   et al . , 2010 ) , a metric appropriate for computing   the weighted similarity of disjoint rankings . We   choose a “ weighted " metric because we care more   about having similar results in the top- kcountries   ( the ones most represented ) so that the metric is   not dominated by the long tail of countries that   may have minimal representation and thus similar   rank . We also need a metric that can handle disjoint   rankings , since there ’s no guarantee that the top- k   countries produced by the processes using differentmodels will be different .   The results for varying values for k(top- kcoun-   tries ) are presented in Table 6 . We overall obtain   very high RBO values ( > : 75 ) for all language por-   tions and all settings .   F On the Cross - Lingual Consistency of   NER / EL Models   Deﬁnition Bianchi et al . ( 2021 ) in concurrent   work point out the need to focus on consis-   tency evaluation of language - invariant proper-   ties ( LIP ) : properties which should not be changed   via language transformation models . They suggest3394   LIPs include meaning , topic , sentiment , speaker   demographics , and logical entailment We propose   a deﬁnition tailored to entity - related tasks : cross-   lingual consistency is the desirable property that   two parallel sentences in two languages , which   should in principle use the same named entities   ( since they are translations of each other ) , are actu-   ally tagged with the same named entities .   F.1 NER Experiments   Models We study two models : SpaCy ( Honnibal   and Montani , 2017 ): a state - of - art monolingual li-   brary that supports several core NLP tasks ; and a   mBERT - based NER model trained on datasets from   WikiANN using the transformers library ( Wolf   et al . , 2020 ) .   Training To task - tune the mBERT - based model   on the NER task we use the WikiANN dataset with   data from the four languages we study : Greek ( el ) ,   Italian ( it ) , Chinese ( zh ) , and English ( en ) .   Evaluation To evaluate cross - lingual consis-   tency , ideally one would use parallel data where   both sides are annotated with named entities . What   we use instead , since such datasets do not exist to   the best of our knowledge , is ‘ silver ’ annotations   over parallel data . We start with unannotated par-   allel data from the WikiMatrix dataset ( Schwenk   et al . , 2021 ) and we perform NER on both the   English and the other language side , using the re-   spective language model for each side . In the process of running our experiments , we   identiﬁed some sources of noise in the WikiMatrix   dataset ( e.g. mismatched sentences that are clearly   not translations of each other ) . Thus , we calculated   the average length ratio between two matched sen-   tences , and discarded data that diverged by more   than one standard deviation from the mean ratio ,   in order to keep 95 % of the original data that are   more likely to indeed be translations of each other .   We use the state - of - the - art AWESOME - align   tool ( Dou and Neubig , 2021 ) as well fast-   align ( Dyer et al . , 2013 ) to create word - level links   between the words of each English sentence to their   corresponding translations . Using these alignment   links for cross - lingual projection ( Padó and Lap-   ata , 2009 ; Tiedemann , 2014 ; Ni et al . , 2017 , inter   alia ) allows us to calculate cross - lingual consis-   tency , measuring the portion of labels that agree   following projection . In particular , we use the   cross - lingual projections from the English side as   ‘ correct ’ and measure precision , recall , and F - score   against them .   Results In preliminary experiments we found   that , consistently with the literature , AWESOME-   align performed generally better than fast - align ,   hence for the remainder of our experiments we   only use AWESOME - align .   For the three languages we study , the cross-   lingual consistency of the monolingual SpaCy mod-   els is really low , with scores of 8:6 % for Greek–3395(a ) Swahili NER - Informed Swahili NER - Relaxed   Dataset Rank Biased Overlap ( RBO ) for top- kranked countries with k=   Portion 1 2 3 5 10 20 50 100 200   Amharic 0.00 0.25 0.50 0.57 0.53 0.51 0.59 0.65 0.76   Yoruba 1.00 1.00 1.00 0.87 0.82 0.87 0.85 0.83 0.87   Hausa 1.00 1.00 1.00 0.87 0.80 0.80 0.83 0.83 0.88   Igbo 1.00 1.00 1.00 0.96 0.89 0.82 0.79 0.79 0.86   Kinyarwanda 1.00 0.75 0.83 0.86 0.91 0.89 0.83 0.80 0.86   Luganda 1.00 0.75 0.83 0.81 0.81 0.82 0.78 0.76 0.83   Dholuo 1.00 0.75 0.83 0.77 0.66 0.58 0.57 0.62 0.76   Nigerian Pidgin 1.00 1.00 1.00 0.95 0.91 0.90 0.89 0.86 0.90   Wolof 1.00 1.00 1.00 0.96 0.85 0.77 0.68 0.70 0.81   Swahili 1.00 0.75 0.83 0.90 0.89 0.89 0.84 0.85 0.90   Average 0.90 0.82 0.88 0.85 0.81 0.79 0.76 0.77 0.84   Model Greek Italian Chinese 8.6 3.1 14.1   mBERT 53.4 62.9 25.5   English , 3:1 % for Italian – English and 14:1 % for   Chinese – English . The SpaCy models are indepen-   dently trained for each language and can produce   18 ﬁne - grained NE labels e.g. distinguishing dates   from time , or locations to geopolitical entities . As   such , there was no a priori expectation for high   cross - lingual consistency . Nevertheless , these ex-   tremely low scores reveal deeper differences , such   as potentially widely different annotation protocolsacross languages .   For the mBERT - based model we again label both   sides of the parallel data , but now evaluate only on   locations ( LOC ) , organizations ( ORG ) and persons   ( PER ) ( the label types present in WikiANN ) . The   mBERT models have signiﬁcantly higher cross-   lingual consistency : on the same dataset as above ,   we obtain 53:4 % for Greek to English , 62:9 % for   Italian to English and 25:5 % for Chinese to En-   glish .   Discussion To further understand the source of   cross - lingual discrepancies , we performed man-   ual analysis of 400 Greek - English parallel sen-   tences where the mBERT - based model ’s outputs   on Greek and the projected labels through English3396disagreed . We sampled 100 sentences where the   English - projected label wasbut the Greek one   was(location ) , 100 sentences with English-   projected asbut Greek as , and similarly for   persons ( ) .   We performed annotation using the following   schema :   •Greek wrong : for cases where only the English-   side projected labels are correct   •English wrong : for cases where the English - side   projected labels are wrong but the Greek - side are   correct   •both wrong : for cases where the labels on both   sides are incorrect   •alignment wrong : for cases where the two   aligned phrases are not translations of each other ,   so we should not take the projected labels into   account nor compare against them .   •all correct : both sides as well as the alignments   are correctly tagged ( false negatives ) .   Encouragingly , the entity alignments were   wrong in less than 10 % of the parallel sentences   we manually labelled . This means that our results   are quite robust : a 10%-level of noise can not ac-   count for an almost 50 % lack of consistency on   the Greek - English dataset . Hence , the system   deﬁnitely has room for improvement . A second   encouraging sign is that less than 2 % of the cases   were in fact false negatives , i.e. due to the phrasing   of the translation only one of the two sides actually   contained an entity .   Going further , we ﬁnd that mistakes vary signif-   icantly by label type . In about 75 % of the   cases it was the Greek - side labels that were wrong   in outputtingtags . A common pattern ( about   35 % of these cases ) was the Greek model tagging   months as locations . In the case of cases ,   62 % of the errors were on the English side . A   common pattern was the English - side model not   tagging persons when they are the very ﬁrst token   in a sentence , i.e. the ﬁrst token in Appendix K extends this discus-   sion with additional details and examples .   The above observations provide insights into   NER models ’ mistakes , which we were able to eas-   ily identify by contrasting the models ’ predictions   over parallel sentences . We argue this proves the   utility and importance of also evaluating NER mod-   els against parallel data even without gold NER   annotations . Improving the NER cross - lingual   consistency should in principle also lead to better   NER models in general . Potential solutions could   use a post - pretraining alignment - based ﬁne - tuned   mBERT model as the encoder for our data , or oper-   ationalize our measure of cross - lingual consistency   into an objective function to optimize .   F.2 Entity Linking Experiments   We now turn to entity linking ( EL ) , evaluating   mGENRE ’s cross - lingual consistency ( under the   NER - Relaxed setting , so the results below should   be interpreted under this lens , as the NER - Informed   – which we can not run due to the lack of NER mod-   els for some languages – could very well yield dif-   ferent results and analysis ) .   Dataset We use parallel corpora from the WMT   news translation shared tasks for the years 2014 to   2020 ( Bojar et al . , 2014 , 2015 , 2016 , 2017 , 2018 ;   Barrault et al . , 2019 , 2020 ) . We work with 14   English - to - target language pairs , with parallel sen-   tence counts in the range of around 1 - 5k .   Evaluation Unlike our NER experiment settings ,   we do not need word - level alignments to calculate   cross - lingual consistency . We can instead compare   the sets of the linked entities for both source and   target sentences . As before , we use mGENRE in   a NER - Relaxed manner . In an ideal scenario , the   output of the model over both source and target lan-   guage sentences will include the same entity links ,   yielding a perfect cross - lingual consistency score   of 1 . In this manner , we calculate and aggregate   sentence - level scores for the top- klinked entities   fork=1;3;5 . In Figure 6 , we present this score as   a percentage , dividing the size of the intersection3397src - tgtk=1   % k=3   % k=5   % sentence   count   en - ro 19.91 15.42 13.98 1999   en-ﬁ 17.40 15.25 14.29 1500   en - pl 16.60 14.19 13.43 2000   en - fr 16.53 14.42 13.42 1500   en - tr 14.09 13.02 12.01 1001   en - lt 13.45 11.96 10.77 2000   en - et 13.40 11.88 10.74 2000   en - ja 13.36 11.88 11.57 1998   en - zh 12.19 11.66 10.26 2002   en - lv 9.59 9.21 8.55 2003   en - kk 7.79 8.84 7.88 2066   en - ta 7.09 6.94 6.19 1989   en - gu 3.75 2.70 2.24 1998   en - iu 1.47 1.34 1.31 5173   ( of the source and target sentence outputs ) by the   number of source sentence entities .   Additionally , in Table 8 , we report the detailed   cross - lingual consistency score percentages for 14   english - language source - target pairs from WMT   news translation shared tasks ( Bawden et al . , 2020 ) .   Results As Figure 6 shows , we obtain low consis-   tency scores across all 14 language pairs , ranging   from 19.91 % for English - Romanian to as low as   1.47 % for English - Inukitut ( k=1 ) . The particularly   low scores for languages like Inuktitut , Gujarati ,   and Tamil may reﬂect the general low quality of   mGENRE for such languages , especially because   they use non - Latin scripts , an issue already noted   in the literature ( Muller et al . , 2021 ) .   The low percentage consistency scores for all   languages makes it clear that mGENRE does not   produce similar entity links for entities appearing   in different languages . In future work , we plan   to address this limitation , potentially by weight-   ing linked - entities according to the cross - lingual   consistency score when performing entity disam-   biguation in a multilingual setting .   Discussion We further analyze whether speciﬁc   types of entities are consistently recognized and   linked across language . We use SpaCy ’s English   NER model to categorize all entities . Figure 7   presents a visualization comparing consistent entity   category counts to source - only ones . Entity category Common Source - only   Unknown 1720 16709   PERSON 1358 5713   ORG 1047 6911   GPE 666 7379   NORP 176 1895   DATE 102 1427   CARDINAL 78 565   EVENT 77 777   LOC 62 453   WORK_OF_ART 20 133   PRODUCT 15 91   FAC 14 161   QUANTITY 8 85   TIME 6 43   MONEY 4 14   LAW 3 113   LANGUAGE 3 80   ORDINAL 2 90   PERCENT 1 3   TOTAL 5362 42642   From Figure 7 , it is clear that geopolitical enti-   ties ( ) are the ones suffering the most from low   cross - lingual consistency , with an order of magni-   tude less entities linked on both the English and   the other language side . On the other hand , person   names ( ) seem to be easier to link . While the   most common types of entities are , ( i.e.   organization ) and(i.e . geopolitical entity ) , we   found that the NER model still failed to correctly   categorize entities like ( Surat , , ) , ( Au-   rangzeb , , ) . However , these entities   were correctly linked by the NER - Relaxed pipeline ,   indicating its usefulness . We hypothesize , and plan   to test in future work , that a NER - Relaxed entity3398further regularized towards cross - lingual consis-   tency will perform better than a NER - Informed   pipeline , unless the NER component also shows   improved cross - lingual consistency .   From Figure 7 , it is clear that geopolitical enti-   ties ( ) are the ones suffering the most from low   cross - lingual consistency , with an order of magni-   tude less entities linked on both the English and   the other language side . On the other hand , person   names ( ) seem to be easier to link . While the   most common types of entities are , ( i.e.   organization ) and(i.e . geopolitical entity ) , we   found that the NER model still failed to correctly   categorize entities like ( Surat , , ) , ( Au-   rangzeb , , ) . However , these entities   were correctly linked by the NER - Relaxed pipeline ,   indicating its usefulness . We hypothesize , and plan   to test in future work , that a NER - Relaxed entity   further regularized towards cross - lingual consis-   tency will perform better than a NER - Informed   pipeline , unless the NER component also shows   improved cross - lingual consistency .   G Additional Dataset Maps   We present all dataset maps for the datasets we   study :   •MasakhaNER languages are available in Fig-   ures 8 and 9 .   •TydiQA languages are available in Figures 10   and 11 .   •WikiANN ( panx ) languages are available in   Figures 12 through 16 .   • SQuAD ( English ) in Figure 17 .   H NER Dataset Socioeconomic Factors   Table 1 presents the same analysis as the one de-   scribed in Section 3.3 for the X - FACTR and the   NER datasets . The trends are similar to the QA   datasets , with GDP being the best predictor and in-   cluding population statistics hurting the explained   variance .   I Socioeconomic Correlates Breakdown   You can ﬁnd the breakdown of the socioeconomic   correlates in Table 12 for TyDi - QA , Table 13 for   MasakhaNER , and Table 14 for WikiANN .   J NER Models Confusion Matrices   See Figure 18 for the confusion matrices of the   SpaCy and our WikiANN neural model . K Greek - English NER Error Discussion   We ﬁnd that the mistakes we identify vary sig-   niﬁcantly by label . In about 75 % of the   cases it was the Greek - side labels that were   wrong in tagging a span as a location . A com-   mon pattern we identiﬁed ( about 35 % of these   cases ) was the Greek model tagging as location   what was actually a month . For instance , in the   sentence ( In May 1990 ,   they visited Hungary for four days . ) the model tags   the ﬁrst two words ( “ in May " ) as a location , while   the English one correctly leaves them unlabelled .   In the case of cases , we found an even split   between the English- and the Greek - side labels be-   ing wrong ( with about 40 % of the sentences each ) .   Common patterns of mistakes in the English side   include tagging persons as locations ( e.g. “ Heath "   in “ Heath asked the British to heat only one room   in their houses over the winter . " where “ Heath " cor-   responds to Ted Heath , a British politician ) , as well   as tagging adjectives , often locative , as locations ,   such as “ palaeotropical " in “ Palaeotropical refers   to geographical occurrence . " and “ French " in “ A   further link [ .. ] by vast French investments and   loans [ ... ] " .   Last , in the case of cases we studied , we   found that 62 % of the errors were on the English   side . A common pattern was the English - side   model not tagging persons when they are the very   ﬁrst token in a sentence , i.e. the ﬁrst tokens in   “ Olga and her husband were left at Ay - Todor . " , in   “ Friedman once said , ‘ If you want to see capital-   ism in action , go to Hong Kong . ’ " , and in “ Evans   was a political activist before [ ... ] " were all tagged   as . To a lesser extent , we observed a similar is-   sue when the person ’s name followed punctuation ,   e.g. “ Yavlinsky " in the sentence “ In March 2017 ,   Yavlinsky stated that he will [ ... ] " .   L Comparing X - FACTR to mLAMA   These two similar projects aim at testing the   memorization abilities of large language models   ( X - FACTR and multi - LAMA ( mLAMA ; Kassner   et al . , 2021 ) ) – see corresponding Figures in Ta-   ble ? ? . Both of these build on top of Wikidata   and the mTREx dataset . Hence , their English por-   tions are equally representative of English speak-   ers , sufferring from under - representation of En-   glish speakers of the Global South . For the other   language , however , mLAMA translates English3399   Language Country Expl . Var . Mean Error   Greek GRC 0.586 0.343   Yoruba NGA 0.575 0.219   Bengali BGD 0.552 0.349   Marathi IND 0.587 0.29   French FRA 0.569 0.452   Hebrew ISR 0.604 0.369   Hungarian HUN 0.621 0.375   Russian RUS 0.601 0.406   Spanish ESP 0.552 0.457   Turkish TUR 0.613 0.36   Vietnamese VNM 0.521 0.398   Average 0.504 0.398   prompts and uses entity - relation triples mined from   the English portion of Wikidata , unlike X - FACTR   which uses different data for each language , mined   from their respective portion of Wikidata . Both are   still western - biased , since they rely on Wikipedia ,   but one ( X - FACTR ) is better at giving an indication   of potential downstream utility to users .   Language Country Expl . Var . Mean Error   Arabic SAU 0.501 0.415   Bengali BGD 0.498 0.385   English USA 0.562 0.335   Finnish FIN 0.566 0.376   Indonesian IDN 0.515 0.387   Japanese JPN 0.558 0.388   Korean KOR 0.546 0.336   Russian RUS 0.522 0.400   Swahili KEN 0.428 0.469   Telugu IND 0.534 0.294   Thai THA 0.550 0.333   Average 0.550 0.333   Language Country Expl . Var . Mean Error ETH 0.131 0.220 NGA 0.338 0.258 NGA 0.321 0.317 NGA 0.326 0.207 RWA 0.198 0.229 UGA 0.302 0.195 ETH 0.000 0.110 NGA 0.493 0.231 CMR 0.378 0.160 KEN 0.443 -0.285   Average 0.378 0.1603400   Language Country Expl . Var . Mean Error   af ZAF 0.497 0.338   ar SAU 0.570 0.454   az AZE 0.566 0.395   bg BGR 0.511 0.475   bn BGD 0.442 0.502   de DEU 0.613 0.402   el GRC 0.484 0.456   es ESP 0.497 0.462   et EST 0.565 0.398   eu ESP 0.565 0.387   fa IRN 0.589 0.426   ﬁ FIN 0.590 0.411   fr FRA 0.597 0.408   gu IND 0.068 0.030   he ISR 0.551 0.456   hi IND 0.529 0.279   hu HUN 0.563 0.451   i d IDN 0.488 0.442   it ITA 0.569 0.436   ja IDN 0.591 0.343   jv JPN 0.062 0.069   ka GEO 0.474 0.435   kk KAZ 0.411 0.205   ko KOR 0.519 0.423   lt LTU 0.533 0.395   ml IND 0.495 0.367   mr IND 0.530 0.320   ms MYS 0.496 0.463   my MMR 0.105 0.038   nl NLD 0.582 0.435   pa IND 0.052 0.064   pl POL 0.584 0.436   pt PRT 0.567 0.432   qu PER 0.301 0.090   ro ROU 0.581 0.436   ru RUS 0.576 0.435   sw KEN 0.402 0.223   ta LKA 0.524 0.367   te IND 0.351 0.107   th THA 0.567 0.215   tl PHL 0.473 0.399   tr TUR 0.619 0.409   uk UKR 0.576 0.447   ur PAK 0.512 0.463   vi VNM 0.557 0.440   yo NGA 0.079 0.086   zh CHN 0.591 0.376   Average 0.591 0.3763401MasakhaNER Geographic Coverage(a ) Amharic ( b ) Hausa(c ) Igbo ( d ) Kinyarwanda(e ) Luganda ( f ) Dholuo3402MasakhaNER Geographic Coverage(g ) Nigerian English ( h ) kiSwahili(i ) Wolof ( j ) Yoruba3403TyDi - QA Geographic Coverage(a ) Arabic ( b ) Bengali(c ) Finnish ( d ) Indonesian(e ) Japanese ( f ) Korean3404TyDi - QA Geographic Coverage(g ) Russian ( h ) Swahili(i ) Telugu ( j ) Thai3405Pan - X ( WikiANN ) Geographic CoverageAfrikaans ArabicAzerbaijani BulgarianBengali German3406Pan - X ( WikiANN ) Geographic CoverageGreek SpanishEstonian BasqueChinese Finnish3407Pan - X ( WikiANN ) Geographic CoverageFrench HebrewHungarian IndonesianJapanese Korean3408Pan - X ( WikiANN ) Geographic CoverageMarathi RussianSwahili TeleguThai Turkish3409Pan - X ( WikiANN ) Geographic CoverageVietnamese Yoruba   SQuAD Geographic Coverage3410Greek Italian Chinese   diagonal : 46.0 % diagonal : 59.1 % diagonal : 16.8%diagonal : 45.4 % diagonal : 60.5 % diagonal : 11.5%3411