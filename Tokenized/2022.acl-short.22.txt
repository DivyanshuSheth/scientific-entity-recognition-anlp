  Zheng LiZijian WangMing TanRamesh NallapatiParminder Bhatia   Andrew ArnoldBing XiangDan RothCornell UniversityAWS AI LabsUniversity of Pennsylvania   zl634@cornell.edu { zijwan , mingtan}@amazon.com   { rnallapa , parmib , anarnld , bxiang , drot}@amazon.com   Abstract   Large - scale pre - trained sequence - to - sequence   models like BART and T5 achieve state - of-   the - art performance on many generative NLP   tasks . However , such models pose a great   challenge in resource - constrained scenarios ow-   ing to their large memory requirements and   high latency . To alleviate this issue , we pro-   pose to jointly distill and quantize the model ,   where knowledge is transferred from the full-   precision teacher model to the quantized and   distilled low - precision student model . Empir-   ical analyses show that , despite the challeng-   ing nature of generative tasks , we were able   to achieve a 16.5x model footprint compres-   sion ratio with little performance drop relative   to the full - precision counterparts on multiple   summarization and QA datasets . We further   pushed the limit of compression ratio to 27.7x   and presented the performance - efficiency trade-   off for generative tasks using pre - trained mod-   els . To the best of our knowledge , this is the   first work aiming to effectively distill and quan-   tize sequence - to - sequence pre - trained models   for language generation tasks .   1 Introduction   Pretrained sequence - to - sequence ( seq2seq ) mod-   els such as BART ( Lewis et al . , 2020 ; Liu et al . ,   2020 ) and T5 ( Raffel et al . , 2020 ; Xue et al . , 2021 )   have shown great success in various natural lan-   guage processing ( NLP ) tasks , such as text sum-   marization ( Nallapati et al . , 2016 ; See et al . , 2017 ;   Narayan et al . , 2018 ) , machine translation , ques-   tion answering ( Fan et al . , 2019 ) and information   extraction ( Zhou et al . , 2021 ) . However , such   large - scale pre - trained language models come with   hundreds of millions of parameters : Lewis et al.(2020 ) trained a BART model with 400 M parame-   ters , while Raffel et al . ( 2020 ) pushed the limit to   11 billion parameters in T5 .   The continual growth in model sizes leads to sig-   nificant demand in both computation and memory   resources during inference , and poses a huge chal-   lenge on deployment , especially in real - time and/or   resource - constrained scenarios . This motivates re-   searchers to compress large pre - trained models to   be smaller and faster while retaining strong perfor-   mance . Among existing compression approaches   such as weight - sharing ( Dehghani et al . , 2019 ; Lan   et al . , 2020 ) , low - rank approximation ( Ma et al . ,   2019 ; Lan et al . , 2020 ) , and pruning ( Michel et al . ,   2019 ) , quantization approaches have received at-   tention recently since they reduce model footprints   using lower bits for the weight values without   changing the carefully - designed model architec-   ture . Most prior work on transformer quantization   focused on BERT - based transformers ( Zhang et al . ,   2020 ; Zafrir et al . , 2019 ; Bai et al . , 2021 ) . How-   ever , efficient quantization on the encoder - decoder   transformers is insufficiently studied . Prato et al .   ( 2020 ) achieve 8 - bit quantization for a seq2seq   transformer without significant loss of performance   but low - bit quantization proved to be difficult for   this model ( 4 - bit performance in Table 2 in their   work ) due to the accumulation of quantization er-   rors in seq2seq models . Moreover , their work did   not target quantizing large - scale pre - trained lan-   guage models , nor could it be applied to other   NLP tasks besides machine translation . Meanwhile ,   model distillation which transfers knowledge from   a large teacher model to a smaller student model   has been widely investigated for BERT compres-   sion ( Sanh et al . , 2019 ; Jiao et al . , 2020 ) .   Recently , Shleifer and Rush ( 2020 ) applied   “ shrink and fine - tune ” distillation method on BART   for text summarization , yet their work focuses more   on the methodology for distilling text summariza-   tion only . Besides , their work did not yield a sig-203nificant model footprint reduction , one of the most   challenging issues in the deployment of large mod-   els in resource - constrained scenarios .   In this work , we try to address the challenge   of building a more efficient seq2seq model by an-   swering two research questions : first , how well   does the quantized seq2seq model perform on var-   ious tasks ? Second , how do we combine quan-   tization and distillation to push the limit of com-   pressing the seq2seq model without significant per-   formance losses in challenging tasks like summa-   rization and question answering ? To this end , we   proposed a joint distillation and quantization frame-   work , which efficiently transfers the knowledge   from a full - precision teacher seq2seq model to its   student with fewer layers and ultra - low bits for   encoding its parameters . Experimental results on   BART show that the proposed models reduce the   model footprint by 16.5x while preserving competi-   tive performances on multiple language generation   benchmarks , and further illustrate the performance-   efficiency trade - off of compressing seq2seq models   up to 27.7x smaller . To the best of our knowledge ,   this is the first work aiming to effectively distill and   quantize seq2seq pre - trained models for language   generation tasks .   2 Distilling and Quantizing BART   In this section , we consider two directions for re-   ducing the size of our generative language model :   quantization ( § 2.1 ) and distillation ( § 2.2 ) . We ap-   ply distillation - aware training ( § 2.3 ) to train a quan-   tized and distilled low - precision model as a student   model to emulate the full - precision teacher model .   2.1 Quantization   Quantization refers to the operation of mapping a   real ( high - precision ) number to its low - precision   counterpart in order to achieve model footprint re-   duction . There has been extensive study on ap-   plying quantization to training neural networks .   Different quantization schemes include , e.g. , lin-   ear quantization ( e.g. , Hubara et al . , 2016 , 2017 ;   Jacob et al . , 2018 ) , non - linear quantization ( Li   and Sa , 2019 ) , approximation - based quantization   method ( Lin et al . , 2016 ) , and loss - aware quanti-   zation ( Hou and Kwok , 2018 ) . In our work , we   used the approximation - based method with linear   quantization following Zhang et al . ( 2020 ) .   Quantizing BART We applied quantization to   the weights of all the hidden layers and most ofthe embeddings . Following previous work ( Zhang   et al . , 2020 ) , we did not quantize positional embed-   dings and quantized activations only to 8 bits .   Weight Quantization We dive into the mathe-   matical details of how to quantize the weights in   BART models . Let us denote w∈ Ras the   vector obtained by stacking all the columns of the   full - precision weight matrix Wthat we wish to   quantize at iteration t. By quantizing w , we are   looking for a scaling factor ( also known as quanti-   zation step ) αand a low - precision number b , to   replace full precision weight wwithαb . When   quantizing with more than 2 bits , we are applying   the commonly used symmetric linear quantization ,   with   α= max|w|/ th   b∈ { − th,···,−1,0,1 , · · · , th }   where th= 2−1andnis the number of   bits we use for quantization . Then bcan be ob-   tained by b = round ( w / α ) . When quantizing   with 2 bits , we use the approximation based TWN   method ( Li et al . , 2016 ) . The mathematical details   are provided in Appendix A.   2.2 Distillation   The second task we consider is knowledge dis-   tillation , where we train a smaller student model   to mimic the behavior of a larger teacher model ;   specifically , we want to reproduce the output logits ,   attentions , and hidden states of the teacher model .   Following Shleifer and Rush ( 2020 ) , we initialize   the student model by copying the weights from   maximally spaced layers of the teacher model , e.g. ,   when initializing a 3 - layer student encoder ( de-   coder ) from a 6 - layer teacher encoder ( decoder ) ,   we copy the 0th , 3th and 5th layers from the teacher   to the student . When copying only 1 layer , we   choose the last instead of the first , which has been   shown empirically to yield better performance . Dif-   ferent than Shleifer and Rush ( 2020 ) who only dis-   till the decoder , we distill both the encoder and   the decoder . After initialization , we fine - tune the   student model with the combined objective of task   loss and distillation loss , i.e. L+L , with   L = L+L+L   where the RHS are MSE losses measuring the dif-   ference between the student and teacher with re-   gard to output logits , attention scores ( including204   encoder attention , decoder attention and cross at-   tention ) , and hidden states ( including all encoder   and decoder layers).We include the details of the   loss in Appendix B for completeness .   2.3 Distillation - aware quantization   To fine - tune our quantized and distilled model , we   use the technique of distillation - aware quantization   with a teacher - student architecture from ( Zhang   et al . , 2020 ) . We treat the quantized and distilled   low - precision model as a student model trained to   emulate the full precision model , which in this case   is the teacher model . Meanwhile , we also keep the   full - precision distilled counterpart of the student   model for parameter update . At each iteration , we   first quantize the full precision student model to   get its quantized version , then do the forward pass   with the low - precision student model and get the   task loss as well as the distillation losses discussed   in § 2.2 . Finally , we use these losses to update the   parameters in the full - precision student model.3 Experiments and Discussions   In this section , we evaluate the efficacy of jointly   Distilling and Quantizing BART ( hereinafter , DQ-   BART ) on text summarization and long - form ques-   tion answering using three benchmarks : CNN / Dai-   lyMail ( See et al . , 2017 ) , XSUM ( Narayan et al . ,   2018 ) , and ELI5 ( Fan et al . , 2019 ) . We additionally   study machine translation with mBART on WMT   English - Romanian ( En - Ro ) ( Bojar et al . , 2016 ) .   3.1 Experimental Setup   We followed the standard splits of these datasets .   The statistics could be found in Appendix C. For   ELI5 , we reproduced the author ’s implementation   to train a dense retriever that retrieves 10 support-   ing documents from Wikipedia for each question .   Additional details could be found in Appendix D.   As our target is achieving efficient seq2seq gener-   ative models , we used base - sized BART for summa-   rization and question answering tasks . For machine   translation , we used mBART - large due to the lack   of pretrained base - sized multilingual BART mod-   els . We reused existing models , and finetuned our   own models on end tasks when no open - sourced   model is available . We trained our quantized - only   models for 10 epochs and distilled - and - quantized205models for 20 epochs . We used a batch size of   128 , a learning rate of 3×10with 5 % linear   warmup , and selected the best model based on   rouge - L scores on the development set . We set gen-   erative hyperparameters following previous work   ( Lewis et al . , 2020 ) . All experiments were per-   formed on A100 GPUs .   3.2 DQ - BART Results and Discussions   We summarized the main results in Table 1 and   visualized the performance on text summarization   on the CNN / DailyMail dataset in Figure 1 . Addi-   tional visualizations are in Appendix E. We found   that :   1.Direct quantization performs poorly in genera-   tion tasks . The rouge - L score drops50 - 75 %   relatively compared with the baseline .   2.The performance of 8 - bit distillation - aware   quantized models ( “ 8 - 8 - 8 6 - 6 ” ) achieves com-   parable or even better performance compared   with the full precision models across all tasks ,   signaling that 8 - bit is not too challenging for   generative models like BART , similar to the   findings for BERT ( Zhang et al . , 2020 ) .   3.We were able to achieve a 13.6x model size   compression ratio when using 2 - bit quantiza-   tion with the trade - off of slight performance   drop for summarization tasks and even no per-   formance drop for the long - form QA task .   4.Combining quantization and distillation gives   us a further boost in model compression ratio   without significant further sacrifice in perfor-   mance . For example , when using 2 - bit quan-   tization , by cutting the layers of the decoderin half ( from “ 2 - 2 - 8 6 - 6 ” to “ 2 - 2 - 8 6 - 3 ” ) , we   only saw < 0.5rouge - L performance drop   across all tasks while getting another 2.9x   compression .   5.When pushing the compression rate to the   limit ( “ 2 - 2 - 8 1 - 1 ” ) , we were able to achieve   a 27.7x compression ratio while still preserv-   ing reasonable performance . We observed   a rouge - L drop of 5.67 for CNN / DailyMail   ( 42.09→36.42 ) , 12.24 for XSUM ( 35.71→   23.47 ) , and 1.06 for ELI5 ( 15.36→14.30 ) .   Thus , for certain tasks a large model com-   pression ratio would not lead to a significant   performance drop while for others the drop   could be huge , suggesting that the specific   compression ratio to use should be decided   on a task - by - task basis with the trade - off of   performance and efficiency in mind .   3.3 DQ - mBART for Translation   We further extend our study to see how distilla-   tion and quantization work for mBART ( Liu et al . ,   2020 ) , a deeper multilingual model . We experi-   mented mBART - large on WMT English - Romanian   translation task ( Bojar et al . , 2016 ) . The results are   in Table 2 .   We found that distillation - aware quantization   yields reasonably good performance , similar to the   findings in DQ - BART ( Table 1 ) . However , the   performance drops substantially when performing   2 - bit quantization with distillation , possibly due to   the accumulation of the distillation / quantization er-   ror becoming more significant with deeper models   and the challenging nature of machine translation.206Future work may explore how to improve the per-   formance of joint distillation and quantization for   deep models under a low - bit setting .   3.4 Distillation and Quantization v.s.   Distillation Only   We want to understand how much gain there is   when doing joint distillation and quantization com-   pared with distillation - only method ( Shleifer and   Rush , 2020 ) . To do so , we trained distillation - only   models and compared them with DQ - BART with   a similar size . From Table 3 , we found that joint   distillation and quantization performs much better   across all tasks , signaling the huge gain with joint   distillation and quantization . Additional ablation   study on “ Shrink and Finetune ” could be found in   Appendix F.   4 Conclusion   Transformer - based pre - trained seq2seq language   models like BART have greatly advanced the state   of the art in a range of NLP tasks . Yet , these   extremely large - scale models pose a challenge in   resource - constrained scenarios . To alleviate this is-   sue , we proposed DQ - BART , a jointly distilled and   quantized BART model . Empirical results show   that , despite the difficult nature of language gen-   eration tasks , we achieve a 16.5x model footprint   compression ratio with little performance drop on   three generative benchmarks , and further present   the performance - efficiency trade - off for seq2seq   models up to a 27.7x compression ratio . Addition-   ally , we studied distillation and quantization for   mBART on a machine translation task , and high-   lighted the challenge of joint low - bit quantization   with distillation for deeper models on cross - lingual   tasks . To the best of our knowledge , our method   is the first to apply joint quantization and distilla-   tion on pretrained language models , and this is the   first work aiming to effectively distill and quantize   seq2seq pretrained models for language generationtasks . We hope this work could open doors for de-   veloping and applying efficient seq2seq language   models . We leave additional compression methods   like attention head pruning ( Michel et al . , 2019 )   and sequence - level distillation ( Kim and Rush ,   2016 ) , and the measurement of latency improve-   ments in various settings for future work . Our code   is available at https://www.github.com/   amazon - research / dq - bart/ .   Acknowledgment   We would like to thank colleagues at AWS AI Labs   and our anonymous ARR reviewers for their con-   structive feedback .   References207208   A Details of TWN Quantization   When quantizing using 2 bits ( which is also know   as ternarization ) , following Zhang et al . ( 2020 ) ,   we apply the TWN method ( Li et al . , 2016 ) . To   quantize w , we are looking for scaling factor α > 0   andb∈ { − 1,0,1}such that w∼αbwhere nis   the dimension of w. To minimize the quantization   error , we have the following optimization problem :   α , b= arg max||w−αb||   where α > 0,b∈ { − 1,0,1}Denote ∆as a threshold and I(x)be a function   such that   I(x ) =       1,ifx > ∆   0,if−∆≤x≤∆   −1,ifx < −∆   and denote set J={i|I(w)̸= 0 } , then ac-   cording to Hou and Kwok ( 2018 ) , the solution to   the previous optimization problem can be reached   at   b = I(w ) , α=||w⊙b||   ||b|| ,   with∆= arg max1   |J|   X|w|      where ⊙is element - wise multiplication and || · ||   is the l - norm . To approximate this result , we set   ∆= 0.7||w||/dim ( w)then compute αandb   accordingly .   B Details of Distillation Losses   The distillation losses is defined as the following :   L = L+L+L   In this section we ’ll go through each part of the   losses . We denote ϕ ( · ) , ϕ(·)as the functions   that map the index of an encoder / decoder layer of   the student model to the index of the teacher model   layer that it is trained to emulate , the details of   which is discussed in § 2.2 , and we use l , lto   denote the number of encoder layers and decoder   layers of the student model . To illustrate , if l=   3 , l= 2 , we would have :   ϕ(0,1,2 ) = 0 , 3,5 , ϕ(0,1 ) = 0 , 5   For simplicity , we use superscript · , · to dis-   tinguish counterparts from the student model and   teacher model respectively .   Next , we will explain the definition of each part   of the distillation losses .   Firstly , L is the Mean Squared Error ( MSE )   between the output logits of the student model and   that of the teacher model , i.e.   L = MSE ( logits , logits )   Secondly , Lis the attention distillation loss ,   which is the sum of distillation losses of encoder209attentions ( EA ) , decoder attentions ( DA ) , and cross   attention ( CA ) , i.e.   L = L+L+L   where   L = XMSE ( EA , EA )   L = XMSE ( DA , DA )   L = XMSE ( CA , CA )   with the subscripts i , ϕ(i)specifying the indices of   the layers .   Finally , Lis the distillation loss between   all the hidden states between student layers and   teacher layers , which include encoder hidden states   ( EHS ) and decoder hidden states ( DHS ):   L = L+L   where   L = XMSE ( EHS , EHS )   L = XMSE ( DHS , DHS )   C Dataset Statistics   D ELI5 Additional Details   In this section , we present additional details for the   ELI5 dataset .   D.1 Dense Retriever   We were not able to find a public version of sup-   porting documents for ELI5 , and thus followed the   author ’s implementationto train a dense retrieverthat retrieves support documents from Wikipedia .   Our trained retriever achieves a similar perfor-   mance compared with the one reported in the au-   thor ’s implementation ( recall : ours 0.3273 , re-   ported 0.3247 ) .   D.2 Evaluating ELI5 Results   We use the - packageto calculate   rouge scores through the paper . However , as the   author of ELI5 pointed out , the original rouge   implementation used in ELI5 and BART papers   performs additional normalization . For consistency ,   we also reported results for ELI5 using the same - package , which differs from the   one used in ELI5 / BART . Here we compared the   performance of our trained ELI5 baseline model   with the public one using the rouge implementation   used in ELI5 / BART papers .   Results in Table 5 shows that the performance of   our base - size model is close to the one with large-   size reported in Lewis et al . ( 2020 ) . This signals   that our baseline model for ELI5 is well - trained .   E Visualizations of Experimental Results   on XSUM and ELI5 datasets210   F Comparisons on “ Shrink and   Finetune ”   We benchmarked the performance of three ran-   domly picked models with the “ Shrink and Fine-   tune ” schema proposed in Shleifer and Rush ( 2020 ) .   We ran the models using the same hyperparame-   ter settings we used in this paper . The results are   shown in Table 6 .   We found that when using distillation losses be-   tween the teacher and the student , the performance   are slightly better than the “ Shrink and Finetune ”   method under our setting . This signals that having   guidance in weighting is important for a quantized   and distilled model to learn well.211