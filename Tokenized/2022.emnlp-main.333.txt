  Hui LiuWenya WangHaoliang LiCity University of Hong KongNanyang Technological UniversityUniversity of Washington   liuhui3-c@my.cityu.edu.hk , wangwy@ntu.edu.sg , haoliang.li@cityu.edu.hk   Abstract   Sarcasm is a linguistic phenomenon indicating   a discrepancy between literal meanings and im-   plied intentions . Due to its sophisticated nature ,   it is usually challenging to be detected from the   text itself . As a result , multi - modal sarcasm   detection has received more attention in both   academia and industries . However , most exist-   ing techniques only modeled the atomic - level   inconsistencies between the text input and its   accompanying image , ignoring more complex   compositions for both modalities . Moreover ,   they neglected the rich information contained   in external knowledge , e.g. , image captions .   In this paper , we propose a novel hierarchi-   cal framework for sarcasm detection by explor-   ing both the atomic - level congruity based on   multi - head cross attention mechanism and the   composition - level congruity based on graph   neural networks , where a post with low con-   gruity can be identified as sarcasm . In addition ,   we exploit the effect of various knowledge re-   sources for sarcasm detection . Evaluation re-   sults on a public multi - modal sarcasm detection   dataset based on Twitter demonstrate the supe-   riority of our proposed model .   1 Introduction   Sarcasm refers to satire or ironical statements   where the literal meaning of words is contrary to   the authentic intention of the speaker to insult some-   one or humorously criticize something . Sarcasm   detection has received considerable critical atten-   tion because sarcasm utterances are ubiquitous in   today ’s social media platforms like Twitter and   Reddit . However , it is a challenging task to distin-   guish sarcastic posts to date in light of their highly   figurative nature and intricate linguistic synonymy   ( Pan et al . , 2020 ; Tay et al . , 2018 ) .   Early sarcasm detection methods mainly relied   on fixed textual patterns , e.g. , lexical indicators ,   syntactic rules , specific hashtag labels and emoji   occurrences ( Davidov et al . , 2010 ; Maynard andFigure 1 : An example of sarcasm along with the corre-   sponding image and different types of external knowl-   edge extracted from the image . The sarcasm sentence   represents the need for some good news . However , the   image of the TV program is switched to bad news de-   picting severe storms ( bad weather ) which contradicts   the sentence .   Greenwood , 2014 ; Felbo et al . , 2017 ) , which usu-   ally had poor performances and generalization abil-   ities by failing to exploit contextual information .   To resolve this issue , ( Tay et al . , 2018 ; Joshi et al . ,   2015 ; Ghosh and Veale , 2017 ; Xiong et al . , 2019 )   considered sarcasm contexts or the sentiments of   sarcasm makers as useful clues to model congruity   level within texts to gain consistent improvement .   However , purely text - modality - based sarcasm de-   tection methods may fail to discriminate certain   sarcastic utterances as shown in Figure 1 . In this   case , it is hard to identify the actual sentiment of the   text in the absence of the image forecasting severe   weather . As text - image pairs are commonly ob-   served in the current social platform , multi - modal   methods become more effective for sarcasm predic-   tion by capturing congruity information between   textual and visual modalities ( Pan et al . , 2020 ; Xu   et al . , 2020a ; Schifanella et al . , 2016 ; Liu et al . ,   2021 ; Liang et al . , 2021 ; Cai et al . , 2019 ) .   However , most of the existing multi - modal tech-   niques only considered the congruity level between   each token and image - patch ( Xu et al . , 2020a ; Tay   et al . , 2018 ) and ignored the importance of multi-   granularity ( e.g. , granularity such as objects , and   relations between objects ) alignments , which have   been proved to be effective in other related tasks ,   such as cross - modal retrieval ( Li et al . , 2021b ) and4995image - sentence matching ( Xu et al . , 2020b ; Liu   et al . , 2020 ) . In fact , the hierarchical structures of   both texts and images advocate for composition-   level modeling besides single tokens or image   patches ( Socher et al . , 2014 ) . By exploring com-   positional semantics for sarcasm detection , it helps   to identify more complex inconsistencies , e.g. , in-   consistency between a pair of related entities and a   group of image patches .   Moreover , as figurativeness and subtlety inherent   in sarcasm utterances may bring a negative impact   to sarcasm detection , some works ( Li et al . , 2021a ;   Veale and Hao , 2010 ) found that the identification   of sarcasm also relies on the external knowledge of   the world beyond the input texts and images as new   contextual information . What ’s more , it has drawn   increasing research interest in how to incorporate   knowledge to boost many machine learning algo-   rithms such as recommendation system ( Sun et al . ,   2021 ) and relation extraction ( Sun et al . , 2022 ) .   Indeed , several studies extracted image attributes   ( Cai et al . , 2019 ) or adjective - noun pairs ( ANPs )   ( Xu et al . , 2020a ) from images as visual semantic   information to bridge the gap between texts and   images . However , constrained by limited training   data , such external knowledge may not be suffi-   cient or accurate to represent the images ( as shown   in Figure 1 ) which may bring negative effects for   sarcasm detection . Therefore , how to choose and   leverage external knowledge for sarcasm detection   is also worth being investigated .   To tackle the limitations mentioned above , in   this work , we propose a novel hierarchical frame-   work for sarcasm detection . Specifically , our pro-   posed method takes both atomic - level congruity   between independent image objects and tokens , as   well as composition - level congruity considering   object relations and semantic dependencies to pro-   mote multi - modal sarcasm identification . To obtain   atomic - level congruity , we first adopt the multi-   head cross attention mechanism ( Vaswani et al . ,   2017 ) to project features from different modalities   into the same space and then compute a similarity   score for each token - object pair via inner products .   Next , we obtain composition - level congruity based   on the output features of both textual modality   and visual modality acquired in the previous step .   Concretely , we construct textual graphs and visual   graphs using semantic dependencies among words   and spatial dependencies among regions of objects ,   respectively , to capture composition - level featurefor each modality using graph attention networks   ( Veli ˇckovi ´ c et al . , 2018 ) . Our model concatenates   both atomic - level and composition - level congruity   features where semantic mismatches between the   texts and images in different levels are jointly con-   sidered . Specially , we elaborate the terminology   used in our paper again : congruity represents the   semantic consistency between image and text . If   the meaning of the image and text pair is contra-   dictory , this pair will get less congruity . Atomic is   between token and image patch , and compositional   is between a group of tokens ( phrase ) and a group   of patches ( visual object ) .   Last but not the least , we propose to adopt the   pre - trained transferable foundation models ( e.g. ,   CLIP ( Radford et al . , 2021 , 2019 ) ) to extract text   information from the visual modality as external   knowledge to assist sarcasm detection . The ratio-   nality of applying transferable foundation models   is due to their effectiveness on a comprehensive   set of tasks ( e.g. , descriptive and objective caption   generation task ) based on the zero - shot setting . As   such , the extracted text contains ample informa-   tion of the image which can be used to construct   additional discriminative features for sarcasm de-   tection . Similar to the original textual input , the   generated external knowledge also contains hier-   archical information for sarcasm detection which   can be consistently incorporated into our proposed   framework to compute multi - granularity congruity   against the original text input .   The main contributions of this paper are summa-   rized as follows : 1 ) To the best of our knowledge ,   we are the first to exploit hierarchical semantic in-   teractions between textual and visual modalities   to jointly model the atomic - level and composition-   level congruities for sarcasm detection ; 2 ) We pro-   pose a novel kind of external knowledge for sar-   casm detection by using the pre - trained founda-   tion model to generate image captions which can   be naturally adopted as the input of our proposed   framework ; 3 ) We conduct extensive experiments   on a publicly available multi - modal sarcasm de-   tection benchmark dataset showing the superiority   of our method over state - of - the - art methods with   additional improvement using external knowledge .   2 Related Work   2.1 Multi - modality Sarcasm Detection   With the rapid growth of multi - modality posts on   modern social media , detecting sarcasm for text4996and image modalities has increased research atten-   tion . Schifanella et al . ( 2016 ) first defined multi-   modal sarcasm detection task . Cai et al . ( 2019 ) cre-   ated a multi - modal sarcasm detection dataset based   on Twitter and proposed a powerful baseline fusing   features extracted from both modalities . Xu et al .   ( 2020a ) modeled both cross - modality contrast and   semantic associations by constructing the Decom-   position and Relation Network to capture common-   alities and discrepancies between images and texts .   Pan et al . ( 2020 ) and Liang et al . ( 2021 ) modeled   intra - modality and inter - modality incongruities uti-   lizing transformers ( Vaswani et al . , 2017 ) and graph   neural networks , respectively . However , these   works neglect the important associations played   by hierarchical or multi - level cross - modality dis-   matches . To address this limitation , we propose to   capture multi - level associations between modali-   ties by cross attentions and graph neural networks   to identify sarcasm in this work .   2.2 Knowledge Enhanced Sarcasm Detection   Li et al . ( 2021a ) and Veale and Hao ( 2010 ) pointed   out that commonsense knowledge is crucial for sar-   casm detection . For multi - modal based sarcasm   detection , Cai et al . ( 2019 ) proposed to predict five   attributes for each image based on the pre - trained   ResNet model ( He et al . , 2016 ) as the third modal-   ity for sarcasm detection . In a similar fashion , Xu   et al . ( 2020a ) extracted adjective - noun pairs(ANPs )   from every image to reason discrepancies between   texts and ANPs . In addition , as some samples can   contain text information for the images , Pan et al .   ( 2020 ) and Liang et al . ( 2021 ) proposed to apply the   Optical Character Recognition ( OCR ) to acquire   texts on the images . More recently , Liang et al .   ( 2022 ) proposed to incorporate objection detection   framework and label information of detected visual   objects to mitigate modality gap . However , the   knowledge extracted from these methods is either   not expressive enough to convey the information of   the images or is only restricted to a fixed set , e.g. ,   nearly one thousand classes for image attributes   or ANPs . Moreover , it should be noted that not   every sarcasm post has text on images . To this   end , in this paper , we propose to generate a de-   scriptive caption with rich semantic information   for each image based on the pre - trained Clipcap   model ( Mokady et al . , 2021 ) , which uses the CLIP   ( Radford et al . , 2021 ) encoding as a prefix to the   caption by employing a simple mapping networkand then fine - tunes GPT-2 ( Radford et al . , 2019 ) to   generate the image captions .   3 Methodology   Our proposed framework contains four main com-   ponents : Feature Extraction , Atomic - Level Cross-   Modal Congruity , Composition - Level Cross - Modal   Congruity and Knowledge Enhancement . Given an   input text - image pair , the feature extraction mod-   ule aims to generate text features and image fea-   tures via a pre - trained text encoder and an image   encoder , respectively . These features will then   be fed as input to the atomic - level cross - modal   congruity module to obtain congruity scores via a   multi - head cross attention model ( MCA ) . To pro-   duce composition - level congruity scores , we con-   struct a textual graph and a visual graph and adopt   graph attention networks ( GAT ) to exploit complex   compositions of different tokens as well as image   objects . The input features to the GAT are taken   from the output of the atomic - level module . Due   to the page limitation , we place our illustration   figure in Figure 6 . Our model is flexible to incor-   porate external knowledge as a " virtual " modality ,   which could be used to generate complementary   features analogous to the image modality for con-   gruity score computation .   3.1 Task Definition & Motivation   Multi - modal sarcasm detection aims to identify   whether a given text associated with an image   has a sarcastic meaning . Formally , given a multi-   modal text - image pair ( X , X ) , where Xcorre-   sponds to a textual tweet and Xis the correspond-   ing image , the goal is to produce an output label   y∈ { 0,1 } , where 1indicates a sarcastic tweet and   0otherwise . The goal of our model is to learn a   hierarchical multi - modal sarcasm detection model   ( by taking both atomic - level and composition - level   congruity into consideration ) based on the input of   textual modality , image modality and the external   knowledge if chosen .   The reason to use composition - level modeling is   to cope with the complex structures inherent in two   modalities . For example , as shown in Figure 2 , the   semantic meaning of the sentence depends on com-   posing your life , awesome andpretend to reflect a   negative position , which could be reflected via the   dependency graph . The composed representation   for text could then be compared with the image   modality for more accurate alignment detection.4997   3.2 Feature Extraction   Given an input text - image pair ( X , X ) , where   X={w , w , . . . , w}consists of ntokens , we   utilize the pre - trained BERT model ( Devlin et al . ,   2019 ) with an additional multi - layer perceptron   ( MLP ) to produce a feature representation for each   token , denoted as T= [ t , t , . . . , t ] , where   T∈R. As for image processing , given the im-   ageXwith the size L×L , following existing   methods ( Xu et al . , 2020a ; Cai et al . , 2019 ; Liang   et al . , 2021 ; Pan et al . , 2020 ) , we first resize the   image to size 224×224 . Then we divide each im-   age into rpatches and reshape these patches into a   sequence , denoted as { p , p , . . . , p } , in the same   way as tokens in the text domain . Next , we feed   the sequence of rimage patches into an image en-   coder to get a visual representation for each patch .   Specifically , in this paper , we choose two kinds of   image encoders including the pre - trained Vision   Transformer ( ViT ) ( Dosovitskiy et al . , 2020 ) and   a ResNet model ( He et al . , 2016 ) , both of which   are trained for image classification on ImageNet .   Hence , the embedding of image patches derived   by ViT or ResNet contains rich image label in-   formation . Here we adopt the features before the   final classification layer to initialize the embed-   dings for visual modality . We further use a two-   layer MLP to obtain the feature representations   for{p , p , . . . , p}asI= [ i , i , . . . , i ] , where   I∈R.   3.3 Atomic - Level Congruity Modeling   To measure atomic - level congruity between a text   sequence and an image , an intuitive solution is to   compute a similarity score between each token and   a visual patch directly . However , due to the huge   gap between two different modalities , we propose   to use cross attention mechanisms with hheads to   firstly align the two modalities in the same space ,   which can be computed as   where I∈RandT∈Rare feature repre-   sentations of the given text and image , respectively . W∈R , W∈RandW∈R   are query , key and value projection matrices , re-   spectively , for head∈R. It is worth noting   that we also consider taking image as query , text   as key and value for Equation ( 1 ) . However , we   empirically find that the performance is not desired   in this case . We conjecture the reason to be the   fact that the visual modality may not contain suffi-   cient information and is less expressive compared   to the textual modality to provide attentive guid-   ance , which can lead to negative impact of the final   performance .   Then , by concatenating all heads followed by   a two - layer MLP and a residual connection , we   obtain updated text representations ˜T∈Rafter   aligning with the visual modality as   where “ norm " denotes the layer normalization op-   eration and “ ∥ " denotes the concatenation opera-   tion . Next , to perform atomic - level cross - modal   congruity detection , we adopt the inner product as   Q=(˜TI)where Q∈Ris the matrix   consisting of Q[i , j]fori - th row and j - th col-   umn representing the similarity score between the   i - th token of the text and the j - th patch of the im-   age . Intuitively , different words can have different   influence on the sarcasm detection task . For ex-   ample , noun , verb and adjacent words are usually   more important for understanding sarcastic utter-   ances . As such , we feed features of words to a   fully - connected ( FC ) layer with a softmax activa-   tion function to model the token importance for   sarcasm detection . The final atomic level congruity   scorescan be obtained by a weighted sum of Q   with the importance score of each token as   where W∈Randb∈Rare trainable   parameters in the FC layer for token importance   score computation . s∈Rcontains the predicted   atomic - level congruity score corresponding to each   of the rpatches .   3.4 Composition - Level Congruity Modeling   The composition - level congruity detection consid-   ers the more complex structure of both the text and   image modalities , compared to the atomic - level   computations . To achieve that , we propose to first   construct a corresponding textual graph and a vi-   sual graph for the input text - image pair . For the4998textual graph , we consider tokens in the input text   as graph nodes and use dependency relations be-   tween words extracted by spaCyas edges , which   have been proved to be effective for various graph-   related tasks ( Liu et al . , 2020 ; Liang et al . , 2021 ) .   Concretely , if there exists a dependency relation   between two words , there will be an edge between   them in the textual graph . For the visual graph ,   given rimage patches , we take each patch as a   graph node and connect adjacent nodes according   to their geometrical adjacency . Additionally , both   two kinds of graphs are undirected and contain   self - loops for expressiveness .   Then , we model the graphs in text and visual   modalities with graph attention networks ( GAT )   ( Veli ˇckovi ´ c et al . , 2018 ) . GAT leverages self-   attention layers to weigh the extent of informa-   tion propagated from corresponding nodes . By   using GAT , atomic - level semantic information will   propagate along with the graph edge to learn   composition - level representations for both textual   modality and image modality . Here , we take the   textual graph for illustration given as   where k∈ N(i)∪ { i},Θ∈Randv∈R   are learnable parameters of the l - th textual GAT   layer . αis a scalar indicating the attention score   between node iand its neighborhood node j.t   represents the feature of node iin the l - th layer ,   witht=˜tinitialized from the atomic - level fea-   tures ˜T. We use ˆT= [ t , t , . . . , t]with   ˆT∈Rto represent the composition - level em-   beddings of the textual modality after LGAT lay-   ers that incorporate complex dependencies among   related tokens . In some cases , we may not be able   to construct a reliable textual graph due to the lack   of sufficient words in a sentence or errors from the   parser . Hence , we further propose to concatenate ˆT   with a sentence embedding c∈Rwhich is com-   puted by a weighted sum of each word embedding   in˜T :   with learnable W∈Randb∈R.   Likewise , we can obtain ˆI= [ i , i , . . . , i ] ,   ˆI∈Ras the composition - level representa-   tions in the visual modality . At last , we computecomposition - level alignment scores sbetween ˆT   andˆIin a similar way as atomic - level congruity as   whereis the ma-   trix of composition - level congruity between tex-   tual modality and visual modality , W∈R   andb∈Rare trainable parameters . s∈R   contains the final predicted composition - level con-   gruity score for each of the rimage patches .   3.5 Knowledge Enhancement   While using text - image pair can benefit sarcasm   detection compared with only using a single modal-   ity , recent works have shown that it might be still   challenging to detect sarcasm solely from a text-   image pair ( Li et al . , 2021a ; Veale and Hao , 2010 ) .   To this end , we explore the effect of fusing various   external knowledge extracted from an image for   sarcasm detection . For example , the knowledge   could be image attributes ( Cai et al . , 2019 ) , ANPs   ( Xu et al . , 2020a ) as they provide more informa-   tion on the key concepts delivered by the image .   However , such information lacks coherency and   semantic integrity to describe an image and may in-   troduce unexpected noise , as indicated in Figure 1 .   To address this limitation , we propose to generate   image captions as the external knowledge to assist   sarcasm detection . We further compare the effect   of each knowledge form in the experiments .   To fuse external knowledge into our model , we   treat knowledge Xas another “ virtual ” modal-   ity besides texts and images . Then the augmented   input to the model becomes ( X , X , X ) . As   the knowledge is given in textual form , we fol-   low the process of generating text representations   to attain the knowledge features . Specifically , we   first obtain the input knowledge representations as   K= [ k , k , . . . , k]using BERT with a MLP ,   which is analogous to T. Then , we propose to   reason the congruity score between text and knowl-   edge modalities at atomic - level by following the   procedure of computing atomic - level congruity   score between text and image modalities ( as shown   in Equations ( 1)-(3 ) ) with another set of parameters .   Concretely , for cross - modality attentions between   texts and knowledge , we replace Iin Equation ( 1 )   withKandTin Equation ( 1 ) with ˜T , which is   the updated text representations after aligning with   the visual modality . Inheriting information from   the image modality , using ˜Tas the query to at-   tend to knowledge enhances deeper interactions4999across all the three modalities . By further replacing   Tin Equation ( 2 ) with ˜T , we denote the atomic-   level text representations after aligning with the   knowledge by ˜T. The similarity matrix between   texts and knowledge becomes Q=(˜TK ) .   Then the atomic - level congruity score , denoted as   s∈R , can be obtained as   By adopting the dependency graph for X , we   further generate the updated knowledge representa-   tions ˆKvia GAT and obtain the composition - level   congruity score s∈Rbetween text and knowl-   edge modalities , following the same procedure for   text - image composition - level congruity score as   described in Section 3.4 .   3.6 Training & Inference   Given both the atomic - level and composition - level   congruity scores sands , respectively , the final   prediction could be produced considering the im-   portance of each image patch for sarcasm detection .   where W∈R , b∈R , W∈Rand   b∈Rare trainable parameters , p∈Ris a   r - dim attention vector , ⊙is element - wise vector   product . It is flexible to further incorporate external   knowledge by reformulating Equation ( 10 ) to   where sandsare atomic - level and composition-   level congruity scores between post and external   knowledge . p∈Rmeasures the importance   of each word in the knowledge obtained by . The entire model can be trained   in an end - to - end fashion by minimizing the cross-   entropy loss given the ground - truth label y.   4 Experiments   4.1 DatasetWe evaluate our model on a publicly available   multi - modal sarcasm detection dataset in English   constructed by Cai et al . ( 2019 ) . The statistics are   shown in Tabel 1 . Based on our preliminary anal-   ysis , the average numbers of tokens and entities   in a text are approximately 17and4 , respectively ,   where complex compositions among atomic units   have a higher chance of being involved . This find-   ing provides the basis for our framework using   atomic - level and composition - level information to   capture hierarchical cross - modality semantic con-   gruity .   4.2 Implementation   For a fair comparison , following the pre - processing   in ( Cai et al . , 2019 ; Liang et al . , 2021 ; Xu et al . ,   2020a ) , we remove samples containing words that   frequently co - occur with sarcastic utterances ( e.g. ,   sarcasm , sarcastic , irony andironic ) to avoid in-   troducing external information . The dependencies   among tokens are extracted using spaCy toolkit .   For image preprocessing , we resize the image to   224×224and divide it into 32×32patches ( i.e. ,   p= 7,r= 49 ) . For knowledge extraction , we   extract image attributes following ( Cai et al . , 2019 ) ,   ANPs following ( Xu et al . , 2020a ) and image cap-   tions via Clipcap ( Mokady et al . , 2021 ) .   Next , we employ a pre - trained BERT - base-   uncased modelas textual backbone network to   obtain initial embeddings for texts and knowledge ,   and choose the pre - trained ResNet and ViTmod-   ules as visual backbone networks to extract initial   embeddings for images . These textual and visual   representations are mapped to 200 - dim vectors by   corresponding MLPs . We use Adam optimizer to   train the model . The dropout and early - stopping   are adopted to avoid overfitting . The details of im-   plementations are listed in Table 6 in Appendix .   Our code is avaliable at https://github.com/   less - and - less - bugs / HKEmodel .   4.3 Baseline Models   We divide the baseline models into three categories :   text - modality methods , image - modality methods   and multi - modality methods . For text - based mod-   els , we adopt TextCNN ( Kim , 2014 ) , Bi - LSTM   ( Graves and Schmidhuber , 2005 ) , SMSD ( Xiong   et al . , 2019 ) which adopts self - matching networks5000   and low - rank bilinear pooling for sarcasm detec-   tion , and BERT ( Devlin et al . , 2019 ) that generates   predictions based on the [ CLS ] token as baseline   models . For pure image - based models , we follow   ( Cai et al . , 2019 ; Liang et al . , 2021 ) to utilize the   feature representations after the pooling layer of   ResNet and [ CLS ] token in each image patch se-   quence obtained by ViT to generate predictions .   For multi - modal based methods , we adopt HFM   ( Cai et al . , 2019 ) , D&R Net ( Xu et al . , 2020a ) ,   Att - BERT ( Pan et al . , 2020 ) , InCrossMGs ( Liang   et al . , 2021 ) and a variant of CMGCN ( Liang et al . ,   2022 ) without external knowledge as the multi-   modal baselines .   4.4 Results without External Knowledge   We first evaluate the effectiveness of our proposed   framework by comparing with the baseline models   as shown in Tabel 2 . It is shown that our proposed   model achieves state - of - the - art performance . Ob-   viously , text - based models perform far better than   image - based methods , which implies that text is   more comprehensible and more informative than   images . This supports our intuition of extracting   textual knowledge from images as additional clues .   On the other hand , multi - modal methods outper-   form all those models in single modality . This   illustrates that considering information from both   modalities contributes to the task by providing ad-   ditional cues on modality associations .   Note that compared with multimodal methods   using ResNet as the visual backbone network , our   model achieves a 0.97 % improvement in terms of   Accuracy and a 1.00 % improvement in terms of   F1 - score over the state - of - art method Att - BERT .   Besides , using ViT as the image feature extractor ,   our model outperforms the InCrossMGs model   with a 1.26 % improvement in Accuracy and 1.25 %   improvement in F1 - score . Our method can also   achieve better performance with improvement of   0.82 % based on Accuracy compared with the re-   cent proposed CMGCN . The results demonstrate   the effectiveness and superiority of our framework   for sarcasm detection by modeling both atomic-   level and composition - level cross - modality con-   gruities in a hierarchical manner .   4.5 Results with External Knowledge   We then evaluate the effectiveness of our method   by considering external knowledge . Table 3 reports   the accuracy and F1 - score for our proposed sar-   casm detection method enhanced by considering   different types of knowledge . By incorporating   image captions , the performance further improves   compared with the original model ( w/o external   knowledge ) . On the contrary , Image Attributes and   ANPs bring negative effects and deteriorate the per-   formance . We conjecture two possible reasons , 1 )   image attributes and ANPs can sometimes be mean-   ingless or even noisy for identifying sarcasm , 2 )   image attributes and ANPs are rather short , lacking   rich compositional information for our hierarchical   model . Last but not the least , it is worth mentioning   that only exploiting texts and captions in textual   modality ( Image Captions w/o image ) without the   visual modality also achieves superior performance   compared with all multi - modal baselines in Table   2 . Such observation illustrates that the pre - trained   models such as CLIP and GPT-2 can provide mean-   ingful external information for sarcasm detection .   4.6 Ablation Study   Impact of Different Components . We conduct   ablation studies using ViT as the visual backbone   network without external knowledge to further un-   derstand the impact of different components of our   proposed method . To be specific , we consider three   different scenarios , 1 ) remove the atomic - level con-5001   gruity score s(denoted as w/o atomic - level ) , 2 )   remove both sand the multi - head cross attention   ( MCA ) module by replacing ˜TtoTin all the com-   putations ( denoted as w/o MCA and atomic - level ) ,   3 ) remove composition - level congruity score s   ( denoted as w/o composition - level ) .   The results are shown in Table 4 . It is clear   that our model achieves the best performance when   composing all these components . It is worth noting   that the removal of composition - level sleads to   significant performance drop , compared to atomic-   level removal . This indicates that the composition-   level congruity plays a vital role for discovering   inconsistencies between visual and textual modali-   ties by exploiting complex structures through prop-   agating atomic representations along the semantic   or geographical dependencies . Moreover , the re-   moval of MCA leads to slightly lower performance   which indicates that cross attention is beneficial for   modeling cross modality interactions and reducing   the modality gap in the representation space .   Impact of MCA Layers . We measure the perfor-   mance change without external knowledge when   varying the number of MCA layers from 1 to 8 in   Figure 3a . As can be seen , the performance first   increases along with the increasing number of lay-   ers and then decreases after 6 layers . This shows   excessive MCA layers in atomic - level congruity   module may overfit to textual and visual modality   alignment instead of sarcasm detection .   Impact of GAT Layers . We analyse the impact of   the number of GAT layers for our proposed model   and report Accuracies and F1 scores in Figure 3b .   The results show that the best performance can   be achieved when using a two - layer GAT model   and the performance further drops when increasing   the number of layers . We conjecture the reason   to be the over - smoothing issue of Graph Neural   Networks when increasing the number of proœpa-   gation layers , making different nodes indistinguish-   able .   Impact of Different Sentence Embeddings . We   perform experiments using Universal Sentence En-   coder ( USE ) ( Cer et al . , 2018 ) , CLIP ( Radford   et al . , 2021 ) , CLS Token of Bert ( Devlin et al . ,   2019 ) , and Word Averaging to extract sentence   embedding ( i.e. , cin Eq 6 ) as shown in Tabel 5 .   Although CLIP outperforms other methods by a   slight margin , we prefer Word Averaging to keep   our model concise and reduce the number of pa-   rameters .   4.7 Case Study   To further justify the effectiveness of external   knowledge for sarcasm detection task , we provide   case studies on the samples that are incorrectly pre-   dicted by our proposed framework with only text-   image pair but can be accurately classified with the   help of image captions . For example , intravenous   injection depicted in Figure 4a can indicate a flu or   hospitalization via image modeling , which aligns   with fluorhospital in the text input . However , by   generating an image caption expressing a bad mood   indicated by suffered , it becomes easy to detect   the sarcastic nature of this sample by contrasting   funin the text description and suffer in the image   caption . As another example shown in Figure 4b ,   the image encoder only detects a human holding   a torch without any contexts and wrongly predicts   the sample as sarcasm because of the disalignment   between the image and text description . By gen-   erating the image caption expressing an olympic   athlete , the knowledge - fused model is able to de-   tect the alignment and correctly classifies this sam-   ple . This reflects that by further utilizing CLIP   ( Radford et al . , 2021 ) and GPT-2 ( Radford et al . ,   2019 ) models pre - trained using large - scale data as   an external knowledge source , the generated image   captions are more expressive to understand some5002   sophisticated visual concepts and to mitigate the   furtiveness and subtlety of sarcasm .   We further illustrate the effectiveness of our hier-   archical modeling by showing the congruity score   maps in Figure 5 . Given a sarcastic sample in Fig-   ure 5a , we visualize the congruity scores between   the text and image in both atomic - level module s   ( left side of Figure 5b ) and composition - level mod-   ules(right side of Figure 5b ) . The smaller the   values , the less similar between the text and image   ( i.e. , more likely to be detected as sarcasm ) . It can   be shown that the atomic - level module attends to   furniture in the image whereas the composition-   level module down - weighs those patches , making   the text and image less similar for sarcasm predic-   tion . Correspondingly , our proposed hierarchical   structure has the power to refine atomic congruity   to identify more complex mismatches for multi-   modal sarcasm detection using graph neural net-   works .   5 Conclusion   In this paper , we propose to tackle the problem of   sarcasm detection by reasoning atomic - level con-   gruity and composition - level congruity in a hierar-   chical manner . Specifically , we propose to model   the atomic - level congruity based on the multi - head   cross attention mechanism and the composition-   level congruity based on graph attention networks .   In addition , we propose to exploit the effect of   various knowledge resources on enhancing the dis-   criminative power of the model . Evaluation results   demonstrate the superiority of our proposed model   and the benefit of image captions as external knowl-   edge for sarcasm detection .   Limitations   We present two possible limitations : 1 ) we only   use the Twitter dataset for evaluation . However , to   the best of our knowledge , this dataset is the only   benchmark for the evaluation of multi - modal sar - casm detection in our community . Nevertheless , we   conduct extensive experiments with various metrics   to show the superiority of our proposed method .   We leave the construction of more high - quality   benchmarks in our future work ; 2 ) our knowledge   enhancement strategy in Section 3.5 may not be   suitable for ANPs and Image Attributes . We ana-   lyze the results in Section 4.5 . Consequently , there   is a pressing need for a more general and elegant   knowledge integration method in view of the im-   portance of external knowledge for multi - modality   sarcasm detection .   Ethics Statement   This paper is informed by the ACM Code of Ethics   and Professional Conduct . Firstly , we respect valu-   able and creative works in sarcasm detection and   other related research domains . We especially cite   relevant papers and sources of pre - trained mod-   els and toolkits exploited by this work as detailed   and reasonable as possible . Besides , we will re-   lease our code based on the licenses of any used   artifacts . Secondly , our adopted dataset does not in-   clude sensitive privacy individual information and   will not introduce any information disorder to so-   ciety . For precautions to prevent re - identification   of data , we mask facial information in Figure 4b .   At last , as our proposed sarcasm detection method   benefits the identification of authentic intentions in   multi - modal posts on social media , we expect our   proposed method can also bring positive impact on   related problems , such as opinion mining , recom-   mendation system , and information forensics in the   future .   ACKNOWLEDGEMENT   This work was supported in part by CityU New Re-   search Initiatives / Infrastructure Support from Cen-   tral ( APRC 9610528 ) , the Research Grant Council   ( RGC ) of Hong Kong through Early Career Scheme   ( ECS ) under the Grant 21200522 and Hong Kong   Innovation and Technology Commission ( InnoHK   Project CIMDA ) .   References50035004   A Model Overview   For illustration , we give a figure of the text-   image branch that can capture atomic - level and   composition - level congruity between textual and   visual modalities for multimodal sarcasm detec-   tion . Specifically , this figure is comprised of three   main components : Feature Representation , Atomic-   Level Cross Modality Congruity and Composition-   Level Cross Modality Congruity , where Feature   Representation extracts feature representations cor-   responding to texts and images , Atomic - Level   Cross Modality Congruity obtains congruity scores   via a multi - head cross - attention mechanism , and   Composition - Level Cross Modality Congruity pro-   duces composition - level congruity scores based on   constructed textual and visual graphs .   B Modal Parameters   For our model , the max length of sarcasm text is set   to100and the max length of generated image cap-   tion is set to 20 . For the architecture , the number   of the multihead cross - attention layer is set to 6for   text - image branch and 3for text - knowledge branch   to capture atomic - level congruity score . The head5005   number is set to 5 . The number of graph atten-   tion layer is set to 2to obtain composition - level   congruity score for both branches . We use Adam   optimizer with a learning rate of 2e−5 , weight decay   of5e−3 , batch size as 32 and dropout rate as 0.5 to   train the model . For more detail , please refer to the   checklist in our implementation.5006