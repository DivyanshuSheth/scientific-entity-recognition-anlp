  Ali ModarressiMohsen Fayyaz   Yadollah YaghoobzadehMohammad Taher PilehvarIran University of Science and Technology , IranUniversity of Tehran , IranTehran Institute for Advanced Studies , Khatam University , Iran   m_modarressi@comp.iust.ac.ir   { mohsen.fayyaz77 , y.yaghoobzadeh}@ut.ac.ir   mp792@cam.ac.uk   Abstract   There has been a growing interest in inter-   preting the underlying dynamics of Transform-   ers . While self - attention patterns were initially   deemed as the primary option , recent studies   have shown that integrating other components   can yield more accurate explanations . This   paper introduces a novel token attribution anal-   ysis method that incorporates all the compo-   nents in the encoder block and aggregates this   throughout layers . Through extensive quanti-   tative and qualitative experiments , we demon-   strate that our method can produce faithful and   meaningful global token attributions . Our ex-   periments reveal that incorporating almost ev-   ery encoder component results in increasingly   more accurate analysis in both local ( single   layer ) and global ( the whole model ) settings .   Our global attribution analysis significantly   outperforms previous methods on various   tasks regarding correlation with gradient - based   saliency scores . Our code is freely available at   https://github.com/mohsenfayyaz/GlobEnc .   1 Introduction   The stellar performance of Transformers ( Vaswani   et al . , 2017 ) has garnered a lot of attention to ana-   lyzing the reasons behind their effectiveness . The   self - attention mechanism has been one of the main   areas of focus ( Clark et al . , 2019 ; Kovaleva et al . ,   2019 ; Reif et al . , 2019 ; Htut et al . , 2019 ) . How-   ever , there have been debates on whether raw at-   tention weights are reliable anchors for explain-   ing model ’s behavior or not ( Wiegreffe and Pinter ,   2019 ; Serrano and Smith , 2019 ; Jain and Wallace ,   2019 ) . Recently , it was shown that incorporating   vector norms should be an indispensable part of any   attention - based analysis(Kobayashi et al . , 2020,Figure 1 : Aggregated attribution maps ( N ) for the   [ CLS ] token for fine - tuned BERT on SST2 dataset ( sen-   timent analysis ) . Our method ( GlobEnc ) is able to accu-   rately quantify the global token attribution of the model .   2021 ) . However , these norm - based studies incor-   porate only the attention block into their analysis ,   whereas Transformer encoder layer is composed of   more components .   Another limitation of the existing analysis tech-   niques is that they are usually constrained to the   analysis of single layer attributions . In order to   expand the analysis to multi - layered encoder - based   models in their entirety , an aggregation technique   has to be employed . Abnar and Zuidema ( 2020 )   proposed two aggregation methods , rollout and   max - flow , which combine raw attention weights   across layers . Despite showing the outcome of their   method to be faithful to a model ’s inner workings   in specific cases , the final results are still unsatis-   factory on a wide range of fine - tuned models .   Additionally , gradient - based alternatives ( Si-   monyan et al . , 2014 ; Kindermans et al . , 2016 ; Li   et al . , 2016 ) have been argued to provide a more ro-   bust basis for token attribution analysis ( Atanasova   et al . , 2020 ; Brunner et al . , 2020 ; Pascual et al . ,   2021 ) . Nonetheless , the gradient - based alternatives   have not been able to fully replace attention - based   counterparts , mainly due to their high computa-258tional intensity .   In this paper , we propose a new global token   attribution analysis method ( GlobEnc ) which is   based on the encoder layer ’s output . In GlobEnc ,   the second layer normalization is also included in   the norm - based analysis of each encoder layer . To   aggregate attributions over all layers , we applied   a modified attention rollout technique , returning   global scores .   Through extensive experiments and comparing   the global attribution with the input token attribu-   tions obtained by gradient - based saliency scores ,   we show that our method produces faithful and   meaningful results ( Figure 1 ) . Our evaluations on   models with distinct pre - training objectives and   sizes ( Devlin et al . , 2019 ; Clark et al . , 2020 ) show   high correlations with gradient - based methods in   global settings . Furthermore , with comparative   studies on each aspect of GlobEnc , we find that :   ( i ) norm - based methods achieve higher correla-   tions than weight - based methods ; ( ii ) incorporat-   ing residual connections plays an essential role in   token attribution ; ( iii ) considering the two layer   normalizations improve our analysis only if cou-   pled together ; and ( iv ) aggregation across layers   is crucial for an accurate whole - model attribution   analysis .   In summary , our main contributions are :   •We expand the scope of analysis from atten-   tion block in Transformers to the whole en-   coder .   •Our method significantly improves over exist-   ing techniques for quantifying global token   attributions .   •We qualitatively demonstrate that the attribu-   tions obtained by our method are plausibly   interpretable .   2 Background   In encoder - based language models ( such as BERT ) ,   a Transformer encoder layer is composed of several   components ( Figure 2 ) . The core component of the   encoder is the self - attention mechanism ( Vaswani   et al . , 2017 ) , which is responsible for the informa-   tion mixture of a sequence of token representations   ( x , ... , x ) . Each self - attention head computes a   set of attention weights A={α|1≤i , j≤n } ,   where αis the raw attention weight from the   itoken to the jtoken in head h∈ { 1 , ... , H } .   Therefore , the output representation ( z∈R ) for   theitoken of a multi - head ( with Hheads ) self-   attention module is computed by concatenating the   heads ’ outputs followed by a head - mixing W   projection :   z = C ( z , ... , z)W ( 1 )   where each head ’s output vector is generated by   performing a weighted sum over the transformed   value vectors v(x)∈R :   z=/summationdisplayαv(x ) ( 2 )   Norm - based attention . While one may inter-   pret the attention mechanism using the attention   weights A , Kobayashi et al . ( 2020 ) argued that do-   ing so would ignore the norm of the transformed   vectors multiplied by the weights , elucidating that   the weights are insufficient for interpretation . Their   solution enhanced the interpretability of attention   weights by incorporating the value vectors v(x )   and the following projection W. By reformulat-   ing Equation 1 , we can consider zas a summation   over the attentions heads :   z=/summationdisplay / summationdisplayαv(x)W / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright(3)259Using this reformulation , Kobayashi et al . pro-   posed a norm - based token attribution analysis   method , N:= ( ||z||)∈R , to measure   each token ’s contribution in a self - attention mod-   ule :   z=/summationdisplayαf(x ) ( 4 )   They showed that incorporating the magnitude of   the transformation function ( f(x ) ) is crucial in   assessing the input tokens ’ contribution to the self-   attention output .   Residual connections & Layer Normalizations .   Kobayashi et al . ( 2021 ) added the attention block ’s   Layer Normalization ( LN#1 ) and Residual con-   nection ( RES # 1 ) to its prior norm - based anal-   ysis to assess the impact of residual connec-   tions and layer normalization inside an attention   block . N:= ( ||z||)∈Ris the analy-   sis method which incorporates the attention block ’s   residual connection . The input vector xis added to   the attribution of each token to itself to incorporate   the influence of RES#1 :   z=/summationdisplayαf(x ) +1[i = j]x(5 )   They proposed a method for decomposing LNinto   a summation of normalizations :   LN(z ) = /summationdisplayg(z ) + β   g(z ) : = z−m(z )   s(z)⊙γ(6 )   where m(.)ands(.)are the element - wise mean and   standard deviation of the input vector ( cf . § A.1 ) .   The decomposition can be applied to the contribu-   tion vectors :   ˜z = g(/summationdisplayαf(x ) +1[i = j]x)(7 )   Accordingly , we can compute the magnitude   N:= ( ||˜z||)∈R , which represents   the amount of influence of an encoder layer ’s inputtoken jon its output token i. Based on this formu-   lation , a context - mixing ratio could be defined as :   r=||/summationtext˜z||   ||/summationtext˜z||+||˜z||(8 )   Experiments by Kobayashi et al . ( 2021 ) revealed   considerably low rvalues which indicate the huge   impact of the residual connections . In other words ,   the model tends to preserve token representations   more than mixing them with each other .   3 Methodology   Our method for input token attribution analysis has   a holistic view and takes into account almost ev-   ery component within the encoder layer . To this   end , we first extend the norm - based analysis of   Kobayashi et al . ( 2021 ) by incorporating the en-   coder ’s output LN#2 . We then apply an aggre-   gation technique to combine the information flow   throughout all layers .   Encoder layer output ̸=Attention block output .   While the RES#1 and the LN#1 from the attention   block are included in the analysis of Kobayashi   et al . ( 2021 ) , the subsequent FFN , RES#2 , and out-   put LN#2 are ignored ( see Fig . 2 ) . Hence , N   might not be indicative of the entire encoder layer ’s   function . To address this issue , we additionally in-   clude the encoder layer components from the atten-   tion block outputs ( ˜z ) to the output representations   ( ˜x ) . The output of each encoder ( ˜x ) is computed   as follows :   ˜z = FFN(˜z ) + ˜z   ˜x = LN(˜z)(9 )   We apply the LN decomposition rule in Eq . 7 to   separate the impacts of residual and FFN output :   ˜x=/summationdisplay / parenleftig   g(FFN(˜z ) ) + g(˜z)/parenrightig   + β   ( 10 )   Given that the activation function between the two   fully connected layers in the FFN component is   non - linear ( Vaswani et al . , 2017 ) , a linear decom-   position similar to Eq . 7 can not be derived . As   a result , we omit FFN ’s influence on the contri-   bution of each token and instead consider RES#2 ,   approximating ˜xasg(˜z ) . Nevertheless ,   it should be noted that the FFN still preserves some   influence on this new setting due to the presence of260s(˜z)ing(˜z).Similarly to Eq . 7 , we can in-   troduce a more inclusive layerwise analysis method   N:= ( ||˜x||)∈Rfrom input token j   to output token iusing :   ˜x≈g(˜z ) = ˜z−m(˜z )   s(˜z)⊙γ   ( 11 )   Aggregating multi - layer attention . To create an   aggregated attribution score , Abnar and Zuidema   ( 2020 ) proposed describing the model ’s attentions   via modelling the information flow with a directed   graph . They introduced attention rollout method ,   which linearly combines raw attention weights   along all available paths in the pairwise attention   graph . The attention rollout of layer ℓw.r.t . the   inputs is computed recursively as follows :   ˜A=/braceleftiggˆA˜Aℓ > 1   ˆA ℓ= 1(12 )   ˆA= 0.5¯A+ 0.5I ( 13 )   ¯Ais the raw attention map averaged across all   heads in layer ℓ. This method assumes equal contri-   bution from the residual connection and multi - head   attention ( See Fig . 2 ) . Hence , an identity matrix is   summed and renormalized , giving ˆA.   For aggregating the layerwise analysis methods ,   we use the rollout technique with minor modifi-   cations . As many of the methods already include   residual connections , we only use Eq . 12 ( replac-   ingˆAwith the desired method ’s attribution ma-   trix in layer ℓ ) to calculate the rollout of a given   method . However , for methods that do not assume   the residual connection , we define a corresponding   “ Fixed ” variation using Eq . 13 that incorporates   a fixed residual effect ( r≈0.5).We refer to   our proposed global method — aggregating the   Nacross all layers by the rollout method — as   GlobEnc .In what follows we report our exper-   iments , comparing GlobEnc with several other   settings .   4 Experiments   In this section , we introduce the datasets and the   token attribution analysis methods used in our eval-   uations , followed by the experimental setup and   results.4.1 Datasets   All analysis methods are evaluated on three differ-   ent classification tasks . To cover sentiment detec-   tion tasks we use SST2 ( Socher et al . , 2013 ) , MNLI   ( Williams et al . , 2018 ) for Natural Language Infer-   ence and Hatexplain ( Mathew et al . , 2021 ) in hate   speech detection .   4.2 Analysis Methods   We use two categories of explainability approaches   in our work : Weight - based andNorm - based .The   Weight - based approaches employed in our experi-   ments are as follows :   •W : The raw attention maps averaged across   all heads ( See Ain § 2 ) .   •W : Abnar and Zuidema ’s assump-   tion ; add an identity matrix as a fixed residual   toA(seeˆAin Eq . 13 ) .   •W : The corrected version of Win which   accurate residuals are added based on the   context - mixing ratios of N :   ˆr=/vextenddouble / vextenddouble / vextenddouble / summationtext˜x / vextenddouble / vextenddouble / vextenddouble   /vextenddouble / vextenddouble / vextenddouble / summationtext˜x / vextenddouble / vextenddouble / vextenddouble+∥˜x∥(14 )   In order to enforce Wto have a context-   mixing ratio equal to ˆr , it is essential to   zero - out the diagonal elements ( the tokens ’   attentions to themselves ) of ¯Aand renormal-   ize it :   A= ( I−diag / parenleftbig¯A / parenrightbig   ) ( ¯A−diag / parenleftbig¯A / parenrightbig   )   W:=diag(ˆr,···,ˆr)A   + diag(1−ˆr , . . . , 1−ˆr)I   ( 15 )   The Norm - based analysis methods , namely N ,   NandNwere discussed in detail in § 2 .   Our proposed norm - based method Nwas ex-   plained in § 3 . For an ablation study , we introduce   Nwhich is N , corrected with a fixed resid-   ual similar to W.   ˆN=/parenleftigg   ||z||/summationtext||z||/parenrightigg   ∈R   N:= 0.5ˆN+ 0.5I(16)261In § 4.5 , we will demonstrate our comparative   studies between the aforementioned methods and   GlobEnc .   4.3 Gradient - based Methods for Faithfulness   Analysis   Gradient - based methods are widely used as alter-   natives for attention - based counterparts for quanti-   fying the importance of a specific input feature   in making the right prediction ( Li et al . , 2016 ;   Atanasova et al . , 2020 ) . In this section we dis-   cuss the specific gradient - based methods we use ,   namely saliency , HTA , and our adjusted HTA .   4.3.1 Saliency   Gradient - based saliency is based on the gradient   of the output ( y ) w.r.t . the input embeddings ( e ) .   One of the most accurate variations of the saliency   family is the gradient ×input method ( Kindermans   et al . , 2016 ) where the input embeddings is multi-   plied by the gradients . Thus , the contribution score   of input token iis determined by first computing   the element - wise product of the input embeddings   ( e ) and the gradients of the true class output score   ( y ) w.r.t . the input embeddings . Then , the L2   norm of the scaled gradients is computed to derive   the final score :   Saliency=/vextenddouble / vextenddouble / vextenddouble / vextenddouble∂y   ∂e⊙e / vextenddouble / vextenddouble / vextenddouble / vextenddouble(17 )   4.3.2 HTA x Inputs   To determine an upper bound on the information   mixing within each layer , we use a modified ver-   sion of Hidden Token Attribution ( Brunner et al . ,   2020 , HTA ) . In the original version , HTA is the   sensitivity between any two vectors in the model ’s   computational graph . However , inspired by the   gradient ×input method ( Kindermans et al . , 2016 ) ,   which has shown more faithful results ( Atanasova   et al . , 2020 ; Wu and Ong , 2021 ) , we multiply the   input vectors by the gradients and then apply a   Frobenius norm . We compute the attribution from   hidden embedding j(e ) to hidden embedding i   ( e ) in layer ℓas :   c=/vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble∂e   ∂e⊙e / vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble(18 )   Computing HTA - based attribution matrices is an   extremely computation - intensive task ( especially   for long texts ) due to the high dimensionality of hid-   den embeddings . Hence , we only use this methodfor 256 examples from the SST-2 task ’s validation   set . It is worth noting that extracting the HTA-   based contribution maps for the aforementioned   data took approximately 2 hours , whereas comput-   ing the maps for the entire analysis methods stated   in § 4.2 took only 5 seconds .   4.4 Setup   We employ HuggingFace ’s Transformers library   ( Wolf et al . , 2020 ) and the BERT - base - uncased   model . For fine - tuning BERT , epochs vary from 3   to 5 , and the batch size and learning rate are 32 and   3e-5 , respectively . We also carried out the main   experiment on BERT - large and ELECTRA ( Devlin   et al . , 2019 ; Clark et al . , 2020 ) where the results   are reported at § A.2 .   After rollout aggregation of each analysis   method , we obtain an accumulated attribution ma-   trix for every layer ( ℓ ) of BERT . These matrices   indicate the overall contribution of each input token   to all token representations in layer ℓ. Since the   classifier in a fine - tuned model is attached to the   final layer representation of the [ CLS ] token , we   consider the first row ( corresponding to [ CLS ] at-   tributions ) of the last layer attribution matrix . This   vector represents the contribution of each input to-   ken to the model ’s final decision . As a measure of   faithfulness of the resulting vector with the saliency   scores , we report the Spearman ’s rank correlation   between the two vectors .   4.5 Results   Table 1 shows the Spearman correlation of saliency   scores with the aggregated attribution scores from   [ CLS ] to input tokens at the final layer . In order   to determine the contribution of each component   of encoder layer to the overall performance , we   report the results for attribution analysis methods   discussed in § 4.2 . Our results demonstrate that in-   corporating the vector norms , residual connection ,   and both layer normalizations yields the highest   correlation ( N ) . In what follows , we discuss   the impact of incorporating various parts in the   analysis .   4.5.1 On the role of vector norms   As also suggested by Kobayashi et al . ( 2020 ) , vec-   tor norms play an important role in determining262Attention Rollout   SST2 MNLI HX   Weight - based −0.11 ± 0.26 −0.06 ± 0.22 0.12 ± 0.26   w/ Fixed Residual−0.24 ± 0.26 −0.05 ± 0.26 0.13 ± 0.28   w/ Residual 0.19 ± 0.26 0.27 ± 0.25 0.53 ± 0.24   Norm - based 0.44 ± 0.20 0.47 ± 0.16 0.43 ± 0.22   w/ Fixed Residual 0.48 ± 0.20 0.55 ± 0.16 0.48 ± 0.22   w/ Residual 0.73 ± 0.13 0.75 ± 0.10 0.66 ± 0.17   w/ Residual + Layer Norm 1 −0.21 ± 0.26 −0.06 ± 0.26 0.08 ± 0.28   w / GlobEnc : [ Residual + Layer Norm 1 , 2]0.77 ± 0.12 0.78 ± 0.09 0.72 ± 0.17   attention outputs . This is highlighted by the signif-   icant gap between weight - based and norm - based   settings across all datasets in Table 1 .   We also show the correlation of the aggregated   attention for all layers in Figure 3 . The norm - based   settings ( NandN ) attain higher correlation   than the weight - based counterparts ( WandW )   almost in all layers , confirming the importance of   incorporating vector norms.4.5.2 On the role of residual connections   Kobayashi et al . ( 2021 ) showed that in the encoder   layer , the output representations of each token is   mainly determined by its own representation , and   the contextualization from other tokens ’ plays a   marginal role . This is in contrary to the simplifying   assumption made by Abnar and Zuidema ( 2020 )   who used a fixed context - mixing ratio of 0.5(as-   suming that BERT equally preserves and mixes the   representations ) . This setting is shown as weight-   based with fixed residual ( W ) in Table 1 .   We compare this setting against W(see § 4.2 ) .   Wis similar to W(in that it does not   take into account vector norms ) but differs in that   it considers a dynamic mixing ratio ( the one from   N ) . The huge performance gap between the   two settings in Table 1 clearly highlights the im-   portance of considering accurate context - mixing   ratios . Therefore , it is crucial to consider the resid-   ual connection in the attention block for input token   attribution analysis .   To further demonstrate the role of residual con-   nections , we utilize the introduced method in § 4.2 ,   where we modified the norm - based attentions with   fixed residual ( r≈0.5 ) . The comparison of norm-   based without any residual ( N ) and with a fixed   residual ( N ) shows a consistent improve-   ment for the latter across all the datasets . This   provides evidence on that having a fixed uniform   context - mixing ratio is better than neglecting the   residual connection altogether .   Finally , when we aggregate the norm - based anal-   ysis with an accurate dynamic context - mixing ratio   ( N ) , we observe the highest correlation up to263   this point , without layer normalization .   4.5.3 On the role of layer normalization   In Table 1 we see a sudden drop in correlations for   N. Although this method considers vector   norms and residuals , incorporating LN#1 in the en-   coder seems to have deteriorated the accuracy for   token attribution analysis . To determine whether   this deterioration of correlation in aggregated attri-   butions is also present in individual single layers ,   we compare the HTA maps as a baseline with the   attribution matrices extracted from different anal-   ysis methods . Figure 4 shows the correlation of   HTA attribution maps with the maps obtained by   N , N , andNmethods . The results   indicate that Nexhibits a significantly lower   association .   The question that arises here is that how incor-   porating an additional component of the encoder   ( LN#1 ) in Ndegrades the results ( compared   toN ) . To answer this question , we investigated   the learned weights of LN#1 and LN#2 . The outlier   weightsin specific dimensions of LNs are shown   to be significantly influential on the model ’s perfor-   mance ( Kovaleva et al . , 2021 ; Luo et al . , 2021 ) . It   is interesting to note that based on our observations ,   the outlier weights of the two layer norms seem to   be the opposite of each other . Figure 5 demon-   strates the weight values in layer 11 and also the   correlation of the outlier weights across layers . The   large negative correlations confirm that the outlier   weights work contrary to each other . We speculate   that the effect of outliers in the two layer norms is   partly cancelled out when both are considered .   As shown in Figure 2 , the FFN and the sec-   ond layer normalization are on top of the atten-   tion block . However , Ndoes not incorpo-   rate the components outside of the attention block .   As described in § 3 , in our local analysis method   Nwe incorporate the second layer normaliza-   tion in the transformer ’s encoder ( Figure 2 ) , thus   considering the whole encoder block ( except FFN ) .   Overall , our global method , GlobEnc , yields the   best results among all the methods evaluated in   our experiments . In general , Table 1 suggests that   incorporating each component of the encoder will   increase the correlation ; however , the two layer   normalizations should be considered together .   4.5.4 On the role of aggregation   We carried out an additional analysis to verify if   incorporating vector norms , residual connection   and layer normalizations in individual layers is ade-   quate for achieving high correlations , or if it is also   necessary to aggregate them via rollout . Table 2264shows the correlation results in different layers for   raw attributions ( without aggregation ) and for the   aggregated attributions using the rollout method .   Applying rollout method on attribution maps up to   each layer results in higher correlations with the   saliency scores than the raw single layer attribution   maps , especially in deeper layers . Therefore , atten-   tion aggregation is essential for global input token   attribution analysis .   An interesting point in Figure 3 , which shows   the correlation of the aggregated methods through-   out the layers , is that the correlation curves flatten   out after only a few layers . This indicates that   BERT identifies decisive tokens only after the first   few layers . The final layers only make minor ad-   justments to this order . Nevertheless , it is worth   noting that the order of attribution does not nec-   essarily imply the model ’s final decision and the   final result may still change for the better or worse   ( Zhou et al . , 2020 ) .   4.5.5 Qualitative analysis   To qualitatively answer if the aggregated attribu-   tion maps provide plausible and meaningful in-   terpretations , we take a closer look at the attribu-   tion maps generated by GlobEnc . Figure 1 shows   the GlobEnc attribution of the model trained on   SST-2 . Each layer demonstrates the [ CLS ] token ’s   aggregated attribution to input tokens up to the   corresponding layer . The example inputs are “ a   deep and meaningful film . ” and “ big fat waste of   time . ” , both correctly classified by the model . In   both cases , GlobEnc focuses on the relevant words   for sentiment classification , i.e. , “ meaningful ” and   “ waste ” . An interesting observation in Figure 1 is   that in the first few layers , the [ CLS ] token mostly   attends to itself while other tokens have marginal   impact . As the representations get more contextual-   ized in deeper layers , the attribution correctly shifts   to the words which indicate the sentiment of the   sentence . More examples from MNLI and SST2   datasets , including misclassified examples are avail-   able at § A.3 . Our qualitative analysis suggests that   GlobEnc can be useful for a reasonable interpreta-   tion of attention mechanism in BERT , ELECTRA ,   and possibly any other transformer - based model.5 Related Work   While numerous studies have used attention   weights to analyze and interpret the self - attention   mechanism ( Clark et al . , 2019 ; Kovaleva et al . ,   2019 ; Reif et al . , 2019 ; Htut et al . , 2019 ) , the use   of mere attention weights to explain a model ’s in-   ner workings has been an active topic of debate   ( Serrano and Smith , 2019 ; Jain and Wallace , 2019 ;   Wiegreffe and Pinter , 2019 ) . Several solutions have   been proposed to address this issue , usually through   converting raw attention weights to scores that pro-   vide better explanations . Brunner et al . ( 2020 ) used   the transformation function f(x)to introduce   effective attentions — the orthogonal component of   the attention matrix in f(x)null space — to ex-   plain the inner workings of each layer . However ,   this technique ignores other components in the en-   coder and is computationally expensive due to the   SVD required to compute the effective attentions .   Kobayashi et al . ( 2020 ) incorporated the modified   vector and introduced a vector norms - based analy-   sis . This was later extended by integrating residual   connections and layer normalization components to   enhance the accuracy of explanations ( Kobayashi   et al . , 2021 ) . But , as discussed in § 4.5 , relying   solely on LN#1 does not produce accurate results .   While these methods can be employed for single-   layer ( local ) analysis , multi - layer attributions are   not necessarily correlated with single - layer attribu-   tions due to the significant degree of information   combination through multi - layer language mod-   els ( Pascual et al . , 2021 ; Brunner et al . , 2020 ) .   Various saliency methods exist for explaining the   model ’s decision based on the input ( Li et al . , 2016 ;   Bastings and Filippova , 2020 ; Atanasova et al . ,   2020 ; Wu and Ong , 2021 ; Mohebbi et al . , 2021 ) .   However , these approaches are not primarily de-   signed for computing inter - token attributions . To   fill this gap , Brunner et al . ( 2020 ) proposed HTA ,   which is based on the gradient of each hidden em-   bedding in relation to the input embeddings . In   § 4.3.2 , we extend HTA to incorporate the impact   of the input vectors . However , HTA is extremely   computationally intensive . Attention rollout ( see   § 3 ) and attention flow — which involve solving a   max - flow problem on the attention graph — are two   aggregation approaches introduced by Abnar and   Zuidema ( 2020 ) , in which raw attention weights   ( with equally weighted residual weights ) are ag-   gregated within multiple layers . We showed that   attention rollout does not perform well on the raw265attention maps of language models fine - tuned on   downstream tasks and that this problem can be re-   solved by utilizing attribution norms .   6 Conclusions   In this work , we proposed a novel method for single   layer token attribution analysis which incorporates   the whole encoder layer , i.e. , the attention block   and the output layer normalization . When aggre-   gated across layers using the rollout method , our   technique achieves quantitatively and qualitatively   plausible results . Our evaluation of different analy-   sis methods provided evidence on roles played by   individual components of the encoder layer , i.e. ,   the vector norms , the residual connections , and the   layer normalizations . Furthermore , our in - depth   analysis suggested that the two layer normaliza-   tions in the encoder layer counteract each other ;   hence , it is important to couple them for an accu-   rate analysis .   Additionally , using a newly proposed and im-   proved version of Hidden Token Attribution , we   demonstrated that encoder - based attribution analy-   sis is more accurate when compared to other partial   solutions in a single layer ( local - level ) . This is con-   sistent with our global observations . Quantifying   global input token attribution based on our work   can provide a meaningful explanation of the whole   model ’s behavior . In future work , we plan to apply   our global analysis method on various datasets and   models , to provide valuable insights into model   decisions and interpretability .   References266267   A Appendix   A.1 LN Formulation   m(a ) : = /summationtexta ,   s(a ) : = /radicalig / summationtext(m(a)−a+ϵ )   where ϵis a small constant   A.2 More Models   In this section we provide the results for BERT-   large and ELECTRA - base . For both models , our   method outperforms the previous analysis methods .   The results are reported in Tables A.1 and A.2 .   A.3 More Examples   Aggregated attributions by different methods   throughout layers is shown in Figure A.2 . Our   proposed method shows more plausible results .   Aggregated attribution map for layer 12 is shown   in Figure A.3 . In this figure , the effect of each   token can be seen on all other tokens and not just   the [ CLS ] token .   More examples for MNLI dataset are shown for   BERT - base in Figure A.4 , for BERT - large in Fig-   ure A.6 , and for ELECTRA in Figure A.5 . More-   over , misclassified examples of SST2 dataset are   shown in Figure A.1.268BERT - large Attention Rollout   SST2 MNLI HX   Weight - based −0.38 ± 0.16 −0.61 ± 0.14 −0.41 ± 0.25   w/ Fixed Residual −0.25 ± 0.19 −0.48 ± 0.19 −0.21 ± 0.30   w/ Residual −0.10 ± 0.21 0.33 ± 0.23 0.09 ± 0.30   Norm - based 0.44 ± 0.24 0.13 ± 0.27 0.48 ± 0.25   w/ Fixed Residual 0.49 ± 0.24 0.26 ± 0.25 0.49 ± 0.30   w/ Residual 0.77 ± 0.11 0.66 ± 0.12 0.73 ± 0.16   w/ Residual + Layer Norm 1 −0.07 ± 0.23 −0.35 ± 0.24 0.06 ± 0.32   w / GlobEnc : [ Residual + Layer Norm 1 , 2]0.83 ± 0.08 0.77 ± 0.09 0.76 ± 0.17   ELECTRA - base Attention Rollout   SST2 MNLI HX   Weight - based −0.37 ± 0.19 −0.31 ± 0.22 0.02 ± 0.29   w/ Fixed Residual −0.37 ± 0.19 −0.24 ± 0.23 0.01 ± 0.29   w/ Residual −0.10 ± 0.22 0.08 ± 0.25 0.20 ± 0.27   Norm - based 0.18 ± 0.21 0.12 ± 0.21 0.21 ± 0.26   w/ Fixed Residual 0.23 ± 0.22 0.32 ± 0.23 0.28 ± 0.26   w/ Residual 0.54 ± 0.17 0.54 ± 0.14 0.44 ± 0.21   w/ Residual + Layer Norm 1 −0.24 ± 0.23 −0.16 ± 0.24 −0.07 ± 0.28   w / GlobEnc : [ Residual + Layer Norm 1 , 2]0.64 ± 0.15 0.68 ± 0.12 0.47 ± 0.22269270271