  Yuning Mao , Lambert Mathias , Rui Hou , Amjad Almahairi ,   Hao Ma , Jiawei Han , Wen - tau Yih , Madian KhabsaUniversity of Illinois Urbana - Champaign { yuningm2 , hanj}@illinois.eduMeta AI { mathiasl , rayhou , aalmah , haom , scottyih , mkhabsa}@fb.com   Abstract   Recent parameter - efﬁcient language model   tuning ( PELT ) methods manage to match the   performance of ﬁne - tuning with much fewer   trainable parameters and perform especially   well when training data is limited . However ,   different PELT methods may perform rather   differently on the same task , making it non-   trivial to select the most appropriate method   for a speciﬁc task , especially considering the   fast - growing number of new PELT methods   and tasks . In light of model diversity and   the difﬁculty of model selection , we propose a   uniﬁed framework , UPELT , which incorpo-   rates different PELT methods as submodules   and learns to activate the ones that best suit   the current data or task setup via gating mech-   anism . On the GLUE benchmark , UPELT   consistently achieves 1~4 % gains compared to   the best individual PELT method that it incor-   porates and outperforms ﬁne - tuning under dif-   ferent setups . Moreover , UPELT generally   surpasses the upper bound that takes the best   performance of all its submodules used indi-   vidually on each task , indicating that a mixture   of multiple PELT methods may be inherently   more effective than single methods .   1 Introduction   As pre - trained language models ( PLMs ) ( Devlin   et al . , 2019 ) grow larger and larger ( Brown et al . ,   2020 ) , it becomes increasingly infeasible to per-   form conventional ﬁne - tuning , where separate repli-   cas of the model parameters are modiﬁed per single   task . To solve the issue , there has recently been   a surge of studies on parameter- efﬁcient language   model tuning ( PELT ) , namely how to effectively   tune the PLMs with fewer trainable parameters .   Existing PELT research generally aims at achiev-   ing performance comparable to ﬁne - tuning withFigure 1 : Illustration of UPELT , which subsumes   existing PELT methods as submodules and controls   them via gating mechanism G. Different ( combinations   of ) submodules can be activated for different samples .   Thetrainable parameters are shown in blue .   as few trainable parameters as possible , which has   seen signiﬁcant progress – the task - speciﬁc train-   able parameters used in most recent approaches   ( Lester et al . , 2021 ; Guo et al . , 2021 ) are almost   negligible compared to the total parameters of the   PLM ( < 1 % ) . A more challenging yet less studied   problem is whether one can achieve better perfor-   mance than ﬁne - tuning with fewer parameters . Re-   cent studies ( He et al . , 2021 ; Li and Liang , 2021 ;   Karimi Mahabadi et al . , 2021b ) ﬁnd that some   PELT methods are more effective than ﬁne - tuning   on certain tasks when training data is limited , possi-   bly due to the reduced risk of overﬁtting . However ,   as found in our experiments ( Table 1 ) , different   PELT methods exhibit diverse characteristics and   perform rather differently on the same task , which6253makes it nontrivial to select the most appropriate   method for a speciﬁc task , especially considering   the fast - growing number of new PELT methods   and tasks ( Ding and Hu , 2021 ) .   In light of the diverse performance of PELT   methods and the cost of selecting the best method ,   we propose a uniﬁed PELT framework , named   UPELT , which incorporates different PELT   methods as submodules and learns to dynamically   activate the ( combination of ) submodules that best   suit the current data or task setup . As a result ,   model selection is no longer needed and consis-   tently better performance is achieved under dif-   ferent setups . The activation of each submodule   inUPELT is controlled by gating mechanism ,   which learns to favor ( assign more weight to ) the   submodules that positively contribute to a given   task . In addition , since the number of parameters   introduced by each submodule is generally small ,   combining multiple methods leads to negligible   losses in model efﬁciency .   We select four representative PELT methods for   our study – adapter ( Houlsby et al . , 2019 ) , preﬁx-   tuning ( Li and Liang , 2021 ) , LoRA ( Hu et al . ,   2021 ) , and BitFit ( Ben Zaken et al . , 2021 ) , which   largely cover the major categories of PELT meth-   ods . We perform two sets of analysis that carefully   examines ( i ) the characteristics of individual PELT   methods and ( ii ) their effectiveness when coordi-   nated by UPELT under various setups .   Extensive experiments on the GLUE bench-   mark ( Wang et al . , 2019 ) , with 32 setups ( 8 tasks   4 data sizes ) and 1,000 + runs , not only reveal the   diverse behavior of PELT methods , but also show   thatUPELT is more effective and robust than   using each method alone in various task and data se-   tups . Speciﬁcally , UPELT consistently improves   the best submodule that it incorporates by 1~4   points and even outperforms ﬁne - tuning , achieving   the best average performance on the GLUE bench-   mark under different setups . Moreover , UPELT   generally surpasses the upper bound that takes the   best performance of all its submodules used individ-   ually on each task , which suggests that UPELT   maintains ( near ) optimal performance under differ-   ent setups . The fact that UPELT outperforms the   upper bound also implies that a mixture of PELT   methods involving different parts of the PLM ar-   chitecture may be inherently more effective thanindividual methods .   Contributions . ( 1 ) We conduct a comprehensive   study of representative PELT methods and care-   fully examine their differences and commonalities   in terms of performance and characteristics . ( 2 )   We propose a uniﬁed PELT framework that can   incorporate existing methods as submodules and   automatically learn to activate the appropriate sub-   modules for a given task . ( 3 ) Our proposed frame-   work achieves better average performance than ﬁne-   tuning and the PELT methods that it incorporates   under various setups , often performing the best and   never the worst at per - task level , exhibiting supe-   rior effectiveness and robustness with negligible   losses in model efﬁciency .   2 Preliminaries   2.1 PELT Methods without Additional   Parameters   PLMs can be used as feature extractors where only   the top layers or prediction head are ﬁne - tuned   without additional parameters ( Lee et al . , 2019 ) .   However , such ﬁne - tuning approaches generally   lead to degenerate model performance that is much   worse than ﬁne - tuning all parameters ( Lee et al . ,   2019 ; Pfeiffer et al . , 2021 ) . A recent method BitFit   ( Ben Zaken et al . , 2021 ) only tunes the bias terms   of the PLM and is shown to achieve performance   comparable to ﬁne - tuning on certain tasks when   training data is limited . Therefore , we select BitFit   as the representative of this category for analysis .   2.2 PELT Methods with Additional   Parameters   Alternatively , one may ﬁx the entire PLM and intro-   duce a small number of new trainable parameters .   Notable examples in this category include adapter   ( Houlsby et al . , 2019 ) and its extensions ( Pfeif-   fer et al . , 2021 ; Karimi Mahabadi et al . , 2021b ) ,   preﬁx - tuning ( Li and Liang , 2021 ) and its exten-   sions ( Lester et al . , 2021 ) , and additive methods   ( Guo et al . , 2021 ; Hu et al . , 2021 ) .   Next , we will brieﬂy describe these methods to   facilitate the introduction of our proposed frame-   work . An illustration is shown in Fig . 1 for better   understanding .   Adapter . Adapter ( Houlsby et al . , 2019 ) adds a   trainable bottleneck layer after the feedforward net-   work in each Transformer layer of the PLM . A bot-   tleneck layer consists of a down+up projection pair   that shrinks and recovers the size of token hidden6254states . Mathematically , if we denote the output of   thefeedforward network after residual connection   and layer normalization as hwith hidden size   D and bottleneck size D , then the output   of a bottleneck layer his :   h = W  ( Wh ) ; ( 1 )   where W2R , W2   R ,  is a nonlinear activation function ,   and the bias terms are omitted for brevity . The pa-   rameters in layer normalization and the ﬁnal predic-   tion head sometimes are also ﬁne - tuned depending   on the speciﬁc adapter variants .   Adapter has shown to be on par with ﬁne - tuning   and sometimes exhibits better effectiveness in the   low - resource setting ( He et al . , 2021 ) . Later stud-   ies extend adapter to multi - lingual ( Pfeiffer et al . ,   2020b ) and multi - task ( Karimi Mahabadi et al . ,   2021b ) settings , or further reduce its trainable pa-   rameters ( Karimi Mahabadi et al . , 2021a ) , which   can be easily incorporated into UPELT as a re-   placement of the vanilla adapter .   Preﬁx - tuning . Preﬁx - tuning ( Li and Liang , 2021 )   prepends a number of task - speciﬁc trainable vec-   tors to the input of multi - head attention in each   Transformer layer , which the original tokens can at-   tend to as if they were virtual tokens . Speciﬁcally ,   we denote the original sequence length L , the   number of trainable vectors ( i.e. , preﬁx length ) L ,   and the Transformer layer input h2R.   First , three linear projections W , W , W2   R transform hinto Query Q , Key   K , and Value V. Then , two preﬁx matrices P   andP2Rare prepended to KandV.   To stabilize optimization , the preﬁx matrix Pis   reparameterized by a feedforward network :   P = W  ( WP ) ; ( 2 )   where W2R , W2   R , andNdenotes the number   of Transformer layers . The parameters of this   network can be discarded after training , and only   2Npreﬁx matrices2Rare needed ( 2   matrices for each layer ) .   Preﬁx - tuning is originally evaluated on natural   language generation and we adapt it to understand-   ing tasks . A follow - up method named prompt-   tuning ( Lester et al . , 2021 ) further reduces task-   speciﬁc parameters by limiting the preﬁx to the   ﬁrst layer but only performs competitively withvery large model sizes ( billions of total parame-   ters ) , and is thus not considered in our study . Note   that preﬁx - tuning ( or prompt - tuning ) is different   from prompt - based ﬁne - tuning methods ( Schick   and Schütze , 2021 ; Gao et al . , 2021 ) ( see App . A   for speciﬁc differences ) .   Additive Methods . Additive PELT methods treat   the model parameters after ﬁne - tuning as an ad-   dition of the pre - trained parameters  and   task - speciﬁc differences  , where is   ﬁxed and a new ( sub)set of model parameters are   added on top : = + . There are   various ways to parameterize  , leading to dif-   ferent additive methods such as LoRA ( Hu et al . ,   2021 ) , diff pruning ( Guo et al . , 2021 ) , and side-   tuning ( Zhang et al . , 2020 ) . We take LoRA as a   representative and incorporate it into UPELT .   Other methods are conceptually similar and can be   incorporated in the same fashion .   LoRA introduces trainable low - rank matrices   and combines them with the original matrices   in the multi - head attention . Speciﬁcally , two   matrices W2RandW2   Rare added for the query and key pro-   jections along with the original matrix Wand   W2R :   Q= ( W+  WW)h ; ( 3 )   where  is a ﬁxed scalar hyperparameter for scaling   the task - speciﬁc differences . The form of the train-   able matrices in LoRA is quite similar to those in   adapter or preﬁx - tuning , but there is no activation   function  in between .   3 Unifying PELT Methods   3.1 Task Formulation   Given a large PLM Mwith sizejMj that can not be   ﬁne - tuned directly due to computational or storage   cost , suppose that we have a list of PELT methods   fmg , the trainable parameters of which are negli-   gible ( i.e. ,Pjmj  jMj ) , our goal is to design a   uniﬁed PELT framework that incorporates fmgas   submodules and learns to dynamically activate ( up-   weight ) different submodules when appropriate un-   der different scenarios , such that one could achieve   satisfactory results in terms of both model effective-   ness and robustness without the hassle of permuting   all the methodtaskdata combinations.62553.2 Proposed Method   Motivation & Intuition . During the analysis of   individual PELT methods , we observe that differ-   ent PELT methods exhibit diverse characteristics   and perform rather differently on the same task .   For example , preﬁx - tuning generally performs well   on natural language inference tasks regardless of   the size of training data . Also , as can be seen in   Fig . 1 and Sec . 2 , different PELT methods often in-   volve different parts of the PLM architecture ( e.g. ,   before multi - head attention for preﬁx - tuning and   after feedforward layer for adapter ) , making it fea-   sible to combine multiple PELT methods without   ( directly ) interfering with each other .   In light of the two observations above , we pro-   pose a uniﬁed PELT framework , UPELT , which   takes a hybrid approach by incorporating multi-   ple PELT methods as submodules . At a high level ,   UPELT improves over single PELT methods due   to two factors . First , UPELT learns to activate   ( upweight ) the submodules that best suit the current   task or speciﬁc data sample and deactivate ( down-   weight ) the rest . Second , we ﬁnd that UPELT   generally performs better than taking the best per-   formance of all its submodules used individually   on each task , suggesting that there could be some   compounding effects that lead to better model effec-   tiveness when multiple PELT methods ( that modify   different parts of the PLM ) are used .   Next , we will introduce how different PELT   methods can be incorporated into UPELT via   gating mechanism .   Gating Mechanism . To achieve ﬁne - grained con-   trol of submodule ( de)activation , we add a trainable   gateGfor each submodule m2fA , P , Lgin   every Transformer layer ( see Fig . 1 ) . The letters A ,   P , L stand for Adapter , Preﬁx - tuning , and LoRA ,   respectively . Intuitively , if mis useful for a given   datatask setup ( or a particular instance ) , the gate   output formwould be higher such that mplays   a more important role . The actual interplay of sub-   modules , however , is more complicated given the   interdependency of the submodules and the com-   pounding effects of multiple layers .   Speciﬁcally , for adapter , there is a residual con-   nection between the feedforward network and the   adapter submodule that sums the adapter input ( be-   fore normalization ) hand output has its ﬁnal   output : h = h+h . We design a gating func-   tionG2(0;1)that estimates the importance of   adapter by its direct input husing a feedforwardnetwork with sigmoid activation and then scales its   output : h = Gh+h . The adapter submodule   is effectively bypassed if G0 .   Similarly , for preﬁx - tuning , we design a gating   functionG2(0;1)that is applied to the preﬁx   vectors ( PandP ) with the representation of the   original tokens ( KandV ) intact . In this way , the   impact of the preﬁx would be diminished if the gate   output of the preﬁx - tuning submodule is low . The   gating functionGis estimated by the Transformer   layer input hwith another feedforward network .   As for LoRA , we note that there is already a   constant scaling factor  in its original design that   resembles the purpose of our gating mechanism .   We thus simply make the factor learnable per layer   by a third feedforward network that takes has   input instead of specifying a constant manually :   = + G.   Despite the seeming simplicity of UPELT ,   we note that it is nontrivial for a uniﬁed approach   to work well under different scenarios . Naively   combining different PELT methods as a hybrid ap-   proach could lead to mixed or worse performance   than using individual methods , as observed in both   our experiments and prior studies ( Hu et al . , 2021 ) .   4 Experiments   We conduct extensive experiments with 8 tasks    4 data sizes7 methods5 runs per setup , along   with additional analysis for particular methods , re-   sulting in 1,000 + runs in total .   4.1 Experiment Setup   Task Setup . We conduct experiments on the Gen-   eral Language Understanding Evaluation ( GLUE )   benchmark ( Wang et al . , 2019 ) , which involves   four types of natural language understanding tasks   including linguistic acceptability ( CoLA ) , senti-   ment analysis ( SST-2 ) , similarity and paraphrase   tasks ( MRPC , STS - B , QQP ) , and natural language   inference ( MNLI , QNLI , RTE ) . We exclude the   WNLI dataset following prior studies ( Houlsby   et al . , 2019 ; Devlin et al . , 2019 ) .   Data Setup . We mainly consider a low - resource   setting where training data is limited and the per-   formance of different methods varies much . We   sample a small subset of the training set for each   task with size K = f100;500;1000 g. As it is in-   feasible to submit considerable runs to the GLUE6256   leaderboard ( 2 submissions / day ) , we take 1,000   samples on the training set as the development set   to select the best checkpoint and use the original   development set as the test set . To reduce variance ,   we shufﬂe the data with 5 random seeds and re-   port the average performance . Additionally , we   consider a high - resource setting where the whole   training set is used and the best performance on the   GLUE development set is reported .   Compared Methods . We mainly compare   UPELT with ﬁne - tuning and four representa-   tive PELT methods : adapter ( Houlsby et al . , 2019 ) ,   preﬁx - tuning ( Li and Liang , 2021 ) , BitFit ( Ben Za-   ken et al . , 2021 ) , and LoRA ( Hu et al . , 2021 ) .   For completeness , we consider two model vari-   ants UPELT ( AP ) and UPELT ( APL ) , which   incorporate 2 and 3 PELT methods , respectively .   Implementation Details . We use BERT(De-   vlin et al . , 2019 ) as the base model in the experi-   ments . Consistent results are observed in our pre-   liminary experiments with BART(Lewis et al . ,   2020 ) ( provided in App . C ) . We implement and   evaluate all the methods in the same codebase to   ensure a fair comparison . We largely follow thedefault hyperparameters of different methods and   keep them the same on all the tasks for generaliz-   ability . We set the preﬁx length L= 10 , adapter   bottleneck size D= 48 , LoRA rank D= 8   if not speciﬁed otherwise . More implementation   and hyperparameter details can be found in App . B.   4.2 Analysis of Individual PELT Methods   In Table 1 , we show the performance of different   methods on the GLUE benchmark with various   sizes of training data . The results on the devel-   opment sets are generally consistent with the test   sets and provided in App . D. Although the average   performance of different methods over 8 tasks is   sometimes similar , the differences between tasks   are quite signiﬁcant under certain setups and can   be as large as 5~9 points on a speciﬁc task ( e.g. ,   STS - B and MNLI , K= 500 ) even when excluding   cases where some methods fail to learn effectively   ( e.g. , preﬁx - tuning on QQP , K= 100 ) .6257Next , we will analyze and examine each individ-   ual PELT method more closely .   Analysis of Adapter . The performance of adapter   is relatively stable – there is no signiﬁcantly bet-   ter or worse result than ﬁne - tuning consistent on   different tasks or sizes of training data . In gen-   eral , adapter is slightly worse than ﬁne - tuning in   most cases . We do not observe that adapter consis-   tently outperforms ﬁne - tuning in the low - resource   setting as in He et al . ( 2021 ) , possibly because   they tune model hyperparameters on each task ,   which could be computationally prohibitive when   there are considerable tasks . For example , they   choose the bottleneck size Dfrom { 64 , 128 ,   256 } , while D= 48 is ﬁxed across tasks in our   experiments . Also , we only add one adapter in   each Transformer layer instead of two following   Pfeiffer et al . ( 2021 ) . These two differences result   in 62.4%~90.5 % fewer parameters than the adapter   used in He et al . ( 2021 ) .   To further study the effect of bottleneck size   Din adapter , we increase Dand re - evaluate   adapter on a setup that it performs poorly ( CoLA ,   K= 100 ) . As shown in Fig . 2 , the performance   of adapter is increased gradually and becomes sig-   niﬁcantly better only when D= 256 , which in-   volves 5.3trainable parameters than the adapter   used originally ( D= 48 ) , 4.3than UPELT   ( AP ) , and 3.4than UPELT ( APL ) , suggest-   ing that a larger bottleneck size could be beneﬁcial   when adapter learns ineffectively .   On the other hand , there are certain tasks ( e.g. ,   STS - B ) that adapter largely outperforms compet-   itive methods such as preﬁx - tuning and LoRA re-   gardless of the size of training data , suggesting that   one should favor adapter over other PELT methods   under certain scenarios as well .   Analysis of Preﬁx - tuning . Preﬁx - tuning performspoorly with K = f100;500gand becomes on par   with ﬁne - tuning when Kreaches 1000 . We also ob-   serve that preﬁx - tuning fails to learn effectively on   certain tasks when the training data is limited ( e.g. ,   K= 100 on SST-2 and K= 500 on QQP ) , lead-   ing to unsatisfactory performance and ( or ) large   variance across different runs . Similar phenomena   have been observed in a concurrent study ( Gu et al . ,   2021 ) on few - shot prompt - tuning .   To ensure that the poor performance of preﬁx-   tuning is not due to its fewer trainable parameters   ( based on its default setting ) , we further increase   the preﬁx length to L= 50 such that its train-   able parameters are comparable to adapter , and re-   evaluate preﬁx - tuning on all 8 tasks with K= 100 .   For the 4 tasks where preﬁx - tuning ( L= 10 ) per-   forms poorly ( SST2 , CoLA , STS - B , and QQP ) ,   while its performance is signiﬁcantly improved on   3 tasks , it also performs signiﬁcantly worse on the   other task ( STS - B ) , which suggests that training   instability in the low - resource regime is still an   issue for preﬁx - tuning even with more trainable   parameters . Besides , preﬁx - tuning ( L= 50 ) still   lags behind adapter or UPELT ( AP ) on 3 of the   4 tasks . Furthermore , the average performance of   preﬁx - tuning ( L= 50 ) on 8 tasks is even slightly   worse than with L= 10 , which indicates that in-   creasing preﬁx length may not be a panacea for   all the scenarios . A larger Lalso leads to signiﬁ-   ca nt training / inference slowdown due to the costly   multi - head attention . More broadly , such results   suggest that using more trainable parameters does   not guarantee better performance .   On the bright side , preﬁx - tuning performs well   on certain tasks such as natural language inference   ( RTE and MNLI ) with various sizes of training   data , which suggests that one should also prefer   preﬁx - tuning in certain cases .   Analysis of BitFit & LoRA . Tuning only the bias   terms of the model does not lead to very satisfac-   tory results in our experiments – BitFit never per-   forms the best and generally performs the worst in   different data and task setups . Therefore , we do   not consider BitFit in the following experiments   and exclude BitFit as a submodule of UPELT .   As for LoRA , there are a few setups where LoRA   fails to learn effectively as well , such as STS - B   and QQP ( K = f100;500 g ) , leading to high vari-   ance across runs . Apart from that , LoRA performs6258   quite competitively despite using fewer trainable   parameters than methods like adapter , especially   whenK= 1000 , achieving the best or 2nd best   performance on 4 of 8 tasks .   As LoRA has a scaling factor  that can be seen   as a static gating function under our formulation ,   we further investigate its importance by evaluating   LoRA with different  . As shown in Fig . 3 , LoRA   is quite sensitive to the scaling factor and there   seems to be no single optimal value that works   well across multiple task and data setups . Such   ﬁndings suggest that gating is critical and motivate   us to use more ﬁne - grained and dynamic control   forUPELT . Besides , we observe that increasing   consistently results in faster convergence , possi-   bly because the trainable parameters would receive   larger gradient updates with a larger  .   4.3 Analysis of UPELT   Next , we will turn to our proposed framework   UPELT , which incorporates multiple existing   PELT methods as submodules .   Low - Resource Performance . Overall , UPELT   ( APL ) and UPELT ( AP ) consistently achieve   the best and second best average performance on   both the development and test sets regardless of   the number of training samples . The gains are   generally 1~4 % over the submodule that performs   the best ( when used individually ) . Such results   demonstrate the advantages of our hybrid approach   regarding model effectiveness and generalizability .   At the per - task level , UPELT ( APL ) and   UPELT ( AP ) perform the best or second best on   7/6/7 of 8 tasks when trained with 100/500/1,000   samples , and never perform the worst in any setup .   When comparing the two variants , UPELT(APL ) outperforms UPELT ( AP ) on 4/6/8 of   8 tasks when trained with 100/500/1,000 samples .   Such results indicate that UPELT is quite ro-   bust and performs reliably under different scenar-   ios . The improvements of UPELT over its sub-   modules are generally larger when having fewer   training samples , suggesting that UPELT per-   forms especially well in the low - resource regime .   In particular , on the tasks where other PELT meth-   ods fail to learn effectively such as CoLA and QQP   ( K= 100 ) , UPELT manages to achieve perfor-   mance better than ﬁne - tuning .   UPELT vs. Upper Bound . In Table 2 , we   show the comparison of UPELT and the up-   per bound that takes the best performance of its   submodules on each task . We observe that both   UPELT ( AP ) and UPELT ( APL ) perform   similarly or even better than their upper bound ,   which suggests that UPELT successfully learns   to leverage different submodules and maintains   ( near ) optimal performance under different setups .   The fact that UPELT can outperform the upper   bound also hints that a mixture of PELT methods   ( involving different parts of the PLM ) might be in-   herently more effective than single methods ( with   a limited scope of the PLM architecture ) .   High - Resource Performance . In Table 3 , we   list the performance of different methods when   all training samples are used . UPELT again   achieves the best overall performance . The gains   are not as signiﬁcant as in the low - resource set-   ting , which is somewhat expected as existing PELT   methods typically perform on par with ﬁne - tuning   given abundant training data and the potential of   improvement is not as high . That said , the perfor-   mance of UPELT is still the best or 2nd best on   all 8 tasks , and generally comparable to the best   submodule used individually on each task . Besides ,   simply combining multiple PELT methods without   gating does not work well in the high - resource set-   ting – although UPELT -NoGate never performs   the worst in each task , its average performance is   unsatisfactory ( -0.89 vs. UPELT).6259   4.4 Efﬁciency of PELT Methods   We benchmark the efﬁciency of PELT methods and   list in Table 4 their number of trainable parameters   and training / inference time relative to ﬁne - tuning .   Parameter Efﬁciency . As the trainable parame-   ters in PELT methods are almost negligible , com-   bining multiple methods does not lead to signiﬁcant   losses in parameter efﬁciency . UPELT still has   few trainable parameters compared to ﬁne - tuning   ( 0.99%~1.26 % ) . The parameters can be further re-   duced if one uses more parameter - efﬁcient variants   ( e.g. , Karimi Mahabadi et al . ( 2021a ) ) , which can   be easily swapped with the vanilla version used in   our current framework . Also , note that more train-   able parameters do not always lead to better per-   formance , as shown in our experiments and prior   studies ( He et al . , 2021 ; Pfeiffer et al . , 2021 ) .   Training and Inference Efﬁciency . Due to   parameter efﬁciency , all PELT methods train   30%~50 % faster than ﬁne - tuning and incorporating   multiple PELT methods into UPELT does not   suffer from slower training . On the other hand , the   inference time of PELT methods is generally longer   since they involve more FLOPs . UPELT has a   slightly larger inference overhead ( 4%~11 % com-   pared to its slowest submodule ) , which we argue is   insigniﬁcant since larger models that may achieve   similar performance gains ( e.g. , BERT ) needaround 300 % inference time ( Wolf et al . , 2020 ) .   5 Related Work   Parameter - Efﬁcient Tuning of PLMs . As it is   increasingly infeasible to train and store full copies   of large PLMs for various downstream tasks , how   to efﬁciently tune the PLMs with few trainable pa-   rameters becomes critical . Existing PELT methods   can be largely divided into two categories based on   whether new trainable parameters are introduced .   Speciﬁcally , one may either train a subset of the   model parameters such as the prediction head ( Lee   et al . , 2019 ) and bias terms ( Ben Zaken et al . , 2021 ) ,   or introduce task - speciﬁc parameters to different   parts of the PLM such as before multi - head at-   tention ( Li and Liang , 2021 ) or after feedforward   layer ( Houlsby et al . , 2019 ) . As the number of   PELT methods keeps increasing , the purpose of   UPELT is to better understand and leverage the   distinctions of various methods instead of propos-   ing yet another method .   Mixture - of - Experts .UPELT is also related to   approaches that involve a high - capacity network   and activate ( upweight ) different parts of the net-   work given different inputs . One notable example   is Mixture - of - Experts ( MoE ) ( Jacobs et al . , 1991 ;   Shazeer et al . , 2017 ) , which maintains a set of ex-   perts ( neural networks ) and one or more trainable   gates that select a combination of the experts spe-   ciﬁc to each input . Despite being conceptually   similar , UPELT is different from MoE : the sub-   modules in UPELT are not combined explicitly   by summation like MoE but in sequential order   and affect each other implicitly . Moreover , the   “ experts ” are diverse in UPELT while usually   homogeneous or identical in MoE methods .   6 Conclusion   In this paper , we present a comprehensive study of   representative parameter - efﬁcient language model6260tuning ( PELT ) methods and propose a uniﬁed   framework , which incorporates different PELT   methods as submodules and learns to activate the   most appropriate submodules for a given task or   data setup . Our proposed framework consistently   outperforms conventional ﬁne - tuning as well as the   submodules that it incorporates under different se-   tups , and generally surpasses the upper bound that   takes the best performance of each submodule used   individually on each task . Our ﬁndings suggest that   a mixture of multiple PELT methods that involve   different parts of the PLM may be favorable regard-   ing both model effectiveness and robustness . For   future work , we will try to better understand the   discrepancy of various PELT methods in different   scenarios . We also plan to investigate a multi - task   setting where multiple submodules can be activated   and cooperate at the task level .   Acknowledgements   We thank Xiang Lisa Li , Hai Ye , Rabeeh Karimi   Mahabadi , Junxian He , Yiqing Xie , Yaqing Wang ,   and Liyuan Liu for helpful discussions and feed-   back . We thank anonymous reviewers for valuable   comments and suggestions .   References62616262A Preﬁx - tuning vs. Prompt - based   Fine - tuning   We note that preﬁx - tuning ( or prompt - tuning )   is different from prompt - based ﬁne - tuning meth-   ods ( Schick and Schütze , 2021 ; Gao et al . , 2021 )   in many ways : ( 1 ) Prompt - based ﬁne - tuning is not   parameter - efﬁcient as it updates all model param-   eters while preﬁx - tuning only updates the preﬁx   matrix P. ( 2 ) The prompts are only used in model   input for prompt - based ﬁne - tuning but added to   every Transformer layer in preﬁx - tuning ( stored as   different vectors ) . ( 3 ) Prompt - based ﬁne - tuning   typically leverages carefully designed natural lan-   guage prompts while preﬁx - tuning uses continuous   prompts ( virtual tokens ) .   B Implementation Details   Data Preparation . We shufﬂe the training set with   seeds , take the ﬁrst Ksamples as the new training   set , and the next 1,000 samples as the development   set . We use s = f111;222;333;444;555gas the   data seeds and the same seed ( s= 42 ) for model   training . We also conduct another set of prelim-   inary experiments by ﬁxing the data and using 5   different random seeds for model training , the re-   sults of which are similar ( Table 5 ) .   Hyperparameters . We adopt AdapterHub ( Pfeif-   fer et al . , 2020a ) , a library based on HuggingFace   Transformers ( Wolf et al . , 2019 ) , as our codebase .   We largely follow the recommended hyperparame-   ters used in different methods for a fair comparison .   We set the input length to 128 and the training   batch size to 16 . We set the number of epochs to   50 and adopt early stopping with a patience of 10   non - increasing epochs . We set the learning rate of   ﬁne - tuning and adapter to 2e-5 and 1e-4 according   to the ﬁndings in prior studies ( Pfeiffer et al . , 2020a ;   He et al . , 2021 ) . For preﬁx - tuning and UPELT ,   as they are not previously evaluated on NLU tasks ,   we tune their learning rates from { 1e-4 , 2e-4 , 5e-4 }   on the development set and set to 2e-4 and 5e-4 ,   respectively . For BitFit and LoRA , we choose the   learning rates commonly used in their own experi-   ments ( 1e-4 and 5e-4 , respectively ) . We set  = 2   andr= 8 in LoRA according to its ofﬁcial scripts .   C BART Results   In our preliminary experiments , we also evaluated   UPELT on BART(Lewis et al . , 2020 ) . We   show the results of ﬁne - tuning , adapter , preﬁx-   tuning , and UPELT ( AP ) in Table 5 . 1000 train-   ing examples are used and the average best perfor-   mance on the GLUE development set is reported   ( excluding QQP ) . The results are largely consistent   with those on BERT.UPELT again achieves   the best performance with notably smaller variance .   D Detailed Performance   In Table 6 , we list the detailed results on both devel-   opment and test sets of the GLUE benchmark . The   observations and ﬁndings are largely consistent on   the two evaluation splits.62636264