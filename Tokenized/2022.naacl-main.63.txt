  Ann Lee , Hongyu Gong , Paul - Ambroise Duquenne , Holger Schwenk ,   Peng - Jen Chen , Changhan Wang , Sravya Popuri , Yossi Adi ,   Juan Pino , Jiatao Gu , Wei - Ning Hsu   Meta AI   { annl,wnhsu}@fb.com   Abstract   We present a textless speech - to - speech trans-   lation ( S2ST ) system that can translate speech   from one language into another language and   can be built without the need of any text data .   Different from existing work in the literature ,   we tackle the challenge in modeling multi-   speaker target speech and train the systems with   real - world S2ST data . The key to our approach   is a self - supervised unit - based speech normal-   ization technique , which finetunes a pre - trained   speech encoder with paired audios from multi-   ple speakers and a single reference speaker to   reduce the variations due to accents , while pre-   serving the lexical content . With only 10 min-   utes of paired data for speech normalization , we   obtain on average 3.2 BLEU gain when train-   ing the S2ST model on the V oxPopuli S2ST   dataset , compared to a baseline trained on un-   normalized speech target . We also incorporate   automatically mined S2ST data and show an   additional 2.0 BLEU gain . To our knowledge ,   we are the first to establish a textless S2ST tech-   nique that can be trained with real - world data   and works for multiple language pairs .   1 Introduction   Speech - to - speech translation ( S2ST ) technology   can help bridge the communication gap between   people speaking different languages . Conventional   S2ST systems ( Lavie et al . , 1997 ; Nakamura et al . ,   2006 ) usually rely on a cascaded approach by first   translating speech into text in the target language ,   either with automatic speech recognition ( ASR ) fol-   lowed by machine tranlsation ( MT ) , or an end - to-   end speech - to - text translation ( S2 T ) model ( Bérard   et al . , 2016 ) , and then applying text - to - speech   ( TTS ) synthesis to generate speech output .   On the other hand , researchers have started ex-   ploring direct S2ST ( Jia et al . , 2019 , 2021 ; Tjandraet al . , 2019 ; Zhang et al . , 2020 ; Kano et al . , 2021 ;   Lee et al . , 2021 ) , which aims at translating speech   in the source language to speech in the target lan-   guage without the need of text generation as an   intermediate step . However , text transcriptions or   phoneme annotations of the speech data is often   still needed during model training for multitask   learning ( Jia et al . , 2019 ; Lee et al . , 2021 ) or for   learning a decoder that generates intermediate rep-   resentations ( Jia et al . , 2021 ; Kano et al . , 2021 ) to   facilitate the generation of speech output .   More than 40 % of the languages in the world   are without text writing systems , while very lim-   ited work exist to tackle the challenge of train-   ing direct S2ST systems without the use of any   text data ( Tjandra et al . , 2019 ; Zhang et al . , 2020 ) .   Moreover , due to the lack of S2ST training data ,   previous work on direct S2ST mainly rely on TTS   to generate synthetic target speech for model train-   ing . The recent release of the large - scale S2ST data   from V oxPopuli ( Wang et al . , 2021c ) has opened   up the possibility of conducting S2ST research on   real data . In addition , Duquenne et al . ( 2021 ) have   demonstrated the first proof of concept of direct   S2S mining without using ASR or MT systems .   The approach may potentially mitigate the data   scarcity issue , but the authors had not evaluated the   usefulness of such data for S2ST frameworks .   Most recently , Lee et al . ( 2021 ) have proposed   to take advantage of self - supervised discrete rep-   resentations ( Lakhotia et al . , 2021 ) , or discrete   units , learned from unlabeled speech data as the   target for building a direct S2ST model . Experi-   ments conducted with synthetic target speech data   have shown significant improvement for transla-   tion between unwritten languages . In this work ,   we extend the textless S2ST setup in ( Lee et al . ,   2021 ) , i.e. training an S2ST system without the use   of any text or phoneme data , and conduct exper-   iments on real S2ST datasets , including V oxPop-860uli ( Wang et al . , 2021c ) and automatically mined   S2ST data ( Duquenne et al . , 2021 ) . To tackle the   challenge of modeling real target speech where   there are multiple speakers with various accents ,   speaking styles and recording conditions , etc . , we   propose a speech normalization technique that   finetunes a self - supervised pre - trained model for   speech with a limited amount of parallel multiple-   to - single speaker speech . Experiments on four   language pairs show that when trained with the   normalized target speech obtained from a speech   normalizer trained with 10 - min parallel data , the   performance of a textless S2ST model can be im-   proved by 3.2 BLEU points on average compared   with a baseline with un - normalized target speech .   The main contributions of this work include :   •We propose a speech normalization technique   based on self - supervised discrete units that   can remove the variations in speech from mul-   tiple speakers without changing the lexical   content . We apply the technique on the tar-   get speech of real S2ST data and verify its   effectiveness in the context of textless S2ST .   •We empirically demonstrate that with the   speech normalization technique , we can fur-   ther improve a textless S2ST system ’s perfor-   mance by augmenting supervised S2ST data   with directly mined S2ST data , demonstrating   the usefulness of the latter .   •To the best of our knowledge , we are the first   to establish a textless S2ST technique that   can be trained with real - world data , and the   technique works for multiple language pairs .   2 Related work   Direct S2ST Jia et al . ( 2019 , 2021 ) propose a   sequence - to - sequence model with a speech encoder   and a spectrogram decoder that directly translates   speech from one language into another language   without generating text translation first . The model   can be trained end - to - end , while phoneme data   is required in model training . On the other hand ,   Tjandra et al . ( 2019 ) ; Zhang et al . ( 2020 ) build di-   rect S2ST systems for languages without text writ-   ing systems by adopting Vector - Quantized Vari-   ational Auto - Encoder ( VQ - V AE ) ( van den Oord   et al . , 2017 ) to convert target speech into discrete   codes and learn a speech - to - code translation model .   Most recently , Lee et al . ( 2021 ) propose a directS2ST system that predicts self - supervised discrete   representations of the target speech . The system ,   when trained without text data , outperforms VQ-   V AE - based approach in Zhang et al . ( 2020 ) . As a   result , in this work , we follow the design in Lee   et al . ( 2021 ) and focus on training direct S2ST sys-   tems with real data .   S2ST data V oxPopuli ( Wang et al . , 2021c ) pro-   vides 17.3k hours of S2ST data from European par-   liament plenary sessions and the simultaneous inter-   pretations for more than 200 language directions ,   the largest to - date . There exists few S2ST cor-   pora as the creation process requires transcribing   multilingual speech ( Tohyama et al . , 2004 ; Bendaz-   zoli et al . , 2005 ; Zanon Boito et al . , 2020 ) or high-   quality ASR models ( Wang et al . , 2021c ) . On the   other hand , Duquenne et al . ( 2021 ) extend distance-   based bitext mining ( Schwenk et al . , 2021 ) to the   audio domain by first learning a joint embedding   space for text and audio , where sentences with sim-   ilar meaning are close , independent of the modality   or language . The technique was applied to mine for   speech - to - speech alignment in LibriV ox , creating   1.4k hours of mined S2ST data for six language   pairs . The usefulness of the S2ST datasets is of-   ten showcased indirectly through a speech retrieval   task ( Zanon Boito et al . , 2020 ) or human evaluation   of the data quality ( Duquenne et al . , 2021 ) , since   existing direct S2ST systems are mostly trained   with synthetic target speech ( Jia et al . , 2019 ; Tjan-   dra et al . , 2019 ; Zhang et al . , 2020 ; Lee et al . , 2021 ;   Jia et al . , 2021 ) . In this work , we develop an S2ST   system that can be trained on real target speech to   mitigate the discrepancy between the S2ST system   and corpus development process .   Speech normalization Speech normalization re-   duces the variation of factors not specified at the   input when building TTS systems . One manual   approach is to use clean data from a single speaker   with minimal non - textual variation ( Wang et al . ,   2017 ; Shen et al . , 2018 ; Ren et al . , 2019 ; Ito and   Johnson , 2017 ) . For automatic methods , silence   removal with voice activity detection ( V AD ) is   a fundamental approach ( Gibiansky et al . , 2017 ;   Hayashi et al . , 2020 ; Wang et al . , 2021a ) . Speech   enhancement can remove the acoustic condition   variation when building TTS models with noisy   data ( Botinhao et al . , 2016 ; Adiga et al . , 2019 ) .   Speaker normalization through voice conversion,861(a)(b)(c )   which maps target speech into the same speaker   as the source speech in the context of S2ST ( Jia   et al . , 2021 ) , can be considered as another speech   normalization method . In this work , we propose   a novel speech normalization technique based on   self - supervised discrete units , which maps speech   with diverse variation to units with little non - textual   variation .   3 System   We follow Lee et al . ( 2021 ) to use HuBERT ( Hsu   et al . , 2021 ) to discretize target speech and build   a sequence - to - sequence speech - to - unit translation   ( S2UT ) model . We describe the proposed speech   normalization method and the S2UT system below .   3.1 Self - supervised Unit - based Speech   Normalization   HuBERT and discrete units Hidden - unit BERT   ( HuBERT ) ( Hsu et al . , 2021 ) takes an iterative pro-   cess for self - supervised learning for speech . In   each iteration , K - means clustering is applied on the   model ’s intermediate representations ( or the Mel-   frequency cepstral coefficient features for the first   iteration ) to generate discrete labels for comput-   ing a BERT - like ( Devlin et al . , 2019 ) loss . After   the last iteration , K - means clustering is performed   again on the training data , and the learned Kclus-   ter centroids are used to transform audio into a   sequence of cluster indices as [ z , z , ... , z ] , z∈   { 0,1 , ... , K −1},∀1≤i≤T , where Tis the num-   ber of frames . We refer to these units as orig - unit .   Unit - based speech normalization We observe   that orig - unit from audios of different speakers   speaking the same content can be quite different   due to accent and other residual variations such   as silence and recording conditions , while there   is less variation in orig - unit from speech from   the same speaker ( Figure 1 ) . Following the suc-   cess of self - supervised pre - training and Connec-   tionist Temporal Classification ( CTC ) finetuning   for ASR ( Graves et al . , 2006 ; Baevski et al . , 2019 ) ,   we propose to build a speech normalizer by per-   forming CTC finetuning with a pre - trained speech   encoder using multi - speaker speech as input and   discrete units from a reference speaker as target .   Figure 2 illustrates the process . First , a pair   of audios from a random speaker and a reference   speaker speaking the same content is required .   Then , we convert the reference speaker speech into   orig - unit with the pre - trained HuBERT model fol-   lowed by K - means clustering . We further reduce   the full orig - unit sequence by removing repeat-   ing units ( Lakhotia et al . , 2021 ; Lee et al . , 2021 ;   Kharitonov et al . , 2021 ; Kreuk et al . , 2021 ) . The862   resulting reduced orig - unit serves as the target in   the CTC finetuning stage with the speech from the   random speaker as the input .   The process can be viewed as training an ASR   model with the “ pseudo text ” , i.e. units from speech   from a single reference speaker . The resulting   speech normalizer is a discrete unit extractor that   converts the input speech to units with CTC decod-   ing . We refer to these units as norm - unit .   3.2 Textless S2ST   Figure 3 shows the main components of the system .   Speech encoder The speech encoder is built by   pre - pending a speech downsampling module to a   stack of Transformer blocks ( Vaswani et al . , 2017 ) .   The downsampling module consists of two 1D-   convolutional layers , each with stride 2 and fol-   lowed by a gated linear unit activation function ,   resulting in a downsampling factor of 4 ( Synnaeve   et al . , 2019 ) for the log - mel filterbank input .   Discrete unit decoder We train the S2UT system   with norm - unit as the target . The unit decoder is   a stack of Transformer blocks as in MT ( Vaswani   et al . , 2017 ) and is trained with cross - entropy loss   with label smoothing . The setup can be viewed   as the same as the “ reduced ” strategy in Lee et al .   ( 2021 ) , as the speech normalizer is trained on re-   duced orig - unit sequences .   Auxiliary task We follow the unwritten language   setup in Lee et al . ( 2021 ) and incorporate an auto-   encoding style auxiliary task to help the model con-   verge during training . We add a cross - attention   module and a Transformer decoder to an inter-   mediate layer of the speech encoder and use re-   duced orig - unit of the source speech as the target . Unit - based vocoder The unit - to - speech conver-   sion is done with the discrete unit - based HiFi - GAN   vocoder ( Kong et al . , 2020 ) proposed in Polyak   et al . ( 2021 ) , enhanced with a duration prediction   module ( Ren et al . , 2020 ) . The vocoder is trained   separately from the S2UT model with the com-   bination of the generator - discriminator loss from   HiFi - GAN and the mean square error ( MSE ) of   the predicted duration of each unit in logarithmic   domain .   4 Experimental Setup   We examine four language pairs : Spanish - English   ( Es - En ) , French - English ( Fr - En ) , English - Spanish   ( En - Es ) , and English - French ( En - Fr ) . All experi-   ments are conducted using fairseq ( Ott et al . ,   2019 ; Wang et al . , 2020a , 2021b ) .   4.1 Data   Multilingual HuBERT ( mHuBERT ) As we fo-   cus on modeling target speech in En , Es or Fr , we   train a single mHuBERT model ( Section 4.2 ) by   combining data from three languages . We use the   100k subset of V oxPopuli unlabeled speech ( Wang   et al . , 2021c ) , which contains 4.5k hrs of data for   En , Es and Fr , respectively , totaling 13.5k hours .   Speech normalization We use multi - speaker   speech from the V oxPopuli ASR dataset ( Wang   et al . , 2021c ) and convert text transcriptions to ref-   erence units for training the speech normalizer . The   text - to - unit ( T2U ) conversion is done with a Trans-   former MT model ( Vaswani et al . , 2017 ) trained863   on single - speaker TTS data ( described later ) with   characters as input and reduced orig - unit as target .   We build training sets of three different sizes ( 10-   min , 1 - hr , 10 - hr ) for each language ( Table 1 ) . We   remove the audios that exist in the V oxPopuli S2ST   dataset ( described later ) and randomly sample from   the Common V oice ASR dataset ( Ardila et al . ,   2020 ) if there is no enough data . We also randomly   sample 1000 audios from Common V oice dev sets   and combine with the filtered V oxPopuli ASR dev   sets for model development . Though the reference   target is created synthetically , we believe that col-   lecting a maximum of 10 - hr speech from a single   speaker is reasonable as in TTS data collection ( Ito   and Johnson , 2017 ; Park and Mulc , 2019 ) .   S2UT We use the V oxPopuli S2ST dataset ( Wang   et al . , 2021c ) as the supervised S2ST data for model   training . Take Es - En for example . We combine   data from Es source speech to En interpretation   with Es interpretation to En source speech for train-   ing . We evaluate on the dev set and test set from   Europarl - ST ( Iranzo - Sánchez et al . , 2020 ) , as it pro-   vides text translation for BLEU score computation   and is of the same domain as V oxPopuli . In addi-   tion , we investigate incorporating S2ST data auto-   matically mined from LibriV ox ( Duquenne et al . ,   2021).Table 2 summarizes the statistics of the   data for each language pair .   TTS data We train the unit - based HiFi - GAN   vocoder using TTS data , pre - processed with V AD   to remove silence at both ends of the audio . No   text data is required during vocoder training . In   addition , we use the same TTS dataset to train the   T2U model for generating reference target unitsin speech normalizer training and to build the cas-   caded baselines described in Section 4.3 .   4.2 Multilingual HuBERT ( mHuBERT )   We build a single mHuBERT model for all three   languages using the combination of 13.5k - hr data   without applying any language - dependent weights   or sampling , since the amount of data is similar   between all three languages . A single codebook   is used for all three languages , and no language   information is required during pre - training . The   mHuBERT model is pre - trained for three iterations   following Hsu et al . ( 2021 ) ; Lakhotia et al . ( 2021 ) .   In each iteration , model weights are randomly ini-   tialized and optimized for 400k steps . We find that   K= 1000 with features from the 11 - th layer of   the third - iteration mHuBERT model work the best   for our experiments .   4.3 Baselines   S2UT with reduced orig - unit First , we consider   a basic setup by training the S2UT system using   reduced orig - unit extracted from the target multi-   speaker speech with mHuBERT ( Lee et al . , 2021 ) .   For the second baseline , we concatenate a d - vector   speaker embedding ( Variani et al . , 2014 ) to each   frame of the speech encoder output to incorporate   target speaker information . A linear layer is applied   to map the concatenated feature vectors to the same   dimension as the original encoder output . The 256-   dimensional speaker embedding , which remains   fixed during the S2UT model training , is extracted   from a speaker verification model pre - trained on   V oxCeleb2 ( Chung et al . , 2018 ) . During inference ,   we use the speaker embedding averaged from all   audios from the TTS dataset of the target language .   S2T+TTS We transcribe all the S2ST data with   open - sourced ASR models ( Section 4.4 ) and train a   S2T+TTS system for each language pair . We build   2000 unigram subword units ( Kudo , 2018 ) from   the ASR decoded text as the target . For TTS , we   explore two approaches : ( 1 ) Transformer TTS ( Li   et al . , 2019 ) , and ( 2 ) text - to - unit ( T2U ) . The Trans-   former TTS model has a text encoder , a spectro-   gram decoder and a HiFi - GAN vocoder ( Kong   et al . , 2020 ) . The T2U model is the same model   used in preparing reference units for speech nor-   malizer training ( Section 4.1 ) , and we apply the   same unit - based vocoder for the S2UT model for   unit - to - speech conversion . Both Transformer TTS   and T2U are trained with characters as input.864   4.4 Evaluation   To evaluate translation quality , we first use open-   sourced ASR modelsto decode all systems ’   speech output . As the ASR output is in lower-   case and without digits and punctuation except   apostrophes , we normalize the reference text by   mapping numbers to spoken forms and removing   punctuation before computing BLEU using S -   BLEU ( Post , 2018 ) . To evaluate the naturalness of   the speech output , we collect mean opinion scores   ( MOS ) from human listening tests . We randomly   sample 200 utterances for each system , and each   sample is rated by 5 raters on a scale of 1 ( the   worst ) to 5 ( the best ) .   4.5 Textless S2ST training   Speech normalization We finetune the mHu-   BERT model for En , Es and Fr , respectively , re-   sulting in three language - dependent speech normal-   izers . We perform CTC finetuning for 25k updates   with the Transformer parameters fixed for the first   10k steps . We use Adam with β= 0.9 , β=   0.98 , ϵ= 10 , and 8k warm - up steps and then   exponentially decay the learning rate . We tune the   learning rate and masking probabilities on the dev   sets based on unit error rate ( UER ) between the   model prediction and the reference target units . S2UT We follow the same model architecture   and training procedure in Lee et al . ( 2021 ) , except   having a larger speech encoder and unit decoder   with embedding size 512 and 8 attention heads .   We train the models for 600k steps for V oxPop-   uli S2ST data , and 800k steps for the combination   of V oxPopuli and mined data , and use Adam with   β= 0.9 , β= 0.98 , ϵ= 10 , and inverse square   root learning rate decay schedule with 10k warmup   steps . We use label smoothing of 0.2 and tune the   learning rate and dropout on the dev set . The model   with the best BLEU on the dev set is used for eval-   uation . All S2UT systems including the baselines   are trained with an auxiliary task weight of 8.0 .   Unit - based vocoder We train one vocoder for   each language , respectively . All vocoders are   trained with orig - unit sequences as input , since   they contain the duration information of natural   speech for each unit . We follow the training pro-   cedure in Polyak et al . ( 2021 ) and train for 500k   updates with the weight on the MSE loss set to 1.0 .   The vocoder is used for generating speech from ei-   therorig - unit ornorm - unit , as they originate from   the same K - means clustering process .   5 Results   5.1 Textless S2ST   S2ST with supervised data Table 4 summa-   rizes the results from systems trained with V ox-   Populi S2ST data . We also list the results from   applying TTS on the ground truth reference text ( 8 ,   9 ) to demonstrate the impact from ASR errors and   potentially low quality speech on the BLEU score .   First , compared with the basic setup , the base-   line with target speaker embedding can give a 1.2-   3 BLEU improvement on three language pairs ( 1   vs. 2 ) , implying that there exists variations in orig-   unit sequences which are hard to model without   extra information from the target speech signals.865   However , with only 10 minutes of paired multiple-   to - single speaker speech data , we obtain norm-   unit that improves S2UT model performance by   1.5 BLEU on average ( 2 vs. 3 ) . The translation   quality improves as we increase the amount of par-   allel data for training the speech normalizer . In the   end , with 10 hours of finetuning data , we obtain   an average 4.9 BLEU gain from the four language   pairs compared to the basic setup ( 1 vs. 5 ) .   On the other hand , compared with S2T+TTS   systems that uses extra ASR models for converting   speech to text for training the translation model ( 6 ,   7 ) , our best textless S2ST systems ( 5 ) can perform   similarly to text - based systems without the need of   human annotations for building the ASR models .   We see that the MOS of S2UT systems trained   with orig - unit is on average 0.85 lower than that   of systems trained with norm - unit ( 1 vs. 5 ) . We   notice that the former often produces stuttering in   the output speech , a potential cause to lower MOS .   While worse audio quality may affect ASR - basedevaluation and lead to lower BLEU , we verify that   this was not the case as the ASR models could   still capture the content . We also see that the pro-   posed textless S2ST system can produce audios   with similar naturalness as Transformer TTS mod-   els ( 5 vs. 6 ) .   S2ST with supervised data and mined data   Next , we add the mined S2ST data for model train-   ing , and the results are summarized in Table 5 .   We apply the speech normalizer trained with 1 - hr   data , as it provides similar translation performance   as a speech normalizer trained with 10 - hr data in   V oxPopuli - only experiments ( 4 vs. 5 in Table 4 ) .   On the Europarl - ST test set , we see consistent   trend across the S2UT models trained with norm-   unitand the two baselines with orig - unit , where the   proposed approach gives on average 3.9 BLEU im-   provement compared to the basic setup ( 10 vs. 12 ) ,   indicating that the speech normalizer trained on   V oxPopuli and Common V oice data can also be   applied to audios from different domains , e.g. Lib-866riV ox , where the mined data is collected . The ad-   dition of mined data with the proposed speech nor-   malization technique achieves an average of 2.0   BLEU gain over four language directions ( 4 vs. 12 ) .   We also examine model performance on the CoV-   oST 2 test set ( Wang et al . , 2020b ) and see even   larger improvements brought by mined data ( 10 ,   11 , 12 vs. 4 ) . One possible reason for this is that   LibriV ox is more similar to the domain of CoV oST   2 than that of Europarl - ST . With target speaker em-   bedding , mined data improves S2ST by 7.1 BLEU   on average ( 4 vs. 11 ) . S2UT with norm - unit does   not perform as well , and one explanation is that we   select the best model based on the Europarl - ST dev   set during model training .   Compared with S2T+TTS systems trained with   text obtained from ASR , there is an average of 0.6   BLEU gap from our proposed system on Europarl-   ST test sets ( 12 vs. 14 ) . As the En ASR model was   trained on Libripeech ( Panayotov et al . , 2015 ) , it   can decode high quality text output for the mined   data . We also list results from the S2 T systems   from Wang et al . ( 2021c)(15 , 16 ) , which shows   the impact of having oracle text and in - domain   training data and serves as an upper bound for the   textless S2ST system performance .   5.2 Analysis on the speech normalizer   We analyze norm - unit to understand how the   speech normalization process helps improve S2UT   performance . First , to verify that the process pre-   serves the lexical content , we perform a speech   resynthesis study as in Polyak et al . ( 2021 ) . We   use the V oxPopuli ASR test sets , run the unit - based   vocoder with different versions of discrete units ex-   tracted from the audio as input , and compute word   error rate ( WER ) of the audio output . In addition   to comparing between norm - unit andreduced orig-   unit , we list the WER from the original audio to   demonstrate the quality of the ASR models and the   gap caused by the unit - based vocoder .   We see from Table 6 that norm - unit from a   speech normalizer finetuned on 1 - hr data achieves   similar WER as orig - unit , indicating that the nor-   malization process does not change the content of   the speech . In addition , we observe that norm-   unit sequences are on average 15 % shorter than   reduced orig - unit sequences . We find that this is   mainly due to the fact that the speech normalizerWER ( ↓ ) En Es Fr   original audio 14.2 15.5 18.5   reduced orig - unit 22.4 22.7 24.1   norm - unit ( 10 - min ) 23.5 25.3 31.7   norm - unit ( 1 - hr ) 21.2 20.5 24.6   norm - unit ( 10 - hr ) 22.0 25.3 24.2   UER ( ↓ ) En Es Fr   reduced orig - unit 74.4 70.6 73.5   norm - unit ( 1 - hr ) 48.2 31.6 46.4   does not output units for the long silence in the   audio , while reduced orig - unit encodes non - speech   segments such as silence and background noises .   Therefore , norm - unit is a shorter and cleaner target   for training S2UT models .   Next , to examine that the speech normalizer   reduces variations in speech across speakers ,   we sample 400 pairs of audios from Common   V oice ( Ardila et al . , 2020 ) for En , Es and Fr , respec-   tively . Each pair contains two speakers reading the   same text prompt . Table 7 shows the unit error rate   ( UER ) between the unit sequences extracted from   the paired audios . We see that norm - unit has UER   that is on average 58 % of the UER of reduced orig-   unit , showing that norm - unit has less variations   across speakers .   5.3 Analysis of mined data   Each pair of aligned speech in the mined data has   an associated semantic similarity score . In exper-   iments above , we set the score threshold as 1.06 ,   and use all mined data with scores above it . Given   0 100 200 300 40019202122   amount of mined data ( hrs)867the trade - off between the quality and quantity of   mined data , we analyze how the S2ST performance   changes with the threshold set in mined data se-   lection . Figure 4 demonstrates BLEU scores on   Europarl - ST Es - En test set from S2UT systems   trained with 1 - hr norm - unit . The mined data is use-   ful at different thresholds given its gains over the   model trained without mined data . As we increase   the threshold from 1.06to1.07 , the performance   drops due to less training data .   6 Conclusion   We present a textless S2ST system that can be   trained with real target speech data . The key to   the success is a self - supervised unit - based speech   normalization process , which reduces variations in   the multi - speaker target speech while retaining the   lexical content . To achieve this , we take advantage   of self - supervised discrete representations of a ref-   erence speaker speech and perform CTC finetuning   with a pre - trained speech encoder . The speech nor-   malizer can be trained with one hour of parallel   speech data without the need of any human annota-   tions and works for speech in different recording   conditions and in different languages . We conduct   experiments on the V oxPopuli S2ST dataset and   the mined speech data to empirically demonstrate   its usefulness in improving S2ST system transla-   tion quality for the first time . In the future , we plan   to investigate more textless approaches to improve   model performance such as self - supervised pre-   training . All the experiments and ASR evaluation   are conducted with public datasets or open - sourced   models .   Acknowledgements   The authors would like to thank Adam Polyak and   Felix Kreuk for initial discussions on accent nor-   malization .   References868869870A mHuBERT Training details   Table 8 lists the details for the three iterations of   mHuBERT training .   B Unit - based Vocoder   Table 9 shows the resynthesis performance of the   unit - based vocoder of each language . The WER   on the original audio indicates the quality of the   open - sourced ASR model we use for evaluation .   The WER difference between original audio and   orig - unit shows the quality of the vocoder , and   the difference between orig - unit andreduced orig-   unitshows the further impact brought by the dura-   tion prediction module .   WER ( ↓ ) En Es Fr   original audio 2.0 8.4 24.0   orig - unit 2.8 12.0 29.3   reduced orig - unit 3.4 11.9 31.3   C Text - to - Unit ( T2U )   Table 10 lists the WER of the audios generated by   the T2U model , which is used in generating the   reference target units for speech normalizer train-   ing . As the T2U model is trained with reduced unit   sequences as the target , during synthesis , we apply   the unit - based vocoder with duration prediction .   We can see that T2U with a unit - based vocoder   can produce high quality audio and can serve as   another option of TTS .   WER ( ↓ ) En Es Fr   original audio 2.0 8.4 24.0   T2U 4.2 9.1 24.4D Hyper - parameters   Table 11 lists the best hyper - parameters for train-   ing the speech normalizers for the three languages   and three data setups , respectively . All models are   trained on 8 GPUs with a batch size of 100 - second   ( maximum total input audio length ) .   Table 12 lists the best learning rate tuned on the   dev set for the S2UT experiments listed in Table 4   and Table 5 . All models are trained on 8 GPUs   with a total batch size of 160k tokens and dropout   of 0.3 , except for Es - En experiment ID 1 which   uses 0.1 .   ID Es - En Fr - En En - Es En - Fr   10.0005 0.0003 0.0003 0.0003   20.0003 0.0003 0.0003 0.0003   30.0003 0.0003 0.0003 0.0003   40.0003 0.0003 0.0003 0.0003   50.0003 0.0003 0.0003 0.0003   10 0.0005 0.0005 0.0005 0.0005   11 0.0005 0.0003 0.0005 0.0005   12 0.0005 0.0005 0.0005 0.0005   E Dev BLEU   Table 13 shows the BLEU scores on the Europarl-   ST dev sets from systems in Table 4 and Table 5.871ID Es - En Fr - En En - Es En - Fr   1 15.4 16.0 15.9 14.7   2 18.4 17.4 19.1 15.5   3 20.5 19.8 20.5 16.2   4 21.4 21.0 20.8 17.6   5 21.6 21.1 22.0 17.8   7 22.3 20.5 21.8 18.0   10 19.0 18.7 19.8 17.2   11 20.5 20.7 20.8 17.8   12 23.8 23.7 23.8 19.3   14 23.7 23.6 25.0 20.6   16 28.6 29.1 - -872