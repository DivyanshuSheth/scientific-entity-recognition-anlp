  Matthias Orlikowski , Paul Röttger , Philipp Cimiano , and Dirk HovyBielefeld UniversityUniversity of OxfordComputing Sciences Department , Bocconi University , Milan , Italy   Abstract   Many NLP tasks exhibit human label variation ,   where different annotators give different labels   to the same texts . This variation is known to de-   pend , at least in part , on the sociodemographics   of annotators . Recent research aims to model   individual annotator behaviour rather than pre-   dicting aggregated labels , and we would expect   that sociodemographic information is useful   for these models . On the other hand , the eco-   logical fallacy states that aggregate group be-   haviour , such as the behaviour of the average   female annotator , does not necessarily explain   individual behaviour . To account for sociode-   mographics in models of individual annotator   behaviour , we introduce group - specific layers   to multi - annotator models . In a series of exper-   iments for toxic content detection , we find that   explicitly accounting for sociodemographic at-   tributes in this way does not significantly im-   prove model performance . This result shows   that individual annotation behaviour depends   on much more than just sociodemographics .   1 Introduction   Different annotators will not necessarily assign the   same labels to the same texts , resulting in human   label variation ( Plank , 2022 ) . Previous work finds   that this variation depends at least in part on the   sociodemographics of annotators , such as their age   and gender ( Binns et al . , 2017 ; Al Kuwatly et al . ,   2020 ; Excell and Al Moubayed , 2021 ; Shen and   Rose , 2021 ) . These results are particularly pro-   nounced for subjective tasks like toxic content de-   tection ( Sap et al . , 2019 ; Kumar et al . , 2021 ; Sap   et al . , 2022 ; Goyal et al . , 2022 ) . Since human la-   bel variation is relevant to a wide range of NLP   tasks , recent research has begun to model individ-   ual annotator behaviour , rather than predicting ag-   gregated labels ( Davani et al . , 2022 ; Gordon et al . ,   2022 ) . In this setting , we would expect sociodemo-   graphic attributes to help explain annotator deci-   sions . Therefore , we investigate whether explicitlyFigure 1 : Group - specific layers representing annotator   sociodemographics in multi - annotator models .   accounting for the sociodemographic attributes   of annotators leads to better predictions of their   annotation behaviour .   There is a risk of misreading these efforts as an   example of the ecological fallacy : aggregate group   behaviour does not necessarily explain individual   behaviour ( Robinson , 1950 ; Freedman , 2015 ) . For   example , while on average , white annotators may   be more likely to label African - American Vernac-   ular English as toxic ( Sap et al . , 2019 ) , that does   not mean it is true for every white annotator indi-   vidually . However , we aim at exactly this distinc-   tion to discuss the relevance of sociodemographic   groups in models of individual annotator behaviour .   Likewise , we do not assume prior work to commit   ecological fallacies , even if a less - nuanced read   might suggest it .   Davani et al . ( 2022 ) introduce a simple multi-   annotator model , where each annotator is modelled   with a separate classification head . We expand   their model with group - specific layers , which are   activated for each annotator based on their sociode-   mographic attributes . We compare the two model   setups to a control setup where we randomise group   assignments . All comparisons use annotator - level   toxicity data from Kumar et al . ( 2021 ) . We find   that find that explicitly accounting for sociodemo-1017graphic attributes does notsignificantly improve   model performance . This result suggests that hu-   man label variation happens at a more individual   level than sociodemographics , and that annotator   decisions are even more complex .   Contributions 1 ) We introduce group - specific   layers to model groups of annotators with shared at-   tributes in multi - annotator models . 2 ) We evaluate   the effect of group - specific layers for toxic content   detection , and show that explicitly accounting for   sociodemographic attributes does not significantly   improve performance , thus highlighting the risk of   the ecological fallacy in annotator modelling .   As a corollary , we show that multi - annotator   models can be applied to many times more annota-   tors than in prior work .   2 Related Work   Sociodemographics in Annotation Behaviour   A growing body of research studies how annota-   tor sociodemographics relate to their annotation   decisions , for tasks ranging from natural language   inference ( Biester et al . , 2022 ) to the detection of   racist ( Larimore et al . , 2021 ) or generally toxic   ( Sap et al . , 2022 ) language . Goyal et al . ( 2022 ) ,   for example , find that annotators from certain so-   ciodemographic groups ( e.g. , LGBTQ people ) tend   to find content attacking their own groups ( e.g. ,   homophobic content ) to be more toxic . This mo-   tivates our research into explicitly accounting for   sociodemographics to model annotation behaviour .   However , the link between sociodemographics and   behaviour is not uncontested . Biester et al . ( 2022 ) ,   for example , do not find significant differences in   annotation behaviour between annotators of differ-   ent genders for four different tasks .   Predicting Annotators ’ Decisions on Text Dif-   ferent from analyses of annotation behaviour , a   recent line of research attempts to learn models   based on individual annotations ( Plank et al . , 2014 ;   Jamison and Gurevych , 2015 ; Akhtar et al . , 2020 ;   Fornaciari et al . , 2021 ; Cercas Curry et al . , 2021 ) .   These models are motivated by the concern that   aggregating labels into a single “ truth ” is too sim-   plistic for many tasks ( Uma et al . , 2021 ; Basile   et al . , 2021 ) and might introduce uneven represen-   tation of perspectives ( Prabhakaran et al . , 2021 ;   Abercrombie et al . , 2022 ) .   A particular way of learning from disaggregated   labels are models that predict individual annotator   decisions for an example . Our work builds directlyon such a model , multi - annotator models ( Davani   et al . , 2022 ) , which we describe in more detail sep-   arately ( § 4 ) . Gordon et al . ( 2022 ) present a model   which also predicts individual annotations and al-   lows a user to interactively aggregate them based   on “ a jury ” inspired by the US judicial system .   Their work is similar to ours in central aspects as   they explicitly model annotators ’ sociodemograph-   ics and use the same dataset as we do ( Kumar et al . ,   2021 ) . Different from our work , they frame the   task as a regression problem and develop a model   based on recommender systems . While they also   explore ecological fallacies , they focus on usage   risks of their system and countermeasures . In con-   trast , we consider the issue of the ecological fallacy   in modelling annotation behaviour more generally .   We compare our findings to their results ( § 6 ) .   3 Data   We use a sample of the Kumar et al . ( 2021 ) dataset   for our experiments . The full dataset contains   107,620 English comments from Twitter , Reddit ,   and 4Chan , annotated for toxicity by 17,280 anno-   tators . The annotation process encouraged anno-   tator subjectivity ( Röttger et al . , 2022 ) which is a   desired feature for modelling annotator behaviour .   For each annotator , there is extensive sociodemo-   graphic information , collected with a survey . An-   notations are given as ratings on a five - point scale   which we convert to binary annotations by map-   ping ratings of 2 to 4 to toxic , and ratings 0 and 1   tonon - toxic .   We randomly sample comments from the dataset   until we reach annotations from more than 5,000   annotators . We then add all other annotations by   these annotators . This approach maximizes the   number of examples while controlling the number   of annotators in our sample .   Our final sample contains 111,780 annotations   from 5,002 annotators on 22,360 comments with   20 to 120 annotations per annotator ( mean 22.35 ) .   Most comments have five annotations . 20 com-   ments have four because we removed any underage   annotators before sampling . In total 78,357 anno-   tations ( 70.10 % ) are toxic , and 33,423 annotations   ( 29.90 % ) are non - toxic .   We focus on four sociodemographic attributes :   gender , age , education , and sexual orientation .   Group sizes vary by attribute . For gender , 2,450   annotators ( 48.98 % ) identify as female , 2,116   ( 42.30 % ) as male , 23 ( 0.46 % ) as non - binary ( rest   in residual categories , full statistics in A.1).10184 Experiments   We compare three models . The baseline model is   the multi - annotator model by Davani et al . ( 2022 ) .   We use their multi - task variant : For each annota-   tor , there is a separate classification layer trained   on annotations from that annotator . All annotator   layers share a pre - trained language model used to   encode the input . We use RoBERTa ( Liu et al . ,   2019 ) for this , motivated by computational con-   straints . The other models in our experiments build   on this baseline model .   For the sociodemographic models , we add   group - specific layers based on sociodemographic   attributes of the annotators . A single attribute ,   e.g. , age , implies several groups , e.g. , ages 25-   34,ages 35 - 44 . We add the group - specific layers   between the pre - trained model and the annotator   layers . Each group of annotators shares a separate   group - specific layer . We implement group - specific   layers as fully - connected , linear layers , each learn-   ing a feature transformation applied for one group   of annotators .   Finally , for the random models , we shuffle the   assignment of annotators to groups from the so-   ciodemographic model , retaining the relative group   sizes . In other words , the probability of each anno-   tator staying in the same group or being reassigned   to another group corresponds to the relative size   of each group . This approach keeps the model   architecture constant while removing the connec-   tion between actual sociodemographic attributes   and group assignment . It allows us to distinguish   the effects of additional parameters , which group-   specific layers add in comparison to the baseline ,   from the effects of sociodemographic information .   4.1 Evaluation Setup   We evaluate all models on individual annotations   from gender , age , education , and sexual orientation   groups . This setup is comparable to the “ individ-   ual label ” evaluations in Davani et al . ( 2022 ) and   Gordon et al . ( 2022 ) , but with scores calculated per   group of annotators . We measure performance in   macro - average F , to weigh each class equally .   Cross - Validation As there is no standard split   available for our dataset , we perform three iter-   ations of a four - fold cross - validation with differ-   ent seeds ( training details in Appendix A.3 ) . We   choose four folds , so that even very small groups   have more than a hundred annotations in each test   set . Across folds , the numbers of annotations persociodemographic group are similar ( see Appendix   A.4 ) . We construct test sets that only contain com-   ments unseen by the annotators in the training set .   We also ensure that all test sets have similar pro-   portions of toxic or non - toxic comments ( assigned   by the majority of annotators ) to address the class   imbalance in the dataset ( 70.62 % toxic , see § 3 ) .   Statistical Significance We test for statistical   significance of our results from multiple runs of   k - fold cross - validation via replicability analysis   ( Dror et al . , 2017 ) . We report the number of signifi-   ca nt folds and the Bonferroni - corrected count ( Dror   et al . , 2018 ) in Appendix A.2 . We compute the p-   values for each fold via a paired bootstrap - sampling   test with BooStSa ( Fornaciari et al . , 2022 ) . We set   the significance level α= 0.05 , draw 1000 boot-   strap samples per fold , and use a sample size of   50 % of the respective test set .   Remarks on Groups Annotators from different   groups of the same attribute will in most cases   not have annotated the same examples . Therefore ,   comparisons between models are only meaningful   within each group .   The groups modeled via group - specific layers   and those in the result tables are always the same .   For example , if we report scores for gender groups ,   then the sociodemographic and randomized models   are also based on gender groups . In the following ,   we focus on a subset of groups , omitting , e.g. , " Pre-   fer not to say " ( see Appendix A.5 ) .   5 Results   Table 1 shows the results for gender , age , educa-   tion , and sexual orientation . A naive majority class   baseline that predicts all input to be toxic performs   worse than all other models with a large margin   ( exact results in Appendix A.5 ) .   Sociodemographics vs. Baseline Across at-   tributes , the average scores of the sociodemo-   graphic model and the baseline are similar . The   sociodemographic model often has a slightly higher   average macro F1 than the baseline , but no statisti-   cally significant gains . Where average performance   is better by several points , as for homosexual an-   notators , this gain is offset by a large variance in   performance ( a consequence of small group sizes ) .   Sociodemographics vs. Random We also do not   find significant performance differences between   sociodemographic group - layer models and the cor-   responding random group assignment models . For1019most groups , the randomized models achieve the   highest average scores , but differences to the so-   ciodemographic model are never statistically sig-   nificant .   6 Discussion   We do not find strong evidence that explicitly mod-   elling sociodemographics helps to predict annota-   tion behaviour with multi - annotator models . These   results might seem counter - intuitive , given the evi-   dence of systematic annotation differences between   sociodemographic groups ( see § 2 ) . This discrep-   ancy , however , echoes the issue highlighted by eco-   logical fallacies ( Robinson , 1950 ): Not every anno-   tator will be a perfect representative of their group ,   so we will not necessarily learn additional infor-   mation based on their group identity . This seems   especially true if we already have access to individ-   ual behaviour ( i.e. , individual annotations ) .   In contrast to Davani et al . ( 2022 ) , we made so-   ciodemographic information explicit in our experi-   ments , as one of the factors influencing annotationbehaviour . Group - specific layers can be seen as   an inductive bias putting emphasis on the sociode-   mographic relations between annotators . However ,   there are potentially many other factors influencing   annotation behaviour ( e.g. , attitudes , moral values ,   cognitive biases , psychological traits ) . In light of   our results , it seems plausible that multi - annotator   models learn about these factors implicitly as part   of predicting individual behaviour , so that mak-   ing one factor explicit does not change prediction   quality , at least in the case of sociodemographics .   Still , we also know that generally group at-   tributes can help predict individual decisions , i.e. ,   as base rates or priors . To avoid ecological fallacies   in modelling annotation , we therefore need to bet-   ter understand when and how modelling sociode-   mographic information is useful in predicting an   individual annotator ’s decisions . For example , we   have only evaluated group - specific layers for single   attributes . In contrast , social scientists have long   adopted the idea of intersectionality ( Crenshaw ,   1989 ) , which also informs research on fairness in   machine learning ( Wang et al . , 2022 ) . Intersection-   ality means that the effect of interactions between   sociodemographic attributes enables specific expe-   riences that are not captured by the attributes in   isolation . For example , identifying as a man means   something different depending on the person ’s edu-   cation . Groups derived from single attributes might   simply be too coarse to improve classifiers learnt   from individual labels , as in multi - annotator mod-   els .   The dataset we use ( Kumar et al . , 2021 ) has   many characteristics which are ideal for our study   ( see § 3 ) . However , it uses a broad notion of toxi-   city , in contrast to other studies of toxic language   ( Larimore et al . , 2021 ; Sap et al . , 2022 ) , which   match content and analysed groups . When model-   ing the groups frequently referenced in the datasets   themselves , we would expect greater benefits from   group - specific layers . Similar to us , Biester et al .   ( 2022 ) who do not find significant differences be-   tween annotators of different genders , do so in a   more general setting .   We can only partially compare to Gordon et al .   ( 2022 ) , despite using the same dataset . In addi-   tion to differences in approach ( see § 2 ) , our and   their work also differ in their research questions   and thus experimental conditions . Gordon et al .   ( 2022 ) compare their full model ( group and in-   dividual ) against using group information alone.1020We compare our full model ( group and individual )   against using individual information alone . So it   is unclear if their model would benefit from group   information in comparison to individual - level in-   formation alone . While they find an improvement   from group information it is only in comparison   to a baseline predicting not individual but aggre-   gated labels . Additionally , the composition of test   sets sampled from the full dataset differs between   the studies : Gordon et al . ( 2022 ) use a test set of   5,000 comments , while we use 22,360 comments   in a four - fold cross - validation . We leave an explicit   comparison to future work .   Group - specific layers ( § 4 ) are a natural exten-   sion of annotator - specific classification layers in   multi - annotator models . However , other architec-   tures to predict annotator - level labels use different   ways to represent sociodemographic information ,   e.g. , via embeddings in a recommender system   ( Gordon et al . , 2022 ) . Future work could explore   additional representations of annotator attributes   ( e.g. , as part of the input , either textual or as sep-   arate features ) and other approaches to modelling   the relation of individual labeling decisions and   attributes ( e.g. , probabilistic graphical models ) .   7 Conclusion   We ask how relevant modelling explicit sociodemo-   graphic information is in learning from individual   annotators . Our experiments with group - specific   layers for four sociodemographic attributes on so-   cial media data with toxicity annotations ( Kumar   et al . , 2021 ) show no significant benefit of mod-   elling sociodemographic groups in multi - annotator   models . However , as the issue of ecological fal-   lacies highlights , it is not implausible that these   models do not learn additional information from   group information beyond the inherent variation .   However , our results do not refute the usefulness   of sociodemographic attributes in modelling anno-   tation , but underscore the importance of their judi-   cious use . Different tasks and model architectures   will likely benefit to different extents . Ultimately ,   annotation behaviour is driven by complex factors   and we will need to consider more than annotators ’   sociodemographics .   Acknowledgements   We thank Deepak Kumar for providing access to   the disaggregated dataset and his continued sup-   port . We also thank Aida Mostafazadeh Davani   for providing information on implementation de - tails of multi - annotator models . Members of Mi-   laNLP ( Bocconi ) and the Semantic Computing   Group ( Bielefeld ) provided feedback on earlier ver-   sions of this paper , for which we thank them again .   This work has in part been funded by the Euro-   pean Research Council ( ERC ) under the European   Union ’s Horizon 2020 research and innovation pro-   gram ( grant agreement No . 949944 , INTEGRA-   TOR ) . Likewise , this work has in part been funded   by the V olkswagenStiftung as part of the " 3B Bots   Building Bridges " project .   Limitations   While the dataset by Kumar et al . ( 2021 ) enabled   us to test models for a range of often overlooked   groups ( e.g. , non - binary or bisexual annotators ) ,   we ultimately modelled only four specific attributes   ( gender , age , education , sexual orientation ) . There   are likely to be more factors that could play a role .   Additionally , annotators in the Kumar et al . ( 2021 )   dataset are exclusively from the United States of   America , so that results do not necessarily hold for   other countries or cultures ( Hovy and Yang , 2021 ) .   Specifically perceptions of harmful content online   are known to vary across countries ( Jiang et al . ,   2021 ) .   We used only the ( Kumar et al . , 2021 ) dataset .   This is mainly due to our strict criteria regarding   dataset size and availability of annotator - level la-   bels and sociodemographic information . These   characteristics were a prerequisite for our exper-   iments across different attributes with sufficient   numbers of annotators . Most datasets which in-   clude annotator - level labels and sociodemographic   information contain much smaller numbers of an-   notators and attributes . Nevertheless , with the Mea-   suring Hate Speech Corpus there is at least one   additional dataset ( Sachdeva et al . , 2022 ) with com-   parable characteristics that could be used in future   experiments . Also , additional small - scale , more fo-   cused experiments could use datasets like Sap et al .   ( 2022 ) or HS - Brexit ( Akhtar et al . , 2021 ) which   was annotated by 6 annotators , each from one of   two sociodemographic groups .   We do not study the aggregation of individual   predictions or evaluate against majority labels , as   these are not directly relevant to our investigation   of sociodemographic attributes in models of an-   notation behaviour . Consequently , we can not de-   rive a conclusion about performance in those set-   tings from our results . This is a noteworthy limita-   tion , because part of the experiments introducing1021multi - annotator models in Davani et al . ( 2022 ) com-   pare labels aggregated from multi - annotator mod-   els against predictions from a standard classifier   ( directly trained on aggregated labels ) .   For computational reasons , our experiments use   a comparatively small pre - trained language model   ( RoBERTa , Liu et al . 2019 ) . Thus , results might   differ with larger models .   Ethics Statement   As sociodemographic attributes are sensitive infor-   mation , we do not infer attributes , but build on a   self - reported , IRB - reviewed dataset ( Kumar et al . ,   2021 ) . We also see potential for a discussion of   “ privacy by design ” in modelling human label vari-   ation based on our results : There can be circum-   stances in which knowing more about annotators is   not relevant , and indeed might lead to violations of   privacy .   As multi - annotator models attempt to capture   the preferences of individual annotators , there are   valid concerns around privacy and anonymity . As   discussed in Davani et al . ( 2022 ) , increasing the   annotator count can be one option to reduce privacy   risks . We show it is feasible to learn a model for   a large number of individual annotators ( 5002 vs.   18 and 82 in their work ) . But a prerequisite for   improved privacy is to apply effective aggregation   on top of individual predictions , which we do not   study in the present work .   References10221023   A Appendix   A.1 Annotator Sociodemographics in Sample   Table 2 shows how many annotators the sample   contains . Counts are given per group of the four   attributes gender , age , education and sexuality . In the Kumar et al . ( 2021 ) dataset , sociodemo-   graphic attributes are given for each individual   annotation - not once per annotator . For some   annotators , conflicting attribute values exist ( e.g. ,   two different age groups ) . As the data collec-   tion spanned several months ( Kumar et al . , 2021 ) ,   these value changes can in principle be reasonable   ( e.g. , because an annotator got older , finished a de-   gree , changed sexual preference or gender identity ) .   However , as reasonable changes can not easily be   discerned from erroneous input , we disambiguate   values based on a heuristic : If an annotator reports   several values for an attribute , we assume the most   frequent value to be valid . In cases of no clear   most frequent value , we set the attribute to " Prefer   not to say " . Thus , the main results do not contain   annotators with ambiguous attributes .   A.2 Significance Tests   Results of a replicability analysis ( Dror et al . , 2017 )   testing for significant differences in macro Fon   scores from three runs of four - fold cross - validation .   Table 3 shows results for a comparison of the so-   ciodemographic models against the baseline mod-   els . Table 4 shows results for a comparison of the   sociodemographic models against the randomized   assignment models . The Bonferroni correction for   the corrected count of significant folds ˆk   is used to account for the fact that we have over-   lapping test sets from multiple runs of four - fold   cross - validation .   A.3 Training Details , Hyperparameters and   Computational Resources   We implement models and the training loop us-   ing the Hugging Face Transformers library ( ver-   sion 4.19.2 , Wolf et al . 2020 ) . Maximum sequence   length is 512 tokens , with truncation and padding   to the maximum length . We train for 3 epochs   with a batch size of 8 and an initial learning rate   of0.00001 . Otherwise , we used default parame-   ters . We found results to particularly depend on the   learning rate , with higher or lower values leading   to worse results .   We use a weighted loss function . Label weights   are calculated per annotator on the training set of   each fold . Label weights , evaluation scores and the   four - fold dataset splits ( StratifiedKFold ) are calcu-   lated using the scikit - learn library ( version 1.0.2 ,   Pedregosa et al . 2011 ) . The folds are based on a   fixed random seed per iteration : 2803636207 ,   165043843 , 29232623581024   The majority of parameters in our model be-   long to the pre - trained language model shared be-   tween all group - specific and annotator - specific lay-   ers . Specifically , RoBERTa ( Liu et al . , 2019 ) in the   roberta - base variant has 125 Million parameters .   We keep the pre - trained model ’s default output di-   mensionality of 768 , so that each group - specific   layer adds 768∗768 + 768 = 590 , 592parameters   and each annotator layer adds 768∗2 + 2 = 1 , 538   parameters .   All experiments ran on a single GPU ( GeForce   GTX 1080 Ti , 12 GB GPU RAM ) . Per fold , training   and evaluation together take about three and a half   hours in our setting . Three runs of four - fold cross-   validation ( 12 folds ) , thus take around 42 hours   ( 1.75 days ) . With four attributes and three train-   able models the combined run time of the reported   experiments is estimated to be 21 days . Including   preliminary experiments , which , however , mostly   were not full runs of k - fold cross - validation and   also utilized DistilBERT ( Sanh et al . , 2019 ) with   slightly faster run times , it will be many times more .   There is no discernible difference in experiment   run times between multi - annotator models with or   without groups or different numbers of groups .   A.4 Number of Annotations per Group across   all Test Sets   Table 5 contains the number of annotations we have   per group across the total of 12 folds ( from three   runs of four - fold cross - validation ) . This number of   annotations is the effective test set size per group .   As the numbers do not vary substantially , perfor-1025   mance on each fold is equally representative for all   groups .   A.5 Full Results   Table 6 shows full results of experiments ( see 4 ) ,   including results for all residual categories and a   naive baseline which always predicts toxic .10261027ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations , 8   /squareA2 . Did you discuss any potential risks of your work ?   Ethics Statement , 9   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   3 , Appendix A.3   /squareB1 . Did you cite the creators of artifacts you used ?   3 , Appendix A.3   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Clear from context , citations   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Clear from context , citations   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   3 , Ethics Statement 9   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   3 , Appendix A.1   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   3 , 4 , Appendix A.4   C / squareDid you run computational experiments ?   4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.31028 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A.3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1029