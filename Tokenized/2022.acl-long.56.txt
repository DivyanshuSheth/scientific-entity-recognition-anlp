  Hideo Kobayashi , Yufang Houand Vincent NgHuman Language Technology Research Institute , University of Texas at Dallas , USAIBM Research Europe , Ireland   { hideo,vince}@hlt.utdallas.edu   yhou@ie.ibm.com   Abstract   We examine the extent to which supervised   bridging resolvers can be improved without   employing additional labeled bridging data by   proposing a novel constrained multi - task learn-   ing framework for bridging resolution , within   which we ( 1 ) design cross - task consistency con-   straints to guide the learning process ; ( 2 ) pre-   train the entity coreference model in the multi-   task framework on the large amount of pub-   licly available coreference data ; and ( 3 ) inte-   grate prior knowledge encoded in rule - based   resolvers . Our approach achieves state - of - the-   art results on three standard evaluation corpora .   1 Introduction   Bridging ( Clark , 1975 ) plays an important role in   establishing entity coherence in a text . In con-   trast to direct anaphors , which indicate the coref-   erence relation between a nominal expression and   its antecedent , bridging anaphors orassociative   anaphors link to their antecedents via non - identical   relations . Bridging resolution is the task of recog-   nizing and resolving bridging anaphors in a text .   Bridging resolution and coreference resolution   are closely related to Information Status ( IS hence-   forth ) classification , the goal of which is to assign   an IS to each discourse entity that indicates how   these entities are referred to in a text ( Prince , 1981 ;   Nissim et al . , 2004 ; Markert et al . , 2012 ) . In gen-   eral , an entity is oldif it is coreferent with an entity   that has been mentioned before ( e.g. , “ [ The busi-   ness ] ” and “ [ its ] ” in Figure 1 ) . Bridging anaphors   are discourse - new but hearer - old . They have not   been introduced in the discourse directly , but are   inferrable from previously mentioned entities ( e.g. ,   “ [ the customers ] ” in Figure 1 ) . New entities are   introduced into the discourse for the first time and   are not known to the hearer before ( e.g. “ [ The   Bakersfield Supermarket ] ” in Figure 1 ) .   Progress on bridging resolution research is lim-   ited in part by the scarcity of annotated trainingFigure 1 : Illustration of information status , bridging and   coreference . Example is from Yu and Poesio ( 2020 ) .   data . While one of the largest annotated entity   coreference resolution datasets , OntoNotes , is com-   posed of 2802 English documents in its training   split , the two most commonly used English corpora   for bridging resolution research , ISNotes ( Mark-   ert et al . , 2012 ) and BASHI ( Rösiger , 2018 ) , are   composed of 50 WSJ documents each . Perhaps   the most straightforward way to mitigate this data   scarcity problem is to combine existing annotated   bridging datasets to create a larger training set ( Yu   and Poesio , 2020 ) . While it makes sense to com-   bine corpora that are created using the same an-   notation guidelines ( e.g. , ISNotes and BASHI ) , at-   tempting to combine corpora created using differ-   ent guidelines ( e.g. , ARRAU ( Poesio and Artstein ,   2008 ) and ISNotes ) will likely confuse the learner ,   thus limiting the applicability of this method . Some   researchers have instead attempted to create auto-   matically labeled data via lexico - syntactic patterns   ( Hou , 2018 ) and distant supervision ( Hou , 2020 ) ,   but a manual analysis of the resulting data instances   reveals that they may be too noisy for training : on   average only one - fourth of them are correctly la-   beled ( Hou , 2020 ) .   By contrast , we aim to investigate the extent   to which supervised bridging resolvers can be im-   proved without increasing the amount of labeled   bridging data . To this end , we begin by propos-   ing a novel constrained multi - task learning ( MTL )   framework for bridging resolution . While Yu and   Poesio ( 2020 ) develop a standard MTL model for759bridging resolution and use coreference resolution   as the only auxiliary task , we propose to ( 1 ) ex-   ploit the close connection between IS and bridg-   ing / coreference resolution by introducing IS classi-   fication as the third task into the MTL framework   and ( 2 ) guide the learning process by designing   cross - task consistency constraints . For instance , in   Figure 1 , the prediction from the coreference reso-   lution module indicating that both “ [ The business ] ”   and “ [ The murder ] ” are oldentities can help the   bridging resolution module to avoid misclassifying   these two mentions as bridging anaphors . Simi-   larly , if the IS classification module predicts “ [ the   customers ] ” as a bridging anaphor , then the bridg-   ing resolution module should find an antecedent   for it . We hypothesize that such constraints can   guide the training of a complex model to produce a   more coherent output across different tasks , thereby   improving bridging resolution performance .   While the cross - task consistency constraints   could improve performance , they could also hurt   performance . Returning to our example in Fig-   ure 1 , if the IS classification module misclassifies   " [ the customers ] " as non - bridging , the constraints   will propagate this error to the bridging resolution   module , causing it notto resolve the mention . To   address this problem , we ( 1 ) formulate these con-   straints as softrather than hard constraints , and   ( 2 ) improve entity coreference resolution perfor-   mance by leveraging the large amount of publicly-   available coreference - annotated data in OntoNotes   topre - train the coreference module .   Finally , since previous work ( Hou et al . , 2014 ;   Roesiger et al . , 2018 ) has shown that manually de-   fined rules based on various syntactic and semantic   properties are valuable to recognize and resolve   bridging anaphors , we integrate such prior knowl-   edge about bridging into our MTL framework .   Note that the only hybrid rule - based and learning-   based approach to bridging resolution ( Kobayashi   and Ng , 2021 ) merely applies the rule - based re-   solver and the learning - based resolver in a sequen-   tial manner , without combining them into a single   model .   In sum , our contributions are two - fold . First ,   we propose a novel constrained MTL framework   that jointly learns three tasks , bridging resolution ,   coreference resolution , and IS classification , via the   use of soft cross - task consistency constraints , prior   knowledge provided by rule - based approaches , and   pre - training on coreference data . Second , exper - imental results demonstrate that our framework   achieves new state - of - the - art results for full bridg-   ing resolution on three datasets ( ISNotes , BASHI ,   and ARRAU ) .   The rest of the paper is structured as follows .   Section 2 describes related work on bridging resolu-   tion and constrained multi - task learning with deep   neural networks . Section 3 describes our model , in-   cluding our multi - task framework for jointly learn-   ing IS classification , entity coreference resolution   and bridging resolution , our cross - task consistency   constraints , and how we integrate rule knowledge   into the framework . We present evaluation results   in Section 4 and our conclusions in Section 5 .   2 Related Work   Bridging resolution . Bridging resolution is com-   posed two sub - tasks : bridging anaphora recogni-   tionandantecedent selection . Most previous work   tackles them separately . One line of research mod-   els bridging recognition as part of IS classification   ( Rahman and Ng , 2011 ; Markert et al . , 2012 ; Cahill   and Riester , 2012 ; Rahman and Ng , 2012 ; Hou ,   2021 ) , while others have focused on antecedent   selection based on gold bridging anaphors ( Poesio   et al . , 2004 ; Lassalle and Denis , 2011 ; Hou et al . ,   2013 ; Hou , 2020 ) .   There are a few studies tackling the challeng-   ing task of full bridging resolution ( i.e. , bridging   anaphor recognition and resolution ) . Hou et al .   ( 2014 ) and Roesiger et al . ( 2018 ) develop rules to   identify bridging links based on syntactic and se-   mantic constraints . Hou et al . ( 2018 ) propose a   pipeline system built on top of complex manually   designed features . Yu and Poesio ( 2020 ) design   a MTL neural model for bridging resolution that   uses coreference resolution as an auxiliary task .   Recently , Kobayashi and Ng ( 2021 ) show the effec-   tiveness of a hybrid rule - based and MTL approach   for bridging resolution . For a detailed overview of   these approaches , we refer the reader to a recent   survey by Kobayashi and Ng ( 2020 ) .   Constrained multi - task learning with deep   neural networks . Multi - task learning has been   widely adopted in various NLP applications to im-   prove the performance of individual tasks ( Ruder ,   2017 ) . Recently , several studies have demonstrated   that multi - task training in neural networks can be   further improved by integrating logical constraints   to enforce a coherent output across different tasks   ( Li et al . , 2019 ; Wang et al . , 2020 ; Lu and Ng,7602021 ) . However , for a complex task like bridg-   ing resolution , it is non - trivial to choose auxiliary   tasks and model the relationships between these   tasks in deep neural networks . In this work , we ( 1 )   jointly train three tasks ( i.e. , bridging resolution ,   coreference resolution , and IS classification ) ; ( 2 )   design five soft cross - task consistency constraints   to guide the training process ; and ( 3 ) integrate prior   knowledge about bridging into our MTL model .   3 Model   In this section , we present our constrained MTL   framework for bridging resolution . Inspired by Yu   and Poesio ’s ( 2020 ) span - based model for bridg-   ing resolution , which employs an unconstrained   MTL framework that jointly learns bridging and   coreference , our model takes as input a document   Drepresented as a sequence of word tokens and   gold mentions M , from which we create span repre-   sentations . Our model simultaneously learns three   tasks , namely IS classification , bridging , and coref-   erence , as defined below .   The IS classification task aims to assign each   span ian IS ytaken from an IS inventory .   The model predicts the IS of ito be y=   arg maxs(i , y ) , where sis a function sug-   gesting i ’s likelihood of having yas its IS .   The bridging resolution task involves determin-   ing an antecedent for each bridging anaphor . For-   mally , it assigns span ian antecedent y , where   y∈ Y(i ) = { 1 , ... , i−1 , ϵ } . In other words , the   value of each yis the i d of its antecedent , which   can be one of the preceding spans or a dummy   antecedent ϵ(if the mention underlying iis not a   bridging anaphor ) in the associated document . We   define the following scoring function :   s(i , j ) =(   0 j=ϵ   s(i , j)j̸=ϵ(1 )   where s(i , j)is a pairwise bridging score com-   puted over iand a preceding span j. The   model predicts the antecedent of ito be y=   arg maxs(i , y ) .   The entity coreference resolution task involves   determining an antecedent for each identity   anaphor . Formally , it aims to assign span ian   antecedent ybased on a scoring function sthat   can be defined in an analogous manner as the s   function in the bridging resolution task.3.1 Model Structure   Figure 2 shows the structure of our constrained   MTL framework . Below we describe the details .   Span Representation Layer Following Yu and   Poesio ( 2020 ) , we use BERT embeddings as the   input to a bidirectional LSTM ( Hochreiter and   Schmidhuber , 1997 ) to encode tokens and their con-   texts . Then , we set g , the representation of span   i , to[h;h;h;f ] , where h   andhare the hidden vectors of the start and   end tokens of i , his an attention - based head   vector and fis a span width feature embedding .   IS Prediction Layer For each span i , we pass   its representation gto FFNN , a standard feed-   forward neural network . FFNNoutputs a vector   oiof dimension of S , where Sis the number of   possible IS labels . Specifically :   oi = FFNN(g ) ( 2 )   s(i , y ) = oi(y ) ( 3 )   where oi(y ) , they - th element of oi , is a score   that indicates i ’s likelihood of belonging to IS y.   This score is then used to compute s.   Bridging Prediction Layer To predict bridging   links , we define the pairwise score between span i   and span jas follows :   s(i , j ) = FFNN([g;g;g ◦ g;u ] ) ( 4 )   where ◦ denotes element - wise multiplication , g ◦ g   encodes the similarity between span iand span j ,   uis a feature embedding encoding the distance   between two spans , and FFNNis the FFNN used   in the bridging prediction layer . This pairwise score   is then used to compute s(see Equation ( 1 ) ) .   Coreference Prediction Layer The coreference   prediction layer is defined in the same way as the   bridging prediction layer , with the coreference pair-   wise score s(i , j)between two spans iandjcom-   puted by another FFNN , FFNN . Note that the first   few layers of FFNNand FFNNare shared .   3.2 Incorporating Consistency Constraints   As noted before , we propose to guide the learning   process by incorporating consistency constraints   on the three tasks involved in our model . Below we   design five cross - task consistency constraints and   show how they can be incorporated into our model   in asoftmanner.761   Constraint P1 : If a span ihasB as its   IS value , then its bridging antecedent must not be   the dummy antecedent .   To enforce P1 in a soft manner in our model ,   we define a penalty function p , which imposes a   penalty on span iif it violates the constraint , as   shown below :   ( 5 )   whereYis the set of possible IS labels . Intuitively ,   pestimates the minimum amount that needs to be   adjusted so that span i ’s IS type is not B .   In particular , preturns 0 ( i.e. , no penalty ) if i ’s IS   type is not B .   We incorporate pinto the model as a penalty   term in s(Equation ( 1 ) ) . Specifically , we redefine   s(i , j)when j=ϵ , as shown below :   s(i , ϵ ) = s(i , ϵ)−γp(i ) ( 6 )   where γis a positive constant that controls the   hardness of the constraint . The smaller γis , the   softer the constraint is . Intuitively , if P1is violated ,   s(i , ϵ)will be lowered by the penalty term , and   the dummy antecedent will less likely be selected   as the antecedent of i.   Constraint P2 : If a span ihasOas its IS   value , then its coreference antecedent must not be   the dummy antecedent .   The penalty function pused to enforce P2is   formulated in the same way as P1 .   Constraint P3 : If the IS task predicts a span ias   non - B , then its antecedent selected in the   bridging task must be the dummy antecedent . Similar to P1 , we define a penalty function pto   enforce P3 :   ( 7 )   We employ pto update sas follows :   s(i , j ) = s(i , j)−γp(i ) ( 8)   where γ , like γ , is the hardness coefficient . This   penalty is applied only when P3is violated . Specif-   ically , if IS task predicts a span ias non- B   but its antecedent selected in the bridging task is   not the dummy antecedent , then the penalty term   will lower the sscore for each of i ’s non - dummy   antecedents , which in turn makes it more likely   for the dummy antecedent to be selected as the   antecedent of i.   Constraint P4 : If a span idoes not have Oas   its IS value , then its coreference antecedent must   be the dummy antecedent .   The penalty function pused to enforce P4is   formulated in the same way as P3 .   Constraint P5 : If a span ihas a non - dummy   antecedent as its coreference antecedent , then   its bridging antecedent must be the dummy an-   tecedent .   The penalty function pused to enforce P5is   defined as follows :   ( 9)762where Y(i)is the set of candidate antecedents of   spani . We employ pto update sas follows :   s(i , j ) = s(i , j)−γp(i ) ( 10 )   where γis the hardness coefficient .   3.3 Incorporating Prior Knowledge   Next , we incorporate the prior knowledge provided   by rule - based resolvers into our model . Specifi-   cally , we employ the set of corpus - specific rules   designed by Rösiger et al . ( 2018 ) . Recall that the   output of a rule - based bridging resolver is a set of   links between a bridging anaphor and one of its an-   tecedents . We incorporate these bridging links into   our model by encoding them as a binary feature ,   r , whose value is 1 if and only if the rule - based   resolver posits a bridging link between span iand   spanj . This feature will be used as an additional   feature for FFNNand FFNN .   As noted by Rösiger et al . ( 2018 ) , rule - based   resolvers are precision- rather than recall - oriented .   The reason is that these hand - crafted rules are de-   signed to resolve specific ( rather than all ) cate-   gories of bridging anaphors . For instance , one   rule is designed to resolve a building part ( e.g. ,   " the door " ) to the building of which it is a part   ( e.g. , " the house " ) . Because of the low - recall nature   of rule - based resolvers , the feature r , which we   compute based on the rule - based outputs , could be   perceived as not particularly useful by our model .   Consequently , to encourage the model to seriously   take into consideration the potentially useful in-   formation encoded in r , we design a rule loss   ( see Section 3.4 ) , which imposes a penalty on the   model during training if the antecedent selected by   the model is a non - dummy antecedent that is nei-   ther a correct antecedent of inor the one selected   by the rules ( as encoded in r ) .   3.4 Training   The loss function , L(Θ ) , consists of the losses of   the three tasks and the rule loss as follows :   L(Θ ) = X(λL+λL+λL+λL)(11 )   where dis the number of training documents and   the hyperparameters ( i.e. , the λ ’s ) , which determine   the trade - off between the task losses , are tuned us-   ing grid search to maximize the average resolution   F - scores on development data . Task Losses We employ a max - margin loss for   the bridging and coreference resolution tasks .   Defining the bridging loss is tricky since the   antecedents for each bridging anaphor are evalu-   ated in the form of coreference clusters . We adopt   the entity coreference loss function originally de-   fined by Wiseman et al . ( 2015 ) . Specifically , let   GOLD(i)denote the set consisting of span i ’s   bridging antecedent as well as the spans preced-   ingithat are coreferent with the antecedent , and   ybearg maxs(i , y ) . In other words ,   yis the highest scoring ( latent ) antecedent of i   according to samong all the antecedents of i.   The loss function for bridging is defined as :   ( 12 )   where ∆(i , j)is a mistake - specific cost function   that returns the cost associated with a particular   type of error if an error exists and 0 otherwise ( Dur-   rett and Klein , 2013).Intuitively , the loss function   penalizes a span iif the predicted antecedent jhas   a higher score than the correct latent antecedent y.   The task loss for coreference , L , is defined in   the same way as the bridging loss , having an analo-   gous mistake - driven cost function ∆(i , j ) .   The task loss for the IS prediction task , L , is the   weighted softmax cross entropy loss , where mis-   classified bridging mentions and non - bridging men-   tions are weighted according to a mistake - driven   cost function ∆(i , j ) .   The rule loss is motivated by the bridging loss .   Specifically , the model will be penalized if there ex-   ists an incorrect non - dummy candidate antecedent   whose sscore is higher than the score of the an-   tecedent chosen by the rules , as shown below :   ( 13 )   where Nis the set of candidate anaphors for which   the rule - based system found a ( non - dummy ) an-763tecedent , yis the antecedent selected by the rules ,   and∆(i , j)is an indicator function that returns 0   ifjis the correct antecedent and 1 otherwise .   3.5 Pre - Training   As mentioned in the introduction , we pre - train the   coreference module in our MTL framework on the   English portion of OntoNotes 5.0 , excluding those   documents that appear in ISNotes or BASHI . To   do so , we pre - train the full model shown in Fig-   ure 2 , setting λto 1 and the remaining λ ’s to 0 in   the loss function so that only the network weights   associated with the coreference module will be up-   dated . Note that we follow Yu and Poesio ( 2020 )   and use the softmax cross entropy loss rather than   the max - margin loss for Lduring pre - training , the   reason being that this could simplify pre - training   by obviating the need to tune the hyperparameters   associated with the mistake - specific cost functions .   4 Evaluation   4.1 Experimental Setup   4.1.1 Corpora   We use three English corpora that are arguably   the most widely used corpora for bridging evalu-   ation , namely ISNotes ( composed of 50 WSJ arti-   cles in OntoNotes ) ( Markert et al . , 2012 ) , BASHI   ( The Bridging Anaphors Hand - annotated Inventory ,   composed of another 50 WSJ articles in OntoNotes )   ( Rösiger , 2018 ) , and ARRAU ( composed of arti-   cles from four domains , RST , GNOME , PEAR , and   TRAINS ) ( Poesio and Artstein , 2008 ; Uryupina   et al . , 2020 ) . Following previous work , we report   results only on RST , the most comprehensively an-   notated segment of ARRAU . Table 1 shows the   statistics on these corpora .   For ARRAU RST , we use the standard train-   test split . For ISNotes and BASHI , we divide the   documents in each corpus into 10 folds ( 8 folds   for training , 1 fold for development , and 1 fold for   testing ) and report 10 - fold cross - validation results .   4.1.2 Evaluation Setting   Following previous work ( Hou et al . , 2014 ; Roe-   siger et al . , 2018 ) , we report results for full bridging   resolution based on gold mentions . In this setting ,   a system is given as input both a document and its   thegold mentions . The goal is to identify bridging   anaphors from the gold mentions and resolve them   to their antecedents , which are also chosen from   the gold mentions .   There is a caveat in this evaluation setting , how-   ever . In ISNotes and BASHI , some bridging an-   tecedents correspond to events ( see Example ( 4 )   in Table 5 ) , and previous studies differ in terms   of how event antecedents should be handled . The   reason is that while these event antecedents are an-   notated , they are notannotated as gold mentions .   When reporting results on resolving gold mentions ,   some previous work ( e.g. , Hou et al . ( 2014 ) , Hou   et al . ( 2018 ) ) chose not to include these event an-   tecedents in the list of candidate antecedents and   others ( e.g. , Roesiger et al . ( 2018 ) , Yu and Poe-   sio ( 2020 ) ) did . Obviously , the setting in which   gold event antecedents are not included in train-   ing / evaluation is harsher because it implies that   anaphors with event antecedents will always be re-   solved incorrectly . We believe that including gold   event antecedents during evaluation does not repre-   sent a realistic setting , and will only report results   using the " harsh " setting in this paper .   4.1.3 Evaluation Metrics   Following Yu and Poesio ( 2020 ) , we report results   for bridging recognition and resolution in terms   of precision ( P ) , recall ( R ) , and F - score ( F ) . For   recognition , recall is the fraction of gold bridg-   ing anaphors that are correctly identified , whereas   precision is the fraction of bridging anaphors iden-   tified by the system that is correct . For resolution ,   recall and precision are defined in a similar fashion .   In addition , we report IS classification results in   terms of accuracy and coreference results in terms   of CoNLL score ( Pradhan et al . , 2014 ) , which is   the unweighted average of the F - scores provided by   three metrics , MUC ( Vilain et al . , 1995 ) , B(Bagga   and Baldwin , 1998 ) , and CEAF(Luo , 2005 ) .   4.1.4 Implementation Details   To train the neural models in our experiments , we   use ADAM ( Kingma and Ba , 2014 ) as the opti-   mizer and set all model parameters that originated   in Yu and Poesio ’s ( 2020 ) model to the same val-   ues as those reported in their paper . Each model is   trained for up to 150 epochs in ISNotes and BASHI764   and up to 200 epochs in ARRAU , with early stop-   ping based on the development set .   For our model , we pre - train the corefer-   ence model for 15 epochs , and the remain-   ing parameters are chosen jointly using grid   search to maximize resolution F - score on de-   velopment data . Specifically , the weights as-   sociated with each task and the rule in the   loss function ( i.e. , the λ ’s ) are searched out of   { 0.1,0.5,1,5,10,20,30 } . The weights associated   with the mistake - driven cost functions ( i.e. , the   ∆ ’s ) are searched out of { 0.1,0.5,1,5,10,15,20 } .   The hardness coefficients of the consistency   constraints ( i.e. , the γ ’s ) are searched out of   { 0.05,0.1,0.5,1,5,10,20,30 } .   4.2 Baseline Systems   We employ three baselines . The first one is Rösiger   et al . ’s ( 2018 ) rule - based approach , which con-   sists of rules that are built on top of Hou et al .   ( 2014).The second one , Y&P - MTL , is Yu and   Poesio ’s ( 2020 ) MTL system . The third one is   the Hybrid rule - based and learning - based system   proposed by Kobayashi and Ng ( 2021 ) in which   the rules are first applied and then Y&P - MTL is   used to resolve the remaining bridging anaphors.4.3 Results and Discussion   Results are shown in Table 2 . A few points about   the baseline results deserve mention . First , in terms   of bridging recognition and resolution performance ,   the best baselines are Hybrid for both ISNotes and   BASHI and Y&P - MTL for ARRAU RST . Hence ,   these two baselines can be viewed as the prior state   of the art . Second , while Rösiger et al . ’s rule - based   model never achieves the best results on any of the   three datasets , it is not always the worst performer :   Y&P - MTL is the worst baseline on BASHI in terms   of resolution . Third , Hybrid fails to improve the   performance of Y&P - MTL in ARRAU RST , mean-   ing that the rules fail to provide additional benefits   to Hybrid . This could be attributed to the fact that   the rules in ARRAU RST have much lower recog-   nition and resolution precision scores than those in   ISNotes and BASHI ( Roesiger et al . , 2018 ) .   While Y&P - MTL uses undersampling ( to reduce   the number of negative examples used to train the   bridging module ) and a likelihood loss , we addi-   tionally experiment with a max - margin loss ( see   Section 3.4 ) without undersampling in our model .   To see how these two changes impact performance ,   we create another model , MM - MTL , which is sim-   ply a max - margin version of Y&P - MTL without765undersampling . Results on the development set   are mixed : while MM - MTL outperforms Y&P-   MTL on ISNotes and BASHI , the reverse is true   on ARRAU RST . Consequently , we use the max-   margin loss without undersampling when training   our model on ISNotes and BASHI , but fall back on   the likelihood loss with undersampling for ARRAU   RST . To better understand the impact of using a   max - margin loss with undersampling , we show in   Table 2 the test results of MM - MTL . As we can see ,   MM - MTL outperforms Y&P - MTL by 6.7–10.7 %   points in F - score for bridging recognition and 1.7 –   2.8 % points in F - score for bridging resolution in   ISNotes and BASHI .   The last row of each section of Table 2 shows   the results of our full model , which outperforms   the best baseline by 5.2–11.3 % points in F - score   for bridging recognition and 2.6–4.1 % points in   F - score for bridging resolution . Hence , the full   model establishes new state - of - the - art results on   these three datasets . For bookkeeping purposes , we   also report the scores for each component of our   model in terms of IS classification accuracy and   coreference CoNLL score .   4.4 Model Ablations   To evaluate the contribution of the different com-   ponents in our full model , we show in Tables 3 and   4 ablation results on ISNotes , which we obtain by   removing one component at a time from the model   and retraining it . Note that for coreference we show   the anaphor recognition results as they are affected   by the consistency constraints .   Consistency constraints . Ablating the consis-   tency constraints means removing all the penalty   terms from sands . The resulting system resem-   bles a typical multi - task learning setup , where the   different tasks only interact via a shared representa-   tion . As we can see in Table 3 , bridging resolution   F - score drops by 1.7 % points , coreference recogni-   tion F - score drops by 0.5 % points , and IS bridging   recognition F - score drops by 1.2 % points . These re-   sults suggest the effectiveness of using consistency   constraints in a multi - task setup .   Soft→Hard . Next , we replace soft constraints   with hard constraints . Comparing with the results   in row 2 , bridging resolution F - score drops by 1.2 %   points . This indicates that having hard constraints   is worse than having noconstraints at all .   Rule loss and feature . Bridging resolution F-   score drops by 1.1 % points when ablating only the   rule loss and by 2.4 % points when ablating both the   rule loss and the rule feature . These results suggest   that the rule feature is useful and that the rule loss   enhances the effectiveness of the rule feature .   Pre - training . Next , we do not pre - train the coref-   erence component in the multi - task framework .   This causes bridging resolution F - score and corefer-   ence recognition F - score to drop abruptly by 5.8 %   points and 3.9 % points respectively , suggesting the   important role played by pre - training .   Coreference resolution and IS classification   tasks . Next , we ablate one of the tasks in the   multi - task framework . Bridging resolution F - score   drops by 3.4 % points when ablating coreference   and by 3.3 % points when ablating IS classification .   These results suggest that both tasks contribute con-   siderably to bridging resolution performance .   Individual soft constraints . Finally , we ablate   one soft constraint at a time from the full model .   Results are shown in Table 4 . Bridging resolution   F - score drops by 1.2–2.3 % points , suggesting the   positive contribution of each soft constraint .   While our discussion of these results has focused   on bridging resolution , the same trends can be ob-   served for bridging recognition for the most part .   Overall , these results suggest that each component   contributes positively to bridging resolution .   4.5 Error Analysis   Although our full model outperforms all previous   models for bridging resolution , it is still far from   perfect . To better understand what areas of im-   provement are required , we discuss some common766errors made by our full model in this subsection .   Bridging anaphora recognition errors . Recall   errors in bridging anaphora recognition are the re-   sult of a system ’s failure in identifying bridging   anaphors . We find that on the three datasets , the   highest proportion of the recall errors ( 57 % on   ISNotes , 61 % on ARRAU , and 82 % on BASHI )   is due to the fact that a large number of bridg-   ing anaphors are misclassified as new orother   mentions in the IS classification module , such as   “ income ” in Example ( 1 ) in Table 5 .   Precision errors in bridging anaphora recogni-   tion are the result of a system ’s misclassification of   non - bridging mentions as bridging anaphors . Sim-   ilar to the recall errors described above , most pre-   cision errors are neworother mentions being mis-   classified as bridging , which account for 50 % , 74 %   and 82 % of the precision errors in ISNotes , AR-   RAU , and BASHI , respectively . In Example ( 2 ) ,   “ service ” is misclassified by both the bridging and   IS components as a bridging anaphor .   In general , it seems that our system struggles   to distinguish bridging anaphors from generic new   mentions with simple syntactic structures , an obser-   vation that has also been reported in previous work   ( Hou , 2021 ; Kobayashi and Ng , 2021 ) . Note that   most of these bridging ornew mentions are rela-   tional nouns ( de Bruin and Scha , 1988 ) . Normally ,   whether additional implicit arguments are required   to interpret such relational nouns depends on the   surrounding context . In Example ( 1 ) , “ the indus-   try ” is necessary to fully understand the meaning   of “ income ” ; while in Example ( 2 ) , no additional   implicit arguments are required to understand the   meaning of “ service ” .   Bridging anaphora resolution precision errors .   Precision errors in bridging anaphora resolution   appear when a system selects the wrong antecedent   for a bridging anaphor . A major reason for this   error is that our model largely fails to exploit con-   textual information . In Example ( 3 ) , the model   links the bridging anaphor “ a spokesman ” to the   wrong antecedent “ [ the state ] ” , which is reason-   able if one does not look into the context . However ,   according to the context , the correct antecedent   should be “ Gov. Deukmejian ” , which requires a   system to know that “ Gov. ” is the abbreviation for   “ Governor ” and that normally a governor will have   a spokesman .   In addition , on ISNotes , 6 % of the bridging   anaphors have a non - mention antecedent ( see “ a   result ” in Example ( 4 ) ) and 12 % of the bridging   anaphors have antecedents that are more than five   sentences away . Currently our system does not   handle these difficult cases .   5 Conclusion   We proposed the first neural model for full bridging   resolution that ( 1 ) exploits the connection between   information status classification , entity coreference   resolution , and bridging resolution in a multi - task   learning framework , ( 2 ) employs soft cross - task   consistency constraints to guide the learning pro-   cess , ( 3 ) pre - trains the entity coreference model ,   and ( 4 ) integrates prior knowledge encoded in hand-   crafted bridging resolution rules into the learn-   ing framework . Our model outperformed several   strong baselines and achieved state - of - the - art re-   sults on three evaluation datasets . Ablation results   provided suggestive evidence that each component   of our model contributed positively to bridging res-   olution performance .   Acknowledgments   We thank the four anonymous reviewers for their   insightful comments on an earlier draft of the paper .   This work was supported in part by NSF Grants IIS-   1528037 and CCF-1848608 . Any opinions , find-   ings , conclusions or recommendations expressed in   this paper are those of the authors and do not nec-   essarily reflect the views or official policies , either   expressed or implied , of the NSF.767References768   A Final Hyperparameters and   Computing Environment   We conduct our experiments using a NVIDIA   QUADRO RTX 6000 . The estimated GPU hour   per model in this paper is approximately 6 hours on   average . Table 6 shows the final hyperparameters   for our full model on the three datasets.769770