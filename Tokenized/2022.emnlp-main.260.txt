  Shujian Zhang Chengyue Gong Xingchao Liu   The University of Texas at Austin   { szhang19 , cygong , xcliu}@utexas.edu   Abstract   Retriever - reader models achieve competitive   performance across many different NLP tasks   such as open question answering and dialogue   conversations . In this work , we notice these   models easily overfit the top - rank retrieval pas-   sages and standard training fails to reason over   the entire retrieval passages . We introduce a   learnable passage mask mechanism which de-   sensitizes the impact from the top - rank retrieval   passages and prevents the model from overfit-   ting . Controlling the gradient variance with   fewer mask candidates and selecting the mask   candidates with one - shot bi - level optimization ,   our learnable regularization strategy enforces   the answer generation to focus on the entire   retrieval passages . Experiments on different   tasks across open question answering , dialogue   conversation , and fact verification show that   our method consistently outperforms its base-   lines . Extensive experiments and ablation stud-   ies demonstrate that our method can be general ,   effective , and beneficial for many NLP tasks .   1 Introduction   Retriever - reader based approaches are popularly   considered in the knowledge - intensive tasks ( e.g. ,   open Question Answering ( QA ) , fact verification ) .   It is designed to retrieve a set of support docu-   ments and extract the answer from these documents .   Mostly adopted retrieve and read models ( e.g. , Izac-   ard and Grave , 2020 ) are trained to generate the   annotated gold answers using the reader model ,   based on passages obtained by the retrievers ( e.g. ,   Robertson and Zaragoza , 2009 ; Karpukhin et al . ,   2020 ) . This training process of reader disregards   the evidentiality of all retrieval passages and can   easily overfit the top ranked passages ( Xu et al . ,   2021 ; Lee et al . , 2021 ) . Even if the top - rank pas-   sages in the test setting do not have the correct   answers , these models still tend to find the answer   in the top - rank passages and yield worse perfor-   mance ( Xu et al . , 2021 ) . It happens to the readermodel due to the overfitting and the memorization   of outdated information ( Longpre et al . , 2021 ) .   To what extent does the reader model quality   depend on the retrieval passages ? We analyze the   ranking impact of the retrieval passages from mask-   ing ( e.g.mask out the top three passages ) , permut-   ing , and removing . The overfitting , as well as the   performance degradation , is observed . To desen-   sitize the impact from the top - rank passages , we   consider masking passages during training which   serves as a desensitizer and can improve the reader   model ability to reason over all retrieval passages .   However , the standard masking and dropout   strategies are not designed for our focused tasks   and also bring an increased gradient variance dur-   ing training due to their randomness . In the mean-   time , each neuron plays the same role and has the   same mask . However , in the reader model , in-   tuitively , top - rank passages often have a higher   chance to overfit during the training . To this end ,   we introduce our passage mask ( PM ) , which en-   courages to mask top - rank passages . Reducing the   gradient variance with fewer mask candidates and   optimizing the mask candidates with bi - level opti-   mization , the mask magnitude for each candidate   can be learned . Overall , the proposed mask param-   eters are jointly optimized with the entire network .   We run extensive experiments across represen-   tative knowledge - intensive tasks : open - domain   QA ( Natural Questions Open ( Kwiatkowski et al . ,   2019 ) ; TriviaQA unfiltered ( Joshi et al . , 2017 ) ) , fact   verification ( FaVIQ ( Park et al . , 2021 ) ) , and knowl-   edge grounded dialogue ( Wizard pf Wikipedia ( Di-   nan et al . , 2018 ) ) . Our method shows large per-   formance improvements across different tasks and   datasets . Furthermore , we provide extensive ab-   lation studies on different design choices for the   proposed method , including the designs of masking   candidate space and efficiency . Our analysis shows   the passage mask contributes the performance im-   provement , helping the reader learn to focus on3931the retrieval passages without being distracted by   high - ranked passages with more lexical overlaps .   With little modification , our regularization can be   easily applied to other NLP tasks for a better an-   swer generation strategy . To the best of our knowl-   edge , we present the first mask regularization in   the open retriever - reader setting by preventing the   rank - related overfitting in Open QA , dialogue con-   versation , and fact verification . Our contributions   are summarized as follows :   •Demonstrate that current models , e.g. , Fusion-   in - Decoder ( Izacard and Grave , 2020 ) , tend   to find answers in top - rank passages . These   models are neither robust to passage drop nor   able to utilize the entire retrieval passages .   •Present a passage mask mechanism for re-   trieval reader models . It improves the model   generalization and encourages the model to   extract answers from all the passages .   •Propose an efficient and effective way to train   the model and the mask hyper - parameters   jointly , which can one - shot search passage   mask hyper - parameters . First , we use smaller   number of mask candidates to reduce train-   ing gradient variance . Second , we jointly   optimize the model parameters and mask   candidate choices ( a.k.a . , parameters ) with   theoretically - converged bi - level optimization .   •Verify the effectiveness and general applica-   bility of the proposed method in knowledge   intensive NLP tasks , e.g. , open question an-   swering , fact verification , and dialogue tasks ,   and provide a rich analysis of this method with   various design choices such as the masking   position and efficiency . The proposed strat-   egy can be easily incorporated or extended to   many other NLP tasks .   2 Method   2.1 Knowledge - intensive Tasks   Knowledge - intensive tasks ( e.g. , open QA , dia-   logue conversations ) require to access a large body   of retrieval information . A retrieval - augmented   generation framework such as Fusion - in - Decoder   ( FiD ) ( Izacard and Grave , 2020 ) that consists of   two components : a retriever model Rand a gen-   erator model Ghas demonstrated the competitive   performance and scalability to the large collection   of retrieval evidence . FiD uses Dense Passage Re-   trieval ( DPR ) ( Karpukhin et al . , 2020 ) to retrieve a   set of documents , and the decoder attends over the   concatenation of all encoded document representa-   tions to generate the final answer . Specifically , the   retriever model Ris trained to retrieve a set of pas-   sages Pwith the highest top K relevance score for   each training query . Gis then trained to generate   the final output ˆygiven an input query xand the   top retrieved passages : ˆy = G(x , P ) .   Although FiD does not use the unnormalized   passage score as DPR , we still find out that FiD has   a preference over passages with higher retrieval   passage scores . Our analysis in Table 1shows   thatGtrained in this manner overfits the passages   ranked high by the retriever . In this work , our goal   is to prevent the overfitting , extract the answers in   all given passages and improve the model general-   ization during the reader training .   2.2 Reader Model   The overall FiD reader model is composed of the   encoder and the answer generator .   Encoder . Each retrieved passage and its title are   concatenated with the question and processed inde-   pendently by the encoder . We add tokens question : ,   title : andcontext : before the question , title and text   of each passage . The input query xis prepended   to each passage ( Asai et al . , 2021 ) . The encoder is   usually a pre - trained T5 ( Raffel et al . , 2020 ) .   Answer Predictor . Mark has a summary rep-   resentation of the input , formed by concatenating   the final - layer hidden state of passages . his fed   into the answer predictor and the final answer is   autoregressively output .   Objective . In the encoder - decoder structure , we   train the answer generator Ggiven the originally3932   available data ( x , y ) . In particular , our framework   with the model parameter θis defined as :   L=−/summationdisplaylogp(y|y , x , h),(1 )   where ydenotes the jth token of the annotated   gold answer y. The generator is based on the T5   architecture and uses cross attentions to model the   interactions between retrieved passages ( Izacard   and Grave , 2021 ) . This probability is normalized   over T5 vocabulary .   2.3 Passage Mask   Since the over - parameterized neural networks are   prone to overfitting , regularization methods such as   mask and dropout ( Srivastava et al . , 2014 ; Tomp-   son et al . , 2015 ; DeVries and Taylor , 2017 ; Fan   et al . , 2021 ) are usually adopted during training to   reduce the generalization error . Specifically , these   methods randomly drop part of units in each neu-   ral network layer to avoid co - adapting and overfit-   ting . Intuitively , mask and dropout approximately   perform to combine exponentially many different   neural network architectures efficiently ( Srivastava   et al . , 2014 ; Ghiasi et al . , 2018 ) .   There are few studies of the mask about the   reader model training in the retrieval - reader set-   tings . In a standard training setting , each neuron   plays the same role and has the same mask rate . In   the reader model , intuitively , top - rank passages of-   ten contain the answer and are easy to overfit while   the other passages have fewer chances to be fitted .   Based on our above observations , we propose the   Passage - Mask ( PM ) to regularize the top - rank pas-   sages which have larger probabilities to overfit asdemonstrated in Figure 1 . Briefly , we propose to   drop top - rank passages during training .   Though simple and effective , masking increases   the gradient variance during training due to its ran-   domness . To reduce gradient variance and lead to   stable training , we propose to downsize and select   the candidate set of masking with one - shot bi - level   optimization in this work .   2.4 Mask Candidates   Denote Ppassage each with lentokens as t=   ( t,···,t)where t= ( t , t , · · · , t ) . We   pass the passages tthrough the reader model   and get h= ( h , h,···,h)where h=   ( h , h , · · · , h)is the corresponding final-   layer hidden state of a passage . Let DPbe a set   of mask choice ( e.g. , retrieval passages ) with N   candidates and each is denoted as o. For a typically   selected mask candidate , we define the mask index   set{i|i≤P , i∈N}where Pis the number of   passages and mask all the corresponding h.   To relieve the noisy gradient ( large gradient vari-   ance ) , we reduce the size of candidate set . Numer-   ous works ( e.g. , Ge et al . , 2015 ; Jin et al . , 2017 ;   Daneshmand et al . , 2018 ; Chen et al . , 2020 ) have   shown that the strong noisy gradient in the back-   ward pass caused by the dropout mask is detrimen-   tal to the model optimization . The gradient noise   is highly related to the number of drop candidates .   As only the top - rank passages play a huge impact   during the reader model training , we reduce the   size of mask candidates with preferences to mask   top - rank passages .   2.5 Fast Search for Mask Candidate Set   To decide the final candidate subset , instead of   manual search or grid search ( Bergstra and Ben-3933gio , 2012 ; Li and Talwalkar , 2020 ) all the possi-   ble candidates , we propose to do a one - shot fast   search of mask candidate with an almost negligible   additional computation cost compared to standard   training schedule . First , we define the search space .   Discrete Search Space . To automatically choose   candidates , we consider a set DPwithNcandi-   dates and target at selecting Scandidates for our   Passage - Mask ( S < N ) . Inspired by Zoph et al .   ( 2018 ) ; Liu et al . ( 2018 ) ; Hong et al . ( 2022 ) , we   create Svectors , and each is a N - dimension vec-   tor representing the selected probability for all the   Ncandidates . We denote the hyper - parameter as   w∈R , and each mask candidate as a function   o(h)where his hidden representations for P.   To make the search space continuous , during   training , we relax the categorical choice of a par-   ticular operation to a SoftMax over all possible   operations , and the output is defined as ,   ¯h=/summationdisplayexp ( w)/summationtextexp / parenleftbig   w / parenrightbigo(h),(2 )   where s∼ { 1 , · · · , S}is sampled with equal prob-   ability . Then we pass the ¯hto the answer gener-   ator . During inference or evaluation , a discrete ar-   chitecture can be obtained by replacing each mixed   operation ˆhwith the most likely operation . i.e. ,   h = o(h ) , o= argmaxw .   Bi - level Optimization . To avoid grid - search over   the mask schedule , we target at jointly learning the   model parameter θand the mask hyper - parameter   w. Formally , the training and the validation sets are   denoted by DandD , respectively . The goal   for this optimization is to find θthat minimizes   the train loss θ= arg minL(θ , w ) , where   wis obtained by minimizing the validation loss   w= arg minL(θ , w ) . For simplicity , we   writeL andLasfandg , respectively .   This implies a bi - level optimization problem   ( Franceschi et al . , 2017 ; Shaban et al . , 2019 ; Grazzi   et al . , 2020 , 2021 ) which has been shown the effec-   tiveness in the many machine learning fields such   as hyperpameter optimization and meta - learning   ( e.g. Yang et al . , 2021 ; Guo et al . , 2021 ; Khanduri   et al . , 2021 ) . We optimize ,   ( 3)Algorithm 1 : Passage Mask ( PM)Input : Passage P , query x. Model para-   meter θwith learning rate α , mask para-   meter wwith learning rate β , update   frequency uand time step t.fort= 0to final step doθ←−θ−α∇(θ ) , ift%u==u−1then w←−w−βgradwhere gradis   calculated by Eqn ( 4 ) . end ifend for   where f , g : R×R→Rwith θ∈   R andw∈R ; In practice , we do   stochastic sample to estimate the expectation value   E ( · ) . Note here that fdepends on the minimizer   of the mask hyper - parameter objective g , and we   refer to ℓ(θ)as the training objective function .   We adopt the recursive momentum techniques   developed in ( Cutkosky and Orabona , 2019 ; Tran-   Dinh et al . , 2019 ) which yield for - free one - shot   training . In summary , our updated mask schedule   can be summarized as the below . Define η∈   [ 0,1 ] , for the problem involving x , we utilize the   following momentum - assisted gradient estimator ,   grad∈R , defined recursively as   grad = η∇g(θ , w ) .   ( 4 )   The gradient estimator gradare computed from   the current and past gradient estimates ∇g(θ , w )   and∇g(θ , w ) . Recent theoretical works in   ( Khanduri et al . , 2021 ; Ji et al . , 2021 ; Yang et al . ,   2021 ) have provided the convergence analysis for   the momentum - based recursive optimizer . Thus ,   PM takes benefits from the model - independent   sample complexity and good convergence .   The Proposed Algorithm . Our passage mask   with momentum - based recursive bi - level optimiza-   tion is shown in Algorithm 1 . We iteratively up-   date the model parameter θand mask parameter   win a single - loop manner . The model parameter   θis updated by standard gradient descent , while   wis updated in a momentum recursive technique   ( Cutkosky and Orabona , 2019 ) with a given fre-   quency uto save computation . We further show   in the experiments that the proposed method can3934effectively prevent overfitting , improve the model   generalization and introduce little additional time   cost .   3 Experimental Settings   Table 2 shows the experimental data configuration .   3.1 Task and Evaluation Metrics   Open Question Answering . We use Natural   Questions ( NQ ) ( Kwiatkowski et al . , 2019 ) and   TriviaQA ( Joshi et al . , 2017 ) to evaluate our method   on open QA . Natural questions consists of 79,168   train , 8,757 dev , and 3,610 test question answer   pairs . It contains questions corresponding to   Google search queries . The open - domain version   of this dataset is obtained by discarding answers   with more than five tokens . TriviaQA ( Joshi et al . ,   2017 ) contains questions gathered from trivia web-   sites . The unfiltered version of TriviaQA is used   for open - domain question answering . Following   the open domain splits from ( Lee et al . , 2019 ) , it   contains 78,785 train , 8,837 dev , and 11,313 test   question answer pairs . For both datasets , we use   publicly available DPR retrieval results for training   and inference data , and do not further fine - tune re-   trievers . Following prior work ( Lee et al . , 2019 ) ,   we use Exact Match ( EM ) as our primary metric .   Dialogue Conversation . Wizard of Wikipedia   ( WoW ) ( Dinan et al . , 2018 ) is a large dataset with   conversations directly grounded with knowledge   retrieved from Wikipedia . The utterances of the   speaker should be based on a specific knowledge   sentence from a Wikipedia page . We utilize the   officially available KILT DPR ( Petroni et al . , 2020 )   to extract top passages and report F1 score for eval-   uation ( Asai et al . , 2021 ) . Pre - process to match   our setting : As PM prevents the model from over-   fitting the top - rank passages , we preprocess the   existing development and test dataset by removing   the examples with the answers in the top four pas-   sages . Evaluating such a dataset , a model can not   provide the true answers if it is overfitted on top 4   passages . This results in 974 dev and 989 test . We   report both the preprocess results ( Section 4.3 ) and   the non - preprocess results ( Section 5 ) .   Fact Verification . FaVIQ ( Park et al . , 2021 ) rep-   resents fact verification derived from information   seeking questions , where the model is given a nat-   ural language claim and predicts support or refute   with respect to the English Wikipedia . FaVIQ Am - big ( FaVIQ - A ) is composed from Natural Ques-   tions ( Kwiatkowski et al . , 2019 ) and AmbigQA   ( Min et al . , 2020 ) . It is constructed from ambigu-   ous questions and their disambiguation . We use   the retrieved passages and baseline code provided   by Park et al . ( 2021 ) . Accuracy is adopted as our   evaluation metric .   3.2 Implementation Details   Due to the computational budget , we use the pro-   vided checkpoint for the reader model and con-   tinue the finetuning with our method . To have   fair comparisons , we also finetune the checkpoint   with standard training ( Details are included in Sec-   tion 5 ) . For Open QA , following the setting in   Izacard and Grave ( 2021 ) , we utilize the provided   checkpoint for the reader and use the top 100 pas-   sages during training and inference . We set the   training steps as 30k and take the checkpoint that   achieves the highest score on the development set .   The batch size and the gradient accumulation step   are both set to be 1 . The learning rate is set to   5×10and the number of warm - up steps is   3k . For dialogue conversation and fact verifica-   tion , following the setting and the checkpoints in   ( Asai et al . , 2021 ) , we use the top 20 passages   during training and inference . We set the gradi-   ent accumulation step to be 4 , with learning rate   10and 1k warm - up steps . The development set   is used for bi - level optimization . Search Space .   In all experiments , we use the top four retrieval   passages to compose our candidate search space ,   { ( 1,2),(1,3),(1,4),(2,3),(2,4),(3,4 ) } , where   ( 1,3)is a candidate which indicates that the hid-   den representation of the 1st and 3rd passages are   masked . More detailed experimental settings are   included in Appendix A.   4 Experiments   We evaluate the performance of our mask and learn-   ing framework in this section . We bold the best   result within each column block . The results of our   method are obtained with three independent runs3935to determine the variance . See Appendix A for full   results with error bars .   4.1 Open - Domain QA Results   We first report the results in Table 1 . We use the   FiD ( Izacard and Grave , 2021 ) base reader model   on Natural Questions Open ( Kwiatkowski et al . ,   2019 ) . To verify that the model overfits the top-   rank passages , we purposely mask top retrieval   passage representations based on the mask position .   We observe huge performance degradation ( e.g. ,   50.1to44.5 ) by masking the top one passage rep-   resentation and even larger performance drop ( 50.1   to35.7 ) by masking the top five retrieval passages .   Table 3 reports our results on two open question   answering datasets . ❶The top block displays the   performance of baselines on the NQ and TriviaQA   datasets , and the bottom block shows the results of   incorporating the PM during the reader model train-   ing . We report the results on both base and large   settings . With PM , it shows consistent performance   gains and better model generalization on both de-   velopment and test dataset ( e.g. ,50.1→51.3on   NQ with FiD base , 54.4→55.3on NQ with FiD   large ) . ❷Through these results , it further confirms   that PM can work as an effective module to be in-   corporated into different - scale models to prevent   the overfitting on the top retrieval passages and rea-   son over the entire passages . ❸PM on improving   the reader model can be also seen as a comple-   mentary module to works focusing on improving   retrieval components ( Paranjape et al . , 2021 ; Mail-   lard et al . , 2021 ) .   4.2 Fact Verification   We further show the experimental results on FaVIQ-   A in Table 4 . We adopt several baselines from the   existing literature . ①For TF - IDF + BART , follow-   ing Park et al . ( 2021 ) , it takes a concatenation ofa claim and retrieved passages by TF - IDF from   Chen et al . ( 2017 ) . ②DPR + BART , the baseline ,   takes a concatenation from passages retrieved by   DPR ( Karpukhin et al . , 2020 ) . ③For EQA , follow-   ing Asai et al . ( 2021 ) , it is built on FiD ( Izacard   and Grave , 2020 ) pipeline with T5 base and fur-   ther incorporates evidentiality of passages into the   training of the generator .   In Table 4 : ❶We observe sizable gains over   all baselines with a clear margin ( from FiD ’s 64.3 ,   from EQA ’s 65.7 to ours 66.5 ) , yielding SOTA per-   formance on this dataset . ❷PM demonstrates the   strong capability of avoiding overfitting during the   training and allowing the reader model to extract   the information from all passages . Thus , it comes   to the best performance in most of the settings .   4.3 Dialogue Conversations   Table 5 shows the results on the Wizard of   Wikipedia development dataset . We use the FiD   ( Izacard and Grave , 2021 ) as our primary baseline ,   and also include the recent generator model EQA   ( Asai et al . , 2021 ) . Following Asai et al . ( 2021 )   and Petroni et al . ( 2020 ) , we load the official check-   point from KILTand pre - processed Wikipedia file   using the DPR official implementation to retrieve   top passages . On Wizard of Wikipedia , by desensi-   tizing the impact from the top - retrieval candidate ,   our model improves the F1 score from the EQA   by 0.7 and the base FiD model by 1.6 . Although   the input format is conversation and output format   is long abstractive sentences , it is interesting to   see the consistent improvement of our proposed   mask in knowledge - enhanced dialogue . It further   demonstrates that PM can be utilized for many   ranking - related problems in general NLP tasks .   5 Analysis   What is the influence of the vanilla mask and   Dropout ? Here we verify whether PM is better3936   than the standard dropout and masking out strate-   gies . With the designed mask candidates , PM tar-   gets the top retrieval passages . We compare PM   with two standard masking out setting - dimension-   wise dropout and vanilla mask . Dimension - wise   dropout represents the standard dropout while   vanilla mask represents per - passage mask with a   scaling factor 1/(1−p)where pdenotes the mask   rate . We set the dropout rate and masking as 0.5   and study whether the standard masking out is ap-   plicable to our focused tasks . As shown in Table 6 ,   these two strategies only achieve marginal improve-   ments ( e.g.0.1 ) while PM yields better results with   a clear margin . Training Loss Variance . To verify   the small number of candidates coming to a smaller   gradient variance , we investigate the training loss   variance for vanilla mask with the different number   of candidates . We notice that the vanilla mask with   a smaller number of candidate set achieves smaller   variance ( for s.t.d . , 0.042 for six mask candidates   vs. 0.046 for sixteen mask candidates ) . This gets   along with our intuition .   More evidence for rank - related overfitting ? ❶   We observe huge performance degradation by only   masking the top retrieval passage representation   during evaluation in Table 1 . These results con-   firm our analysis and motivation for the rank - aware   mask . ❷However , would these results and ob-   servations still hold if we try different masking   strategies ? We use more masking strategies , such   as permuting ( i.e. ,random permute the top - K re-   trieval passages ) and removing ( i.e. ,remove the   top one retrieval passage and only use the succeed   passages ) , to give more evidences . Similar trend is   observed in Table 7 .   Efficiency and running time . We provide the pa-   rameter sizes , GPU peak memory , and per step time   comparisons between the baseline and PM . Experi-   ments in this part are performed on a Tesla V100   GPU during training with batch size as 1 . ❶Table   8 shows that PM keeps the parameter size at the   same level as the FiD base . The GPU memory and   running time of PM are slightly higher ( 2.7 % for   memory and 1.6 % for running time ) than FiD. PM   gives the best Exact Match score outperforming   FiD , while keeping the comparable efficiency and   running time . ❷Even with the momentum - based   recursive optimizer , our passage - aware mask is still   computational productive as the bi - level optimiza-   tion ( e.g. , applying mask operators and optimizing   low - dimension w ) has almost zero cost .   Ablation studies on the components in PM . We   conduct the ablation study to exam the role of bi-   level optimization and reduced mask candidate set .   For ablation , instead of searching the mask proba-   bility for different mask candidates , we randomly   sample a candidate in the search space . Through   isolating performance of each components , our fo-   cus here is to identify the impact of the introduced   mask parameter wand the reduced mask set . ❶   Table 9 shows that each component of our method   brings benefits . ❷We find that even without w ,   ‘ w ’ still shows a superior performance to the FiD   across both base and large models , indicating that   it is often beneficial to have the reduced mask can-   didate set and target the potential overfitting candi-   dates.❸Optimizing wfurther increases the perfor-   mance from 50.8to51.3and from 55.0to55.3for   FiD base and Large , respectively . It demonstrates   the necessity and effectiveness of the fast search   for mask candidate set in PM structure.3937   WoW additional results . We show the non-   preprocessed development set results on the Wiz-   ard of Wikipedia in Table 5 . We include the RAG   ( Lewis et al . , 2020 ) , DPR + BART ( Petroni et al . ,   2020 ; Park et al . , 2021 ) , and EQA ( Asai et al . ,   2021 ) as baselines . Even without removing the ex-   amples which has the answers in the top 4 passages ,   PM consistently yields better results than all the   baselines . These results verify our conjecture in   Section 4.3 that PM not only improves the model   generalization for specific cases but also can serve   as a plug - in module for general settings since it   never hurts the performance in our case .   Would we see improvements if finetuning the   given checkpoint with baselines ? As discussed   in Section 3.2 , due to computation cost limita-   tion , we use the provided checkpoint for the reader   model and continue the finetuning with our method .   However , if we continue finetuning the baseline   checkpoint , would we still see the improvements ?   We conduct the experiments on open QA , dialogue   and fact verification tasks . We adopt the best base-   line models for each task such as FiD base for NQ   and TriviaQA , and EQA base for dialogue con-   versations and fact verification . In Table 11 , ours   indicates strong improvements . This further proves   that our selection method is capable of reasoning   over the retrieval passages . By only finetuning the   baselines , it keeps similar performance such as the   baseline on WoW and FaVIQ - A.   6 Related Work   Retrieval Read Architecture Recent retriever   models ( e.g. , Lee et al . , 2019 ; Karpukhin et al . ,   2020 ; Khattab et al . , 2021 ) learn to encode the   input query and large - scale passage collection to   score their similarities . Readers ( generators ) aim   to generate answers condition on the question and   the retrieved passages ( Yang et al . , 2019 ; Lewis   et al . , 2020 ; Mao et al . , 2020 ) . Our work relies on   this architecture and further fine - grain the reader   model to introduce the passage - aware masking and   promote the reasoning over the entire passage set .   Rank - Related Studies Passage ranking has   shown promising performance improvements . The   most popular approach is combining the passage   score and answer score together ( Karpukhin et al . ,   2020 ; Xiong et al . , 2020 ; Qu et al . , 2020 ) . Other   works ( e.g. , Nogueira et al . , 2020 ; Fajcik et al . ,   2021 ; Zhang et al . , 2021b ) propose additional mod-   ules or operations to re - identify the passage rank .   Nogueira et al . ( 2020 ) uses seq2seq model to iden-   tify the document ’s relevance to the query , Fajcik   et al . ( 2021 ) introduces a passage re - ranking mod-   ule , and Zhang et al . ( 2021b ) proposes to use the   calibrator as an answer reranker . There are some   works that focus on the ranking efficiency . Luan   et al . ( 2021 ) creates a simple neural model that   combines the efficiency of dual encoders . Simi-   larly , we also find out that directly taking the rank   makes the model overfitting . Different from ex-   isting works , PM rethinks the impact of retrieval   passage ranking from the regularization and gener-   alization perspective . We focus on preventing the   overfitting and improving the reasoning generaliza-   tion during training . In the meantime , PM is also   compatible with other previous ranking works with   the potential to jointly improve the performance .   7 Conclusion   Our work demonstrates the benefits of introduc-   ing a passage mask mechanism . The proposed   mask can desensitize the impact from the top - rank   retrieval passages and prevent the model from over-   fitting . The proposed strategy shows noticeable   gains in performance across open question answer-   ing , dialogue conversation , and fact verification .   We further conduct the detailed study with the pro-   posed masking strategy in different settings , e.g. ,3938comparing with vanilla masking , providing more   evidence for rank - related overfitting , and verifying   the impact of different components . To summarize ,   the proposed PM is effective and general , with the   potential to be incorporated into existing models   for various NLP tasks .   8 Limitations   In real practices or real - life scenarios , the data is   often biased . The gap between the training and   testing data might be large and unexpected . Thus ,   incautious implementation or vague understanding   of model output might lead to unanticipated false   consequences . In addition , with computational con-   sumption , environmentally sustainability and users   friendly should be considered .   Acknowledgments   The authors thank Eunsol Choi and Anqi Lou for   helpful comments on the paper draft .   References393939403941A Experimental details   A.1 Full Results With Error Bar   We report the full results of our method with the   error bar for open question answering and dialogue   conversations in Table 12 and 13 , respectively . The   full result of fact verification is demonstrated in   Table 14 .   A.2 Experimental Datasets   Open Question Answering . Following the set-   ting in Lee et al . ( 2019 ) and Karpukhin et al . ( 2020 )   for Natural Questions and TriviaQA , the original   development set is used as the test set , and 10 %   of the training set is used as the development set .   All questions with answers longer than five tokens   are discarded for the Natural Questions . We use   the Wikipedia dumps from Dec. 20 , 2018 for NQ   and TriviaQA and apply the same preprocessing as   Chen et al . ( 2017).Fact Verification . FA VIQ ( Park et al . , 2021 ) rep-   resents fact verification derived from information   seeking questions , where the model is given a nat-   ural language claim and predicts support or refute   with respect to the English Wikipedia . It con-   sists of 188k claims derived from an existing cor-   pus of ambiguous information - seeking questions .   FaVIQ Ambig ( FaVIQ - A ) is composed from Natu-   ral Questions ( Kwiatkowski et al . , 2019 ) and Am-   bigQA ( Min et al . , 2020 ) . AmbigQA provides dis-   ambiguated question - answer pairs for NQ ques-   tions , thereby highlighting the inherent ambiguity   in information - seeking questions . FaVIQ - A uses   the disambiguated question - answer pairs and gener-   ates support and refute claims from matching pairs   ( filmed–2000 , released–2001 ) and crossover pairs   ( filmed–2001 , released–2000 ) , respectively ( Park   et al . , 2021 ) .   Dialogue Conversation . With the goal of mak-   ing virtual assistant conversations more engaging   and interactive , Sun et al . ( 2020 ) develops an en-   gaging chatbot that can discuss a variety of topics   with a user . The conversation history and the next   utterance are used as input and output , respectively   ( Petroni et al . , 2020 ) . Wizard of Wikipedia ( WoW )   ( Dinan et al . , 2018 ) is a large dataset of conver-   sation grounded with knowledge retrieved from   Wikipedia . In the conversation , the utterances from   the speaker should be relied on a specific knowl-   edge sentence from a Wikipedia page .   A.3 Experimental Settings   For Open QA , we follow the setting in ( Izacard   and Grave , 2020 , 2021 ) and initialize our models   with the pretrained T5 model ( Raffel et al . , 2020 )   from the HuggingFace Transformer library(Fan   et al . , 2020 ; Zhang et al . , 2021a ) . Two model sizes ,   base ( 220 M parameters ) and large ( 770 M param-   eters ) , are considered . We finetune the models on   each dataset independently and use provided check-   points from ( Izacard and Grave , 2021 ) . Following   Izacard and Grave ( 2021 ) , we adopt the AdamW   ( Loshchilov and Hutter , 2017 ; Zhang et al . , 2022 )   with the learning rate 5×10and weight decay   0.25 . The training step is 30k . The batch size and   gradient accumulation step are both set to 1 . The   development dataset is used for bi - level optimiza-3942tion and the warm - up steps is 3000 . We evaluate   models every 500 steps and select the best one on   the validation set based on the Exact Match score .   For Natural Question , we sample the target among   the list of answers during the training . For Trivi-   aQA , we use the unique human - generated answer .   For both training and testing , we retrieve 100 pas-   sages and truncate them to 250 word pieces . The   retrieval passages are from DPR ( Karpukhin et al . ,   2020 ) for NQ and TriviaQA .   For fact verification and dialogue conversation ,   following Petroni et al . ( 2020 ) and Asai et al .   ( 2021 ) , we use the top 20 passages during train-   ing and inference . The batch size is set to 1 . We set   the gradient accumulation step to be 4 to keep the   same batch size as previous works . The AdamW   ( Loshchilov and Hutter , 2017 ) with the learning   rate1×10and weight decay 0.25 are utilized .   The training steps are 30k and warm - up steps are   1k . Following ( Asai et al . , 2021 ) , for fact verifi-   cation , we report the accuracy as evaluation metric   and report the results on FaVIQ - A test set in Table   4 . For dialogue , we evaluate model based on the F1   score and report the results on WoW development   set in Table 5.3943