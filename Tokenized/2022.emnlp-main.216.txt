  Xin Li , Xiaojie Zhang , Jiahao Peng , Rui Mao , Mingyang Zhou , Xing Xie , Hao Liao   Shenzhen University , China   Microsoft Research Asia   { 1910273046 , 1800271040 , 2070276145}@email.szu.edu.cn   { mao , zmy , haoliao}@szu.edu.cn   xing.xie@microsoft.com   Abstract   The bloom of the Internet and the recent break-   throughs in deep learning techniques open a   new door to AI for E - commence , with a trend   evolved from using a few financial factors such   as liquidity and profitability to using more ad-   vanced AI techniques to process complex and   multi - modal data . In this paper , we tackle the   practical problem of restaurant survival predic-   tion . We argue that traditional methods ignore   two essential aspects , which are very helpful for   the task : 1 ) modeling customer reviews and 2 )   jointly considering status prediction and result   explanation . Thus , we propose a novel joint   learning framework for explainable restaurant   survival prediction based on the multi - modal   data of user - restaurant interactions and users ’   textual reviews . Moreover , we design a graph   neural network to capture the high - order inter-   actions and design a co - attention mechanism to   capture the most informative and meaningful   signal from noisy textual reviews . Our results   on two datasets show a significant and consis-   tent improvement over the SOTA techniques   ( average 6.8 % improvement in prediction and   45.3 % improvement in explanation ) .   1 Introduction   Business survival prediction is a hot topic in man-   agement and finance literature . Traditional meth-   ods rely heavily on financial factors to research(e.g . ,   liquidity , solvency , and profitability)(Ziman , 1991 ;   Lussier , 1996 ; Pereira et al . , 2020 ) . However , there   are two significant drawbacks : 1 ) the financial fac-   tors of a shop / company are hard to obtain due to   privacy issues ; 2 ) meanwhile , financial factors are   macro indicators that reveal the status of a business   only on a coarse level . With the development of in-   formation techniques , much restaurant - related data   can be collected online . For example , people can   post check - ins after consuming in a restaurant , and   they can share reviews to show how / why they like   the restaurant via an online review platform , suchas Yelp.com . Moreover , reviews contain informa-   tive users ’ feedback on a fine - grain level . More   importantly , the feedback which deeply reflects the   restaurant ’s operating status , can in turn help to   generate explainable prediction reasons . Some re-   cent research works also verify this , and the use   of online reviews to understand business perfor-   mance is an emerging trend(Babi ´ c Rosario et al . ,   2016)(Kong et al . , 2017 ) .   Recent advances in deep learning have various   models that research reviews and interactions for   different kinds of tasks , such as recommendation   ( Wang et al . , 2019 ) , fake news detection ( Potthast   et al . , 2018 ; Wang , 2017 ) , rating prediction ( Tay   et al . , 2018 ) , but little attention has been paid to the   application of restaurant survival analysis . In this   paper , we propose a novel joint learning framework   to tackle the challenging task of explainable restau-   rant survival prediction . Our model consists of two   compulsory modules : the co - attention network for   selecting valuable review texts and the graph neural   network for learning high - order interactions on the   user - restaurant graph . Specifically , the co - attention   mechanism is used to select meaningful review   text , which is a feature selection and learning pro-   cess . The graph of user - item interactions could   reveal the preference similarity between users ( or   items ) . Therefore , the construction of graph neural   networks , on which we encode high - order relation-   ships , can en- hance the representation of reputed   users and high - quality restaurants by modeling the   high - order interaction between user and restaurant ,   which is the key to our modeling exemplification .   Merely predicting the future status of restaurant   survival is inadequate . It is also critical for busi-   nesses to understand why they will prosper or close   in the future . Fortunately , we can leverage NLP   models to encode the massive user reviews and   output some explanations , just like a document   summarization process . To this end , we jointly   train the survival prediction and explanation task.3285The prediction task predicts the future status of   the restaurant , and the explanation task generates   some explainable texts to provide an informative   summarization for the restaurant ’s management .   We named the unified framework Restaurant Sur-   vival Prediction and Explanation ( RSPE ) . Through   experiments , we find that RSPE significantly im-   proves the performance of both tasks compared   with several competitive baselines .   The main contributions of our framework are as   follows .   1 ) We propose a new joint learning framework   for predicting the survival of restaurants and gener-   ating summarizing texts through reviews and inter-   actions .   2 ) We design two key components in RSPE ,   i.e. , the co - attention component , which mines high-   quality and informative reviews , and the graph rep-   resentation component to encode high - order inter-   actions on the user - restaurant graph .   3 ) We conduct extensive experiments on Dian-   ping and Yelp datasets . Our model outperforms all   SOTA methods significantly on both prediction and   explanation tasks , with an average improvement of   up to 6.8 % on the prediction task and 45.3 % on the   explanation task .   2 Related Work   Restaurant Survival Analysis : Store survival   analysis is an essential and practical research topic   in the financial and marketing field , which offers   deep insights into stores ’ financial affairs , market-   ing strategies , and management ( Parsa et al . , 2005 ;   Kim and Gu , 2006 ; Liang et al . , 2016 ; Du Jardin ,   2017 ) . Traditionally , researchers usually leverage   restaurant financial factors to build linear forecast-   ing models , which are sensitive and hard to ob-   tain . With the development of online services , re-   searchers find that User Generated Content ( UGC ) ,   such as textual reviews from Yelp.com or Dian-   ping.com , contains massive information covering   diverse aspects of stores ( restaurants in this paper ) .   Leveraging the heterogeneous UGC can effectively   improve the performance of restaurant survival   prediction models ( Lian et al . , 2017 ) . However ,   the main weaknesses of this group of methods are   tthreefold : 1 ) They used traditional NLP models   such as LDA , bag - of - words , or word2vec ; 2 ) they   did not consider the interaction graph between cus-   tomers and restaurants ; 3 ) they did not explicitly   reduce the noisy information from the raw UGC.Pre - trained Model : The pre - trained model has   been widely used in the field of NLP . It is trained   on large - scale open - domain datasets with self-   supervised learning tasks to encode common lan-   guage knowledge into the model . The well - trained   model can be fine - tuned with a small amount of   labeled data to perform well on the given target   task . For example , BERT ( Devlin et al . , 2019 )   is a multi - layer bidirectional Transformer encoder   and uses Masked Language Model ( MLM ) and   Next Sentence Prediction ( NSP ) to capture word   and sentence - level representations . UniLM ( Dong   et al . , 2019 ) is based on Bert , which achieved   great success on NLP tasks such as unidirectional ,   bidirectional , and sequence - to - sequence prediction .   Moreover , some studies ( Qiu et al . , 2020 ) have   shown that the pre - trained model is capable of cap-   turing hierarchy - sensitive and syntactic dependen-   cies , which is beneficial to downstream NLP tasks .   Graph Representation : Graph Neural Net-   work ( GNN ) is a key component in our framework .   GNNs represent a node by fusing self - information   with neighborhood information on the graph in   a message - passing manner . For example , Light-   GCN ( He et al . , 2020 ) simplifies the classical GCN   ( Kipf and Welling , 2017 ) and NGCF ( Wang et al . ,   2019 ) by removing the transformation layer and   non - linear activation functions , and uses a mean   pooling aggregator to fuse the neighborhood in-   formation . It handles the homogeneous graph .   The Heterogeneous Graph Neural Networks model   ( HetGNN ) ( Zhang et al . , 2019 ) considers hetero-   geneous structural ( graph ) information as well as   heterogeneous contents information of each node .   Several investigations ( Battiston et al . , 2021 ) have   already shown that the presence of higher - order   interactions may substantially impact the dynamics   of networked systems . Thus , we argue that it is nec-   essary to encode high - order interactions from the   user - restaurant graph to better model user prefer-   ence and restaurant status , which existing literature   ignores .   3 Problem Statement   We first introduce some definitions and notations ,   then introduce the problem formulation .   User - Restaurant Interaction Graph : letG=   ( U , V , E ) represent the user - restaurant interaction   graph , where U={u , u ... , u}denotes the set   of users , and V={v , v ... , v}is a set of restau-   rants . E={(u , v)|u∈U , v∈V}denotes the3286set of edges , where an edge ( u , v)∈Emeans that   useruhas reviewed restaurant v.   Reviews : the reviews of a restaurant are defined   as(R , ... , R ) , where Rrepresents the i-   th review of restaurant vandlis the number of   reviews of restaurant v. Similarly , the reviews of   the user uare defined as ( R , ... , R ) . We   further use U= ( u , u ... )to denote the list of   users who have reviewed restaurant v. The reviews   of users related to restaurant vcan be defined as   ( R , ... R ) , u∈U , where Rrepresent-   ing the j - th review comes from the reviews of user   u.   Prediction Task : to predict the future status of   the restaurant . This is a binary classification task ,   and 0 means the restaurant will be shut down and 1   means normal operation .   Explanation Task : besides the binary predic-   tion task , the model also contains an explanation   task , in which a sentence of summarization text   Y= ( w , . . . , w)will be generated . We invited   30 evaluators who are split into two groups to   manually select a few sentences ( about 30 words )   from all the restaurant reviews to represent the   key reasons for each restaurant ’s business pros-   perity , which will be used as ground - truth for   training and evaluation .   Problem Formulation : with the interaction   graph Gand review collections RandRas   input data , we want to make predictions for a given   restaurant regarding its future status and meanwhile   generate an explaining text .   4 The Proposed Model   In this section , we introduce our model , a joint   learning framework for restaurant survival predic-   tion and explanation , which is illustrated in Figure1 .   There are four components in RSPE , including an   input module , a co - attention module , a graph repre-   sentation module and a joint learning module . We   will introduce the details in the following sections .   4.1 Input Module   The function of the input module is to en-   code the input feature , and the input includes   two types of sequences : the reviews of restau-   rants ( R , ... , R ) , and related users ’ reviews   ( R , ... R ) , u∈U. Each sequence includes   a list of reviews . This module encodes reviews to   embedding representations . Each review is com-   posed of a sequence of sentences . We use UniLM(Dong et al . , 2019 ) to transform each sentence into   ad - dimensional embedding representation z∈R ,   because UniLM is pre - trained on a large - scale un-   supervised dataset and through our experiments ,   we find that it is better than BERT . Given a review   R , its embedding vector r=/summationtextzis   represented by the average of sentence embeddings   in the review . In addition , we use an embedding-   lookup operation to get a trainable embedding vec-   tor representation for each user and restaurant from   her / its ID , which will be used as the input for the   graph representation module ( will be introduced in   Section 4.3 ) .   4.2 Co - attention Module   The intuition is simple but powerful . Each user   is represented by all reviews that he / she wrote ,   and the restaurant is represented by all reviews   belonging to it . The goal of the co - attention   module is to select high - quality reviews from   the user / restaurant ’s review collection and finally   merge reviews ’ embedding into one user / restaurant   embedding .   Affinity Matrix : given user review embedding   a(a∈R)and restaurant review embedding   b(b∈R ) , the affinity matrix is calculated by :   M = f(a)Af(b ) , ( 1 )   where A∈Ris the weight matrix , and f(.)is   a feed - forward neural network .   Max Pooling Function : we use arg max to ob-   tain the maximum value of each row and each col-   umn of the matrix , then weigh the review aandb   respectively . The calculation process is as follows :   ζ= ( Gumbel ( max ( M)))a , ( 2 )   η= ( Gumbel ( max ( M)))b , ( 3 )   where ζandηrepresent the co - attention embed-   dings of the user and the restaurant . Gumbel ( )   is Straight - Through Gumbel softmax ( Jang et al . ,   2017 ) , due to the arg max function is not differ-   entiable , we use Gumbel ( ) to return a discrete   vector and turn the unnormalized vectors e=   ( e , e , ... , e)into a probability distribution :   s = exp / parenleftbig / parenrightbig   /summationtextexp / parenleftig / parenrightig , ( 4 )   where τis a temperature parameter , and gis a   Gumbel noise . In the feedforward process , swill3287   be transformed into a one - hot vector k , we denote   this function as Gumbel ( s ) = k :   k=/braceleftbigg1 , i= arg max(s )   0,otherwise(5 )   4.3 Graph Representation Module   Restaurants are not isolated . Sometimes we can not   understand why a restaurant becomes so popular   if we only consider its review content . Many fac-   tors influence the business status of a restaurant ,   such as nearby competitors and the general social   trend . In order to model the global context of restau-   rants , we construct a bipartite graph , on which   the nodes are restaurants and users , and the edges   are user - restaurant interactions . Since GNNs have   demonstrated great superiority in learning useful   information from graph - structure data ( Veli ˇckovi ´ c   et al . , 2018 ; Hamilton , 2020 ) , in this section , we   introduce a graph representation module to learn   meaningful patterns on the user - restaurant inter-   action graph , so that restaurants ’ information are   enhanced by their neighborhood .   The interaction graph Gis illustrated in Figure   1 . It stems from the idea that a specific interaction   between the user and the restaurant can reveal the   restaurant ’s survival .   Node Embedding : we obtain the trainable em-   bedding vector by the user ID and the restaurant   ID , denoted by pandqrespectively . High - order Neighbor Aggregation : the neigh-   bour nodes embedded in the propagation layer of   different orders have different effects on the tar-   get node . By stacking multiple propagation layers ,   we can explore high - level connectivity informa-   tion and enhance the representation . According   to the propagation rules , we obtain the neighbour   nodes of the first - order , second - order , and third-   order propagation layers adjacent to the target node ,   and the propagation layer embedding is calculated   as follows :   p = /summationdisplay1 / radicalbig   |S|/radicalbig   |S|p , ( 6 )   p = /summationdisplay1 / radicalbig   |S|/radicalbig   |S|p , ( 7 )   where pandprepresent the embeddings of   useruand restaurant vafter tlayer propaga-   tion respectively , SandSrepresent the first - hop   neighbors of user uand restaurant v.   To avoid the large embedding scale , each layer   of convolution nodes needs to be regularized . Then ,   the obtained propagation embedding layer is aggre-   gated to obtain the final target node embedding .   The calculation process is as follows :   p=/summationdisplayαp , p=/summationdisplayαp , ( 8)   where αrepresents the weight of the T(T=   0,1,2,3 ) layer embedding.3288For each restaurant v , there will be many user   reviews . Therefore , we use a mean pooling to ag-   gregate the vector representations of u∈Sof   all users who have reviewed restaurant v , which is   expressed as follows :   p=/summationdisplayp ( 9 )   4.4 Joint Learning Module   Joint learning is an inductive transfer method to   improve generalization by using the domain infor-   mation in the training signals of related tasks as an   inductive bias . Since the prediction and explana-   tion tasks are associated , we jointly train them in   a unified framework to make a better - generalized   performance .   We aggregate the embeddings of users and   restaurants in the co - attention module and graph   representation module . The formula is as follows :   q = ζ+p , q = η+p . ( 10 )   Prediction Task : the factorization machine   ( Rendle , 2010 ) helps extract the most essential la-   tent or hidden features , which can solve the classi-   fication problem . The formula is as follows :   f(q ) = b+/summationdisplaywq+/summationdisplay / summationdisplay⟨h , h⟩qq ,   ( 11 )   where q∈Ris the ientry of q= [ q , q ] ,   b∈Ris the bias , w∈Randh∈Rare   parameters to be learned . The loss function uses   sigmod cross entropy :   L=1   2|Θ|/summationdisplay(−[ylogˆy+(1−y ) log(1 −ˆy ) ] ) ,   ( 12 )   where yis truth label and Θrepresents the training   set .   Explanation Task : since the Gated Recurrent   Unit(GRU ) ( Cho et al . , 2014 ) performs well in the   generation , we choose it for the explanation task .   The details of GRU are as follows . First , calculate   the initial hidden state h :   h= tanh / parenleftbig   wq+wq+wˆy+b / parenrightbig   , ( 13 )   where w , wandware parameters to learned .   bis the bias .   The current tstate is related to the last t−1state :   h = GRU ( h , w ) , ( 14)where wis the word generated at time t.   The final output layer generates the distribution   dof words from the hidden state at time t :   η = O / parenleftbig   wh+b / parenrightbig   , ( 15 )   where ware parameters to learned and b∈   Ris the bias . |V|is the vocabulary size and   O()is the softmax function . Then , we use beam   search to select the best text generated Y.   We expect to maximize the probability of the   ground - truth text . Thus , the loss function for the   explanation task is :   L=1   |Θ|/summationdisplay / summationdisplay / parenleftig   −logη / parenrightig   , ( 16 )   where ˆlis the word of ground - truth text at time t.   Multi - task Loss : by sharing the representations   between related tasks , we aggregate the three loss   functions of the two tasks for optimization :   L = λL+λL+λ∥Ψ∥ , ( 17 )   where λ(ξ= 1,2,3 ) are hyper - parameters that   control the weight of different loss functions . Ψ   denotes the set of trainable parameters . For more   details on the setting of hyper - parameters , please   refer to the appendix .   5 Experiments   5.1 Datasets   We experiment with two public datasets , the basic   statistics are listed in Table 1 :   Dianping : it is the largest consumer review site in   China . This dataset records reviews from Jan.2011   to Dec.2011 and restaurants ’ status in Dec.2011   as the binary label . In the Dianping dataset , the   top3popular cities are used in the experiments :   Shanghai ( SH ) , Beijing ( BJ ) and Guangzhou ( GZ ) .   Yelp : it is the largest review site for business .   We use the latest restaurant records reviews from   Jan.2019 to Dec.2019 and restaurants ’ status in   Dec.2019 as the binary label . In the Yelp dataset ,   the top 3popular states are Nevada ( NV ) , Arizona   ( AZ ) in the United States , and Ontario ( ON ) in   Canada .   Due to space limitations , for more details on data   processing , please refer to the appendix.3289   5.2 Metrics   In our experiments , we use AUC ( Hanley and Mc-   Neil , 1982 ) to evaluate the prediction task . BLEU   ( Papineni et al . , 2002 ) and ROUGE ( Lin , 2004 ) are   used to evaluate the explanation task . ROUGE ’s   evaluation is based on the co - occurrence informa-   tion of n - grams in the text . ROUGE - N ( N=1,2 )   mainly counts on the N - grams . ROUGE - L is cal-   culated by matching the longest common subse-   quence . ROUGE - SU4 is calculated by the skip-   gram strategy , when generating explanation text   and ground - truth text for matching , which does not   require that the words must be continuous , and sev-   eral words could be " skipped " . The larger value of   BLEU and ROUGE indicates better explainability .   5.3 Performance Evaluation   To evaluate the prediction task , we compare the   RSPE with two groups of baselines which perform   binary classification tasks as our prediction mod-   ule :   Traditional Machine Learning : we take the   heterogeneous information obtained by encoding   reviews through Word2Vec ( Church , 2017 ) and   Bag of Word as input features for traditional ma-   chine learning methods , including : 1 ) LR ( Cortes   and Vapnik , 1995 ) . 2 ) SVM ( Cortes and Vapnik ,   1995 ) . 3 ) GBDT ( Friedman , 2001 ) .   Deep Learning : we also compare with several   competitive deep learning based methods , includ-   ing : 1 ) text - CNN ( Kim , 2014 ): a modified con-   volutional neural networks model . 2 ) text - RNN   ( Lai et al . , 2015 ): a modified long short - term mem-   ory model . 3 ) MPCN ( Tay et al . , 2018 ): a review-   based attention network model that combines multi-   pointer for recommendations . 4 ) HetGNN ( Zhang   et al . , 2019 ): a heterogeneous graph neural network   for various graph mining tasks by aggregating dif-   ferent types of nodes . 5 ) DCA ( Liao et al . , 2020 ):   a review based attention neural model for data aug-   mentation by selecting concepts . 6 ) HGAT ( Li   et al . , 2020 ): a hierarchical graph attention network   to accomplish the semi - supervised node classifica-   tion tasks .   To evaluate the explanation task , we compare   RSPE with two groups of baselines that both per-   form well on text generation .   Generative - based Methods : NRT ( Li et al . ,   2017 ) is a framework based on user review infor-   mation , which generates abstractive text with good   linguistic quality for prediction explanation . DCA   ( Liao et al . , 2020 ) is a framework based on atten-   tion neural , which generates diverse texts through   a large amount of text learning . PETER ( Li et al . ,   2021 ): a personalized Transformer that shows good   performance in text generation tasks .   Retrieval - based Method : the retrieval method   selects the most important text from reviews   as explanation sentence . Lexrank ( Erkan and   Radev , 2004 ) is an unsupervised text summariza-   tion method based on graph - based lexical centrality ,   which generates summary text by reviews .   5.4 Implementation Details   In our experiments , we randomize the datasets into   a training set ( 70 % ) , validation set ( 15 % ) , and test   set ( 15 % ) . We follow the corresponding papers   to adjust the baselines to ensure the best results .   The hyperparameter settings and implementation   details are listed in the appendix .   5.5 Results on the Prediction Task   The overall prediction results are shown in Table 2 .   Our model ’s improvement over the best baseline is   quite significant . For example , a performance gain   up to 6.9%/5.4%/8.6 % on the Dianping dataset of   city SH / BJ / GZ , and 4.8%/9.5%/5.9 % on the Yelp   dataset on state AZ / NV / ON , which demonstrates   the effectiveness of our model .   In addition , we have the following 4 observa-   tions about the results . First , the MPCN , DCA ,   and HGAT are generally better than the traditional   methods . Those methods use an attention mecha-   nism to build their model . HGAT also considers   heterogeneous graph convolution , demonstrating3290   that the information of the heterogeneous graph and   attention mechanism may contribute to the model   performance . Second , a simple graph structure can-   not perform well in the prediction task , such as   HetGNN . Third , our model performs well on Di-   anping and Yelp datasets , demonstrating that our   model is robust across different datasets . Fourth ,   our model achieves better performance . Our model   can not only automatically dig important informa-   tion in massive reviews through the co - attention   module but also combine the interaction informa-   tion between users and restaurants to capture the   most informative and meaningful signal from noisy   textual reviews .   5.6 Results on the Explanation Task   The detailed results are shown in Table 3 .   First , the performance of our model on the ex-   planation task is significantly better than the   SOTA methods . Take the BLEU metric as   an example , RSPE achieves an improvement   of 46.0%/63.2%/83.2%/27.4%/24.2%/27.7 % in   SH / BJ / GZ / NV / AZ / ON , and the average improve-   ment of 45.3 % . The ROUGE ( ROUGE-1/2 / L / SU4 )   indicator mainly considers overall accuracy , RSPE   achieves an improvement of 49.1 % in BJ city , and   the average improvement in all datasets is as high   as 23.8 % . Second , in 6 cities / states , NRT ’s expla-   nation performance is not good because it is based   on historical records to learn the latent factors and   can only output some general - purpose expressions .   Third , the retrieval method Lexrank does not per-   form well because it focuses on similarity matching   while lacking personalized expression . Because the   framework of DCA is too complex , its feature se-   lection ability is insufficient , so the overall perfor-   mance is lower than our model . Although PETER   proposes a new Transformer structure to generate   text , the results show that its performance improve-   ment is modest . At last , our RSPE performs signifi-   cantly better in the text of both Chinese and English   datasets because we leverage a graph convolutional   neural network to enhance hidden collaborative sig-   nals modeling from the user - restaurant interaction ,   which enables the model to learn the reputed re-   views to improve the quality of explanation text .   This observation is in line with the results men-   tioned above . It further verifies that by including   graph structure in the modeling process , our model   can learn the interaction information between user   and restaurant and thus generate informative textual   expressions for the restaurant survival.3291   5.7 Ablation Analysis   In order to study the effectiveness of joint learning   in the model , we performed an independent task   experiment for prediction and explanation respec-   tively , denoted as " Prediction Only " and " Expla-   nation Only " . Additionally , we remove the graph   representation module from the model , denoted as   " RSPE - G " . The results are shown in Figure 2 . It   has been proved that independent tasks can achieve   better results , but RSPE could achieve a balance be-   tween prediction accuracy and interpretation ability   through the joint learning framework . In addition ,   it is clear that the graph representation module in-   deed plays a significant contribution . This proves   again that the graph of high - order interaction en-   hances the power to capture the most informative   and meaningful signal from noisy textual reviews ,   thus more accurate prediction and more reasonable   explanations .   5.8 Case Analysis   We take three cases generated from LexRank , NRT ,   DCA , and RSPE as examples , which are shown in   Table 4 . We bold the frequent adjective and nouns   in the reviews as keywords , and the cases of Di-   anping are transformed from Chinese to English .   This table shows that : 1 ) The explanation words   generated by RSPE are more comprehensive and   cover many important factors such as environment ,   service , taste , and price . Meanwhile , the generated   content is highly consistent with the ground - truth   text . 2 ) RSPE has a powerful generalizing ability   to summarize relevant sentences , such as The osten-   tation and environment are very good in Case 3 . 3 )   RSPE can generate personalized language expres-   sions , such as The only issue was the front of the   best in Case 1 and in short , it is not recommended   in Case 2 .   5.9 Fluency Evaluation   Next , we evaluate the model ’s usefulness in im-   proving the fluency of the generated sentences . The   fluency evaluation experiment is done by human   judgment . We randomly selected 100 samples and   invited 5 annotators to assign scores . Five points   mean very satisfied , and 1 point means very bad .   The human evaluation results are reported in Table   5 . Results demonstrate that our model outperforms   the other three methods on Fluency and Kappa ( Li   et al . , 2019 ) metrics .   5.10 Survival Discussions   A restaurant ’s survival is not only related to user   reviews but also affected by many off - site factors ,   such as financial breakdown and competitive pres-   sure . Therefore , we hope to mine some instructive   explanations for the sustainable development of the   restaurant industry through some data analysis .   As shown in Figure 3 ( a ) and ( c ) , users of Dian-   ping pay more attention to taste ( taste , good , fresh ) .   In contrast , users of Yelp are more concerned about   the environment and service ( service , place , way ,   location ) . As shown in Figure 3 ( c ) and ( d ) , the per   capita consumption of medium cities is generally   higher than that of big and small cities , and the fail-   ure rate of restaurants in small cities is much lower   than that in big cities . We found that we could   explore the restaurant ’s survival from a more fine-   grained perspective , which to mine the rules , and   helped adjust their strategies to promote business .   6 Conclusion   In this paper , we tackle the problem of restaurant   survival , which is an essential task for social good .   Unlike traditional methods , which highly rely on   sensitive financial indicators , we use deep learn-   ing techniques to mine useful signals from massive3292UGC . We are the first to conduct both future sta-   tus prediction and explanation simultaneously as   a joint framework . Our model has two key com-   ponents , i.e. , the graph representation module and   the co - attention module . We conduct extensive ex-   periments on two datasets . Results demonstrate   that our proposed model achieves the SOTA perfor-   mance on both prediction and explanation tasks .   7 Limitations   Current limitations of this paper are threefold . First ,   a limited set of features are used in this paper .   Whether a restaurant can survive is influenced   by many factors , such as finances , social circum-   stances ( such as Covid-19 ) , and other issues that   can exacerbate a restaurant ’s survival . In this pa-   per , we ca n’t fully explain those additional factors   out of the review text . We just took a new per-   spective on the restaurant survival prediction task   from NLP . Second , the model structure is not light-   weight enough , and there is still room for model   simplification , such as the combination of attention   mechanisms and graph neural networks . Third , the   data application scope of the model is not large   enough . Currently , only two datasets have been   tested in 6 cities / states . We do not test the model   on data samples on more different online service   platforms .   8 Acknowledgments   Hao Liao is the corresponding author . Thanks   a lot for Dr. Jianxun Lian and Dr. Xiting   Wang ’s valuable suggestions and help . This work   was supported by the Natural Science Founda-   tion of China under Grant no . 62276171 and   62072311 , the Natural Science Foundation of   Guangdong Province of China under Grant Nos .   2019A1515011173 and 2019A1515011064 , the   Shenzhen Fundamental Research - General Project   under Grant No . JCYJ20190808162601658 , CCF-   Baidu Open Fund , NSF - SZU and Tencent - SZU   fund .   References329332943295A Appendix On Reproducibility   A.1 Experimental Environment   This experiment runs on GPU V100 and CentOS 7   servers . The code is implemented with Tensorflow .   A.2 Reproducibility   A.2.1 Code Resources   We compared the proposed framework , RSPE ,   with 11 baseline methods discussed in Section   5.3 , the prediction task methods including LR ,   SVM , GBDT , text - CNN , text - RNN , MPCN , Het-   GNN , HGAT , DCA and the explanation task meth-   ods including Lexrank , NRT and DCA . Our pro-   posed framework , RSPE ’s code that we have   implemented are available through the follow-   ing link : https://github.com/Complex-data/RSPE .   Other codes were obtained as follows :   •LR , SVM , GBDT : we used the scikit - learn ,   which is a publicly machine learning project   at : https://scikit-learn.org/stable/index.html   •text - CNN : we used the publicly available im-   plementation at : https://github.com/FinIoT/   text_cnn   •text - RNN : we used the publicly available im-   plementation at : https://github.com/luchi007/   RNN_Text_Classify   •MPCN : we used the publicly available im-   plementation at : https://github.com/vanzytay/   KDD2018_MPCN   •HetGNN : we used the publicly available im-   plementation at : https://github.com/ chuxu-   zhang/ KDD2019_HetGNN   •HGAT : we used the publicly available im-   plementation at : https://github.com/BUPT-   GAMMA / HGAT   •DCA : we used the publicly available im-   plementation at : https://github.com/Complex-   data/   •Lexrank : we used the publicly available   implementation at : https://github.com/ crab-   camp / lexrank   •NRT : we used the publicly available imple-   mentation at : https://github.com/lipiji/NRT-   theano   A.2.2 Data Processing   We can download dataset from DianPingand   Yelp . Because of the contents of the datasets aredifferent , we conduct data processing for these two   datasets respectively .   Dianping : the download content includes two   files . One is checkins.json , and the other one is   business.json .checkins.json includes all user and   shop review records , while business.json includes   all records about shops . The data processing steps   are as follows : 1 ) Read checkins and business data ,   and merge these according to restId . 2 ) Filter non-   restaurant data . 3 ) Filter cities , in our experiment ,   we used Beijing , Shanghai and Guangzhou data . 4 )   Filter out 10 % of users and restaurants with few   reviews . 5 ) Select the attributes required for the   experiment : userid , restId , review , label .   Yelp : the download content include two files .   One is review.json , and the other one is busi-   ness.json .review.json includes all user and shop   review records , while business.json includes all   records about shops . The data processing steps   are as follows : 1 ) Read review and business data ,   and merge these according to restId . 2 ) Filter Year ,   we only use data from 2019 . 3 ) Since Yelp data   does n’t have a detailed survival status , we deter-   mined restaurants by determining whether Restau-   rantsReservations exist . If this field exists , it means   that the store is a restaurant . 4 ) Filter states , in our   experiment , we used Nevada , Arizona and Ontario .   5 ) Filter out 10 % of users and restaurants with few   reviews . 6 ) Select the attributes required for the   experiment : userid , restId , review , label   A.2.3 Pre - trained Model   We encode words and sentences by UniLM model .   First , we need to download UniLM model from   https://github.com/microsoft/ unilm . Then , we use   Tensorflow to load the UniLM model , which pro-   vides that have been trained . Then , we add our   training data to continue training . Finally , we can   get a semantic vector representation for each sen-   tence through this pretrained model .   A.2.4 Hyperparameter Setting   For hyperparameter settings for RSPE , we intro-   duce the details of major hyperparameter setting   as shown in Table 6 . In our experiments , we set   λ(pred_lambda=1 ) by default , and then tune the   model by adjusting λ(gen_lambda ) . The descrip-   tions of the major hyperparameter are as follows :   •gen_lambda : the threshold to control the gen-   erating loss weight .   •rnn_type : the threshold to control the compo-   sitional model name.3296   •emb_size : the threshold to control the embed-   dings dimension .   •rnn_size : the threshold to control the model-   specific dimension .   •epoch : the threshold to control the number of   epochs .   • lr : the threshold to control the learning rate .   •att_pool : the threshold to control the pooling   type for attention .   •dmax : the threshold to control the max num-   ber of reviews .   • beam_size : the threshold to control the beam   search size .   •pred_lambda : the threshold to control the   weight of prediction task   A.2.5 Evaluation   •Results on the Prediction Task : we use AUC   to evaluate the prediction task , and execute   test_RSPE.py to get the accuracy in the test   set .   •Results on the Explanation Task : BLEU   andROUGE are used to evaluate the expla-   nation task . For BLEU metrics , we execute   evaluate/ compute_bleu.py to get the result   score . For ROUGE metrics , we used the   publicly available implementation at : https   : //github.com/ kavgan/ ROUGE-2.0.3297