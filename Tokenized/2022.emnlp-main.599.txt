  Javier Ferrando , Gerard I. Gállego , Belen Alastruey ,   Carlos Escolano , Marta R. Costa - jussàTALP Research Center , Universitat Politècnica de CatalunyaMeta AI   { javier.ferrando.monsonis , gerard.ion.gallego ,   belen.alastruey,carlos.escolano}@upc.edu   costajussa@meta.com   Abstract   In Neural Machine Translation ( NMT ) , each   token prediction is conditioned on the source   sentence and the target prefix ( what has been   previously translated at a decoding step ) . How-   ever , previous work on interpretability in NMT   has mainly focused solely on source sentence   tokens ’ attributions . Therefore , we lack a full   understanding of the influences of every input   token ( source sentence and target prefix ) in the   model predictions . In this work , we propose an   interpretability method that tracks input tokens ’   attributions for both contexts . Our method ,   which can be extended to any encoder - decoder   Transformer - based model , allows us to bet-   ter comprehend the inner workings of current   NMT models . We apply the proposed method   to both bilingual and multilingual Transformers   and present insights into their behaviour .   1 Introduction   Transformers ( Vaswani et al . , 2017 ) have become   the state - of - the - art architecture for natural language   processing ( NLP ) tasks ( Devlin et al . , 2019 ; Raffel   et al . , 2020 ; Brown et al . , 2020 ) . With its suc-   cess , the NLP community has experienced an urge   to understand the decision process of the model   predictions ( Jain and Wallace , 2019 ; Serrano and   Smith , 2019 ) .   In Neural Machine Translation ( NMT ) , attempts   to interpret Transformer - based predictions have   mainly focused on analyzing the attention mecha-   nism ( Raganato and Tiedemann , 2018 ; V oita et al . ,   2018 ) . A large number of works in this line have in-   vestigated the capabilities of the cross - attention to   perform source - target alignment ( Kobayashi et al . ,   2020 ; Zenkel et al . , 2019 ; Chen et al . , 2020 ) , com-   pared with human annotations . Gradient - based   ( Ding et al . , 2019 ) and occlusion - based methods   ( Li et al . , 2019 ) have also been evaluated against   human word alignments . The former computes gra-   dients with respect to the input token embeddingsFigure 1 : ALTI+ results for a De - En translation example .   We obtain source sentence and target prefix ( columns )   interpretations for every predicted token ( row ) .   to measure how much a change in the input changes   the output , the latter generates input attributions by   measuring the change in the predicted probability   after deleting specific tokens . However , there is   a tension between finding a faithful explanation   and observing human - like alignments , since one   does not imply the other ( Ferrando and Costa - jussà ,   2021 ) .   The decoding process of NMT systems consists   of generating tokens in the target vocabulary based   on the information provided by the source sequence   and the previously generated tokens ( target prefix ) .   However , most of the work on interpretability of   NMT models only analyses source tokens . Re-   cently , V oita et al . ( 2021a ) proposed using Layer   Relevance Propagation ( LRP ) ( Bach et al . , 2015 )   to analyze the source and target contributions to the   model prediction , and later analyzed its behaviour   during training ( V oita et al . , 2021b ) . Nonethe-   less , they apply their method to obtain global ex-   planations , as an average over the entire dataset ,   not to get input attributions of a single prediction .   Gradient - based methods have also been extended to   the target prefix ( Ferrando and Costa - jussà , 2021 ) ,   although they do not quantify the relative contribu-   tion of source and target inputs.8756Concurrently , encoder - based Transformers , such   as BERT ( Devlin et al . , 2019 ) and RoBERTa ( Liu   et al . , 2019 ) , have been analysed with attention   rollout ( Abnar and Zuidema , 2020 ) , which models   the information flow in the model with a Directed   Acyclic Graph , where nodes are token representa-   tions and edges , attention weights . In the computer   vision literature , Chefer et al . ( 2021b , a ) combined   this method with gradient information . Recently ,   Ferrando et al . ( 2022 ) have presented ALTI ( Aggre-   gation of Layer - wise Tokens Attributions ) , which   applies the attention rollout method by substitut-   ing attention weights with refined token - to - token   interactions . In this work , we present the first ap-   plication of a rollout - based method to sequence   to sequence Transformers . Our key contributions   are :   •We propose a method that measures the contri-   butions of each input token ( source and target   prefix ) to the encoder - decoder Transformer   predictions ;   •We show how contextual information is mixed   across the encoder of NMT models , with the   model keeping up to 47 % of token identity ;   •We evaluate the role of residual connections   in the cross - attention , and show that attention   to uninformative source tokens ( EOS and final   punctuation mark ) is used to let information   flow from the target prefix ;   •We analyze the role of both input contexts in   low and high - resource scenarios , and show   the model behaviour under hallucinations .   2 Background   In this section , we provide the background to under-   stand our proposed method by briefly explaining   the encoder - decoder Transformer - based model in   the context of NMT ( Vaswani et al . , 2017 ) and the   Aggregation of Layer - wise Token - to - token Interac-   tions ( ALTI ) method ( Ferrando et al . , 2022 ) .   2.1 Encoder - Decoder Transformer   Given a source sequence of tokens x=   ( x , . . . , x ) , and a target sequence y=   ( y , . . . , y ) , an NMT system models the condi-   tional probability :   P(y|x ) = /productdisplayP(y|y , x ) ( 1 )   where y= ( y , . . . , y)represents the prefix   ofy , with x = y=</s > used as a special token   to mark the beginning and end of sentence . The   Transformer is composed by a stack of encoder   and decoder layers ( Figure 2 ) . The encoder gener-   ates a contextualized sequence of representations   e= ( e , . . . , e)of the source sentence . The de-   coder , at each time step t , uses both the encoder   outputs ( e ) and the target prefix ( y ) to compute a   probability distribution over the target vocabulary ,   from which a prediction is sampled .   Multi - head attention . The Transformer core   building block , the multi - head attention mechanism   ( MHA ) is in charge of combining contextual infor-   mation in the hidden representations . Consider   herex= ( x , . . . , x)as the sequence of token   representationsof dimension dentering layer l ,   and˜x= ( /tildewidex , . . . , /tildewidex)the output layer representa-   tions . Each of the Hheads inside MHA computes   vectors of dimension d = d / H :   z=/summationdisplayαWx ( 2 )   withαreferring to the attention weight where   token iattends token j , andW∈Rto a   learned weight matrix.8757The output of MHA for the i - th token ( MHA ) is   calculated by concatenating each zand projecting   the joint vector through W∈R. This is   equivalent to a sum over heads where each zis   projected through the partitioned weight matrix   W∈Rand adding the bias b∈R :   MHA(x ) = WConcat ( z , . . . , z ) + b   = /summationdisplayWz+b   ( 3 )   Layer normalization . Finally , a layer normaliza-   tion ( LN ) is applied over the sum of the residual   vector xand the output of the multi - head attention   module , giving as output /tildewidex :   /tildewidex = LN(MHA(x ) + x ) ( 4 )   Merging Equations ( 2 ) to ( 4 ) , we get :   /tildewidex = LN / parenleftigg / summationdisplay / summationdisplayWαWx+b+x / parenrightigg   Considering F(x ) = /summationtextWαWx , we   can formulate the previous equation as :   /tildewidex = LN / parenleftigg / summationdisplayF(x ) + b+x / parenrightigg   ( 5 )   2.2 Aggregation of Layer - wise Token - to - token   Interactions ( ALTI )   The layer normalization operation over a sum of   vectors LN(/summationtextu ) , as in Equation ( 5 ) , can be re-   formulated as / summationtextL(u ) + β , where L : R∝ ⇕ ⊣√∫⊔≀→R   ( see Appendix A.1 ) . This allows us to express   Equation ( 5 ) ( Kobayashi et al . , 2021 ) as an inter-   pretable expression of the layer input representa-   tions ( Figure 3 ):   /tildewidex=/summationdisplayT(x ) + ϵ ( 6 )   where ϵcontains bias terms ( see Appendix A.2   for full derivation ) and Ttransforms the layer input   vectors :   T(x ) = /braceleftbiggL(F(x ) ) ifj̸=i   L(F(x ) + x)ifj = i(7 )   with the residual connection xonly considered   in the transformed vector T(x ) . Ferrando et al .   ( 2022 ) propose to use the Manhattan distance be-   tween the output vector and the transformed vector   as a measure of the impact of xon / tildewidex :   d=∥/tildewidex−T(x)∥(8 )   By taking −d , larger distances reflect lower   ( more negative ) influence . Then , distances are nor-   malized ∈[0,1]to obtain the contribution of token   representation j to token representation i :   c = max(0,−d+∥/tildewidex∥)/summationtextmax(0,−d+∥/tildewidex∥)(9 )   giving the matrix of layer - wise contributions   C∈R , where each row contains the con-   tribution , or influence , of each xin / tildewidex .   ALTI method ( Ferrando et al . , 2022 ) follows   the Transformer ’s modeling approach proposed by   Abnar and Zuidema ( 2020 ) , where the informa-   tion flow in the model is simplified as a Directed   Acyclic Graph , where nodes are token represen-   tations , and edges represent the influence of each   input layer token xin the output token /tildewidex . ALTI   proposes using token contributions Cinstead of   raw attention weights α . The amount of informa-   tion flowing from one node to another in different   layers is computed by summing over the different   paths connecting both nodes , where each path is   the result of the multiplication of every edge in the   path . This is computed by the matrix multiplica-   tion of the layer - wise contributions , giving the full   encoder contribution matrix :   C = C·C · · · · · C ( 10 )   We refer to Cas the contributions in the last   layer of the encoder , where output vectors are e.8758   3 ALTI for the Encoder - Decoder   Transformer ( ALTI+ )   The attention rollout and ALTI methods work for   encoder - based Transformers . However , in the   encoder - decoder Transformer , the cross - attention   hinders its integration . In this section , we present   ALTI+ , which is the adaptation of ALTI method to   the encoder - decoder Transformer .   3.1 Decoder Layer Decomposition   We decompose the self - attention and cross-   attention of a decoder layer into interpretable ex-   pressions ( Equation ( 6 ) ) , from which we can get   the degree of interaction between input and out-   put token representations ( Equation ( 9 ) ) . Consider   y= ( y , . . . , y , . . . , y)the set of vector rep-   resentations of the target prefix tokens as input of a   decoder layer , and /tildewideythe layer output ( Figure 4 ) .   Decoder self - attention . The layer normalization   in the decoder self - attention ( LN ) is applied over   the sum of the multi - head attention output and the   residual y. The self - attention blockcan be   written as :   /tildewidey = LN(MHA(y ) + y )   = LN   /summationdisplayF(y ) + b+y   (11)where Fconsiders α , WandWof the de-   coder self - attention . Analogous to Equation ( 7 ) we   can obtain the transformed vectors of y :   T(y ) = /braceleftbiggL(F(y ) ) ifj̸=t−1   L(F(y ) + y)ifj = t−1   Following Equations ( 8) and ( 9 ) we get the decoder   self - attention contributions C∈Rre-   flecting the strength of the interaction between y   and / tildewidey .   Decoder cross - attention . The output of the   cross - attention block at time step tcan be decom-   posed as :   /tildewidey = LN(MHA(e ) + /tildewidey )   = LN   /summationdisplayF(e ) + b+/tildewidey   (12 )   where / tildewidey , the residual connection , is the output of   the self - attention block , and ethe encoder outputs .   We can obtain the transformed vectors of the en-   coder outputs eand the residual connection /tildewidey :   T(e ) = L(F(e ) )   T(y ) = L(/tildewidey)(13 )   Following Equation ( 8) , we can compute the Man-   hattan distance between the transformed vectors   and / tildewideyand get the contributions [ C;C ] ,   withC∈RandC∈R.   The cross - attention residual /tildewideycontribution to   /tildewideyreflects the total influence of the self - attention   inputs yto the decoder layer output /tildewidey . Thus ,   we can get the full decoder layer contribution ma-   trix[C;C](Figure 5 ) by substituting   the residual contributions ( C ) with the self-   attention contributions ( C ) , and weighting   every row of Cby the corresponding value   of the residual contribution of each time step.8759   3.2 Aggregating Contributions Through the   Encoder - Decoder Transformer   In order to get input token attributions , we apply   the same principle as attention rollout method . As   described in § 2.2 , ALTI builds a graph where nodes   are token representations and edges represent the   contributions between tokens in each layer . The   amount of information flowing from one node to   another in different layers is computed by summing   over the different paths connecting both nodes ,   where each path is the result of the multiplication   of every edge in the path ( Figures 6 and 7 ) .   Algorithm 1 : ALTI+ source relevance .   Input : C – encoder contributions   C – contributions decoder   layers   L – number of layers   Output : R – source input relevancies   forl←[1,2 ... L ] do   C = C·C   R = C   forl←[2,3 ... L ] do   R = C·R+C   R = R   return R   ALTI+ source tokens relevance . Algorithm 1   shows the process to obtain source sentence to-   kens relevance for the model prediction R   ( Figure 6 ) . We first update the cross - attention con-   tribution matrices ( to C ) by multiplying each   of them with the contributions of the entire encoder   Cto account for all the paths in the encoderand cross - attentions . We then iteratively aggregate   edges from paths of the target prefix contributions   C.   ALTI+ target prefix tokens relevance . Target   prefix input attributions ( Figure 7 ) are computed   by multiplying Cin each layer :   R = C·C · · · · · C   ( 14 )   4 Experimental Setup   We analyze input token attributions in both bilin-   gual and multilingual Machine Translation mod-   els . For the bilingual setting , we train a 6 - layer   Transformer model for the German - English ( De-   En ) translation task . We use Europarl v7 corpus   and follow Zenkel et al . ( 2019 ) and Ding et al .   ( 2019 ) data setup . We use byte - pair encoding   ( BPE ) ( Sennrich et al . , 2016 ) with 10k merge oper-   ations . For the multilingual model , we use M2 M   Transformer ( Fan et al . , 2021 ) , a many - to - many   multilingual translation model that can translate   directly between any pair of 100 languages . We   useF ( Ott et al . , 2019 ) implementations ,   and the provided checkpoint for the M2 M model   ( 418 M ) . We perform the quantitative analysis in   1000 sentences of the test set of IWSLT’14 German-   English dataset . For the analysis in § 5.5 we use   F -101 ( Goyal et al . , 2022 ) devtest split .   5 Analysis   In this section , we perform a set of experiments   to measure the quality of the obtained contribu-8760   tions , and unveil different aspects of bilingual and   multilingual NMT models .   5.1 Information Mix in the Encoder   Information from input source tokens gets mixed   throughout the encoder . Intermediate layer rep-   resentations acquire contextual information from   other tokens in the sentence due to the self - attention   mechanism . Brunner et al . ( 2020 ) analyze , for   an encoder - based model , the contribution of input   source tokens to its intermediate layer represen-   tations . They conclude that input source tokens   contribute little ( around 10 % on average ) to its   corresponding last layer representation ( encoder   output ) . However , by training a linear classifier   and , with nearest neighbor lookup based on the co-   sine distance , they are able to recover input token   identity 93 % of the times . We apply ALTI method   ( Equation ( 10 ) ) across the Transformer encoder   and analyze the input relevance of source tokens   to intermediate encoder representations ( Figure 9 ) .   Our results in the bilingual and multilingual models   show that , indeed , input tokens highly contribute   to their associated layer representations . In the last   layer , 41 % of the input contribution comes from   the input token at the same position . The multilin-   gual model is able to retain above 47 % despite its   12 layers . The curves of both models in Figure 9   closely match the results obtained by V oita et al .   ( 2019 ) relying on the mutual information between   the input tokens and tokens representations across   layers .   5.2 Alignment in Cross - attention   In order to evaluate the quality of the proposed   cross - attention contributions ( § 3.1 ) , we measure   Alignment Error Rate ( AER ) against human-   annotated alignments . As found out by Garg et al .   ( 2019 ) , the penultimate layer of Transformers tends   to focus on learning the source - target alignment of   words . Therefore , we analyze the cross - attention   contributions Cextracted from the 5th layer   from the bilingual 6 - layer model . We use gold   alignments from Vilar et al . ( 2006 ) , containing 508   sentence pairs . For comparison , we compute the   AER of the raw attention weights and previous   methods based on vector norms . Vector - Norms   ( Kobayashi et al . , 2020 ) compute ∥F∥from Equa-8761   tion ( 5 ) , and Vector - Norms + LN + Res ( Kobayashi   et al . , 2021 ) ∥T∥from Equation ( 6 ) . As shown in   Table 1 , our method for estimating layer - wise con-   tributions obtain the lowest AER , outperforming   similar previous methods by at least 2.6 points on   average . As can be observed in Figure 8 , attention   weights fail at showing alignments , with the < /s >   token concentrating large attention weights . Our   method is able to filter this noise , showing almost   no contribution from < /s > . In § 5.3 , we analyze   this phenomenon and try to find an explanation for   it .   5.3 The Role of the End - of - Sentence Token   It has been hypothesized that attention given to spe-   cial tokens is used by the model as a ‘ no - op ’ ( Clark   et al . , 2019 ) . Ferrando and Costa - jussà ( 2021 ) an-   alyze attention weights of the cross - attention to   source finalizing tokens ( final punctuation mark   and < /s > ) , and find the value vectors ( see Ap-   pendix B ) associated with these tokens to be almost   zero norm . Additionally , they find that attention   weights to source finalizing tokens tend to increase   when predicting tokens that heavily rely on the   target prefix , such as postpositions , particles , or   closing subwords . The proposed cross - attention   decomposition in § 3.1 allows us to analyze both   the contributions of source tokens , and the residual   connection ( Figure 8 ( b ) ) . We measure the Pearson   correlation between attention weights to < /s > to-   ken and the contribution of the residual connection   in the cross - attention . We can see in Figure 10 that   there is a high correlation in almost every layer ,   especially in the last layers . This demonstrates   that finalizing tokens are used to skip source at-   tention , since the higher their attention score , the   more information is flowing from the decoder ( in   the residual ) coming from the target prefix .   5.4 Analyzing Hallucinations   A common issue of NMT models is hallucination ,   which are translations that are disconnected from   the source text , despite being fluent in the target lan-   guage ( Müller et al . , 2020 ) . Hallucinations should   be reflected in our method as a drop in the contri-   bution of the source sentence . Thus , in this section ,   we induce hallucination and measure the source   sentence contribution with ALTI+ .   To induce hallucination , we perturb the target   prefix sequence of the bilingual model by adding   the < unk > token . Then , we follow the algorithm   proposed by Lee et al . ( 2018 ) to detect which per-   turbed translations are hallucinations . They mea-   sure BLEU score of the generated translation with   and without perturbation . They fix a minimum   threshold BLEU score for the original translations   ( 20 BLEU in our experiments ) , and a maximum   score for the perturbed translations ( 3 BLEU in our   experiments ) . The model is considered to halluci-   nate when both translations satisfy the thresholds .   Analyzing ALTI+ contributions , we can confirm   that the bilingual model largely ignores source to-8762   kens during hallucinations ( Figures 11 and 12 ) .   5.5 Multilingual Model Analysis   We analyze the behaviour of the multilingual model   in different language pairs of F -101 dataset .   We include in the analysis high - resource languages ,   English ( En ) , Spanish ( Es ) , and French ( Fr ) and   low - resource languages , Zulu ( Zu ) and Xhosa ( Xh ) .   High - resource languages have been defined in   ( Goyal et al . , 2022 ) as languages with available bi-   text data beyond 100 M samples , and low - resource   languages are those with less than 1M.   Figure 13 shows an Es - En example in the mul-   tilingual model . We observe an almost uniform   contribution of the language tags across different   outputs . The only drop in its contribution seems to   happen when translating proper nouns ( e.g. , " Mr.   Williams " ) or anglicisms ( e.g. , " hobby " ) , which is   observed for other language pairs too ( Appendix C ) ,   and repeated across the dataset . We hypothesize   that the model does n’t need to rely on the lan-   guage tag since these words appear across different   languages . Dependencies between generated to-   kens are also observed , the prediction " for " relieson " thanks " , " Williams " on " Mr. " and " into " on   " introduc ing " . The same example can be found in   Appendix C for En - Zu and Zu - En pairs .   Figure 14 shows results of the source sentence   contribution for En - Zu , En - Xh , En - Fr and En-   Es pairs . We observe similar source contribution   patterns between the high - resource pairs , and be-   tween those pairs involving a low - resource lan-   guage . However , in the low - resource scenario , the   source contribution is remarkably lower when trans-   lating from English . We hypothesize that , when the   low - resource language is in the target prefix , the   model tends to behave similarly to when it halluci-   nates ( Figure 12 ) , ignoring the source . But , when   a high - resource language ( En ) is in the target pre-   fix , it is less likely to lose track of the source and ,   thus , less prone to enter hallucination mode . Low-   resource language sentences in the target side may   be seen by the model as target prefix perturbations   ( § 5.4 ) , although further research is required .   6 Conclusions   We propose ALTI+ , an interpretability method for   the encoder - decoder Transformer that provides to-   ken influences to the model predictions for the two   input contexts : source sentence and target prefix .   By applying ALTI+ to a bilingual and a multilin-   gual NMT model we are able to discover insights   into the behaviour of these black - box models . Un-   like previous methods , we can now observe depen-   dencies between tokens in the predicted sentence ,   and quantify the total contribution of each of the   contexts . This allows a deeper exploration of cur-   rent NMT models . Our findings include : the role   of the source EOS ( < /s > ) token as a mean to avoid8763incorporating source information , the absence of   source contribution when producing hallucinations ,   and the lack of source contributions when trans-   lating from English to a low - resource language .   ALTI+ overcomes the limitations of previous inter-   pretability methods in NMT , and we believe it can   help researchers and practitioners to better under-   stand any encoder - decoder Transformer model .   Limitations   ALTI+ is able to measure the amount of contex-   tual information in each layer representation of the   Transformer . We use the influences of each input   token to the last layer representation for evaluating   input attributions for the model prediction . How-   ever , our method does not consider the softmax   layer on top of the Transformer . Therefore , ALTI+   does n’t provide explanations for each of the output   classes ( target vocabulary ) , as opposed to gradient-   based methods .   Ethical Considerations   ALTI+ provides explanations about input attribu-   tions in the Encoder - Decoder Transformer . By it-   self , we are not aware of any ethical implications of   the methodology , which does not take into account   any subjective priors . We perform experiments in   Machine Translation . While we do not study biases   in this application , we know they exist ( Costa - jussà   et al . , 2022 ) . In the future , we plan to further ex-   plore and mitigate them by using the information   of source input attributions that ALTI+ provides .   Also , understanding hallucinations by means of   ALTI+ can help to avoid catastrophic and unsafe   translations .   7 Acknowledgements   We would like to thank the anonymous review-   ers for their useful comments . Javier Fer-   rando and Gerard I. Gállego are supported by   the Spanish Ministerio de Ciencia e Innovación   through the project PID2019 - 107579RB - I00 / AEI   / 10.13039/501100011033 .   References876487658766A ALTI   A.1 Layer Normalization   The Layer normalization operation over input xcan   be defined as : LN(x ) = ⊙γ+β , where µ   computes the mean , σthe standard deviation , and γ   andβrefer to an element - wise transformation and   bias respectively . LN(x)can be decomposed intoLx+β , where Lis a linear transformation in-   cluding the mean and element wise multiplication .   Given a sum of vectors / summationtextxas input to LN   we can rewrite the expression as :   LN(/summationdisplayx ) = 1   σ(/summationtextx)L / summationdisplayx+β   = /summationdisplay1   σ(/summationtextx)Lx+β   = /summationdisplayL(x ) + β   A.2 Full derivation   /tildewidex = LN / parenleftigg / summationdisplay / summationdisplayWαWx+b+x / parenrightigg   = LN / parenleftigg / summationdisplayF(x ) + b+x / parenrightigg   = /summationdisplayL(F(x ) ) + L(b ) + L(x ) + β   Defining ϵ=L(b)+βwe get to the expression   in Equation ( 6 ):   /tildewidex=/summationdisplayT(x ) + ϵ ( 15)B Values Norms   C Examples   We include examples for the En - Zu language pair in   the multilingual model in Figure 16 and 17 , as well   as for Es - En in Figure 18 and Fr - En in Figure 19.876787688769