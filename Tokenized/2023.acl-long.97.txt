  Mingzhu Cai Siqi Bao Xin Tian Huang He Fan Wang Hua Wu   Baidu Inc. , China   { caimingzhu , baosiqi , tianxin06 , hehuang , wang.fan , wu_hua}@baidu.com   Abstract   In this paper , we propose an unsupervised   query enhanced approach for knowledge-   intensive conversations , namely QKConv .   There are three modules in QKConv : a query   generator , an off - the - shelf knowledge selector ,   and a response generator . QKConv is opti-   mized through joint training , which produces   the response by exploring multiple candidate   queries and leveraging corresponding selected   knowledge . The joint training solely relies on   the dialogue context and target response , get-   ting exempt from extra query annotations or   knowledge provenances . To evaluate the effec-   tiveness of the proposed QKConv , we conduct   experiments on three representative knowledge-   intensive conversation datasets : conversational   question - answering , task - oriented dialogue ,   and knowledge - grounded conversation . Experi-   mental results reveal that QKConv performs   better than all unsupervised methods across   three datasets and achieves competitive perfor-   mance compared to supervised methods .   1 Introduction   In addition to open - domain chitchat , there exist   various knowledge - intensive conversations , such as   conversational question - answering , task - oriented   dialogue , and knowledge - grounded conversation .   Although large - scale language models can implic-   itly store common knowledge within parameters   ( Petroni et al . , 2019 ; Zhao et al . , 2020b ) , they are   known to suffer from producing plausible state-   ments with factual errors ( a.k.a . knowledge halluci-   nation ) ( Roller et al . , 2021 ; Marcus , 2020 ) . There-   fore , there is a trend to rely on external resources ,   such as Wikipedia databases or search engine re-   sults , to facilitate knowledge - intensive conversa-   tions ( Dinan et al . , 2019 ; Komeili et al . , 2022 ) .   In knowledge - intensive conversations , the most   straightforward way to retrieve external knowledge   is to take the dialogue context as the query and use   an off - the - shelf retriever to return the knowledgeentry . However , it encounters some difficulties in   retrieving appropriate knowledge ( Shuster et al . ,   2021 ) . As the focus or topic changes along with   the conversation flow , the outdated information in   the dialogue context brings extra noise to the re-   triever , resulting in obsolete or irrelevant knowl-   edge retrieved . Moreover , the dialogue context has   a native misalignment with the short and interroga-   tive query preferred in existing retrievers .   Some methods choose to finetune a task - specific   retriever to enhance the performance of knowledge   selection ( Guu et al . , 2020 ; Shuster et al . , 2021 ;   Glass et al . , 2022 ) . However , this strategy is usu-   ally computationally expensive ( e.g. , finetuning a   dense retriever requires constant recomputation for   massive knowledge entries ) or even infeasible for   complex retrieval systems ( e.g. , retraining a search   engine is impractical ) . Some other methods choose   to generate a self - contained query based on the dia-   logue context ( Yu et al . , 2020 ; Anantha et al . , 2021 ;   Chen et al . , 2022 ) . This strategy relies on careful   query annotations to guarantee the completeness of   essential information extraction and the adaptation   to the knowledge selector .   In this paper , we introduce a novel unsupervised   query enhanced approach for knowledge - intensive   conversations , namely QKConv . As shown in Fig-   ure 1 , QKConv consists of three modules : a query   generator , an off - the - shelf knowledge selector , and   aresponse generator . Specifically , QKConv is opti-   mized through joint training , which produces the re-   sponse by exploring multiple candidate queries and   leveraging corresponding selected knowledge . We   also integrate two types of query guidance to reg-   ulate query generation and facilitate joint training :   context - sensitive guidance ( e.g. , the last context ut-   terance ) and response - sensitive guidance ( e.g. , the   target response ) .   The benefits brought by QKConv ’s design are   three - fold . Firstly , the training of QKConv solely   relies on the dialogue context and target response,1730   getting exempt from extra query annotations or   knowledge provenances . Secondly , the joint train-   ing of QKConv boosts query generation toward   better knowledge selection and ensures end - to - end   performances , compared to the individual optimiza-   tion of each module . Thirdly , thanks to the query   generation module , QKConv gets rid of the expen-   sive computation of tuning knowledge selectors   and has the generality to adopt various knowledge   selectors .   To evaluate the effectiveness of the proposed QK-   Conv , we conduct experiments on three represen-   tative knowledge - intensive conversation datasets :   conversational question answering QReCC ( Anan-   tha et al . , 2021 ) , task - oriented dialogue SMD   ( Eric et al . , 2017 ) , and knowledge - grounded con-   versation WoW ( Dinan et al . , 2019 ) . Experi-   mental results reveal that QKConv performs bet-   ter than all unsupervised methods across three   datasets and even outperforms supervised methods   on some datasets . Specifically , QKConv ’s gener-   ated query achieves superior knowledge selection   performance , and QKConv exhibits robust knowl-   edge utilization in response generation . We have   released QKConv ’s code and model checkpoints ,   hoping to facilitate further research in knowledge-   intensive conversations .   In summary , the main contributions of this paper   are : ( 1 ) We propose an unsupervised query en-   hanced approach via joint training for knowledge-   intensive conversations , namely QKConv . To the   best of our knowledge , we are the first to utilizejoint training for query generation . ( 2 ) We show   that QKConv achieves state - of - the - art end - to - end   results against all unsupervised methods and outper-   forms supervised methods on certain datasets . ( 3 )   We show that QKConv exhibits superior query qual-   ity and robust knowledge utilization in response   generation .   2 Methodology   This paper introduces a query enhanced approach   of QKConv , which incorporates query generation   to boost knowledge - intensive conversations and   optimizes the dialogue system via unsupervised   joint training . As shown in Figure 1 , QKConv   consists of three modules : Query Generator to   generate multiple queries based on the dialogue   context ; an off - the - shelf Knowledge Selector to   find relevant knowledge given queries ; Response   Generator to produce the final response . In the   following , we will elaborate the design of these   modules and discuss the process of joint training   in detail .   2.1 Query Enhanced Knowledge - Intensive   Conversation Modeling   Query Generator   The query generator aims to produce an effective   query to retrieve appropriate knowledge for re-   sponse generation . In the training process , with   the dialogue context as input , the query genera-   tor will explore and produce multiple queries as   candidates . The dialogue context is the concatena-   tion of previous utterances c={u , u , . . . , u } ,   and the candidate query q∈ Q is generated with1731probability p(q|c ) .   Knowledge Selector   The knowledge selector needs to find relevant   knowledge from the knowledge base for a given   query . To guarantee selection relevance , the off-   the - shelf knowledge selector consists of one re-   triever for fast knowledge recall and one succes-   sive reranker for fine - grained relevance estimation .   Given a candidate query q , the final knowledge se-   lection score is the combination of two - stage scores   ( Gallagher et al . , 2019 ):   p(k|q ) = σ / parenleftbig   S ( k|q ) + S ( k|q)/parenrightbig   ( 1 )   where σ(·)refers to the sigmoid function . Unless   specified , the knowledge with the highest score   will be selected for the given query and used in the   response generation .   Response Generator   The response generator aims to produce an appro-   priate response grounded on selected knowledge .   In the training process , with the dialogue context   and candidate knowledge as input , the probabil-   ity of producing the target response is estimated   asp(r|c , k ) . In addition , the response and query   generators share model parameters , with prompts   added for task differentiation .   2.2 Joint Training   Under such a design , the response generation in   knowledge - intensive conversations is modeled as   follows :   p(r|c)∝/summationdisplayp(q|c)p(k|q)p(r|c , k ) ( 2 )   where cis the dialogue context , ris the target re-   sponse , qis one candidate query , and kis its corre-   sponding knowledge . The training objective is to   maximize the generation probability of the target   response through marginalization over candidate   queries . Exploring multiple query candidates leads   to diverse knowledge selection and generation prob-   ability of target response . Supposing one candidate   query stimulates the knowledge coherent with the   dialogue context and relevant to the target response ,   the joint training will encourage this query genera-   tion and facilitate knowledge utilization in responsegeneration . Otherwise , the joint optimization will   suppress the corresponding query generation and   restrain knowledge utilization in response genera-   tion .   During training , we propose to integrate context-   sensitive guidance ( e.g. , the last context utterance   u ) and response - sensitive guidance ( e.g. , the tar-   get response r ) into the candidate query set . The   benefits brought by the guidance integration are   two - fold . Firstly , the query guidance can regu-   late query generation . Context - sensitive guidance   suggests extracting essential information from the   context , and response - sensitive guidance suggests   predicting the focus of the target response . These   two guidance act as references and help the query   generator avoid non - sense queries in unsupervised   training . Secondly , the two types of query guidance   can facilitate joint training . Since selecting the rel-   evant knowledge for the target response is chal-   lenging , constant exposure to irrelevant knowledge   will make the model ignore given knowledge and   generate generic responses . Incorporating context-   sensitive ( prior ) and response - sensitive ( posterior )   guidance amplifies knowledge diversity and en-   hances the selection of relevant knowledge . The   exposure to diverse knowledge ( relevant and irrele-   vant ) helps facilitate end - to - end joint training . In   short , such incorporation helps avoid the degrada-   tion of non - sense query generation and knowledge-   independent response generation in joint training .   To alleviate the costly query generation and   knowledge selection at each training step , we uti-   lize iterative training to speed up the training pro-   cess , which embraces an inner - outer loop structure   for model training and data collection . In the outer   loop , the inference is carried out over the train set to   collect candidate queries with the up - to - date query   generator and corresponding knowledge with the   off - the - shelf knowledge selector . In the inner loop ,   the query and response generators are optimized   jointly to maximize the probability of the target   response . The inner - outer loop will iterate several   times until convergence .   3 Experiments   3.1 Experiment Settings   3.1.1 Datasets   We conduct experiments on three datasets over   diverse knowledge - intensive conversation tasks :   QReCC ( Anantha et al . , 2021 ) for conversational   question answering , Standford Multi - Domain1732   ( SMD ) ( Eric et al . , 2017 ) for task - oriented dia-   logue , and Wizard of Wikipedia ( WoW ) ( Dinan   et al . , 2019 ) for open - domain knowledge - grounded   dialogue .   QReCCcontains 14 K open - domain conversa-   tions with 80 K question - answer pairs , where each   conversational question is rewritten into a self-   contained query by human crowdworkers . The   knowledge base is a collection of 54 M passages   split from 10 M web pages and indexed by BM25 .   SMD is a task - oriented dialogue dataset in-   cluding 3 K conversations . Each conversation is   equipped with a small knowledge base .   Wizard of Wikipedia ( WoW)is an open - domain   dialogue dataset with 18 K conversations . The   conversations are grounded on knowledge from   Wikipedia retrieved by TF - IDF .   3.1.2 Baselines   We compare QKConv to the previous state - of - the-   art supervised and unsupervised models on each   dataset . Details about the compared models are   summarized in Table 1 . Supervised models lever-   age either query annotations or knowledge selec-   tion annotations , while unsupervised models only   rely on the dialogue context and response . Among   these models , tuning dense retrievers is employed   in DPR ( IHN)-FiD ( Kim and Kim , 2022 ) , Re2 G   ( Glass et al . , 2022 ) , Hindsight ( Paranjape et al . ,   2022 ) , while the query generation method is pre-   ferred by Q - TOD ( Tian et al . , 2022 ) and Raposo   et al . ( 2022 ) . Compared to methods augmented by   knowledge selection , UnifiedSKG ( Xie et al . , 2022 )   utilizes the entire knowledge base to generate the   response.3.1.3 Implementation Details   Knowledge Selector Following the retriever set-   ting of the original dataset , BM25 and TF - IDF   are employed for QReCC and WoW , respectively .   However , the SMD dataset does not involve a re-   triever due to the fairly small knowledge base . For   reranking , an off - the - shelf model RocketQA ( Ren   et al . , 2021 ) is used for all datasets .   Generator We employ the same pre - trained model   as the state - of - the - art supervised model to per-   form query and response generation , i.e. , T5 - base   ( 220 M ) ( Raffel et al . , 2020 ) for QReCC , T5 - large   ( 770 M ) ( Raffel et al . , 2020 ) for SMD , and BART-   large ( 400 M ) ( Lewis et al . , 2020a ) for WoW.   Training QKConv is trained in an inner - outer   loop structure that iteratively executes query gen-   eration , knowledge selection in the outer loop , and   model updating in the inner loop . For query gen-   eration , we adopt beam search with a beam size   of 4 as the decoding strategy and use all decod-   ing results as candidate queries . Therefore , the   set of query candidates consists of four generated   queries , one response - sensitive guidance , and one   context - sensitive guidance . The response - sensitive   guidance refers to the target response . In light   of previous common queries ( Raposo et al . , 2022 ;   Shuster et al . , 2021 ) , the context - sensitive guidance   refers to the last utterance of dialogue on QReCC   and dialogue context on SMD and WoW. To famil-   iarize pre - trained models with dialogue tasks , the   generator is warmed up with the response genera-   tion task for a few epochs .   Inference The decoding strategy of query and   response generation is beam search with a beam   size of 4 . We use the decoding result with the   highest probability as the final result .   More details about hyperparameter settings are   provided in Appendix A.1733   3.2 Results   We evaluate the end - to - end performance of our   models on the three knowledge - intensive dialogue   datasets following the metrics used in prior stud-   ies ( Anantha et al . , 2021 ; Eric et al . , 2017 ; Petroni   et al . , 2021 ) . In particular , Entity - F1 ( Eric et al . ,   2017 ) measures overlapping entities between gen-   erated response and ground truth . KILT - F1 and   KILT - Rouge - L ( KILT - RL ) ( Petroni et al . , 2021 )   only award points to instances with accurate knowl-   edge selection . Table 2 summarizes the results of   our models and the state - of - the - art models trained   with and without supervision on three datasets .   QKConv consistently outperforms the unsuper-   vised results on three datasets and even surpasses   the supervised results on QReCC and WoW. Com-   pared to unsupervised models , on the F1 score , QK-   Conv achieves a relative improvement of 78.2 % on   QReCC , 4.7 % on SMD , and 1.9 % on WoW , respec-   tively . The encouraging improvements demonstrate   that our proposed QKConv has strong effectiveness   and robustness to generate high - quality responses   across various knowledge - intensive conversations .   In comparison to supervised SOTA with retriever   finetuning , QKConv obtains the best F1 scores with   a relative increment of 10.8 % on QReCC , and 5.1 %   on WoW , respectively . As for the supervised mod-   els with query annotations , the relatively lower   Entity - F1 on SMD suggests some room for im-   provement for unsupervised QKConv .   4 Discussion   In this section , to further dissect the proposed   QKConv , more experiments are conducted on the   QReCC dataset . Unless specified , the pre - trained   model of T5 - large is employed in the following   experiments .   4.1 Query Generation Analysis   In this paper , a query enhanced approach is intro-   duced for knowledge - intensive conversations . For   an in - depth analysis of query incorporation , we   will discuss three research questions regarding QK-   Conv ’s query on essential , modality , and superior-   ity .   RQIs it essential to generate queries for knowl-   edge selection ?   It is known that the most straightforward way is   to employ the dialogue context or the last utterance   as the query for knowledge selection . We compare   the impact of various query types on knowledge   selection , with results summarized in Table 3.The   knowledge selection results by the target response   and golden query are also provided for reference .   Measure by the Recall@1 score , QKConv ’s gen-   erated query improves knowledge selection perfor-   mance by 4.16 % compared to the dialogue con-   text and narrows the gap to 5.75 % compared to the   golden query . In addition , the improvement reaches   34.04 % compared to the widely adopted last utter-   ance . These results suggest that query generation   is essential in boosting knowledge selection .   RQWhat is the generated query ’s modality ,   similar to the dialogue context or the response ?   As described in Section 2.2 , QKConv incorpo-   rates context - sensitive and response - sensitive guid-   ance to regulate query generation . After joint train-   ing , what is the modality of the generated query,1734   similar to the dialogue context or the response ?   For this investigation , we estimate the similarity   of the generated query to the dialogue context and   the target response using the word overlapping F1   metric . The Context - F1 and Response - F1 results   are summarized in Table 3 , together with the query   length statistics .   The relatively high value of Context - F1 indicates   that the generated query gathers intensive informa-   tion from the context . Meanwhile , the relatively   high value of Response - F1 indicates that the gener-   ated query includes relevant information with the   response . In short , the generated query exhibits a   hybrid modality , incorporating intensive informa-   tion from the dialogue context and some predicted   hints toward the response . One qualitative example   is also provided in Table 8 to illustrate this phe-   nomenon .   RQIs the performance of the generated query   superior to other state - of - the - art approaches ?   On the QReCC dataset , CONQRR ( Wu et al . ,   2021 ) is the state - of - the - art query generation ap-   proach , which leverages query annotations and   a reward function to optimize the query genera-   tion through supervised and reinforcement learn-   ing . CONQRR utilizes the BM25 retriever as the   knowledge selector and employs T5 - base as the pre-   trained model . Table 4 summarizes the knowledge   selection performance of CONQRR and QKConv .   When compared under the same retriever , de-   spite that QKConv is optimized via unsupervised   joint training , the generated query achieves 4.79 %   higher MRR@10 than CONQRR . The remarkable   improvement of generated queries confirms the   superior performance of QKConv on knowledge   selection . In addition , QKConv equipped with a   reranker raises MRR@10 by 6.52 % and Recall@1   by 5.39 % significantly . These results confirm the   benefits of adopting the combinatorial knowledge   selector.4.2 Knowledge Utilization Ability   QKConv also demonstrates strong knowledge uti-   lization ability in response generation , apart from   accurate knowledge selection in query generation .   As the selected knowledge is not always appro-   priate , the response generator encounters the chal-   lenge of properly utilizing the selected knowledge .   When confronting appropriate knowledge , the re-   sponse generator is expected to ground on the   knowledge and then incorporate it properly . In   contrast , with irrelevant knowledge , the response   generator should denoise and eliminate high re-   liance on it .   To investigate the knowledge utilization ability   of QKConv , we divide the selected knowledge into   accurate and inaccurate knowledge according to   the Recall@1 metrics . We compare the response   generator of QKConv with the response generator   baseline . The baseline model is trained in an in-   dividually optimized manner ( not joint training ) ,   with the dialogue context and knowledge selected   by golden queries as input and the target response   as output . In the evaluation phase , the same data is   applied for comparisons .   Automatics evaluation We compute the F1 score   between generated responses and ground truth and   the KR - F1 score for both models . The KR - F1 score ,   adapted from Paranjape et al . ( 2022 ) , evaluates   the F1 score between generated response and se-   lected knowledge ( not golden knowledge ) . The op-   timal value for KR - F1 is the one being close to the   KR - F1 by ground truth , which indicates a natural   knowledge utilization rather than under - utilization   or over - reliance .   Table 5 summarizes knowledge utilization abil-   ity with ground - truth results as references . For the   overall F1 score , QKConv outperforms the base-   line model by 1.87 % . Considering results based   on knowledge correctness , the KR - F1 for correct   knowledge is more significant than incorrect knowl-   edge by 3.73 % in QKConv . The notable gap reveals   that QKConv can distinguish knowledge associated   with dialogue context and rely more on the correct   knowledge . A similar but smaller gap ( 2.13 % ) can   be found in the baseline model , which suggests   that this ability is introduced by exposing diverse   knowledge quality to response generation during   training . Furthermore , with the correct knowledge ,   QKConv demonstrates a significantly higher F1   and closer KR - F1 than the baseline model.1735   Human evaluation We randomly sampled 50   examples with correct knowledge and another 50   with incorrect knowledge . Crowd - sourcing work-   ers evaluate each sample on three aspects with a   range of [ 0 , 1 , 2 ] :   •Coherence assesses whether the response is rele-   vant and consistent with the dialogue context .   •Groundedness assesses whether the response con-   tains information from the given knowledge .   •Engagingness measures the willingness to have a   long conversation .   Table 6 demonstrates that QKConv outperforms   the baseline model regarding Coherence and Engag-   ingness , while achieving similar levels of Ground-   edness with accurate knowledge and lower Ground-   edness ( by 0.27 ) with inaccurate knowledge . These   results indicate that compared to the individually-   optimized baseline , QKConv can incorporate cor-   rect knowledge to a more natural degree and yield   higher - quality responses .   In short , both automatic and human evaluation   results confirm that QKConv attains robustness to   different qualities of knowledge and a remarkable   knowledge utilization ability to correct knowledge .   4.3 Effect of Guidance   We propose context - sensitive and response-   sensitive guidance to regulate query generation   and facilitate joint training . The query generation   demonstrates a hybrid modality under the regula-   tion of guidance as described in Section 4.1 . To   scrutinize the efficacy of guidance in joint training ,   we conduct ablation experiments with QKConv ,   detailed in Table 7 .   In the absence of all guidance , our model wit-   nesses a marked decrease in all metrics , resulting in   2.92%/1.09%/2.93 % declines in F1 / EM / Recall@1 .   With the incorporation of either guidance , knowl-   edge selection and end - to - end performances are en-   hanced to a considerable extent but remain inferior   to QKConv . These results indicate that both types   of guidance contribute to joint training , and the   combined implementation yields the most signifi-   ca nt benefits . Despite the decline in performance ,   QKConv trained without guidance still outperforms   the state - of - the - art models ( Raposo et al . ( 2022 )   with 18.90 F1 and 1.00 EM ) , highlighting that the   advantages of our method are brought by joint train-   ing and boosted by two types of query guidance .   4.4 Case Studies   We provide a cherry - picked example and a lemon-   picked example in Table 8 to gain insight into the   performance of QKConv . Additional examples are   available in Appendix E.   The cherry - picked example inquires about the   reaction of a previously stated book . For query   generation , the query generated by QKConv is   response - looking , attempting to reply to the con-   versation . Although the response - looking query   contains certain counterfeit information , the book ’s   full title extracted from the conversation history   contributes to accurate knowledge selection . For   response generation , QKConv locates the relevant   sentence within the long knowledge paragraph and   generates an appropriate response .   The lemon - picked example inquires about an   actor ’s films in addition to the previously men-   tioned one . Our model ’s generated query is also   response - looking , extracting relevant information   from the previous text and organizing it into a re-1736   sponse . However , the model fails to consider the   limiting word " other " in the last utterance , resulting   in inappropriate knowledge selection and a similar   response as in the previous dialogue history .   5 Related Work   Knowledge - Intensive Conversation To attend   knowledge in conversations , some prior studies   concentrate on how to ground the given knowl-   edge ( Ma et al . , 2020 ; Xie et al . , 2022 ) or elicit   parametric knowledge from large language models   ( Zhao et al . , 2020b ) . Recently , access to an external   knowledge corpus has attracted a spate of interest ,   in line with our paper , and has come up with sev-   eral datasets . For instance , some datasets provide a   fixed small knowledge base for each sample ( Eric   et al . , 2017 ; Wen et al . , 2017 ; Dinan et al . , 2019 ;   Moghe et al . , 2018 ) . In more natural circumstances ,   using a uniform large - scale knowledge base for all   samples , such as Wikipedia dumps , web crawl data ,   or even search engines , has become a trend ( Zhou   et al . , 2018 ; Petroni et al . , 2021 ; Anantha et al . ,   2021 ; Komeili et al . , 2022 ) . However , it should be   noted that knowledge selection challenges increase   with the size of the knowledge base , and selection   performance bounds the performance of response   generation . Therefore , the performance of knowl-   edge selection is crucial for knowledge - intensive   dialogue . Two primary directions to address knowl-   edge selection are finetuning knowledge selectorsand generating a context - independent query .   Retrieval - Augmented Generation Recently , an   increasing interest has been shown in modeling   a dense knowledge selector and response genera-   tor simultaneously , with the dialogue context as   the query . Many of these works utilize joint train-   ing ( Lewis et al . , 2020b ; Guu et al . , 2020 ; Shuster   et al . , 2021 ; Huang et al . , 2021 ; Thulke et al . , 2021 ;   Glass et al . , 2022 ) or reinforcement learning ( Zhao   et al . , 2020a ) to modify the prior selection distri-   bution . As opposed , some studies directly involve   the posterior distribution of knowledge to enhance   knowledge selection ( Lian et al . , 2019 ; Kim et al . ,   2020 ; Paranjape et al . , 2022 ) . However , repeated in-   dex rebuilding for the updated knowledge selector   is time - consuming with the large - scale knowledge   base , and the involvement of posterior distribution   may render the training - inference discrepancy . Fur-   thermore , a few works consider a complicated se-   lection process attributed to the challenging and in-   terrupted gradient propagation ( Glass et al . , 2022 ) .   This paper investigates the query generator rather   than the selector and exploits off - the - shelf selectors   to refrain from the above problems .   Query Generation A lengthy dialog context as   the query reduces the efficiency of the knowledge   selector and may be misaligned with the form pre-   ferred in off - the - shelf selectors . Prior works ( Yu   et al . , 2020 ; Anantha et al . , 2021 ; Vakulenko et al . ,   2021 ; Komeili et al . , 2022 ; Tian et al . , 2022 ) lever-1737age query annotations as supervision to train query   generators that convert a dialogue context into a   context - independent query , but facing the problem   of human - written queries often unavailable in prac-   tice . With the absence of external supervision , Mao   et al . ( 2021 ) regards response and knowledge as   training targets to expand the original query . How-   ever , memorizing response and knowledge has a   heavy burden on the model for a large - scale knowl-   edge base . Moreover , some current studies argue   that the supervised learning of queries disconnects   from knowledge selection and end - to - end perfor-   mance ( Wu et al . , 2021 ; Chen et al . , 2022 ) . In-   stead , they exploit reinforcement learning with   extra query and retrieval annotations to generate   queries adaptive to downstream performance . In   this paper , we propose a novel query enhanced ap-   proach that jointly trains the query generator with   the response generator without additional super-   vision . The end - to - end training also ensures the   downstream performance of queries . Furthermore ,   our approach with two query guidance gets exempt   from the risk of generating unreadable sentences   experienced frequently in reinforcement learning   ( Ouyang et al . , 2022 ) .   6 Conclusion   This paper introduces a query enhanced approach   of QKConv for knowledge - intensive conversations ,   which is optimized via unsupervised joint training   without any reliance on query annotations or knowl-   edge provenances . The experiments are carried out   on three knowledge - intensive conversation datasets :   conversational question answering QReCC , task-   oriented dialogue SMD , and knowledge - grounded   conversation WoW. The proposed QKConv out-   performs all unsupervised methods across three   datasets . Compared to supervised methods , QK-   Conv even establishes new state - of - the - art results   on QReCC and WoW. Further analysis demon-   strates that with joint training , the query genera-   tion adapts well to the knowledge selector , and   the response generation has utilization robustness   towards various knowledge .   Limitations   As shown in Table 2 , our approach underperforms   the state - of - the - art supervised model on the SMD   dataset , where the supervised SOTA labels a search   instruction for each sample . In addition , the lemon-   picked example in Table 8 demonstrates that some - times it is challenging for the query generator to   learn complicated expressions automatically . De-   spite our model ’s superiority over all unsupervised   methods , these gaps reveal some improvement   room of QKConv . In Appendix D , we try to bridge   the gaps by incorporating a few query annotations .   Another limitation is that our approach suffers from   the time - consuming off - the - shelf knowledge selec-   tion when given a large dataset and knowledge   base . It takes half of the training hours in knowl-   edge selection since it involves heavy computation   of retrieval from a large - scale knowledge base and   reranking with a cross - encoder .   Acknowledgement   We would like to thank the anonymous reviewers   for valuable comments . We thank Hua Lu and   Yingzhan Lin for helpful discussions ; Jingzhou   He , Shiwei Huang , and Dou Hong for the help on   resource coordination .   References17381739   A Model Details   We apply iterative training of our model with an   inner - outer loop structure several times until con-   vergence . We used 8 NVIDIA A100 GPUs with   approximately 4 hours for each iteration .   The outer loop executes query generation and   knowledge selection to collect training data . Given   a query for QReCC and WoW , we retrieve top-50   knowledge from the knowledge base and get the   top-1 after reranking . For SMD , we obtain top-3   knowledge after reranking due to the requirement   of multiple knowledge for response generation .   The inner loop updates the model with collected   data . The hyperparameters are the same for all   datasets but differentiate the learning rate by model   scale , detailed in Table 9 . The model checkpoint is   determined by the F1 score in the validation set .   B Scoring Criteria in Human Evaluation   The criteria of human evaluation are provided in   Table 10 .   C Model Scalability   Motivated by the generally observed phenomenon   that the generation ability improves with the model   size , we evaluate the scalability of QKConv on   the QReCC dataset with T5 - base , T5 - large , and   T5 - 3B. The metrics of EM and Recall@1 are cri-   teria to evaluate response generation and query1740   generation , respectively . As shown in Figure 2 ,   the EM scores of generated response increase by   roughly 0.9 % with each scale - up , and Recall@1   scores of generated query experience a 1.4 % av-   erage boost for each scale - up . Specifically , there   is a more significant benefit when increasing the   model size from T5 - base to T5 - large than T5 - large   to T5 - 3B. Furthermore , as the improved knowl-   edge selection also contributes to response gener-   ation , the EM scores have a more notable relative   increase ( +16.4 % ) compared to the Recall@1 score   ( +3.4 % ) .   D Few Query Supervision   QKConv has limitations in resolving complex   query conditions . To bridge the gaps , we incor-   porate a few query annotations into training . To   be specific , 1 % or 10 % of human - rewritten queries   replace the context - sensitive guidance during train-   ing to regulate query generation and facilitate joint   training . Figure 3 shows that some query annota-   tions can further improve query generation and re-   sponse generation , especially with more supervised   data . It is worth noting that the marginal benefit of   knowledge selection on response generation is rel-   atively small in models of the same scale . Accord-   ing to the examples in Table 11 , adding 1 % super-   vised data has a minor impact on the queries , while   adding 10 % supervised data enables the model to   rewrite the last utterance without impairing its orig-   inal ability to extract previous contexts .   E Additional Qualitative Results   The following tables provide qualitative results of   models in Table 2 for all datasets . For query gen-   eration , Table 13 and Table 14 contain examples   of SMD and WoW where the generated queries   also support the heterogeneous query generation   modality . The query generator of QKConv tends to   extract relevant information from the dialogue con-   text or generate a plausible response - looking query   interfusing the essential information from the dia-   logue context . The response - looking query guides   knowledge selection toward the target response ,   while the authentic information derived from the di-   alogue context ensures the relevance of knowledge   selection . For response generation , the generated1741   response on all datasets exemplifies our model ’s su-   perior knowledge utilization ability . The selected   knowledge passages are lengthy on QReCC and   WoW , while top-3 knowledge is selected on SMD .   Therefore , the knowledge inevitably includes use-   less information for reply . However , QKConv is   capable of denoising and locating the appropriate   span within the long knowledge passage.17421743ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After Section 6 .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   In Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   In Section 3 and Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In Appendix A.1744 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   In Section 3.1 and Appendix A.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   In Section 3.1 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1745