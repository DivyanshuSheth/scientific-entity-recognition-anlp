  Mingyang Song , Yi Feng and Liping Jing   Beijing Key Lab of Trafﬁc Data Analysis and Mining   Beijing Jiaotong University , China   mingyang.song@bjtu.edu.cn   Abstract   Keyphrase extraction is a fundamental task in   natural language processing that aims to ex-   tract a set of phrases with important informa-   tion from a source document . Identifying im-   portant keyphrases is the central component of   keyphrase extraction , and its main challenge is   learning to represent information comprehen-   sively and discriminate importance accurately .   In this paper , to address the above issues , we   design a new hyperbolic matching model ( Hy-   perMatch ) to explore keyphrase extraction in   hyperbolic space . Concretely , to represent in-   formation comprehensively , HyperMatch ﬁrst   takes advantage of the hidden representations   in the middle layers of RoBERTa and integrates   them as the word embeddings via an adaptive   mixing layer to capture the hierarchical syntac-   tic and semantic structures . Then , considering   the latent structure information hidden in natu-   ral languages , HyperMatch embeds candidate   phrases and documents in the same hyperbolic   space via a hyperbolic phrase encoder and a   hyperbolic document encoder . To discriminate   importance accurately , HyperMatch estimates   the importance of each candidate phrase by   explicitly modeling the phrase - document rele-   vance via the Poincaré distance and optimizes   the whole model by minimizing the hyperbolic   margin - based triplet loss . Extensive experi-   ments are conducted on six benchmark datasets   and demonstrate that HyperMatch outperforms   the recent state - of - the - art baselines .   1 Introduction   Keyphrase Extraction ( KE ) aims to extract a set   of phrases related to the main points discussed in   the source document , a fundamental task in Natu-   ral Language Processing ( NLP ) . Because of their   succinct and accurate expression , keyphrase extrac-   tion is helpful for a variety of applications such as   information retrieval ( Kim et al . , 2013 ) and text   summarization ( Liu et al . , 2009a).Figure 1 : Sample partial of the document in OpenKP   dataset . For ease of presentation , we assume “ a large   region of land ” is a 5 - gram candidate phrase as an ex-   ample in the document .   Typically , most existing keyphrase extraction   models mainly include two procedures : candidate   keyphrase extraction and keyphrase importance es-   timation . Speciﬁcally , the former extracts candi-   date phrases from the document via some heuristics   ( Nguyen and Phan , 2009 ; Liu et al . , 2009b ; Grineva   et al . , 2009 ; Wan and Xiao , 2008 ; Liu et al . , 2009c ) ,   and the latter determines which candidate phrases   are keyphrases via unsupervised or supervised mod-   els ( Mihalcea and Tarau , 2004 ; Xiong et al . , 2019 ;   Sun et al . , 2020 ; Song et al . , 2021 ) . The keyphrase   importance estimation procedure usually plays a   more critical role than the candidate keyphrase ex-   traction procedure in the supervised setting .   In the supervised neural keyphrase extraction   models , the keyphrase importance estimation pro-   cedure can be subdivided into information represen-   tation and importance discrimination . Speciﬁcally ,   the information representation part focuses on mod-   eling the encoding procedure , and the importance   discrimination part focuses on measuring the im-   portant scores of candidate phrases . To represent   information comprehensively , recent keyphrase ex-   traction studies have been proposed to build better   representations via Bi - LSTM ( Meng et al . , 2017 ) ,   GCNs ( Sun et al . , 2019 ; Zhang et al . , 2020 ) , and the5710pre - trained language models ( e.g. , ELMo ( Xiong   et al . , 2019 ) , BERT , and RoBERTa ( Liu et al . , 2020 ;   Sun et al . , 2020 ) ) . To discriminate the importance   of candidate phrases precisely , most existing su-   pervised keyphrase extraction models ( Sun et al . ,   2020 ; Mu et al . , 2020 ; Song et al . , 2021 ) estimate   and rank the importance of candidate phrases to   extract keyphrases by using different approaches ,   such as classiﬁcation and ranking models .   Although the existing keyphrase extraction mod-   els mentioned above have achieved signiﬁcant per-   formance , the keyphrase extraction task still needs   improvement . Among them , there are the following   two main issues . The ﬁrst issue lies in the informa-   tion representation . Typically , candidate phrases   often exhibit the inherent hierarchical structures   ingrained with complex syntactic and semantic in-   formation ( Dai et al . , 2021 ; Zhou et al . , 2020 ) . In   general , the longer phrases contain more complex   structures . ( as shown in Figure 1 , the phrase " a   large region of land " has more complex inherent   structures than " region " or " a large region " . Simi-   larly , the phrase " a large region " is more complex   than " region " ) . Besides the phrases , since linguis-   tic ontologies are intrinsic hierarchies ( Dai et al . ,   2021 ) , the conceptual relations between phrases   and their corresponding document can also form   the hierarchical structures . Therefore , the hierar-   chical structures need to be considered when repre-   senting both phrases and documents and estimating   the phrase - document relevance . However , it is dif-   ﬁcult to capture such structural information even   with inﬁnite dimensions in the Euclidean space   ( Linial et al . , 1995 ) . The second issue lies in dis-   tinguishing the importance of phrases . Keyphrases   are typically used to retrieve and index their cor-   responding document , so they should be highly   related to the main points of the source document   ( Hasan and Ng , 2014 ) . However , most existing   supervised keyphrase extraction methods ignore   explicitly modeling the relevance between candi-   date keyphrases and their corresponding document ,   resulting in biased keyphrase extraction .   Motivated by the above issues , in this paper , we   explore the potential of hyperbolic space for the   keyphrase extraction task and propose a new hyper-   bolic relevance matching model ( HyperMatch ) for   supervised neural keyphrase extraction . Firstly , to   capture hierarchical syntactic and semantic struc-   ture information , HyperMatch integrates the hid-   den representations in all the intermediate layers ofRoBERTa to collect the adaptive contextualized   word embeddings via an adaptive mixing layer   based on the self - attention mechanism . And then ,   considering the hierarchical structure hidden in the   natural language content , HyperMatch represents   both phrases and documents in the same hyperbolic   space via a hyperbolic phrase encoder and a hyper-   bolic document encoder . Meanwhile , we adopt the   Poincaré distance to calculate the phrase - document   relevance by considering the latent hierarchical   structures between candidate keyphrases and the   document . In this setting , the keyphrase extraction   task can be regarded as a matching problem and ef-   fectively implemented by minimizing a hyperbolic   margin - based triplet loss . To the best of our knowl-   edge , we are the ﬁrst work to explore the super-   vised keyphrase extraction in hyperbolic space . Ex-   periments on six benchmark datasets demonstrate   that HyperMatch outperforms the state - of - the - art   keyphrase extraction baselines .   2 Preliminaries   Hyperbolic space is an important concept in hyper-   bolic geometry , which is considered as a special   case in the Riemannian geometry ( Hopper and An-   drews , 2011 ) . Before presenting our model , this   section brieﬂy introduces the basic information of   hyperbolic space .   In a traditional sense , hyperbolic spaces are not   vector spaces ; one can not use standard operations   such as summation , multiplication , etc . To rem-   edy this problem , one can utilize the formalism of   Möbius gyrovector spaces allowing the generaliza-   tion of many standard operations to the hyperbolic   spaces ( Khrulkov et al . , 2020 ) . Similarly to the pre-   vious work ( Nickel and Kiela , 2017 ; Ganea et al . ,   2018 ; Tifrea et al . , 2019 ) , we adopt the Poincaré   ball and use an additional hyper - parameter cwhich   modiﬁes the curvature of Poincaré ball ; it is then de-   ﬁned as D={x∈R : c / bardblx / bardbl<1,c≥0 } . The   corresponding conformal factor now takes the form   λ:=. In practice , the choice of callows   one to balance the hyperbolic and the euclidean   geometries , which is made precise by noting that   whenc→0 , all the formulas discussed below take   their usual Euclidean form .   In the following , we restate the deﬁnitions of   fundamental mathematical operations for the gen-   eralized Poincaré ball model ( Ganea et al . , 2018 ) .   Next , we give the details of the closed - form formu-   las of several Möbius operations.5711   Möbius Addition . For a pair x , y∈D , the   Möbius addition is deﬁned as ,   Möbius Matrix - vector Multiplication . For a lin-   ear map M : R→Rand∀x∈D , ifMx / negationslash= 0 ,   then the Möbius matrix - vector multiplication is de-   ﬁned as ,   where M⊗x= 0ifMx= 0 .   Poincaré Distance . The induced distance function   is deﬁned as ,   Note that with c= 1 one recovers the geodesic   distance , while with c→0we obtain the Euclidean   distance limd(x , y ) = 2 / bardblx−y / bardbl .   Exponential and Logarithmic Maps . To perform   operations in hyperbolic space , one ﬁrst needs to   deﬁne a mapping function from RtoDto map   the euclidean vectors to the hyperbolic space . Let   TDdenote the tangent space of Datx . The   exponential map exp ( · ) : TD→Dforv / negationslash= 0   is deﬁned as :   As the inverse of exp ( · ) , the logarithmic map   log ( · ) : D→TDfory / negationslash = xis deﬁned as : Hyperbolic Averaging Pooling . The average pool-   ing , as an important operation common in natural   language processing , is averaging of feature vec-   tors . In the euclidean setting , this operation takes   the following form :   Extension of this operation to hyperbolic spaces   is called the Einstein Midpoint and takes the most   simple form in Klein coordinates :   whereγ=√is the Lorentz factor . Re-   cent work ( Khrulkov et al . , 2020 ) demonstrates that   the Klein model is supported on the same space as   the Poincaré ball ; however , the same point has dif-   ferent coordinate representations in these models .   Letxandxdenote the coordinates of the same   point in the Poincaré and Klein models correspond-   ingly . Then the following transition formulas hold .   Therefore , given points in the Poincaré ball , we   can ﬁrst map them to the Klein model via Eq.(9 ) ,   compute the average using Eq.(7 ) , and then move   it back to the Poincaré model via Eq.(8).57123 HyperMatch   Given a document D={w, ... ,w, ... ,w } , the   candidate phrases are ﬁrst extracted from the source   document by the n - gram rules , where Mindicates   the max length of the input document . Then , to   determine which candidate phrases are keyphrases ,   we design a new hyperbolic relevance matching   model ( HyperMatch ) , which mainly consists of   two components : information representation and   importance discrimination . Figure 2 illustrates the   overall framework of HyperMatch .   3.1 Information Representation   Information representation is one of the essential   parts of keyphrase importance estimation , which   needs to represent information comprehensively .   To capture rich syntactic and semantic information ,   HyperMatch ﬁrst embeds words by the pre - trained   language model RoBERTa with the adaptive mix-   ing layer . Then , phrases and documents are embed-   ded in the same hyperbolic space by a hyperbolic   phrase encoder and a hyperbolic document encoder .   In the following subsections , the information repre-   sentation procedure will be described in detail .   3.1.1 Contextualized Word Encoder   Pre - trained language models ( Peters et al . , 2018 ;   Devlin et al . , 2019 ; Liu et al . , 2019 ) have emerged   as a critical technology for achieving impressive   gains in natural language tasks . These models ex-   tend the idea of word embeddings by learning con-   textualized text representations from large - scale   corpora using a language modeling objective . Thus ,   recent keyphrase extraction methods ( Xiong et al . ,   2019 ; Sun et al . , 2020 ; Wang et al . , 2020 ; Mu et al . ,   2020 ) represent words / documents by the last in-   termediate layer of pre - trained language models .   However , various probing tasks ( Jawahar et al . ,   2019 ; de Vries et al . , 2020 ) are proposed to discover   linguistic properties learned in contextualized word   embeddings , which demonstrates that different in-   termediate layers in pre - trained language models   contain different linguistic properties or informa-   tion . Speciﬁcally , each layer has speciﬁc specializa-   tions , so combining features from different layers   may be more beneﬁcial than selecting the last one   based on the best overall performance .   Motivated by the phenomenon above , we pro-   pose a new adaptive mixing layer to combine all   intermediate layers of RoBERTa ( Liu et al . , 2019 )   to obtain word representations . Firstly , each wordin the source document Dis represented by all the   intermediate layers in RoBERTa , which is encoded   to a sequence of vector H={h, ... ,h, ... ,h }   as follows ,   H = RoBERTa{w, ... ,w, ... ,w } . ( 10 )   Specially , h∈Rindicates the i - th contextual-   ized word embedding of w , whereLanddare set   to12and768 . Then , the self - attention mechanism   is adopted to aggregate the multi - layer representa-   tions of each word from RoBERTa as follows :   α = softmax ( Vh ) , ( 11 )   ˆh = Wαh , ( 12 )   where V∈RandW∈Rdenote the   learnable weights . Here , α∈Rrepresents the   adaptive mixing weights of the proposed adaptive   mixing layer in HyperMatch . In this case , each   word in the source document Dis transferred to a   sequence of vector ˆH={ˆh, ... ,ˆh, ... ,ˆh } . The   adaptive mixing layer allows our model to obtain   more comprehensive word embeddings , capturing   more meaningful features ( e.g. , surface , syntactic ,   and semantic ) .   3.1.2 Hyperbolic Phrase Encoder   Phrases often exhibit inherent hierarchies ingrained   with complex syntactic and semantic information   ( Zhu et al . , 2020 ) . Therefore , representing infor-   mation requires sufﬁciently encoding semantic and   syntactic information , especially for the latent hier-   archical structures hidden in the natural languages .   Recent studies ( Sun et al . , 2020 ; Xiong et al . , 2019 )   typically obtain phrase representations in Euclidean   space , which makes it difﬁcult to learn representa-   tions with such latent structural information even   with inﬁnite dimensions in Euclidean space ( Linial   et al . , 1995 ) . On the contrary , hyperbolic spaces are   non - Euclidean geometric spaces that can naturally   capture the latent hierarchical structures ( Sarkar ,   2011 ; Sa et al . , 2018 ) .   Lately , the use of hyperbolic space in NLP ( Dhin-   gra et al . , 2018 ; Tifrea et al . , 2019 ; Nickel and   Kiela , 2017 ) is motivated by the ubiquity of hier-   archies ( e.g. , the latent hierarchical structures in   phrases , sentences , and documents ) in NLP tasks .   Therefore , in this paper , we propose to embed   phrases in hyperbolic space . Concretely , the phrase   representation of the i - thn - gramcis computed   as follows ,   ˆh = CNN(ˆh ) , ( 13)5713where ˆh∈Rrepresents the i - thn - gram rep-   resentation , n∈[1,N]indicates the length of n-   grams , andNis the maximum length of n - grams .   Eachn - gram has its own set of convolution ﬁlters   CNNwith window size nand stride 1 .   To capture the latent hierarchies of phrases , we   map phrases representation to the Poincaré ball   using the exponential map ,   ˜h = exp(ˆh ) , ( 14 )   where ˜his thei - thn - gram candidate phrase repre-   sentation in the hyperbolic space . By mapping the   representations of candidate phrases into the hyper-   bolic space , it is possible to implicitly capture the   latent hierarchical structure of candidate phrases   during the training procedure .   3.1.3 Hyperbolic Document Encoder   When using the source document as the query to   match keyphrases , the representation of the docu-   ment should cover its main points ( important infor-   mation ) . Meanwhile , documents are usually long   text sequences with richer semantic and syntactic   information than candidate phrases . Many current   BERT - based methods ( Mu et al . , 2020 ; Zhong et al . ,   2020 ) in NLP obtain documents representation by   using the ﬁrst output token ( the [ CLS ] token ) of the   pre - trained language models . However , recent stud-   ies ( Reimers and Gurevych , 2019 ; Li et al . , 2020 )   demonstrate that in many NLP tasks , documents   representation obtained by the average pooling of   words representation is better than the [ CLS ] token .   Motivated by the above issues , we use the aver-   age pooling , a simple and effective operation , to   encode documents . To further consider the latent   hierarchical structures of documents , we map word   representations and transfer the average pooling   operation to the hyperbolic space . In this case , we   ﬁrst map word representations to the hyperbolic   space via the exponential map as follows :   ˜H={˜h, ... ,˜h, ... ,˜h}=exp(ˆHW),(15 )   where W∈Rmaps the original BERT em-   bedding space to the tangent space of the origin of   the Poincaré ball . Then exp(·)maps the tangent   space inside the Poincaré ball . Next , we use the   hyperbolic averaging pooling to encode the source   document as follows :   ˜h = HyperAP ( { ˜h, ... ,˜h, ... ,˜h } ) , ( 16 )   where ˜h∈Rindicates the hyperbolic document   representation ( called Einstein Midpoint poolingvectors in the Poincaré ball ( Gulcehre et al . , 2019 ) ) .   The hyperbolic average pooling emphasizes seman-   tically speciﬁc words that usually contain more   information but occur less frequently than general   ones . It should be noted that points near the bound-   ary of the Poincaré ball get larger weights in the   Einstein Midpoint formula , which may be more rep-   resentative content in the source document ( Dhin-   gra et al . , 2018 ; Zhu et al . , 2020 ) .   3.2 Importance Discrimination   Importance discrimination is one of the primary   parts of the keyphrase importance estimation proce-   dure , which estimates the important scores of can-   didate phrases accurately to extract keyphrases . To   reach this goal , we ﬁrst calculate the scaled phrase-   document relevance between candidate keyphrases   and their corresponding document via the Poincaré   distance as the important score of each candidate   keyphrase . Then , the whole model is optimized by   the hyperbolic margin - based triplet loss to extract   keyphrases accurately .   3.2.1 Scaled Phrase - Document Relevance   Besides the intrinsic hierarchies of linguistic on-   tologies , the conceptual relations between candi-   date phrases and their corresponding document can   also form hierarchical structures . Once the docu-   ment representation ˜hand phrase representations   ˜hare obtained , it is expected that the phrases   and their corresponding document embedded close   to each other based on their geodesic distanceif   they are highly relevant . Speciﬁcally , the scaled   phrase - document relevance of the i - thn - gram rep-   resentationccan be computed as follows :   S(c , D ) = −λ(d(˜h,˜h ) )   √d+ ( 1−λ)f(˜h ) ,   ( 17 )   where S(·)indicates the scaled phrase - document   relevance . Here , dindicates the Poincaré dis-   tance , which is introduced in Eq.(3 ) . Here , f   indicates the linear transformation in hyperbolic   space . Speciﬁcally , for Eq . 17 , the ﬁrst term mod-   els the phrase - document relevance explicitly , and   the second term models the phrase - document rele-   vance implicitly . Estimating the phrase - document   relevance via the Poincaré distance in hyperbolic5714space allows HyperMatch to model the latent hier-   archical structures between candidate phrases and   their document , accurately estimating the impor-   tance of candidate keyphrases . In addition , we ﬁnd   that increasing the dimension dof representations   will increase the value of the phrase - document rele-   vance , causing the optimization collapse of our   model . To counteract this effect , we scale the   phrase - document relevance by .   3.2.2 Margin - based Triplet Loss   To select phrases with higher importances , we   adopt the margin - based triplet loss in our model   and optimize for margin separation in hyperbolic   space . Therefore , we ﬁrst treat the candidate   keyphrases in the document that are labelled as   keyphrases , in the positive set P , and the others to   the negative set P , to obtain the matching labels .   Then , the loss function is calculated as follows :   L = max(0,δ√d−S(p , D ) + S(p , D)),(18 )   whereδindicates the margin . It enforces Hyper-   Match to sort the candidate keyphrases pahead of   pwithin their corresponding document . Through   this training objective , our model will tend to ex-   tract the keyphrases , which are more relevant to the   source document .   4 Experimental Settings   4.1 Benchmark Datasets   Six benchmark keyphrase datasets are used in our   experiments , which contain OpenKP ( Xiong et al . ,   2019 ) , KP20k ( Meng et al . , 2017 ) , Inspec ( Hulth ,   2003 ) , Krapivin ( Krapivin and Marchese , 2009 ) ,   Nus(Nguyen and Kan , 2007 ) , and SemEval ( Kim   et al . , 2010 ) ) . We follow the previous work ( Sun   et al . , 2020 ) to preprocess each dataset with the   same procedure .   4.2 Implementation Details   Implementation details of HyperMatch are summa-   rized in Table 1 . The maximum document length is   512 tokens due to RoBERTa limitations ( Liu et al . ,   2019 ) and documents are zero - padded or truncated   to this length . Our model was implemented in   Pytorch 1.8(Paszke et al . , 2019 ) using the hug-   ging face reimplementation of RoBERTa(Wolf   et al . , 2019 ) and was trained on eight NVIDIA RTX   A4000 GPUs to achieve the best performance .   4.3 Evaluation Metrics   For the keyphrase extraction task , the performance   of the existing models is typically evaluated by   comparing the top- Kpredicted keyphrases with   the target keyphrases ( the ground - truth labels ) . The   evaluation cutoff Kcan be a ﬁxed number ( e.g. ,   F1@5 compares the top- 5keyphrases predicted   by the model with the ground - truth to compute an   F1 score ) . Following the previous work ( Meng   et al . , 2017 ; Sun et al . , 2020 ; Song et al . , 2021 ) , we   adopt macro - averaged recall and F - measure ( F1 ) as   evaluation metrics , and Kis set to be 1 , 3 , 5 , and 10 .   In the evaluation , we apply Porter Stemmerto both   the target keyphrases and the extracted keyphrases   when determining the exact match of keyphrases .   4.4 Baselines   We compare two kinds of solid baselines to give   a comprehensive evaluation of the performance   of HyperMatch : unsupervised keyphrase extrac-   tion models ( e.g. , TextRank ( Mihalcea and Tarau ,   2004 ) and TFIDF ( Jones , 2004 ) ) and supervised   keyphrase extraction models ( e.g. , classiﬁcation   and ranking models based variants of BERT ( Sun   et al . , 2020 ) ) . Noticeably , HyperMatch extracts   keyphrases without using additional features on the   OpenKP dataset . Therefore , for the sake of fair-   ness , we do not compare with the methods ( Xiong   et al . , 2019 ; Wang et al . , 2020 ) which use additional   features to extract keyphrases .   In addition , this paper mainly focuses on ex-   ploring keyphrase extraction in hyperbolic space   via a matching framework ( similar to the ranking   model ) . Hence , the compared baselines we mainly   choose are keyphrase extraction methods based on5715Model   the classiﬁcation and ranking models rather than   some existing studies based on integration models   ( Ahmad et al . , 2021 ; Wu et al . , 2021 ) or multi - task   learning ( Song et al . , 2021 ) .   5 Results and Analysis   In this section , we test the performance of Hyper-   Match on six widely - used benchmark keyphrase   extraction datasets ( OpenKP , KP20k , Inspec ,   Krapivin , Nus , and Semeval ) from three facets . The   ﬁrst one demonstrates its superiority by comparing   HyperMatch with the recent baselines in terms of   several metrics . The second one is to verify the   effect of each component via ablation tests . The   last one is to test the sensitivity of the hyperbolic   margin - based triplet loss with different margins .   5.1 Performance Comparison   The experimental results are given in Table 2 and   Table 3 . Overall , HyperMatch outperforms the   recent BERT - based keyphrase extraction models   ( the results are reported in their own articles ) in   most cases . Concretely , on the OpenKP andKP20k   datasets , HyperMatch achieves better results than   the best ranking models RoBERTa - Ranking - KPE .   The main reason for this result may be that learn-   ing representation in hyperbolic space can cap-   ture more latent hierarchical structures than the   euclidean space . Meanwhile , compared with the   results on the other four zero - shot datasets ( Inspec , Krapivin , Nus , and Semeval ) in Table 3 , it can be   seen that HyperMatch outperforms both unsuper-   vised and supervised baselines . We consider that   the main reason is the scaled phrase - document rel-   evance explicitly models a strong connection be-   tween phrases and their corresponding document   via the Poincaré distance , obtaining more robust   performance even in different datasets .   5.2 Ablation Study   In this section , we report on several ablation experi-   ments to analyze the effect of different components .   The ablation experiment on the OpenKP dataset is   shown in Table 4 .   To measure the effectiveness of hyperbolic space   for the keyphrase extraction task , we compare it   with the same model in the euclidean space and   use the euclidean distance to explicitly model the   phrase - document relevance . As shown in Table 4 ,   HyperMatch outperforms EuclideanMatch , which   shows that using the hyperbolic space can capture   the latent hierarchical structures more effectively   than the euclidean space .   To verify the effectiveness of the adaptive mix-   ing layer , we propose a model HyperMatch w/o   AML , which indicates HyperMatch without using   the adaptive mixing layer module and only uses   the last intermediate layer of RoBERTa to embed   phrases and documents . As shown in Table 4 , the   performance of our model HyperMatch without us-5716Model   Model   ing the adaptive mixing layer drops in all evaluation   metrics . These results demonstrate that combining   all the intermediate layers of RoBERTa may cap-   ture more helpful information ( e.g. , surface , syntac-   tic , and semantic ) for obtaining candidate phrases   and documents representations .   Unlike our model , most recent keyphrase extrac-   tion methods ( e.g. , RoBERTa - Ranking - KPE ) im-   plicitly model relevance between candidate phrases   and their corresponding document by a linear trans-   formation layer as the phrase - document relevance .   Therefore , to verify the effectiveness of explicitly   modeling the phrase - document relevance , we built   the HyperMatch w/o Relevance , which only implic-   itly computes the phrase - document relevance by   the hyperbolic linear transformation layer ( Ganea   et al . , 2018 ) . The results of HyperMatch w/o Rel-   evance show a drop in all evaluation metrics , in-   dicating that explicitly considering the relevance   between phrases and the document is essential for   estimating the importance of candidate phrases in   the keyphrase extraction task .   5.3 Sensitivity of Hyperparameters   In this section , we verify the sensitivity of Hyper-   Match with different margins ( δ ) of the hyperbolic   margin - based triplet loss . For keyphrase extraction   methods equipped with the margin - based triplet   loss , the margin design signiﬁcantly impacts the   ﬁnal result , where a poor margin usually causes   performance degradation . Therefore , we verify the   effects of different margins on HyperMatch in Fig-   ure 3 . We can see that HyperMatch achieves the   best results when δ= 1 .   6 Related Work   This section brieﬂy describes the related work from   two ﬁelds : keyphrase extraction and hyperbolic   deep learning .   6.1 Keyphrase Extraction   Most existing KE models are based on the two-   stage extraction framework , which consists of two   main procedures : candidate keyphrase extraction   and keyphrase importance estimation . Candidate   keyphrase extraction extracts a set of candidate   phrases from the document by some heuristics   ( e.g. , essential n - gram - based phrases ( Hulth , 2004 ;   Medelyan et al . , 2009 ; Xiong et al . , 2019 ; Sun et al . ,   2020 ; Wang et al . , 2020 ) ) . Keyphrase importance   estimation ﬁrst represents candidate phrases and   documents by the pre - trained language models ( De-   vlin et al . , 2019 ; Liu et al . , 2019 ) and then estimates   the phrase - document relevance implicitly as the im-   portance scores . Finally , the candidate phrases are   ranked by their importance scores , which can be5717learned by either unsupervised ( Mihalcea and Ta-   rau , 2004 ; Liu et al . , 2009c ) or supervised ( Xiong   et al . , 2019 ; Sun et al . , 2020 ; Mu et al . , 2020 ) rank-   ing approaches .   Different from the existing KE models , we map   phrases and documents representations from the   euclidean space to the same hyperbolic space to   capture the latent hierarchical structures . Next , we   adopt the Poincaré distance to explicitly model the   phrase - document relevance as the important score   of each candidate phrase . Finally , the hyperbolic   margin - based triplet loss is used to optimize the   whole model . To the best of our knowledge , we   are the ﬁrst study to explore supervised keyphrase   extraction in hyperbolic space .   6.2 Hyperbolic Deep Learning   Recent studies on representation learning ( Nickel   and Kiela , 2017 ; Tifrea et al . , 2019 ; Mathieu et al . ,   2019 ) demonstrate that hyperbolic space is more   suitable for embedding symbolic data with hierar-   chies than the Euclidean space since the tree - like   properties ( Hamann , 2018 ) of the hyperbolic space   make it efﬁcient to learn hierarchical representa-   tions with low distortion ( Sa et al . , 2018 ; Sarkar ,   2011 ) . As linguistic ontologies are innately hier-   archies , hierarchies are ubiquitous in natural lan-   guage ( Dai et al . , 2021 ) . Some recent studies show   the superiority of hyperbolic space for many natural   language processing tasks ( Gulcehre et al . , 2019 ;   Zhu et al . , 2020 ) . Chen et al . ( 2021 ) demonstrate   that mapping contextualized word embeddings ( i.e. ,   BERT - based embeddings ) to the hyperbolic space   can capture richer hierarchical structure informa-   tion than the euclidean space when encoding natu-   ral language text . Inspired by the above methods ,   we transfer the embeddings obtained by the pre-   trained language models to hyperbolic space for   extracting keyphrases .   7 Conclusions and Future Work   A new hyperbolic relevance matching model Hy-   perMatch is proposed to map candidate phrases   and documents representations into the hyperbolic   space and model the relevance between candidate   phrases and the document via the Poincaré dis-   tance . Speciﬁcally , HyperMatch ﬁrst combines the   intermediate layers of RoBERTa via the adaptive   mixing layer for capturing richer syntactic and se-   mantic information . Then , phrases and documents   are encoded in the same hyperbolic space to cap - ture the latent hierarchical structures . Next , the   phrase - document relevance is estimated explicitly   via the Poincaré distance as the importance scores   of all the candidate keyphrases . Finally , we adopt   the hyperbolic margin - based triplet loss to optimize   the whole model for extracting keyphrases .   In this paper , we explore keyphrase extraction   in hyperbolic space and implicitly model the la-   tent hierarchical structures hidden in natural lan-   guages when representing candidate keyphrases   and documents . In the future , it will be interesting   to introduce external knowledge ( e.g. , WordNet ) to   explicitly model the latent hierarchical structures   when representing candidate keyphrases and docu-   ments . In addition , our code is publicly available   to facilitate other research .   8 Acknowledgments   This work was supported in part by the Na-   tional Key Research and Development Program   of China under Grant 2020AAA0106800 ; the Na-   tional Science Foundation of China under Grant   61822601 and 61773050 ; the Beijing Natural Sci-   ence Foundation under Grant Z180006 ; the Funda-   mental Research Funds for the Central Universities   ( 2019JBZ110 ) .   References571857195720