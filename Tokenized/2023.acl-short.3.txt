  Young Min Kimand Kalvin Changand Chenxuan Cui and David Mortensen   Language Technologies Institute , Carnegie Mellon University   { youngmik , kalvinc , cxcui , dmortens}@cs.cmu.edu   Abstract   Protoform reconstruction is the task of infer-   ring how morphemes or words sounded in   ancestral languages of a set of daughter lan-   guages . Meloni et al . ( 2021 ) achieved the state-   of - the - art on Latin protoform reconstruction   with an RNN - based encoder - decoder with at-   tention model . We update their model with   the state - of - the - art seq2seq model — the Trans-   former . Our model outperforms their model   on a suite of different metrics on two differ-   ent datasets : Meloni et al . ’s Romance data of   8,000 + cognates ( spanning 5 languages ) and a   Chinese dataset ( Hóu,2004 ) of 800 + cognates   ( spanning 39 varieties ) . We also probe our   model for potential phylogenetic signal con-   tained in the model . Our code is publicly avail-   able .   1 Introduction   Languages change over time and sometimes di-   verge into multiple daughter languages . The com-   mon ancestor of a set of genetically related lan-   guages is their proto - language . While there are   proto - languages such as Latin that are attested ,   they are the exception . Reconstructed words and   morphemes in proto - languages are called proto-   forms . The task of reconstructing unattested proto-   languages is called protoform reconstruction .   Historical linguists reconstruct proto - languages   by identifying systematic sound changes that can   be inferred from correspondences between attested   daughter languages ( see Table 1 ) . They com-   pare the sounds between a set of cognates , or   words with a common ancestor , to develop hy-   potheses about the types and chronologies of sound   changes . ‘tooth ’ ‘ two ’ ‘ ten ’   English tooth two ten t   Dutch tand twee tien t   German Zahn zwei zehn z   PWG * tanþ * twai- * tehun * t   Table 1 : Sound correspondences in West Germanic Lan-   guages and Proto - West - Germanic ( PWG ) .   This task is inherently data - constrained , espe-   cially for under - documented languages . Such data   scarcity makes it a particularly difficult task for   contemporary neural network architectures such as   the Transformer ( Vaswani et al . , 2017 ) , which are   data hungry .   The contributions of this paper are as follows :   •Application of the Transformer architecture   to the protoform reconstruction task , achiev-   ing state of the art performance , contrary to   expectation .   •Expansion of prior digital versions of Hóu   ( 2004 ) ’s Chinese dataset to include a total of   804 cognate sets across 39 modern varieties   and Middle Chinese .   2 Related Work   Applying machine learning to protoform recon-   struction is not new . Bouchard - Côté et al . ( 2013 )   learn an unsupervised protoform reconstruction   model for the large Oceanic language family using   Monte Carlo Expectation Maximization ( Demp-   ster et al . , 1977 ;Bouchard - Côté et al . , 2008 ) , super-   vising the model with a gold phylogeny and using   a probabilistic , generative model of sound change .   He et al . ( 2022 ) modernize an earlier version of   Bouchard - Côté et al . ( 2013 ) ’s model with RNNs   for a 4 language subset of Romance , but they rely   on a bigram language model of Latin , making their   model technically not unsupervised.24List et al . ( 2022 ) apply an SVM classifier to   supervised reconstruction by treating sound corre-   spondences as training examples . Note that there   were no word boundaries in the input matrix; that   is , all sound correspondences across the training   set are flattened into one matrix . Furthermore ,   each language has an independent phonemic inven-   tory . To learn contextual information , the authors   experiment with adding features encoding the po-   sition of phonemes , among others .   Ciobanu and Dinu ( 2018 ) learn a conditional   random field ( Lafferty et al . , 2001 ) using n - gram   features for supervised reconstruction and ensem-   ble 5 daughter - to - protoform models . They use a   dataset of 3,218 complete cognate sets spanning   Latin ( the proto - language ) and 5 Romance lan-   guages : Romanian , French , Italian , Spanish , Por-   tuguese .   Meloni et al . ( 2021 ) employ a GRU - based   seq2seq approach ( Cho et al . , 2014 ) to Latin   protoform reconstruction and achieve state - of - the-   art character edit distances . They extend Dinu   and Ciobanu ( 2014 ) ’s Romance data using data   from Wiktionary — for a total of 8,799 cognate sets   across 5 Romance languages plus Latin — in both   orthographic and phonetic ( IPA ) representations .   In their model , all entries comprising the cognate   set are concatenated together in a fixed order to   form a training example . Chang et al . ( 2022 ) ap-   plied Meloni et al . ( 2021 ) ’s architecture to the re-   construction of Middle Chinese on a dataset of   5000 + cognate sets spanning 8 languages they   compiled from Wiktionary .   Fourrier ( 2022 ) compares statistical machine   translation , RNN , and Transformer architectures   for protoform reconstruction , but they evaluate   their results using BLEU scores ( Papineni et al . ,   2002 ) instead of edit distance . They find that their   Transformer model did not outperform the RNN   models on protoform reconstruction . In addition ,   their multilingual NMT ( neural machine transla-   tion ) model predicts many languages instead of   one target language and is trained on bilingual   pairs for protoform reconstruction ( e.g. Italian-   Latin and Spanish - Latin ) , unlike comparative re-   construction . In contrast , we encode the entire cog-   nate set consisting of multiple daughter languages   ( 5 for the Romance dataset; 39 for Chinese ) and   predict the corresponding protoform.3 Datasets   We train and test our model on Romance and   Sinitic ( Chinese ) language datasets . For Romance   languages , we use Meloni et al . ( 2021 ) ’s dataset   which consists of 8,799 cognate sets of Romanian ,   French , Italian , Spanish , Portuguese words and   the corresponding Latin form ( approximately , a   protoform ) . There are two versions of this dataset :   phonetic and orthographic . The phonetic dataset   ( Rom - phon ) represents words with IPA symbols   whereas the orthographic dataset ( Rom - orth ) rep-   resents words in the orthographic form of each   language . We preserved all diacritics , except for   vowel length . This dataset is an extension of Dinu   and Ciobanu ( 2014 ) ’s original dataset of 3,218 cog-   nate sets , which is not publicly available . Refer to   Table 2 for more information .   3.1 Expanding digital versions of Hóu ( 2004 )   For Sinitic languages , we created a dataset of Mid-   dle Chinese and its modern daughter languages .   Middle Chinese is an unattested language , and we   thus have to rely on Baxter and Sagart ( 2014 ) ’s   reconstructions of forms corresponding to 4,967   Chinese characters . We scraped Wiktionary to ob-   tainHóu ( 2004 ) ’s phonetic representations of their   modern reflexes . The resulting dataset contains   804 cognate sets of 39 modern Sinitic languages   and the corresponding reconstructed Middle Chi-   nese word . List ( 2021 ) ’s version previously had   894 cognate sets across 15 varieties .   4 Model   We propose a Transformer - based encoder - decoder   architecture ( Vaswani et al . , 2017 ) because such   models have produced state - of - the - art results on   many sequence processing tasks . Transformers are   by reputation data hungry , though , which poses a   challenge to our problem setting , where the num-   ber of available training examples is often very   small.25   We modify the standard encoder - decoder ar-   chitecture to accommodate the structure of our   datasets , where multiple daughter sequences corre-   spond to a single protoform sequence . Like Meloni   et al . ( 2021 ) , the daughter sequences are concate-   nated into a single sequence before being fed into   the encoder . Because we only care about the rela-   tive position between tokens within each daughter   sequence but not across daughter sequences , posi-   tional encoding is applied to each individual daugh-   ter sequence before concatenation . Along with po-   sitional encoding , an additive language embedding   is applied to the token embeddings to differenti-   ate between input tokens of different daughter lan-   guages .   5 Experiments   5.1 Baselines   We compare our Transformer model to a variety   of baselines . For Meloni et al . ( 2021 ) , we use   Chang et al . ( 2022 ) ’s PyTorch re - implementation   and reran a Bayesian hyperparameter search us-   ing WandB ( Biewald , 2020 ) to ensure a more fair   comparison ( since our model is tuned with WandB   as well ) . We also include the random daughter   ( randomly designate a daughter form as the proto-   form and assume no sound change ) and the major-   ity constituent baselines ( predict the most common   phoneme in each syllable constituent ) from Chang   et al . ( 2022 ) . For the SVM and CoRPaR classi-   fiers ( List et al . , 2022 ) , we experiment with dif-   ferent contextual features , such as Pos ( position ) ,   Str ( prosodic structure ) , and Ini ( whether or not the   phoneme appears word - initially or word - finally).We publish results on Meloni et al . ( 2021 ) ’s   full set of 8,799 cognates but can not redistribute   this set due to Dinu and Ciobanu ( 2014 ) ’s restric-   tions . For reproducibility , we include results on   Meloni et al . ( 2021 ) ’s public subset of 5,419 cog-   nates in the Appendix ( Table 7 ) , both of which in-   clude vowel length . Observe that these results are   worse than those obtained on the full set , suggest-   ing that the RNN and Transformer are dependent   on a wealth of training data .   5.2 Preprocessing   In all our datasets , we merge diacritics to their base   segments to form a multi - character token . For in-   stance , the sequence [ t , ʰ ] is concatenated to [ tʰ ] .   This ensures that phonemes are treated as one to-   ken . For Chinese , tone contours ( a sequence of   tones ) are treated as one token . When multiple pro-   nunciation variants are listed for a single Chinese   character , we arbitrarily pick the first one .   6 Results and Discussion   6.1 Evaluation criteria   We evaluate the predicted protoforms using edit   distance ( Levenshtein et al . , 1966 ) , normalized   edit distance ( edit distance normalized by the   length of the target ) and accuracy ( the percent-   age of protoforms that are reconstructed without   any mistakes ) . Like Chang et al . ( 2022 ) , we also   use feature error rate calculated using articulatory   feature vectors from PanPhon ( Mortensen et al . ,   2016 ) because it reflects the phonetic similarity be-   tween the prediction and the gold protoform . For   datasets with phonetic transcriptions ( Romance-   phonetic and Chinese ) , we use phoneme edit dis-   tance and normalized phoneme edit distance . As   List ( 2019 ) suggests , we use B - Cubed F Scores   ( Amigó et al . , 2009 ) to capture the structural sim-   ilarity between the gold and predicted protoforms   ( 0 : structurally dissimilar , 1 : similar ) . With the   exception of character and phoneme edit distance ,   the metrics enable fair comparison across different   language families , which will differ in the average   word length .   6.2 Results   Table 3 shows that our model consistently has the   best performance on all datasets with regards to   most metrics . The results were averaged across 5   runs . Out of all datasets , our model performs best   on the Rom - orth dataset , where we achieve a 7.0%26Language Family Source # varieties Cognate sets Proto - language   Rom - phon Dinu and Ciobanu ( 2014 ) , 5 8,799 Latin   Meloni et al . ( 2021 )   Rom - orth Dinu and Ciobanu ( 2014 ) , 5 8,799 Latin   Meloni et al . ( 2021 )   Sinitic ( Chinese ) Hóu ( 2004 ) 39 804 Middle Chinese   decrease in phoneme edit distance and a 1.43p.p   improvement in accuracy relative to the RNN base-   line . We observe the most dramatic performance   difference with the RNN baseline on the Sinitic   dataset : a 10.48 % decrease in phoneme edit dis-   tance and a 5.47p.p increase in accuracy . For re-   producibility , results on the publicly available por-   tion of the Rom - phon and Rom - orth datasets are   provided in Table 7 in the Appendix .   6.3 Analysis   We observe that the BCFS is relatively high for the   Romance non - neural baselines compared to those   of the Chinese ones . This suggests that the sound   changes in the Romance datasets are more regular   than that of Chinese , which corroborates List et al .   ( 2014 ) ’s results that more than half of the Chinese   characters in their dataset could not be explained   by a tree model .   We examine the errors made by the Transformer   model on the Rom - phon datasest . Substitutions   constitute around 61 % of the errors made by the   Transformer; deletions , 21 % , and insertions , 18 % .   The highest number of substitution errors occur be-   tween [ i , ɪ ] , [ e , ɛ ] , [ o , ɔ ] and [ u , ʊ]—vowel pairs   that contrast only in tenseness . This is consistent   with the analysis of Meloni et al . ( 2021 ) , where   substitutions between tense - lax vowel pairs take   up the largest portion of errors .   We observe that other common substitution er-   rors also happen between phonemes that share ma-   jor phonetic features . This demonstrates that al - though no explicit phonetic information is fed di-   rectly into the model , the model makes mistakes   motivated by phonetic similarity , like Meloni et al .   ( 2021 ) .   We do not observe notable differences in the   error statistics between the Transformer and the   RNN .   6.4 Language relatedness   Inspired by Fourrier ( 2022 ) , we probe our model   for diachronic information on how genetically re-   lated each Romance language is to each other . We   create a distance matrix between every pair of lan-   guages in a dataset by taking the cosine similar-   ity between a pair ’s language embeddings . We   then use sklearn ( Pedregosa et al . , 2011 ) ’s imple-   mentation of the Ward variance minimization algo-   rithm ( Ward Jr , 1963 ) to perform hierarchical clus-   tering on the distance matrix . We take a consen-   sus of the dendrograms from 5 different runs us-   ing the consense program from PHYLIP ( Felsen-   stein , 2013 ) .   As we see in Figure 2 , the Transformer captures   more of the phylogenetic relationships among the   languages correctly for the Rom - phon dataset . In-   deed , the Generalized Quartet Distance ( GQD )   ( Sand et al . , 2013 ;Pompei et al . , 2011 ;Rama et al . ,   2018 ) between the gold and predicted tree , calcu-   lated using quartetDist from the tqDist library   ( Sand et al . , 2014 ) , is 0.4 for the Transformer but   0.8 for the RNN . See Figure 5 in the Appendix for   the results of the orthographic dataset.27Dataset Model PED ↓ NPED ↓ Acc % ↑ FER ↓ BCFS ↑   Sinitic Random daughter ( Chang et al . ,   2022 ) 3.7702 0.8405 0 % 0.2893 0.2748   Majority constituent ( Chang et al . ,   2022 ) 3.5031 0.7806 0 % 0.2013 0.3695   CorPaR ( List et al . , 2022 ) 3.2795 0.7278 0 % 0.3972 0.3332   SVM + PosStr ( List et al . , 2022 ) 1.6894 0.3692 15.52 % 0.1669 0.5418   RNN ( Meloni et al . , 2021 ) 1.0671 0.2421 35.65 % 0.0899 0.6781   Transformer ( present work ) 0.9553 0.2150 41.12 % 0.0842 0.7033   Rom - phon Random daughter ( Chang et al . ,   2022 ) 6.1534 0.6914 0.06 % 0.6264 0.4016   CorPaR + PosIni ( List et al . , 2022 ) 1.6847 0.1978 22.18 % 0.0728 0.7403   SVM + PosStrIni ( List et al . , 2022 ) 1.5787 0.1861 24.69 % 0.0713 0.7610   RNN ( Meloni et al . , 2021 ) 0.9655 0.1224 52.31 % 0.0384 0.8296   Transformer ( present work ) 0.8926 0.1137 53.75 % 0.0373 0.8435   Rom - orth Random daughter ( Chang et al . ,   2022 ) 4.2567 0.4854 2.97 % - 0.5147   CorPaR + Ini ( List et al . , 2022 ) 0.9531 0.1160 47.23 % - 0.8400   SVM + PosStr ( List et al . , 2022 ) 0.8988 0.1105 50.43 % - 0.8501   RNN ( Meloni et al . , 2021 ) 0.5941 0.0770 69.80 % - 0.8916   Transformer ( present work ) 0.5525 0.0720 71.23 % - 0.9002   Since the Romance dataset only includes 5   daughter languages , our results are insufficient   to corroborate or contradict Cathcart and Wandl   ( 2020 ) ’s findings : the more accurate the proto-   forms , the less accurate the phylogeny will be . It   is not clear if the model ’s language embeddings   are learning information that reflects shared inno-   vations ( sound changes that if shared among a set   of daughter languages , would be acceptable justi-   fication for grouping them)—the only acceptable   criterion for phylogenetic inference in historical   linguistics ( Campbell , 2013 ) — or if the model is   learning superficial phonetic similarity .   7 Conclusion   By showing that Transformers can outperform pre-   vious architectures in protoform reconstruction de-   spite the inherent data scarcity of the task , our work   motivates future research in this area to take full   advantage of the recent advancements in the Trans-   former space .   Accurate supervised reconstruction can help pre - dict protoforms for cognate sets where linguists   have not reconstructed one yet . Future work could   reconstruct proto - languages whose linguist recon-   structions are not available , by transferring knowl-   edge learned from languages with already recon-   structed protoforms . Furthermore , future work can   leverage the abundance of work in unsupervised   NMT to adapt our Transformer model for the un-   supervised setting , a more realistic scenario for the   historical linguist .   Limitations   One limitation of our work is that the RNN ( Meloni   et al . , 2021 ) actually outperforms our Transformer   on the Chinese dataset in Chang et al . ( 2022 ) . In ad-   dition , as with other neural approaches , our model   requires significant amounts of data , which is of-   ten not available to historical linguists research-   ing less well - studied language families based on   field reports . Romance and Chinese have rela-   tively many cognate sets because the protoforms28are documented , but a low resource setup with   200 cognate sets would not fare well on our data-   hungrier Transformer model . Furthermore , con-   catenating the entire cognate set may not work   on language families with hundreds of languages   such as Oceanic because the input sequence would   be too long compared to the output protoform se-   quence .   Finally , we obtain our Chinese gold protoforms   from Baxter and Sagart ( 2014 ) ’s Middle Chinese   reconstruction , which was actually a transcription   of the Qieyun , a rhyme dictionary . Norman and   Coblin ( 1995 ) disagree with relying on such a   philological source and prefer comparative recon-   structions that begin from daughter data . However ,   there is no available comparative reconstruction   of Middle Chinese with protoforms corresponding   to thousands of characters to use as a gold stan-   dard . Be that as it may , it seems clear that Mid-   dle Chinese as recorded in the Qieyun is not identi-   cal to the most recent ancestor of the Chinese lan-   guages . Its preface concedes that it is a compro-   mise between Tang Dynasty dialects . The situa-   tion with Romance is , in some ways , comparable .   Classical Latin — the variety on which we train —   is not the direct ancestor of modern Romance lan-   guages . Instead , they are descended from Vulgar   Latin or Proto - Romance , which is not well - attested   and is primarily through graffiti and other informal   inscriptions . Proto - Romance reconstructions are   also not exhaustive . As a result , it is difficult to   find a dataset like Meloni et al . ( 2021 ) with thou-   sands of such ancestor forms . We are also limited   to the faithfulness of espeak - ng ’s Latin G2P , from   which Meloni et al . ( 2021 ) obtain their phonetic   Romance dataset .   For most language families , protoforms are not   attested . In fact , as the term is often used , proto-   form refers to a form that is inferred only through   linguists ’ comparative method . We adopt the other   usage for simplicity . In practice , our approach   would require reconstructions made by a linguist   to serve as training labels for cognate sets .   Acknowledgements   We would like to thank Liang ( Leon ) Lu for finding   a bug in our implementation , Ying Chen for writ-   ing the code for the baselines , and Brendon Boldt   and Graham Neubig for providing useful feedbackfor the first iteration of our paper .   References2930A Training   We split 70 % , 10 % , and 20 % of our dataset into   train , validation , and test sets , respectively . We   conduct hyperparameter searches using WandB   ( Biewald , 2020 ) and use early stopping , picking   the epoch with lowest edit distance on validation   data . All experiments are performed on a Ubuntu   server with 4 GPUs and 20 CPUs . For both the   RNN and the Transformer , Meloni et al . ( 2021 ) ’s   dataset takes less than 7 GPU hours to run , while   Hóu ( 2004 ) takes less than 1 GPU hour . For   the large Romance orthographic dataset , the RNN   model has around 480,000 parameters , while the   Transformer has around 800,000 parameters .   B Hyper - parameters   Refer to Table 5 andTable 6 for the best hyperpa-   rameters we found during hyperparameter search   via WandB.   C Supplementary Results   In order to compare our model to earlier work , we   used the Rom - phon and Rom - orth datasets from   Meloni et al . ( 2021 ) . However , this set includes a   subset from Ciobanu and Dinu ( 2018 ) which is not   freely redistributable . So that our results can be re-   produced , we also computed them on the publicly   available subset of Meloni et al . ( 2021 ) ’s dataset ,   which is presented in Table 7 .   Phylogenetic trees for Chinese were also ex-   tracted from the RNN and Transformer models .   These are shown in Figures 3and4 .   We also plot the dendrograms derived from the   Rom - orto dataset in Figure 5 .313233Dataset Model PED ↓ NPED ↓ Acc % ↑ FER ↓ BCFS ↑   Sinitic Random   daughter3.7702 0.8405 0 % 0.2893 0.2748   Majority   constituent3.5031 0.7806 0 % 0.2013 0.3695   CorPaR 3.2795 0.7278 0 % 0.3972 0.3332   SVM   + PosStr1.6894 0.3692 15.52 % 0.1669 0.5418   RNN 1.0671 ±   0.06190.2421 ±   0.014035.65 % ±   1.60%0.0899 ±   0.00480.6781 ±   0.0174   Transformer   ( present   work)0.9553 ±   0.03920.2150 ±   0.007541.12 % ±   2.3%0.0842 ±   0.00700.7033 ±   0.0087   Rom - phon Random   daughter6.1534 0.6914 0.06 % 0.6264 0.4016   CorPaR   + PosIni1.6847 0.1978 22.18 % 0.0728 0.7403   SVM   + PosStrIni1.5787 0.1861 24.69 % 0.0713 0.7610   RNN 0.9655 ±   0.01890.1224 ±   0.002252.31 % ±   0.63%0.0384 ±   0.00110.8296 ±   0.0029   Transformer   ( present   work)0.8926 ±   0.01660.1137 ±   0.001753.75 % ±   0.40%0.0373 ±   0.00090.8435 ±   0.0026   Rom - orth Random   daughter4.2567 0.4854 2.97 % - 0.5147   CorPaR   + Ini0.9531 0.1160 47.23 % - 0.8400   SVM   + PosStr0.8988 0.1105 50.43 % - 0.8501   RNN 0.5941 ±   0.01000.0770 ±   0.001569.80 %   ±0.22%- 0.8916 ±   0.0019   Transformer   ( present   work)0.5525 ±   0.01040.0720 ±   0.001771.23 % ±   0.52%- 0.9002 ±   0.001734Romance ( phon & orth ) Sinitic   learning rate 0.00013 0.0007487   num_encoder_layers 3 2   num_decoder_layers 3 5   embedding size 128 128   n_head 8 8   dim_feedforward 128 647   dropout 0.202 0.1708861   training epochs 200 200   warmup epochs 50 32   weight decay 0 0.0000001   batch size 1 32   Romance - phon Romance - orth Sinitic   learning rate 0.00055739 0.000964 0.000864   num_encoder_layers 1 1 1   num_decoder_layers 1 1 1   embedding size 107 51 78   hidden size 185 130 73   dim_feedforward 147 111 136   dropout 0.1808 0.323794 0.321639   training epochs 181 193 237   warmup epochs 15 15 15   batch size 8 8 435Dataset Model PED ↓ NPED ↓ Acc % ↑ FER ↓ BCFS ↑   Rom - phon Random daughter ( Chang et al . ,   2022 ) 7.1880 0.8201 0 % 1.1396 0.3406   CorPaR + Ini ( List et al . , 2022 ) 2.0885 0.2491 14.29 % 0.0874 0.6799   SVM + PosStrIni ( List et al . , 2022 ) 1.9005 0.2276 17.05 % 0.0883 0.7039   RNN ( Meloni et al . , 2021 ) 1.4581 0.1815 36.68 % 0.0592 0.7435   Transformer ( present work ) 1.2516 0.1573 41.38 % 0.0550 0.7790   Rom - orth Random daughter ( Chang et al . ,   2022 ) 6.3272 0.6542 0.55 % - 0.4023   CorPaR + PosStrIni ( List et al . ,   2022 ) 1.8313 0.2001 18.89 % - 0.7227   SVM + PosStr ( List et al . , 2022 ) 1.6995 0.1867 21.66 % - 0.7454   RNN ( Meloni et al . , 2021 ) 1.3189 0.1505 38.89 % - 0.7742   Transformer ( present work ) 1.1622 0.1343 45.53 % - 0.7989   Latin Romanian French Italian Spanish Portuguese   [ kɔlleːktɪoːnɛm ] [ kolektsie ] [ kɔlɛksjɔ̃ ] [ kolletsione ] [ kolekθjon ] [ kulɨsɐ̃ʊ̃]36ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 8   /squareA2 . Did you discuss any potential risks of your work ?   Section 8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Not applicable . Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Sections 3 , 4   /squareB1 . Did you cite the creators of artifacts you used ?   Sections 3,4,5,6   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Sections 3 , 5.1   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Table 2 and Appendix Section A   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A37 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Hyperparameter search : 5.1 Hyperparameter values : Appendix Section B   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Table 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   5.1 , 6.1 , 6.3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.38