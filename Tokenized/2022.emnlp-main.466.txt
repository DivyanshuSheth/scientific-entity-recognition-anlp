  Yong WangShilin HeGuanhua ChenYun ChenDaxin JiangTencent CorporationMicrosoft CorporationSouthern University of Science and TechnologyShanghai University of Finance and Economics   seaywang@gmail.com shilin.he@microsoft.com   ghchen08@gmail.com yunchen@sufe.edu.cn djiang@microsoft.com   Abstract   Pre - training language models have achieved   thriving success in numerous natural language   understanding and autoregressive generation   tasks , but non - autoregressive generation in   applications such as machine translation has   not sufﬁciently beneﬁted from the pre - training   paradigm . In this work , we establish the   connection between a pre - trained masked lan-   guage model ( MLM ) and non - autoregressive   generation on machine translation . From this   perspective , we present XLM - D , which seam-   lessly transforms an off - the - shelf cross - lingual   pre - training model into a non - autoregressive   translation ( NAT ) model with a lightweight   yet effective decorator . Speciﬁcally , the dec-   orator ensures the representation consistency   of the pre - trained model and brings only one   additional trainable parameter . Extensive ex-   periments on typical translation datasets show   that our models obtain state - of - the - art perfor-   mance while realizing the inference speed-   up by 19.9×. One striking result is that on   WMT14 En⇒De , our XLM - D obtains 29.80   BLEU points with multiple iterations , which   outperforms the previous mask - predict model   by2.77points .   1 Introduction   As a dominant pre - training paradigm in natural lan-   guage processing ( NLP ) , masked language models   such as BERT and its variants ( Devlin et al . , 2019 ;   Liu et al . , 2019 ) , were initially proposed and have   achieved state - of - the - art performance on various   natural language understanding tasks . For gener-   ation tasks , previous studies either leverage the   pre - trained BERT as an external component for   representation fusing ( Zhu et al . , 2019 ) or simply   initialize the generation models with a pre - trained   model ( Ma et al . , 2020 ) . Although straightforward ,   these methods suffer from either heavy compu-   tation costs during inference or only supportingautoregressive generation with sequential depen-   dency . Additionally , these approaches result in   inconsistency from two perspectives : 1 ) mismatch   of architectures between pre - trained models and   the generation model ; 2 ) disagreement of training   objectives between pre - training and autoregressive   generation tasks .   Different from previous studies , we reexamine   the problem from another point of view . We ﬁnd   that the training objective in pre - trained MLM can   align well with the one in non - autoregressive gen-   eration , which is an emergent generation paradigm   due to its excellent inference speed - up ( Gu et al . ,   2018 ; Lee et al . , 2018 ) . Intuitively , both train-   ing objectives are formalized as a series of inde-   pendent token predictions in the output sequence .   Based on this observation , we incorporate a pre-   trained MLM into the non - autoregressive genera-   tion . Speciﬁcally , in this work , we focus on the   classic non - autoregressive machine translation task .   Albeit studied for ages , NAT models have not per-   formed very well , lagging behind the autoregres-   sive translation counterpart .   We propose to adapt the cross - lingual pre-   training model ( XLMR ) into NAT models with   a lightweight , effective and user - conﬁgurable dec-   orator . Following Occam ’s razor principle and to   better leverage the capability of pre - training mod-   els , we design the decorator component based on   two key criteria : 1 ) involve additional trainable pa-   rameters as few as possible , e.g. , parameter - free ; 2 )   keep the model intermediate representation consis-   tent after using the decorator . Guided by the two   criteria , the decorator consists of a distance - based   latent transformation module and a position - wise   add and scale module , which contains only one   trainable scalar parameter . Moreover , the decorator   can be ﬂexibly incorporated into a user - speciﬁed   layer of XLMR to balance the translation perfor-   mance and inference speed . We use the connec-   tionist temporal classiﬁcation ( CTC ) ( Graves et al . ,69342006 ) loss as the training objective .   To validate the effectiveness of our approach ,   we systematically conduct the evaluation on sev-   eral widely - used translation datasets . Extensive   experiments demonstrate that our proposed model   signiﬁcantly and consistently improves the trans-   lation performance and achieves new state - of - the-   art results in both single - step and iterative NAT   models . Results show that our single - step XLM-   D model achieves 27.46/34.70BLEU points on   WMT14 En⇒De and WMT16 En ⇒Ro transla-   tion tasks with 19.9×speed - up . Encouragingly ,   our iterative XLM - D model obtains 29.80/35.65   points on both tasks and outperforms previous well-   performed models ( CMLM ) ( Ghazvininejad et al . ,   2019 ) by a large margin , i.e. , 2.77/2.57points . Fur-   ther analyses reveal that our approach enhances the   capability in long sentence translation and can be   user - conﬁgured to balance the trade - off between   the translation quality and inference speed .   2 Approach   2.1 Problem Formulation   A common part of the masked language model   ( MLM ) and non - autoregressive generation is that   the prediction of each output token is made inde-   pendently . In this section , we compare the train-   ing objective in pre - training MLM with the one   in non - autoregressive generation and identify their   inherent yet unnoticed connections .   Masked Language Model Given an input sen-   tence x={x, ... ,x } , a masked language model   randomly masks out this sequence by replacing   words with a special token [ MASK ] . Conditioned   on the manipulated sequence , the model predicts   the masked tokens in parallel . Formally , we de-   notex as a set of output tokens and x=   x\x as the input sentence . The minimized   training objective can be formulated as :   L(θ ) = −/summationdisplaylogP(x|x;θ ) ( 1 )   = −/summationdisplay / summationdisplay1(x=[MASK ] ) logP(x|x;θ ) ,   whereNis the number of training examples and θ   is a set of trainable parameters in pre - training mod-   els . The training objective has been successfully   applied to the state - of - the - art pre - training BERT   model and its variants ( Devlin et al . , 2019 ; Liuet al . , 2019 ) . In particular , Conneau et al . ( 2020 )   extends this to monolingual data with one hundred   languages for cross - lingual understanding , which   attributes pre - training MLM models with cross-   lingual properties of word , syntax and semantics .   Non - Autoregressive Machine Translation Re-   cently , non - autoregressive models , which predict   the target sentence in parallel conditioned on the in-   put sentence , have obtained considerable attention   in machine translation . Speciﬁcally , given an input   sentence x={x, ... ,x}and an output sentence   y={y, ... ,y } , a standard non - autoregressive   framework ( Gu et al . , 2018 ) breaks the probabilistic   factorization with a product of independent proba-   bility for each output token :   L(θ ) = −/summationdisplaylogP(y|x;θ )   = −/summationdisplay / summationdisplaylogP(y|x;θ ) , ( 2 )   whereθis a set of trainable parameters . Typically ,   NAT models have an auxiliary length predictor ,   which is used to determine the translation length J.   From equation ( 1)and ( 2 ) , we observe that   although the output sequences are different in   MLM and NAT models , the output tokens are   independently predicted . In essence , we ﬁnd   that a masked language model could be endowed   with non - autoregressive generation inherently . To   utilize available pre - training models effectively ,   we propose to decorate the existing cross - lingual   pre - training model ( XLMR ) as non - autoregressive   machine translation . Speciﬁcally , we present a   lightweight and effective approach by reducing the   incorporation of additional learnable parameters   and maintaining the consistency of representations   in pre - training models .   2.2 XLM - D   In this section , we introduce the proposed approach   of decorating XLMR as non - autoregressive ma-   chine translation , termed as XLM - D , which is   shown in Figure 1 . First of all , we use the latent   alignment loss as the training objective . Mean-   while , we augment the standard XLMR model   with a distance - based latent transformation mod-   ule , which transforms the hidden states of a source   sentence into the representations with extended   length . Besides , we incorporate a position - wise6935   add and scale component into our model to equip   the presented approach with iterative reﬁnement .   Latent Alignment as Training Objective In   this work , we train the model with the connec-   tionist temporal classiﬁcation ( CTC ) ( Graves et al . ,   2006 ; Chan et al . , 2020 ) loss , a.k.a . latent align-   ment . The beneﬁts are twofold . Firstly , utilizing   latent alignment is effective in dealing with the to-   ken repetition problem ( Libovick ` y and Helcl , 2018 ;   Ghazvininejad et al . , 2020 ; Saharia et al . , 2020 ) in   the output sentence . In addition , CTC loss does not   need the prediction of target length . Speciﬁcally ,   the predicted output has the length s×I , where s   is the predeﬁned factor and Iis the source length .   Formally , let adenote an aligned sequence , we de-   ﬁne the collapsing function Γ(a)as collapsing   consecutive repeated tokens and removing blank   tokens . Given the conditional independence as-   sumption , CTC models the alignment distribution   and marginalizes the following log - likelihood :   logP(y|x;θ ) = log / summationdisplayP(a|x;θ).(3 )   Distance - based Latent Transformation Sup-   pose the source sentence of length Ihas hidden   states h={h , ... , h}and the predicted   output is of length s×I. To bridge the length gap ,   we transform the hidden states into s×Ivectors .   Inspired from the attention mechanism ( Bahdanau   et al . , 2015 ) , we employ a monotonic distance-   based attention ( Shu et al . , 2020 ) to produce thetransformed latent variables . We denote the result-   ing vectors of latent transformation as z=   { z, ... ,z } . Each output vector is calculated   with a weighted sum of the hidden states h :   z=/summationdisplaywh , w = exp(α)/summationtextexp(α ) ,   α=−1   2σ(i−I   s×Ij ) ,   where the weight is calculated with a softmax func-   tion over a set of relative distances to the source   sentence . In this distance - based attention mecha-   nism , the variance σis the only trainable parameter .   Position - Wise Add and Scale To support iter-   ative reﬁnement , we introduce the conditional   masked strategy ( Ghazvininejad et al . , 2019 ) into   our model , which selects a subset of tokens to   mask and then predicts them in parallel , by us-   ing a position - wise add and scale component . Let   /epsilon1∈Rdenote the word embeddings borrowed   from pre - trained XLMR , which does not intro-   duce additional trainable parameters . The input   alignment a={a, ... ,a}is represented as   e={e , ... , e}={/epsilon1[a], ... ,/epsilon1 [ a ] } .   Inspired by previous works ( Dou et al . , 2018 ;   Bapna and Firat , 2019 ) , we propose to combine the   transformed latent variables with alignment repre-   sentations . Speciﬁcally , we employ the position-   wise add and scale component to generate the latent6936representations :   z= ( z+e)×γ+p , ( 4 )   where pis a positional embedding and we nor-   malize the output by a factor γto maintain the   consistency of representations with the pre - trained   XLMR model .   2.3 Training and Inference   Algorithm 1 Training for XLM - D   In this section , we describe the overall train-   ing algorithm . As shown in Algorithm 1 , we   provide the model with masked alignments dur-   ing training in XLM - D by using Viterbi algo-   rithm to produce the best alignment , namely   a= arg maxP(a|x , a;θ ) . For the masked   alignment , we use a block - wise mask policy .   Speciﬁcally , we divide the sequence into blocks   with equal length Band mask the tokens within   each block randomly . For the loss computation ,   we force the output in non - masked positions of   alignment ato be the tokens of input alignment ¯a ,   termed as constrained CTC loss . For efﬁciency con-   sideration , we implement the Viterbi algorithm and   constrained CTC loss computation using CUDA   programming with C++ language extension .   During inference , the translation is produced   with a constant number of generative steps . Specif-   ically , we initialize the equal - size blocks with all   masked - out tokens . Then , we obtain a new align-   ment output of the model by selecting the tokens   with the largest predicted probability , and merging   the alignment output with the previous one . We   iteratively update the translation for each block by   masking the tokens with low predicted probability .   2.4 Conﬁgurability   Considering that the introduced decorator is tied   to neither a speciﬁc layer nor a speciﬁc type ofmasked language model , our approach is ﬂexible   and user - conﬁgurable . Moreover , Mcontrols the   trade - off between the translation performance and   inference speed . Intuitively , a larger Mwill lead   to fewer layers accumulatively that a translation   iteration needs during inference . In the analyses ,   we will explore the inﬂuence of Mon translation   performance and decoding speed . In the above il-   lustration , we mainly elaborate our approach in an   iterative scenario . In fact , our model can be sim-   pliﬁed to a single - step NAT model by removing   theposition - wise add and scale component . Dur-   ing training in our single - step XLM - D model , we   eliminate the block - wise mask policy and only use   vanilla CTC loss to train the model .   3 Experiments   3.1 Setup   Datasets We evaluate the effectiveness of our ap-   proach on two widely adopted benchmark datasets :   WMT14 English - German ( En - De)and WMT16   English - Romanian ( En - Ro ) , which consist of   4.0 M and 610 K sentence pairs respectively . We   also show the generality of our approach on four   other datasets , including WMT14 English - French   ( En - Fr ) and WMT20 Japanese - English ( Ja - En ) and   IWSLT17 Korean / Arabic - English ( Ko / Ar - En ) . For   the WMT14 En - De task , newstest2013 and new-   stest2014 are used as the validation and test set   respectively . For the WMT16 En - Ro task , we use   newsdev2016 and newstest2016 as the validation   and test set . We follow the dataset conﬁgurations   of previous works ( Gu et al . , 2018 ; Lee et al . , 2018 )   strictly . For our model , we segment each word   into tokens with a sentencepiece model ( Kudo and   Richardson , 2018 ) , learned on the full CC-100 data ,   that includes 250 K subword tokens in XLMR ( Con-   neau et al . , 2020 ) . To keep a consistent compar-   ison with previous researches , we apply the sen-   tencepiece model to the preprocessed data directly .   For evaluation , we report tokenized case - sensitive   BLEU scores ( Papineni et al . , 2002 ) for En - De ,   En - Ro and En - Fr , while using SacreBLEU ( Post ,   2018 ) for Ja - En , Ko - En and Ar - En .   Model Conﬁguration For model hyper-   parameters , we mainly follow the XLMR - base   conﬁgurations in ( Conneau et al . , 2020 ) . Specif-   ically , for all translation tasks , we use the6937   hyper - parameters ( d = 768 , d = 3072 ,   n= 12 , n= 12 , p = 0.1 ) . We   employt = 10000 as the warm - up learn-   ing rate schedule . In our implementation , the   upsampling ratio ( s ) of the distance - based latent   transformation module and the scale factor ( γ ) of   theposition - wise add and scale module are set to   2and0.5respectively . We use weight decay 0.01   and learning rate 0.0002 . For more implementation   details , please refer to Appendix A.1 .   Knowledge Distillation As a key component in   NAT models , knowledge distillation ( KD ) ( Zhou   et al . , 2020 ) has been proven to effectively reduce   the complexity of target data , which is beneﬁcial to   the training in NAT models . We strictly follow pre-   vious work ( Gu et al . , 2018 ) and utilize sequence-   level knowledge distillation ( Kim and Rush , 2016 )   to produce the training data . Speciﬁcally , for each   sentence pair in the training corpus , we replace the   target sentence with the translation generated by a   pre - trained autoregressive model .   3.2 Results   Table 1 and 2 show the translation quality and   inference speed of our approach and baselines on   WMT14 En - De and WMT16 En - Ro datasets . For   a fair comparison , we divide the results into two   types ( including iterative and single - step NAT mod-   els ) based on the number of iterations . Iterative NAT Models Table 1 shows that   CMLM ( Ghazvininejad et al . , 2019 ) achieves   comparable translation performance with the au-   toregressive Transformer - base models while the   inference speed - up is still unsatisfactory . As   seen , our iterative models outperform the baseline   model ( CMLM ) by 2.77and2.75BLEU points on   WMT14 En⇒De and De⇒En respectively , while   maintaining a faster inference speed . Besides , our   models achieve signiﬁcant improvements by up to   2.57/2.53points on both WMT16 En - Ro tasks . In   particular , our approach outperforms the autore-   gressive Transformer - base models by a very large   margin . Speciﬁcally , on WMT14 En ⇒De , our best   model achieves 29.80BLEU points with 2.8×in-   ference speed - up . These results clearly indicate the   tremendous potential of our approach .   Single - step NAT Models As presented in Ta-   ble 2 , our approach achieves signiﬁcant and   consistent improvements over all previous base-   lines across all translation tasks . Speciﬁcally ,   on WMT14 En⇒De and De⇒En , our single-   step models outperform the strong baseline   model ( GLAT ) by 2.25and0.84BLEU points re-   spectively . In addition , on the WMT16 En ⇒Ro   translation task , our model achieves a signiﬁcant   improvement by up to 3.51BLEU points . Encour-   agingly , our approach outperforms the Transformer-   base models by 0.42/0.33BLEU points on the6938   WMT16 En - Ro dataset . We attribute this to the   incorporation of additional language knowledge   beneﬁted from the pre - training model . These re-   sults clearly demonstrate the effectiveness of dec-   orating cross - lingual pre - training models as non-   autoregressive machine translation .   Inference Speed To evaluate the decoding speed   of our approach , we run all models with one sen-   tence at a time on a single GPU and calculate the   inference speed - up on the WMT14 En - De task ( for   more computation details , please refer to Appendix   A.2 ) . As demonstrated in Table 2 , our single - step   NAT models achieve 19.9×inference speed over   theTransformer - base counterpart and exceed all   previous single - step models . We credit this to   removing the overhead caused by cross - attention   modules in the conventional encoder - decoder ar-   chitecture . Moreover , our iterative models also   achieve a good acceleration as the number of itera-   tions increases , especially for M= 6 . The results   reveal that our model is lightweight and efﬁcient .   Translation Quality on Different Datasets Ta-   ble 3 shows results on four other datasets : WMT14   En⇒Fr , WMT20 Ja⇒En , IWSLT17 Ko⇒En and   IWSLT17 Ar⇒En , covering large - scale and small-   scale training data ( i.e. 35.8 M , 16.8 M , 0.23 M and   0.23 M ) . We follow the widely used translation di-   rections ( Vaswani et al . , 2017 ; Liu et al . , 2020 ; Gu   and Kong , 2021 ) . As seen , the superiority of our   approach holds across different language pairs and   data sizes , demonstrating the universality of the   proposed approach . In addition , the results in Ta-   ble 3 , show that the gains of our approach on small-   scale data ( Ko⇒En and Ar⇒En ) are more signiﬁ-   ca nt than on large - scale data ( En ⇒Fr and Ja⇒En ) .   This observation suggests that low - resource lan-   guage pairs beneﬁt more from pre - training models ,   which is also veriﬁed by Liu et al . ( 2020 ) .   3.3 Analyses   In this section , we perform extensive analyses on   the WMT14 En - De task to better demonstrate the   effectiveness of our model in terms of : 1 ) transla-   tion quality on original data , 2 ) effects of upsam-   pling ratio , 3 ) effects of different M , 4 ) effects of   pre - training , and 5 ) effects of the sentence length .   For more analyses , please refer to Appendix A.3.6939   Translation Quality on Original Data Previ-   ous works ( Gu et al . , 2018 ; Lee et al . , 2018 ) re-   ported that it was necessary to train NAT mod-   els on distillation data , which is generated with   a well - trained autoregressive model . To evaluate   our model ’s dependence on this process , we also   present the results of translation performance with   Transformer - base , CMLM and our approach on   WMT14 En⇒De original training data , which are   shown in Table 4 . Overall , our approach ( XLM-   D ) performs better than the CMLM baseline model   signiﬁcantly . For instance , our model obtains 27.49   BLEU scores , which outperforms the CMLM   model by 2.88points while remaining a faster infer-   ence speed with 2.8×. In particular , our model can   achieve comparable results ( 27.49vs.27.74 ) with   autoregressive Transformer - base , which is a very   promising result to compensate for the performance   gap on the original data . These results conﬁrm the   robustness and effectiveness of our approach .   Effects of Upsampling Ratio In Table 5 , we   study the inﬂuence of different upsampling ra-   tios ( s ) in our model . We ﬁnd that a higher upsam-   pling value achieves a signiﬁcant improvement in   the translation performance with a single decoding   iteration . For instance , the translation performance   is improved by 1.38points when increasing sfrom   1.5to2.0 . Besides , the upsampling ratio does not   inﬂuence the performance notably with multiple   iterations . This reveals that multiple iterations can   remedy the translation performance effectively .   Effects of Different M Previous researches   demonstrate that to obtain reliable performance ,   NAT models usually sacriﬁce the inference speed   by multiple reﬁnement iterations . In our model ,   we introduce an extra user - conﬁgurable hyper-   parameterMto balance the translation quality and   inference speed . To verify its efﬁcacy , we study   the inﬂuence of different Mon translation perfor-   mance and decoding latency . As shown in Figure 2 ,   we can ﬁnd that a higher Mvalue brings more   speed - up by multiple iterations without damaging   much translation performance . Although the litera-   ture shows that the translation quality and inference   speed is irreconcilable , our results suggest that the   proposed approach can achieve their balance by   modifying the user - conﬁgurable M.   Effects of Pre - training To study the inﬂuence   of additional knowledge brought by pre - training ,   we conduct an ablation analysis on the WMT14   En⇒De translation task . Speciﬁcally , we follow6940our approach but reset the parameters by the strat-   egy of random initialization in XLMR . As shown   in Table 6 , our model with pre - trained parameters   achieves signiﬁcant and consistent improvements   over the counterparts with parameters reset across   all model variants . For instance , in single - step NAT   models , our system outperforms the “ Reset ” model   by1.66BLEU points ( 27.46vs.25.80 ) . Besides ,   our iterative NAT models achieve improvements by   over1.0BLEU points as the increase of multiple it-   erations . These results conﬁrm that the pre - training   provides additional language knowledge , which   helps non - autoregressive machine translation .   Effects of Sentence Length To explore the bene-   ﬁts of our iterative models , we investigate the trans-   lation results of our approach on WMT14 En ⇒De   with respect to different lengths of target sentences .   Speciﬁcally , based on the respective lengths of tar-   get sentences , translations are allocated into dif-   ferent buckets . The evaluation results of BLEU   scores for each bucket are shown in Table 7 . As ex-   pected , our approach achieves improvements with   the increase of iterations across all buckets . In addi-   tion , the improvements of translation performance   are limited when the lengths of target sentences   are constrained ( ≤10 ) . In particular , for longer   sentences ( ≥40 ) , our model improves the perfor-   mance by 2.70BLEU points with T= 2 . This   evidence suggests that our model clearly beneﬁts   long - distance dependencies in NAT models .   4 Related Work   Pre - training Models Unsupervised representa-   tion learning ( Peters et al . , 2018 ; Devlin et al . , 2019 ;   Liu et al . , 2019 ) has achieved remarkable success   in natural language understanding . Peters et al .   ( 2018 ) ﬁrst introduced a general approach for learn-   ing high - quality context - dependent representations ,   which encode syntactic and semantic information   efﬁciently . By using the objective of masked lan - guage model , Devlin et al . ( 2019 ) introduced a new   bidirectional representation model and obtained   state - of - the - art results on various tasks . Most re-   cently , Conneau et al . ( 2020 ) extended pre - training   models to one hundred languages by using more   than two terabytes of data . Different from these   studies , we observe that cross - lingual pre - trained   masked language models are endowed with the at-   tribute of non - autoregressive generation inherently .   Hence , we propose to decorate a cross - lingual pre-   training model as non - autoregressive translation .   Non - Autoregressive Neural Machine Transla-   tion Many previous works investigate reducing   the decoding latency by empowering the parallel se-   quence generation capability ( Gu et al . , 2018 ; Lee   et al . , 2018 ) . Gu et al . ( 2018 ) pioneered to propose   a non - autoregressive translation model by model-   ing fertility as latent variables . Although accelerat-   ing the inference process signiﬁcantly , NAT mod-   els suffer from the serious multi - modality problem ,   which results in considerably worse performance   than their autoregressive counterparts . To alleviate   this issue , extensive methods have been investi-   gated in vanilla non - autoregressive training ( Kaiser   et al . , 2018 ; Sun et al . , 2019 ; Bao et al . , 2021 ; Du   et al . , 2021 ) . Another line of researches ( Lee et al . ,   2018 ; Ghazvininejad et al . , 2019 ; Gu et al . , 2019 ;   Ding et al . , 2021a , 2022 ; Huang et al . , 2022 ) em-   ploy an iterative reﬁnement process to maintain   the translation quality . Ghazvininejad et al . ( 2019 )   used a masked language modeling to predict any   subset of target words by conditioning on a partially   masked target translation .   5 Conclusion   We propose to decorate cross - lingual pre - training   models ( XLMR ) as non - autoregressive machine   translation in a lightweight , effective and ﬂexible   way . Firstly , we bridge pre - training masked lan-   guage models with non - autoregressive generation ,   revealing that a pre - trained masked language model   can be directly used to produce sequences in paral-   lel . Moreover , we propose to incorporate distance-   based latent transformation andposition - wise add   and scale components into a user - speciﬁed layer of   XLMR . Empirical results on a variety of language   pairs demonstrate the effectiveness and universal-   ity of our approach . Further analyses conﬁrm that   the proposed method beneﬁts translations for long   sentences and achieves a better balance between   translation quality and inference speedup.6941Limitations   This work has several limitations which are ex-   pected to explore sufﬁciently in future work .   Firstly , we only conduct experiments on non-   autoregressive machine translation . Although our   method signiﬁcantly improves translation perfor-   mance and obtains state - of - the - art results on trans-   lation tasks , we believe that the proposed approach   is generally applicable to other generation tasks   as well , including text summarization ( See et al . ,   2017 ) and dialogue generation ( Vinyals and Le ,   2015 ) . Future explorations on these tasks are   needed to extend the application scope of XLM - D.   At the same time , we hypothesize that a cross-   lingual pre - training model can be seamlessly trans-   formed as a non - autoregressive translation model   and take bilingual machine translation as a testbed   in this work . Actually , a multilingual pre - training   model , such as XLMR ( Conneau et al . , 2020 ) , can   be employed in the multilingual scenario ( Aha-   roni et al . , 2019 ; Freitag and Firat , 2020 ) and has   great potential for building multilingual / zero - shot   non - autoregressive machine translation ( Johnson   et al . , 2017 ; Zhang et al . , 2020 , 2022 ) . XLM - D   is bilingual and only evaluated on the language   pair involved in training . It is interesting to fur-   ther extend XLM - D to the multilingual scenario   that can beneﬁt from effective cross - lingual trans-   fer and improved serving efﬁciency . We leave more   explorations on this direction as future work .   Acknowledgement   We thank the anonymous reviewers for their insight-   ful feedback on this work . Yun Chen is supported   by National Natural Science Foundation of China   ( No . 62106138 ) .   References694269436944A Appendix   A.1 Implementation Details   We implement our approach with open - source   toolkit - fairseq(Ott et al . , 2019 ) . For WMT tasks ,   all models were trained for 200 K updates on 8   NVIDIA Tesla A100 GPUs with a batch size of   128 K tokens using Adam optimizer ( Kingma and   Ba , 2015 ) . For IWSLT17 Ko - En and IWSLT17   Ar - En tasks , we trained the model for 100 K up-   dates on 1 GPU with a batch size of 8 K tokens .   Following previous studies ( Gu et al . , 2018 ; Lee   et al . , 2018 ) , we evaluate the speed - up by averag-   ing the decoding latency for each sentence on the   WMT14 En - De test set with batch size 1on a single   NVIDIA Tesla A100 GPU for Transformer - base   and our model .   A.2 Computation of Inference Speed   To compute the inference speed , we run our own   baselines and perform 5 runs to reduce the potential   noise for timing . Speciﬁcally , we run our model   andTransformer - base model on a single GPU with   the WMT14 En⇒De test set and evaluate the speed   up by comparing the translation latency . The details   are shown in Table 8 .   A.3 More Analyses   Effects of Different Decoding Strategies The   block - wise mask policy is to ensure that for ev-   ery decoding iteration , the tokens in each sen-   tence block have the chance to be selected and   predicted , and thereby improves the translation per-   formance . As shown in Table 9 , we systemati-   cally compare different decoding strategies on both   WMT14 En⇒De and De⇒En translation tasks . As   seen , the dynamic strategy performs better than the   static one across all number of iterations and tasks .   Therefore , we use the strategy of dynamic decoding   throughout our experiments .   Comparison with Pre - training Models Guo   et al . ( 2020b ) propose to incorporate BERT into   parallel sequence decoding with adapters . Dif-   ferent from their work , our study builds the con-   nection between the pre - training MLM and non-   autoregressive generation . Furthermore , we pro-   pose a lightweight , effective and user - conﬁgurable   approach to decorating XLMR as NAT models ,   achieving both inference speed - up and huge per-   formance gain . We compare the translation quality   and inference speed for both approaches , which   are shown in Table 10 . The results show that com-   pared with AB - Net , our approach achieves more   improvements ( 29.59vs.28.69 ) while remaining a   faster inference speed ( 4.1×vs.2.4× ) . This again   demonstrates the superiority of our method .   Case Study We carried out a case study to il-   lustrate the performance of our method and base-   line approach . Table 11 shows a translation ex-   ample randomly selected from the test set in the   WMT14 De⇒En task . As seen , XLM - D can pro-   duce more adequate and ﬂuent translations . For in-   stance , the German words “ beﬁnden sich in einem   Dauer - Kampf ” are mistranslated by baseline , while   the XLM - D model accurately translates it into “ en-   gaged in an ongoing ﬁght ” . Besides , the baseline6945Source Sowohl die US - Behörden als auch die mexikanischen Sicherheitskräfte beﬁnden sich in   einem Dauer - Kampf gegen die Drogenkartelle .   Target Both the US authorities and the Mexican security forces areengaged in an ongoing battle   against the drug cartels .   Baseline Both the US authorities and Mexico ’s forces are in a long - term ﬁght against the drug   cartels .   XLM - D Both the US authorities and the Mexican security forces areengaged in an ongoing ﬁght   against drug cartels .   tends to generate incomplete words ( e.g. , “ Mexico ’s   forces ” ) , while our model corrects this issue . This   demonstrates that our model improves the ﬂuency   and adequacy of translations in terms of words ,   phrases and patterns.6946