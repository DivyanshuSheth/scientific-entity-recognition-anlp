  David Samuel , Jeremy Barnes , Robin Kurtz , Stephan Oepen ,   Lilja Øvrelidand Erik VelldalUniversity of Oslo , Language Technology GroupUniversity of the Basque Country UPV / EHU , HiTZ Center – IxaNational Library of Sweden , KBLab   { davisamu , oe , liljao , erikve}@ifi.uio.no   jeremy.barnes@ehu.eus , robin.kurtz@kb.se   Abstract   This paper demonstrates how a graph - based   semantic parser can be applied to the task of   structured sentiment analysis , directly predict-   ing sentiment graphs from text . We advance   the state of the art on 4 out of 5 standard bench-   mark sets . We release the source code , models   and predictions .   1 Introduction   The task of structured sentiment analysis ( SSA ) is   aimed at locating all opinion tuples within a sen-   tence , where a single opinion contains a ) a polar   expression , b ) an optional holder , c ) an optional   sentiment target , and d ) a positive , negative or neu-   tral polarity , see Figure 1 . While there have been   sentiment corpora annotated with this information   for decades ( Wiebe et al . , 2005 ; Toprak et al . , 2010 ) ,   there have so far been few attempts at modeling   the full representation , rather focusing on various   subcomponents , such as the polar expressions and   targets without explicitly expressing their relations   ( Peng et al . , 2019 ; Xu et al . , 2020 ) or the polarity   ( Yang and Cardie , 2013 ; Katiyar and Cardie , 2016 ) .   Dependency parsing approaches have recently   shown promising results for SSA ( Barnes et al . ,   2021 ; Peng et al . , 2021 ) . Here we present a   novel sentiment parser which , unlike previous at-   tempts , predicts sentiment graphs directly from   text without reliance on heuristic lossy conversions   to intermediate dependency representations . The   model takes inspiration from successful work in   meaning representation parsing , and in particular   the permutation - invariant graph - based parser of   Samuel and Straka ( 2020 ) called PERIN .   Experimenting with several different graph en-   codings , we evaluate our approach on ﬁve datasets   from four different languages , and ﬁnd that it com-   pares favorably to dependency - based models acrossFigure 1 : A sentiment graph for the phrase “ Nowadays   I actually enjoy the bad acting , ” which contains an ex-   ample of nesting of two opposing opinions .   all datasets ; most signiﬁcantly on the more struc-   turally complex ones – NoReC andMPQA .   2 Related work   Proposing a dependency parsing approach to the   full task of SSA , Barnes et al . ( 2021 ) show that it   leads to strong improvements over state - of - the - art   baselines . Peng et al . ( 2021 ) propose a sparse fuzzy   attention mechanism to deal with the sparseness of   dependency arcs in the models from Barnes et al .   ( 2021 ) and show further improvements . However ,   in order to apply the parsing algorithm of Dozat   and Manning ( 2018 ) , both of these approaches have   to rely on a lossy conversion to bi - lexical depen-   dencies with ad - hoc internal head choices for the   nodes of the abstract sentiment graph , see Section   3 for a discussion of these issues .   More generally , decoding structured graph infor-   mation from text has sparked a lot of interest in   recent years , especially for parsing meaning repre-   sentation graphs ( Oepen et al . , 2020 ) . There has   been tremendous progress in developing complex   transition - based and graph - based parsers ( Hersh-   covich et al . , 2017 ; McDonald and Pereira , 2006 ;   Dozat and Manning , 2018 ) . In this paper , we adopt   PERIN ( Samuel and Straka , 2020 ) , a state - of - the-   art graph - based parser capable of modeling a su-   perset of graph features needed for our task.470   3 Issues with dependency encoding   As mentioned above , previous dependency parsing   approaches to SSA have relied on a lossy bi - lexical   conversion . This is caused by an inherent ambigu-   ity in the dependency encoding of two nested text   spans with the same head ( deﬁned as either the ﬁrst   or the last token in Barnes et al . ( 2021 ) ) .   To be concrete , we can use the running exam-   ple“Nowadays I actually enjoy the bad acting , ”   which has two opinions with nested targets ; “ the   bad acting , ” , which is associated with a positive   polarity indicated by the polar expression “ enjoy ” ,   and“acting , ” , with a negative polarity expressed by   “ bad ” . As shown in the dependency representation   in Figure 2 , both expression – target edges correctly   lead to the word “ acting ” but it is impossible to   disambiguate the preﬁx of both targets in the bi-   lexical encoding , i.e. , to determine that the tokens   “ the ” and“bad ” are part of the target only for the   positive opinion . For that , we need a more abstract   graph encoding , such as the ones suggested in this   paper .   4 PERIN model   PERIN is a general permutation - invariant text - to-   graph parser . The output of the parser can be a   directed graph with labeled nodes connected by la-   beled edges where each node is anchored to a span   of text ( possibly empty or discontinuous ) . We pro-   pose three graph representations for SSA that meet   these constrains and thus can be easily modeled by   this parser .   We use only a subset of the full PERIN ’s func-   tionality for our SSA version – it does not need to   use the “ relative label rules ” and model node prop-   erties or edge attributes . Please consult the origi-   nal work for more technical details about PERIN   ( Samuel and Straka , 2020 ) .   4.1 Architecture   PERIN processes the input text end - to - end in four   steps , illustrated in Figure 3 : 1 ) To encode the in-   put , PERIN uses contextualized embeddings from   XLM - R ( base size ; Conneau et al . , 2020 ) and   combines them with learned character - level em-   beddings;2 ) each token is mapped onto latent   queries by a linear transformation ; 3 ) a stack of   Transformer ( encoder ) layers without positional   embedding ( Vaswani et al . , 2017 ) optionally mod-   els the inter - query dependencies ; and 4 ) classiﬁ-   cation heads select and label queries onto nodes ,   establish anchoring from nodes to tokens , and pre-   dict the node - to - node edges .   4.2 Permutation - invariant query - to - node   matching   Traditional graph - based parsers are trained as au-   toregressive sequence - to - sequence models . PERIN   does not assume any prior ordering of the graph   nodes . Instead , it processes all queries in parallel   and then dynamically maps them to gold nodes.471Based on the predicted probabilities of labels   and anchors , we create a weighted bipartite graph   between all queries and nodes . The goal is to ﬁnd   the most probable matching , which can be done   efﬁciently in polynomial time by using the Hungar-   ian algorithm . Finally , every node is assigned to a   query and we can backpropagate through standard   cross - entropy losses to update the model weights .   4.3 Graph encodings   PERIN deﬁnes an overall framework for general   graph parsing , it can cater to speciﬁc graph encod-   ings by changing the subset of its classiﬁcation   heads . In parsing the abstract sentiment structures ,   there are several possible lossless graph encodings   depending on the positioning of the polarity in-   formation and the sentiment node type . We ex-   periment with three variations ( Figure 4 ) and later   show that while the graph encoding improves per-   formance , this improvement largely depends on the   type of encoding used .   1.Node - centric encoding , with labeled nodes   and directed unlabeled arcs . Each node cor-   responds to a target , holder or sentiment ex-   pression ; edges form their relationships . The   parser uses a multi - class node head , an anchor   head and a binary edge classiﬁcation head .   2.Labeled - edge encoding , with deduplicated   unlabeled nodes and labeled arcs . Each node   corresponds to a unique text span from some   sentiment graph , while edge labels denote   their relationships and functions . The model   has a binary node classiﬁer , an anchor classi-   ﬁer and a binary and multi - class edge head .   3.Opinion - tuple encoding , which represents   the structured sentiment information as a se-   quence of opinion four - tuples . This encoding   is the most restrictive , having the lowest de-   grees of freedom . The parser utilizes a multi-   class node head and three anchor classiﬁers , it   does not need an edge classiﬁer .   5 Data   Following Barnes et al . ( 2021 ) we employ ﬁve   structured sentiment datasets in four languages ,   the statistics of which are shown in Table 1 . The   largest dataset is the NoReCdataset ( Øvrelid   et al . , 2020 ) , a multi - domain dataset of professional   reviews in Norwegian . EUandCA(Barnes et al . ,   2018 ) contain hotel reviews in Basque and Catalan ,   respectively . MPQA ( Wiebe et al . , 2005 ) annotates   news wire text in English . Finally , DSU ( Toprak   et al . , 2010 ) annotates English reviews of online   universities . We use the SemEval 2022 releases of   MPQA andDSU ( Barnes et al . , 2022 ) .   5.1 Nested dependencies   Returning to the issue of dependency encoding   for nested elements discussed in Section 3 , Table   2 shows that the amount of nesting in the SSA   datasets is not negligible , further motivating our   abstract graph encodings for this task .   Table 3a further shows the amount of depen-   dency edges lost because of overlap . Finally , Table   3b shows the S Fscore when converting the gold   sentiment graphs to bi - lexical dependency graphs   and back – an inherent upper bound for any depen-   dency parser.472   6 Experiments   6.1 Evaluation   Following Barnes et al . ( 2021 ) , we evaluate our   models using Sentiment Graph F(SF ) . This met-   ric considers that each sentiment graph is a tuple   of ( holder , target , expression , polarity ) . A true pos-   itive is deﬁned as an exact match at graph - level ,   weighting the overlap in predicted and gold spans   for each element , averaged across all three spans .   For precision it weights the number of correctly   predicted tokens divided by the total number of   predicted tokens ( for recall , it divides instead by   the number of gold tokens ) . S Fallows for empty   holders and targets .   In order to further analyze the models , we also   include token - level Ffor extraction of Holders ,   Targets , and Polar Expressions , as well as Non-   polar Sentiment Graph F(NSF ) .   6.2 Models   We compare our models to the head-ﬁnal depen-   dency graph parsers from Barnes et al . ( 2021 ) as   well as the second - order Sparse Fuzzy Attention   parser of Peng et al . ( 2021 ) . For all models , we   perform 5 runs with 5 different random seeds and   report the mean and standard deviation . Results   on development splits are provided in Appendix C ,   training details are in Appendix D.6.3 Results   Table 4 shows the main results . Our models out-   perform both dependency graph models on S F ,   although the results are mixed for span extraction .   The opinion - tuple encoding gives the best perfor-   mance on S F(an average of 6.2 percentage points   ( pp . ) better than Peng et al . ( 2021 ) ) , followed by   the labeled edge encoding ( 3.0 ) and ﬁnally the   node - centric encoding ( 2.1 ) .   For extracting spans , the opinion tuple encoding   also achieves the best results on NoReC , either   labeled - edge or node centric on CAandMPQA ,   while Peng et al . ( 2021 ) is best on EUandDSU .   This suggests that the main beneﬁt of PERIN is at   the structural level , rather than local extraction .   7 Analysis   There are a number of architectural differences   between the dependency parsing approaches com-   pared above . In this section , we aim to isolate   the effect of predicting intermediate dependency   graphs vs. directly predicting sentiment graphs   by creating more comparable dependencyand   PERIN models . We adapt the dependency model   from Barnes et al . ( 2021 ) by removing the to-   ken , lemma , and POS embeddings and replacing   mBERT ( Devlin et al . , 2019 ) with XLM - R ( Con-   neau et al . , 2020 ) . The ‘ XLM - R dependency ’   model thus has character LSTM embeddings and   token - level XLM - R features . Since these are   not updated during training , for the opinion - tuple   ‘ Frozen PERIN ’ model , we ﬁx the XLM - R weights   to make it comparable .   As shown in Table 5 , predicting the sentiment   graph directly leads to an average gain of 3.7 pp . on   the Sentiment Graph Fmetric . For extracting the473   spans of holder , target , and polar expressions , the   beneﬁt is less clear . Here , the PERIN model only   outperforms the XLM - R dependency model 5 of   15 times , which seems to conﬁrm that its beneﬁt is   at the graph level . This is further supported by the   fact that the highest gains are found on the datasets   with the most nested sentiment expressions anddependency arcs lost due to overlap , which are   difﬁcult to encode in bi - lexical graphs .   8 Conclusion   Previous work cast the task of structured sentiment   analysis ( SSA ) as dependency parsing , converting   the sentiment graphs into lossy dependency graphs .   In contrast , we here present a novel sentiment   parser which predicts sentiment graphs directly   from text without reliance on lossy dependency rep-   resentations . We adapted a state - of - the - art meaning   representation parser and proposed three candidate   graph encodings of the sentiment structures . Our   experimental results suggest that our approach has   clear performance beneﬁts , advancing the state of   the art on four out of ﬁve standard SSA bench-   marks . Speciﬁcally , the most direct opinion - tuple   encoding provides the highest performance gains .   More detailed analysis of the results shows that the   beneﬁts stem from better extraction of global struc-   tures , rather than local span prediction . Finally , we   believe that various structured prediction problems   in NLP can similarly be approached in a uniform   manner as parsing into directed graphs.474References475   A Changes to datasets   We found out that the ofﬁcial data pub-   lished at was slightly changed from   the data used in previous related work . Speciﬁcally   theMPQA andDSU datasets had removed a num-   ber of errors resulting from the annotation and from   the conversion scripts used to create the sentiment   graph representations . We re - run the experiments   for the comparable baseline model and show the   performance differences in Table 7 .   B Bootstrap Signiﬁcance Testing   In order to see whether the performance differences   for the experiments are signiﬁcant , we do boot-   strap signiﬁcance testing Berg - Kirkpatrick et al .   ( 2012 ) , combining two variations . First , we resam-   ple the test sets with replacement from all 5 runs   together , b= 1 000 000 times , setting the threshold   atp= 0:05 . Additionally , we test each pair outof the 55combinations for all runs , resampling   the test set with replacement b= 100 000 times ,   setting the threshold again at p= 0:5 . When one   system is signiﬁcantly better in 15 out of the 25   comparisons , and additionally signiﬁcantly better   in the ﬁrst joint test , we ﬁnally mark it as signiﬁ-   cantly better .   C Results on development data   To make any future comparison of our approach   easier , we show the development scores of all re-   ported models in Table 6 .   D Training details   Generally , we follow the training regime described   in the original PERIN paper ( Samuel and Straka ,   2020 ) . The trainable parameters are updated with   the AdamW optimizer ( Loshchilov and Hutter ,   2019 ) , and their learning rate is linearly warmed-   up for the ﬁrst 10 % of the training to improve sta-   bility , and then decayed with a cosine schedule .   The XLM - R parameters are updated with a lower   learning rate and higher weight decay to improve   generalization . Similarly to PERIN , we freeze the   embedding parameters for increased efﬁciency and   regularization . Following the ﬁnding by Zhang   et al . ( 2021 ) , we use small learning rates and ﬁne-   tune for a rather long time to increase the training   stability . Unlike the authors of PERIN , we did not   ﬁnd any beneﬁts from a dynamic scaling of loss   weights ( Chen et al . , 2018 ) , so we simply set all   loss weights to constant 1:0 .   We trained our models on a single Nvidia P100   with 16 GB memory , the runtimes are given in Ta-   ble 6 . We made ﬁve runs from different seeds   for each reported value to better estimate the ex-   pected error . The hyperparameter conﬁgurations   for all runs follow , please consult the released   code for more details and context : .   General hyperparameters476   NoReC node - centric hyperparameters   NoReC labeled - edge hyperparameters   NoReC opinion - tuple hyperparameters   NoReC frozen opinion - tuple hyperparame-   ters477EU node - centric hyperparameters   EU labeled - edge hyperparameters   EU opinion - tuple hyperparameters   EU frozen opinion - tuple hyperparameters   CA node - centric hyperparameters   CA labeled - edge hyperparameters   CA opinion - tuple hyperparameters   CA frozen opinion - tuple hyperparameters   MPQA node - centric hyperparameters   MPQA labeled - edge hyperparameters   MPQA opinion - tuple hyperparametersMPQA frozen opinion - tuple hyperparame-   ters   DSU node - centric hyperparameters   DSU labeled - edge hyperparameters   DSU opinion - tuple hyperparameters   DSU frozen opinion - tuple hyperparameters478