  Zhiling Zhangand Siyuan Chenand Mengyue Wuand Kenny Q. Zhu   Shanghai Jiao Tong University   Shanghai , China   { blmoistawinde , chensiyuan925 , mengyuewu}@sjtu.edu.cn , kzhu@cs.sjtu.edu.cn   Abstract   Mental disease detection ( MDD ) from social   media has suffered from poor generalizability   and interpretability , due to lack of symptom   modeling . This paper introduces PsySym , the   first annotated symptom identification corpus   of multiple psychiatric disorders , to facilitate   further research progress . PsySym is anno-   tated according to a knowledge graph of the 38   symptom classes related to 7 mental diseases   complied from established clinical manuals and   scales , and a novel annotation framework for   diversity and quality . Experiments show that   symptom - assisted MDD enabled by PsySym   can outperform strong pure - text baselines . We   also exhibit the convincing MDD explanations   provided by symptom predictions with case   studies , and point to their further potential ap-   plications .   1 Introduction   Mental health has been a significant challenge in   global healthcare . Nearly 1 in 5 U.S. adults live   with a mental illness or condition ( NIMH , 2022 ) ,   and there are about 1 billion people suffering from   mental disorders worldwide ( UN , 2020 ) . Due to the   stigma of mental disorders and lack of professional   mental health services , many people can not receive   proper diagnose or treatment for their conditions .   Social Media can be a promising source for studies   on this problem , as we may detect hidden traces   of mental disorders from the symptoms that users   may reveal in their free sharing , and provide perti-   nent help for those in need . Consequently , Mental   Disease Detection ( MDD ) from social media has   received increasing attention ( Coppersmith et al . ,   2015 ; Cohan et al . , 2018 ) .   However , most automatic MDD methods still   struggle in this task , especially for their unsatis - Figure 1 : Comparison between text - based and the   proposed symptom - assisted mental disease detection   method , which leverages psychiatric knowledge for   symptom identification to improve the effectiveness and   interpretability of MDD .   fying generalizability and explainability . Firstly ,   these models may learn dataset - specific spurious   correlations between certain words and the labels   ( usually diseases ) , and thus fail to generalize ( Har-   rigian et al . , 2020 ) . Moreover , most deep learning   based methods work as black boxes , and can not   provide explanations for their prediction , which   differs from the clinical practice which leverages   symptom - based diagnostic criterions from author-   itative manuals like DSM-5 ( APA et al . , 2013 ) .   Therefore , it may be hard for current MDD meth-   ods to gain trust from their users .   To tackle these issues , there has been a rising   interest in utilizing symptoms for MDD , as they   are the bases that human psychiatrists use to make   diagnoses . Pioneering research has shown their   potential benefits of improving the accuracy , gener-   alizability and interpretability of MDD ( Lee et al . ,   2021 ; Nguyen et al . , 2022 ; Zhang et al . , 2022 ) . Nev-   ertheless , due to the lack of large - scale annotated   corpus for supervised learning , they can only ex-   tract symptom features with unsupervised / weakly   supervised methods or simple pattern matching ,   which may not guarantee the quality of the ex-   tracted features . Moreover , most of these works9970only focus on the detection of depression . How-   ever , many mental disorders may share similar set   of symptoms . For example , depressed mood can   not only be seen on the patient of depression , but   also those suffering from bipolar disorder , anxiety ,   etc . Jointly modeling the symptoms of multiple dis-   eases may enhance the performance on all classes .   These limitations call for the establishment of   a large - scale , multi - disease annotated dataset for   symptom identification , which will face many   novel challenges . Initially , the symptoms of multi-   ple diseases are scattered over the different chapters   of DSM-5 and other materials . Similar symptoms   would have varied expressions in different places ,   which causes difficulty in setting up the annotation   standard . Furthermore , the free and diverse lan-   guage style on social media ( Yadav et al . , 2020 )   can make the retrieval of candidate posts for annota-   tion difficult . Last but not least , the relatively large   amount of symptoms from different disorders and   the nuanced differences and similarities between   them make it hard to get high - quality annotations   ( i.e. inter - rater agreement will be low ) .   In this work , we propose a novel data anno-   tation framework , and introduce the first multi-   disease symptom identification dataset based on   social media posts , PsySym ( Psychiatric - disorder   Sym ptoms ) , which contains the multi - label anno-   tations of 38 symptom classes from 7 mental dis-   eases on 8,554 Reddit post sentences . We establish   our annotation target ( symptom classes ) mainly   based on the diagnostic criterions from DSM-5 ,   with symptom descriptions on clinical question-   naires as supplementary . We leverage embedding-   based retrieval methods ( Reimers and Gurevych ,   2019 ) instead of keyword matching to get the candi-   date sentences for annotation , which can effectively   constrain our efforts to a precise but diverse sub-   set of posts for efficient annotation . To guarantee   the quality of data , we apply several quality con-   trol approaches , and divide the annotation tasks   by separate diseases so as to reduce the cognitive   burdens of annotators , resulting in high inter - rater   agreement .   Finally , we propose a symptom - assisted MDD   framework ( Figure 1 ) . We use models trained on   PsySym to extract symptom features for MDD , out-   performing strong BERT - based baseline ( Devlin   et al . , 2018 ) . We also explore the interpretability of   symptom identification for MDD . We find that they   can reasonably provide DSM-5 compliant expla - nations for diagnosed patients . We will also show   that symptom - based interpretations can also help   us find incorrect labels in automatically constructed   MDD datasets , indicating their further potential .   Our contributions are :   •We build the first social - media based symp-   tom identification dataset of multiple men-   tal diseases , PsySym , with novel annotation   framework to guarantee the diversity and qual-   ity of the dataset .   •We propose symptom - assisted MDD , which   leverages the features extracted from PsySym-   trained models , and can outperform strong   baselines in MDD .   •We demonstrate the intuitive interpretability   for MDD results enabled by symptoms , and   its promising applications with case studies .   2 Related Work   Mental Disease Detection Mental Disease De-   tection ( MDD ) from social media is enabled by the   users ’ self disclosure of their diagnosis , or their   participation in mental - disease related topics and   forums . These proxy signals can be leveraged to   automatically label the diagnosed diseases of users   for the supervised learning of machine learning al-   gorithms . Early researches mainly focus on the de-   tection of depression ( De Choudhury et al . , 2013 ) ,   and following works further extend the scope to   multiple diseases ( Coppersmith et al . , 2015 ; Cohan   et al . , 2018 ) .   Approaches for MDD can be mainly divided   into two types . The first utilizes features like bag-   of - words , topic modeling and LIWC ( Pennebaker   et al . , 2001 ) with traditional machine learning al-   gorithms ( Shen et al . , 2017 ; Trotzek et al . , 2018 ) .   These methods can provide word / topic - level inter-   pretability , but they can not leverage the temporal   pattern of the posts . The second type leverage deep   neural networks that can encode the posts as a se-   quence for better temporal modeling ( Yates et al . ,   2017 ; Sekuli ´ c and Strube , 2019 ; Gui et al . , 2019 ) .   However , these methods work as black - boxes and   can not provide explanations . Recent works have   also revealed that both types of methods suffer from   the lack of generalizability ( Harrigian et al . , 2020 ;   Nguyen et al . , 2022 ) .   Symptom Identification There have been some   pioneering attempts to leverage symptom - related9971features for MDD . Karmen et al . ( 2015 ) uses man-   ually complied lexicon to detect symptoms , and   aggregate them into a score for the detection of de-   pression . Lee et al . ( 2021 ) and Zhang et al . ( 2022 )   leverage the embedding similarity between a post   and symptom - related descriptions to decide the   presence or risk of a symptom . Nguyen et al . ( 2022 )   uses regular expressions and heuristics to automat-   ically build weakly - supervised training data for   symptom identification . These methods have exhib-   ited superior generalizability and interpretability .   Nevertheless , the efforts on establishing annotated   corpus for symptom identification are still limited   ( Mowery et al . , 2017 ) , which may hinder the poten-   tial of symptom - assisted MDD methods for lever-   aging stronger supervised symptom models .   3 Dataset Construction   In this section , we introduce the construction of   PsySym , the first annotated multi - disease symptom   identification dataset based on social media posts .   3.1 Disease - Symptom Knowledge Graph   Before data construction , we need to decide the   annotation targets , i.e. which diseases and their   corresponding symptoms to annotate . Considering   our downstream application of MDD , we choose   7 diseasesthat are used in the established SMHD   dataset ( Cohan et al . , 2018 ): Depression , Anxi-   ety , ADHD , Bipolar Disorder , OCD , PTSD , Eating   Disorder . We then find symptoms used in the di-   agnostic criterion of these diseases within DSM-   5 ( APA et al . , 2013 ) . However , the symptoms in   DSM-5 are not represented as standard classes , but   expressed in natural language with similar symp-   tom having nuanced differences in different places .   Therefore , we manually merge similar expressions   of a symptom into one standardized class , and store   the expressions as its detailed descriptions ( also   referred to as sub - symptoms ) . After merging the   symptoms , the diseases and the standardized symp-   toms constitute a bipartite Knowledge Graph ( KG ) ,   where we can clearly see the shared symptoms of   different diseases . We also supplement the KG   with representative clinical questionnaires , where   we also merge the symptoms mentioned in each   question / item into the standard classes , and add   links between the targeted disease and the mea-   sured symptoms if such edges are not found inDSM-5 . The final knowledge graph ( Figure 2 )   has 45 nodes ( 7 diseases and 38 symptoms ) , and   162 edges . Symptoms like Depressed Mood and   Inattention are shared by as many as 5 diseases ,   suggesting the potential of learning such shared   features with multi - disease modeling .   3.2 Annotation Candidates Retrieval   We then search for candidate posts to annotate the   symptoms . We choose Reddit as our data source   for its public availability and wide acceptance in   previous literature ( Losada and Crestani , 2016 ; Co-   han et al . , 2018 ; Wolohan et al . , 2018 ) . Specifically ,   the candidate pool consists of all self - posts from   2005 to 2016 in the PushShift dataset ( Baumgartner   et al . , 2020 ) , and all posts are split into sentences   for later usage .   The huge amount of posts necessitate a pre - step   of selecting candidates that are likely to express   certain symptoms for acceptable annotation efforts .   First , we only select candidates from mental health   related subreddits ( Cohan et al . , 2018 ) , where more   posts will be relevant . Moreover , we leverage   embedding - based retrieval to further narrow down   the range . Specifically , we use Sentence - BERT   ( Reimers and Gurevych , 2019 ) to encode the post   sentences and the symptom descriptions in the KG   into embeddings . Then we will calculate the cosine   similarity between them , and estimate a sentence ’s   relevance to a symptom with its max similarity with   all of the symptom ’s sub - symptoms . The rationale   is that a symptom can have different manifesta-   tions expressed in its descriptions , and a sentence   can be considered to convey that symptom if it9972   resembles any one of them . However , since the   original symptom descriptions in DSM-5 are ex-   pressed in a professional style and usually observed   as a third party , they do not necessarily reflect the   self - reporting nature of the content on social media .   The descriptions collected from clinical question-   naires , however , can alleviate the problem , as they   are designed to be easy to understand and fill in .   To further tackle the mismatching that can not be   solved even with the questionnaires , we also di-   rectly collected some typical Reddit posts about a   symptom for the similarity calculation in place of   the official descriptions . We will show in § 5.6 that   our final method can lead to better precision and   recall for the retrieved candidates , compared with   the keyword / pattern matching methods commonly   used in previous works ( Mowery et al . , 2017 ; Ya-   dav et al . , 2020 ) .   3.3 Annotation Design   Annotations for symptom identification usually in-   volve the binary decisions if a symptom can be   identified from the sentence . Such decision can   sometimes be tricky when a symptom is mentioned   in a sentence , but is not actually present . For exam-   ple , “ I do n’t have panic attack . ” implies a negation   of the symptom , and “ Is it panic attack ? ” expresses   theuncertainty about the symptom . Although most   previous works like Nguyen et al . ( 2022 ) treat such   cases as negative samples , we think that they are   different from the other negatives like sentences   totally irrelevant to any symptoms or only about   other symptoms . Distinguishing between them   may enable a more informative analysis and benefit   downstream applications . Therefore , we decided   to divide the annotation into two tasks : relevance   judgment andstatus inference , as are exempli-   fied in Table 1 and Table 11 . For relevance judg-   ment , the annotator needs to judge whether the sen-   tence is relevant to the given symptoms . Note that   the symptoms can be described in figurative lan-   guage instead of standard patterns , and they can be   negated or uncertain . For status inference , the an-   notator needs to decide , if the relevant symptom(s)are indeed present . We denote positive / negative   status as ‘ True’/‘Uncertain ’ .   Before crowdsourcing annotation , we first con-   ducted preliminary annotations ourselves . We   found it hard to annotate all 38 symptoms among   the candidates from all mental health subreddits .   The relatively large number of classes , and the nu-   anced differences and similarities between them   can pose heavy cognitive burden to the annotators .   Therefore , we decided to separate the annotation   job queues by disease . In each queue , the annota-   tors only need to read the posts from the subreddits   of that disease , and the symptoms are restricted to   only the typical symptoms of the disease accord-   ing to our KG . Although this transformation can   potentially affect the recall of atypical symptoms ,   we find it significantly improve the annotation effi-   ciency and agreement in our preliminary tests .   We then invite volunteers for the annotation   tasks , who are all well - educated and also include   professional psychiatrists . To ensure the data qual-   ity , each sentence is annotated by 3 participants ,   and we also utilize a series of quality control proto-   cols . The annotation proceeds as follows :   1.Training Session : We train the annotators   about the annotation rules and also demon-   strate some example annotations by ourselves   through video meetings .   2.Screening Tests : We collect test questions   from samples on which the authors have con-   sensus in preliminary annotations . The invited   volunteers need to first annotate on these ques-   tions and achieve certain score to be eligible   for further annotation . They can take the test   for several times , and we require them to read   the reason for our decision after the test for   their better alignment with our requirements .   3.Annotation : Those who passed the screening   tests can proceed for further annotation . Dur-   ing annotation , they can always discuss with   us about any questions encountered.99734.Sampling Inspection : At intervals , we will   sample 10 % of the completed annotations   from an annotator for checking . We will   correct any annotations we find inappropri-   ate , and give a score according to the num-   ber of corrections . If the score is below cer-   tain threshold , all annotations in the checking   batch will be rejected for re - labeling .   Finally , we recruited 31 volunteers contributed   valid annotations for 8,554 sentences . The average   Fleiss ’s κfor the relevance judgement of all 38   symptoms is 0.7708 , and the κof status inference   is 0.2518 . Among all symptoms , Anxious Mood   has the most labels ( 1764 ) , while Avoid Stimuli has   the least ( 78 ) . For status inference , the ‘ Uncertain ’   annotation constitutes 13.75 % for all annotations .   More details are provided in Appendix A.4 .   3.4 Labels and Data Splits   To merge the multiple annotations into a single   gold label for each sentence , we consider a symp-   tom to be relevant to a sentence as any one of the   annotation is positive , and we use the portion of un-   certain annotations as the label for status inference   ( more discussion in § 4.2 ) . We split the dataset into   training / validation / testing set by 5:1:4 to preserve   enough samples for all classes in the test set for a   stable evaluation .   To allow models to accurately identify symp-   toms from all posts of social media , where the ma-   jority of them are not related to any mental disease   symptoms , we also collect such posts ( referred to   ascontrol posts ) for PsySym from the same can-   didate pool . We randomly sample posts whose   author does n’t have any post or comment in men-   tal health related subreddits , and further remove   posts that contain any mental health related terms   provided by Cohan et al . ( 2018 ) . Finally , we ran-   domly sample sentences from the remaining posts ,   resulting in 83,779 sentences , distributed into train-   ing / validation / testing set by 5:1:4 .   3.5 Disease Detection Dataset   To demonstrate the helpfulness of PsySym for   the downstream task of MDD , we also construct   a dataset by reimplementing the data collection   method of SMHD ( Cohan et al . , 2018 ) . We find   diagnosed users by the pattern - matching of two   components : one that indicates a self - reported di-   agnosis ( e.g. “ diagnosed with ’ ) , and another that   maps relevant keywords to the 9 mental diseases(e.g . “ panic disorder ” to Anxiety ) . A user is la-   beled with a disease if one of its keywords occurs   within 40 characters of the diagnosis pattern . Con-   trol users are randomly sampled from those who   never posted or commented in mental health re-   lated subreddits . Similar to SMHD , we eliminate   the diagnostic posts from the dataset to prevent the   direct leakage of label , but we do n’t remove other   mental health related posts to allow the extraction   of symptom - related features . The final dataset con-   sists of 5,624 diagnosed users and 20,981 control   users with average number of posts per user be-   ing 102.5 and 119.4 , respectively . We provide the   distribution of each disease in Appendix A.4 .   4 Models   This section will introduce the proposed models   for the two sub - tasks of symptom identification :   relevance judgment andstatus inference that can   leverage the data from multiple diseases simulta-   neously , and how to leverage the above models for   mental disease detection .   4.1 Symptom Relevance Judgment   The task of relevance judgment can be viewed as a   multi - label classification problem , where we need   to predict for each symptom if it is relevant to the   sentence . However , when we want to train the   model on the annotations from different diseases ,   we will face the problem of missing labels , as we   will not annotate the relevance of symptom sin the   dataset of disease difsis not considered to be a   typical symptom of d. A naive solution is to treat   all such missing labels as negative . However , since   the co - existence of multiple diseases on the same   person is not uncommon , it is likely that the typical   symptom of other diseases will also be present .   Therefore , such solution will lead to false negatives   and harm the model performance .   Inspired by Fonseca et al . ( 2020 ) and Gururani   and Lerch ( 2021 ) , we experimented with two tech-   niques to address the problem of missing labels .   Loss Masking Missing labels will be ignored   during the loss calculation . This can prevent the   model from learning incorrect negative labels , but   also restricted the exploitation of the true negatives   among missing labels .   Label Enhancement This method involves two-   stage trainings of a teacher model and a student   model . First , a teacher model is trained using Loss9974Masking . Then we use the teacher model to predict   the probabilities of the missing labels in the train-   ing set . If the predicted probability of a symptom   is lower than certain threshold , we change its label   to negative , otherwise it will still be treated as miss-   ing . A second student model will be trained on the   enhanced labels together with Loss Masking . In   contrast to previous works which set the threshold   to enhance the missing labels with top k%confi-   dence , we search for the cutting point where the   teacher model can achieve 90 % True Negative Rate   on the ROC curve of existing annotations as the   threshold for each symptom , which can quantita-   tively guarantee the quality of the enhanced labels .   Overall , we use a BERT - based encoder ( Devlin   et al . , 2018 ) with a linear layer on top of the rep-   resentation of [ CLS ] to predict the probabilities of   all symptoms with a sigmoid activation , and train   the model with binary cross entropy loss against   the labels adjusted with the methods above .   When trained on PsySym with control posts , the   especially unbalanced positive / negative ratio can   make the training hard . We thus additionally im-   plement a Balanced Sampler which samples equal   amount of annotated sentences and control sen-   tences for each batch .   4.2 Symptom Status Inference   Status Inference aims to predict if the symptoms rel-   evant to the sentence are truly present instead of be-   ing a negation ( e.g. denying or recovery ) or an un-   certain guess . Therefore , it is natural to formulate   it as a single - label binary classification problem .   However , due to the ambiguity in the expression ,   lack of context and the different understandings   of annotators , the agreement on the status labels   are relatively low ( § 3.3 ) despite our quality control   efforts . Consequently , even if we can derive binary   labels from the majority voting of annotators , mod-   els trained on such ambiguous targets will hardly   show satisfying performance .   However , we may not simply attribute such dis-   agreement to poor annotation quality , since there   is inherent ambiguity in the annotations of natural   language inference tasks , as is reported by Nie et al .   ( 2020 ) . We can still make reasonable probabilistic   estimation of the status by embracing the ambiguity   and directly learn from the annotation distribution   ( Meissner et al . , 2021 ) . Therefore , we change the   learning target from binary labels to the portion of   annotators who label the status as uncertain , andthe possible values are thus 0 , 1/3 , 2/3 and 1 . Then   we use BERT - based model with sigmoid activation   and cross entropy loss to predict the non - binary   labels .   4.3 Mental Disease Detection   The task of MDD is to predict if a user suffers   from certain mental diseases with his / her posting   history . Figure 3 illustrates the proposed symptom-   assisted MDD pipeline . First , we utilize the pre-   dicted probabilities from relevance judgment mod-   els trained on PsySym as the extracted symptom   features .   Next , to further improve the symptom features ,   we also introduce subject andstatus feature . The   subject feature is a binary variable indicating if the   discussed symptoms of a post is about the poster   himself . We calculate this feature by counting the   mentions / pronouns of other people and the use of   first person pronouns , and set the feature as 1 if   the latter is no less than the former . The status   feature is the predicted probability of the status   inference model that the symptoms are present . It   is obvious that a post not about the poster himself   or exhibiting symptoms clearly should not count   much to disease detection . We thus experiment   with the Reweighting approach similar to Karmen   et al . ( 2015 ) to incorporate them into the symptom   features :   f = p×w×w ( 1 )   where pis the probabilities predicted by the rel-   evance model ; w is the probability predicted   by the status model ; w = 0.9if the subject is   the poster , otherwise w = 0.1 .   To conduct MDD , we incorporate these features   into the model proposed by Nguyen et al . ( 2022 ) .   This model utilizes CNN of various kernel sizes   on top of the sequence of feature vectors extracted   from a user ’s posting list to aggregate the informa-   tion from consecutive posts . The features can either   be pure - text features like the sentence embeddings   from pretrained BERT , or the proposed symptom   features ( denoted as Symp below ) . Note that the9975symptom features can be much more condense than   pure - text features ( 38 - dim symptom probabilities   versus 768 - dim BERT embedding ) .   Finally , for the ease of explaining MDD results   with symptoms ( § 5.5 ) , we may need binary deci-   sion on the their presence . To achieve this , we use   0.5 to threshold on the reweighted symptom fea-   tures , where the re - weighting procedure can help   eliminate posts that should not be counted for the   diagnose .   5 Experiments   In this section , we present experimental results to :   ( 1 ) exhibit the benefits brought by PsySym ’s design   choices such as multi - disease modeling , and the   incorporation of status inference for symptom iden-   tification . ( 2 ) examine the effectiveness of symp-   tom features for MDD . ( 3 ) demonstrate the inter-   pretability enabled by symptom identification for   MDD .   5.1 Methods of Comparison   For all prediction tasks , we mainly compared the   proposed methods with 2 types of baselines : TF-   IDF+LR is a representative non - deep learning   method which utilizes TF - IDF to extract textual fea-   tures , followed by a Logistic Regression model for   prediction . BERT / MBERT uses pretrained , base   size of BERT and MentalBERT ( Ji et al . , 2021 ) ,   which can establish a strong baseline . More de-   tails like hyperparameter settings can be seen in   Appendix B.   5.2 Symptom Relevance Judgment   For symptom relevance judgment , we first con-   duct experiments on PsySym without control posts   mainly to check the effectiveness of different mod-   eling choices . We report the performance in Table   2 according to the threshold - free metric AUC , av-   eraged across each symptom class in the subset of   each disease .   The single disease methods on the first 3 rowsleverage models trained separately on the each dis-   ease subset . We can see that BERT significantly   outperforms TF - IDF+LR , while the further pre-   training on mental - health corpus done by MBERT   can bring additional improvement . We thus use   MBERT in the following experiments . The last 3   multi - disease methods only train one model on the   combined dataset of all diseases , where we will   tackle the problem of missing labels with the tech-   niques introduced in § 4.1 . Comparing the third   and fourth row , we can see that the multi - disease   model ’s performance drops with the default strat-   egy of treating all missing labels as negative . How-   ever , with Loss Masking , the multi - disease model   can now outperform the single disease counter-   part , and Label Enhancement brings additional   gain . This proves our hypothesis that the simul-   taneous modeling of multi - disease data can help   improve the relevance judgment performance .   In order to predict symptom features for gen-   eral user posts , we then train a relevance model   on PsySym with control posts , leveraging the addi-   tional balanced sampler ( § 4.1 ) . Its AUC is 98.54   and F1 ( with threshold 0.5 ) is 67.03 , averaged   across 38 symptoms on the full test set contain-   ing all diseases and control posts , while directly   transfer the model not trained with control posts   would lead to only 30.53 F1 .   5.3 Symptom Status Inference   Since the status inference model is trained with the   non - binary targets of annotation distribution , we   use Mean Absolute Error ( MAE ) as the evaluation   metric . To get a better grounding for understanding   the model performance , we establish a no - model   baseline as the performance lower bound , using the   mean probability in the test set as the prediction for   all samples . We also use the expected MAE of a   single annotator to estimate the performance upper   bound . We train a Mental - BERT based model for   status inference , achieving 0.1360 MAE , compared   to a lower bound of 0.1940 and an upper bound of   0.1172 , which indicates a plausible performance .   5.4 Mental Disease Detection   We show MDD performance in Table 5 and com-   pare the performance of methods utilizing pure   text and symptoms . The training and evaluation for   each disease is conducted in a binary setting , where   the model needs to distinguish the diagnosed users   of that disease and the control users , so users with9976   other diseases will not be involved . We then report   the average F1 across all 9 diseases .   We can see that Symp outperforms all pure - text   methods including the strong BERT model , sug-   gesting the usefulness of symptom features for   MDD . The Reweight method that incorporates sta-   tusandsubject feature into symptom feature can   bring further improvement , indicating that these ad-   ditional aspects can help properly decide symptom   risks for better MDD .   5.5 Case Study on Interpretability   One of the major goals of symptom identification   is to enable machine learning models to provide   explanations for disease diagnoses just as human   psychiatrists . The binarized prediction of symp-   toms of our model , and the disease - symptom re-   lations from our knowledge graph ( derived from   clinical manuals ) can help achieve this . We provide   concrete examples below .   Table 3 shows that , for a patient predicted to have   OCD , the symptom model with reweighting ( § 4.3 )   can find all typical OCD symptoms from his / her   posting history , and thus justify the diagnose .   Being able to interpret symptoms can also help   us spot spurious diagnosis . Here we use the model   to examine the correctness of disease labels pro-   duced by pattern matching based automatic method   ( § 3.5 ) . Despite its careful design , it can still makeboth false - positive and false - negative errors , as has   been reported by Cohan et al . ( 2018 ) . Such prob-   lematic labels can negatively affect the generaliz-   ability of the model trained on them ( Ernala et al . ,   2019 ) , but they can be hard to detect .   Symptom - based explanations may provide an ef-   ficient way to detect these false labels . As is shown   in Table 4 ( left ) , we can easily find and remove the   falsely labeled diseases when there are few or no   history of their corresponding symptoms . For the   example at right , Anxiety is missed by the label-   ing method , while the high prevalence of anxiety   symptoms like anxious mood and social anxiety   predicted from the user ’s post can indicate its pres-   ence . Therefore , the interpretability brought by   symptoms can justify correct predictions and may   further serve as reference for human correction of   labels .   5.6 Evaluation of Candidate Retrieval   Strategy   In this section , we show the benefits of the pro-   posed embedding - based candidate retrieval strat-   egy with quantitative experimental results , com-   pared with the keyword / pattern matching methods   commonly used in previous works ( Mowery et al . ,   2017 ; Yadav et al . , 2020 ) . Specifically , we test the   effectiveness of different strategies in retrieving the   positive samples among all annotated sentences of   PsySym , and report the average precision andre-   callacross 38 symptoms . A low recall would harm   the diversity of the annotated corpus , as we would   be unable to annotated on sentences supposed to be   symptom relevant , while a low precision will lead   to low annotation efficiency , since we will have   to read through more posts in order to find those   actually convey the symptom.9977The compared methods are :   MeSH The Medical Subject Headings ( MeSH )   thesaurus is a large - scale vocabulary of medical   terms produced by National Library of Medicine   ( NLM ) . We use the entity linker in scispacy ( Neu-   mann et al . , 2019 ) to detect all MeSH terms ( with   alias ) related to somatic symptoms and mental sta-   tus in the sentence , and sentence with any single   matched term will be retrieved . We tried to esti-   mate its recall upper bound by greedily including   all symptom terms so that some of them may be   beyond the scope of mental health , and we thus   do n’t report its precision .   LIWC ( negemo ) Linguistic Inquiry and Word   Count ( LIWC ) ( Pennebaker et al . , 2001 ) is a cate-   gorized vocabulary that can provide useful dimen-   sions to analyze a person ’s thoughts , feelings , per-   sonality from language use . It has been shown   in many works that some dimensions of LIWC   are closely related to mental disorders Shen et al .   ( 2017 ) ; Cohan et al . ( 2018 ) , especially the Nega-   tive Emotion ( negemo ) words that include sadness ,   anxiety and anger related words , which are also   symptoms in our KG . To leverage LIWC ( negemo ) ,   we simply retrieve all sentences that contain any   single negative emotion words .   SBERT This is our proposed method for candi-   date retrieval ( § 3.2 ) , which leverages embedding   similarity instead of keyword matching used in the   previous two methods . We study two variants of   this method . SBERT ( manual only ) is our first   attempt that only uses the symptom description   collected from DSM-5 and clinical questionnaires .   SBERT ( manual+post ) additionally incorporates   representative posts of a symptom for some symp-   tom classes that we found to have poor retrieval   results with the previous methods . To get a binary   retrieval decision for the calculation of precision   and Recall , we use 0.5 to threshold on the calcu-   lated cosine similarity . This does not totally reflect   the actual setting in our data collection , but allows   a direct comparison with previous methods .   We can see from Table 6 that the recall of MeSH   is not high despite our greedy inclusion of match-   ing terms . This suggests can keyword matching   with professional terms ( even with alias in MeSH )   failed to identify the diverse expressions of symp-   toms on social media potentially due to its figura-   tive language ( Yadav et al . , 2020 ) . LIWC ( negemo )   received a relatively high recall at the expense of   precision . The possible reason is that the negative   emotion words are too broad and does not target   certain symptom . SBERT ( manual only ) is enough   to achieve both satisfying precision and recall . The   symptom - specific descriptions can precisely de-   tect candidates for each symptom , and embedding   based retrieval can overcome the limitation of exact   word matching methods , be tolerant to misspelling   and synonyms , and recall sentences expressing the   semantics of a symptom but with no specific key-   words . SBERT ( manual+post ) can further improve   the recall while almost preserving the precision .   This indicates that the inclusion of posts can allevi-   ate the mismatching between manual and candidate   posts in the language style and the perspective ( ob-   servation versus self expression ) .   6 Conclusions   In this work , we introduce PsySym , the first anno-   tated multi - disease symptom identification dataset   based on social media posts . A novel annotation   framework is proposed to guarantee the diversity   and quality of the annotations . PsySym defined   two sub - tasks of symptom identification : relevance   judgment and status inference , to enable a more   comprehensive analysis . Strong baselines are es-   tablished on the two sub - tasks with multi - disease   modeling techniques that can properly handle miss-   ing labels and distribution - targeted learning to   deal with the ambiguity in status inference . With   PsySym - trained models , symptom - assisted MDD   method can outperform strong pure - text baselines .   Qualitative examples also demonstrate the explain-   ability enabled by symptom predictions for MDD.9978Limitations   There are some limitations to this study that could   be addressed in future research .   First , although we tried our best to improve   the diversity of the annotated sentences with   embedding - based retrieval methods that can find   symptom expressions without standard keywords .   There can still be blind points we can not cover ,   such as the posts outside mental health related sub-   reddits , and those can not be found due to the limi-   tations of the retrieval model itself .   Moreover , there are some types of symptoms we   are unable to annotate due to the characteristics   of our data source . For example , hallucination ( a   symptom of schizophrenia ) usually requires the ob-   servations from another person to be identified , and   can hardly be found on Reddit where user mainly   shares subjective experience . The fact that Reddit   is dominated by adult users ( Gjurkovi ´ c et al . , 2021 )   also prevents us from finding the typical symptoms   of autism and ADHD among children .   Last but not least , our dataset does not include   useful signals from modalities other than text . For   instance , the time pattern of posting may also reveal   the symptom of insomnia , and the features of the   user ’s ego centric network may show the troubles   in his / her social relations ( De Choudhury et al . ,   2013 ) . The faces and colors of the posted image   may also help identify depression ( Gui et al . , 2019 ) .   If videos or sounds can be leveraged , the acoustic   features of speech can help recognize the emotions   like sadness , fear and anger ( Busso et al . , 2008 )   for better detection of mental diseases ( Wu et al . ,   2022 ) .   Ethics Statement   Annotation We pay the annotators a fair wage   above the minimum requirement . If workers have   any questions or concerns , we will respond to them   immediately . Since the content involves the expres-   sion of mental disease symptoms , we may expect   negative effects on the annotators . Therefore , the   annotators can freely take breaks or quit the task   at anytime . We also interviewed some annotators   about their feeling after annotation . They only re-   ported slight discomfort at the time of reading sad   or frightening posts due to empathy , and they found   no long - term negative effects on them .   Application Mental disease detection can be re-   lated to some sensitive topics , so we should becareful with its applications . First , since mental   diseases like depression are still not well under-   stood or even stigmatized in many regions , im-   proper usage of MDD techniques may do harm   to the users . Moreover , the precision and recall   of the algorithm is far from prefect . It may make   false / missing diagnoses which can prevent the user   from getting proper treatment , but may still be an   useful auxiliary tool for those who are unaware of   their mental conditions or can not access mental   services . Therefore , the predictions of the model   should be carefully re - examined by professionals   for a confirmed diagnosis , where the symptom pre-   diction results may facilitate quick inspection when   served as the disease - specific summary of the long   posting history . Also posts demonstrated in this   paper are paraphrased for anonymity . We will also   require the users of PsySym to comply with a data   usage agreement to prevent the invasion of privacy   or other potential misuses .   Acknowledgements   Kenny Q. Zhu is partially supported by SJTU-   CMBCC Joint Research Scheme and SJTU-   Meituan Joint Research Scheme . Mengyue Wu   is supported by National Natural Science Foun-   dation of China ( Grant No.61901265 , 92048205 )   and Shanghai Municipal Science and Technology   Major Project ( 2021SHZDZX0102 ) .   References99799980   A Data Construction Details   A.1 Candidate Selection Algorithm   Previous attempts to establish symptom annotation   dataset ( Mowery et al . , 2017 ) have shown that the   distribution of different symptoms can be highly   unbalanced , with many classes having too few pos-   itive samples to train a plausible classifier . On the   other hand , we found in our preliminary annota-   tions that it ’s hard to achieve high agreement on   atypical expressions of symptoms . Therefore , to   encourage a more balanced symptom distribution ,   while retrieving the most typical symptom posts   for high annotation agreement , we designed a pri-   ority queue based algorithm for the selection of   candidate posts with embedding similarity ( § 3.2):1.For the annotation of a disease d , we set up a   queue with max capacity of 300 sentences for   each symptom of d   2.For each sentence xind ’s related subreddits ,   and each symptom sofd , we estimate the x ’s   relevance to swith its max similarity with all   sub - symptoms of s. We add xinto the queue   ofs , if it is not full , or x ’s relevance is larger   than the minimum one in s   3.We aggregate the sentences from all queues ,   and perform deduplication with MinHash and   Local Sensitive Hashing ( LSH ) ( Leskovec   et al . , 2020 ) to eliminate identical or similar   sentences . The remaining posts will be used   as the annotation candidates for d.   Since we only use sentences with the highest es-   timated relevance to symptoms , the selected posts   tend to show more typical symptom expressions .   The equal size of each symptom queue can further   alleviate the class imbalance problem ( see class   distribution in Table 8) ( but not totally eliminate   it , because of the error of relevance approximation ,   the potential overlapping between queues and the   inherent unbalanced distribution with in the origi-   nal posts . )   A.2 Status Annotation   Initially , the symptom status is annotated on the   basis of each symptom in a sentence . The reason   is that a sentence can express multiple symptoms   with different status . For example , in “ I do n’t have   insomnia any more , but I ’m still depressed . ” , the   status of the previous symptom is negative while   that of the latter is positive . However , after annota-   tion , we find that the negative status annotation for   each symptom is scarce . We then consider merging   all symptom - level status into sentence - level . The   motivation are two - folds . First , we observed that   most sentences with negative status share similar   characteristics like having negation words or being   a question . Moreover , we found that the conflicts   between status annotations of different symptoms   in the same sentence is rare , accounting for only   2.8 % of the sentences . We thus set the sentence-   level status as negative if there is any negative status   annotation in its relevant symptoms . If not speci-   fied , the PsySym discussed in this paper refers to   the version with sentence - level status labels.9981   A.3 Quality Control   Score Calculation One core component of our   quality control measures is the annotation qual-   ity score . Given the annotations and references   ( judgment from the authors ) , the quality score is   calculated as the symptom - level Fscore between   them . The βis set to 2 to put more emphasis on   recall .   The score is used in several phases of the annota-   tion 3.3 . In Screening Tests , the annotators need toachieve above 75 score to be qualified to conduct   further annotation . We will also calculate the score   inSampling Inspection , and reject all annotations   in the checking batch if the score is below 60 . To   motivate accurate annotation , the volunteers will   be rewarded with higher subsidy for high scores   achieved according to our inspection .   Annotation Interface The annotation interface   is a customized webpage with many designs to   promote efficient and accurate annotations . First ,   the annotator will be enter a disease description   page , where all symptoms and their descriptions   are shown for annotators to get familiar with . They   will then enter the annotation page ( Figure 4 ) . All   symptom names are shown in list below the sen-   tence to annotate . Annotators can click on the ques-   tion mark beside the symptom to expand its descrip-   tions , which can serve as a quick reminder . We will   also make a symptom bold , if the sentence hit any   keywords in the symptom descriptions , which can9982   help the annotators quickly focus on likely classes ,   and also prevent them from missing annotations in   case the annotators are unaware of some items in   the symptom descriptions .   A.4 Detailed Data Statistics   We show the number of annotated sentences for   each disease in Table 7 , where diseases with more   symptoms will have more annotations . We also   show the number of annotations and the inter-   annotator agreement by each symptom in Table   8 . It can be seen that almost all symptom classes   received reasonable amount of annotations , and   have high agreement , indicating the effectiveness   of proposed annotation framework to guarantee the   quality of PsySym . We provide more examples in   Table 11 .   For the disease detection dataset ( § 3.5 ) , we also   provide the number of users suffering from each   disease in Table 9 . The distribution of the 9 dis-   eases are similar to SMHD ( Cohan et al . , 2018 ) .   B Detailed Experimental Settings   For all models , we empirically set hyperparameters   following existing implementations and previous   works without fine - tuning them for optimized per-   formance . Specifically , we use a learning rate ( lr )   of 3e-4 , max sequence length of 64 for BERT and   MBERT models used for symptom relevance judg-   ment and status inference . For the CNN model   used in mental disease detection , the structure is   identical to that of Nguyen et al . ( 2022 ) , the lr is   0.01 when using symptom features only and 0.003   when using BERT embeddings or the combined   features . The posting list will be truncated to pre-   serve the earliest 256 posts at most . We also em-   ploy early - stopping with a patience of 4 epochs ac-   cording to validation performance to prevent over-   fitting . For efficiency of inference on the MDD   dataset with large amount of posts , we use bert - tinymodels trained on PsySym to extract the symptom   and status features , whose performance is close to   the best performing MBERT model ( AUC=98.80 ,   F1=62.28 for relevance judgement with control   posts and MAE=0.1509 for status inference ) . For   the multi - task learning of multiple symptoms , the   losses of each classes are the simple arithmetic   mean of them without weighting .   For each post , we conduct several preprocess-   ing steps . First , they are split into sentences with   blingfire . We will then eliminate sentences like   “ [ Removed ] ” . We also use regular expressions to   detect the hyperlink format like “ [ anchor text](web   url ) ” , and transform it into only anchor text.9983Disease TF - IDF BERT Symp Symp ( Reweighting )   Depression 68.95 71.58 66.67 69.88   Anxiety 66.48 71.08 71.82 71.39   ADHD 59.55 60.05 60.14 60.45   Bipolar Disorder 66.67 43.56 67.76 68.82   OCD 26.09 44.83 48.33 56.52   PTSD 30.43 27.45 48.48 45.9   Eating Disorder 11.11 37.04 52.17 46.15   Autism 30.23 49.59 35.64 37.04   Schizophrenia 34.04 57.97 48.15 57.63   Mean ( 7 Diseases ) 47.04 50.80 59.34 59.87   Mean ( 9 Diseases ) 43.73 51.46 55.46 57.09   C Detailed Disease Detection Results   We show the MDD performance of different meth-   ods on disease level in Table 10 . It can be seen   that the proposed symptom - based methods perform   slightly worse than BERT on the 2 diseases not in-   cluded in PsySym , which is reasonable . When   comparing only the 7 PsySym diseases the advan-   tage of Symp over BERT is even more significant ,   which further demonstrate the strength of the pro-   posed method . We also observe a more significant   improvement on diseases with fewer positive sam-   ples like OCD , PTSD and Eating Disorder , suggest-   ing the usefulness of symptom knowledge for the   MDD in low - resource scenarios .   D Per - Symptom Analysis   We further conduct analysis on the identification   performance of each symptom to gain insights on   their varied difficulty . We show the classification   AUC and F1 ( given threshold=0.5 ) on these symp-   toms with our best performing model MBERT ( la-   bel enhance ) model trained on data with control   posts in Figure 5 . We can see that AUC and F1 are   strongly correlated ( Pearson ’s r=0.78 ) , and from   F1 we can clearly see that many symptom classes   still have abundant room for improvement . Inter-   annotator agreement ( κin Table 8) can also affect   the identification performance , with Pearson ’s r   between κand F1 = 0.86 . This is because some   classes are inherently more ambiguous than others ,   which makes both human agreement and model   classification difficult . For example , as the first   row of Table 11 shows , the symptom “ Hyperactiv-   ity ” and “ Inattention ” do not have explicit terms   in the post and needs to be inferred , which makesthem difficult to identify . However , we can easily   find “ Gastrointestinal Symptoms ” in the second   row with the phrase “ stomach aches”.99849985