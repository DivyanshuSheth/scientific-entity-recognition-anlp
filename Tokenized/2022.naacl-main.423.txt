  Ghazi Felhi , Joseph Le Roux   LIPN   Université Sorbonne Paris Nord - CNRS UMR 7030   F-93430 , Villetaneuse , France   { felhi , leroux}@lipn.frDjamé Seddah   INRIA Paris   Paris , France   djame.seddah@inria.fr   Abstract   We propose a generative model for text genera-   tion , which exhibits disentangled latent repre-   sentations of syntax and semantics . Contrary to   previous work , this model does not need syntac-   tic information such as constituency parses , or   semantic information such as paraphrase pairs .   Our model relies solely on the inductive bias   found in attention - based architectures such as   Transformers .   In the attention of Transformers , keys handle   information selection while values specify what   information is conveyed . Our model , dubbed   QKV AE , uses Attention in its decoder to read   latent variables where one latent variable infers   keys while another infers values .   We run experiments on latent representations   and experiments on syntax / semantics transfer   which show that QKV AE displays clear signs   of disentangled syntax and semantics . We also   show that our model displays competitive syn-   tax transfer capabilities when compared to su-   pervised models and that comparable super-   vised models need a fairly large amount of data   ( more than 50 K samples ) to outperform it on   both syntactic and semantic transfer . The code   for our experiments is publicly available .   1 Introduction   Disentanglement , a process aimed at obtaining neu-   ral representations with identified meaning , is a   crucial component of research on interpretability   ( Rudin et al . , 2022 ) . A form of disentanglement   that received a lot of interest from the NLP commu-   nity is the separation between syntax and semantics   in neural representations ( Chen et al . , 2019 ; Bao   et al . , 2019 ; Zhang et al . , 2019 ; Chen et al . , 2020 ;   Huang and Chang , 2021 ; Huang et al . , 2021 ) . Pre-   vious works perform disentanglement using para-   phrase pairs as information for semantics , and/or   constituency parses as information for syntax . Thedependence of models on labeled data is known to   entail high cost ( see Seddah et al . , 2020 on syntactic   annotation ) , and to often require new labels to han-   dle problems such as concept drift ( Lu et al . , 2019 )   and domain adaptation ( Farahani et al . , 2021 ) .   In light of the above , we propose an unsuper-   vised model which directs syntax and semantics   into different neural representations without se-   mantic or syntactic information . In the Trans-   former architecture ( Vaswani et al . , 2017 ) , the   attention mechanism is built upon a query from   a set Q , which pools values Vthrough keysK.   For each query , values are selected according to   their matching score computed by the similarity   between their corresponding keys and the query .   Building on an analogy between the ( K , V ) cou-   ple and syntactic roles with their lexical realiza-   tions ( explicited in § 4.2 ) we present QKV AE , a   Transformer - based Variational Autoencoder ( V AE ;   Kingma and Welling , 2014 ) .   To build our model , we modify a previous   Transformer - based V AE , called the Attention-   Driven V AE ( ADV AE ; Felhi et al . , 2021 ) . Using   Cross - Attention , our model encodes sentences into   two latent variables : zto infer values for V , and   zto assign keys in Kfor values in V. These   keys and values are then used in the Attention mech-   anism of a Transformer Decoder to generate sen-   tences . We show that ztends to contain syn-   tactic information , while ztends to represent   semantic information . Additionally , comparisons   with a supervised model show that it needs a con-   siderable amount of data to outperform our model   on syntactic and semantic transfer metrics .   Our contributions can be summarized as follows :   •We describe QKV AE , a model designed to dis-   entangle syntactic information from semantic   information by using separate latent variables   for keys and values in Transformers Attention.5763•We run experiments on a dataset for English   which empirically show that the two types   of latent variables have strong preferences re-   spectively for syntax and semantic .   •We also show that our model is capable of   transferring syntactic and semantic informa-   tion between sentences by using their respec-   tive latent variables . Moreover , we show   that our model ’s syntax transfer capabilities   are competitive with supervised models when   they use their full training set ( more than 400k   sentences ) , and that a supervised model needs   a fairly large amount of labeled data ( more   than 50k samples ) to outperform it on both   semantic and syntactic transfer .   2 Related Work   We broadly divide works on explainability in NLP   into two research directions . The first seeks post   hocexplanations for black - box models , and led to   a rich literature of observations on the behavior of   Neural Models in NLP ( Tenney et al . , 2019 ; Jawa-   har et al . , 2019 ; Hu et al . , 2020 ; Kodner and Gupta ,   2020 ; Marvin and Linzen , 2020 ; Kulmizev et al . ,   2020 ; Rogers et al . , 2020 ) . Along with these ob-   servations , this line of works also led to numerous   advances in methodology concerning , for instance ,   the use of attention as an explanation ( Jain and   Wallace , 2019 ; Wiegreffe and Pinter , 2020 ) , the   validity of probing ( Pimentel et al . , 2020 ) , or con-   trastive evaluation with minimal pairs ( Vamvas and   Sennrich , 2021 ) . The second research direction on   explainability in NLP seeks to build models that   are explainable by design . This led to models with   explicit linguistically informed mechanisms such   as the induction of grammars ( RNNG ; Dyer et al . ,   2016 , URNNG ; Kim et al . , 2019 ) or constituency   trees ( ON - LSTM ; Shen et al . , 2019 , ONLSTM-   SYD ; Du et al . , 2020 ) .   Disentangled representation learning is a sub-   field of this second research direction which aims   at separating neural representations into neurons   with known associated meanings . This separation   was performed on various characteristics in text   such as style ( John et al . , 2020 ; Cheng et al . , 2020 ) ,   sentiment and topic ( Xu et al . , 2020 ) , or word mor-   phology ( Behjati and Henderson , 2021 ) . In works   on disentanglement , consequent efforts have been   put in the separation between syntax and semantics ,   whether merely to obtain an interpretable special-   ization in the embedding space ( Chen et al . , 2019;Bao et al . , 2019 ; Ravfogel et al . , 2020 ; Huang et al . ,   2021 ) , or for controllable generation ( Zhang et al . ,   2019 ; Chen et al . , 2020 ; Huang and Chang , 2021 ;   Li et al . , 2021 ) . However , all these works rely on   syntactic information ( constituency parses and PoS   tags ) or semantic information ( paraphrase pairs ) .   To the best of our knowledge , our work is the first   to present a method that directs syntactic and se-   mantic information into assigned embeddings in   the challenging unsupervised setup .   From a broader machine learning perspective ,   using knowledge of the underlying phenomena in   our data , we design our model QKV AE with an in-   ductive bias that induces understandable behavior   in an unsupervised fashion . Among the existing   line of applications of this principle ( Rezende et al . ,   2016 ; Hudson and Manning , 2018 ; Locatello et al . ,   2020 ; Tjandra et al . , 2021 ) , ADV AE ( Felhi et al . ,   2021 ) , the model on which QKV AE is based , is de-   signed to separate information from the realizations   of different syntactic roles without supervision on   a dataset of regularly structured sentences .   3 Background   In this section , we go over the components of our   model , namely V AEs , attention in Transformers ,   and ADV AE , the model on which QKV AE is based .   3.1 VAEs as Language Models   Given a set of observations w , V AEs are a class of   deep learning models that train a generative model   p(w ) = /integraltextp(z)p(w|z)dz , where p(z)is a prior   distribution on latent variables zthat serve as a   seed for generation , and p(w|z)is called the de-   coder and generates an observation wfrom each   latent variable value z. Since directly maximizing   the likelihood p(w)to train a generative model is   intractable , an approximate inference distribution   q(z|w ) , called the encoder , is used to formulate   a lower - bound to the exact log - likelihood of the   model , called the Evidence Lower - Bound ( ELBo ):   logp(w)≥   E[logp(w|z)]−   KL[q(z|w)||p(z ) ] = ELBo ( w;z)(1 )   Early works on V AEs as language models have   shown that , contrary to non - generative sequence-   to - sequence ( Sutskever et al . , 2014 ) models , they   learn a smooth latent space ( Bowman et al . , 2016 ) .   In fact , this smoothness enables decoding an inter-   polation of latent codes ( i.e.a homotopy ) coming5764from two sentences to yield a well - formed third   sentence that clearly shares characteristics ( syntac-   tic , semantic . . . ) with both source sentences . This   interpolation will be used as a control baseline in   our experiments .   3.2 Attention in Transformers .   The inductive bias responsible for the disentangle-   ment capabilities of our model is based on the de-   sign of Attention in Transformers ( Vaswani et al . ,   2017 ) . In attention mechanisms , each element of   a series of query vectors Q={q , . . . , q}per-   forms a soft selection of values V={v , . . . , v }   whose compatibility with the query is given by their   corresponding key vector in K={k , . . . , k }   via dot product . For each q∈Q , the series of dot   products is normalized and used as weights for a   convex interpolation of the values . Formally , the   result is compactly written as :   Attention ( Q , K , V ) = Softmax ( QK)V(2 )   Here , we stress that Kis only capable of con-   trolling what information is selected from V , while   Vis responsible for the value of this information .   Using the above operators and the embedding level   concatenation operator Cat , Multi - Head Attention   ( MHA ) in Transformers is defined as follows :   MHA ( ˜Q,˜K,˜V ) = Cat ( head , ... head)W   s.t : head= Attention ( ˜QW,˜KW,˜V W )   Where W , W , W , and Ware trainable pa-   rameter matrices . In turn , Self - Attention ( SA ) and   Cross - Attention ( CA ) are defined , for sets of ele-   ments called source Sand target T , as follows :   SA(T ) = MHA ( T , T , T )   CA(T , S ) = MHA ( T , S , S )   The above SAmechanism is used to exchange   information between elements of target T , while   inCA , targets Tpull ( or query for ) information   from each element of the source S. Transformer   Encoders ( Enc ) are defined as the composition of   layers each consisting of an attention followed by   a Feed - Forward Network F :   Enc(T ) = ˜T , s.t.˜T=/braceleftbiggTifd= 0,else :   F(SA ( ˜T))Transformer Decoders ( Dec ) are defined with   instances of SA , CAandF :   Dec(T , S ) = ˜T , s.t . :   ˜T=/braceleftbiggTifd= 0,else :   F(CA(SA ( ˜T ) , S ) )   where DandDabove are respectively the   number of layers of Enc andDec . For autoregres-   sive decoding , Vaswani et al . ( 2017 ) define a ver-   sion of Dec we will call Dec. In this version , the   result of each QK(Eq . 2 ) in Self - Attention is   masked so that each tinTonly queries for infor-   mation from twithj≤i . Even though Dec yields   a sequence of length equal to that of target T , in the   following sections we will consider its output to be   only the last element of ˜Tin order to express   auto - regressive generation in a clear manner .   3.3 ADVAE   ADV AE is a Variational Autoencoder for unsuper-   vised disentanglement of sentence representations .   It mainly differs from previous LSTM - based ( Bow-   man et al . , 2016 ) and Transformer - based ( Li et al . ,   2020b ) V AEs in that it uses Cross - Attention to en-   code and decode latent variables , which is the cor-   nerstone of our model . In ADV AE , Cross - Attention   is used to : i)encode information from sentences   into a fixed number of vectorial latent variables ;   ii)decode these vectorial latent variables by using   them as sources for the target sentences generated   by a Transformer Decoder .   Formally , let us define M , M , and Mto   be linear layers that will respectively be used to   obtain the latent variables ’ means and standard   deviations , and the generated words ’ probabili-   ties , Lthe number of vectorial latent variables   z={z , . . . , z } , and finally E={e , . . . , e }   andD={d , . . . , d}two sets of Ltrainable em-   beddings . Embeddings eanddserve as fixed   identifiers for the latent variable zrespectively in   the encoder and in the decoder .   Given input token sequence w , the encoder   q(z|w ) = /producttextq(z|w)first yields parameters µ   andσto be used by the diagonal Gaussian distri-   bution of each of the latent variables zas follows:5765   ˜z= Dec ( e ; Enc ( w ) )   ∀ls.t.1≤l≤L :   µ=M(˜z ) , σ= SoftPlus ( M(˜z ) )   z∼ N(µ;σ ) ( 3 )   Cross - Attention is also used by the ADV AE   decoder to dispatch information from the source   latent variable samples to the target generated   sequence . Accordingly , using a beginning - of-   sentence token w , p(w|z ) = /producttextp(w|w , z )   yields probabilities for the categorical distribution   of the generated tokens wby decoding latent vari-   ables zconcatenated with their embeddings d :   y= Cat ( d;z )   ∀is.t.1≤i≤ |w| :   ˜w = Dec(w , . . . , w ; Enc ( y ) )   w∼Categorical(Softmax ( M ( ˜w ) ) )   4 QKVAE : Using separate latent   variables for Keys and Values   In this section , we describe the architecture of our   model , the behavior it entails , and how we deal   with the optimization challenges it poses .   4.1 QKVAE architecture   The modification we bring to ADV AE is aimed at   controlling how information is selected from the   latent space with the value of a newly introduced   latent variable . We call this latent variable z ,   and refer to the latent variables already formulated   in ADV AE as z={z , . . . , z}.zis   obtained with the same process as each z(Eq .   3),i.e.by adding an additional identifier embed-   dinge , and matrices MandMto obtain its   mean and standard - deviation parameters .   For the QKV AE Decoder , we modify the Trans-   former Decoder Dec intoQKVDec so as to use   Multi - Head Attention with separate inputs for keys   and values instead of Cross - Attention : QKVDec ( T;S;S ) = ˜T , s.t . :   ˜T=/braceleftbiggTifd= 0,else :   F(MHA(SA ( ˜T ) , S , S )   where Dis the number of layers . Similar to   Dec , we define QKVDec to be the auto - regressive   version of QKVDec . The QKV AE decoder yields   probabilities for the generated tokens by using this   operator on values given by zconcatenated with   embeddings d , and keys given by a linear transfor-   mation on z :   v= Cat ( d;z ) , k = M(z )   ∀is.t.1≤i≤ |w| :   ˜w = QKVDec ( w , . . . , w;k;v )   w∼Categorical(Softmax ( M ( ˜w ) ) )   where Mis a linear layer . While ADV AE   already uses Cross - Attention to encode and decode   latent variables , our model uses separate variables   to obtain keys and values for Multi - Head Attention   in its decoder .   4.2 QKVAE Behavior   In the Multi - Head Attention of our decoder , z   controls keys , and zcontrols values . In other   words , the value of each zis called to be passed   to the target sequence according to its key which   is given by the variable z. Therefore , given a   query , zdecides which content vector zpar-   ticipates most to the value of the generated token   at each generation step . To better get a gist of the   kind of behavior intended by this construction , we   assume in Table 1 for explanatory purposes , that   our decoder has one layer and one attention head ,   that the value of each kin key matrices kand   kcorresponds to syntactic roles , and that each v   informs on the realization of the corresponding syn-   tactic role . Table 1 displays the resulting sentence   when each of k1andk2are coupled with v.5766In the examples in Table 1 , the generator uses   a query at each generation step to pick a word in   a manner that would comply with English syntax .   Therefore , the key of each value should inform   on its role in the target structure , which justifies   syntactic roles as an adequate meaning for keys .   Although our model may stray from this possi-   bility and formulate non - interpretable values and   keys , keys will still inform on the roles of values in   the target structure , and therefore influence the way   values are injected into the target sequence . And   given the fact that our model uses multiple layers   and attention heads and the continuous nature of   keys in Attention ( as opposed to discrete syntac-   tic role labels ) , our model performs a multi - step   and continuous version of the behavior described   in Table 1 .   Injecting values into the structure of a sentence   requires the decoder to model this structure . Previ-   ous works have shown that this is well within the   capabilities of Transformers . Specifically , Hewitt   and Manning ( 2019 ) showed that Transformers em-   bed syntactic trees in their inner representations ,   Clark et al . ( 2019 ) showed that numerous atten-   tion heads attend to specific syntactic roles , and we   ( Felhi et al . , 2021 ) showed that Transformer - based   V AEs can capture the realizations of syntactic roles   in latent variables obtained with Cross - Attention .   4.3 Balancing the Learning of zandz   Similar to ADV AE , we use a standard Normal   distribution as a prior p(z ) = p(z)p(z)and   train QKV AE with the β - V AE objective ( Higgins   et al . , 2017 ) which is simply ELBo ( Eq . 1 ) with a   weight βon its Kullback - Leibler ( KL ) term . Hig-   gins et al . ( 2017 ) show that a higher βleads to   better unsupervised disentanglement . However , the   KLterm is responsible for a phenomenon called   posterior collapse where the latent variables be-   come uninformative and are not used by the de-   coder ( Bowman et al . , 2016 ) . Therefore , higher val-   ues for βcause poorer reconstruction performance   ( Chen et al . , 2018 ) . To avoid posterior collapse , we   follow Li et al . ( 2020a ): i)We pretrain our model   as an autoencoder by setting βto 0 ; ii)We linearly   increase βto its final value ( KLannealing ; Bow-   man et al . , 2016 ) and we threshold each dimension   of the KLterm with a factor λ(Free - Bits strategy ;   Kingma et al . , 2016 ) .   In preliminary experiments with our model , we   observed that it tends to encode sentences usingonlyz . As we use conditionally independent   posteriorsq(z|w)andq(z|w)for our latent   variables , their KLterms ( Eq . 1 ) can be written   seperately , and they can therefore be weighted sep-   arately with different values of β . Using a lower β   forzas was done by ( Chen et al . , 2020)did   not prove effective in making it informative for the   model . Alternatively , linearly annealing βforz   before zdid solve the issue . This intervention   on the learning process was inspired by the work of   Li et al . ( 2020c ) which shows that latent variables   used at different parts of a generative model should   be learned at different paces .   5 Experiments   5.1 Setup   Data To compare our model to its supervised   counterparts , we train it with data from the En-   glish machine - generated paraphrase pairs dataset   ParaNMT ( Wieting and Gimpel , 2018 ) . More   specifically , we use the 493 K samples used by   Chen et al . ( 2020)to train their model VGV AE .   Since our model is unsupervised , we only use the   reference sentences ( half the training set ) to train   our model . Using the development and test sets of   ParaNMT , Chen et al . ( 2020 ) also provide a curated   set of triplets formed by a target sentence ( target ) ,   a semantic source ( sem_src ) , and a syntactic source   ( syn_src ) . The semantic source is a paraphrase of   the target sentence , while the syntactic source is   selected by finding a sentence that is syntactically   close to the target ( i.e.edit distance between the   sequence of PoS Tags of both sentences is low )   and semantically different from the paraphrase ( has   low BLEU score with it ) . Contrary to paraphrases   in the training set of ParaNMT , paraphrases from   this set were manually curated . These triplets are   divided into a development set of 500 samples and   a test set of 800 samples . We display results on the   test set in the main body of the paper . The results   on the development set , which lead to the same   conclusions , are reported in Appendix A.   Training details & hyper - parameters Encoders   and Decoders in QKV AE are initialized with pa-5767rameters from BART ( Lewis et al . , 2020 ) . After   manual trial and error on the development set , we   set the sizes of zandzto 768 , and Lto   4 . Further Hyper - parameters are in Appendix B.   We train 5 instances of our model and report the   average scores throughout all experiments .   Baselines We compare our system to 4 previ-   ously published models , where 2 are supervised   and 2 are unsupervised : i ) VGVAE ( Chen et al . ,   2020 ): a V AE - based paraphrase generation model   with an LSTM architecture . This model is trained   using paraphrase pairs and PoS Tags to separate   syntax and semantics into two latent variables . This   separation is used to separately specify semantics   and syntax to the decoder in order to produce para-   phrases ; ii ) SynPG ( Huang and Chang , 2021 ): A   paraphrase generation Seq2Seq model based on   a Transformer architecture which also separately   encodes syntax and semantics for the same pur-   pose as VGV AE . This model is , however , trained   using only source sentences with their syntactic   parses , without paraphrases ; iii ) Optimus ( Li et al . ,   2020b ): A large - scale V AE based on a fusion be-   tween BERT ( Devlin et al . , 2019 ) and GPT-2 ( Rad-   ford et al . , 2019 ) with competitive performance on   various NLP benchmarks ; iv ) ADVAE : This model   is QKV AE without its syntactic variable . The size   of its latent variable is set to 1536 to equal the total   size of latent variables in QKV AE .   Official open - source instancesof the 4 mod-   els above are available , which ensures accurate   comparisons . The off - the - shelf instances of VG-   V AE and SynPG are trained on ParaNMT with   GloVe(Pennington et al . , 2014 ) embeddings . We   fine - tune a pre - trained Optimus on our training set   following instructions from the authors . Similar   to our model , we initialize ADV AE with param-   eters from BART(Lewis et al . , 2020 ) and train 5   instances of it on ParaNMT with L= 4 .   5.2 Syntax and Semantics Separation in the   Embedding Space   We first test whether zandzrespectively   specialize in syntax and semantics . A syntactic   ( resp . semantic ) embedding should place syntacti-   cally ( resp . semantically ) similar sentences close   to each other in the embedding space .   Using the ( target , sem_src , syn_src ) triplets , we   calculate for each embedding the probability that   target is closer to sem_src than it is to syn_src in the   embedding space . For simplicity , we refer to the   syntactic and semantic embeddings of all models   aszandz . For Gaussian latent variables , we   use the mean parameter as a representation ( respec-   tively the mean direction parameter from the von   Mises - Fisher distribution of the semantic variable   of VGV AE ) . We use an L2 distance for Gaussian   variables and a cosine distance for the others . Since   Optimus and ADV AE do not have separate embed-   dings for syntax and semantics i)We take the whole   embedding for Optimus ; ii)For ADV AE , we mea-   sure the above probability on the development set   for each latent variable z(Eq . 3 ) . Then , we choose   the latent variable that places target sentences clos-   est to their sem_src ( resp . syn_src ) as a semantic   ( resp . syntactic ) variable . The results are presented   in Table 2 .   Table 2 clearly shows for QKV AE , SynPG , and   VGV AE that the syntactic ( resp . semantic ) vari-   ables lean towards positioning sentences in the em-   bedding space according to their syntax ( resp . se-   mantics ) . Surprisingly , the syntactic variable of   our model specializes in syntax ( i.e.has low score )   as much as that of SynPG . The generalist latent   variable of Optimus seems to position sentences   in the latent space according to their semantics .   Accordingly , we place its score in the zcol-   umn . Interestingly , the variables in ADV AE have   very close scores and score well below 50 , which   shows that the entire ADV AE embedding leans   more towards syntax . This means that , without   the key / value distinction in the Attention - based de-   coder , the variables specialize more in structure   than in content.5768   5.3 Syntactic and Semantic Transfer   Similar to ( Chen et al . , 2020 ) , we aim to produce   sentences that take semantic content from sem_src   sentences and syntax from syn_src sentences . For   each of SynPG , VGV AE , and QKV AE we simply   use the syntactic embedding of syn_src , and the   semantic embedding of sem_src as inputs to the de-   coder to produce new sentences . Using the results   of the specialization test in the previous experiment ,   we do the same for ADV AE by taking the 2 latent   variables that lean most to semantics ( resp . syntax )   as semantic ( resp . syntactic ) variables . The out-   put sentences are then scored in terms of syntactic   and semantic similarity with sem_src , syn_src and   target .   Control and reference baselines Beside model   outputs , we also use our syntactic and semantic   comparison metrics , explicited below , to compare   syn_src andsem_src sentences to one another and   totarget sentences . Additionally , using Optimus ,   we embed sem_src andsyn_src , take the dimension-   wise average of both embeddings , and decode it .   As V AEs are known to produce quality sentence in-   terpolations ( Bowman et al . , 2016 ; Li et al . , 2020b),the scores for this sentence help contrast a naïve   fusion of features in the embedding space with a   composition of well identified disentangled fea-   tures .   Transfer metrics We measure the syntactic and   semantic transfer from source sentences to output   sentences . i ) Semantics : For semantics , previous   works ( Chen et al . , 2020 ; Huang and Chang , 2021 )   rely on lexical overlap measures such as BLEU   ( Papineni et al . , 2001 ) , ROUGE ( Lin , 2004 ) , and   Meteor ( Denkowski and Lavie , 2014 ) . As will   be shown in our results , the lexical overlap signal   does not capture semantic transfer between sen-   tences when this transfer is too weak to produce   paraphrases . Therefore , we use Meteor ( M ) in con-   junction with ParaBART ( Huang et al . , 2021 ) a   model where BART ( Lewis et al . , 2020 ) is fine-   tuned using syntactic information to produce neural   representations that represent maximally semantics   and minimally syntax . We measure the cosine sim-   ilarity between sentences according to ParaBART   embeddings ( PB).ii ) Syntax : We use the script of   ( Chen et al . , 2020 ) to produce a syntactic tree edit   distance ( STED ) between the constituency trees   of sentences , as was done to assess VGV AE . Ad-   ditionally , following the evaluation procedure de-   signed by Huang and Chang ( 2021 ) for SynPG , we   measure the Template Matching Accuracy between   sentences , where the template is the constituency   tree truncated at the second level ( TMA2 ) . TMA2   is the percentage of sentence pairs where such tem-   plates match exactly . We extend this measure by   also providing it at the third level ( TMA3 ) . Results   are presented in Tables 3 and 4 . In both Tables , the   comparison scores between sentences and syn_src   that are not significantlydifferent from the same5769scores produced with regard to sem_src are marked   with .   Sanity checks with metrics and baselines We   notice in Table 4 that using Meteor as a semantic   similarity measure results in various inconsisten-   cies . For instance , paraphrases target have a higher   Meteor score with the syntactic sources than with   interpolations from Optimus . It can also be seen   that the Meteor score between outputs from VG-   V AE and both syntactic and semantic sources are   rather close . In contrast , ParaBART score be-   haves as expected across comparisons in Table 4 .   Consequently , we retain ParaBART score as a se-   mantic similarity measure . In the following , we   use the scores between sem_src , syn_src , and tar-   get(first two rows in Tables 4 and 3 ) as reference   scores for unrelated sentences , paraphrase pairs ,   and syntactically similar sentences .   Comparing the supervised baselines VGV AE   and SynPG greatly differ in scores . It can be seen   that SynPG copies a lot of lexical items from its se-   mantic input ( high Meteor score ) which allows for   higher semantic similarity scores . However , Table   3 shows that SynPG transfers syntax from syn_src   at a high level ( high TMA2 , but low TMA3 ) . In   contrast , VGV AE transfers syntax and semantics in   a balanced way and achieves the best syntax trans-   fer scores overall ( lowest STED with syn_src and   target ) .   Analysing the scores of QKVAE The seman-   tic similarity scores PBof QKV AE outputs with   target andsem_src are close to those of Optimus   outputs . Although these scores are low compared   to supervised models , they are notably higher than   semantic similarity scores between unrelated sen-   tences ( e.g. syn_src andsem_src ) . However , in   contrast to Optimus , QKV AE outputs display low   PB scores with syn_src , which show that they draw   very little semantic information from the syntactic   sources . Concerning syntactic transfer in Table 3 ,   QKV AE outputs share syntactic information with   syn_src on all levels ( low STED , and high TMA2   and TMA3 ) . Our model is even competitive with   SynPG on TMA2 , and better on TMA3 and STED .   As expected , the scores comparing QKV AE outputs   tosem_src show that they share very little syntac-   tic information . On the other hand , ADV AE shows   poor transfer performance on syntax and semantics , with only slight differences between scores w.r.t   syn_src and scores w.r.t sem_src .   5.4 Comparing our Model to a Supervised   Model with Less Data   Since VGV AE displays balanced syntactic and se-   mantic transfer capabilities , we use it for this ex-   periment where we train it on subsets of sizes in   { 10K,25K,50K,100K}from its original train-   ing data . Our goal is to find out how much labeled   data is needed for VGV AE to outperform our unsu-   pervised model on both transfer metrics .   In Figure 1 , we plot for QKV AE and instances of   VGV AE the STED of their outputs w.r.t syn_src and   thePBof these outputs w.r.t sem_src . All values   are averages over 5 runs , with standard deviations   plotted as ellipses . Figure 1 shows that to outper-   form QKV AE on syntactic and semantic transfer ,   VGV AE needs more than 50 K labeled samples .   6 Discussion and conclusion   In Table 5 , we display example outputs of SynPG ,   VGV AE , and QKV AE along with their syntactic   sources , semantic sources , and targets . We gen-   erally observed that the outputs of QKV AE range   from paraphrases ( line 6 ) to broadly related sen-   tences ( line 3 ) . As was shown by our quantitative   results , outputs from V AE - based models ( VGV AE   and QKV AE ) share relatively few lexical items with   the semantic input . This can be seen in the qual-   itative examples where they often swap words in   the semantic source with closely related words ( e.g.5770   " armored division " to " military force " in line 1 , or   " lunch boxes " to " snacks " in line 2 ) . We attribute   this quality to the smoothness of the latent space   of V AEs which places coherent alternative lexical   choices in the same vicinity . The examples above   also show that our model is capable of capturing   and transferring various syntactic characteristics   such as the passive form ( line 1 ) , the presence of   subject - verb inversion ( lines 3 , 4 , and 7 ) , or inter-   jections ( lines 4 and 6 ) .   We presented QKV AE , an unsupervised model   which disentangles syntax from semantics without   syntactic or semantic information . Our experiments   show that its latent variables effectively position   sentences in the latent space according to these   attributes . Additionally , we show that QKV AE   displays clear signs of disentanglement in trans-   fer experiments . Although the semantic transfer is   moderate , syntactic transfer with QKV AE is com-   petitive with SynPG , one of its supervised counter-   parts . Finally , we show that VGV AE , a supervised   model , needs more than 50 K samples to outperform   QKV AE on both syntactic and semantic transfer .   We plan to extend this work in three directions : i )   Finding ways to bias representations of each z   towards understandable concepts ; ii)Applying QK-   V AE to non - textual data since it is data agnostic   ( e.g.to rearrange elements of a visual landscape . ) ;   iii)Investigating the behavior of QKV AE on other   languages . Acknowledgments   This work is supported by the PARSITI project   grant ( ANR-16 - CE33 - 0021 ) given by the French   National Research Agency ( ANR ) , the Laboratoire   d’excellence “ Empirical Foundations of Linguis-   tics ” ( ANR-10 - LABX-0083 ) , as well as the ON-   TORULE project . It was also granted access to   the HPC resources of IDRIS under the allocation   20XX - AD011012112 made by GENCI .   References5771577257735774A Results on the development set   We hereby display the scores on the development   set . The encoder scores concerning the specializa-   tion of latent variables are in Table 6 , while the   transfer scores are in Table 7 for semantics , and   Table 8 for syntax . The values on the development   set concerning the comparison of QKV AE with   VGV AE trained on various amounts of data is in   Figure 2 .   B Hyper - parameters   Hyper - parameter values Theβweight on the   KLdivergence is set to 0.6 for zand to 0.3 for z ,   and the λthreshold for the Free - Bits strategy is set   to 0.05 . KLannealing is performed between steps   3 K and 6 K for z , and between steps 7 K and   20 K for z. The model is trained using Adafac-   tor ( Shazeer and Stern , 2018 ) , a memory - efficient   version of Adam ( Kingma and Ba , 2015 ) . Using   a batch size of 64 , we train for 40 epochs , which   takes about 30 hours on a single Nvidia GEForce   RTX 2080 GPU . We use 4 layers for both Trans-   former encoders and decoders . The encoders ( resp .   decoders ) are initialized with parameters from the   4 first layers ( resp . 4 last layers ) of BART encoders   ( resp . decoders ) . In total , our model uses 236 M   parameters .   Manual Hyper - parameter search Given that   the architecture for Transformer layers is fixed by   BART , we mainly explored 3 parameters : number   of latent variables L , number of Transformer lay-   ers , values for β . Our first experiments have shown   that setting Lto 8 or 16 does not yield good re-5775sults , which is probably due to the fact that a high   Lraises the search space for possible arrangements   of values with keys , and consequently makes con-   vergence harder . Concerning the number of layers ,   we observed that results with the full BART model   ( 6 layers ) have high variance over different runs .   Reducing the number of layers to 4 solved this is-   sue . In regards to β , we observed that it must be   0.6or less for the model to produce adequate recon-   structions and that it is beneficial to set it slightly   lower for zthan for zso as to absorb more   syntactic information with z.5776