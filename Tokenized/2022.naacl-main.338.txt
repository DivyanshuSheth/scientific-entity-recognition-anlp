  Laura Burdick , Jonathan K. Kummerfeld andRada Mihalcea   Computer Science & Engineering   University of Michigan , Ann Arbor   { lburdick,jkummerf,mihalcea}@umich.edu   Abstract   We use paraphrases as a unique source of data   to analyze contextualized embeddings , with   a particular focus on BERT . Because para-   phrases naturally encode consistent word and   phrase semantics , they provide a unique lens   for investigating properties of embeddings .   Using the Paraphrase Database ’s alignments ,   we study words within paraphrases as well   as phrase representations . We ﬁnd that con-   textual embeddings effectively handle polyse-   mous words , but give synonyms surprisingly   different representations in many cases . We   conﬁrm previous ﬁndings that BERT is sensi-   tive to word order , but ﬁnd slightly different   patterns than prior work in terms of the level   of contextualization across BERT ’s layers .   1 Introduction   Contextualized embedding algorithms , such as   BERT ( Devlin et al . , 2019 ) , have achieved impres-   sive performance on a wide variety of tasks ( Huang   et al . , 2019 ; Chan and Fan , 2019 ; Yoosuf and Yang ,   2019 ) . One application of BERT is using it as a   measure of sentence similarity ( Zhang * et al . , 2020 ;   Sellam et al . , 2020 ) , based on the assumption that   BERT will produce similar representations for the   words in two sentences with similar semantics .   We propose to use paraphrases with alignments   between words as a tool for studying how BERT   represents words and phrases . Figure 1 shows an   example . Critically , when considering an aligned   word pair , we can assume the context has a similar   impact on both words because we know the phrases   are semantically similar . Previously , paraphrases   have been used to probe whether compositionality   is accurately captured by BERT ( Yu and Ettinger ,   2020 ) , but we believe they can be used to explore   many other questions .   Using the second version of the ParaphraseFigure 1 : Example paraphrase from the PPDB with   word alignment and word cosine similarities using the   last layer of BERT .   Database ( PPDB , Pavlick et al . , 2015 ) , we ex-   plore how consistent contextual representations   are when controlling for the semantics of the con-   text . First , we use the human - annotated portion   of the PPDB data to conﬁrm that BERT consis-   tently represents paraphrases . Next , we use the   highest - quality ( but not all human - annotated ) sec-   tion of the PPDB to probe BERT ’s behavior in   more detail . Looking at words , BERT effectively   handles variations in spelling , but does less well   with spelling errors . BERT also effectively han-   dles words of varying levels of polysemy , but the   representations for synonyms are surprisingly di-   verse , with a much broader distribution of similarity   scores . These ﬁndings conﬁrm results from prior   work using other methods , while uncovering new   insights about contextual embedding models .   We also consider a range of other models ’ word   representations , ﬁnding that they have similar pat-   terns to BERT , but with aligned words that are the   same receiving even more consistent representa-   tions than from BERT . BERT gives less contextu-   alized representations to paraphrased words than   non - paraphrased words , with the exception of punc-   tuation . Finally , we re - evaluate work looking at   patterns across BERT ’s layers and ﬁnd that when   controlling for semantics , the later layers actually   produce more similar representations ( in contrast   to previous work ) .   These results show that paraphrases are a useful   tool for studying representations . By controlling4558for meaning while presenting interesting surface   variations , they provide a unique probe of behavior .   2 Background   2.1 BERTology   There has been a growing body of research study-   ing the inner workings of BERT and trying to quan-   tify what it learns in various scenarios , dubbed   “ BERTology ” ( Rogers et al . , 2020 ) . Of particular   interest to this paper is work that analyzes BERT ’s   output embeddings . Recent studies have found   that embeddings created from the ﬁnal layer of   BERT tend to cluster according to word senses   ( Wiedemann et al . , 2019 ) , though this varies some-   what based on the position of a word in a sentence   ( Mickus et al . , 2020 ) . The ﬁnal BERT layers also   produce more contextualized word embeddings   than the earlier layers ( Ethayarajh , 2019 ) , a ﬁnding   we revisit using paraphrases in Section 3.3 .   2.2 The Paraphrase Database   To analyze BERT , we take advantage of the unique   properties of paraphrases . We use the Paraphrase   Database ( PPDB , Ganitkevitch et al . , 2013 ; Pavlick   et al . , 2015 ) , a database of paraphrases collected   using bilingual pivoting , the process of taking a   particular English phrase , looking at all the foreign   language phrases it can be translated into , ﬁnding   all occurrences of these foreign language phrases ,   and then translating them back into English ( Ban-   nard and Callison - Burch , 2005 ) . PPDB 2.0 con-   tains 100m+ English paraphrases , each with word   alignment information , an automatically generated   quality rating , and , for a subset , a human quality   rating . Word alignments are the by - product of   the bilingual pivoting method used to collect the   paraphrases . When using alignments , we only con-   sider phrases from the highest quality section of the   PPDB , which are most likely to have accurate align-   ments . Example paraphrases with their average   human annotations and automatically generated   scores are shown in Table 1 . In general , the phrases   in this dataset are short . The longest phrases have   six tokens , and the majority have fewer than six .   Human quality ratings are included for 26,455   paraphrase pairs , with ﬁve annotations per para-   phrase . Agreement is measured using Spearman ’s   ρ(Spearman , 1910 ) ; the average ρbetween two   workers is 0.57 , and the average ρbetween each   worker with the other four annotators is 0.65.Phrases Human PPDB   Score Score   are you talking 1.0 2.7   do n’t they   what ’s this all about ? 4.2 3.9   what ’s she saying ?   where did they come from ? 4.8 4.4   where are they from ?   The automatic quality ratings ( PPDB score ) are   generated by using the human annotations to ﬁt a   supervised ridge regression model . The input to   the model consists of 209 hand - crafted paraphrase   features , including WordNet features ( Fellbaum ,   1998 ) , distributional similarity features , and cosine   similarities of generated Multiview Latent Seman-   tic Analysis embeddings ( Rastogi et al . , 2015 ) . The   PPDB score achieves a Spearman ’s ρof 0.71 . In   comparison , Pavlick et al . ( 2015 ) report that using   the word2vec embedding of the rarest word in each   paraphrase obtains Spearman ’s ρof 0.46 .   3 Experiments   In our experiments , we want to use the PPDB to   examine BERT ’s ability to consistently represent   paraphrase semantics . In order to do this , we con-   sider both phrase - level and word - level embeddings .   Except where explicitly indicated otherwise , all ex-   periments are run using the uncased base model of   BERT , using a maximum sequence length of 128   and a batch size of 8 . We use the pretrained models   provided by the Transformers library .   There is a slight mismatch between the PPDB ’s   tokenization and the format of the BERT training   data . The mismatch primarily occurs with contrac-   tions and apostrophes ( e.g. , BERT expects “ do n’t ” ,   while the PPDB is tokenized “ do n’t ” ) . This does   not substantially affect the results ; less than 8 %   of the human - annotated paraphrase pairs contain   apostrophes . When words are broken into multi-   ple pieces by the wordpiece tokenizer , we use the4559average of the pieces as the word representation .   3.1 Phrase - Level Embeddings   First , we consider phrase - level embeddings that   capture aggregate information about all of the   words in a given phrase . These embeddings show   us that BERT is able to distinguish between two   paraphrases , and two unrelated phrases .   We use 25,736 phrase pairs with human anno-   tations in the PPDB.Each human annotation is   between 1 and 5 , reﬂecting the similarity of the   two phrases . We run each phrase through the pre-   trained BERT model . For each pair of phrases , we   average together the embeddings for each word to   get a phrase embedding . We create phrase embed-   dings using averaging because previous research   has shown that this method is effective . For exam-   ple , Reimers and Gurevych ( 2019 ) created sentence   embeddings using three methods : ( 1 ) averaging   word embeddings , ( 2 ) taking the maximum of word   embeddings , and ( 3 ) using the CLS token vector .   They found that averaging created the best sentence   embeddings for semantic textual similarity tasks .   After creating phrase embeddings , we take the   cosine similarity between the two embeddings . We   compare with ground truth annotations using Spear-   man’sρ . We do this for each of the twelve BERT   layers , and the concatenation of all layers . We use   cosine similarity to compare embeddings because   this metric is commonly used when working with   BERT ( e.g. , Mahmoud and Torki ( 2020 ) ; Garí Soler   and Apidianaki ( 2020 ) ; Kovaleva et al . ( 2019 ) ) .   We compare BERT to a more traditional em-   bedding method , the continuous bag - of - words ap-   proach in word2vec ( w2v ) ( Mikolov et al . , 2013 ) .   We train w2v on an English Wikipedia corpus of   5,269,686 sentences , using dimension size 200 ,   a window size of ﬁve , and a minimum count of   ﬁve . We choose to train w2v on Wikipedia data , in   order to replicate the correlations in Pavlick et al .   ( 2015 ) . We train ﬁve w2v models , using ﬁve dif-   ferent random seeds . For each pair of phrases , we   average together the embeddings for each word to   get a phrase embedding , and then take the cosinesimilarity between the two phrase embeddings .   We report the average and standard deviation of   Spearman ’s ρover the ﬁve models .   Comparing Sentences and Phrases One differ-   ence between our work and the way BERT is nor-   mally used is that we have phrases rather than sen-   tences . To check that this does not substantially   change BERT ’s behavior , we compare the embed-   dings for phrases in a sentence and the phrases on   their own . We take 9,780 paraphrases from the   PPDB . We choose paraphrases where one of the   phrases has at least six tokens , the paraphrase has a   relatively good PPDB score and no syntactic place-   holders . This is described further in Section 3.2 .   For each phrase , we ﬁnd up to 100 sentences ( on   average , 80.5 sentences ) in Gigaword ( Parker et al . ,   2011 ; Rush et al . , 2015)and OpenSubtitles ( Tiede-   mann , 2012)that contain that phrase . For each   sentence , we run it through BERT and average to-   gether the word embeddings for words in the phrase   to create a phrase embedding . The phrase embed-   dings are very similar across different sentences   ( average cosine similarity of 0.82±0.07 ) .   Now we can compare ( 1 ) the average of phrase   embeddings derived from sentences , with ( 2 ) em-   beddings for phrases in isolation , to see if BERT   will be confused by not having a complete sentence .   For each phrase , we take the cosine similarity be-   tween the phrase embeddings created using these   two methods . The phrase embeddings are fairly   similar ( average similarity of 0.74±0.12 ) . This   gives us conﬁdence that BERT produces embed-   dings for phrases on their own that are very similar   to phrases in the context of a sentence . For the rest   of our experiments , we run phrases individually   through BERT , rather than in the context of com-   plete sentences , which allows us to focus on the   semantics of the phrase itself .   Results on the PPDB . Table 3 shows results for   BERT , w2v , and the PPDB model , broken down   by the average length of each paraphrase . For all   layers , BERT improves on longer paraphrases . This   is intuitive , because the longer the phrase , the more   it will be able to leverage contextual information .   The last layer of BERT behaves slightly differently   than the other layers . While it continues to perform4560   better on longer paraphrases , it does substantially   worse on short paraphrases and slightly worse on   medium paraphrases .   Similarly , w2v also improves on longer para-   phrases . By taking the average of all the word   embeddings for each word in the phrase , w2v has   more information to incorporate into its phrase em-   beddings for longer paraphrases . Though w2v im-   proves as the paraphrases grow longer , it underper-   forms BERT for all but the shortest paraphrases .   We also see that the automatic PPDB score does   better on longer paraphrases . This could be because   it incorporates distributional information , which is   richer when there are more words . Finally , the   human annotation scores show that longer para-   phrases are more similar .   From Table 3 , we see that the ﬁnal layer of BERT   outperforms w2v and performs comparably to the   PPDB score on the longest paraphrases . This is   not a completely fair comparison ; the PPDB model   is trained speciﬁcally on this data , and has accessto outside information that BERT does not , such   as WordNet features and additional features de-   rived from the translation process used to create   the PPDB . These results give us conﬁdence that   BERT can distinguish between phrases that are   paraphrases of each other and phrases that are not .   Looking at BERT ’s output , we can see several   patterns in the paraphrases that receive high and   low similarities . Table 2 shows examples of these   patterns . For phrases with high similarity according   to BERT , a single word changes or a single word   is added . On the surface , these changes have very   little impact on the meaning , though the addition of   the word ‘ special ’ in the second case could change   who is being referred to . For phrases with low sim-   ilarity according to BERT , they frequently required   world knowledge ( e.g. , deﬁnition of an acronym )   or appeared to be errors . We also observed idioms   getting reasonably high scores , but not as high as   the literal paraphrases .   Conclusion : The standard way of using BERT   to produce a representation of a phrase is consis-   tent with human scores of paraphrases . All layers   are effective , though the last layer struggles with   shorter phrases .   3.1.1 One - Word Paraphrases   In Section 3.1 , we saw that BERT does not do as   well on short phrases as it does on longer ones .   We explore the extreme case of single word para-   phrases here . Among the subset of one - word para-   phrases , there is a wide range of human annotations   ( average annotation 2.27±0.99 ) . To explore this   further , we focus on one - word paraphrases with   a human annotation of 5 , the highest annotation   score , indicating that these are the strongest syn-   onyms . Among these high - quality synonyms , co-   sine similarities are consistently high for the last   layer of BERT ( average similarity 0.76±0.12 ) .   Table 4 shows synonyms with both the high-   est and lowest BERT similarities . Misspelled4561Phrase 1 Phrase 2 Cos . Sim .   laboratoires laboratories 0.51   completly totally 0.51   fervor enthusiasm 0.52   79.0 seventy - nine 0.53   approximatly around 0.54   -mom -mother 0.91   1.350 1.35 0.92   characterises characterizes 0.92   km kilometres 0.92   garbage trash 0.96   words ( e.g. , completly , approximatly ) and   pairs that involve different languages ( e.g. , French   laboratoires ) have low cosine similarities .   Numbers appear on both the low end ( e.g. , 79.0   andseventy - nine ) and the high end ( e.g. ,   1.350 and1.35 ) of the similarity spectrum . One   difference between the similar and dissimilar num-   ber pairs is that in the similar case they both use   digits , while in the dissimilar case , one uses digits   while the other uses words .   Conclusion : Looking at single words shows that   BERT struggles to identify synonyms , and does   particularly poorly with misspellings and cross-   lingual comparisons .   3.2 Word - Level Embeddings   PPDB provides alignments between words in the   paraphrases , automatically generated as part of   bilingual pivoting . We use these alignments to   consider four different sets of words :   Same , Aligned Words that are the same in both   phrases and aligned .   Same , Unaligned Words that are the same in both   phrases , but not aligned . These tend to be   function words . 90 % of our examples are   one of ( the , of , " , to , i , in , that , as , what ) .   This category may have more examples of   other word types if longer paraphrases are   considered in future work .   Different , Aligned Words that are aligned , but not   the same . This case covers synonyms .   Different , Unaligned Words that are not aligned   and not the same ( but still one from each   phrase in a paraphrase pair ) . Note , these   words are not completely unrelated . They are   from the same paraphrase , making them more   related than words from two random phrases .   In this section , we go beyond the human-   annotated data considered in the previous section .   We restrict our experiments to the highest qual-   ity paraphrases in the PPDB : dataset S. We also   only consider long paraphrases ( where one of the   phrases has at least six tokens ) , and paraphrases   that have no syntactic placeholders ( a subset of the   PPDB contains general syntactic symbols , such as   wishes to be [ VP / NP ] ) . From this set , we   randomly sample 4,000 paraphrases . Our sample   yields 22,751 aligned same words , 25,973 aligned   different words , 2,782 unaligned same words , and   163,474 unaligned different words . We randomly   sample 2,500 words from each category . For the   aligned words , we only use cases where there is a   1 - 1 alignment .   To generate word - level embeddings , we run each   phrase through a set of transformer models and for   each pair of words , we take the cosine similarity   between the embeddings of the two words .   3.2.1 Results   Figure 3 shows the distributions of similarity scores   for all four sets of words for several models . Same ,   aligned words consistently have the highest sim-   ilarity . The other categories tend to overlap . Be-   cause we are using paraphrases , we would hope   that aligned different words would have higher sim-   ilarity , but that is not consistently the case .   Comparing the models , there are some notable   variations . Between BERT base and BERT large ,   the biggest shift is that unaligned words that are the   same have much lower similarity in BERT large,4562   though there is also a new peak for aligned words   around 0.2 . Comparing the two BERT models with   BART and GPT-2 , there is a much sharper peak forBART and GPT-2 for same aligned words , which   is consistent with prior work ( Ethayarajh , 2019 ) .   BART is the only model to have a substantial   number of negatively correlated word pairs . Many   of these involve function words or punctuation . For   the unaligned cases , negative cosine similarity is   ﬁne because the words should not have the same   meaning . For the 26 cases of aligned pairs , it   is unclear why the representations are so differ-   ent . For example , the plays the same role in   ( , the commission considered and ,   the commission had before it . Simi-   larly , aim andview should be very similar in aim   of improving the andwith a view to   improving the .   Qualitatively looking at examples , we notice   that when a token appears in a different position   in the paraphrase , the similarity tends to be on   the lower end of the distribution ( e.g. , action   in the phrases plans of action for the   implementation andaction plan for   the implementation has a similarity of   0.28 ) . To explore this , we consider 2,181 aligned   same words . We measured the cosine similarity of   the last layer of BERT broken down by the variation   in position ( plotted in Figure 7 in the Appendices ) .   Spearman’sρ=−0.29(p - value<10e−42 ) , indi-   cating that similarity decreases for larger changes   in position . This supports observations in prior   work ( Mickus et al . , 2020 ) , but now with the knowl-   edge that the overall context has the same meaning .   This is not intuitive behavior ; because these words   are aligned in a paraphrase , we would expect that   the position of the word would not substantially   affect its representation . This may indicate that   the representations are encoding some information   about syntactic structure , which can vary without   changing semantics .   Conclusions : ( 1 ) Contextual word embedding   methods consistently handle aligned words in para-   phrases , but with substantial variations across mod-   els in how peaked the distributions of same - aligned   words are . ( 2 ) Even when controlling for the mean-   ing of the context , BERT represents words differ-   ently depending on their position .   3.2.2 Punctuation   Punctuation is a core part of language that func-   tions quite unlike words ; punctuation groups words   together or separates them , and contributes to the   overall structure and meaning of a phrase or sen-   tence . Punctuation plays an important role in dis-4563   tinguishing between different types of text , such   as texts by different authors ( Soler - Company and   Wanner , 2017 ) or texts produced by different Twit-   ter communities ( Tatman and Paullada , 2017 ) . Em-   beddings are used to generate punctuation for text   that is lacking punctuation , such as recorded tran-   scripts ( Yi and Tao , 2019 ) . To explore how BERT   handles punctuation , we consider the cosine simi-   larity distribution for different sets of punctuation   tokens for the last layer of BERT . We ﬁnd that   punctuation has a broader distribution of cosine   similarities than other tokens , indicating that punc-   tuation embeddings vary widely dependent on the   surrounding context . Phrases Cos . Sim .   ( Last Layer )   it is important , however , 0.08   however , it should be   okay , i ’ m sorry 0.84   oh , i am so sorry   well , it ’s true . 0.19   this is true .   , that ’s all right . 0.15   , this is good .   where have you come from ? 0.94   where are you from ?   news - politics - world - 0.71   news - international politics -   In Figure 4 , we break these trends down by indi-   vidual punctuation marks , focusing only on aligned   same words . We look at the most common punc-   tuation marks . Of these punctuation marks , the   comma and period show the widest distributions .   Even when they play the same role in the para-   phrase , they can be given very different embed-   dings , indicating how highly contextualized these   punctuation marks are . The question mark and   dash are less contextualized ; this is most likely be-   cause these punctuation marks are used in more   prescribed circumstances . In this dataset , in all but   one example , the question mark is the last token ;   the dash is the ﬁrst token in all but two examples .   Table 5 shows examples in context of each of   these punctuation marks . Looking at the low simi-   larity cases , one common pattern is that the phrase   contains a contraction that is expanded in one   phrase ( e.g. , “ it is ” and “ it ’s ” ) .   Conclusion : BERT ’s representation of punctua-   tion is surprisingly context sensitive , with substan-   tial variation even when we control for meaning .   3.2.3 Polysemy   Previous work has shown that BERT embeddings   form clusters based on word senses ( Wiedemann   et al . , 2019 ) . In the context of aligned words in a   paraphrase , we would expect even a highly polyse-   mous word to have similar embeddings in the two   phrases.4564   To measure polysemy , we consider the number   of WordNet synsets of a word , focusing on same   aligned and same unaligned words . In order to have   enough data to make a good comparison , we use   the 4,000 sampled paraphrases from Section 3.2 ,   as well as an additional random sample of long   paraphrases with at least one unaligned same word .   We then downsample the aligned same words to   get 1,597 instances of both unaligned and aligned   same words that are present in WordNet , with up   to 52 synsets .   In Figure 5 , we show the cosine similarity distri-   butions for both aligned and unaligned words with   different levels of polysemy across the last layer of   BERT . There is not a substantial difference between   words with different synsets , which supports our   conclusion that BERT successfully captures the se-   mantics of aligned same words in paraphrases . We   do see a difference between aligned and unaligned   words . Aligned words peak at a high cosine similar-   ity , while unaligned words roughly follow a normal   distribution centered around 0.5 . Note that for un-   aligned words with two or three synsets , there is   not enough data to draw conclusions about the co-   sine similarity distributions . Overall , these plots   show that even highly polysemous aligned same   words have very similar embeddings in the context   of a paraphrase .   Conclusion : How polysemous a word is does not   substantially impact BERT ’s ability to consistently   represent it .   3.3 Contextualization in BERT Layers   In this section , we consider how context - speciﬁc   the embeddings in a paraphrase are . Ethayarajh   ( 2019 ) showed that BERT word embeddings are   more context - speciﬁc in higher layers . They mea-   sure this using the self - similarity of words , deﬁned   as the average cosine similarity between a word ’s   contextualized representations across its unique   contexts , and show that self - similarity consistently   decreases with higher layers of BERT , indicating   that the contextualization of words is increasing .   We compare this observation to the paraphrase   setting that we have been exploring in this paper .   Because there are only two phrases in a paraphrase,4565we can not implement the full self - similarity metric .   Instead , we measure the cosine similarity between   two words aligned in a paraphrase , shown in Fig-   ure 6 . This revised metric measures is similar to   self - similarity .   We see two trends in Figure 6 . The ﬁrst , de-   creasing cosine similarity , is seen with same words ,   whether aligned or unaligned , and is similar to   what Ethayarajh ( 2019 ) report with decreasing self-   similarity scores . This trend is stronger with un-   aligned words than with aligned words , indicating   the model is capturing the fact that while these   words have the same form , they are being used   differently . The second trend that we see is the   opposite , increasing cosine similarity , and we see   this trend with different words , both aligned and   unaligned . This indicates decreasing contextualiza-   tion .   Conclusion : As seen in prior work , the standard   way of using vectors from BERT ’s layers does not   capture the same level of contextualization in all   layers . However , in contrast to prior work , when   controlling for semantics of the context , it seems   that later layers are capturing more of the context ,   appropriately making words less similar when they   are being used in different ways .   4 Conclusion   Paraphrases with word alignments are a useful tool   for studying the behavior of contextual language   models . In this paper , we used them to study sev-   eral contextual models , with a particular focus on   BERT . Where possible , we compared our results   with prior work , ﬁnding patterns that are consis-   tent with the literature . Speciﬁcally , our results   conﬁrm that BERT consistently represents para-   phrases , even for cases with polysemous words ,   but that individual word representations are overly   sensitive to position , particularly for punctuation .   One exception is that we found that words in a   sentence are more similar to each other in later lay-   ers of BERT , in contrast to prior work that did not   control for meaning using paraphrases .   The analysis method we introduced opens up   new opportunities , such as the comparison of   aligned and unaligned , same and different words ,   which shows the sensitivity of these models to the   speciﬁc word used . Paraphrases have the poten-   tial to inform exploration of other representation   methods , showing which way of using the output   of language models most accurately captures se - mantics consistently . We hope our ﬁndings will   inform future work on contextualized models , and   the applications that rely on them .   References45664567A Extra breakdowns of results   Table 6 presents an expanded version of Table 3 ,   with results for each layer of BERT . All layers   perform better with longer paraphrases , but the   improvement is largest for the last layer   Figure 7 shows the speciﬁc values for similar-   ity broken down by distance apart of words in the   phrases . This shows the pattern of decreasing simi-   larity as words are further away.4568