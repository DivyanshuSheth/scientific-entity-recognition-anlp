  Sarik GhazarianYijia ShaoRujun HanAram GalstyanNanyun PengUniversity of Southern California / Information Sciences InstitutePeking UniversityAWS AI LabsComputer Science Department of University of California , Los Angeles   { sarik , galstyan}@isi.edu , shaoyj@pku.edu.cn , rujunh@amazon.com , violetpeng@cs.ucla.edu   Abstract   Commonsense reasoning is omnipresent in hu-   man communications and thus is an impor-   tant feature for open - domain dialogue systems .   However , evaluating commonsense in dialogue   systems is still an open challenge . We take   the first step by focusing on event common-   sense that considers events and their relations ,   and is crucial in both dialogues and general   commonsense reasoning . We propose AC-   CENT , an event commonsense evaluation met-   ric empowered by commonsense knowledge   bases ( CSKBs ) . ACCENT first extracts event-   relation tuples from a dialogue , and then eval-   uates the response by scoring the tuples in   terms of their compatibility with the CSKB .   To evaluate ACCENT , we construct the first   public event commonsense evaluation dataset   for open - domain dialogues . Our experiments   show that ACCENT is an efficient metric for   event commonsense evaluation , which achieves   higher correlations with human judgments than   existing baselines .   1 Introduction   Open - domain dialogue systems aim to have natural   and engaging conversations with users ( Chen et al . ,   2017 ) . The abundance of dialogue corpus ( Dziri   et al . , 2018 ) and the development of neural mod-   els ( Radford et al . , 2019 ; Lewis et al . , 2020 ) en-   able open - domain dialogue systems to generate   grammatically correct and meaningful responses   ( Zhang et al . , 2020d ; Bao et al . , 2021 ; Ghazar-   ian et al . , 2021 ) . Despite the success , systems   still struggle to consistently produce commonsense-   compliant responses as humans do . As shown   in Figure 1 Example A , the generated response   is not compliant with commonsense since “ needFigure 1 : Examples of nonsensical system responses in   open - domain dialogues .   an oxygen mask ” is not a reasonable prerequisite   for “ like to paint ” . Commonsense issues for di-   alogue systems can also be manifested when we   consider the dialogue history . For instance , in Fig-   ure 1 Example B , the system ’s response “ That   is interesting ! ” after the user talks about their   car accident violates commonly accepted social   norms ( Frischmann , 2021 ) .   In this work , we study automatic dialogue com-   monsense evaluation by focusing on event com-   monsense ( Sap et al . , 2020 ; Rashkin et al . , 2018 ) ,   which concerns commonsense knowledge about   events and their relations . Our focus on event   commonsense is motivated by the following three   observations : First , advanced open - domain dia-   logue systems have been pre - trained on large cor-   pus and thus suffer less from factoid commonsense   issues ( Petroni et al . , 2019 ) . Second , events and   their relations are key components of commonsense   reasoning ( McCarthy and Hayes , 1981 ) , and our4398study shows overall commonsense and event com-   monsense are highly correlated ( see § 4 ) . Third ,   event commonsense aligns well with the interactive   nature of open - domain dialogue systems ( Huang   et al . , 2020 ) to complete certain social goals .   To automatically evaluate event commonsense   in open - domain dialogues , we propose ACCENT ,   a reference - free Automati CEvent Commonsene   Evaluatio NmeTric which leverages commonsense   knowledge bases ( CSKBs ) and measures the qual-   ity of generated responses without having ground-   truth reference responses . For example , comparing   the examples in Figure 1 against the CSKB easily   reveals commonsense errors in the responses be-   cause when “ PersonX likes to paint ” , what he / she   needs may be “ to get a paint brush ” instead of “ to   get an oxygen mask ” , and when “ PersonX loses   arms from a car accident ” , the other person is ex-   pected to feel “ sad ” .   While these judgments are intuitive to human ,   two challenges exist in automating the evaluation   process . First , there is a considerable gap between   free - form conversational data and the compact com-   monsense knowledge in the CSKB . Second , locat-   ing relevant knowledge in the CSKB is non - trivial .   ACCENT addresses these challenges through   a pipeline method that uses an intermediate sym-   bolic representation for commonsense reasoning .   ACCENT first extracts event - relation tuples from   the target response and its preceding dialogue his-   tory via a prompt - based generative model trained   in a low - resource setting . Those extracted tuples   bridge the gap between the free - form dialogue and   the compact form of CSKB . Then , a compatibil-   ity score is computed to decide how well each ex-   tracted tuple aligns with the CSKB .   To train and evaluate ACCENT , we construct the   first publicly available event commonsense evalua-   tion dataset for open - domain dialogues ( see § 4 ) .   Besides collecting human commonsense judg-   ments , we request annotators to manually extract   event - relation tuples for further analysis .   Our main contributions are three - fold :   •We propose ACCENT , an event commonsense   evaluation metric for open - domain dialogue   systems . To the best of our knowledge , this is   the first work that systematically studies event   commonsense in dialogue systems .   •We construct the first publicly available event   commonsense evaluation dataset for open - domain dialogues .   •Extensive experiments show that ACCENT   achieves better correlation with human judg-   ments for dialogue commonsense evaluation   than several well - designed baselines , and en-   ables easier interpretability of results .   2 Background : Event Commonsense   Endowing machines with human - like common-   sense reasoning capabilities has been an ulti-   mate goal of artificial intelligence research for   decades ( McCarthy and Hayes , 1981 ; LeCun ,   2022 ) . While many early works focused on fac-   toid commonsense or the knowledge about con-   cepts ( Lenat , 1995 ; Liu and Singh , 2004 ) , event   commonsense emerges as an important aspect for   machine commonsense measurement ( Chen et al . ,   2021 ) . Compared with concepts or entities , events   are more informative , involving actions , partici-   pants , time etc . Besides , event commonsense also   requires understanding various relations between   events ( Kuipers , 1984 ; Rashkin et al . , 2018 ) which   would facilitate complex reasoning , especially in   interactive scenarios such as dialogues .   Among the current commonsense resources ( re-   lated works in Appendix A ) , ATOMIC(Hwang   et al . , 2021 ) is a comprehensive CSKB includ-   ing physical - entity , event - centered , and social-   interaction knowledge . Its event - centered and   social - interaction components take up 84.4 % tu-   ples of the entire knowledge base , providing knowl-   edge regarding how events / human actions are as-   sociated with other events / actions . For example ,   given the event “ X runs out of stream ” , according   toATOMIC , this event may happen after “ X ex-   ercises in the gym ” , and the person X is likely to   “ feel tired ” .   3 Method   We present ACCENT , as a framework for event   commonsense evaluation . Figure 2 gives an   overview of ACCENT with two major components .   3.1 Symbolic Intermediate Representation   ACCENT uses event - relation tuples as the sym-   bolic intermediate representation . Each tuple con-   tains a head event and a tail event which are con-   nected through an event relation . We formally de-   fine events and relations below.4399   Event Following Pustejovsky et al . ( 2003 ) , we   define events as short phrases with a trigger word   and its arguments ( e.g. , I like to paint ) . To better   align with ATOMIC , we normalize the event by   replacing tokens referring to people with Person   variable ( e.g. , PersonX likes to paint ) .   Relation We select ˜R = { xIntent , xWant ,   oWant , xReact , oReact , xNeed , xAttr , xEffect , oEffect , HinderedBy , IsAfter , HasSubEvent }   from ATOMICrelations . These relations cover   human behaviors , i.e. , motivation , want , reac-   tion , need , description , towards events ( Sap et al . ,   2019b ) , the cause - effect and constraint in force dy-   namic ( Talmy , 1988 ) , the temporal information , as   well as the parent - child relation in event hierarchy .   Examples for each relation are in Appendix C.   3.2 Event - Relation Extraction   The input of the event commonsense evaluation   task is a list of utterances { u , u , · · · , u}rep-   resenting the dialogue history and the target re-   sponse u. ACCENT first converts the free - form   text into event - relation tuples . To retain the infor-   mation in u , ACCENT extracts tuples whose head   and tail events are both from the target response   ( denoted as “ Single ” ) . Besides , to capture event   commonsense issues conditioned on the dialogue   history ( e.g. , Figure 1 Example B ) , ACCENT also   extracts tuples whose two events come from u   andurespectively ( denoted as “ Pair ” ) .   As illustrated in Figure 3 , the event - relation ex-   tractor in ACCENT is a T5 model M(Raffel et al . ,   2020 ) guided to generate the head and tail events   via designed prompts for each relation . ACCENT   concatenates the prompt for r∈˜Rand the dia-   logue as the input and fine - tunes Min a low re-   source setting . When the relation rexists in the   input utterances , the fine - tuned Mis expected to   generate the head and tail events following a partic-   ular format , i.e. , “ event1 : { head } ; event2 : { tail } ” ,   so that the tuple can be parsed from the decoded   sequence ( from Block A to Block B in Figure 3).4400   Otherwise , the fine - tuned Mis expected to out-   put “ None ” . For each r∈˜R , the designed prompt   explains the semantic meaning of rand triggers   the model to generate the head and tail events ( the   prompts are included in Appendix C ) . At the infer-   ence time , we query Mwith prompts for each r   and parse the generated outputs to get handtto   construct tuples .   3.3 Compatibility Test   After extracting event - relation tuples , ACCENT   checks whether these tuples are sensible through   a compatibility test . Denoting the CSKB as C , the   compatibility test aims to learn a scoring function   fbased on C , where f((h , r , t ) |C)represents the   compatibility of the target tuple ( h , r , t ) with the   CSKB C. we propose to score ( h , r , t ) by querying   adynamic version of Cwithhandr . Figure 4   gives an example of the whole process .   Specifically , ACCENT uses COMET ( Bosselut   et al . , 2019 ) as the dynamic CSKB . COMET adapts   the pre - trained language model by fine - tuning it   onCthrough a conditional generation task where   “ { head } { relation } [ GEN ] ” is the source and a tail   event is the target . To score ( h , r , t ) , we query the   model by requiring it to generate tgiven “ { h }   { r } [ GEN ] ” . The beam search method is applied   for decoding , so we obtain a set of generated tail   events , { t } , where kis the beam size .   The compatibility score for ( h , r , t ) is then com-   puted by checking the similarity between tand the   most similar tamong { t } :   Here , embed ( · ) is parameterized by a Sentence-   BERT model ( Reimers and Gurevych , 2019 ) .   After getting the compatibility scores for each   extracted tuple , we average them to get the final   score for the target response ( see Figure 2).4 Data Collection   We construct the first event commonsense evalua-   tion dataset for open - domain dialogues through   crowdsourcing on Amazon Mechanical Turk   ( MTurk ) . In this section , we describe the collection   procedure and the details of the dataset .   4.1 Dialogue Data Preparation   We select dialogue histories from DailyDialog ( Li   et al . , 2017 ) , PersonaChat ( Zhang et al . , 2018 ) , and   TopicalChat ( Gopalakrishnan et al . , 2019 ) human-   human dialogues . The dialogue history is limited   to at most 4 consecutive utterances . Since human   utterances barely contradict event commonsense ,   to better evaluate machine generated dialogues , we   collect responses using advanced dialogue systems ,   DialoGPT ( Zhang et al . , 2020d ) , PLATO-2 ( Bao   et al . , 2021 ) , DiSCoL ( Ghazarian et al . , 2021 ) .   To ensure most samples contain events and are   meaningful for event commonsense evaluation , we   filter samples using the following criteria : ( 1 ) the   response contains at least 5 words ; ( 2 ) the response   contains at least 1 non - interrogative sentence ; ( 3 )   the response is more than a courtesy ( e.g. , “ It ’s   been nice chatting with you . ” ) . After filtering ,   we randomly select 300 samples and split them   into 200 for training and 100 for testing . We name   this dataset DECO ( Dialogue Event Commonsense   Dataset ) .   4.2 Tuple Extraction   To train the event - relation extractor of ACCENT ,   we collect human extracted event - relation tuples   from DECO training set . Annotators are shown   with the target response , the dialogue history , a spe-   cific relation , and are requested to compose event-   relation tuples . They are allowed to tick “ I can not   find any tuple ” if no tuple can be found . We also   request them to select whether the tuple belongs to   “ Single ” or “ Pair ” ( defined in § 3.2 ) for each tuple   they extract . Figure 8 in Appendix D shows our   data collection panel . We launched HITsfor re-   lations in ˜Rrepeatedly until we obtained at least   20 tuples for each relation . In order to ensure the   test set is comprehensive , we particularly request   annotators to compose tuples for all 12 relations in   ˜R(100 samples ×12 relations in total).4401   A separate validation round was conducted to   check whether each extracted tuple satisfies ( 1 ) the   head and tail are events , ( 2 ) the head and tail come   fromuoru , ( 3 ) the relation between the head   and tail can be inferred from the dialogue . A tuple   is deemed valid if the majority of 3 annotators vote   “ yes ” . After removing invalid tuples ( the dialogue   numbers remain unchanged ) , we collected 307 tu-   ples for training and 467 tuples from the DECO   test set . Figure 5 shows the relation distribution in   the densely annotated test set . More details about   DECO statistics are included in Appendix D.   4.3 Commonsense Scoring   We instruct annotators to score target responses in   terms of event commonsense by focusing on the   events and their relations ( the guideline is shown   in Figure 7 ) . Each response was annotated by 3   individual annotators with a scale of 1 to 5 . Follow-   ing Mehri and Eskenazi ( 2020 ) , we measure the in-   ter annotator agreement ( IAA ) by correlating each   annotation with the mean of the other annotations   for the same sample , and the Spearman correlation   is 0.578 showing an acceptable agreement . The   final event commonsense score assigned to each   sample is the average of 3 individual ratings .   We also requested the annotators to judge the   overall commonsense of a dialogue response   before introducing event commonsense to an-   notators . Among the 900 annotation pairs we   collected , the Spearman correlation between the   two scores reaches 0.862 , which indicates that   event commonsense is a key component in overall   commonsense reasoning .   4.4 Additional Human - Machine Dialogues   We further explore the generalization ability of   ACCENT on responses with human - machine di-   alogue histories . We select 100 samples from Con-   TurE ( Ghazarian et al . , 2022a ) , a turn - level evalua-   tion dataset , to annotate event commonsense scores .   We denote this dataset as ConTurE Subset . Its   statistics are also included in Appendix D.5 Experiments   5.1 Setups   We compare ACCENT with baseline methods for   event commonsense evaluation and also examine   its two components separately . Therefore , our ex-   periments include three setups for the evaluation :   Setup 1 ( Metrics Performance ) Our main goal   is to evaluate the commonsense metric , and we   achieve this by computing the correlation between   automatic scores and human judgments . ACCENT   and baseline metrics are tested on DECO test set   and ConTurE Subset .   Setup 2 ( Event - Relation Extraction ) We evalu-   ate the performance of the event - relation extraction   component of ACCENT by comparing the automat-   ically extracted tuples with human extracted tuples   on DECO test set . We view checking whether a tu-   ple with relation ris extracted from the utterances   uanduas a binary classification problem and   compute the F1 score . We also measure how “ close ”   the automatically extracted head and tail events are   to human extraction results . We convert the tuple   into a sentence by concatenating the head and tail ,   and then compute BLEU-2 ( Papineni et al . , 2002 )   and BERTScore ( Zhang et al . , 2020c ) .   Setup 3 ( Compatibility Test ): The compatibil-   ity test component of ACCENT can be viewed as   a tuple scoring task . We compare our proposed   approach with other tuple scoring methods on a   large - scale benchmark ( Fang et al . , 2021a ) which   contains event - relation tuples with 0 ( compatible to   a given CSKB ) or 1 ( not compatible to the CSKB )   scores . Since the training relations in this bench-   mark differ from relations supported by the off - the-   shelf COMET , we train our own COMET on its   training set ( see Appendix E.2 for more details ) to   make our compatibility test component applicable   to this test set . This benchmark dataset covers all   12 relations in ˜Ras well as 6 more relations .   5.2 Baselines   We compare ACCENT with 5 baseline metrics :   ( 1 , 2 ) FED - understandable /appropriate ( Mehri   and Eskenazi , 2020 ) are two off - the - shelf baselines .   “ Understandable ” and “ Semantically Appropriate ”   are closer to commonsense compared to the rest of   the criteria in FED . ( 3 ) Cross - encoder is a widely   used model for sentence - pair regression tasks . We   use BART ( Lewis et al . , 2020 ) as the backbone .   ( 4)Cross - encoder ( COMET ) is a variant of ( 3 )   with COMET trained on ATOMICas the back-4402bone . ( 5 ) MLP regressor ( Zhou et al . , 2021 ) is   trained with neural features from DialoGPT and   symbolic features from ConceptNet ( details in § 7 ) .   The cross - encoders and the MLP regressor require   event commonsense scores to train the model in an   end - to - end manner . We use the annotated scores in   DECO training set to train them , and split 20 % data   for validation to conduct hyperparameter search .   For Setup 2 , we consider the following baseline   approaches : ( 1 ) ASER Extractor ( Zhang et al . ,   2020b ) first extracts events through patterns from   dependency parsing and then uses a neural classi-   fier to predict the relation . ( 2 ) CSKB Search ( Zhou   et al . , 2021 ) searches the one - hop neighbors in   ATOMICthrough keyword matching .   For Setup 3 , we consider 4 tuple scoring base-   lines . These baselines convert a tuple to an em-   bedding and train a binary classifier to give score :   ( 1)B feeds h , r , t toB and concatenates   their [ CLS ] embeddings to get the tuple embedding .   ( 2)BSAGE ( Fang et al . , 2021b ) further con-   catenates the average embedding of the neighbors   ofhandtin an event knowledge graph . ( 3 ) KG-   B ( Yao et al . , 2019 ) inputs “ [ CLS ] , h , [ SEP ] ,   r , [ SEP ] , t ” to get the tuple embedding . ( 4 ) KG-   BSAGE ( Fang et al . , 2021a ) further concate-   nates the average embedding of neighboring nodes .   We use RoBERTa ( Liu et al . , 2020 ) as the   backbone which has roughly the same parameter   budget with COMET to have a fair comparison .   The details of the baseline implementations are   in Appendix E.1 .   5.3 ACCENT Implementation   The proposed ACCENT framework is implemented   using the Transformers library ( Wolf et al . , 2020 ) .   For event - relation extraction , we fine - tune T5-   basefor 50 epochs with the batch size of 4 and   the learning rate of 5e-5 . The training data comes   from the human extracted tuples from DECO train-   ing set . We additionally select 5 negative sam-   ples ( dialogues that do not have a certain rela-   tion ) per relation from the training set and set   their target output as “ None ” to guide the model   to handle cases which do not contain a certain re-   lation . During inference , if no tuple is extracted   after considering all relations , we assign a score   of 0.5 to the sample . For compatibility test , we   use the off - the - shelf COMET model trained on   ATOMIC(Hwang et al . , 2021 ) . When querying   COMET through generation , we use beam search   with a beam size of 10 to get commonly sensible   tail events . The embed ( · ) in Equation ( 1 ) is pa-   rameterized by paraphrase - MiniLM - L6 - v2   provided in the Sentence - Transformers library .   6 Results and Analysis   6.1 Metrics Performance   Table 1 shows the correlations between auto-   matic scores and human annotations . ACCENT   uniformly outperforms the baselines on both   two test sets . Specifically , off - the - shelf metrics   ( “ FED - appropriate ” , “ FED - understandable ” ) per-   form poorly . For “ Cross - encoder ( COMET ) ” , its re-   sults show that implicitly using the CSKB through   transfer learning can not yield satisfactory perfor-   mance . Besides , cross - encoders fail to generalize   well to ConTurE Subset whose dialogue histories   are from human - machine dialogues . For “ MLP   Regressor ” , although it tries to utilize the CSKB   explicitly , it is not as effective as ACCENT .   Some examples from the DECO test set and their   event commonsense scores given by ACCENT are   shown in Table 2 . These scores are close to human   judgements and enjoy great interpretability owning   to the extracted event - relation tuples .   Apart from the sample - level correlation , we fur-   ther examine whether ACCENT can reflect model   performance in terms of event commonsense . Fig-   ure 6 shows the rankings of three dialogue systems   used in DECO construction given by human and   ACCENT . Human and ACCENT rank the three sys-   tems exactly the same and the two sets of averaged   scores highly correlates with each other.4403   6.2 Tuple Extraction Performance   Table 3 shows the results of Setup 2 where we eval-   uate the event - relation extraction performance on   DECO test set . Our proposed method achieves   much higher BLEU and BERTScore than two base-   lines , indicating that the composed events in tu-   ples have reasonable quality . However , joint event-   relation extraction remains challenging because it   combines the event extraction and relation identifi-   cation . Although our proposed method has higher   score than ASER Extractor by F1 , it still has plenty   of room for improvement . As for CSKB Search , it   usually returns a lot of tuples , thus resulting in high   recall and very poor precision . Also , searching   CSKB is not applicable in our framework because   this method can only return sensible tuples .   6.3 Compatibility Test Performance   Table 4 depicts the test results on the benchmark   dataset . Our method outperforms all baselines ,   and it does not require negative samples for train-   ing . The major difference between our method   and those tuple scoring baselines is that we use   the tuples in the existing CSKB to train a dynamic   CSKB , i.e. , COMET , instead of a discriminative   model . We assume our strong results may be due   to the generalization ability of the dynamic CSKB .   6.4 Ablation Studies   We conduct ablation studies to explore ( 1 ) whether   the proposed event - relation extraction method can4404   lead to better final metric performance ; ( 2 ) given   the automatically extracted tuples , whether the pro-   posed compatibility test method can lead to higher   correlation with human judgment .   To answer ( 1 ) , we compare different methods   to get the event - relation tuples ( Part I in Table 5 ) .   Among the event - relation extraction baselines , we   only consider ASER Extractor because CSKB   search is not applicable in our framework as dis-   cussed in § 6.2 . Note that the event - relation extrac-   tor in ACCENT considers tuples in both “ Single ”   and “ Pair ” settings to cover two potential types   of errors ( see § 3.2 ) . To verify this , we compare   the variations of our proposed method where we   only use tuples marked as “ Single ” or “ Pair ” for   model training . Also , the human extracted tuples in   DECO test set are used to provide an upper bound .   To answer ( 2 ) , we fix the event - relation extrac-   tion part and change the compatibility test part ( Part   II in Table 5 ) . We consider B andKG - B   trained on the CSKB compatibility benchmark be-   cause they do not need event graph information   and can be seamlessly applied to our compatibil-   ity test . Also , while we query COMET through   tail generation , another intuitive design is using the   model loss with “ { h } { r } [ GEN ] ” as the source and   tas the target to give scores . We map the loss to   ( 0,1)through an exponential function , and name   this alternative as “ COMET ( neural ) ” for it skips   the symbolic decoding of t.   Table 5 demonstrates that the whole ACCENT   gives the best result . Considering the variations of   our design , “ w/o Pair ” gives much lower results ,   indicating that limiting the symbolic intermediate   representation to only the information contained in   the target response is not enough . This observation   is in accord with our finding that some event com-   monsense errors occur when we take the dialogue   history into account . Another empirical discovery is that although   “ COMET ( neural ) ” is a direct way of using the dy-   namic CSKB , its performance is poorer than what   we propose in ACCENT . We assume that compar-   ingtandtin a symbolic fashion can yield more   comparable scores among tuples with different re-   lations ( details in Appendix F ) .   In our implementation of ACCENT , the compar-   ison of tandtis done by calculating the cosine   similarity between their Sentence - BERT embed-   dings . We further experiment with other sentence   embedding methods based on contrastive learning .   Specifically , we consider DiffCSE ( Chuang et al . ,   2022 ) , ESimCSE ( Wu et al . , 2022 ) which are two   unsupervised contrastive learning frameworks for   learning sentence embeddings . We also consider   Sup - SimCSE ( Gao et al . , 2021 ) which leverages   annotated natural language inference datasets by   using “ entailment ” pairs as positives and “ contra-   diction ” pairs as hard negatives in the contrastive   learning objective . As shown in Table 6 , ACCENT   can benefit from the improvement of the sentence   embedding method , i.e. , using Sup - SimCSE ( Gao   et al . , 2021 ) . We support both Sentence - BERT and   Sup - SimCSE in our released ACCENT codebase .   6.5 Error Analysis   Since ACCENT is a pipeline framework , there is   likely error propagation . In section 6.4 , we rule   out the errors introduced by the event - relation ex-   traction component by using human - extracted gold   tuples . Results show that ACCENT with gold tu-   ples ( see “ Gold Tuples ” in Table 5 ) gives higher   correlation with human judgment than “ ACCENT   ( whole ) ” which uses the model - extracted tuples ,   indicating that ACCENT can benefit from high   quality symbolic intermediate representation . We   further include a qualitative analysis of the automat-   ically extracted tuples in Appendix G , and believe   improving the joint event - relation extraction is a   worthwhile direction for future work .   7 Related Work   Automatic Open - Domain Dialogue Evaluation   The evaluation of open - domain dialogue systems   has long been a challenge due to the system ’s open-4405ended goal ( Huang et al . , 2020 ) , and simply scor-   ing the overall quality is far from enough ( Finch   and Choi , 2020 ) . Thus , researchers have decom-   posed the evaluation of open - domain dialogues   into multiple facets and developed corresponding   automatic evaluation metrics ( Pang et al . , 2020 ;   Mehri and Eskenazi , 2020 ) . While aspects like con-   text coherence ( Tao et al . , 2018 ; Ghazarian et al . ,   2022b ) , diversity ( Hashimoto et al . , 2019 ) , engage-   ment ( Ghazarian et al . , 2020 ) , have been systemat-   ically studied in the literature , the aspect of com-   monsense has long been neglected .   The closest related work is Zhou et al . ( 2021 )   which is mainly about commonsense - focused dia-   logues collection but also proposes an automatic   metric for commonsense evaluation by training   an MLP regressor on both symbolic and neural   features . The symbolic features include the num-   bers of one - hop and two - hop triplets in Concept-   Net ( Speer et al . , 2017 ) that can be found between   the target response and its dialogue history . Al-   though this metric utilizes the CSKB explicitly , it   is limited to the direct search with surface form   and only considers the number of triplets , and the   CSKB used in the work is more about concepts but   not event commonsense .   Joint Event - Relation Extraction While event   extraction ( Ahn , 2006 ) and relation identifica-   tion ( Do et al . , 2011 ) are well - studied , how to   jointly acquire them remains a challenge . We argue   that joint event - relation extraction is an important   problem because in practical use cases , the input is   usually free - form text without pre - extracted events .   Zhang et al . ( 2020b ) is a pioneer work trying to   jointly extract event and relation through a pipeline   to automatically construct large knowledge graphs .   Researchers in this work resort to rule - based meth-   ods for event extraction and train a classifier to   predict the relation between a pair of events .   CSKB Compatibility CSKB population en-   larges CSKB automatically by adding new links   or nodes which are compatible with the common-   sense knowledge to the existing CSKB . In Fang   et al . ( 2021a , b ) , researchers try to add events from   a large event knowledge graph to a CSKB . Compat-   ibility test component of ACCENT is relevant to   CSKB population task and it is defined in a more   general setting where the head and tail of the tuple   can be arbitrary events.8 Conclusion   We present ACCENT , an automatic evaluation met-   ric for event commonsense evaluation of open-   domain dialogue systems . We show that by using   event - relation tuples as the symbolic intermediate   representations , ACCENT can effectively utilize   the CSKB and achieve a decent correlation with   human judgments for dialogue commonsense eval-   uation .   9 Limitations   In this work , we conduct research on event com-   monsense of open - domain dialogue systems for   the first time . While achieving higher correlations   with human judgments than existing baselines , AC-   CENT has some limitations :   First , the ACCENT framework is based on a   fixed set of event relations and the commonsense   knowledge in ATOMICwhich may fail to cover   some potential event commonsense aspects . We be-   lieve augmenting the current framework with more   commonsense resources is a worthwhile direction   for the further improvement of ACCENT .   Second , the event - relation extractor in ACCENT   framework is a T5 model fine - tuned in a low   resource setting . Although the current model   can yield fairly strong performance , it is an im-   portant research direction to improve the joint   event - relation extraction component because the   extracted tuples serve as the symbolic represen-   tation for commonsense reasoning in ACCENT   framework . Since human extracted tuples are very   costly to collect , we hope to explore whether we   can improve this component through high - quality   synthetic data construction or transfer learning in   the future .   10 Acknowledgments   We thank the PlusLab members and the anonymous   reviewers for their constructive feedback . This   work is supported in part by the DARPA Machine   Common Sense ( MCS ) program under Coopera-   tive Agreement N66001 - 19 - 2 - 4032 , and a Meta   Sponsored research award .   References4406440744084409A Commonsense Knowledge Bases   To endow machines with commonsense reason-   ing abilities , a growing number of CSKBs are   developed through human annotation and infor-   mation extraction . From earlier CSKBs , Concept-   Net ( Liu and Singh , 2004 ; Speer et al . , 2017 ) fo-   cuses more on taxonomic ( e.g. , “ IsA ” ) and lexical   ( e.g. , “ Synonym ” , “ RelatedTo ” ) knowledge ; Tran-   sOMCS ( Zhang et al . , 2020a ) automates the knowl-   edge base construction by leveraging the same lim-   ited set of relations defined in ConceptNet .   Recent CSKBs give more focus on event com-   monsense knowledge . In this work , we select   ATOMIC(Hwang et al . , 2021 ) as the knowl-   edge source of ACCENT framework because it is a   comprehensive CSKB with rich knowledge regard-   ing how events and human actions are associated   with each other . For comparison , ATOMIC ( Sap   et al . , 2019a ) , as the pioneer of ATOMIC , con-   sists of only nine relations and therefore poses   limitations . Another recent event - centric CSKB   is GLUCOSE ( Mostafazadeh et al . , 2020 ) , which   however focuses on a specific part of event com-   monsense ( mostly on causal inference ) and is less   comprehensive and suitable for our work .   B Pseudo Code of ACCENT   In § 3 , we introduce the symbolic intermediate rep-   resentation and the two components in ACCENT .   Algorithm 1 displays the skeleton of the whole   framework .   Line 3 - 9 in Algorithm 1 show the joint event-   relation extraction in ACCENT . We query the event-   relation extraction model Mwith prompts for each   relation . The head and tail events can be parsed   from the generated result if it is not “ None ” and fol-   lows the pre - defined format , i.e. , “ event1 : { head } ;   event2 : { tail } ” . Line 16 - 21 in Algorithm 1 show   the compatibility test in ACCENT . Each tuple is   given a score based on the maximum cosine sim-   ilarity between its tail and the commonsense tails   obtained from the dynamic CSKB C. After calcu-   lating scores for each extracted tuple , we average   them to get the event commonsense score for the   target utterance ( Line 24 in Algorithm 1 ) .   C Event Relations   As introduced in § 3.1 , ACCENT selects relations   from ATOMICwhich are related to event com-   monsense . These event relations can help coverAlgorithm 1 : ACCENT framework .   Input : Dialogue history h , target   utterance u , prompt dict P ,   extractor M , dynamic CSKB C ,   sentence embedder E   Output : Event commonsense score stuples← [ ] // Event - relation extraction.foreach ( rel , prompt ) in Pdo raw_output ←generate ( M , h , u , p ) ifcheck_format ( raw_output ) then ( head , tail ) ←parse ( raw_output ) tuples.append((head , rel , tail ) ) endend// Compatability test.ifis_empty ( tuples ) then return 0.5else tuple_scores ← [ ] foreach ( head , rel , tail ) in tuples do score←0 cs_tails ←query ( C , head , rel ) foreach cs_tail in cs_tails do x←cos(E(tail),E(cs_tail ) ) score←max(score , x ) end tuple_scores.append(score ) end return average ( tuple_scores)end4410different types of insensibility for event common-   sense evaluation of open - domain dialogues . Ta-   ble 7 shows examples in DECO where the system   response violates event commonsense in terms of   different types of event relations .   Note that although “ Cause ” and “ xReason ” in   ATOMICare also related to event commonsense ,   we exclude them from the selected subset ˜R. This   is because the cause - effect relation can be cov-   ered by “ xEffect”/“oEffect ” and tuples with “ Cause ”   and “ xReason ” relations take up less than 0.1 % per-   cent of ATOMIC . Moreover , we exclude “ IsBe-   fore ” because a tuple with “ IsBefore ” relation can   be equivalently converted to a tuple with “ IsAfter ”   relation by switching the head and tail . As shown   in Table 8 , for each relation in ˜R , a prompt is   manually designed to explain its semantic meaning .   These designed prompts give more hints to the pre-   trained model and allow a single model to extract   tuples for different relations .   D Additional Details of Data Collection   D.1 Quality Control   To ensure the annotators have a good understanding   of event and event commonsense , we restrict the   annotators from English - speaking countries , and   those who have finished at least 5,000 HITs with   an acceptance rate > 97 % . The compensation rate   for annotators is calculated using a per hour wage   of $ 16 .   For commonsense scoring ( see § 4.3 ) , we re-   quested 3 annotators to score each sample , and   we instructed them to specifically consider events   and their relations in the dialogue to give the event   commonsense score . Figure 7 shows the annotation   guideline we used for event commonsense scoring .   We also set a sample for attention check in each   HIT . HITs that failed the check were reassigned to   other annotators .   For tuple extraction ( see § 4.2 ) , we conducted   a training round before the large scale annotation   and 8 annotators proceeded to the final round . Each   HIT in this task was assigned to 2 individual anno-   tators . The template used to collect event - relation   tuples is shown in Figure 8 . When validating   the extracted tuples , 3 annotators judged each tu-   ple , and we achieved Fleiss ’ Kappa ( Fleiss , 1971 )   κ= 0.491(moderate aggreement ) . Tuples markedas invalid by the majority vote are not included in   the final dataset .   D.2 Dataset Statistics   Table 9 gives the statistics of DECO and ConTurE   Subset . Although machine generated responses in   DECO are given by advanced open - domain dia-   logue systems , some event commonsense errors   still exist . For ConTurE Subset , we use it to test   the generalization ability of different metrics . Ta-   ble 10 gives the numbers of human extracted event-   relation tuples . Note that the test set of DECO   is thoroughly annotated ( we consider every rela-   tion on each sample ) to provide a test set for the   joint event - relation extraction task . All the data we   collected are in English .   D.3 License   The development of DECO and ConTurE Subset   is based on the dialogues coming from DailyDia-   log , PersonaChat , TopicalChat , and ConTurE. Per-   sonaChat , TopicalChatand ConTurEare li-   censed . We ensure that we did not violate any   license conditions when developing our datasets .   E Additional Details of Experiment   This section describes more details about base-   line implementation , applying ACCENT to CSKB   benchmark , and computational resources . The   implementation details of the proposed ACCENT   framework are discussed in § 5.3   E.1 Baseline Implementation   We compare ACCENT with 5 baseline metrics on   event commonsense evaluation . All the metrics are   tested on DECO test set and ConTurE Subset . For   metrics which require training , the training set of   DECO is used , and we split 20 % data for validation .   The implementation details are as follows :   •FED - understandable / appropriate : We use   their released model .   •Cross - encoder : We use the cross - encoder   with a regression head implemented in the   Sentence - Tranformers library ( Reimers and44114412   Gurevych , 2019 ) . We use BART as the back-   bone model to align with the COMET model   used in our method . For hyperparameter   search , we fine - tune the cross - encoder for 10   epochs with the batch size of 8 and the learn-   ing rate from { 1e-5 , 3e-5 , 5e-5}.•Cross - encoder ( COMET ) : We use the   off - the - shelf COMET model trained on   ATOMICas the backbone model . Other   implementation details are the same with   Cross - encoder .   •MLP regressor : Since the code of Zhou   et al . ( 2021 ) is not publicly available , we pro-   duce the results using our own implementation   based on scikit - learn . Our implementation   is available in our released codebase .   For event - relation extraction baselines , the im-4413   plementation details are as follows :   •ASER Extractor : We use their provided   codeto extract events . The neural classi-   fier for relation prediction is trained on the   human annotated tuples in DECO training set .   •CSKB Search : We search tuples related to   the target response and its previous response   inATOMICfollowing the CSKB search   pipeline described in Zhou et al . ( 2021 ) . A po-   tential concept set is built from the utterances   by identifying nouns , verbs , and adjectives   that are not stopwords through part - of - speech   ( POS ) tagging and lemmatizing them . We re-   turn tuples whose head and tail both contain   words in the concept set as the search result .   For CSKB population baselines , we use the im-   plementation in Fang et al . ( 2021a ) . For the back-   bone model , we use RoBERTa which has   roughly the same parameter budget with COMET .   We train all models for 1 epoch with the batch size   of 64 . The learning rate is searched from { 1e-7 ,   1e-5 , 1e-3 } on the validation set .   E.2 Applying ACCENT to CSKB Benchmark   In § 5 Setup 3 , we apply the compatibility test ap-   proach in ACCENT to a CSKB benchmark . Such   an application is seamless because the compatibil-   ity test also assigns a score to each tuple , and tuples   which receive higher compatibility scores are nat-   urally more suitable to populate the CSKB . We   train the COMET model on the positive samples   in the training set of the benchmark dataset for 1   epoch with the batch size of 64 . The learning rate is   searched from { 1e-7 , 1e-5 , 1e-3 } on the validation   set . Note that our approach does not require any   negative sample in the training stage . It also does   not need the event graph information provided in   the benchmark dataset , but results in Table 4 showsPearson Spearman   COMET ( neural ) 0.14 0.25   ACCENT approach 0.40 0.42   that our method outperforms baselines which re-   quire manually created negative data and take in   graph information .   E.3 Computational Resources   We run BSAGE andKG - BSAGE for the   CSKB benchmark experiments on a single Nvidia   V100 GPU with 32 GB memory where these mod-   els require large memory consumption in the run   time . The rest of experiments is done on a single   Nvidia A10 GPU with 24 GB memory .   Note that although we develop the ACCENT   framework based on large language models , the   only part which requires training is the T5 model   ( with 223 M parameters ) for event - relation extrac-   tion . As discussed in § 3.2 , the model is fine - tuned   in a low resource setting and the training takes less   than 0.5 GPU hour .   F More Discussion of the Compatibility   Test Approach in ACCENT   ACCENT checks whether an event - relation tu-   ple(h , r , t ) is compatible with the commonsense   knowledge by comparing the similarity between   tand commonsense tails generated by the Dy-   namic CSKB ( COMET ) . Ablation results in Ta-   ble 5 show that the compatibility test approach   in ACCENT yields better performance than the   “ COMET ( neural ) ” alternative which also uses the   COMET model . To exclude the potential noise in-   troduced by the automatically extracted tuples , we   further compare these two methods using human-   extracted tuples on DECO test set . Results in Ta-   ble 11 demonstrate that the conclusion still holds   under this experimental setting . Table 12 gives4414   two samples with a breakdown of tuple results .   Compared with the compatibility scores given by   ACCENT approach , the scores given by “ COMET   ( neural ) ” are less comparable among tuples with   different relations , thus making this method unsuit-   able for ACCENT framework .   G Error Analysis   We conduct a qualitative analysis of the event-   relation component in ACCENT . Table 13 shows   some examples of the extracted tuples . While most   of the head and tail events are nicely composed   and capture the major information in the given text ,   they are not perfect . Multiple participants involved   in the dialogue further increase the difficulty of the   task . We note that the model sometimes confuses   the multiple participants in the dialogue and makes   mistakes when using “ PersonX ” or “ PersonY ” . For   example , in the third sample of Table 13 , the model   confuses different participants since the subjects   of “ raise money ” and “ go tomorrow for treatment ”   should be different . Such confusion will lead to the   wrong tuples which can not truly reflect the mean-   ing of the dialogue . Also , identifying relation from   the given dialogue is challenging . Although we in-   clude negative samples ( dialogues that do not have   a certain relation ) when fine - tuning T5 , errors still   exist ( e.g. , the tuple with “ xAttr ” relation in the   third sample of Table 13).441544164417ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Sections 6.2 , 6.4 and Appendix H include the error analysis of the ACCENT framework . A separate   section of “ Limitations ” is also included in Appendix A.   /squareA2 . Did you discuss any potential risks of your work ?   We do not include potential risks of our work , since we believe that none of the components in   our model and the dataset by itself can produce offensive results . The responses generated and   augmented to DECO dataset are coming from previously proposed state - of - the - art models which   are trained on datasets without profanity and inappropriate utterances . Other than that , our work is   pertinent to evaluation and has less feasibility of potential risks .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Introduction summarize the paper ’s main claims . Three experimental setups and results   show the ACCENT ’s superiority versus baselines .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   In sections 5.1 and 6.3 , we cite the CSKB population benchmark . Section 4.1 has the citations for   each of the dialogue models used for the DECO response generation .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Some dialogue sources we used come with licence . We discuss them in Appendix D.3 .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Sections 5.1 and 6.3 use existing CSKB population benchmark that are compatible with the conditions   that data was granted to be used . Our collected data is anonymized and section 4 describes the data   use cases and statements .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 4 is mainly about data collection and creation , it discusses the data included in the collection   process which are human judgments and it does n’t reveal the identity of users participated in the data   collection process . The inputs for evaluation are coming from existing human - written conversational   data and responses generated by dialogue models do not include inappropriate content .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Appendix D includes the language of collected dataset alongside the information about the data and   its details.4418 / squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 and Section D.2 in the Appendix are about the size of datasets .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix E.3 reports the computational resources .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Yes we discussed them in Appendix E.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Sections 5 and 6 include the descriptive statistics about your results . Table 1 also demonstrates the   results for ACCENT that all are signiﬁcant ( p < 0.05 ) .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 4 and Appendix D contain all the details about human annotations and conducted experi-   ments .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   The crowdsourcing platform is discussed in Section 4 , and we discuss how we pay the annotators in   Appendix D.1 .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Yes , it was indicated in the AMT HIT pages .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . We did not include ethics review board for out data collection process .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix D.1 reports the basic demographic and geographic characteristics of the annotator   population.4419