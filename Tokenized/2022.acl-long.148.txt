  Yunlong Liang , Fandong Meng , Chulun Zhou , Jinan Xu ,   Yufeng Chen , Jinsong SuandJie ZhouBeijing Key Lab of Traffic Data Analysis and Mining , Beijing Jiaotong University , ChinaPattern Recognition Center , WeChat AI , Tencent Inc , ChinaSchool of Informatics , Xiamen University   { yunlongliang,jaxu,chenyf}@bjtu.edu.cn clzhou@stu.xmu.edu.cn   jssu@xmu.edu.cn { fandongmeng,withtomzhou}@tencent.com   Abstract   The goal of the cross - lingual summarization   ( CLS ) is to convert a document in one language   ( e.g. , English ) to a summary in another one   ( e.g. , Chinese ) . Essentially , the CLS task is the   combination of machine translation ( MT ) and   monolingual summarization ( MS ) , and thus   there exists the hierarchical relationship be-   tween MT&MS and CLS . Existing studies on   CLS mainly focus on utilizing pipeline meth-   ods or jointly training an end - to - end model   through an auxiliary MT or MS objective . How-   ever , it is very challenging for the model to di-   rectly conduct CLS as it requires both the abili-   ties to translate and summarize . To address this   issue , we propose a hierarchical model for the   CLS task , based on the conditional variational   auto - encoder . The hierarchical model contains   two kinds of latent variables at the local and   global levels , respectively . At the local level ,   there are two latent variables , one for transla-   tion and the other for summarization . As for the   global level , there is another latent variable for   cross - lingual summarization conditioned on the   two local - level variables . Experiments on two   language directions ( English ⇔Chinese ) verify   the effectiveness and superiority of the pro-   posed approach . In addition , we show that our   model is able to generate better cross - lingual   summaries than comparison models in the few-   shot setting .   1 Introduction   The cross - lingual summarization ( CLS ) aims to   summarize a document in source language ( e.g. ,   English ) into a different language ( e.g. , Chinese ) ,   which can be seen as a combination of machine   translation ( MT ) and monolingual summarization   ( MS ) to some extent ( Or ˘asan and Chiorean , 2008 ;   Zhu et al . , 2019 ) . The CLS can help people ef-   fectively master the core points of an article in aforeign language . Under the background of glob-   alization , it becomes more important and is now   coming into widespread use in real life .   Many researches have been devoted to deal-   ing with this task . To our knowledge , they   mainly fall into two categories , i.e. , pipeline and   end - to - end learning methods . ( i ) The first cate-   gory is pipeline - based , adopting either translation-   summarization ( Leuski et al . , 2003 ; Ouyang et al . ,   2019 ) or summarization - translation ( Wan et al . ,   2010 ; Or ˘asan and Chiorean , 2008 ) paradigm . Al-   though being intuitive and straightforward , they   generally suffer from error propagation problem .   ( ii ) The second category aims to train an end-   to - end model for CLS ( Zhu et al . , 2019 , 2020 ) .   For instance , Zhu et al . ( 2020 ) focus on using a   pre - constructed probabilistic bilingual lexicon to   improve the CLS model . Furthermore , some re-   searches resort to multi - task learning ( Takase and   Okazaki , 2020 ; Bai et al . , 2021a ; Zhu et al . , 2019 ;   Cao et al . , 2020a , b ) . Zhu et al . ( 2019 ) separately   introduce MT and MS to improve CLS . Cao et al .   ( 2020a , b ) design several additional training objec-   tives ( e.g. , MS , back - translation , and reconstruc-   tion ) to enhance the CLS model . And Xu et al .   ( 2020 ) utilize a mixed - lingual pre - training method   with several auxiliary tasks for CLS .   As pointed out by Cao et al . ( 2020a ) , it is chal-   lenging for the model to directly conduct CLS as   it requires both the abilities to translate and sum-   marize . Although some methods have used the   related tasks ( e.g. , MT and MS ) to help the CLS ,   the hierarchical relationship between MT&MS and   CLS are not well modeled , which can explicitly en-   hance the CLS task . Apparently , how to effectively   model the hierarchical relationship to exploit MT   and MS is one of the core issues , especially when   the CLS data are limited . In many other related   NLP tasks ( Park et al . , 2018 ; Serban et al . , 2017;2088Shen et al . , 2019 , 2021 ) , the Conditional Varia-   tional Auto - Encoder ( CV AE ) ( Sohn et al . , 2015 )   has shown its superiority in learning hierarchical   structure with hierarchical latent variables , which is   often leveraged to capture the semantic connection   between the utterance and the corresponding con-   text of conversations . Inspired by these work , we   attempt to adapt CV AE to model the hierarchical   relationship between MT&MS and CLS .   Therefore , we propose a Variational Hierarchi-   cal Model to exploit translation and summarization   simultaneously , named VHM , for CLS task in an   end - to - end framework . VHM employs hierarchical   latent variables based on CV AE to learn the hier-   archical relationship between MT&MS and CLS .   Specifically , the VHM contains two kinds of latent   variables at the local and global levels , respectively .   Firstly , we introduce two local variables for trans-   lation and summarization , respectively . The two   local variables are constrained to reconstruct the   translation and source - language summary . Then ,   we use the global variable to explicitly exploit   the two local variables for better CLS , which is   constrained to reconstruct the target - language sum-   mary . This makes sure the global variable captures   its relationship with the two local variables without   any loss , preventing error propagation . For infer-   ence , we use the local and global variables to assist   the cross - lingual summarization process .   We validate our proposed training framework   on the datasets of different language pairs ( Zhu   et al . , 2019 ): Zh2EnSum ( Chinese ⇒English ) and   En2ZhSum ( English ⇒Chinese ) . Experiments   show that our model achieves consistent improve-   ments on two language directions in terms of both   automatic metrics and human evaluation , demon-   strating its effectiveness and generalizability . Few-   shot evaluation further suggests that the local and   global variables enable our model to generate a   satisfactory cross - lingual summaries compared to   existing related methods .   Our main contributions are as follows :   •We are the first that builds a variational hi-   erarchical model via conditional variational   auto - encoders that introduce a global variable   to combine the local ones for translation and   summarization at the same time for CLS .   •Our model gains consistent and significant   performance and remarkably outperforms themost previous state - of - the - art methods after   using mBART ( Liu et al . , 2020 ) .   •Under the few - shot setting , our model still   achieves better performance than existing ap-   proaches . Particularly , the fewer the data are ,   the greater the improvement we gain .   2 Background   Machine Translation ( MT ) . Given an input se-   quence in the source language X={x } , the   goal of the neural MT model is to produce its trans-   lation in the target language Y={y } . The   conditional distribution of the model is :   p(Y|X ) = Yp(y|X , y ) ,   where θare model parameters and yis the   partial translation .   Monolingual Summarization ( MS ) . Given   an input article in the source language   X={x } and the corresponding summa-   rization in the same language X={x } ,   the monolingual summarization is formalized as :   p(X|X ) = Yp(x|X , x ) .   Cross - Lingual Summarization ( CLS ) . In CLS ,   we aim to learn a model that can generate a   summary in the target language Y={y }   for a given article in the source language   X={x } . Formally , it is as follows :   p(Y|X ) = Yp(y|X , y ) .   Conditional Variational Auto - Encoder ( CV AE ) .   The CV AE ( Sohn et al . , 2015 ) consists of one prior   network and one recognition ( posterior ) network ,   where the latter takes charge of guiding the learn-   ing of prior network via Kullback – Leibler ( KL )   divergence ( Kingma and Welling , 2013 ) . For ex-   ample , the variational neural MT model ( Zhang   et al . , 2016a ; Su et al . , 2018a ; McCarthy et al . ,   2020 ; Su et al . , 2018c ) , which introduces a random   latent variable zinto the neural MT conditional   distribution :   p(Y|X ) = Zp(Y|X , z)·p(z|X)dz .   ( 1 )   Given a source sentence X , a latent variable zis2089firstly sampled by the prior network from the en-   coder , and then the target sentence is generated   by the decoder : Y∼p(Y|X , z ) , where   z∼p(z|X ) .   As it is hard to marginalize Eq . 1 , the CV AE   training objective is a variational lower bound of   the conditional log - likelihood :   where ϕare parameters of the CV AE .   3 Methodology   Fig . 1 demonstrates an overview of our model , con-   sisting of four components : encoder , variational   hierarchical modules , decoder , training and infer-   ence . Specifically , we aim to explicitly exploit the   MT and MS for CLS simultaneously . Therefore ,   we firstly use the encoder ( § 3.1 ) to prepare the rep-   resentation for the variational hierarchical module   ( § 3.2 ) , which aims to learn the two local variables   for the global variable in CLS . Then , we introduce   the global variable into the decoder ( § 3.3 ) . Fi-   nally , we elaborate the process of our training and   inference ( § 3.4 ) .   3.1 Encoder   Our model is based on transformer ( Vaswani et al . ,   2017 ) framework . As shown in Fig . 1 , the encoder   takes six types of inputs , { X , X , X , Y ,   X , Y } , among which Y , X , and Yare   only for training recognition networks . Taking X   for example , the encoder maps the input Xinto a   sequence of continuous representations whose size   varies with respect to the source sequence length .   Specifically , the encoder consists of Nstacked   layers and each layer includes two sub - layers : a   multi - head self - attention ( SelfAtt ) sub - layer and   a position - wise feed - forward network ( FFN ) sub-   layer :   s= SelfAtt ( h ) + h ,   h= FFN ( s ) + s ,   where hdenotes the state of the ℓ-th encoder layer   andhdenotes the initialized embedding .   Through the encoder , we prepare the representa-   tions of { X , X , X } for training prior net-   works , encoder and decoder . Taking Xfor ex-   ample , we follow Zhang et al . ( 2016a ) and apply   mean - pooling over the output h of the N - th   encoder layer :   h=1   |X|X(h ) .   Similarly , we obtain handh .   For training recognition networks , we obtain the   representations of { Y , X , Y } , taking Y   for example , and calculate it as follows :   h=1   |Y|X(h ) .   Similarly , we obtain handh .   3.2 Variational Hierarchical Modules   Firstly , we design two local latent variational mod-   ules to learn the translation distribution in MT pairs   and summarization distribution in MS pairs , respec-   tively . Then , conditioned on them , we introduce   a global latent variational module to explicitly ex-   ploit them.20903.2.1 Local : Translation and Summarization   Translation . To capture the translation of the   paired sentences , we introduce a local variable z   that is responsible for generating the target infor-   mation . Inspired by Wang and Wan ( 2019 ) , we   use isotropic Gaussian distribution as the prior dis-   tribution of z : p(z|X)∼ N ( µ,σI ) ,   where Idenotes the identity matrix and we have   µ= MLP(h ) ,   σ= Softplus(MLP(h)),(2 )   where MLP(·)andSoftplus ( · ) are multi - layer per-   ceptron and approximation of ReLU function , re-   spectively .   At training , the posterior distribution conditions   on both source input and the target reference , which   provides translation information . Therefore , the   prior network can learn a tailored translation dis-   tribution by approaching the recognition network   viaKLdivergence ( Kingma and Welling , 2013 ):   q(z|X , Y)∼ N ( µ,σI ) , where µ   andσare calculated as :   µ= MLP(h;h ) ,   σ= Softplus(MLP(h;h)),(3 )   where ( · ; · ) indicates concatenation operation .   Summarization . To capture the summarization   in MS pairs , we introduce another local vari-   ablez , which takes charge of generating the   source - language summary . Similar to z , we   define its prior distribution as : p(z|X)∼   N(µ,σI ) , where µandσare calcu-   lated as :   µ= MLP(h ) ,   σ= Softplus(MLP(h)).(4 )   At training , the posterior distribution conditions   on both the source input and the source - language   summary that contains the summarization clue ,   and thus is responsible for guiding the learning   of the prior distribution . Specifically , we define the   posterior distribution as : q(z|X , X)∼   N(µ,σI ) , where µandσare calcu-   lated as :   µ= MLP(h;h ) ,   σ= Softplus(MLP(h;h)).(5 )   3.2.2 Global : CLS   After obtaining zandz , we introduce the   global variable zthat aims to generate a target-   language summary , where the zcan simultane - ously exploit the local variables for CLS . Specifi-   cally , we firstly encode the source input Xand   condition on both two local variables zandz ,   and then sample z. We define its prior distribu-   tion as : p(z|X , z , z)∼ N(µ,σI ) ,   where µandσare calculated as :   µ= MLP(h;z;z ) ,   σ= Softplus(MLP(h;z;z)).(6 )   At training , the posterior distribution conditions   on the local variables , the CLS input , and the cross-   lingual summary that contains combination infor-   mation of translation and summarization . There-   fore , the posterior distribution can teach the prior   distribution . Specifically , we define the posterior   distribution as : q(z|X , z , z , Y)∼   N(µ,σI ) , where µandσare calculated   as :   ( 7 )   3.3 Decoder   The decoder adopts a similar structure to the en-   coder , and each of Ndecoder layers includes an   additional cross - attention sub - layer ( CrossAtt ):   s= SelfAtt ( h ) + h ,   c= CrossAtt ( s , h ) + s ,   h= FFN ( c ) + c ,   where hdenotes the state of the ℓ-th decoder layer .   As shown in Fig . 1 , we firstly obtain the local   two variables either from the posterior distribution   predicted by recognition networks ( training process   as the solid grey lines ) or from prior distribution   predicted by prior networks ( inference process as   the dashed red lines ) . Then , conditioned on the   local two variables , we generate the global variable   ( z / z ) via posterior ( training ) or prior ( infer-   ence ) network . Finally , we incorporate zinto   the state of the top layer of the decoder with a   projection layer :   o= Tanh ( W[h;z ] + b ) , ( 8)   where Wandbare training parameters , his   the hidden state at time - step tof the N - th decoder   layer . Then , ois fed into a linear transformation   and softmax layer to predict the probability distri-2091bution of the next target token :   p= Softmax ( Wo+b ) ,   where Wandbare training parameters .   3.4 Training and Inference   The model is trained to maximize the conditional   log - likelihood , due to the intractable marginal like-   lihood , which is converted to the following vari-   tional lower bound that needs to be maximized in   the training process :   where the variational lower bound includes the re-   construction terms and KL divergence terms based   on three hierarchical variables . We use the repa-   rameterization trick ( Kingma and Welling , 2013 ) to   estimate the gradients of the prior and recognition   networks ( Zhao et al . , 2017 ) .   During inference , firstly , the prior networks of   MT and MS generate the local variables . Then , con-   ditioned on them , the global variable is produced   by prior network of CLS . Finally , only the global   variable is fed into the decoder , which corresponds   to red dashed arrows in Fig . 1 .   4 Experiments   4.1 Datasets and Metrics   Datasets . We evaluate the proposed approach   on Zh2EnSum and En2ZhSum datasets released   by ( Zhu et al . , 2019).The Zh2EnSum and   En2ZhSum are originally from ( Hu et al . , 2015 )   and ( Hermann et al . , 2015 ; Zhu et al . , 2018 ) , respec-   tively . Both the Chinese - to - English and English-   to - Chinese test sets are manually corrected . The   involved training data in our experiments are listed   in Tab . 1 .   Zh2EnSum . It is a Chinese - to - English summariza-   tion dataset , which has 1,699,713 Chinese short   texts ( 104 Chinese characters on average ) paired   with Chinese ( 18 Chinese characters on average )   and English short summaries ( 14 tokens on aver-   age ) . The dataset is split into 1,693,713 training   pairs , 3,000 validation pairs , and 3,000 test pairs .   The involved training data used in multi - task learn-   ing , model size , training time , are listed in Tab . 2 .   En2ZhSum . It is an English - to - Chinese summa-   rization dataset , which has 370,687 English docu-   ments ( 755 tokens on average ) paired with multi-   sentence English ( 55 tokens on average ) and Chi-   nese summaries ( 96 Chinese characters on aver-   age ) . The dataset is split into 364,687 training pairs ,   3,000 validation pairs , and 3,000 test pairs . The   involved training data used in multi - task learning ,   model size , training time , are listed in Tab . 3 .   Metrics . Following Zhu et al . ( 2020 ) , 1 ) we eval-   uate all models with the standard ROUGE met-   ric ( Lin , 2004 ) , reporting the F1 scores for ROUGE-   1 , ROUGE-2 , and ROUGE - L. All ROUGE scores2092 M # ModelsZh2EnSum En2ZhSum   RG1 RG2 RGL MVS RG1 RG2 RGL   M1 GETran ( Zhu et al . , 2019 ) 24.34 9.14 20.13 0.64 28.19 11.40 25.77   M2 GLTran ( Zhu et al . , 2019 ) 35.45 16.86 31.28 16.90 32.17 13.85 29.43   M3 TNCLS ( Zhu et al . , 2019 ) 38.85 21.93 35.05 19.43 36.82 18.72 33.20   M4 ATS - A ( Zhu et al . , 2020 ) 40.68 24.12 36.97 22.15 40.47 22.21 36.89   M5 MS - CLS ( Zhu et al . , 2019 ) 40.34 22.65 36.39 21.09 38.25 20.20 34.76   M6 MT - CLS ( Zhu et al . , 2019 ) 40.25 22.58 36.21 21.06 40.23 22.32 36.59   M7 MS - CLS - Rec ( Cao et al . , 2020a ) 40.97 23.20 36.96 NA 38.12 16.76 33.86   M8 MS - CLS * 40.44 22.19 36.32 21.01 38.26 20.07 34.49   M9 MT - CLS * 40.05 21.72 35.74 20.96 40.14 22.36 36.45   M10 MT - MS - CLS ( Ours ) 40.65 24.02 36.69 22.17 40.34 22.35 36.44   M11 VHM ( Ours ) 41.3624.6437.1522.5540.9823.0737.12   M12 mBART ( Liu et al . , 2020 ) 43.61 25.14 38.79 23.47 41.55 23.27 37.22   M13 MLPT ( Xu et al . , 2020 ) 43.50 25.41 29.66 NA 41.62 23.35 37.26   M14 VHM + mBART ( Ours ) 43.9725.6139.1923.88 41.9523.5437.67   are reported by the 95 % confidence interval mea-   sured by the official script;2 ) we also evaluate the   quality of English summaries in Zh2EnSum with   MoverScore ( Zhao et al . , 2019 ) .   4.2 Implementation Details   In this paper , we train all models using standard   transformer ( Vaswani et al . , 2017 ) in Base setting .   For other hyper - parameters , we mainly follow the   setting described in Zhu et al . ( 2019 , 2020 ) for   fair comparison . For more details , please refer   to Appendix A.   4.3 Comparison Models   Pipeline Models . TETran ( Zhu et al . , 2019 ) . It   first translates the original article into the target   language by Google Translatorand then summa-   rizes the translated text via LexRank ( Erkan and   Radev , 2004 ) . TLTran ( Zhu et al . , 2019 ) . It first   summarizes the original article via a transformer-   based monolingual summarization model and then   translates the summary into the target language by   Google Translator .   End - to - End Models . TNCLS ( Zhu et al . , 2019 ) .   It directly uses the de - facto transformer ( Vaswaniet al . , 2017 ) to train an end - to - end CLS system .   ATS - A ( Zhu et al . , 2020).It is an efficient model   to attend the pre - constructed probabilistic bilin-   gual lexicon to enhance the CLS . MS - CLS ( Zhu   et al . , 2019 ) . It simultaneously performs summa-   rization generation for both CLS and MS tasks   and calculates the total losses . MT - CLS ( Zhu   et al . , 2019).It alternatively trains CLS and MT   tasks . MS - CLS - Rec ( Cao et al . , 2020a ) . It jointly   trains MS and CLS systems with a reconstruction   loss to mutually map the source and target repre-   sentations . mBART ( Liu et al . , 2020 ) . We use   mBART ( mbart.cc 25 ) as model initialization to   fine - tune the CLS task . MLPT ( Mixed - Lingual Pre-   training ) ( Xu et al . , 2020 ) . It applies mixed - lingual   pretraining that leverages six related tasks , cover-   ing both cross - lingual tasks such as translation and   monolingual tasks like masked language models .   MT - MS - CLS . It is our strong baseline , which is   implemented by alternatively training CLS , MT ,   and MS . Here , we keep the dataset used for MT   and MS consistent with Zhu et al . ( 2019 ) for fair   comparison .   4.4 Main Results   Overall , we separate the models into three parts   in Tab . 4 : the pipeline , end - to - end , and multi - task2093   settings . In each part , we show the results of exist-   ing studies and our re - implemented baselines and   our approach , i.e. , the VHM , on Zh2EnSum and   En2ZhSum test sets .   Results on Zh2EnSum . Compared against   the pipeline and end - to - end methods , VHM   substantially outperforms all of them ( e.g. ,   the previous best model “ ATS - A ” ) by a large   margin with 0.68/0.52/0.18/0.4 ↑scores on   RG1 / RG2 / RGL / MVS , respectively . Under   the multi - task setting , compared to the exist-   ing best model “ MS - CLS - Rec ” , our VHM   also consistently boosts the performance in   three metrics ( i.e. , 0.39 ↑ , 1.44 ↑ , and 0.19 ↑   ROUGE scores on RG1 / RG2 / RGL , respec-   tively ) , showing its effectiveness . Our VHM   also significantly surpasses our strong baseline   “ MT - MS - CLS ” by 0.71/0.62/0.46/0.38 ↑scores on   RG1 / RG2 / RGL / MVS , respectively , demonstrating   the superiority of our model again .   After using mBART as model initialization , our   VHM achieves the state - of - the - art results on all   metrics .   Results on En2ZhSum . Compared against the   pipeline , end - to - end and multi - task methods , our   VHM presents remarkable ROUGE improvements   over the existing best model “ ATS - A ” by a large   margin , about 0.51/0.86/0.23 ↑ROUGE gains on   RG1 / RG2 / RGL , respectively . These results sug-   gest that VHM consistently performs well in differ-   ent language directions .   Our approach still notably surpasses our strong   baseline “ MT - MS - CLS ” in terms of all metrics ,   which shows the generalizability and superiority of   our model again.4.5 Few - Shot Results   Due to the difficulty of acquiring the cross - lingual   summarization dataset ( Zhu et al . , 2019 ) , we con-   duct such experiments to investigate the model per-   formance when the CLS training dataset is limited ,   i.e. , few - shot experiments . Specifically , we ran-   domly choose 0.1 % , 1 % , 10 % , and 50 % CLS train-   ing datasets to conduct experiments . The results   are shown in Fig . 2 and Fig . 3 .   Results on Zh2EnSum . Fig . 2 shows that VHM   significantly surpasses all comparison models un-   der each setting . Particularly , under the 0.1 % set-   ting , our model still achieves best performances   than all baselines , suggesting that our variational   hierarchical model works well in the few - shot set-   ting as well . Besides , we find that the performance   gap between comparison models and VHM is grow-   ing when the used CLS training data become fewer .   It is because relatively larger proportion of trans-   lation and summarization data are used , the influ-   ence from MT and MS becomes greater , effectively   strengthening the CLS model . Particularly , the   performance “ Gap - H ” between MT - MS - CLS and   VHM is also growing , where both models utilize   the same data . This shows that the hierarchical re-   lationship between MT&MS and CLS makes sub-   stantial contributions to the VHM model in terms   of four metrics . Consequently , our VHM achieves   a comparably stable performance .   Results on En2ZhSum . From Fig . 3 , we observe   the similar findings on Zh2EnSum . This shows   that VHM significantly outperforms all comparison   models under each setting , showing the generaliz-   ability and superiority of our model again in the   few - shot setting.2094   5 Analysis   5.1 Ablation Study   We conduct ablation studies to investigate how well   the local and global variables of our VHM works .   When removing variables listed in Tab . 5 , we have   the following findings .   ( 1 ) Rows 1 ∼3 vs. row 0 shows that the model   performs worse , especially when removing the two   local ones ( row 3 ) , due to missing the explicit trans-   lation or summarization or both information pro-   vided by the local variables , which is important to   CLS . Besides , row 3 indicates that directly attend-   ing to zleads to poor performances , showing the   necessity of the hierarchical structure , i.e. , using   the global variable to exploit the local ones .   ( 2 ) Rows 4 ∼5 vs. row 0 shows that directly   attending the local translation and summarization   can not achieve good results due to lacking of the   global combination of them , showing that it is very   necessary for designing the variational hierarchicalmodel , i.e. , using a global variable to well exploit   and combine the local ones .   5.2 Human Evaluation   Following Zhu et al . ( 2019 , 2020 ) , we conduct hu-   man evaluation on 25 random samples from each   of the Zh2EnSum and En2ZhSum test set . We com-   pare the summaries generated by our methods ( MT-   MS - CLS and VHM ) with the summaries generated   by ATS - A , MS - CLS , and MT - CLS in the full set-   ting and few - shot setting ( 0.1 % ) , respectively . We   invite three graduate students to compare the gener-   ated summaries with human - corrected references ,   and assess each summary from three independent   perspectives :   1.How informative ( i.e. , IF ) the summary is ?   2 . How concise ( i.e. , CC ) the summary is ?   3.How fluent , grammatical ( i.e. , FL ) the sum-   mary is ?   Each property is assessed with a score from 1   ( worst ) to 5 ( best ) . The average results are pre-   sented in Tab . 6 and Tab . 7 .   Tab . 6 shows the results in the full setting . We   find that our VHM outperforms all comparison   models from three aspects in both language direc-   tions , which further demonstrates the effectiveness   and superiority of our model .   Tab . 7 shows the results in the few - shot setting ,   where only 0.1 % CLS training data are used in all   models . We find that our VHM still performs best   than all other models from three perspectives in   both datasets , suggesting its generalizability and   effectiveness again under different settings .   6 Related Work   Cross - Lingual Summarization . Conventional   cross - lingual summarization methods mainly fo-   cus on incorporating bilingual information into2095   the pipeline methods ( Leuski et al . , 2003 ; Ouyang   et al . , 2019 ; Or ˘asan and Chiorean , 2008 ; Wan   et al . , 2010 ; Wan , 2011 ; Yao et al . , 2015 ; Zhang   et al . , 2016b ) , i.e. , translation and then summariza-   tion or summarization and then translation . Due   to the difficulty of acquiring cross - lingual sum-   marization dataset , some previous researches fo-   cus on constructing datasets ( Ladhak et al . , 2020 ;   Scialom et al . , 2020 ; Yela - Bello et al . , 2021 ; Zhu   et al . , 2019 ; Hasan et al . , 2021 ; Perez - Beltrachini   and Lapata , 2021 ; Varab and Schluter , 2021 ) ,   mixed - lingual pre - training ( Xu et al . , 2020 ) , knowl-   edge distillation ( Nguyen and Tuan , 2021 ) , con-   trastive learning ( Wang et al . , 2021 ) or zero - shot   approaches ( Ayana et al . , 2018 ; Duan et al . , 2019 ;   Dou et al . , 2020 ) , i.e. , using machine translation   ( MT ) or monolingual summarization ( MS ) or both   to train the CLS system . Among them , Zhu et al .   ( 2019 ) propose to use roundtrip translation strat-   egy to obtain large - scale CLS datasets and then   present two multi - task learning methods for CLS .   Based on this dataset , Zhu et al . ( 2020 ) leverage   an end - to - end model to attend the pre - constructed   probabilistic bilingual lexicon to improve CLS . To   further enhance CLS , some studies resort to shared   decoder ( Bai et al . , 2021a ) , more pseudo training   data ( Takase and Okazaki , 2020 ) , or more related   task training ( Cao et al . , 2020b , a ; Bai et al . , 2021b ) .   Wang et al . ( 2022 ) concentrate on building a bench-   mark dataset for CLS on dialogue field . Different   from them , we propose a variational hierarchical   model that introduces a global variable to simulta-   neously exploit and combine the local translation   variable in MT pairs and local summarization vari - able in MS pais for CLS , achieving better results .   Conditional Variational Auto - Encoder . CV AE   has verified its superiority in many fields ( Sohn   et al . , 2015 ; Liang et al . , 2021a ; Zhang et al . , 2016a ;   Su et al . , 2018b ) . For instance , in dialogue , Shen   et al . ( 2019 ) , Park et al . ( 2018 ) and Serban et al .   ( 2017 ) extend CV AE to capture the semantic con-   nection between the utterance and the correspond-   ing context with hierarchical latent variables . Al-   though the CV AE has been widely used in NLP   tasks , its adaption and utilization to cross - lingual   summarization for modeling hierarchical relation-   ship are non - trivial , and to the best of our knowl-   edge , has never been investigated before in CLS .   Multi - Task Learning . Conventional multi - task   learning ( MTL ) ( Caruana , 1997 ) , which trains   the model on multiple related tasks to promote   the representation learning and generalization per-   formance , has been successfully used in NLP   fields ( Collobert and Weston , 2008 ; Deng et al . ,   2013 ; Liang et al . , 2021d , c , b ) . In the CLS , con-   ventional MTL has been explored to incorporate   additional training data ( MS , MT ) into models ( Zhu   et al . , 2019 ; Takase and Okazaki , 2020 ; Cao et al . ,   2020a ) . In this work , we instead focus on how to   connect the relation between the auxiliary tasks at   training to make the most of them for better CLS .   7 Conclusion   In this paper , we propose to enhance the CLS model   by simultaneously exploiting MT and MS . Given   the hierarchical relationship between MT&MS and   CLS , we propose a variational hierarchical model   to explicitly exploit and combine them in CLS pro-   cess . Experiments on Zh2EnSum and En2ZhSum   show that our model significantly improves the   quality of cross - lingual summaries in terms of auto-   matic metrics and human evaluations . Particularly ,   our model in the few - shot setting still works better ,   suggesting its superiority and generalizability .   Acknowledgements   The research work descried in this paper has been   supported by the National Key R&D Program of   China ( 2020AAA0108001 ) and the National Na-   ture Science Foundation of China ( No . 61976015 ,   61976016 , 61876198 and 61370130 ) . Liang is sup-   ported by 2021 Tencent Rhino - Bird Research Elite   Training Program . The authors would like to thank   the anonymous reviewers for their valuable com-   ments and suggestions to improve this paper.2096References20972098   Appendix   A Implementation Details   In this paper , we train all models using standard   transformer ( Vaswani et al . , 2017 ) in Base setting .   For other hyper - parameters , we mainly follow the   setting described in ( Zhu et al . , 2019 , 2020 ) for fair   comparison . Specifically , the segmentation gran-   ularity is “ subword to subword ” for Zh2EnSum , and “ word to word ” for En2ZhSum . All the pa-   rameters are initialized via Xavier initialization   method ( Glorot and Bengio , 2010 ) . We train our   models using standard transformer ( Vaswani et al . ,   2017 ) in Base setting , which contains a 6 - layer   encoder ( i.e. ,N ) and a 6 - layer decoder ( i.e. ,N )   with 512 - dimensional hidden representations . And   all latent variables have a dimension of 128 . Each   mini - batch contains a set of document - summary   pairs with roughly 4,096 source and 4,096 target to-   kens . We apply Adam optimizer ( Kingma and Ba ,   2015 ) with β= 0.9 , β= 0.998 . Following Zhu   et al . ( 2019 ) , we train each task for about 800,000   iterations in all multi - task models ( reaching conver-   gence ) . To alleviate the degeneration problem of   the variational framework , we apply KL annealing .   The KL multiplier λgradually increases from 0   to 1 over 400 , 000 steps . All our methods with-   out mBART as model initialization are trained and   tested on a single NVIDIA Tesla V100 GPU . We   use 8 NVIDIA Tesla V100 GPU to train our models   when using mBART as model initialization , where   the number of token on each GPU is set to 2,048   and the training step is set to 400 , 000 .   During inference , we use beam search with a   beam size 4 and length penalty 0.6.2099