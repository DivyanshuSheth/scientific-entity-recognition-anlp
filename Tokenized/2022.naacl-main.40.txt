  Xueqing Wu , Kung - Hsiang Huang , Yi R. Fung , Heng Ji   University of Illinois Urbana - Champaign   { xueqing8,khhuang3,yifung2,hengji}@illinois.edu   Abstract   For emerging events , human readers are often   exposed to both real news and fake news . Mul-   tiple news articles may contain complemen-   tary or contradictory information that readers   can leverage to help detect fake news . In-   spired by this process , we propose a novel   task of cross - document misinformation de-   tection . Given a cluster of topically related   news documents , we aim to detect misinforma-   tion at both document level and a more ﬁne-   grained level , event level . Due to the lack   of data , we generate fake news by manipulat-   ing real news , and construct 3new datasets   with 422,276 , and 1,413 clusters of topi-   cally related documents , respectively . We fur-   ther propose a graph - based detector that con-   structs a cross - document knowledge graph us-   ing cross - document event coreference resolu-   tion and employs a heterogeneous graph neu-   ral network to conduct detection at two levels .   We then feed the event - level detection results   into the document - level detector . Experimen-   tal results show that our proposed method sig-   niﬁcantly outperforms existing methods by up   to7F1 points on this new task .   1 Introduction   The dissemination of fake news has become an im-   portant social issue . For emergent complex events ,   human readers are usually exposed to multiple   news documents , where some are real and others   are fake . News documents from different sources   naturally form a cluster of topically related doc-   uments . We notice that articles about the same   topic may contain conﬂicting or complementary   information , which can beneﬁt the task of misinfor-   mation detection . An example is shown in Figure   1 . As shown in the knowledge graph , the death of   Rosanne Boyland in 2021 US Capitol attack is a   shared event across all four documents . Each docu-   ment is internally consistent , making it difﬁcult toidentify misinformation when judging each news   separately . However , the three real news documents   complement each other ’s statements regarding the   death of Boyland , while the fake news document   contradicts the other stories . Such cross - document   connections can be leveraged to help detect misin-   formation .   Most of the existing work on fake news detec-   tion is limited to judging each document in isola-   tion . In contrast , we propose a novel task of cross-   document misinformation detection that aims to   detect fake information from a cluster of topically   related news documents . We perform the task at   both the document level and event level . Each   event describes a speciﬁc type of real - world event   mentioned in the text ( e.g. , the death of Boyland   in Figure 1 ) , and usually involves certain partic-   ipants to represent different aspects of the event   ( e.g. , the cause of death and the victim of the death   event ) . Document - level detection aims to detect   fake news documents . Event - level detection is   a more ﬁne - grained task that aims to detect fake   events , thereby pinpointing speciﬁc fake informa-   tion in news documents .   Existing work on ﬁne - grained misinformation   detection detects triplets of false knowledge ( Fung   et al . , 2021 ) . However , we focus on identifying   false events instead of relations or entities , because   events are more important for storytelling and eas-   ier to compare across multiple documents through   cross - document coreference resolution .   To the best of our knowledge , there are no fake   news detection datasets with clusters of topically   related documents . Therefore , we construct 3 new   benchmark datasets based on existing real news   corpus with such clusters . Following Fung et al .   ( 2021 ) , we train a generator that generates a doc-   ument from a knowledge graph ( KG ) , and feed   manipulated KGs into the generator to generate   fake news documents . Tracking the manipulation   operations , we also obtain supervision for event-543   level detection .   We further propose a detection approach as   shown in Figure 2 . Given a cluster of documents ,   we ﬁrst use an information extraction ( IE ) system   ( Lin et al . , 2020 ) to construct a within - document   KG for each document . Then , we connect the   within - document KGs to form a cross - document   KG using cross - document event coreference reso-   lution ( Lai et al . , 2021 ; Wen et al . , 2021 ) . Eventu-   ally , we use a heterogeneous graph neural network   ( GNN ) to encode the cross - document KG and con-   duct detection at two levels .   Our contributions are summarized as follows :   1.We propose the novel task of cross - document   misinformation detection , and conduct the   task at two levels , document level and the   more ﬁne - grained event level .   2.We construct 3new datasets for our proposed   task based on existing document clusters cate-   gorized by topics .   3.We propose a detector that leverages cross-   document information and improve document-   level detection by utilizing features produced   by the event - level detector . Experiments on   three datasets demonstrate that our method   signiﬁcantly outperforms existing methods .   2 Related Work   Fake News Detection : Early work on fake news   detection uses hand - crafted features to perform   document classiﬁcation ( Rubin et al . , 2016 ; Wang ,   2017 ; Rashkin et al . , 2017 ; Pérez - Rosas et al . , 2018 ;   Sarkar et al . , 2018 ; Atanasova et al . , 2019 ) . Recentwork uses neural networks such as recurrent neural   networks ( Karimi et al . , 2018 ; Nasir et al . , 2021 )   and Transformer ( Zellers et al . , 2019 ) to encode the   news document . To model the internal structure of   a news document , Karimi and Tang ( 2019 ) model   the inter - sentence dependency tree , Vaibhav et al .   ( 2019 ) ; Hu et al . ( 2021 ) model the interactions be-   tween sentences ; and Pan et al . ( 2018 ) and Fung   et al . ( 2021 ) model the knowledge graph extracted   by IE systems . Similar to our work , Hu et al . ( 2021 )   compare each news with external knowledge base   ( KB ) to check for inconsistencies . However , the   correlation between news and KB is not as close   as the correlation between related news documents   due to the incompleteness of these KBs . Other   work utilizes additional information such as user   engagements and behaviors on social media ( Shu   et al . , 2019 ; Nguyen et al . , 2020 ) and multi - modal   features ( Khattar et al . , 2019 ; Tan et al . , 2020 ;   Fung et al . , 2021 ) . However , to the best of our   knowledge , no published work has considered us-   ing cross - document inference for misinformation   detection .   In addition to document - level detection , the task   of ﬁne - grained detection is also important but rarely   explored . The most relevant work detects fake   knowledge triplets extracted from each individual   news article ( Fung et al . , 2021 ) .   Another related task is fact veriﬁcation which   aims to verify a statement based on retrieved evi-   dence . Fact veriﬁcation has been explored in multi-   ple domains such as general domain ( Thorne et al . ,   2018 ) , climate change ( Diggelmann et al . , 2020)544   and COVID-19 ( Wadden et al . , 2020 ; Saakyan   et al . , 2021 ) . However , fact veriﬁcation focuses on   short single - sentence statements and can not model   the complicated internal structure of a news docu-   ment .   Fake News Datasets : The main difﬁculty in con-   structing a fake news dataset is to obtain annota-   tions . Rashkin et al . ( 2017 ) and Rubin et al . ( 2016 )   obtain labels from the source information and con-   sider news from reliable sources as real news , and   unreliable sources as fake news . A potential issue   is that the detector may only learn to distinguish   the style of different news sources , rather than   the authenticity of the content . Shu et al . ( 2020 )   collect annotations from fact - checking websites ,   and Pérez - Rosas et al . ( 2018 ) collect annotations   via crowd - sourcing . These approaches produce   datasets of higher quality , but require extensive   manual efforts . With the development of powerful   generative models capable of mimicking human-   written news ( Zellers et al . , 2019 ) , recent work has   constructed datasets by using generative models to   generate fake news ( Tan et al . , 2020 ; Fung et al . ,   2021 ) . Fung et al . ( 2021 ) further generate fake   news from manipulated KG , which we follow to   construct our dataset .   3 Task Formulation   Given a cluster of documents about the same story ,   the task of cross - document misinformation detec - tion aims to detect the fake information included in   the cluster .   Formally , let S={d,···,d}be the docu-   ment cluster , and N=|S|be the size of the cluster .   Some documents in Sare real , while others are   fake . From each document d∈S , we extract   events E(d ) = { e,···,e } , wherem=|E(d)|   is the number of events in document d. In an ex-   tracted event set E(d ) , some events are real and   others are fake .   We conduct the task of misinformation detec-   tion at two levels , document level and event level .   Document - level detection aims to predict whether   each document d∈Sis real or fake . Event - level   detection is a more ﬁne - grained task that aims to   predict whether each event e∈E(d),d∈Sis real   or fake . In the example in Figure 1 , the dieevent   in the fake news is fake , since it falsely describes   Boyland being killed by the police , but she actually   died of drug overdose .   4 Approach   An overview of our approach is shown in Figure   2 . Given a cluster of documents , we ﬁrst construct   a within - document KG for each document using   an IE system ( Lin et al . , 2020 ) , and then connect   the within - document KGs into a cross - document   KG using cross - document event coreference reso-   lution . Based on the cross - document KG , we use   a hetereogeneous GNN ( Schlichtkrull et al . , 2018;545Hu et al . , 2019 ) to perform detection . We further   incorporate the results of event - level detection to   help the document - level detector .   4.1 Knowledge Graph Construction   Within - document KG : We ﬁrst construct a   within - document IE - based knowledge graph for   each document . We leverage OneIE ( Lin et al . ,   2020 ) , a joint IE system , to extract the entities ,   relations , and events contained in a given docu-   ment . Then , we conduct entity linking ( Pan et al . ,   2017 ) and entity coreference resolution ( Lee et al . ,   2017 ) to merge multiple mentions of the same   entities together . Eventually , we obtain a within-   document KG where entities and events are nodes ,   relations are edges between entities , and arguments   are edges between events and entities .   Cross - document KG : We leverage cross-   document event coreference resolution to connect   the within - document KGs into a cross - document   KG as illustrated in Figure 2 . We employ a cross-   document event coreference resolution system   ( Lai et al . , 2021 ; Wen et al . , 2021 ) to identify   clusters of events from multiple documents that   refer to the same real - world events . The system   utilizes both textual contexts of the event mentions   and symbolic features such as the event type   information . An example of the detected event   cluster is shown in Table 1 , where the four events   of four documents all refer to the same explosion   attack on Venezuela ’s President Nicolas Marduro .   These four events contain complementary or   contradictory details , which can be used for   misinformation detection . For each event cluster ,   we add a node to represent the overall information   of the real - world complex event corresponding to   the cluster . Then , an edge is added between each   event node and corresponding cluster node to allow   reasoning among cross - document coreferential   events .   To indicate which document each entity or event   belongs to and capture the global information of   each document , we further introduce a document   node and connect it to the associated entity and   event nodes for each document .   The resulting KG contains 4types of nodes ( i.e.   entity nodes , event nodes , document nodes , and   event cluster nodes ) and 5types of edges ( i.e. re-   lation edges , event argument edges , document - to-   entity edges , document - to - event edges , and edges   connecting event nodes to event cluster nodes ) .   Since all edges are directional , we add an inverse   edge for each edge to propagate features along both   directions , and the ﬁnal KG contains 10edge types ,   accounting for the inverse of existing edge types .   KG representation : We use BERT ( Devlin et al . ,   2019 ) to initialize the node and edge embeddings in   the KG . For a document node , we use BERT to en-   code the entire document and take the embeddings   of[CLS ] tokens . Similarly , for an entity node , we   encode its canonical mention . For an event node ,   we encode the sentence where the event trigger oc-   curs . For an event cluster node , we take the average   of the embeddings of all events in the cluster . For a   relation edge or an event argument role edge , we en-   code the linearized representation of the relation tu-   ple . For example , the Leadership relation between   “ Nicolas Maduro ” and “ Venezuelan ” is represented   by “ Nicolas Maduro , Leadership , Venezuelan ” ,   and “ guns ” as the ExplosiveDevice argument of the   DetonateExplode event is represented by “ Detona-   teExplode , ExplosiveDevice , guns ” .   4.2 Knowledge Graph Encoder   Heterogeneous GNN : Given the heterogeneous   nature of the cross - document KG , we adopt a het-546erogeneous GNN to encode the KG .   Formally , letGdenote KG andVdenote the   nodes inG. We useRto denote the 10types of   edges as discussed in the previous section , and for   each edge type r∈R , we useGto denote the sub-   graph ofGthat only contains edges of type r. At the   l - th layer , the inputs are output features produced   by the previous layer denoted as h , i∈V. For   each edge type r∈R , we apply a separate GNN   to encodeGand produce a set of features denoted   ash . Then , we aggregate the outputs for all edge   types into the ﬁnal output as follows :   h=/summationdisplayh/|R| ( 1 )   For document - to - entity edges , document - to-   event edges , and edges connecting event nodes to   event cluster nodes , we use standard graph attention   network ( GAT ) . For relation edges and event argu-   ment edges , we apply edge - aware GAT to leverage   the edge features . Here , the edge features refer to   the BERT embeddings of text descriptions such   as “ Nicolas Maduro , Leadership , Venezuelan ” or   “ DetonateExplode , ExplosiveDevice , guns ” as de-   scribed in Section 4.1 . The remainder of Section   4.2 presents details of GAT and edge - aware GAT ,   i.e. , how to produce hbased on h .   Graph attention network : For each given node ,   GAT aggregates the node features of its neighbors   via attention mechanism ( Velickovic et al . , 2018 ) .   For a given edge type r∈R , letNdenote the   neighbors of node iinG. At thel - th layer , the   attention weights αare calculated as follows :   e = LeakyReLU / parenleftBig   a / bracketleftBig   Wh / bardblWh / bracketrightBig / parenrightBig   ( 2 )   α = softmax(e ) = exp(e)/summationtextexp(e)(3 )   where aandWare trainable parameters , and /bardbl   denotes the feature concatenation . The output fea-   tureshfor nodeiinGare calculated as follows :   h=/summationdisplayαWh ( 4 )   Edge - aware graph attention network : Edge-   aware GAT is an extension of GAT that considers   edge features in addition to node features ( Huang   et al . , 2020 ; Yasunaga et al . , 2021 ) . Let rdenotethe features of the edge between nodes iandj .   For a given edge type r∈R , at thel - th layer , the   attention weights αare computed as follows :   r = W / bracketleftBig   h / bardblh / bardblr / bracketrightBig   ( 5 )   α = softmax / parenleftBig   ( Wh)(Wr)/parenrightBig   ( 6 )   where W , WandWare trainable parame-   ters . The output features hfor nodeiinGare   computed as follows :   h=/summationdisplayαWr ( 7 )   where Wis a learnable matrix .   4.3 Misinformation Detector   Using the previously described graph encoder , we   are able to obtain representations of the document   and event nodes . We conduct document - level de-   tection using the document node representations ,   and event - level detection using the event node rep-   resentations . We separately train two detectors for   these two levels of tasks .   However , these two tasks are not mutually in-   dependent . Intuitively , document - level detection   can beneﬁt from the results of event - level detec-   tion , because the presence of a large number of   false events indicates that the document is more   likely to be fake . Therefore , we feed the results   produced by a well - trained event - level detector into   each layer of the document - level detector . Let e   denote the representations of node iproduced by   the event - level detector . At the l - th layer of the   document - level detector , instead of using the out-   put features of the previous layer h as input   features , we use a linear projection of the concate-   nation of eandh calculated as follows :   W / bracketleftBig   e / bardblh / bracketrightBig   ( 8)   where Wis a learnable matrix .   5 Dataset Construction   Currently , there are no existing resources for cross-   document misinformation detection . We propose   to construct datasets based on real news datasets   with clustering information . For each cluster , we   randomly sample 50 % real news and replace them   with manipulated fake news . Figure 3 shows an547   overview of the fake news generation process , and   more examples are presented in Appendix D.   Following Fung et al . ( 2021 ) , we train a KG - to-   text generator from the real news in our datasets ,   and generate fake news from manipulated KGs .   The main differences between Fung et al . ( 2021 ) ’s   method and ours in terms of manipulating KG are :   ( 1 ) we only conduct entity swapping , and do not   adopt other types of manipulation including adding   relations or events and subgraph replacement ; ( 2 )   since we focus on events , we select entities to be re-   placed that are arguments of high - frequency events ,   instead of based on entity node degree ; ( 3 ) we   select entities from other documents in the same   document cluster to replace the original entities , so   that the entities before and after replacement are   more similar .   We record the manipulation operations , and use   a heuristic rule to obtain supervision for event - level   detection as explained below . In a fake document ,   if an event involves manipulated entities as argu-   ments , we consider this event as fake .   6 Experiments   6.1 Data   We constructed three new benchmark datasets   based on three datasets that naturally have clusters   of topically related documents . IED is a complex   event corpus , where each complex event refers to   a real - world story ( e.g. , Boston bombing ) and is   described by multiple documents ( Li et al . , 2021 ) .   Therefore , a complex event can be considered as a   document cluster . TL17 andCrisis are two time-   line summarization datasets containing multiple   news timelines . Each timeline contains multiple   documents describing an evolving long - term event   such as Inﬂuenza H1N1 and Egypt Revolution   ( Tran et al . , 2013 , 2015 ) , and thus can be regarded   as a document cluster . The detailed statistics of the   original datasets are shown in Appendix A.   However , documents within the same cluster   may not be closely related as the story described   by a cluster can span up to three years . To obtain   smaller and more closely related clusters , we split   each timeline into smaller clusters of approximately   size10based on publication dates . Then , we em-   ploy the methods described in Section 5 to generate   fake documents . The statistics of the constructed   datasets are in Table 2 .   6.2 Experimental Settings   For our proposed method , we use a   4 - layered heterogeneous GAT and use   bert - base - uncased to initialize the node   and edge embeddings . For comparison , on the   document - level detection task , we compare our   method against two baselines : HDSF that models   the inter - sentence dependency tree ( Karimi and   Tang , 2019 ) , and GROVER ( Zellers et al . , 2019 ) ,   a Transformer - based detector . On the event - level   detection task , we compare our method against   random guessing , logistic regression andBERT .   For logistic regression , we use hand - crafted   features to represent the event including the event   type , the number of arguments , and the size of the   event cluster . The detailed settings are presented in   Appendix C.   For evaluation , we use F1 to evaluate document-   level detection . Considering the label imbalance   of event - level detection , we use F1 and the area   under the ROC curve ( AUC ) to evaluate event - level548IED TL17 Crisis   HDSF 78.42 80.62 82.14   GROVER - medium 79.06 79.40 86.84   GROVER - mega 82.90 90.00 87.13   Ours 86.76 90.21 93.89   detection . For the F1 metric , we select the optimal   threshold on the validation set .   6.3 Document - level Detection Results   Table 3 shows the results of document - level detec-   tion . Our method yields consistent improvements   on all three datasets and signiﬁcantly outperforms   the baselines that judge the authenticity for each   document in isolation . To understand the effective-   ness of each component , we conducted an ablation   study and show the results in Table 4 . We have the   following ﬁndings :   ( 1 ) We remove the edges between event nodes   and event center nodes to analyze the impact of   cross - document event coreference resolution , and   ﬁnd that such information signiﬁcantly improves   the performance on IED and TL17 . We also train   our detector with smaller clusters on TL17 and get   worse performance ( 84.53 % and87.37 % on clus-   ters with size 1 and 2 respectively ) , which veriﬁes   that our model beneﬁts from more cross - document   information . The beneﬁt of cross - document event   coreference resolution is less signiﬁcant on the   large - scale Crisis dataset containing 1.7 kdocu-   ments . This may imply that cross - document mis-   information detection is more useful for emerging   new events where large - scale training data is not   available .   ( 2 ) Using the event - level detection results con-   sistently improves the performance by 1 - 3points   on all datasets . Since the projection modules in-   troduce additional parameters , we further train a   detector utilizing random features and ﬁnd that us-   ing random features reduces the performance . This   veriﬁes that the improvement is brought by utiliz-   ing the knowledge learnt by the event - level detector   rather than additional parameters .   6.4 Event - level Detection Results   We track the manipulation operations during the   dataset construction process , which allows us to   obtain supervision for event - level detection . The   results are shown in Table 5 . We compare our   method with random guessing , logistic regression   with hand - crafted event features , and BERT . We   ﬁnd that random guessing performs the worst , logis-   tic regression and BERT achieve satisfactory perfor-   mance , and our method signiﬁcantly outperforms   all baselines by a large margin . As in document-   level detection , we conduct an ablation study on the   use of cross - document event coreference resolution   by removing edges between event nodes and event   cluster nodes , and ﬁnd that such information brings   slight improvements in the AUC metric .   6.5 Analysis and Discussion   To demonstrate the beneﬁts of using cross-   document event coreference resolution , we show   an example in Figure 4 , with 4 documents from   the same cluster . As shown in Figure 4 , by per-   forming cross - document reasoning on events in   the same event cluster , our model achieves bet-   ter performance compared to Ours , i.e. ,   our model without edges between event nodes and   event cluster nodes .   We further analyze the remaining errors in our   model . Figure 5 shows two representative cases549   where both document - level and event - level detec-   tors fail to detect misinformation . In the ﬁrst exam-   ple , the manipulated entity is not captured by the   IE system , and the error of IE system is propagated   into the detector . A potential solution is to use an   OpenIE system ( Stanovsky et al . , 2018 ) that is able   to cover more event and entity types . The second   example is a more challenging case where the event   containing fake information is not mentioned in any   other document . This makes it difﬁcult to either   verify or disprove via cross - document reasoning ,   and may require the detector to actively search for   external information related to the event .   There are some remaining challenges and limi-   tations in our proposed methodology . First , some   cross - document contradictions are difﬁcult to cap-   ture by coreference resolution only . In the example   in Figure 1 , knowing that the police are unlikely to   help and attack Boyland at the same time requires   commonsense reasoning , which we leave as our   future work . Second , an underlying assumption of   our framework is that real news articles are consis-   tent and complementary with each other , while fake   news often contradicts each other . This assump-   tion is true for our constructed datasets because we   manipulate the KGs via random entity swapping .   However , certain types of human - written fake news   documents , such as conspiracy theories , tend to be   closely related to each other and convey highly sim-   ilar information because they share the same biases   or aim to manipulate readers in the same way . This   may limit the performance of our proposed system   in real - world scenarios .   6.6 Human Evaluation on Fake News   Generation   To evaluate the quality of the generated fake news ,   we conducted a Turing Test by 13 human readers   as in Fung et al . ( 2021 ) . We randomly select 100   documents from the IED dataset , half real and half   fake , and ask the human readers to assess the au-   thenticity for each document . The overall accuracy   achieved by human readers is 66.88 % , with 77.44 %   accuracy on real documents but only 56.32 % ac-   curacy on fake documents . This shows that it is   difﬁcult for human readers to detect the generated   fake news .   7 Conclusions and Future Work   We are the ﬁrst to study the new task of cross-   document misinformation detection . We conduct   the task at two levels , document level and the more   ﬁne - grained event level , and construct three new   datasets to handle the lack of training data . We fur-   ther propose a graph - based cross - document detec-   tor that conducts reasoning over a cross - document   knowledge graph and feed the event - level detec-   tion results into document - level detector . The ex-550perimental results show that our proposed method   signiﬁcantly outperforms existing methods .   For future work , we intend to extend our method   to conduct cross - document reasoning over more   types of information ( e.g. , entities and relations )   in addition to events . We also plan to extend our   method to multi - media news including texts , im-   ages , audios and videos , which requires the con-   struction of cross - document multi - modal knowl-   edge graphs . Finally , a challenging but important   task is to construct a large - scale fake news detec-   tion corpus with human - written fake news contain-   ing document clusters and study our method in this   scenario .   8 Ethical Considerations   The goal of this work is to advance state - of - the-   art research in the ﬁeld of misinformation detec-   tion by analyzing multiple documents on the same   topic . We build new benchmark datasets using a   fake news generator , and propose a detector that   achieves high performance in such scenarios . We   have released the constructed datasets and detector   codes in this submission as a useful reference for   future research . We hope that our work will encour-   age more efforts in this direction and beneﬁt the   community .   However , as with any work that utilizes text gen-   eration , our work involves the risk of being applied   to produce false information to mislead or manip-   ulate readers . Therefore , we promise not to share   codes or checkpoints of our generator to avoid po-   tential negative consequences . To improve repro-   ducibility , we describe the general idea and a few   crucial details of the fake news generator .   Acknowledgement   This research is based upon work supported by U.S.   DARPA SemaFor Program No . HR001120C0123   and DARPA KAIROS Program No . FA8750 - 19 - 2-   1004 . The views and conclusions contained herein   are those of the authors and should not be inter-   preted as necessarily representing the ofﬁcial poli-   cies , either expressed or implied , of DARPA , or   the U.S. Government . The U.S. Government is   authorized to reproduce and distribute reprints for   governmental purposes notwithstanding any copy-   right annotation therein . References551552553A Statistics of Original Datasets   Statistics of the original IED , TL17 and Crisis   dataset are presented in Table 6 .   # Cluster # Doc # Doc per   cluster   IED 433 7403 17   TL17 17 4650 273   Crisis 4 20463 5116   B Method Details   B.1 Information Extraction   We use OneIE ( Lin et al . , 2020 ) , a BERT - based   end - to - end IE system to extract entities , relations ,   and events . OneIE conducts IE in four steps : ( 1 ) en-   code a sentence with a pre - trained BERT encoder ,   ( 2 ) identify entity mentions and event triggers us-   ing a conditional random ﬁelds layer , ( 3 ) classify   types of entity mentions , events , entity relations ,   and event arguments using feed - forward networks ,   and ( 4 ) search for a globally optimal IE graph via   beam search . In this work , we use the model re-   leased by Wen et al . ( 2021 ) . The model achieves   64.1 , 49.7 , and 49.5 F1 on trigger extraction , argu-   ment extraction and relation extraction respectively   on ACE 2005 and ERE ( Song et al . , 2015 ) .   In addition , we use entity linking and entity   coreference resolution to identify coreferential en-   tity mentions . For entity linking , we use an LSTM-   based entity linker to link ( Pan et al . , 2017 ) to link   entity mentions to WikiData entries . The entity   linker achieves 91.8 F1 and 84.3 accuracy . For en-   tity coreference resolution , we use an extension   of the e2e - coref model ( Lee et al . , 2017 ) based on   XLM - RoBERTa ( Conneau et al . , 2020 ) . The model   is released by Wen et al . ( 2021 ) and achieves a 92.4   CoNLL score on OntoNotes ( Pradhan et al . , 2012 ) .   Eventually , entity mentions that are linked to the   same WikiData entry or identiﬁed as coreferences   will be considered as the same entity , and their   entity nodes in the KG will be merged .   B.2 Event Coreference Resolution   For event coreference resolution , we use Lai et al .   ( 2021 ) , a within - document coreference resolution   model . We extend it to the cross - document sce-   nario following Wen et al . ( 2021 ) . Given a clus-   ter containing Ndocuments , we concatenate each554pair of documents into a “ mega - document ” . The   model then conducts coreference resolution on   each mega - document . More speciﬁcally , for each   event mention , the model uses SpanBERT ( Joshi   et al . , 2020 ) to extract contextualized text embed-   dings and builds manually designed symbolic fea-   tures such as event types , attributes , and arguments .   Then , the two features are combined selectively   using a gated mechanism . Eventually , for each   pair of event mentions in a mega - document , the   model predicts whether they are coreferential . In   this work , we use the model released by Wen et al .   ( 2021 ) . The model achieves 84.8 CoNLL score on   ACE 2005 .   B.3 KG - to - Text Generator   We train the KG - to - text generator by following   Fung et al . ( 2021 ) .   We ﬁrst linearize the IE - based KG . For ex-   ample , the Leadership relation between “ Nico-   las Maduro ” and “ Venezuelan ” is represented by   “ Nicolas Maduro , Leadership , Venezuelan ” , and   theDetonateExplode event with “ drone ” as Ex-   plosiveDevice argument and “ ﬂat ” as Place argu-   ment is represented by “ [ DetonateExplode | Explo-   siveDevice = drone , Place = ﬂat ] ” . We represent   the entire KG in graph by concatenating the text   representations of all relations and events .   Since generating the entire document is very   challenging , we ﬁne - tune a sentence - level KG - to-   text generator from BART ( Lewis et al . , 2020 ) . The   generator takes the linearized KG and the previous   sentence as input and generates the next sentence .   Here , the KG only contains information presented   in the sentence rather than in the entire document .   During inference , the generator generates the entire   document sentence - by - sentence in an autoregres-   sive manner .   C Experiment Details   Detailed settings of our method : For our pro-   posed method , we use a 4 - layered heterogeneous   GNN , where each GAT layer contains 8heads . To   initialize the node and edge embeddings , we use   bert - base - uncased model with the feature   dimension of 768 . Our model contains 233 M pa-   rameters .   For hyperparameters , we use a batch size   of16 , and search the learning rate from   { 10,10,10}and the number of layers   within{2,4,8 } . Our best - found hyperparametersare a learning rate of 10and a number of layers   of4 . We train our model with Adam optimizer   until convergence . To reduce computation cost , we   freeze BERT ’s parameters . The training process   takes approximately 6hours on a Tesla P100 GPU .   Detailed settings of KG - to - text generator : We   ﬁne - tune the model from bart - large model   containing 24layers , 1024 hidden dimensions , 16   heads , and 406 M parameters . on the three datasets   respectively . We train the model on a Tesla P100   GPU using the batch size of 1024 tokens , the gra-   dient accumulation step of 16 , the learning rate of   3×10 , the warmup steps of 500steps , and the   total training steps of 12000 .   Document - level baselines : For document - level   detection , we compare our method against two   baselines : HDSF that models inter - sentence depen-   dency tree ( Karimi and Tang , 2019 ) , and GROVER   ( Zellers et al . , 2019 ) , a Transformer - based detector .   For HDSF , we use the implementation at https :   //github.com / hamidkarimi / HDSF/ . We   train the model on our datasets using their de-   fault hyper - parameters . For GROVER , we use   the implementation at https://github.com/   rowanz / grover and experiment with two set-   tings , medium setting and mega setting . Since   ﬁne - tuning the GROVER model is computation-   ally expensive , we use GROVER in the zero - shot   setting .   Event - level baselines : For event - level detection ,   since there are no existing methods , we use three   baselines , random guessing , logistic regression ,   and BERT . In random guessing , for each event ,   we randomly draw a value from a uniform distri-   bution between [ 0,1]as the probability that the   event is false . In logistic regression , we use the   following features : event type ( represented by one-   hot feature ) , number of arguments , and the size   of the event cluster that the given event belongs   to . The features are normalized on the training set .   We use the implementation of logistic regression   and default parameters provided by sklearn . In   the BERT baseline , we use the same BERT - based   event features as our method , and replace the 4-   layer GNN in our model with a feed - forward net-   work . We use the same hyper - parameters to train   the model.555D Examples of Fake News Generation   We present two examples of generated fake news   in Figure 6 and 7 , including the original real news ,   manipulated KG , and generated fake news . The   generated fake news conveys the manipulated mis-   information and meanwhile is stylistically similar   to real news .   E Scientiﬁc Artifacts   In this work , we use three datasets including IED   ( Li et al . , 2021 ) , TL17 ( Tran et al . , 2013 ) and Crisis   ( Tran et al . , 2015 ) . There are no licenses or terms   of use associated with all three datasets .   We use ﬁve software . Among them , HDSF   ( Karimi and Tang , 2019 ) , OneIE ( Lin et al . , 2020 )   and RESIN ( Wen et al . , 2021 ) have no license or   terms of use . GROVER ( Zellers et al . , 2019 ) and   huggingface are licensed under the Apache License   2.0 . Fairseq ( Ott et al . , 2019 ) is licenced under the   MIT License .   We use two models , BERT ( Devlin et al . , 2019 )   and BART ( Lewis et al . , 2020 ) , licenced under the   Apache License 2.0 and the MIT License respec-   tively .   In summary , all artifacts involved either have no   associated licenses or terms of use , or are licensed   under the Apache License 2.0 or the MIT License .   Both the Apache License 2.0 or the MIT License   permit commercial and private use . Therefore , our   use is consistent with their intended use . We will   release the datasets and software with licenses com-   patible with the original access conditions.556557558