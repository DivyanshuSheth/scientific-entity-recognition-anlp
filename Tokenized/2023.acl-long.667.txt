  Yang Deng , Wenxuan Zhang , Qian Yu , Wai LamThe Chinese University of Hong Kong , DAMO Academy , Alibaba Group , JD.com   { dengyang17dydy,isakzhang}@gmail.com , yuqian81@jd.com , wlam@se.cuhk.edu.hk   Abstract   Product question answering ( PQA ) , aiming to   automatically provide instant responses to cus-   tomer ’s questions in E - Commerce platforms ,   has drawn increasing attention in recent years .   Compared with typical QA problems , PQA ex-   hibits unique challenges such as the subjectivity   and reliability of user - generated contents in E-   commerce platforms . Therefore , various prob-   lem settings and novel methods have been pro-   posed to capture these special characteristics .   In this paper , we aim to systematically review   existing research efforts on PQA . Specifically ,   we categorize PQA studies into four problem   settings in terms of the form of provided an-   swers . We analyze the pros and cons , as well as   present existing datasets and evaluation proto-   cols for each setting . We further summarize the   most significant challenges that characterize   PQA from general QA applications and discuss   their corresponding solutions . Finally , we con-   clude this paper by providing the prospect on   several future directions .   1 Introduction   E - Commerce is playing an increasingly important   role in our daily life . During the online shopping ,   potential customers inevitably have some questions   about their interested products . To settle down   their concerns and improve the shopping experi-   ence , many AI conversational assistants have been   developed to solve customers ’ problems , such as   Alexa ( Carmel et al . , 2018 ) and AliMe ( Li et al . ,   2017a ) . The core machine learning problem under-   lying them , namely Product Question Answering   ( PQA ) , thus receives extensive attention in both   academia and industries recently . Figure 1 depicts   an actual PQA example from Amazon . There are aFigure 1 : An PQA example from Amazon .   tremendous amount of product - related data avail-   able within the product page , which contains nat-   ural language user - generated content ( UGC ) ( e.g. ,   product reviews , community QA pairs ) , structured   product - related information ( e.g. , attribute - value   pairs ) , images , etc . Generally , PQA aims to auto-   matically answer the customer - posted question in   the natural language form about a specific product ,   based on the product - related data .   Typical QA studies ( Rajpurkar et al . , 2016 )   and some other domain - specific QA studies ( e.g. ,   biomedical QA ( Jin et al . , 2023 ) and legal QA ( Gil ,   2021 ) ) mainly focus on the questions that ask for   a certain factual and objective answer . Differently ,   product - related questions in PQA typically involve   consumers ’ opinion about the products or aspects   of products . Therefore , early studies ( Moghaddam   and Ester , 2011 ; Yu et al . , 2012 ) regard PQA as   a special opinion mining problem , where the an-   swers are generated by aggregating opinions in the   retrieved documents . Most of recent works essen-   tially follow the same intuition , but formulate PQA   as different problems in terms of the form of tar-   get answers . Accordingly , existing PQA studies11951   can be categorized into four types : opinion - based ,   extraction - based , retrieval - based , and generation-   based . As shown in Figure 1 , opinion - based PQA   approaches only provide the common opinion po-   larity as the answer , while extraction - based PQA   approaches extract specific text spans from the sup-   porting documents as the answer . Retrieval - based   PQA approaches further re - rank the documents to   select the most appropriate one to answer the given   question , while generation - based PQA approaches   generate natural language sentences based on the   available documents as the response . In this paper ,   we systematically review methods of these four   mainstream PQA problem settings , as well as the   commonly - used datasets and evaluation protocols .   Besides the task - specific challenges in each type   of PQA systems , there are several common chal-   lenges across all types of PQA systems , which   differentiate PQA from other QA systems . ( 1 ) Sub-   jectivity . Subjective questions constitute a large   proportion of questions in PQA , which requires   to aggregate the crowd ’s opinions about the ques-   tions , reflected through related reviews and QAs .   ( 2)Reliability & Answerability . Different from   those supporting documents constructed by profes-   sionals in biomedical or legal QA , product reviews   and community QA pairs come directly from non-   expert users , which may suffer from some typicalflaws as other UGC , such as redundancy , inconsis-   tency , spam , and even malice . ( 3 ) Multi - type re-   sources . The supporting documents usually consist   of heterogeneous information from multi - type data   resources , such as text , table , knowledge graph , im-   age , etc . ( 4 ) Low - resource . PQA systems often   encounter the low - resource issue , since different   product categories may need different training data ,   and it is generally time - consuming and costly to   manually annotate sufficient labeled data for each   domain . Accordingly , we introduce existing solu-   tions to each challenge .   To our knowledge , this survey is the first to focus   on Product Question Answering . We first systemat-   ically summarize recent studies on PQA into four   problem settings as well as introduce the available   datasets and corresponding evaluation protocols   in Section 2 . Then we analyze the most signifi-   ca nt challenges that characterize PQA from other   QA applications and discuss their corresponding   solutions in Section 3 . Finally , we discuss sev-   eral promising research directions for future PQA   studies and conclude this paper in Section 4 and 5 .   2 Problems and Approaches   Product question answering ( PQA ) aims to pro-   duce an answer ato a given natural language ques-   tionqbased on a set of supporting documents D,11952   where the supporting documents can be product   reviews , community QA pairs , product informa-   tion , etc . In terms of the form of provided an-   swers , we systematically categorize the existing   PQA studies into four problem settings , includ-   ing Opinion - based PQA , Extraction - based PQA ,   Retrieval - based PQA , Generation - based PQA , and   introduce corresponding approaches proposed to   solve the problem , as summarized in Table 1 . We   present an overview of the general framework for   each problem setting in Figure 2 . In addition , the   key information of the datasets adopted in existing   PQA studies is summarized in Table 2 .   2.1 Opinion - based PQA   Opinion - based PQA studies focus on yes - no type   questions , i.e. , questions that can be answered by   “ Yes ” or “ No ” , which constitute a large proportion   on PQA platforms .   2.1.1 Problem Definition   Given a product - related question qand a set of   supporting documents D(product reviews in most   cases ) , the goal is to predict a binary answer a∈   { Yes , No } . Some studies also consider the neutral   answer , e.g. , “ Not Sure " .   2.1.2 Datasets & Evaluation Protocols   One of the largest and widely - adopted public PQA   datasets is the Amazon Product Dataset ( denoted   as “ Amazon ” in Table 1 and hereafter ) , composed   by Amazon Question / Answer Data ( McAuley and   Yang , 2016 ; Wan and McAuley , 2016 ) and Amazon   Review Data ( He and McAuley , 2016 ; Ni et al . ,   2019 ) . It consists of around 1.4 million answered   questions and 233.1 million product reviews across   over 20 different product categories . The Amazon   dataset contains the information of question types   ( “ yes - no " or “ open - ended " ) , answer types ( “ yes " ,   “ no " , or “ not sure " ) , helpful votes by customers , andproduct metadata , which is suitable for opinion-   based PQA evaluation .   Due to the existence of a certain proportion   of unanswerable questions based on the available   reviews , it is difficult to achieve an acceptable   performance with the ordinary classification ac-   curacy metric Acc(Q)for any method . There-   fore , McAuley and Yang ( 2016 ) propose Acc@ k ,   which has become the de facto metric for evaluat-   ing opinion - based PQA methods , which only calcu-   lates the classification accuracy of top- kquestions   ranked by the prediction confidence . The confi-   dence with each classification is its distance from   the decision boundary , i.e. ,|−P(a|q , D)| . A   good model is supposed to assign high confidence   to those questions that can be correctly addressed .   whereP(Q)is the set of k - sized subsets of Q , and   kis commonly set to be 50 % of the total number   of questions .   2.1.3 Methods   McAuley and Yang ( 2016 ) propose a Mixtures of   experts ( MoEs ) ( Jacobs et al . , 1991 ) based model ,   namely Mixtures of Opinions for Question Answer-   ing ( Moqa ) , to answer yes - no questions in PQA ,   where each review is regarded as an “ expert ” to   make a binary prediction for voting in favor of a   “ yes ” or “ no ” answer . The confidence of each re-   view is further weighted by its relevance to the   question as follows :   Moqa is later enhanced by modeling the ambiguity   and subjectivity of answers and reviews ( Wan and   McAuley , 2016 ) . Yu and Lam ( 2018b ) further im-   prove Moqa by computing the aspect - specific em-   beddings of reviews and questions via a three - order11953   auto - encoder network in an unsupervised manner .   In these early studies , the features either extracted   by heuristic rules or acquired from unsupervised   manners may limit the performance and application   of opinion - based PQA approaches .   To better model the relation between the ques-   tion and each review , Fan et al . ( 2019 ) and Zhang   et al . ( 2019 ) explore the utility of neural networks   ( e.g. , BiLSTM ( Schuster and Paliwal , 1997 ) ) and   pretrained language models ( e.g. , BERT ( Devlin   et al . , 2019 ) ) to learn the distributed feature rep-   resentations , which largely outperform previous   methods . Recently , Rozen et al . ( 2021 ) propose an   approach , called SimBA ( Similarity Based Answer   Prediction ) , which leverages existing answers from   similar resolved questions about similar products   to predict the answer for the target question .   2.1.4 Pros and Cons   Opinion - based PQA approaches can tackle a large   proportion of product - related questions that ask for   certain opinion by using comparatively simple and   easy - to - deploy methods . However , opinion - based   approaches could only provide the classification   result of the opinion polarity , based on the com-   mon opinion reflected in the supporting documents ,   without detailed and question - specific information .   2.2 Extraction - based PQA   Similar to typical extraction - based QA ( Rajpurkar   et al . , 2016 ) ( also called Machine Reading Com-   prehension ( MRC ) ) , extraction - based PQA studies   aim at extracting a certain span of a document to be   the answer for the given product - related questions .   2.2.1 Problem Definition   Given a product - related question qand a supporting   document d={t , ... , t } ∈D , which consists ofone or more product reviews , the goal is to find a   sequence of tokens ( a text span ) a={t , ... , t }   indthat answers qcorrectly , where 1≤s≤n ,   1≤e≤n , ands≤e .   2.2.2 Datasets & Evaluation Protocols   Xu et al . ( 2019 ) build the first extraction - based   PQA dataset , called ReviewRC , using reviews   from SemEval-2016 Task 5 ( Pontiki et al . , 2016 ) .   Similarly , Gupta et al . ( 2019 ) conduct extensive   pre - processing on the Amazon dataset ( McAuley   and Yang , 2016 ; He and McAuley , 2016 ) to build   a dataset for extraction - based PQA , called Ama-   zonQA . It annotates each question as either an-   swerable or unanswerable based on the available   reviews , and heuristically creates an answer span   from the reviews that best answer the question .   Bjerva et al . ( 2020 ) propose SubjQA dataset to   investigate the relation between subjectivity and   PQA in the context of product reviews , which con-   tains 6 different domains that are built upon Tri-   pAdvisor ( Wang et al . , 2010 ) , Yelp , and Ama-   zon ( McAuley and Yang , 2016 ) datasets .   Given the same setting as typical MRC ,   extraction - based PQA adopts the same evaluation   metrics , including Exact Match ( EM ) and F1 scores .   EM requires the predicted answer span to exactly   match with the human annotated answer , while   F1 score is the averaged F1 scores of individual   answers in the token - level .   2.2.3 Methods   Due to the limited training data for extraction - based   PQA , Xu et al . ( 2019 ) employ two popular pre-   training objectives , i.e. , masked language model-   ing and next sentence prediction , to post - train the   BERT encoder on both the general MRC dataset ,   SQuAD ( Rajpurkar et al . , 2016 ) , and E - Commerce   review datasets , including Amazon Review ( He   and McAuley , 2016 ) and Yelp datasets . In real-   world applications , there will be a large number11954of irrelevant reviews and the question might be   unanswerable . To this end , Gupta et al . ( 2019 )   first extract top review snippets for each question   based on IR techniques and build an answerabil-   ity classifier to identify unanswerable questions   based on the available reviews . Then , a span - based   QA model , namely R - Net ( Wang et al . , 2017 ) , is   adopted for the extraction - based PQA . Besides ,   Bjerva et al . ( 2020 ) develop a subjectivity - aware   QA model , which performs the multi - task learning   of the extraction - based PQA and subjectivity classi-   fication . Experimental results show that incorporat-   ing subjectivity effectively boosts the performance .   2.2.4 Pros and Cons   Extraction - based PQA approaches can provide pin-   pointed answers to the given questions , but it may   be less user - friendly to provide an incomplete sen-   tence to users and may also lose some additional   information . Since there are a large proportion   of questions that ask for certain user experiences   or opinions based on the statistics in ( McAuley   and Yang , 2016 ; Deng et al . , 2022 ) , extraction-   based paradigm is less practical and favorable in   real - world PQA applications . Therefore , it can   be observed that there are relatively few works in   extraction - based PQA studies in recent years .   2.3 Retrieval - based PQA   Retrieval - based PQA studies treat PQA as an an-   swer ( sentence ) selection task , which retrieves the   best answer from a set of candidates to appropri-   ately answer the given question .   2.3.1 Problem Definition   Given a question qand a set of supporting doc-   umentsD , the goal is to find the best answer a   by ranking the list of documents according to the   relevancy score between the question qand each   document d∈D , i.e. ,a= arg maxR(q , d ) .   2.3.2 Datasets & Evaluation Protocols   Due to the absence of ground - truth question - review   ( QR ) pairs , several efforts ( Chen et al . , 2019a ; Yu   et al . , 2018b ; Zhang et al . , 2020f ) have been made   on annotating additional QR pairs into the Amazon   dataset for retrieval - based PQA . Nevertheless , the   original Amazon dataset can be directly adopted for   retrieval - based PQA studies ( Zhang et al . , 2020e , c )   that aim to select reliable or helpful answers from   candidate community answers .   Since the retrieval - based PQA methods are es-   sentially solving a ranking problem , most studiesadopt standard ranking metrics for evaluation , in-   cluding mean average precision ( MAP ) , mean re-   ciprocal rank ( MRR ) , and normalized discounted   cumulative gain ( NDCG ) .   2.3.3 Methods   Cui et al . ( 2017 ) first demonstrate a retrieval - based   PQA chatbot , namely SuperAgent , which contains   different ranking modules that select the best an-   swer from different data sources within the product   page , including community QA pairs , product re-   views , and product information . Kulkarni et al .   ( 2019 ) further propose a pipeline system that first   classifies the question into one of the predefined   question categories with a question category clas-   sifier , and then uses an ensemble matching model   to rank the candidate answers . However , these   systems usually contain multiple modules with dif-   ferent purposes , which require a large amount of   annotated data from different sources . Therefore ,   most recent retrieval - based PQA works use one or   two sources as the supporting documents and build   the model in an end - to - end manner .   When facing a newly posted product - related   question , a straight - forward answering strategy is   to retrieve a similar resolved question and provide   the corresponding answer to the target question .   However , such a solution relies heavily on a large   amount of domain - specific labeled data , since QA   data differs significantly in language characteris-   tics across different product categories . To handle   the low - resource issue , Yu et al . ( 2018a ) propose a   general transfer learning framework that adapts the   shared knowledge learned from large - scale para-   phrase identification and natural language infer-   ence datasets ( e.g. , Quoraand MultiNLI ( Williams   et al . , 2018 ) ) to enhance the performance of rerank-   ing similar questions in retrieval - based PQA sys-   tems . Besides , Mittal et al . ( 2021 ) propose a   distillation - based distantly supervised training algo-   rithm , which uses QA pairs retrieved by a syntactic   matching system , to help learn a robust question   matching model .   Another approach to obtain answers for new   questions is to select sentences from product re-   views . The main challenge is that the informa-   tion distributions of explicit answers and review   contents that can address the corresponding ques-   tions are quite different and there are no annotated   ground - truth question - review ( QR ) pairs which can11955be used for training . Yu et al . ( 2018b ) develop   a distant supervision paradigm for incorporating   the knowledge contained in QA collections into   question - based response review ranking , where the   top ranked reviews are more relevant to the QA pair   and are useful for capturing the knowledge of re-   sponse review ranking . Chen et al . ( 2019a ) propose   a multi - task deep learning method , namely QAR-   net , which can exploit both user - generated QA data   and manually labeled QR pairs to train an end - to-   end deep model for answer identification in review   data . Zhao et al . ( 2019 ) aim at improving the inter-   pretability of retrieval - based PQA by identifying   important keywords within the question and asso-   ciating relevant words from large - scale QA pairs .   Zhang et al . ( 2020f ) employ pre - trained language   models ( e.g. , BERT ) to obtain weak supervision   signals from the community QA pairs for measur-   ing the relevance between the question and hetero-   geneous information , including natural language   reviews and structured attribute - value pairs .   For the situation where multiple user - generated   answers have already been posted , Zhang et al .   ( 2020c ) propose an answer ranking model , namely   MUSE , which models multiple semantic relations   among the question , answers , and relevant reviews ,   to rank the candidate answers in PQA platforms .   2.3.4 Pros and Cons   Retrieval - based approaches select complete and in-   formative sentences as the answer , which may not   answer the given question precisely since the sup-   porting document ( e.g. , reviews ) is not specifically   written for answering the given question .   2.4 Generation - based PQA   Inspired by successful applications of sequence - to-   sequence ( Seq2seq ) models on other natural lan-   guage generation tasks , several attempts have been   made on leveraging Seq2seq model to automati-   cally generate natural sentences as the answer to   the given product - related question .   2.4.1 Problem Definition   Given a product - related question qand a set of sup-   porting documents Dthat are relevant to the given   question , the goal is to generate a natural language   answer a={t , t , ... } based on the question q   and supporting documents D.   2.4.2 Datasets & Evaluation Protocols   The Amazon dataset can be directly adopted for   generation - based PQA . Another popular datasetused for geneartive PQA is from JD ( Gao et al . ,   2019 ) , which is one of the largest e - commerce web-   sites in China . In total , the JD dataset contains   469,953 products and 38 product categories , where   each QA pair is associated with the reviews and   attributes of the corresponding product .   Evaluating generation - based methods often in-   volves both automatic evaluation and human eval-   uation . Common automatic evaluation metrics   include ( i ) ROUGE ( Lin , 2004 ) and BLEU ( Pa-   pineni et al . , 2002 ) for evaluating lexical similarity   between generated answers and ground - truth an-   swers , ( ii ) Embedding - based Similarity ( Forgues   et al . , 2014 ) , BertScore ( Zhang et al . , 2020b ) , and   BleuRT ( Sellam et al . , 2020 ) for evaluating seman-   tic relevance , ( iii ) Distinct scores ( Li et al . , 2016 )   for evaluating the diversity of the generated an-   swers . Human evaluation protocols are designed   for evaluating different perspectives of the gener-   ated answer by human annotations , such as fluency ,   consistency , informativeness , helpfulness , etc .   2.4.3 Methods   Generation - based PQA studies typically regard the   retrieval of relevant documents as a pre - processing   step , and build the method upon the retrieved docu-   ments . Due to the noisy nature of retrieved docu-   ments , Gao et al . ( 2019 ) employ a Wasserstein dis-   tance based adversarial learning method to denoise   the irrelevant information in the supporting reviews ,   while Chen et al . ( 2019c ) design an attention - based   weighting strategy to highlight the relevant words   appearing in the retrieved review snippets . Besides   identifying relevant information from the retrieved   documents , Deng et al . ( 2020 ) find that the rich   personal opinion information in product reviews   also attaches great importance in generation - based   methods , as there are a large number of subjec-   tive questions in PQA . To this end , a joint learning   model of answer generation and opinion mining is   proposed to generate opinion - aware answers . Like-   wise , Lu et al . ( 2020 ) propose a cross - passage hi-   erarchical memory network to identify the most   prominent opinion across different reviews for an-   swer generation in PQA .   Some recent works focus on leveraging docu-   ments from multi - type resources to generate the   answer . Feng et al . ( 2021 ) model the logical re-   lation between unstructured documents ( reviews )   and structured documents ( product attributes ) with   a heterogeneous graph neural network . Gao et al .   ( 2021 ) aim at solving the safe answer problem dur-11956ing the generation ( i.e. , neural models tend to gener-   ate meaningless and general answers ) , by systemat-   ically modeling product reviews , product attributes ,   and answer prototypes . Shen et al . ( 2022b ) propose   present the semiPQA dataset to benchmark PQA   over semi - structured data .   2.4.4 Pros and Cons   Generation - based methods can provide natural   forms of answers specific to the given questions .   However , the hallucination and factual inconsis-   tency issues are prevalent in generation - based meth-   ods . In addition , it is still lack of robust automatic   evaluation protocols for generation - based methods .   3 Challenges and Solutions   Although the aforementioned PQA methods are de-   veloped based on different problem settings , there   are some common challenges in PQA , as presented   in Table 1 . Several main challenges and their cor-   responding solutions are summarized as follows .   3.1 Subjectivity   Different from typical QA whose answers are usu-   ally objective and unique , a large proportion of   questions in PQA platforms are asking for sub-   jective information or opinions . Meanwhile , the   UGC in E - commerce such as product reviews also   provides rich information about other customers ’   opinion . Therefore , early studies regard PQA as a   special opinion mining problem ( Moghaddam and   Ester , 2011 ; Yu et al . , 2012 ) , which is followed   by recent opinion - based PQA studies ( McAuley   and Yang , 2016 ; Wan and McAuley , 2016 ) . Ideal   answers to this kind of questions require informa-   tion describing personal opinions and experiences .   There are two specific challenges in exploiting such   subjective information to facilitate PQA :   •Detect question - related opinion . A common so-   lution is to regard the question as the target aspect   for aspect - based opinion extraction . For exam-   ple , Bjerva et al . ( 2020 ) use OpineDB ( Li et al . ,   2019c ) and some syntactic extraction patterns to   extract opinion spans . Deng et al . ( 2020 ) em-   ploy a dual attention mechanism to highlight the   question - related information in reviews for the   joint learning with an auxiliary opinion mining   task . Zhang et al . ( 2021 ) study aspect - based sen-   timent analysis in PQA , which classifies the sen-   timent polarity towards certain product aspects   in the question from the community answers.•Aggregate diverse opinion information . Since   users may differ in opinions towards the same   question , a good PQA system should avoid ex-   pressing a random opinion , or even being con-   tradictory to the common opinion . To this   end , Deng et al . ( 2020 ) employ an opinion self-   matching layer and design two kinds of opinion   fusion strategies to uncover the common opin-   ion among multiple reviews for generation - based   PQA . Likewise , Lu et al . ( 2020 ) propose a cross-   passage hierarchical memory network to iden-   tify the most prominent opinion . However , ex-   isting studies pay little attention on resolving   conflicting user opinions , which is a common   issue in opinion summarization of product re-   views ( Pecar , 2018 ; Suhara et al . , 2020 ) and   worth exploring in the future studies of PQA .   3.2 Answer Reliability & Answerability   Similar to other UGC , product reviews and commu-   nity answers in E - commerce sites , which are also   provided by online users instead of professionals ,   vary significantly in their qualities and inevitably   suffer from some reliability issues such as spam ,   redundancy , and even malicious content . There-   fore , it is of great importance to study the answer   reliability and answerability issue when building   automatic PQA systems using these UGC . In terms   of the availability of candidate answers , existing   solutions can be categorized into two groups :   •Reliability of user - generated answers . When   there are a set of candidate user - generated an-   swers for the concerned question , the reliability   measurement of these answers has been investi-   gated from different perspectives . For example ,   Zhang et al . ( 2020e ) predict the helpfulness of   user - generated answers by investigating the opin-   ion coherence between the answer and crowds ’   opinions reflected in the reviews , while Zhang   et al . ( 2020d ) tackle the veracity prediction of the   user - generated answers for factual questions as   an evidence - based fact checking problem . How-   ever , these studies mainly focus on the content   reliability while neglecting the reliability degree   of the answerer ( Li et al . , 2017b , 2020 ) .   •Unanswerable questions based on the avail-   able documents . Question answerability detec-   tion has drawn extensive attention in typical QA   studies ( Rajpurkar et al . , 2018 ) . Similarly , Gupta   et al . ( 2019 ) train an binary classifier to classify   the question answerability for PQA . Zhang et al.11957(2020a ) propose a conformal prediction based   framework to reject unreliable answers and return   nilanswers for unanswerable questions . Mean-   while , the answerablity in PQA is also highly   related to the reliability of product reviews ( Roy   et al . , 2022a ; Shen et al . , 2022a ) .   3.3 Multi - type Resources   Another characteristic of PQA is the necessity of   processing heterogeneous information from multi-   type resources , including natural language UGC   ( e.g. , reviews , community QA pairs ) , structured   product information ( e.g. , attribute - value pairs ( Lai   et al . , 2018 ; Roy et al . , 2020 ) , knowledge graph ( Li   et al . , 2019a ) ) , E - manuals ( Nandy et al . , 2021 ) , im-   ages , etc . Early works ( Cui et al . , 2017 ; Kulkarni   et al . , 2019 ) design separated modules to handle   the questions that require different types of data re-   sources . However , these PQA systems rely heavily   on annotated data from different types of resources   and neglect the relation among heterogeneous data .   Therefore , some recent studies focus on manipu-   lating heterogeneous information from multi - type   resources in a single model for better answering   product - related questions . For instance , Zhang et al .   ( 2020f ) design a unified heterogeneous encoding   scheme that transforms structured attribute - value   pairs into a pesudo - sentence . Gao et al . ( 2019 )   employ a key - value memory network to store and   encode product attributes for answer decoding with   the encoded review representations , which is fur-   ther combined with answer prototypes ( Gao et al . ,   2021 ) . Feng et al . ( 2021 ) propose a heterogeneous   graph neural network to track the information prop-   agation among different types of information for   modeling the relational and logical information .   3.4 Low - resource   Since there are a large amount of new questions   posted in PQA platforms every day and the required   information to answer the questions varies signif-   icantly across different product categories ( even   across different single products ) , traditional super-   vised learning methods become data hungry in this   situation . However , it is time - consuming and labor-   intensive to obtain sufficient domain - specific anno-   tations . Existing solutions typically leverage exter-   nal resources to mitigate the low - resource issue . In   terms of the external resources , these solutions can   be categorized into two groups :   •Transfer learning from out - domain data . This   group of solutions typically leverages large - scaleopen - domain labeled datasets and design appro-   priate TL strategy for domain adaptation in PQA .   For example , Yu et al . ( 2018a ) transfer the knowl-   edge learned from Quora and MultiNLI datasets   to retrieval - based PQA models , by imposing a   regularization term on the weights of the out-   put layer to capture both the inter - domain and   the intra - domain relationships . Xu et al . ( 2019 )   perform post - training on the SQuAD dataset to   inject task - specific knowledge into BERT for   extraction - based PQA .   •Distant supervision from in - domain data . An-   other line of solutions adopt the resolved QA   pairs from similar products ( Rozen et al . , 2021 )   or products in the same categories ( Yu et al . ,   2018b ; Chen et al . , 2019a ; Zhao et al . , 2019 ;   Roy et al . , 2022b ) as weak supervision signals .   For example , Zhang et al . ( 2020f ) and Mittal   et al . ( 2021 ) employ syntactic matching systems   ( e.g. , BM25 ) or pre - trained text embeddings ( e.g. ,   BERT ) to obtain resolved QA pairs for facilitat-   ing the distantly supervised training process .   4 Prospects and Future Directions   Considering the challenges summarized in this pa-   per , we point out several promising prospects and   future directions for PQA studies :   •Question Understanding . Due to the diversity   of product - related questions , some attempts have   been made on identifying the user ’s intents ( Yu   and Lam , 2018a ) , the question types ( Cui et al . ,   2017 ) , and even the user ’s purchase - state ( Kuchy   et al . , 2021 ) from the questions . In addition ,   some researches investigate the user ’s uncertainty   or the question ’s ambiguity towards the product   by asking clarifying questions ( Majumder et al . ,   2021 ; Zhang and Zhu , 2021 ) . Despite the ex-   tensive studies for QA , question understanding   has not been deeply studied in the context of   PQA . For example , the system should be capable   of identifying the subjectivity from the product-   related questions ( Bjerva et al . , 2020 ) , such as   opinionated questions ( Deng et al . , 2020 ) , com-   parative questions ( Bondarenko et al . , 2022 ) , etc .   •Personalization . As mentioned before , com-   pared with typical QA studies ( Rajpurkar et al . ,   2016 ) , there is a large proportion of subjective   questions ( McAuley and Yang , 2016 ) on PQA   platforms , which involve user preference or re-   quire personal information to answer , rather than11958objective or factoid questions that look for a cer-   tain answer . Besides , in E - Commerce , different   customers often have certain preferences over   product aspects or information needs ( Chen et al . ,   2019b ; Li et al . , 2019b ) , leading to various ex-   pectations for the provided answers . Therefore ,   Carmel et al . ( 2018 ) state that a good PQA sys-   tem should answer the customer ’s questions with   the context of her / his encounter history , taking   into consideration her / his preference and interest .   Such personalization can make the answer more   helpful for customers and better clarify their con-   cerns about the product ( Deng et al . , 2022 ) .   •Multi - modality . Compared with the widely-   studied natural language UGC and structured   product knowledge data , image data has received   little attention in PQA studies . On E - Commerce   sites , there exist not only a great number of of-   ficial product images , but also increasing user-   shared images about their actual experiences ,   which benefit many other E - Commerce appli-   cations ( Liu et al . , 2021 ; Zhu et al . , 2020 ) . The   multimodal data can provide more valuable and   comprehensive information for PQA systems .   •Datasets and Benchmarks . Despite the increas-   ing attentions on developing PQA systems , the   publicly available resources for PQA are still   quite limited . Most existing PQA studies are   evaluated on the Amazon dataset ( McAuley and   Yang , 2016 ) , which is directly crawled from the   Amazon pages . Some researches ( Roy et al . ,   2022a ; Shen et al . , 2022a ) have discussed several   drawbacks of evaluating PQA systems on this   dataset : 1 ) The ground - truth answers are quite   noisy , since they are the top - voted community   answers posted by non - expert users . 2 ) There are   no annotations for assessing the relevance of the   supporting documents , which may cast potential   risks on the reliability of the PQA systems . To   facilitate better evaluations , many other data re-   sources for PQA studies have been constructed   as presented in Table 2 . However , due to the   privacy or the commercial issues , some of the   datasets can not be publicly released . Therefore ,   there is still a great demand for a large - scale ,   high - quality , and publicly available benchmark   dataset for the future studies on PQA .   •Evaluation Protocols . The types of questions   vary in a wide range , from yes - no questions   to open - ended questions ( McAuley and Yang,2016 ) , from objective questions to subjective   questions ( Bjerva et al . , 2020 ) , from factual   questions to non - factual questions ( Zhang et al . ,   2020d ) . Different types of questions may involve   different specific evaluation protocol . For ex-   ample , it is necessary to evaluate the precision   of opinion in the answers for subjective ques-   tions ( Deng et al . , 2020 ) , while the veracity or fac-   tualness is important in factual questions ( Zhang   et al . , 2020d ) . Especially for generation - based   PQA methods , the evaluation is still largely using   lexical - based text similarity metrics , which are   not correlated well with human judgements .   5 Conclusions   This paper makes the first attempt to overview re-   cent advances on PQA . We systematically cate-   gorize recent PQA studies into four problem set-   tings , including Opinion - based , Extraction - based ,   Retrieval - based , and Generation - based , and sum-   marize the existing methods and evaluation proto-   cols in each category . We also analyze the typical   challenges that distinguish PQA from other QA   studies . Finally , we highlight several potential di-   rections for facilitating future studies on PQA .   Limitations   Since product question answering ( PQA ) is actu-   ally a domain - specific application in general QA ,   the scope of the problem may be limited . However ,   in recent years , PQA has received increasing atten-   tion in both academy and industry . ( 1 ) From the   research perspective , PQA exhibits some unique   characteristics and thus brings some interesting re-   search challenges as discussed in Section 3 . For   example , some studies use PQA as an entrypoint   to analyze the subjectivity in QA tasks . ( 2 ) From   the application perspective , it has great commer-   cial value . Online shopping is playing an increas-   ingly important role in everyone ’s daily life , so that   many high - tech companies develop AI conversa-   tional assistants for promptly solving customer ’s   online problems , including but not limited to Ama-   zon , eBay , Alibaba , JD , etc . Regarding the large   amount of research efforts that have been made ,   there is not a systematic and comprehensive review   about this research topic . Similar to recent surveys   of other domain - specific QA , such as biomedical   QA ( Jin et al . , 2023 ) and legal QA ( Gil , 2021 ) ,   we hope that this paper can serve as a good refer-   ence for people working on PQA or beginning to11959work on PQA , as well as shed some light on future   studies on PQA and raise more interests from the   community for this topic .   References119601196111962ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitation section   /squareA2 . Did you discuss any potential risks of your work ?   It ’s a survey paper . There is no potential risk .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   No response.11963 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   No response .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.11964