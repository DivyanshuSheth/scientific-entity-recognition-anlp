  Youssef MohamedMohamed AbdelfattahShyma AlhuwaiderFeifan Li   Xiangliang ZhangKenneth Ward ChurchMohamed ElhoseinyKAUSTUniversity of Notre DameNortheastern UniversityFigure 1 : ArtELingo , a multilingual dataset and benchmark of WikiArt with captions & emotions   Abstract   This paper introduces ArtELingo , a new bench-   mark and dataset , designed to encourage work   on diversity across languages and cultures . Fol-   lowing ArtEmis , a collection of 80k artworks   from WikiArt with 0.45 M emotion labels and   English - only captions , ArtELingo adds another   0.79 M annotations in Arabic and Chinese , plus   4.8 K in Spanish to evaluate “ cultural - transfer ”   performance . More than 51 K artworks have 5   annotations or more in 3 languages . This diver-   sity makes it possible to study similarities and   differences across languages and cultures . Fur-   ther , we investigate captioning tasks , and find   diversity improves the performance of baseline   models . ArtELingo is publicly availablewith   standard splits and baseline models . We hope   our work will help ease future research on mul-   tilinguality and culturally - aware AI.1 Introduction   Figure 1 compares and contrasts annotations on   WikiArt across language / culture . We believe these   differences are interesting and important , and far   from random . One might suggest using machine   translation to translate English captions to many   other languages , but we believe that doing so would   miss much of the opportunity . Building human-   compatible AI that is more aware of our emotional   being is important for increasing the social accep-   tance of AI . ArtEmis ( Achlioptas et al . , 2021 ) is   an important step in this direction , introducing a   collection of 0.45 M emotion labels and affective   language explanations in English on more than   80,000 artworks from WikiArt . However , by de-   sign , ArtEmis is limited to English , lacking cover-   age of other cultures and languages .   Cultural differences are a major source of diver-   sity ( Meyer , 2014 ) . The customs , social values,8770lifestyles , and history of different countries and   cultures greatly influence human behavior . Emo-   tional experiences are no exception ; people from   different countries respond differently to similar   scenarios . For example , a person born and raised   in a Nordic country would be more comfortable in   a lush forest than in a desert , but a Bedouin may be   more comfortable in a desert than in a forest .   Consider Figure 1c , where an Arabic annota-   tor assigned the image the label contentment , but   the other two annotators used the label : sadness .   Captions are useful for diving deeper into these dif-   ferences . The sadness annotations mention death   and disasters , in contrast with the contentment   annotation that ends with : feeling of satisfaction .   There can be interesting differences between lan-   guages / cultures even when annotators use the same   label . Consider Figure 1b , where all three labels are   contentment . Although the three captions agree on   the label , two of the captions imply that some / all of   the girls are sisters , but there is no such implication   in the English caption .   We believe deep nets will be viewed as more cul-   turally aware , if they can capture linguistic / cultural   patterns such as these . Emotions are based on past   experience , and play an integral role in determining   human behavior . Not only they reflect our inter-   nal state but also directly effect how we perceive ,   interpret external stimuli ( Izard , 2009 ) , and how   to act based on them ( Lerner et al . , 2015 ) . Hence ,   studying emotions is essential to exploring a con-   founding aspect of human intelligence .   In summary , our contributions are :   1.0.79 M annotations ( labels + captions ) in Ara-   bic and Chinese , plus 4.8k in Spanish ,   2 . a benchmark with standard splits , and   3.baseline models for two tasks : ( 1 ) label pre-   diction and ( 2 ) affective caption generation .   The rest of the paper is organized as follows :   related work is discussed in § 2 , followed by our   main motivation in § 3 , and data collection in § 4 .   § 5 provides qualitative and quantitative analyses   of ArtELingo . Baseline models for emotion label   prediction and caption generation are presented in   § 6 and § 7 , respectively .   2 Related Work   2.1 Captions with Emotions   Work on captioning is moving beyond factual cap-   tions in early benchmarks such as COCO ( Lin et al . ,   2014 ) . Figure 2 shows two images of families , one   from ArtEmis and the other from COCO . Both cap-   tions capture the facts , but ArtEmis enhances the   facts with emotion / commentary .   Table 1 compares three benchmarks : COCO ( Lin   et al . , 2014 ) , ArtEmis and ArtELingo . Art Emis en-   courages work on emotions by replacing COCO   photos with WikiArt , and by introducing 9 emo-   tion classes , 4 positive,4 negativeand Other .   ArtELingo encourages researchers to work on vi-   sually grounded multilinguality by providing af-   fective annotations in three languages ( henceforth ,   ACE / ACES ): Arabic , Chinese and English . In addi-   tion , we provide a small set of Spanish ( S ) . Figure 3   shows that positive emotions are more frequent   than negative emotions , especially in Arabic .   COCO ArtEmis ArtELingo Photos WikiArt WikiArt 328k 80k 80k 2.5 M 0.45 M 1.2 M 7.6 5.68 15.3 0 9 9 E E ACES8771   2.2 Related Work in Other Fields   There is a considerable literature on emotions , es-   pecially in Psychology ( Russell and Barrett , 1999 ) .   One can find quite a few benchmarks on emotion in   HuggingFace : ( Saravia et al . , 2018 ; Demszky et al . ,   2020 ; Xiao et al . , 2018).There are a number of   papers in computational linguistics on emotion and   Chinese ( Chen et al . , 2020 ; Quan and Ren , 2009 ;   Wang et al . , 2016 ; Lee et al . , 2010 ) , and on emotion   and Arabic ( Abdullah and Shaikh , 2018 ) . There is   also considerable work on emotion in other fields   such as vision ( Mittal et al . , 2021 ) .   Many datasets have been collected to study emo-   tional responses to modalities such as :   •Text ( Strapparava and Mihalcea , 2007 ; Dem-   szky et al . , 2020 ; Mohammad et al . , 2018 ; Liu   et al . , 2019 ) ,   •Image ( Mohammad and Kiritchenko , 2018 ;   Kosti et al . , 2017 ) , and   • Audio ( Cowen et al . , 2019 , 2020 ) .   Bias is the flip side of inclusiveness . There has   been considerable discussion recently about biases   ( Bender et al . , 2021 ; Bolukbasi et al . , 2016 ; Buo-   lamwini and Gebru , 2018 ; Mehrabi et al . , 2021;Liu et al . , 2021 ) . Some of this work is more rele-   vant to our interest in Chinese ( Jiao and Luo , 2021 ;   Liang et al . , 2020 ) , and Arabic ( Abid et al . , 2021 ) .   Many machine learning methods will , at best , learn   what is in the training data . There have been some   attempts to remove biases in corpora , but it might   also be constructive to create more inclusive bench-   marks such as ArtELingo .   Awareness of different cultures is becoming in-   creasingly important . Gone are the days when it   was sufficient for datasets to focus on a single cul-   ture . Recently , the Vision & Language community   has been producing more multicultural multilin-   gual datasets ( Bugliarello et al . , 2022 ; Srinivasan   et al . , 2021 ; Armitage et al . , 2020 ) . ArtELingo   contributes cultural diversity over emotional expe-   riences . The effect of culture on psychology has   been studied in separate studies ( Henrich et al . ,   2010 ; Abu - Lughod , 1990 ; Norenzayan and Heine ,   2005 ) . ArtELingo provides empirical evidence that   might motivate cultural psychology studies .   3 Opportunities for Improvement   Many of the resources mentioned above have ad-   vanced our understanding of the relationship be-   tween emotion and various stimuli , through there   are always opportunities for improvement . We   are particularly interested in three such opportuni-   ties : scale , multimodality and multilinguality / muli-   culturalism . As for scale , demand for larger train-   ing sets is expected to continue to increase , given   the rise of large scale foundation models ( Bom-   masani et al . , 2021 ) .   As for multimodality , although most benchmarks   mentioned above focus on a single modality , there   are a few multimodal exceptions such as IEMO-   CAP ( Busso et al . , 2008 ) , COCO and ArtEmis .   IEMOCAP collected speech and facial and hand   movements of 10 actors . Unfortunately , this ap-   proach may be expensive to scale up .   The use of Amazon Mechanical Turk in   ArtEmis is easier for scaling , however , ArtEmis   is limited to English . ArtELingo addresses   multilinguality / multi - culturalism by adding Ara-   bic and Chinese annotations . We use languages as   a proxy to reflect different cultures . English is a   representative sample of the West , and Chinese is   a representative sample of the East , and Arabic is a   representative sample of the Middle East.8772Region # Artworks %   West ( Non English ) 142.8k 57.1 %   West ( English ) 54.0k 21.6 %   Other 38.0k 15.2 %   Middle East ( Non Arabic ) 12.2k 4.8 %   Middle East ( Arabic ) 1.6k 0.6 %   East ( Chinese ) 1.4k 0.5 %   Total 250.0k 100 %   3.1 Representation of Regions in WikiArt   ArtELingo assumes that WikiArt is a representative   sample of the cultures of interest . While WikiArt   is remarkably comprehensive , Table 2 suggests the   WikiArt collection has better coverage of the West   than other regions of the world . This table is based   on WikiArt ’s assignment of artworks to national-   ities . We assigned each nationality to West ( En-   glishand Non English ) , Middle East ( Arabic   and Non Arabic ) , East ( Chinese ) and Other .   4 ArtELingo   Following ArtEmis , we employ Amazon Mechani-   cal Turk ( AMT ) platform to collect our data using   interfaces ( see Figures 8 , 9 , 10 in the appendix ) .   We faced a lack of Arabic and Chinese speaking   annotators on AMT which led us to devise differ-   ent strategies to recruit annotators . Arabic speak-   ers were recruited by advertising the task in mid-   dle eastern universities encouraging students and   their families to join our data collection efforts .   Whereas Chinese speakers were recruited through   Baidu who we ’d like to thank .   Annotators are asked to carefully examine each   artwork before selecting the dominant emotion in-   duced by it from a list of four positive , four negative   E C A S   # Annotators 6377 745 656 31   # Annotations 429k 426k 369k 4.8 K   # Work Hours 10k 13k 9.0k 178   emotions , and Other to indicate a different emo-   tion . Annotators are then asked to write captions   that reflects the content of the artwork and explains   their choice of emotion . Similar to ArtEmis , we   collect annotations from five annotators for each   artwork .   For a better cultural representation in ArtELingo ,   we restrict the collection of different languages an-   notations to countries with large numbers of native   speakers . Chinese data is collected from China .   For Arabic , we collect our data mainly from Saudi   Arabia and Egypt . Finally , Spanish is collected   from Latin America and Spain . Figure 4 shows   that most of the annotations are from a long tail   of workers who annotated less than 1000 artworks   ensuring a diverse representation of cultures .   Quality Control . Annotations were rejected if they   are too short , or if they are too similar to captions   for other artworks . In addition , a manual review   was conducted by multiple reviewers , ensuring cap-   tions reflect the selected emotion label and the de-   tails of the artwork . Table 3 reports some statistics   on annotations that passed this review process .   5 Dataset Analysis   5.1 Qualitative   There are some interesting similarities and differ-   ences between language and culture , as discussed   in Figure 1 . There is a considerable inter - annotator   agreement ( IAA ) in the dataset , and there are also   some interesting disagreements . There is agree-   ment in Figure 2a that a mother ’s love is universally8773   warm and pleasant . It is an instinct for mothers to   be loving , caring and protective of their children .   On the other hand , there is a difference in Figure 1a .   All three annotators agree to observe a waterfall   though some mention energy and growth , while   others saw horses and wedding veils .   5.2 Quantitative   Table 4 reports multicultural agreement over the   9 emotionsin each genre . WikiArt classifies   artworks into 10 genres , as well as 27 styles .   Agreement is computed as a log likelihood agree-   ment score , A = log(Pr(G|D)/Pr(G|U ) ) ,   where Gis one of the 10 genres , and UandD   are two sets of artworks . Let Pr(G|U)be the frac-   tion of artworks in Uwith genre G , andPr(G|D )   be the fraction of artworks in Dwith genre G.   LetUbe the universal set of artworks . That   is , Ucontains all artworks in ArtELingo with 5   annotations in each of the 3 languages . Dis a dis-   agreement set of 2000 artworks . Dwas selected by   computing Cohen Kappa scores ( Cohen , 1960 )   for artworks in U. LetDbe the 2000 artworks with   the most disagreement ( based on Kappa ) .   Table 4 shows that there is more agreement for   some genres ( landscapes ) , and more disagreement   for other genres ( sketches ) . When the agreement   score is near 0 , then the genre is about equally   likely in UandD. This is to be expected for gen-   res near the middle of the list such as misc . Figure 5   shows 8 artworks in genres with high agreement   and high disagreement . Figure 6 reports the Co-   hen ’s Kappa score of annotations from language   pairs . Annotators belonging to the same language   have higher agreement .   We created Dfor zero - shot experiments to be   reported in § 6 . The 4.8k Spanish annotations in   Table 3 are on the set of Dartworks with low IAA   ( inter - annotator agreement ) in ACE ( Arabic , Chi-   nese and English).87746 Emotion Label Prediction   Baseline models for two tasks , emotion label pre-   diction and caption generation , will be discussed in   this section and the following section . These discus-   sions assume familiarity with deep nets including   fine - tuning BERT ( Devlin et al . , 2019 ) and cross   language models XLM ( Conneau et al . , 2020 ) , as   well as HuggingFace ( Wolf et al . , 2019 ) .   Emotion Classification . Given an input caption ,   c , we wish to predict an output emotion label , ˆe ,   where ˆeis one of the 9 emotions . The model starts   with a pretrained language model , LM , and a to-   kenizer . The tokenizer converts cinto a sequence   ofLtokens x. The language model converts x   into more useful representation , LM(x)∈R ,   where dis the number of hidden dimensions ( a   property of the LM ) . Finally , we feed LM(x)into   a linear layer to predict the emotion label , ˆe .   Majority Baseline . We use the majority emotion   label for each artwork as the predicted emotion for   all captions belonging to that artwork . Concretely ,   each artwork , I , has a set of caption - emotion pairs ,   S. The majority classifier outputs the most frequent   emotion , ˆe , in the set Sfor all of the captions in the   set , c∈S ,   Language Models . We finetune 3 models based   on BERT ( BERT - E , BERT - A and BERT - C ) , where   BERT - E is tuned for English , and BERT - A is tuned   for Arabic and BERT - C is tuned for Chinese . Sec-   tion 11.2 discusses more pretraining and finetuning   details . We also finetune 4 models based on cross   language models , XLM - roBERTa ( Conneau et al . ,   2020 ) , where XLM - E , XLM - A and XLM - C corre-   spond to English , Arabic , and Chinese languages ,   as before . In addition , we create XLM - ACE by   training on the combination of all 3 languages .   3 - Headed Transformer . Finally , we create a   model with XLM - R backbone but replace the sin-   gle classifier head with 3 classifier heads , one for   each of the 3 languages . While training , we feed   the captions from each language to the shared back-   bone and then use the corresponding head to predict   an emotion that would ultimately reflect the culture   of that language . Geva et al . ( 2021 ) analyzed simi-   lar multi - headed transformers and showed how the   non - target heads can be used to interpret the results   of the target head . Similarly , our 3 - headed trans-   former can be used to predict 3 different emotions   each one reflecting the culture norms represented in   each language . We can then use these predictions   to better understand the similarities and differences   between cultures .   Experimental Setup . We use the base versions   of both the BERT and XLM - R models with their   default tokenizers from HuggingFace . We use   the standard finetuning procedure where we use   the ADAM optimizer to finetune the model for 5   epochs on batches of size 32 with learning rate of   2×10 . We use cross entropy as the loss function   for updating the full model parameters , including   the transformer backbone . We follow the standard   ArtEmis ( Achlioptas et al . , 2021 ) splits introduced   in ( Mohamed et al . , 2022 ) and adopt them for both   Arabic and Chinese datasets . The same training   and testing images are used in all cases . For BERT   models , we only evaluate on the same language   as the training set because BERT tokenizers are   language specific .   Baseline Results . Table 5 reports accuracy for   several BERT / XLM models . There are 4 test sets ,   one for each language , plus ACE ( a combination   of 3 languages ) . XLM models perform better than   BERT , because there is no data like more data , as   well as the cross language setup used during pre-   training . Interestingly , scores on the Chinese test   set are higher than for English and Arabic , sug-   gesting that Chinese captions are easier to classify .   Finally , notice that XLM - ACE ( XLM trained on   3 languages ) outperforms other conditions , show-   casing benefits of multiple languages . Note that   XLM - ACE even outperforms matching conditions,8775   where training language = test language .   3 - Headed Transformer Analysis . Although the 3-   Headed transformer did not improve accuracy , the   3 classification heads are useful for error analysis .   We feed the entire ArtELingo dataset to the model   and predict 3 ˆevalues , one for each head / language .   Confusion matrices are reported in Figure 7 . There   is more agreement on negative emotions , and less   agreement on positive emotions .   We are interested in large off - diagonal values in   Figure 7 , especially between positive and negative   emotions . For example , Arabic disgust is often   confused with English amusement .   Upon further investigation , we found nude paint-   ings contributed ∼15 % of these confusions . Ex-   plicit content and alcohol are frowned upon in some   Arabic speaking communities , as illustrated by the   second and third rows of Table 6 , where the label is   positive in English and Chinese , but not in Arabic .   Religious symbols are also associated with large   off - diagonal values in confusion matrices . The first   row in Table 6 mentions Jesus and how a beautiful   girl holds his cross and stomps on the devil . The   annotation is positive ( awe ) in English and Arabic ,   but negative ( fear ) in Chinese . In China , the cross   holds less meaning , and stomping on the devil is   more scary than reassuring . Many symbols are   associated with religion , holidays and legends that   mean more in some places than others .   While there are a few off - diagonal cells with   large values , most of the large values in the con-   fusion matrices are on the main diagonal . That is ,   the similarities across languages tend to dominate   the differences . Consider the last row in Table 6 ,   which receives a positive label ( contentment ) in   all 3 languages . Babies make people feel happy   ( nearly ) everywhere . In this case , all 3 heads of   our 3 - headed transformer predict positive labels for   this caption . For training models across multiple   languages , similarities across languages may be   more useful than differences .   Zero - Shot Evaluation . We use Spanish annota-   tions in ArtELingo to evaluate models mentioned   above in a zero - shot setting . The last column in   Table 5 reveals two interesting relations :   1 . 3 - Headed - E > XLM - ACE   2 . 3 - Headed - E > 3 - Headed - A > 3 - Headed - C   The first relation suggests that 3 - Heads may not   perform as well as XLM when there is plenty of   data , but 3 - Heads may have advantages in low-8776resource and zero - shot settings . 3 - Heads are better   for capturing interactions between languages .   The second relation suggests that language trans-   fer may be more effective across some language   pairs than others . Historically , Spanish and En-   glish are both relatively close Indo - European lan-   guages , compared to Semitic languages such as   Arabic . There has been much less contact ( Thoma-   son , 2001 ) between those languages and Chinese .   7 Affective Caption Generation   The previous section described baseline models   for the first task : label prediction . This section   will describe baseline models for the second task :   affective caption generation .   To this end , we follow Achlioptas et al . ( 2021 )   and train two affective captioning models : Show ,   Attend , and Tell ( SAT ) ( Xu et al . , 2015 ) and   Meshed Memory Transformer ( M ) ( Cornia et al . ,   2020 ) . We use Affective Captioning Models to refer   to captioning models that generate affective cap-   tions . These captions connect the dots between   input paintings and emotions .   SAT is a LSTM ( Hochreiter and Schmidhuber ,   1997 ) based captioning model with an attention   module , it consists of a visual encoder and a text   decoder . The visual encoder extracts visual fea-   tures from an input image . The decoder then uses   a stack of an attention module and LSTM recur-   rent unit to generate a caption autoregressively .   Mis a transformer based model ( Vaswani et al . ,   2017 ) which utilizes a pretrained Faster - RCNN   ( Ren et al . , 2015 ) object detector to extract visual   region features . These features are used as an input   sequence to a multi - layer attention based encoder .   Mdiffers from basic transformers by feeding the   encoded features from all encoder layers to the   cross attention module in each decoder ’s layer . In   order to include Emotion and Language grounding ,   we use a simple embedding layer to convert the   emotion and language labels into feature vectors   and then concatenate them to the visual features .   Experimental Setup . For both models , we use   the default parameters proposed in ( Achlioptas   et al . , 2021 ) . We train four different versions of   each model , three versions are trained on English ,   Arabic , and Chinese only datasets , while the fourth   version is trained on the three languages combined .   We then test all the models on all the languages . In   order to allow the models to work on an arbitrary   languages during testing , we create our custom   tokenizer which is based on xlm - roberta - base tok-   enizer from HuggingFace . The available tokenizer   has a vocabulary of size 200 K tokens which makes   the training inefficient . To mitigate this , we use the   same xlm - roberta - basetokenizer training strat-   egy to create a tokenizer with 60 K vocabulary size   on ArtELingo .   Results . We report the results of our baseline   models in Table 7 . Models trained using all the   languages perform very similarly to their language   specific counterparts on every metric except for the   Chinese language . This provides additional evi-   dence that English and Arabic speaking cultures   are more closely related to one another than either   is to Chinese ones . In other words , English caption-   ing models do not lose much performance when   Arabic data is added to the training set and vice   versa . On the other hand , Chinese models suffer   when such data is added . Moreover , we also ob-   serve that for models trained on single languages ,   the scores on the combined test set is proportional   to the language specific test sets.87778 Conclusion   This paper introduced ArtELingo , a multilingual   dataset and benchmark on WikiArt images with   more than 1.2 M captions and emotion labels . The   benchmark has diverse emotional experiences con-   structed over different cultures , and communicated   in four languages ( English , Chinese , Arabic , and   Spanish ) . We found more agreement for some gen-   res such as landscapes and more disagreement for   other genres such as sketches . These differences   are interesting and important , and far from random .   Annotations for trees in Figure 1c are labeled as   sadness in English and Chinese but contentment in   Arabic . People are likely to feel more comfortable   with what they know . People raised in countries   with lush forests are likely to prefer that , whereas   people brought up in less humid environments are   likely to prefer that .   Towards building more socially and multi-   culturally aware AI , we created baseline models   for two tasks on ArtELingo : ( 1 ) emotion label pre-   diction and ( 2 ) affective caption generation . For   emotion label prediction , our best baseline model   trained XLM on a combination of training data   from all three languages ( XLM - ACE ) . We also cre-   ated 3 - headed transformers , training three heads   for three languages ( Arabic , Chinese , and English )   at the same time . The performance of this model   is close to XML - ACE , but generalizes better in   a zero - shot experiment on Spanish . For the cap-   tion generation task , we trained two models on   ArtELingo , SAT and M. For English and Arabic ,   models on all three languages have a similar perfor-   mance to language specific models , but for Chinese ,   it is best to train without the other languages since   the performance drop is significant .   We hope our benchmark and baselines will help   ease future research in visually - grounded language   models that can communicate affectively with us .   In addition , ArtELingo can provide empirical exam-   ples of cross - cultural similarities and differences .   Sociologists and Cultural Psychologists may for-   mulate hypotheses and conduct field studies based   on ArtELingo . Data , code , and models are publicly   available at www.artelingo.org/ .   9 Limitations   ArtELingo ’s artworks are extracted from WikiArt .   Although ArtELingo is diverse in language and   culture , it inherits WikiArt ’s bias toward western   artworks as discussed in Table 2 in § 3.1 . Thereis room to improve the representation of certain   regions of the world . Due to globalisation , peo-   ple tend to follow similar trends around the world ,   causing others to follow their lead ( for better and   for worse ) .   Many cultures , such as Arabic , do not have a rich   heritage of oil paintings . Instead , they have other   forms of Art like poetry and calligraphy . Such art   forms are interesting to study on their own , but   mixing them with paintings is not obvious . Based   on the original ArtEmis dataset , we chose WikiArt   with the intent to be a continuation of their work .   Also , artworks are more accessible and can be in-   terpreted easier by different cultures compared to   poetry and other art forms .   The addition of affective captions for Arabic ,   Chinese , as well as a small set of Spanish is a   step toward cultural diversity . However , more than   four regions and languages are indeed needed to   cover the world . Scalability can be a challenge .   However , we hope that progress can be accelerating   by developing affective vision and language models   that can learn with limited data for each additional   language by distilling knowledge from language-   only models as in ( Chen et al . , 2022 ; Alayrac et al . ,   2022 ) .   ArtELingo was also collected through AMT ’s   online platform . This suggests that the work-   ers are familiar with technology and social media ,   imposing an influence on the data . Social media   influences many concepts such as : trending news ,   and standards , which may lead to the presence of   similarities between cultures . There have been , of   course , other concerns about the use of AMT and   the so - called “ gig ” economy and workers ’ rights .   10 Acknowledgements   The authors would like to thank Baidu for solicit-   ing Chinese annotators , all the annotators for their   effort in the data collection , Eric Macedo Esparza   for reviewing the Spanish dataset , and the anony-   mous reviewers for their valuable comments . We   also would like to thanks all the middle eastern   universities , mainly in Egypt , who contributed to   collecting the Arabic version .   This work was supported by King Abdullah Uni-   versity of Science and Technology ( KAUST ) , under   Award No . BAS/1/1685 - 01 - 01.8778References87798780878111 Appendix   11.1 GitHub Repo   You can find the dataset and more visuals in   artelingo.org or our github repo github .   com / Vision - CAIR / artelingo   11.2 Pretrained BERT models   In the emotion prediction experiment , we finetune   pretrained BERT models . For each language , we   use a BERT model pretrained only on that language .   In particular , we use “ bert - base - uncased”for   English ; “ CAMeL - Lab / bert - base - arabic - camelbert-   mix”for Arabic ; and “ bert - base - chinese”for   Chinese .   Language specific models are finetuned on sub-   sets of ArtELingo having captions written in the   same language . On the other hand , multilingual   models are pretrained “ XLMroBERTa”and they   are finetuned on the whole of ArtELingo .   For each model , we finetune the pretrained   model for 5 epochs . We use an ADAMW opti-   mizerwith a learning rate of 2×10with a   linear schedule . We use cross - entropy as the loss   function . Please check our GitHub repo for all of   the implementation details .   11.3 Ethical Concerns   We received approval for the data collection from   KAUST Institutional Review Board ( IRB ) . The   IRB requires informed consent ; in addition , there   are terms of service in AMT . We respected fair   treatment concerns from EMNLP ( compensation )   and IRB ( privacy ) . We compensated the workers   well above the minimum wage ( < $ 1 USD / hour in   Egypt and $ 2.48 USD / hour in China ) . We paid   our workers $ 0.07 USD per completed task . Eachtask takes on average 50 seconds to complete . In   addition , we paid bonuses ( mostly 30 % ) to workers   who submitted high - quality work .   The workers were given full - text instructions   on how to complete tasks , including examples of   approved and rejected annotations ( please refer   to§11.4 ) . Participants ’ approvals were obtained   ahead of participation . Due to privacy concerns   from IRB , comprehensive demographic informa-   tion could not be obtained.878211.4 User Interfaces878387848785