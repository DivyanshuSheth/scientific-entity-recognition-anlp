  HyoJung Han   Computer Science   University of Maryland   hjhan@cs.umd.eduMarine Carpuat   Computer Science   University of Maryland   marine@cs.umd.eduJordan Boyd - Graber   CS , UMIACS , iSchool , LCS   University of Maryland   jbg@umiacs.umd.edu   Abstract   Detractors of neural machine translation ad-   mit that while its translations are fluent , it   sometimes gets key facts wrong . This is par-   ticularly important in simultaneous interpre-   tation where translations have to be provided   as fast as possible : before a sentence is com-   plete . Yet , evaluations of simultaneous ma-   chine translation ( S MT ) fail to capture   if systems correctly translate the most salient   elements of a question : people , places , and   dates . To address this problem , we introduce   a downstream word - by - word question answer-   ing evaluation task ( SQA ): given a source   language question , translate the question word   by word into the target language , and answer   as soon as possible . SQAjointly measures   whether the S MT models translate the   question quickly and accurately , and can re-   veal shortcomings in existing neural systems —   hallucinating or omitting facts .   1 Introduction   Recent advances in simultaneous machine trans-   lation ( S MT ) hold the promise of breaking   language barriers by democratizing simultaneous   interpretation , a demanding form of real - time trans-   lation which currently requires trained human ex-   perts ( Grissom II et al . , 2014 ; Cho and Esipova ,   2016 ) . However , progress in this field is hampered   by evaluation challenges . Since S MT sys-   tems must output translations in real time before   the input sentence is complete , evaluation methods   should account for the timeliness of outputs , in ad-   dition to capturing the dimensions of translation   quality that also matter in traditionalsettings   ( Ma et al . , 2019 ; Arivazhagan et al . , 2019 ) . Ade-   quate evaluation on S MTis critical to build   models that help a user decide when to take the   right action quickly and correctly in a multilingual   situation . Since human evaluation is too costly ( and   slow ) to guide system development , evaluation isFigure 1 : Overview of the Question Answering Evalua-   tion for S MT(SQA ) . Given a word - by - wordtask , the S MTsystem produces translations   of questions word by word . A good S MT sys-   tem will allow the downstreamsystem to answer   correctly as quickly as possible .   currently limited to quantifying the trade - off be-   tween translation latency and quality , as measured   by standard translation quality metrics ( Ma et al . ,   2020a ; Stewart et al . , 2018 ) or further addition of   time penalty ( Grissom II et al . , 2014 ) . However ,   it remains unclear how to interpret such scores in   practice . Metrics designed to assess the quality of   full - input machine translation ( ) are not well-   suited to evaluating partial translations , and can fail   to capture salient errors . Measuring timeliness by   the average rate by which the S MTsystem   lags behind an ideal synchronous translator does   not tell us to what degree S MTtranslations   are useful for practical purposes .   To address these issues , we propose to evaluate   S MTby measuring how well it helps execute   a cross - lingual word - by - word question answering   task ( SQA ) . This task measures the quality of   timely adequacy , or immediacy of adequacy , of   S MTmore directly than existing evaluations .   Given a source language question , the task con-   sists of translating the question word - by - word into   the target language , and answering it as soon as   possible . This evaluation is inspired by the use   of Question Answering ( ) to evaluatesys-   tems in full - input settings , where a correct answer   is considered an indication that the salient source   content was adequately translated ( Tomita et al . ,55981993 ; Sugiyama et al . , 2015 ; Scarton and Specia ,   2016 ; Krubi ´ nski et al . , 2021 ) . Unlike prior work ,   we evaluate translations on a word - by - word basis   rather than waiting for a complete translation of the   source , thereby evaluating whether source content   is immediately conveyed adequately enough for asystem to answer correctly .   We introduce a novel Cross - lingual Quizbowl   Dataset , to systematically evaluate Polish-   English and Spanish - English S MTsystems   of varying latency , and compare them against full-   inputthat translates complete inputs . By jointly   accounting for timeliness and translation quality ,   theSQAevaluation reveals different trends that   complement full - inputandevaluations   based on complete inputs , and also it can reveal   critical S MTerrors by monitoringquality   over time .   2 Motivation   2.1 S MT Evaluation Challenges   Defining the quality of simultaneous interpretation   is challenging . An early version of a practical guide   to interpreters by the International Association of   Conference Interpreters describes quality as “ that   elusive something which everyone recognizes but   no one can successfully define ” ( AIIC , 1982 ) . Gr-   bi´c ( 2008 ) shows that there is no consensus in   the definition of quality in the field of interpret-   ing studies , where perspectives such as “ quality   as perfection ” co - exist with “ quality as fitness for   purpose ” , where quality is viewed as satisfying   user needs . Zwischenberger ( 2010 ) identifies more   specific quality criteria that span three different di-   mensions : content ( e.g. , is the translation faithful ,   logical , complete ? ) , form ( e.g. , is the translation   grammatical ? does it use the appropriate style and   terminology ? ) , and delivery ( e.g. , is the delivery   synchronous , fluent , lively ? ) . However , she finds   that while interpreters all value faithfulness over-   all , the relative importance of these criteria varies   depending on the type of interpretation assignment   ( e.g. , technical congress vs. press conference ) and   the interpreter community surveyed .   Additional issues arise when evaluating the qual-   ity of S MT rather than human interpreting ,   since systems might err differently and can not re-   pair as humans do . The simultaneous nature of   S MTmakes its evaluation more difficult than   full - input , which is already recognized to beone of the hardest evaluations in natural language   processing ( Stewart et al . , 2018 ; Cherry and Fos-   ter , 2019 ; Iranzo - Sánchez et al . , 2021 ; Zhao et al . ,   2021 ) . For instance , human experts agree that in   simultaneous interpreting dropping extraneous in-   formation is sometimes acceptable , and that para-   phrases that decrease the time between an utter-   ance and its translation are not just acceptable but   desired ( He et al . , 2016a ) . Given all of these dif-   ficulties , automatic evaluation for S MThas   either asked experts to evaluate translations along   these dimensions ( a high quality but slow and non-   scalable solution ) or adapted standardevalu-   ations to minimize delay between utterance and   translation . Current assessment of S MT is   thus simply based on two criteria : latency com-   pared to an ideal synchronous interpreter and trans-   lation quality based on standard reference - based   metrics designed for full - input . Also , the prior   work of Grissom II et al . ( 2014 ) suggests Latency- ( ) to jointly account for quality and   ‘ expeditiousness ’ by adding a time penalty to .   However , these metrics do not capture all the errors   that neural systems are particularly prone to make ,   and do not tell us how useful the translations are .   2.2 Task - driven S MT Evaluation   We take a different approach on evaluating   S MT and focus on task - driven evaluation .   Imagine that you are transported to Warsaw , Poland   and do not speak Polish . You are a contestant on a   game show where you need to answer trivia ques-   tions . The questions give clues that get easier over   time , and you can interrupt at any time to give your   answer , with the goal to answer correctly before   your opponent . Even though you know nothing   about Polish , you get help from a S MTsys-   tem . Our hypothesis is that the higher the mono-   lingual contestant ’s score on this game — where   the latency of every word matters — is a reasonable   proxy for how good the S MTsystem is . As   we discuss in Section 6 , the quality of the underly-   ing contestant ( here asystem ) affects the score   as well , but keeping it constant nevertheless makes   it possible to compare S MT systems .   While the specific scenario above might seem   unlikely , this approach evaluates essential aspects   ofS MT for a broad range of use cases . It   can be seen as directly measuring the “ fitness for   purpose ” of S MTon a task where adequacy   is particularly important , which aligns with impor-5599   tant dimensions of quality from interpreting studies   discussed above . This setup targets error types   that are likely to be particularly problematic with   state - of - the - art neuralsystems .   3 Simultaneous Question Answering   Evaluation Framework   In this section , we describe the two components   of our framework , simultaneous translation ( Sec-   tion 3.1 ) and the word - by - wordtask ( Sec-   tion 3.2 ) , and how they are integrated ( Section 3.3 ) .   3.1 Simultaneous Machine Translation   Simultaneous Machine Translation ( S MT )   starts translating the prefix of an input sequence   before the complete sequence is available , unlike   traditional full - inputwhich translates given a   complete source sequence as an input . We use the   term “ full - input”to refer to translating entire   sequences one at a time , which is sometimes also   called “ offline”in the literature .   The key design consideration for S MTsys-   tems is therefore to choose the policy that governs   whether to wait to receive more source tokens , or   to write the output given the current prefix . Fig-   ure 2a illustrates this process . Among the many   strategies that have been proposed in the literature   ( Cho and Esipova , 2016 ; Gu et al . , 2017 ; Ma et al . ,   2019 ; Zheng et al . , 2020b ; Arivazhagan et al . , 2019 ;   Ma et al . , 2020b ) , we choose -k(Ma et al . ,   2019 ) as a simultaneous translation model , because   it is conceptually simple , relatively easy to imple-   ment , and achieves competitive results in existing   evaluations . -kmodels simply wait until k   source tokens have been produced , and then start toalternate writing target tokens and reading source   tokens . When the model exhausts the source to-   kens , it continues to write until end of sequence .   For measuring latency , we use differentiable   average lagging ( Arivazhagan et al . , 2019 , ) ,   which represents the average rate by which the   S MT system lags behind an ideal syn-   chronous translator . Usually , iskfor -k   S MT models .   3.2 Word - by - Word Question Answering   We choose Quizbowl ( Boyd - Graber et al . , 2012 , ) as a proxy task for an evaluation of the simul-   taneous translation model since the task also deals   with incremental inputs and with sequential deci-   sion making.is a trivia game where questions   come in word - by - word : the questions get easier   over time . Players ofmust interrupt the ques-   tion and answer as soon as they know the answer   ( i.e. , before their opponent ) . This problem has been   investigated in monolingual settings in prior work .   Here we consider a new cross - lingual version to   evaluate a S MT model .   As in simultaneous translation , the key trade - off   inis accuracy vs. speed . After every input word ,   the system produces its top Nguesses given the   current input — which consists of the words used   in the question in the monolingual case , and of   the outputs of the S MTsystem in the cross-   lingual case . Thesystem must decide whether   to trust that partial translation , as is explicitly mod-   eled in some simultaneous interpretation systems   ( Grissom II et al . , 2014 ; Stewart et al . , 2018 ) .   Where the value ofcomes through is in the   ease of evaluation : it is much easier to decide   whether an answer is correct than to decide if a   translation is “ good ” . When humans compete on , it is typically head - to - head : on how many ques-   tions did Player A answer before Player B. While   this metric has been used ( He et al . , 2016b ) , it is   cumbersome . At the same time , while answering   earlier is better than answering later , the number of   characters that the system needs to answer is not an   informative metric , as some parts of the question   are more important than others .   As a result , we follow Rodriguez et al . ( 2019 ) ,   who propose the expected wins ( ) metric to eval-   uate a system ’s ability to win at : represent   the probability that a system would beat the aver-   ageplayer on the question . Each question ’s   score is the value ofwhich is a function of rela-5600tive position only if the prediction is correct , and 0   if the answer is wrong . Initially , thefunction   is near 1.0 — answering at the start of the question   will always defeat an opponent — while at the end   of the question it is near 0.0 : most players will have   answered the question correctly . In between , the   function monotonically decreases , reflecting the   average player ’s ability to answer a question . This   function is trained on existing English questions   over a variety of topics .   3.3 SQA — QA Evaluation for S MT   Task Overview . We define the task of Question   Answering with S MT(SQA ) as follows :   an input query text in the source language is fed   into a S MTmodel and the translated target   query text is then processed with amodel to   produce an answer in the target language . Both sys-   tems process their respective inputs on a word - by-   word basis , which results in the tightly integrated   pipeline of Figure 1 .   Concretely , this task is made possible by a cross-   lingual Quizbowl dataset ( , described in Sec-   tion 4 ) , which provides parallel question - answer   pairs in Polish - English and Spanish - English . The   S MT system translates the Polish question   into English and an Englishmodel provides an   answer . Thebuzzer model decides whether to   buzz and return the answer , or to wait . The system   answer is then compared to the English reference .   Integration Details . Sincequestions are long   sequences that can include multiple sentences , we   first split a paragraph of an input query ( P ) into   unit sentences ( S={S , ... , S , ... } ) , and then   feed each source sentence token by token to the   S MTmodel ( S={s , ... , s , ... } ) . For each   output time step ( T={t , ... , t , ... } ) , a generated   token is fed into the word - by - wordsystem . It   decides whether to buzz and give its current answer ,   or whether to wait for more target tokens .   In the monolingualsetting , the buzzing posi-   tion is simply defined based on the current position   in the input query . In the cross - lingual setting , we   need to define the buzzing position τon the source   side in terms of target buzzing position τas we   now translate corresponding source text into target   text . The buzzing position in source text τis :   τ=∑︂|S|+g(τ−∑︂|T| ) ( 1 )   where g(j)is delay at target position j : the number   of source tokens read by the agent before writing   thejth target token with a certain sentence l. In   the case of -kmodel , g(j)is approximately   min ( k+j−1,|S| ) .   This framework can also compare full - input   on thetask while ignoring incremental input .   In the case of sentence translation for , we gen-   eralize the calculation of source buzz position by   setting kto∞.   4 Experiment Settings   4.1 : the Cross - lingual Quizbowl Dataset   OurSQAevaluation relies onquestions writ-   ten in languages other than English . We collect   multilingualdata in collaboration with Interna-   tional Academic Competitions ( ) .These ques-   tions are used in competitions with grade school   students in their local language . The domain ques-   tions include history , science and geology . The   questions have a fixed distribution over subjects   and difficulty levels to make it fun for many levels   of contestants .   We construct the Cross - lingual Quizbowl test   set ( ) by translating Polish answers into En-   glish using language links across Wikipedia pages ,   augmented with the Wiki - Titles dataset from .5601This yields a set of 965 Polish questions paired with   English answers . In addition , we obtain reference   translations for the Polish questions into English by   asking human translators to post - edit full - input ,   providing randomly ordered Google Translate ( ) , and Transformer - big outputs ( Section 4.2 ) , to   avoid biasing references toward a singlesystem .   This results into a parallel corpus of 512 questions   and answers in Polish and English . An automatic   sentence splitter splits questions into a single sen-   tence for . The statistics of constructed is   detailed in Table 1 .   Following the same process , we collect 148   Spanish questions with English answers , which   also come with human reference translations , as   described in Appendix E.   4.2 Model Settings   Dataset . We use the Polish - English 2020   dataset ( Barrault et al . , 2020 ) to train all the   models . Ourmodels are trained only on the   general domain corpus of . They are notfine-   tuned nor adapted to the domain , which we   treat exclusively as held - out evaluation data . Fast-   align ( Dyer et al . , 2013 ) filters the training set ,   resulting in 8 M sentences . We use the standard   newsdev2020 and newstest2020 as development   and test set . The Quizbowl system is the standard   English training set from Rodriguez et al . ( 2019) .. The default setting for neural machine trans-   lation ( ) and S MT models follow the   big configuration of transformer ( Vaswani et al . ,   2017 ) . The shared vocabulary size is 40k   ( Sennrich et al . , 2016 ) . S MT models are   trained using -k(Ma et al . , 2019 ) , where   k∈ { 3,6,9,12,15 } , with a uni - directional en-   coder . The emission rate is set to 0.97 based on   the tokenized training set . As a full - input baseline ,   we train a Transformer - big model ( “ Transf ” ) . For   statistical machine translation ( ) , we use the   Moses toolkit ( Koehn et al . , 2007 ) . The base   andS MTmodels are trained up to 300k train   steps on single A6000 — each step is a batch   of approximately 16384 tokens . All models ex-   cept for Google Translate , baseline , and are   trained three times and values in the results section   includes means and standard deviations .. is a Guesser and a Buzzer.4.3 Evaluation Metrics   Extrinsicmetrics . We compute the Expected   Win ( ) scores , following Rodriguez et al . ( 2019).is the expected probability of winning against   an average player as a function of buzzing posi-   tion and empirically estimated based on the human   gameplay . Theprobablity is only added when   the buzzer buzzes for the first time and the answer   is correct ( as in an actual game ) . In this paper ,   however , there is only one player , so the score   calculation under the competitive setting may be   strict . Therefore , we also calculatewith an   oracle buzzer ( ) where the buzzer would buzz   as soon as the guesser gets the correct answer . As   a result , is a more lenient version of .   Intrinsicmetrics . We compute ( Pap-   ineni et al . , 2002 ; Post , 2018 ) , ( Rei et al . ,   2020 ) , and BertScore ( Zhang * et al . , 2020).We use   Differentiable Average Lagging ( Arivazhagan et al . ,   2019 , ) as a latency metric for S MT .   5 Main Results   Figure 3 showsandmetrics acrosssys-   tems on the Polish - English task .   MTMetrics . As expected , traditional metrics   ofquality all increase monotonically as the   S MTsystem waits longer before translating   in Figure 3b . Translation quality increases steeply   by 10 BLEU points from -3to -9and   plateaus around 48 BLEU with longer wait times ,   4 BLEU points below the translation quality of   the full - input system that uses the same architec-   ture and training data as S MT ( “ Transf ” ) .   BertScore and COMET show consistent trends .   The quality of S MTmodels falls roughly be-   tween that of full - input , which is close to or   slightly above -3depending on the metric ,   and that of full - input systems , with our Trans-   former followed by Google Translate .   SQAResults . By jointly capturing how   quality and latency impact the probability of win-   ning , theevaluation in Figure 3a paints a dif-   ferent picture . Whileand initially im-   prove with longer latency , the translation quality   gains have diminishing returns onas latency   increases . Thescores shows that the -9   models have better timely adequacy than -12 ,   even though -12models have access to three   additional source tokens , andscores decrease5602   further with higher latency . The scores fol-   low the same trend but peak with -6 , indicat-   ing that timely adequacy could be obtained with a   lower latency given an oracle buzzer .   Full - input MT . We compare S MTsystems   with full - input . The scores of full - input   systems are represented by horizontal lines in Fig-   ure 3 . First , we compare theaccuracy of these   models based on the complete translation of the   input question , thus ignoring the consideration of   timeliness ( Figure 3a ) . In this setting , S MT   systems are less accurate than full - inputsys-   tems by at most 6 points , and the -15setting   is on par with full - inputsystems . While   outperforms by a wide margin according toevaluation metrics , their accuracy is close on   thetask , falling roughly 2 points below the   upper - bound achieved with human translation .   Next , we return to the SQA evaluation of   timely adequacy viaand scores . We com-   pute these scores for full - inputmodels by as-   suming that they are -kmodels where kis∞ ,   and calculate the source buzzing position and cor-   respondingscore accordingly . In this setting ,   the best S MTsystem ( -9 ) outperforms   full - inputsystems according to . Given an   oracle buzzer ( ) , all -kmodels improve   over full - input , and four of them even improveover human translation . While the accuracy of   Transformer and lag slightly behind that of   Google Translate on full input accuracy , the Trans-   former and systems are the best and second   best of full - inputaccording to and   scores , indicating that our local models ’ timely ad-   equacy is higher than that of a commercial system .   Taken together , these results confirm the po-   tential of SQA to evaluate the joint impact   of adequacy and timeliness . This is further aug-   mented by the results of the experiment with-   eswhich shows consistent trends in Appendix E.   Design choices for S MTsystems , namely the -kpolicy , have a clear impact on SQAmet-   rics . The discrepancy between the SQAand   results confirms that traditional S MT eval-   uation does not suffice to assess systems ’ ability   to convey core source information correctly and   quickly .   6 Consistency on multiplemodels   Section 5 only uses the model . This con-   sistency evaluates ’s effect onmetrics . To   see how the choice ofsystem affects the results ,   we compare the guesser with ( Devlin   et al . , 2018 ) and ElasticSearch ( Gormley and Tong ,   2015 ) guesser . This experiment uses the English an-   swer matched set Min Table 1 to use all available5603   questions and report for the -kmodels .   While the guesser outperforms and Elas-   ticSearch , follows consistent trends with all   guessers as increases , with peak timely ade-   quacy for -6 . This confirms the soundness   of our evaluation approach . The details of the ex-   periment and full results of eachmetric are in   Appendix C and Figure 7 .   7 Step - wise Visualization of S MT   Errors   This section moves away from aggregate evalu-   ations and analyzes the behavior of the SQA   system step - by - step on a single question at a time .   We show that monitoring the quality of question   answering over time can reveal translation errors .   Method . Figures 5a and 5b show how the an-   swers of thesystem and their quality change   as the SQAsystem consumes the input ques-   tion . The x - axis represents the relative position of   a guess based on the question on the source side .   The target word where thesystem generates   a guess can be mapped to the location of the part   of the source question that has been consumed by   SQA . We normalize by the length of the source   question to obtain the relative character position   used on the x - axis of our plots . The y - axis uses   mean reciprocal rank ( MRR ) to represent the good-   ness of the current answer . Reciprocal rank is the   inverse of the position of the highest ranked an-   swer , and if there are no correct answers within   the top Nthen the reciprocal rank is 0 . ( higher is   better ) We use log MRR for more even plots . Here   Nis set to 50 , and the lowest value of log MRR   is -4 . MRR can also be used for overall systemevaluation , which results in similar trends as those   described above with theand metrics ( re-   fer to the Appendix D for details ) .   Hallucination . In the first example ( Figure 5a   and Table 2a ) , the -9model is confused dur-   ing the first half of the question and mostly guesses   the correct answer ( “ Longitude ” ) in the next half .   The red curve , which plots a binary indicator for   S MToutput words that are unaligned to the   source , shows that unaligned words often are crit-   icalerrors and correlate with steep drops of log   MRR .   At position ( 1 ) , marked in Figure 5a and Ta-   ble 2a , the Polish word “ dwu ´ scienny ” is translated   into a close but incorrect translation “ double - wall ” ,   and this causes log MRR to drop slightly . At ( 2 )   and ( 3 ) , the word “ southern ” is hallucinated by the   S MT model at each point , and there is no   similar corresponding word in the given source .   This hallucination causes log MRR to drop to zero .   By contrast , at position ( 4 ) , thesystem is robust   enough to ignore the unaligned stopword “ a ” . does reveal translation errors , but it treats   every word the same and is calculated after the full   output , while SQAfocuses on essential words   that change the answer prediction in real - time . For   example , when “ półpłaszczyzna ” is erroneously   translated as “ southern ” instead of “ half - plane ” in   Figure 5a , is misled and guesses wrong , but   ignores minor errors such as excluding “ the ” or   “ a ” , while both cases have same impact on .   Under Translation . In Figure 5b , there is a flat   span where log MRR dosen not change for more   than half the question . This is because even though   the -3models consume source words continu-   ously , it does not output any new words due to an   under - translation error . As can be seen in Table 2b ,   the translation output of -3is too short , and   as a result thesystem generates a wrong guess .   These two examples illustrate thatprovides   useful signals to pinpoint critical translation errors   inS MT outputs , and suggest that SQA   might provide a strategy for more systematic error   detection on the fly in future work .   8 Related Work   QA as Evaluation . Answering questions pro-   vides a natural way to evaluate whether humans   can comprehendoutput . For instance , Tomita56045605et al . ( 1993 ) use a reading comprehension task from   the TOEFL test to evaluate the quality of English   to Japanese . Jones et al . ( 2005 ) find that an   Arabic - to - Englishsystem made it possible for   English speakers to pass Arabic Level 2 on a stan-   dard defense language proficiency test . Scarton and   Specia ( 2016 ) use reading comprehension ques-   tions to obtain human assessments ofat the   document level . Forcada et al . ( 2018 ) find the use   of gap - filling can be a reasonable alternative to   reading comprehension questionnaires for rankingsystems .   Automaticcan evaluate whether full - inputadequately conveys core elements of the source   text on a larger scale . Some approaches rely on   manually curatedtest beds ( Sugiyama et al . ,   2015 ; Sun et al . , 2020 ) , while Krubi ´ nski et al .   ( 2021 ) usegeneration to generate questions   from references and extract an answer . While these   papers show the promise offor evaluating ,   they focus on the full - input setting.has also been used as an extrinsic evaluation   for text summarization systems ( Eyal et al . , 2019 ;   Wang et al . , 2020 ; Durmus et al . , 2020 ; V olpi and   Malagò , 2020 ) to complement existing evaluation   metrics , which are notoriously insensitive to factual   inaccuracies and inconsistencies .   S MTEvaluation . Most S MT met-   rics focus on quantifying the timeliness of   S MT , using Average Proportion ( Cho and   Esipova , 2016 , AP ) , Continuous Wait ( Gu et al . ,   2017 , CW ) , Average Lagging ( Ma et al . , 2019 ,   AL ) , or Differentiable Average Lagging ( Arivazha-   gan et al . , 2019 ; Cherry and Foster , 2019 , DAL ) .   Though widely used as official metrics of IWSLT   shared tasks ( Ma et al . , 2020a ) , these metrics are   used at the sentence - level , which departs from real-   istic usage . Recent work improves latency measure-   ment by adapting them to streaming input ( Iranzo-   Sánchez et al . , 2021 ) and by introducing dedicated   interpretation test sets ( Zhao et al . , 2021 ) . However ,   all of these are used in combination with traditionalmetrics on complete outputs , which does not   directly measure the impact of timely adequacy .   Adding a time penalty to BLEU could also   jointly account for quality and latency in one metric .   Grissom II et al . ( 2014 ) define the Latency - BLEU   ( LBLEU ) to measure ‘ expeditiousness ’ and qual-   ity by calculating a word - by - word discrete integral   across the input . However , LBLEU and related   metrics retain all the weaknesses of BLEU . In par - ticular , BLEU treats every word equally , and does   not explicitly penalize mistranslating the most im-   portant words for answering a question . As a result ,   it does not directly align with users ’ information   needs , while our method can directly measure the   impact of keyword ( mis)translations and of transla-   tion delays .   9 Conclusion   The main motivation of simultaneous translation   ( S MT ) is to convey the core meaning in a   source sentence as quickly as possible . However ,   current S MTresearch measures the quality   of output on “ finalized ” translations and separately   considers latency measures , which can not fully re-   flect the quality of timely adequacy .   This paper introduces a cross - lingual word - by-   word question answering task ( SQA ) to quantify   the timely adequacy of S MTmore directly .   Our experiments on a cross - lingual Quizbowl task   show that some -kS MT systems can   win more often than full - inputsystems with   higher BLEU score , and can outperform full - input   human translations with an oracle buzzer . Overall   ourSQAresults complement intrinsicandmetrics by jointly accounting for timeliness and   translation quality , and suggest that SQAcan   diagnose critical S MT errors on the fly .   These results represent a first step toward broad-   ening the scope of S MTevaluation to more   directly assess its usefulness in specific scenarios .   In future work , we would like to evaluate on more   languages and investigate the impact of varying   each component of the SQApipeline , includ-   ing using alternate S MT architectures , and   providing S MToutputs to human contestants   in addition to automaticsystems . Finally , we   would like to expand the modality to speech for   more realistic S MT and .   Limitations   The experiments in this paper are limited to Euro-   pean languages , and to one direction , into English .   Polish is a West Slavic language , which differs   from English in several ways : it is a highly fusional   language which has seven grammatical cases . It   has relatively free word order , although it often fol-   lows an SVO structure . Therefore , we can expect   some reordering to be needed but probably not as5606much as when translating from pairs with signifi-   cantly different word order like Korean or Japanese   into English . We also experiment with Spanish ,   which requires minimal word reordering into En-   glish . Given that monotonicity between source and   target language has an impact on the performance   ofS MT(Chen et al . , 2021 ; Han et al . , 2021 ) ,   experimental results in languages with different   word order could be different .   However , we expect SQA to remain an in-   sightful evaluation tool for languages with largely   different word orders , as it will capture delayed   translations from S MT . For example , in a   question with “ married ” ending a long sentence ,   thesystem may fail to answer because it lacks   the essential relationship between entities . On the   other hand , conventional BLEU would consider   this omission equivalent to any other word . There-   fore , we expect SQAto be no worse on different   word orders .   Some evaluation settings were directly borrowed   from English Quizbowl for our cross - lingual ver-   sion of the game . Specifically , the Expected Wins   function ( ) is trained on English Quizbowl ques-   tions . However , since the Polish version of the   game is directly modeled after the English game ,   we do not expect this to be an issue .   More importantly , our experiments are all sim-   ulations . It remains to be seen how S MT   systems would fare when used by human contes-   tants rather thansystems , or when pitted against   bilingual or monolingual human contestants , pos-   sibly assisted by human simultaneous interpreters .   This work relies on human reference translations   ( obtained under the full - input setting ) as a first step ,   and did not compare against simultaneous interpre-   tation by humans .   Finally , our work only considers the text modal-   ity due to limited resources . However , the most   practical modality of S MT andfor that   matter is speech .   Ethics Statement   As mentioned in 4.1 , the Multilingualquestions   are established by an international academic organi-   zation , and we are using these questions with their   explicit permission . The questions are in the public   domain .   The workers who translated or post - edited   Quizbowl questions were paid at a rate of USD   0.5 per question . The resulting estimated hourlywage is above the minimum wage per hour in the   United States .   Acknowledgements   We thank International Academic Competitions for   creating and providing multilingual Quizbowl ques-   tions . Specifically , we would like to thank David   Madden ( Executive Director ) , Lee Holden ( Senior   Director of Question Production ) , Natalia Stasik   ( Director , Polska ) , Franek Alverado ( Director   of Question Production — Polska ) , Gabriel Rol-   dos ( Director , Ecuador ) , and Saad Bashir ( Di-   rector of Question Production — Ecuador ) for   their valuable support . We thank the anonymous   reviewers , Pedro Rodriguez , Shi Feng , Weijia Xu ,   Elijah Rippeth , Aquia Richburg and the members   of the lab at for their insightful and con-   structive feedback . This work is supported by   Grants-1822494 and Grant-1750695 and by , , via the Program contract   # 2019 - 19051600005 . The views and conclusions   contained herein are those of the authors and should   not be interpreted as necessarily representing the   official policies , either expressed or implied , of , , or theGovernment . TheGov-   ernment is authorized to reproduce and distribute   reprints for governmental purposes notwithstand-   ing any copyright annotation therein .   References5607560856095610   A Examples of   Table 5 shows example questions of - plinclud-   ing full text of source question , human translation ,   andresult of -3 & -6 .   B AdditionalMetrics   Figure 9 show additionalevaluation metrics   of YiSi-1(Lo , 2019 ) , Prism ( Thompson and Post ,   2020 ) , and BLEURT ( Sellam et al . , 2020 ) . We   also provide the performance of all the models on   standard WMT test set in Figure 8.C Results and Details of multiple   models   In this section , we describe the details and show   the full results of the experiment of different   guesser models mentioned in Section 6 . For addi-   tionalmodels , we use BERT ( Devlin et al . ,   2018 ) as Transformer model and ElasticSearch   ( Gormley and Tong , 2015 ) as Information Retrieval   model . Both guessers are trained and indexed with   same data used in the GRU guesser . The results   on , and full - input accuracy are shown in   Figure 7 . The trends for themetrics are not as   consistent , which we attribute to the fact thatis   based on fewer data points than .   D Average Mean Reciprocal Rank   Evaluation   We show overall performance on a set of guess-   able questions ( Gin Table 1 ) for each model in   Figure 6 . The relative character position is binned   into 25 section and y - axis is average of log MRRs   of each bins for all guessable questions . As rela-   tive character position move toward one , or as the   SQAconsumes more information , log MRR in-   creases to zero which means a model get closer to   the answer . In Figure 6 , -9show generally top   performance except for first one third part , which is   consistent with our main result in Figure 3a . Since -kwith higher kis closer to full - input ,   the MRR scores is usually positioned around the   ending position of each sentence , and this causes   the fluctuation of -12∼model .   E Spanish and other languages   E.1 - es   In this section , we describe newly added Spanish   set,-esand its results onandmetrics .   We collect small set of Spanish questions from lo-   cal competitions in Ecuador as presented in Table 3 .   We use Spanish - English WMT2013 ( Bojar et al . ,   2013 ) dataset to train all themodels . We use   standard set of newstest2012 , newstest2013 as dev   and test set . We run experiments with same settings   on - plexcept that we remove the ElasticSearch   result due to severely low performance .   In Figure 10 , present similar trends acrossmodels as well as languages where the peak is   mostly on k= 6and the performance diminishes   as latency increases.with RNN models shows   similar trends with other languages while BERT56115612   has different conclusion compare to Polish . We hy-   pothesise it is due to high impact of noisy example   among small set and lower full - input accuracy on   S MTmodels with high latency . Overall , we   observe similar trends on SQAresults with-   escompared to - plwhich indicates robustness   of our SQA task .   E.2 Collection of   We collect multilingual QA data in collaboration   with an International Academic Competitions via   local competitions in each country . Therefore , the   creation of is a gradual procedure as local   competitions progress rather than a one - time mass   production , and the ease of collection for new lan-   guages depends on competition popularity in a lan-   guage . However , since QA competitions are ac-   tive and the dataset is for evaluation ( not for train-   ing ) , we can collect enough questions to S MT   models .   E.3 Overlaps across languages   For Polish , there are about 65 % overlap between   the Qanta English(Rodriguez et al . , 2019 ) and - planswers . “ France ” and “ sun ” are the most   frequent common answers , while “ Białowie ̇za For-   est ” or “ Jan Brzechwa”(Polish poet ) are in Pol-   ish only . For Spanish , the overlap is 64 % , while   the most common answer is “ Spain ” , and “ Julio   Jaramillo ” ( Ecuadorian singer ) as an example of   Spanish only answer .   F Additional Related Works   Alongside the evaluation methods of latency , vari-   ousS MTmodels is also propose to optimally   achieve faster translations with better accuracy .   S MT models can be categorized by the kind   of policy . The S MTmodel with fixed policy   generates translation based on pre - defined policy   without considering current status of the translationprocess . For example , the -kmodel proposed   by Ma et al . ( 2019 ) waits for ktokens and alternates   READ and WRITE . Even though this deterministic   feature in the policy results in anticipation error ,   the quality is competitive with not - to - small ks , and   faster speed on making decision of action is one   of the strong point with the easy implementation   as well . Many variations of -khave been pro-   posed to address the shortcomings . For example ,   Caglayan et al . ( 2020 ) exploits additional visual   context to complement missing source information .   Finally , Zheng et al . ( 2020a ) extended the -k   to an adaptive policy by integrating a set of fixed   policies models . Also , Zhang and Feng ( 2021 )   propose universal S MTmodel with mixture-   of - experts -kto achieve optimal performance   under arbitrary latency .   Many adaptive policies have also been suggested   ( Cho and Esipova , 2016 ; Gu et al . , 2017 ; Zheng   et al . , 2019 ; Arivazhagan et al . , 2019 ; Ma et al . ,   2020b ) . Cho and Esipova ( 2016 ) suggest to use   greedy decoding to decide an action to write or   read , while Gu et al . ( 2017 ) utilize a reinforcement   learning to train agent with the objective of maxi-   mizing quality and minimizing latency . Advancing   this work , Alinejad et al . ( 2018 ) adds a new action   called PREDICT that anticipates upcoming source   words .   Beside the kind of policy , availability of revi-   sion could be another characteristics of S MT .   While streaming approach only appends the gener-   ated tokens , re - translation allows limited revisions   to the already presented partial translation ( Niehues   et al . , 2018 ; Arivazhagan et al . , 2020 ; Han et al . ,   2020 ) . Re - translation can provide more accurate   translation via correction of prior mistakes , how-   ever frequent revision can harm the user experience   and may not suitable for speech applications .   Recently , Arivazhagan et al . ( 2019 ) utilize hard   attention for decision making and introduced56135614QA Metrics MT Metrics   Russian F1 EM BLEU COMET BLEURT Prism   Human 37 29.4 _ _ _ _ 24.43 18.82 25.7 0.13 0.63 -1.90   m2m_418 M 24.26 17.65 36.1 0.52 0.73 -1.64   m2m_1.2B 26.31 19.61 40.67 0.63 0.77 -1.33   m2m_12B 26.76 20.78 39.92 0.58 0.76 -1.44   Finnish F1 EM BLEU COMET BLEURT Prism   Human 34.8 26.00 _ _ _ _ 25.02 17.50 19.10 0.04 0.59 -2.30   m2m_418 M 25.08 18.06 33.97 0.45 0.67 -1.27   m2m_1.2B 29.52 20.56 41.20 0.64 0.73 -1.10   m2m_12B 29.84 21.10 39.87 0.59 0.71 -1.18   which is a differentiable version of average lagging   ( Ma et al . , 2019 , ) , in order to integrate latency   measures into training losses . Ma et al . ( 2020b )   incorporate this work into the multi - headed Trans-   former model . Furthermore , Zhang et al . ( 2020 )   proposes learning method to segment source input   corresponds to possible target output .   G Full - inputandEvaluation   Alternatively , we conduct experiments on non-   dynamicsystem where complete and non-   incremental static input generates one time output   to verify our findings fromin more general   frameworks where the feature of dynamic inputs   and decision making is removed . In this case , the   query is a single interrogative question sentence .   We choose to set up the experiment to translate   only a question query for simplicity rather than   translating both the query and the given passages .   Formetrics , standard Exact Match ( EM ) and   F1 - Score are used . We use a multilingual devel-   opment set of XOR - TyDidata ( Asai et al . ,   2021 ) . Among seven languages of XOR - TyDi ,   we choose Finnish ( fi ) and Russian ( ru ) for the   experiments due to the availability ofmodel   pairs . For the QA model , we use a pre - trained   DPRmodel . Formodels , we use , Google   Translate ( GT ) and M2 M ( Fan et al . , 2021 ) mod-   els in different parameter size — m2m100_418 M   ( small ) , m2m100_1.2B ( medium ) , m2m100 - 12B-   avg-5 - ckpt ( large).Since human translations areonly given on training set of XOR , we randomly   choose 2000 source - reference pairs from training   set and use it as evaluatingscores . The value of   human in Table 4 is taken from Asai et al . ( 2021 ) .   In Table 4 , we evaluate the quality of full - inputwith XOR - TyDi dataset . Our experiment in-   cludes Russian and Finnish languages , and we use   m2 m model in different size and a model .   The findings in this setup is not exactly same as   theperformance of translated questions is   not the best or second best compare to other full-   inputmodels . However , we can observe thatevaluation of model still the better or sim-   ilar compare to that of m2m_418 M model while   those two model shows large gap inevaluations .   Also , m2m_1.2B and m2m_12B shows opposite   measurement inandmetrics , and it is inter-   esting to see such disagreement is consistent across   languages . While m2m_1.2B constantly outper-   forms m2m_12B in allmetrics , m2m_12B is   outperforming m2m_1.2B in extrinsicmetrics .   This indicates thatandmetrics show clear   disagreement in the range of higher quality and in   the range of lower quality as well.5615Q Text   SourceT˛ e współrz˛ edn ˛ a wyznacza k ˛ at dwu ´ scienny mi˛ edzy półpłaszczyzn ˛ a   południka zerowego a półpłaszczyzn ˛ a południka przechodz ˛ acego przez   okre´slony punkt na powierzchni Ziemi . T˛ e miar˛ e liczy si˛ e od południka   zerowego ( Greenwich ) a ̇z do południka 180 ° . Aby otrzyma ´ c punkt , nazwij   t˛ e długo ´ s´c , która mo ̇ze przyjmowa ´ c miary od 0 ° do 180 ° i mo ̇ze by ´ c   wschodnia lub zachodnia .   Answer - src Długo ´ s´c geograficzna   HumanThis coordinate is determined by the dihedral angle found between the   half - plane prime meridian and the half - plane meridian which passed   through a specific point on the Earth ’s surface . This measure is counted   from the prime meridian ( Greenwich ) to the 180 ° meridian . To gain a   point , give the name of the length , which can measure between 0 ° to 180 °   and can be east or west .   Answer - tgt Longitude -9This coordinate determines the double - wall angle between the southern   half of the meridian plane and the southern half - plane passing through   a certain point on the surface of the Earth . This measure counts from   the south of the zero ( Greenwich ) to the south 180 ° . To get a point , name   the length that can take measures from 0 ° to 180 ° and can be eastern   or western .   Q Text   SourceW 1899 Halford Mackinder twierdził , ̇ze jest pierwsz ˛ a osob ˛ a , która   wspi˛ eła si˛ e na t˛ e gór˛ e , a ludzie Kikuju nazywaj ˛ a t˛ e gór˛ e Kirinyaga ,   co oznacza „ t˛ e ze strusiem ” , podczas gdy Masajowie wierz ˛ a , ̇ze ich   przodkowie zeszli z tej góry na pocz ˛ atku czasu . Góra ta jest głównym   obszarem zlewiska rzeki Tana , najwi˛ ekszej rzeki w kraju , która mo ̇ze   by´c równie ̇z nazywana tak samo jak ta góra . Aby zdoby ´ c punkt , podaj   nazw˛ e tej drugiej najwy ̇zszej góry w Afryce .   Answer - src Mount Kenia   HumanIn 1899 , Halford Mackinder claimed that he was the first person to   climb this mountain , and the people of Kikuyu named this mountain   “ Mount Kirinyaga ” , meaning „ the one with the ostrich ” , while the   people of Maasai believe that their ancestors descended from this   mountain at the beginning of time . This mountain is the main   catchment area of the Tana river , the largest river in the country ,   which is sometimes refered to by the same name as the mountain .   To gain a point , enter the name of the second highest mountain in Africa .   Answer - tgt Mount Kenya -3In 1899 , Halford Mackinder claimed that he was the first person   to climb the mountain . This mountain is the main area of the Tana   River , the largest river in the country that can also be called the same   as the mountain . To get a point , please specify the name of the   other top in Africa.5616