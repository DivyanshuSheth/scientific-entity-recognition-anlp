  Vipul RathoreKartikeya BadolaParag Singla Mausam   Indian Institute of Technology   New Delhi , India   rathorevipul28@gmail.com , kartikeya.badola@gmail.com   parags@cse.iitd.ac.in , mausam@cse.iitd.ac.in   Abstract   Neural models for distantly supervised rela-   tion extraction ( DS - RE ) encode each sentence   in an entity - pair bag separately . These are then   aggregated for bag - level relation prediction .   Since , at encoding time , these approaches do   not allow information to ﬂow from other sen-   tences in the bag , we believe that they do   not utilize the available bag data to the fullest .   In response , we explore a simple baseline ap-   proach ( PARE ) in which all sentences of a bag   are concatenated into a passage of sentences ,   and encoded jointly using BERT . The contex-   tual embeddings of tokens are aggregated us-   ing attention with the candidate relation as   query – this summary of whole passage pre-   dicts the candidate relation . We ﬁnd that our   simple baseline solution outperforms existing   state - of - the - art DS - RE models in both mono-   lingual and multilingual DS - RE datasets .   1 Introduction   Given some text ( typically , a sentence ) tmention-   ing an entity pair ( e;e ) , the goal of relation ex-   traction ( RE ) is to predict the relationships between   eandethat can be inferred from t. LetB(e;e )   denote the set of all sentences ( bag ) in the cor-   pus mentioning eandeand letR(e;e)denote   all relations from etoein a KB . Distant su-   pervision ( DS ) trains RE models given B(e;e )   andR(e;e ) , without sentence level annotation   ( Mintz et al . , 2009 ) . Most DS - RE models use   the “ at - least one ” assumption : 8r2R(e;e ) ,   9t2B(e;e)such thattexpresses ( e;r;e ) .   Recent neural approaches to DS - RE encode each   sentencet2B(e;e)and then aggregate sen-   tence embeddings using an aggregation operator   – the common operator being intra - bag attention   ( Lin et al . , 2016 ) . Various models differ in their   approach to encoding ( e.g. , PCNNs , GCNs , BERT)and their loss functions ( e.g. , contrastive learn-   ing , MLM ) , but agree on the design choice of en-   coding each sentence independently of the others   ( Vashishth et al . , 2018 ; Alt et al . , 2019 ; Christou   and Tsoumakas , 2021 ; Chen et al . , 2021 ) . We posit   that this choice leads to a suboptimal usage of the   available data – information from other sentences   might help in better encoding a given sentence .   We explore this hypothesis by developing a sim-   ple baseline solution . We ﬁrst construct a pas-   sageP(e;e)by concatenating all sentences in   B(e;e ) . We then encode the whole passage   through BERT ( Devlin et al . , 2019 ) ( or mBERT   for multilingual setting ) . This produces a contex-   tualized embedding of every token in the bag . To   make these embeddings aware of the candidate re-   lation , we take a ( trained ) relation query vector , r ,   to generate a relation - aware summary of the whole   passage using attention . This is then used to predict   whether ( e;r;e)is a valid prediction .   Despite its simplicity , our baseline has some con-   ceptual advantages . First , each token is able to ex-   change information with other tokens from other   sentences in the bag – so the embeddings are likely   more informed . Second , in principle , the model   may be able to relax a part of the at - least - one as-   sumption . For example , if no sentence individually   expresses a relation , but if multiple facts in differ-   ent sentences collectively predict the relation , our   model may be able to learn to extract that .   We name our baseline model Passage - Attended   Relation Extraction , PARE ( mPARE for multi-   lingual DS - RE ) . We experiment on four DS - RE   datasets – three in English , NYT-10d ( Riedel et al . ,   2010 ) , NYT-10 m , and Wiki-20 m ( Gao et al . , 2021 ) ,   and one multilingual , DiS - ReX ( Bhartiya et al . ,   2022 ) . We ﬁnd that in all four datasets , our pro-   posed baseline signiﬁcantly outperforms existing   state of the art , yielding up to 5 point AUC gain .   Further attention analysis and ablations provide   additional insight into model performance . We re-340   lease our code for reproducibility . We believe that   our work represents a simple but strong baseline   that can form the basis for further DS - RE research .   2 Related Work   Monolingual DS - RE : Early works in DS - RE   build probabilistic graphical models for the task   ( e.g. , ( Hoffmann et al . , 2011 ; Ritter et al . , 2013 ) .   Most later works follow the multi - instance multi-   label learning framework ( Surdeanu et al . , 2012 )   in which there are multiple labels associated with   a bag , and the model is trained with at - least - one   assumption . Most neural models for the task en-   code each sentence separately , e.g. , using Piece-   wise CNN ( Zeng et al . , 2015 ) , Graph Convolu-   tion Net ( e.g. , RESIDE ( Vashishth et al . , 2018 ) ) ,   GPT ( DISTRE ( Alt et al . , 2019 ) ) and BERT ( RED-   SandT ( Christou and Tsoumakas , 2021 ) , CIL(Chen   et al . , 2021 ) ) . They all aggregate embeddings us-   ing intra - bag attention ( Lin et al . , 2016 ) . Beyond   Binary Cross Entropy , additional loss terms in-   clude masked language model pre - training ( DIS-   TRE , CIL ) , RL loss ( Qin et al . , 2018 ) , and auxiliary   contrastive learning ( CIL ) . We show that PARE   is competitive with DISTRE , RESIDE , CIL , and   other natural baselines , without using additional   pre - training , side information or auxiliary losses   during training , unlike some comparison models .   To evaluate DS - RE , at test time , the model   makes a prediction for an unseen bag . Unfortu-   nately , most popular DS - RE dataset ( NYT-10d )   has a noisy test set , as it is automatically annotated   ( Riedel et al . , 2010 ) . Recently Gao et al . ( 2021 )   has released NYT-10 m and Wiki-20 m , which have   manually annotated test sets . We use all three   datasets in our work .   Multilingual DS - RE : A bilingual DS - RE model   named MNRE ( tested on English and Mandarin )   introduced cross - lingual attention in language-   speciﬁc CNN encoders ( Lin et al . , 2017 ) . Re-   cently , Bhartiya et al . ( 2022 ) has released a dataset , DiS - ReX , for four languages – English , Spanish ,   German and French . We compare mPARE against   the state of the art on DiS - ReX , which combines   MNRE architecture with mBERT encoder . See   Appendix E for details on all DS - RE models .   Passage Construction from Bag of Sentences :   At a high level , our proposed model builds a pas-   sage by combining the sentences in a bag that   mentions a given entity pair . This idea of passage   construction is related with the work of Yan et al .   ( 2020 ) , but with important differences , both in task   deﬁnitions and neural models . First , they focus on   predicting the tail entity of a given query ( e;r ; ? ) ,   whereas our goal is relation prediction given an   entity pair . There are several model differences   such as in curating a passage , in use of trainable   query vectors for relations , in passage construc-   tion strategy , etc . Importantly , their architecture   expects a natural language question for each candi-   date relation – not only this requires an additional   per - relation annotation ( that might not be feasible   for datasets having too many relations in the ontol-   ogy ) , but also , it makes their method slower , since   separate forward passes are needed per relation .   3 Passage Attended Relation Extraction   PARE explores the value of cross - sentence atten-   tion during encoding time . It uses a sequence of   three key steps : passage construction , encoding   and summarization , followed by prediction . Figure   1 illustrates these for a three - sentence bag .   Passage Construction constructs a passage   P(e;e)from sentences t2B(e;e ) . The con-   struction process uses a sequential sampling of sen-   tences in the bag without replacement . It terminates   if ( a ) adding any new sentence would exceed the   maximum number of tokens allowed by the en-   coder ( 512 tokens for BERT ) , or ( b ) all sentences   from the bag have been sampled .   Passage Encoding takes the constructed passage   and sends it to an encoder ( BERT or mBERT ) to   generate contextualized embeddings zof every341   tokenwin the passage . For this , it ﬁrst creates   an encoder input . The input starts with the [ CLS ]   token , followed by each passage sentence sepa-   rated by [ SEP ] , and pads all remaining tokens with   [ PAD ] . Moreover , following best - practices in RE   ( Han et al . , 2019 ) , each mention of eandein the   passage are surrounded by special entity marker   tokens < e1>,</e1 > , and < e2>,</e2 > , respectively .   Passage Summarization maintains a ( randomly-   initialized ) query vector rfor every relation r. It   then computes  , the normalized attention of r   on each token w , using dot - product attention . Fi-   nally , it computes a relation - attended summary of   the whole passage z = P  z , where   Lis the input length . We note that this summa-   tion also aggregates embeddings of [ CLS ] , [ SEP ] ,   [ PAD ] , as well as entity marker tokens .   Tuple Classiﬁer passes zthrough an MLP   followed by Sigmoid activation to return the prob-   abilitypof the triple ( e;r;e ) . This MLP is   shared across all relation classes . At inference , a   positive prediction is made if p > threshold ( 0.5 ) .   Loss Function is simply Binary Cross Entropy be-   tween gold and predicted label set for each bag . No   additional loss terms are used .   4 Experiments and AnalysisWe compare PARE andmPARE against the state   of the art models on the respective datasets . Wealso perform ablations and analyses to understand   model behavior and reasons for its performance .   Datasets and Evaluation Metrics : We evaluate   PARE on three English datasets : NYT-10d , NYT-   10 m , Wiki-20 m. mPARE is compared using the   DiS - ReX benchmark . Data statistics are in Table 2 ,   with more details in Appendix C. We use the evalu-   ation metrics prevalent in literature for each dataset .   These include AUC : area under the precision - recall   curve , M - F1 : macro - F1 , -F1 : micro - F1 , and   P@M : average of P@100 , P@200 and P@300 ,   where P@k denotes precision calculated over a   model’skmost conﬁdently predicted triples .   Comparison Models and Hyperparameters :   Since there is substantial body of work on NYT-   10d , we compare against several recent models :   RESIDE , DISTRE , REDSandT and the latest state   of the art , CIL . For NYT-10 m and Wiki-20 m , we   report comparisons against models in the original   paper ( Gao et al . , 2021 ) , and also additionally run   CIL for a stronger comparison . For DiS - ReX , we   compare against mBERT based models . See Ap-   pendix E for more details on the baseline models .   ForPARE andmPARE , we use base - uncased check-   points for BERT and mBERT , respectively . Hyper-   parameters are set based on a simple grid search   over devsets . ( see Appendix A).342   4.1 Comparisons against State of the Art   The results are presented in Table 1 , in which , the   best numbers are highlighted and second best num-   bers are underlined . On NYT-10d ( Table 1(a ) ) ,   PARE has 2.6 pt AUC improvement over CIL , the   current state of the art , while achieving slightly   lower P@M. This is also reﬂected in the P - R curve   ( Figure 2 ) , where in the beginning our P - R curve   is slightly on the lower side of CIL , but overtakes   it for higher threshold values of recall . Our model   beats REDSandT by 11 AUC pts , even though both   use BERT , and latter uses extra side - information   ( e.g. , entity - type , sub - tree parse ) .   On manually annotated testsets ( Table 1(b ) ) ,   PARE achieves up to 2.8 pt AUC and 2.1 pt macro-   F1 gains against CIL . We note that Gao et al .   ( 2021 ) only published numbers on simpler base-   lines ( BERT followed by attention , average and   max aggregators , the details for which can be found   in Appendix E ) , which are substantially outper-   formed by PARE .CIL ’s better performance is likely   attributed to its contrastive learning objective – it   will be interesting to study this in the context of   PARE .   For multilingual DS - RE ( Table 1(c ) ) , mPARE   obtains a 4.9 pt AUC gain against mBERT+MNRE .   P - R curve in Figure 3 shows that it convincingly   outperforms others across the entire domain of re-   call values . We provide language - wise and relation-   wise metrics in Appendix L – the gains are consis-   tent on all languages and nearly all relations .   4.2 Analysis and Ablations   Generalizing to Unseen KB : Recently , Ribeiro   et al . ( 2020 ) has proposed a robustness study in   which entity names in a bag are replaced by other   names ( from the same type ) to test whether the   extractor is indeed reading the text , or is simply   overﬁtting on the regularities of the given KB . We   also implement a similar robustness study ( details   in Appendix K ) , where entity replacement results in   an entity - pair bag that does not exist in the original   KB . We ﬁnd that on this modiﬁed NYT-10 m , all   models suffer a drop in performance , suggesting   that models are not as robust as we intend them   to be . We , however , note that CILsuffers a 28.1 %   drop in AUC performance , but PARE remains more   robust with only a 16.8 % drop . We hypothesize   that this may be because of PARE ’s design choice   of attending on all words for a given relation , which   could reduce its focus on entity names themselves .   Scaling with Size of Entity - Pair Bags : Due to   truncation when the number of tokens in a bag   exceed 512 ( limit for BERT ) , one would assume   thatPARE may not be suited for cases where the   number of tokens in a bag is large . To study this ,   we divide the test set of NYT-10 m into 6 differ-   ent bins based on the number of tokens present   in the untruncated passage ( details on the experi-   ment in Appendix J ) . We present results in Figure   4 . We ﬁnd that PARE shows consistent gains of   around 2 to 3 pt in AUC against CILfor all groups   except the smallest group . This is not surprising ,   since for smallest group , there is likely only one   sentence in a bag , and PARE would not gain from   inter - sentence attention . For large bags , relevant   information is likely already present in truncated   passage , due to redundancy .   Attention Patterns : InPARE , each relation class   has a trainable query vector , which attends on ev-   ery token . The attention scores could give us some   insight about the words the model is focusing on .   We observe that for a candidate relation that is not   a gold label for a particular bag , surprisingly , the   highest attention scores are obtained by [ PAD ] to-   kens . In fact , for such bags , on an average , roughly   90 % of the attention weight goes to [ PAD ] tokens ,   whereas this number is only 0.1 % when the rela-   tion is in the gold set ( see Appendices H and I ) .   We ﬁnd this to be an example of model ingenu-343Modiﬁcation Change in AUC   w/o passage summarization -4.9   w/o [ PAD ] attention -3.1   w/o entity markers -36.9   ity – PARE seems to have creatively learned that   whenever the most appropriate words for a relation   are not present , it could simply attend on [ PAD ]   embeddings , which may lead to similar attended   summaries , which may be easily decoded to a low   probability of tuple validity . In fact , as a further   test , we perform an ablation where we disallow rela-   tion query vectors to attend on [ PAD ] tokens – this   results in an over 3 pt drop in AUC on NYT-10d ,   indicating the importance of padding for prediction   ( see Table 3 ) .   Ablations : We perform further ablations of the   model by removing entity markers and removing   the relation - attention step that computes a summary   ( instead using [ CLS ] token for predicting each re-   lation ) . PARE loses signiﬁcantly in performance in   each ablation obtaining 16.5 and 48.5 AUC , respec-   tively ( as against 53.4 for full model ) on NYT-10d   ( table 3 ) . The critical importance of entity mark-   ers is not surprising , since without them the model   does not know what is the entity - pair it is predict-   ing for . We also notice a very signiﬁcant gain due   to relation attention and passage summarization ,   suggesting that this is an important step for the   model – it allows focus on speciﬁc words relevant   for predicting a relation . We perform the same ex-   periments on the remaining datasets and observe   similar results ( Appendix G ) .   Effect of Sentence Order : We build 20 random   passages per bag ( by varying sentence order and   also which sentences get selected if passage needs   truncation ) . On all four datasets ( Appendix M ) ,   we ﬁnd that the standard deviation to be negligi-   ble . This analysis highlights 1 ) the sentence - order   invariance of PARE ’s performance and 2 ) In practi-   cal settings , the randomly sampled sentences with   token limit of 512 in the passage is good enough to   make accurate bag - level predictions .   5 Conclusion and Future Work   We introduce PARE , a simple baseline for the task   of distantly supervised relation extraction . Ourexperiments demonstrate that this simple baseline   produces very strong results for the task , and out-   performs existing top models by varying margins   across four datasets in monolingual and multilin-   gual settings . Several experiments for studying   model behavior show its consistent performance   that generalizes across settings . We posit that our   framework would serve as a strong backbone for   further research in the ﬁeld of DS - RE .   There are several directions to develop the PARE   architecture further . E.g. , PARE initializes relation   embeddings randomly and also constructs passage   via random sampling . Alternatively , one could   make use of label descriptions and aliases from   Wikidata to initialize label query vectors ; one could   also use a sampling strategy to ﬁlter away noisy   sentences ( e.g. a sentence selector ( Qin et al . , 2018 )   module integrated with PARE ) . In the multilingual   setting , contextualized embeddings of entity men-   tions in a passage may be aligned using constrained   learning techniques ( Mehta et al . , 2018 ; Nandwani   et al . , 2019 ) to learn potentially better token embed-   dings . Constraints can be imposed on the label hier-   archy as well ( E.g. PresidentOf)CitizenOf , etc . )   since label query vectors operate independently of   each other on the passage in PARE . Additionally ,   translation - based approaches at training or infer-   ence ( Nag et al . , 2021 ; Kolluru et al . , 2022 ) could   improve mPARE performance . Recent ideas of   joint entity and relation alignment in multilingual   KBs ( Singh et al . , 2021 ) may be combined along   with mPARE ’s relation extraction capabilities .   Acknowledgements   This work is primarily supported by a grant from   Huwaei . Mausam is supported by grants from   Google and Jai Gupta Chair Professorship . Parag   was supported by the DARPA Explainable Artiﬁ-   cial Intelligence ( XAI ) Program ( # N66001 - 17 - 2-   4032 ) . Mausam and Parag are supported by IBM   SUR awards . Vipul is supported by Prime Minis-   ter ’s Research Fellowship ( PMRF ) . We thank IIT   Delhi HPC facility for compute resources . We   thank Abhyuday Bhartiya for helping in reproduc-   ing results from the DiS - ReX paper , and Keshav   Kolluru for helpful comments on an earlier draft of   the paper . Any opinions , ﬁndings , conclusions or   recommendations expressed here are those of the   authors and do not necessarily reﬂect the views or   ofﬁcial policies , either expressed or implied , of the   funding agencies.344References345346A Experimental Settings   We train and test our model on two NVIDIA GeForce GTX 1080 Ti cards . We use a linear LR scheduler   having weight decay of 1e-5 with AdamW ( Loshchilov and Hutter , 2019 ; Kingma and Ba , 2015 ) as the   optimizer . Our implementation uses PyTorch ( Paszke et al . , 2019 ) , the Transformers library ( Wolf et al . ,   2020 ) and OpenNRE(Han et al . , 2019 ) . We use bert - base - uncased checkpoint for BERT initialization in   the mono - lingual setting . For multi - lingual setting , we use bert - base - multilingual - uncased .   For hyperparameter tuning , we perform grid search over { 1e-5 , 2e-5 } for learning rate and { 16 , 32 , 64 }   for batch size and select the best performing conﬁguration for each dataset .   PARE takes 2 epochs to converge on NYT-10d ( 152 mins / epoch ) , 3 epochs for NYT-10 m ( 138   mins / epoch ) , 2 epochs for Wiki-20 m ( 166 mins / epoch ) and 4 epochs for DiS - ReX ( 220 mins / epoch ) .   The numbers we report for the baselines come from their respective papers . We obtained the code base   of CIL , BERT+Att , BERT+Avg , BERT+One from their respective authors , so that we could run them on   additional datasets . We were able to replicate same numbers as reported in their papers . We trained those   models on other datasets as well by carefully tuning the bag size hyperparameter .   B Sizes of different models   We report the number of additional trainable parameters , in each model , on top of the underlying   BERT / mBERT encoder ( all models except MNRE use the bert - base - uncased checkpoint , whereas MNRE   uses the bert - base - multilingual - uncased checkpoint ) in table 4 . We note that the key reason why PARE   has signiﬁcantly lower number of additional parameters ( on top of the BERT / mBERT encoder ) is because   all the other models use entity pooling ( Soares et al . , 2019 ) for constructing instance representations . The   entity pooling operation requires an additional fully - connected layer which projects the concatenated   encoded representations of head and tail entity in an input instance to a vector of the same size ( for   BERT / mBERT , this results in additional ( 2768)weight and 2768bias parameters ) .   Model # Parameters ( excluding BERT )   Att 2400793   One 2399257   Avg 2399257   CIL 2453052   MNRE 2645029   PARE 46082   C Dataset Details   We evaluate our proposed model on four different datasets : NYT-10d ( Riedel et al . , 2010 ) , NYT-10 m   ( Gao et al . , 2021 ) , Wiki-20 m ( Gao et al . , 2021 ) and DiS - ReX ( Bhartiya et al . , 2022 ) . The statistics for   each of the datasets is present in table 2 .   NYT-10d   NYT-10d is the most popular dataset for monolingual DS - RE , constructed by aligning Freebase entities to   the New York Times Corpus . The train and test splits are both distantly supervised .   NYT-10 m   NYT-10 m is a recently released dataset to train and evaluate models for monolingual DS - RE . The dataset   is built from the same New York Times Corpus and the Freebase KB but with a new relation ontology and   a manually annotated test set . It aims to tackle the existing problems with the NYT-10d dataset by 1)347establishing a public validation set 2 ) establishing consistency among the relation classes present in the   train and test set 3 ) providing a high quality , manually labeled test set .   Wiki-20 m   Wiki-20 m is also a recently released dataset for training DS - RE models and evaluating them on manually   annotated a test set . The test set in this case corresponds to the Wiki80 dataset ( Han et al . , 2019 ) . The   relation ontology of Wiki80 is used to re - structure the Wiki20 DS - RE dataset ( Han et al . , 2020 ) , from   which the training and validation splits are created . It is made sure that their is no overlap between the   instances present in the testing and the training and validation sets .   DiS - ReX   DiS - ReX is a recently released benchmarking dataset for training and evaluating DS - RE models on   instances spanning multiple languages . The entities present in this dataset are linked across the different   languages which means that a bag can contain sentences from more than one languages . We use the   publicly available train , validation and test splits and there is no overlap between the bags present in any   two different dataset splits .   We obtain the ﬁrst three datasets from OpenNRE and DiS - ReX from their ofﬁcial repository .   D Description of Intra - Bag attention   Lett;t;:::;tdenoteninstances sampled from B(e;e ) . In all models using intra - bag attention for   instance - aggregation , each tis independently encoded to form the instance representation , E(t ) , follow-   ing which the relation triple representation Bfor the triple ( e;e;r)is given byB = P  E(t ) .   Hereris any one of the relation classes present in the dataset and  is the normalized attention score   allotted to instance representation E(t)by relation query vector  ! rfor relationr . The model then predicts   whether the relation triple is a valid one by sending each Bthrough a feed - forward neural network .   In some variants,  ! ris replaced with a shared query vector for all relation - classes,  ! q , resulting in a   bag - representation Bcorresponding to ( e;e)as opposed to triple - representation .   E Baselines   The details for each baseline is provided below :   PCNN - Att   Lin et al . ( 2016 ) proposed the intra - bag attention aggregation scheme in 2016 , obtaining the then   state - of - the - art performance on NYT-10d using a piecewise convolutional neural network ( PCNN ( Zeng   et al . , 2015 ) ) .   RESIDE   Vashishth et al . ( 2018 ) proposed RESIDE which uses side - information ( in the form of entity types and   relational aliases ) in addition to sentences present in the dataset . The model uses intra - bag attention   with a shared query vector to combine the representations of each instance in the bag . The sentence   representations are obtained using a Graph Convolutional Network ( GCN ) encoder .   DISTRE   Alt et al . ( 2019 ) propose the use of a pre - trained transformer based language model ( OpenAI GPT Radford   et al . ( 2018 ) ) for the task of DS - RE . The model uses intra - bag attention for the instance aggregation step .   REDSandT   Christou and Tsoumakas ( 2021 ) propose the use of a BERT encoder for DS - RE by using sub - tree parse of   the input sentence along with special entity type markers for the entity mentions in the text . The model   uses intra - bag attention for the instance aggregation step.348CIL   Chen et al . ( 2021 ) propose the use of Masked Language Modeling ( MLM ) and Contrastive Learning ( CL )   losses as auxilliary losses to train a BERT encoder + Intra - bag attention aggregator for the task .   BERT+Att / mBERT+Att   The model uses intra - bag attention aggregator on top of a BERT / mBERT encoder .   BERT+Avg / mBERT+Avg   The model uses “ Average ” aggregator which weighs each instance representation uniformly , hence   denoting bag - representation as the average of instance - representations .   BERT+One / mBERT+One   The model independently performs multi - label classiﬁcation on each instance present in the bag and then   aggregates the classiﬁcation results by performing class - wise max - pooling ( over sentence scores ) . In   essence , the “ One ” aggregator ends up picking one instance for each class ( the one which denotes the   highest conﬁdence for that particular class ) , hence the name .   mBERT+MNRE   The MNRE aggregator was originally introduced by Lin et al . ( 2017 ) and used with a shared mBERT   encoder by Bhartiya et al . ( 2022 ) . The model assigns a query vector for each ( relation;language )   tuple . A bag is divided into sub - bags where each sub - bag contains the instances of the same language . In   essence , a bag has Lsub - bags and each relation class corresponds to Lquery vectors , where Ldenotes   the number of languages present in the dataset . These are then used to construct Ltriple representations   ( using intra - bag attention aggregation on each ( sub - bag , query vector ) pair for a candidate relation ) which   are then scored independently . The ﬁnal conﬁdence score for a triple is the average of Ltriple scores .   F Statistical Signiﬁcance   We compare the predictions of our model on the non - NA triples present in the test set with the predictions   of the second - best model using the McNemar ’s test of statistical signiﬁcance ( McNemar , 1947 ) . In all   cases , we obtained the p - value to be many orders of magnitude smaller than 0.05 , suggesting that the   improvement in results is statistically signiﬁcant in all cases .   G Ablation Study   Modiﬁcation NYT-10d NYT-10 m Wiki-20 m DiS - ReX   w/o passage summarization -4.9 -2.9 -4.2 -0.8   w/o [ PAD ] attention -3.1 -2.3 -1.9 -0.1   w/o entity markers -36.9 -16.5 -29.9 -20.5   We perform ablation studies on various datasets to understand which components are most beneﬁcial   for our proposed model . We provide the results in table 5 .   We observe that upon replacing our passage summarization step with multi - label classiﬁcation using   [ CLS ] token ( present at the start of the passage ) , we observe a signiﬁcant decrease in AUC , indicating that   contextual embedding of [ CLS ] token might not contain enough information for multi - label prediction of   bag.349For NYT-10 , it is interesting to note here that the AUC is still higher than that of REDSandT , a model   which uses BERT+Att as the backbone ( along with other complicated machinery ) . This means that one   can simply obtain an improvement in performance by creating a passage from multiple instances in a bag .   Removing entity markers resulted in the most signiﬁcant drop in performance . However , this is also   expected since without them , our model would have no way to understand which entities to consider while   performing relation extraction .   H Attention on [ PAD ] tokens   In the passage summarization step ( described in section 3 ) , we allow the relation query vector  ! rto   also attend over the encodings of the [ PAD ] tokens present in the passage . We make this architectural   choice in - order to provide some structure to the relation - speciﬁc summaries created by our model . If a   particular relation class ris not a valid relation for entity pair ( e;e ) , then ideally , we would want the   attended - summary of the passage P(e;e)created by the relation vector  ! rto represent some sort of a   null state ( since information speciﬁc to that relation class is not present in the passage ) . Allowing [ PAD ]   tokens to be a part of the attention would provide enough ﬂexibility to the model to represent such a state .   We test our hypothesis by considering 1000 non - NA bags correctly labelled by our trained model in the   test set of NYT-10d . Let R(e;e)denote the set of valid relation - classes for entity pair ( e;e)and letR   denote all of the relation - classes present in the dataset . We ﬁrst calculate the percentage of attention given   to [ PAD ] tokens for a given passage P(e;e)for all relation - classes in R. The results are condensed into   two scores , sum of scores for R(e;e)and sum of scores for RnR(e;e ) . The results are aggregated   for all 1000 bags , and then averaged out by dividing with the total number of positive triples and negative   triples respectively . We obtain that on an average , only 0.07 % of attention weight is given to [ PAD ]   tokens by relation vectors corresponding to R(e;e ) , compared to 88.35 % attention weight given by   relation vectors corresponding to RnR(e;e ) . We obtain similar statistics on other datasets as well .   This suggests that for invalid triples , passage summaries generated by the model resemble the embeddings   of the [ PAD ] token . Furthermore , since we do n’t allow [ PAD ] tokens to be a part of self - attention update   inside BERT , the [ PAD ] embeddings at the output of the BERT encoder are not dependent on the passage ,   allowing for uniformity across all bags .   Finally , we train a model where we do n’t allow the relation query vectors to attend on the [ PAD ] token   embeddings and notice a 3.1pt drop in AUC on NYT-10d ( table 5 ) . We also note that the performance   is still signiﬁcantly higher than models such as REDSandT and DISTRE , suggesting that our instance   aggregation scheme still performs better than the baselines , even when not optimized fully .   I Examples of Attention Weighting during Passage Summarization   To understand how the query vector of a relation attends over passage tokens to correctly predict that   relation , we randomly selected from correctly predicted non - NA triples and selected the token obtaining   the highest attention score ( by the query vector for the correct relation ) . For the selection , we ignore the   stop words , special tokens and the entity mentions . The results are presented in table 6 .   J Performance vs Length of test passages   Our instance aggregation scheme truncates the passage if the number of tokens exceed the maximum   number of tokens allowed by the encoder . In such cases , one would assume that the our model is not   suited for cases where the number of instances present in a bag is very large . To test this hypothesis ,   we divide the non - NA bags , ( e;e ) , present in the NYT-10 m data into 6 bins based on the number of   tokens present in P(e;e)(after tokenized using BERT ) . We then compare the performance with CIL on   examples present in each bin . The results in ﬁgure 4 indicate that a ) our model beats CIL in each bin - size   b ) the performance trend across different bins is the same for both models . This trend is continued even for   passages where the number of tokens present exceed the maximum number of tokens allowed for BERT   ( i.e. 512 ) . This results indicate that 512 tokens provide sufﬁcient information for correct classiﬁcation   of a triple . Moreover , models using intra - bag attention aggregation scheme ﬁx the number of instances   sampled from the bag in practice . For CIL , the best performing conﬁguration uses a bag - size of 3 . This350   analysis therefore indicates that our model does n’t particularly suffer a drop in performance on large bags   when compared with other state - of - the - art models .   K Entity Permutation Test   To understand how robust our trained model would be to changes in the KB , we design the entity   permutation test ( inspired by Ribeiro et al . ( 2020 ) ) . An ideal DS - RE model should be able to correctly   predict the relationship between an entity pair by understanding the semantics of the text mentioning them .   Since DS - RE models under the multi - instance multi - label ( Surdeanu et al . , 2012 ) ( MI - ML ) setting are   evaluated on bag - level , it might be the case that such models are simply memorizing the KB on which   they are being trained on .   To test this hypothesis , we construct a new test set ( in fact , 5 such sets and report average over those 5 )   using NYT-10 m by augmenting its KB . Let B(e;e)denote a non - NA bag already existing in the test   set of the dataset . We augment this bag to correspond to a new entity - pair ( which is not present in the   combined KB of all three splits of this dataset ) . The augmentation can be of two different types : replacing   ewitheor replacing ewithe . We restrict such augmentations to the same type ( i.e the type of e   andeis same fori= 1;2 ) . For each non - NA entity pair in the test set of the dataset , we select one such   augmentation and appropriately modify each instance in B(e;e)to have the new entity mentions . We351note that since each instance in NYT-10 m is manually annotated and since our augmentation ensures   that the type signature is preserved , the transformation is label preserving . For the NA bags , we use the   ones already present in the original split . This entire transformation leaves us with an augmented test   set , having same number of NA and non - NA bags as the original split . The non - NA entity pairs are not   present in the KB on which the model is trained on .   L More Analysis on DiS - ReX   L.1 Relation - wise F1 scores   To show how our model performs on each relation label compared to other competitive baselines , we   present relation - wise F1 scores on DiS - ReX in table 7 .   L.2 Language - wise AUC scores   We compare the performance of our model compared to other baselines on every language in DiS - ReX.   For this , we partition the test data into language - wise test sets i.e. containing instances of only a particular   language . The results are presented in table 8 . We observe that the order of performance across languages   is consistent for all models including ours i.e. German < English < Spanish < French . Further we observe   that our model beats the second best model by an AUC ranging from 3 upto 4 points on all languages .   L.3 Do multilingual bags improve performance ?   To understand whether the currently available aggregation schemes ( including ours ) are able to beneﬁt   from multilingual bags or not , we conduct an experiment where we only perform inference on test - set bags   that contain instances from all four languages . In the multilingual case , the passage constructed during   thePassage Summarization step will contain multiple sentences of different languages . To understand   whether such an input allows improves ( or hampers ) the performance , we devise an experiment where   we perform inference by removing sentences from any one , two or three languages from the set of bags   containing instances of all four languages . There are roughly 1500 bags of such kind . Note that removing   anyklanguages ( k<= 3 ) would result in    different sets and we take average of AUC while reporting   the numbers . The results are presented in ﬁgure 5 .   We observe that in all aggregation schemes , AUC increases with increase in number of languages   of a multilingual bag . mPARE consistently beats the other models in each scenario , indicating that the   encoding of a multilingual passage and attention - based summarization over multilingual tokens does n’t   hamper the performance of a DS - RE model with increasing no . of languages.352Relation mPARE mBERT - MNRE mBERT - Avg   http://dbpedia.org/ontology/birthPlace 77.5 75.3 74.9   http://dbpedia.org/ontology/associatedBand 77.9 70.9 74.7   http://dbpedia.org/ontology/director 88.4 83.2 85.5   http://dbpedia.org/ontology/country 88.4 86 85.2   http://dbpedia.org/ontology/deathPlace 71.0 67.3 65.5   http://dbpedia.org/ontology/nationality 70.4 67.7 68.7   http://dbpedia.org/ontology/location 74.2 70.5 67.5   http://dbpedia.org/ontology/related 78.9 75.5 73.2   http://dbpedia.org/ontology/isPartOf 74.8 68.6 64.7   http://dbpedia.org/ontology/inﬂuencedBy 57.7 58.4 57.4   http://dbpedia.org/ontology/starring 87.5 86.1 83.9   http://dbpedia.org/ontology/headquarter 74.0 70.7 66.7   http://dbpedia.org/ontology/successor 74.2 71.8 71.3   http://dbpedia.org/ontology/bandMember 76.2 74.6 74.3   http://dbpedia.org/ontology/producer 56.7 53.6 48.5   http://dbpedia.org/ontology/recordLabel 90.5 86.9 86.1   http://dbpedia.org/ontology/city 83.2 78.8 77.6   http://dbpedia.org/ontology/inﬂuenced 56.3 61.9 51.5   http://dbpedia.org/ontology/author 81.6 78.2 80.5   http://dbpedia.org/ontology/team 84.8 82.5 78.6   http://dbpedia.org/ontology/formerBandMember 56.4 57.4 56.5   http://dbpedia.org/ontology/state 86.9 83.9 82.4   http://dbpedia.org/ontology/region 84.8 80.4 78.8   http://dbpedia.org/ontology/subsequentWork 74.1 72.4 69.6   http://dbpedia.org/ontology/department 96.4 95.4 95.5   http://dbpedia.org/ontology/locatedInArea 76.4 72.5 72.3   http://dbpedia.org/ontology/artist 80.8 77.2 78.6   http://dbpedia.org/ontology/hometown 78.8 73.6 73.7   http://dbpedia.org/ontology/province 82.1 79.2 78.2   http://dbpedia.org/ontology/riverMouth 77.2 72.4 71.9   http://dbpedia.org/ontology/locationCountry 66.9 62.5 64.2   http://dbpedia.org/ontology/predecessor 67.3 68.1 62   http://dbpedia.org/ontology/previousWork 68.6 69.6 65.5   http://dbpedia.org/ontology/capital 68.6 55.1 58   http://dbpedia.org/ontology/leaderName 78.4 70.4 63.3   http://dbpedia.org/ontology/largestCity 65.7 59.1 48.6   Model English French German Spanish   mPARE 83.2 86.8 81.7 85.3   mBERT - Avg 79.9 83.1 77.7 82.1   mBERT - MNRE 79.6 82.2 75.5 81.6353 M Negligible effect of random ordering   Since we order the sentences randomly into a passage to be encoded by BERT , this may potentially cause   some randomness in the results . However , we hypothesize that the BERT encoder must also be getting   ﬁne - tuned to treat the bag as a set ( and not a sequence ) of sentences when being trained with random   ordering technique . And as a result , it ’s performance must be agnostic to the order of sentences it sees   in a passage during inference . To validate this , we perform 20 inference runs of our trained model with   different seeds i.e. the ordering of sentences is entirely random in each run . We measure mean and   standard deviation for each dataset as listed in table 9 . We observe negligible standard deviation in all   metrics . A minute variation in Macro - F1 or P@M metrics may be attributed to the fact that these are   macro - aggregated metrics and a variation in performance over some data points may also affect these to   some extent.354