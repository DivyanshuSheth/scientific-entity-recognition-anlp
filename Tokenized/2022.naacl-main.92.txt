  Yang Trista Cao , Anna SotnikovaHal Daumé IIIRachel RudingerLinda ZouUniversity of Maryland , College ParkMicrosoft Research   Abstract   NLP models trained on text have been shown   to reproduce human stereotypes , which can   magnify harms to marginalized groups when   systems are deployed at scale . We adapt the   Agency - Belief - Communion ( ABC ) stereotype   model of Koch et al . ( 2016 ) from social psy-   chology as a framework for the systematic   study and discovery of stereotypic group - trait   associations in language models ( LMs ) . We   introduce the sensitivity test ( SeT ) for measur-   ing stereotypical associations from language   models . To evaluate SeT and other measures   using the ABC model , we collect group - trait   judgments from U.S.-based subjects to com-   pare with English LM stereotypes . Finally , we   extend this framework to measure LM stereo-   typing of intersectional identities .   1 Introduction   Stereotypes are abstract and over - generalized pic-   tures in people ’s minds that capture attributes   about groups of people in the complex social   world ( Lippmann , 1965 ) . They influence peo-   ple ’s thoughts and behaviors , and allow people   to make predictions beyond their personal expe-   rience or information given ( Bruner et al . , 1957 ;   Wheeler and Petty , 2001 ) . Stereotypes are also   entwined with the production of prejudice , discrim-   ination , and in - group favoritism ( Stangor , 2014 ;   Jackson , 2011 ) . A long line of research in social   psychology has established models of generic di-   mensions that estimate people ’s stereotypes of so-   cial groups ( Koch et al . , 2016 ; Fiske et al . , 2002 ,   i.a . ) . We build on the Agency Beliefs Commu-   nion ( ABC ) model , which measures stereotypes   toward a social group with respect to 16 traits   in three dimensions : Agency / Socioeconomic Suc-   cess , Conservative – Progressive Beliefs , and Com - Figure 1 : Crowdsourced analysis of the social group   “ man”under the ABC model ( Koch et al . , 2016 ) . Colors :   purple = agency , red = belief , green = communion .   munion ( § 2 ) ; an analysis of the group “ man”across   32 traits ( 16 opposing dyads ) is shown in Figure 1 .   Pre - trained language models ( LMs ) encode cor-   relations between social groups and traits , like   associating the group “ Muslim ” with the trait , or“man”with ( e.g. , Ben-   der et al . , 2021 ; Nozza et al . , 2021 ; Hovy and Yang ,   2021 ) . We conduct a systematic study of social   stereotypes in contextualized English masked LMs ,   grounded in group - trait associations from the ABC   model . To capture the group - trait associations in   the LM , we first assess two previously proposed   word association tests and also propose a new mea-   surement : the sensitivity test ( SeT ) ( § 3 ) .   To evaluate the degree to which two LMs —   BERT ( Devlin et al . , 2019 ) and RoBERTa ( Liu   et al . , 2019)—align with human stereotype judg-   ments , we design a human study for collecting   group - trait judgments ( § 4 ) . We show that our mea-   sure , SeT , best aligns with human judgements on   group - trait associations and find that , in general ,   the association from language models have moder-   ate alignment with human judgements .   Finally , with the best - aligned association mea-   surement , we extend the ABC approach to study1276   LM stereotypes on intersectional groups ( § 5.2 ) .   Due largely to the difficulty of extending current ap-   proaches for measuring stereotypes in LMs to large   numbers of groups , most current approaches only   study isolated groups , despite the fact that people ’s   social identities are multifaceted ( Ghavami and Pe-   plau , 2013 ) . Because our approach is generalizable   to unstudied groups , we take a step towards explor-   ing stereotypes of intersectional identities , finding   some correspondence between model behavior and   the literature on intersectional stereotypes .   2 Background and Related Work   People ’s impressions of the world and the actions   they take are guided by their stereotypes . To   systematize this observation , the field of social   psychology has proposed models of stereotypes ,   including traits that can coordinate social behaviors   to serve as fundamental dimensions of stereotyping .   Some models are designed to focus on social   evaluation towards individual persons ( Abele   and Wojciszke , 2014 ) , ingroup members ( Elle-   mers , 2017 ; Yzerbyt , 2018 ) , or a small set of   outgroups ( Fiske et al . , 2002 ) ; the Agency Beliefs   Communion ( ABC ) model — whose traits are de-   signed to distinguish groups — is suited for a larger   set of U.S. social groups ( Abele et al . , 2020 ) . The   ABC model takes a data - driven strategy to select   a set of traits by eliminating those that are less   effective in capturing stereotypes . The list contains   16pairs , where each pair represents two polarities   ( see Table 1 ) , categorized into three dimensions :   agency / socioeconomic success , conservative-   progressive beliefs , and communion / warmth .   Ours is far from the first work to assess stereo-   types in language models , and has both advan-   tages and disadvantages compared to previous ap-   proaches ( see Table 2 ) . Past work has generally   taken one of two approaches . The first approach   tests systems with hand - constructed templates like   “ The is □ ” , where ranges over   social groups ( e.g. , “ woman ” or“Hispanic ” ) , and   □ represents a “ masked word ” and ranges over oc-   cupations ( “ a professor ” or“a nurse ” ) ( e.g. , Boluk- ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓   basi et al . , 2016 ; May et al . , 2019 ) or associa-   tions drawn from implicit association tests ( IAT )   ( e.g. , pleasant / unpleasant words or career / family-   related words ) ( e.g. , Caliskan et al . , 2017 ; Guo   and Caliskan , 2021 ) . In Table 2 we refer to these   as “ unnatural ” prompts . The second approach col-   lects more natural sentences containing stereotypes ,   either by web crawling with crowdworkers anno-   tations for social bias ( Sap et al . , 2019 ) or by hav-   ing crowdworkers directly write stereotyping sen-   tences ( Nangia et al . , 2020 ; Nadeem et al . , 2020 ) .   In our work , we take the first approach with traits   from the ABC model , using prompts . The advan-   tage of this approach is that the templates and the   traits are completely controlled and are easy to ex-   tend to other social groups . The second approach   is harder to control , which also leads to significant   annotation challenges ( Blodgett et al . , 2021 ) . Us-   ing natural sentences limits generalizability , as it   requires a unique collection of prompts ( and em-   bedded traits ) for each social group ; in contrast , the   prompt - based approach easily generalizes to any   plausible group , especially when based on a theo-   retically grounded framework like ABC or IAT .   An advantage of our work is that the ABC traits1277   are more exhaustive in stereotype coverage with   verification from social psychological experiments .   The ABC model covers three dimensions with 16   traits , which are consensual , spontaneous , and have   been tested using expansive range of social groups   ( Koch et al . , 2021 ) . They used a carefully designed   data - driven approach to gather people ’s fundamen-   tal dimensions of social perceptions with as little   sampling bias as possible . Thus the resulted 16   traits cover most stereotypes .   Nevertheless , the main trade - off of our approach   is that the testing data are not as natural and specific   as other approaches . Although we carefully pick   and adjust the templates and the form of the social   group terms so that the testing sentences are gram-   matically correct , they are likely not representative   of sentences seen in the real world or in the training   data of the language models . Further , while our ap-   proach has the benefit of near - exhaustive coverage   of potential stereotypes , this comes at a cost : the   traits we consider are much more high level ( e.g. ,   “ repellent ” ) than more fine - grained stereotypes col-   lected by other means ( e.g. , the angry Black woman   stereotype ( Collins , 2002))—this approach there-   fore trades coverage for specificity .   3 Measuring Stereotypes in LMs   Our goal is to measure stereotypes in ( masked )   LMs , and compare them to stereotypes elicited   from people . In § 4 we describe our approach for   eliciting human judgments of group - trait affinities;here we describe how we measure these in LMs .   Previous work has proposed various ways to mea-   sure word associations in LMs , including increased   log probability score ( ILPS ) and contextualized em-   bedding association test ( CEAT ) , both of which we   summarize below . Finally , we present a new mea-   surement which we call the Sensitivity Test ( SeT ) ,   which adapts concepts from active learning to the   task of measuring a LM ’s associations .   3.1 Measurements of Word Associations   Increased Log Probability Score ( ILPS ) quanti-   fies word associations in language models through   masked word probabilities . It calculates the associ-   ation score with a pre - defined template , “   are □ . ” ( Kurita et al . , 2019 ) , where □ is a masked   token . For example , given a group “ Asian ” and a   trait , P(“Asian ” , ) measures the prob-   ability of given “ Asians ” by filling in the   template . Since this probability is affected by the   prior probability of , ILPS normalizes this   probability by the “ prior ” probability of the trait   given a masked group , as below :   ILPS ( , ) = logP( □ =|are □ . )   P( □ =| □ are □ . )   Intuitively , ILPS measures how much each group   raises the likelihood of a trait filling in the tem-   plate . One can easily show that this equivalent to   theweight of evidence of the trait in favor of the   hypothesis that the group is the target : s ( , ) =   woe(:|template ) ( Wod , 1985 ) .   Contextualized Embedding Association Test   ( CEAT ) estimates word associations with word   embedding distances ( Guo and Caliskan , 2021 ) .   Intuitively , CEAT measures whether some groups   are closer to certain traits in a latent vector space .   Given two sets of target words defining groups   X , Y ( e.g. X = { “ man”,“father ” , ... } ,   Y = { “ woman ” , “ mother ” , ... } ) and two sets   of polar traits A , B ( e.g. A = { , ,   ... } , B = { , , ... } ) , CEAT com-   putes the effect sizes of the difference between X   andYbeing closer to AthanBand corresponding   p - values . Since contextualized word representa-   tions are affected by the contexts around the word ,   for each word in the four word sets , CEAT ran-   domly samples 1000 sentences from Reddit , in   which the word appears , and uses these to approxi-1278   mate the true effect size as below :   CEAT ( A , B , X , Y ) =   s ( , A , B ) = ˆEcos(⃗,⃗)−ˆEcos(⃗,⃗ )   ˆE(resp . ˆS ) is the empirical expectation ( resp . stan-   dard deviation ) , and ⃗denotes the embedding of .   In our setting , since we care about social bias   among multiple groups rather than the difference   between two groups , we modify the CEAT to cal-   culate the effect size of the distance difference be-   tweenwithAandBfor each group as below :   CEAT ( , A , B ) = ˆEcos(⃗,⃗)−ˆEcos(⃗,⃗ )   ˆScos(⃗,⃗ )   Sensitivity Test ( SeT ) is a new approach we pro-   pose to measure word association for social bias   in language models , inspired by ideas from active   learning ( Beygelzimer et al . , 2008 ) . The intuition   of SeT is that even though a model assigns the same   probability to two different words , the robustness   of those two probabilities may be different . For ex-   ample , both p ( |“Blind people are □ . ” )   andp(|“Men are □ ” ) might be low . However ,   the language model may well not have seen many   examples with blind people , as opposed to the pre-   sumably very large number of examples of men . In   this case , a small number of examples may be suf-   ficient to alter the model ’s predictions about blind   people , while a larger number would be required   for men . SeT captures the model ’s confidence in   a prediction by measuring how much the model   weights would have to change in order to change   that prediction . Specifically , SeT computes the min-   imal change to the last - layer of the language modelso that a given trait becomes the highest probability   trait ( over the full vocabulary ) .   For example , consider the template “ The is □ . ” with the group “ woman ” and   the trait . Let ℓℓℓbe the logits at □   when the input is “ The woman is □ . ” , and let   be the index of inℓℓℓ(so that ℓ=   p ( |context ) ) . Let hbe the last hid-   den layer before the logits , and let Abe the matrix   of the last linear layer so that ℓℓℓ=Ah . SeT com-   putes the minimal distance between Aand some   other matrix Aso thatis the top word among the   new logits ℓℓℓ=Ah . Formally :   SeT ( , ) = log∆(A , h , )   ∆(A , h , )   where his the penultimate layer on input   Ais the matrix before the logits   ∆(A , h , ) = min∥A−A∥   s.t.(Ah)≥(Ah)+γ,∀̸=   for a fixed margin γ > 0 , which we set to 1 . SeT   returns the negative distance as measure of the asso-   ciation between the corresponding group and trait ,   normalized by a prior akin to ILPS . This optimiza-   tion problem does not ( to our knowledge ) admit   a closed form solution ; we solve it iteratively us-   ing the column squishing algorithm ( Bittorf et al . ,   2012 ; Daumé and Kumar , 2017 ) .   3.2 Implementation details   We test the above measurements on both BERT and   RoBERTa pretrained large models from an open-   source HuggingFacelibrary.1279Social groups . Table 3 lists all the individual so-   cial groups we cover in this work . We manually   construct the list by combining and picking groups   from the list of social groups from Sotnikova et al .   ( 2021 ) and Koch et al . ( 2016 ) and also adding so-   cial groups we think are stereotyped in U.S. culture .   Traits . We use the 32 adjectives of the 16 traits   from the ABC model ( Table 1 ) . For each traits , we   calculate the score of its left - side adjective from   its right - side adjective : S ( ) =   S ( , ) −S ( , ) , where Sis one   of the scores from § 3.1 .   Templates . ILPS and SeT both require templates   in calculating scores . We thus carefully construct   a list of templates ( Table 4 ) that covers multiple   grammatical and semantic variations , inspired by   work investigating harmful search automatic sug-   gestions ( Hazen et al . , 2020 ) . We find that differ-   ent model structure requires different templates in   order to bring up stereotypes that correlate with   human data . See § 5 for evidence .   Subwords . Due to the nature of BERT and   RoBERTa ’s tokenizers , some of the adjectives are   divided into multiple subwords . This is problem-   atic because all the measurements compute their   scores at token level . Neither ILPS nor CEAT deals   with subwords directly : in their released imple-   mentations , they either take the first or the last   sub - token of the word . To remedy this , we adjust   the ILPS measurement ( denoted as ILPS ) to prop-   erly compute the probability of traits in context   using the chain rule across subwords . For SeT , we   calculate the sensitivity score for each subword   individually and take the maximum SeT score as   the SeT score for the word , which effectively com-   putes a lower - bound on how much the model pa-   rameters would need to change . We did not modify   CEAT ’s measurement as it is not clear what is the   best way to compute comparable word embeddings   for words that consist of multiple subwords .   4 Human Study   In the previous section , we describe how we com-   pute associations between groups and traits in lan - guage models . In this section , we assess stereo-   types of social groups through groups - trait asso-   ciation , like in Figure 1 . We adopt this approach   because it is widely used to evaluate group stereo-   types in social psychology field ( Fiske et al . , 2002 ;   Koch et al . , 2016 ) . It also aligns with Lippmann   ( 1965 ) ’s theory of stereotypes that they are abstract   pictures in people ’s head . We broadly follow pro-   cedures from previous social psychology papers to   collect human evaluation on social groups .   Survey Design . We recruit participants from Pro-   lific . Each participant is paid $ 2.00to rate 5so-   cial groups on 16pairs of traits and on average   participants spend about 10minutes on the sur-   vey . This results in a pay of $ 12.00per hour .   Maryland ’s current minimum wage is $ 12.20 .   First , participants read the consent form , and   if they agree to participate in the study , they   see the survey ’s instructions . For each social   group , participants read " As viewed by Ameri-   can society , ( while my own opinions may dif-   fer ) , how   versus   are ? " They then rate each trait with a 0-   100slider scale where two sides are the two dimen-   sions of the trait ( e.g. and ) .   Each annotated group is shown on a separate page ,   and participants can not go back to previous pages .   To avoid social - desirability bias , we explicitly write   in the instruction that “ we are not interested in your   personal beliefs , but rather how you think people   in America view these groups . ”   Participant Demographics . At the end of the   survey we collect participants ’ demographic in-   formation , including gender , race , age , education   level , type of living area , etc . Our participants rep-   resent 26states , with 63.3%from California , New   York , Texas , or Florida ; the gender breakdown is   48.2%male , 49.6%female , and 2.2%genderqueer ,   agender , or questioning ; and skew young , with over   96 % at most 40years old ; and with racial demo-   graphics that approximately match the U.S. census .   For more details on demographics , see Appendix E.   Quality Assurance . Ensuring annotation qual-   ity in a highly subjective task is a challenge , and   common approaches in NLP like having questions   where we “ know ” the answer as tests , measuring1280interannotator agreement , and calibrating reviewers   against each other ( Paun et al . , 2018 ) do not make   sense here . Yet , it is still important to ensure the an-   notation quality . After much iteration , we include   three test questions , and warn the participants at   the beginning that there are test questions .   1.After the first group , participants must name the   group they just scored .   2.After the second , participants must list one trait   they just marked high and one marked low .   3.The fifth ( final ) group is a repetition of one of   the four groups they previously scored .   We discard annotations with incorrect answers to ei-   ther of the first two questions . For the third test , we   compute intra - annotator ( self ) agreement and dis-   card annotations with accuracy - to - self lower than   80 % . For each group we collect 20annotations that   pass our quality threshold . In total , we collected   annotations from 247participants , with 133pass-   ing the quality tests ( suggesting that having such   tests is important ) . The 114annotations that did   not pass tests were excluded from our dataset , but   all247participants were paid .   Social groups and traits . The social groups we   used for the human study are highlighted in Table 3 .   This table contains only single groups used for the   model § 3 and human experiments . We collect   annotations for 25social groups within 5domains ,   across all 16pairs of traits .   5 Results   In this section we present results on correlations be-   tween human and model stereotypes for individual   groups , comparing across different measurements ,   including our proposed measurement , SeT ( § 5.1 ) .   Next , we analyze how model scores change for in-   tersectional social groups . We consider several pos-   sible factors that may influence the score changes   such as identity order , some domain domination ,   and consider emergent traits ( § 5.2 ) .   5.1 Correlation on Individual Groups   Before we answer the question of how language   model stereotype scores align with human stereo-   types across the measurements introduced in § 3 ,   we first run a pilot experiment to select the best   template(s ) for each measurement - model pair from   the set of templates in Table 4 ( except for CEAT ,   which does not require templates ) . We randomly   picked four social groups ( Asian , Black , Hispanic ,   immigrant ) and five annotations from each groupfor the pilot . Since our goal is to inspect the   alignment between human and model stereotypes ,   we take the averaged score of the five annotations   as “ ground truth ” and select templates that give the   correlation score according to Kendall τ . We limit   the selection to at most two templates to avoid   overfitting on the pilot data , selected to maximize   correlation for each measurement - model pair .   The selected templates and corresponding cor-   relation scores are shown in appendix ( Table 5 ) ;   the score range for weak correlation is 0.10 - 0.19 ,   moderate 0.20 - 0.29 , and strong 0.30and above   ( Botsch , 2011 ) . For a fixed LM , the best templates   tend to be similar across all measures : RoBERTa   tends to achieve highest correlation with templates   like “ That is . ” while for BERT   the preferred templates tend to be “ All are . ” or “ should be . ”   Given the best templates for each measurement-   model pair , we measure to what degree language   model stereotypes are aligned with human stereo-   types with all annotations on 25 social groups . To   quantify alignment , we both calculate the Kendall   rank correlation coefficient ( Kendall ’s τ ) and the   Precision at 3 ( P@3 ) . The former indicates the   correlation between model and human scores on   group - trait associations in terms of the number of   swaps required to get the same order . The latter   indicates the percentage of the model ’s top stereo-   types which accord with human ’s judgements . For   P@3 , we also calculate at both the group level and   overall with all groups . For each group , we com-   pute its P@3 score by taking the average of the   P@3 scores with the top 3 traits ( top at one polar-   ity ) and the score with the bottom 3 ( top at the other   polarity ) because each trait has two polar adjectives   and the group - trait score is calculated with the dif-   ference of the two polarities . To calculate the P@3   scores , we binarize the human group - trait scores   at a threshold of 50 . The overall P@3 score is the   average of the groups ’ individual P@3 scores .   The overall scores are in Table 6 . We see that   in general that RoBERTa contains group - trait as-   sociations that are more similar to human judge-   ments than does BERT . Additionally , we see that   both ILPSand SeT have higher P@3 scores   than CEAT and ILPS . The RoBERTa model with   the SeT measurement approach yields outputs are   the most aligned with human ’s judgements , with   RoBERTa / ILPSa close second . From its scores ,   we see that model ’s group - trait associations have1281   moderate correlation with human ’s judgements .   Moreover , in general , two out of the three top   ranked group - trait associations from the model   agree with human data . See Table A19 for the over-   all scores of test groups only , where the four pilot   groups are excluded , and Appendix B for group   level alignment scores .   5.2 Intersectional Groups in LMs   Background . Intersectionality is a core concept   in Black feminism , introduced in the Combahee   River Collective Statement in 1977 ( 1977 ; 1983 ) ,   considering the ways in which feminist theory and   antiracism need to combine : “ Because the intersec-   tional experience is greater than the sum of racism   and sexism , any analysis that does not take intersec-   tionality into account can not sufficiently address   the particular manner in which Black women are   subordinated . ” The concept was applied in law by   Crenshaw ( 1989 ) to analyze the ways in which U.S.   antidiscrimination law fails Black women .   The concept of intersectionality has broadened   and , while its boundaries remain contested ( e.g. ,   Browne and Misra , 2003 ) , there are a number of   core principles that are central ( Steinbugler et al . ,   2006 ; Zinn and Dill , 1996 ): ( 1 ) social categories   and hierarchies are historically contingent , ( 2 ) the   experience at an intersection is more than the sum   of its parts ( Collins , 2002 ; King , 1988 ) , ( 3 ) in-   tersections create both oppression and opportu-   nity ( Bonilla - Silva , 1997 ) , ( 4 ) individuals may ex-   perience both advantage and disadvantage as a re-   sult of intersectionality , and ( 5 ) these hierarchies   impact social structure and social interaction . Goals and Research Questions . We aim to un-   derstand whether we can measure evidence of inter-   sectional behavior in language models with respect   to stereotyping . In particular , we are interested in   questions surrounding how language models stereo-   type people who simultaneously belong to multiple   social groups . We will only use the term “ inter-   sectionality ” when specifically considering cases   where ( per ( 3 ) above ) the resulting experience ( in   this case , stereotyping ) is more than the sum of   its parts . For example , common U.S. stereotypes   for Black women are as “ welfare queens ” ( which   may show up as low agency in our traits ) , while   common stereotypes for Black men is as “ criminal ”   ( which may show up as low communion ) ( hooks ,   1992 ; Collins , 2002 ) . To limit our scope , we will   only consider pairs of social groups ( e.g. , cis men ) ,   and will refer to the the groups that make up a pair   as the component identities ( e.g. , cis , or men ) . We   aim to answer the following research questions :   1.When presented with a paired identity , is the   language model sensitive to the order in which   the component identities appear ?   2.When paired , do certain social categories domi-   nate others in a language model ’s predictions ?   3.Can the language model detect stereotypes that   belong to an intersectional group ( but not to   either of the components that make up the pair ) ?   To answer these questions , we use the SeT measure-   ment with the RoBERTa model ( the best perform-   ing pair on the single - group experiments ) to com-   pute group - trait associations on our paired groups ,   which are combinations of all the single groups   in Table 3 . We manually omit the groups that do1282not logically exist ( e.g. “ cis non - binary person ” ,   “ teenage elderly person ” ) or are grammatically awk-   ward ( e.g. “ doctor elderly person ” , “ immigrant   blind person ” ) . Note we include both orders of the   single groups in the paired groups when possible   ( e.g. “ Catholic teenager ” and “ teenage Catholic   person ” ) . We then conduct the analysis by com-   puting the correlation between groups ’ list of trait   scores with Kendall ’s τ .   Q1 : Identity Order . Given an paired group with   two identities , the language model may not be able   to capture both of the identities and may predict   stereotypes based only on one of the components .   In fact , the average correlation score between a   paired group and the most correlated of its compo-   nents is 0.56 , which is moderately high . We thus   calculate the correlation of trait scores between the   paired group and both its first and second com-   ponent identities ( when both orders are possible ) .   In addition , we calculate the correlation of paired   groups with reversed identity order ( e.g. “ Asian   teenager ” and“teenage Asian person ” ) . The aver-   age correlation score between a paired group and   its first component is 0.43 ; the correlation score   to its second component is 0.46 , which are quite   close . Further , the average correlation score of in-   tersectional groups with reversed identity is 0.69 ,   which is moderately high . Taken together , these   results indicate that ( a ) many paired groups have   similar group - trait association scores with one of   their component identities alone ; ( b ) the order does   not matter significantly , but the language model   tends to focus slightly more on the second compo-   nent . The implication of this is that we can expect   that the language model may be able to capture   intersectional stereotypes .   Q2 : Dominant Domains . Stryker ( 1980 ) sug-   gests that people tend to identify themselves with   their race / ethnicity identity before other identities ,   though this is contested and , in some cases , thought   to be antithetical to the idea of intersectionality   ( e.g. , Collins , 2002 ) . Prompted by this debate , we   ask if there is a hierarchy of the domains that lan-   guage model picks up on for paired groups . To   answer this question , for each identity domain pair ,   we compute the average correlation score between   the paired groups with each of its two component   identities , and take the difference of the averaged   correlation scores of the two domains . For each   domain , we count the domains it dominates ( i.e.has score difference ≥0.1 ) and is dominated by .   These results show that age and political stance   are dominant domains , which is expected as iden-   tities within these two domains have strong char-   acteristics that may overwhelm domains they are   paired with . On the other end , race and nationality   are , generally , dominated domains . It is surprising   that the race domain is majorly dominated , con-   trasting documented literature in human behavior .   The full results are shown in Appendix Table A8   as well as detailed scores Table A9 .   Q3 : Emergent Intersectional Stereotypes . Fi-   nally , we look into emergent stereotypes of paired   groups , with the goal of finding intersectional be-   havior in the language model . To detect intersec-   tional stereotypes , we need to operationalize the   notion of the whole being greater than its parts .   For a fixed paired group= ( , ) ( e.g. , “ trans   Democrats ” ) , and a given trait(e.g . , ) , we   compute S(,)−max{S ( , ) , S ( , ) } , where   Sis the score from the language model , capturing   whether this trait is more associated with the paired   group than the maximum of its association with   the component identities . ( We consider also the   reverse , where we look for scores much less than   the min . ) We might hope to find some well attested   intersectional identities from the literature , such   as “ Black women ” ( low com-   munion ) and “ White men ” are ( high   agency ) ( Ghavami and Peplau , 2013 ) .   The top 50emergent group - trait associations ac-   cording to our measure are listed in Table A10 .   We also see some good examples are : the lan-   guage model scores “ Hispanic unemployed peo-   ple ” as more than people of the com-   ponent identities , “ Democrat teenagers ” as more , “ male doctors ” as more ,   etc . However , there are also some unexpected pat-   terns ; for instance , almost all nationality identities   combined with “ mechanic ” are and , and almost all nationality identities com-   bined with “ autistic ” are . Looking into   the scores themselves , we find that both “ mechanic ”   and “ autistic ” have low scores on the correspond-   ing traits , and combining them with nationalities   raises to about average levels .   Aside from analyzing face validity — which is   mixed — we compare the results of our model to the   traits that Ghavami and Peplau ( 2013 ) found when   conducting human studies of race / gender pairs . To   do this , we categorize the traits from Ghavami and1283Peplau ( 2013 ) to the ABC dimensionsand com-   pare with our full list of emergent group - trait associ-   ations . Taking their group - trait matches as ground   truth , our detection of traits for these race / gender   intersectional groups achieves a precision 0.83and   recall 0.65 — better than random guessing ( preci-   sion0.72 , recall 0.50 ) but far from perfect .   6Limitations and Ethical Considerations   There are several limitations to our work , which   should be taken into account in the interpretation   of our results .   First , our results are likely affected by reporting   bias and by a defaulting effect where , when people   annotate traits for “ men ” , they may actually have   in their head “ cis straight white men ” , because the   defaults go unremarked . This goes both for the   human scores ( how does a participant conceptu-   alize “ men ” ? ) and language model scores ( what   do sentences containing the word “ man”assume   given that most language a langauge model has   been trained on likely exhibits defaulting ? ) .   Second , our work only focus on assessing stereo-   types within language models and not in any de-   ployed system . Though stereotypes from language   models may impact the outputs of downstream sys-   tems which are built upon these language models ,   it is not clear how exactly the stereotypes trans-   fer ( Cao et al . , 2022 ) . Additionally , our work is   limited to English and U.S. social stereotypes .   Third , although we followed and built on best   practices from social psychology in developing the   human study , it nevertheless has some shortcom-   ings . In particular , even after many iterations on   wording , it was difficult to phrase the survey ques-   tions to encourage people to reporting their true   impressions . There is tension between asking a par-   ticipant what theythink — which risks a counfound-   ing potential social desirability bias ( Latkin et al . ,   2017 ) ( people ’s tendency to respond in socially ac-   ceptable ways)—and asking what they think others   think — which led to comments from a few partici-   pants that they felt unqualified to speak for others .   Asking these questions of participants and collect-   ing the data also raises the possibility of this work   inadvertantly reinforcing stereotypes .   Finally , aggregating human judgements into a   single number by averaging ( or any other statistic)to compare to model predictions risks collapsing a   significant amount of information down to a single   number . This number can not distinguish between a   weakly held but common stereotype and a strongly   held but rare one . Nor can it distinguish between   traits where half of annotators say 0 and the other   half say 100 , from traits where all annotators say   50 . These average judgments should be interpreted   as not what any single person would say , but an   average over people . This limitation is exacerbated   by the defaulting effect , where some people may   imagine a different prototype for a given group ,   and other people may imagine another .   7 Conclusion   In this paper , we measured language model ( LM )   stereotypes by adopting the ABC stereotype model   from social psychology . Comparing to previous   work on detecting LM stereotypes , our approach is   easy to extend to previously unconsidered groups ,   grounded in traits proven effective by social psy-   chology , and exhaustively covering the space of   possible stereotypes , at the cost of being more ab-   stract than in other NLP work . This yields a dif-   ferent set of trade - offs than previous approaches to   measuring stereotypes in LMs .   With the ABC model and data regarding human   stereotypes from our human study , we assessed LM   stereotypes using three different association mea-   surements , including SeT , a metric we proposed .   We showed that LM group - trait stereotypes in gen-   eral have moderate correlation with human judge-   ments , and that SeT provides correlations that bet-   ter align with human ’s . Based on these results , we   extended our analysis to intersectional groups . We   found that the LM may be able to capture inter-   sectional stereotypes but is not particularly good   on identifying emergent intersectional stereotypes .   Our results also show that that , in general , age and   political stance are dominant domains in language   models , whereas race and nationality are domi-   nated domains . We hope that our work provides   insights for future works on measuring and miti-   gating stereotypes in natural language processing   systems , and that the grounding in theories from so-   cial psychology has benefits beyond just studying   stereotypes .   Acknowledgments   This material is based upon work partially sup-   ported by the National Science Foundation under1284Grant No . 2131508 . The authors are also grateful   to all the reviewers who have provided helpful sug-   gestions to improve this work , and thank members   of the CLIP lab at the University of Maryland for   the support on this project . We are grateful to all   those who participated in our human study , without   whom this research would not have been possible .   References12851286A Traits   The full list of traits and respective adjectives is in the Table A71287   B Experiment Results with Single Groups   Table A11 presents the Kendall ’s τcorrelation scores between model and human at group level , while   Table A12 and Table A13 shows the alignment with the precision at 3 scores ( former computed with the   top 3 traits and latter with the bottom 3 traits ) .   C Experiment Results of Intersectional Groups   Table A8 presents the dominating relationship between domains , while Table A9 lists the average   correlation scores of the paired group with each of its identities ’ domain for each domain pairs .   Table A10 shows the top 50emergent group - trait associations .   D Human study setup   The survey for the collection of associated traits is presented in Figure A2.12881289E Annotators demographics   55.4%are white , with 50.6%male annotators , 40.4female annotators and no annotators who provided   another gender . 15.1%of annotators are Black , and 25.6%are Hispanic with slightly more female   annotators 56.4 % . We provide four tables A14 , A15 , A16 , A17 showing how perceptions of White people ,   Black people , White men , and White women are different from each other across annotator demographics .   We see variations between in - group and out - group annotations . For instance , women see themselves as   more powerful than men see women . While overall scores for men and women groups are similar across   white and Black annotators . In Table A18 , we show correlation scores for all social groups and overall   score between the model and Black , white , white female , and white male annotators.129012911292129312941295