  Jinyuan Fang , Xiaobin Wang , Zaiqiao Meng , Pengjun Xie , Fei Huang , Yong JiangSchool of Computing Science , University of GlasgowDAMO Academy , Alibaba Group   fangjy6@gmail.com , xuanjie.wxb@alibaba-inc.com ,   zaiqiao.meng@glasgow.ac.uk , yongjiang.jy@alibaba-inc.com   Abstract   This paper focuses on the task of cross do-   main few - shot named entity recognition ( NER ) ,   which aims to adapt the knowledge learned   from source domain to recognize named en-   tities in target domain with only a few labeled   examples . To address this challenging task ,   we propose M , a variational memory-   augmented few - shot NER model . Specifically ,   M uses a memory module to store in-   formation from the source domain and then re-   trieve relevant information from the memory to   augment few - shot tasks in the target domain . In   order to effectively utilize the information from   memory , M uses optimal transport to   retrieve and process information from memory ,   which can explicitly adapt the retrieved infor-   mation from source domain to target domain   and improve the performance in the cross do-   main few - shot setting . We conduct experiments   on both English and Chinese cross domain few-   shot NER datasets , and the experimental results   demonstrate that M can achieve supe-   rior performance .   1 Introduction   Named Entity Recognition ( NER ) is a fundamental   NLP task that aims at classifying mention spans   into entity types . Previous works mainly study   the NER task in a supervised setting ( Chiu and   Nichols , 2016 ; Devlin et al . , 2019 ; Yamada et al . ,   2020 ) . However , supervised learning requires large-   scale annotated datasets , which can be difficult to   obtain in some scenarios ( e.g. , annotating biomedi-   cal named entities always requires domain exper-   tise ( Ogren et al . , 2008 ) ) . In this paper , we focus   on a more practical and challenging setting in real-   world applications , namely cross domain few - shot   NER — given a source domain with sufficient la-   beled data and a target domain with a few labeledFigure 1 : Illustration of the cross domain few - shot NER   task , where the NER model is first pretrained on a set of   tasks ( each task has a support set , e.g. , Sand a query   set , e.g. , Q ) in source domain and then adapted to a   few - shot task in target domain with a few labeled data .   data , the goal is to correctly recognize named enti-   ties in the target domain ( Hou et al . , 2020 ) . This is   achieved by adapting the knowledge learned from   the source domain to the target domain based on   few - shot examples available in the target domain .   Figure 1 provides an illustration of the cross do-   main few - shot NER task .   Recent work demonstrated that learning proto-   type representations for each label class could be   effective to address few - shot tasks ( Snell et al . ,   2017 ) , and this idea has also been applied to few-   shot NER tasks ( Fritzler et al . , 2019 ; Huang et al . ,   2021 ; Ma et al . , 2022 ) . Specifically , when dealing   with a few - shot task in the target domain , these   models learn prototypes for each entity type based   on a few labeled data available in the support set   and then assign labels to tokens in the query set by   measuring their distances to the prototypes . How-   ever , since there are only a few labeled examples   for each entity type in the support set , the proto-   types obtained from the support set only may not be   accurate and representative , leading to the subopti-   mal and unstable performance of prototype - based   few - shot NER models ( Huang et al . , 2020 ) .   To this end , we propose a variational Memory-   Augme Nted cross domain few - shot NER model ,   abbreviated as M . It introduces an external4261memory module that utilizes information from the   source domain to augment the support set in the   target domain , so as to learn more accurate proto-   types . The basic idea of introducing the memory   module is that the entity type information from   the source domain can provide additional back-   ground knowledge for learning prototypes in the   target domain ( Zhen et al . , 2020 ) . For example , in   Figure 1 , the information of the entity type “ Per-   son ” in the source domain can provide guidance   for recognizing the entity type “ Scientist ” in the   target domain . Specifically , M stores token   representations of entity types from the source do-   main in a memory module . For each entity type   in the target domain , M first retrieves the   most similar entity types from the memory and   then leverages the retrieved information to learn   prototype for the entity type .   One critical issue when using the memory mod-   ule is how to utilize the information from the mem-   ory to augment few - shot tasks in the target domain .   Recent research indicates that the performance of   memory - augmented methods which directly use   neural networks to fuse information from the mem-   ory and the task ( He et al . , 2020 ; Zhen et al . , 2020 ) ,   is suboptimal when dealing with cross domain tasks   ( Du et al . , 2022 ) , such as our cross domain few-   shot NER task . This is because the knowledge   ( i.e. , entity type information ) of the source domain   stored in the memory can be inconsistent with that   of the target domain . Therefore , in cross domain   few - shot NER tasks where the entity types of the   source domain and target domain are disjoint , di-   rectly utilizing the information retrieved from the   memory may be suboptimal . Actually , we empiri-   cally found that this could degrade the model per-   formance ( see § 4.2 ) . To address this problem , we   take inspiration from domain adaption and lever-   age optimal transport ( Villani , 2009 ) to retrieve and   process information from the memory . One benefit   of using optimal transport is that we can adapt the   retrieved information from the source domain to the   target domain via the optimal transport plan . This   adaption process helps alleviate the inconsistency   problem between the two domains .   Our contributions can be summarized as follows :   ( 1 ) We propose M , a novel cross domain   few - shot NER model , which uses a memory mod-   ule to utilize the information from the source do-   main to augment few - shot NER tasks in the target   domain . ( 2 ) We leverage optimal transport to re - trieve and process information from the memory ,   which is conducive to improve the performance   ofM in the cross domain setting . ( 3 ) Ex-   perimental results on English and Chinese cross   domain few - shot NER datasets demonstrate that   M can achieve superior performance com-   pared with existing few - shot NER models .   2 Preliminaries   In this section , we formalize the cross domain few-   shot NER task , and provide a brief introduction to   optimal transport , which serves as the foundation   of our model .   Task Formulation . NER is a sequence labeling   task , where each token in the sequence is assigned   a label representing an entity class or “ O ” ( not an   entity ) . In this paper , we focus on a practical setting   of NER , namely corss domain few - shot NER ( Hou   et al . , 2020 ; Yang and Katiyar , 2020 ) , where the   NER model is first pretrained on data - sufficient   source domain(s ) Dand then tranferred to target   domain(s ) Dwith only a few labeled examples .   Formally , we denote a sentence and its labels as   x={x , x , . . . , x}andy={y , y , . . . , y } ,   respectively . Following previous works ( Hou et al . ,   2020 ; Ma et al . , 2022 ) , we adopt the episode learn-   ing paradigm in this paper , where we first pretrain   the model on a set of tasks D={(S , Q)}from   the source domain and then adapt the model to   another set of tasks D={(S , Q)}from the tar-   get domain . Each task consists of a support set   S={(x , y ) } for task adaption , and a   query set Q={(x , y ) } for evaluation ,   where Ndenotes the number of entity types in a   task , KandKdenote the number of few - shot   samples that belong to each entity type in the sup-   port set and the query set , respectively . Given a   task in the target domain , the goal of our model is   to predict the labels of sentences in the query set   after adapting the model to the task with its sup-   port set ( i.e. , finetuning the model with the support   set ) . Figure 1 provides an illustration of the cross   domain few - shot NER task .   Optimal Transport . Cross domain few - shot   NER task can be considered as a domain adap-   tion task . Optimal transport ( OT ) is a widely used   method to solve the domain adaption tasks in the   field of computer vision ( Courty et al . , 2017a , b ;   Damodaran et al . , 2018 ; Fatras et al . , 2021 ) . Specif-   ically , OT is a metric that measures the distance4262   between two probability distributions . In this pa-   per , we focus on the discrete OT for two discrete   empirical distributions , i.e. , νandν :   W(ν , ν ) = min⟨C , T⟩ , ( 1 )   where Σ(ν , ν ) = { T∈R : T1=   ν , T1 = ν}is a set of joint probabilities , 1   and1denote m - dimensional and n - dimensional   vectors of ones respectively , ⟨·,·⟩is the Frobenius   dot product , and C= [ c]∈R is a cost ma-   trix with each element representing the distance   between the i - th data point of νand the j - th one   ofν . The optimal solution of Tis called optimal   transport plan , denoted as T , which can be effi-   ciently obtained through the Sinkhorn algorithm   ( Cuturi , 2013 ) by solving an entropy regularized   version of Equation ( 1 ) ( see Appendix A ) .   3 Methodology   The overall framework of our M is shown   in Figure 2 . In this section , we first introduce the   details of M in § 3.1 , and then introduce the   pretraining of M on the source domain in   § 3.2 . We finally introduce how to adapt the model   to the target domain in § 3.3 .   3.1 The M Model   Following previous prototype - based NER models   ( Fritzler et al . , 2019 ; Wang et al . , 2021c ) , we learn   a prototype for each entity type , which is the mean   of representations of tokens that belong to this type   in the support set . However , compared with vanilla   prototype - based methods which model prototypesas deterministic vectors , we employ a probabilistic   framework by modeling prototypes as stochastic   variables , which is conducive to learn more infor-   mative prototypes and improve the robustness of   few - shot models by capturing the uncertainties of   prototypes ( Allen et al . , 2019 ; Zhen et al . , 2020 ) .   Moreover , following previous two - stage few-   shot NER models ( Wang et al . , 2021b ; Ma et al . ,   2022 ) , we decompose the label prediction of NER   into two sub - tasks : span detection which aims to   predict the position tags of tokens , such as “ B ”   and “ I ” , and entity typing which aims to predict   theentity types of tokens . Accordingly , for each   sentence , we additionally introduce two types of   labels , namely position tags a={a , a , . . . , a }   and entity types e={e , e , . . . , e } . We adopt   the BIOES tagging scheme in this paper . Therefore ,   for a few - shot task τ={S , Q } , the position tags   are chosen from { O , B , I , E , S } , while the entity   types are chosen from the entity set Ein the task ,   such as { Person , Location , ... } . We define the joint   probability distribution of our model as :   p(y , a , e , Z|x , S , M )   = p(y , a , e,|x , Z)p(Z| S , M ) , ( 2 )   p(y , a , e,|x , Z )   = p(y|a , e)p(a|x)p(e|x , Z)(3 )   where Z∈Rdenotes prototypes of all en-   tity types in the task , which are obtained from the   support set Sand a memory module M(detailed   below ) , i.e. , p(Z| S , M ) . These prototypes are4263   then used to predict the entity types of tokens , i.e. ,   p(e|x , Z ) , which is further combined with the   predicted position tags , i.e. , p(a|x ) , to obtain   the distribution over labels , i.e. , p(y|a , e ) . Fig-   ure 3 illustrates the probabilistic graphical model   ofM . In what follows , we will introduce   the details of the joint probability distribution .   Memory - Augmented Prototypes p(Z| S , M ):   Since there are only a few labeled data in the sup-   port set , the prototypes that are obtained from the   support set may not be accurate and representative .   Therefore , we leverage an external memory module   to store entity type information from the source do-   main to augment the support sets of few - shot tasks   in the target domain . Specifically , we denote the   memory as M , which contains key - value pairs that   correspond to different entity types in the source   domain . The keys are different entity types and   the values are representations of tokens that belong   to the corresponding entity types . For efficient re-   trieval from memory , we limit the number of token   representations of each entity type to be m , which   is referred to as the memory size .   In order to adapt the retrieved information ( i.e. ,   token representations ) from the source domain to   the target domain , we leverage optimal transport to   retrieve and process information from the memory .   Specifically , for an entity type kin a few - shot task   τ , we first retrieve its most similar entity types k   in the memory based on the OT distance :   k= arg minW(M , H )   = arg minmin⟨C , T⟩,(4 )   where H = f(S),S={x , . . . , x } ,   is the contextualized representations of tokens thatbelong to entity type kin the support set , fis a   token encoder such as BERT ( Devlin et al . , 2019 ) ,   Mdenotes the token representations of entity   typekstored in the memory , and Cis a cost matrix   with each element computed as : c(M , H ) =   ||M−H|| . We denote the retrieved infor-   mation for entity type kasM , and the optimal   transport plan between token representations H   andMasT , which is obtained through the   Sinkhorn algorithm ( Cuturi , 2013 ) in this paper .   We next follow previous works ( Courty et al . ,   2017a , b ) to adapt the retrieved information from   the source domain , i.e. , M , to the domain of task   τthrough the following barycentric mapping :   ˆh= arg min / summationdisplayT(i , j)·c(h , H),(5 )   for all i= 1 , . . . , m , where ˆhdenotes the pro-   jected representation of the i - th item in M , and   T(i , j)represents an element of the optimal trans-   port plan T. It has been shown that when the   cost function is squared Euclidean norm , the solu-   tion to above barycenter mapping corresponds to   a weighted average of H(Courty et al . , 2017b ) ,   which is given by :   ˆH= diag ( T1)TH , ( 6 )   where diag(·)is a diagonal matrix .   After obtaining the adapted memory , i.e. , ˆH ,   we combine it with token representations in the   support set to get the prototype distributions :   p(Z| S , M ) = /productdisplayp / parenleftig   z| S , M / parenrightig   = /productdisplayN / parenleftig   z|g(ˆH , H ) , σI / parenrightig   , ( 7 )   where we model the distributions over prototypes   as Gaussian distributions , whose mean is obtained   through a mean function gand the covariance is   given by σI. We define the mean function as :   g(ˆH , H ) = γ·Neural ( [ ˆr , r ] )   + ( 1−γ)·r , ( 8)   where ˆr=/summationtextˆHandr=/summationtextHare   the mean of token representations in the memory   and the support set respectively , [ · , · ] is the concate-   nation operation , and Neural ( · ) is a feed - forward   neural network with Relu activation function . We   introduce a hyperparameter γto interpolate the in-   formation from the memory and the support set.4264Span Detection p(a|x):We formulate span   detection as a sequence labeling task , i.e. , predict-   ing the position tags of tokens . Note that we use   an encoder function f , e.g. , BERT , to obtain the   contextualized representations of tokens when com-   puting prototypes . Based on these token represen-   tations , we use a linear classifier to compute the   probability distributions of position tags . Specifi-   cally , for a sentence x , its distribution of position   tags is :   p(a|x ) = Softmax ( f(x)W+b),(9 )   whereW∈Randbare model parameters .   Entity Typing p(e|x , Z):We follow the prin-   ciple of prototypical networks ( Snell et al . , 2017 )   to compute the probability distributions of entity   types . Specifically , for a sentence x , we compute   its distribution of entity types as :   p(e|x , Z ) = Softmax ( f(x)Z ) , ( 10 )   whereZis the sampled prototypes from prototype   distribution defined in Equation ( 7 ) .   Label Prediction p(y|a , e):Finally , we com-   bine the results of span detection and entity typing   to get the label distributions of a sentence x , i.e. ,   p(y|a , e ) = /producttextp(y|a , e ) , where the   predicted label distribution of each token is :   p(y|a , e)∝p(a|x)·p(e|x , Z).(11 )   For example , for a token xwith label “ B - Person ” ,   the probability of the token being classified as “ B-   Person ” is proportional to the product of p(a=   B|x)andp(e= Person |x , Z ) .   3.2 Learning in Source Domain   We next introduce how to learn our model on source   domain . The goal of learning is to maximize the   likelihood of observations in source domain :   p(D|M ) = /integraldisplay   p(D|Z)p(Z| S , M)dZ   whereD={S , Q}={(x , y , a , e)}represents   the set of observed variables in both support set and   query set . The learning of such probabilistic mod-   els requires inferring the posterior distributions of   stochastic variables , i.e. , inferring the distribution   of prototypes after seeing both the support set andthe query set in our case . However , exact inference   is intractable due to the non - Gaussian likelihood   function in our model . Therefore , we resort to vari-   ational inference to approximate the posteriors and   learn the model ( Kingma and Welling , 2014 ) .   Specifically , we approximate posteriors of proto-   types with the following variational distributions :   q(Z| S , Q ) = /productdisplayq / parenleftig   z| S , Q / parenrightig   = /productdisplayN / parenleftig   z|g / parenleftig   f(Q ) , f(S)/parenrightig   , σI / parenrightig   ,   where Qrepresents tokens that belong to entity   typekin the query set , and σIis the covariance .   For parameter efficiency , we use the same inference   network gin Equation ( 7)to infer the posteriors   of prototypes . However , the variational distribu-   tions are different from Equation ( 7 ) , which can   be regarded as prior distributions , in that the vari-   ational distributions are obtained based on both   support and query sets while the prior distributions   are obtained based on support set and the memory .   With the above variational distributions , we can   derive the Evidence Lower BOund ( ELBO ) of log-   likelihood function of our model as :   L =   −D[q(Z| S , Q)||p(Z| S , M ) ]   + /summationdisplayE[logp(y , a , e,|x , Z ) ]   + const . , ( 12 )   where o= ( x , y , a , e)andD[·||·]is the Kull-   back – Leibler ( KL ) divergence . Please refer to   Appendix B for the detailed derivation of ELBO .   We learn the model parameters θby maximizing   the ELBO defined in Equation ( 12 ) , where KL di-   vergence has a closed - form solution while the ex-   pectation term is approximated with Monte Carlo   method by sampling from the variational distribu-   tions . At each training iteration , in order to remain   the accuracy of the memory , we use the token rep-   resentations , which is obtained through f , in both   the support and query sets to update the informa-   tion in the memory . Specifically , for each entity   type in a task , we randomly select mrepresenta-   tions of tokens that belong to this entity type to   update token representations stored in the memory .   We leave the exploration of selecting representative   token representations as our further research.4265   3.3 Adaption in Target Domain   Finally , we introduce how to adapt our model to   the target domain . Similar to previous work ( Ma   et al . , 2022 ) , we finetune the model with few - shot   examples in the target domain . However , since we   do not have access to the query set in the target do-   main , we can not use the ELBO in Equation ( 12 ) to   finetune our model . Therefore , we propose to adapt   our model to the target domain by maximizing the   likelihood function of the support set in the target   domain . Formally , the objective function is :   minE[logp(S|Z ) ] . ( 13 )   After adapting our model to the target domain , we   make prediction for a sentence xin the query set   withp(y , a , e|x,˜Z ) , where ˜Zrepresents the   mean of prototype distributions . The pseudo code   of the training and adaption process of our model   is provided in Appendix C.   4 Experiments   4.1 Experimental Setups   Datasets . We conduct experiments on two groups   of datasets : ( 1 ) Cross - Dataset ( Hou et al . , 2020 ):   It is an English cross domain few - shot NER dataset   constructed from four datasets : Ontonotes ( Prad-   han et al . , 2013 ) , WNUT-2017 ( Derczynski et al . ,2017 ) , GUM ( Zeldes , 2017 ) , CoNLL-2003 ( Sang   and De Meulder , 2002 ) . For fair comparison , we   use the same sampled episodes and dataset splits as   in ( Hou et al . , 2020 ) , where two of the four datasets   are used for training , one for validation and the   other for test . For example , to evaluate the perfor-   mance on Ontonotes , we take WNUT and GUM as   the training sets and CoNLL as the validation set .   ( 2)Chinese Cross - Dataset : We also construct a   Chinese cross domain few - shot NER dataset using   five publicly available datasets : CCKS , Address ,   Medical ( Zhang et al . , 2022 ) , Weibo ( Peng and   Dredze , 2015 ) and Cluener ( Xu et al . , 2020 ) . Fol-   lowing the settings in ( Yang and Katiyar , 2020 ) , we   first train few - shot models on the training set of the   CCKS dataset and then evaluate their performance   on the other four datasets . For each test dataset ,   we sample K - shot data from their training set as   the support set and use the whole test set as the   query set to construct a test episode . We repeat the   sampling process for five times and obtain five test   episodes for each dataset . We compare the average   performance on the five test episodes . More details   about our datasets are provided in Appendix D.1.4266   Baselines . On Cross - Dataset , we take the fol-   lowing models as our baselines : Decomposed-   MetaNER ( Ma et al . , 2022 ) , CONTaiNER ( Das   et al . , 2022 ) , L - TapNet+CDT ( Hou et al . , 2020 )   and those baselines used in ( Hou et al . , 2020 ) ,   such as TransferBERT , SimBERT , Matching Net-   work , and ProtoBERT ( Fritzler et al . , 2019 ) . On   Chinese Cross - Dataset , we compare against some   strong few - shot NER models such as Decomposed-   MetaNER , CONTaiNER , ProtoBERT , NNShot and   StructShot ( Yang and Katiyar , 2020 ) .   Evaluation . We employ the episode evaluation   as in ( Hou et al . , 2020 ) where we calculate micro   F1 score within each test episode and then average   over all test episodes . We repeat each experiment   for5times with different seeds and report average   micro F1 scores with their standard deviations .   Settings . Following previous works ( Hou et al . ,   2020 ; Ma et al . , 2022 ) , we use bert - base - uncased   ( Devlin et al . , 2019 ) to obtain contextualized token   representations for Cross - Dataset . Similarly , bert-   base - chinese is utilized for Chinese Cross - Dataset .   We instantiate the mean function of the inference   network , i.e. , g , with a two - layered feed - forward   neural network with the ReLU activation function   and set the number of hidden units as 128 . More-   over , to effectively optimize the ELBO , we follow   previous works ( Osawa et al . , 2019 ; Zhang et al . ,   2021 ) to introduce an additional hyperparameter λ   to down - weight the KL - divergence in the ELBO .   Throughout the experiments , we set λas1e , and   sample 5times from the variational distributions to   approximate the expectation term in the ELBO .   We set the maximum sequence length of the   BERT models as 128 and the hyperparameter   γas0.5 . To optimize the parameters , we use   AdamW ( Loshchilov and Hutter , 2019 ) with a 1 %   linearly scheduled warmup as the optimizer and   freeze the embedding layers of bert during opti-   mization . Moreover , we perform grid search to   select hyperparameters . Additional details about   hyperparameter settings are in Appendix D.2.4.2 Results and Analysis   Overall Performance . The performance of   M and baselines on Cross - Dataset and Chi-   nese Cross - Dataset are reported in Table 1 and Ta-   ble 2 , respectively . The results show that M   performs better than all the baselines on F1 score   in all settings and surpass the second best models   by a large margin in most cases . Particularly , on   Cross - Dataset , M achieves an average per-   formance improvement of 5.37 % and7.58 % in 1-   shot and 5 - shot settings respectively compared with   the best baselines . Similarly , on Chinese Cross-   Dataset , the average performance improvement of   M is6.65 % ( 1 - shot ) and 7.38 % ( 5 - shot ) .   The experimental results well demonstrate the ef-   fectiveness of M in handling both English   and Chinese few - shot NER tasks . Moreover , com-   pared with DecomposedMetaNER , a strong base-   line , M achieves performance improvement   up to 9.48 % ( Ontonotes 1 - shot ) on Cross - Dataset   and13.09 % ( Address 1 - shot ) on Chinese Cross-   Dataset , which suggests that M can achieve   superior performance even with very few labeled   data ( e.g. , 1 - shot ) .   Ablation Studies . We conduct ablation studies to   investigate the effect of different components , i.e. ,   memory module , optimal transport and probabilis-   tic framework , in our model . We introduce three   variants of M for the ablation study : ( 1 )   M w/o Memory , where the memory mod-   ule is removed and the prototype distributions are   inferred from the support set only . Note that this   variant does not use OT either as it is unnecessary   to adapt the retrieved information from the mem-   ory to the target domain . ( 2 ) M w/o OT ,   where we remove the OT module and use cosine   similarity to retrieve the most similar entity type   from memory . The retrieved information is directly   used to infer prototype distributions without any   processing . ( 3 ) Deterministic , where we remove   probabilistic framework and model prototypes as   deterministic vectors.4267   The results of ablation studies are reported in   Table 3 . It is shown that M consistently   outperforms M w/o Memory in all settings ,   which indicates the effectiveness of our memory   module in improving the performance . This is be-   cause with appropriate processing , e.g. , OT in this   paper , the information stored in memory can pro-   vide background knowledge for quickly and accu-   rately learning new classes from a few examples   and therefore brings performance improvement .   Table 3 also shows that M outperforms   M w/o OT , which demonstrates the effec-   tiveness of OT in M . Moreover , we found   thatM w/o Memory outperforms M   w/o OT which directly use the information from the   memory without any processing . This is because in   our cross domain few - shot setting , the information   from the memory ( source domain ) is different from   that of the test tasks ( target domain ) , i.e. , the entity   types of two domain are disjoint , and therefore di-   rectly utilizing the information from the memory   may introduce noises to the test tasks , leading to the   performance degradation . The results demonstrate   the necessity of leveraging OT to adapt informa-   tion from the memory to current task to achieve   satisfactory performance .   Table 3 shows that M achieves better or   comparative results compared with its determinis-   tic counterpart , especially on the 1 - shot settings .   The improvement can be explained by the fact that   M introduces small noises to the prototypes   by sampling from the prototype distributions to pre-   vent the model from overfitting the few - shot data   during the finetuning stage .   Effect of Decomposed Framework . It is worth   noting that M decomposes label prediction   of NER into two - subtasks : span detection and en-   tity typing . To investigate the effect of the de-   composed framework , we additionally introduce a   variant of M : VM - ProtoNet where we only   remove the decomposed framework and learn pro-   totypes for each entity label , which is similar to   ProtoBERT . Note that we also use memory and OT   to augment few - shot NER tasks in VM - ProtoNet .   We compare the performance of M and VM-   ProtoNet on Cross - Dataset , which is presented in   Figure 4 . The results show that M surpasses   VM - ProtoNet in all settings and achieves notice-   able performance improvement in most cases , es-   pecially on the CoNLL dataset . The success of the   decomposed framework maybe because it avoids   learning prototype for non - entities ( i.e. , “ O ” class )   which is noisy and meaningless . Overall , experi-   mental results demonstrate the effectiveness of the   decomposed framework in few - shot NER tasks ,   which is consistent with the results of previous   works ( Wang et al . , 2021b ; Ma et al . , 2022 ) .   Effect of Memory Size . InM , we limit   the number of token representations of each entity   type stored in the memory . We further conduct ex-   periments to understand the effect of the memory   size on the performance of M . Specifically ,   we vary the memory size from 1to30and report   the performance of M on Ontonotes and   Address . The results in Figure 5 show that M- can achieve decent performance even with a   low memory size and the performance converges   with the increase of memory size . These findings   suggest that M is insensitive to memory   size , which brings another benefit : it is sufficient   forM to achieve satisfactory performance   by storing only a small number of token repre-   sentations in the memory , which is efficient for   both retrieving and processing information from   the memory .   5 Related Work   Few - Shot NER . Recently , few - shot NER has re-   ceived growing interest . Previous works mainly   address few - shot NER with meta - learning meth-   ods ( Fritzler et al . , 2019 ; Wang et al . , 2021c;4268Huang et al . , 2021 ; Tong et al . , 2021 ; Ma et al . ,   2022 ) . These methods build few - shot models ei-   ther upon prototypical network ( Snell et al . , 2017 ) ,   which learns prototypes for entity types ( Fritzler   et al . , 2019 ; Huang et al . , 2021 ; Tong et al . , 2021 ;   Wang et al . , 2022b ; Ji et al . , 2022 ; Wang et al . ,   2022a ) , or MAML ( Finn et al . , 2017 ) , which adapts   the model parameters to few - shot tasks through   inner - update on the support set ( Li et al . , 2022 ;   Ma et al . , 2022 ) . Another line of work adopts   the transfer learning paradigm , where they first   learn a feature extractor on the source domain and   then transfer the pretrained model to the target do-   main ( Hou et al . , 2020 ; Yang and Katiyar , 2020 ;   Das et al . , 2022 ) . These methods make predic-   tions through the nearest neighbor inference ( Wise-   man and Stratos , 2019 ) . In addition , some re-   cent works focus on the two - stage few - shot NER   model ( Ziyadi et al . , 2020 ; Wang et al . , 2021b ; Ma   et al . , 2022 ) , where they decompose the NER task   into two - subtasks : span detection and entity typ-   ing . Moreover , prompt - based techniques ( Cui et al . ,   2021 ; Ding et al . , 2022 ; Chen et al . , 2022 ) have also   been proposed to address few - shot NER tasks . In   contrast , M stores the information from the   source domain in the memory , which is then used   to augment few - shot task in target domain .   Memory . Memory - augmented methods have   been widely studied in the field of computer vi-   sion ( Santoro et al . , 2016 ; Bornschein et al . , 2017 ;   Ramalho and Garnelo , 2019 ; Munkhdalai et al . ,   2019 ; Zhen et al . , 2020 ; Du et al . , 2022 ) . Particu-   larly , Santoro et al . ( 2016 ) propose to augment neu-   ral network with Neural Turing Machine ( Graves   et al . , 2014 ) for few - shot learning , which enables   quickly encoding and retrieving new information .   Ramalho and Garnelo ( 2019 ) further introduce a   memory controller to select the minimum samples   to be stored in the memory . Memory - augmented   methods have also been successfully applied in   NLP tasks , such as question answering ( Das et al . ,   2017 ) , text classification ( Geng et al . , 2020 ) , text   generation ( He et al . , 2020 ) and slot tagging ( Wang   et al . , 2021a ) . Compared with above methods , our   model utilizes optimal transport to adapt the re-   trieved memory to the target domain instead of   using neural networks , which is more effective .   Optimal Transport . In domain adaption , opti-   mal transport is a widely used method to trans-   port data from the source domain to the target do - main ( Courty et al . , 2017a , b ; Damodaran et al . ,   2018 ; Fatras et al . , 2021 ; Nguyen et al . , 2021 ; Fa-   tras et al . , 2022 ) . Theoretical guarantees have been   provided in ( Redko et al . , 2017 ) to justify the use   of OT in domain adaption . In Courty et al . ( 2017b ) ,   they propose to transport features from the source   domain to the target domain through a barycentric   mapping . However , they only consider transport-   ing feature distributions . In contrast , some works   propose to align the joint distributions of features   and labels in source and target domains ( Courty   et al . , 2017a ; Damodaran et al . , 2018 ) .   6 Conclusion   This paper proposes M to handle the cross   domain few - shot NER task . M uses a mem-   ory module to store information from the source do-   main , which is then leveraged to augment few - shot   task in the target domain . To effectively utilize the   information from the memory , M uses opti-   mal transport to retrieve and process information   from the memory , which enables explicitly adapt-   ing the retrieved information to the target domain   and improve the performance in the cross domain   few - shot setting . Experimental results on both En-   glish and Chinese few - shot NER datasets show that   M can achieve superior performance over   existing methods .   Limitations   One limitation of our work is that M only   explicitly utilizes the memory to enhance the per-   formance of the entity typing module in target do-   main . However , we argue that the memory could   alsoimplicitly enhances the span detection module   through the shared pretrained language model with   entity typing module . We leave how to explicitly   leverage memory to enhance both entity typing and   span detection modules as future work .   Acknowledgements   We thank all the reviewers for their valuable feed-   back and constructive suggestions . We would also   like to thank Zeqi Tan for helping constructing the   Chinese cross domain few - shot NER dataset and   Xuming Hu for helpful discussions on improving   the quality of the paper.4269References42704271   A Sinkhorn Algorithm   Sinkhorn algorithm ( Cuturi , 2013 ) is an efficient   method to approximate the optimal transport ( OT )   distance . It aims to solve an entropy regularized   optimal transport problem , which is defined as :   W(ν , ν ) = min⟨C , T⟩+ϵh(T),(14 )   where h(T ) = /summationtextTlogTdenotes the en-   tropy regularizer and ϵis the regularization parame-   ter . The optimization problem in Equation ( 14 ) can   be efficiently solved through the following iterative   Bregman projections ( Benamou et al . , 2015 ):   a = ν   Gb , b = ν   Ga,(15 )   starting from b=1 , where G= [ G]and   G = e. After Literations , the optimal   transport plan Tis calculated as T = aGb .   B Derivation of ELBO   Note that the joint probability distribution of our   model on source domain is given by :   p(D , Z|M ) = p(D|Z)p(Z| S , M ) ,   whereD={S , Q}={(x , y , a , e)}represents   the set of observed variables . We further define a4272Algorithm 1 : Variational Memory - Augmented Few - Shot NER ( M ) .   Input : Tasks from source domain D , few - shot tasks from target domain D , training steps T ,   finetune steps J , training learning rate η , finetune learning rate ξ . Initialize model parameters θand memory M;/ * Part I : Training on source domain . * /fors= 1 , . . . T do Sample a batch of tasks D fromD ; foreach task T= ( S , Q)∈ D do foreach entity type kinTdo Retrieve the most similar entity type kfrom memory based on Equation ( 4 ) ; Adapt the retrieved content Mto current task based on Equation ( 6 ) ; Calculate the prior distributions of prototypes , i.e. , p(Z| S , M)based on Equation ( 7 ) ; Calculate the variational distributions of prototypes , i.e. , p(Z| S , Q ) ; Sample prototypes Zfrom p(Z| S , Q ) ; Calculate the ELBO based on Equation ( 12 ) ; Accumulate gradients of model parameters θwhich are obtained by maximizing the ELBO ; Update memory with token representations in both support and query sets . Update model parameters θwith learning rate η;/ * Part II : Finetuning on target domain . * /foreach task T= ( S , Q)∈ Ddo Initialize model parameters θ = θ ; fors = 1 , . . . , J do foreach entity type kinTdo Retrieve the most similar entity type kfrom memory based on Equation ( 4 ) ; Adapt the retrieved content Mto current task based on Equation ( 6 ) ; Calculate the prior distributions of prototypes , i.e. , p(Z| S , M)based on Equation ( 7 ) ; Sample prototypes Zfrom p(Z| S , M ) ; Calculate the objective function based on Equation ( 13 ) ; Update model parameters θwith learning rate ξ ;   variational distribution q(Z| S , Q)to approxi-   mate the posteriors of latent variables . Therefore ,   we can derive the ELBO as follows :   logp(D|M )   = log / integraldisplay   p(D , Z|M)q(Z| S , Q )   q(Z| S , Q)dZ   ≥/integraldisplay   q(Z| S , Q ) logp(D , Z|M )   q(Z| S , Q)dZ = E[logp(D|Z ) ]   −D[q(Z| S , Q)||p(Z| S , M ) ]   ≜L , ( 16 )   where the inequality is obtained via the Jensen ’s   inequality . Since the likelihood function of our   model is given by :   p(D|Z ) = /productdisplayp(y , a , e,|x , Z)p(x).4273We can put this function into Equation ( 16 ) and   further derive the ELBO as :   L =   −D[q(Z| S , Q)||p(Z| S , M ) ]   + /summationdisplayE[logp(y , a , e,|x , Z ) ]   + const . , ( 17 )   where const .=/summationtextlogp(x)is a constant .   The KL divergence in Equation ( 17 ) has a closed   form solution , which is given by :   −D[q(Z| S , Q)||p(Z| S , M ) ]   = −1   2 / summationdisplay1   σ(µ−m)(µ−m )   −|E|   2((s−1)D−logs ) , ( 18 )   where µ,mdenote the mean of the prior and   variational distributions of prototypes , respectively ,   s = σ / σ , and Dis the dimension of prototypes .   Moreover , we sample Zfrom variational distribu-   tions q(Z| S , Q)to approximate the expecta-   tion term in ELBO .   C Pseudo Code   The training and inference process of our model is   provided in Algorithm 1 . In the training process ,   we randomly sample a small batch of tasks D ,   accumulate the gradients of their objective func-   tion and then update the model parameters with the   AdamW optimizer . In the inference process , for   each task , we first initialize the model parameters   θwith the learned model parameters in the source   domain , i.e. , θ = θ , and then finetune the parame-   ters by maximizing the likelihood function in the   support set for Jsteps .   D Experimental Details   D.1 Datasets   Table 5 shows the statistics of original datasets used   to construct the experimental datasets and statistics   of the constructed few - shot datasets .   Cross - Dataset is an English cross domain few-   shot NER dataset , which is constructed to evaluate   the performance of meta - learning based few - shot   models . We use the public episodesconstructed   by ( Hou et al . , 2020 ) in our experiments , where   the training , validation and test episodes for each   dataset are provided .   We additionally construct a Chinese cross do-   main few - shot NER dataset from five public Chi-   nese NER datasets : CCKS , Address , Medical ,   Weibo and Cluener . We follow the experimental   settings in ( Yang and Katiyar , 2020 ) , where they   first train few - shot models on a source domain and   then transfer the model to target domain with few-   shot data . We take CCKS as source domain and   the other four datasets as target domains . To con-   struct few - shot data in target domains , we use the   sampling method in ( Ding et al . , 2021 ) to sam-   pleK - shot data from the training set of each test   dataset as support set and use the original test data   as query set . We repeat the sampling process for   five times to obtain accurate experimental results .   D.2 Hyperparameter Settings   We set the memory size m , i.e. , number of token   representations of each entity type in the memory ,   as15 . The hyperparameter γand the standard de-   viation of prototype distributions is set to be 0.5   anderespectively . The dropout rate and weight   decay coefficient is set to be 0.1and1e−3 , re-   spectively . On Cross - Dataset , we choose batch size   from{1,16,32 } , learning rate from { 1e-5 , 3e-5 ,   1e-4 } , training steps from { 300,500,1000 } , and   finetune steps from { 30,50 } . We perform grid   search to choose hyperparameters that have the   best performance on the validation set . The opti-   mal hyperparameter settings on Corss - Dataset are   provided in Table 4 . On Chinese Cross - Dataset ,   we set the batch size as 1 , training steps as 1000 ,   finetune steps as 50 , training learning rate as 3e-5   and finetune learning rate as 3e-5 for all settings .   During training , we evaluate our model on the vali-   dation set every 100 steps and select the checkpoint   with best f1 scores on the validation set as the final   model.4274ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Please see the Limitations Section .   /squareA2 . Did you discuss any potential risks of your work ?   There is not obvious risk regarding our work .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Please see the Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Please see Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Our model is built based on the existing pretrained model , and the computational budget might vary   depending on the used backbone model.4275 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Please see Section 4.1 and Appendix D.2 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Please see Section 4.2 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Please see Appendix D.2 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.4276