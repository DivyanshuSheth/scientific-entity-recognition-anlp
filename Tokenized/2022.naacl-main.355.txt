  Niccolò Campolungo   Sapienza University of RomeTommaso Pasini   Sapienza University of Rome   Denis Emelin   University of Edinburgh , ScotlandRoberto Navigli   Sapienza University of Rome   Abstract   Recent studies have shed some light on a com-   mon pitfall of Neural Machine Translation   ( NMT ) models , stemming from their strug-   gle to disambiguate polysemous words with-   out lapsing into their most frequently occurring   senses in the training corpus . In this paper , we   first provide a novel approach for automatically   creating high - precision sense - annotated paral-   lel corpora , and then put forward a specifically   tailored fine - tuning strategy for exploiting these   sense annotations during training without intro-   ducing any additional requirement at inference   time . The use of explicit senses proved to be   beneficial to reduce the disambiguation bias   of a baseline NMT model , while , at the same   time , leading our system to attain higher BLEU   scores than its vanilla counterpart in 3 language   pairs .   1 Introduction   Translating a sentence requires the underlying   meaning to be captured and then expressed in the   target language . Nonetheless , only little atten-   tion has been devoted to studying the actual ca-   pabilities of Neural Machine Translation ( NMT )   approaches of modeling different senses of am-   biguous words , with recent work showing that sys-   tems tend to be biased towards the most frequent   meanings found within the training corpus ( Emelin   et al . , 2020 ) . This phenomenon is hard to mea-   sure through classical evaluation metrics , such as   the BLEU score ( Papineni et al . , 2002 ) , as they   often rely on word - matching heuristics that fail to   capture the disambiguation capabilities of the eval-   uated systems . Therefore , several efforts have been   recently devoted to shed some light and create test   beds ( Rios Gonzales et al . , 2017 ; Raganato et al . ,   2019 ; Emelin et al . , 2020 ; Campolungo et al . , 2022 )   to challenge NMT models . Results show that these   models still struggle to deal with highly polyse-   mous words , especially when used to express least   frequent senses . For example , given the sentence “ The energy   comes from a distant plant . ” , both Google Trans-   late and DeepL disambiguateplant to its sense   oforganism when translating into Italian , and pro-   duce the following incorrect sentence “ L’energia   proviene da una pianta lontana . ” , rather than   “ L’energia proviene da un impianto lontano . ” ,   where impianto is the translation for the factory   meaning of plant . This suggests that , even when   adequate context is provided ( energy should be   enough to correctly infer the right sense of plant ) ,   state - of - the - art models might still be biased to-   wards the most frequent meanings found within   training data .   Some recent studies have explored how to lever-   age explicit sense information within NMT mod-   els ( Rios Gonzales et al . , 2017 ; Pu et al . , 2018a ;   Nguyen et al . , 2018 ) . Nevertheless , including such   information is not trivial for three main reasons :   i ) sense - tagged parallel data is scarce ; ii ) Word   Sense Disambiguation ( WSD ) systems have not   been accurate enough until very recently ( Blevins   and Zettlemoyer , 2020 ; Barba et al . , 2021 ) ; and iii )   how explicit senses should be incorporated within   neural models is not straightforward .   In this paper , we first introduce a novel approach   to make up for the paucity of sense annotations in   parallel corpora , leveraging a multilingual WSD   system to tag parallel sentences and refine its pre-   dictions by means of cross - lingual word alignments   and information from a multilingual knowledge   base . Then , we fine - tune our baseline models   on our sense - tagged corpora via a specifically de-   signed loss function , allowing the injection of word-   level semantics into the architecture . We evaluate   our approach on standard and challenge test sets ,   showing that it does indeed improve translation ac-   curacy and mitigates the most frequent sense bias .   To summarize , our contributions are manifold:48241.We put forward a novel approach to produce   high - precision sense annotations for parallel   data , which we apply to three language pairs .   2.We propose a fine - tuning strategy that lets   us inject word - level explicit semantics into   Neural Machine Translation models , without   introducing any additional requirement at in-   ference time .   3.We show that employing explicit sense tags is   beneficial in order both to mitigate the sense   bias and to improve the translation quality   in terms of BLEU score on standard bench-   marks .   4.We present a case study on how a state - of - the-   art WSD system compares to an NMT model   on disambiguating words within a challenging   set for detecting sense bias in MT .   We make all the generated datasets , the code   of the model and for the experiments available   athttps://github.com/sapienzanlp/   reducing - wsd - bias - in - nmt .   2 Related Work   Word Sense Disambiguation was first formulated   as a computational task by Weaver ( 1949 ) in the   context of Machine Translation . The two fields   then followed parallel paths , with more or less suc-   cessful attempts over the years to join them back to-   gether ( Carpuat and Wu , 2005 ; Vickrey et al . , 2005 ;   Carpuat and Wu , 2007 ) . Indeed , while Carpuat   and Wu ( 2005 ) reported negative results when   trying to integrate the prediction of a supervised   WSD approach into a Statistical Machine Trans-   lation ( SMT ) model , the same authors , two years   later , successfully improved the performance of a   phrase - based SMT approach by leveraging a new   phrase - based WSD model ( Carpuat and Wu , 2007 ) .   More recently , Pu et al . ( 2018a ) and Nguyen et al .   ( 2018 ) proposed systems that successfully leverage   sense information in NMT models , although they   introduced a heavy requirement , i.e. , that of dis-   ambiguating the ambiguous words in the sentence   prior to generating a translation , which makes them   unfeasible in many real - world settings . Lately , con-   textualized word embeddings have been employed   to produce additional back - translated parallel train-   ing data via mining sense - specific target sentences ,   in order to improve handling of infrequent senses   ( Hangya et al . , 2021).Nevertheless , the proper treatment of lexical   ambiguity is still an open problem , with neural   models struggling to translate least frequent senses   and often relying on spurious correlations among   words ( Emelin et al . , 2020 ; Raganato et al . , 2019 ;   Rios Gonzales et al . , 2017 ) . Thus , the disambigua-   tion bias topic has received renewed interest , and   several benchmarks have been introduced in the   most recent years with the goal of directly mea-   suring the extent to which neural architectures are   able to capture word semantics . One of the first of   this kind was ContraWSD ( Rios Gonzales et al . ,   2017 ) . In this first attempt to evaluate WSD capa-   bilities of NMT models , the authors built an adver-   sarial test set where source sentences containing   an ambiguous word were associated with a cor-   rect translation and several incorrect alternatives .   These latter were built by replacing the reference   translation for the ambiguous word with the trans-   lation of one of its other possible meanings . The   task measured whether a model ranked the correct   translation higher , i.e. , it assigned it a higher prob-   ability than the adversarial ones . This study pro-   vided evaluation data for two language pairs only ,   i.e. , German →English and German →French , and   within a few years it became outdated as modern   NMT models could easily attain high performances   ( Emelin et al . , 2019 ) . Thus , MuCoW ( Raganato   et al . , 2019 ) took things a step further and lever-   aged BabelNet ( Navigli and Ponzetto , 2012 ; Nav-   igli et al . , 2021 ) – a large multilingual knowledge   base – and sense embeddings ( Camacho - Collados   et al . , 2016 ; Mancini et al . , 2017 ) in order to au-   tomatically create adversarial translations for five   language pairs while also increasing the difficulty   of the task itself ; however , the fully automatic na-   ture of these challenge sets made them noisy and   prone to containing irrelevant challenge samples .   More recently , Emelin et al . ( 2020 ) proposed   two challenge sets for the English →German pair ,   one measuring the model sensitivity to most fre-   quent senses and the other estimating , through ad-   versarial injections , its susceptibility to changing   a correct sense to a wrong one . In contrast to pre-   vious studies , these challenge sets were based on   correlations among words in the training set and re-   lied on manually - refined sense clusters , providing   an excellent test bed for measuring semantic bias .   Finally , Campolungo et al . ( 2022 ) proposed   DBMT , the first fully manually annotated test   set for measuring the disambiguation bias of neural4825machine translation models , covering five language   combinations , namely , from English to German ,   Spanish , Italian , Russian and Chinese . In their   work , the authors showed that open neural models   still exhibit strong semantic biases towards frequent   senses , confirming once again the suspicions about   this under - explored issue .   Despite all the effort made in putting forward   challenging sets of data to test WSD capabilities   of NMT models , to the best of our knowledge ,   only a few approaches ( Rios Gonzales et al . , 2017 ;   Liu et al . , 2018 ) have been proposed to mitigate   this issue , and none of these is effective with mod-   ern Transformer - based architectures . Furthermore ,   while parallel corpora have been exploited to pro-   duce sense annotations in the past ( Bonansinga   and Bond , 2016 ; Delli Bovi et al . , 2017 ) , they   were built by utilizing outdated disambiguation ap-   proaches that have recently been surpassed by more   advanced neural architectures . Indeed , the Word   Sense Disambiguation field has received much at-   tention in the last few years , with several supervised   approaches ( Conia and Navigli , 2021 ; Blevins and   Zettlemoyer , 2020 ; Barba et al . , 2021 ) and sense   embedding models ( Loureiro and Jorge , 2019 ; Scar-   lini et al . , 2020a , b ; Wang et al . , 2020 ) performing   close to the upper bound limit of the inter - annotator   agreement , which finally makes them feasible for   inclusion in other downstream tasks , e.g. , Machine   Translation .   Thus , differently from previous studies in the   literature , we focus on closing the gap between   these two fields , i.e. , Neural Machine Translation   and Word Sense Disambiguation , by putting the   recent advances in WSD at the service of NMT   models . We propose a novel approach , similar to   that introduced in Luan et al . ( 2020 ) , for creating   high - quality sense - annotated parallel corpora , and   we use this semantic information to regularize an   NMT model , making it less biased and capable of   producing higher - quality translations .   3 Reducing the Disambiguation Bias in   NMT   Neural Machine Translation models are typically   trained end - to - end to produce a target translation   given a source sentence and , thus , they can only   rely on the input context to resolve the ambiguity   of polysemous words therein . Being pattern recog-   nition algorithms at heart , these models fall prey   to the inherent bias carried by the frequency of co - occurrence of words within parallel sentences , and   thus tend to disambiguate words to the sense they   most frequently encountered during training , even   when the sentence does provide enough context to   identify the correct sense . At the same time , Word   Sense Disambiguation models , i.e. , models special-   ized in associating a word in context with one of   the meanings within a given sense inventory , have   recently displayed remarkable results across differ-   ent benchmarks and languages ( Bevilacqua et al . ,   2021 ) . The time may now therefore be ripe for   them to be successfully included into downstream   applications such as Neural Machine Translation .   However , data that would allow these two worlds   to be brought together , i.e. , parallel corpora where   words are associated with semantic labels , are cur-   rently still produced automatically by leveraging   outdated approaches to WSD ( Delli Bovi et al . ,   2017 ) .   In what follows , we first provide some prelim-   inary information about resources and tools that   we employ in our method ( § 3.1 ) ; then , we intro-   duce a new approach for automatically annotating   tokens within parallel sentences with sense annota-   tions , i.e. , labels explicitly defining their meanings   ( § 3.2 ) ; finally , we propose a fine - tuning objective   for leveraging such annotations in order to mitigate   the sense bias while also improving the transla-   tion quality overall ( § 3.3 ) . The intuition behind   our work is that fixed sense labels describing word   senses would help NMT models better encode the   underlying meaning of the input sentence , thus gen-   erating less biased and overall better translations .   3.1 Preliminaries   We draw sense labels from BabelNet ( Navigli and   Ponzetto , 2012 ) , a multilingual knowledge base   created by merging several semantic resources in   different languages such as WordNet ( Miller et al . ,   1990 ) , Wikipedia , Wikidata , etc . BabelNet is struc-   tured in synsets , i.e. , sets of synonymous senses   in different languages . For instance , the synset   ofplantcontains the following lexicaliza-   tions : plant , pianta , Pflanze , among oth-   ers . Additionally , BabelNet provides lemma - to-   synsets mappings . For example , the English noun   plant belongs to the following nominal synsets : or-   ganism , industrial plant , actor in the audience and   something placed secretly .Since BabelNet con-4826tains millions of synsets , which may make the com-   putation too expensive , we restrict the vocabulary   to just those containing at least one English sense   from WordNet , as is also done in several other   works ( Barba et al . , 2020 ; Scarlini et al . , 2020b ;   Bevilacqua and Navigli , 2020 ) .   3.2 Building a Sense - Annotated Parallel   Corpus   Let us assume that our running example sentence   “ The energy comes from a distant plant . ” appears   within a parallel corpus paired with the following   Italian translation : “ L’energia viene da un impianto   lontano . ” . As we said , by considering the English   sentence alone , the word plant could take several   meanings , among which organism andpower plant .   However , among these , only one is shared with its   translation impianto , i.e. , the power plant meaning .   Therefore , considering the cross - lingual alignment   of words may drastically reduce the set of valid   meanings , making the disambiguation task much   easier . Based on this intuition , given a parallel   corpus , we perform the following two steps :   1.Sense Scoring , where we employ a WSD sys-   tem to assign to each content word a distribu-   tion over its possible meanings ;   2.Annotation Refinement , where we compute   cross - lingual word alignments to reduce lexi-   cal ambiguity and finally assign the most suit-   able sense to each content word .   Sense Scoring In this step , our goal is to assign   to every content word within a sentence a distribu-   tion over its possible senses in BabelNet . To this   end , given as input a sentence sfrom a parallel   corpus C , we first apply Part - of - Speech tagging   and lemmatization to it , then pass it through our   WSD system , which returns a distribution over its   possible meanings .   Formally , let wbe a content word in a sentence   s= [ w , . . . , w ] , and σ(w)the set of synsets as-   sociated with win BabelNet . The WSD system as-   signs a score c(S|w , s)to each synset S∈σ(w ) ;   we denote the synset of wwith the highest confi-   dence as S. As a result , each content word in a   source or target sentence is associated with a sense   distribution . However , applying a WSD system   alone may not be sufficient to ensure high - qualityannotations , as the application domain may be dif-   ferent from the one of its training set . Therefore ,   in the next step we take advantage of the transla-   tion each sentence is paired with to refine sense   annotations .   Annotation Refinement We produce word - level   cross - lingual alignments between the source and   the target sentences of the parallel corpus : given   a pair of parallel sentences ( s , t ) , we compute a   list of alignments A={(w , w)|w∈s , w∈t } .   Thus , given an aligned word pair P= ( w , w)∈   A , letσ(P ) = σ(w)∩σ(w ) , i.e. , the intersection   of synsets that the two words may denote accord-   ing to BabelNet : we discard annotations for any   word pair such that σ(P ) = ∅ ∨ |σ(w)|<2 . In   other words , we retain all the aligned pairs ( w , w )   such that the source word is polysemous and the   intersection of their senses is non - empty , thus en-   suring higher annotation precision by leveraging   the parallelism of words .   Finally , we assign the same synset Sto both   words ( w , w)inPas follows :   S = S = S   = argmax / parenleftigg   c(S|w , s )   Z+c(S|w , t )   Z / parenrightigg   Z=/summationdisplayc(S|w , s )   Z=/summationdisplayc(S|w , t )   that is , the synset with the highest combined confi-   dence score after normalizing over σ(P ) , where Z   andZrepresent the normalization factors of the   probability distributions associated with the synsets   ofwandw , respectively .   3.3 Semantic Injection   Now that we can generate high - quality sense an-   notations , we describe our fine - tuning method to   inject word - level semantics into a Neural Machine   Translation model . Ideally , we want the model   to benefit from such annotations during training ,   while not being dependent on them at inference   time . To satisfy both these desiderata , we adapt   the model ’s vocabulary to handle synsets as well as   subwords , and propose a specific loss that exploits   the injected senses to improve the base model ’s   handling of ambiguous words.4827   Semantically Enhancing Sentences In order to   work with concepts , we need a way to represent   them . Let us consider once more the sentence “ The   energy comes from a distant plant . ” : we rewrite it   in order to also include the exact meaning for plant ,   which we computed as described in § 3.2 : “ The   energy comes from a distant plant plant ” .   Formally , given a source sentence sand a word   wannotated with sense S , we simply repre-   sentwas its standard segmentation followed by   S , represented by its sense embeddingpassed   through a linear projection layer ( as shown in Fig-   ure 2 ) . Additionally , to enforce the connection be-   tween the tagged word and its sense annotation , we   set the position ids for the word and the sense em-   bedding to the same value , as if they were a single   token . This encoding scheme gracefully extends   to the whole sentence , yielding the sense - enhanced   input representation for a given sentence s.   Semantic Consistency Regularization We   hereby propose the Semantic Consistency Regular-   ization ( SCR ) objective , inspired by MVR ( Wang   et al . , 2021 ) .   Formally , let xandxbe two encodings ( plain   and sense - enhanced ) of the same input sentence x   and let ybe the target sentence , we define SCR as :   SCR ( θ ) = −logP(y|x)−logP(y|x )   + D(P(y|x)|| P(y|x ) )   where θis the set of trainable weights , Dis the   unidirectional Kullback - Leibler divergence ( Kull-   back and Leibler , 1951 ) and P(y|x)represents an   output distribution ( a visual representation of SCR   is reported in Figure 1 ) .   With this formulation , SCR jointly uses the same   sentence with and without sense annotations as two   separate inputs : while we train the model to be   able to translate both plain and sense - enhanced   sentences , by minimizing the divergence between   the output distributions we also force the model   to transfer the sense information from the sense-   enhanced input to the plain input , much like in   a self - distillation process . At the same time , we   still maintain the model ’s capability of translating   without sense annotations , thus dropping their re-   quirement at inference time .   4 Experimental Setup   4.1 Our Model   We employ as underlying model the standard Trans-   former architecture ( Vaswani et al . , 2017 ) , with 6   encoder and 6 decoder layers . Note that , while   SCR can be applied to any pre - trained model , we   retrain one from scratch because most of the other   models available online use part of our test data as4828their training data ( see § 4.2 ) . Additional details   about training configuration and hyperparameters   are provided in § A.3 .   Fine - tuning with SCR Additionally , to jump-   start the model ’s capabilities , we encode synsets   not as randomly initialized learnable vectors ( e.g. ,   by extending the vocabulary ) , but with frozen   pre - trained sense embeddings projected into the   model ’s input space by means of a linear layer , the   only additional learnable component of the model   ( Projection in Figure 2 ) , which is dropped after   the fine - tuning stage . As pre - trained sense embed-   dings we use ARES ( Scarlini et al . , 2020b ) , since   they provide multilingual representations for each   synset in our vocabulary . We study the impact of   this choice in § 5.5 . To perform token - level align-   ments , we use MultiMirror ( Procopio et al . , 2021 ) .   4.2 Datasets   We experiment on three distinct language pairs :   EN→DE , EN →ES and EN →FR . Following   ( Emelin et al . , 2020 ) , we gather the data from   WMT14 for German and French and WMT13   for Spanish , considering only sentences coming   from either CommonCrawl , News Commentary   or Europarl , to maintain similar order of mag-   nitudes among language pairs ( and to contain   pre - processing and training times ) . As valida-   tion sets , we employ newstest2014 for EN →DE ,   newstest2013 for EN →FR and newstest2012 for   EN→ES . All datasets employed in this work are   freely available for research purposes .   Sense - Enhanced Datasets   We process each parallel sentence of the consid-   ered corpora with the procedure described in § 3.2 ,   taking into account only content words whose Part-   of - Speech tag is noun , as the challenge sets we   evaluate upon only target nominal words .   For POS - tagging and lemmatization we use   Stanza ( Qi et al . , 2020 ) . As disambiguation sys-   tem , we use EWISER ( Bevilacqua and Navigli ,   2020 ) , a neural WSD model based on BERT ( De-   vlin et al . , 2019 ) , which has attained state - of - the - art   performances on English as well as other languages .   EWISER has been trained on SemCor ( Miller et al . ,   1993 ) – the standard training set for WSD – and   the WordNet Gloss corpus ( Langone et al . , 2004 ) – a semi - automatically annotated dataset featuring   sense definitions . Detailed statistics of the base and   parallel corpora produced are provided in § A.5 .   Translation Test Set   We evaluate standard translation quality through   thenewstest datasets available in the specific   WMT year ( i.e. , WMTXX corresponds to new-   stest20XX ) . The standard evaluation is carried out   by means of SacreBLEU ( Post , 2018 ) , with signa-   tureBLEU+case.mixed+numrefs.1   + smooth.exp+tok.13a+version.1.5.1 .   Disambiguation Bias Challenge Sets   To measure the disambiguation bias of each model   we employ the challenge sets introduced by Emelin   et al . ( 2020 ) , composed of sentences reserved from   the WMT14 English →German corpus . These chal-   lenge sets are based on sense clusters built by   automatically merging together BabelNet synsets ,   which then are manually refined to ensure their cor-   rectness . Each sense cluster contains an English   polysemous word and a set of German monose-   mous terms , which uniquely identify a certain   meaning .   These clusters are used to create the following   two challenge sets : WSD Bias and Adversarial .   The former quantifies the intrinsic bias the model   learned during training , while the latter measures   how sensitive the model is to the insertion of terms   that are usually associated with another sense clus-   ter during training . Both challenge sets evaluate   in terms of accuracy of correct disambiguation . A   more detailed description of these datasets and their   evaluation process is provided in § A.1 .   DBMT We also evaluate on the German and   Spanish portions of DBMT(Campolungo et al . ,   2022 ) , a recent fully - manually annotated disam-   biguation bias challenge set , where models are   asked to translate English sentences containing am-   biguous words , and their translations are checked   for either correct or incorrect translation equiva-   lents , which , in contrast to previous benchmarks ,   are annotated manually and depend on the context   of the sentence instead of relying solely on the   sense of the source word .   4.3 Comparison Systems   We compare our sense - enhanced model with the   following architectures:48291.OPUS ( Tiedemann and Thottingal , 2020 ): a   strong bilingual model which uses the same   architecture and parameter count as ours , al-   though it was trained on order of magnitudes   more data ;   2.MBart-50 ( Tang et al . , 2021 ): the English - to-   many version of the MBart-50 model ;   3.Baseline : our base NMT models , trained on   the datasets described in § 4.2 .   In what follows we refer to our model fine - tuned   with SCR as Baseline+SCR .   We note that , due to the way in which the WSD   Bias Challenge Sets were constructed ( i.e. , by using   sentences reserved from WMT14 , see § 4.2 ) , any   fair evaluation against OPUS and MBart-50 is to be   considered impossible , as such models have seen   the sentences in the challenge sets during training .   We therefore evaluate these two models only on   standard BLEU , and point out that the resulting   scores should only be regarded as references for   our models ’ competence in the translation task .   5 Results   In what follows , first , we show that our model at-   tains BLEU scores in the same ballpark as state - of-   the - art approaches such as OPUS and MBart-50 ,   despite the large gap in terms of parameters or   training data . Then , we focus our evaluation on the   WSD Bias , and compare our full - fledged model   ( Baseline+SCR ) against its baseline variant .   5.1 General Translation Quality   In Table 1 we observe that the trained baselines are   more than competent in the translation task : indeed ,   when considering average BLEU scores , they place   between OPUS , which is trained on much more   data but has the same parameter count , and MBart-   50 ( Tang et al . , 2021 ) , which is ~8 times larger but   is capable of translating English to 50 languages .   In contrast to common debiasing techniques ,   which often observe a degradation in performance   on standard benchmarks ( Clark et al . , 2019 ; He   et al . , 2019 ) , we report consistent BLEU improve-   ments on all language pairs , all of which are sta-   tistically significant at different p - values ( Table 1 ) ,   providing empirical proof that the proposed method   does not hurt the model ’s general translation capa-   bility , while at the same time it helps models gen-   erate less biased translations ( as will be discussed   in the upcoming sections).5.2 Disambiguation Bias   Results on the Disambiguation Bias Challenge Sets   ( § 4.2 ) are reported in Table 2 , for both of which   we show improvements : on the WSD Bias Chal-   lenge Set , the bias is reduced , significantly , by more   than1 % ; similarly , on the Adversarial Challenge   Set , we see a reduction of homographs mistakenly   disambiguated due to the injection of adversarial   adjectives of 0.27 % . We attribute this lower impact   to the artificial nature of the adversarial sentences ,   some of which , by manual inspection , display poor   grammatical fluency .   5.3 WSD Performance   We conduct an analysis of the performance of   EWISER on the English sentences of the WSD   Bias Challenge Set , to see how it fares in com-   parison with our NMT models . Unfortunately , as   the sense clusters are not directly associated with   BabelNet synsets , we reconstruct this association   automatically and manage to retrieve only 1847 of   the3000 sentences in the challenge set .   Having retrieved BabelNet synsets for the tar-   get terms , we can apply EWISER and check   whether the disambiguated synset matches one of   the synsets retrieved for the sense cluster of the   challenge sentence . Let us consider our running   example , “ The energy comes from a distant plant . ” ,   one last time : if EWISER disambiguates the term   plant to its sense of organism , we count it as a mis-   take , similarly to the case where our NMT model   translates it as pianta instead of impianto ( i.e. , its   sense of factory ) . With this in mind , we evalu-   ate EWISER , Baseline and Baseline+SCR on the   aforementioned subset of sentences ; we report the   results of this evaluation in Table 2 ( bottom ) .   The results indicate that , for this setting , both   NMT models actually perform quite a lot better   than a pre - trained disambiguation system . One   reason for this might be the different distributions   the models are trained on : by design , the chal-   lenge sentences follow a distribution similar to   the corpus used to train the NMT model , whereas   EWISER is trained on sentences coming from news   corpora from the 1960s and dictionary - like defini-   tions . Moreover , in theory , if we were to apply   the refinement process described in § 3.2 to disam-   biguate the challenge sentences , we would achieve   a perfect score , as the target German lemmas are   monosemous and thus the disambiguation is im-4830EN→DE EN →FR EN →ES   WMT14 WMT19 WMT14 WMT13   OPUS† 27.58 39.39 39.93 35.00   MBart-50 ‡ 25.60 35.80 36.12 29.50   Baseline 26.34 36.93 38.05 32.82   Baseline+SCR 27.26 37.74 38.48 33.18   Baseline+SCR 26.13 36.45 37.85 33.15   Baseline+SCR 25.75 35.93 37.33 32.49   Baseline+SCR 26.11 36.74 37.38 32.93   Baseline+SCR 25.63 34.79 / /   plicitly solved . The results of using EWISER ’s raw   annotations are discussed in § 5.5 .   Finally , we choose not to perform a similar com-   parison on the Adversarial Challenge Set , as its   examples are designed to specifically target NMT   models via adversarial injections ; we leave study-   ing their impact in WSD systems as future work .   5.4 System Examples   In Table 3 , we report some examples of disam-   biguation corrected by our model according to the   WSD Bias Challenge Set . The baseline is translat-   ing the terms to their most frequent sense ( column   Wrong sense ) , instead of the correct one ( column   Target sense ) . Moreover , the third example shows   that this is not only a word matching task , as the   improved model is able choose the correct subwordand can capture the nuances of meaning in more   uncommon senses .   5.5 Ablation Study   Ablation on SCR To measure the importance of   the KL term in the loss , we fine - tune the model   without including it in the SCR objective ( § 3.3 )   and report the results in Tables 1 and 2 ( row   Baseline+SCR ) . We observe that , without KL ,   the model struggles to leverage the double inputs ef-   ficiently ; indeed , its translation performance drops   around 1BLEU point on average , while the error   rates increase by roughly 1 % on both bias chal-   lenge sets . These results back our intuition that the   KL divergence helps to distill sense information   from the sense - enhanced inputs , and is indeed a   crucial component to our formulation .   Ablation on ARES We also test our system re-   placing the pre - trained sense embeddings provided   by ARES with randomly initialized learnable em-   beddings and report this result in Tables 1 and 2   ( row Baseline+SCR ) . As expected , both   translation quality and disambiguation bias drop   consistently . Indeed , learning sense embeddings   from scratch is much harder than learning a map-   ping between a fixed space and a trainable one .   Ablation on Annotation Refinement We eval-   uate our sense Annotation Refinement process   ( § 3.2 ) by fine - tuning the model on the uncon-   strained sense annotations provided by EWISER   ( Baseline+SCR ) , i.e. , by considering the   synset with the highest confidence on the source   word as the correct one , instead of S. In the bias4831   Model EN →DE EN →ES   OPUS† 27.99 36.66   MBart-50 ‡ 28.73 33.89   Baseline 24.00 26.44   Baseline+SCR 25.00 25.84   evaluation ( Table 2 ) , the performances on both chal-   lenge sets drop significantly ( p < 0.001 ) , which is   in line with EWISER ’s performance on this chal-   lenge set ( § 5.3 ) . Furthermore , the BLEU scores   drop too , although not as significantly ( Table 1 ) ,   but still always under - performing with respect to   Baseline+SCR .   Ablation on Sense Annotations Finally , we test   whether the sense annotations have an impact by   replacing them with random senses for the spe-   cific word , drawn from the sense vocabulary with   uniform probability , during the fine - tuning stage   ( Baseline+SCR ) .As expected , we observe   that randomly injecting senses is detrimental , with   important performance drops in both the standard   and the bias evaluation benchmarks .   5.6 Evaluation on DBMT   In Table 4 we report the results obtained on   DBMT(Campolungo et al . , 2022 ) . For the sake   of conciseness , we only report accuracy scores as   a proxy for the general disambiguation bias dis - played by our models .   While on English →German we observe   an improvement of 1 % , the performance on   English →Spanish decreases by around 0.6 % . We   hypothesize that our English →Spanish model   might be undertrained , as its accuracy differs by   around 10 % from OPUS , its direct comparison ,   while on English →German the difference is only   of around 3 % . We leave further investigation of   this issue , including training larger , more capable   models , as future work .   6 Conclusions   In this paper , we presented a fine - tuning strategy   that , by leveraging the explicit sense annotations   produced by a novel high - precision technique , ef-   fectively reduces the disambiguation bias of a base-   line Neural Machine Translation model while at   the same time also strengthening translation per-   formances , without introducing any requirement at   inference time .   Our analysis on a strong disambiguation system   showed that its ability to disambiguate polysemous   nouns is worse than that of a baseline NMT model ,   at least in the studied out - of - domain setting .   We believe that this work paves the way for bet-   ter bias reduction techniques in MT , while also   fostering interest in the issue represented by the   disambiguation bias . As future work , we plan to   further study the ability of NMT models to perform   Word Sense Disambiguation and to strengthen re-   search at the intersection of these two fields , with a   view to building stronger and more reliable models.4832AcknowledgementsThe authors gratefully acknowledge   the support of the ERC Consolida-   tor Grant MOUSSE No . 726487 and   the PerLIR project ( Personal Lin-   guistic resources in Information Re-   trieval ) funded by the MIUR Pro-   getti di ricerca di Rilevante Interesse   Nazionale programme ( PRIN 2017).This work was also partially supported by the   MIUR under the grant “ Dipartimenti di eccellenza   2018 - 2022 " of the Department of Computer Sci-   ence of Sapienza University .   References483348344835   A Appendix   A.1 Bias Evaluation Challenge Sets   We here provide a more detailed description of the   datasets introduced by ( Emelin et al . , 2020 ) . From   § 4.2 , recall that these challenge sets are based on   sense clusters built on BabelNet , where each sense   cluster contains an English polysemous word and a   set of German monosemous terms , which uniquely   identify a certain meaning .   We highlight that there is no direct link between   the sense clusters and the data produced by our   Annotation Refinement process , as the sense clus-   ters are i ) heavily manually refinedand ii ) based   on the entire BabelNet4 inventory ( 16 M concepts ) ,   while EWISER only covers the subgraph of Ba-   belNet linked to WordNet ( 117k concepts ) , as is   common in the multilingual WSD setting . As such ,   we do not consider the evaluation to be in any way   more favorable towards our system .   WSD Bias contains sentences whose targeted   English term is likely to be translated into a specific   different sense due to co - occurrences of words in   the sentence itself . For example , in the sentence   “ a lot of money was spent to renovate the capital ”   the word capital is likely to be translated into its   sense of amount of money due to the presence of   the words money andspent . A mistake is detected   if the term is translated into any of the German   words contained in the most likely sense cluster .   The goal of this task is to measure the intrinsic bias   the model learned during training .   Adversarial contains two sets of sentences , the   original sentence and its adversarial counterpart ,   built by injecting an adjective that is likely to flip   the disambiguation performed by the NMT model   towards a specific sense . For example , given the   sentence “ they met in the spring of 2020 ” , the   adversarial example would be “ they met in the   hotspring of 2020 ” . The injection of hotleads   the model to translate spring into its sense of wa-   ter source as opposed to its correct sense of sea-   son . A mistake is detected every time the non-   adversarial sentence is translated into the correctsense , whereas its adversarial counterpart is flipped   to the sense cluster the adjective points to . The goal   of this task is to measure how sensitive the model is   to the insertion of terms that are usually associated   with another sense cluster during training .   A.2 Training a Sense - Enhanced NMT Model   Our work is based on the assumption that providing   a neural model with sense annotations for ambigu-   ous words helps in disambiguating them . While   this is rather intuitive , and has been shown to be   the case in previous works ( Nguyen et al . , 2018 ;   Pu et al . , 2018b ) , we test this hypothesis in our   setting by training an NMT model , from scratch ,   with sense - enhanced sentences only ( see § 3.3 for   details ) . We train a model comparable with the   Baseline ( i.e. , same architecture and hyperparame-   ters ) on the English →German training set ( § 4.2 ) ,   and observe that it achieves higher BLEU scores   than the Baseline ( which is trained on the same data   but with plain sentences ) . For instance , the sense-   enhanced model achieves a BLEU score of 27.22   on WMT14 and 36.79 on WMT19 , with the first   being a statistically significant improvement . This   confirms , once again , that sense - enhanced NMT   models are on par or better than plain NMT models ,   although they introduce the heavy requirement of   WSD at inference time , which our work aims at   dropping .   A.3 Reproducibility Details   Preprocessing Times The preprocessing of the   datasets needed to apply Annotation Refinement   ( lemmatization , Part - of - Speech tagging and then   disambiguation through EWISER ) required around   4 days in total on an RTX 2080 Ti ( roughly 3 M   sentences per day ) .   Training infrastructure and duration All our   experiments were carried out on either an NVIDIA   RTX 2080 Ti or a RTX 3090 , depending on avail-   ability .   Model training required on average 4 days on   a 3090 , 7 days on a 2080 Ti . Fine - tuning epochs   required around 10 hours each ( on a 3090 ) , with   most finishing due to early stopping before the end   of the second epoch .   Parameter counts We used HelsinkiNLP   MarianMT models available on Hugging-   Face Transformers ( Wolf et al . , 2020 )   ( e.g. , for EN →DE , the model name is   Helsinki - NLP / opus - mt - en - de ) . For4836WSD Bias Adversarial   MODEL Correct ↑%Error ↓Correct ↑%Error↓%Error↓   Baseline 71.37 12.27 86.10 4.48 0.40   Baseline+SCR 73.27 11.23 87.36 4.21 0.34   Baseline+SCR 70.37 12.43 85.30 5.13 0.40   Baseline+SCR 70.53 12.53 85.75 4.75 0.45   Baseline+SCR 70.20 13.07 86.40 4.93 0.35   Baseline+SCR 68.83 12.56 84.51 5.04 0.63   EWISER 68.54 13.70 / / /   Baseline 72.77 11.86 / / /   Baseline+SCR 75.58 9.91 / / /   instance , EN →DE has 74.4 M parameters ,   EN→ES has 77.9 M , EN →FR has 75.1M.   For the fine - tuning stage we added ARES   ( frozen ) , thus adding a number of parameters equal   to ARES ’s size ( 1536 ) times the number of unique   synsets in the dataset ( refer to Table 6 for approxi-   mate numbers ) . We also added a trainable projec-   tion layer of size 1536∗512(512is the Trans-   former ’s hidden dimension ) , thus adding 786k   trainable parameters ( which we drop after the fine-   tuning ) .   Model training hyperparameters Similarly to   ( Emelin et al . , 2020 ) , we trained it on the entire   dataset for a max of 100,000steps with approxi-   mately 24k tokens per batch , label smoothing at   0.1and an inverse square root learning rate sched-   uler with 4000 warmup steps . As optimizer , we   used Adam ( Kingma and Ba , 2015 ) with betas   ( 0.99,0.98)and learning rate 7·10 , additionally   employing an early stopping strategy with patience   5 , monitoring the BLEU score on a validation set .   We produced translations at inference time using a   beam size of 5 .   Fine - tuning hyperparameters For the fine-   tuning , we resumed training using the weights   of the baseline models , changed the learning to   1·10and reduced the warmup to 1000 steps ;   additionally , we evaluated the model every 10 % of   the fine - tuning steps rather than after each epoch ,   as we observed fast convergence during fine - tuning   and multiple epochs were superfluous . A.4 Disambiguation Bias Results   Table 5 reports the same results displayed in the   paper , but includes the percentage of Correct trans-   lations for both challenge sets as well as the percent-   age of errors made from sentences that , after the   injection of the adversarial adjectives , were trans-   lated into a sense that was neither the correct one ,   nor the one targeted by the adversarial injection   ( i.e. , other ) .   A.5 Data Statistics   A.6 Limitations of this work   Our work focuses on reducing the disambiguation   biases picked up by NMT models during training .   We acknowledge some limitations in our work :   1.Due to limited computational budget and the   large number of resources required to train   and fine - tune NMT models from scratch , we   had to limit ourselves to one run per exper-   iment , though , despite this , the consistency   across languages seems to point to the empiri-   cal correctness of the claims.48372.We evaluated the bias reduction explicitly only   on the English →German language pair . The   reason for this was twofold : first , the datasets   introduced by Emelin et al . ( 2020 ) only cover   said pair , and require the accompanying train-   ing data be used in order to fully exploit the   co - occurrences ( and hence the biases ) that the   model is evaluated upon ; second , upon man-   ual inspection , we found that MuCoW ( Ra-   ganato et al . , 2019 ) contains many irrelevant   candidates in its translation suite , and is in   general very strongly affected by the noisy   nature of BabelNet .   3.Our pipeline is strictly tied to both the ac-   curacy of the multilingual WSD system em-   ployed and by the coverage of the underlying   sense inventory . While EWISER and Babel-   Net work reasonably well for high - resource   languages , the quality of the annotated corpus   might decrease for low - resource ones.4838