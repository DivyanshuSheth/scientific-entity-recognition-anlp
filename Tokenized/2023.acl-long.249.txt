  Chen Zhang , Yang Yang , Jiahao Liu , Jingang Wang , Yunsen Xian ,   Benyou Wang , Dawei SongBeijing Institute of TechnologyMeituan NLPThe Chinese University of Hong Kong , Shenzhen   chenzhang9702@outlook.com   Abstract   Pretrained language models ( LMs ) have shown   compelling performance on various down-   stream tasks , but unfortunately they require   a tremendous amount of inference compute .   Knowledge distillation finds a path to com-   press LMs to small ones with a teacher - student   paradigm . However , when the capacity gap   between the teacher and the student is large , a   curse of capacity gap appears , invoking a defi-   ciency in distilling LMs . While a few studies   have been carried out to fill the gap , the curse   is not yet well tackled . In this paper , we aim   at lifting the curse of capacity gap via enlarg-   ing the capacity of the student without notably   increasing the inference compute . Largely mo-   tivated by sparse activation regime of mixture   of experts ( ME ) , we propose a mixture of min-   imal experts ( MME ) , which imposes extra   parameters to the student but introduces almost   no additional inference compute . Experimen-   tal results on GLUE and CoNLL demonstrate   the curse of capacity gap is lifted by the magic   ofMMEto a large extent . MME   also achieves the state - of - the - art performance   at small FLOPs compared with a range of com-   petitive baselines . With a compression rate as   much as ∼50×,MMEpreserves ∼95 %   GLUE score of the teacher .   1 Introduction   Pretrained language models ( LMs ) have become a   popular choice for various downstream tasks , e.g. ,   text classification , token classification , and ques-   tion answering ( Devlin et al . , 2019 ; Liu et al . , 2019 ;   Raffel et al . , 2020 ) . Unfortunately , appealing per-   formance comes with a huge cost of inference com-   pute due to the scale of LMs . Knowledge distil-   lation ( Hinton et al . , 2015 ; Sun et al . , 2019 ) , as   an alternative to model pruning ( Han et al . , 2015 )   and quantization ( Sung et al . , 2015 ) , discovers aTable 1 : The curse of the capacity gap in terms of   GLUE ( Wang et al . , 2019 ) . The △ denotes the perfor-   mance difference of preceding two numbers . To ensure   students at similar scales , the student / teacher scale ra-   tios are properly reduced for some methods .   way to compress ( Bucila et al . , 2006 ) LMs with a   teacher - student paradigm .   However , in LM distillation , we recognize a   curse of capacity gap as :   “ Large teachers , poor students . ”   The curse of capacity gap refers to a deficiency   that a larger teacher might unexpectedly result in a   poorer student especially when the capacity gap be-   tween the teacher and the student is large ( Mirzadeh   et al . , 2020 ; Cho and Hariharan , 2019 ) , as illus-   trated in Table 1 . Notably , this is the first veri-   fication in LM distillation since previous studies   recognize the curse in vision model distillation . Al-   though a few studies ( Wang et al . , 2020 ; Zhang   et al . , 2022a ; Park et al . , 2021a ) have investigated   to fill the gap , the curse is still not yet tackled .   To the demand , we aim at lifting the curse of   capacity gap via enlarging the capacity of the stu-   dent without notably increasing the inference com-   pute . We propose a mixture of minimal experts   ( MME ) , inspired by the intuition of sparse ac-   tivation of mixture of experts ( ME ) ( Shazeer et al . ,   2017 ) . Thanks to that the activation process can   be parallel on either single or multiple devices ( He   et al . , 2021 ; Rajbhandari et al . , 2022 ) , MME   on the one hand imposes extra parameters to the4535   student , but on the other hand introduces negligibly   additional inference compute brought by routing   algorithm . To our best knowledge , this is the first   work aiming at lifting the curse completely .   Experiments are conducted on GLUE ( Wang   et al . , 2019 ) and CoNLL ( Sang and Meulder , 2003 ) .   The results exhibit that MMElargely lifts the   curse of the gap as in Table 1 . MMEalso   achieves state - of - the - art performance compared   with a range of competitive baselines , as shown   in Figure 1 . With compression as much as ∼50× ,   MMEpreserves ×95 % GLUE score of the   teacher . Thereby , we state that MMEisa   small yet nontrivial magic , making a great differ-   ence in lifting the curse .   2 Curse of Capacity Gap   The curse of capacity gap is not new but is al-   ready recognized in studies on vision model dis-   tillation ( Mirzadeh et al . , 2020 ; Cho and Hariha-   ran , 2019 ) . While a hit - the - mind drawback of the   curse is that the performance of distilling to a small   student can be dramatically worse than that of dis-   tilling to a slightly larger one , a rather counter-   intuitive deficiency is invoked as that the perfor-   mance of distilling from a large teacher can be   unexpectedly worse than that of distilling from a   smaller one ( i.e. , large teacher , poor student ) . We   here give a minor theoretical justification on the   curse , as a plus to the empirical justification .   Proposition 1 ( VC dimension theory , Vapnik ,   1998 ) .Assuming that the teacher function is f∈   F , the labeling function is f∈ F , and the data   isD , we have :   r(f)−r(f)≤ϵ+o(|F|   |D|),where r(·)is the risk function , | · |is the function   class capacity measure , and | · |is the data scale   measure . It should be highlighted that the approx-   imation error ϵis negatively correlated with the   capacity of the teacher model while the estimation   error o(·)is correlated with the learning optimiza-   tion .   Proposition 2 ( Generalized distillation the-   ory , Lopez - Paz et al . , 2016 ) .Additionally provid-   ing that the student function is f∈ F , we have :   r(f)−r(f)≤ϵ+o(|F|   |D| ) ,   where the approximation error ϵis positively cor-   related with the capacity gap between the teacher   and the student models , and 1/2≤α≤1is a   factor correlated to the learning rate .   Theorem 1 . The bound for the student function at   a learning rate can be written as :   r(f)−r(f)≤ϵ+ϵ+o(|F|   |D| ) + o(|F|   |D| )   ≤ϵ+ϵ+o(|F|+|F|   |D| ) ,   Proof . The proof is rather straightforward by com-   bining Proposition 1 and 2 .   Remark 1 . Under the same distillation setting , we   can ignore the estimation error . When we compare   two students of different capacities distilled from   a teacher of the same capacity , the student of a   smaller capacity has a larger ϵthus lower per-   formance . When we compare two students of the   same capacities distilled from teachers of different   capacities , the student distilled from the teacher of   a larger capacity has a smaller ϵyet a larger ϵ   thus a tradeoff .   Remark 1 basically tells that a tradeoff is associ-   ated with the increase of teacher capacity , implying   that increasing teacher capacity would first lead to   improved but then degraded student performance .   This tradeoff naturally corresponds with the curse .   On the other hand , it is accepted that large ca-   pacity gap is a pain and is processed in literature   of LM distillation ( Wang et al . , 2020 ; Zhang et al . ,   2022a ; Zhou et al . , 2022 ) . Being unaware of the   curse of capacity gap , these studies attempt to of-   fer student - friendly teachers by either interpolat-   ing teacher assistants ( Wang et al . , 2020 ; Zhang   et al . , 2022a ) or adapting teacher knowledge ( Zhou   et al . , 2022 ; Yang et al . , 2022 ) . The unawareness4536is largely due to a fun fact that they only distil   LMs like BERT , but neglect the scalability to   LMs like BERT especially when the student is   small . Though the performance of student can be   boosted in this way , the curse still remains in LM   distillation as in Figure 2 . Other related work in   knowledge distillation is given in Appendix E.   Embarrassingly , while the curse is claimed to be   tackled in vision model distillation ( Zhu and Wang ,   2021 ; Park et al . , 2021a ; Zhao et al . , 2022 ) , our   preliminary study ( cf . Table 13 in Appendix H )   indicates they are either expensive or not capable   of LMs . The potential differences are as follows :   tasks ( e.g. , ImageNet v.s. GLUE ) , backbones ( e.g. ,   ResNets v.s. transformers ) , and paradigms ( e.g. ,   from scratch v.s. pretraining ) .   3 MiniMoE   3.1 Motivation   Enlarging the capacity of the student is an intuitive   solution to lift the curse of capacity gap . How-   ever , regarding the inference compute efficiency ,   the increase of capacity should not introduce much   inference compute .   An initial proposal can be using quantized back-   bones ( Zafrir et al . , 2019 ; Bai et al . , 2021 ) . Quan-   tized backbones may decrease the compute pre-   cision , therefore maintaining inference compute   constant , along the course of enlarging the capacity .   But a vital portion of hardware - specific modifica-   tions are needed to do so . We hence move on to   next possibility .   Another alternative is using dynamic net-   works ( Han et al . , 2021 ) based on the idea of con-   ditional computation ( Bengio et al . , 2015 ) . MoE   computation ( Shazeer et al . , 2017 ; Fedus et al . ,   2021 ) is an option derived upon the sparse activa-   tion property to increase the scale with only minor   losses in compute efficiency . The other commonly   used one is depth - adaptive computation ( Xin et al . ,   2020 ; Zhou et al . , 2020 ; Goyal et al . , 2020 ; Kim   and Cho , 2021 ) which involves layers into com-   putation adaptively on either example ( alias early   exiting , Xin et al . , 2020 ; Zhou et al . , 2020 ) or to-   ken ( alias token reduction , Goyal et al . , 2020 ; Kim   and Cho , 2021 ) level . A critical distinction between   MoE and depth - adaptive models is that the com-   pute of an MoE model is accurately under control   while that of a depth - adaptive model is not . We   are impelled by the merits of MoE , and propose   aMMEso that the capacity of the student   can be enlarged without much inference overhead   increment .   Additionally , we argue that MMEis orthog-   onal to alternatives mentioned above , and M - MEcan be incorporated to these alternatives   and makes it possible to serve more extreme sce-   narios . It is noteworthy that a certain stream of   work ( Zhang et al . , 2022b ; Zuo et al . , 2022 ) actu-   ally accelerates LMs via precisely converting them   into MoE models . Nonetheless , the moefication   process is directly exerted to LMs with limited in-   ference compute improvements ( cf . MoEBERT in   Figure 1 ) . Contrarily , MMEis comprised of   minimal experts , each of which can be extremely   small . A comparison between mentioned possibili-   ties and MMEis listed in Table 2 . And other   related work of interest is given in Appendix E.45373.2 Implementation   Minimal Language Models Typical language   models are comprised of a stack of transformers   layers ( Vaswani et al . , 2017 ) , and are pretrained   with language modeling tasks such as masked lan-   guage modeling ( Devlin et al . , 2019 ) . A trans-   former layer can be decomposed to a multi - head   self - attention ( MHA ) block and a feed - forward net-   work ( FFN ) block . Concretely , given an n - length   sequence of d - dimension input vectors X∈R   with the i - th vector being x , the output of the MHA   block with Aindependent heads can be represented   as :   MHA ( X ) = /summationdisplayAttn(X;W , W)XWW ,   Attn(X;W , W ) =   softmax ( XWWX / d ) ,   where the j - th head is parameterized by W , W ,   W∈R , and W∈R. On the other   hand , the output of the FFN block is shown as :   FFN(X ) = GELU ( XW)W ,   where two fully - connected layers are parameterized   byW∈RandW∈Rrespectively .   Details like biases , normalizations of a transformer   layer are omitted for brevity .   To reach an acceptable compute budget , pioneer-   ing studies either pretrain language models or distil   ones of small scales from LMs as in Figure 3 . There   are three lines of work in LM distillation : firstly ,   task - specific distillation ( Sun et al . , 2019 ; Li et al . ,   2020 ; Sun et al . , 2020a ; Park et al . , 2021b ; Hou   et al . , 2020 ; Xia et al . , 2022 ) that conducts distilla-   tion on a specific task at finetuning stage ; secondly ,   task - agnostic distillation ( Turc et al . , 2019 ; Sanh   et al . , 2019 ; Sun et al . , 2020b ; Wang et al . , 2021b )   that conducts distillation at pretraining stage ; and   thirdly , two - stage distillation ( Jiao et al . , 2020 ) that   combines the power of both task - agnostic and -   specific distillation . Here , the distilled language   models only refer to language models distilled   with task - agnostic distillation regarding better task-   scalability as the number of concerned tasks ex-   plodes .   We formally define the distilled language models   as minimal language models ( MiniLMs , somehow   abuse of notation with Wang et al . , 2020 ) notated   withS. In contrast , LMs are notated with T. The   learning objective of MiniLMs can be abstracted   asL(S;T , D ) , where Ddenotes the data . The   specific form of Lcan be adapted to arbitrary align-   ment strategies . We adopt a relation alignment   strategy ( Wang et al . , 2021b ) as follows :   L(S;T , D ) = E / summationdisplay   KL(Reln(X;W),Reln(X;W ) )   + KL(Reln(X;W),Reln(X;W ) )   + KL(Reln(X;W),Reln(X;W ) ) ,   Reln(X;W )   = softmax ( XWWX / d ) ,   where KL stands for kullback - leibler divergence .   Essentially , relation heads are derived by merging   the original Aattention heads and then splitting   them to Rheads . Wis the redistributed query   parameter of the j - th relation head within totally   Rheads from the last layer of the LM , likewiseWandWare the key and value parameters .   An auxiliary MHA block is employed as the last   layer of the MiniLM for better alignment follow-   ing Wang et al . ( 2021a ) . The MiniLM can be then   finetuned on any tasks .   Mixture of Minimal Experts Naturally , in or-   der to enlarge the learning capacity gap of the stu-   dent , we should add more parameters to the student .   However , trivially adding parameters usually leads   to a loss of inference compute efficiency .   To remedy this , a mixture of minimal experts   is proposed as in Figure 3 . Following prior litera-   ture ( Shazeer et al . , 2017 , 2018 ) , if we consider a   FFN block in a MiniLM as a minimal expert , then   extra parameters are exactly imposed as minimal   experts to be added to the FFN block . The FFN4538block is enabled as a mixture of mminimal experts   FFNin an expert gating tactic as :   FFN(x ) = p(x)·FFN(x ) ,   p(x ) = exp(xw)/summationtextexp(xw ) ,   k = argmax p(x ) ,   where the j - th gate is parameterized by w∈R ,   and correspondingly the j - th minimal expert is   denoted as FFN . We further follow Fedus et al .   ( 2021 ) to only allow top- onegating ( i.e. , only the   expert with highest gating probability is reserved )   because we want to keep the inference compute un-   touched . There are also diverse designs to achieve   the sparse routing , such as hashing ( Roller et al . ,   2021 ) which we find performs worse ( cf . Figure 5 ) .   Since only one minimal expert is activated dur-   ing the inference , the compute is only negligibly   increased by expert routing . As a complement , we   can also achieve , if necessary , a mixture of experts   in an MHA block similarly .   To encourage a balanced load across minimal   experts , a differentiable load balancing objective   B(S;D)is added from Lepikhin et al . ( 2021 ) as :   B(S;D ) = α·m / summationdisplayf·P ,   f = E[I{argmax p(x ) , j } ] ,   P = E[p(x ) ] ,   where αis a coefficient that should be manually   tune and is kept as 0.01 throughout this work fol-   lowing ( Fedus et al . , 2021 ) . While fdepicts the   fraction of tokens dispatched to the j - th minimal   expert , Pdescribes the fraction of the routing prob-   ability to the j - th minimal expert . And a multiplier   mis used to make the magnitude of the objective   invariant to the number of minimal experts . The   load balancing objective basically desires a uni-   form routing so that the loss can be minimized .   The objective is added to the MiniLM not only at   task - agnostic distillation stage but also at finetun-   ing stage for practical concerns ( cf . Figure 5 ) .   4 Experiments   4.1 Data and Metrics   We conduct experiments on GLUE ( Wang et al . ,   2019 ) and CoNLL ( Sang and Meulder , 2003 ) . TheGLUE originally consists of two sequence clas-   sification tasks , SST-2 ( Socher et al . , 2013 ) , i.e. ,   CoLA ( Warstadt et al . , 2019 ) , and seven sequence-   pair classification tasks , i.e. , MRPC ( Dolan and   Brockett , 2005 ) , STS - B ( Cer et al . , 2017 ) , QQP ,   MNLI ( Williams et al . , 2018 ) , QNLI ( Rajpurkar   et al . , 2016 ) , RTE ( Bentivogli et al . , 2011 ) ,   WNLI ( Levesque et al . , 2012 ) . We exclude WNLI   and CoLA due to the evaluation inconsistency ( in   other words , MiniLMs get dramatically worse re-   sults while LMs get much better ones as found   out in Xia et al . , 2022 ) and use the left tasks .   The CoNLL is a token classification task . Follow-   ing BERT ( Devlin et al . , 2019 ) , we report Accu-   racy ( Acc ) on SST-2 , MNLI , QNLI , RTE , Spear-   man Correlation scores ( SpCorr ) on STS - B , and   F1 on MRPC , QQP , CoNLL . Average score over   tasks from GLUE ( GLUE Score ) is additionally   computed . Results on development sets are re-   ported . GFLOPs are also attached as theoreti-   cal speedup references . We adopt Wikipedia data   for task - agnostic disitllation . The detailed statis-   tics , maximum sequence lengths , and metrics of   GLUE , CoNLL , and Wikipedia are supplied in Ap-   pendix A.   4.2 Hands - on Details   Experiments are conducted upon distilling   BERT and BERT ( Devlin et al . , 2019 ) .   The distillation carried out on eight Nvidia A100s .   The number of relation heads is set to 32 . After the   distillation , finetuning is carried out on one Nvidia   A100 . The number of minimal experts mis default   to 4 otherwise specified . Other details are supplied   in Appendix B. All experiments are task - agnostic   ones , except those in Table 13 .   4.3 Baselines   We compare MMEwith several state - of - the-   art baselines .   Conventional Distillation FT indicates direct   finetuning the student . KD ( Hinton et al . , 2015 ) ,   PKD ( Sun et al . , 2019 ) , and CKD ( Park et al . ,   2021b ) are methods with different distillation ob-   jectives , i.e. , KD directly distills logits , PKD dis-   tills both logits and hidden states , and CKD distills   high - order relations . While above four methods   originally initialize student structures by dropping   layers , we enable them with a global pruning so   that they can adapt to students of small scales . Dyn-   aBERT ( Hou et al . , 2020 ) uses a two - step pruning4539   to regulate student structures and a distillation ob-   jective akin to PKD . MoEBERT ( Zuo et al . , 2022 )   moefies LMs by decomposing FFN blocks to MoE   layers . For these task - specific distillation methods ,   student structures are denoted either withfor pre-   served number of layers in layer - dropping or withfor preserved portion of parameters in pruning .   As aforementioned methods are task - specific dis-   tillation ones , we then introduce task - agnostic ones .   TinyBERT ( Jiao et al . , 2020 ) exploits a distillation   objective distilled with a combination of various   feature alignments . MiniLM ( Wang et al . , 2021b )   straightforwardly utilizes a distillation objective   with a deep relation alignment exactly the same   with ours . Since task - agnostic distillation allows   both dropping layers and hidden dimensions , stu-   dent structures are denoted with accordingly .   Capacity - aware Distillation MiniLM w/   TA ( Wang et al . , 2020 ) specifically incorporates   a teacher assistant to MiniLM . MiniDisc ( Zhang   et al . , 2022a ) argues that the scale of the teacher   assistant is crucial for student performance and   proposes an automatic teacher assistant scheduler   based on properties of pruning . While MiniLM w/   TA is only inspected under a task - agnostic setting ,   MiniDisc offers results under both task - specific   and task - agnostic settings . Nevertheless , only   task - specific MiniDisc is selected since prunedMiniLMs can be unfair to compare with . There   is scarce work in this direction in which we find   these two are the most comparable ones .   4.4 Main Results   From results in Table 3 , we find that MME   lifts the curse of capacity gap at all concerned times   of compression . For example , MME   disitlled from BERT has an absolute 0.5 per-   formance gain over that distilled from BERT   on GLUE , and the value on CoNLL is 0.9 . On   another note , MiniLM is free of the curse only   at small times of compression , and MiniLM w/   TA can somewhat saves MiniLM from the curse   at intermediate times of compression . For exam-   ple , both MiniLM and MiniLM w/   TA fail to improve the performance via replacing   BERT with BERT . Results on larger LMs   like BERT are supplied in Appendix F for   scalability check .   From results in Table 4 , we also observe that   MMEgenerally outperforms both conven-   tional and capacity - aware baselines and achieves   new state - of - the - art performance at all concerned   times of compression . For example , M-   ME has an absolute 0.8 performance im-   provement over MiniLM on GLUE . And   the reason why MME slightly under-4540   performs TinyBERT is conjectured due to   structure discrepancy . Another observation is that   the larger times of compression , the larger the per-   formance improvements are . For example , M - ME yields an absolute 0.5 performance   improvement over MiniLM in contrast to   that MME only has an absolute 0.2   performance improvement over MiniLM   on GLUE . Two more notes are that , MoEBERT   nearly reaches the compression upper bound , and   TinyBERT is reproduced without additional task-   specific distillation for a fair comparison while the   results with additional task - specific distillation are   supplied in Appendix C.   4.5 Analyses   Practical Inference Compute Since GFLOPs   can only measure the theoretical inference com-   pute , we further provide throughput ( i.e. , tokens   per micro second ) as a practical inference computemeasure . As in Table 5 , 20 ×compression can real-   ize a significant inference compute gain in compar-   ing KDto BERT . The practical speedup is   approximately 6.7 ×. Moreover , MME   can retain most inference compute gain even if the   routing algorithm can slightly reduce the gain when   compared to MiniLM . Although MME   is seemingly memory - inefficient regarding the in-   creased parameter amount , we argue the potential   of a memory - efficient MMEwith parameter   decomposition in Appendix G.4541Student Scale Following the behavior of Fig-   ure 2 , we would like to showcase whether M-   MEcan lift the curse across difference student   scales . From Figure 4 , the curse is lifted to a large   extent by MMEin comparison with MiniLM   and MiniLM w/ TA . However , MMEmeets a   bottleneck that distilling BERT makes no dif-   ference from distilling BERT when the FLOPs   is at an extreme value 0.04 G ( ∼273×compres-   sion from BERT,∼968×compression from   BERT ) . We explore the extreme case by plug-   ging a TA to MMEas supplied in Appendix D.   Routing Algorithm Routing algorithm is also a   crucial part benefiting from a nice design choice .   We compare our used gating with another fancy   choice hashing . We at the same time show the   effect of using load balance at finetuning stage as   well . From the results in Figure 5 , we see that   gating outperforms hashing , and load balancing at   both distillation and finetuning stages is superior to   that at only distillation stage .   Expert Number Regarding the expert number m   is a parameter of great importance for MME ,   we here study its impact on the performance . The   results in Figure 6 reveal a first ascending then de-   scending phenomenon while adding experts at a   time . The phenomenon suggests there is a trade-   off when increasing the number of experts , and weconjecture the tradeoff accords with the famous   bias - variance tradeoff ( Hastie et al . , 2001 , Chapter   7 ) . That is , adding experts grows the parameter   scale , thus decreasing bias yet increasing variance .   Another interesting notice is that smaller students   favor fewer experts . Based on the tradeoff con-   jecture , we hypothesize that smaller students are   more sensitive to variance increment , as the biases   of smaller students can arrive at a minimum more   quickly than those of larger ones .   5 Conclusions   In this work , we uncover a curse of capacity gap   in LM distillation , which is well discussed in pre-   vious studies on vision model distillation but not   recognized in distilling LMs . While there are some   studies investigating to fill the gap , we find they   can hardly tackle the curse . Interestingly , existing   solutions in large vision language model distilla-   tion which are stated to be able to lift the curse fail   to achieve so for LMs . So we aim at lifting the   curse by proposing a well - motivated MME .   TheMMEcan essentially enlarge the capac-   ity of the student but leave the inference compute   nearly untouched . Our experimental results indi-   cate that MMEcan not only lift the curse but   also realize new state of the arts .   Limitations   The central limitation of MMEis the increased   memory footprint , which we could potentially ad-   dress in the near future according to Appendix G.4542Acknowledgements   We thank the anonymous reviewers and chairs for   their constructive suggestions . This research was   supported in part by Natural Science Foundation of   Beijing ( grant number : 4222036 ) and Huawei Tech-   nologies ( grant number : TC20201228005 ) . Jin-   gang Wang is funded by Beijing Nova Program   ( grant number : 20220484098 ) .   References454345444545   A Data Summary   The detailed statistics , maximum sequence lengths ,   and metrics for datasets we use are shown in Ta-4546ble 6 , where the Wikipedia corpus used for distilla-   tion is also attached .   B More Hands - on Details   General Guidelines The details of hyperparam-   eters for distillation and finetuning are shown in   Table 7 . We will be releasing our code and scripts   in the final version for exact reproducibility . For   all cases , students are always randomly initialized   following MiniLM .   Implementation of MiniMoE We strictly fol-   low the design of SwitchTransformer ( Fedus et al . ,   2021 ) and extend it to the design of our M-   ME . We also follow their associated appendices   to implement an MoE for multihead attention . In   detail , based on the original design , we treat an   FFN / MHA as an minimal expert , adopt top- one   gating with load balancing , and employ a capac-   ity factor of 1.25 for a good tradeoff ( where over-   flowed tokens are dropped ) . For the parameter   effect of adding an expert , we take expanding   MiniLM ( 11.3 M ) to MiniMoE -1,2E   ( 14.9 M ) as an example . The number of parame-   ters for embeddings is not changed ( 6.0 M →6.0 M ) ,   but adding an expert ( 1,1E→1,2E ) results in an   increased number of parameters for transformers   ( 5.4M→9.0 M ) .   Further , our design for HashLayer ( Roller et al . ,   2021 ) also strictly follows the original random hash   design , i.e. , per - token hash is used . We strictly   follow the best configuration of DeKD as reported   in their paper ( Zhao et al . , 2022 ) , where αis 1.0   andβis 8.0 .   C Results w/ Task - specific Distillation   The results with task - specific distillation are pro-   duced from released checkpoints . The results in   Table 8 demonstrate that TinyBERT is largely sup-   ported with data augmentation in the task - specific   distillation stage for great performance . Another   intriguing observation is that data augmentation   only works for distillation but not for finetuning po-   tentially due to the noise - resilience of distillation ,   so we preferably replace the finetuning stage with   a task - specific distillation stage in experimenting   with MiniLM .   D MME at Extreme   The results in Table D witness that , MME   sometimes struggles with extreme cases but can beenhanced with the help of TA .   E Related Work   Knowledge Distillation Distillation ( Hinton   et al . , 2015 ) is a de facto way to compression ( Bu-   cila et al . , 2006 ) LMs by transferring the knowledge   of LMs to small language models . During the distil-   lation , a small language model serves as a student   and treats a LM as a teacher to learn from . There   are three lines of work in LM distillation : firstly ,   task - specific distillation ( Sun et al . , 2019 ; Li et al . ,   2020 ; Sun et al . , 2020a ; Park et al . , 2021b ; Hou   et al . , 2020 ; Xia et al . , 2022 ) that conducts distilla-   tion on a specific task at finetuning stage ; secondly ,   task - agnostic distillation ( Turc et al . , 2019 ; Sanh   et al . , 2019 ; Sun et al . , 2020b ; Wang et al . , 2021b )   that conducts distillation at pretraining stage ; and   thirdly , two - stage distillation ( Jiao et al . , 2020 ) that   combines the power of both task - agnostic and -   specific distillation . Though these methods real-   ize promising performance when distilling LMs   like BERT , they can come short of scalabil-   ity to LMs like BERT especially when the   student is of a small scale . In fact , driven by re-   cent observations ( Wang et al . , 2020 ; Zhang et al . ,   2022a ; Mirzadeh et al . , 2020 ; Cho and Hariha-   ran , 2019 ) , distillation with a small student can be   faced with two deficiencies due to the large capac-   ity gap . A few studies including teacher assistant-   based ( Mirzadeh et al . , 2020 ; Zhang et al . , 2022a )   and student - friendly ( Park et al . , 2021a ; Zhou et al . ,   2022 ) distillation can alleviate the first but fail to   resolve the second . It is noteworthy that some work   states they can tackle both deficiencies for vision   models ( Zhu and Wang , 2021 ; Zhao et al . , 2022 ) ,   but preliminary studies have found that they are ei-   ther expensive or not capable of LMs . In our work ,   we follow the line of task - agnostic distillation of   LMs and aims at lifting both efficiencies for the   first time .   Mixture of Experts Based on the idea of condi-   tional computation ( Bengio et al . , 2015 ) , MoE layer   is proposed to scale - up LMs in a sparsely activated   fashion ( Shazeer et al . , 2017 ) . There are diverse   designs to achieve the sparse routing , such as gat-   ing ( Shazeer et al . , 2018 ) and hashing ( Roller et al . ,   2021 ) , with necessary balance constraints ( Lep-   ikhin et al . , 2021 ) . MoE layers are then joined   to LMs in the past one or two years ( Fedus et al . ,   2021 ; Du et al . , 2022 ) . Owing to the sparse activa-   tion property , the scales of LMs are significantly4547   increased with only minor losses in compute effi-   ciency on modern GPU devices so that the under-   neath scaling laws can be uncovered in a compa-   rably cheap manner ( He et al . , 2021 ; Rajbhandari   et al . , 2022 ) . In our work , we are impelled by   the merits of MoE , and propose a MMEso   that the capacity of the student can be enlarged   without much inference overhead increment . M - MEcan be similar to a certain stream of meth-   ods ( Zhang et al . , 2022b ; Zuo et al . , 2022 ) that pur-   sue accelerating LMs via precisely moefying them .   Nonetheless , the moefication process is exerted to   LMs with limited inference compute improvements   compared to those advanced by MME . Note   that there are emergent work exploring compress-   ing MoE LMs ( Xue et al . , 2022 ) to dense students ,   which is walking down the same street in the op-   posite side since we instead focus on compressing   dense LMs to MoE students .   F Results on BERT   LM distillation , under either the task - agnostic set-   ting as in our paper or the task - specific setting , has   seldom been investigated to distil LMs larger than   BERT . Even worse , there is only little work   has been investigated to distil BERT under thetask - agnostic setting .   In the main results , we just follow the paces of   the task - agnostic setting , not only due to the huge   scales of larger LMs like T5 and GPT3 but also   due to that task - agnostic LM distillation requires   the access to the original pretraining data of usu-   ally vast volume . What ’s more , larger LMs like   T5 can be incomparable to BERT owing to the ar-   chitectural difference , and existing task - agnostic   methods including ours may easily fail .   Regarding all the considerations mentioned   above , however , we try to check the existence of the   curse of capacity gap and examine MMEun-   der a comparably larger - scale setting , i.e. , Chinese   BERTv.s . BERT on some datasets from   CLUE ( Xu et al . , 2020 ) ( which can be viewed as   the Chinese GLUE ) . These datasets include a topic   classification dataset TNews , a similar question   matching dataset AFQMC , and a natural language   inference dataset OCNLI . The preliminary results   are shown in Table 10 . As far as we know , while   English BERT with more than one billion   parameters trained by Nvidia Megatron ( Shoeybi   et al . , 2019 ) is not publicly available , Chinese   BERT can be easily downloaded through hug-4548   gingface . It is noteworthy that Chinese BERT   is trained on Chinese Wikipedia ( ∼15 G ) while   Chinese BERT is trained on Wudao Corpus   ( ∼300 G ) ( Yuan et al . , 2021 ) . We use Wikipedia   data as the default choice for distillation , but Wu-   dao data seems to be a more suitable ( though not   that fair ) one for distilling Chinese BERT as   we have found that Wikipedia could not make the   distillation converge properly . Painfully , it con-   sumes around one week to achieve one epoch of   distilling Chinese BERT using Wudao in con-   trast to five epochs of distilling Chinese BERT   on Wikipedia in one day . The results show that Chi-   nese BERT is cursed to realize better studentsthan Chinese BERTdoes , and MMEhas   the potential to lift the curse under the larger - scale   setting .   G Potential of Memory - efficient   MME   One may argue that MMEintroduces much   more memory consumption than MiniLM does ,   largely limiting the application scenarios for   memory - sensitive devices ( e.g. , mobile devices ) .   However , there is no free lunch to enlarge the   capacity of the student . We should claim that ,   in order to increase the capacity , memory / space   consumption is a cheaper choice ( e.g. , more ex-   perts ) than latency / time consumption ( e.g. , more   operations ) , and this is potentially the reason why4549   large LMs like PaLM ( Chowdhery et al . , 2022 ) and   FLAN ( Chung et al . , 2022 ) could become so pop-   ular . We should also highlight that scenarios that   require rather limited memory consumption ( e.g. ,   mobile scenarios ) is currently not ( though can be   in the near future ) the main concern of LMs . In   contrast , LMs are usually served in GPU scenarios ,   where memory / space is easy to access .   Luckily , we find a potential path to address the   memory efficiency concern based on the idea of   parameter decomposition ( e.g. , SVD ) . While em-   bedding parameter decomposition is a general way   to reduce the number of parameters for embed-   dings and could not make MMEas memory-   efficient as MiniLM . We uncover that , without   much performance sacrifice , transformer parameter   decomposition in MMEcan be easier in com-   parison with that in MiniLM owing to the sparse   activation property of MoE. That is , transformer pa-   rameters in MMEhave lower ranks than those   in MiniLM , and this can be shown by analyzing   the magnitudes of the normalized singular values   using SVD . The preliminary results of the output   matrices of the last FFN layers separately from   MiniLM andMME are shown   in Table 11 .   With this finding , MMEcan compress more   parameters than MiniLM does using parameter de-   composition and finally yield a similar memory   efficiency to that of MiniLM .   We also explore another complementary solu-   tion that views MMEas teacher assistant and   further distils from MMEto its dense counter-   part . The results are shown in Table 12 , implying   that this only results in an acceptable performance   degradation on large datasets like MNLI and SST-2   but undesired performance degradation on small   datasets like RTE.H Failure of Vision Method   We examine in a preliminary study the effective-   ness of one of the vision model distillation meth-   ods ( DeKD , Zhao et al . , 2022 ) which can lift the   curse of capacity gap . From the results in Table 13 ,   we unfortunately discover that DeKD can only give   comparable performance in distilling BERT ,   which even lags behind KD w/ TA . It hints that vi-   sion model distillation methods are not that capable   of LMs.45504551ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   the section following the conclusions .   /squareA2 . Did you discuss any potential risks of your work ?   not any known risks .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   the introduction .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   the experiments .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   the results in the experiments.4552 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   the hands - on details in the experiments .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   the results in the experiments .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   not related packages used .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.4553