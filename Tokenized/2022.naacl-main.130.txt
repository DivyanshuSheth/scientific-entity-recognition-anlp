  Marinela Parovi ´ cGoran GlavašIvan Vuli ´ cAnna KorhonenLanguage Technology Lab , TAL , University of CambridgeCAIDAS , University of Würzburg   { mp939,iv250,alk23}@cam.ac.uk goran.glavas@uni-wuerzburg.de   Abstract   Adapter modules enable modular and efficient   zero - shot cross - lingual transfer , where current   state - of - the - art adapter - based approaches learn   specialized language adapters ( LAs ) for indi-   vidual languages . In this work , we show that   it is more effective to learn bilingual language   pair adapters ( BAs ) when the goal is to op-   timize performance for a particular source-   target transfer direction . Our novel-   adapter framework trades off some modularity   of dedicated LAs for improved transfer per-   formance : we demonstrate consistent gains in   three standard downstream tasks , and for the   majority of evaluated low - resource languages .   1 Introduction   Massively multilingual Transformers ( MMTs ) such   as mBERT ( Devlin et al . , 2019 ) , XLM - R ( Con-   neau et al . , 2020 ) , and mT5 ( Xue et al . , 2021 )   have dominated research in multilingual NLP and   cross - lingual transfer recently . Pretrained on large   amounts of unlabelled data in 100 + languages ,   they have been shown to achieve impressive perfor-   mance for a wide range of languages and tasks , and   in zero - shot cross - lingual transfer in particular ( Wu   and Dredze , 2019 ; K et al . , 2019 ) . However , their   representational capacity is known to be limited by   the curse of multilinguality : a trade - off between the   language coverage and model capacity ( Conneau   et al . , 2020 ) , which typically favors high - resource   languages . Their limitations are thus especially   pronounced in low - resource scenarios , in transfer   between distant languages and towards resource-   poor target languages ( Hu et al . , 2020 ; Lauscher   et al . , 2020 ; Ansell et al . , 2021 , inter alia ) .   A standard approach to zero - shot cross - lingual   transfer with MMTs ( i ) fine - tunes the full MMT   on task - specific data in the source language and   then ( ii ) applies it directly to make predictions in   the target language ( Hu et al . , 2020 ) . On top of   the expensive fine - tuning of the entire large model , this standard procedure also does not ‘ prepare ’ the   MMT to excel at a particular target language or   for a particular source - target transfer direction .   This has been alleviated through modular   parameter - efficient adaptations of the MMTs   ( Bapna and Firat , 2019 ; Philip et al . , 2020 ; He   et al . , 2021 ) which bypass full fine - tuning , most   prominently through lightweight adapters ( Rebuffi   et al . , 2017 ; Houlsby et al . , 2019 ): additional train-   able parameters inserted into the MMT ’s layers .   They have recently been used for language and   task specialization of the MMTs ( Pfeiffer et al . ,   2020b ) , offering improved and more efficient zero-   shot cross - lingual transfer .   Previous work ( Pfeiffer et al . , 2020b ; Üstün et al . ,   2020 , 2021 ; Vidoni et al . , 2020 ; Ansell et al . , 2021 ,   inter alia ) focused on creating : 1)dedicated lan-   guage adapters ( LAs ) for each individual language ,   and2)individual task adapters ( TAs ) . Creating   single - language LAs enables a very modular ap-   proach to cross - lingual transfer , where a source   language LA ( used in training ) can be directly   swapped with any target language LA at inference .   Yet , this procedure still does not prepare nor adapt   the MMT for a particular source - target transfer di-   rection . Put simply , if one ’s incentive is to optimize   the performance of a particular target language L   given annotated data in a particular source language   L , especially under low - data regimes , one might   try to capture the interplay between the two lan-   guages instead of learning separate LAs .   To address this gap , in this work we introduce   the - framework : bilingual adapters ( BAs )   for zero - shot cross - lingual transfer ( see Figure 1 ) ,   designed towards improving transfer performance   for a particular transfer direction , with a focus on   low - resource target languages . The goal of-   is to specialize the MMT for a particular language   pair , while preserving all its existing knowledge   encoded into the MMT ’s parameters .   We experiment with three standard tasks in cross-1791lingual transfer ( Lauscher et al . , 2020 ; Ansell et al . ,   2021 ): part - of - speech tagging ( POS ) , dependency   parsing ( DP ) and natural language inference ( NLI ) ,   and with a total of 20 low - resource target languages .   Our results demonstrate that trading off modularity   of single - language LAs for less modular BAs ( tai-   lored for language pairs ) indeed yields improved   transfer performance over the current state - of - the-   art ( SotA ) adapter - based transfer framework -(Pfeiffer et al . , 2020b ) , in all three tasks and for   the large majority of target languages . We also   show that , under the fixed fine - tuning budget and   resources , further task performance gains can be   achieved by varying the ratio of L - vs - Lunan-   notated data when learning BAs . Finally , aiming   to delve deeper into the trade - off between modu-   larity and training efficiency , we experiment with   multilingual adapters that are trained on the source   language and all target languages under considera-   tion at once . We show that such adapters , despite   being more efficient to train , are unable to match   the performance of their more specialized counter-   parts across a diverse set of target languages .   We share our code and pretrained BAs online at :   https://github.com/parovicm/BADX .   2- : Methodology   Motivation and Overview . The main idea can be   summarized into the following : instead of adapting   the MMT to languages LandLseparately as   done in the SotA adapter - based -framework   ( Pfeiffer et al . , 2020b ) , cross - lingual transfer might   be more effective by adapting the MMT directly   to the language pair ( L , L ) . This means that we   learn a bilingual language - pair adapter instead of   two separate monolingual LAs . We then learn a   task adapter directly on top of the BA : since we   focus on the zero - shot setting , this means using   task - annotated examples only from Lto fine - tune   the TA . This procedure is summarized in Figure 1.-Adapters.-adapts the -   adapter framework , where BAs are learnt instead   of single - language LAs . The architecture of the   adapter in each layer lconsists of a down- and   up - projection with a residual connection . More   specifically , let the down - projection be a matrix   D∈Rand the up - projection be a matrix   U∈Rwhere his a hidden size of the MMT   anddis the hidden size of the adapter . Let us de-   note MMT ’s hidden state and the residual at layer l   ashandr , respectively . The adapter computation   of layer lis then given by :   with ReLU as the activation function . This for-   mulation subsumes LAs and TAs in - , as   well as BAs and TAs in- , where LAs / BAs   receive the input from the ( frozen ) Transformer   layer , while TAs receive the input from the ( frozen )   LA / BA put on top of the frozen Transformer layer   ( Figure 1 ) . -LAs are trained via masked - language   modeling ( MLM ) objective on the Wikipedia of   the corresponding language , while TAs are trained   on annotated task data . Once LA for Lis avail-   able , TA is trained by stacking it on top of the fixed   source LA . Transfer is done by replacing the L   LA with the LLA . Unlike -,-trains   a single bilingual adapter via MLM , alternating be-   tween the unlabelled ( Wikipedia ) data from both   LandL. The ‘ data alternations ’ are done ac-   cording to a predefined ratio : e.g. , the ratio of N:1   denotes that the model would see N Lsentences   followed by 1 Lsentence . The motivation for this   is twofold : 1)seeing a data mixture from the two   languages could produce a BA that is better for   transfer than having two independent LAs ; 2)LAs   for low - resource L - s might otherwise overfit due   to unlabelled data scarcity in L , and thus could   benefit from additional Ldata .   In- , TA is then again trained on top of1792the fixed BA , and the same BA - TA configuration is   retained at inference , see Figure 1 again .   Advantages and Limitations.-allows   parameter - efficient transfer to arbitrary tasks and   languages by learning modular bilingual and task   representations . It trades - off some modularity of -for increased performance and expressive-   ness when the goal is to perform a transfer for a   fixed pair of languages . A disadvantage of-   with respect to modularity is that it no longer of-   fers a zero - cost transfer ( once all LAs are learnt )   between all language pairs under consideration : it   requires training of separate BAs for all pairs of in-   terest . However , as we show further in § 3,-   might be preferable over -in the cases when   the goal is to improve a particular source - target   direction , which is our targeted use - case .   3 Experiments and Results   Tasks and Languages . We evaluate BAD - X on   three standard cross - lingual tasks which allow   for experimentation with low - resource target lan-   guages : POS , DP , and NLI . For POS and DP , we   sample ten low - resource languages from the Uni-   versal Dependencies ( UD ) 2.7 dataset ( Zeman et al . ,   2020 ) , taking into account : 1)the availability and   the size of the corresponding Wikipedia ; and 2 )   typological diversity to ensure that different lan-   guage families are covered . For NLI , we rely on   the recent AmericasNLI dataset ( Ebrahimi et al . ,   2022 ) , spanning ten low - resource languages from   the Americas . For AmericasNLI languages , we use   Wikipedia if available ; otherwise we use the unla-   belled data previously used by Ansell et al . ( 2022 ) .   English is the source language in all experiments   for all tasks . All languages along with their lan-   guage codes are listed in Table 3 in the Appendix .   3.1 Experimental Setup   MMT . In all our experiments , we use mBERT , an   MMT model pretrained on the Wikipedias of 104   languages ( Devlin et al . , 2019 ) .   Training Setup : LAs , BAs . To enable a fair   comparison between -and - under thesame training and inference conditions , we train   our own -LAs from scratch with the MLM   objective on monolingual Wikipedias : training is   run for 25,000 steps , with a batch size of 64 and a   learning rate of 1e−4 . We evaluate the LAs every   500 training steps and finally choose the LA that   yields the lowest perplexity , as evaluated on the 5 %   of the Wikipedia data that acts as a validation set .   Pfeiffer et al . ( 2020b ) empirically established   that strong task performance of -on low-   resource languages can be achieved already after   20,000 LA training steps , and that longer train-   ing offers only modest to negligible performance   gains . Driven by their findings , we train -   LAs for 25,000 iterations due to computational   constraints , a large number of experiments , and the   low - resource nature of our target languages.-BAs are trained on the Wikipedia data   of both LandL. The standard - vari-   ant termed Balanced-(also-1:1 ) is   trained by alternating one batch of the Ldata ( i.e.   English ) followed by one batch of the Ldata , for   50,000 iterations ( i.e. , this way we match the total   number of iterations performed by training -LandLLAs for 25,000 iterations each ) , and   we adopt all the hyperparameters from -LA   training . We select as the final BA the one with   the lowest Lperplexity . Bilinguality of the-   BAs allows us to directly train TA on top of it and   perform the inference with the same configuration .   Multilingual Adapter ( MA ) . Given Ntarget lan-   guages of interest , one could alternatively train a   multilingual adapter on unlabelled text from L   and all Ntarget languages L : while this is more   computationally efficient than both BAD - X and   MAD - Xit could , presumably , again lead to the   “ curse of multilinguality ” , as the adapter parame-   ters would be shared across N+1 languages . On   the other hand , MA has the chance to benefit from   similarities between target languages ( especially   in the case of AmericasNLI ) . Concretely , we train   two multilingual adapters : one for the set of UD   languages and the other for the set of AmericasNLI   languages . Multilingual UD adapter is trained by   alternating one batch of English Wikipedia and one   batch from each of 10 UD languages ’ Wikipedia   for 50,000 iterations . Multilingual AmericasNLI1793adapter is obtained following the same procedure ,   only using Wikipedias of the NLI target languages .   Training Setup : TAs . For POS and DP , TA is   trained by stacking it on top of the source ( i.e. En-   glish ) LA ( with - ) , the English- LBA ( with- ) or the multilingual adapter MA and per-   forming 15,000 steps with a batch size of 8 and a   learning rate of 5e−5 . We evaluate the TAs every   250 steps on English validation set , and select as   the final TAs the ones with the best accuracy ( POS )   and LAS score ( DP ) . The adapter reduction factor   ( Pfeiffer et al . , 2020a ) is 2 for LAs and BAs and 16   for TAs . For AmericasNLI , we train its TA using   the English MultiNLI data ( Williams et al . , 2018 )   following the setup of Ebrahimi et al . ( 2022 ): 5   epochs with a batch size of 32 , and a learning rate   of2e−5 . We evaluate the TA every 625 steps and   choose the one with the best English validation   accuracy.- : BA Variants . Besides Balanced- , we consider other variants of - BAs that   differ in the data ratios between LandL ; we   denote these variants as-1 : N , where 1 batch   ofLdata is followed by Nbatches of Ldata ,   and vice versa:-N:1 . With these variants ,   we aim to answer the following question : given a   fixed number of MLM training steps ( i.e. , a fixed   computational budget ) for BAs , is it possible to   further impact / improve transfer performance ? Is   the optimal data sampling ratio task - dependent ?   3.2 Results and Discussion   The results for all languages and tasks with -   and Balanced - are summarized in Table 1 ,   with additional results in the appendix . As a gen-   eral trend , we observe that the proposed Balanced - variant outperforms -and MAs over   a majority of languages and across all three tasks :   besides offering higher average results , we also   report gains on 8/10 ( POS ; accuracy ) , 9/10 ( DP ;   UAS ) , and 8/10 ( NLI ; accuracy ) target languages .   This confirms the positive impact of BA training ,   which is able to capture additional interactions of   each language pair , in lieu of LA training .   Performance across Tasks . In particular,-   gains on average 1.06 % in accuracy and 0.66 % in   Fcompared to -on POS task . It outscores   the multilingual adapter on POS even more : 3.55 %   in accuracy and 2.87 % in F1 on average . The gains   over -are even more pronounced on the   more complex DP task , which shares the target lan - guages set with POS:-outperforms -   on average with a gap of 2.62 % in UAS and 2.38 %   in LAS scores . The gain is particularly high for   Wolof , a West - African language spoken by more   than five million people , with ~9 % improvement   over -in both UAS and LAS scores . Wolof   is also a language with one of the highest gains   in POS . In the DP task,-achieves similar   gains over the multilingual adapter : 2.22 % UAS   and 2.82 % in LAS scores on average . Multilin-   gual adapter achieves high scores on Wolof , which   re - establishes Wolof as a language that highly bene-   fits from the involvement of other languages during   training . We also observe the superiority of Bal-   anced - over -on NLI , now on another   set of low - resource languages , with average accu-   racy gains of 2.4 % . The highest improvement of   6.67 % is observed for Wixarika .   Performance across Languages . Importantly , we   find that improvements in all three tasks are met   for target languages coming from diverse language   families ( e.g. , for Uralic , Indo - European , Niger-   Congo , Turkic , Aymaran families ) and with diverse   typological traits . We speculate that stacking TAs   on top of BAs instead of an English - specialized LA   forces the model to also take into account informa-   tion from the target language , which mitigates over-   fitting to English - only language properties . Further-   more , coupling two languages in the BA training   might also allow for some information flow be-   tween the languages ( e.g. , some sharing at lexical   level ) . This also might provide a positive impact on   transfer performance , while this effect can not be   achieved with individual LAs as in - . Multi-   lingual adapters lag behind -and - as   they aim to fit too many languages into a small num-   ber of adapter parameters , which demonstrates the   necessity of language and especially language - pair   specialization when performance for a particular   source - target transfer direction is paramount.-Variants . Figure 2 shows the ‘ average-   across - languages ’ scores for -and for all   tested - variants ( based on data sampling ra-   tios at BA training ; § 3.1 ) . The results indicate   several findings . First , all - variants outper-   form -on all three tasks on average . Second ,   there is no single best - performing - variant   for all tasks , that is , the ‘ winning ’ variant seems to   be task - dependent . In particular , DP benefits the   most from 5:1 sampling , while for POS and NLI   the 1:2 variant outscores the others although DP1794   and POS share exactly the same BA training data .   Note that , due to computational constraints , we   did not extensively search for the best sampling   ratios of the source and target language during BA   training , thus the optimal strategy might not be cov-   ered by our experiments . However , these findings   warrant further investigation in future work .   Multiple Runs . To validate that our results hold in   the presence of different parameter initializations ,   we perform a comparison of -and Balanced - when the scores are averages of 3 runs with   the same random seeds both for -and- . Due to computational constraints , we select   only a subset of languages for this evaluation . In   particular , we choose 4 UD ( , , and ) and 4 AmericasNLI languages ( , ,   and ) and compare -and Balanced - by taking the average scores obtained from 3   runs . The results are shown in Table 2 , and again   point to- ’s superiority over -in terms   of transfer performance in all three tasks .   4 Conclusion   We have presented- , a novel adapter - based   framework for zero - shot cross - lingual transfer.-targets improving transfer performance for   particular fixed source - target transfer directions   through the introduction and use of dedicated bilin-   gual language - pair adapters ( BAs ) . The effective-   ness of the BAs and the - framework has   been demonstrated on three standard transfer tasks ,   across a plethora of low - resource languages . In fu-   ture work , we will experiment with more efficient   approaches to bilingual adapters , e.g. , based on con-   textual parameter generation ( Ansell et al . , 2021 ) ,   and port the - framework to more languages   and tasks .   AcknowledgementsThis work has been supported by the ERC PoC   Grant MultiConvAI ( no . 957356 ) and a Huawei   research donation to the University of Cambridge .   Marinela Parovi ´ c is supported by Trinity College   External Research Studentship . We would like to   thank Alan Ansell and Jonas Pfeiffer for interesting   discussions and suggestions.1795References17961797A Details of the Experimental Setup   Computing Infrastucture . All experiments were   run on a single NVIDIA GeForce RTX 3090 GPU ;   training one - BA or multilingal LA for   50,000 iterations took around 24 hours ( -   LA for 25,000 steps took around 12 hours ) . Train-   ing of any TA took less than two hours . Evaluation   is performed within the AdapterHub framework   ( Pfeiffer et al . , 2020a ) .   Hyperparameters . All hyperparameters were   taken from ( Pfeiffer et al . , 2020b ) , as discussed   in the main paper , and no hyperparameter search   was done . All reported results except those in table   2 are from a single run .   B Languages   The list of languages in each task along with their   language codes is provided in Table 3 .   C- : Full results   Full results on all languages for -and all - variants are given in Tables 4 , 5 and 6 for   POS , DP and NLI , respectively.17981799