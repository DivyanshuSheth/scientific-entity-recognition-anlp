  Hai Ye Qizhe Xie Hwee Tou Ng   Department of Computer Science , National University of Singapore   { yehai,qizhex,nght}@comp.nus.edu.sg   Abstract   In this work , we study multi - source test - time   model adaptation from user feedback , where K   distinct models are established for adaptation .   To allow efﬁcient adaptation , we cast the prob-   lem as a stochastic decision - making process ,   aiming to determine the best adapted model   after adaptation . We discuss two frameworks :   multi - armed bandit learning and multi - armed   dueling bandits . Compared to multi - armed ban-   dit learning , the dueling framework allows pair-   wise collaboration among Kmodels , which is   solved by a novel method named Co - UCB pro-   posed in this work . Experiments on six datasets   of extractive question answering ( QA ) show   that the dueling framework using Co - UCB is   more effective than other strong baselines for   our studied problem .   1 Introduction   Large language models ( LLMs ) can be ﬁne - tuned   or prompted with texts to achieve good perfor-   mance in NLP tasks ( Devlin et al . , 2019 ; Brown   et al . , 2020 ; Ouyang et al . , 2022 ) . However , be-   cause of the unexpected distribution shift at test   time , the effectiveness of LLMs can degenerate   ( Wang et al . , 2021c ) . They may also generate out-   puts that are untrustworthy or toxic and fail to meet   user expectations ( Ouyang et al . , 2022 ) . One criti-   cal issue that we need to address is to improve the   generalization ability of LLMs . Recent research on   test - time adaptation ( TTA ) suggests a possible way   to do this , by continually updating the deployed   model with target data from an arbitrary test distri-   bution ( Wang et al . , 2021a ) .   Interacting with users is important during test-   time adaptation . First , user feedback allows the   model to better align with humans ( Stiennon et al . ,   2020 ; Ouyang et al . , 2022 ) . Users can directly   teach the model to learn by interaction so that the   Figure 1 : The illustration of multi - source test - time   adaptation from user feedback studied in this work .   Each model is trained from a distinct source do-   main . With unlabeled test data , models are adapted   online from user feedback .   model can be better trained to follow human in-   structions and reduce the generation of toxic and   harmful content . Besides , obtaining feedback from   users can also reduce the cost of data annotation   by experts , and the collected data will be more   in line with the distribution of the users ( Nguyen   et al . , 2017 ; Gao et al . , 2022 ) , which makes the   adaptation more economical and effective .   Leveraging multiple learned models of tasks is   also important for TTA . As in previous work , uti-   lizing multiple known tasks helps the model better   learn new tasks ( or distributions ) , such as meta-   learning ( Hospedales et al . , 2022 ) and multi - source   domain adaptation ( Ramponi and Plank , 2020 ) .   To take advantage of known tasks , compared to   reusing task data , directly using their learned mod-   els has gained popularity recently ( Pfeiffer et al . ,   2021 ; Wang et al . , 2021b ) , which is much cheaper   for online adaptation and has better data privacy   protection ( Kundu et al . , 2020 ) . Recent work on   lightweight tuning empowers LLMs to store knowl-   edge of a large number of tasks cheaply ( Houlsby   et al . , 2019 ; Liu et al . , 2021 ) . Platforms like Hug-   gingface ( Wolf et al . , 2019 ) also allow users to   share locally trained models , promoting a large   amount of knowledge stored as models in the cloud.9647So , it has become more critical for TTA to adapt   from multiple learned models of tasks .   Based on the above discussion , we propose to   study an important but under - explored problem –   multi - source test - time adaptation from user feed-   back – where Ksource models are given , each   trained from a distinct source domain , to adapt to   a new target domain ( Figure 1 ) . Previous work on   leveraging multiple knowledge sources is to learn   an ensemble ( Guo et al . , 2018 ; Ahmed et al . , 2021 ) ,   which means jointly accessing all the models is   needed for training . Due to its high cost , it is not   suitable for real - time updates required by TTA . In   order to adapt efﬁciently , we turn this problem into   a stochastic decision - making process that trades   off model exploration and exploitation . We aim to   determine the best adapted model that can perform   well in the target domain .   We formulate the problem in two frameworks :   multi - armed bandit learning and multi - armed duel-   ing bandits ( Kuleshov and Precup , 2014 ) . Bandit   learning samples one source model each time to   receive binary feedback ( /thumbs_up_altor / thumbs_down_alt ) ( § 4 ) . However ,   it lacks collaboration among sources and can result   in a sub - optimal adapted model . In order not to   introduce too much cost , pairwise collaboration be-   tween models is explored in dueling bandits ( Yue   et al . , 2009 ) , where two distinct source models are   chosen each time for dueling with user preference   feedback ( e.g. , /chevron_sign_right//chevron_sign_left ) ( § 5 ) . A novel method , Co-   UCB , is proposed to allow collaborative updates .   We choose to study the task of extractive ques-   tion answering ( QA ) , since there are large datasets   in different domains that can be used ( Fisch et al . ,   2019 ) . More importantly , extractive QA is suitable   for eliciting users to leave feedback , since the sur-   rounding context around predicted answer spans   can help users to verify the answers . Gao et al .   ( 2022 ) has simulated user feedback for TTA in ex-   tractive QA , but not in the multi - source scenario .   Following previous work ( Gao et al . , 2022 ) , we   simulate user feedback with the annotated answer   spans . We conduct our simulation experiments on   the MRQA benchmark ( Fisch et al . , 2019 ) , where   six domains of extractive QA are studied . We com-   pare the two proposed frameworks to assess their   effectiveness and reveal the differences . We also   look into the effect of noisy preference feedback .   Our contributions in this work are as follows :   •We are the ﬁrst to study multi - source test - time   adaptation from user feedback;•We propose a novel formulation of the prob-   lem as dueling bandits and solve it by a new   method ;   •Preference feedback is discussed for extrac-   tive QA for the ﬁrst time ; and   •Extensive experiments and analysis are con-   ducted to verify our method .   2 Related Work   Domain Adaptation . Adapting from source do-   main(s ) to target domain(s ) is important for gen-   eralized machine learning ( Ramponi and Plank ,   2020 ) . Test - time adaptation ( TTA ) attracts much   attention recently , which adapts with the test data   on the ﬂy ( Sun et al . , 2020 ; Iwasawa and Matsuo ,   2021 ; Wang et al . , 2021a ; Ye et al . , 2022 ) . TTA   is more suitable for domain generalization since   it needs no source data , which is different from   unsupervised domain adaptation ( UDA ) ( Ramponi   and Plank , 2020 ; Ye et al . , 2020 ) . Multi - source DA   is a more challenging problem than UDA , since it   needs to determine suitable source knowledge for   adaptation ( Guo et al . , 2018 , 2020 ) . Multi - source   TTA has not been explored in NLP . Different from   multi - source DA , multi - source TTA has no access   to the source training data , and only the source   models are given , which makes it more challenging   to exploit useful sources for adaptation .   Learning from Human Feedback . Human feed-   back is a useful signal to reﬁne the model outputs   and adapt to new domains ( Gao et al . , 2022 ) , fol-   low human instructions ( Ouyang et al . , 2022 ) , etc .   Human feedback has been explored for different   NLP tasks such as machine translation ( Nguyen   et al . ,2017 ; Kreutzer and Riezler , 2019 ; Mendonça   et al . , 2021 ) , semantic parsing ( Lawrence and Rie-   zler,2018 ; Yao et al . , 2020 ; Elgohary et al . , 2021 ) ,   document summarization ( Gao et al . , 2018 ; Stien-   non et al . , 2020 ) , question answering ( Kratzwald   et al . , 2020 ; Gao et al . , 2022 ) , and dialogue sys-   tems ( Shuster et al . , 2022 ) . In particular , learning   from human feedback has gained a lot of inter-   ests recently in the context of alignment of large   language models ( LLMs ) ( Stiennon et al . , 2020 ;   Ouyang et al . , 2022 ; OpenAI , 2023 ) . Fundamen-   tally , alignment research is necessary and appealing   from two aspects : ( 1 ) Alignment enables a model   to go beyond supervised learning ( Stiennon et al . ,   2020 ) ( 2 ) Alignment leads to a safer system ( Ope-   nAI,2023 ) . The proposed co - UCB could poten-   tially be used for alignment in future work.9648   Figure 2 : The illustration of UCB and Co - UCB for multi - armed bandit learning and dueling bandits   respectively . For UCB , one model is sampled and receives binary feedback . For Co - UCB , two models are   chosen each time and preference feedback is made by the user , then the models are updated .   3 Preliminaries   3.1 Problem Deﬁnition   Multi - source TTA . We study multi - source test-   time domain adaptation from Ksource models by   interacting with users , where each model is trained   from a distinct source domain . Test - time data is   from a target domain Xwhich is unlabeled . We fo-   cus on online adaptation where the test data x∼X   comes as a stream and the model is continually up-   dated with newly emerged test data at each time   t. The parameters of each model at time tare   inherited from the last time t−1 . At each time t ,   we obtain the test data x∼Xwhich is the con-   catenation of the question qand the passage d.   The prediction yis a pair of start and end positions   over the passage denoted as ⟨y , y⟩. Follow-   ing previous work ( Rajpurkar et al . , 2016 ) , we use   cross - entropy loss to update the model which is :   L=−/parenleftBig   log(p ) + log ( p)/parenrightBig   /2 ( 1 )   where pandpare the probabilities of the pre-   dicted start yand end ypositions respectively .   Motivation . It is costly to learn an ensemble of   Ksources , since it has at least Ktimes the train-   ing and inference costs , and even Ktimes the   parameters of a single source model ( Guo et al . ,   2018 ; Ahmed et al . , 2021 ) . In order to adapt   efﬁciently , we cast the problem as a stochastic   decision - making process , where we aim to deter-   mine the best adapted model that can perform well   in the target domain through user interaction . Frameworks . We ﬁrst formulate the problem as   multi - armed bandit learning ( Kuleshov and Precup ,   2014 ) and show how to solve it with Upper Conﬁ-   dence Bound ( UCB ) ( Agrawal , 1995 ; Auer et al . ,   2002 ) ( § 4 ) . We further discuss multi - armed dueling   bandits to address the drawback of bandit learning ,   and propose a novel method Co - UCB ( § 5 ) .   3.2 Background   Multi - Armed Bandit Learning . The learning of   multi - armed bandits ( MAB ) is a stochastic and iter-   ative problem ( Sui et al . , 2018 ) , which repeatedly   selects a model from Ksources . Each selected   model receives a reward from the user . After T   iterations , the goal of MAB is to minimize the cu-   mulative regret compared to the best model :   R(T ) = /summationdisplay / bracketleftBig   µ−µ(a)/bracketrightBig   ( 2 )   where ais the action at time tandµ(a)is the   expected reward of the action a.µis the expected   reward of the best model .   Multi - Armed Dueling Bandits . In the multi-   armed dueling bandits ( MADB ) problem , two dis-   tinct models are sampled among the Kmodels ( Yue   et al . , 2009 ) . Also , the user needs to indicate a   preference over the two selected models . In each   comparison , a model ais preferred over awith   the probability P(a > a ) , which is equal to   ϵ(a , a ) + 1 /2where ϵ(a , a)∈(−1/2,1/2 ) .   Suppose two models aandaare sampled at   timet , and ais the overall best model . We deﬁne9649Algorithm 1 UCB for K - armed bandit learning   Require : Ksource models.¯µ←0,n←0,N←0;forB∈Xdo k←arg max¯µ+/radicalbig   2 ln(N)/n ; Obtain the reward rfor model k ; ¯µ←(¯µn+rr)/(n+|B| ) ; n←n+|B|;N←N+|B| ; Update model kwith loss rL/|B|;//L   is as Eq . 1shows.end forReturn : k←arg max¯µ+/radicalbig   2 ln(N)/n .   the cumulative regret at time Tas :   R(T ) = /summationdisplay / bracketleftBig   ϵ(a , a ) + ϵ(a , a)/bracketrightBig   ( 3 )   which is a strong version discussed in Yue et al .   ( 2009 ) . It is the proportion of users who prefer the   best model over the selected ones each time .   4 UCB for Bandit Learning   As illustrated by Figure 2 , we apply UCB for   multi - armed bandit learning , whose pseudo - code   is shown in Algorithm 1 .   Action . At each time t , the source model kis   selected from Ksource models which maximizes   ¯µ+/radicalBig , where ¯µrepresents the average   reward obtained for the model kby attempting it   forntimes , and Nis the number of all test data   instances received so far./radicalBigrepresents the   conﬁdence interval to the action k , and a larger   one means more uncertainty about the action , in-   tending to explore the action more . As training   proceeds , the policy becomes more conﬁdent about   each action .   Simulated Binary Feedback ( /thumbs_up_altor / thumbs_down_alt).For each   input , the selected model will ﬁrst predict its an-   swer , then the user leaves the feedback to the pre-   diction . Here , we use binary feedback since it is   simple enough for the user to provide and has often   been used in previous work ( Kratzwald et al . , 2020 ;   Gao et al . , 2022 ) . At each time t , a batch input   B∼Xis obtained for training , which is passed to   the model kto obtain the predictions .   Reward . With a batch of predicted answers , the   model kwill obtain a vector of simulated reward   r∈{0,1}decided by the user . For each data   instance in the batch , we follow Gao et al . ( 2022 )   to calculate the simulated reward by comparingthe predicted answer to the annotated span , where   an index - wise exact match is used . If both the   predicted start and end positions exactly match the   annotated positions , the reward is 1 ; otherwise , 0 .   Model Update . After obtaining the reward , the   model kwill be updated with a reward - enhanced   loss , where the task - speciﬁc cross - entropy loss   L(in Eq . 1 ) will be multiplied by the reward r.   Inference . After enough iterations , the best   adapted model can be found to perform well in   the target domain as line 9of Algorithm 1shows .   5Collaborative UCB for Dueling Bandits   5.1 Co - UCB   Motivation . Since only one model is accessed   each time in bandit learning , unlike ensemble learn-   ing ( Guo et al . , 2018 ) , it can not make use of the   collaboration among sources during adaptation . To   address such a drawback and not incur much extra   training cost , we exploit the pairwise collaboration   among Ksources , where each time two distinct   models will be sampled for joint learning . After   adaptation , we also keep the best model for infer-   ence , to have the same cost as bandit learning .   Sampling pairs of models can be formulated as   multi - armed dueling bandits ( MADB ) as discussed   above . However , previous work on MADB only   aims to determine the best model ( Yue et al . , 2009 ;   Zoghi et al . , 2014 ; Sui et al . , 2017 ) , so we further   propose a novel method which is Collaborative   UCB ( Co - UCB ) to let a pair of models collaborate ,   whose pseudo - code is presented in Algorithm 2 ,   and illustrated by Figure 2 .   Action . At each time t , with Ksource models , we   construct Ccombinations for selection , where   each combination is denoted by a pair of model   indices⟨i , j⟩(i < j ) . The combination ⟨i , j⟩se-   lected at time tshould maximize ( ¯µ+ ¯µ)/2 + /radicalbig   2 ln ( N)/n , where ¯µand¯µare the average   reward obtained up to time tof model iandjre-   spectively , and nis the number of combinations   ⟨i , j⟩explored so far . Nis the total number of test   data instances received until now .   Take model ifor example . The reward of ex-   ploiting model irepresents how well model ican   beat the other models during dueling . The average   reward ¯µis calculated as follows :   ¯µ=/summationdisplayr//summationdisplayn ( 4 )   where rdenotes the overall reward that the model9650Algorithm 2 Co - UCB for K - armed dueling ban-   dits   Require : Ksource models.¯µ←0,n←0,N←0;forB∈Xdo Obtain the rewards randrfor model i   andjrespectively as in Eq . 5 ; ¯µ←(¯µ/summationtextn+rr)/(/summationtextn+   |B| ) ; n←n+|B| ; ¯µ←(¯µ/summationtextn+rr)/(/summationtextn+   |B| ) ; n←n+|B| ; N←|B| ; Update the models i , jby lossLas in   Eq.7;end forReturn :   ireceived by dueling with model kandndenotes   the number of times model iduels with model k.   In each selection , to calculate the average reward   ( ¯µ+ ¯µ)/2for the combination ⟨i , j⟩ , we expect   ⟨i , j⟩to be the most worthy action ( exploration - and-   exploitation trade - off ) , where iandjcan mostly   beat the rest of the models , which means they are   the two strongest models among the Ksources so   that they can better collaborate to improve them .   Simulated Preference Feedback . ( e.g. , /chevron_sign_right//chevron_sign_left )   Since for each input , the user will receive two an-   swer candidates instead of one , the binary feedback   used in bandit learning is not directly applicable .   Rather than creating new forms of user interaction ,   we apply preference feedback ( Christiano et al . ,   2017 ; Gao et al . , 2018 ; Ouyang et al . , 2022 ) when   faced with multiple candidates . Since there are   only two candidates , leaving preference feedback   will be as simple as binary feedback .   For the chosen models iandjat time t , the   batch of inputB∼Xwill be given to them in-   dependently , to obtain the predicted answer spans .   Then the users need to compare the two predictions   to indicate a preference , where the more accurate   answer should be picked out .   Reward . For each data instance in the batch , the   reward r∈{0,1}.r= 1means the user prefers   one answer from the two candidates ; r= 0means   the user has no preference – either the two answers   are both good or none is good . This is a strict mea-   surement for preference since the answers withoutpreference are discarded .   To simulate the preference , we calculate the qual-   ity score of the predicted answers against the anno-   tated spans , where the answer with a higher score   would be preferred or no preference is made if the   scores are the same . We use the index - wise F1   value as the quality score , which calculates the F1   score over the predicted indices and the annotated   indices , so the score is continuous from [ 0,1 ] .   For the batch of input B , the quality score for   the model iandjis denoted as a vector sands   respectively . The rewards randrfor the model i   andjrespectively are obtained by :   r = s > s;r = s > s ( 5 )   where randrare one - hot vectors .   Collaborative Model Update . After obtaining   the rewards , we perform collaborative model up-   dates . If there is one preferred model , then it will   be regarded as the teacher , and its prediction will   be used to jointly update the two models . With   the predictions from model iandj , we obtain the   better one⟨y , y⟩ , each as a vector , as :   ⟨y , y⟩=⟨ry + ry ,   ry + ry⟩(6 )   where y(y ) is a vector of the predicted   start ( end ) positions from the preferred model for   the batch of inputB.   Then we jointly update the two models by the   loss :   L= ( r+r)L(y , y)/|B|(7 )   Models updated in this way can better make use   of the beneﬁts of different source models during   training , that is , when one model from a selected   pair can not predict a correct answer , the other one   may make up for it by sharing its prediction .   Inference . After adaptation , there is no need to   access a pair of models for inference anymore , so   we just keep the best performing model by the   method in line 12of Algorithm 2 .   5.2 Noise Simulation   Implicit preference feedback is naturally noisy   since the preferred answer only needs to be better   than the other and is not necessarily fully correct .   However , the users may wrongly provide a prefer-   ence in practice . Thus , we provide a pilot study to   investigate the effect of such noise on adaptation   performance .   There are three options that the user may pro-9651   Figure 3 : Transition probability for noise simula-   tion , e.g. , the correct option ‘ > ’ is corrupted into   ‘ < ’ or ‘ = ’ equally by the same probability .   vide over two candidates , which are ‘ > ’ , ‘ < ’ ( with   preference ) , and ‘ = ’ ( no preference ) . For each   data instance , we have a noise rate to randomly   decide whether its feedback should be corrupted or   not . If the feedback should be corrupted , then the   correct option is changed to one of the remaining   two options with a probability . In this work , we   use the transition probabilities shown in Figure 3 .   We leave more complex transition probabilities to   future work .   6 Experiments   6.1 Simulation Setup   Dataset . We conduct our experiments on   MRQA ( Fisch et al . , 2019 ) , which is a stan-   dard benchmark for domain generalization in   extractive QA . We study six datasets ( do-   mains ) , which are SQuAD ( Rajpurkar et al . ,   2016 ) , HotpotQA ( Yang et al . , 2018 ) , Natu-   ral Questions ( NQ ) ( Kwiatkowski et al . , 2019 ) ,   NewsQA ( Trischler et al . , 2017 ) , TriviaQA ( Joshi   et al . , 2017 ) , and SearchQA ( Dunn et al . , 2017 ) ,   where each dataset forms a distinct domain . The   training and development sets are used in our study .   Setting of 5 - source TTA . To establish multi-   source domain adaptation from the six domains ,   we set each dataset as the target domain and the   remaining ﬁve datasets as the source domains . For   each adaptation , the training set of the target do-   main is used as the unlabeled test data by discard-   ing the labels , and the development set of the target   domain is held out to evaluate the adaptation per-   formance .   Evaluation Metric . We use F1 score to evaluate   the performance on the held - out development set .   Training Details . We use the training set of   each domain to train each source model , which   follows the training details of Hu et al . ( 2020 ) . We   utilize XLMR ( Conneau et al . , 2020 ) and Span-   BERT ( Joshi et al . , 2020 ) as the pre - trained lan - guage model . In each multi - source domain adapta-   tion , we set the batch size as 16and use a constant   learning rate of 5e-7 . The number of unlabeled test   data instances is limited to 100K. The embedding   layer is frozen to save computation . Experiments   were conducted on one NVIDIA A100 GPU .   Baselines . We ﬁrst present the results of the best   source model without adaptation ( Best source ) .   Since our work is the ﬁrst to study multi - source   TTA , there are no existing baselines that address   the same problem , so for the multi - source scenario ,   we mainly compare the two frameworks discussed   above . UCB addresses the problem as bandit learn-   ing from binary feedback . Co - UCB is for dueling   bandits from simulated preference feedback .   We further compare to single - source TTA which   has been studied in Gao et al . ( 2022 ) . We ﬁrst ﬁnd   the best source model before adaptation by evalu-   ating each model on the held - out development set ,   then adapt the best source model from simulated   binary feedback following the method of Gao et al .   ( 2022 ) . This baseline is denoted as Bandit .   6.2 Main Results   We ﬁrst show the results of 5 - source TTA in Fig-   ure4 . First , consistent with the ﬁndings of Gao   et al . ( 2022 ) , successful adaption is hard to see   on TriviaQA and SearchQA just as the baseline   of Bandit ( Gao et al . , 2022 ) indicates , so the fol-   lowing observations are based on the results of the   remaining four target domains .   Bandit and dueling bandits learning are effec-   tive in determining useful sources . We ﬁnd both   UCB and Co - UCB can effectively improve the   adaptation results compared to the best source with-   out adaptation , which indicates that useful sources   are found for adaptation during the training pro-   cess .   Leveraging multiple sources by Co - UCB per-   forms the best . Even without learning a K-   ensemble model , Co - UCB still improves over the   baselines by a large margin . Co - UCB can effec-   tively utilize the beneﬁts of different sources to   outperform the Bandit baseline that adapts only   one source model . On the contrary , UCB is not   effective in making use of multiple sources , since   it only achieves results similar to Bandit .   Results during adaptation . Figure 5and Fig-   ure11plot the F1 scores vs. logging steps , where9652   Figure 4 : Results of 5 - source test - time adaptation . Experiments for the adaptation methods are run three   times with random seeds , and the average results are reported . The results of the ﬁrst row are based on   XLMR , and those of the second row are based on SpanBERT .   Figure 5 : F1 performance on XLMR w.r.t . logging steps . 0.1 , 0.3 , and 0.5 are the noise rates . Results of   TriviaQA and SearchQA are shown in Figure 11 .   we ﬁnd that UCB shows a large variance during   adaptation on NewsQA , TriviaQA , and SearchQA ,   i.e. , it is slow to ﬁnd the best source model on these   target domains . Co - UCB exhibits better perfor-   mance and lower variance than UCB during adap-   tation .   6.3 Noise Simulation   We use the transition probabilities in Figure 3to   simulate the noise , e.g. , ﬂip the feedback from “ > ”   to “ = ” with probability of 0.5 . Results are pre-   sented in Figure 6 . As the noise rate increases   and more test data ’s feedback is corrupted , the per-   formance decreases . The result on XLMR drops   dramatically with a noise rate larger than 0.5 , but   on SpanBERT , the rate of decline is not as fast .   SpanBERT is a stronger LM than XLMR for ex-   tractive QA , so it is more robust to noisy feedback .   As shown in Figure 5 , a large noise rate ( e.g. , 0.5 )   will make the adaptation fail quickly on XLMR .   Table 1 : Results of the ablation study . ‘ w/o co. ’   means removing the collaborative update in Co-   UCB.9653   Figure 6 : Adaptation results of Co - UCB w.r.t .   noise rates using the noise transition in Figure 3 .   Table 2 : Overall rewards ( ×10 ) obtained during   adaptation based on SpanBERT .   6.4 Further Analysis   Ablation study . Firstly , as Table 1shows , with-   out collaborative update , Co - UCB clearly degrades   and it can not compete with UCB . Considering that   preference feedback is naturally noisy , Co - UCB   without collaboration does not perform better than   UCB .   Overall rewards . We calculate the overall rewards   that UCB and Co - UCB obtain during adaptation   in Table 2 . The overall rewards are the sum of the   cumulated rewards from each model . We observe   that Co - UCB has a higher reward than UCB . In Co-   UCB , for a certain input , when one model could   not obtain the reward 1 , the other model may make   up for it by sharing , so that this model can also be   updated and improved . The results support why   Co - UCB performs better than UCB : higher rewards   can better instruct the model to adapt .   Case study . We show the average reward and   chosen count for each source model during adap-   tation in Figure 7 . For Co - UCB , the models 0   and2are the best and second best model respec-   tively , so its combination ⟨0,2⟩is chosen most of   the time.⟨0,1⟩and⟨0,3⟩are also often accessed   since their payoff is close to the best one . How-   ever , UCB would mostly only focus on updating   the best model which is model 0 . As shown in Fig-   ure8 , UCB is able to quickly ﬁnd the best source   model ( model 0 ) , and the other models would be   discarded without updating . For Co - UCB , since the   models are dueling with each other , the changing of   rewards behaves differently . The reward of model   0,1 , and 2decreases , while the reward of model   3and4increases , since dueling bandits learning   Figure 7 : Average rewards and chosen counts of   UCB ( a.1 , b.1 ) and Co - UCB ( a.2 , b.2 ) obtained   during adaptation on the target of HotpotQA .   Figure 8 : Average reward w.r.t . logging step for   each model during adaptation on HotpotQA .   is a zero - sum game in general ( Yue et al . , 2009 ) ,   where one model winning in dueling means another   model loses . However , reward sharing happens in   Co - UCB during training .   Effects of the number of source domains . From   the results in Figure 9 , we can see that the adap-   tation results have a very slight drop when the   number of sources increases . No matter how the   number of source models changes , Co - UCB still   consistently performs better than UCB .   We also discuss the effects of preference feed-   back on UCB in Appendix A.   7 Conclusion   We present the ﬁrst work on multi - source test - time   adaptation from human feedback , where we cast it   as an iterative decision - making problem . We ﬁrst   formulate it as multi - armed bandit learning . More   importantly , to utilize pairwise collaboration , we   further regard it as dueling bandits . Co - UCB is   a novel method proposed in this work . Though   we study online adaptation from the online data   stream , our work can also be applied in ofﬂine9654   Figure 9 : Effects of the number of source models   on adaptation performance based on XLMR .   model reﬁnement . For the ofﬂine setting , we do not   update the model online but only update the policy   for model selection when receiving user feedback   each time . After collecting enough user feedback ,   we ﬁne - tune the found best model ofﬂine with user   feedback .   Limitations   Learning an ensemble of multiple source models   is expensive , especially for large language models .   Hence , to adapt to the new target domain , we cast   the problem as an iterative - decision making pro-   cess . While our work reduces the model access   frequency to 1or2at each training step , contin-   ually updating the language model from a stream   of test data is still costly . Future work can explore   better methods for efﬁcient optimization for a sin-   gle LM . Besides , in some cases , the distribution of   test data may change dynamically over the stream ,   but our work considers only the situation where the   test data is from one speciﬁc distribution . More   complex cases of test distribution can be studied in   future work .   Acknowledgements   This research is supported by the National Research   Foundation , Singapore under its AI Singapore Pro-   gramme ( AISG Award No : AISG - RP-2018 - 007   and AISG2 - PhD-2021 - 08 - 016[T ] ) . The computa-   tional work for this article was partially performed   on resources of the National Supercomputing Cen-   tre , Singapore ( https://www.nscc.sg ) .   References96559656   AEffects of Preference Feedback on UCB   In the main content of this paper , we use preference   feedback for Co - UCB , since each time the user has   a pair of predictions to provide feedback . For UCB ,   the user only has one candidate to leave feedback ,   so we use binary feedback .   Here , we further study how preference feedback   would affect the performance of UCB . To enable   preference feedback , for each input data instance ,   the model ﬁrst generates its top two predictions ( to   be comparable to Co - UCB ) , then the user needs   to provide preference feedback to the two candi-   dates . We follow the same procedure of Co - UCB   to simulate preference feedback for UCB .   The results are presented in Figure 10 . As we   can see , UCB with preference feedback improves   over UCB with binary feedback in some cases ( not   a consistent improvement ) , since top two predic-   tions give the user more choices to select a good   label . However , UCB with preference feedback   can not compete with Co - UCB . Co - UCB aims to   leverage the beneﬁts of different source models in-   stead of the model ’s top several predictions , which   is different from UCB with preference feedback .   Similar to UCB with binary feedback , UCB with   preference also lacks collaboration among source   models , since the top two predictions , though ex-   panding the options to select a good label , are just   from one model . This ﬁnding further demonstrates   the effectiveness and importance of leveraging mul-   tiple source models during test - time adaptation.9657   Figure 10 : The results of UCB with preference feedback ( i.e. , UCB - Preference ) with comparison to UCB   and Co - UCB . The ﬁrst row is based on XLMR , and the second row is based on SpanBERT . Experiments   are run three times with random seeds , and the average results are reported .   Figure 11 : F1 performance on XLMR w.r.t . log-   ging steps . 0.1 , 0.3 , and 0.5 are the noise rates.9658ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   see the section of Limitations in the paper   /squareA2 . Did you discuss any potential risks of your work ?   Our paper is about multi - source test - time adaptation , it does n’t deal with any harmful inputs and   wo n’t generate any harmful outputs , either .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   In the last of the section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   We used Grammarly to check the grammar of the whole paper .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 5 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 5.19659 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5.1   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix B   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.9660