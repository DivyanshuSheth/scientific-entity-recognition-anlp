  Kelvin LuuDaniel KhashabiSuchin Gururangan   Karishma MandyamNoah A. SmithUniversity of WashingtonAllen Institute for AI   Abstract   When an NLP model is trained on text data   from one time period and tested or deployed   on data from another , the resulting tempo-   ral misalignment can degrade end - task perfor-   mance . In this work , we establish a suite of   eight diverse tasks across different domains   ( social media , science papers , news , and re-   views ) and periods of time ( spanning ﬁve years   or more ) to quantify the effects of temporal   misalignment . Our study is focused on the   ubiquitous setting where a pretrained model is   optionally adapted through continued domain-   speciﬁc pretraining , followed by task - speciﬁc   ﬁnetuning . We establish a suite of tasks   across multiple domains to study temporal mis-   alignment in modern NLP systems . We ﬁnd   stronger effects of temporal misalignment on   task performance than have been previously re-   ported . We also ﬁnd that , while temporal adap-   tation through continued pretraining can help ,   these gains are small compared to task - speciﬁc   ﬁnetuning on data from the target time period .   Our ﬁndings motivate continued research to   improve temporal robustness of NLP models .   1 Introduction   Changes in the ways a language is used over time   are widely attested ( Labov , 2011 ; Altmann et al . ,   2009 ; Eisenstein et al . , 2014 ) ; how these changes   will affect NLP systems built from text corpora ,   and in particular their long - term performance , is   not as well understood .   This paper focuses on temporal misalignment ,   i.e. , where training and evaluation datasets are   drawn from different periods of time . In today ’s   pretraining-ﬁnetuning paradigm , this misalignment   can affect a pretrained language model — a situa-   tion that has received recent attention ( Jaidka et al . ,   2018 ; Lazaridou et al . , 2021 ; Peters et al . , 2018 ;   Raffel et al . , 2020 ; Röttger and Pierrehumbert ,   2021)—or the ﬁnetuned task model , or both . Wesuspect that the effects of temporal misalignment   will vary depending on the genre or domain of the   task ’s text , the nature of that task or application ,   and the speciﬁc time periods .   We focus primarily on measuring the extent of   temporal misalignment on task performance . We   consider eight tasks , each with datasets that span at   least ﬁve years ( § 2.4 ) , ranging from summarization   to entity typing , a subproblem of entity recognition   ( Borthwick , 1999 ) . Notably , these task datasets   span four different domains : social media , scien-   tiﬁc articles , news , and reviews . We introduce an   easily interpretable metric that summarizes the rate   at which task performance degrades as function of   time .   Our research questions are :   ( Q)how does temporal misalignment affect   downstream tasks over time ?   ( Q)how does sensitivity to temporal misalign-   ment vary with text domain and task ?   ( Q)how does temporal misalignment affect lan-   guage models across domains and spans of   time ?   ( Q)how effective is temporal adaptation , or ad-   ditional pretraining on a target year , in miti-   gating temporal misalignment ?   We ﬁnd that temporal misalignment affects both   language model generalization and task perfor-   mance . We ﬁnd considerable variation in degra-   dation across text domains ( § 3.2 ) and tasks ( § 3.1 ) .   Over ﬁve years , classiﬁers ’ Fscore can deterio-   rate as much as 40 points ( political afﬁliation in   Twitter ) or as little as 1 point ( Yelp review ratings ) .   Two distinct tasks deﬁned on the same domain can   show different levels of degradation over time .   We explore domain adaptation of a language   model , using temporally selected ( unannotated )   data , as a way to curtail temporal misalignment   ( Röttger and Pierrehumbert , 2021 ) . We ﬁnd that   this does not offer much beneﬁt , especially relative5944to performance that can be achieved by ﬁnetuning   on temporally suitable data ( i.e. , from the same   time period as the test data ) . We conclude that tem-   poral adaptation should not be seen as a substitute   for ﬁnding temporally aligned labeled data .   The evidence and benchmarks we offer motivate   careful attention to temporal misalignment in many   applications of NLP models , and further research   on solutions to this problem .   Contributions . To facilitate the study of tempo-   ral misalignment phenomenon on downstream ap-   plications , we compile a suite of eight diverse tasks   across four important language domains . We de-   ﬁne an interpretable metric that summarizes tempo-   ral misalignment of a model on a task with times-   tamped data . Our experiments reveal key factors   in how temporal misalignment affects NLP model   performance .   2 Methodology Overview   We begin by deﬁning the scope of our study .   2.1 Learning Pipeline   We consider a process for building an NLP model   that is in widespread use by the research commu-   nity , illustrated in Fig . 1 . First , a ( neural network )   language model ( LM ) is pretrained on a large text   collection that is not necessarily selected for topical   or temporal proximity to the text of the target appli-   cation ( our focus is on GPT-2 ; Brown et al . , 2020 ) .   Second , the LM is optionally adapted by continued   training on a collection strategically curated for   closer proximity to the target ( Beltagy et al . , 2019 ) ;   this stage is often referred to as domain - adaptive   pretraining ( DAPT ; Gururangan et al . , 2020 ) . Fi-   nally , the model is ﬁnetuned to minimize a task-   speciﬁc loss , using labeled data representative of   what the model is expected to be exposed to in   testing or deployment .   We study two ways in which temporal misalign-   ment might affect the pipeline ’s performance as   well as straightforward ways to mitigate them .   Task Shift and Temporal Finetuning The rela-   tionship between text inputs and target outputs maychange over time . To the extent that this occurs ,   annotated datasets used to train NLP systems in   the ﬁnetuning stage will become stale over time .   Due to this temporal misalignment , performance   will degrade after deployment , or any in evalua-   tions that use test data temporally distant from the   training data . We seek to quantify this degradation   across a range of text domains and tasks .   Language Shift and Temporal Domain Adapta-   tion Changes in language use can cause a pre-   trained LM , which commonly serves as the back-   bone for most modern NLP models , to become   stale over time ( Lazaridou et al . , 2021 ) , regardless   of the end task . Lazaridou et al . ( 2021 ) explored   temporal adaptation , continuing LM training on   new text data . This is essentially the same proce-   dure as DAPT , where the data is selected by time   period . Their work focused on the LM alone , not   downstream tasks ; we consider both here .   Röttger and Pierrehumbert ( 2021 ) , the closest   to our work , studied temporal adaptation in con-   junction to ﬁnetuning for a classiﬁcation task over   Reddit data . They conclude that temporal adapta-   tion does not help any more than normal DAPT .   We corroborate this work and extend it by studying   a wider variety of tasks over a longer span of time   periods and thus are better able to draw generaliza-   tions from our results .   We believe that the two kinds of shift — task shift   and language shift — are difﬁcult to disentangle , and   we do not attempt to do so in this work . Instead , we   aim to quantify the effect of temporal misalignment   on a range of NLP tasks , as well as the beneﬁts of   these two strategies .   2.2 Evaluation Methodology   Our experiments are designed to measure the effect   of temporal misalignment on task performance . To   do so , for each task , we ﬁx a test set within a given   time period , T. We vary the time period of the   training data , allowing us to interpret differences   in performance as a kind of “ regret ” relative to   the performance of a model trained on data tem-   porally aligned with T.We consider multiple   different test periods for each task . We also seek   to control the effect of training dataset size . We   partition training data into time periods of roughly5945the same size and always train on a single partition ,   keeping the training set size of each time period   constant within each task . We expect that perfor-   mance could be improved by accumulating training   data across multiple time periods , but that would   make it more difﬁcult to achieve our research goal   of quantifying the effect of temporal misalignment   on performance .   2.3 Quantifying Temporal Degradation   Understanding temporal misalignment requires   evaluating a model ’s performance across data with   a range of different timestamps , which makes it dif-   ﬁcult to compare various models in terms of their   misalignment . We deﬁne a metric for temporal   degradation ( TD ) which summarizes the expected   speed of model degradation due to temporal mis-   alignment on a task as a single value . In high - level   terms , the TD score measures the average rate of   performance deterioration ( of perplexity , F , or   Rouge - L ) for each timestep of difference between   that the train and evaluation sets . Higher TD scores   imply greater levels of performance deterioration   due to misalignment .   LetSindicate the performance a model   trained on timestep tdata and evaluated on   timestep t. We deﬁne D(t / shortrightarrowt)as :   D(t / shortrightarrowt ) = −(S−S)×sign(t−t ) .   D(t / shortrightarrowt)is a modiﬁed difference in performance   between two models . Fig . 2 illustrates Das a   function of consecutive training time periods .   We ﬁnd a line of best ﬁt for D(t / shortrightarrowt)for all t   using least - squares regression . The slope of this   line is TD(t ) , the TD score for evaluation time   period t. The ﬁnal TD score is the average of the   TD(t)across all evaluation time periods t. Further   details can be found in Appendix A.   2.4 Domains , Tasks , and Datasets   We describe the eight tasks and four domains used   for this study . Three ( out of eight ) of the tasks are   newly deﬁned in this work , and all tasks required   nontrivial postprocessing . We provide examples   and detailed statistics in Table 1 .   Domain 1 : Twitter Social media platforms like   Twitter have been mined to study aspects of lan-   guage change over time , such as the introduction   or diffusion of new words ( Eisenstein et al . , 2014 ;   Tamburrini et al . , 2015 ; Wang and Goutte , 2017 ) .   We collect unlabeled data for domain adaptation   by extracting a random selection of 12 M tweets ,   spread semi - uniformly from 2015 till 2020.We   experiment with two tasks on Twitter data :   Political afﬁliation classiﬁcation ( PA)We   collect English tweets dated between 2015 and   2020 from U.S. politicians with a political afﬁl-   iation ( Republican orDemocrat ) . We omit any   politician who changed parties over this time pe-   riod or identiﬁed as independent . We consider the   downstream task of detecting political afﬁliations ,   i.e. , given a text of a single tweet we predict the   political alignment of its author at the time of the   tweet . This task can be useful for studies that in-   volve an understanding of ideologies conveyed in   text ( Lin et al . , 2008 ; Iyyer et al . , 2014 ) .   Named entity type classiﬁcation ( TERC ) We   use the Twitter NER dataset from Rijhwani and   Preo¸ tiuc - Pietro ( 2020 ) . The dataset contains tweets   dated from 2014 to 2019 , each annotated with the   mentions of named entities and their types ( Person ,   Organization , orLocation ) . We consider the task   of typing a given mention , which is a subproblem   of named entity recognition.5946   Domain 2 : Scientiﬁc Articles Scientiﬁc re-   search produces vast amounts of text with great   potential for language technologies ( Wadden et al . ,   2020 ; Lo et al . , 2020 ) ; it is expected to show a great   deal of variation over time as ideas and terminology   evolve . For adaptation to this domain , we collect   unlabeled data from science documents available   in Semantic Scholar ’s corpus , which yields 650k   documents , spread over a 30 - year period ( Ammar   et al . , 2018 ) . For this domain , we study two tasks :   Mention type classiﬁcation ( SERC ) We use the   SciERC dataset from Luan et al . ( 2018 ) which con-   tains entity - relation annotations for computer sci-   ence paper abstracts for a relatively wide range of   years ( 1980s to 2019 ) . We subdivide the annotated   data into time periods with roughly equal - sized   numbers of papers ( 1980–1999 , 2000–2004 , 2005 –   2009 , 2010–2016 ) . The task is to map a mention of   a scientiﬁc concept to a type ( Task , Method , Metric ,   Material , Other - Scientiﬁc - Term , orGeneric ) .   AI venue classiﬁcation ( AIC ) We also examine   temporal misalignment on the task of identifying   whether a paper was published in AAAI or ICML .   We group the data into roughly equal - sized time   periods ( 2009–2011 , 2012–2014 , 2015–2017 , and   2018–2020 ) . This task is , loosely , a proxy for topic   classiﬁcation and author disambiguation applica-   tions ( Subramanian et al . , 2021 ) .   Domain 3 : News Articles News articles make   up a signiﬁcant part of the data commonly usedto train LMs ( Dodge et al . , 2021 ) . News articles   convey current events , suggesting strong temporal   effects on topic . For adaptation , we use 9 M articles   from the Newsroom dataset ( Grusky et al . , 2018 ) ,   ranging from 2009–2016.We experiment with   three tasks on news articles :   Newsroom summarization ( NS ) The   Newsroom dataset provides a large quantity of   high - quality summaries of news articles ( Grusky   et al . , 2018 ) . We group articles by years for   this task ( 2009–2010 , 2011–2012 , 2013–2014 ,   2015–2016 ) . Note that this task , unlike the   other tasks considered here , is not a document   classiﬁcation task .   Publisher classiﬁcation ( PC)The News-   room dataset also provides metadata , such as publi-   cation source . We take the documents published by   the 3 most proliﬁc publishers ( Fox News , New York   Times , and Washington Post ) and train models to   classify documents among them . We bin the years   ( 2009–2010 , 2011–2012 , 2013–2014 , 2015–2016 ) .   This task is a proxy for applications that seek to   infer fact provenance ( Zhang et al . , 2020 ) . We note   that , unlike in our other tasks , we downsample to   ensure that the labels are equally balanced .   Media frames classiﬁcation ( MFC ) “ Framing ” of-   ten refers to the emphasis or deemphasis of dif-   ferent social or cultural issues in the media ’s pre-   sentation of the news ( Entman , 1983 ) . Card et al .   ( 2015 ) provide a dataset of news articles annotated5947with framing dimensions . We predict the primary   frame of a document , treating the problem as a   15 - way classiﬁcation task . We bin by timestamp   ( 2009–2010 , 2011–2012 , 2013–2014 , 2015–2016 ) .   Domain 4 : Food Reviews Food and restaurant   reviews have been widely studied in NLP research .   We considered this domain as a possible contrast to   those above , expecting less temporal change . Using   data from the Yelp Open Dataset , we consider one   task :   Review rating classiﬁcation ( YC)This is a   conventional sentiment analysis task , mapping the   text of a review to the numerical rating given by its   author ( Pang et al . , 2002 ; Dave et al . , 2003 ) . We   partition the data by year ( 2013 to 2019 ) and ensure   that each timestep has a roughly equal amount of   reviews .   3 Empirical Results and Analysis   In this section , we summarize our experimental   analysis , resulting from more than 500 experiments .   In our experiments , we primarily explore the effect   of temporal misalignment on GPT2 ( Brown et al . ,   2020 ) , a LM often used for generation . We re-   port the macro Fscore for classiﬁcation tasks and   Rouge - L ( Lin , 2004 ) for NS .   We ﬁrst focus on quantifying temporal misalign-   ment in end tasks . As a preliminary analysis , we in-   vestigate how the marginal distribution over labels   changes over time . We then study how temporal   misalignment affects performance of GPT2 mod-   els in downstream tasks with temporal ﬁnetuning   ( Q , Q ) . We ﬁnd that the amount of performance   degradation can vary by task ; in some cases the   degradation can be severe .   We then study how temporal misalignment af-   fects LMs . As a ﬁrst step , we analyze how vo-   cabularies change over time in our datasets . We   then experiment with ( Q ) how temporal misalign-   ment affects upstream language modeling and ( Q )   how effective temporal adaptation , or additional   pretraining on a target year , is in mitigating mis-   alignment . We ﬁnd that while LMs are affected by   misalignment , temporal domain adaptation is not   enough to mitigate temporal misalignment .   Details on temporal domain adaptation and ﬁne-   tuning , and an extended version of our results , can   be found in Appendices B and D , respectively .   3.1 Temporal Misalignment in Tasks   How much does misalignment affect task perfor-   mance ? We ﬁnd that it depends on the task .   Label Distribution Drift We ﬁrst investigate   how task datasets undergo changes in the marginal   distribution over labels due to time . For each task   and each test period , we calculate the KL diver-   gence between the label distributions in that period   and the ﬁrst test period . Full results are reported   in Fig . 3 . In three cases , we detected notable label   distribution drift : PA , AIC , and MFC .In   PA , Republican tweets outnumbered Demo-   cratic ones by over a 2:1 ratio in 2015 , but the   reverse held by 2020 . This observation shows that ,   regardless of the properties of NLP models , the   nature of many tasks changes over time , if only   because the output distribution changes .   Finetuning As described in § 2.4 , for each task ,   we create training and evaluation sets associated   with different time periods . We ﬁnetune GPT2 on   each of the task ’s training sets and evaluate each on   two evaluation sets . Note that there is no domain   adaptation here .   Fig . 4 shows our results on downstream tasks   ( with no domain adaptation ) . To get more reliable   estimates , each number in this heatmap is an aver-   age of ﬁve independent experiments with different   random seeds . A summary of the ﬁne - tuning re-5948   sults , in terms of TD scores ( § 2.3 ) is in Table 2   which indicates the speed of temporal degradation ,   for every year that the training and evaluation data   diverges . Recall that this score ( applied to task   performance measures ) summarizes the strength of   the effect of temporal misalignment on the score ,   using evidence from across experiments .   ( Q ) Temporal misalignment degrades task per-   formance substantially . Fig . 4 , similar to earlier   work ( Röttger and Pierrehumbert , 2021 ) , shows   that models trained on data from the same timeperiod as the test data perform far better than those   from the past . The performance drop is most severe   for PA(TD= 7.72 ) and PC(TD= 5.45 ) .   ( Q ) Temporal misalignment has a measurable   effect on most tasks . Half of our tasks see an   average loss of at least 1 point for each time period   that the training data diverges from the test data .   For datasets like SERC that make use of data   from three decades or more , this effect could add   up .   Moreover , 1 point of difference can be meaning-   ful , especially for the summarization task where we   measure Rouge - L. According to the leaderboard ,   the best three performing models are within a point   of each other in Rouge - L ( Shi et al . , 2019 , 2021 ;   Mendes et al . , 2019 ) . The task has a TD score of   1.38 . On average , a time period of temporal mis-   alignment results has a larger effect on performance   than changing between the three best models .   ( Q ) Performance loss from temporal misalign-   ment occurs in both directions . Another obser-   vation in Table 4 is that degradation happens in both   directions ( past and future ) . While most of the em-   phasis on temporal misalignment is on how to adapt   our stale models / data to the present time ( Dhin-   gra et al . , 2021 ; Lazaridou et al . , 2021 ; Röttger   and Pierrehumbert , 2021 ) , our experiments also   show that models trained on newer data can be mis-   aligned from the past , as well . Weak performance   in older texts has been noted in NLP for historical   documents ( Yang and Eisenstein , 2016 ; Han and   Eisenstein , 2019 ) . However , our ﬁndings indicate   deterioration can occur sooner — just a few years   rather than decades or centuries.5949(Q ) Tasks , even in the same domain , are af-   fected differently . Consider the two tasks of P - AandTERC ( both in the Twitter domain ) ,   with TD scores of 7.72 and 0.96 , respectively . Of   our 8 tasks , TERC , MFC , and YCare the   most robust to temporal misalignment ( TD scores   of 0.96 , 0.98 and 0.26 , respectively ) . The high lev-   els of variation show that temporal misalignment   affects performance through labeled datasets , not   just unlabeled pretraining data .   3.2 Temporal Misalignment in LMs   As LMs are widely used in modern NLP systems ,   it is important to inspect how robust they are to   temporal misalignment . We seek to understand   how temporal misalignment affects the language   modeling task in our four domains and if temporal   domain adaptation helps in downstream tasks .   Vocabulary Shift We ﬁrst consider an extremely   simple measurement of language shift : how do vo-   cabularies change across time periods?We use   a similar procedure to the one Gururangan et al .   ( 2020 ) used for analyzing domain similarity . Fix-   ing a domain , we compare the ( unigram ) vocabular-   ies of each pair of training sets . The vocabularies   are built using the 10 K most frequent terms from   each time period . We note that vocabulary over-   lap is higher between two time periods the closer   they are . Most domains see a sizeable amount of   shift ; however , Yelp is relatively stagnant . Fig . 5   visualizes the overlap measurement . Table 6 in   Appendix D shows the correlation between model   performance and the word overlap .   Temporal Domain Adaptation Researchers   have studied the broader problem of distributional   shift ( Shimodaira , 2000 ; Zhang et al . , 2013 ) . The   NLP community has historically tackled these prob-   lems via domain adaptation ( Jiang and Zhai , 2007 ;   Daumé III , 2007 ; Gururangan et al . , 2020 ) . Taking   inspiration from these approaches , we next apply   DAPT to GPT2 , treating each time period as a do-   main : for each time period , we continue pretraining   and then evaluate perplexity . We consider how the   perplexity varies with the ( mis)alignment between   the DAPT training data and the evaluation data .   We measure the TD score , which summarizes how   much performance is affected by temporal mis-   alignment ( now applied to perplexity ) . The results   of temporal domain adaptation are in Fig . 6.(Q ) Domains are a major driver of temporal   misalignment in LMs . Consistent with Lazari-   dou et al . ( 2021 ) , Fig . 6 shows degradation of LM   due to temporal misalignment ; it further shows   considerable variation by text domain . Twitter   changes most rapidly , and food reviews are much   slower . This observation is consistent with past   work on language change in social media ( Stew-   art and Eisenstein , 2018 ; Eisenstein et al . , 2014 ) .   To the extent that a LM ’s practical usefulness is   associated with its ﬁt to new data , researchers and   practitioners should understand the temporal dy-   namics of their target text domains and plan LM   updates accordingly .   Joint Effects of Temporal Adaptation and Fine-   tuning As discussed in § 2 , continued pretraining   of an LM on in - domain text has been shown to   improve task performance . Our prior results show   that both downstream tasks and language modeling   are affected by temporal misalignment . Can tem-   poral domain adaptation help mitigate the effects   of misalignment in downstream tasks ?   Here we consider how the time period of the   data selected for continued pretraining affects task   performance . For each task ’s evaluation set , we   apply DAPT twice : once with the earliest available   time period ’s unannotated data and once with the   latest ’s . We then ﬁnetune and evaluate on data from   the same time periods as in the earlier experiment .   ( Q ) Temporal adaptation does not overcome   degradation from temporally misaligned la-   beled data . In Table 3 , we see small performance   gains from temporal domain adaptation on LMs ,   and in some cases it is harmful . These observations   underscore the importance of the labeled data ; ad-   justments to the LM alone do not yet appear sufﬁ-   cient to mitigate the effects of temporal misalign-   ment . In contrast to temporal domain adaptation ,   which does not mitigate temporal misalignment ’s   effects , ﬁnetuning on temporally - updated labeled   data is more effective .   This can be observed in each task - speciﬁc sub-   table of in Table 3 : the top - left and bottom - right   quadrants ( ﬁne - tuning on time - stamp that is used   for evaluation ) generally lead to higher scores .   4 Limitations and Future Work   We provided a well - controlled suite of experiments   to study the effects of temporal misalignment on   model performance . However , the setup has some5950   drawbacks . For example , we expect that models   trained on data accumulated across multiple time   periods would perform well ( Lazaridou et al . , 2021 ;   Röttger and Pierrehumbert , 2021 ; Jin et al . , 2021 ) .   We chose the time periods in our study so that   they would each have sufﬁcient and consistent train-   ing data sizes . However , amounts of data in a   particular domain or task will ﬂuctuate over time .   Moreover , the rate of language use change may not   be uniform . Time periods should be selected with   these two considerations in mind .   Our ﬁndings indicate that temporal misalign-   ment ’s effects depend heavily on the task . Though   not studied here , the same issues may arise in   annotation efforts ; consider , for example , recentwork on controversy ( Zhang et al . , 2018 ) and so-   cial norms ( Xu et al . , 2021 ; Zhou et al . , 2021 ) likely   hinges on constructs that may be time sensitive . An-   notations that are temporally misaligned with the   original data being annotated may be anachronistic .   An opportunity for future exploration is in the   context of real - world events with sudden changes   such as COVID-19 pandemic ( Cao et al . , 2021 ) or   political changes , which inﬂuence tasks such as   question answering ( Dhingra et al . , 2021 ; Zhang   and Choi , 2021 ) .   Extensive work has been done on modeling and   detecting lexical semantic change , or how words   evolve in meaning ( Hamilton et al . , 2016 ; Rudolph   and Blei , 2018 ; Gonen et al . , 2020 ) . Techniques5951   and intuition from this body of work may be useful   in ﬁnding solutions to mitigate degradation due to   misalignment . We believe that this phenomenon   is an important aspect of temporal misalignment ,   but leave disentangling semantic shifts from other ,   perhaps task - related factors , for future work .   Continual learning , which allows models to learn   from a continuous stream of data , could also be one   way to mitigate temporal misalignment . Most prior   work in this space has focused on continual learn-   ing in LMs ( Jin et al . , 2021 ) or learning disparate   tasks ( de Masson d ' Autume et al . , 2019 ; Huang   et al . , 2021 ) . Future work may investigate contin-   ual learning algorithms for tasks that change over   time .   Our results showed that straightforward domain   adaptation was unable to mitigate the effects of   temporal misalignment . Recent work in language   modeling has elevated the importance of domains   by using a mixture of domains ( Gururangan et al . ,   2021 ) or giving domains a hierarchical structure   ( Chronopoulou et al . , 2021 ) . More sophisticated ap-   proach to domains , in line with these works , could   lead to temporally robust models .   While we found that task - speciﬁc ﬁnetuning is   more effective than temporal adaptation , new la-   beled data can be expensive . Ways to characterize   or detect changes in a task could be helpful in ef-   ﬁciently updating datasets ( Lu et al . , 2019 ; Webbet al . , 2018 ) . Future work can also treat dataset   maintenance as an optimization problem between   the cost and gains of annotating new data ( Bai et al . ,   2021 ) .   5 Conclusion   Changes in language use over time , and how lan-   guage relates to other quantities of interest in NLP   applications , has clear effects on the performance   of those applications . We have explored how tem-   poral misalignment between training data — both   data used to train LMs and annotated data used to   ﬁnetune them — affects performance across a range   of NLP tasks and domains , taking advantage of   datasets where timestamps are available . We com-   pile these datasets as a benchmark for future re-   search as well . We also introduced a summary   metric , TD score , that makes it easier to compare   models in terms of their temporal misalignment .   Our experiments revealed considerable variation   in temporal degradation accross tasks , more so than   found in previous studies ( Röttger and Pierrehum-   bert , 2021 ) . These ﬁndings motivate continued   study of temporal misalignment across applica-   tions of NLP , its consideration in benchmark evalu-   ations , and vigilance on the part of practitioners   able to monitor live system performance over time .   Notably , we observed that continued training   of LMs on temporally aligned data does not have   much effect , motivating further research to ﬁnd   effective temporal adaptation methods that are less   costly than ongoing collection of annotated / labeled   datasets over time .   Acknowledgments   We thank Dallas Card , Sihao Chen , Alexander   Fabbri , Jack Hessel , and Shruti Rijhwani for their   help in curating our data . We thank Jacob Eisen-   stein , Rahul Nadkarni , the ARK and XLab research   groups , and our anonymous reviewers for their   feedback on this work . We also acknowledge the   Beaker team ( https://beaker.org ) for their support   with experiments . This research was supported in   part by the Ofﬁce of Naval Research under MURI   grant N00014 - 18 - 1 - 2670.5952References595359545955   A A Metric for Temporal Degradation   Lettbe the time period of the training data and   tthe time period of the evaluation data . We   aim to summarize the general effect of temporal   misalignment ( the difference between tandt ) on   task performance , in an interpretable way that is   comparable across tasks .   LetSindicate the performance a model   trained on timestamp tdata and evaluated on the   timestamp t. Let   D(t / shortrightarrowt ) = −(S−S)×sign(t−t ) ,   In other words , D(t / shortrightarrowt)is a modiﬁed differ-   ence in performance between a aligned and mis-   aligned models . The modiﬁcation ensures that , as   performance deteriorates , Dincreases , regardless   of the direction of time between tandt .   Our temporal degradation ( TD ) score for a ﬁxed   evaluation timestamp tfor models trained on a set   of timestampsTis deﬁned as :   TD(T / shortrightarrowt ) = /vextendsingle / vextendsingle / vextendsingle / vextendsingle / vextendsingle / summationtext / parenleftbig   D(t / shortrightarrowt)−¯D / parenrightbig   ( t−¯t)/summationtext(t−¯t)/vextendsingle / vextendsingle / vextendsingle / vextendsingle / vextendsingle ,   where ¯t = avgtand¯D = avgD(t / shortrightarrowt ) .   This metric is the slope of a line ﬁtting the the per-   formance change of models trained on a variety   of timestamps , when evaluated on a ﬁxed times-   tamp . It can be interpreted as the average rate of   performance deterioration per time period .   Fig . 7 shows three examples of TD scores from   PA(the ﬁrst ) and YC(the latter two ) .   These illustrate cases with and without temporal   sensitivity . In practice , most examples with dete-   rioration showed a linear trend and thus the rate   of degradation was suitible to be approximated by   a line . The ﬁnal TD score is averaged over all   evaluation yearsT.   TD=/summationtextTD(T / shortrightarrowt )   |T|   B Details of Model Development   Training Details for Temporal Adaptation We   train GPT2 over each domain and timestamp for   ksteps using Huggingface ’s implementation of   GPT2 . Hyperparameter details can be seen in Ta-   ble 4 .   Training Details for Temporal Finetuning We   use Huggingface ’s implementation of GPT2 for   ﬁnetuning for both the classiﬁcation and summa-   rization tasks . We train on Quadro RTX 800 GPUs .   See Table 5 for details .   C Data Collection   We describe the postprocessing and data collection   in greater detail . All data released is intended for   non - commercial use .   PA We acquire a list of U.S. politician   names and Twitter handles . One of the authors   manually annotated whether each politician was a   Republican or Democrat . In addition , one volunteer   double checked to ensure correctness . We discard   any politician who changed parties between 2015   and 2020 , any independents , and anyone suspended   by Twitter ( e.g. , RealDonaldTrump).5956   AIC We randomly sample science documents in   Semantic Scholar ’s corpus . Of those , we only   keep documents that ( 1 ) are published in ICML   or AAAI , ( 2 ) are classiﬁed as ‘ computer science ’   documents , and ( 3 ) have an abstract of at least 50   tokens .   Newsroom The following applies to the postpro-   cessing and data selection for both supervised tem-   poral ﬁnetuning and unsupervised temporal adapta-   tion of PCandNS . We use the News-   room dataset .. We only keep articles where ( 1 )   the year in the metadata also appears in the main   text and ( 2 ) no future year is mentioned in the main   text .   PC We carry out additional postprocess-   ing and ensure that each of the three labels ( Fox   News , New York Times , and Washington Post )   have an equal distribution across years . We do so   by uniform - random downsampling .   D Extended Results   We provide further results from our experiments   described in Section 3 .   Word Overlap Correlation with Performance   In addition to measuring vocabularies ’ change over   time in Section 3.2 , we ﬁnd correlations between   the word overlap and model performance of each   task in Table 6 .   Finetuning Results We provide the full results   from our ﬁentuning experiments in Section 3.1 in   Fig . 8 . These results are for downstream tasks with   no domain adaptation .   Finetuning with Temporal Domain Adaptation   We provide the full results from our ﬁnetuning with   temporal domain adaptation in Section 3.2 in Fig . 7.59575958