  Ziwang ZhaoLinmei HuHanyu ZhaoYingxia ShaoYequan WangBeijing University of Posts and TelecommunicationsBeijing Institute of TechnologyBeijing Academy of Artificial Intelligence   { zhaoziwang,shaoyx}@bupt.edu.cn hulinmei@bit.edu.cn   hyzhao@baai.ac.cn tshwangyequan@gmail.com   Abstract   Commonsense question answering is important   for making decisions about everyday matters .   Although existing commonsense question an-   swering works based on fully fine - tuned PLMs   have achieved promising results , they suffer   from prohibitive computation costs as well as   poor interpretability . Some works improve the   PLMs by incorporating knowledge to provide   certain evidence , via elaborately designed GNN   modules which require expertise . In this paper ,   we propose a simple knowledgeable parameter   efficient tuning network to couple PLMs with   external knowledge for commonsense question   answering . Specifically , we design a train-   able parameter - sharing adapter attached to a   parameter - freezing PLM to incorporate knowl-   edge at a small cost . The adapter is equipped   with both entity- and query - related knowledge   via two auxiliary knowledge - related tasks ( i.e. ,   span masking and relation discrimination ) . To   make the adapter focus on the relevant knowl-   edge , we design gating and attention mecha-   nisms to respectively filter and fuse the query   information from the PLM . Extensive exper-   iments on two benchmark datasets show that   KPE is parameter - efficient and can effectively   incorporate knowledge for improving common-   sense question answering .   1 Introduction   Commonsense question answering is the process   of combining observations and the basic knowl-   edge that reflects our natural understanding of the   world and human behaviors , to make presumptions   about ordinary situations in our daily life ( Johnson-   Laird , 1980 ) . It has emerged as an important task   in natural language understanding .   Pre - trained language models ( PLMs ) , which   revolutionize many areas with superior perfor-   mance , have been applied for the commonsense   question answering task based on full fine - tuningFigure 1 : Comparison of existing methods and our pro-   posed method .   as shown in Figure 1(a ) . For example , Lourie   et al . ( 2021 ) fully fine - tuned the PLM Unicorn and   achieved competitive performance on 8 common-   sense benchmarks . However , they inevitably incur   prohibitive computation costs as the scale of param-   eters increases , and are lacking in transparency and   interpretability ( Houlsby et al . , 2019a ; Lin et al . ,   2019 ) .   Furthermore , some works couple the PLMs with   knowledge to improve the interpretability of the   reasoning process . As shown in Figure 1(b ) , they   typically extract the relevant knowledge subgraphs   about entities in the query and then elaborately   design a graph neural network ( GNN ) module to   perform reasoning ( Lin et al . , 2019 ; Feng et al . ,   2020 ; Yasunaga et al . , 2021 ; Sun et al . , 2022 ) . De-   spite the fact that they provide certain evidence for   the reasoning process , it requires expertise to de-   sign effective GNN modules . Additionally , they   generally consider only the structured triple knowl-   edge about the entities in the query , while ignoring   the textual knowledge about the query itself .   In this work , we propose a simple   Knowledgeable Parameter Efficient model   ( KPE ) for commonsense question answering . In   particular , we design a parameter - sharing adapter9051plugin for incorporating knowledge into the   frozen PLM as shown in Figure 1 , which largely   reduces the scale of trainable parameters . Our   adapter plugin integrates both the entity- and   query - related knowledge ( uniformly grounding to   the unstructured commonsense knowledge base   GenericsKB ) through two auxiliary knowledge-   related tasks ( i.e. , span masking and relation   discrimination ) . Additionally , to make the adapter   focus on the relevant knowledge for commonsense   question answering , we design gating and attention   mechanisms to respectively filter and fuse the   query information from the PLM . Overall , our   main contributions can be summarized as follows :   •To the best of our knowledge , we are the first   to propose a knowledgeable parameter effi-   cient tuning network for commonsense ques-   tion answering , which adopts a new parameter-   sharing adapter to incorporate knowledge .   •Our designed adapter integrates both the   entity- and query - related knowledge with two   auxiliary knowledge - related tasks . Addition-   ally , the gating and attention mechanisms are   respectively employed to filter and fuse the   query information from the PLM to make the   adapter focus on relevant knowledge for com-   monsense question answering .   •Extensive experiments on two benchmark   datasets have demonstrated that our proposed   KPE can effectively incorporate knowledge   for improving commonsense question answer-   ing , with a tiny computation cost .   2 Related Work   In this section , we review the related works on   commonsense question answering and parameter   efficient tuning .   2.1 Commonsense Question Answering   With the remarkable success of PLMs on various   tasks ( Liu et al . , 2019 ; Raffel et al . , 2020 ) , some   researchers propose to fully fine - tune PLMs on the   commonsense question answering task . For exam-   ple , Lourie et al . ( 2021 ) fully fine - tuned the PLM   Unicorn on 8 commonsense benchmarks respec-   tively , and achieved promising results . Khashabi   et al . ( 2020 ) built a universal PLM for the ques-   tion answering task and fully fine - tuned it on 10   factoid and commonsense QA datasets . Despitethe prevalence of PLMs , fine - tuning all the param-   eters brings prohibitive computation costs as the   scale of PLM parameters grows . Moreover , due   to the lack of modules explicitly modeling knowl-   edge , the PLMs suffer from poor transparency and   interpretability . In light of this , some methods im-   prove the PLMs with well - designed GNN modules   to integrate relevant knowledge from knowledge   graphs ( Feng et al . , 2020 ; Sun et al . , 2022 ; Wang   et al . , 2022 ) . For example , MHGRN ( Feng et al . ,   2020 ) combines PLMs with a graph relation net-   work to perform multi - hop reasoning on knowledge   subgraphs and provides certain evidence for the rea-   soning process . GreaseLM ( Zhang et al . , 2022 ) and   JointLK ( Sun et al . , 2022 ) introduce GNN - based   modules to perform joint reasoning over both the   text and knowledge subgraphs for commonsense   question answering . Nevertheless , they require ex-   pertise to design an effective GNN module for en-   coding the knowledge subgraph . Additionally , they   only consider the entity - related structured knowl-   edge , ignoring the query - related knowledge which   could be in the form of text .   Differently , in this work , we present a simple   knowledgeable parameter efficient tuning network   which utilizes a parameter - sharing adapter to incor-   porate both entity- and query - related knowledge   for improving commonsense question answering .   2.2 Parameter Efficient Tuning   Since fine - tuning all the parameters of PLMs   causes prohibitively expensive costs , researchers   propose to fine - tune a small part of the model pa-   rameters while freezing the rest . Adapter - tuning ,   firstly proposed by Houlsby et al . ( 2019a ) , is a   prevalent parameter efficient tuning method which   inserts trainable adapter modules between the lay-   ers of frozen PLMs to bootstrap PLMs ( Mahabadi   et al . , 2021 ; Pfeiffer et al . , 2021 ) . Wang et al .   ( 2021 ) adopted adapters to infuse knowledge into   the large pre - trained language model . Inspired   by the prompting methods , some researchers also   exploit the prefix - tuning ( Li and Liang , 2021 )   and prompt - tuning ( Lester et al . , 2021 ; Liu et al . ,   2022b ) . They preset a sequence of trainable prompt   tokens to the input or intermediate layers and only   update these tokens during training . Addition-   ally , some works explore the low - rank adaptation   method which injects and optimizes the low - rank   matrices of attention weight in the frozen PLMs   for parameter efficient tuning ( Hu et al . , 2022 ; Ma-9052   habadi et al . , 2021 ) .   In this work , we focus on the commonsense ques-   tion answering task and propose a knowledgeable   parameter efficient tuning network that effectively   couples PLMs with external knowledge .   3 KPE Model   Following previous works ( Feng et al . , 2020 ; Ya-   sunaga et al . , 2021 ) , we focus on the commonsense   question answering task in the form of multiple-   choice question answering . Formally , given a natu-   ral language query qand a set of candidate answers   A={a } , we will measure the plausibility score   ρ(q , a)for each answer and choose the most plausi-   ble one a. To promote the commonsense question   answering process , we resort to external knowl-   edge bases to extract both entity- and query - related   knowledge pieces K={k}based on qandA.   As shown in Figure 2 , our KPE couples the PLM   with external knowledge via a parameter - sharing   knowledgeable adapter attached to the frozen PLM .   The PLM takes ( q , a)as input and outputs the plau-   sibility score ρ(q , a ) . The knowledgeable adapter   aims to integrate the knowledge pieces k. In the fol-   lowing , we first introduce the knowledge extraction   process . Then we describe the knowledgeable   adapter that effectively integrates the extracted   knowledge based on two auxiliary tasks ( i.e. , span   masking and relation discrimination tasks ) , as well   as gating and attention mechanisms for information   interaction with the PLM.3.1 Knowledge Extraction   A traditional source of commonsense knowledge is   triple - based knowledge graphs such as ConceptNet   ( Speer et al . , 2017 ) . However , they encode limited   types of the knowledge . Here , we use a corpus of   generic sentences about commonsense facts , i.e. ,   GenericsKB ( Bhakthavatsalam et al . , 2020 ) as the   final knowledge source . The text can represent   more complex commonsense knowledge , involving   facts that relate three or more concepts . Next , we   introduce how to extract entity- and query - related   knowledge from GenericsKB .   Entity - related Knowledge . For entity - related   knowledge , we first recognize all the entities in   the query and candidate answers , and ground them   to triples in ConceptNet . Then , we serialize the   triples to sentences and use them as the keys to   retrieve knowledge pieces from GenericsKB .   Triple Grounding . Given the query qand candi-   date answers A={a } , we first extract the entities   efrom them . Then , we ground all the triples in   ConceptNet originating from eto obtain the triple   setT={h , r , t } . To condense and filter the ex-   tracted triples , we follow Xu et al . ( 2022 ) to score   each triple :   p = w∗N   N , ( 1 )   where pdenotes the score of the i - th triple   ( h , r , t),wis the triple weight provided by Con-   ceptNet , Nis the size of TandNis the number   of triples with relation rinT. Ifpis higher   than the predefined score threshold p , the triple9053(h , r , t)will be added to the selected triple set   T⊆ T.   Knowledge Retrieval . Now , we convert these   triples in Tinto a series of sentences for knowl-   edge retrieval from the unstructured commonsense   knowledge base GenericsKB . Specifically , for each   triple ( h , r , t ) , we employ a set of pre - defined   relation templates ( Ma et al . , 2021 ) to generate a   sentence sat first . For example , the triple ( swel-   tering , RelatedTo , hot ) can be serialized to the sen-   tence " sweltering is related to hot " . Then , we take   sas a key to retrieve the related knowledge pieces   ( in the form of sentences ) from GenericsKB . The   knowledge pieces without the entity pair ( h , t )   are directly disregarded . Afterwards , we select   the knowledge piece which is most relevant to the   query to enhance the commonsense question an-   swering . Particularly , we use the pre - trained Sim-   CSE ( Gao et al . , 2021 ) to obtain sentence embed-   dings , based on which , we compute the cosine sim-   ilarity between each retrieved knowledge piece k   and the query qas the knowledge relevance score .   Finally , after processing all the triples , we choose   topK(K= 5 in this work ) retrieved knowledge   pieces as entity - related knowledge K={k }   according to the computed knowledge relevance   scores .   Query - related Knowledge . Considering the rich   semantic information contained in the query q , we   also explore the query - related knowledge for im-   proving the commonsense question answering task .   Specifically , similar to the entity - related knowl-   edge retrieval , we retrieve query - relevant knowl-   edge pieces from GenericsKB by concatenating the   query with all the candidate answers as the key   for retrieval . We also compute the knowledge rele-   vance scores and choose top Kknowledge pieces   with the highest scores as the query - related knowl-   edgeK={k } .   3.2 Knowledgeable Adapter   In this subsection , we detail our knowledgeable   adapter that effectively incorporates the above ex-   tracted knowledge for commonsense question an-   swering .   Knowledgeable Adapter Layer . For parameter-   efficiency , we connect each PLM layer with a   parameter - sharing adapter layer as shown in Fig-   ure 3 . For the l - th ( l∈[1 , L ] ) adapter layer , the   inputH∈Ris formed by vertically con-   catenating the output features ˜H∈Rof   the ( l-1)-th adapter layer and the output features   ˜H∈Rof the l - th PLM layer , where mand   nrespectively denote the length of PLM input se-   quence and knowledge piece , and dis the hidden   size . Note that , a learnable gating function is ap-   plied to filter the PLM output features ˜Hto obtain   crucial information of the query . Formally ,   H= [ ˜H⊙σ(G);˜H ] , ( 2 )   where G∈Ris a trainable matrix and is   learned in the training process , ⊙denotes the   element - wise multiplication .   Now , given the input H , the adapter layer first   projects it down to rdimension with a linear pro-   jection layer . Then we apply a self - attention layer   to better fuse the knowledge and the query infor-   mation from the PLM . After that , another linear   projection layer is applied to project it up to the   original dimension d. Finally , we split the output   features H∈Rof the up projection   layer into two parts : ˜H∈Rfor the residual   connection layer of the PLM and ˜H∈Rfor   the next adapter layer . To enhance the knowledge   modeling ability of the adapter , we also design the   following two knowledge - related tasks , which take   the final output ˜Hof the adapter as input .   Span Masking Task . Mask prediction task can   help promote the knowledge memorization of   the adapter ( Sun et al . , 2021 ) . For the entity-   related knowledge kcorresponding to the triple   ( h , r , t ) , we mask out the corresponding tokens   of the tail entity mention and replace them with   the same number of [ MASK ] to yield the corrupted   sequence . Then , we fed the corrupted sequence   into the adapter for forward reasoning . Based on   the final adapter output ˜H , we predict the masked   tokens and calculate cross - entropy loss L over   them . For the query - related knowledge piece k,9054we mask 15 % tokens in total at the span level and   predict them in the same way as SpanBERT ( Joshi   et al . , 2020 ) .   Relation Discrimination Task . Relation dis-   crimination task can facilitate the adapter to un-   derstand the intrinsic relational facts in text and im-   prove the robustness of the learned representations   through contrastive learning ( Chen et al . , 2022 ) .   This task applies only to entity - related knowledge   pieces that include entity pairs . Given the entity-   related knowledge piece kand its corresponding   triple ( h , r , t ) , we conduct mean pooling opera-   tion over the token embeddings ( from the adapter   output ˜H ) of the entity mentions to obtain entity   representations vandv . Then , we follow Qin   et al . ( 2021 ) to concatenate vandvas the rela-   tion representation v= [ v , v ] . For improving   the understanding of relational facts , we treat the   relation ras its positive sample and the rest rela-   tions as negative samples . Finally , we adopt the   InfoNCE ( van den Oord et al . , 2018 ) loss to make   the positive pair closer and push away the negative   pairs :   L=−logexp ( v·f(r)/τ )   /summationtextexp ( v·f(r)/τ),(3 )   where τis a temperature hyper - parameter , |E|is   the number of relations rin ConceptNet , and f(r )   denotes the lookup operation for the token i d of the   relation rbased on the PLM . If there are multiple   tokens in r , we will apply mean pooling .   3.3 Model Training   Given the query context qand a candidate choice   a∈ A , we leverage the output ˜Hof the fi-   nal PLM layer to compute the plausibility score   ρ(q , a)=MLP ( ˜H)and maximize the plausibility   score of the correct answer avia a cross - entropy   loss :   L = E / bracketleftbigg   −logexp ( ρ(q , a))/summationtextexp ( ρ(q , a))/bracketrightbigg   .   ( 4 )   Overall , the whole training objective of KPE is   formulated as follows :   L = L+L+L. ( 5 )   During training , we will randomly sample one   piece of knowledge from the entity- and query-   related knowledge ( KandK ) at each step . Note   that for the query - related knowledge piece whichis not applicable to the relation discrimination task ,   we will ignore the corresponding loss L.   4 Experiments   In this section , we evaluate the effectiveness of our   proposed KPE .   4.1 Datasets   We evaluate KPE on two benchmark datasets :   OpenbookQA ( Mihaylov et al . , 2018 ) and Com-   monsenseQA 2.0 ( Talmor et al . , 2021 ) .   OpenbookQA is a question answering dataset   about elementary scientific knowledge and each   question has four different options . This dataset   contains 5,957questions in total and we utilize the   official data splits from Mihaylov et al . ( 2018 ) .   CommonsenseQA 2.0 ( CSQA2 ) is a binary   classification dataset including 14,343questions .   Note that the test set of CSQA2 is not public , and   we need to submit the model predictions to the   official leaderboard to get the evaluation results .   4.2 Implementation Details   For knowledge retrieval , we first store GenericsKB   via Elasticsearchand use the retrieval function   of Elasticsearch based on BM25 for retrieval . We   choose the parameter values that achieve the best   results on the development set . Experimentally ,   we set the temperature hyper - parameter τin the   relation discrimination task to 0.1and set the score   threshold pin triple grounding to 3.5 . The down   size in the adapter ris set to 256 . Following pre-   vious works , the hidden dimension of the model   d=1024 , and the number of layers L=24 . We use   AdamW ( Loshchilov and Hutter , 2018 ) optimizer   in our experiments . For model training , we set   the batch size to 32and the learning rate to 2e-5 .   We implement the parameter efficient tuning base-   lines for commonsense question answering based   on AdapterHub ( Pfeiffer et al . , 2020 ) .   4.3 Baselines   We compare our proposed KPE with the follow-   ing parameter efficient tuning based methods and   existing strong commonsense question answering   methods .   Parameter Efficient Tuning based Methods .   We compare KPE with the following parameter   efficient tuning based methods.9055   •Bottleneck Adapter ( Houlsby et al . , 2019b )   is the first method to perform the adapter-   based tuning in NLP .   •Prefix Tuning ( Li and Liang , 2021 ) inserts   a sequence of learnable prompts into the in-   put or intermediate layers to decrease training   costs .   •LoRA ( Hu et al . , 2022 ) presets trainable rank   decomposition matrices in each layer of PLM   for less trainable parameters .   •MAM Adapter ( He et al . , 2022 ) builds an   effective adapter module that combines the   advantages of adapter , prefix tuning and low-   rank methods .   •Compacter ( Mahabadi et al . , 2021 ) is built   on top of ideas from adapters , low - rank op-   timization , and parameterized hypercomplex   multiplication layers , achieving a better trade-   off between task performance and the number   of trainable parameters .   For fair comparison , we improve these baseline   methods with our extracted knowledge by concate-   nating the extracted knowledge with the original   inputs of these baselines .   Existing Commonsense Question Answering   Methods . We also compare KPE with the exist-   ing strong commonsense question answering meth-   ods . For OpenbookQA dataset , we compare our   model with the following baselines that enhance   PLMs with knowledge via GNN modules : ( 1 ) RN   ( Santoro et al . , 2017 ) , ( 2 ) RGCN ( Schlichtkrull   et al . , 2018 ) , ( 3 ) GconAttn ( Wang et al . , 2019 ) , ( 4 )   MHGRN ( Feng et al . , 2020 ) , ( 5 ) QA - GNN ( Ya-   sunaga et al . , 2021 ) , ( 6 ) GSC ( Wang et al . , 2022 ) ,   ( 7 ) JointLK ( Sun et al . , 2022 ) . For fair comparison ,   we use the same PLM ( i.e. , RoBERTa - large ( Liu   et al . , 2019 ) ) in all the above baselines and our KPEon OpenbookQA .   For CSQA2 dataset , we employ the vanilla   Unicorn-11B ( Lourie et al . , 2021 ) as the PLM   model for KPE , and compare KPE with the fol-   lowing fully fine - tuned model from the official   leaderboard : ( 1 ) T5 - large ( Raffel et al . , 2020 ) ,   ( 2 ) Unicorn - large ( Lourie et al . , 2021 ) , ( 3 ) T5-   11B ( Raffel et al . , 2020 ) , ( 4 ) Unicorn-11B ( Lourie   et al . , 2021 ) , ( 5 ) GKP+Unicorn-11B - ft ( Liu et al . ,   2022a ) . Among these baselines , GKP+Unicorn-   11B - ft performs best . It handcrafts demonstration   examples to guide GPT3 ( Brown et al . , 2020 ) to   generate knowledge and integrates the knowledge   via prompting for commonsense question answer-   ing .   4.4 Results and Analysis   Table 1 reports the results of our proposed KPE   in comparison with the prevalent parameter effi-   cient tuning based methods on both OpenbookQA   and CSQA2 datasets . Note that , for a fair com-   parison , we concatenate our extracted common-   sense knowledge with the original inputs of these   baselines . Since the annotation of the CSQA2 test   set is not released , we only report the compari-   son results on the dev set . From table 1 , we can   observe that : ( 1 ) KPE consistently outperforms   all the baselines on both datasets . Compared to   the best baseline method , KPE achieves around   12.5%and1.3%improvements on CSQA2 dev   set and OpenbookQA test set , respectively . We   believe that KPE benefits from the designed knowl-   edgeable adapter which is parameter - efficient and   effectively incorporates the commonsense knowl-   edge . Moreover , KPE achieves a much larger im-   provement on CSQA2 dataset ( +12.5 % ) than Open-   bookQA dataset ( +1.3 % ) . The reason could be that9056   the CSQA2 dataset is much more difficult , in which   the knowledge is more needed . Thus , KPE achieves   a greater improvement on CSQA2 dataset by ef-   fectively incorporating the external knowledge . ( 2 )   Compared to Bottleneck Adapter , Prefix Tuning   and MAM Adapter , KPE introduces fewer param-   eters while achieving conspicuous improvements   on both datasets . The reason is that our knowledge-   able adapter employs an efficient parameter - sharing   strategy and better integrates the knowledge via   two auxiliary knowledge - related tasks . The gating   and attention mechanisms also help the adapter to   focus on useful knowledge for improving common-   sense question answering . ( 3 ) The baseline meth-   ods Compacter and LoRA , although introducing   fewer parameters , achieve much lower performance   than KPE . Our method achieves a better trade - off   between the number of trainable parameters and   task performance .   Table 2 and 3 show the results of our model in   comparison with the existing strong commonsense   question answering methods on OpenbookQA   dataset and CSQA2 dataset , respectively . As we   can see from Table 2 , our KPE outperforms all the   GNN based methods and achieves the best perfor-   mance . It demonstrates the effectiveness of our   KPE with the knowledgeable adapter for incorpo-   rating knowledge to improve commonsense ques-   tion answering . We believe that KPE could further   benefit from the advancement of large language   models and is of much value to the parameter effi-   cient tuning research .   From Table 3 , we can observe that our model   KPE based on the PLM Unicorn-11B achieves com-   parable performance to the best fully fine - tuned   models Unicorn-11B and GKP+Unicorn-11B - ft ,   through updating a much smaller amount of param-   eters ( around 0.019 % compared to their parameter   scale ) .   4.5 Ablation Study   To verify the importance of each module in our   KPE , we compared it with the following variants :   ( 1)KPE - w/o - E : A variant of KPE that removes the   entity - related knowledge . ( 2 ) KPE - w/o - Q : A vari-   ant of KPE that removes the query - related knowl-   edge . ( 3 ) KPE - w/o - E&Q : A variant of KPE that   removes both entity- and query - related knowledge .   Accordingly , the span masking and relation dis-   crimination tasks are removed . ( 4 ) KPE - w/o - S :   A variant of KPE that removes the span masking   task . ( 5 ) KPE - w/o - R : A variant of KPE that re-   moves the relation discrimination task . ( 6 ) KPE-   w/o - S&R : A variant of KPE that removes both   span masking and relation discrimination tasks .   ( 7)KPE - w/o - A : A variant of KPE that replaces   the self - attention mechanism in the knowledgeable   adapter with a conventional nonlinearity function .   ( 8)KPE - w/o - G : A variant of KPE that replaces   the learnable gating function in the knowledgeable   adapter with direct concatenation .   Table 4 shows the results of the ablation study .   We can obtain the following observations : ( 1 ) On   both datasets , removing query - related knowledge   results in a larger performance drop than removing   entity - related knowledge . It demonstrates the im-   portance of the query - related knowledge for com-9057   monsense question answering . When removing   both entity- and query - related knowledge , the per-   formance largely decreases ( -2.8 % and -5.05 % on   OpenbookQA and CSQA2 , respectively ) . ( 2 ) Dis-   abling any auxiliary knowledge - related task will   result in performance degradation , which shows   that both tasks enable the adapter to better cap-   ture the knowledge , thus improving the common-   sense question answering . ( 3 ) KPE consistently   outperforms KPE - w/o - G and KPE - w/o - A on both   datasets , which verifies that both the gating and   self - attention mechanisms promote the knowledge   integration for improving commonsense question   answering .   4.6 Impact of Down Size rin Adapter   To explore the impact of the down size ron model   performance , we vary rfrom 16to1024 , and report   the results on two datasets in Figure 4 . We can   observe that the accuracy on both datasets generally   first grows and reaches the highest value from 16   to256 , while it begins to drop when ris larger than   256 . Overall , KPE achieves the best performance atr=256on both OpenbookQA and CSQA2 datasets .   4.7 Case Study   In order to intuitively understand how the exter-   nal knowledge in KPE helps improve the com-   monsense question answering , we compare KPE   with the variant KPE - w/o - E&Q. We visualize the   predicted score distributions over the candidate   choices using two examples from OpenbookQA   and CSQA2 datasets . As can be seen from Figure   5(a ) , given the query “ Desert environments are gen-   erally _ ” , KPE makes the right choice “ sweltering ”   while KPE - w/o - E&Q assigns a higher score to the   incorrect choice “ arctic like ” . We believe that the   extracted knowledge ( e.g. , “ some plants grow in   the hot , dry desert ” , “ sweltering is related to hot . ” )   facilitates the commonsense question answering .   In addition , we can observe from Figure 5(b ) that   although both KPE and KPE - w/o - E&Q correctly   predict the answer , KPE is more confident with the   prediction results by benefiting from the extracted   knowledge.90585 Conclusion   In this work , we present a knowledgeable param-   eter efficient tuning network KPE to effectively   incorporate both entity- and query - related knowl-   edge for improving commonsense question answer-   ing . Particularly , we design a parameter - sharing   knowledgeable adapter as the plugin attached to   the frozen PLM to incorporate knowledge . Two   auxiliary knowledge - related tasks are specifically   designed for the adapter to better model and cap-   ture the knowledge . Moreover , to make the adapter   integrate relevant knowledge , we introduce gating   and attention mechanisms to respectively filter and   fuse the query information from the PLM . Exper-   iments on two benchmark datasets have demon-   strated the effectiveness and parameter - efficiency   of KPE for commonsense question answering . In   future work , we will explore to integrate other   parameter - efficient tuning tricks in KPE .   Limitations   The performance of KPE is also related to the used   pre - trained language model ( PLM ) , in addition to   the proposed framework . KPE could suffer from   unsatisfactory performance when the base PLM is   not strong enough . Applying our proposed KPE   to stronger PLMs , such as DeBERTa , may lead to   further improvements .   Acknowledgement   This work was supported by the National Key R&D   Program of China ( 2020AAA0105200 ) , the Na-   tional Science Foundation of China ( NSFC No .   U19B2020 , No . 62276029 , No . 62106249 ) , Bei-   jing Academy of Artificial Intelligence ( BAAI ) and   CCF - Zhipu . AI Large Model Fund ( No . 202217 ) .   References905990609061ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations   /squareA2 . Did you discuss any potential risks of your work ?   There are no potential risks in our paper .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.9062 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.9063