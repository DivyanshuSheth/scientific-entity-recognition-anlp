  Ivan Habernal   Trustworthy Human Language Technologies   Department of Computer Science   Technical University of Darmstadt   ivan.habernal@tu-darmstadt.de   www : trusthlt : org   Abstract   As privacy gains traction in the NLP com-   munity , researchers have started adopting var-   ious approaches to privacy - preserving meth-   ods . One of the favorite privacy frameworks ,   differential privacy ( DP ) , is perhaps the most   compelling thanks to its fundamental theoreti-   cal guarantees . Despite the apparent simplicity   of the general concept of differential privacy , it   seems non - trivial to get it right when applying   it to NLP . In this short paper , we formally ana-   lyze several recent NLP papers proposing text   representation learning using DPText ( Beigi   et al . , 2019a , b ; Alnasser et al . , 2021 ; Beigi   et al . , 2021 ) and reveal their false claims of   being differentially private . Furthermore , we   also show a simple yet general empirical san-   ity check to determine whether a given imple-   mentation of a DP mechanism almost certainly   violates the privacy loss guarantees . Our main   goal is to raise awareness and help the com-   munity understand potential pitfalls of apply-   ing differential privacy to text representation   learning .   1 Introduction   Differential privacy ( DP ) , a formal mathematical   treatment of privacy protection , is making its way   to NLP ( Igamberdiev and Habernal , 2021 ; Senge   et al . , 2021 ) . Unlike other approaches to protect   privacy of individuals ’ text documents , such as   redacting named entities ( Lison et al . , 2021 ) or   learning text representation with a GAN attacker   ( Li et al . , 2018 ) , DP has the advantage of quan-   tifying andguaranteeing how much privacy can   be lost in the worst case . However , as Habernal   ( 2021 ) showed , adapting DP mechanisms to NLP   properly is a non - trivial task .   Representation learning with protecting pri-   vacy in an end - to - end fashion has been recently   proposed in DPText ( Beigi et al . , 2019b , a ; Al-   nasser et al . , 2021 ) . DPText consists of an auto-   encoder for text representation , a differential - privacy - based noise adder , and private attribute   discriminators , among others . The latent text rep-   resentation is claimed to be differentially private   and thus can be shared with data consumers for   a given down - stream task . Unlike using a pre-   determined privacy budget " , DPText takes " as a   learnable parameter and utilizes the reparametriza-   tion trick ( Kingma and Welling , 2014 ) for random   sampling . However , the downstream task results   look too good to be true for such low " values . We   thus asked whether DPText is really differentially   private .   This paper makes two important contributions   to the community . First , we formally analyze   the heart of DPText and prove that the employed   reparametrization trick based on inverse continu-   ous density function in DPText is wrong and the   model violates the DP guarantees . This shows that   extreme care should be taken when implementing   DP algorithms in end - to - end differentiable deep   neural networks . Second , we propose an empir-   ical sanity check which simulates the actual pri-   vacy loss on a carefully crafted dataset and a re-   construction attack . This supports our theoretical   analysis of non - privacy of DPText and also con-   ﬁrms previous ﬁndings of breaking privacy of an-   other system ADePT .   2 Differential privacy primer   Suppose we have a dataset ( database ) where each   element belongs to an individual , for example Al-   ice , Bob , Charlie , up to m. Each person ’s entry ,   denoted with a generic variable x , could be an ar-   bitrary object , but for simplicity consider it a real   valued vector x2R. An important premise is   that this vector contains some sensitive informa-   tion we aim to protect , for example an income   ( x2R ) , a binary value whether or not the person771has a certain disease ( x2f0:0;1:0 g ) , or a dense   representation from SentenceBERT containing the   person ’s latest medical record ( x2R ) . This   dataset is held by someone we trust to protect the   information , the trusted curator .   This dataset is a set from which we can create   2subsets , for instance X = fAliceg , X=   fAlice;Bobg , etc . All these subsets form a uni-   verseX , that isX;X;2X , and each of them   is also called ( a bit ambiguously ) a dataset .   Deﬁnition 2.1 . Any two datasets X;X2X are   called neighboring , if they differ in one person .   For example , X = fAliceg;X = fBobgor   X = fAlice;Bobg;X = fBobgare neighboring ,   whileX = fAliceg;X = fAlice , Bob , Charlie g   are not .   Deﬁnition 2.2 . Numeric query is any function f   applied to a dataset Xand outputting a real-   valued vector , formally f : X!R.   For example , numeric queries might return an   average income ( f!R ) , number of persons   in the database ( f!R ) , or a textual summary   of medical records of all persons in the database   represented as a dense vector ( f!R ) . The   query is simply something we want to learn from   the dataset . A query might be also an identity   function that just ‘ copies ’ the input , e.g. , f(X=   f(1;0)g)!(1;0)for a real - valued dataset X=   f(1;0)g .   An attacker who knows everything about Bob ,   Charlie , and others would be able to reveal Al-   ice ’s private information by querying the dataset   and combining it with what they know already .   Differentially private algorithm ( or mechanism )   M(X;f)thus randomly modiﬁes the query out-   put in order to minimize and quantify such attacks .   Smith and Ullman ( 2021 ) formulate the principle   of differential privacy as follows : “ No matter what   they know ahead of time , an attacker seeing the   output of a differentially private algorithm would   draw ( almost ) the same conclusions about Alice   whether or not her data were used . ”   Let a DP - mechanism M(X;f)have an arbi-   trary rangeR(a generalization of our case of nu-   meric queries , for which we would have R = R ) .   Differential privacy is then deﬁned as   Pr(XjM(X;f ) = z )   Pr(XjM(X;f ) = z)exp(")Pr(X )   Pr(X)(1)for all neighboring datasets X;Xand allz2   R , where Pr(X)andPr(X)is our prior knowl-   edge ofXandX. In words , our posterior knowl-   edge ofXorXafter observing zcan only grow   by factor exp(")(Mironov , 2017 ) , where " is apri-   vacy budget ( Dwork and Roth , 2013 ) .   3 Analysis of DPText   In the heart of the model , DPText relies on the   standard Laplace mechanism which takes a real-   valued vector and perturbs each element by a ran-   dom draw from the Laplace distribution .   Formally , let zbe a real - valued d - dimensional   vector . Then the Laplace mechanism outputs a   vector ~zsuch that for each index i= 1;:::;d   ~z = z+s ( 2 )   where each sis drawn independently from a   Laplace distribution with zero mean and scale b   that is proportional to the ` sensitivity and the   privacy budget " , namely   sLap   = 0;b=   "    ( 3 )   The Laplace mechanism satisﬁes differential   privacy ( Dwork and Roth , 2013 ) .   3.1 Reparametrization trick and inverse   CDF sampling   DPText employs the variational autoencoder ar-   chitecture in order to directly optimize the amount   of noise added in the latent layer parametrized   by " . In other words , the scale of the Laplace   distribution becomes a trainable parameter of the   network . As directly sampling from a distribu-   tion is known to be problematic for end - to - end   differentiable deep networks , DPText borrows the   reparametrization trick from Kingma and Welling   ( 2014 ) .   In a nutshell , the reparametrization trick decou-   ples drawing a random sample from a desired dis-   tribution ( such as Exponential , Laplace , or Gaus-   sian ) into two steps : First draw a value from   another distribution ( such as Uniform ) , and then   transform it using a particular function , mainly the   inverse continuous density function ( CDF ) .   As a matter of fact , sampling using the in-   verse CDF is a well - known and widely used772method ( Devroye , 1986 ; Ross , 2012 ) and forms   the backbone of probability distribution generators   in many popular frameworks .   3.2 Inverse CDF of Laplace distribution   The inverse cumulative distribution function of   Laplace distribution Lap(;b)is :   F(u ) =  bsgn(u 0:5 ) ln(1 2ju 0:5j )   ( 4 )   whereuUni(0;1)is drawn from a standard   uniform distribution ( Sugiyama , 2016 , p. 210 ) ,   ( Nahmias and Olsen , 2015 , p. 303 ) . An equivalent   expression without the sgnand absolute functions   is derived , e.g. , by Li et al . ( 2019 , p. 166 ) as   F(u ) =(   bln(2u ) +  ifu<0:5    bln(2(1 u ) ) ifu0:5   ( 5 )   where again uUni(0;1 ) .   An alternative sampling strategy , as shown , e.g. ,   by Al - Shuhail and Al - Dossary ( 2020 , p. 62 ) , as-   sumes that the random variable is drawn from a   shifted , zero - centered uniform distribution   vUni (  0:5;+0:5 ) ( 6 )   and transformed through the following function   F(v ) =  bsgn(v ) ln(1 2jvj ) ( 7 )   While both ( 4 ) and ( 7 ) generate samples from   Lap(;b ) , note the substantial difference between   uandv , since each is drawn from a different uni-   form distribution ( see visualizations in Fig . 1 ) .   3.3 Proofs of DPText violating DP   According to Eq . 3 in ( Alnasser et al . , 2021 ) , Eq . 9   in ( Beigi et al . , 2019a ) which is an extended ver-   sion of ( Beigi et al . , 2019b ) , in Eq . 14 in ( Beigi   et al . , 2021 ) , and personal communication to con-   ﬁrm the formulas , the main claim of DPText is as   follows ( rephrased ):   DPText utilizes the Laplace mech-   anism , which is DP ( Dwork and Roth ,   2013 ) . It implements the mechanism as   follows : Sampling a value from stan-   dard uniform   vUni(0;1 ) ( 8)   and transforming using   F(v ) =  bsgn(v ) ln(1 2jvj )   ( 9 )   is equivalent to sampling noise from   Lap(b ) .   This claim is unfortunately false , as it mixes up   both approaches introduced in Sec . 3.2 . As a con-   sequence , the Laplace mechanism using such sam-   pling is not DP , which we will ﬁrst prove formally .   Theorem 3.1 . Sampling using inverse CDF as   in DPText using ( 8) and ( 9 ) does not produce   Laplace distribution .   Proof . We will rely on the standard proof of sam-   pling from inverse CDF ( see Appendix A ) . The   essential step of that proof is that the CDF is in-   creasing on the support of the uniform distribu-   tion , that is on [ 0;1 ] . However , Fas used in   ( 9 ) is increasing only on interval [ 0;0:5](Fig . 1 ) .   Forv0:5 , we get negative argument to lnwhich   yields a complex function , whose real part is even   decreasing . Therefore ( 9 ) is not CDF of any prob-   ability distribution , if used with Uni ( 0;1 ) .   As a consequence , the output ln(v0)arbi-   trarily depends on the particular implementation .   Innumpy , it is NaN with a warning only . There-   fore this function samples only positive or NaN   numbers . Since DPText sources are not publicly   available , we can only assume that NaN numbers773are either replaced by zero , or the sampling pro-   ceeds as long as the desired number of samples   is reached ( discarding NaNs ) . In either case , no   negative values can be obtained . See Fig . 3 in the   Appendix for various Laplace - based distributions   sampled with different techniques including pos-   sible distributions sampled in DPText .   Theorem 3.2 . DPText with private mechanism   based on ( 8) and ( 9 ) fails to guarantee differential   privacy .   Proof . We rely on the standard proof of the   Laplace mechanism as shown , e.g , by Habernal   ( 2021 ) . Let X= 0 andX= 1 be two neigh-   boring datasets , and the query fbeing the iden-   tity query , such that it outputs simply the value of   X. Let the DPText mechanism M(X;f)outputs   a particular value z.   In order to being differentially private , mecha-   nismM(X;f)has to fulﬁll the following bound   of the privacy loss :   Pr(M(X ) = z )   Pr(M(X ) = z )  exp ( " ) ( 10 )   for all neighboring datasets X;X2 X and all   outputsz2R from the range ofM , provided that   our priors over XandXare uniform ( cf . Eq . 1 ) .   Fixz= 0:1 . Then Pr(M(X ) = 0:1)will have   a positive probability ( recall it takes the query out-   putf(X= 0 ) = 0 and adds a random number   drawn from the probability distribution , which is   always positive as shown in Theorem 3.1 . ) How-   everPr(M(X ) = 0:1)will be zero , as the query   outputf(X= 1 ) = 1 will be added again only   a positive random number and thus never be less   than1 . By plugging this into ( 10 ) , we obtain   Pr(M(X ) = 0:1 )   Pr(M(X ) = 0:1 )  = Pr>0   Pr = 0exp(")(11 )   which results in an inﬁnity privacy loss and vio-   lates differential privacy .   4 Empirical sanity check algorithm   It is impossible to empirically verify that a given   DP - mechanism implementation is actually DP   ( Ding et al . , 2018 ) . However , it is possible to de-   tect a DP - violating mechanism with a fair degree   of certainty . We propose a general sanity checkapplicable to any real - valued DP mechanism , such   as the Laplace mechanism , DPText , or any other .   We start by constructing two neighboring   datasetsX(Alice ) and X(Bob ) such that   X= ( 0 ; : : : ; 0)consists ofnzeros andX=   ( 1 ; : : : ; 1)consists ofnones . The dimensionality   n2f1;2;:::gis a hyperparameter of the experi-   ment . We employ a synthetic data release mecha-   nism ( also called local DP ) . The mechanism takes   XorXand outputs its privatized version of the   same dimensionality n , so that the zeros or ones   are ‘ noisiﬁed ’ real numbers . The query sensitivity   isn .   Thanks to the post - processing lemma , any post-   processing of DP output remains DP . We can thus   turn the output real vector back to all zeros or all   ones , simply by rounding to closest 0or1and   applying majority voting . This process is in fact   our reconstruction attack : given a privatized vec-   tor , we try to guess what the original values were ,   either all zeros or all ones .   What our attacker is doing , and what DP pro-   tects , is that if Alice gives us her privatized data ,   we can not tell whether her private values were all   zeros or all ones ( up to a given factor ) ; the same   for Bob .   By deﬁnition ( 1 ) and having no prior knowledge   overXandXapart from the fact that the val-   ues are correlated , our attacker can not exceed the   guaranteed privacy loss exp ( " ):   Pr(XjM(X;f ) = z )   Pr(XjM(X;f ) = z)exp ( " ) ( 12 )   We can estimate the conditional probability   Pr(XjM(X;f ) = z)using maximum likelihood   estimation ( MLE ) simply as our attacker ’s preci-   sion : How many times the attacker reconstructed   trueXvalues given the observed privatized vec-   tor . We can do the same for estimating the condi-   tional probability of X. In particular , we repeat-   edly run each DP mechanism over XandX10   million times each , which gives very precise MLE   estimates even for small " .774   5 Results and discussion   For the sake of completeness , we implemented   two extreme baselines : One that simply copies   input ( no privacy ) and other one completely ran-   dom regardless of the input ( maximum privacy ) ;   these are shown in Figure 2 left . The vanilla   Laplace mechanism behaves as expected ; all em-   pirical losses for all dimensions ( 1 up to 128 ) are   bounded by " . We re - implemented the Laplace   mechanism from ADePT ( Krishna et al . , 2021 )   that , due to wrong sensitivity , has been shown the-   oretically as DP - violating ( Habernal , 2021 ) . We   empirically conﬁrm that ADePT suffered from   the curse of dimensionality as the privacy loss   explodes for larger dimensions . The last panel   conﬁrms our previous theoretical DPText results ,   which ( regardless of dimensionality ) has inﬁnite   privacy loss .   Note that we constructed the dataset carefully as   two neighboring multidimensional correlated data   that are as distant from each other as possible in   the(0;1)space . However , DP must guarantee   privacy for any datapoints , even the worst case   scenario , as shown by the correct Laplace mech-   anism .   6 Conclusion   We formally proved that DPText ( Beigi et al . ,   2019b , a ; Alnasser et al . , 2021 ; Beigi et al . , 2021 )   is not differentially private due to wrong sampling   in its reparametrization trick . We also proposedan empirical sanity check that conﬁrmed our ﬁnd-   ings and can help to reveal potential errors in DP   mechanism implementations for NLP .   7 Ethics Statement   We declare no conﬂict of interests with the authors   of DPText , we do not even know them personally .   The purpose of this paper is strictly scientiﬁc .   Acknowledgements   The independent research group TrustHLT is sup-   ported by the Hessian Ministry of Higher Edu-   cation , Research , Science and the Arts . Thanks   to Cecilia Liu , Haau - Sing Li , and the anonymous   reviewers for their helpful feedback . A special   thanks to Condor airlines , whose greed to make   passengers pay for everything resulted in the most   productive transatlantic ﬂights I ’ve ever had .   References775   A Proof of sampling from inverse CDF   Important fact 1 : A random variable Uis uni-   formly distributed on [ 0;1]if the following holds   UUni(0;1 ) ( ) Pr(Uu ) = u : ( 13 )   Important fact 2 : For any function g()with an   inverse function g( ) , the following holds   g(g(x ) ) = x;g(g(x ) ) = x : ( 14)776   Important fact 3 : For any increasing function g( ) ,   we have by deﬁnition   xy=)g(x)g(y ): ( 15 )   We know that Pr(Xa)is a shortcut for prob-   ability of event Edeﬁned using the set - builder   notation asE = fs2    : X(s)ag . Then   by plugging ( 15 ) into the predicate of E , we ob-   tain an equal set , namely event E = fs2    :   g(X(s))g(a)g , for which the probability must   be the same . Therefore for any random variable X   and increasing function g()we have   Pr(Xa ) = Pr(g(X)g(a ) ): ( 16 )   Theorem A.1 . LetUbe a uniform random vari-   able on [ 0;1 ] . LetXbe a continuous random vari-   able with CDF ( cumulative distribution function )   F( ) . LetYbe deﬁned such that Y = F(U ) .   ThenYhas CDFF( ) .   Proof . FunctionF()is the CDF of a continuous   random variable X , and as a CDF its range is   [ 0;1 ] . Also , ifF()is strictly increasing , it has   a unique inverse function F()deﬁned on [ 0;1 ] .   We deﬁnedY = F(U ) , so consider   Pr(Yy ) = Pr    F(U)y   : ( 17 )   SinceF()is increasing , using ( 16 ) we get   Pr(Yy ) = Pr    F(F(U))F(y)   :( 18 )   Now plugging ( 14 ) we obtain   Pr(Yy ) = Pr(UF(y ) ) ; ( 19 )   and ﬁnally by ( 13 )   Pr(Yy ) = F(y ): ( 20 )   For an overview of proofs of Theorem A.1 see   ( Angus , 1994).777