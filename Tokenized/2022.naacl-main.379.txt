  Minki KangJinheon BaekSung Ju Hwang   KAIST , AITRICS   { zzxc1133 , jinheon.baek , sjhwang82}@kaist.ac.kr   Abstract   Pre - trained language models ( PLMs ) have   achieved remarkable success on various nat-   ural language understanding tasks . Sim-   ple ﬁne - tuning of PLMs , on the other hand ,   might be suboptimal for domain - speciﬁc tasks   because they can not possibly cover knowl-   edge from all domains . While adaptive   pre - training of PLMs can help them obtain   domain - speciﬁc knowledge , it requires a large   training cost . Moreover , adaptive pre - training   can harm the PLM ’s performance on the   downstream task by causing catastrophic for-   getting of its general knowledge . To over-   come such limitations of adaptive pre - training   for PLM adaption , we propose a novel do-   main adaption framework for PLMs coined   asKnowledge- Augmented Language model   Adaptation ( KALA ) , which modulates the in-   termediate hidden representations of PLMs   with domain knowledge , consisting of entities   and their relational facts . We validate the per-   formance of our KALA on question answer-   ing and named entity recognition tasks on mul-   tiple datasets across various domains . The   results show that , despite being computation-   ally efﬁcient , our KALA largely outperforms   adaptive pre - training . Code is available at :   https://github.com/Nardien/KALA .   1 Introduction   Pre - trained Language Models ( PLMs ) ( Devlin   et al . , 2019 ; Brown et al . , 2020 ) have shown to   be effective on various Natural Language Under-   standing ( NLU ) tasks . Although PLMs aim to ad-   dress diverse downstream tasks from various data   sources , there have been considerable efforts to   adapt the PLMs to speciﬁc domains — distributions   over the language characterizing a given topic or   genre ( Gururangan et al . , 2020 ) — for which the ac-   quisition of domain knowledge is required to accu-   rately solve the downstream tasks ( e.g. , Biomedical   Named Entity Recognition ( Dogan et al . , 2014)).Figure 1 :   This problem , known as Language Model Adap-   tation , can be viewed as a transfer learning prob-   lem ( Yosinski et al . , 2014 ; Ruder , 2019 ) under   domain shift , where the model is pre - trained on   the general domain and the labeled distribution is   available for the target domain - speciﬁc task . The   most prevalent approach to this problem is adaptive   pre - training ( Figure 2a ) which further updates all   parameters of the PLM on a large domain - speciﬁc   or curated task - speciﬁc corpus , with the same pre-   training strategy ( e.g. , masked language modeling )   before ﬁne - tuning it on the downstream task ( Belt-   agy et al . , 2019 ; Lee et al . , 2020 ; Gururangan et al . ,   2020 ) . This continual pre - training of a PLM on the   target domain corpus allows it to learn the distri-   bution of the target domain , resulting in improved   performance on domain - speciﬁc tasks ( Howard and   Ruder , 2018 ; Han and Eisenstein , 2019 ) .   While it has shown to be effective , adaptive pre-   training has obvious drawbacks . First , it is com-   putationally inefﬁcient . Although a PLM becomes   more powerful with the increasing amount of pre-   training data ( Gururangan et al . , 2020 ) , further   pre - training on the additional data requires larger   memory and computational cost as the dataset size   grows ( Bai et al . , 2021 ) . Besides , it is difﬁcult to   adapt the PLM to a new domain without forgetting   the general knowledge it obtained from the initial   pretraining step , since all pre - trained parameters   are continually updated to ﬁt the domain - speciﬁc   corpus during adaptive pre - training ( Chen et al . ,   2020 ) . This catastrophic forgetting of the task-5144   general knowledge may lead to the performance   degradation on the downstream tasks . In Figure 1 ,   we show that adaptive pre - training with more train-   ing steps could lead to performance degeneration .   Thus , it would be preferable if we could adapt   the PLM to the domain - speciﬁc task without costly   adaptive pre - training . To this end , we aim to inte-   grate the domain - speciﬁc knowledge into the PLM   directly during the task - speciﬁc ﬁne - tuning step ,   as shown in Figure 2b , eliminating the adaptive   pre - training stage . Speciﬁcally , we ﬁrst note that   entities andrelations are core building blocks of   the domain - speciﬁc knowledge that are required   to solve for the domain - speciﬁc downstream tasks .   Clinical domain experts , for example , are familiar   with medical terminologies and their complex re-   lations . Then , to represent the domain knowledge   consisting of entities and relations , we introduce   theEntity Memory , which is the source of entity   embeddings but independent of the PLM parame-   ters ( See Entity Memory in Figure 2b ) . Then , we   further exploit the relational structures of the enti-   ties by utilizing a Knowledge Graph ( KG ) , which   denotes the factual relationships between entities ,   as shown in Knowledge Graph of Figure 2b .   The remaining step is how to integrate the knowl-   edge into the PLM during ﬁne - tuning . To this   end , we propose a novel layer named Knowledge-   conditioned Feature Modulation ( KFM , § 3.2 ) ,   which scales and shifts the intermediate hidden rep-   resentations of PLMs by conditioning them with   retrieved knowledge representations . This knowl-   edge integration scheme has several advantages .   First , it does not modify the original PLM architec-   ture , and thus could be integrated into any PLMs   regardless of their architectures . Also , it only re - quires marginal computational and memory over-   head , while eliminating the need of excessive fur-   ther pre - training ( Figure 1 ) . Finally , it can effec-   tively handle unseen entities with relational knowl-   edge from the KG , which are suboptimally em-   bedded by adaptive pre - training . For example , as   shown in Figure 2 , an entity restenosis does not   appear in the training dataset for ﬁne - tuning , thus   adaptive pre - training only implicitly infers them   within the context from the broad domain corpus .   However , we can explicitly represent the unknown   entity by aggregating the representations of known   entities in the entity memory ( i.e. , in Figure 2 ,   neighboring entities , such as asthma andpethidine ,   are used to represent the unseen entity restenosis ) .   We combine all the previously described com-   ponents into a novel language model adapta-   tion framework , coined as Knowledge- Augmented   Language model Adaptation ( KALA ) ( Figure 3 ) .   We empirically verify that KALA improves the   performance of the PLM over adaptive pre - training   on various domains with two knowledge - intensive   tasks : Question Answering ( QA ) and Named Entity   Recognition ( NER ) . Our contribution is threefold :   •We propose a novel LM adaptation framework ,   which augments PLMs with entities and their re-   lations from the target domain , during ﬁne - tuning   without any further pre - training . To our knowl-   edge , this is the ﬁrst work that utilizes the struc-   tured knowledge for language model adaptation .   •To reﬂect structural knowledge into the PLM , we   introduce a novel layer which scales and shifts   the intermediate PLM representations with the   entity representations contextualized by their re-   lated entities according to the KG.5145•We show that our KALA signiﬁcantly enhances   the model ’s performance on domain - speciﬁc QA   and NER tasks , while being signiﬁcantly more   efﬁcient over existing LM adaptation methods .   2 Related Work   Language Model Adaptation Nowadays , trans-   fer learning ( Howard and Ruder , 2018 ) is a dom-   inant approach for solving Natural Language Un-   derstanding ( NLU ) tasks . This strategy ﬁrst pre-   trains a Language Model ( LM ) on a large and un-   labeled corpus , then ﬁne - tunes it on downstream   tasks with labeled data ( Devlin et al . , 2019 ) . While   this scheme alone achieves impressive performance   on various NLU tasks , adaptive pre - training of the   PLM on a domain - speciﬁc corpus helps the PLM   achieve better performance on the domain - speciﬁc   tasks . For example , Lee et al . ( 2020 ) demonstrated   that a further pre - trained LM on biomedical doc-   uments outperforms the original LM on biomed-   ical NLU tasks . Also , Gururangan et al . ( 2020 )   showed that adaptive pre - training of the PLM on   the corpus of a target domain ( Domain - adaptive   Pre - training ; DAPT ) or a target task ( Task - adaptive   Pre - training ; TAPT ) improves its performance on   domain - speciﬁc tasks . However , above approaches   generally require a large amount of computational   costs for pre - training .   Knowledge - aware LM Accompanied with in-   creasing sources of knowledge ( Vrandecic and   Krötzsch , 2014 ) , some prior works have proposed   to integrate external knowledge into PLMs , to en-   hance their performance on tasks that require struc-   tured knowledge . For instance , ERNIE ( Zhang   et al . , 2019 ) and KnowBERT ( Peters et al . , 2019 )   incorporate entities as additional inputs in the pre-   training stage to obtain a knowledge - aware LM ,   wherein a pre - trained knowledge graph embedding   from Wikidata ( Vrandecic and Krötzsch , 2014 ) is   used to represent entities . Entity - as - Experts ( Févry   et al . , 2020 ) and LUKE ( Yamada et al . , 2020 ) use   the entity memory that is pre - trained along with   the LMs from scratch . ERICA ( Qin et al . , 2021 )   further uses the fact consisting of entities and their   relations in the pre - training stage of LMs from   scratch . Previous works aim to integrate external   knowledge into the LMs during the pre - training   step to obtain a universal knowledge - aware LM   that requires additional parameters for millions of   entities . In contrast to this , our framework aims to   efﬁciently modify a general PLM for the domain - speciﬁc task with a linear modulation layer scheme   discussed in Section 3.2 , during ﬁne - tuning .   3 Method   3.1 Problem Statement   Our goal is to solve Natural Language Understand-   ing ( NLU ) tasks for a speciﬁc domain , with a   knowledge - augmented Language Model ( LM ) . We   ﬁrst introduce the NLU tasks we target , followed   by the descriptions of the proposed knowledge-   augmented LM . After that , we formally deﬁne the   ingredients for structured knowledge integration .   NLU tasks The goal of an NLU task is to predict   the labelyof the given input instance x , where the   inputxcontains the sequence of tokens ( Devlin   et al . , 2019):x= [ w , w, ... ,w ] . Then , given   a training datasetD={(x , y ) } , the objec-   tive is to maximize the log - likelihood as follows :   maxL(θ):= max / summationdisplaylogp(y|x;θ ) ,   p(y|x;θ ) = g(H;θ),H = f(x;θ ) ,   wherefis an encoder of the PLM which outputs   contextualized representation Hfromx , andgis   a decoder which models the probability distribu-   tionpof the labely , with trainable parameters   θ= ( θ , θ ) . If the LM is composed of L - layers of   transformer blocks ( Devlin et al . , 2019 ) , the func-   tionfis decomposed to multiple functions f=   [ f, ... ,f ] , where each block gets the output of   the previous block as the input : H = f(H ) .   Knowledge - Augmented Language Model The   conventional learning objective deﬁned above   might be sufﬁcient for understanding the texts if   the tasks require only the general knowledge stored   in PLMs . However , it is suboptimal for tackling   domain - speciﬁc tasks since the general knowledge   captured by the parameters θmay not include the   knowledge required for solving the domain - speciﬁc   tasks . Thus , contextualizing the texts by the do-   main knowledge , captured by the domain - speciﬁc   entities and their relations , is more appropriate for   handling such domain - speciﬁc problems .   To this end , we propose a function h(·;φ)which   augments PLMs conditioned on the domain knowl-   edge . Formally , the objective for a NLU task with5146our knowledge - augmented LM is given as follows :   maxL(θ , φ):= max / summationdisplaylogp(y|x;θ , φ ) ,   p(y|x;θ , φ ) = g(˜H;θ ) ,   ˜H = f(H , h(H , E , M , G;φ);θ ) ,   whereφis parameters for the function h , Eis the set   of entities , Mis the set of corresponding mentions ,   andGis a knowledge graph . In the following ,   we will describe the deﬁnition of the knowledge-   related inputsE , M , G , and the details of h(·,φ ) .   Deﬁnition 1 ( Entity and Mention ) . Given a se-   quence of tokens x= [ w, ... ,w ] , letEbe   a set of entities in x. Then an entitye∈ E   is composed of one or multiple adjacent tokens   within the input text : [ w, ... ,w]/subsetsqequalx . Here ,   m= ( m , m)is amention that denotes the start   and end locations for the entity within the input   tokensx , which term is commonly used for deﬁn-   ing entities ( Févry et al . , 2020 ) . Consequently , for   each given input x , there are a set of entities   E={e, ... ,e}and their corresponding men-   tionsM={m, ... ,m } . For example , given   an inputx= [ New , York , is , a , city ] , we have two   entitiesE={New_York , city}and their associated   mentionsM={(1,2),(4,4 ) } .   We further construct the entity vocabulary   E=/uniontextE , which consists of all entities   appearing in the training dataset . However , at test   time , we may encounter unseen entities that are not   inE. To tackle this , we regard unknown entities   as the null entity e , so that∀e∈E∪{e } .   Deﬁnition 2 ( Entity Memory ) . Given a set of   all entitiesE∪{e } , we represent them in the   continuous vector ( feature ) space to learn meaning-   ful entity embeddings . In order to implement this ,   we deﬁne the entity memory E∈R   that comprises of an entity e∈Ras a key and   its embedding e∈Ras its value . Also , to ac-   cess the value in the entity memory , we deﬁne the   point - wise memory access function EntEmbed   which takes an entity as an input . For instance , e=   EntEmbed ( New_York ) returns the embedding of   theNew_York entity , ande = EntEmbed ( e ) re-   turns the zero embedding . This entity memory E   is the part of the parameter φused in function h. Deﬁnition 3 ( Knowledge Graph ) . Since the en-   tity memory alone can not represent relational in-   formation between entities , we further deﬁne a   Knowledge Graph ( KG)Gthat consists of a set   of factual triplets{(h , r , t ) } , where the head and   the tail entities , handt , are the elements of E ,   and a relation ris an element of a set of relations   R : h , t∈E andr∈R . We assume that a pre-   constructed KGGisgiven for each input x ,   and provide the details of the KGs and how to con-   struct them in Appendix A.   3.2 Knowledge - conditioned Feature   Modulation on Transformer   The remaining problem is how to augment a PLM   by conditioning it on the domain - speciﬁc knowl-   edge , through the function h. An effective ap-   proach to do so without stacking additional layers   on top of the LM is to interleave the knowledge   fromhwith the pre - trained parameters of the lan-   guage model ( Devlin et al . , 2019 ) consisting of   transformer layers ( Vaswani et al . , 2017 ) . Before   describing our interleaving method in detail , we   ﬁrst describe the Transformer architecture .   Transformer Given|x|token representations   H= [ h, ... ,h]∈Rfrom the layer   l−1wheredis the embedding size , each trans-   former block outputs the contextualized representa-   tions for all tokens . In detail , the l - th block consists   of the multi - head self - attention ( Attn ) layer and the   residual feed - forward ( FF ) layer as follows :   ˆH = LN(H+Attn ( H ) )   FF(ˆH ) = σ(ˆH·W)·W ,   H = LN(ˆH+FF(ˆH ) ) ,   whereLNis a layer normalization ( Ba et al . , 2016 ) ,   σis an activation function ( Hendrycks and Gimpel ,   2016),W∈RandW∈Rare weight   matrices , and dis an intermediate hidden size . We   omit the bias term for brevity .   Linear Modulation on Transformer An effec-   tive yet efﬁcient way to fuse knowledge from differ-   ent sources without modifying the original model   architecture is to scale and shift the features of   one source with respect to the data from another   source ( Dumoulin et al . , 2018 ) . This scheme of   feature - wise afﬁne transformation is effective on   various tasks , such as language - conditioned image   reasoning ( Perez et al . , 2018 ) or style - transfer in   image generation ( Huang and Belongie , 2017).5147   Motivated by them , we propose to linearly trans-   form the intermediate features after the layer nor-   malization of the transformer - based PLM , condi-   tioned on the knowledge sources E , M , G. We term   this method as the Knowledge - conditioned Fea-   ture Modulation ( KFM ) , described as follows :   Γ , B,˜Γ,˜B = h(H , E , M , G;φ ) ,   ˆH = Γ ◦ LN(H+Attn ( H ) ) + B ,   FF(ˆH ) = σ(ˆH·W)·W ,   ˜H=˜Γ ◦ LN(ˆH+FF(ˆH ) ) + ˜B , ( 1 )   whereH∈Ris the matrix of hidden rep-   resentations from the previous layer , ◦ denotes   the hadamard ( element - wise ) product , and Γ=   [ γ, ... ,γ]∈R , B= [ β, ... ,β]∈   R.ΓandBare learnable modulation param-   eters from the function h , which are conditioned   by the entity representation . For instance , in Fig-   ure 3,γandβfor token ‘ New ’ are conditioned on   the corresponding entity New_York . However , if   tokens are not part of any entity ( e.g. , ‘ is ’ ) , γand   βfor such tokens are ﬁxed to 1and0 , respectively .   One notable advantage of our KFM is that mul-   tiple tokens associated to the identical entity are   affected by the same modulation ( e.g. , ‘ New ’ and   ‘ York ’ in Figure 3 ) , which allows the PLM to know   which adjacent tokens are in the same entity . This   is important for representing the tokens of the do-   main entity ( e.g. , ‘ cod ’ and ‘ on ’ ) , since the original   PLM might regard them as separate , unrelated to-   kens ( See analysis in § 5.5 with Figure 5 ) . However ,   with our KFM , the PLM can identify associated   tokens and embed them to be close to each other .   Then , how can we design such functional op-   erations inh ? The easiest way is to retrieve the   entity embedding of e , associated to the typical to - ken , from the entity memory E , and then use the   retrieved entity embedding as the input to obtain γ   andβfor every entity ( See Figure 3 ) . Formally , for   each entitye∈Eand its mention ( m , m)∈M ,   v = EntEmbed ( e ) ( 2 )   γ=1+h(v),β = h(v ) ,   ˜γ=1+h(v),˜β = h(v ) , m≤j≤m ,   wherevis the retrieved entity embedding from the   entity memory , h , h , h , andhare mutually in-   dependent Multi - Layer Perceptrons ( MLPs ) which   return a zero vector 0ife = e.   3.3 Relational Retrieval from Entity Memory   Although the simple access to the entity memory   can retrieve the necessary entity embeddings for   the modulation , this approach has obvious draw-   backs as it not only fails to reﬂect the relations with   other entities , but also regards unseen entities as   the same null entity e. If so , all unseen entities are   inevitably modulated by the same parameters even   if they have essentially different meaning .   To tackle these limitations , we further consider   the relational information between two entities that   are linked with a particular relation . For example ,   the entity New_York alone will not give meaningful   information . However , with two associated facts   ( New_York , instance of , city ) and ( New_York , coun-   try , USA ) , it is clear that New_York is a city in the   USA . Motivated by this observation , we propose   Relational Retrieval which leverages a KG Gto   retrieve entity embeddings from the memory , ac-   cording to the relations deﬁned in the given KG   ( See Figure 3 , right ) .   More speciﬁcally , our goal is to effectively uti-   lize the relations among entities in G , to improve5148theEntEmbed function in equation 2 . We tackle   this objective by utilizing a Graph Neural Net-   work ( GNN ) which learns feature representations   of each node using a neighborhood aggregation   scheme ( Hamilton et al . , 2017 ) , as follows :   v = UPDATE ( EntEmbed ( e ) ,   AGG({EntEmbed ( ˆe ) : ∀ˆe∈N(e;G ) } ) ) ,   whereN(e;G)is a set of neighboring entities of   the entitye , AGG is the function that aggregates em-   beddings of neighboring entities of e , andUPDATE   is the function that updates the representation of e   with the aggregated messages from AGG .   However , simple aggregation ( e.g. , mean ) can-   not reﬂect the relative importance on neigh-   boring nodes , thus we consider the attentive   scheme ( Velickovic et al . , 2018 ; Brody et al . , 2021 )   for neighborhood aggregation , to allocate weights   to the target entity ’s neighbors by their importance .   This scheme is helpful in ﬁltering out less use-   ful relations . Formally , we ﬁrst deﬁne a scoring   functionψthat calculates a score for every triplet   ( e , r , e ) , which is then used to weigh each node   during aggregation :   e = EntEmbed ( e),e = EntEmbed ( e ) ,   e= [ e / bardblr / bardble / bardblh ] ,   ψ(e , r , e , h ) = aσ(W·e ) ,   whereσis a nonlinear activation , e∈Ris   concatenated vector where /bardbldenotes the concate-   nation , a∈RandW∈Rare learnable   parameters , r∈Ris a embedding of the rela-   tion , andh∈Ris a context representation of   the entityeobtained from the intermediate hidden   states of the LM .   The scores obtained from ψare normalized   across all neighbors e∈N(e;G)with softmax :   α= softmax ( ψ(e , r , e ) )   = exp(ψ(e , r , e))/summationtextexp(ψ(e , r , e ) ) .   Then , we update the entity embedding with a   weighted average of the neighboring nodes with α   as an attention coefﬁcient , denoted as follows :   v = UPDATE / parenleftBig / summationtextα·e / parenrightBig   .(3)By replacing the EntEmbed function in equa-   tion 2 with the above GNN in equation 3 , we now   represent each entity with its relational information   in KG . This relational retrieval has several advan-   tages over simple retrieval of a single entity from   the entity memory . First , the relational retrieval   with KG can consider richer interactions among   entities , as described in Figure 3 .   In addition , we can naturally represent an un-   seen entity – which is not seen during training but   appears at test time – through neighboring aggre-   gation , which is impossible only with the entity   memory . In Figure 2 , we provide an illustrative   example of the unseen entity representation , where   the unseen entity restenosis is represented with a   weighted sum of representations of its neighboring   entities myocardial_infarction , asthma , and pethi-   dine , which is beneﬁcial when the set of entities   for training and test datasets have small overlaps .   4 Experiment   4.1 Tasks and Datasets   We evaluate our model on two NLU tasks : Ques-   tion Answering ( QA ) and Named Entity Recogni-   tion ( NER ) . For QA , we use three domain - speciﬁc   datasets : NewsQA ( News , Trischler et al . , 2017 )   and two subsets ( Relation , Medication ) of EMRQA   ( Clinical , Pampari et al . , 2018 ) . We use the Exact-   Match ( EM ) and the F1 score as evaluation met-   rics . For NER , we use three datasets from different   domains , namely CoNLL-2003 ( News , Sang and   Meulder , 2003 ) , WNUT-17 ( Social Media , Der-   czynski et al . , 2017 ) and NCBI - Disease ( Biomedi-   cal , Dogan et al . , 2014 ) . We use the F1 score as the   evaluation metric . We report statistics and detailed   descriptions of each dataset in Appendix B.2 .   4.2 Baselines   A direct baseline of our KALA is the adaptive   pre - training , which is commonly used to adapt the   PLM independent to the choice of a domain and   task . Also , to compare ours against a more pow-   erful baseline , we modify a recent method ( Chen   et al . , 2020 ) that alleviates forgetting of PLM dur-   ing ﬁne - tuning . Details for each baseline we use   are described as follows :   1.Vanilla Fine - Tuning ( FT ) : A baseline that di-   rectly ﬁne - tunes the LM on downstream tasks .   2.Fine - Tuning + more params : A baseline with   one more transformer layer at the end of the5149   LM . We use this baseline to show that the per-   formance gain of our model does not come from   the use of additional parameters .   3.Task - Adaptive Pre - training ( TAPT ) : A base-   line that further pre - trains the PLM on task-   speciﬁc corpus as in Gururangan et al . ( 2020 ) .   4.TAPT + RecAdam : A baseline that uses   RecAdam ( Chen et al . , 2020 ) during further   pre - training of PLMs ( i.e. , TAPT ) , to alleviate   catastrophic forgetting of the learned general   knowledge in PLMs from adaptive pre - training .   5.Domain - Adaptive Pre - training ( DAPT ) : A   strong baseline that uses a large - scale domain   corpus outside the training set during further pre-   training ( Gururangan et al . , 2020 ) , and requires   extra data and large computational overhead .   6.KALA ( pointwise ): A variant of KALA that   only uses the entity memory and does not use   the knowledge graphs .   7.KALA ( relational ): Our full model that uses   KGs to perform relational retrieval from the en-   tity memory .   4.3 Experimental Setup   We use the uncased BERT - base ( Devlin et al . , 2019 )   as the base PLM for all our experiments on QA   and NER tasks . For more details on training and   implementation , please see the Appendix B.4.4 Experimental Results   Performance on QA and NER tasks On both   extractive QA and NER tasks , our KALA out-   performs all baselines , including TAPT and   TAPT+RedcAdam ( Gururangan et al . , 2020 ; Chen   et al . , 2020 ) , as shown in Table 1 and 2 . These   results show that our KALA is highly effective   for the language model adaptation task . KALA   also largely outperforms DAPT ( Gururangan et al . ,   2020 ) which is trained with extra data and requires   a signiﬁcantly higher computational cost compare   to KALA ( See Figure 1 for the plot of efﬁciency ,   discussed in Section 5.3 ) .   Effect of Using more Parameters One may sus-   pect whether the performance of our KALA comes   from the increment of parameters . However , the   experimental results in Table 1 and 2 show that in-   creasing the parameters for PLM during ﬁne - tuning   ( + more params ) yields marginal performance im-   provements over naive ﬁne - tuning . This result con-   ﬁrms that the performance improvement of KALA   is not due to the increased number of parameters .   Importance of Relational Retrieval The perfor-   mance gap between KALA ( relational ) and KALA   ( point - wise ) shows the effectiveness of relational   retrieval for language model adaptation , which al-   lows us to incorporate relational knowledge into   the PLM . The relational retrieval also helps address   unseen entities , as discussed in Section 5.4 .   5 Analysis and Discussion   5.1 Ablation Studies   We perform an ablation study to see how much each   component contributes to the performance gain .   KFM Parameters We ﬁrst analyze the effect of   feature modulation parameters ( i.e. , gamma and   beta ) in transformers by ablating a subset of them   in Table 3 , in which we observe that using both5150   gamma and beta after both layer normalization on   a transformer layer obtains the best performance .   Architectural Variants We now examine the ef-   fectiveness of the proposed knowledge condition-   ing scheme in our KALA framework . To this end ,   we use or adapt the knowledge integration methods   from previous literature , to compare their effective-   ness . Speciﬁcally , we couple the following ﬁve   components with KALA : Entity - as - Experts ( Févry   et al . , 2020 ) , Adapter ( Houlsby et al . , 2019 ) , KT-   Net ( Yang et al . , 2019 ) , ERNIE ( Zhang et al . , 2019 ) ,   and ERICA ( Qin et al . , 2021 ) . Note that , most   of them were proposed for improving pre - training   from scratch , while we adapt them for ﬁne - tuning   under our KALA framework ( The details are given   inAppendix B.4 ) . As shown in Table 4 , our KFM   used in KALA outperforms all variants , demon-   strating the effectiveness of feature modulation in   the middle of transformer layers for ﬁne - tuning .   5.2 Robustness to Other PLMs   Although we believe our experimental results on   Table 1 , 2 with BERT ( Devlin et al . , 2019 ) are   enough to show the effectiveness of KALA across   different pre - trained language models ( PLMs ) , one   might be curious that KALA can work on even   other PLMs such as RoBERTa ( Liu et al . , 2020 ) .   Thus , to address such concerns , we additionally   conduct experiments on RoBERTa . As shown in   Table 5 , we observe that our KALA outperforms   all baselines except for one case ( Fine - Tuning +   more params on NewsQA ) . Thus , we believe that   our KALA would be useful to any PLMs , not de-   pending on speciﬁc PLMs .   5.3 Efﬁciency   Figure 1 illustrates the performance and training   FLOPs of KALA against baselines on the NewsQA   dataset . We observe that the performance of TAPT   decreases with the increased number of iterations ,   which could be due to forgetting of the knowledge   from the PLM . On the other hand , DAPT , while   not suffering from performance loss , requires huge   computational costs as it trains on 112 times larger   data for further pre - training ( See Appendix B.3   for detailed explanations on training data ) . On the   other hand , our KALA outperforms DAPT without   using external data , while requiring 17 times fewer   computational costs , which shows that KALA is   not only effective but also highly efﬁcient .   To further compare the efﬁciency in various as-   pects , we report GPU memory , training wall time ,   and training FLOPs for baselines and ours in Ta-   ble 6 . Through this , we verify that our KALA is   more efﬁcient to train for language model adapta-   tion settings than baselines . Note that the resource   requirement of KALA could be further reduced by   adjusting the size of the entity memory ( e.g. , remov-   ing less frequent entities ) . Therefore , to show the   ﬂexibility of our KALA on the typical resource con-   straint , we provide the experimental results on two   different settings ( i.e. , tuning the number of entities   in the entity memory ) – KALA with memory size   of 200 and 62.8k ( full memory ) in Appendix C.6.5151   5.4 Effectiveness on Unseen Entities   One remarkable advantage of our KALA is its abil-   ity to represent an unseen entity by aggregating   features of its neighbors from a given KG . To an-   alyze this , we ﬁrst divide all contexts into one of   Seen and Unseen , where Seen denotes the context   with less than 3 unseen entities , and then measure   the performance on the two subsets . As shown in   Figure 4 , we observe that the performance gain   of KALA over the baselines is much larger on   the Unseen subset , which demonstrates the effec-   tiveness of KALA ’s relational retrieval scheme to   represent unseen entities . DAPT also largely out-   performs ﬁne - tuning and TAPT as it is trained on   an extremely large external corpus for adaptive   pre - training . However , KALA even outperforms   DAPT in most cases , verifying that our knowledge-   augmentation method is more effective for tack-   ling domain - speciﬁc tasks . The visualization of   embeddings of seen and unseen entities in Fig-   ure 2 shows that KALA embeds the unseen entities   more closely to the seen entities , which explains   KALA ’s good performance on the Unseen subset .   5.5 Case Study   To better see how our KFM ( § 3.2 ) works , we show   the context and its fact , and then visualize repre-   sentations from the PLM modulated by the KFM.As shown in Figure 5 right , the token ‘ # # on ’ is not   aligned with their corresponding tokens , such as   ‘ ex ’ ( for exon ) and ‘ cod ’ ( for codon ) , in the baseline .   However , with our feature modulation that trans-   forms multiple tokens associated with the single   entity equally , the two tokens ( e.g. , ( ‘ ex ’ , ‘ # # on ’ ) ) ,   composing one entity , are closely embedded . Also ,   while the baseline can not handle the unseen entity   consisting of three tokens : ‘ re ’ , ‘ # # tina ’ , and ‘ # # l ’ ,   KALA embeds them closely by representing the   unseen retinal from the representation of its neigh-   borhood gene derived by the domain knowledge –   ( retinal , instance of , gene ) .   5.6 Extension to Generative Model   Our KALA framework is also applicable to   encoder - decoder PLMs by applying the KFM to the   encoder . Therefore , we further validate KALA ’s   effectiveness on the encoder - decoder PLMs on the   generative QA task ( Lee et al . , 2021 ) with T5-   small ( Raffel et al . , 2020 ) . Table 7 shows that   KALA largely outperforms baselines even with   such a generative PLM .   6 Conclusion   In this paper , we introduced KALA , a novel frame-   work for language model adaptation , which mod-   ulates the intermediate representations of a PLM   by conditioning it with the entity memory and the   relational facts from KGs . We validated KALA on   various domains of QA and NER tasks , on which   KALA signiﬁcantly outperforms relevant baselines   while being computationally efﬁcient . We demon-   strate that the success of KALA comes from both   KFM and relational retrieval , allowing the PLM to   recognize entities but also handle unseen ones that   might frequently appear in domain - speciﬁc tasks .   There are many other avenues for future work , in-   cluding the application of KALA on pre - training   of knowledge - augmented PLMs from scratch.5152Ethical Statements   Enhancing the domain converge of pre - traind lan-   guage models ( PLMs ) with external knowledge   is increasingly important , since the PLMs can not   observe all the data during training and can not   memorize all the necessary knowledge for solv-   ing down - stream tasks . Our KALA contributes to   this problem by augmenting domain knowledge   graphs for PLMs . However , we have to still con-   sider the accurateness of knowledge , i.e. , the fact in   the knowledge graph may not be correct , which af-   fects the model to generate incorrect answers . Also ,   the model ’s prediction performance is still far from   optimal . Thus , we should be aware of model ’s   failure from errors in knowledge and prediction , es-   pecially on high - risk domains ( e.g. , biomedicine ) .   Acknowledgement   This work was supported by Institute of Infor-   mation & communications Technology Planning   & Evaluation ( IITP ) grant funded by the Ko-   rea government ( MSIT ) ( No.2019 - 0 - 00075 , Ar-   tiﬁcial Intelligence Graduate School Program   ( KAIST ) and No . 2021 - 0 - 02068 , Artiﬁcial In-   telligence Innovation Hub ) ) , AITRICS , Samsung   Electronics ( IO201214 - 08145 - 01 ) , and the En-   gineering Research Center Program through the   National Research Foundation of Korea ( NRF )   funded by the Korean Government MSIT ( NRF-   2018R1A5A1059921 ) .   References5153515451555156   A Details on KG Construction   In this work , we propose to use the Knowledge   Graph ( KG ) that can deﬁne the relational informa-   tion among entities that only appear in each dataset .   However , unfortunately , most of the task datasets   do not contain such relational facts on its context ,   thus we need to construct them manually to obtain   the knowledge graph . In this section , we explain   the way of constructing the knowledge graph that   we used , consisting of facts of entities for each   context in the task dataset .   Relation extraction is the way how we obtain   the factual knowledge from the text of the target   dataset . To do so , we ﬁrst need to extract entities   and their corresponding mentions from the text , and   then link it to the existing entities in wikidata ( Vran-   decic and Krötzsch , 2014 ) . In order to do this , we   use the existing library named as spaCy , and open-   sourced implementation of Entity Linker . To sum   up , in our work , a set of entities Eand corre-   sponding mentions Mfor the given input x   are obtained through this step . Regarding a con-   crete example , please see format ( a ) in Figure 6 . In   the example , “ Text ” indicates the entity mention   within the input x , the “ start ” and “ end ” indicates   its mention position denoted as ( m , m ) , and “ i d ”   indicates the wikidata i d for the entity identiﬁcation   used in the next step .   To extract the relation among entities that we   obtained above , we use the scheme of Relation Ex-   traction ( RE ) . In other words , we use the trainedRE model to build our own knowledge base ( KB )   instead of using the existing KG directly from the   existing general - domain KB . Speciﬁcally , we ﬁrst   ﬁne - tune the BERT - base model ( Devlin et al . , 2019 )   for 2 epochs with 600k distantly supervised data   used in Qin et al . ( 2021 ) , where the Wikipedia doc-   ument and the Wikidata triplets are aligned . Then ,   we use the ﬁne - tuned BERT model to extract the   relations between entity pairs in the text . We use   the model with a simple bilinear layer on top of it ,   which is widely used scheme in the relation extrac-   tion literature ( Yao et al . , 2019 ) . For an example   of the extracted fact , please see format ( b ) in Fig-   ure 6 . In the example , “ h ” denotes the wikidata i d   of the head entity , “ r ” denotes the wikidata i d of   the extracted relation , and “ t ” denotes the wikidata   i d of the tail entity . In the relation extraction , the   model returns the categorical distribution over the   top 100 frequent relations . In general , the relation   of top-1 probability is used as the relation for the   corresponding entity pair . However , this approach   sometimes results in predicting no_relation   on most entity pairs . Thus , to obtain more rela-   tions , we further use the relation of top-2 probabil-   ity in the case where no_relation has a top-15157   probability but the top-2 probability is larger than   a certain threshold ( e.g. , > 0.1 ) . In Figure 6 , we   summarize our KG construction pipeline . In Ta-   ble 8 , we report the hyperparameters related to our   KG construction .   B Experimental Setup   In this section , we introduce the detailed setups for   our models and baselines used in Table 1 , 2 , and 4 .   B.1 Implementation Details   We use the Pytorch ( Paszke et al . , 2019 ) for the   implementation of all models . Also , to easily im-   plement the language model , we use the hugging-   face library ( Wolf et al . , 2020 ) containing vari-   ous transformer - based pre - trained language models   ( PLMs ) and their checkpoints .   Details for KALA In this paragraph , we de-   scribe the implementation details of the compo-   nents , such as four linear layers in the proposed   KFM , architectural speciﬁcations in the attention-   based GNN , and initialization of both the entity   memory and relational embeddings , in the follow-   ing . In terms of the functions h , h , h , andhin   the KFM of Equation 2 , we use two linear layers   with the ReLU ( Nair and Hinton , 2010 ) activation   function , where the dimension is set to 768 .   For relational retrieval , we implement the novel   GNN model based on GATv2 ( Brody et al . , 2021 )   provided by the torch - geometric package ( Fey and   Lenssen , 2019 ) . Speciﬁcally , we stack two GNN   layers with the RELU activation function and also   use the dropout with a probability of 0.1 . For at-   tention in our GNN , we mask the nodes of the null   entity , so that the attention score becomes zero for   them . Moreover , to obtain the context representa-   tion of the entity ( See Footnote 3 in the main paper )   used in the GNN attention , we use the scatter oper-   ationfor reduced computational cost .   For Entity Memory , we experimentally found   that initializing the embeddings of the entity mem-   ory with the contextualized features obtained fromthe pre - trained language model could be helpful .   Therefore , the dimension of the entity embedding   is set to the same as the language model d= 768 .   For relation embeddings , we randomly initialize   them , where the dimension size is set to 128 .   Location of KLM in the PLM Note that , the   number and location of the KFM layers inside the   PLM are hyperparameters . However , we empiri-   cally found that inserting one to three KFM layers   at the end of the PLM ( i.e. , after the 9th - 11th   layers of the BERT - base language model ) is ben-   eﬁcial to the performance ( See Appendix C.4 for   experiments on diverse layer locations ) .   B.2 Dataset Details   Here we describe the dataset details with its statis-   tics for two different tasks : extractive question an-   swering ( QA ) and named entity recognition ( NER ) .   Question Answering We evaluate models on   three domain - speciﬁc datasets : NewsQA , Rela-   tion , and Medication . Notably , NewsQA ( Trischler   et al . , 2017 ) is curated from CNN news articles .   Relation and Medication are originally part of the   emrQA ( Pampari et al . , 2018 ) , which is an auto-   matically constructed question answering dataset   based on the electrical medical record from n2c2   challenges . However , Yue et al . ( 2020 ) extract   two major subsets by dividing the entire dataset   into Relation and Medication and suggest the us-   age of sampled questions from the original em-   rQA dataset . Following the suggestion of Yue et al .   ( 2020 ) , we use only 1 % of generated questions of   Relation for training , validation , and testing . Also ,   we only use 1 % of generated questions of Medica-   tion for training and use 5 % of generated questions   of Medication for validation and testing . Since the   original emrQA is automatically generated based   on templates , the quality is poor – it means that   the original emrQA dataset was inappropriate to   evaluate the ability of the model to reason over   the clinical text since the most of questions can be5158   answered by the simple text matching . To over-   come this limitation , Yue et al . ( 2020 ) suggests   two ways to make the task more difﬁcult . First ,   they divide the question templates into easy and   hard versions and then use the hard question only .   Second , they suggest replacing medical terminolo-   gies in the question of the test set into synonyms   to avoid the trivial question which can be solvable   with a simple text matching . We use both methods   to Relation and Medication datasets to report the   performance of every model . For more details on   Relation and Medication datasets , please refer to   the original paper ( Yue et al . , 2020 ) . The statis-   tics of training , validation , and test sets on all QA   datasets are provided in Table 9 .   Named Entity Recognition We use three dif-   ferent domain - speciﬁc datasets for evaluating our   KALA on NER tasks : CoNLL-2003 ( Sang and   Meulder , 2003 ) ( News ) , WNUT-17 ( Derczynski   et al . , 2017 ) ( Social Network Service ) and NCBI-   Disease ( Dogan et al . , 2014 ) ( Biomedical ) . The   CoNLL-2003 is constructed from the manually cu-   rated 1,393 English news articles , including 301.4k   tokens , which has 9 class labels . The WNUT-   17 dataset consists of 65,124 emerging and rare   entities from social media ( e.g. , Twitter , Reddit ,   YouTube , to name a few ) , which has 13 class la-   bels . The NCBI - Disease dataset consists of the   793 PubMed articles from the biomedical domain ,   which contains 6,892 disease mentions and 790   disease concepts , and also has 3 class labels . The   statistics of training , validation , and test sets are   provided in Table 11 .   B.3 Training details   All experiments are constrained to be done with a   single 12 GB Geforce RTX 2080 Ti GPU for fair-   ness in terms of memory and the availability on the   academic budget , except for the DAPT and gener-   ative QA which use a single 48 GB Quadro 8000   GPU . KALA training needs 3 hours in wall time   with a single GPU . For all experiments , we select   the best checkpoint on the validation set . For the   summary of training setups , please see Table 10   and 12.5159Fine - tuning Setup In the following three para-   graphs , we explain the setting of ﬁne - tuning for   QA , NER , and generative QA tasks . For all ex-   periments on extractive QA tasks , we ﬁne - tune the   Pre - trained Language Model ( PLM ) for 2 epochs   with the weight decay of 0.01 , learning rate of 3e-5 ,   maximum sequence length of 384 , batch size of 12 ,   linear learning rate decay of 0.06 warmup rate , and   half precision ( Micikevicius et al . , 2018 ) .   For all experiments on NER tasks , we ﬁne-   tune the PLM for 20 epochs , where the learning   rate is set to 5e-5 , maximum sequence length is   set to 128 , and batch size is set to 32 . We use   AdamW ( Loshchilov and Hutter , 2019 ) as an opti-   mizer using BERT - base as the PLM .   For the generative QA task in Table 7 , we ﬁne-   tune the T5 - small ( Raffel et al . , 2020 ) for 4 epochs   with the learning rate of 1e-4 , maximum sequence   length of 512 , and batch size of 64 . We also use   the Adafactor ( Shazeer and Stern , 2018 ) optimizer .   Instead of training with the same optimizer as in   BERT for QA and NER , we instead use the inde-   pendent AdamW optimizer with the learning rate   of 1e-4 and weight decay of 0.01 to train the KALA   module with T5 .   Adaptive Pre - training Setup In this paragraph ,   we describe the experimental settings of adaptive   pre - training baselines , namely TAPT , TAPT ( +   RecAdam ) , and DAPT . For QA tasks , we further   pre - train the PLM for { 1,3,5,10 } epochs and then   report the best performance among them . Speciﬁ-   cally , reported TAPT result on NewsQA , Relation ,   and Medication are obtained by 1 epoch of fur-   ther pre - training . We use the weight decay of 0.01 ,   learning rate of 5e-5 , maximum sequence length of   384 , batch size of 12 , and linear learning rate decay   of 0.06 warmup rate , with a half - precision . Also ,   the masking ratio for the pre - training objective is   set to 0.15 , following the existing strategy intro-   duced in the original BERT paper ( Devlin et al . ,   2019 ) .   For NER tasks , we further pre - train the PLM   for 3 epochs across all datasets . In particular , the   learning rate is set to 5e-5 , batch size is set to 32 ,   and the maximum sequence length is set to 128 . We   also use AdamW ( Loshchilov and Hutter , 2019 ) as   the optimizer for all experiments .   In the case of T5 - small for generative QA in Ta-   ble 7 , we further pre - train the PLM for 4 epochs   with the learning rate of 0.001 , batch size of 64 ,   maximum sequence length of 384 , and Adafac - tor ( Shazeer and Stern , 2018 ) optimizer .   Regarding the setting of TAPT ( + RecAdam ) on   all tasks , we follow the best setting in the original   paper ( Chen et al . , 2020 ) – sigmoid as an annealing   function with annealing parameters : k= 0.5,t=   250 , and the pretraining coefﬁcient of 5000 .   For training with DAPT , we need an external   corpus having a large amount of data for adaptive   pre - training . Thus , we ﬁrst choose the datasets of   two domains – News and Medical . Speciﬁcally ,   as the source of corpus for the News domain , we   use the sampled set of 10 million News from the   RealNews dataset used in Gururangan et al . ( 2021 ) .   As the source of corpus for the Medical domain , we   use the set of approximately 100k passages from   the Medical textbook provided in Jin et al . ( 2020 ) .   The size of pre - training data used in DAPT is much   larger than TAPT . In other words , for experiments   on NewsQA , TAPT only uses ﬁne - tuning contexts   containing 5.8 million words from the NewsQA   training dataset , while DAPT uses more than a hun-   dred times larger data – enormous contexts contain-   ing about 618 million words from the RealNews   database . For both News and Medical domains ,   we further pre - train the BERT - base model for 50   epochs with the batch size of 64 , to match the sim-   ilar computational cost used in Gururangan et al .   ( 2020 ) . Other experimental details are the same as   TAPT described above .   B.4 Architectural Variant Details   In this subsection , we describe the details of archi-   tectural variants reported in Section 5.1 . For all   variants , we use the same KGs used in KALA .   Entity - as - Experts ( Févry et al . ( 2020 ) ; EaE )   utilizes the entity memory similar to our work , but   they use the parametric dense retrieval more like the   memory neural network ( Sukhbaatar et al . , 2015 ) .   Similar to Févry et al . ( 2020 ) ; Verga et al . ( 2021 ) ,   we change the formulation of query and memory   retrieval by using the mention representation of the   entity from the intermediate hidden states of PLMs ,   which is formally deﬁned as follows :   h=1   m−m+ 1 / summationdisplayh , ( 4 )   v= softmax(h·E)·E ,   wherehrepresents the average of token represen-   tations of the entity mention m= ( m , m ) . We   also give the supervised retrieval loss ( ELLoss5160 in Févry et al . ( 2020 ) ) , when training the EaE   model . With this retrieval , EaE also can repre-   sent the unseen entity e /∈E if we know the   mention boundary of the given entity on the con-   text . We believe it is expected to work well , if the   entity memory is pre - trained on the enormous text   along with the pre - training of the language model   from the scratch . However , it might underperform   for the language model adaptation scenario , since   it can fall into the problem of circular reasoning   – the PLM does not properly represent the unseen   entity , but it should predict which entity it is similar   from the representation . Regarding the integration   of the knowledge from the entity memory into the   PLM , the retrieved entity representation vis simply   added ( Peters et al . , 2019 ) to the hidden represen-   tationsHafter the transformer block as follows :   ˜H = H+h(v ) ( 5 )   wherehis Multi - Layer Perceptrons ( MLPs ) .   Adapter ( Houlsby et al . , 2019 ) is introduced to   ﬁne - tune the PLM only with a few trainable param-   eters , instead of ﬁne - tuning the whole parameters   of the PLM . To adapt this original implementa-   tion into our KALA framework , we replace our   Knowledge - conditioned Feature Modulation with   it , where the Adapter is used as the knowledge inte-   gration module . We interleave the layer of Adapter   after the feed - forward layer ( FF ) and before the   residual connection of the transformer block . Also ,   instead of only providing the LM hidden states as   an input , we concatenate the knowledge represen-   tation in Equation 3 to the LM hidden states . Note   that we ﬁne - tune the whole parameters following   our KALA setting , unlike ﬁne - tuning the parame-   ters of only Adapter layers in Houlsby et al . ( 2019 ) .   ERNIE ( Zhang et al . , 2019 ) is a notable PLM   model that utilizes the external KB as an input for   the language model . The key feature of ERNIE can   be summarized into two folds . First , they use the   multi - head self - attention scheme ( Vaswani et al . ,   2017 ) to contextualize the input entities . Second ,   ERNIE fuses the entity representation at the end   of the PLM by adding it to the corresponding lan-   guage representation . We assume that those two   features are important points of ERNIE . Therefore ,   instead of using a Graph Neural Network ( GNN )   layer , we use a multi - head self - attention layer to   contextualize the entity embeddings . Then , we add   it to a representation of the entity from the PLM ,   which is the same as the design in equation 5.KT - Net ( Yang et al . , 2019 ) uses knowledge as an   external input in the ﬁne - tuning stage for extractive   QA . Since they have a typical layer for integrating   existing KB ( Miller , 1995 ; Carlson et al . , 2010 )   with the PLM , we only adopt the self - matching   layer as the architecture variant of the KFM layer   used in our KALA framework . The computation   of the self - matching matrix in KT - Net is costly ,   i.e. , it requires a large computational cost that is   approximately 12 times larger than KALA .   ERICA ( Qin et al . , 2021 ) uses contrastive learn-   ing in LM pre - training to reﬂect the relational   knowledge into the language model . We use the   Entity Discrimination task from ERICA on the pri-   mary task of ﬁne - tuning . We would like to note that ,   as reported in Section 5 of the original paper ( Qin   et al . , 2021 ) , the use of ERICA on ﬁne - tuning has   no effect , since the size and diversity of entities and   relations in downstream training data are limited .   Such limited information rather harms the perfor-   mance , as it can hinder the generalization . In other   words , contrastive learning can not reﬂect the entity   and relation in the test dataset .   B.5 FLOPs Computation   In this subsection , we give detailed descriptions of   how the FLOPs in Figure 1 are measured . We ma-   jorly follow the script from the ELECTRA ( Clark   et al . , 2020 ) repository to compute the approxi-   mated FLOPs for all models including ours . For   FLOPs computation of our KALA , we addition-   ally include the FLOPs of the entity embedding   layer , linear layers for h , h , h , h , and GNN   layer . Since the GNN layer is implemented based   on the sparse implementation , we ﬁrst calculate   the FLOPs of the message propagation over one   edge , and then multiply it to the average number of   edges per node . Also , in terms of the computation   on mentions , we consider the maximum sequence   length of the context rather than the average num-   ber of mentions , to set the upper bound of FLOPs   for our KALA . Note that , in NewsQA training data ,   the average number of nodes is 57 , the average   number of edges for each node is 0.64 , and the av-   erage number of mentions in the context is 92.68 .   C Additional Experimental Results   In this section , we provide the analyses on the for-   getting of TAPT , entity memory , number of entities   and facts , location of the KLM layer , and values of   Gamma and Beta.5161   C.1 Analysis on forgetting of TAPT   In Figure 1 , we observe that the performance of   TAPT decreases as the number of training steps   increases . To get a concrete intuition on this par-   ticular phenomenon , we analysis what happens   in the Pre - trained Language Model ( PLM ) , when   we further pre - train it on the task - speciﬁc corpus .   Speciﬁcally , in Figure 7 , we visualize the Masked   Language Model ( MLM ) loss of TAPT on both   domain - speciﬁc corpus from the Relation dataset   and general corpus from the sampled Wikipedia   documents during the adaptive pre - traing . As Fig-   ure 7 shows , the test MLM loss increases while   the training MLM loss persistently increases as the   training step increases . This result indicates that   TAPT on domain - speciﬁc corpus may yield the   catastrophic forgetting of the general knowledge in   the PLM .   C.2 Effects of the Size of Entity Memory   In this subsection , we analyze how the size of en-   tity memory affects the performance of our KALA .   In Figure 8 , we plot the performance of KALA   on the NewsQA dataset by varying the number of   entity elements in the memory . Note that , we re-   duce the size of the entity memory by eliminating   the entity appearing fewer times . Thus , the results   are obtained by only considering the entities that   appear more than [ 1000,100,10,5,0]times , e.g. ,   0 means the model with full entity memory . As   shown in Figure 8 , we observe that the size of the   entity memory is larger , the performance of our   KALA is better in general . However , interestingly ,   we also observe that the smallest size of the entity   memory shows decent performance , which might   be due to the fact that some parameters in the entity   memory are stale . For more discussions on it in-   cluding visualization , please refer to Appendix D.2 .   Finally , we would like to note that , in Figure 1 , we   report the performance of our KALA in the case   of[1000,5,0](i.e . , considering entities appearing   more than [ 1000,5,0]times ) .   C.3 Effects of the Number of Entity and Fact   In this subsection , we aim to analyze which num-   bers of entities and facts per context are appropriate   to achieve good performance in NER tasks . Specif-   ically , we ﬁrst collect the contexts having more   than or equal to the knumber of entities ( or facts ) ,   and then calculate the performance difference from   our KALA to the ﬁne - tuning baseline . As shown   in Figure 9 , while there are no obvious patterns ,   performance improvements from the baseline are   consistent across a varying number of entities and   facts . This result suggests that our KALA is indeed   beneﬁcial when entities and facts are given to the   model , whereas the appropriate number of entities   and facts to obtain the best performance against the   baseline is different across datasets.5162   C.4 Effects of the Location of KFM   In the main paper and Appendix B.1 , we describe   that the location of the KFM layer inside the PLM   architecture is the hyperparameter . However , some-   one might wonder which location of KFM yields   the best performance , and what is the reason for   this . Therefore , in this section , we analyze where   we obtain the best performance in various locations   of the KFM layer on the NewsQA dataset . Specif-   ically , in Figure 10 , we show the performance of   our KALA with varying the location of the KFM   layer insider the BERT - base model . The results   demonstrate that the model with the KFM on the   last layer of the BERT - base outperforms all the   other choices . This might be because , as the ﬁnal   layer of the PLM is generally considered as the   most task - speciﬁc layer , our KFM interleaved in   the latest layer of BERT expressively injects the   task - speciﬁc information from the entity memory   and KGs , to such a task - speciﬁc layer .   C.5 Analysis on Values of Gamma and Beta   To see how much amount of value on gamma and   beta is used to shift and scale the intermediate hid-   den representations in transformer layers , we visu-   alize the modulation values , namely gamma and   beta , in Figure 11 . We ﬁrst observe that , as shown   in Figure 11 , the distribution of values of gamma   and beta approximately follow the Gaussian dis - tribution , with zero mean for beta and one mean   for gamma . Also , we notice that the scale of val-   ues remain nearly around the mean point , which   suggests that the small amount of shifting to in-   termediate hidden representations on transformer   layers is enough to contribute to the performance   gain , as we can see in the main results of Table 1 , 2 .   C.6 Detailed Efﬁciency Comparison   While we provide the efﬁciency on FLOPs in Fig-   ure 1 , we further provide the efﬁciency on GPU   memory , wall time , and FLOPs for training each   method in Table 6 . Speciﬁcally , we measure the   computational cost on the NewsQA dataset with   BERT - base , where we use the single Geforce RTX   2080 Ti GPU on the same machine . For our KALA ,   as we can ﬂexibly manage the cost of GPU mem-   ory by reducing the number of entities in entity   memory ( See Figure 8 with Appendix C.2 for more   analysis on the effects of the size of entity memory ) ,   we provide the experimental results on two settings   – KALA with memory size 0.2k and 62.8k ( full   memory ) . As shown in Table 6 , we conﬁrm that   the computational cost of our KALA with the full   memory is similar to the cost of the more params   baseline that uses one additional transformer layer   on top of BERT - base . However , by reducing the   number of entities in the memory , we can achieve   better efﬁciency than more params in terms of GPU   memory and FLOPs . Also , we observe that the   training cost ( i.e. , Wall Time and FLOPs ) of TAPT   and DAPT is high , especially on DAPT , thus we   verify that our KALA is more efﬁcient to train for   domain adaptation settings .   D Additional Visualization Results   Here we provide the frequency distribution of enti-   ties , additional case studies , and more illustrations   of textual examples and embedding spaces .   D.1 Additional Representation Visualization   While we already show the contextualized repre-   sentations of seen and unseen entities in the latent5163   space in Figure 2 right , we further visualize them   including the missing baselines of Figure 2 , such   as Fine - tuning or TAPT , in Figure 12 on the NCBI-   Disease dataset . Similar to Figure 2 , we observe   that all baselines fail to closely embed the unseen   entities in the representation space of seen enti-   ties . While this visualization result does not give   a strong evidence of why our KALA outperforms   other baselines , we clearly observe that KALA is   beneﬁcial to represent unseen entities in the feature   space of seen entities , which suggests that such an   advantage of our KALA helps the PLM to general-   ize over the test dataset , where the context contains   unseen entities .   D.2 Entity Frequency Distribution   We visualize the frequency of entities in Figure 13   and 14 . The entity frequency denotes the number   of mentions of their associated entities within the   entire text corpus of the training dataset . As shown   in Figure 13 and 14 of QA and NER datasets , the   entity frequency follows the long - tail distribution ,   where most entities appear a few times . For in-   stance , in the NewsQA dataset , more than 20k en-   tities among entire 60k entities appear only once   in the training dataset , whereas one entity ( CNN )   appears approximately 20k times . This observa-   tion suggests that most of the elements in the entity   memory are not utilized frequently . In other words ,   only few entities are accurately trained with many   training instances , whereas there exists the stale   embeddings which are rarely updated . This obser-   vation raises an interesting research question on the   efﬁcient usage of the entity memory , as we can see   in Figure 8 that the small size of entity memory   could result in the better performance ( See Ap-   pendix C.2 ) . We leave the more in - depth analysis   on the entity memory as the future work .   D.3 Additional Case Study   In addition to the case study in Figure 5 , we further   show the case on the question answering task in Fig-   ure 15 , like in Section 5.5 , With this example , we   explain how the factual knowledge in KGs could be   utilized to solve the task via our KALA . The ques-   tion in the example is “ who was kidnapped because   ofherneighbor ” . We observe that DAPT answers   this question as Araceli Valencia . This prediction   may come from matching the word ‘ her ’ in the   question to the feminine name ‘ Araceli Valencia ’   in the context . In contrast , our KALA predicts the   Jaime Andrade as an answer , which is the ground   truth . We suspect that this might be because of   the fact “ ( Jaime Andrade , spouse , Valencia ) ” in   the knowledge graph , which relates the ‘ Valencia ’   to the ‘ Jaime Andrade ’ . Although it is not clear   how it directly affects the model ’s performance , we   can reason that KALA can successfully answer the   question by utilizing the existing facts .   D.4 Additional Data Visualization   In Figure 16 and 17 , we visualize the examples of   the context with its seen and unseen entities and its   relational facts . We ﬁrst conﬁrm that the quality of   facts is moderate to use . For instance , in the ﬁrst   example of Figure 16 , the fact in the context that   Omar_bin_Laden is son of Osama_bin_Laden , is   also appeared in the knowledge graph . In addition ,   we observe that there are facts that link unseen en-   tities to the seen entities in both Figure 16 and 17 .   Thus , while some of the facts in the knowledge   graph are not accurate , we can represent the unseen   entities with their relation to the seen entities . We   expect that there is a still room to improve in terms   of the quality of KGs , allowing our KALA to mod-   ulate the entity representation more accurately . We   leave the study on this as the future work.5164516551665167