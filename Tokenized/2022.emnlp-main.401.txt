  Frank MtumbukaThomas LukasiewiczDepartment of Computer Science , University of Oxford , UKInstitute of Logic and Computation , TU Wien , AustriaCardiff University , Cardiff , UK   firstname.lastname@cs.ox.ac.uk   Abstract   Open information extraction ( OIE ) is the task   of extracting facts “ ( Subject , Relation , Object ) ”   from natural language text . We propose several   new methods for training neural OIE models in   this paper . First , we propose a novel method for   computing syntactically rich text embeddings   using the structure of dependency trees . Sec-   ond , we propose a new discriminative training   approach to OIE in which tokens in the gen-   erated fact are classified as “ real ” or “ fake ” ,   i.e. , those tokens that are in both the generated   and gold tuples , and those that are only in the   generated tuple but not in the gold tuple . We   also address the issue of repetitive tokens in   generated facts and improve the models ’ abil-   ity to generate implicit facts . Our approach   reduces repetitive tokens by a factor of 23 % .   Finally , we present paraphrased versions of the   CaRB , OIE2016 , and LSOIE datasets , and show   that the models ’ performance substantially im-   proves when trained on datasets augmented by   such data . Our best model beats the SOTA of   IMoJIE on the recent CaRB dataset , with an   improvement of 39.63 % in Fscore .   1 Introduction   OIE ( Banko et al . , 2007 ) is a branch of informa-   tion extraction ( IE ) that focuses on extracting struc-   tured information ( Niklaus et al . , 2018 ) from un-   structured natural language text . This structured   information is a set of tuples of the form “ ( Sub-   ject , Relation , Object ) ” , also called facts in OIE .   For instance , given the sentence “ Machine learn-   ing is a subfield of AI . ” , the tuple ⟨machine learn-   ing , is a subfield of , AI⟩can be extracted , where   the relation phrase “ is a subfield of ” indicates the   semantic relationship between the subject “ Ma-   chine learning ” and the object “ AI ” . OIE is use-   ful in many downstream natural language process-   ing ( NLP ) tasks like natural language understand-   ing ( Mausam , 2016 ) , multi - document question an-   swering and summarization ( Fan et al . , 2019 ) , andknowledge base construction from text ( Soderland   et al . , 2010 ) .   Several neural OIE methods in the litera-   ture approach OIE as either a sequence label-   ing ( Stanovsky et al . , 2018 ; Roy et al . , 2019 ; Jiang   et al . , 2019 ; Zhan and Zhao , 2019 ; Hohenecker   et al . , 2020 ) or a sequence generation problem ( Cui   et al . , 2018 ; Sun et al . , 2018 ; Kolluru et al . , 2020 ) .   Sequence labeling approaches label each token in   the input text as either belonging to the subject ,   relation , or object , while sequence generation ap-   proaches generate facts one word at a time given   the input text .   For the task of OIE , it is a common practice to   make use of part - of - speech ( PoS ) and dependency   tags in addition to the actual input text as a way   of incorporating syntactic information ( Stanovsky   et al . , 2018 ; Zhan and Zhao , 2019 ; Jia and Xiang ,   2019 ; Sun et al . , 2018 ; Hohenecker et al . , 2020 ) .   In such work where these tags are used , the em-   beddings for the tags are just concatenated to the   embeddings of the corresponding text tokens . This   formulation does not fully use the rich syntactic   information , especially the one that is expressed   in the structure of dependency trees . Dependency   trees ’ head - dependent relations provide a good ap-   proximation of the semantic relationships between   predicates and their arguments ( Jurafsky and Mar-   tin , 2009 ) . As a result , they are directly applicable   to a wide range of applications , including IE . In   this work , we compute token representations based   on the structure of dependency trees in order to   benefit from their rich syntactic and semantic in-   formation . Furthermore , sequence generation ap-   proaches are susceptible to generating facts that   often express redundant information and are also   prone to generating repetitive text in facts . Sun   et al . ( 2018 ) and Kolluru et al . ( 2020 ) looked into   the issue of generating the same facts more than one   time , which is redundant . No work has been done   to control the generation of repetitive text in facts.5972Additionally , sequence generation approaches are   capable of introducing words that are not in the   input sentence into a generated fact . This capability   enables such approaches to generate facts that are   implicitly stated in the input sentence . However , the   approaches employed by Cui et al . ( 2018 ) and Sun   et al . ( 2018 ) are restrictive in how the models “ pick ”   words when generating facts , which restrains the   models ’ flexibility in generating implicit facts .   In this paper , our approach uses a sequence gen-   eration approach to generate facts from natural lan-   guage text one word at a time . Unlike the previous   approaches , we compute syntactically rich vector   representations of input text tokens guided by the   structure of dependency trees . For each input sen-   tence , we construct a visibility matrix of its tokens   based on the structure of its dependency tree by con-   sidering tokens that are directly related in the depen-   dency tree as visible to each other . A graph neural   network ( GNN ) ( Zhou et al . , 2018 ) encoder takes   as input token embeddings and the corresponding   visibility matrix to compute new token embeddings   guided by the visibility matrix . Furthermore , we   also introduce a new method for training neural   OIE models . We add an extra module , the discrim-   inator , that takes the generated tuple as input and   classifies its tokens as either “ real ” or “ fake ” , where   “ real ” tokens are those that are in both the gener-   ated and gold tuples , while “ fake ” tokens are those   that are only in the generated tuple but not in the   gold tuple . To ensure that such a model does not   generate repetitive text , we use a coverage vector   to monitor the degree of coverage that words in the   input text have received so far . When generating   the next word at timestep t , the coverage vector at   that instant is a sum of all attention distributions   computed over the input text tokens from timesteps   0 tot−1 . This coverage vector is used as part of the   input when computing the next attention distribu-   tion . Intuitively , this informs the current attention   mechanism ’s decision of the previous decisions and   makes it easier to avoid repeatedly attending to   the same words in the input text , hence avoiding   generating repetitive text in a fact . As a means to   explicitly guide the model ’s choice of either gener-   ating a word from a vocabulary or to “ pick ” a word   from the input text , we explicitly compute the prob-   ability of picking a word from a vocabulary or input   text using the model ’s context vectors . Lastly , we   also investigate the effect of data - augmentation on   the performance of our models . Figure 1 illustratesour approach . Our main contributions are briefly   summarized as follows :   •We present several new methods for training   neural OIE models . First , we propose a new   method of computing syntactically rich vector   representations of input tokens guided by the   structure of dependency trees . This is done   by using GNN s , as they are able to take into   account the graph structure of a dependency   tree .   •Second , we propose an additional module , the   discriminator , on top of the model that gener-   ates facts . The additional module performs   a binary classification of tokens in generated   facts as either “ real ” or “ fake ” . This new ap-   proach significantly improves the performance   of sequence - to - sequence neural OIE models .   •Furthermore , we reduce repetitive words in gen-   erated facts by 23 % , from 36 % to 13 % , when   we jointly use the coverage vector and explicitly   guide the model where to pick the next word   when generating a fact .   •Finally , we present paraphrased versions   of the CaRB ( Bhardwaj et al . , 2019 ) ,   OIE2016 ( Stanovsky and Dagan , 2016 ) , and   LSOIE ( Solawetz and Larson , 2021 ) datasets ,   which we use to augment existing datasets .   •Our best performing model uses both the dis-   criminator and the GNN encoder , and beats the   state - of - the - art ( SOTA ) of IMoJIE on CaRB ,   with an improvement of 39.63 % in Fscore .   This model also presents competitive results on   OIE2016 and LSOIE .   2 Task Formulation   In this work , we approach OIE as a sequence gen-   eration task . As illustrated in Fig . 1 , given an input   text , we use a generator to generate a tuple one   word at a time . Then , we pass the generated tuple   through the discriminator , which classifies tokens   in the tuple as either “ real ” or “ fake ” , where “ real ”   tokens are those that are both in the generated and   gold tuples , while “ fake ” tokens are in the generated   tuple but not in the gold tuple . Thus , the discrim-   inator performs binary classification of the tokens   in the generated tuple . We only consider binary   extractions from sentences .   Given a sentence - tuple pair ⟨X , Y⟩ , where X=   ⟨w , . . . , w⟩andY=⟨y , . . . , y⟩are sequences   of tokens in the input sentence and expected tuple ,   respectively , we define the generator ’s conditional5973probability P(Y|X ) = P{Y|⟨w , . . . , w⟩}as   /producttextP{y|⟨w , . . . , w⟩;⟨y , . . . , y⟩},(1 )   where P(y|⟨w , . . . , w⟩;⟨y , . . . , y⟩)is the   probability of generating the i - th word , given the in-   put sequence and the entire generated sequence by   stepi , andP(Y|X)is the probability of generating   an entire tuple Y , given the input sentence X.   Given a generated tuple , Y=⟨y , . . . , y⟩ , and   the input context vector , d , the discriminator seeks   to label each token in Yas either “ real ” or “ fake ” .   The input context vector is weighted average of the   vector representations of the input sequence . To   that end , we define the discriminator ’s likelihood   P(L|Y)as   where L={0,1}with 1 and 0 standing for “ real ”   and “ fake ” tokens , respectively , σis the sigmoid   function , yis the vector representation of the i-   th token yin the generated tuple , dis the input   context vector , vis a set of learnable parameters ,   and “ [ , ] ” is the concatenation operation .   3 Our Approach   We now define and describe the modules that we   designed to solve OIE as defined in Section 2 and   illustrated in Fig . 1 .   3.1 Embedding   The embedding block maps text , pre - processed into   a sequence of tokens , to a corresponding sequence   of embedding vectors . In this paper , we consider   stacked layers of bidirectional LSTMs ( BiLSTM s ) ,   a transformer encoder ( Vaswani et al . , 2017 ) , pre-   trained BERT ( Devlin et al . , 2018 ) , a feedback   transformer encoder ( Fan et al . , 2020 ) , and a pre-   trained ELECTRA model(Clark et al . , 2020 ) for   the embedding block . Unlike transformers in which   the representation at a given layer can only access   representations from lower layers rather than the   higher level representations already available , feed-   back transformers expose all previous representa-   tions to all future representations ( Fan et al . , 2020 ) .   That is , the lowest representation at the current   timestep tis formed from the highest - level rep-   resentations of the past . Unlike other pre - trained   language models that are trained through maskedlanguage modelling ( MLM ) , such as BERT , ELEC-   TRA is pre - trained by detecting replaced tokens in   the input sequence .   For a given sequence of input tokens , the embed-   ding block computes a corresponding sequence of   embedding vectors of the same length as the input   sequence . In addition to input text , we also incor-   porate PoSand dependency tree tags obtained via   the Spacy library . We create embedding vectors   for each PoS and dependency tree tag , and con-   catenate them to the vector representation of the   corresponding input token . For words that are split   into subwords , each subword token is attributed the   same PoS and dependency tree tag as the parent   word that it belongs to .   3.2 Encoding   The encoding block computes vector representa-   tions of the input sequence guided by the structure   of the dependency tree . During the text prepossess-   ing step , we construct a visibility matrix of tokens   in the input sequence based on the proximity of   tokens in the corresponding dependency tree . The   encoder takes a sequence of vector representations   from the embedding block and the visibility matrix   as input , and gives a sequence of vector represen-   tations of the same length as the input sequence   as output . The encoding block is optional . When   the encoding block is not used , the encoder context   vector is calculated using the embedder outputs .   To take into account the graph structure of the   dependency tree when computing vector represen-   tations , we use a graph attention network ( GAT )   ( Veli ˇckovi ´ c et al . , 2018 ) as encoder . GAT s com-   pute the vector representation of the current node   by only attending to nodes in its neighborhood . In   our case , we treat each token in the input sequence   as a node and those that it is connected to in the   dependency tree as its neighbors . Thus , the vec-   tor representation of the current node is computed   by only attending to the vector representations of   the nodes in the neighborhood as presented in the   visibility matrix . For comparison , we also use a   transformer encoder that does not take into account   the visibility matrix when computing vector repre-   sentations .   3.3 Decoding   During decoding , we adopt pointer - generator-   networks ( PGN s ) ( See et al . , 2017 ) and make mod-5974   ifications . For example , we calculate the distribu-   tion over the entire vocabulary from a weighted   sum of the encoder and decoder context vectors ,   as opposed to the concatenation of the two . When   generating the next token in a fact at timestep t ,   the embedding vectors of the entire generated se-   quence so far , ⟨y , . . . , y⟩ , are fed into either the   BiLSTM network , a transformer , or feedback trans-   former to generate contextualized decoder hidden   representations .   The decoder hidden representations ⟨d , . . . ,   d⟩are then used to calculate the decoder context   vector , d , which is a weighted sum of decoder   hidden representations ( Bahdanau et al . , 2015 ):   e = vTanh ( Wd+b ) , where v , W , and   bare learnable parameters , and dare decoder   hidden representations . Then , eis used to calculate   attention scores :   a = Softmax ( e ) . ( 3 )   The attention scores at timestep t , a , are used to   calculate the decoder context vector das weighted   sum of decoder hidden representations :   d=/summationtextad , ( 4 )   where dare decoder hidden representations , and   aare attention scores over decoder hidden repre-   sentations at timestep t.   Coverage mechanism . We use a coverage vector ,   c , to keep track of how much attention each word   in the input text has received while generating a   tuple one word at a time . The coverage vector is   the sum of all attention distributions over encoder   hidden representations up to timestep t−1 : c=/summationtextα   , where α   is the attention distribution   over encoder hidden representations . The coverage   vector is initialized as a zero vector , as on the first   timestep , no input token has been attended to .   Decoder - encoder attention . The decoder context   vector dfrom Eq . 4 , the encoder hidden represen-   tations from Section 3.2 , and the coverage vector care used to compute the encoder context vector h ,   using attention ( Bahdanau et al . , 2015 ):   e = vTanh ( Wh+Wd+Wc+b),(5 )   where v , W , W , W , and bare learnable pa-   rameters , hare encoder hidden representations ,   andcis the coverage vector . The coverage vector   is included in this computation in Eq . 5 , so that the   attention mechanism is informed of previous atten-   tion decisions , hence avoiding repeatedly attending   to the same words in the input text . The atten-   tion distribution over input tokens , α , is calculated   from eusing Eq . 3 . This αand encoder hidden   states , ⟨h , . . . , h⟩ , are then used to compute the   encoder context vector husing Eq . 4 .   The attention distribution αobtained here can   be interpreted as a probability distribution over the   source words , which tells the model where to look   to produce the next word .   To compute the distribution over the entire vocab-   ulary P , we first calculate a weighted average   of the encoder context vector hand decoder con-   text vector d , and then feed the resulting vector   into a linear layer :   P = Softmax ( V[h , d ] + b),(6 )   where Vandb are learnable parameters , and   P is the probability distribution over all words   in the vocabulary . P gives the final probability   of predicting the next word wfrom the vocabulary :   P(w ) = P(w ) .   Generation probability . The generation proba-   bility at timestep t , P , is calculated from the   encoder and decoder context vectors , handd ,   respectively : P = δ(wh+wd+b ) ,   where w , w , andb are learnable parameters ,   δis a sigmoid function , and P∈[0,1 ] .   Pcontrols whether the next word should be   generated from the vocabulary by sampling from   P ( Eq . 6 ) or copied from the input tokens by5975sampling from the attention distribution over in-   put tokens α . This explicitly guides the model on   where to look for the next word . For each input   batch , there is an extended vocabulary that is the   union of the vocabulary and all tokens in the input   batch . As in ( See et al . , 2017 ) , this yields the fol-   lowing probability distribution over the extended   vocabulary :   P(w ) = PP + ( 1−P)/summationtextα.(7 )   This allows the model to generate out - of-   vocabulary ( OOV ) words , because if wis an OOV   word , then P is zero . Similarly , if the word is   not in the input batch , then / summationtextα= 0 .   Fig . 2 summarises the workflow in the generator .   3.4 Discriminator   Once the tuple has been generated , it is passed   through the discriminator along with the input con-   text vector , which classifies the tokens in the tuple   as either “ real ” or “ fake ” , where “ real ” tokens are   those that are in both the generated tuple and the   gold tuple , while “ fake ” tokens are those that are in   the generated tuple but not in the gold tuple . The   input context vector is a weighted average of the   vector representations of the tokens in the input se-   quence . Thus , the discriminator performs binary   classification of the tokens in the generated tuple   while being informed of the input sequence via the   input context vector . We consider a discriminator   that is composed of a transformer encoder and a   sigmoid layer . The transformer encoder computes   vector representations of the generated tuple . The   vector presentations are then passed through the   sigmoid layer for classification as “ real ” or “ fake ”   as in Eq . 2 .   Although this approach is similar in its design   to generative adversarial networks , the generator in   this approach is trained with maximum likelihood   rather than adversarially to fool the discriminator .   3.5 Training loss   During training , the model loss is the sum of the   generator loss and discriminator loss , defined as   follows .   Generator loss . We use both the probability of   generating the next word P(w)(Eq . 7 ) and the   coverage mechanisms to calculate the generator ’s   loss . The loss at timestep t , loss , is the sum of the   negative log likelihood of the target word and thecoverage loss :   loss=−logP(w ) + λ / summationtextmin(α , c ) .   The model is penalised by the coverage loss ,   λ / summationtextmin(α , c ) , if it repeatedly attends to the   same locations . We believe that a specific token   from the source sentence can only appear once in a   generated fact . As a result , if the final coverage vec-   tor is greater or less than one , then it is penalised .   Discriminator loss . For the discriminator , compute   the negative log - likelihood of Eq . 2 :   loss=−/summationtext[(y = y ) log ( σ(w[y , d ] ) )   + ( y̸=y ) log(1 −σ(w[y , d ] ) ) ] .   4 Experiments and Results   In this section , we discuss the various experiments   that we conducted . We present descriptions of the   OIE datasets that we used and the data preparation   routines involved . We also discuss the evaluation   framework that was used and finally present the re-   sults of each experiment in terms of the Fand area   under the precision - recall curve ( AUC - PR ) values .   Experiments , training , and evaluation . We set up   different combinations of the neural network ( NN )   modules in blocks discussed in Section 3 , and each   unique combination resulted in a different model .   Thus , we had 15 unique embedder - decoder combi-   nations , and we present results for each combina-   tion .   We run experiments for all 15 modelsin 6   experimental setups : ( a ) default setup with em-   bedders and decoders only ; ( b ) + discriminator ,   where we add a discriminator to the default   setup ; ( c ) + transformer encoder , where we add   a transformer - based encoder to the default setup ;   ( d)+GNN encoder , where we add a GNN -based   encoder to the default setup ; ( e ) + transformer en-   coder + discriminator , where we add both a trans-   former - based encoder and discriminator to the de-   fault setup ; and ( f ) + GNN encoder + discriminator ,   where we add both a GNN -based encoder and dis-   criminator to the default setup .   For the models with BERT and ELECTRA em-   bedders , we used pre - trained5976and versions , re-   spectively . Due to resource constraints , the pa-   rameters of the pretrained models were frozen . All   configurations for different module combinations   are in Appendix E.   All models were trained by minimizing the loss   defined in Section 3.5 . During training , we define a   teacher - forcing ratio and randomly choose whether   to use teacher forcing or not depending on the value   of the randomly generated number compared to   the teacher - forcing ratio . The confidence of the   extracted fact was calculated by multiplying the   probabilities of all tokens in the fact . The check-   points that were saved after each epoch were then   evaluated on the test partition of each dataset using   the CaRB ( Bhardwaj et al . , 2019 ) evaluation frame-   work . The saved checkpoints did not include the   discriminator , as the discriminator was only being   used during training .   Datasets . Our models were trained and evalu-   ated on three benchmark OIE datasets : OIE2016   ( Stanovsky and Dagan , 2016 ) , CaRB ( Stanovsky   and Dagan , 2016 ) , and LSOIE ( Solawetz and Lar-   son , 2021 ) . In Appendix A , we provide detailed   descriptions of the datasets and their statistics .   Results . In Table 1 , we evaluate our models on   CaRB under the experimental setups described in   the experiments subsection . We note that in each ex-   perimental setup , the model resulting from the com-   bination of an ELECTRA embedder and a feedback   transformer decoder outperforms other combina-   tions . The best model on CaRB has an ELECTRA   embedder , GNN encoder , feedback transformer ,   and a discriminator . This model achieves an F   value of 0.747 and an AUC - PR value of 0.740 .   Based on the results in Appendix C , we note   that the best model on CaRB also achieves the best   results on OIE2016 . The model achieves an F   value of 0.619 and an AUC - PR value of 0.639 . Fur-   thermore , our best model achieves an Fvalue of   0.517 and an AUC - PR value of 0.525 on LSOIE .   The detailed results and analyses of similar experi-   ments on OIE2016 and LSOIE are in Appendices C   and D , respectively . Note that we can not directly   compare our results to the previous works by Zhan   and Zhao ( 2019 ) and Solawetz and Larson ( 2021 )   on OIE2016 and LSOIE , respectively , as they use   different evaluations . Solawetz and Larson ( 2021 )   report the best F1 and AUC - PR values on LSOIEof only 0.380and0.220 , respectively .   Results on augmented datasets . We trained the   best model from Table 1 on paraphrased and mixed   versions of CaRB . When training the model on   the mixed dataset , each batch was made of equal   portions of the paraphrased and original versions .   After training , the model was evaluated on the orig-   inal test set of CaRB dataset . We note that the   model performs poorly when trained only on the   paraphrased dataset . However , when trained on the   mixed dataset , the model performs better than when   trained on just the original dataset . Table 2 presents   the results .   Comparison to other systems . In Table 3 , we com-   pare our best performing model to previous OIE   systems on CaRB . The results from other systems   are quoted directly from previous works , which   share the same experimental settings and can di-   rectly be compared with . We compare our best per-   forming model to results reported by Kolluru et al .   ( 2020 ) ( IMoJIE ) under the same settings . From this   comparison , our best performing model beats the   SOTA model IMoJIE with 39.63 % in Fvalue on   CaRB .   5 Results Analysis and Ablation Studies   In this section , we discuss the results presented in   Section 4 . We also present a detailed analysis of   the contributions of each experimental setup .   Performance across experimental setups . We   considered 6 experimental setups , which we have   defined in Section 4 . Table 5 presents the average   performance of all models in each setup on CaRB   based on the results presented in Table 1 . We note   that each experimental setup introduced significant   improvement in performance compared to the con-   trol experimental setup , default . The setup with   theGNN encoder only , + GNN encoder , achieves   the best average performance in F. It improves   on the default setup with 11.69 % . The setup with   both the GNN encoder and discriminator , + GNN   encoder + discriminator , achieves the best average   performance in AUC - PR . It improves on the de-   fault setup with 6.53 % . Furthermore , we note that   models in the + GNN encoder setup perform better   than models in the + transformer encoder setup   with 1.56 % and 2.19 % improvements in average   FandAUC - PR values , respectively . To that end,5977   we see that the GNN encoder that computes embed-   dings of the input sequence based on the structure   of the dependency tree performs better than the   transformer encoder , which does not take into ac-   count the structure of the dependency tree when   computing embeddings . Additionally , the best per-   formance is achieved when the GNN encoder is   used jointly with the discriminator .   Taking syntactic information into account when   computing embeddings not only improves model   performance but also improves extraction in some   cases . For example , given a sentence “ The twinswho came are identical but are uniquely man-   nered . ” , the best model with the transformer en-   coder , which does not consider the topology of   the dependency tree , yields ⟨who , are , uniquely   mannered ⟩ , whereas the extraction with the GNN   encoder , which does consider the topology of the   dependency tree , yields ⟨The twins , are , uniquely   mannered ⟩. This example shows that syntax can be   useful in OIE . Thus , we conclude that taking into   account the dependency tree ’s structure when com-   puting embeddings and the discriminative training   approach are the main cause of the high boost in   the performance of our models . This is consistently   shown on all three benchmark datasets .   Performance of modules . We also looked into the   average performance of the modules in the embed-   der and decoder blocks . Table 6 summarises the   analysis based on results in Table 1 . Considering   all models , we note that models with a pre - trained   ELECTRA model achieve the best average values   of 0.653 and 0.625 in FandAUC - PR , respectively .   Furthermore , models with the feedback transformer   decoder achieve the best average values of 0.676   and 0.636 in FandAUC - PR , respectively . The   feedback transformer performs well , and this is   largely owing to its ability to capture the input ’s   sequential property , which is critical for tuple gen-   eration . However , the other components that we   introduce in this paper boost performance signifi-   cantly as well . For instance , in F , the + GNN en-   coder setup achieves the best average performance   by 11.69 % over the default setup . In AUC - PR , the   + GNN encoder + discriminator setup achieves the   best average performance by 6.53 % over the default5978   setup .   Token repetition . In this work , the core tools that   control the repetition of tokens during decoding   are the generation probability andcoverage mech-   anisms that we have defined in Section 3.3 . As   such , to check whether these tools are effective ,   we run our best model with and without the tools   and the compute the average percentage of repet-   itive tokens and the model ’s ability to introduce   new words from the vocabulary . When the best   model is run on 50 samples from the test partition   with and without both generation probability and   coverage mechanisms . We compute the number   of tokens that are both in the source sentence S   and the generated tuple T , S∩T , in both settings   to measure the model ’s ability to introduce words   from the vocabulary . Similarly , we also compute a   percentage of repetitive tokens in T. Table 4 sum-   marizes the results . From Table 4 , we note that   the percentage of tokens in |S∩T|drops by 24 %   when using both generation probability andcov-   erage mechanisms . This implies that the model   copies fewer tokens from the source sentence , and   introduces more tokens from the vocabulary , hence   the improved ability to generate implicit facts . Fur-   thermore , the repetitive tokens reduce by 23 % . To   that end , the results confirm that both generation   probability andcoverage mechanisms are effective   in controlling repetitive tokens , and improving the   models ’ ability to generate implicit facts . This is   the first work to look into redundancy at fact level .   Sun et al . ( 2018 ) and Kolluru et al . ( 2020 ) look into   redundant information as an issue of generating the   same facts multiple times .   6 Related Work   In the recent past , neural OIE models have been de-   veloped , and they tackle OIE as either sequence   labeling or sequence generation . The former   ( Stanovsky et al . , 2018 ; Roy et al . , 2019 ; Jiang   et al . , 2019 ; Zhan and Zhao , 2019 ; Hohenecker   et al . , 2020 ) involves tagging each word in the input   text with an appropriate tag that indicates whether   the word belongs to either the subject , relation , or   object . Methods that approach OIE as sequence   generation generate facts one word at a time . These   include CopyAttention ( Cui et al . , 2018 ) , Logician   ( Sun et al . , 2018 ) , and IMoJIE ( Kolluru et al . , 2020 ) .   CopyAttention is an encoder - decoder model en-   hanced with copying and attention mechanisms . Lo-   gician is another encoder - decoder model that uses   coverage attention and gated - dependancy attention   to extract facts from Chinese text . IMoJIE outputs   a variable number of different facts per sentence .   The next fact from a sentence is best determined   in context of all other facts extracted from it so far .   Hence , IMoJIE uses a decoding strategy that gener-   ates facts in a sequential fashion , one after another ,   each one being aware of all the ones generated prior   to it .   Our work substantially differs from previous OIE   works . Firstly , we are the first to compute embed-   dings of input sequences based on the structure of   the corresponding dependency trees . This is done   by first forming a visibility matrix based on which   words are directly related in the dependency tree .   Then , a GNN encoder computes syntactically rich   embeddings of the input sequences controlled by   the formed visibility matrices . This formulation   is different from previous approaches where the   embeddings of PoS and dependency tags are just5979concatenated to the embeddings of the according   text tokens at embedding level . Secondly , this is   the first work that uses an additional module that   classifies tokens in the generated tuple as either   “ real ” or “ fake ” during training . Thirdly , unlike   previous neural approaches that tackle OIE as se-   quence generation , we consider the entire generated   sequence by timestep t,⟨y , . . . , y⟩ , to compute   a decoder context vector , as discussed in Eqs . 3–4 in   Section 3.3 . Additionally , Sun et al . ( 2018 ) and Kol-   luru et al . ( 2020 ) consider redundant information   as the issue of generating the same facts multiple   times , while we consider redundancy at token level   in generated facts . Furthermore , Cui et al . ( 2018 )   and Sun et al . ( 2018 ) are restrictive in how the   models “ pick ” words when generating facts , which   restrains the models ’ flexibility in generating facts .   Cui et al . ( 2018 ) only limit the sample space only   to the source sentence , and Sun et al . ( 2018 ) only   consider the source sentence and keywords , while   we consider the source sentence and the entire vo-   cabulary .   7 Summary and Outlook   This work has shown that computing syntactically   rich embeddings of text based on the structure of   dependency trees significantly improves the per-   formance of neural OIE models and the quality of   extractions . Furthermore , the novel discriminative   training method for OIE models boosts models ’ per-   formance . Additionally , we show that generation   probability andcoverage mechanisms substantially   improve the models ’ ability to introduce new words   into generated facts and reduce repetitive tokens in   generated facts . Finally , we have presented para-   phrased versions of OIE2016 , CaRB , and LSOIE ,   and shown that the models ’ performance substan-   tially improves when trained on datasets augmented   by such data . Our approach , syntactically rich dis-   criminative training , beats the SOTA of IMoJIE on   the latest CaRB benchmark dataset : the model im-   proves the Fscore of IMoJIE by 39.63 % . This   model also presents competitive results on other   benchmark datasets : OIE2016 and LSOIE .   In future work , we will build on this work and in-   vestigate further whether the proposed approaches   could also be beneficial to sequence labelling ap-   proaches for OIE . Furthermore , models trained   on original OIE datasets perform poorly on para-   phrased versions of the datasets . This is clear from   the results shown in Table 2 . Under normal condi - tions , one would expect to identify the same sets of   facts from all paraphrased versions of the same sen-   tence . This is not true of the OIE models , and the   phenomenon shows that models do not understand   sentence meaning . Future studies could build on   this to look into how OIE models can better capture   sentence meanings .   8 Limitations   Like any other research , the work described in this   paper has some space for improvement . First , we   limited ourselves to binary relations that are de-   scribed in a single sentence . However , in the real   world , we are constantly confronted with content   that spans many sentences and relationships that   span multiple sentences . In future research , the con-   cepts and approaches outlined in this paper could   be expanded to include n - ary relations that span   many sentences . Second , models trained on origi-   nal datasets perform poorly on paraphrased datasets .   This is clear from the results shown in Table 2 . Un-   der normal conditions , one would expect to identify   the same sets of facts from all paraphrased versions   of the same sentence . This is not true of the OIE   models , and the phenomenon shows that models do   not understand sentence meaning . Future studies   could build on this to look into how OIE models can   better capture sentence meanings . Finally , we used   the formulation for constructing syntactically rich   vector representations as well as the new discrimi-   native training approach to only train sequence gen-   eration OIE models . It could be examined further   in future research whether the proposed methodolo-   gies could also be advantageous to OIE sequence   labelling approaches .   Acknowledgements   This work was partially supported by the Alan Tur-   ing Institute under the EPSRC grant EP / N510129/1 ,   by the AXA Research Fund , by the EPSRC grant   EP / R013667/1 , and by the ESRC grant “ Unlock-   ing the Potential of AI for English Law ” . We   also acknowledge the use of Oxford ’s ARC facil-   ity , of the EPSRC - funded Tier 2 facilities JADE   ( EP / P020275/1 ) and JADE II ( EP / T022205/1 ) , and   of GPU computing support by Scan Computers In-   ternational Ltd. Frank Mtumbuka was supported by   the Rhodes Trust under a Rhodes Scholarship.5980References5981A Datasets ’ Descriptions and Statistics .   OIE2016 has a total of 5,078 training samples ;   it is small , which makes it hard to train models   that generalize to unseen problem instances . For   training purposes , OIE2016 was augmented with   samples from another dataset created by Cui et al .   ( 2018 ) . This resulted in a huge dataset of more than   36 M training samples . This augmented dataset was   then trimmed to 1.7 M training samples using pre-   processing steps presented in ( Hohenecker et al . ,   2020 ) . LSOIE is constructed from QA - SRL BANK   2.0 ( FitzGerald et al . , 2018 ) and contains over 70 K   sentences and over 150 K extraction tuples . CaRB   is an improved dataset compared to OIE2016 and   is crowdsourced . It was annotated by NLP experts ,   and it is more accurate than OIE2016 ( Bhardwaj   et al . , 2019 ) . In addition to binary and explicit ex-   tractions , CaRB also comes with implicit as well   as n - ary extractions . The n - ary extractions were   truncated to bear some resemblance to the binary   extractions that we considered in this work . For   example , in the sentence “ A year later , he was ap-   pointed Attorney - General for Ireland and on this oc-   casion was sworn of the Privy Council of Ireland ” ,   we have the subject “ he ” , relation “ was appointed ” ,   and objects “ Attorney - General ” , “ for Ireland ” , and   “ A year later ” . From this , we formulate a truncated   tuple⟨he , was appointed , Attorney - General for Ire-   land a year later ⟩.   In each extraction , we introduced special tokens   into the vocabulary to mark the beginning and end   of either subject , relation , or object . “ ⟨arg0⟩ ” and   “ ⟨/arg0⟩ ” were used to indicate the start and end   of a subject span , respectively , while “ ⟨rel⟩ ” and   “ ⟨/rel⟩ ” were used to indicate the start and end   of a relation span , respectively , and “ ⟨arg1⟩ ” and   “ ⟨/arg1⟩ ” were used to indicate the start and end of   an object span , respectively .   In addition to the original versions of the CaRB ,   OIE2016 , and LSOIE datasets , we also generate   paraphrased versions , which we use to train our   models . We only paraphrase the training sets of the   datasets and keep the test sets . We use Parrotto   paraphrase samples . Parrot generates paraphrased   versions that are adequate and fluent , while being   as different as possible from the original versions   on the surface lexical form . We paraphrased the   datasets , so that we get more and diverse data for   training our models . Furthermore , as paraphrasedsamples convey the same meaning as the original   samples but with a different lexical form , models   trained on this data have a better ability to capture   implicit relations .   B Significance of Various Experimental   Setups on CaRB   We further investigated whether the improvements   brought about by each experimental configuration   in Table 1 are substantial rather than a random oc-   currence . For this , we perform a paired t - test on   each new experimental configuration and the de-   fault setup , taking into account all paired Fand   AUC - PR values .   First , the paired t - test produced pvalues smaller   than.05,p < . 05 , in both FandAUC - PR when   the+ transformer encoder setup was compared to   thedefault setup . This means that the default setup   has a less than 5 % chance of outperforming the   + transformer encoder setup . Second , when the   + discriminator and+ transformer encoder + dis-   criminator setups are individually matched with   thedefault setup , both FandAUC - PR have pval-   ues of less than .03,p < . 03 . This means that   thedefault setup has a less than 3 % chance of   outperforming either the + discriminator setup or   the+ transformer encoder + discriminator setup .   Finally , when the + GNN encoder and+GNN   encoder + discriminator setups are individually   paired with the default setup , both FandAUC - PR   produce pvalues of less than .01,p < . 01 . This   means that the default setup has less than a 1 %   chance of outperforming either the + GNN encoder   or+ GNN encoder + discriminator setup .   Based on the above deliberations , each exper-   imental setup significantly improved the default   setup in terms of FandAUC - PR compared to the   default setup . Additionally , the + GNN encoder   setup outperforms the + transformer encoder setup .   The+GNN encoder setup generates syntactically   richer embeddings than the + transformer encoder   setup . Furthermore , the + GNN encoder + dis-   criminator setup produces the best results , which   can be attributed to the formulation for computing   embeddings based on the dependency tree ’s struc-   ture as well as the discriminative training technique .   Finally , these patterns may be seen in the results   shown in Tables 7 and 12 , which were derived   from the OIE2016 and LSOIE datasets , respectively .   These advancements were not coincidental.5982C OIE2016 Results and Analysis   In this section , we present results of our models   on the OIE2016 dataset . All models were trained   and evaluated in all six experimental setups dis-   cussed in Section 4 . We used the CaRB evaluation   framework .   Results . Table 7 presents the results of evaluating   our models on the OIE2016 dataset under the ex-   perimental setups described in the experiments sub-   section . Note that in each experimental setup , the   model resulting from the combination of an ELEC-   TRA embedder and a feedback transformer decoder   outperforms other combinations . The best model   on the OIE2016 dataset has an ELECTRA embed-   der , GNN encoder , feedback transformer , and a   discriminator . The model achieves an Fvalue of   0.619 and an AUC - PR value of 0.639 . Note that   we can not directly compare our results to the pre-   vious work by Zhan and Zhao ( 2019 ) , because the   evaluation framework used is different .   Performance across experimental setups . Ta-   ble 8 presents the average performance of all mod-   els in each experimental setup on OIE2016 based   on the results presented in Table 7 . We note that   each experimental setup introduced considerable   improvement in performance compared to the con-   trol experimental setup , default . The setup with   theGNN encoder only , + GNN encoder , achieves   the best average performance in F. It improves   on the default setup with 18.90 % . The setup with   both the GNN encoder and discriminator , + GNN   encoder + discriminator , achieves the best aver-   age performance in AUC - PR . It improves on the   default setup with 13.74 % . Furthermore , we note   that models in the + GNN encoder setup perform   better than models in the + transformer encoder   setup with 2.32 % and 4.08 % improvements in av-   erage FandAUC - PR values , respectively . To that   end , we see that the GNN encoder that computes   embeddings of the input sequence based on the   structure of the dependency tree performs better   than the transformer encoder that does not take into   account the structure of the dependency tree when   computing embeddings . Additionally , the best per-   formance is achieved when the GNN encoder is   used jointly with the discriminator . Thus , we con-   clude that taking into account the dependency tree ’s   structure when computing embeddings and the dis-   criminative training approach are the main cause of   the high boost in the performance of our models . Performance of modules . We also looked into the   average performance of the modules in the embed-   der and decoder blocks . Table 9 summarises the   analysis based on results in Table 7 . Considering all   models , the models with a pre - trained ELECTRA   model achieve the best average values of 0.545   and 0.550 in FandAUC - PR , respectively . Fur-   thermore , models with the feedback transformer   decoder achieve the best average values of 0.563   and 0.564 in Fand AUC - PR , respectively .   Token repetition . Our best model from Table 7   is run on 50 samples from the test partition of the   OIE2016 dataset with and without both generation   probability andcoverage mechanisms . We compute   the number of tokens that are both in the source sen-   tence Sand the generated tuple T , S∩T , in both   settings to measure the model ’s ability to introduce   words from the vocabulary . Similarly , we also com-   pute a percentage of repetitive tokens in T. Table 10   summarises the results . From Table 10 , we note   that that the percentage of tokens in |S∩T|drops   by 18 % when using both generation probability and   coverage mechanisms . This implies that the model   copies fewer tokens from the source sentence , and   introduces more tokens from the vocabulary , hence   the improved ability to generate implicit facts . Fur-   thermore , the repetitive tokens reduce by 23.70 % .   To that end , the results confirm that both generation   probability andcoverage mechanisms are effective   in controlling repetitive tokens , and improving the   models ’ ability to generate implicit facts .   Performance on augmented dataset . We trained   the best model from Table 7 on paraphrased and   mixed versions of the OIE2016 dataset . After train-   ing , the model was evaluated on the original test   set of the OIE2016 dataset . We note that the perfor-   mance of the model improves when trained on the   mixed dataset . Table 11 summarises the results .   D LSOIE Results and Analysis   In this section , we present results of our models on   the LSOIE dataset . We considered all experimen-   tal setups discussed in Section 4 , and we used the   CaRB evaluation framework .   Results . In Table 7 , we present the results of eval-   uating our models on the LSOIE dataset under all   our experimental setups . We note that in each ex-   perimental setup , the model resulting from the com-   bination of an ELECTRA embedder and a feedback   transformer decoder outperforms other combina-5983   tions . The best model on LSOIE has an ELECTRA   embedder , GNN encoder , feedback transformer ,   and a discriminator . The model achieves an F   value of 0.517 and an AUC - PR value of 0.525 .   Performance across experimental setups . Ta-   ble 13 presents the average performance of all mod-   els in each experimental setup on OIE2016 based   on the results in Table 12 . We note that each ex-   perimental setup introduced considerable improve-   ment in performance compared to the control ex-   perimental setup , default . The setup with the GNN   encoder only , + GNN encoder , achieves the best av-   erage performance in F. It improves on the default   setup with 7.73 % . The setup with both the GNN   encoder and discriminator , + GNN encoder + dis-   criminator , achieves the best average performance5984   inAUC - PR . It improves on the default setup with   3.81 % . Furthermore , we note that models in the   + GNN encoder setup perform better than models   in the + transformer encoder setup with 1.19 % and   4.48 % improvements in average FandAUC - PR   values , respectively . To that end , we see that the   GNN encoder that computes embeddings of the   input sequence based on the structure of the de-   pendency tree performs better than the transformer   encoder that does not take into account the struc-   ture of the dependency tree when computing em-   beddings . Additionally , the best performance is   achieved when the GNN encoder is used jointly   with the discriminator . Thus , we conclude that tak-   ing into account the dependency tree ’s structure   when computing embeddings and the discrimina-   tive training approach are the main cause of the   high boost in the performance of our models .   Performance of modules . We also looked into   the average performance of the modules in the em-   bedder and decoder blocks . Table 14 summarises   the analysis based on results in Table 12 . Con-   sidering all models , the models with a pre - trained   ELECTRA model achieve the best average values   of 0.478 and 0.477 in FandAUC - PR , respectively .   Furthermore , models with the feedback transformer   decoder achieve the best average values of 0.482   and 0.487 in Fand AUC - PR , respectively .   Token repetition . Our best model from Table 12   is run on 50 samples from the test partition of the   LSOIE dataset with and without both generation   probability andcoverage mechanisms . We compute   the number of tokens that are both in the source sen-   tence Sand the generated tuple T , S∩T , in both   settings to measure the model ’s ability to introduce   words from the vocabulary . Similarly , we also com-5985   pute a percentage of repetitive tokens in T. Table 15   summarizes the results . From Table 15 , we note   that that the percentage of tokens in |S∩T|drops   by 14 % when using both generation probability and   coverage mechanisms . This implies that the model   copies fewer tokens from the source sentence , and   introduces more tokens from the vocabulary , hence   the improved ability to generate implicit facts . Fur-   thermore , the repetitive tokens reduce by 23 % . To   that end , the results confirm that both generation   probability andcoverage mechanisms are effective   in controlling repetitive tokens , and improving the   models ’ ability to generate implicit facts .   Performance on augmented dataset . We trained   the best model from Table 12 on paraphrased and   mixed versions of the LSOIE dataset . After training ,   the model was evaluated on the original test set of   the LSOIE dataset . We note that the performance   of the model improves when trained on the mixed   dataset . Table 16 summarises the results .   E Hyperparameters   Table 17 summarizes the hyperparameters for all   the modules that were used in our experiments .   These were determined over a number of initial   experiments , and kept constant throughout all train-   ing runs conducted for this paper .   F Generator ’s Workflow   Fig . 2 summarises the workflow in the generator   module.59865987