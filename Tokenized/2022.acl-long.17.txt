  Nishant Kambhatla Logan Born Anoop Sarkar   School of Computing Science , Simon Fraser University   8888 University Drive , Burnaby BC , Canada   { nkambhat , loborn , anoop}@sfu.ca   Abstract   We propose a novel data - augmentation tech-   nique for neural machine translation based on   ROT- kciphertexts . ROT- kis a simple letter   substitution cipher that replaces a letter in the   plaintext with the kth letter after it in the al-   phabet . We first generate multiple ROT- kci-   phertexts using different values of kfor the   plaintext which is the source side of the parallel   data . We then leverage this enciphered train-   ing data along with the original parallel data   via multi - source training to improve neural ma-   chine translation . Our method , CipherDAug ,   uses a co - regularization - inspired training pro-   cedure , requires no external data sources other   than the original training data , and uses a stan-   dard Transformer to outperform strong data   augmentation techniques on several datasets by   a significant margin . This technique combines   easily with existing approaches to data augmen-   tation , and yields particularly strong results in   low - resource settings .   1 Introduction   Indeed , to a system which treats inputs as atomic   identifiers , the alphabet behind these identifiers   is irrelevant . Distributional properties are of sole   importance , and changes in the underlying encod-   ing should be transparent provided these properties   are preserved . In light of this , a bijective cipher   such as ROT- k(Figure 1 ) is in effect invisible to   modern NLP techniques : distributional features are   invariant under such a cipher , guaranteeing that the   meaning of an enciphered text is the same as the   un - enciphered text , given the key . This work ex-   ploits this fact to develop a novel approach to dataFigure 1 : ROT- kencipherment . The plaintext SRC is   enciphered to generate the ciphertexts ROT-1 ( SRC ) and   ROT-2 ( SRC ) , which share distributional features with   the plaintext but use a new encoding .   augmentation which is completely orthogonal to   previous approaches .   Data augmentation is a simple regularization-   inspired technique to improve generalization in   neural machine translation ( NMT ) models . These   models ( Bahdanau et al . , 2015 ; Vaswani et al . ,   2017 ) learn powerful representational spaces ( Ra-   ganato and Tiedemann , 2018 ; V oita et al . , 2019 ;   Kudugunta et al . , 2019 ) which scale to large num-   bers of languages and massive datasets ( Aharoni   et al . , 2019 ) . However , in the absence of data aug-   mentation , their complexity makes them suscepti-   ble to memorization and poor generalization .   Data augmentation for NMT requires produc-   ing new , high - quality parallel training data . This   is not trivial as slight modifications to a sequence   can have drastic syntactic or semantic effects , and   changes to a source sentence generally require cor-   responding changes to its translation . Existing tech-   niques suffer various limitations : back - translation   ( Sennrich et al . , 2016b ; Edunov et al . , 2018 ; Xia201et al . , 2019a ; Nguyen et al . , 2019 ) can yield seman-   tically poor results due to its use of trained models   that are susceptible to errors ( Edunov et al . , 2018 ) .   Word replacement approaches ( Gao et al . , 2019 ;   Liu et al . , 2021 ; Takase and Kiyono , 2021 ; Be-   linkov and Bisk , 2018 ; Sennrich et al . , 2016a ; Guo   et al . , 2020a ; Wu et al . , 2021a ) may ignore context   cues or fracture alignments between sequences .   This paper overcomes these limitations by ex-   ploiting the invariance of distributional features   under ROT- kciphers . We contribute a novel data   augmentation technique which creates enciphered   copies of the source side of a parallel dataset . We   then leverage this enciphered training data along   with the original parallel data via multi - source train-   ing to improve neural machine translation . We also   provide a co - regularization - inspired training pro-   cedure which exploits this enciphered data to out-   perform existing strong NMT data augmentation   techniques across a wide range of experiments and   analyses . Our technique can be flexibly combined   with existing augmentation techniques , and does   not rely on any external data .   2 Ciphertexts for Data Augmentation   A ROT- kcipher ( Figure 1 ) produces a ciphertext by   replacing each letter of its input ( plaintext ) with the   kth letter after it in the alphabet . Past work ( Dou   and Knight , 2012 ; Dou et al . , 2014 ) has explicitly   used decipherment techniques ( Kambhatla et al . ,   2018 ) to improve machine translation . We empha-   size that decipherment itself is notthe purpose of   the present work : rather , we use ciphers simply to   re - encode data while preserving its meaning . This   is possible because ROT- kis a 1:1 cipher where   each ciphertext symbol corresponds to a unique   plaintext symbol ; this means it will preserve dis-   tributional features from the plaintext . This makes   ROT - kcryptographically weak , but suitable for use   in data augmentation .   Concretely , given a set of ntraining samples   D={(x , y)}and a set of keys K , we use   Algorithm 1 to generate |K|nnew samples ; giving   ( |K|+ 1)nsamples when added to the training set .   2.1 The Naive Approach   The ciphertexts produced by Algorithm 1 are   guaranteed to be lexically diverse , not only   from the plaintext but also from one another .   Given this fact , we can naively regard each D   as a different language and formulate a multi - Algorithm 1 Cipher - Augment Training Data   lingual training setting ( Johnson et al . , 2017 ) .   For a plaintext sample x , ciphertext samples   { ROT−k(x ) , ... , ROT−k(x ) } , and target   sequence y , the multi - source model is trained by   minimizing the cross - entropy   where |K|is the number of distinct keys used to   generate ciphertexts .   While this yields a multilingual model , this for-   mulation does not allow explicit interaction be-   tween a plaintext sample and the corresponding   ciphertexts . To allow such interactions , we design   another model that relies on inherent pivoting be-   tween sources and enciphered sources . We achieve   this by adding ROT - k(source ) →source as a trans-   lation direction ; following Johnson et al . ( 2017 ) we   prepend the appropriate target token to all source   sentences and train to minimize the objective   We refer to ( 2 ) as the naive model .   Discussion . In this setting the decoder must learn   the distributions of both the true target language   and the source language . This may lead to quicker   saturation of the decoder and sub - optimal use of its   capacity , which must now be shared between two   languages ; this is a notorious property of many - to-   many multilingual NMT ( Aharoni et al . , 2019 ) .   2.2 CipherDAug : A Better Approach   To better leverage the equivalence between plain-   and ciphertext data , we take inspiration from multi-   view learning ( Xu et al . , 2013 ) . We rethink enci-   phered samples as different views of the authen-   tic source samples which can be exploited for co-202training ( Blum and Mitchell , 1998 ) . This is moti-   vated by the observation that plain and enciphered   samples have identical sentence length , grammar ,   and ( most importantly ) sentential semantics .   Given an enciphered source cipher ( x)we   model the loss for a plaintext sample ( x , y)as   L = αL(p(y|x ) ) | { z }   + αL(p(y|cipher ( x ) ) ) | { z }   + βL(p(y|x ) , p(y|cipher ( x ) ) ) | { z } ( 3 )   where the original source language sentence xis   called the anchor here since it is always paired with   each enciphered version . The first two terms are   conventional negative log - likelihoods , to encourage   the model to generate the appropriate target for   bothxandcipher ( x ) .   The third term is the agreement loss , measured   as the pairwise symmetric KL divergencebetween   the output distributions for xandcipher ( x ):   This term allows explicit interactions between   plain- and ciphertexts by way of co - regularization .   Co - regularization relies on the assumption “ that   the target functions in each view agree on labels   of most examples ” ( Sindhwani et al . , 2005 ) and   constrains the model to consider only solutions   which capture this agreement .   In cases where there are many output classes   and the model predictions strongly favour certain   of these classes , ( 4 ) may have an outsized influ-   ence on model behaviour . As a precautionary mea-   sure , we use a softmax temperature τto flatten the   model predictions , based on a similar technique   in knowledge distillation ( Hinton et al . , 2015 ) and   multi - view regularization ( Wang et al . , 2021 ) . The   flattened prediction for an ( x , y)pair is given by   p(x|y ) = exp(z)/τPexp(z)/τ(5 )   where zis the logit for the output label y. A higher   value of τproduces a softer , more even distribution   over output classes . The overall training procedure , which we dub   CipherDAug , is summarized in Algorithm 2 .   Algorithm 2 CipherDAug Training Algorithm   3 Experiments and Results   3.1 Experimental Setup   Datasets We use the widely studied IWSLT14   De↔En and IWSLT17 Fr ↔En language pairs as   our small - sized datasets . For high - resource ex-   periments , we evaluate on the standard WMT14   En→De set of 4.5 M sentence pairs . We also ex-   tend our experiments to the extremely low - resource   pair Sk ↔En from the multilingual TED dataset ( Qi   et al . , 2018 ) with 61k training samples , and dev and   test splits of size 2271 and 2245 respectively .   Ciphertext Generation and Vocabularies . We   use a variant of ROT- kwhich preserves whitespace ,   numerals , special characters , and punctuation . As   a result , these characters appear the same in both   plain- and ciphertexts .   For our naive approach , we encipher the Ger-   man side of the IWSLT14 dataset with up to 20   keys{1,2,3,4,5 , . . .,20 } . For our main ex-   periments , we encipher the source side of every   translation directionwith key { 1 } for WMT ex-   periments and keys { 1,2 } for the rest .   We use sentencepiece ( Kudo and Richard-   son , 2018 ) to tokenize text into byte - pair encodings203(BPE ; Sennrich et al . 2016c ) by jointly learning   subwords on the source , enciphered - source , and   target sides . We tune the number of BPE merges   as recommended by Ding et al . ( 2019 ) ; the result-   ing subword vocabulary sizes for each dataset are   tabulated in Table 1 .   In all experiments , we set the loss weight hyper-   parameters α , αto 1 , and βto 5 . Section 4.1   shows an ablation over βto justify this setting . We   find that softmax temperature τ= 1 works well   for all experiments ; τ= 2 results in more stable   training for larger datasets .   Evaluation We evaluate on BLEU scores(Pa-   pineni et al . , 2002 ) . Following previous work   ( Vaswani et al . , 2017 ; Nguyen et al . , 2019 ; Xu   et al . , 2021 ) , we compute tokenized BLEU with   multi_bleu.perlfor IWSLT14 and TED   datasets , additionally apply compound - splitting for   WMT14 En - DeandSacreBLEU(Post , 2018 )   for IWSLT17 datasets . For all experiments , we per-   form significance tests based on bootstrap resam-   pling ( Clark et al . , 2011 ) using the compare - mt   toolkit ( Neubig et al . , 2019 ) .   Baselines Our main baselines are strong and   widely used data - augmentation techniques that do   not use external data . We compare CipherDAug   to back - translation - based data - diversification   ( Nguyen et al . , 2019 ) , word replacement techniques   like SwitchOut ( Wang et al . , 2018 ) , WordDrop   ( Sennrich et al . , 2016a ) , and RAML ( Norouzi et al . ,   2016 ) , and the subword - regularization technique   BPE - Dropout ( Provilkov et al . , 2020 ) .   See supplemental sections A.1 and A.2 for fur-   ther baseline and implementation details .   3.2 Results from the Naive Approach   Table 2 shows our results using the naive method   on the IWSLT14 De →En dev set . Simply us-   ing 2 enciphered sources gives a BLEU score of   35.45 , which nearly matches the performance of   the best baseline , RAML+SwitchOut , at 35.47 .   Adding the ROT- k(source ) →source direction fur-   ther improves the score to 35.85 . Adding the ROT-   k(source ) →source direction consistently yields   better results than the vanilla multi - source model ,   but increasing the number of keys has a less con-   sistent effect . We hypothesize that more keys are   generally beneficial , but that the model becomes   saturated when too many are used . Based on these   observations , we limit later experiments to 2 keys .   We observe further gains by combining the naive   method with the two best performing baselines .   This emphasizes that ciphertext - based augmenta-   tion is orthogonal to other data - augmentation meth-   ods and can be seamlessly combined with these to   yield greater improvements .   3.3 Main Results   We present our main results in Table 3 . While us-   ing a single key improves significantly over the   Transformer model , augmenting with 2 keys out-   performs allbaselines . Table 4 shows additional   comparisons against approaches that introduce ar-   chitectural improvements to the transformer ( such   as MAT ; Fan et al . 2020 ) or that require large pre-   trained models , like BiBERT ( Xu et al . , 2021 ) .   On the IWSLT14 and IWSLT17 language pairs,204   our method yields stronger improvements over the   standard Transformer than any other data augmen-   tation technique ( Table 3 ) . This includes strong   methods such RAML+SwitchOut and data diversi-   fication , which report improvements as high as 1.8   and 1.9 BLEU points respectively . Data diversifica-   tion involves training a total of 7 different models   for forward and backward translation on the source   and target data . By contrast , CipherDAug trains   a single model , and improves the baseline trans-   former by 2.9 BLEU points on IWSLT14 De →En   and about 2.2 BLEU points on the smaller datasets .   On WMT14 En →De , our method using 1 key   improves by 0.6 BLEU over the baseline trans-   former and significantly outperforms word replace-   ment methods like SwitchOut and WordDropout . Low - resource setting The Sk ↔En dataset is   uniquely challenging as it has only 61k pairs of   training samples . This dataset is generally paired   with a related high - resource language pair such as   Cs - En ( Neubig and Hu , 2018 ) , or trained in a mas-   sively multilingual setting ( Aharoni et al . , 2019 )   with 58 other languages from the multilingual TED   dataset ( Qi et al . , 2018 ) . Xia et al . ( 2019b ) intro-   duced a generalized data augmentation technique   that works in this multilingual setting and leverages   over 2 M monolingual sentences for each language   using back - translation . Applying CipherDAug to   this dataset ( Table 5 ) yields significant improve-   ments over these methods , achieving 32.62 BLEU   on Sk→En and 24.61 on En →Sk .   Discussion On the relatively larger WMT14   dataset ( 4.5 M ) , despite improving significantly   over the baseline Transformer , the Base model205   ( 68 M params ) approaches saturation when ∼9 M   enciphered sentences ( 2 keys ) are added . Upgrad-   ing to Transformer Big ( 218 M ) may be viable , but   would be an unfair comparison with other mod-   els . The model capacity becomes a bottleneck   with larger datasets when the model is optimised to   translate each of the source sentences ( 4.5 M plain   and 9 M enciphered ) individually ( single - source )   as well as together ( multi - source ) through the co-   regularization loss . The results indicate that our   proposed approach works best in small and low   resource data settings .   4 Analysis   4.1 Ablations   Number of Keys Figure 2 ( left ) shows the effect   of adding different amounts of enciphered data . We   obtain the best performance using just 2 different   keys . Using more or fewer degrades performance ,   though both cases still outperform the baseline . As   noted in Section 3.2 , the model may become satu-   rated when too many keys are used .   Agreement Loss Figure 2 ( right ) shows an ab-   lation analysis on the agreement loss . We find   that CipherDAug is sensitive to the weight βgiven   to this term : increasing or decreasing it from our   default setting β= 5 incurs a performance drop   of nearly 2 BLEU . Despite the performance gains   attendant to this term , it is equally clear that agree-   ment loss can not fully account for CipherDAug ’s   improvements over the baseline : in the naive set-   ting where β= 0 , CipherDAug still outperforms   the baseline by approximately 1 BLEU .   Learning BPE vocabularies jointly vs. sepa-   rately From Table 7 , we see that there is no sig-   nificant impact on BLEU if we learn BPE vocabu-   laries separately for each language or enciphered   language from IWSLT14 De →En . This is consis-   tent with results from Neubig and Hu ( 2018 ) in the   context of mutilingual NMT .   Note that it is preferable to learn the BPEs jointly   as this allows us to limit the total vocabulary size .   When learned separately , we can not control the   combined vocabulary size which may result in a   larger or smaller vocabulary ( and therefore , a dif-   ferent number of embedding parameters ) than in-   tended.206Disentangling the effects of increased parame-   ters in the embedding layer CipherDAug lever-   ages the combined vocabularies of the original par-   allel bitext and enciphered copies of the source text .   This necessarily increases in the number of param-   eters in the embedding layer even though the rest   of the network remains identical .   To understand the effect of these extra parame-   ters , we compare CipherDAug against the baseline   Transformer model with different vocabulary and   embedding sizes . Results from different settings   are shown in Table 6 .   As we reduce the embedding dimension of our   best model ( CipherDAug with 2 keys ) from 512 to   256 , we observe a small change of -0.6 BLEU in   the final scores . With 1 cipher key , however , our   model exhibits a slight ( statistically insignificant )   improvement of +0.06 BLEU . These results show   that the few extra embedding parameters in Cipher-   DAug do not have an outsized impact on model   performance , but we emphasize that reducing the   dimensionality of the embedding layer diminishes   its expressivity and is therefore not a completely   fair comparison .   4.2 Hallucinations   The attention mechanism of a model might not   reflect a model ’s true inner reasoning ( Jain and   Wallace , 2019 ; Moradi et al . , 2019 , 2021 ) . To better   analyze NMT models , Lee et al . ( 2018 ) introduce   the notion of hallucinations . A model hallucinates   when small perturbations in its input cause drastic   changes in the output , implying it is not actually   attentive to this input .   Using Algorithm 2 of Raunak et al . ( 2021 ) , Ta-   ble 8 shows the number of hallucinations on the   IWSLT14 De - En test set for the baseline and Ci-   pherDAug models . We use the 50 most common   subwords as perturbations . CipherDAug sees a   40 % reduction in hallucinations relative to the base-   line , suggesting it is more resilient against perturba-   tions and more attentive to the content of its input .   4.3 Effect on Rare Subwords   We argue that CipherDAug is effective in part   because it reduces the impact of rare words . On   average , the rarest subword in a ROT- kenciphered   sentence is significantly more frequent than the   rarest subword in a plaintext sentence . This is   apparent in an example like the following:(6 )   Figure 3 plots the frequency of each subword in   this sentence and its ROT- kenciphered variants . In   the plaintext , we observe a series of rare subwords   ically , _ correct , andness coming from the   English borrowing . After encipherment , however ,   these are replaced by a variety of more common   subwords jd , bmm,_d , and so on . The result is   that the enciphered sentences have fewer rare sub-   words ; this allows them to share more information   with other sentences , and allows the more common   enciphered tokens to inform the model ’s encoding   of less common plaintext tokens .   We reiterate that this trend holds across the   whole corpus , and highlights the value of an aug-   mentation scheme that allows a model to see many   different segmentations of each input .   This is not the only mechanism by which Cipher-   DAug improves performance : we find improve-   ments for tokens in every frequency bucket , not   simply those which are rare ( Figure 4 ) .   4.4 Multi - view Learning   In Section 2.2 , we argue that the agreement loss   in ( 4 ) acts as a co - regularization term in a multi-   view learning setting . Multi - view learning works   best when the different views capture distinct infor-   mation . In CipherDAug , this is accomplished by207   allowing enciphered inputs to receive different seg-   mentations than plaintext inputs . As evidence that   the different views capture distinct information , we   note that even after training with co - regularization   the model remains sensitive to the choice of input   encoding , as seen in cases such as Figure 6 where   the model may produce any of three distinct outputs   depending on whether it is given plain- or cipher-   text as input . If all of the input views captured   identical information we should expect no such   variation , especially after training with an explicit   co - regularization term .   4.5 Canonical Correlation Analysis   To further analyze CipherDAug , we turn to canoni-   cal correlation analysis ( CCA ; Hardoon et al . 2004 ;   Raghu et al . 2017 ) , which finds a linear transform   to maximize correlation between values in two high   dimensional datasets . As detailed in Raghu et al .   2017 , it is useful for measuring correlations be-   tween activations from different networks .   For each IWSLT14 De - En test sentence , we save   the activations from each layer of our baseline and   CipherDAug models . For the CipherDAug model ,   we save activations on plaintext and enciphered   inputs . For every pair of layers , we compute the   projection weightedCCA ( PWCCA ) between ac-   tivations from those layers . If this value is high ( rel-   ative to a random baseline ) , this means that there is   a linear transformation under which the activations   from those layers are linearly correlated , implying   that the layers capture similar information .   Figure 5 plots the PWCCA between encoder   states from the baseline and CipherDAug models ,   and between CipherDAug encoder states with dif-   ferent input encodings . It is immediately clear that   CipherDAug learns similar , but not identical , rep-   resentations for plain- and ciphertext inputs : the   state of a layer in the de →en setting is generally   predictive of the state of that same layer in the   ROT-1(de ) →en and ROT-2(de ) →en settings .   We emphasize , however , that representations   for plain- and ciphertexts are not identical , as can   be seen by comparing against the baseline model .   Here , some layers in one model show a moder-   ate correlation to every layer of the other model ;   other layers show a strong correlation with a differ-   entlayer from the other model . This implies that ,   while the two models extract some of the same   information , they do so at different depths in the   encoder . Moreover , CipherDAug states from enci-   phered inputs present an entirely different pattern   of correlations than plaintext inputs . This implies   that CipherDAug not only learns different informa-   tion than the baseline , but that these differences   are distinct for plaintexts and ciphertexts . These   results strengthen Section 4.4 ’s claim that plain-   and ciphertexts capture distinct information .   5 Related Work   Data - augmentation ( Sennrich et al . , 2016b ) can   be broadly categorized into back - translation based   methods and those which perturb or change the in-   put ( Wang et al . , 2018 ) . Back - translation ( Sennrich   et al . , 2016b ) is arguably the de - facto data augmen-   tation method for NMT . Besides back - translating208   external monolingual data ( Edunov et al . , 2018 ) , Li   et al . ( 2019 ) forward - translate the source ( Zhang   and Zong , 2016 ) and/or backward - translate the tar-   get side ( Sennrich et al . , 2016a ) of the original   ( in - domain ) parallel data . Our technique produces   lexically diverse samples using only the original   source data , rather than relying on model predic-   tions which may be of limited quality . Belinkov   and Bisk ( 2018 ) showed that NMT models can be   sensitive to orthographic variation , and that training   with noise improves their robustness ( Khayrallah   and Koehn , 2018 ) . Common noising techniques   include token dropping ( Zhang et al . , 2020 ) , word   replacement ( Xie et al . , 2017 ; Wu et al . , 2021a ) ,   Word - Dropout ( randomly zeroing out word embed-   dings ; Sennrich et al . 2016a ; Gal and Ghahramani   2016 ) and adding synthetic noise by swapping ran-   dom characters or replacing words with common   typos ( Karpukhin et al . , 2019 ) . Adding enciphered   data is distinct from noising as the ciphertexts are   generated deterministically and follow the same dis-   tribution as the underlying natural language , simply   using shifted letters of the same alphabet .   To extend the support of the empirical data dis-   tribution , Norouzi et al . ( 2016 ) introduced RAML   on the target side ; Wang et al . ( 2018 ) proposed   SwitchOut as a more general method which they ap-   plied to the source side . Special cases of SwitchOut   include Word - Dropout and sequence - mixing ( Guo   et al . , 2020a ) , which exchanges words between sim-   ilar source sentences to encourage compositional   behaviour . Such methods generate several different   samples for each sentence because of the large vo-   cabulary to choose replacements from ; they often   give poor coverage despite this . In contrast , Ci-   pherDAug guarantees lexically diverse examples   with semantic equivalence to the source sentences   without having to choose specific replacements .   Adversarial techniques ( Gao et al . , 2019 )   perform soft perturbations of tokens or spans(Takase and Kiyono 2021 , Karpukhin et al . 2019 ) .   An advantage of soft replacements over hard ones   is that they take into account the context of the to-   kens being replaced ( Liu et al . , 2021 ; Mohiuddin   et al . , 2021 ) . These methods require architectural   changes to a model whereas CipherDAug does not .   Ciphertext - based augmentation is orthogonal to   most other data - augmentation methods and can be   seamlessly combined with these to jointly improve   neural machine translation .   6 Conclusion   We introduce CipherDAug , a novel technique for   augmenting translation data using ROT- kenci-   phered copies of the source corpus . This technique   requires no external data , and significantly outper-   forms a variety of strong existing data augmen-   tation techniques . We have shown that an agree-   ment loss term , which minimizes divergence be-   tween representations of plain- and ciphertext in-   puts , is crucial to the performance of this model ,   and we have explained the function of this loss term   with reference to co - regularization techniques from   multi - view learning . We have also demonstrated   other means by which enciphered data can improve   model performance , such as by reducing the im-   pact of rare words . Overall , CipherDAug shows   promise as a simple , out - of - the - box approach to   data augmentation which improves on and com-   bines easily with existing techniques , and which   yields particularly strong results in low - resource   settings .   Acknowledgements   We would like to thank the anonymous reviewers   for their helpful comments and Kumar Abhishek   for the numerous discussions that helped shape this   paper . The research was partially supported by the   Natural Sciences and Engineering Research Coun-   cil of Canada grants NSERC RGPIN-2018 - 06437   and RGPAS-2018 - 522574 and a Department of Na-   tional Defence ( DND ) and NSERC grant DGDND-   2018 - 00025 to the third author.209References210211212213A Appendix   A.1 Baselines   To compare model performance on the small and   mid - sized datasets , we re - implemented most base-   lines :   •we used the pseudocode in appendix A6 along   with proofs in appendices A1 and A2 of   the SwitchOut paper ( Wang et al . , 2018 ) to   implement SwitchOut , WordDrop ( Sennrich   et al . , 2016a ) , RAML ( Norouzi et al . , 2016 ) ,   RAML+SwitchOut and RAML+WordDrop   as special cases of SwitchOut . The hyperpa-   rameter τwas tuned on the dev set for each   language pair . The respective τvalues are   0.9 and 0.95 for De - En and 0.85 and 0.95 for   Fr - En .   •we followed the instructions on the offi-   cial open - sourced repository to reproduce   BPE - Dropout ( Provilkov et al . , 2020 )   with the recommended value of p=0.1 us-   ing the sentencepiece tokenizer . We   trained models on our Fairseq codebase   for IWSLT14 De ↔En and WMT14 En →De .   We reported the SacreBLEU numbers for   IWSLT17 Fr ↔En from literature .   •experiments on data - diversification ( Nguyen   et al . , 2019 ) were reproduced using the offi-   cial open - sourced implementation on top of   theFairseq toolkit . For WMT14 En - De ,   we use a Transformer Base ( 68 M parameters )   for a fair comparison across methods , whereas   the original implementation employs a Trans-   former Big model ( 210 M parameters ) ..   Note that this method requires training 7 in-   dividual models and has a total effective data   size 7 times the original size to produce best   results .   We reported the performance of Mixed-   Representation ( Wu et al . , 2020 ) baseline for   IWSLT14 De →En from the literature as we   could reproduce the experiemnts . However ,   to the best of our knowledge , we employ set-   tings identical to Mixed - Repr . baseline for   IWSLT14 De →En in our model – the same tok-   enizer ( SentencePiece ) , vocabulary size ( 12k),model size ( transformer_iwslt_de_en ) ,   decoding hyper - parameters ( beam 5 , len - pen 1.0 )   and evaluation script ( multi-bleu.perl ) .   A.2 CipherDAug : Models and   Hyperparameters   The smaller datasets ( IWSLT14   De↔En , IWSLT17 Fr ↔Enand   TED Sk ↔En ) are trained with the   transformer_iwslt_de_en config with 6   layers of encoder and decoder with 4 attention   heads , embedding size of 512 , feed - forward size of   1024 , network dropout 0.3 and attention dropout   0.1 . The peak learning rate is 6e−4with 8000   warmup steps .   For training the on WMT14 En →De dataset ,   we use Transformer Base config , dubbed   transformer_wmt_en_de infairseq   toolkit , with 6 layers of encoder and decoder   with 8 attention heads , embedding size of 512 ,   feed - forward size of 2048 , dropout 0.1 . The peak   learning rate is 7e−4with 4000 warmup steps .   Following conventional training of Transform-   ers , we use Adam optimizer with betas ( 0.9 , 0.98 )   andϵ= 10andinverse_sqrt learning rate   scheduler . Label smoothing is set to 0.1 .   We also set an agreement_loss_warmup   to 2000 steps . This signifies that until the specified   number of steps , the model will train with regular   cross - entropy loss without computing KL diver-   gence . This is done to let the model gain some con-   fidence before we start applying co - regularization .   This does not improve or worsen model perfor-   mance , but we find that this helps the model con-   verge slightly faster .   Thetransformer_iwslt_de_en models   ( for IWSLT14 , IWSLT17 and TED datasets )   were run on 2 Titan RTX GPUs while the   transformer_wmt_en_de model for   WMT14 En - De was run on 8 A6000 GPUs . All   models were run until convergence with an early   stopping patience of 15 validation steps . While   smaller models converged within 100k updates,214   the model on WMT14 dataset was force stopped at   400k updates while the model was still improving   ( at a very slow rate ) .   For producing translations , the decoder beam   size is set to 4 and length penalty 0.6 for WMT ,   and 5 and 1.0 for all other experiments . We   evaluate on BLEU scores ( Papineni et al . ,   2002 ) . Following previous work ( Vaswani   et al . , 2017 ; Nguyen et al . , 2019 ; Xu et al . ,   2021 ) , we compute tokenized BLEU with   multi_bleu.perlfor IWSLT14 and TED   datasets , additionally apply compound - splitting   for WMT14 En - DeandSacreBLEU ( Post ,   2018 ) ( Signature : nrefs:1|case : mixed|   eff : no|tok:13a|smooth : exp|version   : 2.0.0 for IWSLT17 datasets .   Finally , all results are reported on translations   obtained after averaging the last 5 checkpoints .   A.3 Additional Experiments   A.3.1 Disentangling the effects of increased   parameters in the embedding layer   Additional experiment based on results from   Sec . 4.1 – Table 6 . CipherDAug uses the com-   bined vocabularies of the original parallel bitext   and enciphered copies of the source text . This nec-   essarily increases in the number of parameters in   the embedding layer even though the rest of the   network remains identical .   Using embeddings largely independent of the   vocabulary size . To completely disambiguate the   effects of the different sizes of vocabularies in the   baseline and CipherDAug transformers , we replacethe embedding layer with ALONE embeddings   ( Takase and Kobayashi , 2020 ) .   While the conventional embedding layer re-   quires an embedding matrix E∈Rwhere   Vis the vocabulary size , ALONE lets different   words in the vocabulary share a vector element   with each other . To concretely obtain a word rep-   resentation for w , ALONE computes an element-   wise product of the base embedding o∈R   and a filter vector , and then applies a feed - forward   network of dimension D to increase its expres-   siveness .   See Takase and Kobayashi ( 2020 ) for more de-   tails on ALONE embeddings . We integrated the   officially released codewith our implementation .   Table 10 compares parameter counts with and with-   out ALONE , and Table 9 details the result of using   ALONE embeddings with CipherDAug .   A.3.2 Effect of different dropout probabilities   To further study the efficacy of our method in   under - regularized scenarios , we compare the base-   line transformer model with CipherDAug for the215dropout values of 0 ( no regularization ) , 0.1 , 0,2   and 0.3 in Table 11 . Evidently , our method shows   consistent gains over the baseline . While a dropout   value of 0.3 is optimal for both models , Cipher-   DAug records a BLEU of +4.5 against the base   model with dropout set to 0 which removes regu-   larization as well any stochasticity from the model .   This suggests that the variation in input data intro-   duced by CipherDAug can yield improvements for   transformer models , with similar effects to adding   dropout ( albeit to a lesser degree ) .   A.3.3 Complimenting data - diversification   with CipherDAug   To further support our claim that our method   can be combined with existing data - augmentation   techniques , we extend CipherDAug into the data-   diversification ( Nguyen et al . , 2019 ) framework .   Data - Diversification : This is a simple technique   that employs the following steps to augment data   without changing the model architecture :   Algorithm 3 Data - diversification   We adapt Algo 3 to incorporate CipherDAug   by modifying steps 1 and 2 – we replace the for-   ward models with one CipherDAug model with 2   keys trained on IWSLT14 De →En and the back-   ward models with a CipherDAug model with 2 keys   trained on IWSLT14 En →De . We leverage the ob-   servation that CipherDAug often produces lexically   diverse translations for the source and enciphered-   source sentences ( Figure 6 ; Figure 9 in Appendix   ) . Following Step 5 above , we finally combine the   3 forward translations and the 3 backward transla-   tions with the original parallel data , and train a finalmodel on the resulting augmented data . The results   in Table 12 demonstrate that the combination is   more effective than data diversification on its own .   A.4 Comparison with other methods   We show a comparison of our method CipherDAug   with a variety of data - augmentation methods as   well as other methods that introduce architectural   changes for better neural machine translation in   Table 13 .   A.5 More Examples of Rare Subwords   The examples in this section further illustrate how   CipherDAug helps to eliminate rare subwords :   de : hey , warum nicht ? ( Rarest subword   _ hey occurs 2 times . )   ROT-1(de ): ifz , xbsvn ojdiu ? ( Rarest   subword _ if occurs 26 times . )   ROT-2(de ): jgß , yctwo pkejv ? ( Rarest   subword _ jg occurs 15 times.)216   de : wir alle lieben baseball ,   oder ? ( Rarest subword _ baseball occurs 7   times . )   ROT-1(de ): xjs bmmf mjfcfo cbtfcbmm ,   pefs ? ( Rarest subword cbmm occurs 14 times . )   ROT-2(de ): ykt cnng nkgdgp dcugdcnn ,   qfgt ? ( Rarest subword dcnn occurs 14 times.)217218