  Mohammad Javad Hosseini Filip Radlinski Silvia Pareti Annie Louis   Google Research   { javadh,filiprad,spareti,annielouis}@google.com   Abstract   Recent advances in language modeling have   enabled new conversational systems . In par-   ticular , it is often desirable for people to make   choices among speciﬁed options when using   such systems . We address this problem of ref-   erence resolution , when people use natural ex-   pressions to choose between the entities . For   example , given the choice ‘ Should we make   a Simnel cake or a Pandan cake ? ’ a natural   response from a dialog participant may be in-   direct : ‘ let ’s make the green one ’ . Such nat-   ural expressions have been little studied for   reference resolution . We argue that robustly   understanding such language has large poten-   tial for improving naturalness in dialog , rec-   ommendation , and search systems . We create   AltEntities(Alternative Entities ) , a new   public dataset of 42 K entity pairs and expres-   sions ( referring to one entity in the pair ) , and   develop models for the disambiguation prob-   lem . Consisting of indirect referring expres-   sions across three domains , our corpus enables   for the ﬁrst time the study of how language   models can be adapted to this task . We ﬁnd   they achieve 82%-87 % accuracy in realistic   settings , which while reasonable also invites   further advances .   1 Introduction   Natural dialog often requires resolving referring   expressions ( REs ) , not only within and across texts ,   but also for grounding natural language expres-   sions to speciﬁc entities or images . We focus on   a speciﬁc conversational setting where a speaker ’s   utterance intends to disambiguate between known   named entities . While many aspects of RE resolu-   tion have been studied extensively , past work has   focused on pragmatic reasoning ( Dale and Reiter ,   1995 ; Frank and Goodman , 2012 ) , inﬂuence of dis-   course ( Orita et al . , 2015 ) , and multimodal ( e.g. ,   image ) context ( Zhang et al . , 2018).Did you mean a Simnel or Pandan cake ?   It looks surprisingly green in color   Without any frosting or fruit   It is made from some leaf   Comes from Indonesia   Is n’t the Easter one   Table 1 : Responses to the question which intend to   choose Pandan cake over the alternative .   In the speciﬁc case of dialog , when people make   choices , the natural REs are not always item names ,   spatial locations or attributes present in the ques-   tion . For instance when the choice is among items   with similar names ( perhaps disambiguating au-   tomatic speech recognition errors ) , or items with   difﬁcult to pronounce names , or where the user   does not even recall which name is correct but in-   stead recalls some higher level attribute , the user   may choose an indirect expression ( Table 1 ) . Most   related to our work , Celikyilmaz et al . ( 2014 ) pre-   viously studied REs in response to a set of related   items ( e.g. , Harry Potter movies ) shown in a user   interface . Their work both contains direct ( using   entity name ) , indirect , as well as locational ( en-   tity ’s position on the screen ) expressions . Predating   recent advances in language models ( LMs ) , their   best model is a decision tree classiﬁer consuming   knowledge graph metadata .   In this work , we created the AltEntities cor-   pus by a multi - step process , soliciting crowdwork-   ers to provide diverse yet realistic natural ex-   pressions for selecting entities in three domains : , , and . To obtain natu-   ral and casual dialogic language , we introduce a   novel cartoon - based annotation approach ( Figure   1).AltEntities consists of 6,247 alternative ques-   tions ( presenting two entities ) along with 42,529   REs . In this context , REs are typically deﬁnite   noun phrases with a pronominal head and a restric-   tive relative phrase or one of its reduced variants .   Our experiments are based on ﬁne - tuned BERT12313(Devlin et al . , 2019 ) and T5 ( Raffel et al . , 2020 )   LMs . We assess the representation of entity names   as well as other sources of entity information . We   ﬁnd that the results depend signiﬁcantly on the   type of entity information provided to the models   alongside the REs : If a LM only has access to   the entity names but no other information , a case   that might happen especially for long tail entities ,   accuracy is around 60 % . On the other hand , if a LM   is ( unrealistically ) given entity information that is   identical to that shown to annotators producing the   REs , accuracy is very high ( up to 95 % ) . However ,   if the model ( more realistically ) only has access to   generic information that may or may not overlap   with annotators ’ knowledge ( Section 5 ) , accuracy   of our models is only 82%-87 % , leaving signiﬁcant   room for methodological improvements .   2 Related Work   Our work adds to recent efforts to allow users to   speak more naturally to conversational systems .   Here , we present the most related studies focusing   on the properties of REs as well as their resolution .   Alternative Questions . Our questions belong   to the class of alternative questions ( e.g. ‘ Are you   staying or leaving ? ’ ) . Several studies have focused   on the form and semantics of such questions , and   differences from yes / no questions particularly on   the basis of prosody ( Beck and Kim , 2006 ; Biezma   and Rawlins , 2012 ; Pruitt and Roelofsen , 2013 ) .   This paper focuses on the deep understanding   of answers to such alternative questions when they   are posed for selecting between two entities .   Speaker - Listener Cooperation . The research   in this space follow the Rational Speech Act Theory   ( Frank and Goodman , 2012 ) , where the way speak-   ers and listeners reason about each others ’ inten-   tions and beliefs explains which attributes speakers   pick to describe an entity , and how listeners disam-   biguate the entity . V ogel et al . ( 2013 ) ; Monroe et al .   ( 2017 ) focus on the pragmatic reasoning involved   during the conversation which helps in reaching a   common understanding of the topic . Wilkes - Gibbs   and Clark ( 1992 ) study how REs change as the con-   versation proceeds . In an experiment , they show   that participants start from long and indeﬁnite de-   scriptions of images , but end up with short and def-   inite references . Jordan and Walker ( 2005 ) study   the subproblem of content and attribute selection   for generating object descriptions .   In our data collection , we assume a conversationbetween two humans in three dialog turns , where   the ﬁrst two turns prime the RE produced in the   last turn ( Section 3 ) .   Common Ground . In addition to the interlocu-   tors ’ intentions , their prior or shared knowledge   also plays an important role in how they under-   stand each other ’s utterances . Sometimes the com-   mon knowledge arises from a shared situation , e.g. ,   in navigation dialog ( Engonopoulos et al . , 2013 ;   Misu et al . , 2014 ; Fang et al . , 2014 ) or the presence   of a visual space ( Yu et al . , 2018 ; Bernardi and   Pezzelle , 2021 ) . In the latter , the common ground   is given , i.e. , it is assumed the image is what all   participants in the interaction see in the same way .   In many other situations , e.g. , in a dialog between   two friends about a movie or a book , the common   ground is hidden and we can only make assump-   tions of what information participants share .   In this work , during data collection , we assume   that annotators have access to rich common ground   involving multiple modalities such as text , image ,   and video ( Section 3.3 ) . During model training   inference , we explore performance with varying   levels of background information ( Sectoin 5.2 ) .   Implicature Understanding . This paper ad-   vances the broad area of understanding implicature   in dialog . For example , a few recent papers devel-   oped datasets and models for indirect boolean re-   sponses ( without saying ‘ yes ’ or ‘ no ’ ) ( Pragst and   Ultes , 2018 ; Louis et al . , 2020 ; Takayama et al . ,   2021 ; Damgaard et al . , 2021 ) . Interestingly , Ruis   et al . ( 2022 ) shows that LLMs can not solve such   implicatures in a zero - shot setting .   RE resolution . There are few prior studies   around the data and models for resolution tasks   such as ours . Stoyanchev et al . ( 2021 ) built a   method where references to items from prior con-   text in a dialog are resolved by detecting state up-   dates . Unlike our work , their REs focus on at-   tributes ( e.g. , Italian inthe Italian restaurant ) dis-   cussed in prior dialog . Celikyilmaz et al . ( 2014 )   collect REs to a target item among others shown on   a screen ( e.g. , a set of Harry Potter movies ) . Their   expressions contain both direct ( reference to entity   name ) and indirect references , where the latter com-   prise about 25 % of the data ( ≈6 K REs ) . To aid the   resolution of indirect ones , they include features   which capture the overlap between an expression   and knowledge graph attributes for each item .   Our work creates a large scale corpus ( 42 K REs )   exclusively for indirect REs , and explores how LMs12314encode the knowledge for disambiguation .   3 Collecting Rich Referring Expressions   To maximize generalizability , we collect data in   three domains : , , and .   These were selected to cover a diverse variety of   entity types with different kinds of available infor-   mation — e.g. plot summaries for books , images   for recipes , and lyrics and videos for songs . We   performed careful and detailed annotations , and   explain the annotation steps in this section .   3.1 Cartoon - driven Annotation Setup   Previous work in question - answering and dialog   typically asks annotators to complete text - based in-   put boxes ( Rajpurkar et al . , 2016 ; Choi et al . , 2018 ;   Rajpurkar et al . , 2018 ; Reddy et al . , 2019 ; Eric   et al . , 2020 ) . We employ a novel cartoon - bubble   completion method , aiming to immerse annotators   in the dialog setting to obtain more natural and in-   formal REs . We start with a brief overview of the   setup , and then explain the steps in detail .   Figure 1 shows the ﬁrst ( of our two ) annotation   screens . Annotators are shown a cartoon with two   characters ( BobandAlice ) in a ﬁctional conversa-   tion , and asked ( as Bob ) to complete the last speech   bubble . This pictorial depiction , and the casting of   the dialog as a casual chat between friends encour-   age the annotators to produce friendly , short , and   dialogic responses . However , annotators are gen-   erally unlikely to know details about entities sam-   pled from a collection . Therefore , we also provide   background information on the entities ( bottom of   Figure 1 ) , corresponding to common knowledge   that the two characters could share on the topic .   After annotators are shown this information , they   proceed to a second screen ( Figure 2 ) . It indicates   one of the entities ( books in this example ) . They   are asked to describe that entity ( indirectly ) with 3   to 5 responses : We found eliciting more entries en-   courages diversity and depth in the responses . Our   data consists of the entity pairs , their descriptions ,   the target entity , and annotator expressions .   From Figure 2 , note that once on the response   screen , annotators can not re - read descriptions . This   encourages recall from memory . The reasoning   behind this , and many other aspects of this design ,   are explained in the next sections.3.2 The Conversational Cartoon   The cartoon has three cells as shown in Figure 1 .   The ﬁrst is a domain - speciﬁc utterance intended   to set context . For example , ‘ Remember that book   we saw at the store ? ’ sets up the dialog as one   recalling a speciﬁc book . These utterances are   from a set of ﬁve manually written expressions for   each domain , with one selected at random for each   conversation . Examples in the and   domains are ‘ That recipe on today ’s Masterchef   was too good ! ’ and‘You sang that song really well   yesterday . ’ Appendix A shows all these utterances .   Thealternative question is presented in the sec-   ond cell . This question follows a ﬁxed template :   Do you mean ‘ A ’ or ‘ B ’ ? where ‘ A ’ and ‘ B ’ are the   names of two related entities . Our entities are sam-   pled from Wikipedia page titles , with any disam-   biguation parentheses removed . When the names   are identical , we retain the Wikipedia disambigua-   tion : For instance , one such question is Do you   mean ‘ The Gladiator ( Turtledove novel ) ’ or ‘ The   Gladiator ( Scarrow novel ) ’ ? .   The third cell is completed by the crowdworkers ,   assuming the role of Bobto enter text that refers to   the target entity . They enter those expressions as   shown in Figure 2 . Further screenshots of our inter-   face for all domains are provided in Appendix B.   3.3 Entity Background   In real dialogs , when people differentiate between   options , they draw on partial knowledge about enti-   ties that they recall . We aimed to foster a similar   situation in our corpus , while doing so in a con-   trolled manner without requiring domain - expert   annotators . As such , when selected entities are   shown to annotators , they are also presented with   background information ( bottom of Figure 1 ) . We   draw the background also from Wikipedia , bias-   ing towards sections relevant to each domain . For , these are the main ( ﬁrst ) and plot summary   sections . For , we used the main , prepa-   ration , and ingredients sections . For each entity ,   up to 750 characters of oneof these sections are   shown on the interface . For , the food ’s   imageis also always shown to help the annotators   quickly realize what it looks like ( Figure 3 ) .   For , however , we found Wikipedia text   to be less useful : Pages contain details and trivia   ( e.g. , 5th single on the album orsold 4 million   copies ) , which we judged unlikely to be included12315   in natural background knowledge about a song .   On the other hand , song lyrics and music are very   relevant in this domain , but are not usually found in   Wikipedia . Consequently , we presented a Google   search link for the song in the background section ,   and asked the annotators to listen to at least some   of each song , and read about them before writing   expressions . The search query contained the song ’s   title and its artist , e.g. , Hello ( by Adele ) . Since   information about the song comes from search , we   also biased our candidates towards popular songs ,   which have more detailed results ( Section 3.4 ) .   3.4 Generating Alternative Questions   The alternative questions ( Do you mean ‘ A ’ or ‘ B ’ ? )   are generated automatically : ( i ) Candidate entities   are extracted from English Wikipedia for each do-   main ( Section 3.4.1 ) , then ( ii ) we substitute ‘ A ’ and   ‘ B ’ by sampling entity pairs ( Section 3.4.2 ) .   3.4.1 Selecting Candidate Entities   For each domain , we collect English Wikipedia arti-   cles by checking the presence of certain Wikipedia   templates ( infoboxes ) , and the presence of particu-   lar sections : For , we additionally included   articles with an ingredients section .   This set was then ﬁltered to exclude very short   articles , or those ambiguous between domains .   For , we use article length ( number of sec-   tions / subsections ) as a proxy for popularity , and   choose the top≈1000 articles . To remove any   sensitive or offensive content , we also ﬁlter articles   whose content matches a list of sensitive words .   Appendix C contains the details of the above ﬁlters .   Table 2 shows the number of candidate entities.12316   3.4.2 Sampling Entity Pairs   Much linguistic work on alternative questions has   focused on the semantics and pragmatics of these   utterances ( Biezma and Rawlins , 2012 ) , but we   also need to make decisions about which entity   pairs could make for a challenging disambiguation   problem . Entity pairs sampled uniformly at random   are less likely to be interesting , since they may not   share many properties , making disambiguation eas-   ier . In this work , we develop entity pair sampling   techniques at different similarity levels , as a proxy   for disambiguation difﬁculty .   Uniform sampling . Entity pairs are sampled   uniformly at random from the domain .   Same name . These entities have the same   name in Wikipedia followed by a disambiguation   phrase within parentheses . An example is Dawn   ( McLaughlin novel ) andDawn ( Andrews novel ) .   Similar title . These entities have a similar title   in terms of character edit distance ( distance ≤3 ) ,   where the title could optionally consists of a disam-   biguation phrase within parentheses .   Similar description . This method looks for   deeper similarity within the text of Wikipedia ar-   ticles : We sample a ﬁrst entity uniformly , then   select the second with the highest similarity using   a Universal Sentence Encoder ( Cer et al . , 2018 ) .   The input to the encoder is the Wikipedia section   shown as the background knowledge to annotators .   Similar infobox attributes . Here we take enti-   ties that share important domain - speciﬁc properties ,   e.g. , recipe origin , or the song genre . We match en-   tities ( except ) using the ‘ attributes ’ listed in   the Wikipedia infobox : { type } and { type , country }   for , and { genre } , { artist } , and { genre ,   artist } for .   We applied the same name method only to , and the similar title method only to and . The other domains did not   contain enough such examples . We applied the   similar description method to all domains . We   applied the similar infobox attributes method to and , but not the domain ;   however , some pairs with identical attributes were   already covered by the other methods for .   Table 3 shows the number of sampled entity pairs   for each domain and sampling method .   3.5 Annotator Instructions and Pilot Runs   To maximize RE naturalness , we also provided an-   notators different domain - speciﬁc examples . Fig-   ure 2 shows those for the book The sympathizer .   The REs are about topic ( about Vietnam war ) ,   timeline ( set in the 70s ) , and contrasts ( Not the   one about slavery , and The one published earlier ) .   They also emphasize use of general statements in-   stead of overly speciﬁc and unrealistic ones , e.g. ,   set in the 70s instead of 1975 . Table 4 shows a   detailed note on desirable expressions .   We performed pilot studies to understand how   annotators responded to our instructions , and used   these to reﬁne the instructions . A ﬁrst study ( for ) examined how annotators should use the   background text , comparing designs where annota-   tors could , or could not , go back - and - forth between   the description screen ( Figure 1 ) , and the data col-   lection screen ( Figure 2 ) . With back - and - forth pos-   sible , the responses contained excessive details ,   e.g. , reiterating large portions of background text   ( The book that was last of three juvenile novels   that Wollheim wrote for Winston ) . With back - and-   forth removed , annotators produced shorter REs   ( 7.99vs9.61words ) , with fewer proper nouns and   numbers per RE ( 0.43vs0.88 ) as they are harder12317to remember . They also used more contrastives ,   e.g. , starting with ‘ not the ’ ( 21.8%vs2.2 % ) which   involve drawing on information about both books .   Thus , we adopted the memory recall setting . After   the ﬁrst pilot study , we performed one pilot per   domain for relatively small instruction reﬁnements .   4 The AltEntities Corpus   Our annotations were carried out using a pool of   around 60in - house crowdworkers . They were all   native English speakers recruited from U.S. , U.K. ,   Canada , and Australia so as to obtain a diverse set   of perspectives . Each question was shown to two   workers to get multiple inputs per question . Around   2 K entity pairs were annotated for each domain   resulting in around 42 K expressions in total . Table   5 shows the ﬁnal corpus statistics , and Table 6   shows example expressions for the three domains .   We release the dataset under the CC - BY SA 3.0   License as per the Wikipedia License .   The REs for were on average a word   longer than for other domains . They also con-   tained more named entities per expression . Each   domain contains some repeated REs ( e.g. , the pop   song ) , that are often high - level responses , e.g. , a   song ’s genre . The domain contains the   most unique responses . The number of contrastives ,   estimated as REs starting with “ not the " , are from   8%in up to 20 % in .For   and , we manually checked 200random   REs for references to modalities other than text .   Around 10 % multi - modal REs were present in the domain ( mostly color ) , and 20 % in the domain ( mostly beat , speed , and mood ) .   We estimated the RE error rate by manually in-   specting 40question samples ( around 250to300   expressions ) per domain . The error rate is between   4.5%to6.8%for the three domains . 78 % of these   errors were due to the RE applying to both items ,   not just the target entity . The remaining errors were   mostly due to confusing the two entities . We also   note that the rate of exact string match between   REs and Wikipedia text is < 1 % .   The annotators were inspired by the provided   stylistic cues in the instructions ( e.g. , starting with   the one orI meant the ) , but followed our guidelines   to vary their responses as well . We observed that   the content of REs ( e.g. , timeline , lyrics , singer or   band information , instrument ) included both the   categories covered by the provided examples ( e.g. ,   timeline for books and songs ) and novel categories   ( e.g. , background information on books and songs   such as The one inspired by a Rolling Stones song ) .   5 Task and Models   Indirect reference resolution can be deﬁned as   follows : Given an alternative question with K   choicesC={c , . . . , c } , and a RE r , models   should disambiguate the choice c∈Cintended   byr . We assume rdoes not directly mention cby   its name or position , but does uniquely refer toc .   5.1 Information Available to Models   At a minimum , all models require the RE rand   the names of the choices C={c , . . . , c } .   In addition , models may use textual descriptions   { s , . . . , s}to aid disambiguation . We deﬁne12318choice text s(1≤i≤K ) as : ( a ) The entity name   c , or ( b ) the concatenation of cand the textual de-   scription s , separated by a delimiter . We consider   the following four experimental setups . : The entity name without further descrip-   tion of the entities . We use this setting as a baseline .   For the remaining models , we add the following   description to the name ( truncated to 512tokens ):   IB : The concatenation of all infobox   key - value pairs ( e.g. , ‘ genre : pop ’ ) .   U B : The IB   text , concatenated with all the Wikipedia sections   of the entity , excluding the section shown to the   annotators as background . Since annotators were   shown a search link and not a speciﬁc Wikipedia   section for the domain , we do not remove   any Wikipedia section for the entities . We   note that the U B might   have some overlap with the information shown to   crowdworkers , but the text is not directly given to   them . Hence , it is a fair setup to evaluate models   in a practical system where the models might not   have all the background information .   O : The same background text that was   shown to the annotators ( Section 3.3 ) . Note that   this only exists for and , as for , annotators were only shown a search link .   5.2 Models   We evaluated 5 different models . For each , we   score match to each entity choices and select c   with the highest score value .   Universal Sentence Encoder : We calculate the   cosine similarity between the universal sentence   encoder ( USE ; Cer et al.2018 ) embeddings for the   RErand each choice ’s text s.   Entailment : Using a textual entailment classi-   ﬁer , we classify whether a choice ’s text sentails   the RE r. We use the conﬁdence of the ‘ entailment ’   label as the score . We use a BERT model trained   on the MNLI dataset ( Williams et al . , 2018 ) as our   classiﬁer . For all models based on BERT , we use   BERT large uncased .   BERT . We turn our task into binary classiﬁca-   tion : We make one example per choice ( c , r ) with   label 1 if rrefers to c ; otherwise , label 0 . We ﬁne-   tune BERT with a binary classiﬁcation layer ( with   two units ) on top of its [ CLS ] token embeddings .   The LM input is the sequence [ CLS]s[SEP]r . Dur - ing inference , for each choice c , we compute the   probability of label 1as its score .   BERT Joint . In contrast to the above bi-   nary setup , we encode all the Ksequences   [ CLS]s[SEP]rwith BERT . We apply a linear layer   ( with one unit ) on top of the [ CLS ] token embed-   dings from each sequence . We normalize the scores   using softmax . Finally , we minimize a categorical   cross entropy loss given the Kscores . During in-   ference , we directly use each choice ’s score .   T5 . We turn our task into binary classiﬁcation ,   as with the BERT binary model . We ﬁne - tune a   T5 XL model ( 3B parameters ) with input sequence   “ expression : rentity : cdescription : s ” and output   sequence 1or0 . For the input type , the input   sequence omits the “ description ” part .   6 Experiments   We split the questions in the AltEntities corpus   in each domain into training ( 70 % ) , development   ( 15 % ) , and test ( 15 % ) sets . To avoid information   leaking between the sets , we allow each target item   to be in only one of the sets . For the USE and en-   tailment models , we do not tune any hyperparame-   ters . For supervised models , we tune the learning   rate , batch size , and number of epochs using a grid   search on the development data ( 96conﬁgurations   for BERT and 24conﬁgurations for T5 ) . We report   the hyper - parameter details in Appendix D.   6.1 Reference Resolution Accuracy   We compute the accuracy of each ( alternative ques-   tion , RE ) pair , i.e. whether the correct choice is   scored highest . As K=2 in our experiments , a   random baseline has accuracy 50 % .   We show the test set results in Table 7 for all do-   mains and input types . For each model , we also   show the average results of all input types . Among   the models , USE performs worst ( 61.03 % ) , fol-   lowed by the entailment model ( 66.91 % ) . BERT   Joint ( 73.56 % ) is on average 1.61 % better than   BERT ( 71.52 % ) , conﬁrming that modeling the   choices jointly is effective . T5 has the highest av-   erage results ( 77.43 % ) , as expected given that we   experimented with T5 XL with 3B parameters com-   pared to BERT large with 360M.   In the O setting for and ,   accuracy is understandably high ( up to 95.10 % for and92.60 % for ) . We note that12319   these results are an over - estimate of the model capa-   bilities . On the other hand , in the setting , in   most cases the results are slightly above 50 % , with   the best result being 61.97 % for the domain   with the T5 model . Here the LMs rely on their   memorized entity knowledge ( Petroni et al . , 2019 ) ,   suggesting that BERT and T5 embeddings are not   sufﬁcient to resolve arbitrary entity references .   With the IBinput , the T5 model accu-   racy is 78.30%,83.33 % and74.28 % for , , and , respectively . It increases to   83.40%,86.76 % , and 82.27 % , respectively , with   theU B input where we add   unstructured text data to the structured infobox data .   This shows the text is helpful when resolving REs .   In practical settings , models should work with rele-   vant , but not necessary the same background knowl-   edge as users because ( 1 ) it is not possible to have   access to users ’ actual knowledge , and ( 2 ) mod-   els always have some limitation in the amount of   text they can input . We thus rely on the U   B setting as a realistic setting for mea-   suring the capabilities of the different models.6.2 Cross - Domain Experiments   Reference resolution is a semantic task , and ide-   ally models would learn general task aspects rather   than domain details . We test generalization by ﬁne-   tuning our models on one domain and testing on   another . We used the U B   setting for these experiments as the most realistic .   Table 8 shows the T5 model results . We do not   observe much difference when models are tested   out of domain , supporting the hypothesis that our   models are indeed generalizable . This observation   is rather important since our models could be used   without separate training for new choice domains .   We also create a mixed training ( and develop-   ment ) set that combines the data of the three do-   mains . The mixed training set gives better results   on average , taking advantage of larger training set   and cues from all the domains . However , since the   dataset in each domain is relatively large , the mixed   training does not increase the results substantially .   6.3 Results and Entity Similarity   Section 3.4.1 explained how we selected entity   pairs to have different levels of similarity . We now   examine how this affects performance . Table 9   shows the results for the T5 model with the U- B input . We compute accu-   racy per test example subset , where each originated   from a speciﬁc similarity sampling method .   As expected , when the two entities are randomly   selected , disambiguation is easiest since they have   little in common . The task becomes harder as enti-   ties become more similar , with entities with similar   infobox attributes having the lowest performance .   6.4 Error Analysis   We analyzed the errors from the T5 model in the   U B setting , to understand12320   if there are systematic errors which could be im-   proved upon in the future . We manually analyzed   40incorrectly predicted development set examples   per domain . We show four different error types and   their percentages per domain in Table 10 .   In most cases , there is no textual overlap between   the RE and the background . This is because either   the relevant text is removed ( by design ) since it is   shown to the raters , or the Wikipedia text does not   contain the information at all ( e.g. , music lyrics ) .   Future research could evaluate how to adapt LMs to   improve their entity knowledge to reason beyond   the input textual evidence . In addition , retrieval   augmented LMs could be applied to retrieve rele-   vant information before performing the prediction   ( Borgeaud et al . , 2022 ; Shi et al . , 2023 ) .   In other cases , the model suffers from poor rea-   soning , e.g. , that clam is seafood , or a vegetarian   dish does not contain seafood . In addition , the   model often misclassiﬁes examples when entity at-   tributes are compared ( e.g. , the newer one ) . Multi-   modality covers around 25 % of the errors in the and domains , e.g. , annotators ref-   erenced visual aspects from music videos or recipes   ( e.g. , looks like shells ) , or an acoustic aspect from   a song ( e.g. , with the piano intro ormore upbeat ) .   The remaining errors are because of wrong anno-   tations , usually with the REs appling to both items .   This wrong annotation rate ( 23%-30 % ) is much   higher than the error rate in the whole dataset ( less   than7%as discussed in Section 4 ) since the model   has learned the task to a good extent .   We also analyzed correctly classiﬁed examples   ( for the domain ) to understand what typesof REs are classiﬁed correctly . The results are   shown in Appendix F.   7 Conclusion   We have revisited RE resolution with a new focus   on indirect expressions , introducing AltEntities ,   a new large dataset for this task – covering , , and examples . The dataset was   collected using a novel cartoon completion ap-   proach to encourage conversational and causal ex-   pressions while avoiding name or position expres-   sions . The experimental results show that in a re-   alistic setting , LMs adapted for this task achieve   82%-87 % accuracy . While an improvement on   existing approaches , this also encourages further   research on this important problem . Moreover , we   showed that the models ’ performance does not drop   when trained and tested on different domains , sug-   gesting that models can learn the semantic task well   and generalize to new domains .   It is notable that in practice , many entities do not   have textual descriptions or rich meta - data . Future   research could study resolving REs with minimal   information , e.g. , when we only have access to   their names or limited meta - data . Future research   could also use multi - modal input for training and   inference . Further , to handle more complex REs   such as the newer one , orthe happy song , one could   decompose a RE into simpler expressions and then   perform the comparison . Similar data collection   methodologies could be applied to collect a dataset   with more number of choices and also cases where   neither or multiple choices match the RE.123218 Limitations   As with any natural language understanding task ,   there are practical limitations and related ethical   aspects that must be considered before deploying   a system . In particular , our corpus and modeling   approach assume that the user - provided REs al-   ways refer to one of the two options . If this is not   the case , or if the RE is particularly contrived , un-   desirable or unexpected behavior may occur : For   any expression , including for instance one made   with arbitrary derisive language , the model would   attempt to resolve this to one of the alternative enti-   ties . One approach system designers may consider   could be to pre - classify any user - provided REs to   avoid interpreting those that are off topic or phrased   in a negative manner .   A second consideration is that of corpus repre-   sentativeness . In our case , as this is a ﬁrst corpus   for this task , we have limited ourselves to English   Wikipedia , native English speaking annotators , and   particular item sampling strategies for practical rea-   sons . However , if used for training a deployed   system , the examples present may bias any model   to understand speciﬁc types of references but not   others . Similarly , the items in our corpus are sufﬁ-   ciently popular to have a relatively long Wikipedia   entry , whereas items not present in Wikipedia , or   with only minimal information , may exhibit differ-   ent characteristics .   9 Ethics Statement   The data collection protocol was reviewed by an   ethics panel to remove potential ethical concerns . A   few ethical concerns were mentioned by the panel   which were then judged to be handled well . These   included ensuring that the entities , texts and REs   were free from biased and sensitive language . We   address this by ﬁltering using a list of sensitive   words ( see Section 3.4.1 and Table 12 ) . The panel   also recommended a diverse representation of en-   tities and domains . Thus our data comes from   diverse domains and the entities are sampled from   a large set of Wikipedia articles .   Still , we note that the limitations mentioned in   Section 8 need to be considered and addressed care-   fully when using our dataset or models for evalua-   tion or training of a deployed system . In addition ,   a biased corpus may lead to an evaluation that is   unaware of RE language forms used in other cul-   tures and languages , or that refer to other types of   items . We expect this consideration to be importantin practical settings .   References123221232312324A Opening Utterances   The ﬁrst annotation screen ( Figure 1 ) starts with   a manually written opening utterance . Table 11   shows all these utterances for the three domains ..   B Annotation Guidelines   In this section , we provide the domain - speciﬁc   guidelines that were shown to the annotators prior   to the start of their annotation . The guidelines   for each domain includes three instruction screens .   The second and third instruction screens are then   repeated for each alternative question as their ﬁrst   and second annotation screens , respectively ( the   two screen discussed in Section 4 ) .   In the ﬁrst instruction screen , a summary of the   task based on a cartoon completion setup is shown   to the annotators . Figure 4 shows the ﬁrst instruc-   tion screen for the domain . We do not   show the ﬁrst instruction screen for the other two   domains as they are very similar to the   domain except that the text is slightly different to   reﬂect the domain , and that the examples are from   those domains .   The second instruction screen provides further   information about the task and describes where   the annotators should acquire the knowledge to   perform the annotations . Figures 5 , and 7 , and 9   show the second instruction screens for the , , and domains , respectively .   The third instruction screen shows which item   should be referred to , and lists ﬁve examples of   appropriate REs . The REs cover different aspects   of the items to encourage the annotators to cover a   variety of the item aspects . It also lists a number   of actions that the annotators should or should not   do . Figures 6 , 8 , and 10 show the third instruc-   tion screen for the , , and   domains , respectively .   C Filtering Wikipedia Articles   Table 12 shows a number of ﬁlters we applied to   narrow down the extracted articles .   D Hyper - parameters Details and   Computing Infrastructure   We tune the hyper - parameters using a grid search   based on the accuracy of the indirect reference   resolution task on the development set of each   domain . For BERT and BERT multiple choice   models , we select the base learning rate from{1e−4,5e−5,3e−5,1e−5,5e−6,3e−6,1e−6 ,   5e−7 } , the training batch size from { 16,32,64 } ,   and the number of epochs from { 1,3,5,10 } .   For T5 , we select the base learning rate from   { 5e−7,1e−7,3e−6,5e−6,1e−5,3e−5,5e−5 ,   1e−4}and the training batch size from   { 16,32,64 } . We train the T5 models for   50 K steps ( batches ) .   Table 13 shows the selected hyper - parameters   for each model , domain , and input type .   We used Cloud TPU v2 accelerators for both   training and inference . In our experiments , each   training epoch took on average around 4 minutes   for BERT , 6 minutes for BERT Multiple Choice ,   and 15 to 25 minutes for T5 models .   E Development Set Results   We reported the test set results in multiple settings   in Section 6 . In this section , we report all those   results on the development sets .   Table 14 shows the development set results of   different models for all domains and input types .   We note that the general trends are very similar to   that of the test sets . On average , the results of differ-   ent models are slightly higher for the development   set compared to the test set ( up to 2.35 % ) . This is   expected as we have tuned the hyper - parameters   on the development sets .   F Analyzing Correctly Classiﬁed   Examples   We analyzed 100 correctly classiﬁed examples in   the domain and assigned one or more cat-   egories ( e.g. , date orgenre ) to each example . We   used the predictions of our T5 model with the U- B input . Table 15 shows the   results which cover a wide range of categories.123251232612327123281232912330123311233212333ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   8   /squareA2 . Did you discuss any potential risks of your work ?   Section 9 , as part of the Ethics Statement .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   3 , 4 , 5 , and 6   /squareB1 . Did you cite the creators of artifacts you used ?   3 , 4 , 5 , and 6   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   4   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 3.4.1 : " To remove any sensitive or offensive content , we also ﬁlter articles whose content   matches a list of sensitive words . " In addition , we did not ask the raters for any Personally Identiﬁable   Information .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Sections 3 and 4 .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Sections 4 and 6 .   C / squareDid you run computational experiments ?   6   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 6 and Appendix D.12334 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 6 and Appendix D.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   We did not observe meaningful differences when running the experiments multiple times in the   preliminary experiments . We therefore reported the results of only one run .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3 , 5 , and 6 . We cited LMs such as BERT and T5 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Sections 3 and 4 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 3 and Appendix A and B.   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   In section 4 , we mention : " We used a pool of around 60 in - house crowdworkers . They were all native   English speakers recruited from U.S. , U.K. , Canada , and Australia . "   This work was carried out by participants who are paid contractors . Those contractors received a   standard contracted wage , which complies with living wage laws in their country of employment . Due   to global privacy concerns , we can not include more details about our participants , e.g. , estimated   hourly wage or total amount spent on compensation .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   This was discussed with the annotators before data collection .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   9   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   412335