  Bhanu Pratap Singh Rawat , Samuel Kovaly , Wilfred R. Pigeon , Hong YuCICS , UMass - Amherst , University of Rochester , U.S. Department of Veterans Affairs , Center of Biomedical and Health Research in Data Sciencesbrawat@umass.edu,skovaly@umass.edu,wilfred_pigeon@urmc.rochester.edu,hong_yu@uml.edu   Abstract   Suicide is an important public health concern   and one of the leading causes of death world-   wide . Suicidal behaviors , including suicide   attempts ( SA ) and suicide ideations ( SI ) , are   leading risk factors for death by suicide . In-   formation related to patients ’ previous and cur-   rent SA and SI are frequently documented in   the electronic health record ( EHR ) notes . Ac-   curate detection of such documentation may   help improve surveillance and predictions of   patients ’ suicidal behaviors and alert medical   professionals for suicide prevention efforts . In   this study , we first built SuicideAttempt and   Ideatio nEvents ( ScAN ) dataset , a subset of the   publicly available MIMIC III dataset spanning   over12k+EHR notes with 19k+annotated   SA and SI events information . The annotations   also contain attributes such as method of sui-   cide attempt . We also provide a strong baseline   model ScANER ( SuicideAttempt and Ideatio n   Events Retreiver ) , a multi - task Ro BERT a-   based model with a retrieval module to extract   all the relevant suicidal behavioral evidences   from EHR notes of an hospital - stay and , and a   prediction module to identify the type of suici-   dal behavior ( SA and SI ) concluded during the   patient ’s stay at the hospital . ScANER achieved   a macro - weighted F1 - score of 0.83for identify-   ing suicidal behavioral evidences and a macro   F1 - score of 0.78and0.60for classification of   SA and SI for the patient ’s hospital - stay , re-   spectively . ScAN and ScANER are publicly   available .   1 Introduction   For decades , suicide has been one of the lead-   ing causes of death ( CBHSQ , 2020 ) . The suicide   rate in the United States increased from 10.5per   100,000in1999 to14.2in2018 , a35 % increase   ( Hedegaard et al . , 2020 ) . Globally , 740,000peopleFigure 1 : An example of positive andunsure evidence   annotations for SA in an EHR note .   commit suicide each year . The rates of suicidal be-   haviors , suicide attempt ( SA ) and suicide ideation   ( SI ) , are much higher ( WHO , 2021 ) .   A prior study shows that a large proportion of   suicide victims sought care well before their death   ( Kessler et al . , 2020 ) . Suicidal behaviors , including   SA and SI are recorded by clinicians in electronic   health records ( EHRs ) . This knowledge can in   turn help clinicians assess risk of suicide and make   prevention efforts ( Jensen et al . , 2012 ) . The diag-   nostic ICD codes include suicidality codes for both   SA and SI . However a study has shown that ICD   codes can only capture 3 % SI events , while 97 % of   SI events are described in notes ( Anderson et al . ,   2015 ) . In addition , of patients described with SA in   their EHR notes , only 19 % had the corresponding   ICD codes ( Anderson et al . , 2015 ) . Therefore , it   is important to develop natural language process-1029ing ( NLP ) approaches to capture such important   suicidality information .   Researchers have developed NLP approaches   to detect SA and SI from EHR notes ( Metzger   et al . , 2017 ; Downs et al . , 2017 ; Fernandes et al . ,   2018 ; Cusick et al . , 2021 ) . These studies either   used rule - based approaches ( Downs et al . , 2017 ;   Fernandes et al . , 2018 ; Cusick et al . , 2021 ) or built   the SA and SI identification models on a small   set ( Metzger et al . , 2017 ) or private set ( Bhat and   Goldman - Mellor , 2017 ; Tran et al . , 2013 ; Haerian   et al . , 2012 ) of EHR notes . It is also difficult to   compare the results of those studies as they varied   in EHR data , data curation , as well as NLP models ,   which were not made available to the public .   In this study , we present ScAN : SuicideAttempt   and Ideatio nEvents Dataset , a publicly available   EHR dataset that is a subset of the MIMIC III   data ( Johnson et al . , 2016 ) . ScAN contains 19,690   expert - annotated SA and SI events with their at-   tributes ( e.g. , methods for SA ) over 12,759EHR   notes . Specifically , experts annotated suicidality   evidence or sentences relevant to SA and SI events   during a patient ’s stay at the healthcare facility , an   example of SA annotations is shown in Fig 1 . The   evidences were put together to assess whether the   patient has an SA or SI event .   We also present ScANER ( SuicideAttempt and   Ideatio n Events Retriever ) , a Ro BERT a - based   NLP model that is built on a multi - task learning   framework for retrieving evidences from the EHRs   and then predicting a patient ’s SA or SI event using   the complete set of EHR notes from the hospital   stay using a multi - head attention model . We fo-   cus on the prediction of SA and SI using all the   EHR notes during a patient ’s stay because for the   whole duration , multiple EHR notes and note types   are generated , including admission notes , nursing   notes , and discharge summary notes . Suicidal infor-   mation are described in multiple notes throughout   the stay . For example , a patient was admitted to   the hospital with opioid overdose . It was docu-   mented initially in the admission note as an SA ,   but later dismissed as an accident after physician ’s   evaluation . In another example , an opioid over-   dose admission was first documented as an acci-   dent on admission , but later documented to be an   SA event after clinical assessment . Both ScAN   and ScANER capture SA and SI information at   the hospital - stay level . ScANER is able to retrieve   suicidal evidences from EHR notes with a macro - weighted F1 - score of 0.83and is able to predict   SA and SI with a macro F1 - score of 0.78and0.60 ,   respectively . Our annotation guidelines , ScAN , and   ScANER system will be made publicly available ,   making ScAN a benchmark EHR dataset for SA   and SI events detection . We will release the train-   ing and evaluations splits used in this study for   benchmarking new models .   2 Related Works   Efforts on detecting SA and SI within EHRs have   been explored in recent years . Most work used   rule - based or traditional machine learning - based   approaches . In one study , experts created hand-   crafted rules from mentions of suicidality ( both SA   and SI ) and then used the rules to identify suicidal-   ity as positive , negative , or unknown in a document   ( Downs et al . , 2017 ) . The rule - based approaches   are limited by their scalability . In another study ,   structured and unstructured EHRs were used to   classify at the hospital - stay level as SA , SI , or no   mention of suicidal behavior ( Metzger et al . , 2017 ) .   The training data consisted of only 112 SA , 49 SI   and 322 unrelated examples . In contrast , ScAN   comprises of 697 hospital - stays with more than   19,000 suicidal event examples over 12,759 clini-   cal notes . Only traditional machine learning mod-   els such as random forest ( Breiman , 2001 ) were   explored . In contrast , ScANER was built on the   state - of - the - art self - attention based model .   Hybrid approaches have also been developed to   identify SA at the hospital - stay level ( Fernandes   et al . , 2018 ) . In that study , a post - processing heuris-   tic rule - based filter ( e.g. , removing negated events )   was applied to the machine - learning - based classi-   fier ( a SVM ( Cortes and Vapnik , 1995 ) classifier )   to reduce false positives . Training and evaluation   were done also on relatively small datasets ( 500 for   training and 500 for evaluation ) .   Finally , weakly supervised approaches have   been developed to identify SI from EHRs ( Cu-   sick et al . , 2021 ) . In that study , authors used ICD   codes to identify 200 patients with SI and then ob-   tained EHR notes of those patients ( 6,588 ) . This   EHR note dataset was then used as the ‘ current ’   SI training data . The remaining 400 patients were   labelled as ‘ potential ’ SI and their 12,227EHRs   were also labelled the same . Authors used multi-   ple statistical machine learning models and one   deep learning model : convolutional neural net-   work . ( Bhat and Goldman - Mellor , 2017 ) also used1030feed - forward neural networks to predict suicide   attempts over 500k unique patients but the EHR   data for this study is not publicly available . ( Ji   et al . , 2020 ) surveyed multiple studies where the   researchers worked on private datasets ( Tran et al . ,   2013 ; Haerian et al . , 2012 ) for suicide attempt and   ideation prediction . Whereas in our study , in con-   trast to using the ICD codes which has considerable   errors , domain experts chart - reviewed a large , pub-   licly available set of EHRs for SI and SA , along   with their attributes ( e.g. , positive or negative SA ,   SI and the type of self - harm such as asphyxiation   and overdose ) .   3 Dataset   In this section , we introduce ScAN ( Suicide   Attempt and Ideatio nEvents Dataset ) and describe   it ’s data collection and annotation process . We   also discuss some examples from ScAN along with   basic dataset statistics .   3.1 Dataset collection   For annotation , we selected the notes from the   MIMIC - III ( Johnson et al . , 2016 ) dataset , which   consists of the de - identified EHR data of patients   admitted to the Beth Israel Deaconess Medical Cen-   ter in Boston , Massachusetts from 2001 to 2012   ( Johnson et al . , 2016 ) . The data includes notes ,   diagnostic codes , medical history , demographics ,   lab measurements among many other record types .   We chose MIMIC - III because it is publicly avail-   able under a data use agreement and allows clinical   studies to be easily reproduced and compared .   The diagnostic ICD codes for the patients are   provided at hospital - stay level in MIMIC with   admission identification numbers ( HADM_ID in   MIMIC database ) . We first filtered the hospital   stays that had ICD codes associated with suicide   and overdose . This resulted in 697hospital - stays   for669unique patients . For each stay , multiple   de - identified notes such as nursing notes , physi-   cian notes , and discharge summaries are available .   For the selected 697hospital - stays we extracted   a total of 12,759notes . Each medical note con-   tains multiple sections about a patient such as fam-   ily and medical history , assessment and plan , and   discharge instructions . We extracted different sec-   tions from these clinical notes using MedSpaCy ’s   clinical_sectionizer and filtered the rele-   vant sections from these clinical notes for annota - tion . The extensive list of these sections is provided   in Appendix A.   3.2 Annotation Process   The aim was to annotate all instances of SA and   SI documented in the medical notes as defined by   Center of Disease Control and Prevention ( CDC )   ( Hedegaard et al . , 2020 ) . The filtered 12,759notes   were annotated by a trained annotator under the   supervision of a senior physician . Each note con-   sisted of instances of SA , SI , both or none . The   senior physician randomly annotated 330 notes   and had a 100 % agreement with the annotator on   hospital - stay level annotation and 85 % agreement   on sentence - level annotations . After adjudication   between the senior physician and the annotator , the   disagreements were discussed and adjusted by the   annotator .   Suicide Attempt ( SA ): The annotator labelled   all the sentences with a mention of SA . Some hos-   pital stays could represent multiple types of SA ,   such as in Fig . 1 , where ‘ tried to hang himself ’ is   labelled as a positive SA and ‘ Tylenol overdose ’   is labelled as unsure since the overdose was never   confirmed as an SA event elsewhere in the medical   notes of the patient ’s hospital - stay . The label un-   sure is used when it is not clearly documented if a   self - harm was an SA event or not . The negative in-   stance , example shown in Fig . 2 , is a sentence that   confirms that the self - harm , an “ accidental over-   dose ” , responsible for the patient ’s hospital - stay is   not an SA event . In this work , we only focused on   suicidal self - harm and not non - suicidal self - harm   ( Crosby et al . , 2011 ) .   Further sub - categories are also provided for an   SA annotation in the form of the ICD label group :   a. ) T36 - T50 : Poisoning by drugs , medications1031and biological substances b. ) T51 - T65 : Toxic ef-   fects on non - medical substances c. ) T71 : Asphyx-   iation or suffocation and d. ) X71 - X83 : Drown-   ing , firearm , explosive material , jumping from a   high place , crashing motor vehicles , other specified   means .   Suicide Ideation ( SI ): SI is defined as any men-   tion and/or indication of wanting to take one ’s own   life or harm oneself . Similar to SA , any sentence   with a mention of SI was labelled within the pa-   tient ’s notes . A SI annotation could be labeled as   positive ornegative , an example for each label is   shown in Fig . 3 .   A sentence without SA or SI annotation would   be considered as a neutral - SA orneutral - SI sen-   tence respectively . Sentence level annotations pro-   vide more visibility to a medical expert for the   hospital - stay level annotation .   3.3 Dataset statistics   ScAN consists of 19,690unique evidence anno-   tations for the suicide relevant sections of 12,759   EHRs of 697patient hospital - stays . There are a to-   tal of 17,723annotations for SA events and 1,967   annotations for SI events . The distribution for both   SA and SI events is provided in Table 1 .   4 Methodology   In this section , we introduce ScANER ( Suicide   Attempt and Ideatio n Events Retreiver ): a strong   baseline model for our dataset . ScANER consists   of two sub - modules : ( 1 ) An evidence retriever mod-   ulethat extracts the evidences related to SA and   SI and ( 2 ) A predictor module that predicts SA   or SI label for the patient ’s hospital - stay using the   evidences extracted by the retriever module .   4.1 Evidence Retriever   Problem Formulation : Given an input clinical   note , the model extracts the evidences ( one or more   sentences ) related to SA or SI ( SA - SI ) from the   note . This is a binary classification problem where   given a text snippet the model predicts whether it   has an evidence for SA - SI or not . We learn this task   at paragraph level where the input is a set of 20con-   secutive sentences because the local surrounding   context provides additional important information   ( Yang et al . , 2021 ; Rawat et al . , 2019 ) . A paragraph   was labeled as evidence no , if all the sentences in   that paragraph are neutral - SA andneutral - SI . If   there was at least one SA - SI sentence , it was con-1032sidered an evidence yes . As the number of non-   evidence sentences significantly outsized the evi-   dence sentences , we decided to use an overlapping   window of 5sentences between the paragraphs to   build more evidence paragraphs . The distribution   of the paragraphs , across all evidence , SA and SI   labels for train , validation , and test set is provided   in Table 2 . We segregated the train and test set such   that any patient observed by the retriever module   during training was not seen in the test set . This   is important as there are patients who had multiple   hospital - stays in ScAN .   Proposed Model : Transformer ( Vaswani et al . ,   2017 ) based language models ( Devlin et al . , 2018 ;   Liu et al . , 2019 ) have shown great performance for   a broad range of NLP classification tasks . Hence ,   to extract the evidence paragraphs we trained a   Ro a ( Liu et al . , 2019 ) based model . It has   been previously shown that the domain - adapted   versions of the pre - trained language models , such   as clinical ( Alsentzer et al . , 2019 ) or Bio   ( Lee et al . , 2020 ) , work better than their base ver-   sions . So , we further pre - trained the Ro a-   base model over the MIMIC dataset to create a   clinical version of Ro a model , hereby refer-   enced as medRo a. During our initial explo-   ration , we experimented with clinical BERT and   BioBERT but found that medRo BERT a consis-   tently outperformed both models . medRo BERT a   achieved an overall F1 - score of 0.88whereas both   clinical BERT and Bio BERT achieved an overall   F1 - score of 0.85 . Our hospital - level SA and SI   predictor would work with any encoder - based evi-   dence retriever model .   Multi - task Learning : We trained medRo a   in a multi - task learning setting where along with   learning the evidence classification task , the model   also learns two auxiliary tasks : ( a. ) Identifying   the label for SA between positive , negative , un-   sure andneutral - SA and , ( b. ) Identifying the label   for SI between positive , negative andneutral - SI .   The training loss ( L(θ ) ) for our evidence retriever   model was formulated as :   L(θ ) = L+α∗L+β∗L ( 1 )   Where Lis the negative log likelihood loss   for evidence classification , LandLare SA   and SI prediction losses respectively , and αandβ   are the weights for the auxiliary tasks ’ losses . The   distribution of labels across all the three tasks is   highly skewed , hence , we applied the following   techniques to learn an efficient and robust model .   •Weighted log loss was used in both main task   and auxiliary tasks . The total loss for each task   was calculated as the weighted sum of loss   according to the label of the input paragraph .   Log weighing helps smooth the weights for   highly unbalanced classes . The weight for   each class was calculated using :   w=   1.0 if(w<1.0 )   log(γ∗N / N )   Where Nis the count of all training para-   graphs for the task tandNis the count of   paragraphs with label lfor the task tandw   is the calculated weight for those paragraphs .   We tuned γas a hyper - parameter . All train-   ing hyper - parameters for our best model are   provided in Appendix B.   •We also employed different sampling tech-   niques ( Youssef , 1999 ) , up and down sam-   pling , to help our model learn from an imbal-   anced dataset . After sweeping for different   sampling combinations as hyper - parameters ,   we found that down - sampling the no - evidence   paragraphs by 10 % resulted in the best perfor-   mance .   •The negative label of SA is severely under-   represented in ScAN making it difficult for   the model to learn useful patterns from such   instances , refer Table 2 . After discussion with   the experts , we decided to group the instances   ofnegative andunsure together and label them   asneg_unsure because for both groups the   general psych outcome is to let the patient   leave after the hospital - stay as there is no solid   evidence defining whether the self - harm was   a SA event.1033   4.2 Hospital - stay level SA and SI Predictor   Problem Formulation Given all the clinical   notes of a patient during the the hospital stay ,   the model predicts the label for SA ( positive ,   neg_unsure andneutral - SA ) and SI ( positive , neg-   ative andneutral - SI ) . The prediction module uses   the evidence paragraphs extracted by the retriever   module .   Robust Finetuning The retriever module is not   perfect and can extract false positives . This results   in extracting irrelevant paragraphs , with evidence   label No , along with evidence paragraphs for a   hospital - stay with SA or SI and extracting irrel-   evant paragraphs as evidences for a hospital - stay   with both SA and SI marked as neutral . To tackle   such situations and train a robust model , we applied   three techniques :   •For a hospital - stay with a non- neutral label for   SA or SI , during training we added some noise   in the form of irrelevant paragraphs ( a para-   graph with no SA or SI annotation ) from thenotes to the set of actual evidence paragraphs   for the input . An irrelevant paragraph from a   clinical note was sampled with a probability   of0.05 . This forced the predictor module to   learn effectively even with noisy inputs .   •For a neutral hospital - stay with no evidence   paragraphs , we randomly chose Xunique ir-   relevant paragraphs from the notes . Xwas   sampled from the distribution of number of ev-   idence paragraphs of the non- neutral hospital-   stays . This prevented the leaking of any infor-   mation to the prediction module during train-   ing by keeping the distribution of number of   input paragraphs the same across neutral and   non - neutral instances .   •Since these hospital - stays were extracted us-   ing the ICD codes related to suicide and over-   dose , the data is quite skewed with only 102   neutral events from a total of 697hospital-   stays . Whereas in a real - world scenario , neu-   tralhospital - stays would be much higher than   non - neutral ones . Hence , to facilitate a bal-1034anced learning of the predictor module we   introduced 1,800neutral hospital - stays from   the MIMIC dataset . The distribution for SA   and SI at hospital - stay level is provided in   Table 3 .   Proposed Model The paragraphs extracted using   theretriever module for a patient ’s hospital - stay   were provided as an input to the predictor module .   We used a multi - head attention model to predict the   SA and SI label for a hospital - stay as self - attention   based models have proved to be quite effective for a   lot of prediction tasks in machine learning ( Devlin   et al . , 2018 ; Cao et al . , 2020 ; Hoogi et al . , 2019 ) .   We encoded the extracted paragraphs   ( [ p , p .... p ] ) using the retriever module ,   medRo BERT a , to get a vector representation   of768 dimensions for each of the paragraphs   ( [ v , v ... v ] ) . Training the retriever module on   auxiliary tasks of predicting SA and SI helped   align these paragraph representations for SA and   SI prediction . Then , we added a prediction vector   ( v ) along with all the vector representations of the   paragraphs to get V= [ v , v , v ... v ] . We passed   Vthrough our multi - head attention model to get   the hidden representations H= [ h , h ... h ] . We   then passed hthrough a SA inference layer and   SI inference layer to predict the labels . During   the whole training process , the weights of the   retriever module were frozen whereas vwas   a learnable vector initialised as an embedding   in the multi - head attention model . We used a   separate vprediction vector so that it could   retain the information from all the other paragraph   representations for hospital - stay level prediction   similar to how [ CLS ] is utilized in different   transformer - based models for sequence prediction   ( Devlin et al . , 2018 ; Liu et al . , 2019 ) . We tuned the   number of layers and number of attention heads   of our prediction module as hyper - parameters and   achieved the best performance using a 2 - layer and   3 - attention head model . Our complete ScANER   model is illustrated in Fig 4 .   5 Results and Discussion   Since the labels for both the retriever and predic-   tion task are imbalanced , we used macro - weighted   precision , recall , and F1 - score to evaluate the over-   all performance of our models . Macro - weighted   metrics provide better model insights across all   labels . Evidence Retriever Performance Our multi-   task learning model achieved a F1 - score of 0.83   for extracting positive evidence paragraphs and   an F1 - score of 0.88overall . The retriever model   has higher recall than precision for the positive   evidence paragraphs ( 0.87>0.79 ) , SA ( 0.74 >   0.71 ) , and SI ( 0.62>0.46 ) events , as shown in   Table 4 . In healthcare , there is an incentive to maxi-   mize recall over precision ( Watson and McKinstry ,   2009 ) . As mentioned in § 4.2 , ScANER was trained   with added noisy paragraphs and is therefore robust   to the extracted evidence paragraphs if they contain   some false positives .   The retriever module achieves an overall F1-   score of 0.63for SA prediction and 0.64for SI   prediction at paragraph - level . The performance for   positive SA and SI evidence is much higher than   the performance for neg_unsure SA and negative   SI . We looked at the confusion matrices for SA   and SI paragraph - level prediction and found that   largely ScANER made mistakes between positive   andneg_unsure labels for SA prediction and be-   tween positive andnegative labels for SI prediction   ( refer Appendix C ) . The poor performance in SA   forneg_unsure evidence prediction is mainly due   to data sparsity where the neg_unsure cases are   only1743 ; in contrast , the positive cases are 4 - fold   higher . Similarly , for SI the positive cases are 1.4   times higher than the negative cases .   Hospital - stay level Prediction Performance   Our multi - head attention model is able to achieve   an overall macro F1 - score of 0.78for SA predic-   tion and 0.60for SI prediction , as shown in Table 5 .   For SA , the prediction module achieves a recall of   0.93for the positive label . After analysing the con-   fusion matrix , the model largely predicts a positive   label for the visits with neg_unsure label , as shown   in Table 6 . The poor performance for neg_unsure   is largely because of its small representation in the   training set of ScAN , 54negative cases as com-   pared to 377positive and 1,381neutral instances .   In our future work , we plan to expand ScAN with   more instances of negative SA events .   For SI , the prediction module achieves an overall   F1 - score of 0.60with a precision of 0.63and recall   of0.66 . The model has a high recall for neutral-   SIandpositive but the positive label has a low   precision of 0.49 . After analysing the test set , we   observed that a lot of patient hospital - stays with   negative labels are getting wrongly predicted as   positive , as shown in Table 6 . After doing error1035   analysis for hospital - stays with negative labels , we   observed that a lot of extracted evidence paragraphs   contain information that suggests that the patient   had SI before the SA but does not have SI anymore   during the hospital - stay . As shown in the example   in Fig 5 , the past SI is an explanation for the SA   but then the patient does not have any further SIduring the hospital - stay . This suggests that period   assertions for these annotations are quite important   and we aim to add period assertion property in our   future work by further annotating ScAN .   6 Conclusion   In this paper , we introduce ScAN : a publicly avail-   able suicide attempt ( SA ) and ideation ( SI ) events   dataset that consists of 12,759EHR notes with   19,960unique evidence annotations for suicidal   behavior . To our knowledge , this is the largest and   publicly available dataset for SA and SI , an impor-   tant resource for suicidal behaviors research . We   also provide a strong Ro a baseline model for   the dataset : ScANER ( SA and SI retriever ) which   consists of two sub - modules : ( a. ) an evidence re-   triever module that extracts all the relevant evi-   dence paragraphs from the patient ’s notes and ( b. )   aprediction module that evaluates the extracted ev-   idence paragraphs and predicts the SA and SI event   label for the patient ’s stay at the hospital . ScAN   and ScANER could help extract suicidal behavior1036 in patients for suicide surveillance and predictions ,   leading to potentially early intervention and pre-   vention efforts by medical professionals .   References10371038A Selected Clinical Sections   The sections selected for annotations after using clinical_sectionizer are enumerated below :   1 . Allergy   2 . Case Management   3 . Consult   4 . Discharge Summary   5 . Family history   6 . General   7 . HIV Screening   8 . Labs and Studies   9 . Medication   10 . Nursing   11 . Nursing / other   12 . Nutrition   13 . Observation and Plan   14 . Past Medical History   15 . Patient Instructions   16 . Physical Exam   17 . Physician   18 . Present Illness   19 . Problem List   20 . Radiology   21 . Rehab Services   22 . Respiratory   23 . Sexual and Social History   24 . Social Work   B Hyper - parameter Settings   All the hyper - parameter settings for both modules of ScANER are provided in Table 7.1039   C Confusion matrices   The confusion matrices for SA and SI prediction at paragraph level is provided in Table 8.1040