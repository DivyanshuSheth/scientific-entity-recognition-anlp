  Kyle RichardsonRonen TamariOren Sultan   Reut TsarfatyDafna ShahafAshish SabharwalThe Hebrew University of JerusalemAllen Institute for AIBar - Ilan University   Abstract   Can we teach natural language understanding   models to track their beliefs through intermedi-   ate points in text ? We propose a representation   learning framework called breakpoint modeling   that allows for learning of this type . Given any   text encoder and data marked with intermediate   states ( breakpoints ) along with corresponding   textual queries viewed as true / false proposi-   tions ( i.e. , the candidate beliefs of a model , con-   sisting of information changing through time )   our approach trains models in an efficient and   end - to - end fashion to build intermediate rep-   resentations that facilitate teaching and direct   querying of beliefs at arbitrary points alongside   solving other end tasks . To show the benefit   of our approach , we experiment with a diverse   set of NLU tasks including relational reasoning   on CLUTRR and narrative understanding on   bAbI. Using novel belief prediction tasks for   both tasks , we show the benefit of our main   breakpoint transformer , based on T5 , over con-   ventional representation learning approaches   in terms of processing efficiency , prediction   accuracy and prediction consistency , all with   minimal to no effect on corresponding QA end-   tasks . To show the feasibility of incorporating   our belief tracker into more complex reasoning   pipelines , we also obtain SOTA performance   on the three - tiered reasoning challenge for the   TRIP benchmark ( around 23 - 32 % absolute im-   provement on Tasks 2 - 3 ) .   1 Introduction   Despite considerable progress made recently in nat-   ural language understanding ( NLU ) , driven largely   by advances in language model pre - training ( De-   vlin et al . , 2019 ; Raffel et al . , 2020 ) and the de-   velopment of large - scale NLU benchmarks ( Wang   et al . , 2018 ) , understanding the behavior of mod-   els remains a formidable and highly consequentialFigure 1 : Deep narrative understanding in natural lan-   guage ( bottom ) involves the ability to answer queries   about arbitrary intermediate points in a given story . We   liken this task to breakpoints in programming ( top ) , or   reporting the state of a program at different stages of ex-   ecution , facilitating human inspection of model beliefs   and consistency with end - task behavior ( bottom ) .   challenge for model safety . Such a challenge is par-   ticularly acute in tasks such as narrative understand-   ing , where one must piece together many individual   ( possibly implicit ) facts through time in order to   solve problems . For example , in the story in Fig-   ure 1 , answering the question Where is the apple ?   requires knowing how to track objects through time   ( e.g. , knowing the location of the John andMary   and their interaction ) and how to compartmentalize   other types of knowledge across the story . In such   a setting , where models are trained to narrowly an-   swer questions , a natural question arises : do models   acquire the kind of requisite background knowledge   and world tracking abilities , and ultimately learn   representations that give rise to correct beliefs   about intermediate states ?   A chief difficulty in answering such questions is9703   that directly inspecting the propositional attitudes   of our current models remains a formidable chal-   lenge due to the latent nature of their knowledge .   Such a complication also makes it unclear what the   right interface should be for eliciting beliefs in the   first place ( e.g. , how can we determine if a model   believes a proposition John is in the kitchen at an   arbitrary point in text ? ) . In addition , for tasks such   as QA , story contexts and questions are usually en-   coded jointly ( often with full attention over context   and query ) , which makes it difficult to tease apart   a model ’s understanding of a story independent of   each question . Entangled story and question repre-   sentations can be inefficient when scaling to a large   space of questions , particularly for novel combina-   tions of questions and stories ( Tamari et al . , 2022 ) .   Such entangled representations also allow models   to exploit spurious patterns in questions that in-   flate performance ( Kaushik and Lipton , 2018 ) and   hinder interpretability .   We present a model - agnostic representation   learning framework called breakpoint modeling   that facilitates teaching models to have proposi-   tional beliefs at arbitrary points in stories ( or break-   points ) using ordinary textual queries as our inter-   face language . Our general modeling approach is   illustrated in Figure 2 . Given any task - specific en-   coder and data marked with the intermediate state   of interest ( or breakpoints , denoted throughout as   [ B ] ) along with a set of textual queries ( i.e. , the   candidate beliefs provided in training as auxiliaryintermediate supervision ) , models are trained in   an end - to - end fashion to learn intermediate task-   specific representations ( pooled from single encod-   ings of stories ) that jointly facilitate making correct   and consistent belief predictions efficiently across   a large space of queries . Making an analogy with   breakpoints in programming ( see top of Figure 1 ) ,   we aim to simulate stopping execution at interme-   diate points during a story to inspect the model ’s   belief state ( e.g. , checking that a model ’s answers   for QA are consistent with their beliefs and sat-   isfy certain high - level constraints ) , as well as teach   the model to have certain beliefs learned through   intermediate supervision at training time .   Using a state - of - the - art pretrained model , T5   ( Raffel et al . , 2020 ) , we develop and investigate   abreakpoint transformer to do belief prediction   on three categories of tasks : narrative understand-   ingon bAbI ( Weston et al . , 2016 ; Tamari et al . ,   2022 ) , relational reasoning on CLUTRR ( Sinha   et al . , 2019 ) and physical commonsense reason-   ingover human authored stories on TRIP ( Storks   et al . , 2021 ) . In the former two cases , we focus   on training and evaluating models on a novel be-   lief prediction task . We report improvements over   a conventional transformer - based representation   learning approach ( Reimers and Gurevych , 2019 )   both in terms of prediction accuracy ( 4 % to 8 %   absolute improvement on CLUTRR dev ) and belief   consistency , all with significantly improved pro-   cessing efficiency ( i.e. , minimal forward calls to   the full transformer ) and minimal effect on end-   task performance when jointly trained with QA . In   the latter case for TRIP , we show how to integrate   our modeling approach into a more complex trans-   former pipeline and report state - of - the - art results   on the three - tiered reasoning task ( with 23 - 32 % ab-   solute improvement on two component tasks ) over   existing task - specific architectures .   Taken together , our results show the viability   of building an end - to - end trainable belief track-   ing mechanism and integrating it within exist-   ing transformer - based reasoning systems . To our   knowledge , our work is among the first to look at at   general - purpose sentence representation learning   for intermediate states in text as a way to facilitate   complex situation reasoning .   2 Related Work   Our work brings together two recent areas that aim   to understand model behavior ( broadly model prob-9704   ing ): probing of the type that includes finding neu-   ral correlates of high - level behavioral phenomena ,   modular structure in networks ( Tenney et al . , 2019 ;   Hewitt and Manning , 2019 ) on the one hand , as   well as diagnostic testing , which aims to understand   model competence through controlled input - output   testing ( Lake and Baroni , 2018 ; Richardson et al . ,   2020 ) , or post - hoc consistency analysis ( Kassner   et al . , 2021 ) . Our work is more closely related   to Li et al . ( 2021 ) , who show that partial world   state information can be decoded from NLMs even   without explicit supervision . In that work , state   information is roughly localized to entity mentions ,   but varies across different datasets . Differently   from such probing work , our breakpoint models are   trained in a supervised manner to localize particu-   lar propositional information at particular locations   ( similar to Geiger et al . ( 2021 ) ) .   Our breakpoint model closely relates to late-   interaction encoder architectures that tease apart   the encoding of problems and solutions . This in-   cludes the sentence transformer from Reimers and   Gurevych ( 2019 ) , which we compare against in   our experiments , as well as read once transformers   ( Lin et al . , 2021 ) , colBERT ( Khattab and Zaharia ,   2020 ) and others . Given that the types of narrative   tasks we focus on require modeling many inter-   mediate points , we follow this work in putting an   emphasis on representation and encoding efficiency .   In contrast to this , and other related work on sen-   tence representation learning ( Gao et al . , 2021 ; Ni   et al . , 2022 ) , we uniquely focus on learning repre-   sentations of intermediate states in text for complex   situational reasoning .   We are also inspired by the situation modeling   literature in cognitive science ( Golden and Rumel-   hart , 1993 ; Frank et al . , 2003 ; Venhuizen et al . ,   2019 ) , and proposals for their integration with NLP   research ( Tamari et al . , 2020 ) . These works also   studied neural models of narrative comprehension   in carefully controlled micro - worlds , but typicallyfocused on relatively short sentence - level inputs .   Our work also relates to efforts on building inter-   pretable models by making the underlying reason-   ing processes transparent , either through explicit   decomposition ( Andreas et al . , 2016 ; Khot et al . ,   2021 ; Bostrom et al . , 2022 ) or generation of ratio-   nales ( Camburu et al . , 2018 ; Wiegreffe and Maraso-   vic , 2021 ) and other reasoning structures ( Tafjord   et al . , 2021 ; Dalvi et al . , 2021 ; Gontier et al . , 2020 ) .   In contrast , we focus on belief representations that   are ultimately faithful ( Jacovi and Goldberg , 2020 )   to end - tasks by training knowledge directly into a   model ’s task - specific representations .   3 Breakpoint Modeling   The goal of breakpoint modeling is to capture the   intermediate states and beliefs of models at arbi-   trary positions in text . Our models take stories as   inputs , or pieces of text containing one or more in-   termediate positions ( breakpoints ) , as well as sets   of text propositions that align to certain intermedi-   ate points ( see Figure 3 ) . Such propositions play   the role of auxiliary supervision if provided at train-   ing time or as queries to the model for performing   probing ; when coupled with predictions they con-   stitute the beliefs of the model .   While breakpoint models can technically take   different forms , their basic function is to assign   encodings to intermediate states in text and their   corresponding propositions ( § 3.1 ) and to make pre-   dictions about the truth / falsity of each proposition   ( § 3.2 ) . Learning ( § 3.3 ) reduces to the problem of   teaching a model to have a correct and consistent   set of beliefs for each target task given a set of rep-   resentative intermediate propositions and beliefs   provided at training time ( § 3.4 ) .   3.1 Breakpoint and Proposition Encoding   As illustrated in Figure 3 , stories are texts   consisting of ntokens within which there can   exist m≥1arbitrarily selected intermedi-9705   ate points or breakpoints . For convenience ,   we will render a story sin the following way :   s:=w . . . w[B ] . . . w. . .[B ] . . . w[B ]   where [ B]is a special token used to explicitly   mark position of each breakpoint b. Intuitively , a   breakpoint token represents all of the information   in the story relevant to building an accurate belief   state at the corresponding ( intermediate ) point   in the text . Associated with each bis a set of   text propositions P={p , p , ... , p } . Truth   assignments to these text propositions constitute   the candidate beliefs at breakpoint b(in the sense   of Footnote 2 ) .   At the core of any breakpoint model are two   encoders , enc , enc , that are used to gener-   ate a representation or embedding for each break-   point in the story and each proposition , respec-   tively . Representations of breakpoints b∈Rare   pooled from a single encoding of an input story   s : c←enc(s)∈Rand representations   for propositions c∈Rare obtained in a sim-   ilar fashion using enc . While the choice of   the encoder and the details of how pooling is done   can vary ( see details in § 5.1 ) , in all of our models   breakpoint representations bare obtained by taking   projections of the hidden states of the [ B]tokens   fromc . We also investigate models that assume a   siamese architecture ( Reimers and Gurevych , 2019 )   where encandencare the same encoder .   An important property of breakpoint models   is that all breakpoints representations bare ob-   tained from a single read encoding of each target   story . We later compare this against a much less   efficient approach that requires multiple forward   passes through the story to obtain intermediate en-   codings ( i.e. , the multi - pass approach shown in   Figure 4 ) . Our model therefore stays within the   spirit of a late - interaction architecture ( Khattab   and Zaharia , 2020 ) by using separate encodings ofbreakpoints and propositions , which allows us to   scale to large sets of propositional queries .   3.2 Proposition Scoring and Semantics   Given a breakpoint encoding band an aligned   proposition encoding c , aproposition scorer   makes a prediction about a proposition at that   breakpoint . As mentioned , our aim is to pre-   dict the truth value of a proposition at an inter-   mediate state , which we take to be the model ’s   belief in that proposition . Our scorer takes   the form of a classifier that maps a breakpoint   encoding and proposition encoding to the dis-   crete space { true , false , unknown } , following   Li et al . ( 2021 ) and the annotation scheme from   NLI ( Dagan et al . , 2005 ; Bowman et al . , 2015 ) .   To make clear that the interpretation of each   proposition is tied to a specific breakpoint , we will   use the symbolic notation from Li et al . ( 2019 ) and   introduce three binary logical predicates E , C , and   U. For each bandp∈P , these predicates cap-   ture whether pisentailed by , is contradicted by ,   or has an unknown relation to the information in   the text at breakpoint b , respectively . For instance ,   E(b , p)is true if the text proposition pis entailed   by the story at breakpoint b.   3.3 Learning   Suppose we have a dataset Dconsisting of n   stories { s}along with the following addi-   tional information . For each story s , we have   mbreakpoints B.For each such breakpoint   b , we have tlabeled text propositionsP ,   where each proposition p∈Pis labeled with   y∈ { true , false , unknown } indicating p ’s   truth value at breakpoint b. Using the above pred-   icate logic notation , we can equivalently think of   having , for each p∈P , exactly one predicate   Y∈ { E , C , U}annotated in D , with the se-   mantics that Y(b , p)is True ( and the other two   predicates for bandpare False ) .   The goal here is to learn a model that assigns   truth values to all text propositions across all   breakpoints — equivalently , truth values for all three   logical predicates — in a way that maximally aligns   withD. Semantically , this can be expressed as9706satisfying the logical formula ( Li et al . , 2019 ):   /logicalanddisplay / logicalanddisplay / logicalanddisplayY(b , p ) ( 1 )   with the added constraint that for each story s   and all j , k , exactly one of E(b , p),C(b , p ) ,   andU(b , p)is True .   Using Pr[y]to denote the model ’s probability   corresponding to the predicate Y(b , p ) , this   formula can be translated into the following loss   using the product translation from Li et al . ( 2019 ):   L=/summationdisplay / summationdisplay / summationdisplay−log Pr [ y ] ( 2 )   which yields the common cross - entropy loss that   we use in our experiments .   3.4 Proposition Sampling   Propositions in breakpoint models have a dual role :   when given at training time , they provide interme-   diate supervision for training models across dif-   ferent situation states . When given at inference   time they allow for post - hoc probing of a model ’s   beliefs . As shown in the Figure 3 , propositions ,   in virtue of being ordinary text , can express many   different types of information and thus provide an   unbounded source of semantic supervision ( Hanjie   et al . , 2022 ) , e.g. , for expressing fluents , or condi-   tions that change through time in a story ( e.g. , John   is in the kitchen , or event pre / post - conditions ( e.g. ,   The radio was powered via English tense ) .   For training models to have beliefs , a necessary   first step is to devise a sampling policy for gener-   ating these intermediate annotations . While such   a strategy needs to be tailored to each target task ,   we experiment with a combination of extracting   propositions from existing task annotations ( Fig-   ure 5 ) and generating propositions based on a set   ofdomain constraints using the semantics of each   target domain ( details in the next section ) .   4 Proposition Prediction Tasks   We focus on three categories of tasks : text - based   relational reasoning , story understanding and   commonsense reasoning , each considered in turn .   In the former two cases , we devise new proposi-   tion and belief prediction tasks that involve training   on intermediate belief state annotations . We also   include out - of - distribution ( o.o.d ) generalization   tests beyond standard i.i.d ( independent and iden-   tically distributed ) evaluation . In the latter case ,   we recast an existing task in terms of breakpoint   models to show the versatility of our approach in a   more complex multi - task setting .   4.1 Relational Reasoning   CLUTRR Sinha et al . ( 2019 ) focuses on QA over   synthetic stories about family relations as shown   in Figure 3 , and has more recently been extended   to focus on proof generation ( Gontier et al . , 2020 ) .   As illustrated in Figure 5 , we use the proof anno-   tations in the latter work to generate intermediate   propositions that track the time - course of family   relations as they emerge at each new sentence .   Relying on the clean subset of CLUTRR stories   Sinha et al . ( 2019 ) and proof annotations , break-   points are added after each sentence . Propositional   renderings of the explicit story facts , as well as   intermediate propositions revealed in the proof an-   notations , were then added to each correspond-   ing breakpoint in the story and serve as the base   proposition set . From these base propositions , ad-   ditional propositions , including negative and un-   known propositions , were added using the follow-   ing general constraints : monotonicity , that beliefs ,   once established to be true /false , can not change ;   themutually exclusivity of certain relations ( e.g. ,   X is the grandfather of Y is mutually exclusive with   X is the grandmother of Y ) ; inverse relations be-   tween certain relations ( e.g. , thatXis a sister   ofYmeans that Yis a sister of X ) , and that all   non - deductively valid propositions are unknown   ( i.e. , with label U).Such ground propositions con-9707straints are included in the breakpoint annotations   as symbolic expressions ( see again Figure 5 ) to al-   low for measuring model consistency at inference   time ( later in Figure 6 . See details in § A.3 .   o.o.d evaluation . Stories in CLUTRR are charac-   terized by their length k(number of events ) and   generalization testing is usually performed to mea-   sure generalization . Our main datasets ( later seen   in Table 1 ) consists of 13k training stories drawn   from stories from k=2, .. ,5 . We tune our mod-   els and evaluate on a mixture of in - domain and   generalization stories of lengths k=2, .. 8 each con-   taining around 1.5k stories ( containing ( avg ) 10   propositions per breakpoint and 15 constraints per   story ) . While these splits deviate from standard   uses of CLUTRR , we also compare against stan-   dard splits ( i.e. , training on k= 2,3and testing on   k= 2 , .. 10 ) to look at the ability of training joint   belief prediction and QA models on the original   QA task .   4.2 Story Understanding   We experiment with the bAbI QA benchmark ( We-   ston et al . , 2016 ) , which contains questions over   stories about agents in controlled micro - worlds ( see   Figure 3 ) . As with CLUTRR , the synthetic nature   of domain makes it possible to automatically ex-   tract proposition annotations that express object   location ( e.g. , PersonX / ObjectX ’ is in Y ) , object   possession ( PersonX has ObjectY ) , abstractions   ofevent post - conditions ( e.g. , PersonX took some-   thing for the event PersonX grabbed the ball ) and   pronoun references ( e.g. , He refers to John ) . We   use the Dyna - bAbI task generator ( Tamari et al . ,   2022 ) to generate initial base propositions and , sim-   ilar with CLUTRR , heuristically add more proposi-   tions using domain constraints ( see § A.2 for more   details).We use propositional versions of the 7-   task set introduced in Tamari et al . ( 2022 ) . We   specifically use the long - form version of this set ,   where stories all contain 20 events / breakpoints , and   train on 500 examples per task ( totaling 3.5k+1.4k   training / evaluation stories , with an average of 10   propositions per breakpoint and 123 constraints per   story ) .   o.o.d evaluation . In addition to training and test-   ing on this set , we also look at joint training on   proposition prediction and the original QA task .   For evaluation we also consider a more challeng - inghardQA generalization task from Tamari et al .   ( 2022 ) , where the test set features compositions   of concepts seen at training time . Appendix A.2   contains example inputs and further task details .   4.3 Physical Commonsense Reasoning   We apply our approach to the recently introduced   Tiered Reasoning for Intuitive Physics ( TRIP )   dataset ( Storks et al . , 2021 ) . TRIP features a story   plausibility end task , similar in scope to our propo-   sition task , as well as a multi - tiered evaluation of   models ’ reasoning process . Given a pair of highly   similar human - authored short stories about every-   day activities , models must jointly identify ( 1 ) the   implausible story ( task1 ) ( 2 ) a pair of conflicting   sentences in the implausible story ( task2 ) ( 3 ) the   underlying physical states in those sentences caus-   ing the conflict ( task 3 ) . While task3 takes the   form of a breakpoint modeling task , where physi-   cal states are rendered as textual propositions , we   model the first two tasks as text2text tasks using   multi - task breakpoint models ( details in the ap-   pendix and in § 5.1 ) . We use the original splits ,   consisting of 675 plausible stories and 1472 implau-   sible stories . While we focus on the multi - tiered   evaluation , we devised a small filtered dev set ( 644   stories ) for later model analysis ( Table 5 ) .   5 Modeling Details and Metrics   Here we detail our main breakpoint transformer   ( § 5.1 ) following the framework in § 3 and all met-   rics used in our experiments ( § 5.2 ) .   5.1 Modeling   Encoder We experimented with the T5 model ( Raf-   fel et al . , 2020 ) using the implementation from   Wolf et al . ( 2020 ) . T5 ’s bi - directional encoder   was used for both our story encoder encand   proposition encoder enc . While any compa-   rable encoder would suffice , we chose T5 due its   common use in NLU and ability to perform gen-   eration , which we used to implement other com-   ponents in the multi - task models discussed below .   For efficiency reasons , we experimented with a   combination of the smaller T5 - base model ( with   220 M parameters ) for datasets with long stories   and many propositions ( TRIP , bAbI ) and T5 - large   ( with 770 M parameters ) for CLUTRR .   Breakpoint and Proposition Embeddings For   each story , individual breakpoint representations   are first pooled from the [ B]token hidden states in9708the story encodings c(see again Figure 4 ) . Fol-   lowing Ni et al . ( 2022 ) , a linear projection and L2   normalization is applied to each representation to   construct initial breakpoint embeddings . To allow   for information transfer between different break-   points , we then apply an additional self - attention   layer ( sit - self ) over these resulting representations   to obtain a self - attention breakpoint representation   ( see Fan et al . ( 2020 ) for a similar idea ) , which gets   concatenated with the initial representation to cre-   ate the final breakpoint embedding . Operationally ,   the self - attention layer takes the form of a standard   transformer block ( Vaswani et al . , 2017 ) with a   single attention head .   One subtlety in using a standard bi - directional   encoder such as T5 is that each breakpoint token   can look at future parts of the story . While the   content of a breakpoint is often determined by the   preceding sentence , in some cases it is important to   have information about the future to obtain an accu-   rate representation . For example , for the story John   has the apple . [ B]He then moved to the kitchen   [ B ] , knowing that John ca n’t be in the kitchen at   [ B](apre - condition ofmove events ) requires look-   ing into the future . To limit the amount of future   information in part of our breakpoint representa-   tions , however , future masking is applied in the   breakpoint self - attention layer described above .   To obtain a proposition embedding , we use the   same T5 encoder over each text proposition pre-   fixed with a special token , then take the hidden   state of the target proposition . A final proposition   representation is then similarly obtained using the   same linear projection and normalization layers .   Proposition Classifier As in Li et al . ( 2021 ) ,   we use a bilinear layer for proposition   classification ( score ( · ) ) . Using the nota-   tion from § 3.3 , probabilities ˆ y(b , p ) =   ⟨Pr[E(b , p)],Pr[C(b , p)],Pr[U(b , p)]⟩for the   3 truth values of a proposition pare computed   in the following way using the final breakpoint   representation band proposition encoding c :   score ( b , p ) = b·M·c+a   ˆ y(b , p ) = softmax ( score ( b , p ) ) .   Learning In addition to optimizing for the objec-   tive described in § 3.3 ( L ) , we also experiment   with multi - task models trained to do generation   ( L ) and QA ( L ) , both of which are formu-   lated as text2text tasks and optimized using stan-   dard cross - entropy - based training . In the formercase , we investigate two analogues to the unsu-   pervised denoising objectives from ( Raffel et al . ,   2020 ) , which aim to increase the amount of local in-   formation contained in breakpoint representations .   The first is an event generation task that in-   volves generating randomly chosen events from   their right - most breakpoint encodings ( e.g. , gen-   erating the text Susan ’s mother is Janice from the   encoding of [ B]in Figure 3 ) . The second , which is   inspired by Gontier et al . ( 2022 ) , generates textual   abstractions either of random events from break-   points ( in the case of TRIP , e.g. , generating the   abstracted text PERSON dropped his OBJ ... from   [ B]in Figure 3 ) or random pairs of events in a   story ( e.g. , generating the text A person received   an apple from the an encoding averaged from the   two breakpoints [ B]and[B]in Figure 3 ) ( see   additional details in § B.2 ) .   Taken together , our full multi - task model ’s loss   is : L = λL+λL+λLwhere λ   are task weights manually tuned during training .   We used ADAM as our optimizer ( Kingma and   Ba , 2014 ) . Standardly , hyper - parameter tuning and   model selection was performed via a random search   search in the style of Devlin et al . ( 2019 ) on held-   out dev sets ( see details in § B.1 ) . Unless stated   otherwise , we report the average of three random   restarts for all models and their standard deviations .   Baselines We compare against two standard sen-   tence representation learning approaches based on   transformers and LSTMs . For the former we use   the sentence transformer approach ( Reimers and   Gurevych , 2019 ) applied to our task , and for the lat-   ter we use a model close to Conneau et al . ( 2017 ) .   The set up is standard : stories and propositions   are encoded separately using a single encoder and   collected via mean ( transformer ) and max ( BIL-   STM ) pooling then aggregated via concatenation   ( in the style of InferSent Conneau et al . ( 2017 ) )   and fed into a softmax classifier to make a belief   prediction . Importantly , these baselines models are   much less efficient compared with our single read   breakpoint model , in that they require making mul-   tiple ( multi - pass late interaction ) forward passes   through stories to create intermediate representa-   tions as illustrated in Figure 4 . For the transformer   models , with use the same T5 encoder as in the   breakpoint models throughout all experiments.9709Given that our breakpoint models take full story   texts as input , to make the baselines fully compa-   rable , we similarly feed in the full story on each   read with a similar special token ( # ) to mark the   target intermediate point ( e.g. , In the story John   went to the store . He bought an apple we feed the   textJohn went to the store . # He bought an apple   when modeling the first breakpoint ) .   Joint Modeling For CLUTRR and bAbI , we also   compare our multi - task breakpoint model trained   for QA against T5 and Bart ( Lewis et al . , 2020 ) ,   both fine - tuned solely for QA .   5.2 Metrics   For proposition prediction tasks we measure over-   allproposition accuracy ( % ) . Similarly for QA   experiments , we follow other work in measuring   exact match EM accuracy ( % ) against a model ’s   generated output . For some of our analysis on   CLUTRR ( Figure 5 ) , we measure the consistency   of belief prediction using the global consistency   metric ρfrom Li et al . ( 2019 ) , which measures the   fraction of stories containing one or more constraint   violation using the constraint annotations described   in § 4 . For example , using the constraint on the   bottom Figure 5 , we first have the model make   predictions about the constituent propositions ( 1 .   Derick is the father of Lisa , 2.Qiana is the wife   of Derick . 3 . etc .. ) and see if those predictions   symbolically satisfy the constraint .   For TRIP , we follow exactly the 3 - tiered evalua-   tion of Storks et al . ( 2021 ) . We calculate : Plausibil-   ity ( task 1 ): % of instances where the implausible   story was correctly identified . Consistency ( task   2):% of correctly identified implausible stories   where the conflicting sentences were correctly iden-   tified . Verifiability ( task 3 ) : % of instances with   correct plausibility / consistency predictions , where   all relevant physical states are also identified .   6 Results and Discussion   We focus on the following questions : 1 . Can our   main model effectively and efficiently solve our   new belief proposition prediction tasks ( introduced   in § 4 ) and model intermediate state ? 2.Can we   effectively integrate our breakpoint model into joint   models for solving more complex tasks ?   Proposition Prediction We found breakpoint mod-   els to be effective at our proposition prediction   tasks , most notably improving on the transformer   Multi - pass baselines for CLUTRR prediction from   81.9 to 85.2 ( top of Table 1 , both an over 23 % im-   provement over our BILSTM baseline , suggesting   task difficulty ) . Based on the plots in Figure 6 , we   also found our models to be more efficient learners   ( e.g. , achieving comparable performance to base-   lines using only 60 % training data ) and to exhibit   less global constraint violations in the i.i.d setting   ( with a 6 % reduction in constraint violations ρ ) ,   thus leading to more consistent belief states .   For bAbI ( Table 2 ) all transformer - based mod-   els achieve near perfect accuracy ( and significantly   outperform our BILSTM model ) ; as such , mod-   els have near perfect consistency on the underly-   ing constraints ( not shown ) . Given that bAbI sto-   ries are considerably longer than CLUTRR stories9710(each containing 20 events / breakpoints ) , these re-   sults show the feasibility of modeling long contexts   with our model and representing complex state in-   formation with individual breakpoints . In contrast   to the baseline transformers , here we also see con-   siderable practical improvements in training time   efficiency due to our single read architecture , re-   sulting in a 54 % reduction in training time ( from   around 63 hours for multi - pass models to around   34 for ours on a single RTX A6000 GPU ) .   Our model ’s proposition prediction consistency   is 7.5 % higher than that of the baseline , in terms   of the ρmetric reported in Table 3 . As an impor-   tant caveat , however , in absolute terms , even our   breakpoint model has much lower consistency on   generalizations tasks ( 69.2 % ) than in the i.i.d . set-   ting ( 95.7 % ) . We discuss this further in § 8 .   Joint Training When trained jointly for both propo-   sition prediction and QA , we found minimal to no   impact on end - task performance , as shown on the   bottom of Table 1 for CLUTRR and in Table 2 for   bAbI ( with a small improvement on the generaliza-   tion QA task at the cost of a mere 2 % degradation   in i.i.d . QA performance ) . This shows the viability   of integrating our belief tracking mechanism into   existing transformer pipelines without significant   performance drops . As first motivated in Figure 1 ,   it also permits the development of more debuggable   systems where the results of QA can be checked   against the model ’s beliefs .   Through our results on TRIP ( Table 4 ) , we also   see the viability of adding our belief tracking mech-   anism into more complex modeling pipelines . We   were specifically able to obtain SOTA performance   on this task and outperform the larger and highly   tailored task - specific model architecture based onRoBERTa - large used by Storks et al . ( 2021 ) .   Additional Analysis We see in Table 5 for   CLUTRR that having an additional self - attention   aggregation layer when constructing breakpoint   representations ( -brk self - attn , § 5.1 ) is very im-   portant for accuracy and consistency ( we find sim-   ilar results for TRIP , bottom ) . This suggests that   further improvements might be achieved through   improved pooling and masking strategies for con-   structing breakpoint representations . We also   see the advantages of having auxiliary generation   losses ( event generation , abstraction ) for improv-   ing accuracy and performance .   7 Conclusion   Being able to track the beliefs of models remains a   formidable challenge at the forefront of model in-   terpretability . In this paper , we presented a new rep-   resentation learning framework , breakpoint model-   ing , that facilitates end - to - end learning and track-   ing of beliefs at intermediate states in narrative   text . On a diverse set of NLU tasks , we show the   benefit of our approach ( based on T5 ) over conven-   tional learning approaches in terms of improved   belief prediction performance on new belief track-   ing tasks and processing efficiency . We also show   the feasibility of recasting existing tasks into our   framework and integrating our approach into exist-   ing transformer - based NLU pipelines , which we   believe can help to improve the interpretability of   these models as part of this larger challenge .   Acknowledgements   The authors thank the Aristo team for valuable   feedback . This work was supported by the Euro-   pean Research Council ( ERC ) under the European   Union ’s Horizon 2020 research and innovation pro-   gramme ( grant no . 852686 , SIAM , Shahaf ) . Part   of this research was also supported by the Euro-   pean Research Council , ERC - StG grant no . 677352   ( Tsarfaty ) , which is gratefully acknowledged.97118 Limitations   Below we summarize the main limitations of our   current breakpoint models and the techniques pur-   sued in this study .   Compositional Generalization Despite richer   supervision over intermediate states , compositional   generalization performance remains a significant   challenge ( on bAbI and CLUTRR generalization   splits , see § 6 ) for future work , which shows that our   approach inherits many of the limitations in the gen-   eralization ability of large - scale LMs more broadly .   Following Kim et al . ( 2021 ) and others , we hypoth-   esize that the all - to - all attention employed by Trans-   formers in creating token encodings ( including the   breakpoint tokens ) is a factor in non - compositional   behavior ; such attention is more vulnerable to over-   fitting spurious patterns . Accordingly , more ad-   vanced attention masking ( Kim et al . , 2021 ) and su-   pervision ( Yin et al . , 2021 ) approaches are promis-   ing directions to explore .   Our notion of “ belief ” While breakpoints pro-   vide an indication of intermediate model “ beliefs ” ,   they are also different from beliefs in important   ways . In particular , the causal relation between   information represented in breakpoints and gen-   erated model outputs is unclear ( see also Li et al .   ( 2021 ) for similar caveats in standard NLMs ) . For   example , models may generate outputs that are in-   consistent with their own breakpoint belief states .   Interestingly , breakpoint models may offer new   ways to address these limitations by more explicitly   representing intermediate reasoning steps ; neural   logic losses ( Li et al . , 2019 ) can help enforce belief   consistency between sets of propositions ( § 3.3 ) .   Task and domain limitations Finally , our exper-   iments are still limited to datasets involving rela-   tively short ( TRIP ) and synthetic ( bAbI , CLUTRR )   inputs with limited semantics . Further work is   needed to address more natural and complex lan-   guage to ultimately develop more robust break-   point models . In contrast to standard end - to - end   QA methods , breakpoint modeling requires more   costly annotation , as training currently requires   some form of supervision on intermediate states ,   beyond the final target output . Thus , developing   new methods for collecting such annotations with   minimal engineering effort remains a challenge . References9712971397149715A Dataset Details   In this section , we provide additional details about   all datasets .   A.1 TRIP   As described in § 4 , the TRIP benchmark con-   sists of 3 tiered tasks : ( 1 ) plausibility ( 2 ) con-   sistency ( 3 ) verifiability . To apply our model to   TRIP , we convert the first two tasks to a text2text   format : the first task involves taking two stories   ( A ) storyA ( B ) storyB $ plaus as input   text and producing a text label { A , B } to identify   the implausible story ; task 2 involves taking a   labeled story sentence1 1 sentence2 2 ..   $ conflict and generating the labels identify-   ing the problematic sentences . We convert the   third task to breakpoint format by converting state   change labels to textual propositions associated   with the corresponding timesteps . Figure 8 shows   an example instance from the TRIP development   set . Note that each task is effectively rendered as   two instances : the first instance addresses task 1   ( as QA ) , and the second jointly addresses tasks 2   ( QA ) and 3 ( proposition prediction ) .   State changes in TRIP are defined either as ef-   fects or preconditions ( Storks et al . , 2021 ) and this   information must be preserved in the conversion to   breakpoint format . Preconditions are propositions   that hold before a described event ; for example ,   the proposition “ oven was open ” should be true   before the sentence “ John closed the oven . ” Ef-   fect propositions are propositions that hold after   a described event ; the proposition “ oven is open ”   should be false after “ John closed the oven . ” We   represent precondition and effect propositions sim-   ply by modifying the proposition tense . Given   breakpoint b , for associated precondition proposi-   tions at time t , we use past tense ( “ oven was open ” ) .   For effect propositions at time t , we use present   tense ( “ oven is open ” ) .   While the TRIP data includes state information   for all time steps and entities , we follow the official   evaluation procedureand only score the subset   of state changes defined to be relevant in the pair   of conflict sentences . At training time , we use all   available state change information for training . Finally , while most state changes in TRIP are   attributes that can be true , false orunknown ( and   thus can be directly converted to proposition form ) ,   location attributes are formulated as k - way classifi-   cation problems . For example , an object location   attribute change is represented by 1 of 9 possible   classes ( see Table 5 in Storks et al . ( 2021 ) and blue   propositions in Fig . 8) . To facilitate equivalent   evaluation of k - class predictions with our break-   point model , we consider the predicted true score   for each of the possible kpropositions and take the   maximum scoring proposition to be the predicted   value .   A.2 bAbI   A.2.1 Proposition generation   As detailed in § 4 , base propositions for bAbI are   generated using the Dyna - bAbI tool ( Tamari et al . ,   2022 ) . From this , new propositions are derived   from the following general constraints : location/-   possession uniqueness that dictate that objects can   only be in one location at a time and possessed by   a single agent ( e.g. , John can not simultaneously be   in the kitchen and living room ) , mutually exclusiv-   itybetween event types ( e.g. , that dropping a ball   is the opposite of picking up a ball ) ; explanation   frame rules ( Haas , 1987 ) that dictate that objects ,   when left unchanged , maintain their location and   their possession through time ( e.g. , John is in the   kitchen orJohn has the apple stays true until there   is an explicit event that changes this ) .   A.2.2 Task details   The training data includes 500 samples per task   type , where the tasks follow the same structure as   theconcat(T7 ) dataset described in ( Tamari et al . ,   2022 ) ( Table 6 in that work ) , with the only dif-   ference being the story length which was fixed to   20 sentences to match the test data . The hardQA   generalization task was generated using the same   settings as the mix(T7 ) evaluation set from ( Tamari   et al . , 2022 ) , including the same 3 question types   with 1,000 samples for each type ( also Table 6 in   Tamari et al . ( 2022 ) ) . Figure 7 shows example   stories from the training and hardQA test splits .   A.3 CLUTRR   We note that all of the underlying story data was   generated from scratch and relies on the publicly   available task generators from Sinha et al . ( 2019)9716   and Gontier et al . ( 2020 ) . As detailed in Gontier   et al . ( 2020 ) , leakage among the proofs and propo-   sitions in stories of the same kcan be a problem .   Using some of their ideas , we avoided this by ex-   panding the inventory of names used in training   and abstracted names for parts of the training . We   verified the hardness of our data by training a no-   story proposition - only baseline an found it to have   low performance , and also manually verified all   inference rules used for generating propositions .   B Training details   B.1 Hyper - parameters   All hyper - parameter tuning for our main models   was performed via a random search in the style   of Devlin et al . ( 2019 ) . Model selection was per-   formed by selecting models with the highest valida-   tion accuracy for each task ( e.g. , proposition accu-   racy for our proposition tasks , exact match for the   QA experiments ) . Unless noted otherwise , we re-   port the average of models with the optimal hyper-   parameters based on 3 random re - starts ; early stop-   ping was applied throughout . All experiments were   performed on NVIDIA AX6000 GPU hardware on   a single GPU .   breakpoint models : learning rate ( we experi-   mented in the range of 1e-3 to5e-6 , we gen-   erally found 5e-5 to be optimal for most experi - ments ) , number of epoch ( up to 35 for CLUTRR ,   TRIP and 150 for bAbI ) , batch size ( in the range   of2to16 , memory permitting , we found 2to be   optimal for bAbI and TRIP experiments , and 4for   CLUTRR ) and weight decay ( set to 0.001 ) and   warmup steps ( from 500 to1ksteps ) . See the   project repository for further details   joint models For multi - task training , parameters   λwere hand tuned , with λset to 1.0 for   all proposition prediction tasks ( with λ=0.1 for   most tasks ) . For joint QA tasks , we found setting   λ=1.0 andλ=1.0 to be optimal , with   an initial warmup before turning on the proposi-   tion prediction loss ( usually between 5 - 10epochs ) .   Given the high cost of training the bAbI breakpoint   QA model in Table 2 , the joint QA + prop models   described on the last row start training from the   BPT - base checkpoints described in the row above .   B.2 Auxiliary Generation Losses   As detailed in § 5.1 , we jointly trained our break-   point models with additional generation losses that   aim to mimic some of the unsupervised denoising   objectives used in Raffel et al . ( 2020 ) . Whereas   in standard denoising you might try to generate   from a text input A dog < mask > while running   the output text < mask > barked loudly < mask > ,   from an original text A dog barked loudly while   running ( with full attention over the input text ) ,   in our case we try to generate from a story John   went to the store [ B]He then picked up the97179718apple [ B]the raw event text John went to the   store from the corresponding raw breakpoint hid-   den state for the special token [ B]alone . In   addition to this event generation task , we also   experimented with a abstraction generation task :   given two stories in a batch and two random break-   points within those stories , e.g. , John went to the   kitchen [ B] ... andMary went to the kitchen   [ B ] .. , we ask the model to generate an abstract   textual description of the two events only from   the mean of the two breakpoint hidden states , i.e. ,   abstraction ( [ B],[B ] ) = A person went   to the kitchen . ( This was inspired by the abstraction   generation ideas from Gontier et al . ( 2022 ) ) .   During training , both forms of generation were   done by randomly selecting a single breakpoint   example and abstraction pair for each story in the   batch and computing a standard loss over the gen-   erated texts and abstractions . Using symbolic an-   notations of both the CLUTRR and bAbI training   events , a deterministic algorithm was implemented   for creating abstracted texts on the fly for training .   For TRIP , where logical annotations are not avail-   able , the abstraction task was replaced by the task   of generating versions of text replaced with POS   tags ( e.g. , John turned off the stove would be turned   intoPER turned off the NOUN ) .9719