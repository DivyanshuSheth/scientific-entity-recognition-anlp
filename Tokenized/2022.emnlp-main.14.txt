  Kayo Yin   University of California , Berkeley   kayoyin@berkeley.eduGraham Neubig   Carnegie Mellon University   gneubig@cs.cmu.edu   Abstract   Model interpretability methods are often used   to explain NLP model decisions on tasks such   as text classiﬁcation , where the output space   is relatively small . However , when applied to   language generation , where the output space   often consists of tens of thousands of tokens ,   these methods are unable to provide informa-   tive explanations . Language models must con-   sider various features to predict a token , such   as its part of speech , number , tense , or seman-   tics . Existing explanation methods conﬂate ev-   idence for all these features into a single expla-   nation , which is less interpretable for human   understanding .   To disentangle the different decisions in lan-   guage modeling , we focus on explaining lan-   guage models contrastively : we look for   salient input tokens that explain why the model   predicted one token instead of another . We   demonstrate that contrastive explanations are   quantiﬁably better than non - contrastive expla-   nations in verifying major grammatical phe-   nomena , and that they signiﬁcantly improve   contrastive model simulatability for human ob-   servers . We also identify groups of contrastive   decisions where the model uses similar evi-   dence , and we are able to characterize what in-   put tokens models use during various language   generation decisions .   1 Introduction   Despite their success across a wide swath of natural   language processing ( NLP ) tasks , neural language   models ( LMs ) are often used as black boxes : how   they make certain predictions remains obscure ( Be-   linkov and Glass , 2019 ) . This is in part due to the   high complexity of the LM task itself , as well as   that of the model architectures used to solve it .   We argue that this is also due to the fact that inter-   pretability methods commonly used in NLP , such   Table 1 : Explanations for the GPT-2 prediction given   the input “ Can you stop the dog from _ _ _ _ _ " . Input to-   kens that are measured to raise or lower the probability   of “ barking ” are in red and blue respectively , and those   with little inﬂuence are in white . Non - contrastive ex-   planations such as gradient ×input ( 1 ) usually attribute   the highest saliency to the token immediately preceding   the prediction . Contrastive explanations ( 2 , 3 ) give a   more ﬁne - grained and informative explanation on why   the model predicted one token over another .   as gradient - based saliency maps ( Li et al . , 2016a ;   Sundararajan et al . , 2017 ) , are not as informative   for LM predictions compared to other tasks like   text classiﬁcation . For example , to explain why an   LM predicts “ barking ” given “ Can you stop the   dog from _ _ _ _ ” , we demonstrate in experiments   that the input token preceding the prediction is of-   ten marked as the most inﬂuential token to the pre-   diction ( Table 1 ) by instance attribution methods .   The preceding token is indeed highly important to   determine certain features of the next token , ruling   out words that would obviously violate syntax in   that context ( e.g. non “ -ing ” verbs in the given   example ) . However , this does not explain why the   model made other more subtle decisions , such as   why it predicts “ barking ” instead of “ crying ” or   “ walking ” , which are all plausible choices if we only   look at the preceding token . In general , language   modeling has a large output space and a high com-   plexity compared to other NLP tasks ; at each time   step , the LM chooses one word out of all vocabu-   lary items , and several linguistic distinctions come   into play for each language model decision.184To better explain LM decisions , we propose inter-   preting LMs with contrastive explanations ( Lipton ,   1990 ) . Contrastive explanations aim to identify   causal factors that lead the model to produce one   output instead of another output . We believe that   contrastive explanations are especially useful to   handle the complexity and the large output space   of language modeling . In Table 1 , the second ex-   planation suggests that the input word “ dog ” makes   “ barking ” more likely than a verb not typical for   dogs such as “ crying ” , and the third explanation   suggests that the input word “ stop ” increases the   likelihood of “ barking ” over a verb without nega-   tive connotations such as “ walking ” .   In this paper , we ﬁrst extend three interpretabil-   ity methods to compute contrastive explanations   ( § 3 ) . We then perform a battery of experiments   aimed at examining to what extent these contrastive   explanations are superior to their non - contrastive   counterparts from various perspectives :   •RQ1 : Are contrastive explanations better at   identifying evidence that we believe , a - priori ,   to be useful to capture a variety of linguistic   phenomena ( § 4 ) ?   •RQ2 : Do contrastive explanations allow hu-   man observers to better simulate language   model behavior ( § 5 ) ?   •RQ3 : Are different types of evidence neces-   sary to disambiguate different types of words ,   and does the evidence needed reﬂect ( or un-   cover ) coherent linguistic concepts ( § 6 ) ?   2 Background   2.1 Model Explanation   Our work focuses on model explanations that com-   municate why a model made a certain prediction .   Particularly , we focus on methods that compute   saliency scores S(x)over input features xto re-   veal which input tokens are most relevant for a   prediction : the higher the saliency score , the more   xsupposedly contributed to the model output .   Despite a large body of literature examining   input feature explanations for NLP models on   tasks such as text classiﬁcation ( for a complete   review see Belinkov and Glass ( 2019 ) ; Madsen   et al . ( 2021 ) ) , or interpreting how language mod-   els use linguistic features such as syntax ( Ravfogel   et al . , 2021 ; Finlayson et al . , 2021 ) , few works   attempt to explain language modeling predictions(Wallace et al . , 2019 ) . Despite the importance of   both language models and interpretability in the   NLP literature , the relative paucity of work in this   area may be somewhat surprising , and we posit that   this may be due to the large output space of lan-   guage models necessitating the use of techniques   such as contrastive explanations , which we detail   further below .   2.2 Contrastive Explanations   Contrastive explanations attempt to explain why   given an input xthe model predicts a target y   instead of afoily . Relatedly , counterfactual   explanations explore how to modify the input xso   that the model more likely predicts yinstead of y   ( McGill and Klein , 1993 ) .   While contrastive and counterfactual explana-   tions have been explored to interpret model deci-   sions ( see Stepin et al . ( 2021 ) for a broad survey ) ,   they are relatively new to NLP and have not yet   been studied to explain language models .   Recently , Jacovi et al . ( 2021 ) produce counter-   factual explanations for text classiﬁcation mod-   els by erasing certain features from the input and   projecting the input representation to the “ con-   trastive space ” that minimally separates two de-   cision classes . Then , they compare model probabil-   ities before and after the intervention .   We , on the other hand , propose contrastive ex-   planations for language modeling , where both the   number of input factors and the output space are   much larger . While we also use a counterfactual ap-   proach with erasure ( § 3.3 ) , counterfactual methods   may become intractable over long input sequences   and a large foil space . We , therefore , also propose   contrastive explanations using gradient - based meth-   ods ( § 3.1,§3.2 ) that measure the saliency of input   tokens for a contrastive model decision .   3 Contrastive Explanations for   Language Models   In this section , we describe how we extend three   existing input saliency methods to the contrastive   setting . These methods can also be easily adapted   to tasks beyond language modeling , such as ma-   chine translation ( Appendix A ) .   3.1 Gradient Norm   Simonyan et al . ( 2013 ) ; Li et al . ( 2016a ) calculate   saliency scores based on the norm of the gradient of   the model prediction , such as the output logit , with185respect to the input . Applying this method to LMs   entails ﬁrst calculating the gradient as follows :   g(x ) = ∇q(y|x )   where xis the input sequence embedding , yis   the next token in the input sequence , q(y|x)is the   model output for the token ygiven the input x.   Then , we obtain the saliency score for the input   token xby taking the L1 norm :   S(x ) = ||g(x)||   We extend this method to the Contrastive Gra-   dient Norm deﬁned by :   g(x ) = ∇(q(y|x)−q(y|x ) )   S(x ) = ||g(x)||   where q(y|x)is the model output for foil ygiven   the input x. This tells us how much an input token   xinﬂuences the model to increase the probability   ofywhile decreasing the probability of y.   3.2 Gradient×Input   For the gradient×input method ( Shrikumar et al . ,   2016 ; Denil et al . , 2014 ) , instead of taking the L1   norm of the gradient , we take the dot product of   the gradient with the input token embedding x :   S(x ) = g(x)·x   We deﬁne the Contrastive Gradient ×Input :   S(x ) = g(x)·x   3.3 Input Erasure   Erasure - based methods measure how erasing cer-   tain parts of the input affects the output ( Li et al . ,   2016b ) . This can be measured as the difference   between the model output given the input xand   given the input where xhas been zeroed out , x :   S(x ) = q(y|x)−q(y|x )   We deﬁne the Contrastive Input Erasure :   S(x ) =   ( q(y|x)−q(y|x))−(q(y|x)−q(y|x ) )   This measures how much erasing xfrom the input   makes the foil more likely and the target less likely .   Although erasure - based methods directly mea-   sure the change in the output due to a perturbation   in the input , while gradient - based methods approx-   imate this measurement , erasure is usually more   computationally expensive due to having to run the   model on all possible input perturbations.4 Do Contrastive Explanations Identify   Linguistically Appropriate Evidence ?   First , we ask whether contrastive explanations are   quantiﬁably better than non - contrastive explana-   tions in identifying evidence that we believe a pri-   orishould be important to the LM decision . In   order to do so , we develop a methodology in which   we specify certain types of evidence that indicate   how to make particular types of linguistic distinc-   tions , and measure how well each variety of expla-   nation method uncovers this speciﬁed evidence .   4.1 Linguistic Phenomena   As a source of linguistic phenomena to study , we   use the BLiMP dataset ( Warstadt et al . , 2020 ) . This   dataset contains 67 sets of 1,000 pairs of minimally   different English sentences that contrast in gram-   matical acceptability . An example of a linguis-   tic paradigm may be anaphor number agreement ,   where an acceptable sentence is “ Many teenagers   were helping themselves . ” and a minimally con-   trastive unacceptable sentence is “ Many teenagers   were helping herself . ” because in the latter , the   number of the reﬂexive pronoun does not agree   with its antecedent .   From this dataset , we chose 12 paradigms be-   longing to 5 phenomena and created a set of rules   to identify the input tokens that enforce grammat-   ical acceptability . In the previous example , the   anaphor agreement is enforced by the antecedent   “ teenagers ” . We show examples for each linguistic   phenomenon and its associated rule in Table 2 .   Anaphor Agreement . The gender and number   of a pronoun must agree with its antecedent . We   implement the coref rule using spaCy ( Honnibal   and Montani , 2017 ) and NeuralCorefto extract all   input tokens coreferent with the target token .   Argument Structure . Certain arguments can   only appear with certain verbs . For example , many   action verbs must be used with animate objects .   We implement the main_verb rule using spaCy to   extract the main verb of the input sentence .   Determiner - Noun Agreement . Demonstrative   determiners and the associated noun must agree .   We implement the det_noun rule by generating the   dependency tree using spaCy and extracting the   determiner of the target noun.186   NPI Licensing . Certain negative polarity items   ( NPI ) are only allowed to appear in certain con-   texts , e.g. “ never ” appears on its own , while “ ever ”   generally must be preceded by “ not ” . In all of our   examples with NPI licensing , the word “ even ” is   an NPI that can appear in the acceptable example   but not in the unacceptable example , so we create   thenpirule that extracts this NPI .   Subject - Verb Agreement . The number of the   subject and its verb must agree . We implement   thesubj_verb rule by generating the dependency   tree using spaCy to extract the subject of the verb .   4.2 Alignment Metrics   We use three metrics to quantify the alignment   between an explanation and the known evidence   enforcing a linguistic paradigm . The explanation is   a vector Sof the same size as the input x , where   thei - th elementSgives the saliency score of the   input token x. The known evidence is represented   with a binary vector E , also of same size as the   inputx , whereE= 1 if the token xenforces a   grammatical rule on the model decision .   Dot Product . The dot product S·Emeasures   the sum of saliency scores of all input tokens that   are part of the known evidence .   Probes Needed ( Zhong et al . , 2019 ; Yin et al . ,   2021b ) . We measure the number of tokens we   need to probe , based on the explanation S , to ﬁnd   a token that is in the known evidence . This corre-   sponds to the ranking of the ﬁrst token xsuch that   G= 1after sorting tokens by descending saliency .   Mean Reciprocal Rank ( MRR ) . We calculate   the average of the inverse of the rank of the ﬁrst   token that is part of the known evidence if the to-   kens are sorted in descending saliency . This also   corresponds to the average of the inverse of the   probes needed for each sentence evaluated . Dot Product and Probes Needed calculate align-   ment for each sentence , and we compute the aver-   age over all sentence - wise alignment scores for the   alignment score over a linguistic paradigm . MRR   calculates alignment over an entire paradigm .   4.3 Results   We use GPT-2 ( Radford et al . , 2019 ) and GPT - Neo   ( Black et al . , 2021 ) to extract explanations . GPT-   2 is a large autoregressive transformer - based LM   with 1.5 billion parameters and trained on 8 mil-   lion web pages . GPT - Neo is a similar LM with 2.7   billion parameters and trained on The Pile ( Gao   et al . , 2020 ) containing 825.18 GB of largely En-187glish text . In addition to the explanation methods   described above , we also set up a random baseline   as a comparison , where we create a vector of the   same size as explanations with values randomly   sampled from a uniform distribution over [ 0,1 ) .   In Figure 1 , we can see that overall , contrastive   explanations have a higher alignment with linguis-   tic paradigms than their non - contrastive counter-   parts for both GPT-2 and GPT - Neo across the differ-   ent metrics . Although non - contrastive explanations   do not always outperform the random baseline , con-   trastive explanations have a better alignment with   BLiMP than random vectors for most cases .   In Table 3 , we further examined alignment be-   tween model explanations and known evidence   on instances where the model correctly allocates   more probability to the acceptable token , or incor-   rectly selects the other token . On examples where   the model makes an incorrect prediction , it is not   clear whether non - contrastive or contrastive meth-   ods have better alignment . On examples where the   model predicts correctly , contrastive explanations   obtain better alignment than their non - contrastive   counterparts for each explanation method and align-   ment metric .   In Figure 2 , we see that for most explanation   methods , the larger the distance between the known   evidence and the target token , the larger the in-   crease in alignment of contrastive explanations over   non - contrastive explanations . This suggests that   contrastive explanations particularly outperform   non - contrastive ones when the known evidence is   relatively further away from the target token , that is ,   contrastive explanations can better capture model   decisions requiring longer - range context .   In Appendix B , we also provide a table with the   full alignment scores for each paradigm , explana-   tion method , metric and model .   5 Do Contrastive Explanations Help   Users Predict LM Behavior ?   To further evaluate the quality of different explana-   tion methods , we next describe methodology and   experiments to measure to what extent explana-   tions can improve the ability of users to predict the   output of the model , namely model simulatability   ( Lipton , 2018 ; Doshi - Velez and Kim , 2017 ) .   5.1 Study Setup   Our user study is similar in principle to previous   works that measure model simulatability given dif-   ferent explanations ( Chandrasekaran et al . , 2018 ;   Hase and Bansal , 2020 ; Pruthi et al . , 2020 ) . In   our study ( Figure 3 ) , users are given the input of   a GPT-2 model , two choices for the next token ,   and an explanation for the model output . They are   asked to select which of the two choices is more   likely the model output , then answer whether the   explanation was useful in making their decision .   We compare the effect of having no explanation ,   explanations with Gradient ×Input , Contrastive   Gradient×Input , Erasure and Contrastive Era-   sure . We do not include Gradient Norm and Con-   trastive Gradient Norm because these methods do188   not provide information on directionality . For non-   contrastive methods , we provide the explanation for   why the model predicted a token . For contrastive   methods , we provide the explanation for why the   model predicted one token instead of another .   We include 20 pairs of highly confusable words   for our study ( Appendix C ) . 10 of these pairs are   selected from BLiMP to reﬂect certain linguistic   phenomena , and the other 10 word pairs are se-   lected from pairs with the highest “ confusion score ”   on WikiText-103 test split ( Merity et al . , 2016 ) .   We deﬁne confusion using the joint probability of   a confusion from token atobgiven a corpus X :   P(x = a , x = b ) =   1   N / summationdisplay / summationdisplayP ( ˆx = b|x )   where xis a sentence in X , pos(x)is the set of   positions of tokens in x , Nis the size of the corpus .   The confusion from atobis the sum of the prob-   abilities assigned by the model to token bwhere   token ais the ground truth , normalized by the num-   ber of sentences in the corpus .   The confusion score for word pair ( a , b)is the   minimum of confusion from atoband vice - versa ,   to ensure that words are mutually confusable :   C(a , b ) = min ( P(x = a , x = b ) ,   P(x = b , x = a ) ) .   We recruited 10 graduate students in machine   learning ( not authors of this paper ) to perform the   study . Each participant is given 10 different word   pairs . For each word pair , one explanation method   was chosen at random to generate the accompany-   ing explanations , and the participant is given 40sentences in a row . We balance the data so that   there were an equal number of examples where   the true output x = aandx = b , and also by   model correctness so that the model chooses the   correct output 50 % of the time , preventing users   from guessing model behavior by selecting a cer-   tain token or the true token . In total , we obtain   4000 data points for model simulatability .   5.2 Results   In Table 4 , we provide the results of our user study .   For each explanation method evaluated , we com-   puted the simulation accuracy over all samples   ( Acc . ) as well as accuracy over samples where   the model output is equal to the ground truth ( Acc .   Correct ) and different from the ground truth ( Acc .   Incorrect ) . We also computed the percentage of ex-   planations that users reported useful , as well as the   simulation accuracy over samples where the user   found the given explanation useful ( Acc . Useful )   and not useful ( Acc . Not Useful ) .   To test our results for statistical signiﬁcance and   account for variance in annotator skill and word   pair difﬁculty , we ﬁtted linear mixed - effects models   using Statsmodels ( Seabold and Perktold , 2010 )   with the annotator and word pair as random effects ,   the explanation method as ﬁxed effect , and the   answer accuracy or usefulness as the dependent   variable . In Appendix D we provide the results of   the mixed - effects models we ﬁtted .   Accuracy First of all , users have the lowest accu-   racy in predicting LM outputs when no explanation   is given , which suggests that all four types of ex-   planations help users simulate model behavior . For   both explanation methods , the contrastive setting   leads to a signiﬁcantly higher contrastive simula-   tion accuracy than the non - contrastive setting.189We also examined examples where annotators   incorrectly predict the model output , and for all   types of explanations given , the most human errors   are made in examples where there are no words   in the input sentence that makes one word more   likely than the other . Notably , the three word pairs   with the lowest user accuracy are “ son / brother ” ,   “ fast / super ” , and “ black / green ” , which are often   interchangeable .   Usefulness Contrastive explanations were also   considered useful to users for model simulation   signiﬁcantly more often than non - contrastive ex-   planations , with a particularly large gain in the   erasure - based setting . Answer accuracy on sam-   ples where the users found the explanation useful is   higher than the accuracy over all samples for each   explanation method , which suggests that users can   also identify useful explanations to some extent .   These results , on the whole , provide evidence   that contrastive explanations help human observers   simulate model predictions more accurately .   6 What Context Do Models Use for   Certain Decisions ?   Finally , we use contrastive explanations to discover   how language models achieve various linguistic   distinctions . We hypothesize that similar evidence   is necessary to disambiguate foils that are similar   linguistically . To test this hypothesis , we propose   a methodology where we ﬁrst represent each token   by a vector representing its saliency map when the   token is used as a foil in contrastive explanation of   a particular target word . Conceptually , this vector   represents the type of context that is necessary to   disambiguate the particular token from the target .   Next , we use a clustering algorithm on these vec-   tors , generating clusters of foils where similar types   of context are useful to disambiguate . We then ver-   ify whether we ﬁnd clusters associated with salient   linguistic distinctions deﬁned a - priori . Finally , we   inspect the mean vectors of explanations associated   with foils in the cluster to investigate how models   perform these linguistic distinctions .   6.1 Methodology   We generate contrastive explanations for the 10   most frequent words in WikiText-103 for each   major part of speech as the target token , and   use the 10,000 most frequent vocabulary items   as foils . For each target y , we randomly select500 sentences from WikiText-103 and obtain a sen-   tence set X. For each foil yand each sentence   x∈X , we generate a single contrastive explana-   tione(x , y , y ) . Then , for each target yand foil   y , we generate an aggregate explanation vector   e(y , y ) = /circleplustexte(x , y , y)by concatenating   the single explanation vectors for each sentence in   the corpus .   Then , for a given target y , we apply k - means   clustering on the concatenated contrastive expla-   nations across different foils yto cluster foils by   explanation similarity . We use GPT-2 to extract   all the contrastive explanations due to its better   alignment with linguistic phenomena than GPT-   Neo ( § 4 ) . We only extract contrastive explanations   with gradient norm and gradient ×input due to the   computational complexity of input erasure ( § 3.3 ) .   In Table 5 , we show examples of the obtained   clusters . Foils in each cluster are sorted in descend-   ing frequency in training data . For the ﬁrst foil in   each cluster , we also retrieve its 20 nearest neigh-   bors in the word embedding space according to   Euclidean distance for comparison , to disentangle   the effect of word embeddings from the effect of   linguistic distinctions on foil clusters .   6.2 Foil Clusters   First , we verify that linguistically similar foils are   indeed clustered together : we discover clusters re-   lating to a variety of previously studied linguistic   phenomena , a few of which we detail below and   give examples in Table 5 . Moreover , foil clusters   reﬂect linguistic distinctions that are not found in   the nearest neighbors of word embeddings . This   suggests that the model use similar types of input   features to make certain decisions .   Anaphor agreement : To predict anaphor agree-   ment , models must contrast pronouns from other   pronouns with different gender or number . We ﬁnd   that indeed , when the target is a pronoun , other   pronouns of a different gender or number are often   clustered together : when the target is a male pro-   noun , we ﬁnd a cluster of female pronouns . The   foil cluster containing “ she ” includes several types   of pronouns that are all of the female gender . On   the other hand , the nearest neighbors of “ she ” are   mostly limited to subject and object pronouns , and   they are of various genders and numbers .   Animacy : In certain verb phrases , the main verb   enforces that the subject is animate . Reﬂecting   this , when the target is an animate noun , inanimate190   nouns form a cluster . While the foil cluster in Table   5 contains a variety of singular inanimate nouns ,   the nearest neighbors of “ fruit ” are mostly both   singular and plural nouns related to produce .   Plurality : For determiner - noun agreement , sin-   gular nouns are contrasted with clusters of plural   noun foils , and vice - versa . We ﬁnd examples of   clusters of plural nouns when the target is a singular   noun , whereas the nearest neighbors of “ tabs ” are   both singular and plural nouns . To verify subject-   verb agreement , when the target is a plural verb ,   singular verbs are clustered together , but the near-   est neighbors of “ doesn ” contain both singular and   plural verbs , especially negative contractions.6.3 Explanation Analysis Results   By analyzing the explanations associated with dif-   ferent clusters , we are also able to learn interesting   properties of how GPT-2 makes certain predictions .   We provide our full analysis results in Appendix E.   To distinguish between adjectives , the model of-   ten relies on input words that are semantically sim-   ilar to the target ( e.g. “ relativity ” to distinguish   “ black ” from other colors ) . To contrast adposi-   tions and adverbs from other words with the same   POS , verbs in the input that are associated with   the target word are useful : for example , the verbs   “ dating ” and “ traced ” are useful when the target   is “ back ” . To choose the correct gender for deter-   miners , nouns and pronouns , the model often uses   gendered proper nouns and pronouns in the input .   To disambiguate numbers from non - number words,191input words related to enumeration or measurement   ( e.g. “ age ” , “ consists ” , “ least ” ) are useful .   Our analysis also reveals why the model may   have made certain mistakes . For example , when   the model generates a pronoun of the incorrect   gender , it was often inﬂuenced by proper nouns   and pronouns of a different gender in the input .   Overall , our methodology for clustering con-   trastive explanations provides an aggregate anal-   ysis of linguistic distinctions to understand general   properties of language model decisions .   7 Conclusion and Future Work   In this work , we interpreted language model de-   cisions using contrastive explanations by extend-   ing three existing input saliency methods to the   contrastive setting . We also proposed three new   methods to evaluate and explore the quality of con-   trastive explanations : an alignment evaluation to   verify whether explanations capture linguistically   appropriate evidence , a user evaluation to mea-   sure model simulatability of explanations , and a   clustering - based aggregate analysis to investigate   model properties using contrastive explanations .   We ﬁnd that contrastive explanations are better   aligned to known evidence related to major gram-   matical phenomena than their non - contrastive coun-   terparts . Moreover , contrastive explanations allow   better contrastive simulatability of models for users .   From there , we studied what kinds of decisions   require similar evidence and we used contrastive   explanations to characterize how models make cer-   tain linguistic distinctions . Overall , contrastive   explanations give a more intuitive and ﬁne - grained   interpretation of language models .   Future work could explore the application of   these contrastive explanations to other machine   learning models and tasks , extending other inter-   pretability methods to the contrastive setting , as   well as using what we learn about models through   contrastive explanations to improve them .   8 Limitations   The experiments and methodology described in   this paper have some limitations , notably their ex-   tensions to other explanation methods , other lan-   guages , and their resource requirements .   First , the applicability of the contrastive set-   ting to other explanation methods may be limited .   While extending gradient - based explanation meth-   ods to the contrastive setting is relatively straight - forward ( we can simply perform the same op-   erations on the gradient over the difference be-   tween model probabilities for the target and foil   tokens ) , it is nontrivial to design contrastive expla-   nations based on other explanation methods such   as attention - based input saliency .   Second , many of our experiments would not be   easily reproduced in languages other than English   that lack sufﬁcient linguistic resources . All the   experiments in our paper aimed at exploring the ca-   pabilities of contrastive explanations are performed   using GPT-2 and GPT - Neo language models , that   have been trained on large amounts of English data .   To reproduce experiments in other languages , we   would need a language model in the other language   of sufﬁcient power , which is not available for most   languages . The experiments in Section 4 address   only a subset of types of grammatical acceptability ,   and require a dataset of minimal pairs along differ-   ent types of grammatical acceptability , which may   not be available for most languages . Moreover , to   automatically extract the expected evidence , we   rely on core NLP tools such as coreference resolu-   tion , POS tagger and dependency parsers . Again ,   these tools are not available for most languages .   Furthermore , the accuracy of the extracted evi-   dence depends of the aforementioned tools , which   have fairly high but not perfect accuracy . While   our experiments are not easily extendable to lan-   guages other than English , our method itself of   contrastive explanations is language agnostic and   can be readily applied to models of any language .   In Section 5 , we perform a human study to eval-   uate explanation methods . This evaluation method   require human annotators and is therefore more re-   source intensive than automatic evaluation methods .   We were motivated to perform this study neverthe-   less as model simulatability for human users is one   important aspect of interpretability .   The experiments in Section 6 are also resource   intensive . In total , we computed : 2 explanation   methods×8 parts of speech×10 target words×   10,000 foils×500 input sentences = 800,000,000   contrastive explanations . For this reason , we omit-   ted the slower contrastive erasure explanation from   this experiment , but generating all the contrastive   explanations using the relatively faster explanation   methods , then clustering them required about 48   hours of computation on 8 RTX 8000 GPUs.192References193   A Contrastive Explanations for Neural   Machine Translation ( NMT ) Models   A.1 Extending Contrastive Explanations to   NMT   Machine translation can be thought of as a speciﬁc   type of language models where the model is condi-   tioned on both the source sentence and the partial   translation . It has similar complexities as mono-   lingual language modeling that make interpretingneural machine translation ( NMT ) models difﬁcult .   We therefore also extend contrastive explanations   to NMT models .   We compute the contrastive gradient norm   saliency for an NMT model by ﬁrst calculating   the gradient over the encoder input ( the source sen-   tence ) and over the decoder input ( the partial trans-   lation ) as :   g(x ) = ∇/parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   g(x ) = ∇/parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   where xis the encoder input , xis the decoder   input , and the other notations follow the ones in   § 3.1 .   Then , the contrastive gradient norm for each x   andxare :   S(x ) = ||g(x)||   S(x ) = ||g(x)||   Similarly , the contrastive gradient ×input are :   S(x ) = g(x)·x   S(x ) = g(x)·x   We deﬁne the input erasure for each xandx   as :   S(x ) = /parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   −/parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   S(x ) = /parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   −/parenleftBig   q(y|x , x)−q(y|x , x)/parenrightBig   A.2 Qualitative Results   In Table 6 , we provide examples of non - contrastive   and contrastive explanations for NMT decisions .   We use MarianMT ( Junczys - Dowmunt et al . , 2018 )   with pre - trained weights from the model trained   to translate from English to Romance languages   to extract explanations . Each example reﬂects a   decision associated with one of the ﬁve types of   linguistic ambiguities during translation identiﬁed   in Yin et al . ( 2021a).194In the ﬁrst example , the model must translate   the gender neutral English pronoun “ it ” into the   masculine French pronoun “ il ” . In both non-   contrastive and contrastive explanations , the En-   glish antecedent “ vase ” inﬂuences the model to   predict “ il ” , however to disambiguate “ il ” from the   feminine pronoun “ elle ” , the model also relies on   the french antecedent and its masculine adjective   “ nouveau vase ” .   In the second example , the model must translate   “ your ” with the formality level consistent with the   partial translation . While in the non - contrastive   explanation , only tokens in the source sentence are   salient which do not explain the model ’s choice   of formality level , in the contrastive explanation ,   other French words in the polite formality level   such as “ V ous ” and “ pouvez ” are salient .   In the third example , the model must translate   “ learned ” using the verb form that is consistent with   the partial translation . Similarly to the previous   example , only the contrastive explanation contains   salient tokens in the same verb from as the target   token such as “ aimais ” .   In the fourth example , the model needs to re-   solve the elided verb in “ I do n’t know ” to translate   into French . The contrastive explanation with a   different verb as a foil shows that the elided verb in   the target side makes the correct verb more likely   than another verb .   In the ﬁfth example , the model must choose the   translation that is lexically cohesive with the partial   translation , where “ carnet ” refers to a book with   paper pages and “ ordinateur ” refers to a computer   notebook . In the non - contrastive explanation , the   word “ notebook ” and the target token preceding the   prediction are the most salient . In the contrastive   explanation , the word “ carnet ” in the partial trans-   lation also becomes salient .   B Alignment of Contrastive   Explanations to Linguistic Paradigms   In Table 7 , we present the full alignment scores of   contrastive explanations from GPT-2 and GPT - Neo   models with the known evidence to disambiguate   linguistic paradigms in the BLiMP dataset .   C Highly Confusable Word Pairs   In Table 8 , we provide the list of contrastive word   pairs used in our human study for model simulata-   bility ( § 5 ) . The ﬁrst 10 pairs are taken from BLiMP   linguistic paradigms and we provide the associated195196unique identiﬁer for each pair . The last 10 pairs are   chosen from word pairs with the highest confusion   score .   D Mixed Effects Models Results   In Table 9 , we show the results of ﬁtting linear   mixed - effects models to the results of our user   study for model simulatability ( § 5 ) .   E Analysis of Foil Clusters   In Figure 5 , we give a few examples of clusters and   explanations we obtain for each part of speech . For   each part of speech , we describe our ﬁndings in   more detail in the following .   Adjectives . When the target word is an adjective ,   other foil adjectives that are semantically similar tothe target are often clustered together . For example ,   when the target is “ black ” , we ﬁnd one cluster   with various color adjectives , and we also ﬁnd a   different cluster with various adjectives relating to   the race or nationality of a person .   We ﬁnd that to distinguish between different   adjectives , input words that are semantically close   to the correct adjective are salient . For example   to disambiguate the adjective “ black ” from other   colors , words such as “ venom ” and“relativity ” are   important .   Adpositions . When the target is an adposition ,   other adpositions are often in the same cluster .   To distinguish between different adpositions , the   verb associated with the adposition is often useful   to the LM . For example , when the target word is   “ from ” , verbs such as “ garnered ” and“released ”   helps the model distinguish the target from other   adpositions that are less commonly paired with   these verbs ( e.g. “ for ” , “ of ” ) . As another example ,   for the target word “ for ” , verbs that indicate a long-   lasting action such as “ continue ” and“lived ” help   the model disambiguate .   Adverbs . When the target is an adverb , other   adverbs are often clustered together . Sometimes ,   when the target is a speciﬁc type of adverb , such as   an adverb of place , we can ﬁnd a cluster with other   adverbs of the same type .   Similarly to adpositions , LMs often use the verb   associated with the target adverb to contrast it from   other adverbs . For example , the verbs “ dating ” and   “ traced ” are useful when the target is “ back ” , and   the verbs “ torn ” and“lower ” are useful when the   target is “ down ” .   Determiners . Other determiners are often clus-   tered together when the target is a determiner . Par-   ticularly , when the target is a possessive determiner ,   we ﬁnd clusters with other possessive determiners ,   and when the target is a demonstrative determiner ,   we ﬁnd clusters with demonstrative determiners .   When the determiner is a gendered possessive   determiner such as “ his ” , proper nouns of the same   gender , such as “ John ” and“George ” , are often   useful . For demonstrative determiners , such as   “ this ” , verbs that are usually associated with a tar-   geted object , such as “ achieve ” and“angered ” are   useful .   Nouns . When the target noun refers to a person ,   for example , “ girl ” , foil nouns that also refer to197a person form one cluster ( e.g. “ woman ” , “ man-   ager ” , “ friend ” ) , commonly male proper nouns   form another ( e.g. “ Jack ” , “ Robin ” , “ James ” ) ,   commonly female proper nouns form another ( e.g.   “ Sarah ” , “ Elizabeth ” , “ Susan ” ) , and inanimate   objects form a fourth ( e.g. “ window ” , “ fruit ” ,   “ box ” ) .   When the target noun is an inanimate object ,   there are often two notable clusters : a cluster with   singular inanimate nouns and a cluster with plu-   ral inanimate nouns . This suggests how clustering   foils by explanations conﬁrm that certain grammat-   ical phenomena require similar evidence for disam-   biguation ; in this case , determiner - noun agreement .   To predict a target animate noun such as “ girl ”   instead of foil nouns that refer to a non - female or   older person , input words that are female names   ( e.g. “ Meredith ” ) or that refer to youth ( e.g.   “ young ” ) are useful . To disambiguate from male   proper nouns , input words that refer to female peo-   ple ( e.g. “ Veronica ” , “ she ” ) or adjectives related   to the target ( e.g. “ tall ” ) inﬂuence the model to   generate a female common noun . To disambiguate   from female proper nouns , adjectives and determin-   ers are useful . To disambiguate from inanimate   objects , words that describe a human or a human   action ( e.g. “ delegate ” , “ invented ” ) are useful .   To predict a target inanimate noun such as “ page ”   instead of nouns that are also singular , input words   with similar semantics are important such as “ sheet ”   and “ clicking ” are important . For plural noun foils ,   the determiner ( e.g. “ a ” ) is important .   Numbers . When the target is a number , non-   number words often form one cluster and other   numbers form another cluster .   To disambiguate numbers from non - number   words , input words related to enumeration or mea-   surement are useful ( e.g. “ age ” , “ consists ” , “ least ” ) .   To disambiguate words like “ hundred ” and“thou-   sand ” from other numbers such as “ 20 ” or“ﬁve ” ,   input words used for counting ( e.g. “ two ” , “ sev-   eral ” ) are useful , because “ hundred ” s are count-   able in English ( i.e. “ twohundreds ” , “ several hun-   dreds ” ) .   Pronouns . When the target word is a gendered   pronoun , foil pronouns of a different gender from   the target form one cluster , foils with proper nouns   of a different gender form a second cluster , and   foils with proper nouns of the same gender as the   target form a third cluster . This shows that themodel uses similar evidence to make decisions to   verify anaphor gender agreement . We also did not   ﬁnd foil clusters associated with distinguishing the   number of the pronoun : often , these decisions fol-   low directly from deciding between a pronoun and   a proper noun , or deciding between a male and   female pronoun .   To disambiguate a gendered pronoun such as   such as “ he ” , from pronouns or proper nouns   with different genders ( e.g. “ she ” or“Anna ” ) ,   proper nouns of the same gender as the target ( e.g.   “ James ” ) and other gendered pronouns or determin-   ers ( e.g. “ his ” ) are useful . To disambiguate from   proper nouns of the same gender as the target , inter-   estingly , the same proper noun as the foil appearing   in the input is positively salient ; GPT-2 is often in-   ﬂuenced by previously appearing proper nouns to   generate a pronoun instead .   Verbs . When the target word is a verb , foil verbs   that have a different verb form are often clustered   together . This suggests that the model uses similar   input features to verify subject - verb agreement .   When the target verb is in present participle form ,   auxiliary verbs in the input are useful ( e.g. “ is ” ,   “ been ” ) to distinguish from verbs in other forms .   Similarly , when the target verb is in inﬁnitive form ,   verbs in the same compound as the target verb are   important.198