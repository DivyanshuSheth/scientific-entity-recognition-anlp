  Xiuying Chen , Guodong Long , Chongyang Tao , Mingzhe Li ,   Xin Gao , Chengqi Zhang , Xiangliang ZhangComputational Bioscience Reseach Center , KAUSTAAII , School of CS , FEIT , University of Technology SydneyMicrosoftAnt GroupUniversity of Notre Dame   xiuying.chen@kaust.edu.sa   Abstract   A robust summarization system should be able   to capture the gist of the document , regard-   less of the specific word choices or noise   in the input . In this work , we first explore   the summarization models ’ robustness against   perturbations including word - level synonym   substitution and noise . To create semantic-   consistent substitutes , we propose a SummAt-   tacker , which is an efficient approach to gen-   erating adversarial samples based on language   models . Experimental results show that state-   of - the - art summarization models have a signif-   icant decrease in performance on adversarial   and noisy test sets . Next , we analyze the vul-   nerability of the summarization systems and   explore improving the robustness by data aug-   mentation . Specifically , the first brittleness fac-   tor we found is the poor understanding of in-   frequent words in the input . Correspondingly ,   we feed the encoder with more diverse cases   created by SummAttacker in the input space .   The other factor is in the latent space , where   the attacked inputs bring more variations to the   hidden states . Hence , we construct adversarial   decoder input and devise manifold softmixing   operation in hidden space to introduce more   diversity . Experimental results on Gigaword   and CNN / DM datasets demonstrate that our ap-   proach achieves significant improvements over   strong baselines and exhibits higher robustness   on noisy , attacked , and clean datasets .   1 Introduction   Humans have robust summarization processing sys-   tems that can easily understand diverse expres-   sions and various wording , and overcome typos ,   misspellings , and the complete omission of letters   when reading ( Rawlinson , 2007 ) . However , stud-   ies reveal that small changes in the input can lead   to significant performance drops and fool state - of-   the - art neural networks ( Goodfellow et al . , 2015 ;   Table 1 : Examples of vulnerability to BART - based sum-   marization model . All examples show an initially cor-   rect summary turning into a wrong summary due to   small changes in the input , e.g. , mis - spelling and syn-   onym substitution .   Belinkov and Bisk , 2018 ; Cheng et al . , 2018 ) . In   text generation fields such as machine translation ,   Belinkov and Bisk ( 2018 ) showed that state - of-   the - art models fail to translate even moderately   noisy texts , Cheng et al . ( 2018 ) found that the gen-   erated translation is completely distorted by only   replacing a source word with its synonym . How-   ever , the robustness on summarization models is   less explored . Here , we show three summarization   examples from the Gigaword dataset in Table 1 .   A fine - tuned BART model will generate a worse6846summary for a minor change in the input including   misspelling errors and synonym substitution , which   often happen in practice due to the carelessness and   habit of word usage in writing . Take the second   case for example , an English user and an Ameri-   can user who use barrister orattorney will obtain   summaries of different qualities . In the third case ,   a synonym word replacement even changes the   subject of canvassing . Such weakness of summa-   rization systems can lead to serious consequences   in practice .   Despite its importance , robustness in summariza-   tion has been less explored . Jung et al . ( 2019 ) and   Kry´sci´nski et al . ( 2019 ) examined positional bias   and layout bias in summarization . Liu et al . ( 2021 )   introduced multiple noise signals in self - knowledge   distillation to improve the performance of student   models on benchmark datasets , but they did not   explicitly evaluate the robustness of summarization   models against noise .   Hence , in this work , we first evaluate the robust-   ness of the existing state - of - the - art summarization   systems against word - level perturbations includ-   ing noise and adversarial attacks . The noise con-   sists of natural human errors such as typos and   misspellings . To create the adversarial attack test   set , we come up with a model named SummAt-   tacker . The core algorithm of SummAttacker is   to find vulnerable words in a given document for   the target model and then apply language models   to find substituted words adjacent in the opposite   direction of the gradient to maximize perturbations .   We validate the effectiveness of SummAttacker on   benchmark datasets with different attributes , i.e. ,   Gigaword and CNN / DailyMail . Experiment results   show that by only attacking one word ( 1 % token )   in Gigaword and 5 % tokens in CNN / DailyMail ,   the existing summarization models have drastically   lower performance .   We next conduct a vulnerability analysis and   propose two corresponding solutions to improve   robustness . Our first conjecture is that worse sum-   maries can be caused by replacing common words   with uncommon and infrequently - used words ,   which the model might not understand well . Hence ,   we employ the outputs from SummAttacker as in-   puts for the encoder , so as to improve the diversity   in the discrete input space . The second influencing   factor is that the attacked inputs introduce more   variations in the latent space . Correspondingly ,   we aim to expose the model to more diverse hid - den states in the training process . Specifically , we   build soft pseudo tokens by multiplying the decoder   output probability with target token embeddings .   These soft pseudo tokens and original tokens are   then manifold softmixed on a randomly selected   decoder layer to enlarge the training distribution .   The interpolations leveraged in deeper hidden lay-   ers help capture higher - level information , improve   semantic diversity , and provide additional training   signal ( Zeiler and Fergus , 2014 ) . Experiments   show that our dual augmentation for both encoder   and decoder improves the robustness of summa-   rization models on noisy and attacked test datasets .   Our main contributions are as follows :   •We empirically evaluate the robustness of re-   cent summarization models against perturbations   including noise and synonym substitutions .   •To improve the robustness of summarization   models , we propose a dual data augmentation   method that introduces diversity in the input and   latent semantic spaces .   •Experimental results demonstrate that our   augmentation method brings substantial improve-   ments over state - of - the - art baselines on benchmark   datasets and attacked test datasets .   2 Related Work   We discuss related work on robust abstractive sum-   marization , adversarial examples generation , and   data augmentation .   Robust Abstractive Summarization . Ideally , a   robust text generation system should consistently   have high performance even with small perturba-   tions in the input , such as token and character   swapping ( Jin et al . , 2020 ) , paraphrasing ( Gan and   Ng , 2019 ) , and semantically equivalent adversarial   rules ( Ribeiro et al . , 2018 ) . Considerable efforts   have been made in the text generation field . For   example , Cheng et al . ( 2019 ) defended a transla-   tion model with adversarial source examples and   target inputs . However , the robustness in the sum-   marization task has been less explored . Jung et al .   ( 2019 ) and Kry ´ sci´nski et al . ( 2019 ) showed that   summarization models often overfit to positional   and layout bias , respectively . In contrast , in this   work , we focus on the robustness of summarization   models against word - level perturbations .   Adversarial Examples Generation . Classic at-   tacks for text usually adopt heuristic rules to mod-   ify the characters of a word ( Belinkov and Bisk,68472018 ) or substitute words with synonyms ( Ren   et al . , 2019 ) . These heuristic replacement strate-   gies make it challenging to find optimal solutions in   the massive space of possible replacements while   preserving semantic consistency and language flu-   ency . Recently , Li et al . ( 2020 ) proposed to gener-   ate adversarial samples for the text classification   task using pre - trained masked language models   exemplified by BERT . In this paper , we focus on   attacking summarization models , which is a more   challenging task , since the model compresses the   input , and perturbations on unimportant parts of   the source might be ignored .   Data Augmentation . Data augmentation aims to   generate more training examples without incurring   additional efforts of manual labeling , which can   improve the robustness or performance of a target   model . Conventional approaches introduce discrete   noise by adding , deleting , and/or replacing char-   acters or words in the input sentences ( Belinkov   and Bisk , 2018 ) . More recently , continuous aug-   mentation methods have been proposed . Cheng   et al . ( 2020 ) generated adversarial sentences from   a smooth interpolated embedding space centered   around observed training sentence pairs , and shows   its effectiveness on benchmark and noisy transla-   tion datasets . Xie et al . ( 2022 ) proposed a target-   side augmentation method , which uses the decoder   output probability distributions as soft indicators .   Chen et al . ( 2023 ) selectively augmented training   dataset considering representativeness and genera-   tion quality . In this work , we propose a dual aug-   mentation method that utilizes discrete and virtual   augmented cases .   3 The Proposed SummAttacker   Formally , given a trained summarization model   with parameters θ , the purpose of an attacking   model is to slightly perturb the input xsuch that the   summarization output of the perturbed ˆxdeviates   away from the target summary y:,(1 )   where R(ˆx , x)captures the degree of impercepti-   bility for a perturbation , e.g. , the number of per-   turbed words . To make a maximal impact on the   summarization output with a perturbation budget ϵ ,   a classical way is to launch gradient - based attacks   ( Cheng et al . , 2019 ) . In this section , we propose   a SummAttacker for crafting adversarial samples   that may differ only a few words from genuine in-   puts but have low - quality summarization results .   Due to its capacity and popularity , we take BART   ( Lewis et al . , 2020 ) as the backbone summarization   model , as shown in Fig.1 .   Attacked Word Selector . Since it is intractable   to obtain an exact solution for Equation 1 , we , there-   fore , resort to a greedy approach to circumvent it .   In BART kind of summarization model based on   Transformer architecture , the sequence representa-   tion vector sof input tokens in xis first projected   to keys Kand values Vusing different linear map-   ping functions . At the t - th decoding step , the hid-   den state of the previous decoder layer is projected   to the query vector q. Then qis multiplied by   keysKto obtain an attention score aand the t - th   decoding output :   where dis the hidden dimension . A token that   obtains the highest attention score over all decoding   steps is the most important and influential one to   the summarization model . We select the word w   to attack if it contains or equals the most important   token . To avoid changing factual information , we   restrict wnot to be people names and locations .   Attacking with LM and Gradients . Next , we   aim to find a replacement word that is semantically   similar to wbut is adversarial to the summarization   model . Language models are empowered to gener-   ate sentences that are semantically accurate , fluent ,   and grammatically correct . We take advantage of   this characteristic to find a replacement word wfor   the target word w. The general idea is to first iden-6848   tify the top likely candidates that are predicted by   the language model for w , and then select the best   candidate with the guidance of prediction gradient .   Concretely , we first feed the tokenized sequence   into the BART model to get a prediction for the   attacked word w. As shown in Fig.1 , for wwith a   single token , we use STP ( Single Token Prediction )   operation to simply obtain the top Kpredictions   that are semantically similar to w. For wwith   multiple tokens , we have MTP ( Multi - Token Pre-   diction ) , which lists c×Kpossible combinations   from the prediction , where cis the token number   in the word . Then we rank the perplexity of all   combinations to get the top- Kcandidate combina-   tions , denoted as V. We filter out stop words and   antonyms using NLTK and synonym dictionaries .   Following the idea of a gradient - based attack   model , we then find the most adversarial word w   that deviates from wtowards a change aligned   with the prediction gradient:(2 )   where sim ( · , · ) is cosine distance , and eis word   embedding function . As shown in Fig . 1 , the re-   placement word wchanges the model state stos   in the opposite direction of optimization , −g .   4 Dual Augmentation   With the proposed attacking model , we first analyze   the influences of attacking , and then propose our   DASum to counter the negative effects . Vulnerability Analysis . We first look into   the word perturbation in attacked inputs that re-   sult in worse summaries . Our conjecture is that   worse summaries can be caused by replacing com-   mon words with uncommon and infrequently - used   words , which the model might not understand well .   Through the analysis of 50 worse summary cases ,   our conjecture is verified by the observation that   the frequency of the replacement words is 4 times   lower than the original words on average . Espe-   cially for those worse summaries including unex-   pected words not existing in the input , we found   that the co - occurrence of the unexpected word in   the generated summary and the replacement word   in the input is usually high . Take the third case with   unexpected work gopin Table 1 for example , the   co - occurrence for the word pair { party , gop } is 6   times higher than that of { government , gop } . These   analysis results imply that the model ’s vulnerability   is highly related to the word frequency distribution   and the diversity of the training documents .   Next , we investigate the influence of attack in   the latent space . It is well known that in the text   generation process , a change of a predicted pre-   ceding word will influence the prediction of words   after it , since the following prediction will attend to   the previously generated words ( Lamb et al . , 2016 ) .   This error accumulation problem can be more se-   vere in attacked scenarios since the perturbations   can bring more variety in the decoder space . To   verify our assumption , we evaluate the change in   hidden states of the BART model for 20 cases in the   original and the corresponding attacked test sets .   The top part of Fig.2 visualizes the hidden states   in the first and last BART decoder layer . It can be   seen that as the information flows from the low to   high layers in the decoder , the hidden states in the   latent space show larger diversity , as the distances   between paired hidden states get larger . We also   calculate the Euclidean distance Eof paired states ,   which increases from 1.8 to 2.5 . To improve the   summarization robustness against attacks , the de-   coder could be trained with augmentation in latent   space to comfort with diversity .   Augmentation Design . Based on the above   analysis , we first propose to incorporate the corpus   obtained by SummAttacker as augmentation input   for encoder , so as to improve the diversity of words   in training documents ( illustrated as yellow squares   with solid lines in Fig.3(a ) ) . To alleviate the im-   pact of perturbation on the decoding process , we6849   propose a continuous data augmentation method in   thelatent space of decoder , where multiple virtual   representations are constructed for each training   instance to make the decoder be exposed to diverse   variants of the latent representation of the same   input document ( illustrated as yellow squares with   dash lines in Fig.3(a ) ) .   Input Space Augmentation . The input space   augmentation in the encoder side is straightfor-   ward , as the output from SummAttacker can be   directly employed as encoder inputs . Concretely ,   we use SummAttacker to automatically generate   an augmented input document for the original doc-   ument , denoted as ˆx . We then train the summa-   rization model with the original and augmented   dataset , where the training objective is denoted as   L= log P(y|x)andL= log P(y|ˆx ) , respec-   tively . We also randomly add noisy words in both   inputs . We show this process in Fig.3(b ) , where we   draw the same encoder twice to denote the training   on original and augmented inputs .   Latent Semantic Space Augmentation . Based   on the vulnerability analysis in the decoding pro-   cess , we are motivated to mitigate the impact of   adversarial attacks by exposing the decoder to di-   verse variants of the latent representations . The   variants are established by an adversarial input and   a manifold softmix technique applied on randomly   selected layers in the decoder .   We first define a virtual adversarial decoder input   ˆyapart from the original input yby integrating   the embedding of words that are all likely to be   generated . Let lbe the decoder ’s predicted logits   before softmax , where t∈ { 1,2 , ... , m},l[v]be   the logit of vtoken , and mis the token length of y. We compute the pseudo decoder inputs as :   where Vis the vocabulary size , Wis the word em-   bedding matrix with size |V| × d , Tis the softmax   temperature .   Next , we construct the virtual adversarial hidden   states in the decoder by interpolating handˆh ,   which are the hidden states of inputs yandˆyat a   randomly selected k - th layer :   where λis the mixup ratio between 0 and 1 . The   mixup layer k∈[0 , L ] , where Lis the decoder   layer number .   In the decoding process , ˆyservers as variants of   yand integrates the embedding of words that are   likely to be generated in each step . The variants   of hidden states ˜hbehave like the hidden states   of attacked input text . The latent space augmenta-   tion objective is L= log P(y|x,ˆy ) . As shown in   Fig.3 , the latent semantic space augmented predic-   tion is a kind of additional training task for decoder   with variant samples indicated by yellow squares   with dash lines . Note that our proposed manifold   softmix differs from the target - side augmentation   in Xie et al . ( 2022 ) , which mixed the pseudo de-   coder input with the ground truth input in the word   embedding layer , and only introduces low - level   token variations .   Lastly , according to recent studies ( Chen et al . ,   2020 ) , maximizing the consistency across various   augmented data that are produced from a single   piece of data might enhance model performance .   Herein , we minimize the bidirectional Kullback-   Leibler ( KL ) divergence between the augmented6850   data and real data , to stabilize the training :   Our final loss function is defined as L+L+   L+L.   5 Experimental Setup   5.1 Dataset   We experiment on two public datasets , Gigaword   ( Napoles et al . , 2012 ) and CNN / DM ( Hermann   et al . , 2015 ) , which have been widely used in previ-   ous summarization works . The input document in   Gigaword contains 70 words , while CNN / DM con-   sists of 700 words on average . Hence , we can ex-   amine the effectiveness of our methods on datasets   of different distributions .   5.2 Comparison Methods   Our baselines include the following models :   BART ( Lewis et al . , 2020 ) is a state - of - the - art ab-   stractive summarization model pretrained with a   denoising autoencoding objective .   ProphetNet ( Qi et al . , 2020 ) is a pre - training   model that introduces a self - supervised n - gram pre-   diction task and n - stream self - attention mechanism .   R3F ( Aghajanyan et al . , 2021 ) is a robust text gen-   eration method , which replaces adversarial objec-   tives with parametric noise , thereby discouraging   representation change during fine - tuning when pos-   sible without hurting performance .   SSTIA ( Xie et al . , 2022 ) augments the dataset from   the target side by mixing the augmented decoder   inputs in the embedding layer .   5.3 Implementation Details   We implement our experiments in Huggingface on   NVIDIA A100 GPUs , and start finetuning based on   pretrained models facebook / bart - large . Concretely ,   there are 12 encoding layers in the encoder and the   decoder . The activation functions are set to GeLUs   and parameters are initialized from N(0,0.02 ) . We   use Adam optimizer with ϵas 1e-8 and βas ( 0.9 ,   0.98 ) . We used label smoothing of value 0.1 , which   is the same value as Vaswani et al . ( 2017 ) . Then   attacking candidate number Kis set to 10 based on   the parameter study . The learning rate is set to 3e-5 .   The warm - up is set to 500 steps for CNN / DM and   5000 for Gigaword . The batch size is set to 128   with gradient accumulation steps of 2 . Following   Xie et al . ( 2022 ) , the temperature in Equation 3 is   set to 0.1 for CNN / DM and 1 for Gigaword , and   the mixup ratio λin Equation 4 is set to 0.7 . We   set the attack budget to 1 % tokens for Gigaword   and 5 % tokens for CNN / DM , based on the con-   sideration of attacking performance and semantic   consistency . We use the original dataset plus the   augmented cases generated by SummAttacker as   our training dataset , where we also randomly add   30 % natural human errors to improve the under-   standing of noises . The training process takes about   8 hours and 4 hours for CNN / DM and Gigaword .   5.4 Evaluation Metrics   We first evaluate models using standard ROUGE   F1 ( Lin , 2004 ) . ROUGE-1 , ROUGE-2 , and   ROUGE - L refer to the matches of unigrams , bi-   grams , and the longest common subsequence , re-   spectively . We use BERTScore ( Zhang et al . , 2020 )   to calculate similarities between the summaries .   We further evaluate our approach with the fac-   tual consistency metric , QuestEval ( Scialom et al . ,   2021 ) following Chen et al . ( 2022 ) . It measures to   which extent a summary provides sufficient infor-   mation to answer questions posed on its document .   QuestEval considers not only factual information   in the generated summary , but also the information   from its source text , and then gives a weighted F1   score.6851   6 Experimental Results   6.1 SummAttacker Evaluation   Before reporting the summarization performance   boosted by our proposed dual augmentation strat-   egy , we first set up human and automatic metrics   to evaluate the quality of the generated adversar-   ial augmentation cases . For human evaluation , we   ask annotators to score the semantic and grammar   correctness of the generated adversarial and origi-   nal sequences , scoring from 1 - 5 following Jin et al .   ( 2020 ) and Li et al . ( 2020 ) . We randomly select   100 samples of both original and adversarial sam-   ples for human judges . Each task is completed by   three Ph.D. students . For automatic metric , fol-   lowing Li et al . ( 2020 ) , we use Universal Sentence   Encoder ( Cer et al . , 2018 ) to measure the semantic   similarity between the adversarial and the original   documents .   As shown in Table 2 , the adversarial samples ’   semantic and grammatical scores are reasonably   close to those of the original samples . The scores   are generally higher on Gigaword dataset than   CNN / DM . This corresponds to the setting that the   number of attacked words is larger on CNN / DM   dataset . The kappa statistics are 0.54 and 0.48   for semantic and grammar respectively , indicating   moderate agreements between annotators . For the   automatic evaluation , the high semantic similarity   demonstrates the consistency between the original   and attacked documents .   We also study the influence of the candidate num-   berKin SummAttacker . In Fig . 4 , all models per-   form worse when the input document is perturbed   by SummAttacker with a larger K , since a bet-   ter replacement word wcan be found in a larger   search space . From the viewpoint of generating   adversarial samples , it is not worth using a large K ,   because the time and memory complexity increase   withKas well . Thus , we use K=10 in our setting .   6.2 Robustness Evaluation   We next report the evaluation results of summa-   rization models when the input documents are per-   turbed by natural human errors ( noise ) and syn-   onym substitutions ( based on SummAttacker ) .   Robustness on Noisy Datasets . Humans make   mistakes when typing or spelling words , but they   have the capability of comprehensive reading to un-   derstand the document without being interrupted by   such noises . Thus , we first examine the robustness   of the recent summarization models against natural   human errors . Since we do not have access to a   summarization test set with natural noise , we use   the look - up table of possible lexical replacements   ( Belinkov and Bisk , 2018 ) , which collects natu-   rally occurring errors ( typos , misspellings , etc . ) .   We replace different percentages of words in the   Gigaword test set with an error if one exists in the6852   look - up table . We show the performance of classic   baseline BART , augmentation - based model SSTIA ,   and our model in Fig . 5 . Both baseline models   suffer a significant drop in all metrics when eval-   uated on texts consisting of different percentages   of noise . Our DASum model is more robust and   drops the least in all four metrics compared with   baselines . We also give an example in the first   row in Table 4 . Humans are quite good at under-   standing such scrambled texts , whereas existing   summarization models are still vulnerable to slight   perturbations and then fail to capture the gist of   the input document , due to the lack of robustness   enhancement training .   Robustness on Datasets Perturbed by Adver-   sarial Attacks . We next examine the robustness   of summarization models on the test datasets per-   turbed by adversarial attacks . For the Gigaword   dataset , we set attack budget ϵto be only 1 word   ( 1 % tokens ) , and for CNN / DM we set ϵto be 5 %   tokens of the input document .   The comparison of performance on attacked and   clean datasets is shown in Fig.6 . It can be seen   that despite the perturbation being only on a fewwords , all four baselines suffer a significant drop   in performance compared with their performance   on the clean test set . Specifically , the ROUGE-1   score of the latest SSTIA model drops by 4.01 on   Gigaword , and the average ROUGE score drops   by 7.33 for R3F model on CNN / DM dataset . This   highlights the vulnerability of the existing sum-   marization models and also demonstrates the ef-   fectiveness of our attacking model . Nevertheless ,   the drop percentage of our model is the least com-   pared with other baselines in all metrics . Specif-   ically , our model drops the least with only 2.22   and 0.28 decreases in ROUGE-2 and BERTScore   metrics , respectively , on the Gigaword dataset . We   show the detailed performance on attacked set in   Table 3 . Our model outperforms baselines on two   datasets in most metrics . Besides , we also ob-   serve that the summarization models of short docu-   ments are more vulnerable than those of long docu-   ments . One potential reason is that the summariza-   tion model is more dependent on each input word   when the input is shorter . When the input is longer ,   the importance of each word decreases , since the   model can resort to other sources to generate sum-   maries .   Ablation Study . We first investigate the influ-   ence of input space augmentation . As shown in   Table 3 , without the Lloss , the performance drops   the most . We also conduct diversity analysis on the   inputs after augmentation , corresponding to the vul-   nerability discussion in § 4 . The ratio of uncommon   words compared with the original common words   increases by 30 % , which directly verifies our as-   sumption that introducing variations in the training   dataset improves the robustness of the summariza-   tion model . Next , we study the effect of latent6853space augmentation . Specifically , the ROUGE-1   score of extractive summarization drops by 0.79   after the Lis removed . This indicates that the   model benefits from hidden states with more di-   versity in the training process . In addition , we   compare the decoder hidden states of DASum with   that of BART in Fig.2 . The deviation of paired   original and attacked hidden states in DASum is   effectively reduced ( Edrops from 2.5 to 1.9 in the   last layer ) . Thirdly , the performance of DASum   w / oLshows that dual consistency can also help   improve robustness . We also note that DASum is   always more robust than the other two baselines , in   regard to different attacking settings in Fig.5 .   7 Conclusion   In this paper , we investigate the robustness prob-   lem in the summarization task , which has not been   well - studied before . We first come up with a Sum-   mAttacker , which slightly perturb the input doc-   uments in benchmark test datasets , and causes a   significant performance drop for the recent sum-   marization models . Correspondingly , we propose   a dual data augmentation method for improving   the robustness , which generates discrete and virtual   training cases in the same meaning but with various   expression formats . Experimental results show that   our model outperforms strong baselines .   Limitations   We discuss the limitations of our framework as   follows :   ( 1 ) In this paper , we take an initial step on the   robustness of the summarization system by focus-   ing on word - level perturbations in the input docu-   ment . However , in practice , the robustness of the   summarization models is reflected in many other   aspects . For example , the summarization perfor-   mance towards sentence - level or document - level   perturbations is also a kind of robustness .   ( 2 ) Although DASum greatly improves the gen-   eration quality compared with other augmentation-   based models , it requires more computational re-   sources with respect to the augmented dataset con-   struction process . For large - scale datasets with   long text ( e.g. , BigPatent ( Sharma et al . , 2019 ) ) , it   is worth considering the time complexity of Trans-   former architecture . Acknowledgments   We would like to thank the anonymous re-   viewers for their constructive comments . The   work was supported by King Abdullah Univer-   sity of Science and Technology ( KAUST ) through   grant awards FCC/1/1976 - 44 - 01 , FCC/1/1976 - 45-   01 , REI/1/5234 - 01 - 01 , RGC/3/4816 - 01 - 01 , and   RGC/3/4816 - 01 - 01 ; .   References68546855ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   in limitation section   /squareA2 . Did you discuss any potential risks of your work ?   in limitation section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   in introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   in experiment section   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   in experiment section6856 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   in experiment section   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   in experiment section   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   in experiment section   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   in appendix   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   in appendix   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.6857