  Jonathan Rusert , Padmini Srinivasan   University of Iowa   { jonathan - rusert , padmini-srinivasan}@uiowa.edu   Abstract   Deep learning ( DL ) is being used extensively   for text classification . However , researchers   have demonstrated the vulnerability of such   classifiers to adversarial attacks . Attackers   modify the text in a way which misleads the   classifier while keeping the original meaning   close to intact . State - of - the - art ( SOTA ) attack   algorithms follow the general principle of mak-   ing minimal changes to the text so as to not   jeopardize semantics . Taking advantage of this   we propose a novel and intuitive defense strat-   egy called Sample Shielding . It is attacker and   classifier agnostic , does not require any recon-   figuration of the classifier or external resources   and is simple to implement . Essentially , we   sample subsets of the input text , classify them   and summarize these into a final decision . We   shield three popular DL text classifiers with   Sample Shielding , test their resilience against   four SOTA attackers across three datasets in a   realistic threat setting . Even when given the ad-   vantage of knowing about our shielding strategy   the adversary ’s attack success rate is < = 10 %   with only one exception and often < 5 % . Addi-   tionally , Sample Shielding maintains near orig-   inal accuracy when applied to original texts .   Crucially , we show that the ‘ make minimal   changes ’ approach of SOTA attackers leads   to critical vulnerabilities that can be defended   against with an intuitive sampling strategy .   1 Introduction   Text classifiers have become ubiquitous . Unfortu-   nately , they are subject to attacks from adversaries ,   typically executed using machine learning methods .   Attackers work by making small modifications to   the text that mislead the classifier . Adversarial at-   tackers are now a growing part of the ecosystem .   Like classifiers , attack algorithms have achieved   strong success due to advances in machine learn-   ing / deep learning . Current text attackers , likeTextFooler ( Jin et al . , 2020 ) and Bert - Attack ( Li   et al . , 2020 ) , are able to reduce near perfect classi-   fication accuracy down to 5 % . Additionally , these   attackers achieve this while perturbing ( changing )   only a small amount of the original text . This   helps preserve the original meaning so that humans   are able to understand the original message even   though classifiers are duped .   As a counter , classifier shielding techniques are   being explored . One such approach is adversarial   training where the classifier , assumed to have ac-   cess to the attacker , uses it to generate perturbed   texts - these are added to the classifier ’s training   data . While this leads to model resilience against   thatattacker it leaves the classifier open to attacks   by new attackers . Other defenses involve modify-   ing classifier structure to reduce the information   an attacker can glean from it ( Goel et al . , 2020 ) .   However , this type of reconfiguration will not be   possible if a third party classifier ( e.g. Google Per-   spective ) is leveraged . Even other approaches in-   volve modifying the input text during classification   time , but are currently limited to classifiers built   from specific masked language models ( Zeng et al . ,   2021 ) or rely on external synonym datasets ( Wang   et al . , 2021a ) . We propose a shielding technique   which is attacker - agnostic , does not require addi-   tional training / reconfiguration to the classifier , can   shield any classifier , does not require an external   data source , and can be used in a more realistic   threat setting . We refer to this as Sample Shielding .   Sample Shielding takes advantage of current con-   straints in SOTA attacks . Mainly , to preserve orig-   inal meaning , these make the minimal changes   needed to deceive the classifier . For example ,   BERT - Attack ( Li et al . , 2020 ) only perturbs up   to 16 % of text , and often far less ( e.g. 1.1 % ) for   some datasets . Thus , if we would look at the 84 %   to 99 % of text that is untouched our model would   be more likely to classify correctly . Hence , in Sam-   ple Shielding we take many samples of the input2716   text , classify these individually and combine their   decisions as an ensemble to classify the text . Our   contributions are as follows :   1 . We propose a new , intuitive shielding tech-   nique called Sample Shielding for text classifiers .   2 . We assess Sample Shielding under a realistic   threat model where the attacker can not query a   website ’s classifier hundreds of times since that   pattern is easily detectable by the website . We   run experiments under two conditions , when the   attacker has knowledge of Sample Shielding and   when it does not . In both cases the attacker uses   a local copy of the websites ’ classifier . This is an   optimistic assumption favouring the attacker and   thus provides a lower bound to our results .   3 . We test against 4 SOTA text attack algorithms ,   3 text datasets and 3 classifiers . When the attacker   does not have knowledge of Sample Shielding , our   defense reduces attack success rate from near total   decimation 90 - 100 % down to 13 - 36 % , while   still maintaining accuracy on original texts . When   the attacker has knowledge of Sample Shielding ,   our defense performs even better , reducing attacks   down to 1 - 10 % success rate . This is partially   due to Sample Shielding ’s random nature providing   unreliable feedback to attackers .   Our success with Sample Shielding is good news   for classifiers – and it raises the bar significantly for   the next generation attackers . We share code and   our perturbed text collections for future research .   2 Methodology   2.1 Threat model   The typical attack strategy perturbing texts with   word synonyms or character substitutions assumes   to have query access to the target web site ’s classi-   fier ( W ) ( Yoo and Qi , 2021 ; Li et al . , 2021a ; Ren   et al . , 2019 ; Jin et al . , 2020 ; Li et al . , 2020 ; Gargand Ramakrishnan , 2020 ; Jia et al . , 2019 ; Li et al . ,   2019 ) . The text is modified by querying Whun-   dreds or thousands of times , each time with a text   version differing only slightly from the previous -   even by just a single word ( Li et al . , 2020 ; Jin et al . ,   2020 ) . Such a querying pattern can be easily iden-   tified as adversarial by the website and countered .   Thus , practically the only way in which such an   attack can take place is when the attacker owns a   local classifier Wwhich is either an exact copy of   Wor a close enough approximation . We adopt this   more realistic threat model , shown in Figure 1 .   In our threat model the attacker uses feedback   from its local Wto generate a final perturbed ver-   sion that defeats Wor is close enough to do so .   The attacker submits only this final version to the   website , expecting Wto make the same error . How-   ever , the website defends Wusing Sample Shield-   ing : sample based pre - processing on the input text ,   prior to applying W. The attacker may or may not   be aware of this fact . Keeping W = Wwhich   is consistent with other defenses , we evaluate our   defense under two conditions :   1 ) The attacker does not know that the website   employs Sample Shielding pre - processing when   classifying text using W.   2 ) The Sample Shielding step is leaked and the   attacker incorporates it locally when using Wto   generate the final perturbed text .   We present results from experiments exploring   both of these attack conditions .   2.2 Sample Shielding approach   Intuition . Current adversarial attackers have two   goals : fool the classifier and maintain the original   meaning . Since they make minimal changes , the   extent of perturbation is in fact one of the reported   statistics . For example , ( Li et al . , 2020 ) note that   their 10 % perturbation rate is far less than in previ-   ous attacks . ( Li et al . , 2019 ) also focus on minimal   changes ( 4 % ) needed in support of their attack suc-   cess rate . Our defense approach capitalizes on this   drive to make minimal changes . Specifically , in   Sample Shielding , we take ksamples each com-   posed of p%of the text . We choose a pwhich min-   imizes the chance of a sample including attacked   ( modified ) words , while maximizing the content   available for the classifier to make a correct classi-   fication . We choose a kwhich is large enough to   cover key information but small enough to reduce   redundancy . We classify each sample and combine2717   their decisions for the final classification . We ex-   plore two sampling and three decision combining   methods .   2.2.1 Sampling methods   Random Sampling . We randomly sample ppor-   tions of the text . We explore both sentences and   words as sampled units . A visualization of random   sampling is in Figure 2 .   Shifting Sampling . We sample the text using a   moving window of length p×length _ of_text .   The first starts at the beginning of the text . The   next window starts right after the previous window   ends . If there is insufficient text for the last window ,   then it wraps back to include the beginning text .   2.2.2 Decision strategy   Majority voting . This is a simple majority vote   across the k samples ( Figure 2 ) .   Classifier trained on sample scores from original   texts ( NN ) . We train a neural network summarizer   to make a final class prediction based on the ksam-   ple probabilities . Since sample ID does not carry   any information , the input to the neural network   is a sorted list of sample probabilities . The intent   is to see if the neural network picks up on latent   patterns in the probabilities that are not captured by   majority voting ( see Figure 2 ) . It should be empha-   sized that the neural network summarizer is trained   only on probabilities generated from original texts   and does not consider probabilities from attacker   modified texts . We use a simple feed forward neu-   ral net composed of 2 linear layers ( size 500 and   300 ) as classification summarizer .   Classifier trained on sample scores from original   and attacked texts ( NN - BB ) . This is similar to   the previous strategy except that the training dataincludes scores from original texts and texts that   have been modified by the attacker . Because this   assumes more knowledge of the attacker we expect   NN - BB to perform better than NN . The ground   truth label for these modified texts is the original   correct class label .   3 Experimental Setup   3.1 Datasets   We examine three standard datasets in our experi-   ments . Two have binary class labels ( Yelp , IMDB )   and the third has multi class labels ( AG News ) .   These have been used in adversarial generation and   defense research ( Zeng et al . , 2021 ; Li et al . , 2020 ) .   All datasets can be found via huggingface .   1 . IMDB - Movie review dataset for binary sen-   timent classification . 25k examples are provided   for training and testing respectively .   2 . Yelp - Yelp dataset for binary sentiment clas-   sification on reviews of businesses extracted from   the Yelp Dataset Challenge . 560k examples are   provided for training and 38k for testing .   3 . AG News - News articles from over 2000   news sources annotated by type of news : Sports ,   World , Business , and Science / Tech . 120k training   and 7k test sets are provided .   Following previous research , ( Li et al . , 2020 ; Jin   et al . , 2020 ) we use all training data , and evaluate   our method on random 1k samples of each dataset   for the case where the local classifier does not em-   ploy Sample Shielding . Due to the high amount of   queries used by the adversaries , we test on a subset   of 100 samples for the case where the attacker’s2718local classifier employs Sample Shielding .   3.2 Adversarial models   We test our text classifier shielding strategy against   4 state - of - the - art ( SOTA ) text classifier attack al-   gorithms . These algorithms have shown excellent   performance in causing misclassifications while   still producing readable texts . We defend against   3 word based attacks : TextFooler ( Jin et al . , 2020 ) ,   Bert - Attack ( Li et al . , 2020 ) , PWWS ( Ren et al . ,   2019 ) . TextFooler leverages word embeddings for   word replacements , Bert - Attack leverages BERT   itself by masking words and using BERT sugges-   tions , PWWS selects and weights word replace-   ments from WordNet . All three use some form of   greedy selection for determining which words to   replace . We also defend against a character based   attack algorithm , TextBugger ( Li et al . , 2019 ) .   3.3 Victim classifier models   We test our shielding approach against 3 standard   classifiersused in previous research , e.g. ( Li et al . ,   2021a ; Jin et al . , 2020 ; Li et al . , 2020 ):   1 . CNN - A word based CNN ( Kim , 2014 ) , with   three window sizes ( 3,4,5 ) , 100 filters per window   with dropout of 0.3 and Glove embeddings .   2 . LSTM - A word based bidirectional LSTM   with 150 hidden units . As with the CNN a dropout   of 0.3 is used and Glove embeddings are leveraged .   3 . BERT - The 12 layer BERT base model which   has been fine - tuned on the corresponding dataset .   These are provided by textattack via huggingface .   3.4 Experimental design   We run experiments on the combination of the three   victim classification models , three datasets , and   four attack algorithms . These combinations are run   on both threat model conditions ( attacker is aware/   not aware of SampleShielding ) . This leads to 72   shielding experiments . For all attacks , we lever-   age TextAttack frameworkwhich provides classi-   fication algorithms and adversarial text generation   algorithms implemented as specified in respective   papers ( Morris et al . , 2020 ) . In all experiments   where the attacker does not use Sample Shielding   we set k= 100 andp= 0.3 . While better perfor-   mance was achieved with other values in prelim-   inary experiments , we chose to go with a single   combination of pandkfor simplicity . In exper-   iments where the attacker uses Sample Shielding   pre - processing we reduce kto30for efficiency . Ex-   cept where otherwise noted , majority voting is used   to generate results . Additionally , shifting sampling   ( Section 2.2.1 ) shielding typically achieved 10 - 20   points lower accuracy compared to random , thus   we do not include it in the results .   3.5 Evaluation measures   We examine accuracy and Attack Success Rate :   accuracy = # examples _ classified _ correctly   # total _ examples   ( 1 )   ASR = Original−Attacked   Original(2 )   4 Results   We first present results for the condition where the   attacker is not aware of Sample Shielding based   pre - processing and then the results for when the   attacker also employs Sample Shielding .   4.1 Condition 1 : Attacker does not know   about Sample Shielding   Results are in Table 1 . BERT is the strongest clas-   sifier achieving 91 - 100 % accuracy on the original   datasets . Attacks are highly successful against un-   shielded texts . TextFooler and Bert - Attack are the   most successful , dropping accuracies to 0 - 5 % gen-   erally . Attacks were able to achieve strong drops   with minimal amount of text perturbed ( about 10 % ) .   Figure 3 shows that the average percent of words   perturbed across datasets for each attack are about2719equal in the mid regions of the plots . For AG News ,   attacks are less successful against BERT ; accuracy   drops to 19 % in the strongest attack ( TextFooler ) ,   and only to 49 % in the weakest ( TextBugger ) . In   general , TextBugger , the character - based attacker ,   is the least effective attacker .   Sample Shielding greatly reduces effectiveness   of attacks while maintaining accuracy on orig-   inal texts . The shielded classifier Wmaintains   accuracy on original texts to within 7 % of the orig-   inal accuracy . Crucially , for attacked texts we see   accuracy improve to between 60 and 80 % ( from   post attack range of 0 - 5 % generally ) . For example ,   TextFooler causes BERT ’s accuracy to drop from   91 % to 1 % for IMDB , however , Sample Shield-   ingreturns accuracy to 78 % . In other words , the   effectiveness of the attack is reduced from 99 % ef-   fective to 14 % effective . Additionally , accuracy on   the original texts is maintained ( 91.3 to 91.5 ) . This   pattern is seen in the other attack classifier models   and dataset combinations as well . For Yelp , LSTM   drops from 92.5 to 0.7 when attacked by BERT-   Attack , however , Word sampling brings it back up   to 66.7 , while achieving an original accuracy of   87.8 . Overall , accuracy after shielding ranges from   60 to 80 % ( avg : 70 ) , which corresponds to a 13 -   36 ( avg : 25 ) attack success rate .   Sample Shielding effective against both word   based and character based attacks . The results   show effectiveness regardless of type of attack   ( word or character based ) . For example , all 4 at-   tacks bring the original accuracy of LSTM from   88.3 down to ∼0 for IMDB . However , word sam-   pling brings the accuracy back up to ∼66 . This is a   great reduction in attack effectiveness . Again , sim-   ilar trends are seen for the other classifiers , CNN   is reduced from 94.1 to ≤5.5 for Yelp , but word   sampling brings it back up to 60 - 70 % .   Word sampling outperforms sentence sampling   for LSTM , CNN , sentence sampling better for   BERT . For example , for CNN on IMDB , word   sampling increases accuracy more than 15 points   over sentence sampling ( 69.8 vs 53.3 ) . Similar   trends hold for LSTM . However , the opposite is   seen for BERT classifiers . For BERT on IMDB ,   we see an average of 6.5 higher points for sentence   sampling over word sampling . These results are not   surprising as LSTM and CNN leverage word em-   beddings for classification , while BERT leverages   the context of the entire sentence .   Word sampling is more appropriate for shorttexts . With AG news , we see a large drop in effec-   tiveness of sentence sampling . The average length   of AG News is 43 words compared to 157 and 215   words of Yelp and IMDB respectively ( Li et al . ,   2020 ) . This shorter length makes it more difficult   to sample enough sentences . For Textfooler - CNN ,   sentence sampling is only able to increase accuracy   from the attacked value of 0.4 to 13.2 . However ,   word sampling is much more effective , increasing   accuracy to 77.3 . Text length may be crucial when   choosing between the two strategies for a dataset .   Neural Network summarizer shows some im-   provements over majority voting . Comparisons   of majority voting and the two neural net - based de-   cision strategies are in Table 2 . We experimented   on the two binary datasets . Replacing majority   voting with a simple neural net ( NN ) gave some-   what disappointing results - accuracies stay the   same or decrease slightly in all cases except for   LSTM on the Yelp dataset ( increases ) . However ,   when the neural nets are trained on perturbed texts   ( NN - BB ) , we see increases . For example , CNN   vs TextFooler on Yelp , the neural net increases ac-   curacy from 64.9 to 72.2 , reducing attack success   rate from 31 to 23 . Possibly a more sophisticated   neural net , such as a sequence aware LSTM , might   better exploit patterns in the sorted probabilities .   4.2 Condition 2 : Attacker knows about   Sample Shielding   Results are in Table 3 . As in the previous condition ,   classifiers perform well on original texts ( Table 1 )   with BERT often achieving the highest accuracies .   In this setting , every query by an attacker requires   ksamples to be processed , which greatly increases   attack time . Thus , we reduce kto 30 for these   experiments .   Sample Shielding repels attacks even when at-   tacker uses Sample Shielding .We see that shield-   ing is extremely successful in almost completely   removing the negative effects of the attacks . For   example , on the IMDB - TextFooler combination ,   attack success rate drops from 100 to 5 for LSTM ,   100 to 1 for CNN , and 99 to 6 against BERT . The   largest protection provided by Sample Shielding   ( 100 % ) is for TextBugger vs CNN in IMDB . The   smallest is for 85 % ( PWWS vs LSTM ) . On average   the protection is 88.8 % . The recovered accuracies   are only 13 to 0 percent away from the originals.27202721   These results show the power of Sample Shielding   as even with knowledge of both the classifier and   Sample Shielding , attacks struggle to perturb the   text in a manner that causes Wto fail . Furthermore ,   the attacks do worse with feedback from Sample   Shielding . This shows the misleading nature of   feedback from Sample Shielding , and unreliability   when guiding attacks .   5 Additional Analysis   5.1 Parameter search   Increasing praises the risk of samples containing   increased amounts of perturbed text . Decreasing   kraises the risk of not covering enough of the   unperturbed portions of the original text . While   our settings of p= 0.3andk= 100 for our main   results are reasonable values ( Table 1 , Table 2 ) they   are not necessarily optimal .   Optimal p. Figure 4 shows the results for all com-   binations of attacks against LSTM on IMDB with   word shielding as the defense , kfixed at 100 . As   we increase p , we see a continued drop in accuracy   which is consistent with the idea that a higher pis   more likely to capture perturbed text . The optimal   value range appears to be in 0.2 - 0.4 range , al-   though we do not see large drops until 0.6 onward .   We also examined the same combination on AG   News ( Figure 5 ) since it ’s texts are considerably   shorter and found consistent results .   Optimal k. Figure 6 shows results for all attacks   against LSTM on IMDB with word sampling as   the defense , pfixed at 0.3 . The optimal kis not   as clear as p. We see clear increases after 30 sam-   ples , but then the optimal kvaries depending on   attack . However , we see a leveling off around 90   samples , which gives some credence to our chosen   kof 100 . We also found similar results when exam-   ining the same combination on AG News ( Figure   7 ) , however , kstabilized lower ( about 50).2722   5.2 Reliability of Sample Shielding   Due to the randomness of samples , there may be   concern over the consistency of Sample Shielding .   To address this , we ran Sample Shielding 100 times   on the IMDB attacked texts from Table 3 against   BERT classifier . Each time 30 random samples   were used to vote . As can be observed from Figure   8,Sample Shielding consistently protects against   attacks . Median accuracies are above 80 % drop-   ping only to 75 % in the worst case . This points to   Sample Shielding as a consistent , reliable defense .   5.3 Comparison with other SOTA Defenses   Comparisons are limited as threat models differ .   As noted earlier , other defenses assume a weaker   threat model where the attacker queries the web - site ’s shielded Wdirectly . To make ours equiv-   alent we compare SOTA results with our accura-   cies obtained by the attacker using Walone ( with   W = W ) . We calculate accuracies right after the   final perturbed text is generated using Weliminat-   ing a followup round of Wwith Sample Shielding .   Table 4 provides our full results against this weaker   threat model .   With BERT as base classifier for AG News ,   FreeLB++ , an adversarial training technique ( Li   et al . , 2021b ) report accuracies of 51 , 56 , and 42   against TextFooler , TextBugger , and Bert - Attack   respectively . RanMask ( Zeng et al . , 2021 ) , which   uses random masking of words report accuracies of   38 , 45 , and 49 . In comparison , Sample Shielding   achieves 48 , 55 , and 38 respectively outperforming   RanMask in 2 out of 3 , while only a fews point   behind FreeLB++ . For IMDB , FreeLB++ reports   45 , 43 , and 40 and RanMask reports 22 , 18 , and   36 respectively . Equivalently , Sample Shielding   achieves 18 , 34 , and 31 . With some wins and some   losses , Sample Shielding is in the mix with current   SOTA defenses in this weaker threat model . How-   ever , when deployed as designed for the realistic   threat model , it wins over these other defenses by   large margins ( see Table 3 ) . While we do not know   how FreeLB++ , RanMask , and similar defenses   would perform with our threat model any determin-   istic shield would give the exact same results when   the classifier is applied once again by the website .   5.4 Limitations / Future work   First , in future work we will add in direct compar-   isons to the two closest methods to Sample Shield-   ing(Zeng et al . , 2021 ; Wang et al . , 2021a ) . They   are similar in spirit as they also work off samples   though these are generated differently . We have not   compared with them because these two papers ap-   peared very recently , one last revised in July ( Zeng   et al . , 2021 ) and the other appeared in arXiv in   September 2021 ( Wang et al . , 2021a ) . Second , the   neural net summarizer leverages a simple linear   layer . Other networks , e.g. , LSTM , maybe better at   finding patterns in sequential data . In future work   we will also explore layering Sample Shielding   onto other defense strategies .   Another limitation of our current method is that   we do not measure Sample Shielding ’s effective-   ness on other common text tasks including Natural   Language Understanding . Additionally , datasets   which contain the shortest texts ( e.g. SST2 ) are2723   not currently tested in our experiments . Since sam-   ple shielding removes texts , it ’s performance could   drop for these tasks and short texts . Thus , future   work will include these comparisons .   6 Related Work   Defenses using voting . The most similar methods   to our own are RanMask and RS&V both appear-   ing within the last five months . RanMask ( Zeng   et al . , 2021 ) randomly masks tokens in input texts .   This random masking occurs ntimes generating   ninputs to be fed to a classifier . RS&V ( Wang   et al . , 2021a ) randomly replaces words in the input   with synonyms . This it does ktimes to produce k   samples which are then voted on . If the samples   vote for a different label than the label produced by   the unsampled input , then the text is labeled as an   adversarial text . Our method is advantageous since   it does not rely on specific models ( i.e. Masked   Language Model ) or synonym sources .   Adversarial training . Classifiers train on per-   turbed data , learning to identify modified versions   of the original input ( Wang and Wang , 2020 ; Wang   et al . , 2021b ; Zhu et al . , 2020 ; Li et al . , 2021b ) .   As an example , Gil et al . ( 2019 ) propose HotFlip   which uses white - box knowledge to generate ad-   versarial attacks to train on . Specifically , they flip   tokens based on the gradients of the one - hot input   vectors . However , adversarial defenses are limited   to known attackers . In contrast , Sample Shielding   is ‘ plug - and - play ’ as it is a pre - processing step .   Other defenses . Several other shielding methods   exist ( Keller et al . , 2021 ; Eger et al . , 2019 ; Zhu   et al . , 2021 ) . For example , Rodriguez and Galeano   ( 2018 ) defend Perspective ( Google ’s toxicity clas-   sification model ) by neutralizing adversarial inputs   via a negated predicates list . Again , these defensesare restricted to contexts where specific lists may   be identified , this is not so with Sample Shielding .   7 Conclusion   Sample Shielding , an intuitively designed defense   which is attacker and classifier agnostic , protects   effectively ; reducing ASR from 90 - 100 % down   to 14 - 34 % with minimal accuracy loss ( 3 % ) in   original texts . The randomness ( through sampling )   provides unreliable feedback for attackers , thus it   even thwarts attackers who have query access to   classifiers protected with Sample Shielding . Attack   strategies will need to increase the amount of per-   turbation to make sure a majority of samples fail at   classification . However , this will risk semantic in-   tegrity . Thus , we expect Sample Shielding to cause   ripples in future adversarial attack strategies while   providing text classifiers with a definite advantage .   References27242725