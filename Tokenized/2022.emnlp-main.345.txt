  Jun - Yu Ma , Beiduo Chen , Jia - Chen Gu , Zhen - Hua Ling , Wu Guo   Quan Liu , Zhigang Chen , Cong LiuNational Engineering Research Center of Speech and Language Information Processing ,   University of Science and Technology of China , Hefei , ChinaState Key Laboratory of Cognitive IntelligenceiFLYTEK Research , Hefei , ChinaJilin Kexun Information Technology Co. , Ltd.   { mjy1999,beiduo}@mail.ustc.edu.cn , { gujc,zhling,guowu}@ustc.edu.cn ,   { quanliu,zgchen,congliu2}@iflytek.com   Abstract   Zero - shot cross - lingual named entity recog-   nition ( NER ) aims at transferring knowledge   from annotated and rich - resource data in source   languages to unlabeled and lean - resource data   in target languages . Existing mainstream meth-   ods based on the teacher - student distillation   framework ignore the rich and complementary   information lying in the intermediate layers   of pre - trained language models , and domain-   invariant information is easily lost during trans-   fer . In this study , a mixture of short - channel   distillers ( MSD ) method is proposed to fully   interact the rich hierarchical information in the   teacher model and to transfer knowledge to   the student model sufficiently and efficiently .   Concretely , a multi - channel distillation frame-   work is designed for sufficient information   transfer by aggregating multiple distillers as   a mixture . Besides , an unsupervised method   adopting parallel domain adaptation is pro-   posed to shorten the channels between the   teacher and student models to preserve domain-   invariant features . Experiments on four datasets   across nine languages demonstrate that the   proposed method achieves new state - of - the - art   performance on zero - shot cross - lingual NER   and shows great generalization and compatibil-   ity across languages and fields .   1 Introduction   Named entity recognition ( NER ) is a fundamental   and important task to locate and classify named   entities in a text sequence . Recently , deep neural   networks have achieved great performance on   monolingual NER in rich - resource languages with   abundant labeled data ( Ye and Ling , 2018 ; Jia   et al . , 2020 ; Chen et al . , 2022 ) . However , it is   too expensive to annotate a large amount of data   in low - resource languages for supervised NERFigure 1 : Comparison between the previous mainstream   method and the proposed MSD . ( a ) Baseline Distil-   lation is the teacher - student distillation framework .   The teacher model is utilized to predict the soft labels   of unlabeled target language data , which are further   utilized to distill a student model . ( b ) MSD constructs a   dozen of channels and shortens the transmission route   between the teacher and student models to transfer NER   knowledge . Θ / Θ : teacher / student models ; D/   D : unlabeled source / target language data .   training . This issue drives research on cross - lingual   NER , which utilizes the rich - resource annotated   data in source languages to alleviate the scarcity   of unlabeled lean - resource data in target languages .   In this paper , following Wu et al . ( 2020a ) , we focus   on the extremely low - resource setting , i.e. , zero-   shot cross - lingual NER , where labeled data is not   available in target languages .   The most popular approaches for zero - shot cross-   lingual NER are based on distillation ( Wu et al . ,   2020a , b ; Chen et al . , 2021 ; Liang et al . , 2021 ) .   They employed a supervisedly trained teacher   model to predict the soft labels of target languages ,   and then utilized the soft labels to distill a student   model , which was first exploited in Wu et al .   ( 2020a ) . Besides , domain - invariant features have   been proven effective for distillation ( Nguyen-   Meidine et al . , 2020 ; Hu et al . , 2019 ) . Chen et al .   ( 2021 ) proposed to alleviate the representation   discrepancy between languages in the teacher   model to exploit language - independent features,5171which were further distilled to the student model .   It is worth noting that distillation - based methods   assisted with auxiliary tasks have become the   mainstream paradigm due to their robustness and   scalability , achieving good performance on zero-   shot cross - lingual NER ( Li et al . , 2022 ) .   However , these methods always ignored the rich   and complementary information lying in the in-   termediate layers of multilingual BERT ( mBERT )   ( Devlin et al . , 2019 ) . Pires et al . ( 2019 ) and   Müller et al . ( 2021 ) have verified that the upper   layers of mBERT are more task - specific and not   as important as the lower ones in terms of cross-   language transfer . But recent studies just adopted   the last layer of mBERT for distillation ( Wu et al . ,   2020b ; Li et al . , 2022 ) , while neglected the explicit   knowledge transfer of the lower layers . Besides ,   domain - invariant features in the teacher model   were first exploited , which were then transferred   to the student model via distillation ( Chen et al . ,   2021 ) . However , due to the limitation of transfer   learning , it is difficult to fully retain the domain-   invariant features to the student model . Further-   more , auxiliary tasks to assist distillation usually   require the operations of translation or data sifting   ( Wu et al . , 2020b ) , resulting in huge pre - processing   costs . The ensemble strategies to generate high-   quality soft labels or augmented data also require   oceans of model parameters and a large number of   computational resources .   On account of the above issues , a mixture of   short - channel distillers ( MSD ) method is proposed   in this paper to transfer cross - lingual NER knowl-   edge sufficiently and efficiently . On the one hand , a   multi - channel distillation framework is designed to   let the hierarchical information in the teacher model   fully interact with each other , and transfer more   complementary information to the student model .   Specifically , the teacher model is first trained on the   annotated source data with each layer being directly   supervised by the labels . Then , each layer of the   teacher model is tasked to predict the soft labels   of the unlabeled target data . Correspondingly ,   the layers of the student model are distilled by   leveraging the mixture sets of soft labels from the   teacher model , constructing multiple information   transmission channels for a “ wider ” bridge between   the teacher and student models . On the other hand ,   an unsupervised auxiliary task of parallel domain   adaptation is proposed to explicitly transfer domain   information . During every batch of distillation , as Figure 1 depicts , unlabeled target data is fed   into the student model , while unlabeled source   data is fed into both the teacher and student   models . The representation discrepancy between   the outputs of the teacher and student source   language , together with that between the outputs of   the teacher source and student target languages is   minimized to preserve the cross - model and cross-   language domain information respectively . In this   way , the domain information can be preserved   across models and languages , so that the domains   of the teacher and student models can be effectively   pulled “ closer ” .   Experiments on four datasets across nine lan-   guages are conducted to evaluate the effectiveness   of the proposed MSD . The results show that our   method achieves new state - of - the - art performance   on all datasets .   In summary , our contributions are as follows :   ( 1 ) A multi - channel framework is proposed to   leverage the rich , hierarchical and complementary   information contained in the teacher model , and to   interactively transfer cross - lingual NER knowledge   to the student model . ( 2 ) An unsupervised auxiliary   method is designed to explicitly constrain the   discrepancy of teacher / student domains without   utilizing any external resources . ( 3 ) Experiments   on four datasets across nine languages verify the   effectiveness and generalization ability of MSD .   2 Related Work   Zero - shot Cross - lingual NER Existing meth-   ods on zero - shot cross - lingual NER are mainly   separated into three categories : translation - based ,   feature - based , and distillation - based . Translation-   based methods generate pseudo labels for the target   language data from the labeled source language   data . Jain et al . ( 2019 ) projected labels from the   source language into the target language by using   entity projection information . Xie et al . ( 2018 )   and Wu et al . ( 2020b ) translated the annotated   source language data to the target language word-   by - word . Feature - based methods use the labeled   source language data to train the language model   for a language - independent representation , such   as Wikifier features ( Tsai et al . , 2016 ) , aligned   word representations ( Wu and Dredze , 2019 ) ,   and adversarial learning encodings ( Keung et al . ,   2019 ) . Distillation - based methods are effective   in cross - lingual NER by transferring knowledge   from a teacher model to a student model ( Hinton5172et al . , 2015 ) . The teacher model is first trained   on the labeled source language data . Then the   student model is trained on soft labels of the   target language data predicted by the teacher model .   Wu et al . ( 2020a ) trained several teacher models   to generate averaged soft labels for the student   model . Liang et al . ( 2021 ) proposed a reinforced   knowledge distillation framework to selectively   transfer useful information .   Domain Adaption Label sparsity causes domain   shift ( Ben - David et al . , 2010 ) in zero - shot cross-   lingual NER . The strategy of cross - domain trans-   fer ( Qin et al . , 2020 ; Zhang et al . , 2021a ; Huang   et al . , 2021 ) is widely adopted . Existing methods   mitigate the discrepancies of sentence patterns   between the source and target domains , mainly   including multi - level adaptation layers ( Lin and   Lu , 2018 ) , tensor decomposition ( Jia et al . , 2019 ) ,   multi - task learning ( Liu et al . , 2020b ) and word   alignment ( Lee et al . , 2021 ) . However , these   methods require sufficient labeled data , in contrast   to zero - shot scenarios .   Generally speaking , previous studies on zero-   shot cross - lingual NER only leverage the last layer   of the teacher model . Besides , the existing NER   domain adaptation strategies only constrain the   domain - invariant information within the teacher   model and transfer them to the student model   implicitly . To the best of our knowledge , this   paper makes the first attempt to let the rich and   hierarchical information in the teacher model fully   interact with each other , and further transfer the   domain information to the student model explicitly .   3 Preliminary   3.1 Problem Definition   NER is typically formulated as a sequence labeling   task . Denote one sentence as x={x}with   its labels y={y } , where ydenotes the label   of its corresponding word xandLdenotes the   length of the sentence . An NER model generates   a sequence of predictions ¯y={¯y } , where ¯y   denotes the label of xannotated by the model .   The labeled data D={(x , y)}is available for   the source language , while the unlabeled D=   { x}and labeled D={(x , y)}are available for   target languages . Formally , zero - shot cross - lingual   NER aims at achieving good performance on D   by leveraging both DandD.3.2 Basic Model   The basic model for cross - lingual NER in this   paper consists of a semantic encoder and a classifier .   The encoder fis used to learn and generate the   contextual representations of input sentences .   Following Wu and Dredze ( 2019 ) , the widely-   used multilingual pre - trained language model ,   mBERT , is utilized as the encoder to extract   semantic representations . A softmax classification   layer is appended to calculate the probability .   Finally , the basic model is formulated as follows :   H = f(x ) , ( 1 )   p(x ; Θ ) = softmax ( W·h+b ) , ( 2 )   where H={h}andhis the representation   ofx.p(x ; Θ)∈Rwith Cbeing a set of   entity labels , and Θ = { f , W , b}denotes all the   parameters to be learned .   3.3 Maximum Mean Discrepancy ( MMD )   MMD ( Long et al . , 2015 ) is a nonparametric   test statistic to measure the distance between the   distributions of two different random variables   ( P , P ) . MMD is defined in particular function   spaces as follows :   where Fis the unit ball in a universal Repro-   ducing Kernel Hilbert Space ( RKHS ) denoted   byH. An important property of MMD is that   MMD ( F , p , p ) = 0 if and only if P = P.   Given the source and target sample sets S=   { s}andT={t}respectively , where s   ortdenotes a sample of the set , the empirical   estimation of MMD can be defined as :   where ϕ ( · ) :X → H is a nonlinear mapping .   In cross - lingual NER , the squared formulation   of MMD between the representations ( horh ) of   the two sets is usually calculated as :   where Gis a Gaussian kernel in this paper.5173   4 Methodology   In this section , we present the detailed frame-   work of the proposed mixture of short - channel   distillers ( MSD ) . On the one hand , the mixture   of distillers module is introduced . Specifically ,   multiple channels are built between corresponding   layers of the teacher and student ’s encoders . Then   a mixture of weights is employed to control the   broadened information transmission route . On the   other hand , parallel domain adaptation is conducted   to explicitly transfer domain information between   the teacher and student models during distillation .   4.1 Mixture of Distillers   Previous studies have verified the importance of   lower layers for cross - language transfer ( Müller   et al . , 2021 ) . Our pilot experiments also further   illustrate that lower layers of mBERT are critical   for NER , which are elaborated in Appendix A.2 .   To this end , we propose the mixture of distillers   framework that fully transfers the complementary   information to the student model to equip it with   stronger cross - language NER ability . To establish   multiple information transmission channels , each   layer of the pre - trained mBERT is appended with   a classifier . Denote each of these classifiers as a   “ channel terminal ” , as shown in Figure 2 . Given asentence xof length Lwith labels yfrom source   language data D , these could be described as :   H = f(x ) , ( 6 )   p(x ; Θ ) = softmax ( W·h+b),(7 )   where His the sentence representation from   them - th layer of mBERT and p(x ; Θ ) is the   probability distribution generated from the corre-   sponding channel terminal .   At the training stage for the teacher , the language   model along with several channel terminals are   jointly trained on the labeled source language data .   Specifically , the channel terminal of the last layer   is employed as the main distiller and the others   are employed as the auxiliary ones in charge of   providing complementary information . Following   Wu et al . ( 2020a ) , the embedding layer and the   bottom three layers of mBERT in the teacher and   student models are frozen . So only the top nine   layers of the teacher are optimized as :   L=1   L / summationdisplayL / parenleftbig   p(x ; Θ ) , y / parenrightbig   , ( 8)   L=1   L / summationdisplay / summationdisplayλL(p(x ; Θ ) , y ) ,   ( 9)5174where λ∈Ris a trainable parameter represent-   ing the contribution degree of the m - th layerand   Lis cross entropy loss . The final loss for the   teacher model is denoted as :   L = L+αL , ( 10 )   where αis a manually set hyperparameter that   regulates the contribution of the auxiliary layers .   For the following knowledge distillation , a stu-   dent model Θis distilled based on the unlabeled   target language data D. In this paper , the   student model has the same structure as the teacher   model . Firstly , Dis fed into the teacher model   Θto obtain its soft labels derived from all the   appended channel terminals . Then , as shown in   Figure 2 , each layer of the student model can be   trained along with the student channel terminals   using the mixture of soft labels generated from the   corresponding layer of the teacher model . Given a   sentence xof length LfromD , the distillation   loss of the m - th layer is as follows :   ( 11 )   Following Eq . ( 10 ) , the loss for the multi - channel   distillation is as follows :   L = L+βL   = L+/summationdisplayλL,(12 )   where λandβhave the same effect on the student   model as λandαdo on the teacher model .   4.2 Parallel Domain Adaptation   As aforementioned , the teacher model is trained   with hard - labeled source data , but the student   model is trained with soft - labeled target data . Thus ,   training in different manners and languages leads   to a huge discrepancy between the domains of   the teacher and student models , which causes the   loss of domain information during distillation and   decreases the transfer efficiency .   In this section , we aim to explicitly transfer   domain information to provide a closer route fordistillation . The parallel domain adaptation method   based on MMD is proposed to preserve domain   information between the teacher and student mod-   els at sentence - level . As Figure 2 depicts , the   cross - model and cross - language MMD losses are   proposed to minimize the cross - model and cross-   language discrepancies respectively , which are   denoted as L andL . During distillation ,   the soft labels D andD are obtained by   applying the teacher and student models to the   source language data respectively . The L   could be formulated as :   ( 13 )   where Hdenotes a set of [ CLS ] token embed-   dingsh . Meantime , the soft labels D is   obtained by applying the student model to the   unlabeled target language data . The L is   formulated as :   ( 14 )   Thus , the discrepancies between the teacher and   student models , as well as between the source   and target languages are both reduced during   distillation , strengthening the domain adaptability   of the proposed framework for efficient transfer .   The training for the final student model contains   two parts : the mixture of distillers and the parallel   domain adaptation . The final loss is denoted as :   L = L+αL + βL , ( 15 )   where αandβare the weights to balance the   contributions of the parallel adaptation methods .   5 Experiments   In this section , the proposed MSD was evaluated   on four zero - shot cross - lingual NER datasets and   compared with several state - of - the - art models .   Some ablation studies were also conducted to   validate the effectiveness of the proposed modules .   5.1 Datasets   We conducted experiments on these widely - used   benchmark datasets : ( 1 ) CoNLL-2002 ( Sang , 2002 )   included Spanish and Dutch ; ( 2 ) CoNLL-2003   ( Sang and Meulder , 2003 ) included English and   German ; ( 3 ) WikiAnn ( Pan et al . , 2017 ) included   English and three non - western languages ( Arabic ,   Hindi , and Chinese ) ; ( 4 ) mLOWNER ( Malmasi5175et al . , 2022 ) included four languages ( English ,   Korean , Farsi , and Turkish ) . CoNLL-2002 and   CoNLL-2003 were annotated with 4 entity types :   LOC , MISC , ORG , and PER . WikiAnn was anno-   tated with 3 entity types : LOC , ORG , and PER .   mLOWNER was annotated with 6 entity types :   LOC , ORG , PER , CW , GRP , PROD .   In this study , the CoNLLand the WikiAnn   datasets were just the same as they were initially   published . As for the mLOWNERdataset , fol-   lowing Malmasi et al . ( 2022 ) , 10000 sentences   were randomly sampled from the original test   set to construct the test set used in this paper .   All datasets were annotated with the BIO entity   labelling scheme and were divided into the training ,   development and testing sets . Table 1 shows the   statistics of all datasets .   Following the previous work ( Wu et al . , 2020a ) ,   English was employed as the source language in   all experiments , and the other languages were   employed as target languages . Only unlabeled   target language data in the training set was utilized .   5.2 Evaluation Metrics   Following Sang ( 2002 ) , entity - level F1 - score was   used as the evaluation metric . Denote Aas the   number of all entities classified by the model , Bas   the number of all correct entities classified by the   model , and Eas the number of all correct entities ,   the precision ( P ) , recall ( R ) , and entity - level F1-   score ( F1 ) of the model were :   P = B   A , R = B   E , F1 = 2×P×R   P+R.(16 )   5.3 Baselines   The proposed method was mainly compared with   the following ( 1 ) distillation - based methods : TSL   ( Wu et al . , 2020a ) , Unitrans ( Wu et al . , 2020b ) ,   AdvPicker ( Chen et al . , 2021 ) , RIKD ( Liang et al . ,   2021 ) , and MTMT ( Li et al . , 2022 ) , and ( 2 ) non-   distillation - based methods : Wiki ( Tsai et al . , 2016 ) ,   WS ( Ni et al . , 2017 ) , BWET ( Xie et al . , 2018 ) ,   Adv ( Keung et al . , 2019 ) , BS(Wu and Dredze ,   2019 ) and TOF ( Zhang et al . , 2021b ) . Readers   can refer to Appendix A.1 for the implementation   details of the baseline models .   5.4 Implementation Details   All code was implemented in the PyTorch frame-   work , and is published to help replicate our re-   sults . All of the feature encoders mentioned in this   paper employed pre - trained cased mBERT ( Devlin   et al . , 2019 ) in HuggingFace ’s Transformers where   the number of transformer blocks was 12 , the   hidden layer size was 768 , and the number of self-   attention heads was 12 .   Some hyperparameters were empirically set   following Wu and Dredze ( 2019 ) . Each batch   contained 32 examples , with a maximum encoding   length of 128 . The dropout rate was set to   0.1 , and AdamW ( Loshchilov and Hutter , 2019 )   with WarmupLinearSchedule in the Transformers   Library ( Wolf et al . , 2020 ) was used as optimizer .   The parameters of the embedding layer and the   bottom three layers of the mBERT used in the   teacher model and the student model were frozen .   Following Keung et al . ( 2019 ) , the other hyper-5176   parameters were tuned on each target language   dev set . All models were trained for 10 epochs   and chosen the best checkpoint with the target dev   set . For the training of teacher model , the learning   rate was set to 5e-5 , and the hyperparameter αin   Eq . ( 10 ) was set to 0.05 . For knowledge distillation ,   keeping the learning rate 2e-5 for the student   models and the hyperparameter βwas set to 0.05 in   Eq . ( 12 ) , αandβwere all set to 0.001 in Eq . ( 15 ) .   Furthermore , each experiment was conducted 5   times and reported the mean F1 - score .   The number of parameters in a teacher or student   model was about 111M. The whole training of   MSD was implemented with one GeForce RTX   3090 and consumed about 3 hours .   5.5 Results and Comparisons   Table 2 , 3 and 4 reported the zero - shot cross-   lingual NER results of different methods on 4   datasets , containing 9 target languages . The   results show that the proposed MSD method   significantly outperformed the baseline method   TSL and achieved new state - of - the - art performance   on all target languages . For results on CoNLL ,   MSD outperformed MTMT ( previous SOTA ) by   absolute margins of 0.76 % and 1.70 % in terms   of German[de ] and Dutch[nl ] respectively . As for   results of non - western languages on WikiAnn and   mLOWNER , MSD outperformed MTMT and Ad-   vPicker by marked absolute margins from 2.41 % to   10.11 % for all target languages . The results clearly   demonstrated the effectiveness and generalization   ability across languages and datasets of MSD .   Obviously , existing distillation - based methods   were outperformed by the proposed MSD . Specifi-   cally , translation and ensemble of teacher models   for high - quality soft labels in Unitrans and Ad-   vPicker , as well as iterative knowledge distillation   in RIKD requiring huge computational resources ,   were not adopted in MSD any more . Instead , the   proposed MSD fully explored the rich hierarchical   information in the teacher model without ensemble ,   and only utilized the unsupervised data without   extra data - process .   Besides , AdvPicker shortened the gap between   the source and target languages to derive the   language - independent features in the teacher5177   model , and then distilled the domain information   to the student model implicitly . However , the   proposed MSD chose to transfer the domain-   invariant information directly from the teacher   to the student via the parallel domain adaptation .   The results demonstrated that the implicit domain   transfer in AdvPicker is overshadowed by the   explicit domain transfer in MSD . As shown in   Figure 3 , domain discrepancy between the teacher   and student models is vividly reduced by MSD ,   contributing to a closer transfer route . Further   analysis of the difference between the domain   transfer manners of AdvPicker and MSD was   elaborated in Section 5.6.3 .   5.6 Analysis   5.6.1 Ablation Study   To validate the contributions of different compo-   nents in MSD , the following variants and baselines   were conducted to perform the ablation study : ( 1 )   MSD w/o . distillers , which only activated the last   channel and removed others between the teacher   and student models . Besides , the two different   MMD losses were still used during distillation . ( 2 )   MSD w/o . L , which only removed the cross-   language MMD loss . In this case , the teacher and   student models had multiple transmission channels   but only the cross - model MMD loss was employed .   ( 3 ) MSD w/o . L , which only removed the   cross - model MMD loss correspondingly . ( 4 ) MSD   w/o . all , which removed all the componentsmentioned above , and was equivalent to a baseline   distillation model as TSL .   Results of the ablation experiments were shown   in the bottom five lines of Table 2 , 3 and 4   respectively . Some in - depth analysis could be   explored : ( 1 ) Compared MSD with MSD w/o .   distillers , we could see that the removal of distillers   caused a significant performance drop , which   further demonstrated the importance of leveraging   information contained in the intermediate layers   of mBERT ; ( 2 ) Compared MSD with MSD w/o .   L and MSD w/o . L , the parallel domain   adaptation contributed to cross - lingual NER signifi-   cantly . The results well demonstrated that explicitly   transfer domain information across models and   languages during distillation was reasonable and   effective ; ( 3 ) The two MMD losses were correlated ,   asL measured both cross - language and cross-   model effects . Removal of L caused larger   performance degradation than removal of L.   The ablation study validated the effectiveness   of all components . Moreover , subtle integration   of these modules achieved state - of - the - art perfor-   mance . Not only multiple channels between the   teacher and student models should be established   to leverage the complementary and hierarchical   information of mBERT , but also these channels   should be shortened for efficient transfer .   5.6.2 Case Study on Domain Discrepancy   To further illustrate the effectiveness of MSD ,   various metrics were employed to measure the5178   distribution discrepancy between different domains :   MMD ( Long et al . , 2015 ) , symmetrical KL di-   vergence ( Jiang et al . , 2020 ; Liu et al . , 2020a ) ,   and cosine similarity ( Bromley et al . , 1993 ) . The   results in Figure 4 is corresponding to the ablation   study . Along with Figure 3 , the effects of different   components could be discussed .   i ) Parallel Domain Adaptation . L pulled the   source domain of the teacher and student models   closer . As shown in Figure 4 ( a ) , the MMD   and symmetrical KL divergence increased and the   cosine similarity decreased without L . Similar   toL , L pulled the source domain of the   teacher and the target domain of the student closer .   ii ) Distillers . From Figure 4 ( c ) and ( d ) , distillers   made the domains within the model closer . This   effect can be seen intuitively in Figure 3 ( b ) . From   Figure 4 ( a ) and ( b ) , the influence of distillers   on the discrepancy between different models was   much smaller than that of L andL. iii )   Other results . L andL were helpful for   reducing the distance between the source and target   domains of the student model , as shown in Figure 4   ( c ) . Besides , they alleviated domains discrepancy   during distillation , as shown in Figure 3 ( c ) .   5.6.3 Comparison of Transfer Manners   To validate the effectiveness of the explicit domain   transfer in MSD , an implicit domain transfer   experiment was designed . Imitating Chen et al .   ( 2021 ) , MMD was employed to get language-   independent features in the teacher model , and   then a baseline distillation was conducted . In   contrast , MSD w/o . distillers actually adopted   an explicit domain transfer manner . As shown in   Appendix A.3 , MSD w/o . distillers outperformedmethods with implicit domain transfer manners .   6 Conclusion   In this paper , we propose a mixture of short-   channel distillers framework for zero - shot cross-   lingual NER , including a multi - channel distillation   framework to fully leverage the complementary and   hierarchical information in the teacher model , and   an unsupervised parallel domain adaptation method   to effectively pull the domains between teacher and   student models closer . Experimental results show   that the proposed method outperforms previous   methods on four datasets across nine languages . In   the future , we will extend this method to languages   where data resources are scarcer .   Limitations   Our method has certain limitations , such as it   can not be used for target languages without any   text data . Furthermore , although the results show   great performance , more efforts are required to   explore the hidden impact of distillers as shown in   the t - SNE graph , which will help the application of   the proposed model in the future .   Acknowledgements   We thank anonymous reviewers for their valuable   comments . This work was partially funded by   the National Natural Science Foundation of China   ( Grant No . U1836219 ) .   References5179518051815182A Appendices   A.1 Baseline Models   We mainly compared our method with the follow-   ing distillation - based methods .   TSL ( Wu et al . , 2020a ) proposed a teacher - student   learning model , via using source - language models   as teachers to train a student model on unlabeled   data in the target language for cross - lingual NER .   Unitrans ( Wu et al . , 2020b ) unified both model   transfer and data transfer based on their comple-   mentarity via enhanced knowledge distillation on   unlabeled target - language data .   AdvPicker ( Chen et al . , 2021 ) proposed a novel   approach to combine the feature - based method and   pseudo labeling via language adversarial learning   for cross - lingual NER .   RIKD ( Liang et al . , 2021 ) proposed a reinforced   knowledge distillation framework .   MTMT ( Li et al . , 2022 ) proposed an unsupervised   multiple - task and multiple - teacher model for cross-   lingual NER .   In addition , Wiki ( Tsai et al . , 2016 ) , WS ( Ni   et al . , 2017 ) , BWET ( Xie et al . , 2018 ) , Adv ( Keung   et al . , 2019 ) , BS(Wu and Dredze , 2019 ) and   TOF ( Zhang et al . , 2021b ) were non - distillation-   based methods .   A.2 Pilot Experiment   Previous studies have verified the importance of   lower layers for cross - language transfer ( Müller   et al . , 2021 ) . Our pilot experiments also further   illustrate that lower layers of mBERT are critical   for NER as shown in Table 5 .   A.3 Comparison of Transfer Manners   Table 6 , 7 and 8 reported the results of different   transfer manners . Imitating AdvPicker ( Chen et al . ,   2021 ) , TSL w. MMD was designed to get language-   independent features in the teacher model , and then   a baseline distillation was conducted , which wasan implicit transfer manner . MSD w/o . distillers   represented the explicit transfer manner.5183