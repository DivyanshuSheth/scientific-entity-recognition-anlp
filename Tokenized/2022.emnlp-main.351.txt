  Anders Søgaard   Dpt . of Computer Science , Pioneer Centre for Artificial Intelligence , and Dpt . of Philosophy   University of Copenhagen   soegaard@di.ku.dk   Abstract   Around two thirds of NLP research at top   venues is devoted exclusively to developing   technology for speakers of English , most   speech data comes from young urban speakers ,   and most texts used to train language models   come from male writers . These biases feed into   consumer technologies to widen existing in-   equality gaps , not only within , but also across ,   societies . Many have argued that it is almost   impossible to mitigate inequality amplification .   I argue that , on the contrary , it is quite simple   to do so , and that counter - measures would have   little - to - no negative impact , except for , perhaps ,   in the very short term .   1 Inequalities   If NLP makes people richer and happier , e.g. , by   allowing them more free time ( Jin et al . , 2021 ) , it   is unfortunate that NLP predominantly serves the   needs of the richest and happiest among us . By   and large , NLP supports languages spoken in the   world ’s wealthiest regions ( Blasi et al . , 2022 ) . Per-   formance disparities within languages and across   demographics are also well - documented ( Amir   et al . , 2021 ; Zhang et al . , 2021 ; Chalkidis et al . ,   2022 ) – and performance correlates strongly with   income levels ( Marrero , 2021 ; Blasi et al . , 2022 ) .   Disparities may reflect data imbalances : If training   data contains less data from a group , predictions for   that group will tend to be worse . But disparities can   also result from outlier behavior or higher degrees   of variance in groups .   Disparities in resources , variation , priority , per-   formance , and turn - around go hand - in - hand to cre-   ate vicious circles that widen existing equality gaps .   Take mobile assistants , for example . Mobile assis-   tants help us organize our calendars , place calls ,   remind us of meetings , etc . Since they are typically   speech - operated , their performance depends heav-   ily on the performance of available speech recogniz - Figure 1 : Validation performance over time when end   user group growth is proportional to the performance   on this group . Each time step corresponds to the in-   clusion of up to 20 new end users . Simulations on   four circles datasets generated at random with https :   //scikit - learn.org/ . The x - axis is time steps , yis   classification accuracy .   ers . For most languages , speech recognizers were   developed for young , urban subpopulations , seen   as early adopters . As a consequence , while young   urbans reap the benefits of mobile assistants , mo-   bile assistants are often much less useful for other   demographics ( Feng et al . , 2021 ; Markl , 2022 ) .   Many have argued that it is almost impossible   to mitigate inequality amplification ( Fazelpour and   Lipton , 2020 ; Lin and Chen , forthcoming ) . I ar-   gue that it is quite simple to do something , and   that doing so would have little - to - no negative im-   pact ( except for , perhaps , in the very short term ) . I   address two levels of inequality in NLP : inequal-   ity across languages and inequality across social   groups . What languages and subgroups are fa-   vored is somewhat task - specific , but generally , NLP   seems biased toward English and the tech - savvy :   The Dominance of English Existing estimates   of how much of top venue NLP research is devoted   to English vary a bit , but typically lie in the range5254of 50 - 90 % , averaging around two thirds . Impor-   tantly , numbers do not seem to have changed for   the better over the last 10 years . The vast major-   ity of publicly available NLP datasets are limited   to English . For this reason , it is much easier for   start - ups and other companies to roll out products   for the English - speaking markets . Naturally , this   means that speakers of English have way more   technologies at hand ( Ananiadou et al . , 2012 ) .   The Dominance of the Tech - Savvy The pene-   tration rate of mobile assistants with young urbans   leads to performance disparities in speech recogni-   tion . Similarly , chatbots are developed for an audi-   ence that more frequently interacts with dialogue   systems . Training data is sampled from end users ,   and user feedback is leveraged as learning signals .   As a result , performance disparities across demo-   graphic groups gradually widen until they become   a matter of night and day . See Figure 1 for an illus-   tration of this effect across four synthetic datasets .   Outline I have argued how NLP favors English   and the tech - savvy . This is not the only way in   which NLP models are biased , but arguably themost important ones . § 2 discusses how these biases   have been justified in the past , arguing how few   of these justifications are sound . § 3 presents pos-   sible mitigation strategies , and § 4 discusses their   limitations .   2 Justifications   Can the inequalities of NLP research be justified ?   One high - level justification of inequalities falls out   of the way we have come to define fairness in NLP   research . NLP researchers have almost uniformly   adopted American philosopher John Rawls ’ defini-   tion of fairness ( Larson , 2017 ; Vig et al . , 2020 ;   Ethayarajh and Jurafsky , 2020 ; Li et al . , 2021 ;   Chalkidis et al . , 2022 ) . Rawls ’ concern ( Rawls ,   1971 ) is with the absolute position of the least   advantaged group rather than their relative posi-   tion , even if we have to abandon strict equality   of income and wealth . Rawls justifies inequality   up to that point and thereby introduces a loophole .   My concern with this form of justification is that   the loophole turns out to be easily exploited be-   cause the effects of both policy and technology are   indirect and must be evaluated in the long term   ( Mukhopadhyay and Mangal , 1997 ; Wörner and   Reiss , 2001 ) . Proponents of a free market argue   that lowering high - income taxes may result in long-   term benefits for the least advantaged , but what if   they eventually do not ? Similarly , can we be sure   that our cutting edge research on English will have   spill - over effects for other languages in the long   run ? It can – in econonomy , as well as in NLP – be   difficult to predict what will get the wheels spin-   ning for everyone , and what will create a downward   spiral ; see , again , Figure 1 for an illustration of this .   Bank ( 2018 ) considers five other common justifica-   tions of ( economic ) inequality ; see Table 1:5255I have , anecdotally , come across all of the five   frames in discussions in the NLP community . The   list is likely incomplete . Some frames are probably   used more explicitly than others . Opportunity argu-   ments ( Utiyama and Isahara , 2007 ; Anastasopoulos   et al . , 2019 ) , Desert arguments ( Blasi et al . , 2022 ;   Lewis et al . , 2020 ) , and Need arguments ( Paetzold   and Specia , 2016 ; Yaneva et al . , 2019 ) are abundant   in the academic literature , whereas you rarely see   explicit Procedure and Reference arguments . Either   way , I have argued that only concerns for special   needs ( Need ) seem to justify inequality .   3 Measures   What measures have NLP researchers proposed to   mitigate inequalities ? Way et al . ( 2022 ) argue that   ’ being able to build neural language models for   other languages with the same quality as English   is key for language equality ’ , and that the step-   ping stone is collecting ’ large amounts of publicly   available corpora of good quality ’ . I think this is   insufficient in my rebuttal of Opportunity in § 2 :   Resources areoften available . Our excessive fo-   cus on English and the tech - savvy is notprimarily   driven by data scarcity . Blasi et al . ( 2022 ) argue it   is the economic prowess of the users of a language   that drives the development of NLP technologies ,   but they do not present specific proposals for mit-   igating inequalities . They only refer to a need for   global coordination .   One common strategy for mitigating perfor-   mance disparities across languages is to make mod-   els language - independent ( Bender , 2009 ) . Multilin-   gual models often still exhibit cross - language dis-   parities ( Singh et al . , 2019 ) , but can be augmented   with an objective minimizing the loss of the worst-   off language ( Ponti et al . , 2021 ; de Lhoneux et al . ,   2022 ) . Similarly , many learning algorithms have   been developed to maximize performance on the   groups with the worst performance . Examples in-   clude square root sampling ( Stickland and Murray ,   2019 ) , adaptive scheduling ( Jean et al . , 2019 ) , loss-   balanced task weighting , ( Liu et al . , 2019 ) , group-   distributional robust optimization ( Sagawa et al . ,   2020 ) , and worst - case - aware automated curriculum   learning ( Zhang et al . , 2020 ) . This does not and will   not bridge existing gaps : The algorithms are com-   monly thought to ensure equal performance but   in fact , because they implement Rawlsian fairness ,   they only prescribe inequality up to a point .   Lin and Chen ( forthcoming ) highlight the chal-5256lenges of achieving fairness in the context of struc-   turally unjust societies ; see also Fazelpour et al .   ( 2022 ) . Such considerations , as well as the urgency   of the matter , has made me wonder what holds us   back in adopting more radical measures . Inspired   by policies proposed to mitigate climate change , I   briefly discuss the pros and cons of three possible   pathways forward :   NLP Cap and Trade Under cap and trade ( Peace   and Stavins , 2010 ) , lawmakers establish a limit ( or   “ cap ” ) on the overall cost or risk , say , the amount   of greenhouse gases . Such caps can be negotiated   from year to year , and are ideally supported by   commonly agreed - upon objectives and scientific   evidence . In NLP , this could be a cap on monolin-   gual language models , a cap on technologies for or   research publications on English , a cap on male an-   notators , etc . Just like governments could initially   auction off emission allowances to the highest bid-   der or allocate them evenly or in light of special   needs , ACL could distribute quota for English mod-   els , biased end user groups , or biased annotator   pools . Subsequent to the initial allocation , research   labs could reduce their ’ emissions ’ and sell excess   allowances to other research labs for quota .   NLP Carbon Tax A carbon tax ( Martin et al . ,   2014 ) is the obverse of cap and trade : rather than   fixing the amount of allowable emissions , it speci-   fies their price . In the same way , NLP researchers   could incur a cost – by paying higher conference   fees , subtracting from their reviewer scores , or by   disqualifying them from paper awards – if they ’ emit   greenhouse gases ’ by , say , working on English or   with a biased set of end users or annotators .   NLP Car - Free Sundays An alternative to the   above is the equivalent of car - free sundays . Car-   free Sundays produce significant mean carbon   emission reductions and reduce overall traffic ac-   tivity . While these effects are variable ( Glazener   et al . , 2022 ) , car - free Sundays also help to promote   the cause of mitigating climate change . Car - free   Sundays are also less intrusive and less bureau-   cratic than cap and trade or the equivalent of a car-   bon tax . Examples of regulatory steps in NLP that   would be comparable to car - free Sundays , would   include a one - year ban on English models , biased   end user groups , biased annotator pools , etc . In   practice , bans could , for example , be implemented   by automatic desk rejection of all such papers sub-   mitted to ACL 2023 or to all the main conferencesof that year . It is easy to see the positive effects of   such an initiative : ACL 2022 accepted 702 papers .   702 papers on other languages than English and/or   with annotator pools would be a big step toward   course - correcting and mitigating existing biases .   4 Discussion   NLP Cap and Trade , NLP Carbon Tax , and NLP   Car - Free Sundays are all possible ways of reduc-   ing the widening digital language divide and to re-   duce performance disparities across groups . If these   ideas seem radical , it is worth remembering that   the public perception of carbon tax has changed   much over time ( Jagers et al . , 2021 ) . The regula-   tion discussed in the above nevertheless goes well   beyond the regulation previously proposed . The   European Union , for example , recently presented   a legal framework for artificial intelligence . In   the framework , NLP and related technologies are   classified as high - risk to low - to - no - risk , and low - to-   no - risk technologies , e.g. , spam filters or syntactic   parsers , are left unregulated . A one - year ban on   English NLP would also mean a one - year ban on   English spam filters . My motivation for extending   regulation to low - to - no - risk technologies is about   inequality , not safety . A one - year ban on English   would also be more radical than earlier attempts by   the ACL to promote linguistic diversity and bias   mitigation , by thematic research tracks and best   paper awards . So why go further now ?   My argument for considering temporary regula-   tion is a ) that we urgently need to act , and b ) that   NLP turn - around is fast , and the field has proven in-   credibly adaptive . In other words , it would not have5257many negative side - effects to impose such regula-   tion . If tomorrow researchers were told that papers   on English NLP would be desk - rejected from ACL   2023 or ACL 2024 ( as a form of NLP Car - Free   Sundays ) , many of us would have to course - correct   a bit . Many papers would have to report results on   different data , and new data would have to be an-   notated . But that , arguably , would serve NLP well .   Course - correcting would , by the end of the day ,   require limited effort .   5 Conclusion   This paper is a position paper , arguing that NLP is   contributing to global inequalities through a digi-   tal language divide , and by implicitly favoring the   tech - savvy . While many of us have promoted lin-   guistic diversity in recent years , numbers suggest   our field is still massively biased toward developing   English NLP for tech - savvy demographics . Maybe   time is ripe to consider more concrete measures ? To   get the discussion off ground , I briefly considered   three types of measures inspired by regulation ad-   vanced to mitigate climate change : cap - and - trade ,   carbon tax , and car - free Sundays . I argue that the   NLP community would quickly adapt to most such   initiatives , course - correcting in little or no time .   Perhaps initially , the equivalent of car - free Sundays   is the least intrusive and bureaucratic form of regu-   lation , but exactly what our next steps should look   like , I leave up for community - wide discussion .   6 Limitations   Generating the above opinions , required no GPUs   and no manual annotations . One major limitation of   this work , though , is that it is all words , no action .   Empty vessels make the loudest sound , Plato said .   On the other hand , I would add , they change course   more easily .   References525852595260