  Ohad Rubin Jonathan Herzig Jonathan Berant   The Blavatnik School of Computer Science , Tel Aviv University   Abstract   In - context learning is a recent paradigm in nat-   ural language understanding , where a large pre-   trained language model ( LM ) observes a test in-   stance and a few training examples as its input ,   and directly decodes the output without any up-   date to its parameters . However , performance   has been shown to strongly depend on the se-   lected training examples ( termed prompts ) . In   this work , we propose an efficient method for   retrieving prompts for in - context learning us-   ing annotated data and an LM . Given an input-   output pair , we estimate the probability of the   output given the input and a candidate train-   ing example as the prompt , and label training   examples as positive or negative based on this   probability . We then train an efficient dense   retriever from this data , which is used to re-   trieve training examples as prompts at test time .   We evaluate our approach on three sequence - to-   sequence tasks where language utterances are   mapped to meaning representations , and find   that it substantially outperforms prior work and   multiple baselines across the board .   1 Introduction   The striking language skills and world knowledge   embedded in large pre - trained language models   ( LMs ) ( Devlin et al . , 2019 ; Petroni et al . , 2019 ; Raf-   fel et al . , 2020 ; Brown et al . , 2020 ) have recently   led to in - context learning , a new paradigm in natu-   ral language understanding . Under this paradigm ,   a language model is given a prompt , which typi-   cally contains a few training examples , as well as a   test instance as input , and generates the output for   the test instance directly , without any update to its   parameters . This approach was first introduced in   GPT-3 ( Brown et al . , 2020 ) , but has quickly spread   to other LMs ( Lieber et al . , 2021 ; Du et al . , 2021 ;   Rae et al . , 2021 ) .   An attractive property of in - context learning is   that it provides a single model for multiple lan-   guage understanding tasks . However , Liu et al . Figure 1 : An overview of prompt retrieval : Given a   question from B , one retrieves similar training   examples from an index of the training set . The question   and training examples ( the prompt ) are passed to an   inference LM that decodes the output .   ( 2021a ) showed that downstream performance can   vary widely depending on the choice of in - context   examples . This has sparked interest in prompt re-   trieval ( see Fig . 1 ) , where given a test instance ,   training examples are chosen for the prompt based   on some similarity metric . Recent work has either   used off - the - shelf unsupervised similarity metrics ,   or trained a prompt retriever to select examples   based on surface similarity ( Das et al . , 2021 ) .   In this work , we suggest to use language mod-   els themselves to label examples that can serve as   good prompts , and train a prompt retriever from   this signal . To train the retriever ( see Fig . 2 ) , we   assume access to a training set of input - output pairs   and to a scoring LM , i.e. , a language model that   will be used to score prompts . For each training   example ( x , y ) , we go over other candidate train-   ing examples , and estimate the probability , accord-   ing to the scoring LM , of yconditioned on xand   the candidate prompt . We label training examples   that lead to high probability as positive and low   probability as negative and train a prompt retriever2655   from this data using contrastive learning . We ar-   gue that using an LM for labeling examples is a   better proxy for training a retriever compared to   previously - proposed surface similarity heuristics .   Importantly , when creating the training data , we   have access to the gold label y , which can be used   to obtain a high - quality set of candidate prompts .   This leads to good positive examples and hard neg-   ative examples , which are beneficial for training   with a contrastive objective .   Using a scoring LM to train an efficient retriever   for a potentially different test time inference LM is   beneficial in two scenarios . First , when the scoring   LM is smaller than the inference LM and serves as   a proxy for it . This results in cheap and efficient   data generation for the retriever , accessible to a   wide range of researchers . Second , our approach   can be used even when the scoring and inference   LMs are identical ( e.g. , both are GPT-3 ) . This is   beneficial when we do not have access to model   parameters and can only use it as a service , an   increasingly popular paradigm . In this case , we use   the LM to train a light - weight retriever that is only   tasked with learning a similarity function . More   generally , given that the scale of LMs is likely to   keep increasing in the foreseeable future , one can   view our approach for Efficient Prompt Retrieval ,   orEPR , as a method for interfacing and learning to   interact with large LMs .   We empirically test EPR on three structured   sequence - to - sequence tasks , where input natural   language utterances are mapped to a meaning rep-   resentation : MT(Li et al . , 2021 ) and SM-   CF ( Andreas et al . , 2020 ) , which focus on   task - oriented dialogue , and B ( Wolfson et al . ,2020 ) , a benchmark for mapping questions to a   language - based meaning representation . We ob-   serve that EPR substantially improves performance   compared to prior work on prompt retrieval . When   the scoring LM and inference LM are identical   ( using GPT - N(Black et al . , 2021 ) ) , perfor-   mance compared to the best baseline improves   from 26 % to 31.9 % on B , from 57 % to   64.2 % on MT , and from 51.4 % to 54.3 % on   SMCF . When using GPT - Nas a proxy   for larger LMs ( GPT - J , GPT-3 , and C ) , we   observe similar gains , where performance improves   substantially in all cases .   To conclude , we propose an approach for retriev-   ing training examples for in - context learning in   large language models , and show it substantially   outperforms prior methods . Given recent develop-   ments in scaling LMs , designing efficient methods   for interacting with LMs is an important direction   for future research .   2 Background : Prompt Retrieval   Problem setup Given a training set D=   { ( x , y)}of input - output sequences , and a   test example x , our goal is to train a retriever ,   R(x , D ) , that will retrieve a subset of training   examples P={(x , y)}⊂ D , where m≪n .   We succinctly refer to Pas the prompt .   Given an inference LM , g , a good prompt should   lead to the target output sequence when the test   example xis concatenated to the prompt Pand   passed as a prefix to g. Specifically , decoding from2656the LM g([P;x])should yield y. In this work ,   we focus on structured tasks , such as semantic pars-   ing , where xis a natural language utterance and y   is a meaning representation for that utterance .   Prior work Liu et al . ( 2021a ) investigated the   effect of different prompts on the performance   of GPT-3 and demonstrated that the choice of in-   context examples strongly affects downstream per-   formance . They used an unsupervised sentence   encoder to encode training examples , and retrieved   for every test instance its nearest neighbors .   Das et al . ( 2021 ) trained a supervised prompt   retriever for knowledge - base question answering .   The retriever was trained with supervision that is   tailored for knowledge - base queries , and relies on   surface similarity between formal queries . Con-   versely , our approach takes advantage of the gener-   ative LM itself and is thus more general .   Shin et al . ( 2021 ) used GPT-3 to select examples   for the prompt for few - shot semantic parsing . How-   ever , rather than training a retriever , they randomly   sample a large set of utterance - program pairs from   the training set , and choose those that are similar   to the target instance question according to GPT-3 .   This results in an expensive inference procedure ,   where GPT-3 is run hundreds of times for each test   instance , unlike our approach , which is based on a   light - weight sub - linear retriever .   3 Efficient Prompt Retriever   We now describe our method for training EPR ,   an efficient prompt retriever for in - context learn-   ing . We first describe how to generate labeled data   ( § 3.1 ) , and then how to use the training data for   training and inference ( § 3.2 ) . Fig . 2 provides an   overview of the training procedure .   3.1 Generating the Training Data   Our approach relies on finding which training ex-   amples can serve as good prompts for other training   examples . Scoring all pairs of training examples is   quadratic in |D| , and thus prohibitive . Hence , we   present a method for choosing a set of candidate ex-   amples ¯E ⊂D , from which we will choose positive   and negative examples for training . Importantly ,   since we are notat test time and are only generating   data for training , we can use the target sequence   yto retrieve a good set of candidates . This can be   approached using a simple retrieval method , given   that our goal is to retrieve examples that are similar   to the input in terms of their output sequence , y. To obtain a high - quality candidate set of train-   ing examples , we take advantage of an unsuper-   vised retriever , ¯E = R((x , y),D ) . For the choice   of the unsupervised retriever , we experiment with   BM25 ( Robertson and Zaragoza , 2009 ) , a sparse   retriever that relies on surface text similarity , and   SBERT ( Reimers and Gurevych , 2019 ) , which is   based on dense sentence encoding . For both , we   experimented with passing the retriever the training   pair(x , y)or the target sequence yonly , and found   that using yleads to slightly higher performance .   Scoring the candidate set Once we retrieve the   set of candidates ¯E={¯e,···,¯e}for a training   example ( x , y),we score each candidate ¯e∈¯E   independently with a scoring LM , ˆg , which serves   as a proxy for the inference LM , g. Specifically ,   the score for a candidate prompt is   s(¯e ) = Prob(y|¯e , x ) ,   which is the probability under the LM , ˆg , of the out-   put sequence conditioned on the candidate prompt   and input sequence . This indicates how helpful this   candidate is for decoding the target ( independent   of all other candidates ) . We argue this score is a   better proxy for the utility of a training example at   inference time compared to prior approaches .   We apply this scoring function to all training ex-   amples , and define for each training example a set   of positive examples E , which includes the top- k   candidates in ¯Eaccording to s(¯e ) , and a set of neg-   ative examples E , which includes the bottom- k   candidates in ¯Eaccording to s(¯e ) . This should lead   to relevant positive examples , assuming that the set   of candidates , ¯Eincludes good prompt candidates   and hard negatives , since all candidates have high   similarity with ( x , y)according to R(y , D ) . With   positive and negative examples at our disposal , we   can now apply contrastive learning , which we de-   scribe next .   3.2 Training and Inference   Training Our training procedure proceeds ex-   actly like the contrastive learning procedure from   DPR ( Karpukhin et al . , 2020 ) . This procedure re-   sults in an input encoder E ( · ) , which receives the   sequence of input tokens , x , and a prompt encoder   E ( · ) , which receives a candidate prompt , namely ,   a concatenation of the tokens in an input - output   pair . Both encoders are initialized with BERT - base2657(Devlin et al . , 2019 ) , and the output vector repre-   sentation is given by the CLS token , as usual . The   goal of training is to learn a similarity metric such   that given a test example x , it will be similar to   training examples that lead to decoding of y.   Our training instances are of the form   ⟨x , e , e , . . . e⟩. Where the positive ex-   ample eis sampled from the set E , and our   negative examples consist of one hard negative ex-   ample sampled from E , B−1positive examples   from the other instances in the same mini - batch ,   and the B−1hard negatives from those instances .   We define the similarity score between an input   and an input - output pair to be the inner product   sim(x , e ) = E(x)E(e ) . We can now define   the typical contrastive learning objective and mini-   mize for each example the negative log likelihood   of the positive example :   L(x , e , e , . . . e ) ( 1 )   = −loge   e+/summationtexte .   An advantage of this approach is that for batch size   Bthe effective batch size is of order B , with the   in - batch negatives trick ( Henderson et al . , 2017 ) .   Inference After training the input encoder and   prompt encoder , we encode the entire set of train-   ing examples with E(·)in a pre - processing step   using FAISS ( Johnson et al . , 2017 ) . At test time ,   given an input sequence , x , we compute its en-   coding E(x ) , and then use maximum inner-   product search over the training data to find the L   most similar training examples , sorted by their in-   ner product ( from high to low ): P= ( e , . . . , e ) .   The final prompt Pis determined by C , the max-   imal context size supported by the inference LM ,   g. Specifically , L≤Lis the largest Lsuch / summationtext|e|+|x|+|y| ≤C , where |y|is the   desired maximal length of the generated output . Fi-   nally , we return the output of greedy decoding on   g([e;e ; . . .;e;x ] ) .   We note that while at training time we score each   training example independently , at test time the   language model observes a prompt , i.e. , a sequence   of examples . We leave modeling the dependence   between different training examples to future work .   4 Experimental Results   We now compare EPR to a wide range of unsu-   pervised and supervised baselines , both when thescoring LM , ˆg , is smaller than the inference LM , g ,   and when they are identical .   4.1 Datasets   We focus on tasks that map utterances to meaning   representations , where in - context examples can be   used to learn the mapping from inputs to outputs .   Examples from each dataset and the number of   examples are in Table 1 .   •B ( Wolfson et al . , 2020 ): A dataset map-   ping complex natural language questions into a   language - based meaning representation , where   a question is decomposed into an ordered list   of atomic steps . We use the low - level B   subset , containing 44K/7K/8 K examples in its   training / development / test sets .   •MT(Li et al . , 2021 ): A semantic parsing   dataset , focused on task - oriented dialogue , where   commands are mapped to complex nested queries   across 11 domains . Similar to past work ( Pasu-   pat et al . , 2021 ) , we use the English subset of   MT , containing 16K/2K/4 K examples in its   training / development / test sets .   •SMCF ( Andreas et al . , 2020 ): A large   English - language task - oriented dataset that cov-   ers tasks such as calendar , weather , places , and   people . The meaning representation is a dataflow   program , which includes API calls , function com-   position and complex constraints . SMCF   includes 15 K development set examples and   134 K training examples , from which we sample   a random set of 44 K examples for training .   4.2 Baselines and Oracles   We consider the following unsupervised baselines ,   which are applied at test time only .   •R : we randomly sample examples from   the training set D.   •SBERT : We use SentenceTransformers ,   a library providing BERT - based sen-   tence embeddings . Specifically , we use   paraphrase - mpnet - base - v2 , a 110 M   parameter model to encode the test utterance   xand retrieve the examples with the most   similar utterances as in - context examples .   •BM25 : We use the classical sparse retrieval   method BM25 ( Robertson and Zaragoza , 2009 ) ,   which is an extension of TF - IDF , to retrieve for2658   each test utterance xthe training examples   with the most similar utterance .   •B F : We apply the prompt selection   method for few - shot semantic parsing from Shin   et al . ( 2021 ) . Given a test example x , we sam-   ple 200 training examples . For each training   example ( x , y ) , compute Prob(x|x ) , and   use the highest scoring examples for the prompt .   Similar to us , this approach uses the inference   LM to choose prompts . However , it does so at   test time , which results in slow inference .   Next , we describe baselines that use the train-   ing set , D , to train a prompt retriever . All super-   vised methods share the following template . First ,   a candidate set ¯EofL= 50 examples is retrieved   with the unsupervised retriever R(y , D ) . We use   BM25 as an unsupervised retriever , since it outper-   formed SBERT ( see § 4.4 ) . We then score each can-   didate prompt ¯e∈¯Ewith some scoring function ,   and label the top- kprompts as positive examples   and the bottom- kas negative examples ( k= 5 ) .   Different supervised methods only differ in the   scoring function itself .   •DR - BM25 : Here , we use the original BM25   scores for labeling positive and negative exam-   ples and training a dense retriever .   •C- R ( CBR ) : We adapt   the scoring function from Das et al . ( 2021 ) ,   which focused on knowledge - base question an-   swering . They define the weight for a pair of log-   ical forms to be the Fscore between the two sets   of relations appearing in those logical forms , and   use this weight to softly label their data . Since   in our setting we do not assume logical forms , we define the score between two output sequence   yandyto be the Fbetween the two sets of   tokens in yandy , omitting stop words .   •E P R ( EPR ) : Our   full approach from § 3 .   Last , we also consider two oracle models .   •BM25 - O : We score test examples   with BM25 using the gold output sequence   R(y , D ) . This provides an upper - bound   on what can be learned by DR - BM25 . EPR can   potentially outperform this oracle , since its train-   ing signal goes beyond surface text similarity .   •LM - O : We use the procedure for labeling   training data at test time . Given a test example   ( x , y ) , we first retrieve Lcandidate training   examples with R(y , D ) , we then sort the   candidates with the scoring LM ˆg , estimating the   probability of ygiven xand the candidate   prompt . This provides an upper bound for EPR ,   since EPR is trained to emulate this behaviour .   4.3 Experimental Details   Language models In this work , we only train   a dense retriever , but use scoring and inference   LMs . For our scoring LM , ˆg , we use GPT - N   ( Black et al . , 2021 ) , a 2.7B - parameter LM trained   on The Pile ( Gao et al . , 2021 ) , an 825 GB English   text corpus , constructed from a wide range of high-   quality resources .   In addition , we consider the following infer-   ence LMs : ( a ) GPT - J ( Wang and Komatsuzaki ,   2021 ): a 6B - parameter LM , also trained on The   Pile . The advantage in this setup , is that GPT - J   was trained on the same corpus as GPT - N. ( b )   GPT-3 ( Brown et al . , 2020 ): A 175B - parameter2659   model , trained mostly on a filtered subset of com-   mon crawl . ( c ) C ( Chen et al . , 2021 ): A   GPT-3 175B - parameter model finedtuned on code   from GitHub . Since our tasks involve mapping   from utterances to programs or meaning represen-   tations , C might potentially perform well at   in - context learning .   For all LMs , we use a maximum context size of   C=2,048 tokens .   Evaluation OnB , we evaluate perfor-   mance on the development set with LF - EM ( Has-   son and Berant , 2021 ) , which is a better metric   compared to Normalized Exact Match ( NEM ) , the   official metric , as it measures whether two mean-   ing representations are semantically equivalent . On   the test set , we use NEM . On MTand SM-   CF , we evaluate with Exact Match ( EM ) ,   i.e. , whether the string output by the inference LM   is identical to the reference string .   We evaluate EPR in two settings : ( a ) LM - as - a-   service , and ( b ) LM - as - a - proxy . In the first set-   ting , we use GPT - Nas both the scoring LM   and inference LM . In this setting , we evaluate on   the full development sets of B , MT , and   SMCF . In the latter setting , as we access   GPT-3 and C through a paid API , we sample   a random subset of 1,000 development examples   from each dataset and evaluate each model once on   this subset .   4.4 Results   LM - as - a - service Table 2 reports results where   the scoring and inference LMs are identical .   EPR substantially outperforms all other baselines .   Specifically , when comparing to the best baseline ,   it improves performance from 26.0 to 31.9 on   B , from 57.0 to 64.2 on MT , and from   51.4 to 54.3 on SMCF . This shows that   using the LM itself to label examples is an effective   approach for obtaining a strong prompt retriever .   Table 3 shows test results on B andMT   corroborating that EPR substantially improves per-   formance compared to BM25 and CBR .   For the unsupervised methods , the R   baseline shows that random sampling of training   examples leads to poor performance . BM25 out-   performs SBERT for prompt retrieval , and con-   sequently we use BM25 in all of our supervised   approaches to retrieve the set of candidates , ¯E. Last ,   B F performs worse than BM25 . We as-   sume this is since the training sets are large ( ∼14-   120 K examples ) , and sampling 200 examples does   not cover examples that are useful for GPT - N.   Interestingly , EPR outperforms BM25 - O   onMTandSMCF and is comparable on   B . This is surprising since BM25 - O   has access to the output sequence yat test time ,   illustrating that the signal provided by the scoring   LM for training goes beyond surface text similarity .   The performance of LM - O is substantially   higher than EPR , showing that the supervision pro-   vided by the scoring LM is strong , and training a   better retriever from this signal can substantially   enhance performance .   We further evaluate our models in the one - shot   setup , i.e. , when the prompt given to the inference   LM includes the highest scoring example only . In   this setup , the inference LM is applied in the same   setting as when we generate labeled data , where   we go over each prompt candidate independently.2660   Since train and test time are now closer , we can ex-   pect the advantage of EPR to be more pronounced .   Table 4 shows the results . Indeed , EPR out-   performs the best baseline by 8.5 % , and BM25-   O by 5 % . In addition , we examine   AC -O , which tests whether any   of the candidates returned by BM25 leads to the   correct output . AC -O reaches   53.6 % , 20 points above LM - O . This shows   the high quality of candidates provided by BM25   ( applied on the y ) , as one can reach more than 50 %   LF - EM with just a single prompt . Moreover , it   hints that a better scoring function can potentially   further improve performance .   LM - as - a - proxy Table 5 shows results when the   scoring LM is GPT - Nand the inference LM is a   larger LM . First , the trends are similar to the LM - as - a - service setup , i.e. , EPR substantially outperforms   prior baselines , including our best unsupervised   baseline , BM25 , and the best supervised baseline ,   CBR , by 2 - 8 points on all datasets and all pre-   trained models . Thus , GPT - Nserves as a good   proxy for choosing training examples .   To further validate this finding , we evaluate the   performance of GPT - J onB with GPT - N   as the scoring LM compared to using GPT - J it-   self as the scoring LM . We find performance im-   proves slightly from 31.5 to 33.6 . Analogously ,   when using C as the scoring LM and infer-   ence LM performance remains roughly the same :   29.5→29.3 . Thus , using a smaller LM ( GPT - N )   is an effective strategy for training a retriever that   will be applied on other LMs . Zooming in on dif-   ferent inference LMs , GPT - J performs slightly bet-   ter than GPT - Nacross the board , since it was2661   trained on the same data and using the same pro-   cedure as GPT - N.C outperforms GPT-   3 , which can be explained by the fact that it was   trained on code , and our datasets involve map-   ping to programs or meaning representations . Sur-   prisingly , GPT - J outperforms C ( except on   MT ) and GPT-3 despite being 30x smaller . This   can perhaps be explained by the fact that GPT - J   was trained on a different dataset ( The Pile ( Gao   et al . , 2021 ) ) .   Analysis Table 6 shows an example from B   where EPR decodes the correct output , while CBR   does not . All training examples retrieved by EPR   perform an argmax ( argmin in the original utter-   ance ) , and return in the final step “ a code ” , while   the third example retrieved by CBR does not per-   form an argmax or argmin , and do not involve “ a   code ” . We provide additional examples in App . A.   Figure 3 shows a t - SNE ( Hinton and Roweis ,   2002 ) projection of the embeddings learned by EPR   for the training examples of B , with a link   to an interactive version , where we applied the   OPTICS ( Ankerst et al . , 1999 ; Schubert and Gertz ,   2018 ) clustering algorithm . Examining clusters   shows that EPR captures both lexical and structure   similarity . Examples for clusters are also available   in App . A.   Prompt copying We analyze how the LM uti-   lizes in - context prompts . Specifically , is the target   output copied from one of the prompts or is it a   composition of different prompt fragments , which   result in generalization to new structures .   To achieve this , we define two types of copy-   ing . ( a ) Exact copying measures if the generated   output exactly matches one of the examples in the   prompt , and ( b ) Abstract copying , that quantifies   if the structure of the decoded output matches any   of the structures seen in the prompt . Specifically ,   we eliminate the effect of non - structural elements   such as entities and function arguments . We re-   place every sequence of words in the logical form   that appears in the input utterance with the string   [ MASKED ] for both the target utterance and in-   context examples . If the masked logical form that   the LM decoded appears in the set of masked ex-   amples defined by the prompt , we say that the LM   copied that abstract pattern .   Table 7 presents the results on the validation   set for each of our three datasets , as well as the   accuracy on each subset . We observe that the   rate of copying is much higher in MTandSM-   CF compared to B , where in MT   andSMCF abstract copying reaches more   than 80 % . Moreover , accuracy on examples where   copying occurred is much higher compared to ac-   curacy where no copying happened . For exam-   ple , on MT , 84.5 % of the examples were ab-   stractly copied , and on that subset of examples ,   EPR achieves 71.6 % EM , compared to 64.2 % on2662the entire validation set . Nevertheless , even though   accuracy is much lower in cases where no copying   occurred , accuracy is not negligible , which shows   that some form of generalization to new structures   is taking place .   Another follow - up question is whether the model   copies patterns from prompts uniformly or does it   attend mostly to the ones with high retrieval score .   To answer this , we look at the subset of exam-   ples where copying occurred . We then identify for   each example the highest - ranking prompt that was   copied from , and define the distance of that prompt   by dividing the rank by the number of prompts that   fit in that example . Figure 4 shows the distribution   over distances for the B dataset . We observe   that copying happens mostly from highly - ranked   prompts .   5 Related Work   In - context learning Our understanding of in-   context learning has grown substantially recently .   Saunshi et al . ( 2021 ) suggests that by conditioning   on a prompt , the task of predicting the next word   approaches linear separability . Xie et al . ( 2021 )   suggests that in - context learning occurs when the   model infers a shared latent concept between ex-   amples in a prompt . Levine et al . ( 2021 ) present   a pre - training scheme theoretically motivated by   the bias of in - context learning , that gives signif-   icant improvements . Recently , Min et al . ( 2022 )   showed that the model does not rely on the ground   truth input - label mapping provided in the demon-   strations as much as previously thought .   Retrieval Research on training dense retrievers   has skyrocketed recently , propelled by interest   in open - domain question answering ( Chen et al . ,   2017 ; Lee et al . , 2019 ; Karpukhin et al . , 2020 ; Guu   et al . , 2020 ; Khattab and Zaharia , 2020 ; Qu et al . ,   2021 ) . Work on retrieval - based methods has also   spread more widely to other knowledge - intensive   tasks ( Lewis et al . , 2020 ) , e.g. , fact verification   ( Samarinas et al . , 2021 ) .   Similar to us , Pasupat et al . ( 2021 ) proposed to   use retrieval in semantic parsing . However , they fo-   cus on controlling the output generated by a model .   Retrieval methods have also been successfully used   in language modeling ( Khandelwal et al . , 2020 ;   Borgeaud et al . , 2021 ; Alon et al . , 2022 ) and ma-   chine translation ( Khandelwal et al . , 2021).Prompts Developing methods for interacting   with LMs and extracting desired behaviours has   attracted considerable attention , under the umbrella   term prompting . In this work , prompts are a set of   in - context training examples , but substantial effort   has also been devoted to casting natural language   tasks as language modeling by phrasing the tar-   get task in natural language ( see survey in ( Liu   et al . , 2021b ) ) . Such approaches include prompt   engineering through manual patterns ( Petroni et al . ,   2019 ; Schick and Schütze , 2021 ) , decoding meth-   ods ( Min et al . , 2021 ; Zhao et al . , 2021 ; Holtzman   et al . , 2021 ) , and methods for extracting either hard   ( Shin et al . , 2020 ; Haviv et al . , 2021 ) or soft ( Li and   Liang , 2021 ; Zhong et al . , 2021 ; Qin and Eisner ,   2021 ) prompts automatically .   Prompt retrieval for supervised models In par-   allel to this work , adding training examples as addi-   tional input has been shown to be useful for super-   vised models as well . Wang et al . ( 2022 ) and Xu   et al . ( 2021 ) used BM25 to retrieve and augment the   input with similar examples from the training set .   Fine - tuning the model with the additional inputs   improved performance on tasks such as summariza-   tion and question answering . Such methods can   also potentially benefit from a stronger retriever .   6 Conclusions   Large pre - trained LMs are becoming an insepara-   ble part of the natural language understanding eco-   system . However , accessing their weights or updat-   ing them can be prohibitive for many researchers .   In this work , we propose EPR , a method for learn-   ing to retrieve good prompts for in - context learning ,   by using language models themselves as the scor-   ing function . This allows us to train a light - weight   retriever and substantially improve performance on   three challenging tasks .   More broadly , given that large LMs models are   likely to play a prominent role in developing lan-   guage understanding models , it is important to de-   velop approaches for interacting with such models   effectively . EPR can be viewed as a step in this   direction .   Acknowledgement   We thank Ori Ram and Itay Itzhak for helpful sug-   gestions and meaningful discussions . This research   was supported in part by The Yandex Initiative for   Machine Learning , and The European Research2663Council ( ERC ) under the European Union Hori-   zons 2020 research and innovation programme   ( grant ERC DELPHI 802800 ) . This work was com-   pleted in partial fulfillment for the Ph . D degree of   Ohad Rubin .   References266426652666   A Appendix   Distribution of the number of in - context exam-   ples Since the selection procedure for in - context   examples is dynamic , the number of in - context ex-   amples differs for each test instance . In Figure 5 ,   we plot the histogram of the number of examples   we fit in C= 2,048tokens .   Effect of hyperparameters We test the effect   ofk , the number of prompts labeled as positive or   negative , and L , the number of prompts retrieved   by the unsupervised retriever . Table 8 shows that   performance is is generally robust w.r.t these hyper-   parameters .   Training details To train EPR , we use the Adam   optimizer ( Kingma and Ba , 2015 ) with batch size   120 and learning rate 1e-4 on eight RTX 3090 . We   run training for 30 epochs . We used the default   DPR hyperparameters without tuning . We used the   final epoch of the model to perform model selec-   tion , and applied minimal learning rate tuning on   the validation set of B .Risk assessment Large language models have   been shown to exhibit various kinds of bias ( Bender   et al . , 2021 ) , since EPR is trained on the signal   obtained from such large LMs , it might also exhibit   these biases .   Additional examples Tables 9 , 10 , and 11 pro-   vide more examples for cases where EPR is cor-   rect while CBR is incorrect along with the top-3   prompts for each method.26672668266926702671