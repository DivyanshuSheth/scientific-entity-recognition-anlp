  Rebecca QianCandace RossJude Fernandes   Eric SmithDouwe KielaAdina WilliamsFacebook AI Research;Hugging Face   rebeccaqian,adinawilliams@fb.com   Abstract   Unwanted and often harmful social biases are   becoming ever more salient in NLP research ,   affecting both models and datasets . In this   work , we ask whether training on demograph-   ically perturbed data leads to fairer language   models . We collect a large dataset of human   annotated text perturbations and train a neu-   ral perturbation model , which we show out-   performs heuristic alternatives . We find that   ( i ) language models ( LMs ) pre - trained on de-   mographically perturbed corpora are typically   more fair , and ( ii ) LMs finetuned on perturbed   GLUE datasets exhibit less demographic bias   on downstream tasks , and ( iii ) fairness improve-   ments do not come at the expense of perfor-   mance on downstream tasks . Lastly , we discuss   outstanding questions about how best to eval-   uate the ( un)fairness of large language mod-   els . We hope that this exploration of neural   demographic perturbation will help drive more   improvement towards fairer NLP .   1 Introduction   There is increasing evidence that models can instantiate   social biases ( Buolamwini and Gebru , 2018 ; Stock and   Cissé , 2018 ; Fan et al . , 2019 ; Merullo et al . , 2019 ; Prates   et al . , 2020 ) , often replicating or amplifying harmful   statistical associations in their training data ( Caliskan   et al . , 2017 ; Chang et al . , 2019 ) . Training models on   data with representational issues can lead to unfair or   poor treatment of particular demographic groups ( Baro-   cas et al . , 2017 ; Mehrabi et al . , 2021 ) , a problem that   is particularly egregious for historically marginalized   groups , including people of color ( Field et al . , 2021 ) ,   and women ( Hendricks et al . , 2018 ) . As NLP moves   towards training models on ever larger data samples ( Ka-   plan et al . , 2020 ) , such data - related risks may grow ( Ben-   der et al . , 2021 ) .   In this work , we explore the efficacy of a dataset alter-   ation technique that rewrites demographic references in   text , such as changing “ women like shopping ” to “ men   like shopping ” . Similar demographic perturbation ap-   proaches have been fruitfully used to measure and often   lessen the severity of social bias in text data ( Hall Maud-   slay et al . , 2019 ; Prabhakaran et al . , 2019 ; Zmigrodet al . , 2019 ; Dinan et al . , 2020a , b ; Webster et al . , 2020 ;   Ma et al . , 2021 ; Smith and Williams , 2021 ; Renduch-   intala and Williams , 2022 ; Emmery et al . , 2022 ) . Most   approaches for perturbing demographic references , how-   ever , rely on rule - based systems , which unfortunately   tend to be rigid and error prone , resulting in noisy and   unnatural perturbations ( see Section 4 ) . While some   have suggested that a neural demographic perturbation   model may generate higher quality text rewrites , there   are currently no annotated datasets large enough for   training neural models ( Sun et al . , 2021 ) .   In this work , we collect the first large - scale dataset of   98 K human - generated demographic text perturbations ,   thePerturbation Augmentation NLPDAtaset ( PANDA ) .   We use PANDA to train a seq2seq controlled genera-   tion model , the perturber . The perturber takes in ( i ) a   source text snippet , ( ii ) a word in the snippet referring to   a demographic group , and ( iii ) a new target demographic   attribute , and generates a perturbed snippet that refers   to the target demographic attribute , while preserving   overall meaning . We find that the perturber generates   high quality perturbations , outperforming heuristic alter-   natives . We use our neural perturber to augment existing   training data with demographically altered examples ,   weakening unwanted demographic associations .   We explore the effect of demographic perturbation   on language model training both during pretraining and   finetuning stages . We pretrain FairBERTa , the first   large language model trained on demographically per-   turbed corpora , and show that its fairness is improved ,   without degrading performance on downstream tasks .   We also investigate the effect of fairtuning , i.e. fine-   tuning models on perturbation augmented datasets , on   model fairness . We find that fairtuned models per-   form well on a variety of natural language understand-   ing ( NLU ) tasks while also being fairer on average than   models finetuned on the original , unperturbed datasets .   Finally , we propose fairscore , an extrinsic fairness   metric that uses the perturber to measure fairness as   robustness to demographic perturbation . Given an NLU   classification task , we define the fairscore as the change   in model predictions between the original evaluation   dataset and the perturbation augmented version . Prior   approaches to measuring fairness in classifiers often rely   on “ challenge datasets ” to measure how predictions dif-   fer in response to demographic changes in inputs ( Zhao9496   2 Approach9497   3 Perturbation Augmentation NLP   DAtaset ( PANDA)94989499   4 Training the Demographic Perturber9500   5 Results95019502   6 Conclusion   7 Broader Impact95039504   8 Limitations95059506   References950795089509951095119512A Problems with Perturbation   Augmentation   B Data Collection Task Layout   C Annotator Demographics9513   D Inter - Annotator Agreement   E Data Quality Hand Audit95149515951695179518   F Perturber Human EvaluationG Perturber Training Parameters   H FairBERTa Training Parameters   I Downstream Task Training Parameters9519   J Additional GLUE Statistics   K Additional Fairness Metrics   L Preserving Classification Labels After   Perturbation95209521