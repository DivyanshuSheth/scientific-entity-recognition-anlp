  Yue Yu , Lingkai Kong , Jieyu Zhang , Rongzhi Zhang , Chao ZhangGeorgia Institute of Technology , Atlanta , GAUniversity of Washington , Seattle , WA   { yueyu , lkkong , rongzhi.zhang , chaozhang}@gatech.edu , jieyuz2@cs.washington.edu   Abstract   While pre - trained language model ( PLM ) ﬁne-   tuning has achieved strong performance in   many NLP tasks , the ﬁne - tuning stage can be   still demanding in labeled data . Recent works   have resorted to active ﬁne - tuning to improve   the label efﬁciency of PLM ﬁne - tuning , but   none of them investigate the potential of un-   labeled data . We propose AT , a new   framework that leverages unlabeled data to   improve the label efﬁciency of active PLM   ﬁne - tuning . AT switches between data   annotation and model self - training based on   uncertainty : it selects high - uncertainty unla-   beled samples for active annotation and low-   uncertainty ones for model self - training . Un-   der this framework , we design ( 1 ) a region-   aware sampling strategy that reduces redun-   dancy when actively querying for annotations   and ( 2 ) a momentum - based memory bank that   dynamically aggregates the model ’s pseudo la-   bels to suppress label noise in self - training .   Experiments on 6 text classiﬁcation datasets   show that AT outperforms the strongest   active learning and self - training baselines and   improves the label efﬁciency of PLM ﬁne-   tuning by 56.2 % on average . Our imple-   mentation is available at https://github .   com / yueyu1030 / actune .   1 Introduction   Fine - tuning pre - trained language models ( PLMs )   has achieved much success in natural language   processing ( NLP ) ( Devlin et al . , 2019 ; Liu et al . ,   2019 ; Brown et al . , 2020 ) . One beneﬁt of PLM   ﬁne - tuning is the promising performance it offers   when consuming only a few labeled data ( Bansal   et al . , 2020 ; Gao et al . , 2021 ) . However , there   are still signiﬁcant gaps between few - shot and   fully - supervised PLM ﬁne - tuning in many tasks .   Besides , the performance of few - shot PLM ﬁne-   tuning can be sensitive to different sets of training   data ( Bragg et al . , 2021 ) . Therefore , there is acrucial need for approaches that make PLM ﬁne-   tuning more label - efﬁcient and robust to selection   of training data , especially for applications where   labeled data are scarce and expensive to obtain .   Towards this goal , researchers have recently re-   sorted to active ﬁne - tuning of PLMs and achieved   comparable performance to fully - supervised meth-   ods with much less annotated samples ( Ein - Dor   et al . , 2020 ; Margatina et al . , 2021a , b ; Yuan et al . ,   2020 ) . Nevertheless , they usually neglect unlabeled   data , which can be useful for improving label efﬁ-   ciency for PLM ﬁne - tuning ( Du et al . , 2021 ) . To   incorporate unlabeled data into active learning , ef-   forts have been made in the semi - supervised active   learning literature ( Wang et al . , 2016 ; Rottmann   et al . , 2018 ; Siméoni et al . , 2020 ) . However , the   query strategies proposed in these works can return   highly redundant samples due to limited representa-   tion power , resulting in suboptimal label efﬁciency .   Moreover , they usually rely on pseudo - labeling   to utilize unlabeled data , which requires greater   ( yet often absent ) care to denoise the pseudo la-   bels , otherwise the errors could accumulate and   hurt model performance . This issue can be even   more severe for PLMs , as the ﬁne - tuning process is   often sensitive to different weight initialization and   data orderings ( Dodge et al . , 2020 ) . Thus , it still   remains open and challenging to design robust and   label efﬁcient method for active PLM ﬁne - tuning .   To tackle the above challenges , we propose A-   T , a new method that improves the label efﬁ-   ciency and robustness of active PLM ﬁne - tuning .   Based on the estimated model uncertainty , A-   T tightly couples active learning with self-   training in each learning round : ( 1 ) when the   average uncertainty of a region is low , we trust   the model ’s predictions and select its most certain   predictions within the region for self - training ; ( 2 )   when the average uncertainty of a region is high ,   indicating inadequate observations for parameter   learning , we actively annotate its most uncertain1422samples within the region to improve model per-   formance . Different from existing AL methods   that only leverage uncertainty for querying labels ,   our uncertainty - driven self - training paradigm grad-   ually leverages unlabeled data with low uncertainty   via self - training , while reducing the chance of er-   ror propagation triggered by highly - uncertain mis-   labeled data .   To further boost model performance for A-   T , we design two techniques to improve the   query strategy and suppress label noise , namely   region - aware sampling ( RS ) and momentum - based   memory bank ( MMB ) . Inspired by the fact that   existing uncertainty - based AL methods often end   up with choosing uncertain yet repetitive data ( Ein-   Dor et al . , 2020 ; Margatina et al . , 2021b ) , we de-   sign the region - aware sampling technique to pro-   mote both diversity and representativeness by lever-   aging the representation power of PLMs . Speciﬁ-   cally , we ﬁrst estimate the uncertainties of the unla-   beled data with PLMs , then cluster the data using   their PLM representations and weigh the data by   the corresponding uncertainty . Such a clustering   scheme partitions the embedding space into small   sub - regions with an emphasis on highly - uncertain   samples . Finally , by sampling over multiple high-   uncertainty regions , our strategy selects data with   high uncertainty and low redundancy .   To rectify the erroneous pseudo labels derived   by self - training , we design a simple but effec-   tive way to select low - uncertainty data for self-   training . Our method is motivated by the fact that   ﬁne - tuning PLMs suffer from instability issues —   different initializations and data orders can lead   to large variance in model performance ( Dodge   et al . , 2020 ; Zhang et al . , 2020b ; Mosbach et al . ,   2021 ) . However , previous approaches only select   pseudo - labeled data based on the prediction of the   current round and are thus less reliable . In con-   trast , we maintain a dynamic memory bank to save   the predictions of unlabeled samples for later use .   We propose a momentum updating method to dy-   namically aggregate the predictions from preced-   ing rounds ( Laine and Aila , 2016 ) and select low-   uncertainty samples based on aggregated predic-   tion . As a result , only the samples with high predic-   tion conﬁdence over multiple rounds will be used   for self - training , which mitigates the issue of label   noise . We highlight that our active self - training   approach is an efﬁcient substitution to existing AL   methods , requiring little extra computational cost . Our key contributions are : ( 1 ) an active self-   training paradigm AT that integrates self-   training and active learning to minimize the label-   ing cost for ﬁne - tuning PLMs ; ( 2 ) a region - aware   querying strategy to enforce both the informative-   ness and the diversity of queried samples during   AL ; ( 3 ) a simple and effective momentum - based   method to leverage the predictions in preceding   rounds to alleviate the label noise in self - training   and ( 4 ) experiments on 6 benchmarks demonstrat-   ingAT improves the label efﬁciency over   existing self - training and active learning baselines   by 56.2 % .   2 Uncertainty - aware Active Self - training   2.1 Problem Formulation   We study active ﬁne - tuning of pre - trained lan-   guage models for text classiﬁcation , formulated   as follows : Given a small number of labeled sam-   plesX={(x , y)}and unlabeled samples   X={x}(|X|/lessmuch|X| ) , we aim to ﬁne - tune   a pre - trained language model f(x;θ ) : X→Y in   an interactive way : we perform active self - training   forTrounds with the total labeling budget b. In   each round , we aim to query B = b / T samples   denoted asBfromXto ﬁne - tune a pre - trained   language model f(x;θ)with bothX , BandX   to maximize the performance on downstream text   classiﬁcation tasks . Here X = X∪Xdenotes   all samples , andY={1,2,···,C}is the label set   whereCis the number of classes .   2.2 Overview of AT Framework   We now present our active self - training paradigm   AT underpinned by estimated uncertainty .   We begin the active self - training loop by ﬁne-   tuning a BERT f(θ)on the initial labeled data   X. Formally , we solve the following optimization   problem   min1   |X|/summationdisplay / lscript / parenleftBig   f(x;θ),y / parenrightBig   .(1 )   In roundt(1≤t≤T)of active self - training , we   ﬁrst calculate the uncertainty score based on a given   functiona = a(x , θ)for allx∈X. Then ,   we query labeled samples and generate pseudo-   labels for unlabeled data Xsimultaneously to   facilitate self - training . For each sample x , the   pseudo - label / tildewideyis calculated based on the current1423Algorithm 1 :   model ’s output :   /tildewidey= argmax / bracketleftBig   f(x;θ)/bracketrightBig , ( 2 )   wheref(x;θ)∈Ris a probability simplex   and[f(x;θ)]is thej - th entry . The procedure   of AT is summarized in Algorithm 1 .   2.3 Region - aware Sampling for Active   Learning on High - uncertainty Data   After obtaining the uncertainty for unlabeled data ,   we aim to query annotation for high - uncertainty   samples . However , directly sampling the most un-   certain samples gives suboptimal results as it tends   to query repetitive data ( Ein - Dor et al . , 2020 ) that   represent the overall data distribution poorly .   To tackle this issue , we propose region - aware   sampling to capture both uncertainty anddiversity   during active self - training . Speciﬁcally , in the t-   th round , we ﬁrst conduct the weighted K - means   clustering ( Huang et al . , 2005 ) , which weights sam-   ples based on their uncertainty . Denote by Kthe   number of clusters and v = BERT ( x)the repre-   sentation ofxfrom the penultimate layer of BERT .   The weighted K - means process ﬁrst initializes the   center of each each cluster µ(1≤i≤K)via   K - Means++ ( Arthur and Vassilvitskii , 2007 ) . Then ,   it jointly updates the centroid of each cluster and   assigns each sample to cluster cas   c= argmin / bardblv−µ/bardbl , ( 3 )   µ=/summationtexta(x , θ)·v / summationtexta(x , θ)(4)whereC={x|c = k}(k= 1, ... ,K )   stands for the k - th cluster . The above two steps in   Eq.(3),(4)are repeated until convergence . Com-   pared with vanilla K - Means method , the weighting   scheme increases the density of the samples with   high uncertainty , thus enabling the K - Means meth-   ods to discover clusters with high uncertainty . After   obtainingKregions with the corresponding data   C , we calculate the uncertainty of each region as   u = U(C ) + βI(C ) ( 5 )   where   U(C ) = 1   |C|/summationdisplaya(x , θ ) , ( 6 )   is the average uncertainty of samples and   I(C ) = −/summationdisplayflogf ( 7 )   is the inter - class diversity within cluster kand   f = is the frequency of class jon   clusterk . Notably , the term U(C)assigns higher   score for clusters with more uncertain samples , and   I(C)grants higher scores for clusters containing   samples with more diverse predicted classes from   pseudo labels since such clusters would be closer   to the decision boundary .   Then , we rank the clusters in an ascending order   inu . A high score indicates high uncertainty of   the model in these regions , and we need to actively   annotate the member instances to reduce uncer-   tainty and improve the model ’s performance . We   adopt a hierarchical sampling strategy : we ﬁrst se-   lect theMclusters with the highest uncertainty ,   and then sample b=⌊⌋data with the highest   uncertainty to form the batch Q.   ( 8)   We remark that such a hierarchical sampling strat-   egy queries most uncertain samples from differ-   entregions , thus the uncertainty and diversity of   queried samples can be both achieved.14242.4 Self - training over Conﬁdent Samples   from Low - uncertainty Regions   For self - training , we aim to select unlabeled sam-   ples which are most likely to have been correctly   classiﬁed by the current model . This requires the   sample to have low uncertainty . Therefore , we   select the top ksamples from the Mlowest uncer-   tainty regions to form the acquired batch S :   Momentum - based Memory Bank for Self-   training . As PLMs are sensitive to the stochas-   ticity involved in ﬁne - tuning , the model suffers   from the instability issue — different weight ini-   tialization and data orders may result in different   predictions on the same dataset ( Dodge et al . , 2020 ) .   Additionally , if the model gives inconsistent pre-   dictions in different rounds for a speciﬁc sample ,   then it is potentially uncertain about the sample ,   and adding it to the training set may harm the ac-   tive self - training process . For example , for a two-   class classiﬁcation problem , suppose we obtain   f(x;θ ) = [ 0.65,0.35]for samplexthe round   ( t−1)andf(x;θ ) = [ 0.05,0.95]for the round t.   Although the model is quite ‘ conﬁdent ’ on the class   ofxwhen we only consider the result of the round   t , it gives contradictory predictions over these two   consecutive rounds , which indicates that the model   is actually uncertain to which class xbelongs .   To effectively mitigate the noise and stabilize the   active self - training process , we maintain a dynamic   memory bank to save the results from previous   rounds , and use momentum update ( He et al . , 2020 ;   Laine and Aila , 2016 ) to aggregate the results from   both the previous and current rounds . Then , during   active self - training , we will select samples with the   highest aggregated score . In this way , only those   samples that the model is certain about over all pre-   vious rounds will be selected for self - training . We   design two variants for the memory bank , namely   prediction - based andvalue - based aggregation .   Prediction based Momentum Update . We adopt   an exponential moving average approach to aggre-   gate the prediction g(x;θ)on roundtas   ( 10 )   wherem= ( 1−)m+m(0 < m≤   m≤1)is a momentum coefﬁcient . We gradu-   ally increase the weight for models on later rounds , since they are trained with more labeled data   thus being able to provide more reliable predic-   tions . Then , we calculate the uncertainty based on   g(x;θ)and rewrite Eq . ( 9 ) and ( 2 ) as   S= bottom - ka / parenleftBig   x , g(x;θ),θ / parenrightBig   /tildewidey= argmax / bracketleftBig   g(x;θ)/bracketrightBig,(11 )   Value - based Momentum Update . For methods   that do not directly use prediction for uncertainty   estimation , we aggregate the uncertainty value as(12 )   Then , we use Eq . ( 12 ) to sample low - uncertainty   data for self - training as   S= bottom - kg(x , θ ) ,   /tildewidey= argmax / bracketleftBig   f(x;θ)/bracketrightBig.(13 )   By aggregating the prediction results over previ-   ous rounds , we ﬁlter the sample with inconsistent   predictions to suppress noisy labels .   2.5 Model Learning and Update   After obtaining both the labeled data and pseudo-   labeled data , we ﬁne - tune a new pre - trained BERT   modelθon them . Although we only include   low - uncertainty samples during self - training , it is   difﬁcult to eliminate all the wrong pseudo - labels ,   and such mislabeled samples can still hurt model   performance . To suppress such label noise , we   use a threshold - based strategy to further remove   noisy labels by selecting samples that agree with   the corresponding pseudo labels . The loss objective   of optimizing θis(14 )   whereL = X∪Qis the labeled set ,   λis a hyper - parameter balancing the weight   between clean and pseudo labels , and ω=   1{/bracketleftbig   f(x;θ)/bracketrightbig > γ}stands for the thresh-   olding function .   Complexity Analysis . The running time of A-   T is mainly consisted of two parts : the in-   ference time O(|X|)and the time for K - Means   clusteringO(dK|X| ) , wheredis the dimension   of the BERT feature v. For self - training , the size1425   of the memory bank g(x;θ)is proportional to|X| ,   while the extra computation of maintaining this dic-   tionary is ignorable since we do not inference over   the unlabeled data for multiple times in each round   as BALD ( Gal et al . , 2017 ) does . The running time   of AT will be shown in section C.   3 Experiments   3.1 Experiment Setup   Tasks and Datasets . In our main experiments ,   we use 4 datasets , including SST-2 ( Socher et al . ,   2013 ) for sentiment analysis , AGNews ( Zhang   et al . , 2015 ) for news topic classiﬁcation , Pubmed-   RCT ( Dernoncourt and Lee , 2017 ) for medical ab-   stract classiﬁcation , and DBPedia ( Zhang et al . ,   2015 ) for wikipedia topic classiﬁcation . For   weakly - supervised text classiﬁcation , we choose   2 datasets , namely TREC ( Li and Roth , 2002 )   andChemprot ( Krallinger et al . , 2017 ) from the   WRENCH benchmark ( Zhang et al . , 2021 ) for eval-   uation . The statistics are shown in Table 1 .   Active Learning Setups . Following ( Yuan et al . ,   2020 ) , we set the number of rounds T= 10 , the   overall budget for all datasets b= 1000 and the ini-   tial size of the labeled |X|is set to 100 . In each AL   round , we sample a batch of 100 samples from the   unlabeled setXand query their labels . Since large   development sets are impractical in low - resource   settings ( Kann et al . , 2019 ) , we keep the size of   development set as 1000 , which is the same as the   labeling budget . For weakly - supervised text clas-   siﬁcation , since the datasets are much smaller , we   keep the labeling budget and the size of develop-   ment set tob= 500 .   Implementation Details . We choose RoBERTa-   base ( Liu et al . , 2019 ) from the HuggingFace code-   base ( Wolf et al . , 2020 ) as the backbone for A-   T and all baselines except for Pubmed and   Chemprot , where we use SciBERT ( Beltagy et al . ,   2019 ) , a BERT model pre - trained on scientiﬁc cor - pora . In each round , we train from scratch to avoid   overﬁtting the data collected in earlier rounds as   observed by Hu et al . ( 2019 ) . More details are in   Appendix B.   Hyperparameters . The hyperparameters setting   is in Appendix B.5 . In the t - th round of active   self - training , we increase the number of pseudo-   labeled samples by k , wherekis500for TREC and   Chemprot , 3000 for SST-2 and Pubmed - RCT , and   5000 for others . For the momentum factor , we tune   mfrom [ 0.6,0.7,0.8]andmfrom [ 0.8,0.9,1.0 ]   and report the best { m , m}based on the perfor-   mance of the development set .   Baselines .   Self - training Methods : ( 1)Self - training ( ST ,   Lee ( 2013 ) ) : It is the vanilla self - training method   that generates pseudo labels for unlabeled data .   ( 2)UST ( Mukherjee and Awadallah , 2020 ; Rizve   et al . , 2021 ): It is an uncertainty - based self - training   method that only uses low - uncertainty data for self-   training . ( 3 ) COSINE ( Yu et al . , 2021 ): It uses   self - training to ﬁne - tune LM with weakly - labeled   data , which achieves SOTA performance on vari-   ous text datasets in WRENCH benchmark ( Zhang   et al . , 2021 ) . Note that for these two baselines , we   randomly sample blabeled data as the initialization .   Active Learning Methods : ( 1)Random : It ac-   quires annotation randomly , which serves as a base-   line for all methods . ( 2 ) Entropy ( Holub et al . ,   2008 ): It is an uncertainty - based method that ac-   quires annotations on samples with the highest pre-   dictive entropy . ( 3 ) BALD ( Gal et al . , 2017 ): It   is also an uncertainty - based method , which calcu-   lates model uncertainty using MC Dropout ( Gal   and Ghahramani , 2015 ) . ( 4 ) BADGE ( Ash et al . ,   2020 ): It ﬁrst selects high uncertainty samples then   uses KMeans++ over the gradient embedding to   sample data . ( 5 ) ALPS ( Yuan et al . , 2020 ): It uses   the masked language model ( MLM ) loss of BERT   to query labels for samples . ( 6 ) CAL ( Margatina   et al . , 2021b ) is the most recent AL method for pre-   trained LMs . It calculates the uncertainty of each   sample based on the KL divergence between the   prediction of itself and its neighbors ’ prediction .   Semi - supervised Active Learning ( SSAL ) Meth-   ods : ( 1)ASST ( Tomanek and Hahn , 2009 ;   Siméoni et al . , 2020 ) is an active semi - supervised   learning method that jointly queries labels for AL   and samples pseudo labels for self - training . ( 2 )   CEAL ( Wang et al . , 2016 ) acquires annotations   on informative samples , and uses high - conﬁdence1426samples with predicted pseudo labels for weights   updating . ( 3 ) BASS ( Rottmann et al . , 2018 ) is sim-   ilar to CEAL , but use MC dropout for querying   labeled sample . ( 4 ) REVIV AL ( Guo et al . , 2021 )   is the most recent SSAL method , which uses an   adversarial loss to query samples and leverage label   propagation to exploit adversarial examples .   Our Method : We experiment with both Entropy   and CAL as uncertainty measures for AT .   Note that when compared with active learning base-   lines , we do not augment the train set with pseudo-   labeled data ( Eq . ( 9 ) ) to ensure fair comparisons .   3.2 Main Result   Figure 1 reports the performance of AT and   the baselines on 4 benchmarks . From the results ,   we have the following observations :   •AT consistently outperforms baselines in   most of the cases . Different from studies in the   computer vision ( CV ) domain ( Siméoni et al . ,   2020 ) where the model does not perform well in   the low - data regime , pre - trained LM has achieved   competitive performance with only a few labeled   data , which makes further improvements to the   vanilla ﬁne - tuning challenging . Nevertheless , A-   T surpasses baselines in more than 90 % of the   rounds and achieves 0.4%-0.7 % and 0.3%-1.5 %   absolute gain at the end of AL and SSAL respec-   tively . Figure 3 quantitatively measures the num-   ber of labels needed for the most advanced active   learning model and self - training model ( UST ) to   outperform AT with 1000 labels . These   baselines need > 2000 clean labeled samples to   reach the performance as ours . AT saves   on average 56.2 % and57.0 % of the labeled sam-   ples than most advanced active learning and self-   training baselines respectively , which justiﬁes its   promising performance under low - resource scenar-   ios . Such improvements show the merits of two key   designs under our active self - training framework :   the region - aware sampling for active learning and   the momentum - based memory bank for robust self-   training , which will be discussed in the section 3.5 .   •Compared with the previous AL baselines , A-   T can bring consistent performance gain , while   previous semi - supervised active learning methods   can not . For instance , BASS is based on BALD   for active learning , but sometimes it performs even   worse than BALD with the same number of la-   beled data ( see Fig . 1(b ) and Fig . 1(f ) ) . This is   mainly because previous methods simply combine   noisy pseudo labels with clean labels for trainingwithout explicitly rectifying the wrongly - labeled   data , which will cause the LM to overﬁt these haz-   ardous labels . Moreover , previous methods do not   exploit momentum updates to stabilize the learning   process , as there are oscillations in the beginning   rounds . In contrast , AT achieves a more   stable learning process and enables an active self-   training process to beneﬁt from more labeled data .   •The self - training methods ( ST & UST ) achieve   superior performance with limited labels . However ,   they mainly focus on leveraging unlabeled data   for improving the performance , while our results   demonstrate that adaptive selecting the most useful   data for ﬁne - tuning is also important for improving   the performance . With a powerful querying policy ,   AT can improve these self - training baselines   by 1.05 % in terms of accuracy on average .   3.3 Weakly - supervised Learning   AT can be naturally used for weakly-   supervised classiﬁcation , where Xis a set of noisy   labels derived from linguistic patterns or rules .   Since the initial label set is noisy , the model trained   with Eq . ( 1)can overﬁt the label noise ( Zhang et al . ,   2022a ) , and we can actively query labeled data to   reﬁne the model . We conduct experiments on the   TREC and Chemprot dataset , where we ﬁrst use   Snorkel ( Ratner et al . , 2017 ) to obtain weak label   setX , then ﬁne - tune the pre - trained LM f(θ )   onX. After that , we adopt AT for active   self - training .   Fig . 2 shows the results of these two datasets .   When combining AT with CAL , the perfor-   mance is unsatisfactory . We believe it is because   CAL requires clean labels to calculate uncertain-   ties . When labels are inaccurate , it will prevent A-   T from querying informative samples . In con-   trast , AT achieves the best performance over   baselines when using Entropy as the uncertainty   measure . The performance gain is more notable   on the TREC dataset , where we achieve 96.68 %   accuracy , close to the fully supervised performance   ( 96.80 % ) with only ∼6 % of clean labels .   3.4 Combination with Other AL Methods   Fig . 5(a ) demonstrates the performance of A-   T combined with other AL methods ( e.g.   BADGE , ALPS ) on SST-2 dataset . It is clear that   even if the AL methods are not uncertainty - based1427   ( e.g. BADGE ) , when using the entropy as an un-   certainty measure to select pseudo - labeled data for   self - training , AT can further boost the perfor-   mance . This indicates that AT is a general   active self - training approach , as it can serve as an   efﬁcient plug - in module for existing AL methods .   3.5 Ablation and Hyperparameter Study   The Effect of Different Components in A-   T.We inspect different components of   AT , including the region - sampling ( RS ) ,   momentum - based memory bank ( MMB ) , andweighted clustering ( WClus ) . Experimental re-   sults ( Fig . 5(b ) ) shows that all the three compo-   nents contribute to the ﬁnal performance , as remov-   ing any of them hurts the classiﬁcation accuracy .   Also , we ﬁnd that when removing MMB , the perfor-   mance hurts most in the beginning rounds , which   indicates that MMB effectively suppresses label   noise when the model ’s capacity is weak . Con-   versely , removing WClus hurts the performance on   later rounds , as it enables the model to select most   informative samples .   Hyperparameter Study . We study two hyperpa-   rameters , namely βandKused in querying la-   bels . Figure 4(a ) and 4(b ) show the results . In   general , the model is insensitive to βas the per-   formance difference is less than 0.6 % . The model   can not perform well with smaller Ksince it can not   pinpoint to high - uncertainty regions . For larger   K , the performance also drops as some of the   high - uncertainty regions can be outliers and sam-   pling from them would hurt the model perfor-   mance ( Karamcheti et al . , 2021 ) .   A Closer Look at the Momentum - based Mem-   ory Bank . To examine the role of MMB , we show   the overall accuracy of pseudo - labels on AG News   dataset in Fig . 4(c ) . From the result , it is clear that   the momentum - based memory bank can stabilize   the active self - training process , as the accuracy of   pseudo labels increases around 1 % , especially in1428   later rounds . Fig 4(d ) and 4(e ) illustrates the model   performance with different mandm . Over-   all , we ﬁnd that our model is robust to different   choices as AT outperform the baseline with-   out momentum update consistently . Moreover , we   ﬁnd that the larger mwill generally lead to bet-   ter performance in later rounds . This is mainly   because in later rounds , the model ’s prediction is   more reliable . Conversely , at the beginning of the   training , the model ’s prediction might be oscillat-   ing on unlabeled data . In this case , using a smaller   mwill favor samples with consistent predictions   to improve the robustness of active self - training .   Another ﬁnding is that for different AL methods ,   the optimal memory bank can be different . For   Entropy , probability - based memory bank leads to   a better result , while for CAL , simple aggregating   over uncertainty score achieves better performance .   This is mainly because the method used in CAL   is more complicated , and using probability - based   memory bank may hurt the uncertainty calculation .   3.6 Case Study   We give an example of our querying strategy on   AG News dataset for the 1st round of active self-   training process in ﬁgure 6 . Note that we use t - SNE   algorithm ( Van der Maaten and Hinton , 2008 ) for   dimension reduction , and the black triangle stands   for the queried samples while other circles stands   for the unlabeled data . We can see that the existing   uncertainty - based methods such as Entropy and   CAL , are suffered from the issue of limited diver-   sity . However , when combined with AT , the   diversity is much improved . Such results , com - pared with the main results in ﬁgure 1 , demonstrate   the efﬁcacy of AT empirically .   4 Related Work   Active Learning . Active learning has been widely   applied to various NLP tasks ( Yuan et al . , 2020 ;   Shelmanov et al . , 2021 ; Karamcheti et al . , 2021 ) .   So far , AL methods can be categorized into   uncertainty - based methods ( Gal et al . , 2017 ; Mar-   gatina et al . , 2021a , b ) , diversity - based methods ( Ru   et al . , 2020 ; Sener and Savarese , 2018 ) and hy-   brid methods ( Yuan et al . , 2020 ; Ash et al . , 2020 ) .   Ein - Dor et al . ( 2020 ) offer an empirical study of   active learning with PLMs . Very recently , there   are also several works attempted to query labeling   functions for weakly - supervised learning ( Boeck-   ing et al . , 2020 ; Hsieh et al . , 2022 ; Zhang et al . ,   2022b ) . In our study , we leverage the power of   unlabeled instances via self - training to further pro-   mote the performance of AL .   Semi - supervised Active Learning ( SSAL ) . Gao   et al . ( 2020 ) ; Guo et al . ( 2021 ) design query strate-   gies for speciﬁc semi - supervised methods , Zhang   et al . ( 2020a ) ; Jiang et al . ( 2020 ) combine active   learning with data augmentation and Tomanek and   Hahn ( 2009 ) ; Rottmann et al . ( 2018 ) ; Siméoni et al .   ( 2020 ) exploit the most - certain samples from the   unlabeled with pseudo - labeling to augment the   training set . So far , most of the SSAL approaches   are designed for CV domain and it remains un-   known how this paradigm performs with PLMs on   NLP tasks . In contrast , we propose AT to   effectively leverage unlabeled data during ﬁnetuing   PLMs for NLP tasks .   Self - training . Self - training is one of the earliest   and simplest approaches to semi - supervised learn-   ing ( Lee , 2013 ) . It ﬁrst generates pseudo labels   for high - conﬁdence samples , then ﬁts a new model   on pseudo labeled data to improve the generaliza-   tion ability . However , it is known to be vulnera-   ble to error propagation ( Arazo et al . , 2020 ; Rizve   et al . , 2021 ; Zuo et al . , 2021 ) . To alleviate this ,   we adopt a simple momentum - based method to se-   lect high conﬁdence samples , effectively reducing1429   the pseudo labels noise for active learning . Note   that although Mukherjee and Awadallah ( 2020 ) ;   Rizve et al . ( 2021 ) also leverage uncertainty infor-   mation for self - training , their focus is on develop-   ing better self - training methods , while we aim to   jointly query high - uncertainty samples and gener-   ate pseudo - labels for low - uncertainty samples . The   experiments in Sec . 3 show that with appropriate   querying methods , AT can further improve   the performance of self - training .   5 Conclusion and Discussion   In this paper , we develop AT , a general active   self - training framework for enhancing both label   efﬁciency and model performance in ﬁne - tuning   pre - trained language models ( PLMs ) . We propose   a region - aware sampling approach to guarantee   both the uncertainty the diversity for querying la-   bels . To combat the label noise propagation issue ,   we design a momentum - based memory bank to   effectively utilize the model predictions for pre-   ceding AL rounds . Empirical results on 6 public   text classiﬁcation benchmarks suggest the superi-   ority of AT to conventional active learning   and semi - supervised active learning methods for   ﬁne - tuning PLMs with limited resources . There are several directions to improve AT .   First , since our focus is on ﬁne - tuning pre - trained   language models , we use the representation of   [ CLS ] token for classiﬁcation . In the future work ,   we can consider using prompt tuning ( Gao et al . ,   2021 ; Schick and Schütze , 2021 ) , a more data-   efﬁcient method for adopting pre - trained language   models on classiﬁcation tasks to further promote   the efﬁciency . Also , due to the computational re-   source constraints , we do not use larger pre - trained   language models such as RoBERTa - large ( Liu et al . ,   2019 ) which shown even better performance with   only a few labels ( Du et al . , 2021 ) . Moreover , we   can explore more advanced uncertainty estimation   approach ( Kong et al . , 2020 ) into AT to fur-   ther improve the performance . Last , apart from   the text classiﬁcation task , we can also extend our   work into other tasks such as sequence labeling and   natural language inference ( NLI ) .   Acknowledgements   We thank the anonymous reviewers for their feed-   back . This work was supported in part by NSF   IIS-2008334 , IIS-2106961 , CAREER IIS-2144338 ,   and ONR MURI N00014 - 17 - 1 - 2656.1430References1431143214331434A Datasets Details   A.1 Data Source   The seven benchmarks in our experiments are all   publicly available . Below are the links to down-   loadable versions of these datasets .   /diamondmathSST-2 : We use the datasets from https://   huggingface.co/datasets/glue .   /diamondmathAGNews : We use the datasets from https://   huggingface.co/datasets/ag_news .   /diamondmathPubmed - RCT : Dataset is available at https :   //github.com / Franck - Dernoncourt/   pubmed - rct .   /diamondmathDBPedia : Dataset is available at   https://huggingface.co/datasets/   dbpedia_14 .   For two weakly - supervised classiﬁcation tasks ,   we use the data from WRENCH benchmark ( Zhang   et al . , 2021 ) .   /diamondmathTREC : Dataset is available at https :   //drive.google.com / drive / u/1/   folders/1v55IKG2JN9fMtKJWU48B_5 _   DcPWGnpTq .   /diamondmathChemProt : The raw dataset is avail-   able at http://www.cbs.dtu.dk/   services / ChemProt / ChemProt-2.0/ .   The preprocessed dataset is available at   https://drive.google.com/drive/u/   1 / folders/1v55IKG2JN9fMtKJWU48B _   5_DcPWGnpTq .   A.2 Train / Test Split   For all the datasets , we use the original   train / dev / test split from the web . To keep the size   of the development set small , we randomly sample   1000 data for SST-2 , AGNews , Pubmed - RCT , DB-   Pedia and randomly sample 500 samples for TREC ,   ChemProt .   B Details on Implementation and   Experiment Setups   B.1 Computing Infrastructure   System : Ubuntu 18.04.3 LTS ; Python 3.6 ; Pytorch   1.6 .   CPU : Intel(R ) Core(TM ) i7 - 5930 K CPU @   3.50GHz .   GPU : NVIDIA 2080Ti . B.2 Number of Parameters   AT and all baselines use Roberta - base ( Liu   et al . , 2019 ) with a task - speciﬁc classiﬁcation head   on the top as the backbone , which contains 125 M   trainable parameters . We do not introduce any   other parameters in our experiments .   B.3 Experiment Setups   Following ( Ein - Dor et al . , 2020 ; Yuan et al . , 2020 ;   Margatina et al . , 2021b ) , all of our methods and   baselines are run with 3 different random seed and   the result is based on the average performance   on them . This indeed creates 4(the number of   datasets)×3(the number of random seeds ) ×   11(the number of methods ) ×10(the number of   ﬁne - tuning rounds in AL ) = 1320 experiments for   ﬁne - tuning PLMs , which is almost the limit of our   computational resources , not to mention additional   experiments on weakly - supervised text classiﬁca-   tion as well as different hyper - parameter tuning .   We have show both the mean and the standard de-   viation of the performance in our experiment sec-   tions . All the results have passed a paired t - test   withp<0.05(Dror et al . , 2018 ) .   B.4 Hyper - parameters for General   Experiments   We use AdamW as the optimizer , and the learning   rate is chosen from 1×10,2×10 } . A lin-   ear learning rate decay schedule with warm - up 0.1   is used , and the number of training epochs is 15   for ﬁne - tuning . For active self - training & SSAL   baselines , we tune the model with 2000 steps , and   evaluate the performance on the development set in   every 50 steps . Finally , we use the model with best   performance on the development set for testing .   B.5 Hyper - parameters for AT   Although AT introduces several hyper-   parameters including K , M , m , m , β , γ , λ ,   most of them are keep ﬁxed during our experiments ,   thus it does not require heavy hyper - parameter tun-   ing . All results are reported as the average over   three runs .   In our experiments , we keep β= 0.5,λ= 1for   all datasets . For other parameters , we use a grid   search to ﬁnd the optimal setting for each datasets .   Speciﬁcally , we search γfrom [ 0.5,0.6,0.7],m   from [ 0.6,0.7,0.8],mfrom [ 0.8,0.9,1 ] . For A-   T with Entropy , we use probability based ag-   gregation and for AT with CAL , we use value1435Hyper - parameter SST-2 AG News Pubmed DBPedia TREC Chemprot   Dropout Ratio 0.1   Maximum Tokens 32 96 96 64 64 128   Batch Size forX 8   Batch Size forXin Self - training 32 48 48 32 16 24   Weight Decay 10   Learning Rate 2×10   β 0.5   M 25 30 30 40 40 40   K 5 10   γ 0.7 0.6   m 0.8 0.9 0.7 0.8 0.8 0.8   m 0.9 0.9 0.8 0.9 0.9 1.0   λ 1   based aggregation by default .   C Runtime Analysis   Table 3 shows the time in one active learning round   ofAT and baselines . Here we highlight that   the additional time for region - aware sampling and   momentum - based memory bank is rather small   compared with the inference time . Also , we ﬁnd   that BALD and REVIV AL are not so efﬁcient . For   BALD , it needs to infer the uncertainty of the   model by passing the data to model with multit-   ple times . Such an operation will make the total   inference time for PLMs very long . For REVIV AL ,   we ﬁnd that calculating the adversarial gradient   needs extra forward passes and backward passes ,   which could be time - consuming for PLMs withmillions of parameters.1436