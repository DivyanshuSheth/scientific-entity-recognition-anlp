  Emily GoodwinSiva ReddyTimothy J. O’DonnellDzmitry BahdanauDepartment of LinguisticsSchool of Computer Science McGill University , CanadaQuebec Artiﬁcial Intelligence Institute ( Mila)ServiceNow ResearchCanada CIFAR AI ChairFacebook CIFAR AI Chair   Abstract   Compositionality — the ability to combine fa-   miliar units like words into novel phrases and   sentences — has been the focus of intense in-   terest in artiﬁcial intelligence in recent years .   To test compositional generalization in seman-   tic parsing , Keysers et al . ( 2020 ) introduced   Compositional Freebase Queries ( CFQ ) . This   dataset maximizes the similarity between the   test and train distributions over primitive units ,   like words , while maximizing the compound   divergence — the dissimilarity between test and   train distributions over larger structures , like   phrases . Dependency parsing , however , lacks   a compositional generalization benchmark . In   this work , we introduce a gold - standard set   of dependency parses for CFQ , and use this   to analyze the behavior of a state - of - the art   dependency parser ( Qi et al . , 2020 ) on the   CFQ dataset . We ﬁnd that increasing com-   pound divergence degrades dependency pars-   ing performance , although not as dramatically   as semantic parsing performance . Addition-   ally , we ﬁnd the performance of the depen-   dency parser does not uniformly degrade rel-   ative to compound divergence , and the parser   performs differently on different splits with   the same compound divergence . We explore   a number of hypotheses for what causes the   non - uniform degradation in dependency pars-   ing performance , and identify a number of   syntactic structures that drive the dependency   parser ’s lower performance on the most chal-   lenging splits .   1 Introduction   People understand novel combinations of familiar   words in part due to the principle of composition-   ality : We expect the meaning of a phrase to be   a predictable composition of the meanings of its   parts . Unlike humans , many neural models fail toSPARQL Query :   SELECT count ( * ) WHERE {   ? x0 ns : film.actor.film M1 .   ? x0 ns : film.editor.film M0 .   ? x0 ns : film.producer.film M0 .   ? x0 ns : people.person.gender ns : m.02zsn   }   Dependency Parse :   DidM1’sfemale actor edit andproduce M0 ?   1Figure 1 : An example question from the CFQ dataset ,   with the associated SPARQL query and dependency   parse .   generalize compositionally ; a growing interest in   this area has led to novel architectures and datasets   designed to test compositional generalization ( see   § 7 ) .   One recently - introduced semantic parsing   dataset , Compositional Freebase Queries ( CFQ ) ,   consists of English questions with corresponding   database queries written in SPARQL . Figure 1   shows an example question and SPARQL query .   To test compositional generalization , CFQ includes   test and train sets with a highly similar distribution   of primitive units ( like words ) and increasingly di-   vergent distribution of larger compound units ( like   phrases ) . The most challenging of these splits , with   the highest compound divergence , are dubbed max-   imum compound divergence ( MCD ) splits .   Although CFQ has proven to be a valuable re-   source , the difﬁculty of the splits appears to be   inﬂuenced by factors other than compositional gen-   eralization . First , some evidence suggests that the   complexity of the SPARQL output is in part re-   sponsible for CFQ performance ( Furrer et al . , 2020 ;   Herzig et al . , 2021 ) . Furthermore , splits of the same   compound divergence are not equally difﬁcult . One6482possible explanation is a difference in the syntactic   constructions of different splits ; however , this has   not yet been explored in CFQ . To address these   issues , we created a dependency - parsing version of   CFQ . Using our dataset , we evaluated a state - of-   the - art dependency parser for compositional gener-   alization , and used the dependency annotations to   identify syntactic structures predictive of parsing   failure on each MCD split .   We found that the dependency parser is more   robust to increased compound divergence than the   semantic parser , but performance still decreased   with higher compound divergence . We also found   the dependency parser , like semantic parsers , var-   ied widely in performance on different splits of   the same compound divergence . Finally , we found   that a small number ( less than seven ) of syntactic   constructions seem to drive the difﬁculty of the   MCD splits . Our dataset is publically available on   github .   1.1 Motivation for Dependency Parsing   In this section , we discuss three problems of CFQ ,   and our motivation for studying compositional gen-   eralization in dependency parsing .   First , CFQ is hard : seq2seq models trained from   scratch score at most 12 % on MCD2 and MCD3   sets ( Google Research , 2020 ) . Because of its difﬁ-   culty , CFQ may lack sensitivity to capture small but   signiﬁcant progress in neural modelling of compo-   sitionality . Second , recent work shows that CFQ ’s   difﬁculty is in part due to the output representation   being raw SPARQL : Models perform better when   outputs are replaced with compressed versions of   SPARQL , that are more aligned with the natural-   language - like questions ( Furrer et al . , 2020 ; Herzig   et al . , 2021 ) . In interpreting performance on CFQ ,   we might be conﬂating challenges of compositional   generalization with challenges related to the output   representation .   Third , different splits of the same compound di-   vergence vary widely in difﬁculty : seven of the nine   semantic parsers currently listed on the leaderboard   perform at least twice as well on MCD1 as MCD   2 , despite the splits having the same compound   divergence ( Google Research , 2020 ) . Performance   on CFQ is thus heavily inﬂuenced by some factor   about the splits other than compound divergence .   Finally , CFQ lacks a description of the speciﬁc   syntactic generalizations tested by each split . Re - lated benchmarks , like COGS ( Kim and Linzen ,   2020 ) and CLOSURE ( Bahdanau et al . , 2020 ) , test   a clearly - deﬁned set of generalizations ( for exam-   ple , training a noun in subject position and testing   in object position ) . CFQ splits , by contrast , opti-   mize a gross metric over the distribution of all syn-   tactic compounds in the dataset . This complicates   in - depth analyses of CFQ results : For a particular   split , it is unclear what syntactic constructions are   tested in out - of - distribution contexts . Meanwhile ,   for a particular test sentence , it is unclear which of   its syntactic structures caused the model to fail .   To address the issues with the CFQ semantic   parsing benchmark , we studied compositional gen-   eralization in syntactic parsing . While syntactic   parsing is simpler than mapping to a complete   meaning representation , a language - to - SPARQL   semantic parser must understand the question ’s   syntax . For example , to generate the triple ? x0   ns : film.editor.film M0 in the SPARQL   query shown in Figure 1 , a semantic parser must   ﬁrst identify that “ actor ” is the subject of “ edit ” .   We chose dependency trees as the target syntac-   tic formalism due to the maturity of the universal   dependencies annotation standard , the popularity   of dependency trees among the NLP practitioners ,   and the availability of popular high - performance   software such as Stanza ( Qi et al . , 2020 ) . Impor-   tantly , dependency parsing does not require auto re-   gressive models ; instead , graph dependency parsers   independently predict edge labels . This different   way of employing deep learning for parsing has   the additional advantage of allowing us to sepa-   rate the challenge of compositional generalization   from challenges related to auto regressive models ’   teacher forcing training . Finally , having gold de-   pendency annotations for CFQ questions enables   detailed analysis of the relation between the model   errors and syntactic discrepancies that are featured   by the MCD splits .   2 Compound Divergence in CFQ   CFQ is designed to test compositional generaliza-   tion by combining familiar units in novel ways . To   ensure the primitive units are familiar to the learner ,   CFQ test and train sets are sampled in a way that en-   sures a low divergence in the frequency distribution   ofatoms . Here , atoms refers to individual predi-   cates or entities , ( like “ produced ” or “ Christopher   Nolan ” ) , and the rules used to generate questions .   To ensure the compounds in test are novel , train6483and test sets were sampled in a way that ensures   higher divergence between the frequency distribu-   tion of compounds , weighted to prevent double-   counting of any nested compounds which co - occur   frequently .   Keysers et al . ( 2020 ) released dataset splits with   compound divergence on a scale between 0 ( a ran-   dom split ) and .7 ( Maximum Compound Diver-   gence , or MCD , splits ) .   3 Corpus Construction : Dependency   Parses for CFQ   To train a dependency parser and analyze syntactic   structures in the CFQ dataset , we created a corpus   of gold dependency parses . Because the questions   in CFQ are synthetically generated , we were able to   write a full - coverage context - free grammar for the   CFQ language ( see Appendix C ) . Using this gram-   mar , and the chart parser available in Python ’s nat-   ural language toolkit , we generated a constituency   parse for each question . Finally , we designed an   algorithm to map to the dependency parse .   To map from constituency to dependency parses ,   we wrote a dependency - mapping rule for each pro-   duction rule in the CFG ( Collins , 2003 ) . Each   dependency rule describes the dependency relation   between the elements in the constituent ; for exam-   ple , if the production rule is VP !V NP , the   dependency - mapping rule connects the head of the   right - hand node ( the head of NP ) as a dependent of   the left - hand node ( the V ) , with the arc label .   We follow version two of the Universal Depen-   dencies Corpus annotation standards ( Nivre et al . ,   2020),but simplify the categorization of nominal   subjects for active and passive verbs into one cat-   egory ( ) , and do not include part of speech   tags in the dataset .   Our algorithm then recursively walks the con-   stituency tree from bottom to top , mapping non-   head children of each node to their syntactic heads   and passing the head of each constituent up the   tree . A number of sentences in the CFQ dataset   exhibit dependency structures which can not be di-   rectly read off the constituency parse in this man-   ner : Such right - node - raising constructions involve   a word without a syntactic head in the immediate   constituent . For example , in “ Was Tonny written   by and executive produced by Mark Marabella ? ”   the ﬁrst instance of “ by ” is a dependent of “ Mark   Marabella ” , but its immediate constituent is “ di - rected by ” . To handle right - node raising cases , our   dependency - mapping algorithm identiﬁes preposi-   tions with no head in the immediate constituent ,   and passes them up the tree until they can be at-   tached to their appropriate syntactic head .   Finally , we performed a form of anonymization   on the questions , replacing entities with single-   word proper names . This reﬂects the anonymiza-   tion strategy used in Keysers et al . ( 2020 ) , and   prevents the dependency parser from failing be-   cause of named entities with particularly complex   internal syntax ( for example , “ Did a Swedish ﬁlm   producer edit Giliap andWho Saw Him Die ? ” )   The experiments in this paper are based on the   original CFQ splits . However , these validation sets   are constructed from the same distribution as the   test sets ; some information about the test distribu-   tion is therefore available during train . To ensure   that the model only had access to the training distri-   bution during the training phase , we followed the   suggestion of Keysers et al . ( 2020 ) and discarded   the MCD validation sets , randomly sampling 20 %   of the training data to use instead ( see § 5.1 of that   paper for more details ) . The resulting splits have   11;968test sentences and 76;595train sentences .   4 Compound Divergence Effect on   Dependency Parsing   4.1 Training Stanza   To evaluate the effect of compound divergence on   dependency parsing , we used Stanza ( Qi et al . ,   2020 ) , a state - of - the - art dependency parser , on the   gold label dependency parses described in § 3 . We   trained Stanza ﬁve times on each of 22 splits from   the CFQ release : one random split ( which has a   compound divergence of 0 ) , 18 splits with increas-   ing compound divergence ( ranging from .1 to .6 )   and three MCD splits ( divergence of .7 ) .   To evaluate performance on each test set , we   used the CoNLL18 shared task dependency pars-   ing challenge evaluation script ( CoNLL Shared   Task , 2018 ) , which gives a Labeled Attachment   Score ( LAS ) and Content - word Labeled Attach-   ment Score ( CLAS ) , reﬂecting how many of the   total dependency arcs in the test set were correctly   labeled , and how many of the arcs connecting con-   tent words were correctly labeled , respectively .   In addition , we calculated the percentage of6484test questions for which every content word arc   was correctly labeled , which we call Whole Sen-   tence Content - word Labeled Attachment Score   ( WSCLAS ) . This all - or - nothing evaluation scheme   for each sentence more closely resembles the exact-   match accuracy of semantic parser evaluation .   4.2 Dependency Parsing Results   We plot Stanza ’s performance as a function of the   split compound divergence in Figure 2 . Increas-   ing compound divergence had a negative effect   on performance : Stanza ’s accuracy on the random   split ( zero compound divergence ) was near perfect ,   with an average CLAS of 99:98 % and WSCLAS of   99:89 % . Meanwhile , accuracy on the three MCD   splits ( divergence of .7 ) dropped to an average   CLAS of 92:85 % and WSCLAS of 74:92 % .   A linear regression predicting CLAS found a   slope of 6:91 , and predicting WSCLAS found   a slope of 28:89 ; in other words , for each .1 in-   crease in compound divergence the linear model   predicts a 2:889 % lower WSCALS , and : 691 %   lower CLAS . These linear models are also shown   in Figure 2 .   We note , however , two exceptions to the gener-   ally negative relationship between compound di-   vergence and accuracy , which indicate that other   characteristics of the test set have a large effect   on accuracy . First , all splits with a target com-   pound divergence of : 4performed stronger than   those at divergence : 3and:2 . Secondly , we ob-   served considerable variation in performance on   different splits that have the same compound diver-   gence , particularly the MCD splits .   Stanza ’s performance on the three maximum-   compound - divergence splits and one random split   is shown in Table 1 . While all three MCD splits   were harder than the random split , performance   varied from 96:57 % WSCLAS ( MCD1 ) to 56:76 %   WSCLAS ( MCD3 ) . Thus , while compound diver-   gence is a factor in performance , idiosyncrasies   in the individual splits also have large effects on   performance .   Finally , we note that while Stanza was more ro-   bust to compound divergence than the semantic   parser , it also ranked the splits differently in dif-   ﬁculty . Table 1 reproduces mean accuracies from   Keysers et al . ( 2020 ) ’s strongest - performing seman-   tic parser , a universal transformer ( Dehghani et al . ,   2019 ) . The universal transformer ’s exact - match   is lower than Stanza ’s WSCLAS on every MCD   split . Additionally , while Stanza performed worst   on MCD3 , the universal transformer and most other   semantic parsers in the CFQ leaderboard performed   worst on MCD2 ( Google Research , 2020 ) . In the   next sections , we explore what causes the variation   in performance on different MCD splits .   5 Construction Complexity and the   MCD Splits   The compound divergence metric treats all com-   pounds of any number of words identically ; there-   fore , the differences between the MCD splits may   be driven by differing distributions of compounds   of different complexities . In this section , we show6485that this is not the case . We ﬁrst describe how   we characterize syntactic constructions using the   dependency annotations .   5.1 Syntactic Constructions   We explored differences in the distributions of   syntactic constructions by looking at a restricted   set of the subtrees of each dependency parse , which   we will now describe .   With respect to any target node in the corpus ,   we consider a syntactic construction to be any sub-   tree that consists of that target node together with a   constituent - contiguous subset of the target node ’s   immediate children . Here , constituent - contiguous   means the subsets of child nodes which are heads   of phrases that are adjacent to one another or to   the target node in the string . We include only the   immediate children in the subtree ( excluding their   descendants ) . We also replace words with their   category label in CFQ : in addition to traditional   parts of speech like verb andadjective , the cate-   gory labels include nominal categories role(which   occurs in possessive constructions like “ mother ” in   “ Alice ’s mother ” ) , entity for proper nouns , and noun   for common nouns .   For the analyses in this and the following section ,   we extract every syntactic construction for every   dependency parse in our corpus , and compare their   complexity . We deﬁne complexity to be the number   of arcs in the subtree , discounting the dummy   arc . Two of the subtrees for sentence “ Did M1 ’s   female actor edit and produce M0 ? ” are shown in   Figure 3 ( these subtrees have a complexity of two ) .   Table 2 shows the number of unique constructions   in each test and train set .   5.2 Analysis of MCD Splits   One possible source of the differences between   MCD splits may be that they differ in their distribu-   tions of subtrees at differing complexities . In this   section , we present two analyses showing that this   is not the case .   In our ﬁrst analysis , we analyzed the distance   between test and train distribution for each split .   To do this we calculated the Jensen - Shannon ( JS )   distance between the test and train histograms of   syntactic constructions at differing complexities .   The JS distances for constructions of each com-   plexity are plotted in Figure 4 . As can be seen in   the ﬁgure , the distances between test and train are   similar for all MCD splits at all subtree complex-   ities . Even the MCD1 distances pattern with the   other MCD splits , despite the parser performance   on MCD1 being more similar to the random split.6486Thus , differences between the test and train distri-   butions at different complexities can not explain the   MCD splits ’ differential performance .   In our second analysis , we examined whether the   MCD splits differ in the proportion of untrained   subtrees at different complexities . The proportions   are plotted in Figure 5 . The MCD splits pattern   together , with far more untrained constructions at   each complexity than the random split .   We thus conclude it is unlikely that gross dis-   tributional properties of the MCD splits explain   the differences in parser performance . In the next   section , we show that parser mistakes for all splits   seem to be driven by a very small number of hard-   to - parse subtrees . Thus , performance differences   between splits likely depend on idiosyncratic inter-   actions between the speciﬁc data splits and models .   6 Syntactic Analyses of Dependency   Parser Outputs   6.1 Identifying Difﬁcult Subtrees   To identify syntactic constructions that are predic-   tive of dependency parsing error , we ﬁt a logistic   model predicting Stanza ’s performance on each   test question from the question ’s syntactic construc-   tions . Because we trained ﬁve randomly - initialized   versions of Stanza , the model was ﬁt with ﬁve in-   stances of each question . To encourage sparse sub-   tree feature weights , we used L1 regularization .   We used 90 % of the test set to train the logistic   model , and the remaining 10 % to test it and select   a regularization coefﬁcient of : 01 .   To analyze the subtrees most predictive of pars-   ing failure , we extracted from the model all sub - trees with a coefﬁcient less than or equal to  1 .   Finally , to quantify the effect these trees have on   test performance , we removed all the sentences   containing the trees for each split , and calculated   Stanza ’s accuracy on the remaining test sentences .   6.2 Subtrees Predictive of Parsing Error   Table 3 shows the number of subtrees found to   be predictive of parsing error , together with the   accuracy when those trees are removed from test .   Removing ﬁve subtrees from MCD2 ’s test set im-   proves the accuracy to 92:46 % ( an increase of   21:05 % ) , and removing seven trees from MCD3 ’s   test set improves the accuracy to 93:09 % ( an in-   crease of 36:33 % ) . We thus conclude that the per-   formance degradation of Stanza on higher com-   pound divergence splits is driven by a relatively   small number of syntactic constructions .   Table 4 shows the subtrees most predictive of a   dependency parsing error , with their test and train   frequency . To quantify the effect of each subtree   on the test accuracy , we also report the Test set  :   WSCLAS ( T) WSCLAS ( T)where Tis the orig-   inal test set and Tis all test sentences which do not   include the construction . A positive means that   removing the subtree from the test set improved   performance , while a negative indicates that re-   moving the subtree from the test set degraded per-   formance .   Subtrees that are predictive of error for a particu-   lar split are often missing from train , together with   others that share a similar syntactic structure . For   instance , there are a set of trees that form questions   with common nouns as subject and predicate , and   a copula verb “ was ” appearing to the left of the   subject ( e.g. “ Was an art director of Palm County   a person?”).The fourth , ﬁfth and sixth subtrees   in Table 4 are subtrees which form these questions ;   all three are missing from train for both MCD2   and MCD3 , and all are predictive of parser error   for these splits . In contrast , the MCD1 training set   includes one of the subtrees ( fourth in Table 4 )   and leaves the other two untrained ; none are pre-   dictive of parser error ( with of 0.0 , -0.06 and   0.02 , the performance on these trees is close to av-   erage for MCD1 ) . The model performs better on   the untrained trees in MCD1 , perhaps because of   the similar trees in train ; with no evidence of this64876488kind of structure in MCD2 and MCD3 , the model   struggles .   Another group of subtrees with similar syntactic   structure is the second and last subtrees in Table 4 .   These coordinate three and four entities in an “ of”-   type prepositional phrase , which occurs in phrases   like “ the mother of Alice , Bob , Carl and Dave ” .   Both trees are absent from MCD1 train , and both   have a large effect on performance for MCD1 (    of 1.29 and 1.33 ) . In MCD2 , only the tree with   four coordinated entities is absent from train , and it   is not difﬁcult for the model ( of -0.15 , indicating   that removing it from test reduces performance ) ;   the model is likely able to parse four coordinated   entities based on the training examples with three   coordinated entities .   7 Related work   A growing body of work uses CFQ to investigate   better models for compositional generalization in   semantic parsing ( Herzig and Berant , 2021 ; Guo   et al . , 2020 ; Furrer et al . , 2020 ) . Tsarkov et al .   ( 2020 ) also recently released an expanded version   of CFQ called * -CFQ , which remains challenging   for transformers even when they are trained on   much more data . Our methodology can be easily be   applied to * -CFQ at the cost of a straight - forward   extension of the grammar .   Other datasets focused on compositional gener-   alization include SCAN ( Lake and Baroni , 2018 ) ,   a dataset of English commands and navigation se-   quences ; gSCAN ( Ruis et al . , 2020 ) , a successor to   SCAN with grounded navigation sequences ; and   COGS ( Kim and Linzen , 2020 ) , where English   sentences are paired with semantic representations   based on lambda calculus and the UDepLambda   framework ( Reddy et al . , 2017 ) . In contrast to   CFQ , these datasets challenge models by targeting   speciﬁc , linguistically - motivated generalizations .   For example , COGS includes tests of novel verb   argument structures ( like training on a verb in ac-   tive voice and testing in passive voice ) , and novel   grammatical roles for primitives ( like training with   a noun in object position and testing in subject po-   sition ) ; similarly , SCAN includes splits which testnovel combinations of speciﬁc predicates ( train-   ing a predicate “ jump ” or “ turn left ” in isolation ,   and testing it composed with additional predicates   from train ) . Finally , the CLOSURE benchmark for   visual question answering tests systematic gener-   alization of familiar words by constructing novel   referring expressions ; for example , “ a cube that is   the same size as the brown cube ” ( Bahdanau et al . ,   2020 ) .   8 Conclusion   In this paper , we presented a dependency pars-   ing version of the Compositional Freebase Queries   ( CFQ ) dataset . We showed that a state - of - the - art   dependency parser ’s performance degrades with   increased compound divergence , but varies on dif-   ferent splits of the same compound divergence . Fi-   nally , we showed the majority of the parser failures   on each split can be characterized by a small ( seven   or fewer ) number of speciﬁc syntactic structures .   To our knowledge , this is the ﬁrst explicit test of   compositional generalization in dependency pars-   ing . We hope that the gold - standard dependency   parses that we have developed will be a useful re-   source in future work on compositional generaliza-   tion . Existing work on syntactic ( and in particu-   lar dependency ) parsing can provide researchers   in compositional generalization with ideas and in-   spiration which can then be empirically validated   using our corpus .   Finally , our work represents a step forward in   understanding the syntactic structures which drive   lower performance on MCD test sets . Predicting   parser performance from the syntactic construc-   tions contained in the question provides a new   method for understanding the syntactic structures   that can cause parser failure ; in future work , simi-   lar methods can also be used to better understand   failures of semantic parsers on the CFQ dataset .   Ethical Considerations   This article contributes to compositional general-   ization research , a foundational concern for neural   natural natural language processing models . Break-   throughs in this research might eventually lead to   smaller , and more efﬁcient models , as well as bet-   ter performance on low - resource languages . The   ethical and societal consequences of these improve-   ments will depend on downstream applications .   The resource released in this work is a new set of   annotations for CFQ , an existing dataset . The origi-6489nal CFQ dataset was artiﬁcially generated , so there   was no process of data collection and therefore no   ethics review process . The dataset was annotated   by the author , so there was no ethics review of the   annotation process or demographic information of   this population to report .   Acknowledgements   We thank Christopher Manning , the Montreal   Computational and Quantitative Linguistics lab at   McGill University , and the Human and Machine In-   teraction Through Language group at ServiceNow   Research for helpful feedback . We are grateful   to ServiceNow Research for providing extensive   compute and other support . We also gratefully   acknowledge the support of the MITACS accel-   erate internship , the Natural Sciences and Engi-   neering Research Council of Canada , the Fonds de   Recherche du Québec , Société et Culture , and the   Canada CIFAR AI Chairs Program .   References6490   A Correlation of Semantic Parsing and   Dependency Parsing Errors   Because syntactic parsing is a necessary sub - task   for semantic parsing , we also explored the pos-   sibility that dependency parsing errors might be   predictive of semantic parsing errors . We extracted   the predictions from Keysers et al . ( 2020 ) ’s trans-   former model ( which is based on Vaswani et al .   ( 2017 ) ’s model ) , and compared them to those of   Stanza on the same test set . For each test sentence ,   we calculated the number of times the parsers cor-   rectly parsed the sentence ( out of ﬁve experiments   each ) .   The results for MCD1 and MCD3 are shown   in Figure 6 : for example , the top - right hand cor-   ner of the MCD1 matrix means that 23.99 % of   the test set was correctly parsed in all semantic   and dependency parsing experiments , while the top   right - hand corner in the MCD3 matrix indicates   that only 7.68 % of the sentences were correctly   parsed by both models in all experiments . The se-   mantic parser fails for all ﬁve experiments on the   majority of sentences . We do note some trends in   error patterns between the models : for example ,   no sentences are correctly parsed by all semantic   parsers without also being correctly parsed by the   dependency parser at least a few times . However ,   overall it does not appear that dependency parsing   performance is strongly related to semantic parsing   performance . B Proportion of Few - shot Constructions   in MCD Splits   We examined whether the MCD splits differ in the   proportion of test syntactic constructions which are   few - shot , meaning they appear in train fewer than   four times . This analyses is similar to the ones   described in § 5.2 .   The proportions are plotted in Figure 7 . The   MCD splits pattern together , with far more few-   shot constructions at each complexity than the ran-   dom split .   C CFG   Below are the rules in our Context Free Grammar .   Using these rules we parsed CFQ into constituency   trees , and then mapped to dependency trees as de-   scribed in § 3 .   S !NPQ VP Qmark   S !NPQ was Nominal Qmark   S !NPQ did NPV Qmark   S !was Nominal V obl Qmark   S !NPQ V obl Qmark   S !was Nominal Adj Qmark   S !was Nominal Nominal Qmark   S !did Nominal VP Qmark   NPV !Nominal V   NPV !Nominal VPrep   VP !V Nominal   VP !was V obl   VPrep !was VPrep   VPrep !V by6491V obl !VPrep Nominal   NPQ !WhW Nominal   NPQ !WhW role caseO   commonNoun !commonNoun RC   RC !V obl   RC !R VP   RC !R NPV   RC !whose role VP   VP !VP andVP   VP !VPx andVP   VPx !VP punctVP   VPx !VPx punctVP   andVP !conj VP   andVP !punct conj VP   punctVP !punct VP   V obl !V obl andV obl   V obl !V oblx andV obl   V oblx !V obl punctV obl   V oblx !V oblx punctV obl   andV obl !conj V obl   andV obl !punct conj V obl   punctV obl !punct V obl   VPrep !VPrep andVPrep   VPrep !VPrepX andVPrep   VPrepX !VPrep punctVPrep   VPrepX !VPrepX punctVPrep   andVPrep !conj VPrep   andVPrep !punct conj VPrep   punctVPrep !punct VPrep   V !V andV   V !Vx andV   Vx !V punctV   Vx !Vx punctV   andV !conj V   andV !punct conj V   punctV !punct V   Vx !Vx punctVPrep   Vx !V punctVPrep   V !Vx andVPrep   V !V andVPrep   VPrep !VPrep andV   VPrep !VPrepX andV   VPrepX !VPrep punctV   VPrepX !VPrepX punctVNPV !NPV andNPV   NPV !NPVx andNPV   NPVx !NPV punctNPV   NPVx !NPVx punctNPV   andNPV !conj NPV   andNPV !punct conj NPV   punctNPV !punct NPV   V !F V   Nominal !Name   Nominal !DP   Nominal !commonNoun   DP !caseS role   caseS !DP pS   caseS !Name pS   DP !det role caseO   caseO !of DP   caseO !of Name   DP !det commonNoun   Name !Name andName   Name !Namex andName   Namex !Name punctName   Namex !Namex punctName   andName !conj Name   andName !punct conj Name   punctName !punct Name   commonNoun  ! commonNoun andCom-   monNoun   commonNoun !commonNounx andCommon-   Noun   commonNounx !commonNoun punctCommon-   Noun   commonNounx  ! commonNounx punctCom-   monNoun   andCommonNoun  !conj commonNoun   andCommonNoun  !punct conj commonNoun   punctCommonNoun  !punct commonNoun   role !role androle   role !rolex androle   rolex !role punctrole   rolex !rolex punctrole   androle !conj role   androle !punct conj role6492punctrole !punct role   commonNoun !F commonNoun   role !F role   role !Cnt of nat   commonNoun !P commonNoun   commonNoun !Adj commonNoun   role !Adj role   punct  ! ,   Cnt !country   nat !nationality   P !production   F !ﬁlm | art | executive | costume   V !VP_SIMPLE | direct | produce ...   Name !entity | Alice | Bob ...   commonNoun  ! NP_SIMPLE | character |   person ...   role !ROLE_SIMPLE | character | person ...   NPQ !who | what   WhW !What | Which | what | which   did !did | Did   conj !and   pS !‘s   of !of   det !a | an   by !by   Adj !ADJECTIVE _ SIMPLE | female | Ameri-   can ...   was !was | were   R !that   whose !whose   Qmark !?6493