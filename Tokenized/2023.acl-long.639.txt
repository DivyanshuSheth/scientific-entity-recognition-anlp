  Derek Xu , Shuyan Dong , Changhan Wang , Suyoun Kim , Zhaojiang Lin ,   Bing Liu , Akshat Shrivastava , Shang - Wen Li , Liang - Hsuan Tseng ,   Guan - Ting Lin , Alexei Baevski , Hung - yi Lee , Yizhou Sun , Wei WangUniversity of California , Los AngelesMeta AINational Taiwan University   Abstract   Recent studies find existing self - supervised   speech encoders contain primarily acoustic   rather than semantic information . As a re-   sult , pipelined supervised automatic speech   recognition ( ASR ) to large language model   ( LLM ) systems achieve state - of - the - art results   on semantic spoken language tasks by utilizing   rich semantic representations from the LLM .   These systems come at the cost of labeled   audio transcriptions , which is expensive and   time - consuming to obtain . We propose a task-   agnostic unsupervised way of incorporating   semantic information from LLMs into self-   supervised speech encoders without labeled au-   dio transcriptions . By introducing semantics ,   we improve existing speech encoder spoken   language understanding ( SLU ) performance by   over 5 % on intent classification ( IC ) , with mod-   est gains in named entity resolution ( NER ) and   slot filling ( SF ) , and spoken question answering   ( SQA ) FF1 score by over 2 % . Our approach ,   which uses no ASR data , achieves similar per-   formance as methods trained on over 100 hours   of labeled audio transcripts , demonstrating the   feasibility of unsupervised semantic augmenta-   tions to existing speech encoders .   1 Introduction   Realizing artificial intelligence ( AI ) that can under-   stand and respond to spoken language is a north star   for many speech and natural language processing   ( NLP ) researchers . A particularly effective frame-   work for this is the encoder - decoder architecture ,   where an encoder represents input audio signals   as high - dimensional embeddings and a decoder   converts said embeddings to outputs for different   downstream tasks . Benchmarks for such systems   include spoken language understanding , where in-   tent , named entities , or slot values are predicted   from input utterances ( Yang et al . , 2021 ; Bastianelli   et al . , 2020 ; Shon et al . , 2022 ) , and spoken ques-   tion answering , where the start and end frames ofan input audio passage answering an input audio   question are predicted ( Lin et al . , 2022a ) .   A particularly notable setup of the encoder-   decoder framework is the universal representation   setup ( Yang et al . , 2021 ) , where a shared self-   supervised speech encoder is pretrained upstream   once and frozen for all downstream tasks , then a   different lightweight decoder is fine - tuned on each   downstream task . This setup is appealing for build-   ing speech systems as maintaining a separate large   specialized model for every task is not computation-   ally efficient . The universal representation setup   has been widely adopted in other areas of research ,   such as computer vision ( Goyal et al . , 2019 ; Eric-   sson et al . , 2021 ) and NLP ( Rogers et al . , 2020 ;   Qiu et al . , 2020 ) , and production when there are   many downstream tasks or domains ( Molino et al . ,   2019 ) . The current state - of - the - art speech encoders   under this setup are22andH ( Yang   et al . , 2021 ; Baevski et al . , 2020 ; Hsu et al . , 2021 ) ,   which are transformer - based models trained with   self - supervised learning ( SSL ) on raw audio and   have achieved impressive performance on various   tasks .   Recently , analytical works found SSL speech   encoders capture primarily acoustic , not semantic ,   information ( Pasad et al . , 2021 ) . Thus , researchers   proposed end - to - end systems ( Chung et al . , 2020b ;   Kim et al . , 2021 ; Qian et al . , 2021 ; Le et al . , 2022 ;   Seo et al . , 2022 ; Lin et al . , 2022a ) that introduce se-   mantic information through large language models   ( LLMs ) , such as RBERT(Liu et al . , 2019 ) or   BART ( Lewis et al . , 2019 ) , which are pretrained   to capture language semantics ( Clark et al . , 2019 ) .   This is typically accomplished by the pipeline ap-   proach ( Bastianelli et al . , 2020 ) , which passes au-   dio input through the SSL speech encoder , then   bridge module , then LLM . The bridge module con-   verts speech encoder embedding outputs into LLM   token inputs ( Lugosch et al . , 2019 ; Rao et al . , 2021 ;   Lin et al . , 2022a ; Seo et al . , 2022).11413Unsupervised ASR models ( ASR - U ) ( Liu et al . ,   2020b ; Baevski et al . , 2021 ; Liu et al . , 2022 )   have also seen recent success . The state - of - the - art   ASR - U model uses generative adversarial networks   ( GANs ) ( Goodfellow et al . , 2020 ) to generate text   transcription from input audio ( Liu et al . , 2022 ) .   Current works combining SSL speech encoders   and LLMs do not satisfy the universal represen-   tation framework , as they either ( 1 ) rely on ASR   data on the downstream task , which is expensive   to collect and maintain , ( 2 ) are not lightweight , re-   quiring training the whole system end - to - end , or   ( 3 ) are not general , as they do not consider a wide   variety of downstream tasks ( Lugosch et al . , 2019 ;   Rao et al . , 2021 ; Lin et al . , 2022a ; Seo et al . , 2022 ) .   Similarly , ASR - U was proposed for speech recog-   nition and the focus is not improving SSL speech   encoders ( Baevski et al . , 2021 ; Liu et al . , 2022 ) .   We propose introducing Semantics into Speech   Encoders , SSE , a task - agnostic unsupervised way   of incorporating semantic information from LLMs   into self - supervised speech encoders without la-   beled audio transcriptions . Concretely , SSE adopts   the pipeline approach to obtain semantic embed-   dings , with an ASR - U bridge connector to extract   information from LLMs . As ASR - U is inherently   noisy , SSE introduces attention residual connec-   tion ( He et al . , 2016 ; Vaswani et al . , 2017 ) be-   tween the speech encoder and LLM . SSE also ef-   ficiently aligns the LLM with the speech encoder   through adapter modules ( Houlsby et al . , 2019 ) .   SSE improves22(Baevski et al . , 2020 ) and   H ( Hsu et al . , 2021 ) on 3 SLU tasks across   3 datasets , all under the universal representation   setup . SSE also outperforms state - of - the art no-   ASR method , D ( Lin et al . , 2022a ) , in SQA .   While recent works use ASR - U to augment   existing speech encoders with phoneme - level   LLMs ( Feng et al . , 2022 ; Meng et al . , 2022 ; Shi   et al . , 2022 ; Hsu et al . , 2022 ) , subword - level LLMs   contain much more pertinent and measurable se-   mantic information ( Clark et al . , 2019 ) . Other   works in SQA rely on clustering to assign audio   frames to frequent subword tokens , but this requires   heavy finetuning on the downstream task ( Lin et al . ,   2022a ) .   To the best of our knowledge , we are the first to   propose a task - agnostic SSL speech encoder which   directly interfaces with subword - based LLMs , un-   blocking many other applications and future work   in this domain . To this end , attention residual con - nections and adapters are essential to successfully   extracting semantic information from noisy inter-   mediary transcriptions . We summarize our contri-   butions below :   •We propose using ASR - U components to aug-   ment SSL speech encoders for generating sub-   word tokens with semantic information .   •The augmented SSL speech encoders can be   connected with powerful LLMs seamlessly   and yields state - of - the - art performance under   the universal representation setup .   •We show attention residual connections and   adapters are essential to combining and align-   ing speech and text encoders .   2 Related Works   2.1 Self - Supervised Speech Encoders   SSL speech encoders ( Liu et al . , 2020a ; Chung   et al . , 2020a ; Ling and Liu , 2020 ; Liu et al . , 2021 ,   2020c ; Chung et al . , 2019 ; Baevski et al . , 2019 ;   Schneider et al . , 2019 ; Baevski et al . , 2020 ; Hsu   et al . , 2021 ; Qian et al . , 2022 ; Zhang et al . , 2022 )   are trained to learn and reconstruct pooled clus-   tered representations of input audio from the orig-   inal audio . The intuition for this objective comes   from linguistics , where speech can be broken down   into phoneme groups , where different chunks of   input audio represent different phoneme groups.2(Schneider et al . , 2019 ) trains a convolu-   tional neural network model to reconstruct the   quantized cluster representations.22(Baevski   et al . , 2020 ) uses transformers and a discrete code-   book quantization module . H ( Hsu et al . ,   2021 ) improves22by disentangling the clus-   tering and SSL objectives and using a BERT -style   encoder ( Devlin et al . , 2018 ) . The speech pro-   cessing universal performance benchmark ( SU-   PERB ) ( Yang et al . , 2021 ; Lin et al . , 2022b ; Tsai   et al . , 2022 ) shows SSL speech encoders are the   most effective method for solving multiple down-   stream tasks with minimal fine - tuning . A recent   analytical work finds SSL speech encoders success-   fully encode acoustic information , but lack seman-   tic information ( Pasad et al . , 2021 ) . In response ,   C V(Qian et al . , 2022 ) propose disen-   tangling the speaker and semantic content of audio   via an SSL objective . S LM ( Zhang et al . ,   2022 ) propose training a multi - modal speech and   text encoder.114142.2 Large Language Models   In contrast to speech encoders , pretrained LLMs   are shown to capture rich semantic informa-   tion ( Clark et al . , 2019 ) . These methods opti-   mize variants of the masked language modeling   ( MLM ) objective to train a large transformer model .   BERT ( Devlin et al . , 2018 ) uses MLM to learn a   transformer encoder . RBERT(Liu et al . , 2019 )   introduces dynamic masking and a larger text cor-   pus . BART ( Lewis et al . , 2019 ) supports genera-   tive modeling and adds a denoising objective , mak-   ing it less susceptible to noisy text inputs . L - ( Beltagy et al . , 2020 ) is pretrained for   long documents by increasing the document length   limit during pretraining . LLMs have been success-   fully integrated with speech models for specific   semantic tasks ( Chung et al . , 2020b ; Kim et al . ,   2021 ; Qian et al . , 2021 ; Le et al . , 2022 ; Seo et al . ,   2022 ; Lin et al . , 2022a ) , but not under the universal   representation framework .   2.3 Task - Specific Speech Models   Task - specific SLU systems outperform generic SSL   speech encoders typically by using a LLM . These   systems rely on ASR data to reliably interface the   LLM . L ( Lugosch et al . , 2019 ) trains a   LSTM bridge module to convert audio features into   phonemes then text . CTI ’s ( Seo et al . , 2022 ) bridge   module uses ASR logits to compute a weighted   average of token embeddings . In addition to im-   proving the bridge module , other works attempt   to also distill LLM embeddings into speech repre-   sentations ( Chung et al . , 2020b ; Cha et al . , 2021 ;   Kim et al . , 2021 ; Agrawal et al . , 2022 ) . For op-   timizing targeted metrics , researchers have also   experimented with reinforcement learning ( Rao   et al . , 2021 ) . While combinations of these meth-   ods achieve impressive performance , they do not   satisfy the universal representation setup .   2.4 Unsupervised ASR   Recent work show the viability of unsupervised   speech recognition.22 - U ( Baevski et al . , 2021 )   accomplished this by running Principal Component   Analysis ( PCA ) , k - means clustering , and mean   pooling to convert22(Baevski et al . , 2020 ) fea-   tures into phoneme - granularity features , then trains   a GAN model to output phoneme text from the   post - processed model ( Baevski et al . , 2021 ) . The   state - of - the - art method for phoneme - level unsuper-   vised ASR is22 - U2.0 ( Liu et al . , 2022 ) whichdirectly trains a CNN to output phonemes from22features and uses a reconstruction loss to tie   the input audio with corresponding generated text .   Both methods use WFSTs to decode the phonemes   into raw text . While there have been preliminary   attempts ( Feng et al . , 2022 ; Meng et al . , 2022 ) to   use22 - U2.0 with phoneme language models ,   we are the first to combine it with semantically - rich   subword - based LLMs .   2.5 Adapters   Adapters are intermediary layers added to a large   pretrained encoder . Adapter weights are learned   during fine - tuning while the rest of the pretrained   model is frozen . Adapters serve the dual purpose   of efficient fine - tuning and preventing overfitting .   First used by computer vision researchers ( Rebuffi   et al . , 2017 ) , adapters now enjoy much success in   the natural language processing community by ef-   ficiently tuning LLMs ( Houlsby et al . , 2019 ) . In   particular , the multilingual speech translation com-   munity found that adapters can effectively align   SSL speech encoders and LLMs for spoken trans-   lation tasks ( Li et al . , 2020 ; Le et al . , 2021 ) .   3 Proposed Method   We propose to introduce semantics into SSL speech   encoders by using ASR - U to interface with LLMs .   Section 3.2 describes how to use ASR - U to link a   speech encoder with a LLM . Section 3.3 describes   how to combine both acoustic and semantic infor-   mation and deal with ASR transcriptions errors .   Finally , Section 3.4 describes how to align LLMs   with the speech encoder for downstream tasks .   3.1 Problem Setting   Following the universal representation frame-   work ( Yang et al . , 2021 ) , our model consists of   a large speech encoder , E :X → Z , mapping in-   put audio , X∈ X , to embeddings , Z∈ Z , and a   light - weight task decoder , D : Z → Y , mapping   embeddings to downstream task outputs , Y∈ Y.   The speech encoder , E , is pretrained once , then   shared on all downstream tasks . The task decoder ,   D , is fine - tuned on its respective task , ω∈Ω.   During fine - tuning , the majority of model weights   are frozen . This ensures the model can be effi-   ciently stored and deployed .   During pretraining , the speech encoder is trained   on unlabelled audio , X∈ X , and unlabeled text,11415   T∈ T. During finetuning , the model is trained   on the labelled downstream dataset , ( X , Y)∈   X × Y. Notice , costly labelled ASR data is not   required during pretraining or finetuning .   3.2 Unsupervised Semantic Representation as   a Bridge   To incorporate semantic information into SSL   speech encoders , E :X → Z , we wish to lever-   age subword - based LLMs , M : S → Z , that   capture language semantics ( Devlin et al . , 2018 ;   Liu et al . , 2019 ; Lewis et al . , 2019 ; Beltagy et al . ,   2020 ) . The major challenge is the mismatch of   input spaces . Speech encoders take raw audio   as input , X∈ X . LLMs take subword tokens   as input , S∈ S .SSE uses22 - U2.0 ( Liu   et al . , 2022 ) as a bridge module ( Seo et al . , 2022 ) ,   B : Z → S , to convert speech encoder embedding   output into LLM subword tokens in a pipelined   approach , E = E ◦ B ◦ M .   Following22 - U2.0 , the bridge module , B   uses a GAN ( Goodfellow et al . , 2020 ) to output   phoneme sequences , P∈ P , conditioned on in-   put audio , X∈ X . The GAN does not directly   predict subword - level transcriptions , because sub - Model Component % of Parameters   SSE- 90.40 %   residual attention 0.73 %   BART adapters 0.18 %   downstream decoder 8.69 %   word barriers are not easily deducible from acoustic   speech embeddings and requires implicitly learn-   ing phoneme - to - subword mappings . Instead , the   bridge module , B , uses a Weighted Finite State   Transducer ( WFST ) , W :P → S , which is fed   known phoneme - to - subword mappings , to map   the generator outputs into subword tokens . The   generator , G : Z → P , and the discriminator ,   C :P → [ 0,1 ] , are both convolutional neural   networks ( CNNs ) . The GAN model is trained on   the same regularized GAN objective as in22-   U2.0 ( Liu et al . , 2022 ) .   The vanilla version of our final model is com-   posed of ( 1 ) SSL speech encoder , E :X → Z pre-   trained on unlabelled audio data , ( 2 ) a CNN+WFST   bridge module , B = W ◦ G : Z → S , trained on   unlabelled text and audio data , and ( 3 ) a LLM ,   M : S → Z , pretrained on unlabelled text data .   We also add an upsampling layer , U : Z → Z to   make the sequence length of the LLM output match   the speech encoder output , such that EandE   share the same output space .   We choose the 15th layer of the22(Baevski   et al . , 2020 ) as our speech encoder , as the last layers   overfit the self - supervised training objective hence   providing worse acoustic representations ( Fan et al . ,   2020 ; Baevski et al . , 2021 ; Pasad et al . , 2021 ) . We   choose BART ( Lewis et al . , 2019 ) as our LLM , as   it is trained to denoise noisy input subword tokens ,   and we expect the bridge module to introduce some   noise . We call this version of our model SSE- .   A depiction can be found in Figure 1a .   3.3 Combining Semantics and Acoustics with   Residual Attention   We hypothesize certain tasks may require more   acoustic information than others . For instance ,   named entity recognition ( NER ) requires the model11416ModelFSC SLURP - IC SLURP - SF SLUE - NER   ( Acc ) ( Acc ) ( F1 ) ( F1)2224 95.28 % 39.77 % 36.48 % 46.10%2215 95.60 % 49.97 % 62.43 % 78.77 %   H 98.76 % 58.11 % 66.97 % 82.88 %   S LM ( H - ) * 97.6 % -% -% -%   S LM ( P - ) * 98.6 % -% -% -%   C V(H - ) 99.10 % 34.03 % 63.83 % 75.19 %   SSE- 95.99 % 55.28 % 61.59 % 79.62 %   SSE- ( 2215 ) 98.71 % 63.64 % 64.48 % 80.10 %   SSE- ( H - ) 98.30 % 58.69 % 64.64 % 76.61 %   SSE- ( H ) 99.44 % 64.33 % 68.82 % 82.02 %   ModelNMSQA   FF1 AOS   D-64 39.0 % 33.0 %   D-128 55.9 % 49.1 %   D-512 17.3 % 12.5 %   SSE- ( A)57.2 % 46.4 %   SSE- ( A)62.0 % 54.7 %   P64.2 % 57.1 %   to implicitly transcribe parts of the input speech , a   primarily acoustic task . Since the pipelined model   may suffer from transcription errors introduced by   ASR - U , naively using the pipelined approach in-   troduces an information bottleneck at the bridge   module . Hence , we propose adding a residual con-   nection ( He et al . , 2016 ) between SSE- and   the speech encoder , E.   This can be done in two ways : ( 1 ) upsam-   pling semantic embeddings and concatenating   with speech embeddings , Z= [ Z||U(Z ) ] ,   or ( 2 ) using multihead attention ( Vaswani et al . ,   2017 ) to merge the two embeddings , Z=   [ Z||MHA ( Z , Z , Z ) ] , where Z∈ Z is the   output of the2215(Baevski et al . , 2020 ) and   Z∈ Z is the output of BART ( Lewis et al . ,2019 ) . The former is a simpler but more naive   method . The latter is more effective as the at-   tention layers are able to learn the alignment be-   tween speech and semantic embeddings . Notice ,   ( 2 ) introduces more learnable parameters to the   finetuning - step , but we find the number of new pa-   rameters inconsequential compared to the size of   the lightweight decoder .   3.4 Aligning Pretrained Text Model with   Adapters   Inspired by works from speech translation ( Li et al . ,   2020 ; Le et al . , 2021 ) , we hypothesize that the LLM   can easily be adapted for speech tasks through the   use of adapters . We adopt the general recipe for   adapters , where an adapter ( Houlsby et al . , 2019 ) ,   composed of a LayerNorm and 2 - layer ReLU neu-   ral network , is added to the end of each feed for-   ward layer in the LLM and finetuned on down-   stream tasks . This introduces additional parame-   ters to finetuning , but we find the number of new   parameters inconsequential compared to the size of   the lightweight decoder . We call the model using   both residual attention and adapters SSE- ,   and outline it in Figure 1b .   4 Experiments   4.1 Dataset   To show the effectiveness of introducing seman-   tics into speech encoders , we evaluate 3 SLU   tasks , intent classification ( IC ) , slot filling ( SF ) ,   and named entity recognition ( NER ) , and SQA11417Augmentation FSC - IC ( Acc ) SLURP - IC ( Acc ) SLURP - SF ( F1 ) SLUE - NER ( F1)2215 95.60 % 49.97 % 62.43 % 78.77 %   SSE- 95.99 % 55.28 % 61.59 % 79.62 %   SSE- ( Byt5 ) 95.80 % 35.50 % 59.15 % 76.44 %   SSE- ( T5lephone ) 95.94 % 41.19 % 60.87 % 77.88 %   SSE- ( R ) 97.55 % 59.59 % 63.37 % 79.66 %   SSE- ( RA ) 98.97 % 62.39 % 64.21 % 80.04 %   SSE- ( A ) 96.07 % 60.28 % 63.85 % 79.97 %   SSE- 98.71 % 63.64 % 64.48 % 80.10 %   task across 4 datasets : Fluent Speech Commands   ( FSC ) ( Lugosch et al . , 2019 ) , Spoken Language   Understanding Resource Package ( SLURP ) ( Bas-   tianelli et al . , 2020 ) , Spoken Language Understand-   ing Evaluation ( SLUE ) ( Shon et al . , 2022 ) , and   Natural Multi - speaker Spoken Question Answering   ( NMSQA ) ( Lin et al . , 2022a ) , covering a wide va-   riety of speakers , microphones , and environments   4.2 Encoder Setup and Baselines   4.2.1 Spoken Language Understanding   To show SSE improves SSL speech encoders , we   augment two state - of - the art speech encoders un-   der the universal representation setup:22and   H . Following prior works that found inter-   mediary layers of22contain better representa-   tions ( Pasad et al . , 2021 ; Baevski et al . , 2021 ) , we   consider the 15th layer and the last layer of22 ,   named2215 and2224 respectively .   As mentioned in Section 3 , we show 2 ver-   sions of our model , SSE- andSSE- .   The former uses the pipelined approach to con-   nect2215with BART ( Lewis et al . , 2019 )   with no additional modifications . The latter intro-   duces an attention residual connection and learn-   able adapters to combine acoustics and semantics   together and align the LLM with the speech en-   coder respectively . We either connect the residual   connection to the output of2215 , yielding   SSE- ( 2215 ) , or to the output of H- , yielding SSE- ( H ) .   To show the importance of using LLMs , we   compare against 2 very recent approaches for   improving SSL speech encoders without LLMs ,   S LM ( Zhang et al . , 2022 ) and C- V(Qian et al . , 2022 ) . As H -was used as the base speech encoder by both base-   lines , we also provide results where SSE- is   used to augment H - .   4.2.2 Spoken Question Answering   To show the effectiveness of SSE , we compare   it against D ( Lin et al . , 2022a ) , the state - of-   the - art SQA model which does not use ASR data .   While both SSE andD obtain frame - level to-   kens from speech input , SSE uses ASR - U to obtain   its tokens , whereas D uses clustering . As a   result , SSE ’s output tokens exists in the LLM ’s ex-   isting vocabulary , whereas D ’s output tokens   does not . Hence , D must retrain the LLM on   its output tokens .   We compare D to the closest analogous SSE   model , which is SSE- but with adapter layers ,   SSE- ( A ) . Similar to D , both meth-   ods modify the LLM weights . Unlike D , SSE- ( A)is lightweight , tuning only around   10 % of the total parameters . To produces frame-   level predictions , we remove the upsampling layer   from SSE- ( A ) . We choose2215   as our speech model and BART as our LLM , as it   is robust to ASR errors .   We also show a P model , which trains   a22model on ASR data and a L   LLM on text - only question answering data . It is   worth noting that since evaluation is based on the   frame - level , SSL speech encoders are not a baseline   since they operate at the audio level .   4.3 Decoder Setup   To satisfy the universal representation setup ,   we adopt lightweight SLU decoders from SU-   PERB ( Yang et al . , 2021 ) . For IC , the decoder11418Bridge Module ASR dataFSC SLURP SLUE   WER IC Acc WER IC Acc SF F1 WER NER F122 - ASR 960h 9.19 % 99.34 % 45.83 % 66.18 % 65.62 % 15.51 % 80.58%22 - ASR 100h 11.89 % 99.10 % 53.22 % 63.20 % 63.87 % 17.70 % 79.67%22 - ASR 10h 59.06 % 98.50 % 74.77 % 59.91 % 63.42 % 53.00 % 79.76 %   SSE- nothing 21.28 % 98.71 % 51.51 % 63.64 % 64.48 % 31.22 % 80.10 %   is sum pooling followed by a multilayer perceptron   classifier trained with cross entropy loss . For the   SF and NER tasks , the decoder is recursive neural   network ( RNN ) that transcribes input audio into   text . The decoder identifies named entities or slot   values by surrounding them with named special   tokens and is trained with connectionist temporal   classification loss . For SQA , we adopt the same   decoder as D ( Lin et al . , 2022a ) , which is a   linear layer classifying each subword embedding   as the start or end or neither of an answer span .   5 Results   5.1 Spoken Language Understanding   5.1.1 Improving SSL Speech Encoders   As seen in Table 2 , SSE significantly improves   the SLU performance of both22and H- , confirming that including semantic infor-   mation drastically improves existing SSL speech   encoder performance . Specifically , SSE-   ( 2215)improves2215on all tasks . SSE- ( H ) improves H on 3 out of   4 tasks , and is the best performing model over-   all . Comparing SSE- with SSE- shows   residual attention and adapters effectively counter-   acts bridge module transcription errors .   The relative performance gain for IC is more   than SF or NER . Unlike IC , both SF and NER re-   quire the speech encoder to transcribe identified au-   dio snippets , and transcription is a primarily acous-   tic task . Hence SF and NER require less seman-   tic information than IC . Nevertheless , combining   both acoustic and semantic information , as done   bySSE- , provides the most consistent perfor-   mance improvement , since the skip connection can   learn which type of information is more needed.5.1.2 Importance of LLMs   As seen in Table 2 , SSE- ( H - )   outperforms alternative approaches augmenting   speech encoders , S LM ( H - )   andC V(H - ) . Unlike these   alternative approaches , SSE- incorporate in-   formation from LLMs , which we found to be very   beneficial for capturing semantic information as   they are carefully pretrained objectives on large   amounts of unlabelled text data .   It is noteworthy that SSE- is a general   framework which can augment any speech encoder   of our choice , including S LM andC- V. Similarly , SSE- can directly in-   tegrate new LLMs without costly pretraining . We   leave incorporating such encoders into SSE-   as future work .   5.2 Spoken Question Answering   As seen in Table 3 , SSE outperforms recent unsu-   pervised clustering - based approaches , D. In   contrast to D’sH cluster tokens , SSE ’s   ASR - U tokens are better aligned with LLMs and   share the same space . Thus , SSE can better uti-   lizes pretrained LLMs . Furthermore , SSE does not   require carefully tuning the number of H   cluster counts , as the vocabulary size of the LLM   is fixed and consistent with ASR - U.   5.3 Ablation Study   5.3.1 Choice of Language Model   We find subword - based LLMs contain more infor-   mation than phoneme - based LLMs ( Clark et al . ,   2019 ) . We empirically verify this by replac-   ing our subword - based LLM , BART ( Lewis   et al . , 2019 ) , with popular character - based LLM ,   ByT5 ( Xue et al . , 2022 ) , and phoneme - based LLM ,   T5lephone ( Hsu et al . , 2022 ) in SSE- . As11419seen in Table 4 , the subword - based LLM perform   the best as each subword token is more seman-   tically meaningful than a phoneme or character .   We believe T5lephone outperforms the Byt5 as it   has better robustness to ASR - U errors . Overall ,   subword - based LLMs are the best choice for em-   bedding semantic information in transcribed text .   5.3.2 Residual Attention and Adapters   To more carefully analyze the affect of residual   attention and adapters in SSE- , we run exper-   iments on all SLU datasets with and without each   component . We denote these two design choices   as ( ResAtt ) and ( Adap ) respectively . As seen in Ta-   ble 4 , both components provide ample performance   improvement over SSE- .   We also try the naive residual connection ap-   proach described in Section 3.3 by directly concate-   nating the LLM upsampled semantic embeddings   to the speech embeddings . We call this approach   SSE- ( R ) . This method is less effective   than SSE- ( RA)as it does not learn   how to align speech and semantic embeddings , but   still improves SSE- , further validating our   hypothesis that merging acoustic and semantic in-   formation is beneficial .   As seen in parameter breakdown for the SSE- ( 2215)model in Table 1 , the number   of new learnable parameters introduced by ( Re-   sAtt ) and ( Adap ) are unsubstantial compared to   the size of the lightweight downstream decoder .   Specifically , the downstream task decoder accounts   for 9.60 % of the total model parameters . SSE- introduces only 10.47 % more parameters   than SSE- during fine - tuning and 0.91 % to   the total model parameter count , but often provides   significant performance improvement .   5.4 Comparison with Supervised ASR   Methods   To quantify the effect of transcription errors intro-   duced by the bridge module , we compute the word   error rate ( WER ) of the bridge connector in SSE- , and compare it against standard22su-   pervised ASR models ( Baevski et al . , 2020 ) trained   on 10 minutes , 100 hours , and 960 hours of la-   beled ASR data . Table 5 confirms that less noisy   transcripts , transcripts with lower WER , correlates   with better downstream performance . The unsu-   pervised model , which uses 960 hours of unla-   belled data , can reach similar WER as a super-   vised model trained on 100 hours of labelled data , Model IC ( Acc ) SF ( F1)2215 49.97 % 62.43 %   H 58.11 % 66.97 %   SSE- ( 2215 ) 63.64 % 64.48 %   SSE- ( H ) 64.33 % 68.82 %   Kaldi+HerMiT 78.33 % 70.84 %   CTI 82.93 % 71.12 %   indicating the effectiveness of the bridge module .   OnSLURP andSLUE , the relative drop in WER   ( > 20 % ) is substantially more than the relative   drop in downstream performance ( < 5 % ) , verify-   ingSSE- ’s tolerance to noisy transcriptions .   The robustness to ASR errors come from our choice   of LLM , BART , which is trained to handle noisy   inputs , residual connection to acoustic embeddings ,   and LLM alignment with adapters .   5.5 Comparison to Specialized SLU Models   To better quantify the performance improvement   introduced by SSE , we compare against 2 special-   ized SLU models that do not abide by the universal   representation framework : Kaldi+HerMiT , which   is a pipelined Kaldi ASR ( Povey et al . , 2011 ) and   HerMiT NLU ( Vanzo et al . , 2019 ) model reported   in the SLURP paper ( Bastianelli et al . , 2020 ) , and   CTI ( Seo et al . , 2022 ) , which is an end - to - end   pipelined22(Baevski et al . , 2020 ) ASR and   RBERT(Liu et al . , 2019 ) NLU model . To the   best of our knowledge , CTI is the state - of - the - art   SLU model .   In addition to unlabelled text , unlabelled au-   dio , and downstream data , both Kaldi+HerMiT   andCTI require 40 hours of downstream SLURP   ASR data ( Bastianelli et al . , 2020 ) . Kaldi+HerMiT   requires an additional 24,000 hours of ASR   data ( Povey et al . , 2016 ) . CTI requires an addi-   tional 960 hours of ASR data ( Panayotov et al . ,   2015 ) . Neither use lightweight fine - tuning . Thus ,   such specialized SLU models are less general , more   expensive , and require much more data . As seen in   Table 6 , SSE helps bridge the gap between tailor-   made models and more practical SSL speech en-   coders . We believe ASR - U errors plays a major   role in the remaining gap , as the ASR - supervised   Kaldi+HerMiT and CTI models have WER of   16.20 % and 16.67 % respectively , compared to11420Most Common Mix - ups % Mistakes   qa_factoid , general_quirky +5.83 %   calendar_set , calendar_query -20.00 %   general_quirky , calendar_query +8.57 %   weather_query , calendar_query -34.72 %   play_music , play_audiobook -7.27 %   play_music , play_radio -14.03 %   calendar_set , calendar_remove -32.26 %   play_music , play_game -18.87 %   ... ...   SSE ’s ASR - U bridge with a WER of 51.51 % .   5.6 Error Analysis   To better understand the semantic information cap-   tured by SSE , we study predictions made by both   H andSSE- ( H ) onSLURP -   IC ’s test set . We find HUBERT errors are made   primarily between intents within the same or simi-   lar domains ( e.g. calendar_set vs calendar_query ) .   The performance bottleneck lies with distinguish-   ing finer - grained in - domain intents . Table 7 shows   that SSE- is better at differentiating finer-   grained intents .   SSE- ’s misclassifications come primarily   from errors made by its ASR - U bridge component .   As seen in Table 8 , the ASR - U WER of incor-   rect predictions made by H is much lower   than that of incorrect predictions made by SSE- . When ASR - U returns resonable transcrip-   tions ( typically < 50 % WER ) , SSE- can cor-   rectly classify inputs that H can not . Hence ,   the effectiveness of SSE is tightly coupled with the   effectiveness of ASR - U.   5.7 Representation Visualization   To better see the impact of including semantic rep-   resentations , we visualize the pooled audio snippet   embedding for intent classification on SLURP -IC   using t - distributed stochastic neighbor embedding   ( t - SNE ) ( Van der Maaten and Hinton , 2008 ) . WeSSE- ( H )   ✓ ✗   H ✓ 40.8 % ( 6395 ) 75.2 % ( 1204 )   ✗ 48.2 % ( 2019 ) 78.1 % ( 3460 )   denote the ground truth label of each audio snippet   by the color of its pooled embedding . As seen in   Figure 2 , the clusters produced by semantic em-   beddings are more spread out and better separated   than those produced by just acoustic speech embed-   dings , indicating that SSE introduces new semantic   information that existing speech encoders lack .   6 Conclusion   We presented a compelling case for introducing   semantics into SSL speech encoders and an effec-   tive method of doing so . Our approach boosts the   performance of existing speech encoders on multi-   ple SLU and SQA tasks and datasets . We provide   reasoning for what tasks may benefit more or less   from incorporating semantics . Furthermore , our   approach is task agnostic and can augment any   existing SSL speech encoder . With SSE- ,   we show merging acoustic and semantic informa-   tion and effectively aligning LLMs to the speech   encoder on downstream tasks can further boost per-   formance with minimal parameter overhead . As   it can generalize to many downstream tasks , SSE   provides an important step towards AI that can un-   derstand and respond to spoken language.11421References114221142311424A Appendix   A.1 Acknowledgments   We thank Zoey Zhiyu Chen for checking ex-   perimental results and Jiatong Shi for providing   helpful discussion . This work was done dur-   ing an internship at Meta AI and was partially   supported by NSF 1829071 , 1937599 , 2106859 ,   2200274 , 2211557 , 2119643 , 2303037 , DARPA   # HR00112290103 / HR0011260656 , NASA , SRC ,   Okawa Foundation Grant , Amazon Research   Awards , Cisco research grant , Picsart Gifts ,   Snapchat Gifts , and an NEC research award .   A.2 ASR - U Bridge Training Objective Details   We adopt the same unsupervised training scheme   as22 - U2.0 ( Liu et al . , 2022 ) . Specifically ,   we train the generator , G , on GAN loss , L ,   a gradient penalty term , L , for better conver-   gence , a smoothness penalty term , L , to encour-   age consecutive speech segments to generate the   same phonemes , a phoneme diversity term , L ,   to diverse phoneme usage in output transcripts   by maximizing entropy , and a self - supervised re-   construction loss , L , to encourage the generated   phonemes to match the input audio .   The reconstruction term uses a separate linear   head to classify each speech embedding into 1 of   64 clusters , ζ , obtained from running k - means   on the Mel - frequency cepstral coefficient ( MFCC)features of the input audio ( Hsu et al . , 2021 ; Liu   et al . , 2022 ) . The final GAN training objective ,   minmaxL , is summarized in Equation 1 . The   training procedure for the bridge module is outlined   in Figure 3 . Similar to22 - U2.0 ( Baevski et al . ,   2021 ) , SSE bridge models are trained on unla-   belled audio and text from the Librispeech ( Panay-   otov et al . , 2015 ) dataset .   L = L+λL+γL+ηL+δL   L = E[logC(T ) ] + E[log(1− C(G(X ) ) ]   L = E[(||∇C(µG(X ) + µT|| −1 ) ]   L=/summationdisplay||p−p||   L=1   |B|/summationdisplay−H(G(S ) )   L=−/summationdisplaylogP(ζ|X ) . ( 1 )   A.3 Hyperparameter Settings   A.3.1 Speech Encoder   We augment both22(Baevski et al . , 2020 ) and   H ( Hsu et al . , 2021 ) by introducing seman-   tics . Specifically , we use the22 - Large LV-60   model and H -Large models , which are pre-   trained on just unlabelled audio and implemented   with the fairseq library ( Ott et al . , 2019 ) .   A.3.2 Large Language Model   We use BART ( Lewis et al . , 2019 ) as our LLM   since it is pretrained to handle noisy input . In   our SLU experiments , we use BART -Base model ,   which has lower computational overhead . For our   SQA experiments , we use BART -Large , since   SQA is a more challenging task . Note , unlike   baselines that train the whole LLM , SSE freezes   all weights in its LLM except adapters optionally ,   hence SSE has lower overhead . All LLMs were   implemented using the huggingface library ( Wolf   et al . , 2019 ) .   A.3.3 Residual Attention and Adapters   We choose the residual attention layer to be the   same dimension as our speech encoder , which is   1024 for both22(Baevski et al . , 2020 ) and H- ( Hsu et al . , 2021 ) . We implement our general   recipe for adapters using the adapter - transformers11425package ( Pfeiffer et al . , 2020 ) and pyTorch ( Paszke   et al . , 2019 ) .   A.3.4 Bridge Connector   We follow the same hyperparameter settings re-   ported in the22 - U2.0 paper ( Liu et al . , 2022 ) .   Specifically , we use a 2 - layer CNN with stride 3 .   The model is trained on unlabelled Librispeech-   960 ( Panayotov et al . , 2015 ) data for 100,000   epochs with a learning rate of 5e-5 and 3e-4 for the   generator and discriminator respectively . Decoding   is done using a WFST in the same way as22-   U2.0 ( Liu et al . , 2022 ) . Similar to22 - U2.0 ,   we pre - process the Librispeech-960 by removing si-   lences with an unsupervised model , but not during   fine - tuning or testing . We believe such techniques   could further improve performance , but leave it as   future work . The regularized GAN loss function   hyperparameters , as stated in Section A.2 are set to   1.0/1.5,1.5/2.5,0/3 , and 0.3/0.5forλ , γ , η , and   δrespectively .   A.3.5 SLU Training Details   As mentioned in Section 4.3 , we use the standard   decoders provided by SUPERB ( Yang et al . , 2021 ) .   We ran a grid search on 5 settings for learning rate   on an exponential scale of 2 around the default set-   tings from SUPERB ( Yang et al . , 2021 ) and found   said default hyperparameters optimal . Specifically ,   we set the learning rate to 1e−4,1e−4,2e−4 ,   and2e−4forFSC -IC , SLURP -IC , SLURP -SF ,   andSLUE -NER respectively . All methods use the   AdamW ( Loshchilov and Hutter , 2017 ) optimizer   with gradient clipping set to 1 for 200,000 total   steps to convergence . Validation performance is   used to pick the best model for all datasets except   SLUE , since SLUE test data is not publicly avail-   able .   A.3.6 SQA Training Details   As mentioned in Section 4.3 , we use a frame - level   linear layer classification head as our decoder . We   follow D ’s ( Lin et al . , 2022a ) default hyperpa-   rameter settings with a learning rate of 1e-4 . We   train the models using the same warm - up and decay   strategies as D with the AdamW ( Loshchilov   and Hutter , 2017 ) optimizer for 5,000 steps to con-   vergence .   A.4 Training Setup and Time   All models were trained on a server with 8 Nvidia   Tesla V100 GPUs . The total training time for thebridge module takes around a day . The total train-   ing time for downstream tasks take between half a   day and one day .   A.5 Dataset Details   As mentioned in Section 4.1 , we evaluate SSE on   3 SLU tasks , intent classification ( IC ) , slot filling   ( SF ) , and named entity recognition ( NER ) , and the   SQA task . The goal of IC is to classify the intent of   an input audio snippet . The goal of SF is to extract   certain attributes of a given intent from an audio   snippet . The goal of NER is to identify named   entities in an audio snippet . The goal of SQA is   to find the start and end frames of the answer in a   spoken passage given a spoken question .   A.5.1 FSC   The FSC dataset ( Lugosch et al . , 2019 ) is an IC   dataset for a smart home virtual assistant . The in-   put is a single audio file containing spoken English   commands and the output class is the intent of the   spoken command . The data was obtained through   crowd - sourcing from 97 native and non - native En-   glish speakers . In total , there are 31 intents . The   number of utterances and hours of each split can   be found in the Table 9 .   A.5.2 SLURP   TheSLURP dataset ( Bastianelli et al . , 2020 ) is an   IC and SF dataset for an in - home personal robot   assistant . The input is a single audio file containing   spoken English commands and the output is the   scenerio , action , and entities . In total , there are 18   different scenarios , 46 different actions ( IC ) , and   56 different entities ( SF ) . The data was collected   from 177 native and non - native English speaking   Amazon Mechanical Turk workers . The number   of utterances and hours of each split can be found   in Table 9 . SLURP use both headsets and micro-   phones with various placement configurations .   A.5.3 SLUE   The SLUE dataset ( Shon et al . , 2022 ) is a NER   dataset using European Parliament event record-   ings . The input is a single audio file containing   spoken English passages and the output are the   named entities . There are in total 7 categories that   were based on the OntoNotes Release 5.0 ( Hovy   et al . , 2006 ) entity labels . The dataset was collected   from the official European Parliament website . The11426Dataset # of Utterances # of Hours   FSC - train 23,132 14.7   FSC - dev 3,118 1.9   FSC - test 3,793 2.4   SLURP - train 50,628 40.2   SLURP - dev 8,690 6.9   SLURP - test 13,078 10.3   SLUE - train 5,000 14.5   SLUE - dev 1,753 5.0   number of utterances and hours of each split can   be found in the Table 9 .   A.5.4 NMSQA   TheNMSQA dataset ( Lin et al . , 2022a ) is a SQA   dataset generated from a standard text question   answering dataset , SQAD-1.1 , using Amazon   Polly Text - to - Speechfor the train and dev split ,   and 60 human speakers for the test set . NMSQA   contains 297.18 hours , 37.61 hours , and 2.67 hours   of train , dev , and test split audio respectively . We   follow D ( Lin et al . , 2022a ) by evaluating on   Frame - level F1 score ( FF1 ) and Audio Overlapping   Score ( AOS).11427ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.11428 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.11429