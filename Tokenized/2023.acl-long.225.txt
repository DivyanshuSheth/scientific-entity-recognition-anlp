  Yang Deng , Wenxuan Zhang , Yifei Yuan , Wai LamThe Chinese University of Hong Kong , DAMO Academy , Alibaba Group   { dengyang17dydy,isakzhang}@gmail.com   { yfyuan,wlam}@se.cuhk.edu.hk   Abstract   Unlike empathetic dialogues , the system in   emotional support conversations ( ESC ) is ex-   pected to not only convey empathy for comfort-   ing the help - seeker , but also proactively assist   in exploring and addressing their problems dur-   ing the conversation . In this work , we study   the problem of mixed - initiative ESC where the   user and system can both take the initiative   in leading the conversation . Specifically , we   conduct a novel analysis on mixed - initiative   ESC systems with a tailor - designed schema   that divides utterances into different types with   speaker roles and initiative types . Four emo-   tional support metrics are proposed to evaluate   the mixed - initiative interactions . The analysis   reveals the necessity and challenges of building   mixed - initiative ESC systems . In the light of   this , we propose a knowledge - enhanced mixed-   initiative framework ( KEMI ) for ESC , which   retrieves actual case knowledge from a large-   scale mental health knowledge graph for gener-   ating mixed - initiative responses . Experimental   results on two ESC datasets show the superi-   ority of KEMI in both content - preserving eval-   uation and mixed initiative related analyses .   1 Introduction   As the world is making efforts to recover from   Covid-19 and plans for future construction , emo-   tional support is of great importance in resolving   the widespread emotional distress and increased   risk for psychiatric illness associated with the pan-   demic ( Pfefferbaum and North , 2020 ; Suh et al . ,   2021 ) . A wide range of emotional support con-   versation ( ESC ) systems are emerging to provide   prompt and convenient emotional support for help-   seekers , including mental health support ( SharmaFigure 1 : Examples from E D   and ESConv datasets with a similar job loss problem .   et al . , 2021 ; Lokala et al . , 2022 ) , counseling ( Al-   thoff et al . , 2016 ; Shen et al . , 2020 , 2022 ) or mo-   tivational interviewing ( Pérez - Rosas et al . , 2016 ;   Saha et al . , 2021 , 2022 ) . Generally , the ESC system   aims at reducing the user ’s emotional distress as   well as assisting the user to identify and overcome   the problem via conversations ( Liu et al . , 2021 ) .   Mixed initiative is commonly defined as an in-   trinsic feature of human - AI interactions where the   user and the system can both take the initiative   in leading the interaction directions ( Allen et al . ,   1999 ; Kraus et al . , 2020 ) . For example , mixed-   initiative conversational information - seeking ( CIS )   systems ( Aliannejadi et al . , 2019 ; Deng et al . , 2023 )   can proactively initiate clarification interactions for   resolving the ambiguity in the user query , instead   of only reacting to the query . Accordingly , a mixed-   initiative ESC system can proactively switch the   initiative to provide an empathetic response or initi-   ate a problem - solving discussion when appropriate .   Many efforts have been made on the emotion rea-   soning for generating empathetic responses ( Shen4079et al . , 2020 ; Zhang and Danescu - Niculescu - Mizil ,   2020 ; Cheng et al . , 2022 ; Peng et al . , 2022 ) . An-   other line of work focuses on identifying the dia-   logue acts of the utterances ( Welivita and Pu , 2020 ;   Malhotra et al . , 2022 ; Svikhnushina et al . , 2022 ) or   predicting the next conversational strategies ( Pérez-   Rosas et al . , 2017 ; Liu et al . , 2021 ; Tu et al . , 2022 )   in ESC systems . However , the feature of mixed   initiative has not been investigated in existing ESC   studies .   To facilitate the analysis on mixed - initiative ESC   systems , we first propose an EAFR schema to anno-   tate the utterances into different types with speaker   roles and initiative types , named Expression ( User-   initiative ) , Action ( Support - initiative ) , Feedback   ( User Non - initiative ) , and Reflection ( System Non-   initiative ) . Besides , four emotional support metrics   are designed to measure the characteristics of ini-   tiative and non - initiative interactions in ESC , in-   cluding Proactivity , Information , Repetition , and   Relaxation .   To analyze the necessity of considering mixed   initiative in ESC systems , we conduct a preliminary   analysis on the different interaction patterns be-   tween ESC and empathetic dialogues ( ED ) . Firstly ,   the dialogue flow analysis shows that the system   in ED generally serves as a passive role , while the   system in ESC proactively switches the initiative   role during the conversation . As shown in Figure 1 ,   the system in ED solely targets at comforting the   user by reflecting their feelings or echoing their   situations , i.e. ,Non - Initiative . Differently , ESC   systems are further expected to proactively explore   the user ’s problem by asking clarifying questions   and help the user overcome the problem by provid-   ing useful information or supportive suggestions ,   i.e. ,Initiative . Furthermore , the analysis of the   conversation progress and the emotional support   metrics reveal three challenges in building a mixed-   initiative ESC system : 1 ) When should the system   take the initiative during the conversation ? 2 ) What   kind of information is required for the system to   initiate a subdialogue ? 3 ) How could the system   facilitate the mixed - initiative interactions ?   According to these challenges , we define the   problem of mixed - initiative ESC , which includes   three sub - tasks : 1 ) Strategy Prediction to de-   termine the mixed - initiative strategy in the next   turn , 2 ) Knowledge Selection to collect the nec-   essary knowledge for the next turn , and 3 ) Re-   sponse Generation to produce emotional support re - sponses with appropriate mixed - initiative strategy   and knowledge . To tackle this problem , we propose   a novel framework , named Knowledge Enhanced   Mixed - Initiative model ( KEMI ) , to build a mixed-   initiative dialogue system for emotional support   conversations with external domain - specific knowl-   edge . In detail , KEMI first employs a knowledge   acquisition module to acquire emotional support   knowledge from a large - scale knowledge graph   on mental health dialogues . Specifically , we ex-   pand the user utterance with generated common-   sense knowledge as a query graph and then per-   form subgraph retrieval over the knowledge graph .   Secondly , a response generation module conducts   multi - task learning of strategy prediction and re-   sponse generation in a sequence - to - sequence man-   ner to generate mixed - initiative responses with ex-   ternal knowledge .   The main contributions of this work are sum-   marized as follows : ( 1 ) To measure the mixed-   initiative interactions in ESC systems , we pro-   pose an innovative analysis method , including an   EAFR annotation schema and corresponding emo-   tional support metrics . ( 2 ) We propose a novel   knowledge - enhanced mixed - initiative framework   for ESC , which retrieves external knowledge from   mental health knowledge graph by subgraph re-   trieval using the query graph expanded with com-   monsense knowledge . ( 3 ) Experimental results   show that the mixed initiative is of great impor-   tance in ESC , and the proposed method effectively   outperforms existing methods on both content-   preserving evaluation and mixed initiative analyses .   2 Related Works   Emotional Support Conversation Similar to   fine - grained sentiment analysis ( Zhang et al . , 2022 ,   2021c , b ) in conversations ( Li et al . , 2022a ; Zhang   et al . , 2021a ) , early works on emotional chatting   mainly investigate approaches to detecting user   emotions ( Li et al . , 2017 ; Zhou et al . , 2018 ) or   incorporating emotional signals into response gen-   eration ( Wei et al . , 2019 ; Song et al . , 2019 ) . As for   empathetic dialogue systems ( Rashkin et al . , 2019 ;   Welivita et al . , 2021 ) , evolving from emotion - aware   response generation ( Lin et al . , 2019 ; Majumder   et al . , 2020 ) and emotional style transfer ( Sharma   et al . , 2021 ) , more efforts have been made on emo-   tional reasoning techniques ( Li et al . , 2021 ; Kim   et al . , 2021 ; Gao et al . , 2021 ; Cheng et al . , 2022 ) .   Some latest studies explore the utilization of ex-4080ternal knowledge for enhancing the model capabil-   ity of emotion reasoning , including commonsense   knowledge graph ( Zhong et al . , 2021 ; Li et al . ,   2022b ) , generative commonsense model ( Sabour   et al . , 2021 ) , and domain - specific knowledge ( Shen   et al . , 2020 , 2022 ) . Shen et al . ( 2022 ) collectively   exploit three kinds of external knowledge . Like-   wise , many ESC systems also leverage common-   sense knowledge for response generation ( Tu et al . ,   2022 ; Peng et al . , 2022 ) . However , the common-   sense knowledge is rather abstractive without de-   tailed information , so that it is less helpful for the   ESC system to generate meaningful and informa-   tive responses . In this work , we employ the gen-   erative commonsense model for query expansion   to retrieve actual case knowledge from an external   knowledge graph .   Mixed - initiative Dialogue Recent years have wit-   nessed many efforts on developing mixed - initiative   conversational systems for various dialogues , such   as information - seeking dialogues ( Zamani et al . ,   2020 ; Aliannejadi et al . , 2019 ) , open - domain dia-   logues ( Wu et al . , 2019 ; Rachna et al . , 2021 ; Lei   et al . , 2022 ) , recommendation dialogues ( Deng   et al . , 2021 ) , conversational question answer-   ing ( Deng et al . , 2022a ) . Despite the importance   of mixed initiative in ESC systems , this area has   not been investigated . One closely related re-   search scope is to recognize the conversation strate-   gies ( Liu et al . , 2021 ; Pérez - Rosas et al . , 2017 ) or   the dialogue acts ( Malhotra et al . , 2022 ; Welivita   and Pu , 2020 ; Svikhnushina et al . , 2022 ; Deng   et al . , 2022b ) of the utterances in ESC systems .   However , these studies only focus on predicting   the support strategies , instead of actually involving   mixed - initiative interactions in ESC .   In addition , measuring mixed initiative is also re-   garded as an essential perspective for assessing   dialogue quality ( Vakulenko et al . , 2021 , 2020 ,   2019 ) . Due to the high expenses in human evalu-   ation , Sekulic et al . ( 2022 ) and Zhang and Balog   ( 2020 ) investigate user simulation for evaluating   the mixed - initiative interactions in conversational   systems . In this work , we investigate several met-   rics for measuring the characteristics of the mixed   initiative in ESC systems .   3 Preliminary Analysis   3.1 EAFR Schema & Metrics   Inspired by the ConversationShape ( Vakulenko   et al . , 2021 ) for the analysis of mixed - initiativeCIS systems , we first propose an EAFR annota-   tion schema to study the mixed initiative in ESC   systems . The EAFR annotation schema classifies   the utterance in ESC into four categories w.r.t the   role of speakers and the type of initiative , includ-   ingExpression ( User - initiative ) , Action ( System-   initiative ) , Feedback ( User Non - Initiative ) , and Re-   flection ( System Non - Initiative ) . Definitions and   examples of each type are presented in Table 1 .   Then , each utterance iin a dialogue is anno-   tated as a tuple ( r , t , v , e)for analysis . r∈   { User(U),System ( S)}denotes the speaker role .   t∈ { Initiative ( I),Non - Initiative ( N)}denotes   the initiative type . v∈ { 0,1}denotes the one-   hot vocabulary embeddings . e∈[1,5]denotes the   level of emotion intensity . We further design four   emotional support metrics for investigating patterns   of mixed initiative in ESC systems as follows :   •Proactivity : how proactive is the system in the   emotional support conversation ?   denotes the ratio of system - initiative interactions .   •Information : how much information does the   system contribute to the dialogue ?   represents the average number of new frequent   termsthat are introduced by the system .   •Repetition : how often does the system follow up   on the topic introduced by the user ?   represents the average number of repeated fre-   quent terms that are introduced by the user and   mentioned by the system .   •Relaxation : how well does the system relax the   emotional intensity of the user?4081   represents the change of the user ’s emotion in-   tensity . e[r = U]ande[r = U]denote   the emotion intensity of the first user utterance   before and after the utterance i , respectively .   3.2 Analysis of Mixed Initiative in ESC   To reveal the necessity of incorporating mixed ini-   tiative into ESC systems , we analyze the differ-   ent interaction patterns between empathetic dia-   logues ( ED ) and emotional support conversations   ( ESC ): ( i ) E D ( Rashkin   et al . , 2019 ) , a dataset for ED that aims to pro-   vide empathetic responses for comforting the help-   seeker , and ( ii ) ESConv ( Liu et al . , 2021 ) , a dataset   for ESC that aims to not only reduce users ’ emo-   tional distress , but also help them understand and   overcome the issues they face .   Due to the space limitation , we present the de-   tailed analysis in Appendix A , including ( i ) the   visualization of dialogue flow that indicates the ini-   tiative patterns between the user and system ( A.2 ) ;   ( ii ) the visualization of conversation progress that   shows the phased change of the user ’s emotion in-   tensity ( A.3 ) ; and ( iii ) the evaluation of emotional   support metrics that quantifies different aspects of   mixed - initiative interactions ( A.4 ) .   3.3 Challenges of Mixed Initiative in ESC   The preliminary analysis reveals the importance   of mixed - initiative interactions in ESC systems .   Meanwhile , it is also challenging to balance the   mixed - initiative interactions , as overacting in one   way or taking the initiative inappropriately can be   harmful to the emotional support conversations .   Based on these analyses , we identify three key chal-   lenges in building a mixed - initiative ESC system:1 ) When should the system take the initiative   during the conversation ? The analysis of conver-   sation progress ( A.3 ) shows that taking initiative   at different phases of the conversation may lead to   different impacts on the user ’s emotional state . In   particular , support strategies or dialogue acts attach   great importance to conversational effectiveness in   ESC ( Zhang and Danescu - Niculescu - Mizil , 2020 ;   Tu et al . , 2022 ) . Therefore , it is a crucial capability   for the ESC system to determine whether to take   the initiative at each conversation turn .   2 ) What kind of information is required for the   system to initiate a subdialogue ? The analysis of   mixed initiative metrics ( A.4 ) show that the initia-   tive system utterances are much informative than   the non - initiative ones . Therefore , it is of great   importance to discover necessary information and   knowledge to make an appropriate mixed - initiative   interaction . Researchers ( Burleson , 2003 ) in com-   munication and sociology states that the helpful-   ness of supportive statement is contingent on the   following knowledge : ( i ) Affective Knowledge , the   emotion recognition of the user ’s affective state ,   ( ii)Causal Knowledge , the emotional reasoning   of stressors that cause the current affective state of   the user , and ( iii ) Cognitive Knowledge , the cogni-   tive analysis of coping processes to solve the core   problematic situation that the user faces .   3 ) How could the system facilitate the mixed-   initiative interactions ? Since the system in ESC   ultimately provides a natural language utterance to   interact with the user , this challenge can be defined   as a function that generates an initiative - aware ut-   terance based on the given information .   3.4 Problem Definition   Similar to the ED problem , the ESC problem is   typically defined as : given the dialogue context4082C={u , u , ... , u}and the description of the   user ’s problematic situation s , the goal is to es-   timate a function p(r|C , s)that generates the tar-   get response r. In the light of the challenges dis-   cussed in Section 3.3 , we further define the mixed-   initiative emotion support conversation problem   with the following three sub - tasks , corresponding   to the above three challenges :   1)Strategy Prediction predicts the support strategy   ythat can be regarded as the fine - grained initiative .   2)Knowledge Selection selects appropriate knowl-   edgekfrom the available resources K.   3)Response Generation generates the mixed-   initiative response rbased on the predicted strategy   and the selected knowledge .   4 Method   Motivated by the analysis in the last section , we   propose the KEMI framework that aims to generate   mixed - initiative responses with external knowledge .   As illustrated in Figure 2 , KEMI contains two parts :   1 ) Knowledge Acquisition , and 2 ) Mixed - initiative   Response Generation .   4.1 Knowledge Acquisition   Commonsense knowledge is widely adopted to en-   hance the emotion reasoning in ESC systems . De-   spite the wide usage of commonsense knowledge   in ESC systems , it is usually succinct and lacks spe-   cific context information . We propose an approach   to retrieve relevant actual cases of ESC from a   large - scale mental health knowledge graph , namely   HEAL ( Welivita and Pu , 2022 ) , for compensating   the deficiency of commonsense knowledge .   4.1.1 Query Expansion with COMET   Given the user utterance uat the current turn t , a   straight - forward knowledge acquisition approach   is to use uas the query to directly retrieve actual   cases from the HEAL KG . However , there is limited   information provided by the user utterance , which   may hinder the preciseness and explainability of   the knowledge retrieval . To this end , we exploit   COMET ( Bosselut et al . , 2019 ) , a commonsense   knowledge generator , to expand the query with   multi - perspective additional information regarding   the user ’s affective and cognitive state .   Specifically , the current user utterance uis fed   intoCOMET with five special relation tokens , p∈   { [ xReact ] , [ xIntent ] , [ xWant ] , [ xNeed ]   , [ xEffect ] } , to generate commonsense infer-   encecfor the relation p , i.e. ,c = COMET ( p , u).Definitions of each commonsense relation can   be found in Appendix B. Then the original user   utterance ucan be expanded with commonsense   knowledge { c } .   4.1.2 Query Graph Construction   The actual case in HEAL ( Welivita and Pu , 2022 )   is represented as a graph structure . Specifically , we   consider 4 out of 5 types of nodes in HEAL that   are related to response generation : 1 ) expectation :   commonly asked questions by the user in an emo-   tional support conversation ; 2 ) affective state : emo-   tional states associated with each speaker ; 3 ) stres-   sor : the cause of emotional issues ; and 4 ) response :   frequent types of responses by the system to ad-   dress the user ’s problems . Edges are constructed   to build the connections between nodes according   to actual emotional support conversations . More   details of HEAL can be found in Appendix C.   In accordance with the HEAL knowledge graph ,   the relation [ xReact ] , which reveals the user ’s   emotional state , provides the same information as   nodes in HEAL with the type of affective state . The   relation [ xIntent ] , which reveals the causes of   the user ’s current situation , also shares the same in-   formation as nodes in HEAL with the type of stres-   sor . The rest of relations , including [ xWant ] ,   [ xNeed ] , and [ xEffect ] , which reveal the   user ’s cognitive state , are relevant to the responses   for addressing the user ’s problem . Therefore , the   expanded query ˆu={u,{c}}can be repre-   sented as a graph with abstractive entity descrip-   tions , as shown in Figure 2 .   4.1.3 Subgraph Retrieval   To avoid enumerating all the subgraphs in HEAL ,   which is a densely - connected graph ( over 2 mil-   lion subgraphs ) , we propose a subgraph retrieval   approach to select the top relevant subgraphs to   form a candidate set . We first retrieve top- Kenti-   ties relevant to each abstractive entity description   in the expanded query graph ˆu . Specifically , we   use sentence - BERT ( Reimers and Gurevych , 2019 )   to be an embedding - based retriever f(·)for mod-   eling the semantic similarity between the entities in   the query and HEAL . With the retrieved top- Ken-   tities for each type of nodes , we merge them based   on the edge connections in the knowledge graph   to induce candidate subgraphs . Finally , we adopt   top - Ncandidate subgraphs as the retrieved knowl-   edgeK. The subgraphs are ranked by the sum   of similarity scores of each node in the subgraph4083   E={e , e , e , e } :   4.2 Mixed - initiative Response Generation   Given the dialogue context Cand the retrieved   knowledge K , we first encode them into dis-   tributed representations with contextualized en-   coders . Specifically , we add special tokens to dif-   ferentiate the roles of user and system as well as   different types of knowledge as : = , s , , u , , u , ... = , c , , ... , , e , ...   Pretrained language models ( PLMs ) , e.g. ,   GPT2 ( Radford et al . , 2019 ) , have shown superior   capability of generating high - quality responses in   many dialogue systems , especially those PLMs pre-   trained on dialogue corpus , e.g. , BlenderBot ( Roller   et al . , 2021 ) . To leverage the advantages of   these generative PLMs , we reformulate the mixed-   initiative emotional support conversation problem   as a Seq2Seq problem , which linearizes the input   and output as a sequence of tokens as follows :   X= , , , , ...   Y= , y , , r   where XandYare the linearized input and output   sequences for Seq2Seq learning . Then the model   is trained to maximize the negative log likelihood :   5 Experiment   5.1 Experimental Setups   Datasets We adopt the following two datasets   for the evaluation : ( i ) ESConv ( Liu et al . , 2021),an emotional support conversation dataset , con-   tains 1,300 dialogues with 38,365 utterances and   8 types of support strategies . We adopt the orig-   inal train / dev / test split ; and ( ii ) MI ( Pérez - Rosas   et al . , 2016 ) , a motivational interviewing dataset ,   contains 284 counseling sessions with 22,719 utter-   ances and 10 types of behavior strategies . We ran-   domly split the dataset for train / dev / test by 8:1:1 .   Evaluation Metrics As for automatic evaluation ,   we adopt Macro F1 as the strategy prediction met-   ric . Following previous studies ( Liu et al . , 2021 ; Tu   et al . , 2022 ) , Perplexity ( PPL ) , BLEU- n(B - n ) , and   ROUGE - L ( R - L ) are included for the evaluation of   response generation .   Baselines We provide extensive comparisons   with both non - PLM and PLM - based methods , in-   cluding three Transformer - based methods ( Trans-   former ( Vaswani et al . , 2017 ) , MoEL ( Lin et al . ,   2019 ) , and MIME ( Majumder et al . , 2020 ) )   and four BlenderBot - based methods ( Blender-   Bot ( Roller et al . , 2021 ) , BlenderBot - Joint ( Liu   et al . , 2021 ) , GLHG ( Peng et al . , 2022 ) , and   MISC ( Tu et al . , 2022 ) ) . Details about these base-   lines can be found in Appendix D.   Implementation Details KEMI is based on the   BlenderBot model ( Roller et al . , 2021 ) . Follow-   ing previous BlenderBot - based models ( Liu et al . ,   2021 ; Peng et al . , 2022 ; Tu et al . , 2022 ) , we adopt   the small versionof BlenderBot in experiments .   The learning rate and the warmup step are set to4084   be 3e-5 and 100 , respectively . The max input se-   quence length and the max target sequence length   are 160 and 40 , respectively . We retrieve the top- 1   subgraph from HEAL as the knowledge . The train-   ing epoch is set to 5 and the best model is saved   according to the PPL score in the dev set .   5.2 Overall Performance   Table 2 and Table 3 summarize the experimental   results on the ESConv and MI dataset , respectively .   Among the baselines , BlenderBot - based methods   largely outperform Transformer - based methods   by leveraging the valuable pretrained knowledge .   GLHG and MISC effectively exploit the common-   sense knowledge to improve the performance of   response generation . Besides , the joint learning   with strategy prediction task is beneficial to the per-   formance of response generation . Finally , KEMI   substantially outperforms other methods with a no-   ticeable margin . This indicates the domain - specific   actual case knowledge from HEAL can alleviate the   reliance on large - scale PLMs . Compared with com-   monsense knowledge , the knowledge from HEAL   is much more effective in predicting support strate-   gies , as this relevant knowledge can serve as an real   example for guiding the system to respond.vs . BlenderBot - Joint MISC   Win Tie Loss Win Tie Loss   Flu . 26 % 51 % 23 % 37 % 47 % 16 %   Ide . 50 % 38 % 12 % 46 % 30 % 24 %   Com . 46 % 40 % 14 % 44 % 30 % 26 %   Sug . 52 % 22 % 26 % 52 % 16 % 28 %   Ove . 62 % 20 % 18 % 70 % 12 % 18 %   5.3 Human Evaluation   Following previous studies ( Liu et al . , 2021 ; Peng   et al . , 2022 ) , we conduct human evaluation to com-   pare the generated responses from two given mod-   els on five aspects : 1 ) Fluency : which model ’s   response is more fluent ? 2 ) Identification : which   model ’s response is more skillful in identifying the   user ’s problem ? 3 ) Comforting : which model ’s   response is better at comforting the user ? 4 ) Sug-   gestion : which model can give more helpful and in-   formative suggestions ? 5 ) Overall : which model ’s   response is generally better ? We randomly sample   100 dialogues from ESConv and three annotators   are asked to determine the Win / Tie / Lose for each   comparison .   Table 4 presents the human evaluation re-   sults . We compare the generated responses from   KEMI with those produced by other two baselines ,   BlenderBot - Joint and MISC . The results show that   KEMI achieves remarkable improvement on initia-   tive interactions , including Identification andSug-   gestion . Consequently , KEMI can generate more   satisfactory and helpful responses than other meth-   ods , according to the Overall metric .   5.4 Ablation Study   In order to investigate the effect of each sub - task   and each type of knowledge on the final perfor-   mance , we report the experimental results of the   ablation study in Table 5 . In general , both the strat-   egy prediction and the knowledge selection tasks   as well as all types of knowledge contribute to the   final performance more or less . There are several   notable observations in detailed comparisons : ( i )   The knowledge from HEAL is the key to the im-   provement on the strategy prediction task , since   the actual case knowledge can provide a good guid-   ance for the next support strategy . ( ii ) Different   from discarding the actual case knowledge ( w/o   HEAL ) , discarding the commonsense knowledge4085   ( w / oCOMET ) brings a positive effect on the fluency   metrics ( PPL ) , as the commonsense knowledge is   not a natural sentence . However , the COMET con-   tributes more on the content - preserving metrics   ( BLEU and ROUGE ) than the HEAL , indicating   that the succinct commonsense knowledge can be   more precise . ( iii ) Among the three types of knowl-   edge , cognitive knowledge is the most effective one   for both strategy prediction and response genera-   tion tasks . ( iv ) Using Oracle strategy and Oracle   knowledge substantially improves the overall per-   formance , which demonstrates the effectiveness of   considering these two sub - tasks in ESC systems .   The performance gap between KEMI and Oracle   also shows that the knowledge selection is very   challenging and there is still much room for im-   provement .   5.5 Analysis of Mixed Initiative   We conduct the mixed initiative analysis introduced   in Section 3.2 over the proposed KEMI method and   other baselines . Since the calculation of the Relax-   ation metric in Eq.(4 ) requires the emotion intensity   score of the user feedback , we adopt a model - based   user simulator for automatic evaluation , which is   described in Appendix A.1.3.5.5.1 Emotional Support Metrics   Table 6 summarizes the results of the four emo-   tional support metrics for the generated responses   from four BlenderBot - based methods and the ref-   erence responses in the test set . Note that , for a   fair comparison , we also adopt Eq.(9 ) to calculate   the Relaxation metric for the reference responses   in the test set ( i.e. , REF ) . It can be observed that   ( i ) As for the Proactivity metric , BlenderBot tends   to act passively in ESC . While BlenderBot - Joint   and MISC overly take the initiative after simply   taking into account the support strategies . KEMI   effectively balances the proportion of initiative and   non - initiative interactions in ESC . ( ii ) With the ac-   tual case knowledge , KEMI can generate much   informative responses than other baselines w.r.t the   Information metric . However , there is still a large   gap to reach the reference responses . ( iii ) Indeed , it   is relatively easier to generate responses that repeat   the previous information w.r.t the Repetition metric .   ( iv ) KEMI outperforms other baselines in terms of   the Relaxation metric on the initiative interactions   with a large margin , which shows the superiority   of KEMI on taking the initiative role for helping   the user to solve emotional problems .   5.5.2 Conversation Progress   We conduct the conversation progress analysis by   dividing the whole conversation into five equal   length intervals and observing the change of users ’   emotion intensity levels at each conversation phase .   As the results shown in Figure 3 , we observe that   BlenderBot and MISC have a clear inclination to   take non - initiative and initiative interactions in all   stages of the conversation , respectively . Our KEMI   method shares a more similar progress as the ref-   erence conversation with a balanced interaction   pattern . More importantly , the initiative responses   generated by KEMI has a more positive impact on   the user ’s emotional intensity than other baselines ,   especially in the last two stages of the conversa-   tion . This result indicates that KEMI effectively   takes the initiative to generate responses that can   provide suggestions or information for relaxing the   help - seekers by solving their emotional problems .   5.6 Case Study   To intuitively show the superiority of KEMI over   other baselines , Figure 4 presents a case study   of generated responses with the scores of mixed-   initiative metrics . In the reference response , the   system takes the initiative to provide useful sug-4086   gestions to the user for solving her / his problem ,   which effectively reduce the user ’s emotional inten-   sity . Among the generated responses , BlenderBot   and BlenderBot - Joint decide to convey empathy   to the user by paraphrasing the previous informa-   tion , while MISC and KEMI proactively initiate   a discussion about potential solutions to the prob-   lem . Based on the Relaxation metric , two initiative   responses can better comfort the emotional inten-   sity of the user than two non - initiative responses .   Furthermore , KEMI can generate more informative   and specific responses with actual case knowledge .   6 Conclusions   In this paper , we design a novel analysis frame-   work for analyzing the feature of mixed initiative   in ESC . The analysis demonstrates the necessity   and importance of mixed - initiative interactions in   ESC systems . To this end , we propose the KEMI   framework to tackle the problem of mixed - initiative   ESC . KEMI first retrieves actual case knowledge   from a large - scale mental health knowledge graph   with query expansion and subgraph retrieval . Then   KEMI performs multi - task learning of strategy   prediction and response generation with the re-   trieved knowledge . Extensive experiments show   that KEMI outperforms existing methods on both   automatic and human evaluation . The analysis alsoshows the effectiveness of incorporating actual case   knowledge and the superiority of KEMI on the   mixed - initiative interactions .   Limitations   In this section , we analyze the limitations of this   work :   •As it is the first attempt to analyze the mixed-   initiative interactions in emotional support con-   versations , the proposed metrics can be further   improved for more robust evaluation .   •Since the knowledge retrieval is not the focus of   this work , we did not spend much space on dis-   cussing the choice of different retrieval methods .   As shown in Table 5 , there is still much room for   improving the knowledge retrieval from a large   scale knowledge graph . It is also worth study-   ing more efficient retrieval methods for retrieving   knowledge from a densely connected KG .   •The proposed method requires an additional men-   tal health related knowledge graph constructed   by experts or knowledgeable workers , which   is probably difficult to obtain in some applica-   tions . However , different from other knowledge-   intensive tasks that can be benefited from open-   domain knowledge ( e.g. , Wikipedia ) , it attaches4087great importance in the professionals of the   knowledge for building a helpful and safe ESC   system .   Ethical Considerations   The datasets adopted are publicly available and   widely studied benchmarks collected from profes-   sionals or well - trained annotators . All personally   identifiable and sensitive information , e.g. , user   and platform identifiers , in these dataset has been   filtered out . We do not make any treatment rec-   ommendations or diagnostic claims . Compared   with existing methods for emotional support con-   versations , the proposed method can be regarded   as one step further to a more safer ESC system .   The proposed method retrieves knowledge from a   well - established mental health knowledge graph ,   which can be maintained by filtering out harm-   ful information when applying into applications .   Then the knowledge - enhanced approach can allevi-   ate the randomness during the response generation   and provide the guidance towards more positive   responses . In order to prevent the happening of   unsafe cases , the analysis of emotion intensity pre-   diction can also serve as an alarming mechanism   that calls for handoffs to an actual psychologist .   References40884089   Appendix   A Analysis of Mixed Initiative   A.1 Tools for Analysis   We introduce two models that are adopted as off-   the - shelf tools for the analysis of mixed initiative .   A.1.1 Utterance Initiative Classification   To facilitate automatic analysis of utterance ini-   tiative , we train two utterance classification mod-   els by fine - tuning the pre - trained RoBERTa   model ( Liu et al . , 2019 ) on the ESConv ( Liu et al . ,   2021 ) dataset , one for system utterance classifica-   tion and the other for user utterance classification .   We concatenate the previous utterance from another   participant and the current utterance as the input   for the binary initiative classification , either Initia-   tive ( I ) or Non - Initiative ( N ) . However , there is no   initiative label in ESConv . Therefore , we manu-   ally annotate the initiative labels , I or N , for each   utterance according to the EAFR schema . The re-   sulting dataset contains ∼38 K utterance - label pairs   ( E : 13 K , A : 9 K , F : 7 K , R : 10K).4090A.1.2 Emotion Intensity Prediction   Similarly , we also fine - tune an emotion intensity   prediction model f(·)based on the pre - trained   RoBERTamodel ( Liu et al . , 2019 ) on the ES-   Conv ( Liu et al . , 2021 ) dataset . Given a user utter-   ance , the model aims to predict the negative emo-   tion intensity level e = f(u ) , ranging from 1 to   5 , which indicates the user ’s emotional state . In ES-   Conv , the initial and final emotion intensity levels   of the user have already been annotated . Therefore ,   we regard the first user utterance after greetings to   match with the initial emotion intensity , while the   last user utterance before goodbyes to match with   the final emotion intensity . The resulting dataset   contains 2,450 utterance - label pairs ( 1 : 331 , 2 : 506 ,   3 : 557 , 4 : 629 , 5 : 427 ) .   A.1.3 User Simulator   Inspired by the evaluation of mixed - initiative   CIS ( Sekulic et al . , 2022 ) , we simulate a user based   on a large - scale generative language model , namely   BlenderBot ( Roller et al . , 2021 ) . In our case , we   fine - tune a semantically - conditioned generation   model g ( · ) , guided by the underlying problematic   situation :   where ais the user ’s feedback to the generated   response r. The generation model is fine - tuned   on the whole dataset , including the test set . If the   ESC system generates a perfect response , the user   simulator should give the ground - truth feedback as   the real user .   We adopt the same utterance initiative classifica-   tion model and emotion intensity prediction model   f(·)described in Appendix A.1.2 to annotate the   generated response . The annotation results are used   for calculating the emotional support metrics . In   particular , the calculation of Relaxation metric in-   volves the user ’s emotion intensity after receiving   the generated response . The user simulator g(·)is   employed to simulate the user ’s feedback . Then   the calculation of the Relaxation metric in Eq.(4 )   becomes :   A.2 Dialogue Flow   Following previous studies on mixed - initiative CIS   systems ( Vakulenko et al . , 2019 , 2021 ) , we draw   the dialogue flow diagram to observe the initiative   switch patterns between dialogue participants in   ED and ESC . As shown in Figure 5 , the circles   represent the beginning and ending of the dialogue ,   while the boxes represent the EAFR utterance la-   bels . The color intensity denotes the proportions   of the utterance labels and the initiative transitions .   There are several notable observations : ( i ) As for   the proportion of EAFR labels , Expression andRe-   flection constitute the majority of the utterances in   ED , while four labels are more equally distributed   in ESC . ( ii ) As for the beginning and ending of di-   alogues , users are more often to take the initiative   to start a conversation in ED , and the dialogue will   be ended by the system . Differently , in ESC , the   conversation is usually started by the system . ( iii )   As for the initiative switches , most of cases in ED   are that users express their feelings and then the   system tries to comfort them with empathy . How-   ever , the proportion of each type of initiative tran-   sitions in ESC is relatively equal . Therefore , we   conclude that the system in ED generally serves   as a passive role , while the system in ESC needs   to switch the initiative role during the conversa-   tion .   A.3 Conversation Progress   We analyze the conversation progress by dividing   the whole conversation into five equal length in-   tervals . To alleviate the noise from greeting ( Hi )   and farewell ( Bye ) utterances , we heuristically   identify these utterances by rules , e.g. , containing   “ Hi / Hello ” at the beginning or “ Bye / Goodbye ” at   the end of the conversation . Specifically , we com-   pute the distribution of initiative labels for system   utterances and the average change of emotion in-   tensity levels at each conversation phase . As shown   in Figure 6 , under both cases , the system tends to   take the initiative at the beginning of the conversa-   tion for exploring the user ’s problem , while acting   passively at the latter stage of the conversation.4091   Interestingly , at the early phase of conversations ,   compared with non - initiative utterances , system-   initiative ones fail to relax the emotion intensity of   help - seekers in ESC . This is because the request for   information from users to understand their prob-   lems is likely to raise users ’ negative emotions .   However , at the latter stage of the conversation ,   initiative utterances can better lower down users ’   intensity levels , leading to a higher emotional in-   tensity change rate than non - initiative ones . This   indicates that ( i ) the timing for system - initiative   interactions is important , and ( ii ) it is more help-   ful to provide suggestions or information for   users to solve the problem when the emotion of   users has been eased .   A.4 Emotional Support Metrics   Table 7 summarizes the scores of the emotional   support metrics introduced in Section 3.1 for two   datasets . Firstly , the proportion of system - initiative   interactions in ESC is much higher than that in   ED , showing the importance of mixed initiative   in ESC systems . Secondly , system - initiative utter-   ances provide more information than non - initiative   utterances in ESC , while an opposite result is ob-   served in ED . This shows that the ESC system   provides informative responses when taking the   initiative . Thirdly , in both datasets , there is more   repetitive information in non - initiative system ut-   terances than initiative ones , indicating that reflec-   tion is more important in non - initiative interactions .   Last but not least , the average relaxation score in   ESC is much lower than that in ED . We attribute   this to two reasons : ( i ) Empathetic responses have   more positive effects on the user ’s emotions . ( ii )   The system - initiative interactions sometimes may   increase the user ’s emotion intensity , as discussed   in Appendix A.3 .   B Definition of COMET Relations   We adopt five types of commonsense relations in   COMET ( Bosselut et al . , 2019 ) , whose original defi-   nitions are as follows :   •xEffect : The effect that the event would have   on Person X.   •xIntent : The reason why X would cause the   event .   •xNeed : What Person X might need to do before   the event .   •xReact : The reaction that Person X would have   to the event .   •xWant : What Person X may want to do after the   event .   C Details of HEAL   HEAL ( Welivita and Pu , 2022 ) is a knowledge   graph developed upon 1 M distress discussions and   their corresponding consoling responses curated   from mental health support conversations . It con-   sists of 22 K nodes with five different types : stres-   sors , expectations , responses , feedback , and affec-   tive states associated with distress dialogues , and   forms 104 K connections between different types   of nodes . The statistics of the adopted HEAL are   presented in Table 8 .   D Baselines   We provide extensive comparisons with the follow-   ing strong baselines , including both non - PLM and   PLM - based methods :   •Transformer ( Vaswani et al . , 2017 ) for Seq2Seq   response generation.4092•MoEL ( Lin et al . , 2019 ) is a Transformer - based   model that involves multi - decoders to enhance   the empathy for different emotions .   •MIME ( Majumder et al . , 2020 ) is a Transformer-   based model that mimics the emotion of the   speaker for empathetic response generation .   •BlenderBot ( Roller et al . , 2021 ) is an open-   domain dialogue model pretrained with mul-   tiple skills , including empathetic responding .   BlenderBot - Joint ( Liu et al . , 2021 ) jointly pre-   dicts strategies and generates responses .   •GLHG ( Peng et al . , 2022 ) is a BlenderBot - based   model , which employs a hierarchical graph net-   work to encode multi - source information .   •MISC ( Tu et al . , 2022 ) is a BlenderBot - based   model , which incorporates commonsense knowl-   edge and mixed support strategy to jointly pre-   dicts support strategies and generates responses.4093ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitation Section   /squareA2 . Did you discuss any potential risks of your work ?   Ethical Considerations Section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Not applicable . Left blank.4094 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5.1   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5.1   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5.1   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.4095