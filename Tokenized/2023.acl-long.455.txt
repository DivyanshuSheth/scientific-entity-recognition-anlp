2   Benfeng Xu , Quan Wang , Yajuan Lyu , Dai Dai   Yongdong Zhangand Zhendong MaoUniversity of Science and Technology of ChinaBeijing University of Posts and Telecommunications , Baidu Inc. Institute of Artiﬁcial Intelligence , Hefei Comprehensive National Science Center   benfeng@mail.ustc.edu.cn , zdmao@ustc.edu.cn   Abstract   Current relation extraction methods suffer from   the inadequacy of large - scale annotated data .   While distant supervision alleviates the prob-   lem of data quantities , there still exists do-   main disparity in data qualities due to its re-   liance on domain - restrained knowledge bases .   In this work , we propose SynRE , a frame-   work of two - stage Self - training with Synthetic   data for Relation Extraction . We ﬁrst lever-   age the capability of large language models   to adapt to the target domain and automati-   cally synthesize large quantities of coherent ,   realistic training data . We then propose an ac-   companied two - stage self - training algorithm   that iteratively and alternately learns from syn-   thetic and golden data together . We conduct   comprehensive experiments and detailed abla-   tions on popular relation extraction datasets   to demonstrate the effectiveness of the pro-   posed framework . Code is available at https :   //github.com / BenfengXu / S2ynRE .   1 Introduction   Relation extraction systems aim at discovering rela-   tional knowledge between entities by reading from   unrestricted texts ( Cardie , 1997 ) . Although neural   methods , especially pre - trained language models ,   have greatly advanced the state - of - the - art relation   extraction capability ( Zeng et al . , 2014 ; Wu and He ,   2019 ) , they still require large quantities of training   data ( Han et al . , 2020 ) . However , high - quality an-   notations are usually very expensive to obtain , mak-   ing low - resource relation extraction a very practical   challenge in many real - world scenarios .   Distant supervision ( Mintz et al . , 2009 ) , which   automatically annotates relational statements by   aligning entities with an existing knowledge   bases ( Bollacker et al . , 2008 ; Vrande ˇci´c and   Krötzsch , 2014 ) , has been widely explored as an   effective way to construct large scale relationaldataset . Many recent works exploit such data in   a pretraining stage to learn relational representa-   tions ( Baldini Soares et al . , 2019 ; Peng et al . , 2020 ;   Qin et al . , 2021 ) . Although this line of meth-   ods have seen certain improvements , they still in-   evitably raise the concern that the distantly anno-   tated data can vary considerably from downstream   tasks both in target schema and in context distribu-   tions , thus may not be able to offer optimal trans-   ferability . For instance , due to the reliance on ex-   isting knowledge bases , current works mostly re-   sort to Wikidata as the source of relational triples   and Wikipedia ( Vrande ˇci´c and Krötzsch , 2014 ) as   the corpus for distant supervision . This circum-   scribes distant data to only factual knowledge be-   tween world entities , while downstream tasks may   be of other special interests involving various do-   mains , ranging from semantic relation between   nominals ( Hendrickx et al . , 2009 ) to chemical-   protein interactions ( Kringelum et al . , 2016 ) .   Meanwhile , recent advances in large - scale pre-   trained language models ( LLM ) ( Radford et al . ,   2019 ; Brown et al . , 2020 ; Raffel et al . , 2020 ) have   demonstrated their great potential in generating   realistic texts of various domains ( Radford et al . ,   2019 ) . Accordingly , several very recent works have   explored the possibility to exploit LLM as an al-   ternative training data pool ( Schick and Schütze ,   2021 ; Vu et al . , 2021 ) . However , these attempts   are conﬁned to NLI task , while still not effectively   explored in the area of relation extraction .   In this paper , we study the construction of syn-   thetic data for relation extraction tasks to simul-   taneously address both training data scarcity in   low resource scenarios and domain disparity in dis-   tant supervision . We employ LLM to estimate and   adapt to the target domain distribution with only   a few training instances , and synthesize a large   amount of ones accordingly . The procedure is over-   all very simple but also carefully designed with   two critical choices : 1 ) we linearize relational state-8186ments into natural language sequences where en-   tity pairs are indicated by special marker tokens ;   2 ) we resort to unconditional generation instead of   label - conditioned ones , which relaxes the require-   ments for strict label - semantic correspondence but   increases sample availability and diversity .   We experiment with both GPT2 and the recent   very large LLMs like GPT-3.5 . For standard size   generative LMs like GPT2 - Large , we ﬁrst ﬁnetune   it to adapt to the target domain , while for the capa-   ble GPT-3.5 model , we directly apply In - Context   Learning ( Brown et al . , 2020 ; Xu et al . , 2023 ) . We   empirically found ﬁnetuned GPT2 - Large produces   synthetic data of equivalent quality to prompted   GPT-3.5 . In general , it is observed that with only a   few accessible samples , we are able to successfully   synthesize a large amount of domain - customized   training data with satisfactory quality .   To effectively learn from such synthetic data ,   we novelly advocate a two - stage self - training al-   gorithm . The approach in general follows the self-   training framework ( Yarowsky , 1995 ; Xie et al . ,   2020 ) , which is widely employed to exploit unla-   beled data . Typically , such methods iteratively an-   notate and learn pseudo labels for unlabeled data to   bootstrap the model ’s performance . Distinctively ,   we make a two - stage adaptation where in each of   the iterations , the model is ﬁrstly trained on syn-   thetic instances , then on golden ones . Such se-   quential training procedure favors golden data with   more importance since they are introduced in the   latter stage of the training curriculum .   We refer to our method as SynRE , a frame-   work of two - stage Self - training with Synthetic data   forRelation Extraction . The contributions of this   paper is three - fold :   •Conceptual Contribution We exploit LLM   to generate large quantities of domain adap-   tive synthetic data for low - resource relation   extraction , and challenge the long - prevailing   distant supervised methods restricted by KB   domain coverages . The proposed solution   novelly mitigates the problems of both data   scarcity and domain disparity .   •Technical Contribution We propose a novel   two - stage self - training algorithm to effec-   tively learn from unlabeled synthetic data and   golden data together . We demonstrate that this   is a non - trivial adaptation that signiﬁcantly   outperforms standard self - training widely em-   ployed in semi - supervised learning.•Experimental Contribution We conduct   comprehensive experiments on 6 popular re-   lation extraction datasets to investigate , ana-   lyze the propose method and make compar-   isons . We achieve new state - of - the - art for   low - resource relation extraction . Compared to   standard ﬁnetuning baseline , we obtain up to   17.18 % absolute improvements , and 11.09 %   on average across all datasets .   2 Related Works   Relation Extraction Relation extraction is one   of the fundamental tasks in natural language pro-   cessing ( Cardie , 1997 ) , where lots of research ef-   forts have been made to advance the state - of - the-   art methods ( Zeng et al . , 2014 ; Zhou et al . , 2016 ;   Zhang et al . , 2018 ; Baldini Soares et al . , 2019 ) ,   as well as the low - resource scenario ( Han et al . ,   2018 ; Sainz et al . , 2021 ; Dong et al . , 2021 ; Chen   et al . , 2022 ) . One of the most prominent methods   is distant supervision ( Mintz et al . , 2009 ) , which   automatically constructs annotated relational data   by aligning corpus with existing knowledge base .   Many recent works investigate how to learn effec-   tively with such distant data ( Baldini Soares et al . ,   2019 ; Peng et al . , 2020 ; Ding et al . , 2021 ; Qin   et al . , 2021 ) . Generally , they propose various pre-   text tasks that pre - train a model to learn relational   representation . We will further explain some of   these works for comparison in Section 5.3 .   Learning from Synthetic Data Built upon mas-   sive corpora , pre - trained language models are   promising at producing texts of eligible quality ,   resulting in a surge of research interests in its   usage for data augmentation ( Feng et al . , 2021 ) .   One straightforward way is to introduce mask   corruptions in the way language models are pre-   trained , then collect predictions as augmented   data ( Kobayashi , 2018 ; Ng et al . , 2020 ) . Later   works further developed such technique into condi-   tional augmentation ( Wu et al . , 2019 ; Kumar et al . ,   2020 ) . Nevertheless , these methods are mostly edit-   ing existing instances , which limits the diversity   and scale of augmented data .   With increasingly powerful LLMs , recent works   turn to direct synthesis of new instances ( Schick   and Schütze , 2021 ; Wang et al . , 2021 ; Meng et al . ,   2022 ; Ye et al . , 2022 ) . Different from this work ,   most of them focus on zero - shot language under-   standing where no labeled data is available ( Schick   and Schütze , 2021 ; Wang et al . , 2021 ; Meng et al . ,81872022 ; Ye et al . , 2022 ) . They investigate ways   to generate label - conditioned data by prompting   LLMs , but these methods can hardly be applied   to low - resource or full data scenarios while still   preserving effectiveness .   With the existence of labeled data , synthetic data   needs to be of higher quality to bring further utility .   Several works thus propose to ﬁnetune the genera-   tor ( Anaby - Tavor et al . , 2020 ; Vu et al . , 2021 ; He   et al . , 2021 ) . There are also explorations for learn-   ing from synthetic and golden data together , includ-   ing threshold - based conﬁdence ﬁltering ( Anaby-   Tavor et al . , 2020 ) , classical semi - supervised learn-   ing ( He et al . , 2021 ) or restricting the usage of   synthetic data within a supplemental intermediate   task ( Vu et al . , 2021 ) .   For structured learning tasks , Ding et al . ( 2020 )   similarly formulates NER task data as sequential   language . Speciﬁcally for relational data synthe-   sis , Papanikolaou and Pierleoni ( 2020 ) explore the   biomedical domain and Chia et al . ( 2022 ) focus on   zero - shot setting of triplet extraction . By contrast ,   SynRE distinguishes not only in applied scenario   and synthesis strategy , but also in the two - stage   learning framework , which is specially designed   for improved synthetic data adaptation .   3 Preliminary   This section formulates the task of relation extrac-   tion and the baseline models used throughout all   experiments .   Task Formulation A typical relation extraction   task is deﬁned by a corpus of relational statements   and a set of relations , i.e. , schema S. Assume   the training dataset D={(x , s , o)}and its   corresponding labels Y={y } , where xis a   sequence of words{w},y∈S , s= [ w :   w]ando= [ w : w]are subject and   object entities within the context . The target is   to learn a function f(x , s , o)that predicts the   correct relation label y.   Baseline Model As SynRE is a data - centric   framework , we keep the model architecture sim-   ple but competitive , which is the vanilla ﬁnetuning   of pre - trained language models . Instead of auto-   regressive LMs , we use auto - encoding networks   like BERT as they usually perform better on lan-   guage understanding downstream tasks . Follow-   ing Baldini Soares et al . ’s ( 2019 ) comprehensive   study of building relation extractors , we inject spe - cial marker tokens to the input word sequence :   x = ( ... , , s , , ...   ... , , o , , ... ) ( 1 )   After the encoding process of transformer , the rep-   resentation hin corresponding positions will be   concatenated for classiﬁcation :   /hatwidey = softmax ( W[h;h ] ) ( 2 )   where Wis a feedforward network and the pre-   dicted categorical distribution /hatwideywill be trained   against yusing cross - entropy loss .   4 Methodology   4.1 Relational Data Synthesis   Training instances of relation extraction task is   of speciﬁc structure ( x , s , o ) , i.e. , the relational   statement is expected to be a sentence containing   exact two entities as subject and object . Inspired   by Paolini et al . ( 2021 ) , we linearize relational   data into marked natural language sequence as in   Eq 1 . The synthesizer can be built upon any exist-   ing LLMs . In this paper , we explore both GPT2   and the even larger GPT-3.5 as two representative   LLMs and respectively employ ﬁnetuning or in-   context learning treatment .   4.1.1 Finetuning for GPT2   The ﬁnetuning process is performed in the same   autoregressive way as how it is pre - trained :   L=−/summationdisplaylogP(w|w , ... , w;LLM ) ( 3 )   where{w}=x , and a < bos > token is   prepended as w. Note that we ignore relation   labels yin training data and approach it as uncondi-   tional generation . This eliminates the noise caused   by label - semantic inconsistency , and leaves it to   model itself to learn from unlabeled synthetic data .   After the ﬁnetuning is completed , we simply   prepend the < bos > token to prompt the generation ,   and repeatedly perform inference using multino-   mial sampling until we obtain the expected scale   of synthetic data D. We show in appendix G   that these synthetic data are coherent , realistic , and   most importantly , customized to the target domain .   We elaborate on the framework of SynRE ( see   Fig . 2 ) in this section , including the construction   of an LLM - based synthesizer , and the two - stage   self - training algorithm.8188   4.1.2 In - Context Learning for GPT-3.5   Even larger LLMs naturally exhibit few - shot learn-   ing capabilities , and can be elicited through very   few in - context demonstrations ( Brown et al . , 2020 ;   Xu et al . , 2023 ) . We directly prepend 5 - shot exem-   plars and provide a speciﬁc instruction asking the   LLM to generate more examples . The process is   illustrated in Figure 1 .   4.2 Two Stage Self - training   Self - training is a widely adopted learning algo-   rithm for semi - supervised learning . Typically , to   jointly learn from an unlabeled dataset and a la-   beled dataset , it iteratively samples from the unla-   beled set , assigns them with pseudo labels , merges   them with the labeled dataset , and re - trains the   model . In this paper , we argue that this design   of naive merging is built upon a strong assump-   tion that the unlabeled dataset must be in the exact   distribution with the labeled ones , for which the   synthetic data does not strictly satisfy .   In SynRE , differently , we make a two - stage   adaptation : where synthetic data and golden data   are trained sequentially ( Figure 2 ) . We start from   a base model initialized using any auto - encoding   language models , e.g. , BERT ( Devlin et al . , 2019 ) ,   and train it on Dto produce a teacher model η , as   introduced in Section 3 . We ﬁrst use ηto annotate   the unlabeled synthetic data D :   /hatwidey = η(x , s , o ) ( 4)and we keep /hatwideY={/hatwidey}as soft pseudo la-   bels ofD , note that here the ˆdenotes softas   we keep the categorical distribution intact instead   of keeping its argmax . Inspired by Li and Qian   ( 2021 ) , to further eliminate ﬂuctuations in pseudo   labels , we train multiple teachers using different   random seeds , and the pseudo labels annotated by   k - th teacher is referred to as /hatwideY.   We then re - initialize a new student model θ , and   apply a two - stage training strategy . In stage - one   training , student θis trained on synthetic data using   soft pseudo labels :   θ←L(θ , D,{/hatwideY } ) ( 5 )   This can be seen as a distillation procedure that   transfers knowledge from ηtoθbased on synthetic   dataD. AndLis calculated as :   L=1   K / summationdisplayD(/hatwidey / bardblθ(x , s , o ) )   ( 6 )   where Dis the Kullback - Leibler divergence .   Then in stage - two training , we take from θ , and   train it on labeled training dataset :   θ←L(θ , D , Y ) ( 7 )   whereLis the standard cross - entropy loss , and   θis the resulting model in this iteration . We then   useθas the teacher model ηfor the next iter-   ation to re - annotate D , and this procedure is8189   repeated T times . Following the standard practice   of self - training , in each iteration , we incrementally   sample 1 / Tmore synthetic data from Duntil   in iteration T , where Dwill be running out of   new instances . The entire two - stage self - training   process can be formulated as Algorithm 1 .   Algorithm 1 : Two - stage Self - training .   Input : Golden training dataset D , Y ,   synthetic dataset D   t= 1 ;   D=∅ ;   Initialize θfrom auto - encoding LM ;   θ←Train ( θ , D , Y ) ;   θ←θ ;   repeat   t = t+ 1 ;   D = D∪D [: ] ;   /hatwideY←Annotate ( θ , D ) ;   Re - initialize θfrom auto - encoding LM ;   θ←Train ( θ , D,/hatwideY ) ;   θ←Train ( θ , D , Y ) ;   θ←θ ;   until performance converges or t reaches   maximum iteration limit T ;   Output : Final model θ   5 Experiments   5.1 Experimental Settings   We evaluate SynRE on popular datasets includ-   ingSemEval 2010 Task 8 ( Hendrickx et al . ,   2009 ) , TACRED ( Zhang et al . , 2017 ) , TACRED-   Revisited ( Alt et al . , 2020 ) , Re - TACRED ( Stoica   et al . , 2021 ) , ChemProt ( Kringelum et al . , 2016 )   andWiki80 ( Han et al . , 2019 ) . Their statistics are   given in Table 1 and we refer to detailed introduc-   tion in Appendix A.   For each dataset , we set three different prerequi-   sites of resource availability . Respectively , FULL   for 100 % training data , LIMITED for 10 % train-   ing data and FEWfor 1 % training data . To pro-   vide robust and convincing conclusions , we run   all experiments ( including ablation studies ) with   5 different random seeds and report their average .   With each random seed , we employ grid search to   select the best model as well as the teacher model   in each iteration . We use only development set for   such selection , and report the corresponding test   set score as the ﬁnal results .   For data synthesis , we use GPT2 - Large and GPT-   3.5 as the aforementioned LLMs . Speciﬁcally , for   ChemProt , we use an adapted version of GPT-2 ( Pa-8190   panikolaou and Pierleoni , 2020 ) , which is further   trained on 500k PubMed abstracts . When generat-   ing , we restrict sequence length to 128 , and perform   necessary ﬁltering by removing instances that do   not conform with the relational structure , i.e. , there   must exist 4 exact special markers and each start   position marker shall appear before its end position   marker . The synthesis efﬁciency is 24.05 instances   per second before any ﬁltering . In total , we collect   10,000 samples for FEWsetting , and 100,000 syn-   thetic samples for LIMITED andFULL settings .   We leave other hyper - parameters to Appendix B.   5.2 Capability of LLM   We ﬁrst validate the capability of LLMs as data syn-   thesizer and their respective treatment . Considering   both affordability and model capability , we use the   recently released gpt-3.5 - turbo-0301 APIas   even larger LLMs in comparison with GPT2 - Large .   For in - context learning , we repeatedly sample 5-   shot random examples from the golden training set   as demonstrations , followed by an instruction that   asks LLMs to generate more , with domain , format   and diversity constraints . We sent 1,321 queries in   total to collect 10,000 synthetic data ( the same as   our previous experimental settings using GPT-2 ) .   Each query produces different synthetic results due   to LLM sampling strategy and varied selection and   permutation of in - context demonstrations .   The results are shown in Table 2 . We empirically   found ﬁnetuned GPT2 - Large produces synthetic   data of equivalent quality to prompted GPT-3.5 ,   while both are effective training data synthesizers   that signiﬁcantly outperforms baselines . In the fol-   lowing experiments , we use GPT2 - Large to further   verify the proposed SynRE framework .   5.3 Main Results   We choose competitive baselines and reproduce   them under comparable settings to provide more   reliable conclusions . These baseline methods are :   BERT We ﬁnetune BERT model ( Devlin et al . ,2019 ) in a straightforward way for relation extrac-   tion as explained in Section 3 and implemented   in many existing works . This serves as our re-   implemented Finetune Baseline and will be re-   ferred to in the following ﬁgures .   MTB ( Baldini Soares et al . , 2019 ) pre - trains a re-   lational encoder using matching the blanks task ,   which is built on the hypothesis that two relational   statements containing the same entity pair should   express similar relational representations . Note that   this is a weaker reliance than distant supervision as   it only aligns entities , and does not need relations .   CP(Peng et al . , 2020 ) proposes a contrastive learn-   ing pretext task that encourages sentence represen-   tations with the same relation to be similar and   different ones to be disparate .   ERICA ( Qin et al . , 2021 ) further extends distant   supervision to document - level corpus , and design   similar pretext task that discriminates relational   representations across sentences .   We provide an overview of these works regard-   ing various resource usage and requirements in   Table 4 . The main results are shown in Table 3 .   On Wiki80 , we directly use distant data as they are   available in the general wiki domain , we analysis   the effects later in Table 5 . Under all three set-   tings across ﬁve datasets , SynRE outperforms the   BERT ﬁnetune baseline . Speciﬁcally for the FEW   setting , improvements are much more signiﬁcant ,   respectively +17.18 , +15.47 , +16.86 , +8.07 , +5.59 ,   and+3.34 , resulting an average improvements of   +11.09 across all 6 datasets . We further employ CP   as a stronger base model to initialize the students ,   and the performances are even better . This im-   plies that the improvements of SynRE are mostly   orthogonal with those of the distantly pre - trained   methods . In general , SynREachieves new state-   of - the - art for low resource relation extraction tasks.8191   F   L   F   5.4 Ablation Study   We investigate the advantages of SynRE via com-   prehensive ablations . In accordance with the main   claim , all experiments are conducted under the low-   resource ( FEW ) setting unless otherwise stated .   Synthetic Data Instead of Distant Data Distant   supervision has long been the prevailing solution to   automatically construct relational data . We make   its comparison against the proposed synthetic data   in Table 5 . We keep the two - stage self - training al-   gorithm intact , only replace the synthetic data with   distant data . On 5 investigated datasets , distant   data can provide appreciable improvements rang-   ing from +2.06 to+13.25 , however , synthetic data   brings much more signiﬁcant improvements rang-   ing from +5.59 to+17.18 , which clearly demon-   strates the superiority of being domain - customized   for target tasks . However , on Wiki80 , which very   closely follows identical distribution of distant data   as both are constructed using distant supervision on   wikipedia and wikidata , result shows that synthetic   data provides competitive improvements but no   longer outperforms distant ones . This veriﬁes the   importance and advantage of domain - customized   data from an opposite perspective . Nevertheless ,   real - world scenarios mostly involve distribution be-   yond the scope of wikipedia , and only the proposed   synthetic approach can offer such advantage . We   also provide qualitative comparisons for synthetic   and distant data in Appendix G to better illustrate   the discussed domain disparity.8192   Two Stage Self - training Typical self - training al-   gorithms merge the pseudo - labeled data into exist-   ing labeled data in each iteration , and minimize the   model ’s empirical loss on a mixture of both . We   refer to such classical implementation as Mixed   Self - training as opposed to the proposed Two-   stage Self - training . Fig . 3 compares these two ap-   proaches . In each iteration ( transparent blue bar ) ,   there will be one evaluation for mixed self - training   ( blue curve ) , but two evaluation for Two - stage Self-   training ( tealfor stage one , Red for stage two ) . We   observe that in stage - one training , the performance   might drop a few compare to its previous iteration ,   however , it effectively provides a better initializa-   tion where the model can further learn from the   golden data . Overall , the model can continually   bootstrap its performance by learning from syn-   thetic and golden data iteratively and alternately .   While in mixed self - training , the golden data are   treated equally as synthetic ones , and the model is   overwhelmed by large amounts of the latter . There-   fore , the improvement quickly saturates to a lim-   ited plateau . We also provide illustrations of the   bootstrapping performance over iterations on other   datasets in Appendix C.   Comparison Under Semi - supervised Setting   Standard semi - supervised setting also investigates   low - resource relation extraction by joint learning   from both labeled data and unlabled data . However ,   they make a strong assumption of identical distribu-   tion between unlabeled data and labeled ones , and   most existing works actually directly sample from   the golden training data and remove the labels to   construct the unlabled set . We provide comparison   with state - of - the - art methods of semi - supervised   learning in Table 6 ( under the LIMITED setting ) .   Results show that 1 ) the proposed two - stage self-   training outperforms other semi - supervised learn-   ing algorithms , and 2 ) synthetic data demonstrates   better or comparable performance compared to un-   labled set constructed from golden training data .   We attribute the latter to its domain - customized   quality and unlimited large - scale quantity .   Unconditional Generation Although a lot of   previous works intuitively resort to conditional syn-   thesis , we show that this is not the optimal choice   for relation extraction task . We ﬁnetune the synthe-   sizer by prepending label - speciﬁc prompts : " write   a sentence describing relation V(r ): " , where V(r )   is the verbalizer for each relation rand we directly   use corresponding label strings , e.g. , Component-   Whole(e2,e1 ) . We synthesize each relation class   proportional to its original distribution in golden   dataset . As conditional generation provides already   labeled data , we can directly ﬁnetune the student   model instead of self - training . We still train syn-   thetic and golden data sequentially as we empir-   ically found it a better choice . The results show   that conditional generation only brings minimum   or no beneﬁts . We attribute this to the difﬁculty   of preserving required label semantics for highly   abstractive tasks like relation extraction . As a con-   sequence , while these extra amounts of data can   still provide certain usability , they also most likely8193   cause considerable distractions .   Scale of Synthetic Samples Figure 4 investigates   the scale of synthetic samples . The improvements   are approximately increasing in log scale w.r.t . the   number of synthetic samples . The best perfor-   mance is reached at 10,000 , after which if we keep   adding more samples , the performance saturates .   As the synthesis of data is a repeatedly sampling   process , we think exploiting too much data will   deteriorate the diversity at the same time . We ver-   ify this by evaluating its diversity using type - token   ratio ( Roemmele et al . , 2017 ; Kumar et al . , 2020 ) ,   which is deﬁned as the ratio of unique n - grams out   of all n - grams ( see Table 8) . We can see that the   diversity gap between synthetic and golden data is   enlarged when increasing the data scale .   We also report supervised results using addi-   tional golden training data to measure the utility of   synthetic data . We can achieve two conclusions : 1 )   the advantage of golden training data are more sig-   niﬁcant when it is scaled up ( 10 ) . However , this   also takes substantially expensive costs . 2 ) SynRE   approximately achieves the utility of 1,000 addi-   tional annotated golden data ( 10 ) , and it only costs   several hours of GPU computation to produce ac-   cording synthetic data ( 10 ) as needed.6 Discussion   Distant supervision is the most prevalent solution   for low - resource relation extraction , and also the   main investigated and compared baseline in this   paper . Both distant data and the proposed syn-   thetic data can essentially be recognized as ways   of data augmentation to produce sufﬁcient number   of additional data . The critical difference which de-   termines the effectiveness lies in their consistency   with golden training data , i.e. , domain afﬁnity . And   in this paper , the superiority of synthetic data is   both experimentally proved ( Table 5 ) and quali-   tatively explained ( Appendix G ) . In conclusion ,   leveraging LLM to adapt to target domain and gen-   erate synthetic data of high utility is in general an   performant solution and we hope this novel per-   spective can further inspire future insights in many   related areas that have been greatly impacted by   the idea of distant supervision .   7 Conclusion   In this paper , we present SynRE , a framework   of two - stage self - training with synthetic data for   relation extraction . We show that synthetic data   generated using LLMs can resolve data scarcity in   low - resource scenarios and mitigate domain dis-   parity compared to distant supervision . To enable   effective learning from such synthetic data , we then   propose a novel two - stage self - training algorithm   that continually bootstraps model performance by   iteratively and alternately training the synthetic and   golden data together . The proposed framework   brings substantial improvements and achieves new   state - of - the - art for low - resource relation extraction .   In the future , we expect new possibilities brought   by LLMs and will further explore accompanied   techniques to exploit their potential .   Ethical Considerations   Synthetic data generated by language models may   involve potential ethical risks regarding fairness   and bias ( Bommasani et al . , 2021 ; Blodgett et al . ,   2020 ) , which results in further consideration when   they are employed in downstream NLP tasks . Al-   though the scope of this paper remains how to pro-   duce and leverage such synthetic data to improve   relation extraction system , it is worth further in-   vestigation to investigate in conjunction with well-   established methods that can measure ( Nadeem   et al . , 2021 ) and mitigate ( Nadeem et al . , 2021 ;   Gupta et al . , 2022 ) such ethical risks.8194Acknowledgements   This work is supported by the National Key   Research and Development Program of China   No.2020AAA0109400 , the National Natural Sci-   ence Foundation of China under Grant 62222212 ,   U19A0527 , 62121002 and 61876223 .   References8195819681978198A Datasets   SemEval 2010 Task 8 ( Hendrickx et al . , 2009 )   is a widely used testbed for relation extraction ,   the schema targets at semantic relations between   pairs of nominals , which requires certain level of   abstractive capabilities . TACRED ( Zhang et al . ,   2017 ) is a large - scale dataset annotated using Ama-   zon Mechanical Turk crowdsourcing . It was ini-   tially created for the TAC knowledge base popula-   tion and mainly covers common relations between   people , organizations , and locations based on the   TAC KBP scheme . TACRED - Revisited ( Alt et al . ,   2020 ) is a label - corrected version of the TACRED   dataset , which motivates from the unresolved chal-   lenging cases in original TACRED dataset . Re-   TACRED ( Stoica et al . , 2021 ) further conducted   a more comprehensive analysis and re - annotated   the entire dataset . Besides , it made alternations   to the schema to make it more clear and intu-   itive , which greatly improved the dataset qual-   ity . ChemProt ( Kringelum et al . , 2016 ) is a bio-   domain dataset that extracts 13 kinds of chemical-   protein interactions . It is widely used for evaluat-   ing domain - speciﬁc model capabilities ( Lee et al . ,   2019 ; Beltagy et al . , 2019 ) .   B Experimental Settings   SynRE involves three different training processes ,   respectively the ﬁnetuning of LLM , stage - one train-   ing , and stage - two training . Except for training   steps or epochs , we do not exhaust further search   for other hyper - parameters and set them empiri-   cally .   For the ﬁnetuning of LLM as synthesizer , we set   batch size to 64 , learning rate to 3e-5 . We found   that the quality of generated samples is sensitive to   the ﬁnetuning steps . Considering that the scale of   training samples varies from 73 ( SemEval 1 % ) to   68,124 ( TACRED 100 % ) w.r.t . different datasets   and different settings , we search steps within differ-   ent ranges accordingly . The ﬁnal choices are listed   in Table 9 .   For stage - one training , we set batch size to 64 ,   learning rate to 3e-5 , and ﬁx the training steps as   1500 . We save the checkpoint from 500 , 1000 , and   1500 steps respectively and select the best one . For   stage - two training , we set batch size to 16 , learning   rate to 3e-5 , and the epochs are set as Table 10 .   These epoch settings are empirically chosen in our   pilot study to obtain a competitive baseline per-   formance . We set the number of teacher modelsKin each iteration to 5 without further searching .   We use bert - base - uncased to initialize the student   model . All experiments are conducted on 40 GB   A100 machines .   C Performance Over Self - training   Iterations   We provide the performance curve w.r.t . iterations   in Figure 5 . It shows that the iterative training pro-   cedure following the classical self - training method   is indeed effective . We simply set iteration to 10   as most of the self - training methods did and ﬁnd it   already a robust choice across different datasets .   D Scale of Synthesizer Model   We test SynRE with a different scale LLM , i.e. ,   GPT - Small with 117 M parameters . The results in   Table 11 show that even with such a small size LM ,   SynRE can still bring signiﬁcant improvements .   But in general , larger model unsurprisingly per-   forms better . With the emergence and applicability   of increasingly stronger LLMs , we can look for-   ward to further advancement of relation extraction   task .   E Ablation on Multi - teacher Distillation   We provide ablation on multi - teacher distillation   in Table 12 , and demonstrate that the primary im-   provements come from the utilization of synthetic   data and the proposed two - stage self - training . The   choices of ablations are :   + Multi - teacher ( Niave Ensemble ) We train k   models on the golden training set and ensemble   their predicted logits . This only provides marginal   beneﬁts ( +3.62 , +3.13 ) .   + Synthetic Data We incorporate synthetic data   by performing knowledge distillation from multi-   teacher to students . This provides rather signiﬁcant   beneﬁts ( +6.01 , +7,83 ) .   + Two - stage Self - training We further introduce   the proposed two - stage self - training method , which   brings about the most remarkable improvements   ( +16.72 , +14.55 ) .   F Effects of Domain - Augmented LLM   In the main results ( Table 3 ) we have speciﬁcally   used a domain - augmented version of GPT2 ( GPT2-   PubMed ) for biomedical task ChemProt . This is   our initial choice of design and intuitively should8199   F   L   F   F   L   F   bring better performance . Here we further analysis   the effects of such LLM choices in detail . Table 13   provides comparison between GPT2 - PubMed and   vanilla GPT2 , both LLMs can effectively produce   synthetic data and bring expected improvements .   Nonetheless , we empirically ﬁnd that vanilla GPT2   would need a bit more ﬁnetuning steps to adapt to   the target domain ( 256 steps compared to 32 steps   using GPT2 - PubMed ) . In general , the proposed   method is rather robust to choices of LLM.G Case Study   We provide randomly sampled case studies of syn-   thetic data for SemEval , TACRED , and ChemProt   in Table 14 , 15 , and 16 respectively as well as dis-   tant data in Table 17 . These cases show that LLMs   are capable of synthesizing coherent , realistic sen-   tences with relational structure . Most importantly ,   such synthetic data are customized to target do-   mains with various topics and styles.8200   Nevertheless , we also notice several limitations ,   especially in low - resource scenarios where it ’s still   challenging to get a good estimation of the target   dataset distribution :   •Lack of diversity . For example , instances 2.1 ,   2.2,2.3all start with " the marmalade " .   •Fragmentary structure . For example , in-   stances 2.4and2.8contain atypically lengthy   object .   For pseudo labels , most of the time teacher   model conﬁdently assigns one speciﬁc label with   very high probabilities ( > 0.95 ) , but for some other   cases , it goes for more than one possible label , such   as1.8,2.8,4.1 , etc . We attribute this to two possi-   ble reasons : 1 ) the limited capability of the teacher   model to accurately recognize all relations , and   2 ) the imperfections of certain synthetic data , i.e. ,   some synthetic instances do not well align with   pre - deﬁned schema and are difﬁcult to be assigned   exact relation labels . In these cases , forcing the   student to learn from hard labels assigned using   argmax might introduce severe noise , while the   proposed knowledge distillation process using soft   labels in SynRE can properly put these imperfect   data still into usage .   For distant data , as these instances are produced   from wikipedia texts , we can clearly identify that   they are quite different from other downstream task   data ether in content , or in relation schema . This   further veriﬁes the superiority of the proposed syn-   thetic in - domain data qualitatively .   H Potential Limitations   We empirically conclude two limitations for   SynRE in the hope of inspiring more future re-   search . On one hand , its advantages are less sig-   niﬁcant when a large amount of annotated data   is available . For example , TACRED training set   has 68,142 annotated instances . Under this setting ,   even if we add another 100,000 synthetic samples ,   the improvement is only +0.98 compared to +22.02   under 1 % training set . This means that the quality   of synthetic data , although superior to distant ones ,   is still not as good as golden ones . Thus they can   hardly provide identical utility the same as 100,000   golden data . Nevertheless , with the development   of LLMs and their powerful generation ability , we   look forward to accessing higher - quality synthetic   data .   On the other hand , when training data are lim-   ited to a few samples ( for example , 1 % setting for   SemEval only includes 73 training instances ) , even   strong LLMs like GPT-2 can not perfectly ﬁt the   structure of relational statements within a few steps   of ﬁnetuning ( See Appendix G for illustration of   cases ) . Therefore , many generated sentences may   not contain correct subject or object entity markers   as requested and have to be discarded . In general ,   although the formation of marked natural language   sequence proposed in this work made such struc-   tured synthesis feasible , we look forward to further   improving the synthesis efﬁcacy in future works.8201   SemEval F   SemEval F8202   TACRED F   TACRED F8203   ChemProt F   ChemProt F8204   Distant Supervision8205ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.8206 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.8207