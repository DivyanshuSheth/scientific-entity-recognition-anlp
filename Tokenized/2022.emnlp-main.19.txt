  Min - Hsuan Yeh , Vicent Chen , Ting - Hao ‘ Kenneth ’ Haung , Lun - Wei Ku   University of Massachusetts Amherst , University of Illinois Urbana - Champaign ,   Pennsylvania State University , Institute of Information Science , Academia Sinica   myeh@umass.edu , vfchen2@illinois.edu ,   txh710@psu.edu , lwku@iis.sinica.edu   Abstract   Generating engaging content has drawn much   recent attention in the NLP community . Ask-   ing questions is a natural way to respond to   photos and promote awareness . However , most   answers to questions in traditional question-   answering ( QA ) datasets are factoids , which   reduce individuals ’ willingness to answer . Fur-   thermore , traditional visual question genera-   tion ( VQG ) confines the source data for ques-   tion generation to single images , resulting in   a limited ability to comprehend time - series in-   formation of the underlying event . In this pa-   per , we propose generating engaging questions   from multiple images . We present MVQG , a   new dataset , and establish a series of baselines ,   including both end - to - end and dual - stage ar-   chitectures . Results show that building stories   behind the image sequence enables models to   generate engaging questions , which confirms   our assumption that people typically construct   a picture of the event in their minds before ask-   ing questions . These results open up an excit-   ing challenge for visual - and - language models   to implicitly construct a story behind a series   of photos to allow for creativity and experi-   ence sharing and hence draw attention to down-   stream applications .   1 Introduction   The popularity of image - sharing behavior in chats   and social media applications shows that this is a   natural way to increase participant engagement ( Hu   et al . , 2014 ) . In response , asking questions based   on these photos is a straightforward method to pro-   mote awareness , sustain attention , and acquire use-   ful information . An obvious example is that when   we see someone share a photo of a car accident   on Facebook , commenting “ Was anyone injured   in the crash ? ” draws more attention and replies   from both the author and other readers than “ OhFigure 1 : Two examples of MVQG . Each data point   consists of an image sequence and two to five engaging   questions written by humans . The machine should gen-   erate a question over a given image sequence .   my goodness , that is serious . ” Furthermore , from   the author ’s aspect , posting photos on social media   with an engaging , image - related question helps the   author to hear the public voice of their feelings and   thoughts , and keeps the author connected with the   world ( Lu et al . , 2021 ) .   However , not all the questions have the same   effect . Questions in traditional text - based QA   datasets such as SQuAD ( Rajpurkar et al . , 2016 ) ,   NarrativeQA ( Kociský et al . , 2018 ) , or Fairy-   taleQA ( Zhao et al . , 2022 ) are for educational pur-   poses and language understanding , which do not   seek to encourage people to reply . Meanwhile ,   questions in the VQA dataset ( Antol et al . , 2015a )   usually ask about the color of an object or its po-   sition , to which the answers are too obvious for   humans to respond . In fact , these two kinds of   questions are rarely seen in daily chat and social   media posts . Shuster et al . ( 2019 ) state that humans   consider engaging and effective captions those that   “ avoid stating the obvious . ” As with image cap-   tions , an engaging and effective question asks about   things behind the scenes and is usually open - ended .   Moreover , the input information for question   generation also matters . Images are more straight-277forward for humans than text , and they provide   plenty of room for imagination . In addition , im-   ages shared on social media are often sequential   instead of solitary , as a single image gives readers   only a limited understanding of the experience be-   ing shared . To this end , despite the existence of   question generation ( QG ) datasets such as those   created by Lu et al . ( 2021 ) , Wei et al . ( 2021 ) , or   Mostafazadeh et al . ( 2016 ) , which contain engag-   ing questions , their text - based or single - image set-   tings limit the usage of current QG models in pop-   ular applications .   To facilitate downstream applications involving   a set of shared images , e.g. , accompanying robots ,   social media robots , automatic assistants , or rem-   iniscence therapy , we propose generating an en-   gaging question from multiple images . We create   MVQG , a new dataset , by asking workers to write   down a question following instructions based on   a sequence of photos from VIST ( Huang et al . ,   2016 ) , a dataset consisting of five sequential im-   ages and a story about those images . For a better   illustration of the task , Figure 1 shows two exam-   ples of MVQG . Unlike the instruction of VQA ’s   data collection ( Antol et al . , 2015b ) asked workers   to imagine “ a smart robot “ that “ understands a lot   about images , “ such as objects , scenes , or color , or   texture , and come up with questions to “ stump this   smart robot . ” Our instruction , on the other hand ,   asked workers to imagine that they want to have a   conversation with people on Twitter and hence to   write a question to start that conversation . The data   analysis shows that our instructions help collect   more engaging questions than VQG ( Mostafazadeh   et al . , 2016 ) , the benchmark dataset for visual ques-   tion generation . Furthermore , we establish a series   of baselines , including both end - to - end and dual-   stage architectures . The experimental results show   that information about stories behind the image   sequence helps baselines generate engaging ques-   tions , which confirms our assumption that humans   typically construct stories in their heads before ask-   ing questions . These results open up an exciting   challenge for visual - and - language models : implic-   itly constructing a story behind a series of photos to   allow for creativity and experience sharing , hence   drawing attention to its downstream applications .   The contributions of our paper are threefold :   first , we introduce a novel task multi - VQG and   MVQG , a new dataset : given a sequence of rele-   vant images , generate a corresponding engagingquestion ; second , we propose several baselines and   show that story information helps baselines to gen-   erate engaging questions from image sequences ;   third , we propose five aspects for human evaluation   as benchmarks to better evaluate the engagement   of generated questions .   2 Related Work   User engagement has received much recent atten-   tion in the NLP community . Mostafazadeh et al .   ( 2016 ) created the first visual question generation   dataset comprised of natural and engaging ques-   tions . However , engagement is not well - stated in   this work ; it simply means “ the first question that   comes to mind ” . Shuster et al . ( 2019 ) present an en-   gaging image captioning task to improve the ability   of machines to communicate with humans ; engag-   ing captions are defined as captions that “ avoid stat-   ing the obvious . ” Lu et al . ( 2021 ) develop a dataset   for poll - question generation for social media posts .   This work demonstrates that the poll question is   an engaging question that can be utilized to help   us hear the public voice for decision - making and   thus better understand our society . Wei et al . ( 2021 )   state that an engaging and attractive question may   incorporate additional details or emotional phrases .   Such questions are more likely to be answered . Im-   ages and questions are two prominent elements in   these works , indicating that visual stimulation and   inquiry are typical means to communicate aware-   ness and sustain connections . However , these stud-   ies primarily consider single images , limiting the   use of current QG models in popular applications   because individuals typically share multiple photos   to express more comprehensive experiences . In our   study , we propose generating engaging questions   over an image sequence and creating a dataset com-   prised of five photos and human - written questions .   A visual - and - language ( VL ) model is typically   used to generate engaging questions from images .   After the development of BERT ( Devlin et al . ,   2019 ) , various BERT - based VL models were pro-   posed . These VL models are designed to integrate   information from both vision and language modal-   ities via an encoder , and are categorized into fu-   sion encoders and dual encoders based on how   input from distinct modalities is aggregated ( Du   et al . , 2022 ) . Fusion encoder models such as   VisualBERT ( Li et al . , 2019 ) , XLMERT ( Cho   et al . , 2020 ) , SOHO ( Huang et al . , 2021 ) , and VL-   T5 ( Cho et al . , 2021 ) encode text embeddings and278image features in the same model with different fu-   sion approaches . Following self- or cross - attention ,   the hidden state of the last layer is treated as a   fused representation of different modalities . Be-   cause fusion encoder models require image and   text pairings as input , the model must input all pos-   sible pairs in image - text matching tasks , resulting   in poor inference speed . Dual encoder models such   as CLIP ( Radford et al . , 2021 ) , on the other hand ,   use two single - modal encoders to encode the two   modalities separately and use the dot product to   project the image embedding and text embedding to   the same semantic space to compute VL similarity   scores . Although dual encoder models are lighter ,   they frequently fail in difficult VL understanding   tasks . As a result , we continue to employ fusion en-   coder models as baselines in our work . We choose   VL - T5 ( Cho et al . , 2021 ) as the backbone in partic-   ular because it treats all VL tasks as text - generating   tasks , which is appropriate for our question gener-   ation scenario . Inspired by Shen et al . ( 2022 ) , we   propose an additional baseline model by employing   the visual encoder of CLIP ( Radford et al . , 2021 )   instead of the self - trained image feature extractor   in our fusion encoder , so that the image features   are better projected into the semantic space .   3 Dataset : MVQG   3.1 Selection of Image Sequences   Given that the proposed task is to generate engag-   ing questions based on a cohesive narrative , the   input photographs can not be randomly selected   from an image set . As a result , we choose image   sequences from the VIST dataset , the first dataset   of sequential photos accompanied by stories . In the   VIST dataset , each image sequence containing five   photos extracted from a Flickr album of a human   event ( e.g. , “ wedding ” or “ first day of school ” ) ; five   photos must be taken within a 48 - hour span . Work-   ers constructing VIST arranged the five photos in   the order chosen , and then wrote a sentence for   each photo to create a story . This procedure guar-   anteed that the chosen image sequences were “ sto-   ryable ” , i.e. , they contained at least one continuous   narrative of an event or scene for question genera-   tion . In addition , although many social - media posts   include multiple images that are not necessarily se-   quential , when social media users create a post   that includes multiple photos , these photos often   capture the same scene , event , or concept ; that is ,   these photos can have similar properties to those inthe VIST dataset . To this end , we randomly chose   7,700 image sequences from the VIST training set   and chose all 1,999 sequences from the VIST test   set and assigned them to workers to annotate the   engaging questions .   3.2 Question Annotations   Human brains are excellent at object recognition ;   they can quickly recognize the most significant   details in photographs . However , finding the rela-   tionship between visuals and developing a unified   narrative of events or scenes behind those items   requires more time for humans . Thus , if workers   are asked to write down a question immediately   after seeing the image sequence , they may merely   inquire about the first object that comes to mind ,   rather than ask engaging questions based on a co-   hesive narrative behind the photos . To solve this   problem , we created a data annotation approach   to assist workers in writing suitable sentences by   answering a three - stage question :   Q1 . Please list the top five objects ( e.g. , dogs ,   trees ) or events ( e.g. , weddings , parties ) you   regard as being the most important in the im-   age sequence .   Q2 . Please describe the visual sequence using one   or more sentences based on the items and   events you observed in Q1 .   Q3 . Imagining that you decide to post this image   sequence on Twitter and want to expand the   conversation by solely commenting on a ques-   tion connected to these images . What is the   question you would ask based on the descrip-   tion you gave in Q2 ?   This strategy implicitly prompted workers to for-   mulate an abstract notion of the image sequence   according to their observations . As a result , we   were able to obtain engaging questions that corre-   sponded to the cohesive narratives of the events   depicted in the visual sequences . Furthermore , the   descriptions provided in Q2 qualify this dataset   for multi - image captioning , making it suited for   use in a wider range of vision - and - language appli-   cations , e.g. , image captioning . Moreover , many   recent question generation models are answer-   agnostic ( Dugan et al . , 2022 ; Chowdhury et al . ,   2022 ) . Their findings show that adding context   summaries as the intermediary layer can improve   the relevance and interpretability of generated ques-   tions . Inspired by their research , the descriptions   provided in Q2 can also serve as summaries to279   generate event - centric questions .   We gathered MVQG questions by crowdsourc-   ing the task on Amazon Mechanical Turk ( AMT ) .   For each image sequence , we assigned 2 to 5 work-   ers to annotate questions ( at $ 0.2 / HIT ) . We only   accepted participants with a 98 % or more HIT ac-   ceptance rate , had 3,000 or more finished HITs , and   were located in the US . We also required turkers to   spend at least 30 seconds on each assignment . In   total , we asked workers to annotate 9,699 image   sequences and obtained 31,421 questions . After   the annotation process , we manually revised the   grammatical errors in all questions . The dataset   will be released after the paper is accepted .   4 Dataset Analyses   4.1 Data Statistics   Table 1 reports the statistics of the crowdsourcing   task . Figure 2 shows the histogram of the sentence   length of the questions in MVQG , where the aver-   age question length is 10 tokens ( Std=3 ) . Table 2   list the top-15 frequent n - gram ( with n=3 ) of ques-   tions opening in MVQG ; this suggests that users   on social media tend to ask open - ended questions   ( beginning with “ Have you ever ” , “ Do you think ” ,   or “ How do you ” ) , inviting others to share their   opinions and expand the conversation . The top-30   frequent words in MVQG are listed in Table 3 ; this   demonstrates that the questions we gathered con-   tain subjective words such as like , think , favorite ,   andfeel , indicating that the collected questions are   more related to people ’s perspectives than objective   facts , encouraging individuals to answer them .   4.2 Disentangling MVQG Effectiveness   Experimental Settings Two sources contribute   to the efficacy of MVQG questions : 1 ) our question   annotation approach , and 2 ) the cohesive narrative   of events resulting from the five - photo arrangement .   We conducted an experiment to investigate the ef-   fect of these two factors on the question quality .   First , to evaluate the influence of the annota-   tion approach , we selected VQG images and an-   notated them with different instructions . We ran-   domly chose 200 samples from VQG and hired one   worker per sample to annotate the image with our   instruction . The annotated questions ( VQG )   were then compared to the questions collected with   original VQG instruction ( VQG ) . Then , to   evaluate the effect of the number of images , we   randomly selected 200 samples from MVQG . For   each five - image sample , we randomly chose one   image and hired one worker to annotate the selected   image per our instructions . The questions with the   one - photo setup ( VIST ) were then compared to   the questions with the original five - photo MVQG   setup ( VIST ) .   Quality Criteria Following Ferraro et al . ( 2015 ) ,   we evaluated the quality of questions according to   the following criteria :   •V ocabulary size : the number of unique vocabu-   lary words .   •Average sentence length : this shows how rich   and descriptive the sentences are ( Ferraro et al . ,   2015 ) . Writing a sentence is a high - cognitive   task . However , to complete numerous jobs fast ,   MTurk workers typically write short and simple   sentences ( e.g. , “ What is the girl doing ? ” ) . These   short questions are not detailed and are frequently   similar to those from other workers . In other280   words , long question requires more effort from   MTurk workers and can be more diverse , which   may lead to higher quality .   •Syntactic complexity : the amount of embed-   ding / branching in a sentence ’s syntax . We report   the mean Yngve score ( Yngve , 1960 ) normalized   by the sentence length .   •Percentage of abstract terms : this indicates the   range of visual and non - visual concepts covered   by the dataset . Of all noun tokens on Word-   Net ( Fellbaum , 1998 ) , tokens belonging to Ab-   stract ( Physical ) Entity are regarded as abstract   ( concrete ) terms .   •Average term depth : noun terms on WordNet   with a smaller depth indicate higher - level con-   cepts ( Liu et al . , 2021 ) .   Results The first two columns in Table 4 show   that questions in VQGhave a 1.7 times larger   vocabulary size and are about 2 times longer on   average than questions in VQG , which reflects   the fact that the proposed annotation approach   yields more diverse and descriptive sentences . The   third and forth columns in Table 4 indicate that   questions in VQGexhibit more complex sen-   tence structure and have more abstract words than   VQG , implying that writing down descriptions   first helps individuals think more about the abstract   events behind the images and thus yields more com-   plex questions . This then makes our collected ques-   tions much easier for individuals to engage with .   The last column in Table 4 shows that questions in   VISThave a smaller term depth than questions   inVIST , suggesting that questions in VIST   use more high - level concepts . Basic - level cate-   gories were typically used to name things ( Rosch   and Mervis , 1975 ; Anglin , 1977 ; Brown , 1958 ) ,   whereas in multi - image scenarios , higher - level   ideas were more often used to cover things in var-   ious photos ( Murphy , 2022 ) . This encourages in-   dividuals to answer questions not only based on   the things they saw , but by imagining the story or   the relations of objects in the five images . Thisshows that our instructions contributed more to   the engagement of the collected questions than the   multi - image setting .   5 Baselines   We propose both end - to - end and dual - stage VL   baselines for MVQG . We introduce each baseline   here and provide the details in Appendix A.   For the end - to - end baselines , we chose the VL-   T5 model ( Cho et al . , 2021 ) as the backbone . VL-   T5 inputs contain the visual embedding Vand the   visual semantic grounding G. Each image Vis han-   dled as a sequence of visual embeddings consisting   of the whole image embedding and its object re-   gion embeddings . As visual embeddings from RoI   features lack semantic meaning , we inject visual   semantic grounding into VL - T5 to facilitate seman-   tic understanding and cross - image reasoning . We   adopt grounded situation recognition ( GSR ) ( Pratt   et al . , 2020 ) and the corresponding JSL model to   produce structured semantic summaries of images .   For each image V , JSL outputs a verb representing   the salient activity of Vand its 1 to 6 correspond-   ing semantic roles . The predicted verb and nouns   are combined as the visual semantic grounding G   of each image .   We propose three fine - tuned versions of VL-   T5 respectively pretrained on VCR ( Zellers et al . ,   2019 ) ( VL - T5 ) , VIST ( VL - T5 ) , and   VQG ( VL - T5 ) , and fine - tuned on MVQG .   He et al . ( 2019 ) show that after standard fine-   tuning , the model forgets important language gen-   eration skills acquired during pretraining . There-   fore , we propose the adapt - tuned version of VL-   T5 by adding the adapter layer to each Trans-   former block of the baseline , and replacing the   fine - tuning stage with adapt - tuning . In the adapt-   tuning stage , we update only the parameters of the   adapter layer and freeze all other parameters . We   pretrain our model on VIST ( VL - T5 ) and   VQG ( VL - T5 ) , and adapt - tune on MVQG .   Moreover , inspired by Shen et al . ( 2022 ) , which281   shows that the CLIP visual encoder ( Radford et al . ,   2021 ) can be used as visual embedding and im-   prove the VL model performance , we propose the   CLIP version of VL - T5 by replacing the visual em-   beddings of VL - T5 with the output of the CLIP   visual encoder ( VL - T5 ) .   For the dual - stage baselines , we first used an   image captioning model to generate a description   from an image sequence , after which we used   a question generation model to generate a ques-   tion from the description . The image caption-   ing model used was a VL - T5 model pretrained   on VCR , and the question generation model was   a T5 model ( Raffel et al . , 2020 ) pretrained on   SQuAD ( Rajpurkar et al . , 2016 ) . We provided   three types of text as descriptions : ( 1 ) captions   from the VIST dataset ( CAP2Q ) , ( 2 ) stories from   the VIST dataset ( STY2Q ) , and ( 3 ) summaries   from Q2 in MVQG ( SUM2Q ) . The VL - T5 image   captioning model and question generation model   were fine - tuned on these three description types , re-   spectively . As the end - to - end baselines used CLIP   to encode visual input , we adopted the CLIP visual   encoder in our dual - staged baselines . We replaced   the T5 model in the second stage with VL - T5 and   then used the result of the CLIP visual encoder as   visual input and the descriptions as textual input .   For the different types of descriptions , we propose   CAP2Q , STY2Q , and SUM2Q.   6 Experiment and Discussion   We randomly divided MVQG into the training   ( 70 % ) , val ( 20 % ) , and test ( 10 % ) sets , and eval - uated the models introduced earlier with human   and automatic metric evaluation .   6.1 Human Evaluation   Recent work has demonstrated the unreliability of   automatic evaluation and recommends relying on   human evaluation ( Liu et al . , 2016 ) . Therefore ,   we first conducted a human evaluation to under-   stand how people feel about the generated ques-   tions , specifically whether they are natural , engag-   ing , and focus on high - level relations among ob-   jects . We randomly selected 100 image sequences   from the MVQG test set and generated questions   for each using our established baselines and mod-   els . For each sequence , we hired five workers from   Amazon MTurk to rank the generated questions   according to the following benchmarks :   Benchmark 1 : When you see images like these   on social media , it is natural to ask this question .   Benchmark 2 : This question focuses primarily   on the essential objects of the images and the   relationships between these objects .   Benchmark 3 : This question focuses primarily   on the story or event behind all the images rather   than one specific image .   Benchmark 4 : This question is specific to the   event where the photos were taken . It could be   irrelevant or weird to ask this question for other   similar events .   Benchmark 5 : This is an engaging question for   this set of photos . You would want to answer this   question if you saw it on social media .   Empirically , it is difficult for workers to rank   many items at the same time ; results thereof are282   unreliable . Therefore , we divided our baselines   into four groups for further discussion . Results are   shown in Table 5 .   Group 1 : Different Pretrained Datasets First ,   we compare three VL - T5 baselines pretrained on   VIST , VCR , and VQG , respectively . The first group   of results in Table 5 reveals that the VL - T5 baseline   pretrained on VIST performs best on most of the   benchmarks . The substantial difference between   VL - T5 and VL - T5 on Benchmark 5   suggests that story information in the pretraining   stage helps models ask more engaging questions .   Group 2 : Image Description Type We com-   pare three Description2Q baselines , each contain-   ing captions , stories , and summaries as the input   text . The result is displayed in the second group   of Table 5 . CAP2QandSUM2Qperform   well on the Benchmarks 2 and 4 because captions   and summaries of photos are better able to provide   details of objects and lead to more specific ques-   tions . However , STY2Q has the most rank-1   questions based on Benchmark 1 . This suggests   that story information results in more natural ques-   tions . This finding also suggests that naturalness   may not be the main factor leading to engagement ,   which contradicts the premise in VQG .   Group 3 : With or Without Story Informa-   tion Third , we investigate the differences be-   tween baselines with and without story information .   We compare with - story baselines ( VL - T5   andSTY2Q ) and the without - story baseline   ( VL - T5 ) . The result in the third group of Table 5   shows that humans prefer questions generated by   baselines with story information . Moreover , the   fact that STY2Q outperforms VL - T5 onthe Benchmark 5 suggests that the generated ques-   tions could be even more engaging if the story in-   formation were more explicit .   Group 4 : Fine - tuning and Adapt - tuning Fi-   nally , we compare the difference between fine-   tuning and adapt - tuning strategies on VL - T5 base-   lines pretrained on VIST . The result in the last   group of Table 5 shows that VL - T5 out-   performs VL - T5 on Benchmarks 1 to 4 ,   whereas VL - T5 surpasses VL - T5 on   Benchmark 5 . Because adapt - tuning retains more   information gained via VIST , this result confirms   the prior finding that explicit story information re-   sults in engaging questions . Also note that this   result shows that engagement does not rely only on   Benchmarks 1 to 4 as shown in related work .   Ranking vs. Rating In addition to ranking , sev-   eral studies evaluated the generated text via human   rating ( Hu et al . , 2020 ; Wang et al . , 2020 ) . Though   literature has shown that rating result is almost   with no correlation with direct ranking ( Hsu et al . ,   2022 ) , here we still provide both results among   VL - T5 , VL - T5 , and VL - T5 for   reference . For the rating experiment , we ask work-   ers to rate the generated questions from 1 ( the   worst ) to 5 ( the best ) according to the 5 bench-   marks . We conduct both ranking and rating ex-   periments on the whole testing set ( N=599 ) . The   result in Table 6 shows that for ranking evaluation ,   VL - T5 outperforms other two baselines on   all benchmarks significantly ( the Kruskal - Wallis   test , p=0.02 ) , aligning the result in Table 5 , while   for rating evaluation , VL - T5 performs bet-   ter insignificantly ( p=0.87 ) . These results overall   confirm that VL - T5 is a better setting and283   MVQG should use ranking for evaluation .   6.2 Automatic Evaluation   Although human evaluation is already a good indi-   cator of model performance , we still provide the   automatic evaluation results here for reference . We   evaluate the baselines with BLEU ( Papineni et al . ,   2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , and   BLEURT ( Sellam et al . , 2020 ) .   Table 7 shows the results . VL - T5 outper-   forms other baselines , particularly VL - T5   and VL - T5 . In addition , STY2Q and   SUM2Qoutperform other dual - stage baselines .   These two results support the human evaluation   result : that models with story information gener-   ate more engaging questions . Moreover , VL - T5   outperforming VL - T5 andSTY2Q out-   performing STY2Q indicate that the CLIP model   provides better embeddings for question genera-   tion . Furthermore , all the dual - staged models with   CLIP encoder outperform those without it . Since   the second stage of those without CLIP generates   questions from only text , and the second stage of   those with CLIP generates questions from both   texts and images , this result illustrates the assis-   tance of visual information for MVQG . The only   result that differs from the human evaluation is   that VL - T5 outperforms VL - T5 and   STY2Q. However , this is straightforward to   explain : the end - to - end fine - tuned model maintains   the least information from pretraining and leads to   the most similar outcome to the fine - tuning data ,   which gives it an advantage in the automatic metric   evaluation where exact matches are rewarded .   6.3 Effect of Multi - Image Setting   We study the impact of the multi - image setting on   beselines . Here we seek to determine whether the   most relevant image can represent the entire image   sequence , as questions can focus on only one cer-   tain event or object . We begin by determining the   most representative image in the image sequence   by calculating the CLIP score , the cosine similarity   between each image and the ground truth ques-   tion . Then we examine questions generated from   three types of input : ( 1 ) the entire image sequence ,   ( 2 ) only the most relevant image , and ( 3 ) the image   sequence without the most relevant image .   Table 8 shows the experiment results . Using the   most relevant image leads to the lowest score in   most of the baselines , implying that a single image   can not in fact represent the whole image sequence   and the underlying event or scenario . Surprisingly ,   the results also show that even after removing the   most relevant image , the performance of some base-   lines is still high . This suggests that other images in   the sequence assist in the reconstruction of missing   information and even leave room for more imagi-   nation . It also shows that the collected questions   cover information from all images in the sequence .   6.4 Case Study   Table 9 displays example image sequences and   questions generated by baselines . Cases 1 and 2   provide clues for the reason why human eval-   uation and automatic metrics produce inconsis-   tent results for VL - T5 , VL - T5 , and   STY2Q. In case 1 , both the ground truth284and the VL - T5 output mention the flower ,   whereas VL - T5 focuses on the insects and   the bird . Because fine - tuned models are more likely   to forget the pretrained task and fit the ground truth   of the fine - tuned task , VL - T5 may obtain a   higher score from the match - based automatic met-   rics . Adapt - tuning , on the other hand , retains more   information from the pretrained task and results in   models that do not always follow the guide of the   ground truth . As a result , while VL - T5 may   have a lower automatic evaluation score , it may   generate questions that follow the story , reflecting   human preferences . Case 2 shows the case of di-   verse images . As the first three photos are very dif-   ferent from the last two , it is hard for VL - T5   to generate an engaging question using implicit   story information , resulting in a general question .   STY2Q , in contrast , takes an explicit story as   input , which enables the model to generate a ques-   tion connected to the underlying story .   Although STY2QandVL - T5 appear   to be better than VL - T5 , the generated ques-   tions may still include errors . Case 3 is an example   illustrating several commonly - seen errors . First , an   object and relationship detection error is observed   in the output of VL - T5 . The baseline mistak-   enly detects the objects in the last image as football   players . As a result , it asks “ Are you fascinated by   football ? ” instead of “ baseball . ” Second , an infer-   ence error is shown in the output of STY2Q ,   where people are dressed up in costumes in the   second image but it mistakenly detects the event   as a costume party . Here we see that grounding   and event inference are two major directions for   improving the quality of the generated questions .   7 Conclusion   We propose a novel task : given a sequence of im-   ages , generate an engaging question . This task   extends visual question generation by enabling rea-   soning across images to comprehend a complete   story . We collect MVQG by asking workers to   write down five obvious objects , a summary of the   image sequence , as well as an engaging question   that they would want to post on social media . We   establish several baselines for this task . Experi-   mental results reveal that image - related stories help   models generate engaging questions , and that using   multiple images as input helps models understand   the overall picture of the current situation , leading   to a better question . The task , dataset , and ex-   perimental results we provide open up an exciting   challenge for visual - and - language models to im-   plicitly construct a story behind a series of photos   for creativity and experience sharing and further   attracting attention for downstream applications .   Limitations   Like most crowdsourced datasets , MVQG inherits   the common biases of using online crowdsourc-   ing platforms to collect data . For example , the   crowd workers on Amazon Mechanical Turk do   not represent the user population of popular social   media , such as Twitter . Furthermore , although we   instructed workers to write questions as if they were   posting on Twitter , the used language would still   be different . People on social media use informal   words and netspeak frequently , but crowd workers   are incentivized to get their work approved and285might prefer to use more formal languages or po-   lite tones . Moreover , since we specifically encour-   aged MTurk workers to imagine they are writing   questions that they would ask on Twitter , MVQG   may be potentially biased on Tweet - liked data . We   expect that different platforms will encourage dif-   ferent text styles , but given the amount of data we   could financially afford to collect in the first study   for this research problem , we decide to focus on   only one platform ’s style to reduce possible factors .   Asking workers to imagine Facebook or Instagram   can be another practice , but it will still introduce   different biases .   Another limitation is the evaluation of engage-   ment . We evaluated the question engagement by   asking crowd workers to rank the questions using   different criteria . However , this approach does not   capture the in - the - moment feelings or authentic   reactions of social media users . The human evalua-   tion results may not reflect the actual performance   when the technology is being deployed in the wild .   Ethical Considerations   Although our research aims to produce natural and   engaging questions , we are aware of the possibil-   ity of employing a similar approach to generate   inappropriate , sexist , or racist questions . Further-   more , as the proposed methods use a pre - trained   grounded situation recognition and a T5 model as   components , the generated questions might inherit   the biases of their training data . More research is   required to understand and mitigate these risks .   Acknowledgements   This research are partially supported by National   Science and Technology Council , Taiwan , under   Grant no . 110 - 2634 - F-002 - 051- and 108 - 2221 - E-   001 - 012 - MY3 .   References286287288   A Implementation Details of Baselines   A.1 VL - T5   The input of the VL - T5 model is depicted in Fig-   ure 3 . The input contains the task prompt , the vi-   sual embedding , and the visual semantic grounding .   Each semantic grounding embedding is the sum of   the token embedding and the image positional em-   bedding . The semantic grounding is produced by   grounded situation recognition ( GSR ) ( Pratt et al . ,   2020 ) and the corresponding JSL model . Consider   the image in Figure 3 , which depicts a man teaching   a boy . JSL predicts the primary activity teaching   ( verb frame ) and then the agent man and place room   as its semantic roles . The predicted verb and nouns   are combined as the visual semantic grounding G   of each image . In particular , when tokenizing , we   quote the verb with the starting and ending tokens   < b_verb > and < e_verb > to highlight the ac-   tivity , and the < b_[role ] > and < e_[role ] >   tokens to spot the roles and their types , as illus-   trated in Figure 3 . The decoder , which is similar   to the original T5 decoder , is omitted from the fig-   ure for brevity . The embeddings of text tokens for   these semantic roles are randomly initiated during   training , and each text embedding is combined with   the image ’s positional index embedding of its as-   sociated visual embedding Vto link the semantic   role tokens to their corresponding visual images .   Figure 4 illustrates how images are encoded .   Each image Vis handled as a sequence of vi-   sual embeddings V={v , v , . . . , v}consist-   ing of the entire image embedding vand its k   object region embeddings vtov . Each visual   embedding vincludes ( 1 ) RoI features : the hidden   representation of the bounding box created by a   ResNet50 ( He et al . , 2015 ) model , ( 2 ) RoI bound-   ing box coordinates : the upper left and the lower   right points of the box and its area , ( 3 ) image po-   sitional indices : ι∈ { 1 , . . . , n } , where nis the   number of images , used to discriminate regions   from different images , and ( 4 ) object positional in-   dices : ι∈ { 0 , . . . , k } , which serve as positional   embeddings in an image . These are all projected   to 768 - dimensional vectors , summed , and layer-   normalized to form the final visual embedding v.   Note that ιinvis 0 .   We used the AdamW optimizer with a learning   rate of 1e-4 and a batch size of 8 for both pretrained   and fine - tuned tasks . During inference , we used   nucleus sampling with p= 0.9 , which has been   shown effective in generating diverse text ( Holtz-   man et al . , 2019 ) .   A.2 Adapter Layer   For the adapt - tuned baselines , we employed an   adapter layer after the original Transformer layers   for both the VL - T5 encoder and decoder , as shown   in Figure 5 . The adapter layer down - projects the   input as a 384 - dimensional vector , passing it into   a GELU activation function ( Hendrycks and Gim-   pel , 2016 ) , up - projecting it to the original size , and   finally using a residual layer to sum the projected289   vector with the input . During the pretraining stage ,   we bypassed the adapter layers and trained only   the parameters of the original Transformer layers .   During the adapt - tuning stage , we considered and   trained the adapter layers while fixing the parame-   ters of the original parts .   A.3 CLIP as Visual Encoder   Figure 6 depicts how the CLIP visual encoder is   used in the VL - T5 baseline . Instead of finding RoI   features and bounding boxes in each image , we put   the entire image into the CLIP visual encoder and   obtained the visual embedding . Because the visual   embedding from CLIP was a 1024 - dimensional   vector , we projected it onto 768 dimensions using a   linear layer . The CLIP visual encoder and the linear   layer were tuned during the training stage . The   CLIP variant we used was CLIP - RN50 ( ResNet50   as the visual backbone ) .   A.4 Description2Q   The architecture of Description2Q is shown in Fig-   ure 7 . We used a VL - T5 model to generate descrip-   tions from image sequences . The input of VL - T5   was the same as in A.1 , and the output description   was a caption , story , or summary , depending on the   fine - tuning tasks . Then the generated description   was fed into a T5 model pretrained on SQuAD and   fine - tuned on the ground truth of descriptions to   generate a question.290