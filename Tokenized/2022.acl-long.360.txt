  Wen XiaoIz BeltagyGiuseppe CareniniArman CohanUniversity of British Columbia , Vancouver , CanadaAllen Institute for AI , Seattle , WA , USAPaul G. Allen School of Computer Science & Engineering , University of Washington ,   Abstract   We introduce P , a pre - trained model   for multi - document representation with a fo-   cus on summarization that reduces the need   for dataset - speciﬁc architectures and large   amounts of ﬁne - tuning labeled data . P   uses our newly proposed pre - training objective   designed to teach the model to connect and ag-   gregate information across documents . It also   uses efﬁcient encoder - decoder transformers to   simplify the processing of concatenated input   documents . With extensive experiments on 6   multi - document summarization datasets from   3 different domains on zero - shot , few - shot   and full - supervised settings , P outper-   forms current state - of - the - art dataset - speciﬁc   and pre - trained models on most of these set-   tings with large margins .   1 Introduction   Multi - Document Summarization is the task of   generating a summary from a cluster of re-   lated documents . State - of - the - art approaches to   multi - document summarization are primarily ei-   ther graph - based ( Liao et al . , 2018 ; Li et al . , 2020 ;   Pasunuru et al . , 2021 ) , leveraging graph neural net-   works to connect information between the docu-   ments , or hierarchical ( Liu and Lapata , 2019a ;   Fabbri et al . , 2019 ; Jin et al . , 2020 ) , building inter-   mediate representations of individual documents   and then aggregating information across . While ef-   fective , these models either require domain - speciﬁc   additional information e.g. Abstract Meaning   Representation ( Liao et al . , 2018 ) , or discourse   graphs ( Christensen et al . , 2013 ; Li et al . , 2020 ) , or   use dataset - speciﬁc , customized architectures , mak-   ing it difﬁcult to leverage pretrained language mod-   els . Simultaneously , recent pretrained language   models ( typically encoder - decoder transformers)Figure 1 : P vs existing pretrained models .   have shown the advantages of pretraining and trans-   fer learning for generation and summarization ( Raf-   fel et al . , 2020 ; Lewis et al . , 2020 ; Beltagy et al . ,   2020 ; Zaheer et al . , 2020 ) . Yet , existing pretrained   models either use single - document pretraining ob-   jectives or use encoder - only models that do not   work for generation tasks like summarization ( e.g. ,   CDLM , Caciularu et al . , 2021 ) .   Therefore , we argue that these pretrained models   are not necessarily the best ﬁt for multi - document   summarization . Alternatively , we propose a simple   pretraining approach for multi - document summa-   rization , reducing the need for dataset - speciﬁc ar-   chitectures and large ﬁne - tuning labeled data ( See   Figure 1 to compare with other pretrained mod-   els ) . Our method is designed to teach the model to   identify and aggregate salient information across   a “ cluster ” of related documents during pretrain-   ing . Speciﬁcally , our approach uses the Gap Sen-   tence Generation objective ( GSG ) ( Zhang et al . ,   2020 ) , i.e. masking out several sentences from   the input document , and recovering them in or-   der in the decoder . We propose a novel strategy   for GSG sentence masking which we call , En-   tity Pyramid , inspired by the Pyramid Evaluation   method ( Nenkova and Passonneau , 2004 ) . With   Entity Pyramid , we mask salient sentences in the   entire cluster then train the model to generate them ,   encouraging it to ﬁnd important information across   documents and aggregate it in one summary .   We conduct extensive experiments on 6multi-   document summarization datasets from 3differ-   ent domains . We show that despite its simplic-5245   ity , P achieves superior performance com-   pared with prior state - of - the - art pretrained models ,   as well as dataset - speciﬁc models in both few - shot   and full ﬁne - tuning settings . P performs   particularly strong in zero- and few - shot settings ,   signiﬁcantly outperforming prior state - of - the - art   up to 5Rouge-1 points with as few as 10 examples .   Our contributions are summarized below :   1.We release P , the ﬁrst pretrained gener-   ation model for multi - document inputs with focus   on summarization .   2.We propose Entity Pyramid , a novel pretraining   strategy that trains the model to select and aggre-   gate salient information from documents .   3.We extensively evaluate P on 6 datasets   from 3 different domains for zero - shot , few - shot   and fully - supervised settings . We show that   P outperforms current state - of - the - art on   most of these evaluations with large margins .   2 Model   In this section , we discuss our proposed model   P , a new pretrained general model for   multi - document summarization . Unlike prior work ,   P minimizes dataset - speciﬁc modeling by   simply concatenating a set of documents and pro-   cessing them with a general efﬁcient encoder-   decoder transformer model ( § 2.1 ) . The underlying   transformer model is pretrained on an unlabeled   multi - document dataset , with a new entity - based   sentence masking objective to capture the salient in-   formation within a set of related documents ( § 2.2 ) .   2.1 Model Architecture and Input Structure   Our goal is to minimize dataset - speciﬁc modeling   to leverage general pretrained transformer models   for the multi - document task and make it easy to   use in practice . Therefore , to summarize a set of   related documents , we simply concatenate all the   documents in a single long sequence , and processthem with an encoder - decoder transformer model .   Since the concatenated sequence is long , instead of   more standard encoder - decoder transformers like   BART ( Lewis et al . , 2020 ) and T5 ( Raffel et al . ,   2020 ) , we use the Longformer - Encoder - Decoder   ( LED ) Model ( Beltagy et al . , 2020 ) , an efﬁcient   transformer model with linear complexity with   respect to the input length . LED uses a sparse   local+global attention mechanism in the encoder   self - attention side while using the full attention on   decoder and cross - attention .   When concatenating , we add special document   separator tokens ( < doc - sep > ) between the doc-   uments to make the model aware of the document   boundaries ( Figure 2 ) . We also assign global at-   tention to these tokens which the model can use   to share information across documents ( Caciularu   et al . , 2021 ) ( see § 5 for ablations of the effective-   ness of this input structure and global attention ) .   2.2 Pretraining objective   In summarization , task - inspired pretraining ob-   jectives have been shown to provide gains   over general - purpose pretrained transformers   ( P ; Zhang et al . , 2020 ) . In particular , P- introduces Gap Sentence Generation ( GSG )   as a pretraining objective where some sentences are   masked in the input and the model is tasked to gen-   erate them . Following P , we use the GSG   objective , but introduce a new masking strategy   designed for multi - document summarization . As in   GSG , we select and mask out msummary - like sen-   tences from the input documents we want to sum-   marize , i.e. every selected sentence is replaced by a5246   single token [ sent - mask ] in the input , and train   the model to generate the concatenation of those   sentences as a “ pseudo - summary ” ( Figure 2 ) . This   is close to abstractive summarization because the   model needs to reconstruct the masked sentences   using the information in the rest of the documents .   The key idea is how to select sentences that   best summarize or represent a set of related in-   put documents ( which we also call a “ cluster ” ) ,   not just a single document as in standard GSG .   Zhang et al . ( 2020 ) use three strategies - Random ,   Lead ( ﬁrst msentences ) , and “ Principle ” . The   “ Principle ” method computes sentence salience   score based on ROUGE score of each sentence ,   s , w.r.t the rest of the document ( D = fsg ) , i.e.   Score ( s ) = R ( s ; D = fsg ) . Intuitively , this   assigns a high score to the sentences that have a   high overlap with the other sentences .   However , we argue that a naive extension of   such strategy to multi - document summarization   would be sub - optimal since multi - document inputs   typically include redundant information , and such   strategy would prefer an exact match between sen-   tences , resulting in a selection of less representative   information .   For instance , Figure 3 shows an example of sen-   tences picked by the Principle strategy ( Zhang et al . ,   2020 ) vs our Entity Pyramid approach . The ﬁgure   shows a cluster containing three news articles dis-   cussing a wildﬁre happened in Corolado , and thepseudo - summary of this cluster should be related   to the location , time and consequence of the wild-   ﬁre , but with the Principle strategy , the non - salient   sentences quoting the words from an ofﬁcer are   assigned the highest score , as the exact same sen-   tence appeared in two out of the three articles . In   comparison , instead of the quoted words , our strat-   egy selects the most representative sentences in the   cluster with high frequency entities .   To address this limitation , we propose a new   masking strategy inspired by the Pyramid Evalua-   tion framework ( Nenkova and Passonneau , 2004 )   which was originally developed for evaluating sum-   maries with multiple human written references .   Our strategy aims to select sentences that best rep-   resent the entire cluster of input documents .   2.2.1 Entity Pyramid Masking   Pyramid Evaluation The Pyramid Evaluation   method ( Nenkova and Passonneau , 2004 ) is based   on the intuition that relevance of a unit of informa-   tion can be determined by the number of references   ( i.e. gold standard ) summaries that include it . The   unit of information is called Summary Content Unit   ( SCU ) ; words or phrases that represent single facts .   These SCUs are ﬁrst identiﬁed by human annota-   tors in each reference summary , and they receive a   score proportional to the number of reference sum-   maries that contain them . A Pyramid Score for a   candidate summary is then the normalized mean   of the scores of the SCUs that it contains . One   advantage of the Pyramid method is that it directly   assesses the content quality .   Entity Pyramid Masking Inspired by how con-   tent saliency is measured in the Pyramid Evalua-   tion , we hypothesize that a similar idea could be   applied in multi - document summarization to iden-   tify salient sentences for masking . Speciﬁcally , for   a cluster with multiple related documents , the more   documents an SCU appears in , the more salient that   information should be to the cluster . Therefore , it   should be considered for inclusion in the pseudo-   summary in our masked sentence generation objec-   tive . However , SCUs in the original Pyramid Eval-   uation are human - annotated , which is not feasible   for large scale pretraining . As a proxy , we explore   leveraging information expressed as named entities ,   since they are key building blocks in extracting in-   formation from text about events / objects and the   relationships between their participants / parts ( Ju-   rafsky and Martin , 2009 ) . Following the Pyramid5247   Algorithm 1 Entity Pyramid Sentence Selection   framework , we use the entity frequency in the clus-   ter as a proxy for saliency . Concretely , as shown in   Fig . 4 , we have the following three steps to select   salient sentences in our masking strategy :   1.Entity Extraction . We extract named entities   using SpaCy ( Honnibal et al . , 2020 ) .   2.Entity Pyramid Estimation . We then build an   Entity Pyramid for estimating the salience of en-   tities based on their document frequency , i.e. the   number of documents each entity appears in .   3.Sentence Selection . Similar to the Pyramid eval-   uation framework , we identify salient sentences   with respect to the cluster of related documents . Al-   gorithm 1 shows the sentence selection procedure .   As we aim to select the entities better representing   the whole cluster instead of a single document , we   ﬁrst remove all entities from the Pyramid that ap-   pear only in one document . Next , we iteratively se-   lect entities from top of the pyramid to bottom ( i.e. ,highest to lowest frequency ) , and then select sen-   tences in the document that include the entity as the   initial candidate set . Finally , within this candidate   set , we ﬁnd the most representative sentences to the   cluster by measuring the content overlap of the sen-   tence w.r.t documents other than the one it appears   in . This ﬁnal step supports the goal of our pre-   training objective , namely to reconstruct sentences   that can be recovered using information from other   documents in the cluster , which encourages the   model to better connect and aggregate information   across multiple documents . Following Zhang et al .   ( 2020 ) we use ROUGE scores ( Lin , 2004 ) as a   proxy for content overlap . For each sentence s ,   we speciﬁcally deﬁne a Cluster ROUGE score as   Score ( s ) = P ( s ; doc )   Where Cis the cluster of related documents .   Note that different from the importance heuristic   deﬁned in P ( Zhang et al . , 2020 ) , Entity   Pyramid strategy favors sentences that are repre-   sentative of more documents in the cluster than   the exact matching between fewer documents ( See   Figure 3 for a qualitative example . ) . The beneﬁt   of our strategy is shown in an ablation study ( § 5 ) .   3 Experiment Goals   We aim to answer the following questions :   •Q1 : How does P perform , compared   with existing pre - trained generation models in zero-   and few - shot settings ? See § 4.2 .   •Q2 : How does P perform , compared   with current state - of - the - art models , in the fully   supervised setting ? See § 4.5 .   •Q3 : How much is the contribution of each compo-   nent in P , i.e. input structure , pretraining ,   and masking strategy ? See § 5 .   •Q4 : What is the effect of our entity pyramid5248   strategy , compared with the strategy used in PEGA-   SUS ? See § 5 .   •Q5 : Is P able to capture salient informa-   tion and generate ﬂuent summaries ? See § 6 .   With these goals , we explore the effectiveness of   P quantitatively on multi - document sum-   marization benchmarks , verify the improvements   by comparing P with multiple existing pre-   trained models and SOTA models , and further vali-   date the contribution of each component with care-   fully controlled ablations . An additional human   evaluation is conducted to show P is able   to capture salient information and generate more   ﬂuent summaries .   4 Experiments   4.1 Experimental Setup   Implementation Details We use the   Longformer - Encoder - Decoder ( LED ) ( Belt-   agy et al . , 2020 ) large as our model initialization ,   The length limits of input and output are 4096 and   1024 , respectively , with sliding window size as   w= 512 for local attention in the input . ( More   implementation details of pretraining process can   be found in Appx § A )   Pretraining corpus For pretraining , our goal is   to use a large resource where each instance is a   set of related documents without any ground - truth   summaries . The Newshead dataset ( Gu et al . , 2020 )   ( row 1 , Table 1 ) is an ideal choice ; it is a relatively   large dataset , where every news event is associated   with multiple news articles .   Evaluation Datasets We evaluate our approach   on wide variety of multi - document summarization   datasets plus one single document dataset from   various domains ( News , Wikipedia , and Scientiﬁc   literature ) . See Table 1 for dataset statistics and   Appx . § B for details of each dataset .   Evaluation metrics Following previous   works ( Zhang et al . , 2020 ) , we use ROUGEscores ( R-1 , -2 , and -L ) , which are the standard   evaluation metrics , to evaluate the downstream   task of multi - document summarization . For better   readability , we use A VG ROUGE scores ( R-1 , -2 ,   and -L ) for evaluation in the few - shot setting .   4.2 Zero- and Few - shot Evaluation   Many existing works in adapting pretrained models   for summarization require large amounts of ﬁne-   tuning data , which is often impractical for new   domains . In contrast , since our pretraining strategy   is mainly designed for multi - document summariza-   tion , we expect that our approach can quickly adapt   to new datasets without the need for signiﬁcant   ﬁne - tuning data . To test this hypothesis , we ﬁrst   provide evaluation results in zero and few - shot set-   tings where the model is provided with no , or only   a few ( 10 and 100 ) training examples . Obtaining   such a small number of examples should be viable   in practice for new datasets .   Comparison To better show the utility of our pre-   trained models , we compare with three state - of - the-   art pretrained generation models : BART ( Lewis   et al . , 2020 ) , PEGASUS ( Zhang et al . , 2020 )   and Longformer - Encoder - Decoder(LED ) ( Beltagy   et al . , 2020 ) . These pretrained models have been   shown to outperform dataset - speciﬁc models in   summarization ( Lewis et al . , 2020 ; Zhang et al . ,   2020 ) , and because of pretraining , they are ex-   pected to also work well in the few - shot settings .   As there is no prior work doing few - shot and zero-   shot evaluations on all the datasets we consider ,   and also the results in the few - shot setting might   be inﬂuenced by sampling variability ( especially   with only 10 examples ) ( Bragg et al . , 2021 ) , we   run the same experiments for the compared mod-   els ﬁve times with different random seeds ( shared   with all the models ) , with the publicly available   checkpoints .   Similar to Pasunuru et al . ( 2021 ) , the inputs of   all the models are the concatenations of the docu-   ments within the clusters ( in the same order ) , each   document is truncated based on the input length   limit divided by the total number of documents so5249   that all documents are represented in the input .   To preserve the same format as the correspond-   ing pretrained models , we set the length limit of   output for BART and P exactly as their   pretrained settings on all of the datasets ( except   for the zero - shot experiments , the details can be   found in Sec.4.3 ) . Regarding length limit of in-   puts , we tune the baselines by experimenting with   512 , 1024 , 4096 on Multi - News dataset in few - shot   setting ( 10 data examples ) , and the model with   length limit 512 ( P ) /1024(BART ) achieves   the best performance , thus we use this setting ( de-   tailed experiment results for different input lengths   can be found in Appx . § C.1 ) . We use the same   length limit as our model for the LED model , i.e.   4096/1024 for input and output respectively , for all   the datasets .   4.3 Zero - Shot Results   For zero - shotabstractive summarization experi-   ments , since the models have not been trained on   the downstream datasets , the lengths of generated   summaries mostly depend on the pretrained set-   tings . Thus to better control the length of gener-   ated summaries and for a fair comparison between   all models , following Zhu et al . ( 2021 ) , we set thelength limit of the output at inference time to the av-   erage length of gold summaries . Exploring other   approaches to controlling length at inference time   ( e.g. , Wu et al . , 2021 ) is an orthogonal direction ,   which we leave for future work .   Table 2 shows the performance comparison   among all the models . Results indicate that our   model achieves substantial improvements com-   pared with all the three baselines on most of the   datasets . As our model is pretrained on clusters of   documents with longer input and output , the beneﬁt   is stronger on the dataset with longer summaries ,   e.g. Multi - News and arXiv . Comparing P   and BART models , as the objective of P   is designed mainly for summarization tasks , not   surprisingly it has relatively better performances   across different datasets . Interestingly , LED un-   derperforms other models , plausibly since part of   the positional embeddings ( 1k to 4k ) are not pre-   trained . Encouragingly , our model performs the   best , demonstrating the beneﬁts of our pretraining   strategy for multi - document summarization .   4.4 Few Shot Evaluation   Compared with the strict zero - shot scenario , few-   shot experiments are closer to the practical scenar-   ios , as it is arguably affordable to label dozens of   examples for almost any application.5250We ﬁne - tune all of the four models on different   subsets with 10 and 100 examples , and the results   are shown in Figure 5 . ( hyperparameter settings   in Appx . § D.1 ) Since R-1 , -2 , and -L show the   same trend , we only present the average of the   three metrics in the ﬁgure for brevity ( full ROUGE   scores can be found in Appx . Table 8) To show the   generality , all the results of few - shot experiments   are the average over 5 runs on different subsets   ( shared by all the models ) .   The result of each run is obtained by the ‘ best ’   model chosen based on the ROUGE scores on a   randomly sampled few - shot validation set with the   same number of examples as the training set , which   is similar with Zhang et al . ( 2020 ) . Note that their   reported best models have been selected based on   the whole validation set which may give P - some advantage . Nevertheless , we argue that   sampling few - shot validation sets as we do here   is closer to real few - shot scenarios ( Bragg et al . ,   2021 ) .   Our model outperforms all baselines on all of   the datasets with 10 and 100 examples demonstrat-   ing the beneﬁts of our pretraining strategy and in-   put structure . Comparing the performances of our   model with the different number of training data   fed in , our model converges faster than other mod-   els with as few as 10 data examples .   4.5 Fully Supervised Evaluation   To show the advantage of our pretrained model   when there is abundant training data , we also train   the model with the full training set ( hyperparame-   ter settings can be found in Appx . § D.2 ) . Table 3   shows the performance comparison with previous   state - of - the - art , along with the results of previous   SOTA . We observe that P achieves state-   of - the - art results on Multi - News , WCEP , and arXiv ,   while slightly underperforming the prior work on   Multi - XScience ( R-1 ) . One possible explanation   is that in Multi - XScience clusters have less over-   lapping information than in the corpus on which   P was pretrained . In particular , the source   documents in this dataset are the abstracts of all the   publications cited in the related work paragraphs ,   which might be less similar to each other and the   target related work(i.e . , their summary ) . P   outperforms the LED model ( State - of - the - art ) on   the arXiv dataset while using a sequence length 4x   shorter ( 4 K in P v.s. 16 K in LED ) , further   showing that the pretraining and input structure of   our model not only works for multi - document sum-   marization , but can be also effective for summariz-   ing single documents having multiple sections .   5 Ablation Study   We conduct ablation studies on the Multi - News   dataset in few - shot setting , to validate the contribu-   tion of each component in our pretrained models .   Input structure : In Figure 6 ( a ) we observe the   effectiveness of both pretraining and the input struc-   ture ( < doc - sep > tokens between documents and   global attention on them ) .   Sentence masking strategy : To isolate the effect   of our proposed pretraining approach , we compare   with a model with exactly the same architecture   when pretrained on the same amount of data but   using the P ( Zhang et al . , 2020 ) masking   strategy instead of ours . In other words , we keep   all the other settings the same ( e.g. , data , length   limit of input and output , pretraining dataset , in-   put structure , as well as the separators ) and only   modify the pretraining masking strategy . We run   the same experiments under zero-/few - shot scenar-5251ios on the Multi - News dataset as in § 4.2 , and the   results are shown in Figure 6 ( b ) . The model pre-   trained with our Entity Pyramid strategy shows a   clear improvement under few - shot scenarios .   6 Human Evaluation   We also conduct human evaluations to validate   the effectiveness of P on DUC2007 and   TAC2008 ( Dang and Owczarzak , 2008 ) datasets   in the few - shot setting ( 10/10/20 examples for   train / valid / test ) . Both datasets consist of clusters of   news articles , and DUC2007 contains longer inputs   ( 25 v.s. 10 documents / cluster ) and summaries ( 250   v.s. 100 words ) . Since the goal of our method is to   enable the model to better aggregate information   across documents , we evaluate the content quality   of the generated summaries following the original   Pyramid human evaluation framework ( Nenkova   and Passonneau , 2004 ) . In addition , we also evalu-   ate the ﬂuency of generated summaries following   the DUC guidelines .   Settings Three annotatorsare hired to do both   Pyramid Evaluation and Fluency evaluation , they   harmonize the standards on one of the examples .   Speciﬁcally , for each data example , we provide   three anonymized system generated summaries ,   along with a list of SCUs . The annotators are asked   to ﬁnd all the covered SCUs for each summary ,   and score the ﬂuency in terms of Grammaticality ,   Referential clarity and Structure & Coherence , ac-   cording to DUC human evaluation guidelines , with   a scale 1 - 5 ( worst to best ) . They are also suggested   to make comparison between three generated sum-   maries into consideration when scoring the ﬂuency .   To control for the ordering effect of the given sum-   maries , we re - order the three summaries for each   data example , and ensure the chance of their ap-   pearance in different order is the same ( e.g. BART   appears as summary A for 7 times , B for 7 times   and C for 6 times for both datasets ) . The instruction   for human annotation can be found in Figure 7 and   Figure 8 in the appendix . Annotators were aware   that annotations will be used solely for computing   aggregate human evaluation metrics and reporting   in the scientiﬁc paper .   Compared Models We compare our model with   LED and PEGASUS in human evaluations . Be-   cause PEGASUS is a task - speciﬁc model for ab-   stractive summarization , and LED has the same   architecture and length limits as our model with the   parameters inherited from BART , which is more   comparable with our model than vanilla BART .   Pyramid Evaluation Both TAC and DUC   datasets include SCU ( Summary Content Unit ) an-   notations and weights identiﬁed by experienced   annotators . We then ask 3 annotators to make a   binary decision whether each SCU is covered in a   candidate summary . Following Nenkova and Pas-   sonneau ( 2004 ) , the raw score of each summary is   then computed by the sum of weights of the cov-   ered SCUs , i.e. S = PwI(SCU ) , where   I(SCU)is an indicator function on whether SCU   is covered by the current summary , and wis the   weight of SCU . In the original pyramid evalua-   tion , the ﬁnal score is computed by the ratio of S   to the maximum possible weights with the same   number of SCUs as in the generated summaries .   However , the total number of SCUs of generated   summaries is not available in the simpliﬁed anno-   tations in our design . To take consideration of the   length of generated summaries and make a fair com-   parison , instead , we compute Recall , Precision and   F-1 score regarding lengths of both gold references   and system generated summaries as   R = S   len(gold ) ; P = S   len(sys ) ; F1=2RP   ( R+P )   Fluency Evaluation Fluency results can be   found in Table 5 , and P has the best perfor-   mance on both datasets in terms of all aspects . Only5252for Grammaticality PRIMERA ’s top performance   is matched by PEGASUS .   7 Related Work   Neural Multi - Document Summarization   These models can be categorized into two classes ,   graph - based models ( Yasunaga et al . , 2017 ; Liao   et al . , 2018 ; Li et al . , 2020 ; Pasunuru et al . , 2021 )   and hierarchical models ( Liu and Lapata , 2019a ;   Fabbri et al . , 2019 ; Jin et al . , 2020 ) . Graph - based   models often require auxiliary information ( e.g. ,   AMR , discourse structure ) to build an input graph ,   making them reliant on auxiliary models and less   general . Hierarchical models are another class   of models for multi - document summarization ,   examples of which include multi - head pooling   and inter - paragraph attention ( Liu and Lapata ,   2019a ) , MMR - based attention ( Fabbri et al . ,   2019 ; Mao et al . , 2020 ) , and attention across   representations of different granularity ( words ,   sentences , and documents ) ( Jin et al . , 2020 ) . Prior   work has also shown the advantages of customized   optimization in multi - document summarization   ( e.g. , RL ; Su et al . , 2021 ) . Such models are often   dataset - speciﬁc and difﬁcult to develop and adapt   to other datasets or tasks .   Pretrained Models for Summarization Pre-   trained language models have been successfully   applied to summarization , e.g. , BERTSUM ( Liu   and Lapata , 2019b ) , BART ( Lewis et al . , 2020 ) ,   T5 ( Raffel et al . , 2020 ) . Instead of regular language   modeling objectives , P ( Zhang et al . , 2020 )   introduced a pretraining objective with a focus on   summarization , using Gap Sentence Generation ,   where the model is tasked to generate summary-   worthy sentences , and Zou et al . ( 2020 ) proposed   different pretraining objectives to reinstate the orig-   inal document , speciﬁcally for summarization task   as well . Contemporaneous work by Rothe et al .   ( 2021 ) argued that task - speciﬁc pretraining does   not always help for summarization , however , their   experiments are limited to single - document sum-   marization datasets . Pretraining on the titles of   HTMLs has been recently shown to be useful for   few - shot short - length single - document summariza-   tion as well ( Aghajanyan et al . , 2021 ) . Goodwin   et al . ( 2020 ) evaluate three state - of - the - art models   ( BART , P , T5 ) on several multi - document   summarization datasets with low - resource settings ,   showing that abstractive multi - document summa-   rization remains challenging . Efﬁcient pretrainedtransformers ( e.g. , Longformer ( Beltagy et al . ,   2020 ) and BigBird ( Zaheer et al . , 2020 ) that can   process long sequences have been also proven suc-   cessful in summarization , typically by the ability to   process long inputs , connecting information across   the entire sequence . CDLM ( Caciularu et al . , 2021 )   is a follow - up work for pretraining the Longformer   model in a cross - document setting using global at-   tention on masked tokens during pretraining . How-   ever , this model only addresses encoder - speciﬁc   tasks and it is not suitable for generation . In this   work , we show how efﬁcient transformers can be   pretrained using a task - inspired pretraining objec-   tive for multi - document summarization . Our pro-   posed method is also related to the PMI - based to-   ken masking Levine et al . ( 2020 ) which improves   over random token masking outside summariza-   tion .   8 Conclusion and Future Work   In this paper , we present P a pre - trained   model for multi - document summarization . Unlike   prior work , P minimizes dataset - speciﬁc   modeling by using a Longformer model pretrained   with a novel entity - based sentence masking ob-   jective . The pretraining objective is designed to   help the model connect and aggregate informa-   tion across input documents . P outper-   forms prior state - of - the - art pre - trained and dataset-   speciﬁc models on 6 summarization datasets from   3 different domains , on zero , few - shot , and full   ﬁne - tuning setting . P ’s top performance is   also revealed by human evaluation .   In zero - shot setting , we can only control the   output length of generated summaries at inference   time by specifying a length limit during decoding .   Exploring a controllable generator in which the de-   sired length can be injected as part of the input is a   natural future direction . Besides the summarization   task , we would like to explore using P for   other generation tasks with multiple documents as   input , like multi - hop question answering .   Ethics Concern   While there is limited risk associated with our work ,   similar to existing state - of - the - art generation mod-   els , there is no guarantee that our model will always   generate factual content . Therefore , caution must   be exercised when the model is deployed in prac-   tical settings . Factuality is an open problem in   existing generation models.5253References52545255   A Implementation details of pre - training   As the multi - document summarization task   has a higher compression ratio , deﬁned as   len(Summary ) = len(Input ) , ( e.g. 12 % for   Multi - News dataset and 15 % for Multi - Xscience   dataset ) , we use 15 % as the ratio of masked sen-   tences for generation . In addition to this 15 %   masked sentences , following P ( Zhang   et al . , 2020 ) , we also copy an additional 15 % of   the input sentences to the output without masking   them in the input . This allows the model to also   learn to copy information from the source directly   and found to be useful by Zhang et al . ( 2020 ) .   We pretrain the model for 100 K steps , with early   stopping , batch size of 16 , Adam optimizer with   a learning rate of 3e 5following Beltagy et al .   ( 2020 ) , with 10 K warmup steps and linear decay .   The pretraining process takes likely 7 days on 4   A100 GPUs .   As the backbone of P is the Longformer   Encoder Decoder model ( LED ) , it has the same   number of parameters with LED ( 447 M ) .   B Detailed Description on the Evaluation   Datasets   The details of evaluation datasets can be found   below .   Multi - News ( Fabbri et al . , 2019 ): A multi-   document dataset with summaries written by pro-   fessional editors from the newser.com .   Wikisum ( Liu * et al . , 2018 ) Each summary is a   Wikipedia article , and the source documents are   either citations in the reference section or the Web   Search results of section titles . In our experi-   ments , we use the data crawled by Liu and Lapata   ( 2019a ) .   WCEP ( Gholipour Ghalandari et al . , 2020 ) is   built based on news events from Wikipedia Current   Events Portal and the references are obtained simi-   lar to Wikisum . There are at most 100 documents   within each cluster in the original dataset , thus weremove all the duplicates and only keep up to 10   documents for each cluster based on the relevance   score in the original dataset , which is similar to the   WCEP-10 variant in the original paper .   Multi - X - Science ( Lu et al . , 2020 ) a multi-   document summarization dataset created from sci-   entiﬁc articles , the summaries are paragraphs of   related work section , while source documents in-   clude the abstracts of the query and referred papers .   DUC benchmarks ( Dang , 2005 ) include multi-   document summarization datasets in the news   domain , with 10 - 30 documents and 3 - 4 human-   written summaries per cluster . Since these datasets   are small , we use them primarily for a few - shot   evaluation . We use DUC2003 for training ( only   one of the reference summaries for each document   is used for training ) and DUC2004 as test .   ArXiv ( Cohan et al . , 2018 ) is a single document   summarization dataset in the scientiﬁc paper do-   main . Each document is a scientiﬁc paper , and the   summary is the corresponding abstract . As each   scientiﬁc paper consists of multiple sections , we   treat each section as a separate document within   a cluster in our experiments . This is to evaluate   our model ’s effectiveness on summarizing single   documents having multiple sections .   C Details on Compared models   The details of compared models in the zero-/few-   shot setting can be found below .   BART ( Lewis et al . , 2020 ) an encoder - decoder   transformer model pretrained on the objective of   reconstructing the corrupted documents in multiple   ways , e.g. Token Deletion , Text Inﬁlling , Sentence   Rotation and etc .   P ( Zhang et al . , 2020 ) a pretrained   model designed for abstractive summarization as   the downstream task , especially for the single doc-   ument input . It is trained on the objective of Gap   Sentence Generation on C4 ( Raffel et al . , 2020 ) and   Hugenews datasets ( Note that the pretraining data   size in P is magnitudes larger than ours ) .   As it is only evaluated on one multi - document   summarization dataset ( Multi - news ) , we rerun the   model on all the datasets . To verify the quality   of our reproduction , the average ROUGE scores   of our re - run model vs. ( the ones reported on the   paper ) with 10 examples and 100 examples fed   are23:810:79vs . ( 24.13 ) and 25:860:41vs .   ( 25.48 ) , with minor differences plausibly resulting   from different samplings.5256Length LimitBART P   R-1 R-2 R - L R-1 R-2 R - L   512 - - - 39.0 12.1 20.3   1024 42.3 13.7 19.7 37.6 10.7 18.8   4096 37.9 11.0 17.5 34.9 8.7 17.6   Longformer Encoder - Decoder ( LED ) ( Beltagy   et al . , 2020 ) is the initial state of our model before   pretraining . The parameters of LED are inherited   from the BART model , and to enable the model   to deal with longer input , the position embeddings   are repeatedly copied from BART ’s 1 K position   embeddings . It is different from our model with re-   spect to both pretraining and input structure ( docu-   ment separators and global attentions ) , with global   attention on the ( < s > ) token only and no document   separators .   C.1 Detailed Experiment for Input Length   Limit   We run an experiment to select the proper length   limit for compared pretrained models , i.e. BART   andP . Speciﬁcally , we train both models   with different input length limits ( 512/1024/4096 )   in the few - shot setting ( with 10 data examples ) on   the multi - news dataset . Similar as the few - shot   experiments described in § 4.2 , we train each model   with each speciﬁc input length limit for 5 times   on different subsets , which are shared by all the   models . As shown in Table 6 , BART with length   limit 1024 performs the best and P with   length limit 512 performs the best , thus in all our   experiments , we use 1024 as the input length limit   for BART and 512 for P .   D Hyperparameters in Few - shot and   Full Supervised Experiments   D.1 Few - shot Experiments   We use Adam as the optimizer with linear sched-   uled learning rate 3e 5for BART , LED and our   model , and use the default optimization settings of   the few - shot experiments from Zhang et al . ( 2020 ) ,   i.e. AdaFactor optimizer with scheduled learningrate5e 4 . For all the experiments with 10 exam-   ples , the batch size is 10 , the models are trained   for 200 steps , with warm - up as 20 steps . For the   experiments with 100 examples , we use the same   batch size , with the total step and warm - up step set   to be 1000 and 100 , respectively .   D.2 Fully Supervised Experiments   We use Adam as the optimizer with linear sched-   uled learning rate 3e 5 , and batch size as 16 for   all the datasets in the full supervised experiments .   The number of steps and warm - up steps are set   based on the size of the datasets . The details can   be found in Table 7   E Detailed Results in Few - shot Setting   The exact ROUGE scores in Figure 5 are shown in   Table 8.5257F Detailed Analysis on Fully Supervised   Experiments   To show the advantage of our pre - trained model   when there is sufﬁcient data , we also train the   model with the full training set , and the results   can be found in Table 9 - 12 , along with the re-   sults from previous works . Differently from the   zero-/few - shot experiments , here we report the   state - of - the - art results on different datasets , as they   were presented in the corresponding original pa-   pers . Since we use the same train / valid / test set as   in those prior works , we can perform a fair com-   parison , without re - running all those extremely   time - consuming experiments .   Overall , our model achieves state - of - the - art on   Multi - News ( see Table 9 , WCEP dataset ( see Ta-   ble 11 ) and arXiv dataset ( see Table 12 ) .   Multi - News The experiment results on Multi-   News dataset can be found in Table 9 . Speciﬁcally ,   theP model ( Zhang et al . , 2020 ) is pre-   trained on a large - scale single - document dataset   with the Gap Sentence Generation objective , which   is the same as ours , but with a different mask-   ing strategy , BART - Long ( Pasunuru et al . , 2021 )   uses the same model structure as ours , and BART-   Long - Graph ( Pasunuru et al . , 2021 ) additionally   has discourse graph injected . Comparing the re-   sults with the BART - Long model , our model is   around 1 ROUGE point higher , which may result   from either better model structure or pre - training .   Interestingly , in one of the ablation studies in Pa-   sunuru et al . ( 2021 ) , they ﬁnd that the BART - Long   model achieves its best performance with the length   limit of 1000 , and no further improvement is found   when the length limit is greater than that . Thus we   may conclude the gap between the performances is   mainly from our design on the model , i.e. the doc-   ument separators , proper global attention as well   as the pre - training on a multi - document dataset .   WCEP As for the WCEP dataset , BR   ( Gholipour Ghalandari et al . , 2020 ) is a Regression-   based sentence ranking system with BERT em-   bedding , which is used as extractive summariza-   tion method , while Submodular+Abs is a simple   two - step abstractive summarization model with a   submodular - based extractive summarizer followed   by a bottom - up abstractive summarizer ( Gehrmann   et al . , 2018 ) . DynE is a BART - based abstractive   approach , which is to ensemble multiple input , al-   lowing single document summarization models to   be directly leveraged on the multi - document sum-   marization task . Our model outperforms all the   models by a large margin , including the SOTA   model DynE , and it may indicate that the plain   structure is more effective than purely ensembling   the output of single documents .   arXiv In addition to the experiments on multi-   document summarization datasets , we also com-   pare our fully supervised model with previous   works on the arXiv dataset , with each section   treated as a single document . All the models to   be compared with are based on pre - trained mod-   els , and Bigbird- P and LED utilize the   pre - training of P ( Zaheer et al . , 2020 ) and   BART ( Lewis et al . , 2020 ) , respectively . However ,   both Bigbird and LED apply more efﬁcient atten-   tions , which make the models able to take longer5258   input ( 3k for BigBird , 4 K and 16k for LED ) . Our   model has a better performance than all the models ,   including LED(16 K ) , which allows for the input   4 times longer than ours . It is worth mentioning   that LED(4 K ) has the same structure as our model ,   with the same length limit of the input , and with   the pre - training on multi - document datasets , our   model is more than 3 ROUGE point better than it ,   which shows that the strategy not only works for   multi - document summarization but can also effec-   tively improve single - document summarization for   long documents .   G Examples of Generated Summaries   We show an example ( from Multi - News ) of gener-   ated summaries by P and compared mod-   els trained with different number of examples in   Table 13 . And we show an example from DUC2007   ( which is one of the examples used for human eval-   uation ) with generated summaries by P   and two compared models in Table 14 , with all   the models trained on 10 data examples from   DUC2007 .   H Software and Licenses   Our code is licensed under Apache License 2.0 .   Our framework dependencies are :   • HuggingFace Datasets , Apache 2.0   • NLTK , Apache 2.0   •Numpy , BSD 3 - Clause " New " or " Revised "   • Spacy , MIT• Transformers , Apache 2.0   • Pytorch , Misc   • Pytorch Lightning , Apache 2.0   • Longformer , Apache 2.0   • ROUGE , Apache 2.0   I Annotation Instructions for Human   Evaluation   Figure 7 and Figure 8 shows the annotation instruc-   tion for human annotators.52595260526152625263