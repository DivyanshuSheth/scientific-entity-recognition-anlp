  Najoung Kim   Department of Linguistics   Boston University   najoung@bu.eduSebastian Schuster   Dept . of Language Science and Technology   Saarland University   seschust@lst.uni-saarland.de   Abstract   Keeping track of how states of entities change   as a text or dialog unfolds is a key prereq-   uisite to discourse understanding . Yet , there   have been few systematic investigations into   the ability of large language models ( LLMs )   to track discourse entities . In this work , we   present a task probing to what extent a lan-   guage model can infer the ﬁnal state of an   entity given an English description of the ini-   tial state and a series of state - changing op-   erations . We use this task to ﬁrst investi-   gate whether Flan - T5 , GPT-3 and GPT-3.5 can   track the state of entities , and ﬁnd that only   GPT-3.5 models , which have been pretrained   on large amounts of code , exhibit this abil-   ity . We then investigate whether smaller mod-   els pretrained primarily on text can learn to   track entities , through ﬁnetuning T5 on sev-   eral training / evaluation splits . While perfor-   mance degrades for more complex splits , we   ﬁnd that even when evaluated on a different set   of entities from training or longer operation se-   quences , a ﬁnetuned model can perform non-   trivial entity tracking . Taken together , these   results suggest that language models can learn   to track entities but pretraining on text corpora   alone does not make this capacity surface .   1 Introduction   A key prerequisite to long - context understanding   and generating coherent text is the ability to accu-   rately represent entities as the discourse unfolds   ( Karttunen , 1976 ; Groenendijk and Stokhof , 1991 ;   Heim , 2002 ; Nieuwland and Van Berkum , 2006 ;   Kamp et al . , 2011 , i.a . ) . For example , consider the   following example in the context of a recipe :   ( 1 ) Put the eggs , sugar , ﬂour , and baking pow-   der in a bowl and mix to form a light batter .   Make sure that the ﬁnal batter does not con-   tain any lumps of ﬂour or sugar . Figure 1 : A sketch of our entity tracking task .   In order to understand this instruction , several dis-   tinct abilities are necessary :   New discourse entity recognition : recognizing   when new discourse entities are introduced . E.g. , a   bowl introduces a new discourse entity but the ﬁnal   batter orany lumps of ... does not .   Coreference resolution : associating referring ex-   pressions with discourse entities . E.g. , a light bat-   terandthe ﬁnal batter refer to the same entity .   Discourse entity tracking : tracking the state   changes made to each discourse entity . E.g. , the   eggs are put into the bowl andmixed with the other   ingredients .   There exist many datasets that aim to evaluate   these abilities ( e.g. , Walker et al . , 2006 ; Pradhan   et al . , 2012 ; Rahman and Ng , 2012 ; Weston et al . ,   2015 ; Chen et al . , 2018 ; Bamman et al . , 2020 ;   Uryupina et al . , 2020 ) and many NLP models that   aim to solve these tasks ( e.g. , Haghighi and Klein ,   2010 ; Lee et al . , 2011 ; Hill et al . , 2016 ; Henaff   et al . , 2017 ; Ji et al . , 2017 ; Lee et al . , 2017 ; Bosselut   et al . , 2018 ; Gupta and Durrett , 2019a , b ; Aina et al . ,   2019 ; Toshniwal et al . , 2020 ; Wu et al . , 2020 ) . In   the context of large language models ( LLMs ) , Ten-   ney et al . ( 2019 ) , Clark et al . ( 2019 ) , and Sorodoc   et al . ( 2020 ) found that representations of LSTMs   and Transformer - based models such as BERT ( De-   vlin et al . , 2019 ) do capture coreference relations .   Loáiciga et al . ( 2022 ) and Schuster and Linzen   ( 2022 ) found that pretrained models are able to3835detect whether noun phrases introduce discourse   entities , albeit not fully systematically .   The question of whether LLMs can track the   state of discourse entities , however , has mostly   been indirectly evaluated . Toshniwal et al . ( 2022 )   showed that GPT-2 ( Radford et al . , 2019 ) can learn   to predict valid chess moves based on a compact ,   nonlinguistic description of previous moves . Sim-   ilarly , Li et al . ( 2023 ) showed that a GPT model   trained on Othello can predict valid next moves ,   and that these predictions are tied to the model ’s   internal representations of the board states . Still ,   these results do not tell us whether LLMs track   state changes expressed in natural language dis-   courses . The most relevant evaluation is Li et al .   ( 2021 ) , where they tested whether model represen-   tations encode entity states described in naturalistic   text . Using a probing classiﬁer , they found that the   states can be decoded from T5 ( Raffel et al . , 2020 )   and BART ( Lewis et al . , 2020 ) with high accuracy .   However , as we show in a reanalysis of their results   ( Section 2 ) , they do not provide deﬁnitive evidence   for entity tracking . Hence , whether LLMs can track   entities during the processing of natural language   discourse remains an open question .   Contributions This work attempts to answer this   question by developing a task targeted towards eval-   uating a language model ’s ability to track state   changes of discourse entities ( illustrated in Fig-   ure 1 ) . We use this novel task to evaluate GPT-3   ( Brown et al . , 2020 ) , GPT-3.5 , and Flan - T5 ( Chung   et al . , 2022 ) without any ﬁnetuning . We ﬁnd that   only models in the GPT 3.5 series , which have been   trained on both text and code , are able to perform   non - trivial entity tracking . We then show that a   smaller language model ( T5 ) can learn to perform   non - trivial entity tracking and also demonstrates   some capacity to generalize to state descriptions   with more operations or with low lexical overlap .   Our results suggest that language models can learn   to track entities but pretraining on text corpora   alone does not make this capacity surface . More   broadly , our task can also serve as a useful tool for   investigations into emergent world models in LMs   ( e.g. , Li et al . , 2023 ; Tsai et al . , 2023 ) .   2 Reanalysis of Li et al . ( 2021 )   We start from examining Li et al . ( 2021 ) , the most   relevant work to ours . They adapted two exist - ing datasets , Alchemy ( Long et al . , 2016 ) and   TextWorld ( Côté et al . , 2019 ) , to test a model ’s   ability to track state changes of an entity . The in-   put to the model is a text description of the initial   world state followed by state - changing instructions .   Based on this description , the model is expected to   identify the correct ﬁnal state of each entity . For   example , for Alchemy , the model receives formu-   laic descriptions of 7 beakers containing different   amounts of colored liquids , followed by instruc-   tions that manipulate their contents such as pouring   the liquid from one beaker into another , or draining   a beaker . Given an input like ( 2 ) , the model is ex-   pected to recognize that the ﬁrst beaker has 4 units   of brown liquid , the second beaker has 2 units of   red liquid , and the third beaker is empty .   ( 2 ) The ﬁrst beaker has 1 green , the second   beaker has 2 red , the third beaker has 3 red .   Pour the last red beaker into beaker 1 . Mix .   Using such descriptions , Li et al . ( 2021 ) found that   a probing classiﬁer that takes as input the encoding   of these descriptions from T5 or BART is able to   correctly predict the state of 75–76 % of the enti-   ties , suggesting some degree of success on entity   tracking .   However , this conclusion becomes questionable   when the datasets and the results are scrutinized   further . Speciﬁcally , we conducted a ﬁne - grained   analysis of the success cases of the Alchemy ex-   periment . In this experiment , the state of each   beaker was probed after each state - changing in-   struction . Because each instruction targets at most   two beakers ( e.g. , pour X into Y ) and there are 7   beakers in total , there is a sparse representation of   cases probing a beaker that actually underwent a   change in the dataset . Indeed , 62.7 % of all beaker   states probed were identical to the initial state ,   meaning that a simple baseline that always predicts   the initial state already achieves 62.7 % accuracy   ( this is also noted by Li et al . ) . A second potential   for shortcuts was the high rate of empty ﬁnal states   ( 32.4%).For these cases , the initial state can of-   ten be entirely disregarded , due to the presence of   an emptying instruction such as Drain the fourth   beaker . This instruction alone is sufﬁcient to pre-   dict the fourth beaker ’s ﬁnal state independent of   its initial state . Therefore , such examples are also3836not best suited to fully assess entity tracking . Given   the high prevalence of these two trivial scenarios   ( 87.6 % in total ) , only 12.4 % of the datapoints can   be considered as truly assessing state changes un-   folding over a discourse context . If the accuracy   is computed on the trivial and non - trivial cases   separately , the probing classiﬁer achieves 86.8 %   accuracy on trivial cases but only 3.1 % accuracy on   non - trivial cases , showing that most of the reported   success derives from the trivial cases .   In summary , our reanalysis suggests that the re-   sults of Li et al . ( 2021 ) do not provide conclusive   evidence for non - trivial state tracking abilities in   language models . However , it remains unclear   whether this is due to issues with the setup or a true   lack of entity tracking capacity . To this end , we   propose a new behavioral evaluation .   3 Task Design and Dataset   3.1 Desiderata   The ability to track entities should be largely inde-   pendent of speciﬁc linguistic forms . For a model   that can properly track entities , it should not matter   whether one talks about beakers or recipes or which   speciﬁc syntactic constructions are used . This   makes it an interesting ability to evaluate in the con-   text of assessing whether and how meaning is rep-   resented , since at least classic language models are   only trained on forms ( Bender and Koller , 2020 ) .   At the same time , this independence of form and   entity states that should hold for true entity tracking   poses a challenge for evaluation , since one needs to   ensure that the state of entities can not be predicted   from individual lexical items or phrases ( such as   the word drain in the Alchemy dataset , as discussed   in Section 2 ) . Furthermore , language models pre-   trained on large corpora may have learned common   states of entities ; for instance , that eggs often end   up in a bowl . For these reasons , any task that evalu-   ates entity tracking abilities should conform to the   following four desiderata :   1.The probed states of entities should not follow   similar distributional patterns to those that are   likely to be present in the pretraining data ( see   also Linzen , 2020 ) .   2.Individual words or phrases should not predict   by themselves the state of an entity withoutconsidering the previous discourse in order .   3.If any data is used for demonstration , ﬁne-   tuning or training , the training and evaluation   data should have little lexical overlap .   4.If any data is used for demonstration , ﬁnetun-   ing or training , the task should not be solvable   by slot-ﬁlling based on observed datapoints .   These properties can not be guaranteed with natural-   istic datasets such as recipes ( Kiddon et al . , 2015 ) ,   science texts ( Dalvi et al . , 2019 ) , or the Alchemy   and TextWorld datasets , which have been previ-   ously used to evaluate entity tracking abilities . We   therefore programmatically generated datasets for   which these properties hold .   3.2 Dataset   We take inspiration from Winograd ( 1971 ) and Li   et al . ( 2021 ) in designing our data . Our datasets   consist of text descriptions of a particular state of   the world followed by a sequence of changes . The   worlds contain boxes that can be ﬁlled with objects .   The objects can be placed inside the box , taken out   of the box , or moved from one box to another . We   deﬁne a worldWasW= ( O , n , m , e ) where Ois   a set of objects , nis the number of boxes , mis the   maximum number of objects one box can contain ,   andeis the expected number of objects in each   box in the initial world states . For our datasets , we   usedn= 7,m= 3,e= 2 , and used a set of nouns   denoting items that can plausibly ﬁt inside a box   ( e.g. , book , rock , brain ; |O|= 100 ) , selected from   a list of words with frequency greater than 27 in the   British National Corpus ( BNC ; Leech et al . 2001 ) .   A dataset consists of multiple distinct scenarios .   A scenario consists of an initial state and a set of   operations applied to this initial state . We ﬁxed   the number of operations ( NumOps ) in each sce-   nario to 12 . We randomly sampled 2200 scenarios ,   where the initial state and the 12 operations were   both randomly sampled . The sampling process is   designed such that only valid operations given the   current world state can be sampled . The initial state   and the operations were converted into naturalistic   descriptions using predeﬁned templates .   Relation to Desiderata We selected the task of   moving objects across boxes because this is a do-   main where lexical contents of the entities do not   offer cues to predict the outcome of state changes   ( Desideratum 1 ) . We did not include an opera-   tion that empties a box that allows the previous3837discourse to be discarded ( Desideratum 2 ) . For   Desideratum 2 , we furthermore considered exper-   iments using operation descriptions with greater   context dependence . Speciﬁcally , we tested scenar-   ios with an additional Move contents of Box N to   Box M operation that does not explicitly mention   the object names , and scenarios where object de-   scriptions in the operations can only be fully disam-   biguated by knowing the current state of a box . For   Desideratum 3 , we considered experiments where   the phrasing of the states and operations differ en-   tirely between demonstration/ﬁnetuning and evalu-   ation ( see Table 2 ) . Finally , for all experiments , we   computed a “ signature ” of every initial state that   indicates the number of objects contained in each   box . Then , we ensured that there were no two   examples with identical initial descriptions mod-   ulo the object names where one appeared in the   training split and the other one in the evaluation   split . This prevents this task from being solvable   via slot-ﬁlling ( Desideratum 4 ) .   3.3 Task   We deﬁne the entity tracking task as follows . Given   a natural language description of the initial state   of the world followed by 0–12 state - changing op-   erations , the content of each box at the end of   the description must be correctly identiﬁed . To   evaluate this , we created a test example for each   box after each operation . This corresponds to   n×(NumOps + 1 ) examples per scenario ( 91 exs .   in our datasets ) . Each example is formulated in the   style of a cloze test . That is , the input describes the   initial state followed by a sequence of operations ,   ending in Box N contains _ _ . The expected output   is the correct set of objects in Box Nbased on the   preﬁx description . See Appendix B for an example .   4 Experiment 1 : In - context   Demonstration   In the ﬁrst set of experiments , we evaluated pre-   trained LMs using a small number of in - contextdemonstrations of the entity tracking task . This   provides a way to probe the model without requir-   ing substantial supervision for the task , as well as   guiding the model to output the ﬁnal state in a con-   sistent format that can be automatically assessed .   4.1 Models   We used models that are known to support task   adaptation via in - context demonstrations : GPT-   3 175B ( davinci : Brown et al . 2020 ) , GPT-3.5   ( text - davinci-003 ) , and Flan - T5 ( base and XL :   Chung et al . 2022 ) . The little information that   OpenAI revealed about their modelssuggests that   davinci is an autoregressive language model pri-   marily trained on text corpora . text - davinci-003   was trained on the language modeling objective   on a mix of text and code , and additionally tuned   with human feedback using reinforcement learning .   Flan - T5 is based on T5 , a sequence - to - sequence   model trained on a denoising objective , that has   been further instruction-ﬁnetuned on a battery of   tasks . This has been shown to promote better   responses to instructions , both with and without   demonstrations ( Chung et al . , 2022 ) . We evaluated   the GPT models through the OpenAI API and the   Flan - T5 using the HuggingFace library ( Wolf et al . ,   2020 ) . See Table 1 for a summary of the models ,   and Appendix C for implementation details .   We compared these models against a baseline   that randomly outputs 0 to m= 3objects from the   set of objects that appeared in the same clauses as   the box in question . Note that this baseline is much   stronger than a fully random baseline that selects   outputs from all mentioned objects .   4.2 Prompting and Demonstrations   Our prompts consist of : ( a ) a general instruction   for the task , ( b ) two examples of the task to demon-   strate the expected format , ( c ) an initial state de-   scription followed by a series of operations , and ( d )   an incomplete sentence Box N contains _ _ _ that the   model should complete ( see Appendix E for full   prompts ) . We used demonstrations that output the   state of all boxes at once . However , in early experi-   ments , Flan - T5 frequently only output the state of   the ﬁrst box even when the in - context demonstra-   tions contained ﬁnal descriptions of all box states.3838   Therefore , for Flan - T5 , we adjusted the prompts to   output each box individually .   Demonstration / Test Mismatch for Form - mean-   ing Disentanglement ( AltForms ) As discussed   in Sections 3.1–3.2 , we additionally experimented   with a setup where the demonstration and test ex-   amples were mismatched in the form of the descrip-   tions of the states and operations . Under this setup ,   the models were evaluated with set of object names   and phrasings of the state and operation descrip-   tions that were different from the demonstration   examples ( see Table 2 ) . Except for the determiner   theand the preposition into , the two sets share no   words ( although subwords may be shared depend-   ing on tokenization ) .   Scenarios with Greater Context Dependence   ( MoveContents , AmbiRef ) As also discussed in   Sections 3.1–3.2 , we experimented with two addi-   tional scenarios with greater context dependence .   The ﬁrst scenario ( MoveContents ) introduces an   additional operation Move contents of Box N to   Box M that moves all objects in Box N to Box M ,   that does not provide an explicit enumeration of   objects being moved . This requires the model to   rely on the preceding description to identify the set   of objects being moved , further removing room for   heuristics that allow the prediction of the ﬁnal state   without composing the initial description and the   operations in temporal order . The second scenario   ( AmbiRef ) adds adjectival modiﬁcation to object   names ( e.g. , the big brain andthe small brain ) ,   where the adjective can be omitted in some of the   operations , depending on the state of the box being   described . Speciﬁcally , the modiﬁer is dropped if   there is only one object of a speciﬁc type in a box   ( e.g. , Move the brain from Box 1 to Box 2 if there   is only one brain in Box 1 ) . When predicting the   contents of a box , the model is instructed to output   the object type with the correct adjective to fully   disambiguate the referring expression for the ob-   ject . This again requires composition of the initial   description and the operations in temporal order to   correctly interpret the otherwise ambiguous object   mentions . ( See Appendix B for example scenarios   and operations . )   4.3 Evaluation   We estimated the entity tracking capacity of the   models by computing the accuracy of predicting the   contents of each box after each operation . Given   that we rely on arbitrary - length cloze completion to   predict the contents , we had to score unconstrained   generations . While we only considered instances as   correct where the output mentions all objects ( and   no additional objects ) in a given box , our evaluation   allowed for minor deviations from the exact form   of the response . Namely , we allowed the objects   to appear in any order , the object names may be   separated by commas or and , and both complete   noun phrases with a determiner ( e.g. , the furby ) and   bare nouns ( e.g. , furby ) are considered correct .   The task becomes intrinsically harder as more   operations are applied to a speciﬁc box , since the   initial state description needs to be combined se-   quentially with all subsequent operations . Further ,   similarly to our observation in Section 2 , every   operation changes the state of at most two boxes.3839   This implies that the number of datapoints corre-   sponding to fewer operations is much greater than   the number of datapoints for more operations . For   these reasons , we report accuracy as a function of   the number of operations affecting a particular box   rather than reporting aggregate accuracy , and show   all results with 95 % conﬁdence intervals .   4.4 Results   Figure 2 shows the prediction accuracy for differ-   ent number of operations that affected a box ( e.g. , 3   indicates that three operations changed the content   of the box after the initial state ) . The left panel   shows the instances where the probed state differed   from the initial state ; the right panel shows the in-   stances where the probed state was the same as the   initial state . As the left panel shows , only GPT-3.5   text - davinci-003 consistently outperformed the   ( strong ) random baseline . While , not surprisingly ,   the accuracy of this model also decreases as the   number of operations increases , it still correctly   predicted all contents of a box after 7 operations in   more than 25 % of the cases . The Flan - T5 models ,   on the other hand , seemed to ignore the operations   and primarily predicted the initial state description ,   as indicated by the consistently high accuracy when   the ﬁnal state matches the initial state ( right panel ) ,   as well as the consistently low accuracy when the   ﬁnal state deviates from the initial state ( left panel ) .   GPT-3 davinci also primarily repeated the initial   state , but as indicated by the steep decrease in the   right panel , it was distracted by intervening opera-   tions even when repeating the initial state .   Form - meaning Disentanglement We addition-   ally evaluated text - davinci-003 , the only model   that exhibited a non - trivial entity tracking capacity   in the ﬁrst set of results , under the AltForms setup   as described in Section 4.2 where the demonstra-   tion examples have low lexical overlap with the test   examples . Figure 3 shows the prediction accuracy   oftext - davinci-003 on a representative subsam-   pleof our data . The blue line represents the per-   formance when the descriptions in the demonstra - tion and the test examples have low lexical overlap .   As the comparison to the original results ( red line )   shows , the disjoint demonstrations did lead to a   small drop in performance when there were more   than two operations affecting a box . Nevertheless ,   text - davinci-003 was able to predict the correct   state of entities in many cases , further adding sup-   port for its non - trivial entity tracking capacity .   Greater Context Dependence Finally , we eval-   uated text - davinci-003 on two additional   datasets described in Section 4.2 : the AmbiRef   dataset where adjectival modiﬁcation can be omit-   ted depending on the current context , and the Move-   Contents dataset where a new operation Move con-   tents of Box N to M that requires the contents of   Box N to be contextually identiﬁed . Figure 4 shows   the performance of text - davinci-003 on these   two datasets compared to our random baseline . As   these plots show , even in these more challenging3840cases , we observe non - trivial entity tracking abili-   ties . However , as the number of operations grows ,   performance rapidly approaches the random base-   line , suggesting that entity tracking becomes in-   creasingly more brittle as more state changes need   to be considered jointly .   4.5 Discussion   Our results show that among the models we eval-   uated , only GPT-3.5 text - davinci-003 exhibit   non - trivial entity tracking behavior . While its per-   formance does decrease as the number of opera-   tions increases , the model still produced many accu-   rate predictions even after six or seven sequences of   operations . Furthermore , through the AltForms ex-   periment with low lexical overlap between demon-   stration / test , we also ruled out the possibility that   the demonstrations are teaching the model this task   or that the model is primarily relying on superﬁcial   slot-ﬁlling heuristics . Therefore , we conclude that   text - davinci-003 does have some capacity to   track discourse entities in linguistically expressed   contexts . To a lesser extent , we also observed this   capacity in the highly context - dependent AmbiRef   and MoveContents experiments . This provides fur-   ther evidence against superﬁcial heuristics in the   performance we observe ; at the same time , this   highlights scenarios in which even the most recent   models exhibit difﬁculties .   On the other hand , entity tracking behavior did   not surface in GPT-3 davinci ( likely of similar   size as GPT-3.5 text - davinci-003 ) , a model pre-   trained primarily on text corpora on the next word   prediction objective . This was also true for de-   noising models that have been ﬁnetuned on many   tasks combined with instructions and demonstra-   tions : the Flan - T5 models also showed near - zero   accuracy on non - trivial examples .   Our results overall show that there exists a lan-   guage model that can perform entity tracking to   some degree , but this capacity does not necessarily   surface in all sufﬁciently large models trained on   large corpora . Then , which factors are responsi-   ble for this difference ? Given that davinci and   text - davinci-003 differ along at least two di-   mensions ( text - davinci-003 is based on a model   that was trained on code and it was trained with ad-   ditional human feedback ( Ouyang et al . , 2022 ) ; see   Table 1 ) , our initial results do not shed light on what   exactly contributes to this difference . We therefore   conducted a follow - up experiment where we com-   pared a range of GPT-3 and GPT-3.5 models to   identify factors that contribute to the stark differ-   ence between davinci andtext - davinci-003 .   Training on Code Encourages Entity Tracking   Behavior As Table 1 shows , two key dimensions   of variation across models are additional training   on human feedback and pretraining on code . If   additional training on human feedback imbues lan-   guage models with the ability to track entities , all   models except for GPT-3 davinci and GPT-3.5   code - davinci-002 should be able to track enti-   ties . If , on the other hand , pretraining on code   leads to better entity tracking , we expect all GPT-   3.5 models to outperform GPT-3 on our task . As   Figure 5 shows , GPT-3.5 models that have been   trained on code systematically outperformed GPT-   3 models , including code - davinci-002 that was   not trained on human feedback . This suggests that   a substantial representation of code in the pretrain-   ing data is beneﬁcial for a language model ’s entity   tracking capacity to surface .   A further question that our results so far do not   answer is to what extent model size matters and   whether models at the scale of Flan - T5 can also   exhibit non - trivial entity tracking behavior . Since   there exist no smaller models that have been trained   with the same objective and training data as the   GPT-3.5 models , we explore this question through   ﬁnetuning experiments with T5 .   5 Experiment 2 : Finetuning   We investigated whether smaller models at the scale   of T5 can learn to track entity states through a   series of experiments where we provide supervised3841   training to the models .   5.1 Train / test splits   As discussed in Section 3.1 , one challenge of eval-   uating entity tracking abilities is distinguishing this   capacity from simple heuristics such as templatic   slot-ﬁlling . We therefore designed various types of   training / evaluation mismatches that block several   possible shortcuts as described below .   Base Split Here , we used the same format for train-   ing and evaluation examples . All initial states dif-   fered across training and evaluation to block simple   slot-ﬁlling heuristics , as discussed in Section 3.2 .   NumOps Split The NumOps split restricts the   maximum number of operations within a single   example in the training set to 2 , but includes up   to 12 operations in the evaluation set . This split   was intended to test whether a ﬁnetuned model is   able to generalize to longer sequences of operations   than it has seen during ﬁnetuning .   Vocab / AltForms Splits The vocab split tests   whether objects that are not part of the set of ob-   jects used during training can also be adequately   tracked . We compiled a list of comparatively in-   frequent object names ( e.g. , pomelo , furby , Flav - R-   Straw ; not in BNC ) and sampled the training and   test sets using two completely disjoint sets of object   names . The training set used the infrequent object   list and the test set used the original object list . The   AltForm split follows the design described in Sec-   tion 4.2 . These splits aim to tease apart whether the   model learns to associate speciﬁc words / phrases   with the operations or whether ﬁnetuning leads to   more generalizable entity tracking behavior .   AmbiRef / MoveContents Splits The AmbiRef   and MoveContents splits follow the design de-   scribed in Section 4.2 . These splits aim to test   whether the model can learn to interpret operations   that are underspeciﬁed without considering the cur-   rent state of the affected boxes . For these splits , thetraining and test examples share the same format .   5.2 Models   We evaluated T5 - base , the best - performing model   in Li et al . ( 2021 ) , by ﬁnetuning it on each of the   datasets described above . As an additional baseline ,   we compared against T5 with randomly initialized   parameters trained directly on our datasets .   5.3 Results and Discussion   Pretrained T5 can Learn to Perform Entity   Tracking As shown in Figure 6 ( left ) , ﬁnetuning   T5 leads to near - perfect accuracy on the base split .   This suggests that the model is capable of learning   this task . Training a randomly initialized T5 did   not yield the same result : the accuracy of a model   trained from random weights is considerably lower ,   due to the model almost exclusively predicting that   a box is empty . These two results suggest that pre-   training is crucial for the model to be able to learn   this task . Furthermore , the model ’s entity tracking   capacity is robust to novel object names at test time ,   with only minor degradation on accuracy ( Figure 6 ,   middle ) . Training only on operation sequences   with a maximum length of 2 ( NumOps split ) leads   to a larger degradation in performance for longer   operation sequences , but even for longer operation   sequences , the model is able to infer the correct   ﬁnal state in more than 45 % of the cases . Finally ,   the model performance does degrade substantially   when the training examples have low lexical over-   lap with test examples ( Figure 6 , right ) . Neverthe-   less , model performance remains above the random   baseline when the model is trained on up to 12 op-   erations ( pink line ) . If we trained only on up to two   operations ( blue line ) , however , the performance   degradation was compounded and performance no   longer exceeded the random baseline . These results   suggest that ﬁnetuning on an entity tracking task   does lead to entity tracking abilities that generalize3842   to many challenging scenarios . At the same time ,   however , performance rapidly degrades as the ﬁne-   tuning and evaluation splits become increasingly   dissimilar , and it remains an open question to what   extent ﬁnetuning on a limited domain such as the   boxes environment transfers to more general entity   tracking abilities in more naturalistic discourse .   Interpreting Context - Dependent Operations   Remains Challenging If we include operations   that either contain ambiguous referring expressions   ( Figure 7 , left ) or a Move contents operation ( Fig-   ure 7 , right ) in the ﬁnetuning and evaluation data ,   we see a slight degradation in performance and the   model no longer achieves near - perfect accuracy ,   despite training and evaluating on examples of the   same format . In both cases , the model almost ex-   clusively made mistakes with examples that require   the interpretation of context - dependent operations .   This suggests that the context - dependent operations   do compound the difﬁculty of entity tracking even   when the models receive direct training signal .   6 Conclusion   We set out to investigate whether pretrained LLMs   exhibit entity tracking behavior . We developed a   task that allowed us to evaluate whether LLMs can   predict the state of an entity based on an initial   state description and operations that act upon it . In   the ﬁrst set of experiments , we found that GPT-3   davinci , a vanilla pretrained language model , and   Flan - T5 , an instruction-ﬁnetuned language model ,   fail at this task and simply repeat the initial state   description . The GPT-3.5 models ( the core differ-   ence with the aforementioned models being code   in pretraining corpora ) , on the other hand , exhib-   ited non - trivial entity tracking behavior . Even after   many operations affecting an entity state , they con-   sistently performed above a strong random baseline .   In the second set of experiments , we showed that   this behavior can also to a large extent be learnedby smaller models such as T5 . When we ﬁnetune   the model on this task , it is also able to perform en-   tity tracking on examples that differ along several   dimensions from the training data . Taken together ,   our results provide evidence that ( a ) vanilla lan-   guage models that have mainly been trained on text   data do not exhibit entity tracking abilities out of   the box , ( b ) pretraining on code and text data con-   siderably improves this ability , and ( c ) ﬁnetuning   on this task can make this behavior also surface in   smaller models that have primarily been trained on   text data , although it remains an open question how   general this ability is in smaller ﬁnetuned models .   What are reasons behind the efﬁcacy of training   both on text and code ? For producing correct code ,   tracking the states of variables is important . There-   fore , to speculate , this kind of pretraining data may   provide a stronger signal for the model to track   entities compared to pure text data . It could also   be that , as previously speculated ( Potts , 2020 ; Mer-   rill et al . , 2021 , i.a . ) , pretraining on code provides   additional grounding .   The present results also highlight the importance   of the composition of the pretraining data . Our   results showed that models that are trained on the   language modeling objective on large corpora show   dramatically different behavior on entity tracking   depending on whether the training data includes   substantial amount of code or not . In this light , fu-   ture work could investigate the effect of pretraining   on code more widely , including its effect on the   model representations and to what extent it facili-   tates the emergence of world models in LMs .   Finally , we also laid out several principles that   should be followed when evaluating state tracking   abilities . Apart from these speciﬁc principles , we   make a more general point that in assessing abilities   related to meaning , one needs to consider potential   strategies that the model could use to solve the   task and make sure that the test examples do not   mimic the distributional patterns of the training or   ﬁnetuning data . Only then can we properly assess   meaning - related capacities of LMs .   Limitations   One limitation of this work is that we are only con-   sidering behavioral data which makes it difﬁcult to3843establish a fully causal link between entity tracking   capacities and high performance on our task . En-   tity tracking is a high - level linguistic behavior and   many other capacities are necessary for achieving   high accuracy on our task . Therefore , we can not   rule out that differences in some other capacity ,   such as interpreting sentences compositionally ( see   Bogin 2022 and Bogin et al . 2022 for evidence   that GPT-3 and GPT-3.5 models differ in their com-   positional generalization behavior ) , are the main   driver for the differences in behavior we see across   models .   A possible criticism of our setup is that it re-   quires short - term memory capacities that exceed   the memory capacities of most , if not all , humans .   That is , if we presented humans with the same in-   put as the model , we would not expect them to be   able to keep track of the contents of all 7 boxes due   to memory limitations . Therefore we are poten-   tially expecting models to do super - human entity   tracking , a setup that has been criticized for model   evaluations of other linguistic abilities ( Lampinen ,   2022 ) . We nevertheless believe that our task is justi-   ﬁed given the architecture of the evaluated models .   Transformer - based models can look back to any to-   ken in the entire input sequence within their context   window , so a proper comparison between humans   and models would be to present humans with the   full description in written form and let them re - read   the description after being prompted to state the   contents of a box . While we did not formally eval-   uate whether humans have this ability on a larger   population , we personally did not have any trouble   tracking the contents of boxes when we had access   to the written description .   Relatedly , we designed our task such that the   entire description ﬁts within the context window of   pretrained language models . However , as we men-   tioned in the introduction , entity tracking is an im-   portant ability for understanding long contexts and   given the limited context window , our results do   not apply to texts whose length exceeds a model ’s   context window , and likely different model archi-   tectures will be necessary to perform proper entity   tracking for longer texts .   Further , while we found that GPT-3.5 models as   well as ﬁnetuned T5 models can track entities in   our task with higher accuracy than a strong random   baseline , our results also indicate that this behavior   is not very stable once several operations act on an   entity . Our results should therefore not be takenas justiﬁcation for using these models for critical   applications where high accuracy is needed .   Lastly , we only evaluated English models in this   work . Given that we showed that even without high   lexical overlap between the training and evaluation   examples , models can keep track of entities to some   extent , it seems likely that our results also apply to   other languages . However , whether this actually   the case remains an open question .   Acknowledgements   We thank Jacob Andreas , Ellie Pavlick , Allyson Et-   tinger , Tal Linzen , Will Merrill , and the members   of the NYU Computation and Psycholinguistics lab   for discussions , and Belinda Li for sharing model   outputs and details about their data preparation   procedures and experiments . We thank Cookie for   contributing to the authorship decision and for emo-   tional support . This research was conducted in part   through the NYU IT High Performance Computing   resources , services , and staff expertise , and it was   supported by the NSF under Grant # 2030859 to   the Computing Research Association for the CIFel-   lows Project and the European Research Council   ( ERC ) under the European Union ’s Horizon 2020   Research and Innovation Program ( Grant Agree-   ment # 948878 ) . Any opinions , ﬁndings , and con-   clusions or recommendations expressed in this ma-   terial are those of the authors and do not necessarily   reﬂect the views of the National Science Founda-   tion nor the Computing Research Association .   References3844384538463847A Additional Analyses of Results from Li   et al . ( 2021 )   As mentioned in Footnote 3 , Li et al . ( 2021 ) con-   ducted two more experiments that prima facie pro-   vided additional evidence for implicit meaning rep-   resentations and state tracking abilities in language   models . However , the data and setup of these two   experiments also likely overestimates models ’ abil-   ities .   In the second set of probing classiﬁer experi-   ments , Li et al . ( 2021 ) used data generated using   the TextWorld engine ( Côté et al . , 2019 ) . In this   setup , there is a textual description of several en-   tities in a text - based game ( e.g. , a wooden door )   as well as actions that a player took ( e.g. , opening   the wooden door ) . Their probing classiﬁer takes   the representations of either one entity and a prop-   erty of that entity ( e.g. , that the wooden door is   closed ) or the representations of two entities and   a relation between them ( e.g. , that the king - sized   bed is in the bedroom ) and from these representa-   tions , the classiﬁer has to predict whether a given   proposition is true or false considering the initial   description and the series of actions that a player   took . Only propositions that involve entities that   have been mentioned are probed . The issue with   this setup is that there are many propositions that   are always true in both the training and evaluation   splits ( e.g. , in all the game simulations , the chest   drawer is in the bedroom , so the probing classiﬁer   should always return true for this input independent   of the previous context ) . Furthermore , even for   the entity - property propositions and entity - relation-   entity propositions which are not always true in the   training and evaluation data , the data is very biased   and a baseline that predicts the most common an-   swer in the training data without taking the initial   descriptions or the user actions into account ( a vio-   lation of Desideratum 3 , see Section 3.1 ) , already   achieves an accuracy of 88.5 % , a number that puts   the reported probing classiﬁer accuracy of 96.9 %   into a bit more context .   Further , Li et al . ( 2021 ) , also presented an exper-   iment where they manipulated speciﬁc entity rep-   resentations of a synthetic version of the Alchemy   dataset . In this experiment , they ﬁrst encoded an   initial description Dand an operation Owhich af-   fected a beaker Xusing a language model that had   been ﬁnetuned to predict the next operation , result-   ing in representation R. Then , they encoded the   same initial representation Dand an operation ofthe form Drain nfrom beaker Y , where beaker Y   was always different from beaker Xandnwas the   amount of liquid that was in beaker Yaccording to   the initial description . This resulted in representa-   tionR. Then , they extracted the representation of   the initial description of beaker Yfrom representa-   tionRand replaced the representation of beaker   YinRwith the corresponding representation in   Rto obtain R . They then used R as   an input to T5 and showed that the predicted next   operation based on R was considerably more   often compatible with both operations ( the oper-   ation encoded in Rand the operation encoded   inR ) compared to predicting the next operation   fromRorR , which they took as evidence that   the token representations of the initial description   encoded the entities state after performing the op-   eration . The issue with this experiment is that the   operation from which Rwas computed was al-   ways of the form Drain nfromYth beaker , so the   ﬁnal state of beaker Ywas always empty . There-   fore , this experiment primarily shows that the token   drain affected the representation of the initial state   ( and subsequently the prediction of the next oper-   ation ) but not more generally , that the actual state   of the manipulated beaker Yis fully encoded in   its initial state description . To answer that ques-   tion , one would have to repeat this experiment with   more complex operations that do not give away   the ﬁnal state ( a violation of Desideratum 2 , see   Section 3.1 ) .   In summary , the additional experiments in Li   et al . ( 2021 ) also violate some of the desiderata we   laid out in Section 3.1 , and therefore it is difﬁcult   to draw conclusions about state tracking abilities   from these experiments .   B Example Input - Output Pairs   ( 3 ) shows an example input - output pair from our   base dataset ( NumOps on Box 6 = 2 ) .   ( 3 ) a. I : Box 0 contains the painting ,   Box 1 contains the bell , Box 2 contains   the guitar , Box 3 contains the egg and   the mirror and the sheet , Box 4 con-   tains the chemical , Box 5 contains the   disk and the wire , Box 6 contains the   glass and the knife . Move the glass   from Box 6 to Box 4 . Put the gift into   Box 5 . Move the guitar from Box 2 to   Box 6 . Put the milk into Box 4 . Re-   move the mirror and the sheet from3848Box 3 . Box 6 _ _   b. O : contains the guitar and the   knife .   ( 4 ) shows an example input - output pair from the   dataset with operations that contain referring ex-   pressions that are ambiguous without considering   the initial state and the previous operations ( Nu-   mOps on Box 6 = 2 ) . Note that in order to correctly   interpret Move the guitar from Box 2 to Box 6 , the   model has to consider the information that the blue   guitar ( as opposed to the red guitar ) was in Box   2 prior to the operation . Hence , this operation is   ambiguous out of context .   ( 4 ) a. I : Box 0 contains the yellow   book and the green ﬂower and the red   guitar , Box 1 contains the small bomb   and the small book and the blue bone ,   Box 2 contains the blue guitar , Box 3   contains the blue bell , Box 4 contains   the green paper and the yellow note   and the yellow television , Box 5 con-   tains the yellow bell , Box 6 is empty .   Move the guitar from Box 2 to Box 6 .   Put the blue wire and the big television   into Box 5 . Move the ﬂower from Box   0 to Box 6 . Box 6 _ _   b. O : contains the blue guitar and   the green ﬂower .   ( 5 ) shows an example input - output pair from the   dataset with the Move contents of operation ( Nu-   mOps on Box 6 = 2 ) . Note that in order to correctly   interpret Move the contents of Box 2 to Box 6 , the   model has to consider the information that the tea   ( as opposed to the red guitar ) was in Box 2 prior to   the operation .   ( 5 ) a. I : Box 0 contains the fan and the   gift and the letter , Box 1 contains the   beer and the mirror and the tie , Box   2 contains the tea , Box 3 contains the   boot , Box 4 contains the coat and the   plate and the shirt , Box 5 contains the   bottle , Box 6 is empty . Move the con-   tents of Box 2 to Box 6 . Put the dress   and the painting into Box 5 . Move the   letter from Box 0 to Box 6 . Box 6 _ _   b. O : contains the letter and the   tea . C Implementation Details   In - context For GPT-3 , we used the OpenAI API .   We used greedy decoding with the temperature pa-   rameter set to 0 , used a maximum target generation   length of 150 , and used the newline character as   an additional stop token . Inference time and model   size is not available for GPT-3 , but we generated   about 16 million tokens in total . All GPT-3 ex-   periments that we report here in the paper were   conducted in January 2023 .   For Flan - T5 , we used a beam size of 3 and a   maximum target generation length of 256 . Other   hyperparameters were kept as the default values of   T5ConditionalGeneration in the HuggingFace   library . Inference for T5 - XL took about 5 hours   on a single A100 GPU , and for T5 base , about 2   hours .   Finetuning We ﬁnetuned T5 for a single epoch us-   ing a batch size of 8 and a learning rate of 1×10 .   In initial explorations , increasing the number of   ﬁnetuning epochs did not yield substantial gains on   development set performance , and was sometimes   even harmful . Training and inference took around   3 hours on a single RTX8000 GPU .   D Additional Results   D.1 GPT-3.5 Zero - shot Results   As mentioned in the main text , GPT-3.5 was able   to to output the contents of boxes in the correct   format without any in - context demonstrations ( see   Table 5 for the prompt template ) .   Figure 9 compares the performance of   text - davinci-003 in the 2 - shot setting to the   zero - shot setting . Model performance degraded   slightly without demonstration examples , which   suggests that the examples are indeed helpful   in guiding the model to correctly perform the   task . Nevertheless , we observed non - negligible   performance even in the zero - shot setting , which   corroborates the conclusion that GPT 3.5 exhibits   non - trivial entity tracking capacities .   D.2 GPT-4 Results   We also probed the more recently released GPT-4   model ( OpenAI , 2023 ) through the OpenAI API .   Figure 8 compares the performance of GPT-3.5   text - davinci-003 and GPT-4 on the base dataset   ( left ) as well as the more challenging MoveCon-   tents ( middle ) and AmbiRef ( right ) datasets dis-   cussed in Section 4.2 . Across all datasets , GPT-43849   performed considerably better than GPT-3.5 . But   similarly to GPT-3.5 , performance degraded as   more operations affected a box . Considering that   no public information about the training data , train-   ing procedure or architectural details about GPT-4   exists , we can not draw any conclusions about the   reason behind the improved performance of GPT-4   compared to previous models .   E Prompts   Table 3 shows the 2 - shot prompts used for in-   context experiments . Table 4 shows the 2 - shot   prompts used for the AltForms experiments , where   the demonstrations contain descriptions that have   lower lexical overlap with the test examples . Ta-   ble 5 shows the zero - shot prompts discussed in Ap-   pendix D. All prompts are available at https://   github.com/sebschu/entity-tracking-lms .   F Dataset Statistics and License   Information   See Tables 6 and 7 for descriptive statistics of our   datasets . All datasets are released under the GNU   General Public License v3.0.38502 - shot prompt with all boxes queried at once ( GPT-3 experiments )   Given the description after " Description : " , write a true statement about all boxes and their   contents to the description after " Statement : " .   Description : Box 0 contains the car , Box 1 contains the cross , Box 2 contains the bag and the   machine , Box 3 contains the paper and the string , Box 4 contains the bill , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle and the map .   Statement : Box 0 contains the car , Box 1 contains the cross , Box 2 contains the bag and the   machine , Box 3 contains the paper and the string , Box 4 contains the bill , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle and the map .   Description : Box 0 contains the car , Box 1 contains the cross , Box 2 contains the bag and the   machine , Box 3 contains the paper and the string , Box 4 contains the bill , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle and the map . Remove the car from   Box 0 . Remove the paper and the string from Box 3 . Put the plane into Box 0 . Move the map   from Box 6 to Box 2 . Remove the bill from Box 4 . Put the coat into Box 3 .   Statement : Box 0 contains the plane , Box 1 contains the cross , Box 2 contains the bag and the   machine and the map , Box 3 contains the coat , Box 4 contains nothing , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle .   Description : { description }   Statement : Box 0 contains   2 - shot prompt with boxes queried individually ( T5 experiments )   Given the description after " Description : " , write a true statement about a box and the contents   of this box according to the description after " Statement : " .   Description : Box 0 contains the car , Box 1 contains the cross , Box 2 contains the bag and the   machine , Box 3 contains the paper and the string , Box 4 contains the bill , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle and the map .   Statement : Box 1 contains the cross .   Description : Box 0 contains the car , Box 1 contains the cross , Box 2 contains the bag and the   machine , Box 3 contains the paper and the string , Box 4 contains the bill , Box 5 contains the   apple and the cash and the glass , Box 6 contains the bottle and the map . Remove the car from   Box 0 . Remove the paper and the string from Box 3 . Put the plane into Box 0 . Move the map   from Box 6 to Box 2 . Remove the bill from Box 4 . Put the coat into Box 3 .   Statement : Box 2 contains the bag and the machine and the map .   Description : { description }   Statement : Box { boxnum } contains38512 - shot prompt with disjoint surface forms from test examples   Given the description after " Description : " , write a true statement about all containers or boxes   and their contents to the description after " Statement : " .   Description : The biscotti is in Container A , the icicle is in Container B , the granite and the   machine are in Container C , the folio and the encyclopedia are in Container D , the bill is in   Container E , the spork and the jackknife and the frappuccino are in Container F , the clipper and   the ladybug are in Container G.   Statement : Container A contains the biscotti , Container B contains the icicle , Container C   contains the granite and the machine , Container D contains the folio and the encyclopedia , Con-   tainer E contains the bill , Container F contains the spork and the jackknife and the frappuccino ,   Container G contains the clipper and the ladybug .   Description : The biscotti is in Container A , the icicle is in Container B , the granite and the   machine are in Container C , the folio and the encyclopedia are in Container D , the bill is in   Container E , the spork and the jackknife and the frappuccino are in Container F , the clipper and   the ladybug are in Container G. Take the biscotti out of Container A. Take the folio and the   encyclopedia out of container D. Place the tetrapod inside Container A. Pick up the ladybug in   Container G and place it into Container C. Take the bill out of Container E. Place the gumball   inside Container D.   Statement : Container A contains the tetrapod , Container B contains the icicle , Container C   contains the granite and the machine and the ladybug , Container D contains the gumball , Con-   tainer E contains nothing , Container F contains the spork and the jackknife and the frappuccino ,   Container G contains the clipper .   Description : { description }   Statement : Box 0 contains   Zero - shot prompt   Given the description after " Description : " , write a true statement about all boxes and their   contents according to the description after " Statement : " . Format the statement as Box 0 contains   the A , Box 1 contains the B , Box 2 contains the C , Box 3 contains the D , Box 4 contains the   E , Box 5 contains the F , Box 6 contains the G. A , B , C , D , E , F , G are placeholders for the   contents of each box .   Description : { description }   Statement : Box 0 contains3852Scenarios Examples   Dataset Demonstration Test Demonstration Test   Complete 1 990 14 90,090   Subsample 1 491 14 5,012   AmbiRef . 1 474 14 4,991   MoveContents 1 451 14 4,956   Scenarios Examples   Dataset Train Dev Test Train Dev Test   Base 990 220 990 90,090 20,020 90,090   NumOps 990 220 990 20,790 20,020 90,090   V ocab 990 220 990 90,090 20,020 90,090   AltForms 990 220 990 90,090 20,020 90,090   AltForms+NumOps 990 220 990 20,790 20,020 90,090   AmbiRef 990 220 990 90,090 20,020 90,090   MoveContents 990 220 990 90,090 20,020 90,0903853ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations on page 10 .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . We evaluate existing models , so we do n’t see any potential risks other than misinter-   preting our results . We hope we managed to limit the potential for misunderstanding by discussing in   detail how to interpret the results .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , Section 1 ( " Contributions " paragraph on p2 )   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We developed a novel procedure for programmatically generating datasets and we will release that   code and the generated datasets upon publication ( described in Section 3 ) .   We also discuss an analysis of the probing datasets from Li et al . ( 2021 ) in Section 2 and Appendix A.   /squareB1 . Did you cite the creators of artifacts you used ?   We cited the pretrained models that we used ( Sections 4 and 5 ) . We cite Li et al . and relevant source   datasets in Section 2 and Appendix A.   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   See Appendix E.   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Our data mostly concerns moving objects from one box to another , so do not envision a risk scenario   of use outside of research contexts .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . We generated the data from a set lexicon and templates and therefore , we can rule   out that any identifying information is contained in the data .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   See Section 3 .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   See Appendix E.3854C / squareDid you run computational experiments ?   See Sections 4 and 5 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   See Section 4 and Appendix C.   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   See Appendices B , C , and D.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   See Sections 4 and 5 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   See Section 4 and Appendix C.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.3855