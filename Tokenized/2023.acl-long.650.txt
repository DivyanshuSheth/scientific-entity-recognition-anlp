  Liyan Tang , Tanya Goyal , Alexander R. Fabbri , Philippe Laban ,   Jiacheng Xu , Semih Yavuz , Wojciech Kry ´ sci´nski , Justin F. Rousseau , Greg DurrettThe University of Texas at AustinSalesforce AI Research   lytang@utexas.edu   Abstract   The propensity of abstractive summarization   models to make factual errors has been studied   extensively , including design of metrics to de-   tect factual errors and annotation of errors in   current systems ’ outputs . However , the ever-   evolving nature of summarization systems , met-   rics , and annotated benchmarks makes factu-   ality evaluation a moving target , and drawing   clear comparisons among metrics has become   increasingly difficult . In this work , we aggre-   gate factuality error annotations from nine ex-   isting datasets and stratify them according to   the underlying summarization model . We com-   pare performance of state - of - the - art factuality   metrics , including recent ChatGPT - based met-   rics , on this stratified benchmark and show that   their performance varies significantly across   different types of summarization models . Crit-   ically , our analysis shows that much of the re-   cent improvement in the factuality detection   space has been on summaries from older ( pre-   Transformer ) models instead of more relevant   recent summarization models . We further per-   form a finer - grained analysis per error - type and   find similar performance variance across error   types for different factuality metrics . Our re-   sults show that no one metric is superior in all   settings or for all error types , and we provide   recommendations for best practices given these   insights .   1 Introduction   Although abstractive summarization systems ( Liu   and Lapata , 2019 ; Lewis et al . , 2020 ; Raffel et al . ,   2020 ; Zhang et al . , 2020 ) have improved dra-   matically in recent years , these models still of-   ten include factual errors in generated summaries   ( Kryscinski et al . , 2020 ; Maynez et al . , 2020 ) . A   number of metrics have emerged to detect factuality   errors , including methods based on sentence entail-   ment ( Kryscinski et al . , 2020 ) , finer - grained entail - ment ( Goyal and Durrett , 2020 ; Zhao et al . , 2020 ) ,   question generation and answering ( Wang et al . ,   2020 ; Durmus et al . , 2020 ; Scialom et al . , 2021 ) ,   and discrimination of synthetically - constructed er-   ror instances ( Cao and Wang , 2021 ) . Despite recent   analyses ( Pagnoni et al . , 2021 ; Laban et al . , 2022 ) ,   reliably comparing these metrics remains difficult .   In this paper , we provide a new benchmark that   allows for finer - grained comparison between dif-   ferent factuality systems . We aggregate 9 existing   annotated factuality datasets to create our bench-   mark A F. We stratify it according to   the underlying summarization model , categorized   intoFS , EX andObased on their   development timeline ( see Section 2 ) . First , we   ask : do factuality metrics perform equally well   at identifying errors from state - of - the - art sum-   marization models and from earlier models ?   For nine recent factuality metrics , including re-   cent ChatGPT - based metrics , we show that metric   performance varies substantially between different   categories of summarization models . Most impor-   tantly , we found that the standard way of reporting   improvements on category - agnostic benchmarks   can be misleading , as most of these gains are on   theOorEX subset of the data which   are less important to detect . On summaries gen-   erated by FS models , we found that there is   no single metric that is superior in evaluating sum-   maries from both the CNN / DM ( Hermann et al . ,   2015 ) and XSum ( Narayan et al . , 2018 ) datasets .   To better understand their behavior , we next an-   alyze what error types are different factuality   metrics capable of identifying ( Section 4 ) . To   do this , we leverage datasets from our benchmark   that have fine - grained error annotations and unify   these into a single taxonomy . We find that the er-   ror type distribution changes over time and even   differs between annotations of the same summa-   rization models across factuality datasets . Analysis   of the factuality metrics shows that metrics claim-11626   ing SOTA performance can identify each error type   better in general , but all metrics differ significantly   in how they perform on the same error types across   CNN / DM and XSum .   We conclude with the following recommenda-   tions for best practices in this area :   1.Evaluate factuality metrics on summaries   generated by the state - of - the - art summariza-   tion models . We found generally worse per-   formance when evaluating factuality systems   on summaries generated by FS models   instead of less recent models ( Section 3 ) . We   release A F to support this , which com-   bines existing benchmarks and stratifies them ac-   cording to the base summarization model , sum-   marization dataset and error types . We suggest   future work to augment our benchmark with   LLM - generated summaries , e.g. from ChatGPT ,   which is beyond the scope of this paper .   2.Choose an appropriate factuality metric for   your downstream task at hand . No one met-   ric is superior across all settings ( Section 4 ) .   Fine - grained insights offered by our benchmark   can be useful to compare strengths of different   factuality metrics and make this choice .   3.Annotate error types consistently with prior   work for better comparability . We found thaterror type boundaries in existing works are not   clear and are not easy to leverage for cross-   dataset metric comparisons ( Section 4 ) .   We hope that our analysis can shed light on what   comparisons practitioners should focus on , how   to understand the pros and cons of different met-   rics , and where metrics should go next . Further ,   we hope that future work would extend this to in-   corporate diverse summarization domains such as   dialogue summarization ( Tang et al . , 2022 ; Fabbri   et al . , 2021a ; Zhang et al . , 2021 ) and medical ev-   idence summarization ( Tang et al . , 2023 ) . These   would have different error distributions , and anno-   tated datasets are needed to perform a more compre-   hensive comparison and design domain - invariant   factuality metrics .   2 Benchmark   2.1 Benchmark Standardization   Current factuality metrics are evaluated without   considering the types of summarization models   used to generate the annotated summaries . In these   annotated datasets , a large proportion of summaries   are generated by older models , such as a pointer-   generator network ( See et al . , 2017 ) , that often   make obvious errors that recent models do not   make . We hypothesize that current factuality   systems primarily make progress in identifying11627   factuality inconsistencies in summaries gener-   ated by out - of - date summarization models . If   this hypothesis is correct , comparing factuality sys-   tems on such datasets provide us less useful infor-   mation on how these metrics perform on modern   summarization systems .   Summarization datasets splits We introduce a   new benchmark A F built on top of Sum-   maC from Laban et al . ( 2022 ) . The benchmark   Aggre gates nine publicly available datasets ( see Ta-   ble 1 ) that consist of human evaluations of Factual   consistency on model generated summaries . We fo-   cus particularly on incorporating recent datasets an-   notated on top of state - of - the - art pre - trained Trans-   former models .   All datasets contain summaries generated from   articles in CNN / DM and XSum . Given the   unique characteristics of CNN / DM and XSum ,   our proposed benchmark includes two subsets ,   A F - CandA F - XS , that   evaluate the performance of factuality metrics on   these two datasets separately ( Table 2 ; see also   Table 6 and 7 in the Appendix ) . This facilitates   a more fine - grained and rigorous analysis of the   metric performance .   Our benchmark formulates factual consistency   evaluation as a binary classification task , following   Laban et al . ( 2022 ) . The binary factuality labels   for the summaries are determined by human evalu-   ations on the annotated datasets ( Section 2.2 ) .   Summarization model splits To validate our hy-   pothesis and make a careful comparison of factual-   ity metrics , we further divide models that were used   to generated summaries in the benchmark into three   distinct categories : C={FS , EX ,   O } , as seen in Table 2 . FS represents   state - of - the - art fine - tuned summarization models ,   including BART ( Lewis et al . , 2020 ) , PEGASUS   ( Zhang et al . , 2020 ) and T5 ( Raffel et al . , 2020 ) .   EX is a collection of early Transformer-   based summarization models . Typical models that   fit into this category include BERTSum ( Liu andLapata , 2019 ) , and GPT-2 ( Radford et al . , 2019 ) .   The remaining models , such as Pointer - Generator   ( See et al . , 2017 ) and BottomUp ( Gehrmann et al . ,   2018 ) , are instances of O. A full description   of the models in each category is found in Ap-   pendix A.   2.2 Benchmark Datasets   The S Cbenchmark ( Laban et al . , 2022 )   includes six annotated datasets for factual con-   sistency evaluation . We directly include XSum-   Faith ( Maynez et al . , 2020 ) , FactCC ( Kryscinski   et al . , 2020 ) , SummEval ( Fabbri et al . , 2021b ) , and   FRANK ( Pagnoni et al . , 2021 ) from S Cin   our benchmark . We do not include the CoGen-   Summ ( Falke et al . , 2019 ) dataset as the original   task is ranking pairs of generated summaries in-   stead of detecting factually consistent summaries ,   and pairs of summaries can be both factually con-   sistent or inconsistent . We modify the Polytope   ( Huang et al . , 2020 ) dataset in S Cwhere we   view summaries annotated with addition , omission   orduplication errors as factually consistent since   these three error types are not related to factual   consistency . We use the validation and test splits   from S C for the above mentioned datasets .   In addition to modifying S C , we further   include four annotated datasets . For Wang’20   ( Wang et al . , 2020 ) , CLIFF ( Cao and Wang , 2021 )   and Goyal’21 ( Goyal and Durrett , 2021 ) , we create   data splits based on the parity of indices , following   S C. For Cao’22 ( Cao et al . , 2022 ) , we use   the existing splits from the original work .   Deduplication and label disagreement correc-   tion Some examples may be labeled for errors   in multiple datasets . We removed all duplicates so   that each instance appears only once in our bench-   mark . During this deduplication process , we de-   tected 100 instances of the same summaries that   are annotated in different datasets with different   factual consistency labels . 98 of them are between   FRANK and XSumFaith , and 2 of them are be-   tween FRANK and SummEval . The authors of   this work manually corrected the labels for these   examples based on our judgment .   2.3 Benchmark Evaluation Metrics   We use balanced accuracy to evaluate the perfor-   mance of factuality metrics due to the imbalance   of factually consistent and inconsistent summaries .   We refer readers to Laban et al . ( 2022 ) for further11628justification of balanced accuracy as the evalua-   tion metric . In each dataset , a factuality metric   selects a threshold for FS , EX and   O , respectively , based on the performance on the   corresponding validation set . The chosen thresh-   olds convert raw scores from metrics into binary   labels for balanced accuracy evaluation . We pro-   vide a weighted average of performance across all   datasets in the benchmark ( see Table 3 ) .   3 Comparison of Factuality Metrics   First , we evaluate several SOTA factual consistency   metrics on our benchmark , namely DAE ( Goyal   and Durrett , 2020 , 2021 ) , QuestEval ( Scialom   et al . , 2021 ) , SummaC - ZS , SummaC - Conv ( La-   ban et al . , 2022 ) and QAFactEval ( Fabbri et al . ,   2021c).We also benchmark recent ChatGPT-   based evaluation metics from Luo et al . ( 2023 ) and   Wang et al . ( 2023 ) . ChatGPT - ZS andChatGPT-   CoT ( Luo et al . , 2023 ) prompt LLMs to directly   output a binary factuality decision . On the other   hand , ChatGPT - DA andChatGPT - Star ( Wang   et al . , 2023 ) ask LLMs to score the factuality of   generated summaries on a scale of 0 - 100 and 1 - 5   respectively . More details about these metrics , in-   cluding exact prompts are included in Appendix B.   Unifying these metrics We consider each met-   ric as a function f(d , s)→y , mapping each   ( document , summary ) pair to a score y∈R.   We convert each method into a binary classifier   f(d , s)→ { 0,1}by picking a threshold tsuch   that we predict 1 if f(d , s ) > tand 0 otherwise .   All thresholds are set separately for each met-   ric . We consider two ways of setting the threshold   for a metric : threshold - per - dataset andsingle-   threshold . The first setting has thresholds { t }   within each metric for every dataset we consider ,   where d , candmare any dataset in D , any model   category from C , and any factuality metric , respec-   tively . This allows one to choose the right metric   for the task at hand . The single - threshold setting   defines one threshold { t}per metric .   Threshold Analysis We analyze scores from fac-   tuality metrics using chosen thresholds { t}from   the validation sets . Specifically , for each factuality   metric , we average the values of thresholds for each   ofS , EX andOacross all datasets   ( Figure 1 ) . For all facuality metrics , the average   threshold values for A F - Care greater   than those for A F - XSacross all cat-   egories . This discrepancy of threshold values   shows that evaluating on both of these datasets   with a single threshold is a difficult balancing   act and may lead to poor results on at least one   dataset .   The higher threshold values on CNN / DM are   connected to both the nature of the errors involved   and overall extractiveness of the summaries XSum   summaries are more abstractive and tend to con-   tain a larger number of errors , making it harder   for the metrics to verify the consistency of sum-   maries with respect to the source text and resulting   in lower scores in general , even for factual cases .   For CNN / DM , smaller deviations from the source   may indicate non - factuality .   Binary Classification Results A weighted aver-   age of performance in terms of balanced accuracy   forA F - CandA F - XS   is shown in Table 3.It shows results using both   trained metrics ( upper half ) and ChatGPT - based   metrics ( bottom half ) .   Our results show that for A F - C ,   both trained and ChatGPT - based factuality met-   rics achieve the best performance in evaluating the   summaries in O. This result is intuitive : the sum-11629   maries in Ocontain obvious errors , such as repe-   tition , that can be more easily detected compared to   more nuanced errors made by more recent models .   From Table 2 , the majority of annotated summaries   are generated by models from O , so category   agnostic performance evaluation will weight these   more heavily . There is a significant performance   drop when evaluating the CNN / DM summaries   generated by models from EX orF-   S instead . Approximately a 10 % balanced   accuracy decrease on average occurs from O   toFS . Evaluating on entire datasets , as is   standard in prior work , gives us limited informa-   tion of how these metrics perform on the FS   summaries that are of more interest .   We observe more mixed results for A F-   XS . Here , the trained and ChatGPT - based met-   rics perform best on FS andEX re-   spectively . In fact , the ChatGPT - ZS and ChatGPT-   Star metrics report new state - of - the - art results   for the EX category . In the case of   A F - XSalso , we advocate for compar-   ing metrics according to such a category - wise view   as it provides more information on the most suit-   able metric to use while evaluating a given category   of models .   Binary Classification : FS To encour-   age comparison of factuality metrics on FS   summaries , we provide a separate benchmark   which consists of two subsets A F - C-   FS andA F - XS - FS that   only consider summaries generated by FS   models . This benchmark consists of validation and   test splits from the FS subsets of the two   datasets . This setting allows for comparisons of   metrics to be made using only a single threshold .   We show metric comparisons on the FS   subset in Table 4 . Note that the ranking of factual-   ity metric here ( single - threshold setting ) is slightly   different from the ranking in Table 3 ( threshold - per-   dataset setting ) . For A F - C - FS ,   QuestEval achieves the best performance amongst   all metrics . We did not observe a statistically   significant improvement over other trained eval-   uation metrics ; however , its improvement over   ChatGPT - based metrics is statistically significant .   ForA F - XS - FS , the DAE met-   ric is significantly better than all other metrics .   Interestingly , metrics such as SummaC - Conv ,   QAFactEval and the recent ChatGPT metrics were   all proposed as improved factuality evaluation on   the category - agnostic SummaC benchmark ( differ-   ent from the SummaC metric ) . However , our strati-   fied analysis provides a much clearer picture and   shows that metrics which claim improved perfor-   mance on S Cdo not show similar gains   when evaluated on FS summaries . We rec-   ommend that future work similarly focuses on the   S category of generated summaries when com-   paring factuality metrics.11630   4 Finer - grained Error Analysis   Having established differences among factuality   metrics across underlying summarization models ,   we now explore differences in metrics according to   factuality error types . To do this , we need a way to   unify error types across datasets in our benchmark   and map them into a shared taxonomy .   4.1 A Taxonomy of Error Types   We surveyed existing error type taxonomies in prior   work and unified the types of factual errors among   them into a hierarchical taxonomy in Figure 2 . Ar-   rows relate more specific error types to more gen-   eral “ parent ” errors . The prior works that make   use of each error type can be found in Appendix C.   As shown in the figure , most error types related   to factual consistency fall under the subset { intrin-   sic , extrinsic } ×{noun phrase , predicate } if we   consider the coarsest level of the hierarchy . We dis-   card discourse errors as these are uncommon and   not present in most of our datasets . Therefore , we   consolidate all unique error type taxonomies from   all four datasets we consider here into this error   type subset ( shown in the gray box in Figure 2 ) .   Descriptions and examples for these error types are   in Table 9 . Further , we introduce two additional   error categories { intrinsic - entire sent . , extrinsic-   entire sent . } if an entire sentence is annotated as   erroneous .   We are able to map four of the datasets ( see Sec-   tion 4.2 ) in A F that contain fine - grained   annotations to our unified taxonomy . For all four   datasets , if there are multiple annotators , we assign   an error type to a summary if the error is anno-   tated by more than one annotator . We allow one   summary to have multiple error types . We call theannotated subset related to CNN / DM and XSum   asA F - C - U andA F-   XS - U , respectively .   4.2 Error Mapping   XSumFaith XSumFaith consists of 500 sum-   maries each from human reference , two models   in O , and two models in EX . All sum-   maries are annotated with intrinsic and extrinsic   errors , but no finer categories are distinguished .   For error type mapping , we automatically detect   predicates in a summary and assign each error span   intrinsic- or extrinsic - predicate error if it contains   a predicate . We map the remaining error spans to   intrinsic- or extrinsic - noun phrase error .   FRANK The CNN / DM subset of FRANK con-   sists of three models in O , and one model each in   both EX andFS . The XSum portion   of FRANK has two models each in OandEX- . Each model contains 250 summaries in   the dataset . We mapped Entity error and Out of Ar-   ticle error to extrinsic - noun phrase error ; Predicate   error and Grammatical error to extrinsic - predicate   error ; Circumstance error and Coreference error   to intrinsic - noun phrase error ; and other errors to   intrinsic - predicate error .   Goyal’21 Authors of the original dataset manu-   ally identified all hallucinated text spans for each   summary and classified hallucination types into   { intrinsic , extrinsic } ×{entity , event , noun phrase ,   others } . The dataset consists of summaries for both   CNN / DM and XSum . For the CNN / DM susbset ,   the authors directly annotated 50 summaries from   FactCC , where summaries were generated by O   models . The XSum subset consists of summaries   from FS models . We map entity - related and   noun phrase - related errors to noun phrase errors ,   event errors to predicate errors and others to entire   sentence errors .   CLIFF This dataset consists of 150 summaries   each for both CNN / DM and XSum from two mod-   els in FS . We use the same approach for   error mapping as we do for XSumFaith by only   considering words labeled as extrinsic or intrinsic   errors .   We evaluate the accuracy of our error type map-   ping via manual inspection . Specifically , the au-   thors of this work inspect 30 factually inconsistent   examples each for XSumFaith , FRANK and CLIFF .   Those examples cover summaries generated by all11631   models used in the datasets . Results of the manual   inspection show that the accuracy of our error type   mapping is over 90 % .   A common discrepancy noticed by annotators   was that in several cases the examples were origi-   nally annotated as intrinsic / extrinsic but we believe   those errors are extrinsic / intrinsic . These cases   are not a result of error in our mapping , but in-   stead disagreement or error in the original annota-   tion itself . For error mapping , we found out map-   ping of FRANK to be least accurate among all 4   datasets . For example , we found that the entity   error ( EntE ) can be either intrinsic or extrinsic even   though FRANK explicitly defines an extrinsic error   type , i.e. “ out of article ” error . For Goyal’21 , we   manually correct any mapping errors that occur in   the 150 examples . Corrections mostly happen for   the event - related error defined in Goyal’21 which   can be either noun phrase- or predicate - related .   4.3 Distribution Shift of Error Types   Next , we explore how the number of errors in spe-   cific groups of models from FS , EX ,   andOhas changed with the progress in the field .   Specifically , for each of the FRANK , XSumFaith ,   Goyal’21 , and CLIFF datasets , we calculate the   ratio of error types from factually inconsistent sum-   maries generated by each model . We then study   any distribution shift of error types in A F-   C - U andA F - XS - U   under FS , EX , and O.   Summaries generated by the same models con-   sist of different error distributions over different   annotated datasets . As shown in A F-   XS - U ( Figure 3 ) , BART summaries are   annotated by both Goyal’21 and CLIFF . However ,   it is interesting that BART summaries were anno - tated as making more intrinsic - noun phrase and   intrinsic - predicate errors in Goyal’21 but more   extrinsic - noun phrase errors in CLIFF . Similar   observations can be found in A F - C-   U , where BART summaries have a higher   proportion of extrinsic - predicate error in FRANK   and more intrinsic - noun phrase error in CLIFF .   In addition , although XSumFaith and FRANK   annotate the same set of model generated sum-   maries in A F - XS - U , the dis-   tribution of error types looks dramatically differ-   ent . The main discrepancy lies in the proportion of   extrinsic - noun phrase and intrinsic - predicate errors .   There are two possible reasons for such discrep-   ancy . First , FRANK does not have “ entire sent . ”   errors based on our conversation of its annotation   schema to the unified taxonomy ( Section 4.2 ) . Sec-   ond , and more important , it is not easy to map error   types from FRANK directly to our unified error   types in spite of our validation . For example , the   “ out of article error ” in FRANK is defined as an   error where some statements in the summary do   not show up in the source text . We found this error   can be mapped to either an extrinsic - noun phrase   error or extrinsic - predicate error . These observa-   tions indicate that previous work disagrees about   where the individual error class boundaries are ,   even when aligned with our taxonomy .   A combined meta - analysis shows shifts in er-   ror distributions . Figure 3 shows that error   type distribution can vary among models from   the same category . For example , summaries from   BART contain a higher ratio of intrinsic - noun   phrase errors than PEGASUS in A F-   C - U . We now combine all datasets   together from A F - C - U and   A F - XS - U and show the uni-11632   fied error distributions over three model cate-   gories . As shown in Figure 4 , models make ap-   proximately 50 % extrinsic errors in CNN / DM , with   a slightly decrease from Oto more recent mod-   els . For XSum , the proportion of extrinsic errors   remains unchanged and is at 70 % .   4.4 Error Detection by Type   In this section , we analyze how factuality metrics   perform on summaries that contain certain error   types . Specifically , we collect subsets of exam-   ples from four annotated datasets and group them   intoA F - C - E andA F-   XS - E .Every subset contains summaries   thatinclude only one error type defined in Sec - tion 4.1 . Each factuality metric assigns a bi-   nary label to an instance obtained directly from   A F - CandA F - XS . Note   that each subset only consists of test set exam-   ples from our benchmark since examples from   the validation set were used to choose the opti-   mal thresholds ( Section 3 ) . Since there are limited   annotations for each model category after only con-   sidering examples from the test set of the bench-   mark , we decide not to split data by model cate-   gories in this part of the analysis . We calculate   the recall of identifying error types from those sub-   sets and show the results in Table 5 . Summaries   inA F - C - E andA F-   XS - E primarily come from non- FS   models ( 89.6 % and 92.1 % , respectively ) . On   A F - C - E , where 79.0 % of sum-   maries were generated from O , there are more   extrinsic errors ( 349 ) than intrinsic errors ( 243 ) .   This agrees with our above analysis that also shows   that errors in generated summaries from less recent   models are more likely to be extrinsic ( Figure 4 ) .   Across both A F - C - E and   A F - XS - E , we found that re-   cent metrics like SummaC - Conv , QAFactEval and   ChatGPT - based achieve higher recall for most error   types . This indicates that more recent factuality   metrics are better at capturing obvious errors   generated by less recent models . This mirrors   our earlier finding in Table 3 ( column EX   andO ) . Interestingly , we find that summariza-   tion datasets ( CNN / DM and XSum ) have a non-   negligible effect on the metrics ’ capabilities of11633detecting certain error types , even in the cases of   out - of - date errors . For example , the recall of iden-   tifying extrinsic - noun phrase error drops 10 - 30 %   across all trained factuality metrics when evalu-   ated on XS , compared to CNN / DM . Similarly ,   ChatGPT metrics report 20 - 30 % higher recall on   CNN / DM , compared to its XScounterparts .   Another observation is that although DAE is   trained using annotations from XSumFaith , which   provides supervision for multiple error types , it   does not identify errors as well in A F-   C - E . These findings indicate that summa-   rization models make fundamentally different   errors for each error type , and current factuality   metrics can not be uniformly good at identifying   certain error types across datasets . We believe   this conclusion still holds when evaluating met-   rics on summaries generated from FS models   since they generate less obvious errors .   5 Recommendations   Evaluate factuality models on modern summa-   rization systems We have seen that FS   yields significantly different results than EX- orO. Because of the prevalence of   these systems , we believe that any new work should   prefer evaluating on these S summaries .   Particularly for factuality metrics that are ei-   ther based on latest LLMs or on pre - trained mod-   els , evaluating on modern summarization systems   is needed to see if these metrics are actually im-   proving from the current state - of - the - art or merely   patching errors in outdated systems that have al-   ready been fixed by other advances .   Annotate factual consistency errors from sum-   maries generated by LLMs Recent work ( Goyal   et al . , 2022 ) shows that LLMs like GPT-3 are capa-   ble of generating summaries that are preferred over   FS summaries by human annotators . Further-   more , they show that existing factuality metrics   can not reliably detect errors in summaries from   GPT-3 models as these latter summaries differ sub-   stantially from existing benchmarks and training   sets . We encourage future work to annotate errors   from LLM - generated summaries and evaluate new   factual consistency metrics on this set as well in   addition to the FS set . As such , we believe   that future work should construct “ living ” bench-   marks for factuality evaluation that are consistently   updated as more powerful summarization systems   are introduced . Choose the right metric for the job We note   that there is no one clear winner among the metrics   evaluated here ( Section 3 ) . Depending on the down-   stream application , different methods may be more   or less appropriate , as our analysis shows . More-   over , none of current factuality metrics can identify   certain error types across datasets equally well . As   QG / QA and NLI models get better , we expect all   of these methods to improve further . Alternatively ,   although recent ChatGPT - based metrics ( Luo et al . ,   2023 ; Wang et al . , 2023 ) do not perform well on   modern summarization systems , they can be a start-   ing point for leveraging LLMs to perform factual   consistency evaluation .   Use more consistent error types With our tax-   onomy , we have mapped error types annotated in   previous work . It is relatively easier and more accu-   rate to map errors from XSumFaith , Goyal’21 , and   CLIFF to our unified error types as they have an-   notation granularity finer than sentence - level . We   encourage future work to follow this taxonomy   where possible and leverage definitions in prior   work to make cross - dataset comparisons possible .   Here also , we encourage future work to prioritize   annotation and evaluation of SOTA summaries .   Annotate and evaluate on non - news datasets   Most of current annotated datasets are within the   news domain and factuality metrics are evaluated   on news summaries accordingly . As there is a ris-   ing interest in other domains such as dialogue sum-   marization ( Tang et al . , 2022 ; Fabbri et al . , 2021a ;   Zhang et al . , 2021 ) , and medical evidence sum-   marization ( Tang et al . , 2023 ) , future work could   annotate and analyze errors made by SOTA models   there . We encourage future work to develop factu-   ality metrics that have superior performance over   cross - domain evaluation .   6 Conclusion   In this work , we analyzed several factuality metrics   across a large meta - benchmark assembled from ex-   isting datasets . We find state - of - the - art fine - tuned   summarization models still present challenges for   detecting factual errors , and the performance of   error detectors is often overestimated due to the   reliance on older datasets . Furthermore , we unify   existing datasets into a common taxonomy and use   this to highlight differences between datasets and   summarization models , as well as the complexity   of unifying concepts in this problem space.11634Limitations   There are a few limitations of our work . First , we   focus on evaluating state - of - the - art factuality met-   rics on English newswire datasets . This setting   restricts us to English - language data , a formal style   of text , and topics consisting of what is discussed in   US and UK - centric news sources . Moreover , other   summarization domains such as dialogue summa-   rization have different common error types such as   wrong reference error ( Tang et al . , 2022 ) , which   are not fully evaluated under current metrics . As   settings like this are studied in future work , we be-   lieve that the kinds of analysis we do here can be   extended to these settings as well .   Second , since our work is built on top of pre-   vious work , some analysis such as the error type   mapping is limited by the quality and annotation   agreement from previous work . We chose not to   undertake large - scale reannotation to avoid causing   confusion in the literature with multiple versions   of datasets reflecting divergent annotator opinions .   In spite of these limitations , we believe that our re-   evaluation of these metrics and the analysis of error   types under newswire data can bring insights for   future works in choosing , designing and evaluating   factuality metrics .   Acknowledgments   The UT Austin team on this work was supported by   a gift from Salesforce Inc. , NSF Grant IIS-1814522 ,   and a gift from Amazon .   References116351163611637   A Model Categories   In this section , we briefly describe the summariza-   tion models we use in this paper . ForFS , we include Transformer - based pre-   trained models like BART ( Lewis et al . , 2020 ) , T5   ( Raffel et al . , 2020 ) , and PEGASUS ( Zhang et al . ,   2020 ) . They are pre - trained on massive text corpus   and further fine - tuned on summarization datasets .   ForEX , we use BERTSumExt and   BERTSumAbs from Liu and Lapata ( 2019 ) , GPT-2   ( Radford et al . , 2019 ) , TransS2S ( Vaswani et al . ,   2017 ) , and BERTS2S ( Devlin et al . , 2019 ) .   ForO , we include models FastAbsRl ( Chen   and Bansal , 2018 ) , TConvS2S ( Narayan et al . ,   2018 ) , BottomUp ( Gehrmann et al . , 2018 ) , PGNet   ( See et al . , 2017 ) , NeuSUM ( Zhou et al . , 2018 ) ,   BanditSum ( Dong et al . , 2018 ) , SummaRuNNer   ( Nallapati et al . , 2017 ) , TextRank ( Mihalcea and   Tarau , 2004 ) , CBDec ( Jiang and Bansal , 2018 ) ,   RNES ( Wu and Hu , 2018 ) , ROUGESal ( Pasunuru   and Bansal , 2018 ) , ImproveAbs ( Kry ´ sci´nski et al . ,   2018 ) , MultiTask ( Guo et al . , 2018 ) , and Uni-   fiedExtAbs ( Hsu et al . , 2018 ) .   B Factuality Metrics   We show the descriptions of consistency metrics   we considered in our benchmark .   DAE ( Goyal and Durrett , 2020 ) propose an arc   entailment approach that evaluates the factual-   ityF(a , x ) = P(entailment |a , x)of each   dependency arc a∈Arc(s)of the generated   summary sindependently with respect to the   input article x. It then uses their aggregation / summationtextF(a , x)as the overall score .   We use the default model and hyperparameters pro-   vided by the authors , described in Goyal and Dur-   rett ( 2021 ) , which is trained on data from XSum-   Faith , which we account for later in our compar-   isons .   QuestEval ( Scialom et al . , 2021 ) propose a QA-   based metric that aggregates answer overlap scores   from selected spans rand questions q∈Q(x )   that derived from the input article xand answered   Q(s , q)using the summary s(recall - based ) ; and   those derived from the summary q∈Q(s )   and answered Q(x , q)using the input article x   ( precision - based ) . QandQdenote question   generation and question answering components , re-   spectively . We use the implementation provided by   the authorsand apply the unweighted version of11638the metric as in Laban et al . ( 2022 ) .   SummaC - ZS ( Laban et al . , 2022 ) is a zero - shot   entailment metric that computes a sentence - level   entailment score F(s , x)between each summary   sentence sand input sentence xusing an NLI   model F. It first find the maximum entailment   score score ( s ) = maxF(s , x)for each sum-   mary sentence s , and averaging over all summary   sentences for the final score / summationtextscore ( s ) . We   use the default model and hyperparameters pro-   vided by the authors , which may return a negative   score .   SummaC - Conv ( Laban et al . , 2022 ) extends   SummaC - ZS by replacing the max operation with   a binning of the entailment scores between each   summary sentence sand all input sentences x   to create a histogram hist(s , x ) . The histogram   is then passed through a learned 1 - D convolution   layer Conv to produce the summary sentence score   score ( s ) = Conv(hist ( s , x ) ) . Parameters for   the convolution layer are learned on synthetic data   from FactCC ( Kryscinski et al . , 2020 ) .   QAFactEval ( Fabbri et al . , 2021c ) is a QA - based   metric analogous to the precision - based compo-   nent of QuestEval and includes optimized question   answering , generation , and answer - overlap com-   ponents . We do not make use of the variation of   QAFactEval which combines QA and entailment-   based scores into a single metric .   ChatGPT - ZS ( Luo et al . , 2023 ) uses a zero - shot   template and directly asks for a binary label of   summary factuality .   Decide if the following summary is consis-   tent with the corresponding article . Note that   consistency means all information in the sum-   mary is supported by the article .   Article : [ Article ]   Summary : [ Summary ]   Answer ( yes or no ):   ChatGPT - CoT ( Luo et al . , 2023 ) also uses a   zero - shot template but invokes chain - of - thought   ( CoT ) style reasoning in its prompt . Similar to   ChatGPT - ZS , it directly asks for a binary factuality   label for a given summary .   Decide if the following summary is consis-   tent with the corresponding article . Note that   consistency means all information in the sum-   mary is supported by the article .   Article : [ Article]Summary : [ Summary ]   Explain your reasoning step by step then   answer ( yes or no ) the question :   ChatGPT - DA ( Wang et al . , 2023 ) uses a direct   assessment ( DA ) prompt template that asks to as-   sign a factual consistency score to a summary on a   continuous scale from 0 to 100 .   Score the following news summarization   given the corresponding news with respect   to consistency on a continuous scale from 0   to 100 , where a score of zero means “ incon-   sistency ” and score of one hundred means   “ perfect consistency ” . Note that consistency   measures whether the facts in the summary   are consistent with the facts in the original   article . Consider whether the summary does   reproduce all facts accurately and does not   make up untrue information .   Article : [ Article ]   Summary : [ Summary ]   Scores :   ChatGPT - Star ( Wang et al . , 2023 ) is an alter-   native version of ChatGPT - DA that asks LLMs to   score summaries on a scale of one - to - five .   Score the following news summarization   given the corresponding news with respect   to consistency with one to five stars , where   one star means “ inconsistency ” and five stars   means “ perfect consistency ” . Note that con-   sistency measures whether the facts in the sum-   mary are consistent with the facts in the orig-   inal article . Consider whether the summary   does reproduce all facts accurately and does   not make up untrue information .   Article : [ Article ]   Summary : [ Summary ]   Stars :   C Surveyed Error Types   Here are our surveyed error types that are related   to factual inconsistency .   Negation Error ( Zhang et al . , 2020 ; Kryscinski   et al . , 2020 ; Huang et al . , 2020 ; Zeng et al . , 2021 )   Adjective Error ( Zhang et al . , 2020 )   Coreference Error ( Zhang et al . , 2020 ; Kryscin-   ski et al . , 2020 ; Pagnoni et al . , 2021 ; Nan et al . ,   2021b)11639Number error ( Kryscinski et al . , 2020 ; Nan   et al . , 2021b ; Chen et al . , 2021 ; Cao et al . , 2020 )   Entity error ( Kryscinski et al . , 2020 ; Pagnoni   et al . , 2021 ; Zeng et al . , 2021 ; Wang et al . , 2020 ;   Nan et al . , 2021b , a ; Chen et al . , 2021 ; Cao et al . ,   2020 )   Attribute error ( Pagnoni et al . , 2021 ; Huang   et al . , 2020 )   Pronoun error ( Kryscinski et al . , 2020 ; Zeng   et al . , 2021 ; Cao et al . , 2020 )   Commonsense error ( Kryscinski et al . , 2020 )   Temporal error ( Kryscinski et al . , 2020 ; Cao   et al . , 2020 )   Predicate error ( Pagnoni et al . , 2021 )   Discourse link Error ( Pagnoni et al . , 2021 )   Relation error ( Nan et al . , 2021a , b )   Quantity error ( Zhao et al . , 2020 )   Event error ( Goyal and Durrett , 2021 ) ,   Noun phrase error ( Wang et al . , 2020 ; Goyal   and Durrett , 2021 ) ,   Circumstance error ( Pagnoni et al . , 2021)116401164111642ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.11643 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.11644