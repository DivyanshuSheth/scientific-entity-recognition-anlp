  Samuel Broscheit , Quynh Do , Judith Gaspers , Data and Web Science Research Group , University of Mannheim , GermanyAmazon Alexa AI   Abstract   In this study , we investigate robustness against   covariate drift in spoken language understand-   ing ( SLU ) . Covariate drift can occur in SLU   when there is a drift between training and test-   ing regarding what users request or how they   request it . To study this we propose a method   that exploits natural variations in data to create   a covariate drift in SLU datasets . Experiments   show that a state - of - the - art BERT - based model   suffers performance loss under this drift . To   mitigate the performance loss , we investigate   distributionally robust optimization ( DRO ) for   ï¬netuning BERT - based models . We discuss   some recent DRO methods , propose two new   variants and empirically show that DRO im-   proves robustness under drift .   1 Introduction   A common assumption in machine learning is that   training data and test data are independent and iden-   tically distributed ( i.i.d . ) . Unfortunately , this may   not hold in practice and the test distribution might   have drifted from the training distribution which   can lead to a signiï¬cant drop of performance in real-   world applications ( Moreno - Torres et al . , 2012 ) .   Consider spoken language understanding ( SLU ) ,   i.e. , the task of mapping an utterance to a ma-   chine readable semantic interpretation , which is   commonly used in voice controlled devices like   Alexa , Siri or Google Assistant . Distributional   drifts can be caused by seasonal and non - seasonal   factors . For example , festive holidays can lead   to many requests outside the daily routine . New   users might use an uncommon phrasing to express   their intent or they might request an uncommon   song to be played . Such drifts in the input distribu-   tion are referred to as covariate drift . When users â€™   requests fail to be recognized by the device they   might rephrase their intent until they succeed , es-   sentially adapting to the SLU model â€™s distribution .   This means that , even when new training samplesare drawn from new user utterances , the dominance   of the old distribution already present in the SLU   model is reinforced . Fine - tuned pre - trained lan-   guage models ( PLM ) , such as BERT ( Devlin et al . ,   2019 ) yield strong performance on SLU bench-   marks ( Chen et al . , 2019 ) . Yet , it has been observed   that also PLMs are vulnerable to drifts , and there is   a high interest in understanding the robustness of   PLMs ( Oren et al . , 2020a ; McCoy et al . , 2019 ; Tu   et al . , 2020 ; Cao et al . , 2020 ) .   The goals of this study are to investigate the   impact of covariate drift on BERT â€™s performance ,   and to experimentally investigate distributionally   robust optimization ( DRO ) for ï¬netuning BERT .   While we focus on sequence classiï¬cation for SLU ,   i.e. , intent classiï¬cation ( IC ) and slot ï¬lling ( SF ) ,   we expect the insights of this study to be applicable   also to other sequence classiï¬cation tasks .   To study the impact of covariate drift on model   robustness , we require a dataset with known proper-   ties of the drift . However , real data for this setting   is not publicly available . Therefore , we devised a   method to create a train / test split with a controlled   drift for sequence labeling data , which we call S - D . Roughly speaking , SD creates   clusters of examples based on the example â€™s to-   kens and sequence labels . Then those clusters are   used to create a new train / test split while leaving   the label distribution intact . Notably , SD   does not artiï¬cially alter the utterances and only   exploits natural lexical variations in the data in a   non - adversarial manner . Our experiments on pub-   licly available SLU datasets repartitioned with S - D showed that a state - of - the - art BERT - based   model for SLU ( Chen et al . , 2019 ) trained with   standard optimization suffers up to 5 % absolute   performance loss .   Currently , it is an open question which range of   measures are helpful to improve the generalization   under drift . In this study , we investigated distri-   butionally robust optimization ( DRO ) , which has1970recently gained interest in NLP for overparame-   terized models ( Oren et al . , 2019 ; Sagawa et al . ,   2020 ; Liu et al . , 2021 ; Michel et al . , 2021 ) . It is an   optimization concept which assumes that the train-   ing data is a mixture of distributions , e.g. , different   user demographics . The objective is to be optimal   under each distribution . For example , the methods   proposed by Oren et al . ( 2019 ) and Sagawa et al .   ( 2020 ) assume knowledge about groups of training   instances â€” such as topics or ethnic origin â€” that   can be used by the optimization . Roughly speak-   ing , they propose to compute the loss across groups   instead across individual instances . However , such   group knowledge might not be available and there   are other methods which do not require such prior   knowledge . T ( Levy et al . , 2020 ; Kawaguchi   and Lu , 2020 ) , for example , simply uses the top - k   largest losses in a batch and was shown to obtain   robust models .   We performed an extensive experimental analy-   sis to investigate the usefulness of several DRO   methods across different scenarios . Most stud-   ies only evaluate DRO methods in one setting   with in - distribution validation data and one drift   type per dataset . To achieve a broader insight   into the usefulness of the investigated methods   we evaluated them in 8scenarios per dataset , i.e. ,   for different types of drift , or model selection   with in - distribution and out - of - distribution vali-   dation data . Additionally , we propose an intu-   itive variant of T , namely T - G or   T - AE to investigate if prior group   knowledge or latent group knowledge could im-   prove T. We found that T , T - G   andT - AE can signiï¬cantly im-   prove robustness in many scenarios , where T   is more reliable in terms of signiï¬cant improve-   ment , while T - AE can be better   in terms of relative improvement .   2 Background   In this section , we provide a brief background to   spoken language understanding . Subsequently , we   discuss common categorizations of dataset drifts ,   empirical analyses of drifts and then describe dis-   tributionally robust optimization .   2.1 Spoken Language Understanding   In this study , we focus on SLU for single - turn ut-   terances and non - nested intents . Parsing utterances   into API calls is broadly either done by task ori - ented semantic parsing ( TOP ) , or as intent classi-   ï¬cation ( IC ) and slot ï¬lling ( SF ) . IC is the task   to classify an utterance into user intents , such as   PlayMusic , FindBook orGetWeather . Meanwhile ,   SF is a sequence tagging task to identify spans of   tokens that represent the intent â€™s slot ï¬llers , such   asArtistName , AlbumName orTrackNumber . In   state - of - the - art approaches , IC and SF are typically   modeled jointly using deep neural networks ( Chen   et al . , 2019 ) .   2.2 Distributional Drifts   A common assumption in machine learning is that   the training and test data are independent and   identically distributed ( i.i.d . ) and that the distri-   butions are the same between training and test , i.e.   P(x;y ) = P(x;y ) . Unfortunately , in prac-   tice , the test data is often out of distribution ( o.o.d . ) ,   i.e. P(x;y)6 = P(x;y ) . This can be caused   by sampling bias , such that subpopulations are not   equally represented in the samples of the two dis-   tributions . As this phenomenon is often caused   by time - varying covariates , i.e. , seasonal and non-   seasonal changes , this phenomenon is referred to   as adrift . However , this can be a general mismatch   between the sampled subpopulations , which can   be of geographic or demographic type , or they can   be topics or domains . Drifts can also be caused   by noise or automatic training data generation , in   which ï¬ltering heuristics introduce a systematic   issue , or by adversaries that exploit weaknesses of   a speciï¬c model or model class .   Distributional drifts can be categorized   into ( Moreno - Torres et al . , 2012 ): concept drift ,   i.e. , when the meaning changes , prior probability   drift and covariate drift .   Covariate drift . WhenP(x)6 = P(x ) ,   then there is a drift in the input distribution , and   when the concept P(yjx)does not change be-   tween training and testing , this is referred to as   covariate drift . The challenge is that the pop-   ulation and its subpopulations are unknown be-   cause only samples are observed and thus P(xjy )   can not be perfectly learned . For covariate drift   a model has to generalize to samples from sub-   populations that are almost unseen , e.g. , a spam   classiï¬er should generalize to a new spam cam-   paign ( x;y ) = ( word_in_email=â€œcheap â€ ,   is_spam = True ) while only observing ( x;y)=   ( word_in_email=â€œmoney â€ , is_spam = True ) . One of   the conjectures for transfer learning using PLMs is1971that task ï¬netuning PLMs can generalize to inputs   that are semantically similar to training instances .   One reason for a lack of robustness to covariate   drift is when models overï¬t on patterns between   the input and the desired labels , e.g. , when they   would learn that only â€œ cheap â€ is a predictor for   spam . Another reason can be spurious correlations   between patterns in the input and the label . Sagawa   et al . ( 2020 ) ; Tu et al . ( 2020 ) showed this for en-   tailment prediction . They grouped instances by a   certain input attribute ( does or does not contain   negation ) and target labels ( is or is not entailment ) .   The attribute did not have any direct relation to   the label . Then they partitioned the data in such a   way that there was a correlation between the polar-   ity of the attribute and the label in the training   data and an inverse correlation in the test data :   D = [ ( negation = + ; entailment =   ) ;   ( negation =  ;entailment = + ) ] andD=   [ ( negation = + ; entailment = + ) ] ; ( negation =    ;entailment =   ) ] . They showed that models   learn this correlation instead of the semantics of   the actual task and then fail on test instances .   2.3 Empirical Analyses of Drifts   Currently , there is a rising interest to investigate   distributional drifts in various domains ( Tu et al . ,   2020 ; Sagawa et al . , 2020 ; Koh et al . , 2021 ; Dunn   et al . , 2020 ; Shankar et al . , 2019 ; Oren et al . ,   2020b ) , most prominently in Computer Vision   ( CV ) , but also in NLP . To study distributional drifts ,   researchers need datasets with a controlled drift be-   tween training and test data . This can be broadly   achieved in two ways :   Synthetic methods . A dataset drift is created by   corrupting the input features with synthetic noise ,   for example , adding pixel noise ( Goodfellow et al . ,   2015 ) , perturbing the input with generative models   ( Dunn et al . , 2020 ) or perturbing characters and   words ( Cao et al . , 2020 ) . It has been observed   that robustness against synthetic noise does not   imply robustness against semantically meaningful   perturbations ( Koh et al . , 2021 ; Dunn et al . , 2020 ) .   Natural variations . Another option is to exploit   natural variations , for example , using video frames   for which a model â€™s object prediction ï¬‚ips between   adjacent frames ( Shankar et al . , 2019 ) . Koh et al .   ( 2021 ) collected a large benchmark for naturally   occuring drifts in CV and NLP , e.g. , user demo-   graphics for toxicity detection in online comments .   SÃ¸gaard et al . ( 2021 ) investigated the difference ofmodel performance comparing random splits with   heuristics like splitting the data based on sentence   length or by maximizing the divergence of the to-   ken feature vectors of the train and test split . In   this work , we exploit natural variations in the data   to create a drift in a non - adversarial manner . Our   conjecture is that this setting is a good proxy to a   realistic evaluation scenario .   2.4 Distributionally Robust Optimization   Robustness . The robustness of a machine learn-   ing model is the property that characterizes how   effective the model is while being tested on a new   dataset . In this paper , robustness is formally de-   ï¬ned as follows . Let Dbe a dataset split into   D;D;Dand letEbe a performance   measureE:D!R(w.l.o.g . greater is   better ) . We assume that there is a covariate drift   betweenD andD. Given two models A   andBwith parameters ;2estimated   onD. We call a model Bmore robust than   modelAwhenE(;D)>E(;D)and   E(;D) E(;D)<E(;D)    E(;D )   Empirical Risk Minimization ( E).Com-   monly used optimization algorithms assume that   all examples are from the same population . This   assumption stems from E â€™s optimization ob-   jective which treats each example in DP   with equal importance , i.e. , ^=infE[l(x;y; ) ] .   This optimization may have a negative impact on   model robustness . For example , Tu et al . ( 2020 )   found that using ERM for ï¬netuning PLMs learns   spurious correlations even in the presence of a few   helpful counter examples .   Distributionally Robust Optimization ( DRO ) .   DRO is based on the assumption that D con-   sists of samples from many subpopulations , i.e. ,   distributions Qfrom an uncertainty set U(x;y ) .   The objective in DRO is then to optimize the   parameters such that they are optimal under the   worst case distribution in U(x;y ) , i.e. , ^=   infsupE[l(x;y; ) ] . DRO is effective when   the proportions of the distributions Qare highly   skewed inD. For example , this can help to   avoid learning spurious correlations , because even   very few counter examples in the data are ampliï¬ed .   The challenge in applying the DRO concept is that   the subpopulations are not observable and U(x;y )   has to be modeled by some prior knowledge about   the data.1972   3 The SD Method to create   Covariate Drift Benchmarks   Our goal is to study the impact of covariate drift on   model performance . Therefore , we need a bench-   mark with controlled drift , but currently there are   no publicly available SLU benchmarks in which   real drifts can be studied . As motivated in Sec-   tion 2.3 , we do not want to employ synthetic noise ,   i.e. , our goal is to design a method that exploits nat-   ural variations in the data . Moreover , the method   should not be adversarial , i.e. , not designed or op-   timized to target a speciï¬c model or model class .   Instead , we target two semantic drifts that might   occur in real data due to : how users express their   intent , and what users request .   We conjecture that it is possible to capture how   users express themselves by creating clusters of   utterances with similar slot contexts . To capture   what users request could be achieved by clusters of   utterances with similar slot values . A drift can then   be created by partitioning the data based on those   clusters into training and testing . We avoid creat-   ing a mismatch of the label distributions between   training and testing . If a mismatch would occur , it   would not be possible to derive conclusions about   covariate drift from changes in performance be-   cause the shift in the label distribution also leads   to changes in the measured performance . In the   following , we describe our approach in detail .   3.1 Overview   The high - level overview for creating a drift dataset   version is as follows : ( i ) Join all splits from the   original data . ( ii ) Transform examples into feature   representations . ( iii ) Use spectral clustering to ob-   tainKclusters based on the feature representations .   ( iv ) Create the test split based on the clusters by   sampling clusters instead of sampling examples.3.2 Feature representation for clustering   We propose two variants of feature representations   to capture different drift types .   Slot value drift To cluster examples by â€œ what   users request " we chose the feature representation   of slot value n - grams . Table 1 shows an example   in which only the slot values ( the non - gray cells )   are used to generate n - grams for an utterance , e.g.   â€œ song â€ orâ€œtoo poetic â€ . The expected effect   of splitting the data based on clusters of examples   using this representation is that the training split is   missing certain slot values , and thus we encounter   unseen artists during testing .   Slot context drift The feature representation to   cluster training examples by â€œ how users express an   intent or slot " are n - grams of slot labels and the   tokens around them . For example , using only the   non - gray cells in Table 2 to generate n - grams would   yield â€œ add B - SONG by B - ARTIST â€ orâ€œto   my B - PLAYLIST â€ as features to represent the   example . The expected effect of this drift is that   the test data contains phrases which are not seen   during training .   3.3 Creating new train / valid / test splits   Now , using the feature representation for either   theslot value drift orslot context drift , we use   spectral clustering to create Kclusters and proceed   to create the data splits .   Test split . First the test split is created by sam-   pling clusters and all the clusters â€™ examples are   added to the test split . To avoid a mismatch of   the label distribution between training and the new   test split , the method uses a projected label count   per split to decide whether a cluster can be used .   For example , let â€™s assume we deï¬ned a 5 % test   split percentage and there are 1000 examples with   the intent - slot label PM -ARTIST . Then   the test split should have 50examples with the1973intent - slot label PM -ARTIST . Hence , a   cluster which contains 70examples with the intent-   slot label PM -ARTIST can not be used   for the test split because it would exceed the pro-   jected label count . Thus , when a cluster is sampled   allof its examples are added to the test split if they   do not disturb the projected label count . This is   repeated until all clusters have been sampled once   and have been added to the test split or not . When   the test split does not match the projected label   count , it is ï¬lled using random examples from clus-   ters that have not been used for test so far . These   examples do not count into the controlled drift .   Train and validation split . The training and val-   idation splits are created by sampling from the re-   maining examples .   3.4 Drift dataset variants   We also considered the following variations of the   proposed algorithm to measure various effects .   O.O.D. validation One variation is that the val-   idation data could be o.o.d . instead of distributed   like the training data . This is a hypothetical setting   in which we have access to o.o.d . data for valida-   tion and can observe to what degree hyperparam-   eter tuning and model selection do factor into the   drift effect . To achieve this we create the validation   data in the same way as the test data , but validation   and test do not share drift clusters .   Full drift and partial drift In the default behav-   iorallthe examples of a cluster are shifted into   the test split which we call a full drift . However ,   a natural question is what happens when a small   percentage of a test cluster leaks into training . We   call this setting a partial drift .   4 DRO for Overparameterized Models   In the experiments in Section 5 we will show that   using Eoptimization on the SD parti-   tioned datasets is not robust . There might be many   measures to mitigate this effect , and the best solu-   tion will most likely consist of a mix of methods .   One candidate is DRO that has seen a rising inter-   est to be applied to overparameterized models . In   the following , we ï¬rst brieï¬‚y discuss the setting of   ï¬netuning a pretrained language model ( PLM ) , and   subsequently we describe existing and proposed   DRO methods.4.1 Finetuning Pretrained Language Models   In our setup , a pretrained language model Mcon-   sists of a pretrained encoder ENC , and one ( or   more ) task classiï¬er head(s ) C. LetXbe a   batch of inputs of size b , then the hidden represen-   tations ofMare the output of the encoder X=   ENC ( X ) . For example , in our study we denote   the averaged hidden token representations of size   dafter the last layer of BERT as X2R.   To ï¬netune Mfor a new task , the parameters of   the encoder and the task classiï¬er heads are opti-   mized with a loss function Lto obtain the task   batch lossl = L(C;X)2R. The   following methods differ mainly in the way they   manipulate the task batch loss l.   4.2 Existing DRO methods   The following DRO methods are by no means ex-   haustive . They represent either methods proposed   so far in NLP or have a desirable property , e.g. ,   being simple or conceptually interesting . The main   differences between the methods is that they ei-   ther use or do not use group knowledge in their   objective . Those models that do require knowledge   about groups in the data will use the clusters cre-   ated by the SD algorithm . However , using   theSD clusters is somewhat artiï¬cial be-   cause this is perfect information . Therefore , we are   especially interested in methods that do not require   group knowledge .   T -CV This method was proposed by   Oren et al . ( 2019 ) for language modeling . They   use a topic model to obtain a distribution over top-   ics for each sentence to model the uncertainty set .   The core idea is to accumulate the losses for each   topic over the course of training . In each update a   subset of losses in lis selected , i.e. , the losses   of those batch items that are assigned to the topic   that currently lies in the upper  percentile of accu-   mulated losses .   G -D This method was proposed by   Sagawa et al . ( 2020 ) for data where groups are   known such that each example is assigned to one   group . Similar to T -CVtheir method keeps   statistics of the accumulated losses , but for groups   rather than for topics . In G -Dthe batch   losses inlare ï¬rst averaged per group and the   ï¬nal loss is a weighted average over group losses .   For batch construction their method upsamples   groups reciprocally to their frequency.1974Prec . Adapt . Impl .   T - AE ( our ) x   T - G ( our ) x   T x   G -D x   T -CV x   T This method does not require group   knowledge and is simple to implement : it sim-   ply computes the loss as an average over the top- k   largest losses in l(Levy et al . , 2020 ; Kawaguchi   and Lu , 2020 ) .   4.3 Our proposed variants   We found T to be very effective in initial ex-   periments . By contrast , G -DandT -   C did not perform well in our setting , even   though both have been shown to work well . Thus ,   we propose the following T variants :   T - G .If group information is avail-   able , can T be improved by it ? Here the idea   is â€” similar to T -C andG -D â€”   to use the precomputed SD clusters as   groups and compute the T loss per group . Then   only the largest T group loss is picked , which   has the effect of upsampling â€œ difï¬cult â€ groups and   downsampling â€œ easy â€ groups over the course of   training . However , when the precomputed S-   D clusters are used , this is more an oracle , i.e. ,   an upper bound of how much can be inferred from   the training data using perfect information .   T - A ( T - AE ) . What if   we do not have access to the precomputed clusters ?   Could we approximate them using the PLM â€™s hid-   den representations X ? Our idea is to use the   Xrepresentations to cluster the bbatch items   intoclatent groups . The latent groups are then   used in the loss computation like in T - G .   The clustering is obtained from an autoencoder   which is trained on Xand is continuously up-   dated during training . Thus , the group assignment   of a training example can change over the course   of training according to the model â€™s changing hid-   den representations . We investigated hard cluster   assignment T - AE - Band soft cluster assign-   ment T - AE - P. See Appendix B for all the   details regarding the autoencoder and its training . Discussion Table 3 compares the different meth-   ods discussed in this study , and shows if the method   relies on precomputed groups or if the groups are   implicit or adaptively inferred during training .   5 Experiments   In this section , we present our experiments to inves-   tigate the following questions : ( Q1 ) Does the stan-   dard optimization Esuffer a performance loss   under the SD covariate drift ? ( Q2 ) How   well can Eand DRO methods exploit a scarce   signal about the test distribution , i.e. , when is DRO   relevant ? ( Q3 ) As all optimization methods come   with hyperparameters , how much better could each   method perform with access to o.o.d . validation   data to optimize hyperparameters and perform early   stopping ? Would DRO still be better than E ?   ( Q4 ) Are the DRO methods more robust than E   against the SD covariate drift , and which   DRO method is the most effective ?   SLU Model . We use the JointBERT model for   SLU ( Chen et al . , 2019 ) . Two small changes that   we introduce are : ( i ) an intent loss scaler   for the   joint tasks loss L = L+   L and ( 2 ) us-   ing softmax layer instead of CRF for the sequence   tagging classiï¬er . We established the usefulness of   those two changes with a hyperparameter study .   5.1 SD Datasets   The source datasets for SD are four com-   monly used SLU benchmarks , which are listed in   Table 6 . All technical details and settings for S - D are discussed in Appendix A. Table 5   shows an excerpt from a cluster from the ATIS   dataset and demonstrates how the slot context drift   cluster contains examples with similar phrases , in   this case utterances with the phrase â€œ between   B-from.city and B-to.city â€ .   Use of datasets . To study robustness it is in-   evitable to look at test performance . Thus , we did   not use all datasets for all stages of experimenta-   tion : Prototyping of SD was only done on   ATIS , and then ï¬nal experiments with Ewere   done on all four datasets . The prototyping and ini-   tial experiments for the DRO methods were mostly   done on ATIS and a few trials on SNIPS . The ï¬nal   DRO experiments were conducted on SNIPS and   TOP.1975train valid test   i.i.d . o.o.d .   size % drift size % drift size % drift size % drift   SNIPSfullslot value 10,231 0 1,426 0 1,423 25 1,404 66   slot label 10,197 0 1,390 0 1,416 52 1,400 75   partialslot value 10,238 2 1,412 0 1,415 40 1,419 67   slot label 10,214 2 1,400 0 1,421 59 1,449 79   TOPfullslot value 16,145 0 2,295 0 2,257 23 2,253 44   slot label 16,131 0 2,227 0 2,251 65 2,341 45   partialslot value 16,128 2 2,272 0 2,294 26 2,256 45   slot label 15,940 3 2,255 0 2,316 65 2,439 47   All dataset scenarios In total we can evaluate a   method in eight different scenarios per dataset , i.e. ,   the cross product of   { slot value drift , slot context drift }    { partial drift , full drift }    { i.i.d . validation , o.o.d . validation } .   Table 4 shows the resulting statistics for the   datasets SNIPS and TOP - NN . The percentage of   examples resulting from a controlled drift in the   test set are 66 79 % for SNIPS and 44 47 % for   TOP . For the scenario with partial drift 2 - 3%of   the training data split belong to clusters that have   been deliberately shifted into the test split .   Metrics . We use the following metrics : F1- the   slot F1 metric ; A - the intent accuracy ;   C -IC - SF - the average of F1 and Accu-   racy .   Hyperparameters . To ensure a fair comparison   of methods in the experiments , we performed a   hyperparameter search with the objective to opti-   mize for C -IC - SF for each optimization   method for four scenarios { slot value drift , slot#int . # slot # int.-slot # examp .   ATIS 26 82 389 5871   TOP - NN 18 25 109 29104   MIT 2 18 20 21399   SNIPS 7 39 52 14484   context drift}{i.i.d . validation , o.o.d . validation }   with partial drift ( see Section 3.4 ) . For each set-   ting we ran 8 hyper - parameter optimization steps ,   then picked the two best hyper - parameter settings   and retrained them with a different random seed .   Then we reused the best hyper - parameters for the   full drift . See Appendix C and Table C.9 for the   remaining details about the hyperparameters.1976   5.2 Results   The reported results for each scenario are always   averaged from four models , i.e. , the models ob-   tained with the two best hyperparameter settings   that each have been trained with two random seeds .   We computed signiï¬cance with p<0:05between   models with approximate randomization ( Noreen ,   1989 ) . Due to the repetition with different random   seeds , this effectively results in a family - wise error   rate of 0:185 . In Figures 1b and 1c we count in how   many scenarios a DRO method was signiï¬cantly   better or worse than E. The remaining instances   performed the same as E.   ( Q1 ) Does the SD covariate drift lead to   a drop in performance for E?In Figure 1a ,   it can be observed that E â€™s performance does   drop up to 5 % in slot F1 between validation and   test . However , the amount of change varies be-   tween datasets . In most scenarios , slot F1 suffers   a higher drop in performance than intent accu-   racy . Slot context drift yields a higher loss than   theslot value drift , so it seems that it is easier   to generalize to unseen slot values than to unseen   phrases . This makes intuitively sense , e.g. , â€œ Please   play New Unknown Artist . â€ can be recognized   by just knowing the sequence â€œ please play   B - ARTIST I - ARTIST ... â€ , but it is more   difï¬cult to generalize to a new unseen phrase . See   Appendix D Table 10 for the numerical results .   ( Q2 ) How well can Eand DRO methods ex-   ploit a scarce signal about the test distribution ?   In Section 3.4 we described the partial drift , inwhich 2 3%of the training data are leaked ex-   amples from test clusters . Thus , during training   there is some information about test clusters that   could be exploited . For E , Figure 1a shows   indeed that the partial slot context drift leads to a   smaller drop in performance than the full slot con-   text drift . Thus , we conclude that Ecan exploit   this information .   For DRO , Figure 1b shows that there are less   scenarios with signiï¬cant improvement from DRO   methods over Ewith a partial drift than with   afull drift ( see numerical results Appendix D Ta-   ble 14 ) . It is important to note that all methods   â€” Eand DRO â€” do improve , but Eimproves   more than most of the DRO methods . Only T-   G andT still improve over E. Anec-   dotally , in a SD setting in which only 80 %   or less of the clusters are drifted into the test split   and the percentage of the drift cluster examples   make up more than 5 % of the training data , the   signiï¬cant improvement of all the DRO methods   vanishes .   ( Q3 ) Does o.o.d . validation data help hyperpa-   rameter optimization and early stopping ? In   Figure 1c , we observe that the amount of signiï¬-   ca nt improvement over Eshrinks when the vali-   dation data is o.o.d . and thus contains information   how to perform well for the test split . This affects   hyper - parameter optimization and early stopping   which also helps Eto obtain a model from the   training data that performs better on the test dis-   tribution . This can serve as an upper bound of   possible improvement that can be derived from the1977   training data alone . See Appendix D Table 13 for   the detailed numerical results .   ( Q4 ) Are the DRO methods more robust against   the drifts than E?Figure 2 shows in how   many scenarios the DRO methods improved signif-   icantly over Eon the SNIPS and TOP dataset .   T and T - G only improve signiï¬-   cantly over Eand do not perform worse . T-   AE - Bonly performs worse one time but oth-   erwise the same or better . The results indicate   thatT - based methods do improve robustness .   T - G performs best amongst all methods ,   i.e. , group information helps T - based methods .   T - AE - Bperforms slightly worse in terms of   signiï¬cant improvement than T , however , in   terms of average relative improvement over E   it is on par or better than T ( see Tables 11   and 12 in the Appendix ) . Yet , the lesser amount   of signiï¬cant improvement of T - AE - Bin   comparison to T - G shows that approxi-   mating the group information is difï¬cult . Without   perfect group information a simple method like   T might be the most reliable method to ob-   tain a robust model . See more detailed results in   Appendix D Table 11 and Table 12 .   Discussion G -D , T -CV and   T - G use the SD clusters in their   optimization . Therefore , these results should be   rather seen as an upper bound of how much can   be inferred from the training data using perfect in-   formation . Still , G -DandT -CV   both fail to perform well in this experiment . Note   that both methods had the same amount of budget   for hyper - parameter optimization as other methods .   ForG -Dwe used the authors â€™ publishedcodeand also their implementation of T -   CV . Our conjectures about this ï¬nding are : ( 1 )   G -DandT -CVboth have been   proposed and studied for groups that have much   higher lexical variance than the groups in our data .   The groups in our dataset consist by construction   of many examples with similar lexical patterns and   can be of small size , i.e. , as little as 10 examples .   This might explain why they seem to overï¬t heav-   ily . ( 2 ) Another difference to our methods is that   our proposed methods do not use an exponential   average of historical group loss statistics .   6 Conclusions   We studied ï¬netuning BERT for SLU datasets   with covariate drift . We presented the SD   method to induce a covariate drift for SLU se-   quence classiï¬cation tasks . The experimental re-   sults showed that this drift in the input distribu-   tion leads to a drop in performance on four SLU   datasets for a common BERT - based SLU model   ï¬netuned with E. We investigated DRO meth-   ods that either use or do not use knowledge about   groups in the data . Our empirical results in an ex-   tensive study indicate that T - based DRO meth-   ods are successful in improving robustness on the   drift datasets .   Acknowledgements   We would like to thank Rainer Gemulla , Patrick   Lehnen and ACL reviewers for helpful feedback   for revising and improving the paper .   References197819791980SNIPS TOP   Nr of clusters 100 100   Min . freq . of intents 150 150   Min . freq . of slots 50 50   Min . proj . size 10 10   n - gram min 2 2   max 6 5   Top freq . n - grams 10,000 10,000   Drift percentage Full 100 % 100 %   Partial 90 % 90 %   A Drift dataset creation   The procedure described in Section 3 has a range of   hyperparameters that we did set manually . Our goal   was to improve the clustering not in an adversar-   ial way . Thus those hyperparameter choices were   picked by inspecting the clusters and assessing if   they did display desired properties independently   of the downstream experiments . For example , the   number of clusters was set to 100 as shown in Ta-   ble 7 and was set large enough such that the clus-   tering algorithm did not have to mix clusters into   each other .   Clustering algorithm The clustering algorithm   we used was the spectral clustering imple-   mentation in . The feature vector   of an example was a weighted indicator vector over   the top most frequent n - grams in the dataset . If an   example contains the kth most frequent n - gram ,   then the kth component of this vector was set to   n , otherwise it was set to 0 . The effect of the n   weighting is to create a higher afï¬nity between   examples that share longer n - grams than shorter   n - grams . The afï¬nity matrix for the spectral   clustering was computed with cosine similarity .   Settings for the dataset The following settings   were used in the dataset creation in our experiments   and are listed in Table 7 : ( a ) The number of clusters .   ( b ) Thresholds to ï¬lter out low frequency intents   and low frequency slot label types , i.e. , the mini-   mum frequency of intent labels and slot labels in   the dataset . ( c ) The minimum size of projected   label counts that we attempt to match . If this pa-   rameter is set too low then many clusters might bediscarded because they would violate the projected   label count . Even when we did not match some of   the projected label counts we did achieve correla-   tions98 % between training and testing . ( d ) The   range of n - gram sizes . ( e ) How many of the top   most frequent n - grams will be used for the feature   vector of an example . ( f ) We either create a full   drift in which 100 % of the examples in a cluster are   assigned to the test split and a partial drift in which   we assign only 90 % of the examples in a cluster to   the test split .   Discussion Table 4 lists the statistics for the drift   versions of SNIPS and TOP . As can be seen in the   â€œ % drift â€ column of the train splits , the partial shift   leads to around 2 - 3 % of training data containing   examples from the clusters that have been shifted   into test . Our main objectives during the creation   of the drift splits was to shift entire clusters into the   splits and to match the intent distribution . We did   not constrain the amount of examples in the test   split that have been deliberately shifted into the test   split , i.e. , observe in Table 4 that the â€œ % drift â€ on   the validation and test splits varies from 23 - 79 % .   B T - A   In Algorithm 1 we present our proposed strategy   to train a BERT - based SLU model ( Chen et al . ,   2019 ) with T - AE . In the following we will   ï¬rst describe the autoencoder and its optimization   and then the training steps to train a model with   T - AE .   Autoencoder . The input to the autoencoder are   the averaged token representation Xeach batch   item . LetX2R , withbbeing the batch size   anddthe hidden size . Let the layers of the autoen-   coder be deï¬ned as A2RandA2R   withcbeing the size of the latent code , i.e. the   number of latent groups . The autoencoder is then   deï¬ned as :   H = softmax ( XA )   R = HA(1 )   Notably , we employ a softmax in the bottleneck H   such that the auto - encoder â€™s latent code is a distri-   bution overclatent groups . Ris the reconstruction   of the autoencoder â€™s input .   The auto - encoder is optimized with two losses :   i ) areconstruction loss , i.e. the cross entropy   loss of the objective that each reconstruction R   should be closer to its original input Xthan to1981Intent Type Slot Type Train Valid Test Test - Valid   DepartureTime B - Criterion 57 5 5 0   B - StationStart 62 6 6 0   B - Vehicle 61 5 6 1   FindConnection B - Criterion 11 3 3 0   B - StationStart 99 10 10 0   B - StationDest 106 11 11 0   Algorithm 1 Training SLU with Online Auto - encoder DROMis the main model ( SLU ) with two task losses ( slot and intent)are the main model â€™s parametersare the auto - encoder â€™s parametersfordata_batch ( X , Y ) in training_data doX = ENC ( X)l;l = compute the task losses of Mon(X;Y ) l = L ( X; ) +  L ( X;)=update ( ;rL ) ^l = compute group loss ( A;X;L ) ^l = compute group loss ( A;X;L ) =update ( ;r(^L+^L ) ) t = t+ 1end for   other batch items in X.denote the auto-   encoder â€™s parameters :   L ( X; ) =    1   bXlog(softmax ( XR))(2 )   For regularization we apply dropout to Rbefore   computing the reconstruction loss .   ii ) a diversity loss to prevent the auto - encoder   from collapsing into one mode , which is similar to   the loss used in T - SNE ( van der Maaten and Hinton ,   2008).(3 )   wheredenote the auto - encoder â€™s parameters   andKLthe Kullback - Leibler divergence .   This method adds the following hyper-   parameters for the autoencoder : size cof the   autoencoders bottleneck , learning rate and   weight decay  , and  a scalar for the   reconstruction loss .   We considered the following T - AE variants : T - AE - PThe bottleneck output of the au-   toencoder is H2[0;1 ] , i.e. a distribution over   cgroups for each batch item . Let denote ele-   mentwise multiplication along a matching mode .   Then ^H = Hl , i.e. ^H2Rare the losses   weighted according to each latent group . Instead   of averaging over all losses per latent group , ^His   truncated to the top- klargest weighted losses per   group , i.e. ^H2Rand then averaged per group   to yield ^l2R. The ï¬nal batch loss is max(^l ) .   T - AE - B Convert H into one - hot distribu-   tions , i.e. hard assignments to a latent group for   each batch item , then proceed like in T - AE-   P.   Algorithm . The model is ï¬netuned for two task   losses , one for the intent classiï¬cation task and   one for the sequence tagging classiï¬er for the slot   ï¬lling task . While it would be possible to use a   separate autoencoder for each task , we found it   beneï¬cial to share one autoencoder for both task   losses .   One update step is as follows : ( i ) For each batch   during training , ï¬rst the auto - encoder â€™s parameters1982are updated . ( ii ) Subsequently , we compute the   group losses for the two SLU â€™s tasks ( i.e. slot and   intent ) based on H , i.e. the latent groups . This   yields a vector ^lof loss - aggregations over the   latent groups . H2Ris a distribution over   cgroups for each batch item . ^l = Hl   is the vector of group losses in which each batch   item is weighted according to the autoencoder â€™s   distribution over latent groups . In other words , each   component in ^lcontains the accumulated losses of   all batch items that the autoencoder considers to   be similar . Finally , we update the SLU model â€™s   parameters using the task group losses .   C Hyperparameters   See Table 9 for a detailed list for all hyperparame-   ters and their search range . The hyper - parameters   that were tuned for all methods are the learning   rateand the intent loss scaler   . Each optimiza-   tion method can have additional hyper - parameters :   G -D(Sagawa et al . , 2020 ) has a step size   to compute the exponential average of group losses .   As we discussed in Section 5.2 we observed over-   ï¬tting of the G -Dmethod and not pro-   ducing good results on many occasions . We did   attempt to address this and added a geometric de-   cay of the exponential average as an option in the   hyperparameter search , which did help a little bit .   T -CV(Oren et al . , 2019 ) has the CVaR   percentage and also a step size for the exponential   average of losses . The batch size , weight decay ,   maximum number of epochs and the intent loss   scaler ( see 5.1 ) were determined in a prior larger   hyperparameter search . We did not ï¬nd a lot of   variance for their preferred setting , also not in inter-   play with the other DRO methods , which is why we   ï¬xed them to save computation from this point on .   See Section 4.2 and 4.3 for the hyperparameters   ofT - AE andT - AE - P / Brespectively .   Anecdotally the hyper - parameter kdetermining the   topk losses in their objective which was tuned for   T andT - AE - P / Btypically ended up   in the lower regions of the range , i.e. between 2 8 .   D Results   In the following tables we report the numerical   results for the experiments from Section 5 with the   metrics reported in Section 5.1 . Additional metrics   we report here is the macro average over intents , i.e.   " MA INT . COMBINED " , and S ( semanticerror rate ) - a metric which is deï¬ned as follows :   SEMER = # ( slot+intent errors )   # slots in reference + 1(4 )   The columns containing a " % " indicate relative   change to E.1983Optimization description name type range   E+ all learning rate  loguniform 1.e-5 - 1.e-4   intent loss scaler    loguniform 1.e-2 - 10.0   batch size ï¬xed 32   weight decay ï¬xed SNIPS : 0.02 , TOP : 0.002   max epochs ï¬xed 100   max warmup steps ï¬xed 0   T -CV alpha uniform 1.e-4 - 0.5   gamma loguniform 1.e-4 - 0.5   G -D step size loguniform 1.e-4 - 1 .   geometric decay categorical True , False   T topk k logint 1 - 16   T - AE - P / B topk k logint 2 - 16   ae learning rate  loguniform 1.e-4 - 1.e-3   ae cluster loss weight   loguniform 1.e-1 - 1.0   ae cluster size c int 128 , 256 , 512   T - G topk k logint 2 - 16   full shift partial shift   slot value slot context slot value slot context   Acc F1 Acc F1 Acc F1 Acc F1   ATIS -0.3 -0.7 -1.7 0 -0.3 -1.3 -0.7 -0.1   SNIPS -0.2 -3.7 0.0 -5.2 -0.1 -3.2 -0.2 -3.9   MIT -0.2 -2.0 -2.6 -2.9 0 -2.2 -3.2 -3.4   TOP -1.1 -2.7 -1.2 -5.2 -1.2 -2.6 -0.8 -3.7   COMBINED SIGN . 0.05 E SEMER MA INT . COMBINED   % < > % %   G -D 91.8 -0.6 6 1 11.8 4.8 85.7 -1.3   T - AE - P 91.9 -0.4 5 1 11.7 3.4 86.7 -0.1   T -CV 92.1 -0.2 4 0 11.5 2.0 87.1 0.3   E 92.3 - - - 11.3 - 86.8 -   T 92.5 0.1 0 3 11.0 -2.7 87.2 0.5   T - AE - B 92.5 0.2 0 3 11.0 -2.7 87.5 0.8   T - G 92.5 0.2 0 5 11.0 -2.4 87.5 0.81984COMBINED SIGN . 0.05 E SEMER MA INT . COMBINED   % < > % %   G -D 96.2 -0.2 5 0 7.1 7.8 96.0 -0.2   T -CV 96.4 0.0 2 1 6.6 1.2 96.2 0.0   E 96.4 - - - 6.6 - 96.2 -   T - AE - P 96.5 0.0 1 2 6.6 -0.1 96.3 0.0   T 96.6 0.1 0 4 6.3 -3.7 96.4 0.2   T - AE - B 96.7 0.2 1 2 6.2 -5.4 96.5 0.2   T - G 96.7 0.2 0 4 6.2 -6.0 96.5 0.3   validation i.i.d . validation o.o.d .   0.05 E 0.05 E   % CMB.% SEM . < > % CMB.% SEM . < >   G -D -0.2 5.8 2 0 G -D -0.3 9.7 3 0   E - - - - T - AE - P -0.1 5.4 1 0   T - AE - B 0.1 -2.6 1 1 T -CV -0.1 3.7 2 0   T -CV 0.1 -1.2 0 1 E - - - -   T - AE - P 0.2 -5.5 0 2 T 0.1 -1.5 0 1   T 0.2 -5.8 0 3 T - G - D 0.2 -5.6 0 2   T - G - D 0.3 -6.4 0 2 T - AE - B 0.4 -8.3 0 1   full drift partial drift   0.05 E 0.05 E   % CMB.% SEM . < > % CMB.% SEM . < >   G -D -0.2 5.2 1 0 G -D -0.3 10.6 4 0   E - - - - T -CV -0.1 4.2 2 0   T - AE - P 0.1 -1.4 1 1 T - AE - P 0 1.4 0 1   T -CV 0.1 -1.5 0 1 E - - - -   T 0.2 -3.9 0 3 T 0.1 -3.4 0 1   T - AE - B 0.2 -5.3 0 2 T - G - D 0.2 -5.3 0 2   T - G - D 0.3 -6.7 0 2 T - AE - B 0.2 -5.6 1 01985