  Wei Chen , Yeyun Gong , Can Xu , Huang Hu , Bolun Yao , Zhongyu Wei ,   Zhihao Fan , Xiaowu Hu , Bartuer Zhou , Biao Cheng , Daxin Jiangand Nan DuanSchool of Data Science , Fudan University , ChinaMicrosoft Research Asia , ChinaNanjing University of Science and Technology , ChinaResearch Institute of Intelligent and Complex Systems , Fudan University , China   { chenwei18,zywei,fanzh18}@fudan.edu.cn;{yegong,caxu,huahu}@microsoft.com ,   { xiaowuhu , bazhou,bicheng,djiang,nanduan}@microsoft.com ; yaobl001@njust.edu.cn   Abstract   We study the problem of coarse - grained re-   sponse selection in retrieval - based dialogue sys-   tems . The problem is equally important with   fine - grained response selection , but is less ex-   plored in existing literature . In this paper , we   propose a Contextual Fine - to- Coarse ( CFC )   distilled model for coarse - grained response se-   lection in open - domain conversations . In our   CFC model , dense representations of query ,   candidate contexts and responses is learned   based on the multi - tower architecture using con-   textual matching , and richer knowledge learned   from the one - tower architecture ( fine - grained )   is distilled into the multi - tower architecture   ( coarse - grained ) to enhance the performance   of the retriever . To evaluate the performance   of the proposed model , we construct two new   datasets based on the Reddit comments dump   and Twitter corpus . Extensive experimental re-   sults on the two datasets show that the proposed   method achieves huge improvement over all   evaluation metrics compared with traditional   baseline methods .   1 Introduction   Given utterances of a query , the retrieval - based di-   alogue ( RBD ) system aims to search for the most   relevant response from a set of historical records of   conversations ( Higashinaka et al . , 2014 ; Yan et al . ,   2016 ; Boussaha et al . , 2019 ) . A complete RBD   system usually contain two stages : coarse - grained   response selection ( RS ) and fine - grained response   selection ( Fu et al . , 2020 ) . As shown in Figure 1 ,   in coarse - grained RS stage , the retriever identifies   a much smaller list of candidates ( usually dozens )   from large - scale candidate database ( up to millions   or more ) , then the ranker in fine - grained RS stage   selects the best response from the retrieved candi-   date list . Figure 1 : A common structure of retrieval - based dia-   logue system , where coarse - grained RS provides a much   smaller ( Mâ‰ªN ) candidate set for fine - grained RS .   QYandCand are the abbreviations of query andcandi-   date respectively .   Recent studies ( Whang et al . , 2020 ; Xu et al . ,   2020 , 2021 ; Whang et al . , 2021 ) pay more attention   on fine - grained RS and various complex models   are proposed to compute the similarities between   the query and candidates for response selection .   Although promising improvements have been re-   ported , the performance of fine - grained stage is   inevitably limited by the quality of the candidate   list constructed . Therefore , a high - quality coarse-   grained RS module is crucial , which is less ex-   plored in existing literature ( Lan et al . , 2020 ) .   In this paper , we focus on the task of coarse-   grained response selection , i.e. , dialogue response   retrieval . There are two major challenges . First , dif-   ferent from general text matching tasks such as ad-   hoc retrieval ( Hui et al . , 2018 ) or question answer-   ing ( QA ) retrieval ( Karpukhin et al . , 2020 ) , key-   words overlapping between context and response   in dialogue are potentially rare , such as when a   topic transition ( Sevegnani et al . , 2021 ) occurs in re-   sponse . This makes it difficult to directly match the   query with candidate responses . Second , compared   with fine - grained RS , coarse - grained RS deals with   much larger number of candidates . Therefore , it is   impractical to apply complex matching model that   jointly process query and response for the similar-   ity computation like in fine - grained RS , due to the   retrieval latency ( traverse millions of candidates on-4865line ) . Instead , the efficient BM25 system ( Robert-   son and Zaragoza , 2009 ) based on sparse repre-   sentations is the mainstream algorithm in coarse-   grained text matching .   To mitigate the above mentioned two problems ,   we propose a Contextual Fine - to- Coarse ( CFC )   distilled model for coarse - grained RS . Instead of   matching query with response directly , we propose   a novel task of query - to - context matching in coarse-   grained retrieval , i.e. contextual matching . Given a   query , it is matched with candidate contexts to find   most similar ones , and the corresponding responses   are returned as the retrieved result . In this case , the   potential richer keywords in the contexts can be uti-   lized . To take the advantage of complex model and   keep the computation cost acceptable , we distillate   the knowledge learned from fine - grained RS into   coarse - grained RS while maintaining the original   architecture .   For the evaluation , there is no existing dataset   that can be used to evaluate our model in the setting   of contextual matching , because it needs to match   context with context during training , while positive   pairs of context - context is not naturally available   like context - response pairs . Therefore , we con-   struct two datasets based on Reddit comment dump   and Twitter corpus . Extensive experimental results   show that our proposed model greatly improve the   retrieval recall rate and the perplexity and relevance   of the retrieved responses on both datasets .   The main contributions of this paper are three-   fold : 1 ) We explore the problem of coarse - grained   RS in open domain conversations and propose a   Contextual Fine - to - Coarse ( CFC ) distilled model ;   2 ) We construct two new datasets based on Reddit   comment dump and Twitter corpus , as a new bench-   mark to evaluate coarse - grained RS task ; 3 ) We   construct extensive experiments to demonstrate the   effectiveness and potential of our proposed model   in coarse - grained RS .   2 Related Work   Fine - grained Response Selection In recent   years , many works have been proposed to improve   the performance of fine - grained selection module   in retrieval - based chatbots ( Zhang et al . , 2018 ;   Zhou et al . , 2018 ; Tao et al . , 2019 ; Whang et al . ,   2019 ; Yuan et al . , 2019 ) . Owing to the rapid devel-   opment of pre - trained language models ( PLMs )   ( Radford et al . , 2019 ) , recent works ( Gu et al . ,   2020 ; Whang et al . , 2021 ; Sevegnani et al . , 2021)achieve the state - of - the - art ( SOTA ) results by uti-   lizing PLMs such as BERT ( Devlin et al . , 2018 )   to model cross - attention and complex intersection   between the context and response .   Coarse - grained Response Selection On the   other hand , coarse - grained dialogue retrieval is an   important but rarely explored field . Limited by ef-   ficiency , there are usually two methods for coarse-   grained response selection , i.e. , the sparse repre-   sentations based method represented by BM25   ( Robertson and Zaragoza , 2009 ) , and the dense   representations based method represented by dual-   Encoder ( Chidambaram et al . , 2018 ; Humeau et al . ,   2019 ; Karpukhin et al . , 2020 ; Lan et al . , 2020 ; Lin   et al . , 2020 ) .   3 Method   In coarse - grained response selection , there is a   fixed candidate database containing a large num-   ber of context - response pairs . Formally , given a   query , i.e. , a new context , the goal is to retrieve   Top - K most suitable responses for the query from   the candidate database .   We propose a contextual fine - to - coarse distilla-   tion framework for the task of coarse - grained RS .   First , we formulate the problem as a task of con-   textual matching , i.e. , match query with context   instead response ; Second , we utilize a multi - tower   architecture to deal with the similarity computa-   tion of query and candidates in contextual match-   ing ; Third , we utilize knowledge distillation to   leverage the deep interaction between query and   response learned in one - tower architecture .   3.1 Contextual Matching   An intuitive idea of coarse - grained RS is to treat all   responses as candidate documents and directly use   query to retrieve them , while this non - contextual   approach results in a quite low retrieval recall rate   ( Lan et al . , 2020 ) . Inspired by recent studies of   context - to - context matching in fine - grained RS ( Fu   et al . , 2020 ) , we propose contextual matching in   coarse - grained RS , which is to match the query   with candidate contexts , and return the responses   corresponding to the most similar contexts . We   consider three ways of contextual matching .   Query - Context ( QC ) In QC matching , we treat   contexts instead of responses as candidate docu-   ments . At run - time , we calculate the similarities   between query and candidate contexts , and the re-4866   sponses corresponding to the Top - K most similar   contexts are returned as the retrieved results . The   motivation of using QC matching is similar con-   texts may also share similar responses .   Query - Session ( QS ) Asession represents the   concatenated text of context and corresponding re-   sponse ( Fu et al . , 2020 ) , which we think is more   informative than context alone . In QS matching ,   we treat sessions as candidate documents and re-   turn the responses in Top - K most similar sessions   as the retrieved results .   Decoupled Query - Session ( DQS ) Apart from   QS matching , we also consider a decoupled way   to match query with candidate sessions . In DQS   matching , we treat contexts and responses as inde-   pendent candidate documents . Similarities between   query and contexts , query and responses are first   calculated independently , then the query - session   similarity can be obtained by the weighted sum .   QS and DQS matching are actually two different   ways to calculate query - session similarity .   3.2 Multi - Tower Architecture   For the retriever to search large - scale candidates   with low latency , neural - based retrievers are usu-   ally designed as ( or limited to ) multi - tower archi-   tecture ( Figure 2 ) . In multi - tower models , the   query and the candidates are independently mapped   to a common vector space by different encoders ,   where similarity can be calculated . After training ,   the embeddings of large - scale candidates can be   pre - calculated offline , and only the embedding of   query needs to be calculated online . In this way ,   fast sublinear - time approximation methods such as   approximate nearest neighbor search ( Shrivastava   and Li , 2014 ) can be utilized to search for Top - K   vectors that are most similar to the query , whichcan achieve an acceptable retrieval latency during   inference .   3.2.1 Two - Tower Model   For QC and QS matching , two - tower architecture is   adopted . Taking QS matching as an example ( Fig-   ure 2(a ) ) , the dense session encoder E(Â·)maps   any candidate session to real - valued embedding   vectors in a d - dimensional space , and an index is   built for all the Nsession vectors for retrieval . At   run - time , a different dense query encoder E ( Â· )   maps the query to a d - dimensional vector , and re-   trieves kcandidate sessions of which vectors are   the closest to the query vector . We use the dot   product of vectors as the similarity between query   and candidate session following ( Karpukhin et al . ,   2020 ) .   3.2.2 Three - Tower Model   For DQS matching , dense representations of query ,   context and response are independently calculated ,   the architecture is thus designed as three - tower   with three encoders , which is query encoder E ( Â· ) ,   context encoder E(Â·)andresponse encoder E ( Â· )   ( Figure 2(b ) ) . Similarly , context and response vec-   tors are calculated and cached offline respectively   and two indexes are built for retrieving them . The   final similarity of query and session is weighted   by the dot product of query - context and query-   response . The weighting coefficient Î»can be ad-   justed to determine whether it is biased to match   the context or match the response .   3.2.3 Training Multi - Tower Model   We unify the training of the two - tower and three-   tower models by formalizing them into a same met-4867ric learning problem ( Kulis et al . , 2012 ) . The goal   is to learn a matching space where similarities be-   tween positive pairs is higher than negative ones ,   by learning a better embedding function . We use   the training of three - tower model ( DQS matching )   as an example . Formally , we denote the training set   asD={q,{k , k } } . Each training instance   contains a query q , a set of positive examples k   and a set of negative examples k. Among them ,   kcontain several positive contexts and several   positive responses , similarly , kcontain several   negative contexts and several negative responses .   We optimize the loss function as the sum of nega-   tive log likelihood of all positive pairs simultane-   ously :   L(q ) = âˆ’logPe   Pe(1 )   where the similarity function is defined as :   sim(q , k ) = E(q)Â·E(k ) . ( 2 )   The embedding function E(Â·)ofkin Equation   2 can be E(Â·)orE ( Â· ) , depending on the type of   k.   Positive and negative examples The core is-   sue of training multi - tower models for contextual   matching is to find positive pairs of query - context   ( or query - session ) . In this paper , we assume that   contexts with exactly the same response are pos-   itive samples of each other , which is a cautious   but reliable strategy . Formally , given a response   r , if there are multiple contexts whose response   isr , then we can randomly selected one context   as the query q , and the other contexts are positive   contexts ofq , and ris the positive response ofq .   Negative samples of contexts and responses can be   obtained from in - batch ( Karpukhin et al . , 2020 ) or   random sampling from database . Similarly , pos-   itive query - session is obtained by replacing the   context in positive query - context with the whole   session .   3.3 Distillation from One - Tower Model   In multi - tower architecture , the query and candi-   dates are expressed by their embeddings indepen-   dently , which may cause the loss of information ,   and their monotonous way of interaction ( inner   product ) further limits the capability ( Lin et al . ,2020 ) . Comparing with multi - tower model , one-   tower model takes both the query and the candidate   as a concatenated input and allow the cross atten-   tion between query and candidate in self - attention   layer . Despite fewer parameters , one - tower model   have been shown to learn a more informative rep-   resentations than multi - tower model , thus it is pre-   ferred in fine - grained RS ( Yang and Seo , 2020 ) .   To leverage the richer expressiveness learned by   the one - tower model , knowledge from one - tower   model is distilled into multi - tower model to en-   hance the retriever .   3.3.1 Training One - Tower Model   Before distillation , we need to train teacher mod-   els based on one - tower architecture . Let â€™s take the   training of teacher model for QS matching as an   example . A single encoder is trained to distinguish   whether the query and the session are relevant ( pos-   itive ) , and the form is exactly same as the next   sentence prediction ( NSP ) task in the BERT ( De-   vlin et al . , 2018 ) pre - training . Formally , given a   training set D={q , s , l } , where qis the   query , sis the candidate session and lâˆˆ { 0,1 }   denotes whether qandsis a positive pair . To be   specific , given a query qand candidate session s ,   the encoder obtains the joint representation of the   concatenated text of qands , and then computes the   similarity score through a linear layer , the training   objective is binary cross entropy loss .   We summarize the main difference between   one - tower and multi - tower as follows : one - tower   model is more expressive , but less efficient and can-   not handle large - scale candidates . The main reason   is that feature - based method of calculating similar-   ity scores rather than inner product limits the capa-   bility of offline caching . For new queries , the simi-   larities with all candidates can only be calculated   by traversal . The huge latency makes it impossible   to use one - tower model in coarse - grained response   retrieval . To leverage the expressiveness of one-   tower model , we propose fine - to - coarse distillation ,   which can learn the knowledge of one - tower model   while keeping the multi - tower structure unchanged ,   thereby improving the performance of the retriever .   3.3.2 Fine - to - Coarse Distillation   Take the two - tower student model ( denoted as S )   for QS matching as an example , suppose we have   trained the corresponding one - tower teacher model   ( denoted as T ) . For a given query q , suppose there   are a list of sessions { s , s , ... , s}and the cor-4868responding label y={1,0 , ... , 0 } âˆˆ R , that   is , one positive session and nnegative sessions .   We denote the similarity score vector of query-   sessions computed by student model S(Equation   2 ) aszâˆˆ R , then the objective of Equation 1   is equivalent to maximizing the Kullback â€“ Leibler   ( KL ) divergence ( Van Erven and Harremos , 2014 )   of the two distributions : softmax ( z)andy , where   softmax function turns the score vector to proba-   bility distribution .   The one - hot label ytreats each negative sample   equally , while the similarity between query with   each negative sample is actually different . To learn   more accurate labels , we further use teacher model   Tto calculate the similarity score vector between   qandS , denoted as zâˆˆ R. We then replace   the original training objective with minimizing KL   divergence of the two distributions softmax ( z )   andsoftmax ( z)(Figure 1 ) , where the tempera-   ture parameter is applied in softmax function to   avoid saturation .   The method of fine - to - coarse distillation is to   push the student model ( multi - tower ) to learn the   predicted label of teacher model ( one - tower ) as a   soft target instead of original one - hot label . By   fitting the label predicted by the teacher model ,   the multi - tower model can learn a more accurate   similarity score distribution from the one - tower   model while keeping the structure unchanged .   4 Datasets Construction   To evaluate the performance of the proposed model ,   we construct two new datasets based on the Reddit   comments dump ( Zhang et al . , 2019 ) and Twitter   corpus . We create a training set , a multi - contexts   ( MC ) test set and a candidate database for Reddit   and Twitter respectively . For Reddit , we create an   additional single - context ( SC ) test set . The motiva-   tion for these settings is explained in Â§ 5.3 . The size   of our candidate database is one million in Twit-   ter and ten million in Reddit respectively , which   is very challenging for response retrieval . Table 1   shows the detailed statistics . We use exactly the   same steps to build dataset for Reddit and Twitter ,   and similar datasets can also build from other large   dialogue corpus in this way .   MC test set We first find out a set of responses   with multiple contexts from candidate database , de-   noted as R. For each response rinR , we randomly   select one context cfrom its all corresponding con-   textsCto construct a context - response ( CR ) pair ,   and put the others contexts ( denoted as C ) back   to the database . Our MC test set consists of these   CR pairs . Each response in MC test set has multi-   ple contexts , which ensures that there exits other   contexts in the database that also correspond to   this response , so the retrieval recall rate can be   computed to evaluate the MC test set .   SC test set We create another test set ( SC ) for   Reddit dataset . Contrary to the MC test set , each   response in SC test set has only one context , i.e. ,   there is no context in the database that exactly cor-   responds to the response . Obviously , the retrieval   recall rate is invalid ( always zero ) on SC test set .   We introduce other methods to evaluate SC test set   in Â§ 5.2 . The SC test set is a supplement to the MC   test set which can evaluate the quality of retrieved   responses given those â€œ unique " contexts .   Candidate database To adapt to different re-   trieval methods , the candidate database is designed   with 4 fields , namely context , response , session .   Our candidate database consists of random context-   response pairs except those in the MC and SC test   sets . Besides , as mentioned above , those unse-   lected context - response pairs ( C ) are deliberately   merged into the database .   Train set The construction of training set is   intuitive and similar to test set . It consists of   responses and their corresponding multiple con-   texts . Formally , the training set can be denote as   D={r , c , ... , c},ris a response and   { c , ... , c}are all contexts with response r ,   where qdepends on r , andqâ‰¥2 .   It is worth noting that there is no overlap be-   tween the contexts in the database and the contexts   in the training set , which may prevent potential   data leakage during training process to overesti-   mate the evaluation metrics . The details of dataset   construction are introduced in Appendix A.48695 Experiments   We conduct extensive experiments on the con-   structed datasets . In this section , we present ex-   perimental settings , evaluation metrics , model per-   formance , human evaluation , etc . to demonstrate   the effectiveness of the proposed models .   5.1 Compared Models   For baselines , we select BM25 ( Robertson and   Zaragoza , 2009 ) as sparse representations based   method , which is widely used in real scenarios in   text matching . Based on BM25 system and the two   matching methods ( QC and QS matching ) , two re-   trievers can be obtained , denoted as BM25 - QC and   BM25 - QS respectively . We choose multi - tower   models as dense representations based methods .   They are bi - encoder based two - tower models for   QC matching and QS matching ( denoted as BE-   QC and BE - QS ) , and tri - encoder based three - tower   model for DQS matching ( denoted as TE - DQS ) . In   addition , to demonstrate the advantages of contex-   tual matching , we also report the results of query-   response ( QR ) matching , two retrievers are build   based on BM25 system and two - tower model ( de-   noted as BM - QR and BE - QR ) .   There are three variants of our proposed CFC   models , they are the distilled versions of BE - QC ,   BE - QS and TE - DQS , which are called CFC - QC ,   CFC - QS and CFC - DQS respectively . The distil-   lation of each student model needs to train the   corresponding teacher model . In particular , the   distillation from TE - DQS to CFC - DQS requires   two teacher models , because the similarity between   both query - context and query - response needs to be   calculated .   We summarize the details of compared models   and provide training details in Appendix B.   5.2 Evaluation Metrics   Following previous work ( Xiong et al . , 2020 ;   Karpukhin et al . , 2020 ) , Coverage@K is used   to evaluate whether Top - K retrieved candidates   include the ground - truth response . It is equiva-   lent to recall metric R@Kthat often used in   fine - grained RS , where Nis the size of candidate   database . However , Coverage@K is only suitable   for evaluating the MC test set , and it is incapable   for evaluating the overall retrieval quality due to   the one - to - many relationship between context and   response . As a supplement , we propose two auto-   mated evaluation metrics based on pre - trained mod - els , i.e. , Perplexity@K andRelevance@K . For re-   trieved Top - K responses , DialogGPT ( Zhang et al . ,   2019 ) is used to calculate the conditional perplexity   of the retrieved response given the query . Dialog-   GPT is a language model pre - trained on 147 M   multi - turn dialogue from Reddit discussion thread   and thus very suitable for evaluating our created   Reddit dataset . Perplexity@K is the average per-   plexity of Top - K retrieved responses . In addition to   Perplexity , we also evaluate the correlation between   the query and retrieved response . We use Dialo-   gRPT ( Gao et al . , 2020 ) , which is pre - trained on   large - scale human feedback data with the human-   vs - rand task that predicts how likely the response   is corresponding to the given context rather than   a random response . Relevance@K is the average   predicted correlation degree between query and   Top - K retrieved responses . Perplexity@K and Rel-   evance@K are average metrics based on all Top - K   retrieved responses , so they can reflect the overall   retrieval quality .   5.3 Overall Performance   We demonstrate the main results in Table 2 and Ta-   ble 3 and discuss model performance from multiple   perspectives .   Dense vs. sparse It can be seen that the per-   formance of dense retrievers far exceed that of the   BM25 system , which shows rich semantic informa-   tion of PLMs and additional training can boost the   performance of the retriever . For example , com-   pared with BM25 system , the best undistilled dense   retrievers ( BE - QS ) have a obvious improvement   in three metrics . For Coverage@K , the Top-500   recall rate of BE - QS on the MC test set of Reddit   and Twitter increase by 12.1 % and 17.4 % absolute   compared with BM25 - QS . For Perplexity@K , the   Top-20 average perplexity of BE - QS on the MC   and SC test sets of Reddit is reduced by 8.1 and   8.5 absolute compared with BM25 - QS . For Rele-   vance@K , the Top-20 average relevance of BE - QS   on the MC and SC test sets on Reddit increase by   6.3 % and 6.5 % absolute compared with BM25 - QS .   Coverage@K measures the retriever â€™s ability to   retrieve gold response , while Perplexity@K and   Relevance@K measure the overall retrieval quality .   Our results show the consistency of the three met-   rics , namely , the recall rate and the overall retrieval   quality have a positive correlation .   Matching method Compared with contextual   matching , query - response ( QR ) matching has a4870   much lower retrieval recall rate , which is also ver-   ified in ( Lan et al . , 2020 ) . We think it is because   that response is usually a short text of one - sentence   and contains insufficient information , and there   may be little keywords that overlap with the query .   Therefore , it is important to consider contextual   matching in the RBD system .   Compared to QC matching , QS and DQS match-   ing should be encouraged in practice due to the   additional information provided by the response .   However , the BM25 system can not make good use   of the information of response , as BM25 - QS model   does not show obvious advantages over BM25 - QC   on both Reddit and Twitter datasets . In contrast ,   dense retrieval models can effectively utilize the re-   sponse . For example , BE - QS outperforms BE - QC   greatly by 7.9 % absolute in terms of Top-500 re-   sponse retrieval recall rate in MC test set of Reddit . For QS and DQS matching , there is little differ-   ence in performance . Especially for SC test set on   Reddit and MC test set on Twitter , the performance   difference is minimal . One potential advantage of   DQS is that it can utilize positive query - response   pairs , whose number is much larger than positive   query - context pairs .   Distillation benefit We further focus on the per-   formance gain from fine - to - coarse distillation . The   distilled models achieve obvious improvement in   all three metrics . An obvious pattern is that the dis-   tilled models get more larger improvement with a   smaller K. Take Twitter dataset as example , the Top-   500 retrieval recall rate of CFC models increase by   1.5âˆ¼2.4 after distillation , while the Top-1 retrieval   recall rate increased by 4.6 âˆ¼6.7 . On Perplexity@K   and Relevance@K , our CFC models has similar   performance . The significant improvement in the   retrieval recall rate at small K â€™s is especially bene-   ficial to fine - grained response selection , because it   opens up more possibility to the ranker to choose   good response while seeing fewer candidates . The   above results indicate that our student models ben-   efit from learning or inheriting fine - grained knowl-   edge from teacher models . To more clearly demon-   strate the performance gains of our model after   distillation , we provide the specific values of these   gains in Table 8 in Appendix C.   Difference between Reddit and Twitter Since   DialogGPT and DialogRPT is not pre - trained on   Twitter , Perplexity@K and Relevance@K are not4871   suitable for evaluating Twitter dataset . Therefore ,   we do not build SC test set for Twitter . Com-   pared to Twitter , the Reddit dataset we use is much   larger with more common multi - turn conversations ,   and significantly higher retrieval difficulty . The   Top-500 retrieval recall rate on Twitter reach 60 % ,   while Reddit only reached about 20 % , which in-   dicates that the coarse - grained response retrieval   task in open domain conversations still has great   challenges .   6 Further Analysis   6.1 Parameter Sharing   Sharing parameters in dual - encoder structure is a   common practice . As shown in Figure 2 , for the   encoders in the dotted line , sharing parameters may   be beneficial . We try parameter sharing settings on   the BE - QC and TE - DQS models , respectively . We   add two sets of experiments on the MC test set of   Reddit , as shown in Table 4 . The results show that   whether or not to share parameters has little impact   on Coverage@K. Therefore , we can share encoder   parameters to reduce model complexity with little   loss of performance .   Our guess is as follows , the sampling strategy   ( with replacement ) create a certain probability that   the query and the context are exactly the same , so   the multi - tower model can learn that two identical   samples are positive samples for each other , even   if the parameters of the encoders are not shared .   6.2 Effect of Database Size   We discuss the impact of the size of candidate   database on the performance of the model . For   different candidate database size ( from one million   to ten million ) , we compare the Coverage@500   metric of BM25 - QS , BE - QS , and CFC - QS on the   MC test set of Reddit ( Figure 3 ) . It can be seen that   Coverage@500 shows a slow downward trend as   the database size increases . Increasing the size of   the database will not make the model performance   drop rapidly , which shows the effectiveness and   robustness of our models .   6.3 Human Evaluation   To further evaluate and compare our models , we   conduct a human evaluation experiment . We ran-   dom select 1000 queries from the MC and SC test   set ( 500 each ) of Reddit dataset , and retrieve the   Top-1 response by the BM25 - QS , BE - QS and CFC-   QS models respectively . Three crowd - sourcing   workers are asked to score the responses . For each   query , the annotator will strictly rank the retrieved   responses of the three models . We report the aver-   age rank scores ( between 1 and 3 , the smaller the   better ) and the winning rate in pairwise comparison .   Each two annotators have a certain number ( about   200 ) of overlapping annotated samples . To eval-   uate the inter - rater reliability , the Cohen â€™s kappa   coefficient ( Kraemer , 2014 ) is adopted .   Table 5 and Table 6 report the average rank-   ing score of each model and pairwise comparison   between models respectively . The average rank-   ing score of CFC - QS is the highest , and CFC-   QS can beat BE - QS and BM25 in most cases   ( 74.7 % âˆ¼81.6 % ) , which indicates CFC - QS occu-   pies a clear advantage in Top-1 retrieval . All Co-4872hen â€™s Kappa coefficients is between 0.6 and 0.7 ,   indicating annotators reach moderate agreement .   The results of human evaluation further verify the   performance improvement brought by distillation   to the model . We select several examples with hu-   man evaluation as case study and these results are   presented in Appendix D.   6.4 Retrieval efficiency   We compare the retrieval latency of BM25 - QS and   BE - QS on the reddit MC test set , which represent   the efficiency of the sparse and dense retriever re-   spectively . We fix the batch size to 32 and retrieve   top 100 most similar candidates . With the help   of FAISS index , the average retrieval time of each   batch by BE - QS is 581.8ms . In contrast , the aver-   age retrieval time by BM25 system using file index   is 1882.6ms , about three times that of BE - QS . This   indicates that the dense retriever also has an advan-   tage in retrieval efficiency .   The relatively inferior of dense retriever is that it   needs to compute the embeddings of the candidate   database and establish the FAISS index , which is   quite time - consuming and it takes about 9 hours   for BE - QS to handle 10 million candidates with   8 GPUs , while it only takes about 10 minutes to   build a BM25 index .   Since distillation does not change the structure of   the retriever , it will not affect the retrieval efficiency .   The cost of distillation is mainly reflected in the   training of the teacher model and the extensive   forward calculation in the distillation process .   7 Conclusion   In this paper , we propose a Contextual Fine - to-   Coarse ( CFC ) distilled model . In CFC model , we   adopt matching on both query - response and query-   context . Considering the retrieval latency , we use   multi - tower architecture to learn the dense repre-   sentations of queries , responses and corresponding   contexts . To further enhance the performance of   the retriever , we distill the knowledge learned by   the one - tower architecture ( fine - grained ) into the   multi - tower architecture ( coarse - grained ) . We con-   struct two new datasets based on Reddit comment   dump and Twitter corpus , and extensive experi-   mental results demonstrate the effectiveness and   potential of our proposed model . In the future work ,   we will further explore how the enhancement of   coarse - grained RS can help fine - grained RS.Acknowledgments   This work is partially supported by Natural   Science Foundation of China ( No.6217020551 ,   No.61906176 ) , Science and Technology Com-   mission of Shanghai Municipality Grant   ( No.20dz1200600 , 21QA1400600 , GWV-   1.1 , 21511101000 ) and Zhejiang Lab ( No .   2019KD0AD01 ) .   Ethical Statement   In this paper , different ethical restrictions deserve   discussion .   The datasets we created are derived from large   dialogue corpus that publicly available on the Inter-   net , and we strictly followed the platform â€™s policies   and rules when obtaining data from web platforms .   We did not use any author - specific information in   our research .   Online large dialogue corpus may includes some   bias , such as political bias and social bias , and our   model might have inherited some forms of these   bias . In order to limit these bias as much as pos-   sible , we filter controversial articles and removed   data with offensive information when possible .   References48734874   A Dataset Construction Details   To filter boring and dull content and speed up the   retrieval speed , we set a limit for the length of con-   texts and responses . We limit the context to contain   at least 5 words and less than 128 words , and the   response contains at least 5 words and less than 64   words . It is specially beneficial to limit the length   of the response , since according to our statistics ,   many short responses such as " Fair Enough " and   " Thanks :D " may have large number ( tens of thou-   sands ) of different contexts .   Besides , we also limit the upper limit of the   number of contexts corresponding to the response .   The number of contexts of each response in the   MC test set is limited to no more than 50 , which   is to prevent the selected responses from being   a meaningless universal response . The detailed   construction of the two test sets is described in   Algorithm 1 .   To construct the training set , we need to find   out responses that corresponding multiple contexts . Algorithm 1 Construction of SC & MC test set . R : A set of unique responses . SC=âˆ…MC=âˆ…for each râˆˆRdo C = FindAllContexts ( r ) â–·Find all   contexts whose response is r. if|C|>1then C , c = Split(C)â–·Random pick   one context cfromC , the remaining contexts   is denoted as C. MC = MCâˆª { c , r } else SC = SCâˆª { câˆˆC , r } end ifend for eachMC = RandomSample ( MC)SC = RandomSample ( SC)return SC , MC   We use dict to implement it , where the key is the   response and the value is the list of corresponding   contexts . During the training of the multi - tower   model , in each iteration , a batch of keys is ran-   domly sampled from the dict . For each key ( i.e. ,   each response ) in the batch , two contexts are ran-   domly selected from the corresponding value ( i.e. ,   the list of contexts ) , one of which is used as the   query and the other is used as a positive context ,   and the key is used as a positive response . The   other contexts and responses in the batch are all   negative instances of the query .   B Model Details   Due to the different matching methods , the train-   ing of different retrievers requires slightly different   input . Taking BE - QC as an example , given a query ,   positive and negative contexts are needed to learn   the representation of query and contexts , while in   BE - QS , positive and negative sessions are required .   Besides , the distillation of each student model re-   quires training corresponding teacher model , and   the data of training teacher model is consistent with   the student model . We summarize the input , out-   put , and training objectives of student and teacher   models in Table 7 .   To implement the BM25 method , we use Elastic-   search , which is a powerful search engine based   on Lucene library ( BiaÅ‚ecki et al . , 2012 ) . For dense48754876retrieval methods , FAISS ( Johnson et al . , 2019 )   toolkit is used to retrieve candidate vectors . All   encoders in our tower models ( including one - tower ,   two - tower and three - tower ) are initialized with bert-   base , which includes 12 encoder layers , embed-   ding size of 768 and 12 attention heads . For dense   models ( BE - QC , BE - QS , TE - DQS ) , we use the   same batch size of 32 for Reddit and Twitter , and   we train 30 epochs on Reddit and 10 epochs on   Twitter . For all teacher models , we use the same   batch size of 16 , and we train 40 epochs on Red-   dit and 20 epochs on Twitter . For the distillation   ( CFC - QC , CFC - QS , CFC - DQS ) , we train addi-   tional 10 epochs on reddit and 5 epochs on twitter   respectively , starting from the early checkpoints   ( 20 epochs in Reddit and 5 epochs in Twitter for   fair comparison ) of BE - QC , BE - QS , TE - DQS . We   use Adam ( Kingma and Ba , 2014 ) optimizer with   learning rate of 2e-4 and the warmup steps of 200   to optimize the parameters . We set the knowledge   distillation temperature to 3 and the rate of distilla-   tion loss to 1.0 . All experiments are performed on   a server with 4 NVIDIA Tesla V100 32 G GPUs .   C Distillation Benefit   To more clearly show the performance gains of our   model after distillation , we present the specific val-   ues of these gains in Table 8 . Readers can compare   the results in this table when reading the Distilla-   tion Benefit part in Â§ 5.3 . Positive Coverage@K   and Relevance@K , and negative Perplexity@K all   represent the improvement of model performance .   After the distillation , the accuracy and correlation   between the retrieved responses and the query in-   crease , and the conditional perplexity decreases ,   indicating the huge benefits of distillation .   D Case Study   As sparse representations base method , BM25 sys-   tem tends to retrieve responses that overlaps with   the context . For some complicated cases , BM25   can not correctly retrieve those seemingly unrelated ,   but are the best answer in the current context .   In second case of Table 9 , BM25 selects the   response that contains " Spider Man 2099 " in the   query . But in the context of the forum , " Can I   get Spider Man 2099 " is actually looking for the   e - book files of this comic . Compared to the com-   ments of Spider Man 2099 given by BM25 , ourmodel retrieves " You got it PM ( private message )   sent ! " is a harder to find , but more accurate re-   sponse .   The third case is an in - game item trading query .   In related forums , " keys " are used as currency .   " Knife Scorched FT " and " 19keys " in query re-   spectively represent an item to be sold and its ex-   pected price . The result of BM25 covers " knife "   and " key " , but the meaning of the whole sentence   does not match the query . On the other hand , our   model selected " I only have 15keys " , a standard   bargaining , perfectly match the query .   There are also some examples such as case 4 .   Our model gives worse results than BM25 . In case   4 , CFC - QS retrieves a worse result , and the re-   sponse retrieved by BE - QS is relatively better.4877