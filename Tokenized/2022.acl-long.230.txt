  Marco Tulio Ribeiro   Microsoft Research   marcotcr@microsoft.comScott M. Lundberg   Microsoft Research   scott.lundberg@microsoft.com   Abstract   Current approaches to testing and debugging   NLP models rely on highly variable human   creativity and extensive labor , or only work for   a very restrictive class of bugs . We present   AdaTest , a process which uses large scale lan-   guage models ( LMs ) in partnership with hu-   man feedback to automatically write unit tests   highlighting bugs in a target model . Such bugs   are then addressed through an iterative text-ﬁx-   retest loop , inspired by traditional software de-   velopment . In experiments with expert and   non - expert users and commercial /research   models for 8 di  erent tasks , AdaTest makes   users 5 - 10x more e  ective at ﬁnding bugs than   current approaches , and helps users e  ectively   ﬁx bugs without adding new bugs .   1 Introduction   Although NLP models are often underspeciﬁed   and exhibit various generalization failures , ﬁnding   and ﬁxing such bugs remains a challenge . Cur-   rent approaches include frameworks for testing   ( e.g. CheckList ; Ribeiro et al . , 2020 ) , error analy-   sis ( Wu et al . , 2019 ) , or crowdsourcing ( e.g. Dyn-   abench ; Kiela et al . , 2021 ) , all of which depend   on highly variable human creativity to imagine   bugs and extensive labor to instantiate them . Out   of these , only crowdsourcing can potentially ﬁx   bugs when enough data is gathered . On the other   hand , fully automated approaches such as perturba-   tions ( Belinkov and Bisk , 2018 ; Prabhakaran et al . ,   2019 ) , automatic adversarial examples ( Ribeiro   et al . , 2018 ) , and unguided data augmentation ( Yoo   et al . , 2021 ; Wang et al . , 2021 ) are severely re-   stricted to speciﬁc kinds of problems ( e.g. Ribeiro   et al . ( 2018 ) only deal with inconsistent predictions   on paraphrases ) . Despite their usefulness , current   approaches do not allow a single user to easily   specify , discover , and ﬁx undesirable behaviors . Figure 1 : AdaTest consists of two loops : A Testing   Loop that generates and organizes tests optimized for   the target model , and a Debugging Loop that iteratively   reﬁnes the target model based on test failures .   In this work , we present Adaptive Testing ( AdaT-   est ) , a process and toolthat leverages the comple-   mentary strengths of humans and large scale lan-   guage models ( LMs ) to ﬁnd and ﬁx bugs in NLP   models . The LM is tasked with the slow “ creative ”   burden ( Kahneman , 2011 ) of generating a large   quantity of tests adaptively targeted against the   model being tested , while the user steers the LM   by only selecting high quality tests and organiz-   ing them into semantically related topics – which   drastically improves LM generation and guides it   towards areas of interest .   In an inner Testing Loop ( Figure 1 , unrolled in   Figure 2 ) , users start with a set of unit tests in a   topic . The LM then generates many similar tests   that are designed to highlight bugs in the target   model , of which the user only reviews the top few   failing or near - failing tests ( Figure 2A ) , adding   valid tests to the current topic or organizing them   into additional sub - topics ( Figure 2B ) . These user-3253ﬁltered tests are included in the LM prompt for the   next round of suggestions , nudging them toward the   intersection between user interest and model failure .   Repeating the Testing Loop results in hill climbing   behavior , where even when users can not ﬁnd model   failures on their own , they can start from a small   set of passing tests and quickly iterate with the   LM to produce a large set of tests that reveal model   failures . Once enough bugs are discovered , the user   engages in an outer Debugging Loop ( Figure 1 ) ,   performing an operation to ﬁx bugs ( e.g. ﬁnetuning   on failing tests ) , and ( crucially ) testing the model   again to verify that new bugs were not introduced .   AdaTest can be seen as an application of the test-   ﬁx - retest loop from software engineering to NLP .   We demonstrate the usefulness and generality   of AdaTest by having users with diverse skill sets   ﬁnd and ﬁx bugs in state - of - the - art models for a   wide variety of tasks and domains . In controlled   user studies , expert users consistently discovered   5x more bugs per minute with AdaTest ( com-   pared to CheckList ) , while users with no technical   background discovered 10x more ( compared to a   tool similar to Dynabench ) . Our experiments indi-   cate AdaTest ’s Debugging Loop reliably ﬁxes bugs   without introducing new ones , in contrast to other   forms of data augmentation ( templates , counterfac-   tuals ( Wu et al . , 2021 ) , manual GPT-3 prompting ) .   Finally , we present case studies where experts and   non - experts use AdaTest “ in the wild ” on commer-   cial models , ﬁnding and ﬁxing a large quantity of   previously unknown bugs ( e.g. resulting in an 11:1   F1 improvement over expert GPT-3 augmentation ) .   2 Adaptive Testing   The fundamental unit of speciﬁcation in AdaTest is   atest , deﬁned as an input string or pair and an ex-   pectation about the behavior of the model ( Ribeiro   et al . , 2020 ) . The expectation can specify what the   output should or should not be ( e.g. for sentiment   analysis f(This is so great!! ) = pos ,   f(It 's not bad ) , neg ) , a property   on perturbations such as invariance ( e.g.   f(good ) = f(good. ) ) , or a property of the   output ( e.g. substring containment in translation ;   f(The cake 's icing ) + cereja ,   or the output of a classiﬁer c()for text genera-   tion;c(f(Immigrants are ) ) , toxic ) .   When a test is applied to a model , it produces   atest failure score , such that failing tests have   high scores , while passing tests have low scores .   The score may be a binary pass /fail indicator ,   or a continuous indicator of how strongly a test   passes /fails , e.g. in Figure 2 the score is the   model ’s margin of conﬁdence for class “ negative ” .   To evaluate model behavior at varying levels   of abstraction , tests are organized into a test tree   where each internal node is a topic . For exam-   ple , with the 3 - way Sentiment Analysis model   in Figure 2 , we start with the /Sensitive topic   within the test tree , and organize it further by   deﬁning as children the subtopics /Sensitive /Racial   and / Sensitive /Immigration , each containing re-   lated tests and subtopics . These ﬂexible test trees   are built out by the user as they explore model be-   havior . This allows for ﬁne grained evaluation and   helps both the user and the LM focus , by testing   one topic at a time . They are also persistent sets of   unit tests that can be applied to new model versions ,   iteratively updated , and shared with the community   as starting points for testing other models.32542.1 The Testing Loop   Writing tests that expose bugs in NLP models is   hard for both humans and LMs , but they have com-   plementary strengths and weaknesses . LMs can   generate and run hundreds of test proposals based   on existing tests , but these tests are often invalid   and do n’t represent the behavior expected by the   user . In contrast , humans can quickly perceive if a   test is valid or invalid , but can write new tests only   slowly ( Kahneman , 2011 ) , and with high variabil-   ity depending on user expertise and creativity . The   Testing Loop is designed to leverage these comple-   mentary strengths through an iterative optimization   process : at each iteration , the LMproposes a set of   new tests for a topic , and the user accepts those that   are valid , high scoring , and within the topic ’s scope .   These accepted tests are then used by the LM to   generate the next round of suggestions . This loop   is similar in spirit to Markov - Chain Monte - Carlo   ( Hastings , 1970 ) , with the LM as the proposal func-   tion and the user accepting /rejecting samples .   Test proposals for a topic are generated by con-   catenating several tests ( 7 by default ) from the topic   into a prompt to the LM , selected based on test   score , diversity , and randomization ( details in Ap-   pendix A ) , such that high scoring tests tend to be   placed later in the prompt , where they have more   impact on the output ( Zhao et al . , 2021 ) . When   there are not enough tests in the current topic , we   use tests from nearby topics , preﬁx these demon-   stration tests with their topic location , and ask the   LM to generate a completion conditioned on the   current topic . This allows for zero shot generation   in new empty topics when related topics already   exist in the test tree . For a given prompt , we sample   a large number of independent completions ( typi-   cally 25 - 150 ) from the LM as test proposals , which   are run through the target model , sorted by test   failure score , and presented to the user , such that   users typically inspect only the top few likely test   failures . The user then does an organization round ,   where they accept good test suggestions , and po-   tentially reorganize the hierarchy locally , e.g. by   grouping tests into subtopics ( which can also be   auto - generated by the LM ) . Because the user rarely   writes tests manually , and only looks at a small   fraction of the tests generated by the LM , they can   explore a vastly larger space of tests than would be   possible with manual tools . Because the user ex-   plicitly labels tests as valid , the LM does not needto be highly accurate in its test generation – it just   needs to be good enough to ensure a reasonable   fraction of top - scoring generated tests are valid .   An example unrolled Testing Loop is shown in   Figure 2 , where we initialize the /Sensitive topic   with test cases from a CheckList ( Ribeiro et al . ,   2020 ) that did not identify failures of this type on   commercial models . A round of LM test sugges-   tions , however , quickly identiﬁes notable failures in   Microsoft ’s Text Analytics ( /windows ) , one of the models   tested by Ribeiro et al . ( 2020 ) , such as predicting   “ I am a racial minority ” as negative ( Figure 2A ) .   In Figure 2B the user ignores invalid tests ( e.g.   f(I'm tired of being silenced ) , neg   has the wrong expectation ) and organizes valid sug-   gestions into the new subtopics /Sensitive /Racial   and / Sensitive /Immigration . For the next round of   the Testing Loop ( Figure 2C ) , the user chooses   /Sensitive /Immigration as the focus topic ( it only   has borderline failures ) , and gets LM suggestions   ( Figure 2D ) to ﬁnd higher scoring failures . The   iterative nature of the Testing Loop allows the user   to give constant feedback , nudging the LM towards   high quality tests with high test score . Suggestions   help the user both explore ( ﬁnding new subtopics ,   Figure 2B ) and exploit ( hill - climb towards high   scoring tests , as in Figure 2C ) . The organization   step keeps the LM on - topic , with a hierarchy that   adapts to model behavior rather than being ﬁxed   ahead of time ( Kulesza et al . , 2014 ) .   A few rounds of AdaTest reveals many sensitive   subtopics with a large number of positive /neutral   examples predicted as “ negative ” with high con-   ﬁdence , including the ones in Figure 2 and oth-   ers such as /Sensitive /Homeless ( e.g. “ I opened a   homeless shelter once . ” ) , /Sensitive /Survivor ( e.g.   “ I am a cancer survivor ” ) , and /Sensitive /Disability   ( e.g. “ I am hard of hearing ” ) .   2.2 The Debugging Loop   In the outer Debugging Loop ( Figure 1 , unrolled   in Figure 3 ) the user ﬁxes bugs discovered in the   Testing Loop . We do this by ﬁnetuning the model   on the tests , but other strategies such as collecting   more data or adding constraints are also possible .   Adding the tree to training data in the ﬁx step “ in-   validates ” it for testing , which is not an issue due   to the lightweight nature of the Testing Loop ( but   would be for tests that are costly to produce , e.g.   CheckList ) . The re - test adaptation ( i.e. running   the Testing Loop again ) is critical , as the process3255   of ﬁxing a bug often overcompensates , introducing   shortcuts or bugs in the initial rounds . For exam-   ple , ﬁnetuning a RoBERTa - Large sentiment model   on the test tree in Figure 2 inadvertently results   in a model that often predicts “ neutral ” even on   very positive /negative sentences about immigra-   tion ( Figure 3 ; “ I oppose the muslim ban ” ) . An-   other model might be “ ﬁxed ” for the discovered   subtopics , but still broken on related subtopics ( e.g.   “ I have a work visa ” ) . The user does not have to   exhaustively identify every possible shortcut or im-   balance ahead of time , since AdaTest adaptively   surfaces and ﬁxes whatever bugs are introduced in   the next rounds of testing and debugging . Thus ,   the Debugging Loop serves as a friendly adversary ,   pushing the boundaries of the current “ speciﬁca-   tion ” until a satisfactory model is produced .   2.3 Adapting test trees to new models   Even though AdaTest is adaptive to the speciﬁc   model being tested , we observe that existing AdaT-   est trees are typically good starting points when   testing new models . To illustrate this , we run   the test tree in Figure 2 through Google Cloud ’s   Natural Language ( /uniF1A0 ) , and observe that most of   the topics immediately reveal a variety of fail-   ures ( with no adaptation ) . One exception is the   /Sensitive /Immigration topic , on which /uniF1A0has no   immediate failures . However , a single round of sug-   gestions surfaces within - topic failure patterns ( e.g.   “ I am an immigrant myself ” , “ I am an immigrant ,   my parents are not . ” are both predicted as “ nega - tive ” ) , which are easily exploited in further rounds .   This augmented topic does not reveal any failures   on Amazon ’s Comprehend ( /_585 ) , but a single round   of suggestions reveals related bugs ( e.g. “ I am a   DREAMer ” , “ I am a DACAmented educator ” ) that   can be expanded in further rounds .   In Figure 4 we show a much more extreme form   of adaptation – we start with a test tree from /windows   Sentiment Analysis , and adapt a few of its topics   to / uniF1A0Translate ( English ) Portuguese ) English ) by   running a few rounds of the Testing Loop . While   model outputs are di  erent and thus test expecta-   tions need to be adjusted , certain aspects of the   input are relevant across tasks ( e.g. Negation , Sen-   sitive inputs ) , and having a starting set of tests   makes it easy to bootstrap the Testing Loop . We   then switch the model to /windowsTranslate and adapt   this new topic tree to both ( English ) Portuguese   ) English ) and ( English ) Chinese ) English ) . In   every case , we easily discover a variety of in - topic   bugs , even though these are mature products and   we use a small toy test tree . This illustrates how   AdaTest makes it easy to adapt an existing tree to   a new model , even if the test tree was organized   using a di  erent model – or even a di  erent task .   3 Evaluation   We present controlled user studies on the Testing   Loop with both expert and non - expert users ( 3.1 ) ,   followed by controlled experiments on the Debug-   ging Loop ( 3.2 ) . Finally , we present case studies   where AdaTest is used “ in the wild ” ( 3.3 ) .   3.1 Testing Loop   Expert testing We ran a user study to quantita-   tively evaluate if AdaTest makes experts better at   writing tests and ﬁnding bugs in models , when com-   pared to the SOTA in NLP testing ( CheckList ) .   We recruited ten participants with a background   in ML and NLP from industry and academia , and   asked them to test two models : 1 ) a commercial   sentiment classiﬁer ( /windows ) , and 2 ) GPT-2 ( Radford   et al . , 2019 ) used for next word auto - complete .   Users completed eight separate tasks , where   each task is a unique combination of a model ( sen-   timent or auto - complete ) , topic ( see Figure 5 ) , and   tool ( AdaTest or CheckList ) . For each task , partici-   pants start with a set of four ( passing ) sample tests3256   inside a speciﬁc topic , and try to ﬁnd as many on-   topic model failures as possible within 8 minutes .   The ordering between tools is randomized , while   the order of model and topic is ﬁxed ( Figure 5 ) .   We present the average number of discovered   model failures per minute in Figure 5 , where we   observe a5 - fold improvement with AdaTest , an   e  ect persistent across models and users . Among   all 80 user + task scenarios , a user found less failures   with AdaTest in only one case , and by a single test .   Interestingly , Ribeiro et al . ( 2020 ) had tests in the   same topics , with very low error rates for the same   model ( 4 % for a test that included Clear Positives ,   0 % for Negated positives ) , while study participants   were able to ﬁnd many failures , e.g. “ I really like   this place ” ( predicted as neutral ) , “ Everything was   freaking sensational ” ( predicted as negative ) , “ I   did n’t think the food was that good ” and “ I could n’t   wait to leave ” ( both predicted as positive ) . Qual - itatively , users explored a much wider variety of   behaviors with AdaTest , even considering Check-   Lists ’ template capabilities . When the burden of   test generation is lifted from the user , it is much eas-   ier to explore multiple variations on themes , which   are sometimes required to ﬁnd bugs . For example ,   “ I really liked this place ” is correctly predicted as   positive , while “ I really likethis place ” is ( incor-   rectly ) predicted as neutral . Similarly , “ I will not   be coming back ” is correctly predicted as nega-   tive , while “ I will not be coming back , I am sure   I can ﬁnd a better place ” is predicted as positive .   AdaTest not only surfaces such variations , but also   hill - climbs towards them with user feedback , e.g.   a user iteratively added the following progression   of suggested tests , with model conﬁdence for “ pos-   itive ” in parentheses : “ This is not good ( 0 ) ” ,   “ I did n’t think the pizza was any good ( 0.28 ) ” ,   “ I did n’t think the Thai escargot was good ( 0.6 ) ” ,   “ I did n’t think the eggs were very good ( 0.94 ) ” .   Non - expert testing In order to evaluate if AdaT-   est helps non - experts ﬁnd bugs , and how users ’   backgrounds impact the process , we recruited 24   participants equally divided between those who   self - identify as progressive or conservative . These   were all in the U.S. , with a diverse range of ages   and occupations , and no background in data sci-   ence , programming , or ML . We asked users to test   the Perspectives API toxicity model for content   moderation , as an example of an application that   can impact the general public in group - speciﬁc   ways . Users tried to ﬁnd non - toxic statements pre-   dicted as toxic for two topics : Left ( progressive ) ,   andRight ( conservative ) political opinions . We fur-   ther instructed them to only write statements they3257would personally feel appropriate posting online ,   such that any model failures discovered are failures   that would impact them directly . When testing the   topic that does not match their perspective , they   were asked to role - play and express appropriate   comments on behalf of someone from the opposite   political perspective . For each topic , users test the   model with an interactive interface designed to be   an improved version of Dynabench ( predictions are   computed at each keystroke , making trial - and - error   much faster ) for 5minutes , followed by 10minutes   of AdaTest ( topic order is randomized ) .   We present the results in Figure 6A , where we   observe a 10x increase in test failures per minute   with AdaTest . We believe most of the gain is ex-   plained by the automatic adversarial exploration   done by the LM ( rather than the user ) , coupled   with interactive hill climbing on failed tests . We   recruited six additional participants to verify if   the model failures for their political perspective   are things they could see themselves appropriately   posting online , and report the validation rate in   Figure 6B. Participants had their tests validated by   additional raters twice as often when they were   writing tests reﬂecting their own political perspec-   tive ( in - group vs out - group ) .   These results indicate that non - experts with   AdaTest are much more e  ective testers , even with   minimal instruction and experience . The fact that   users writing tests for another group resulted in a   much poorer representation of that group indicates   it might be important to ﬁnd testers from di  erent   groups that could be impacted by a model . Since   it is often not practical to ﬁnd experts from every   impacted group , empowering non - experts with a   tool like AdaTest can be very valuable .   3.2 Debugging Loop   We evaluate the scenario where a user has found   a bug ( or set of bugs ) and wants to ﬁx it . As base   models , we ﬁnetune RoBERTa - Large for duplicate   question detection on the QQP dataset ( Wang et al . ,   2019 ) , and for 3 - way sentiment analysis on the SST   dataset ( Socher et al . , 2013 ) . We rely on CheckList   suites made available by Ribeiro et al . ( 2020 ) for   evaluation , using a 20 % failure rate threshold for a   topic to “ fail ” . The base model fails 22out of 53   QQP topics and 11 out of 39 Sentiment topics .   We create data in order to “ ﬁx ” a topic by either   taking n=50examples from the topic ’s data in the   CheckList condition , or starting from a seed of 5   examples and running the Debugging Loop with   AdaTest until ﬁnding failures becomes qualitatively   dicult ( on average 2:83rounds for QQP and3:83   rounds for Sentiment ) , yielding an average of 41:6   tests for QQP and55:8tests for Sentiment . We   follow this process for 6distinct high failure rate   topics in each task .   Given a set of “ ﬁxing ” data from a single   test topic or from multiple topics , we ﬁnetune   RoBERTa - Large from the previous checkpoint on   an equal mixture of ﬁxing data and data from the   original training set to prevent catastrophic forget-   ting ( McCloskey and Cohen , 1989 ) , until conver-   gence . Ideally , we want to ﬁx the original topic   ( and perhaps a few more which are also impacted   by similar bugs ) without adding new bugs , and thus   we evaluate the “ ﬁxed ” models by measuring how   many topics in the original CheckList suite they   “ ﬁx ” or “ break ” , i.e. move from error rate from   greater than 20 % to lower than 20%or vice versa .   For each set of ﬁxing data , we ﬁnetune RoBERTa   3times with di  erent random seeds , draw 5;000   bootstrap samples of the predictions , and consider   that a topic is ﬁxed or broken if the change is sig-   niﬁcant with an FDR signiﬁcance level less than   0:05 ( Benjamini and Hochberg , 1995 ) .   We present the results in Figure 7 , where we   vary the number of topics used for training in the   xaxis ( for each tick , we sample 3random topic3258   subsets of size xand average the results ) . In the   vast majority of cases , AdaTest ﬁxes the topics   used for training and a number of other topics   without breaking anytopics , while CheckList data   often introduce new bugs ( and thus break other test   topics ) . Part of this may be due to higher diversity   in terms of sentence structure and length in the   AdaTest generated data , as compared to a ﬁxed   CheckList template . However , models ﬁnetuned   only on data from the ﬁrst round of the Testing   Loop ( roughly equivalent to CheckList , but with   more diversity ) also tend to break other topics ,   which supports the importance of an iterative   debugging loop . Qualitatively , we repeatedly   observed the phenomenon illustrated in Figure 3 ,   where the model initially uses oversimpliﬁed   shortcuts to ﬁx a set of tests , i.e. data from a   single round often introduces non - obvious bugs   that only get discovered and ﬁxed in following   rounds . For example , one of the topics for QQP is   f(more X , less antonym(X) ) = dupl . ,   with examples like ( “ How do I become more pa-   tient ” , “ How do I become less irritable ” ) . Ribeiro   et al . ( 2020 ) anticipated a potential orderingshortcut , since the topic also contains examples of   “ ( less X , more antonym(X ) ) ” . After training on   such data , AdaTest surfaces a bug where examples   in the form “ ( more X , more antonym(x ) ) ” are   predicted as duplicates , as well as examples   of unrelated predicates like ( “ more British ” ,   “ less American ” ) . None of the topics in the   suite capture these exact behaviors , but similar   shortcuts break topics that arepresent such as   f(more X , less X ) , dupl . . The iterative   Debugging Loop identiﬁes and ﬁxes such shortcuts ,   leading to more robust bug ﬁxing .   We evaluate accuracy on the validation dataset   and on challenging out of domain datasets ( Zhang   et al . , 2019 ; Potts et al . , 2021 ) after training on all   6topics ( Table 1 ) . In both tasks , AdaTest augmen-   tation has a negligible or non - signiﬁcant impact   on in - domain accuracy , and improves performance   on out of domain data . While AdaTest may have   introduced new bugs not caught by the CheckList   test suite or these additional test sets , the improved   performance on all of these indicate that the De-   bugging Loop is not ﬁxing bugs at the expense of   signiﬁcantly degrading performance elsewhere . We   also compare AdaTest to labeled Polyjuice coun-   terfactuals ( Wu et al . , 2021 ) available for QQP . De-   spite having more data ( thousands vs AdaTests ’   250 labels ) , the results are strictly inferior ( accu-   racy 37:8on PAWS , ﬁxed 2 topics and broke 1 ,   while Adatest ﬁxes 11 and breaks none ) .   3.3 Case Studies   Non - expert testing of non - classiﬁcation models   In order to evaluate if AdaTest would help non-   experts test models for more complex tasks , we   recruited a bilingual speaker with no technical back-   ground , and asked them to test a translation system   and an NER system commercialized by a large   software company ( and thus subject to extensive   prior testing and validation ) . Speciﬁcally , we asked   the user to ﬁnd English to Portuguese translations   with inconsistent or wrong gender assignments ( e.g.   the equivalent of “ My ( female ) wife ( female ) is a   ( male ) doctor ( male ) ” ) , and to test NER predictions   of the PERSON category . For each task , after being   presented with examples of tests in each topic , the   user wrote tests for 20minutes , divided between an   interactive interface like Dynabench and AdaTest .   Even though the tasks are very di  erent ( gener-   ation and per - token classiﬁcation ) , the results are   consistent with Section 3.1 , with the user ﬁnding3259many more bugs with AdaTest ( 32vs4on transla-   tion , 16vs0on NER ) . Qualitatively , adaptive test   suggestions helped the user ﬁnd bugs covering a   much wider range of phenomena than all of the   attempts without assistance . For example , the user   manually wrote di  erent combinations of 15sub-   jects and 11predicates for translation , all related to   family members and professions ( e.g. “ My mom   is a doctor ” ) . With AdaTest , they found bugs with   30subjects and 27predicates , with much more   diversity in both ( e.g. “ The woman with the red   dress is my best friend ” ) . AdaTest helped the user   ﬁnd a variety of sentences where the NER model   predicted the label “ Person ” for names of organiza-   tions ( e.g. “ What I do for Black Booty is provide   ﬁnancial advice ” ) , products ( e.g. “ I think Alikat   is a good form of cash money ” ) , and animals ( e.g.   “ Nathan the dog likes to spend time at the farm ” ) ,   while they could not ﬁnd any bugs unassisted .   Text to video matching To gauge the useful-   ness of AdaTest for established model develop-   ment and maintenance pipelines , we shared AdaT-   est with a ML development team in charge of a   multi - modal classiﬁer that matches textual inputs   with a database of videos . While their produc-   tion model had gone through several external red-   teaming reviews , a single short ( unaided ) AdaTest   session revealed novel gender bias and related is-   sues that were then fed back into their custom miti-   gation pipeline . The team reported that being able   to quickly generate diverse model - targeted tests ,   while at the same time creating a suite of tests for   future model versions was extremely valuable , and   they have since sought to develop adaptive test trees   for their whole suite of production models .   Task detection A team of ML scientists at a large   software company was building a model to predict   whether a sentence in an email or meeting note rep-   resents an action item or task , such as “ I will run the   experiment tomorrow ” . Prior to our engagement ,   the team had gone through a painstaking process   of gathering and labeling data , using CheckList   ( Ribeiro et al . , 2020 ) to ﬁnd bugs , and generating   data with GPT-3 to ﬁx the discovered bugs . The   team was thus well versed in testing , and had been   trying to accomplish the same goals that AdaTest   is built for , using the same exact LM .   After a ﬁve minute demo , two of the team mem-   bers engaged in the Testing Loop for an hour . In   this short session , they found many previously   unknown bugs , with various topics they had n’t   thought about testing ( e.g. “ While X , task ” , as   in “ While we wait for the manufacturer , let ’s build   a slide deck ” ) , and some they had tested and ( incor-   rectly ) thought they had ﬁxed ( e.g. false positives   related to waiting , such as “ John will wait for the   decision ” or “ Let ’s put a pin on it ” ) . When testing   name invariances with CheckList they had n’t in-   cluded personal pronouns ( e.g. “ Karen will imple-   ment the feature ” = “ I will implement the feature ” ) ,   which AdaTest revealed the model fails on .   One team member ran the Debugging Loop for   approximately 3 hours , ﬁxing bugs with the same   procedure as in Section 3.2 . Consistent with the   previous results , they found that ﬁxing bugs ini-   tially led to new bugs being introduced , e.g. ﬁxing   false negatives on passive statements ( “ the experi-   ment will be run next week ” ) lead to false positives   on non - task factual descriptors ( “ the event will be   attended by the dean ” ) , which were surfaced by   AdaTest and ﬁxed in the next round . In order to   compare the results of using AdaTest to their pre-   vious e  orts , we collected and labeled two new   datasets from sources they had n’t used as training   data . We present the F1 scores of models aug-   mented either with their GPT-3 generated data or   on AdaTest data in Table 2 , where AdaTest shows   signiﬁcant improvement despite involving much   less e  ort . Qualitatively , the team noted that ﬁnd-   ing bugs with AdaTest was much easier than with   CheckList , by virtue of the extensive suggestions   made by the LM . Similarly , after noticing ( and ﬁx-   ing ) potential shortcuts in multiple rounds of the   Debugging Loop , the team realized that their prior   GPT-3 augmentation was almost certainly liable to   such shortcuts , and thus less e  ective .   3.4 Discussion   We evaluated AdaTest on 8 di  erent tasks spanning   text classiﬁcation , generation , and per - token predic-   tion . In terms of ﬁnding bugs , we compare AdaTest   to experts using CheckList and non - experts using   a more responsive version of Dynabench . Users3260consistently found many more bugs per minute   with AdaTest on research models and commercial   models at di  erent development stages ( early ver-   sion , pre - release , and mature models in production ) .   The fact that AdaTest requires minimal training   and is easy enough to be used by users without   any technical background is an asset , especially   when it is important to have testers that represent   diverse groups that may be negatively impacted by   bugs . In terms of ﬁxing bugs , we compared the   Debugging Loop to naively augmenting data with   CheckList templates , using Polyjuice counterfac-   tuals , and having an expert use GPT-3 to create   additional data . In every case , AdaTest improved   performance more than alternatives , and crucially   did not add new bugs that degrade performance on   available measurements , due to the iterative nature   of the Debugging Loop . In contrast to alternatives ,   further testing with AdaTest is low - cost , and thus   this augmentation does not have the e  ect of in-   validating costly evaluation data ( e.g. invalidating   CheckList tests that are laborious to create ) . In fact ,   test trees from previous sessions can be used to test   new models , or to bootstrap a new AdaTest session .   4 Related Work   Even though we used CheckList and Dynabench as   baselines in the previous section , our results indi-   cate that these and other approaches ( Gardner et al . ,   2020 ; Kaushik et al . , 2019 ) where human creativ-   ity and e  ort are bottlenecks ( Bhatt et al . , 2021 )   would beneﬁt from the greatly enhanced bug dis-   covery productivity made possible by AdaTest . On   the other hand , CheckList as a framework provides   great guidance in organizing the test tree , enumer-   ating important capabilities and perturbations to be   tested , as well as a tool for systematically apply-   ing the test tree to future models . Similarly , Dyn-   abench provides model serving capabilities and   a crowdsourcing platform that would greatly en-   hance AdaTest , especially as users share test trees   and adapt them to new models .   In terms of ﬁxing bugs , fully automatic data aug-   mentation with LMs ( Yoo et al . , 2021 ; Wang et al . ,   2021 ) can not incorporate human “ speciﬁcation ” be-   yond already existing data , nor debug phenomena   that is very far from the existing data . On the other   hand , general purpose or contrastive counterfactu-   als have shown mixed or marginally positive re-   sults ( Huang et al . , 2020 ; Wu et al . , 2021 ) similar   to what we observed in Section 3.2 , except whenlarge quantities of data are gathered ( Nie et al . ,   2020 ) . Our hypothesis is that underspeciﬁcation   ( D’Amour et al . , 2020 ) is a major factor limiting   the beneﬁt of many counterfactual augmentation   techniques . We observed that the ﬁrst rounds of   the Debugging Loop often decrease or maintain   overall performance until additional data from later   rounds speciﬁes the correct behavior more thor-   oughly , which indicates that counterfactual data   targeted precisely where the model is underspeci-   ﬁed is often more e  ective than non - targeted data .   If true , this hypothesis argues for AdaTest ’s fast   iteration in the Debugging Loop , rather than longer   cycles ( e.g. Dynabench rounds can take months ) .   5 Conclusion   AdaTest encourages a close collaboration between   a human and a language model , yielding the ben-   eﬁts of both . The user provides speciﬁcation that   the LM lacks , while the LM provides creativity   at a scale that is infeasible for the user . AdaT-   est o  ers signiﬁcant productivity gains for expert   users , while also remaining simple enough to em-   power diverse groups of non - experts . The Debug-   ging Loop connects model testing and debugging   to e  ectively ﬁx bugs , taking model development   a step closer towards the iterative nature of tra-   ditional software development . We have demon-   strated AdaTest ’s e  ectiveness on classiﬁcation   models ( sentiment analysis , QQP , toxicity , media   selection , task detection ) , generation models ( GPT-   2 , translation ) , and per - token models ( NER ) , with   models ranging from well - tested production sys-   tems to brand new applications . Our results indi-   cate that adaptive testing and debugging can serve   as an e  ective NLP development paradigm for a   broad range of applications . To help support this ,   AdaTest ( with various test trees ) is open sourced at   https://github.com/microsoft/adatest .   Acknowledgements   We thank Adarsh Jeewajee , Carlos Guestrin , Ece   Kamar , Fereshte Khani , Gregory Plumb , Gabriel   Ilharco , Harsha Nori , Sameer Singh , and Shikhar   Murty for helpful discussions and feedback . We   also thank Bruno Melo , Hamid Palangi , Ji Li , and   Remmelt Ammerlaan for pilot testing /case studies .   Finally , we thank Tongshuang Wu for all of the   above andhelping us think about ﬁgures , check-   ing translations , o  ering LTEX advice , and other   miscellaneous help.3261References3262   A Language model prompt design   The test suggestion function inside the AdaTest   Testing Loop ( main text Figure 1 ) is implemented   using a large - scale generative LM . We used GPT-3   ( Brown et al . , 2020 ) in our experiments , but we also   support open source HuggingFace models ( Wolf   et al . , 2020 ) . When provided with a prompt in the   form of a list of items , these large LMs can gener-   ate new items that continue the list , and come from   the same distribution of items as the original list .   By carefully controlling the structure and content   of this list , we can steer large LMs to generate new   content on nearly any topic in nearly any form ( ex-   ceptions being very long - form text , and languages   unseen by the LM during training ) .   There is always a current focus topic active dur-   ing the Testing Loop , and it is the goal of the LM   test suggestion process to generate new tests that   will be categorized by the user as direct children of   the focus topic . This means we are not interested   in tests outside the focus topic or inside already-   deﬁned subtopics of the focus topic . We avoid   tests outside the topic in order to maintain a “ focus ”   on the current topic the user has selected , and we   avoid tests inside subtopics because these represent   portions of the current topic that have already been   well explored , and so should be prevented from   dominating the test suggestions . If the user is inter-   ested in a particular subtopic , they simply open it   and generate suggestions speciﬁc to that topic . In   addition to allowing users to guide the LM , focus3263topics also improve the quality of the LM ’s sug-   gestions , since LMs tend to generate higher quality   tests when restricted to a narrower scope . Topics   also enable zero - shot LM test generation for empty   topics , since we can condition on the topic when   generating a test and so use examples from related   topics as demonstrations for the current topic .   The LM prompt itself consists of several tests ( 7   by default ) selected from the current focus topic ( or   from nearby topics if the current topic is empty ) . A   test is written into the prompt as a topic , followed   by a space - separated list of values on the next line   ( see Figure 8) . Prompt parameters are conﬁgurable ,   but we found that 7 examples gave an appropriate   amount of steering information to GPT-3 ( for both   the Davinci and Curie models ) without giving so   many examples that strong patterns would harm the   diversity of the generated tests . We experimented   with a variety of prompt formats , including priming   with “ instruction ” sentences , and found that the   more minimal the notation the better , so as to bias   the generation process as little as possible . We   also remove as much information from the prompt   as possible to further focus and de - bias the LM .   For example , we do not include expected outputs if   they are the same for all the tests in the prompt , and   similarly we only include topic information when   using tests from outside the current focus topic .   We also repeatedly generate a single next list item ,   rather than generating several items in a list . This   is because generating a long list usually reduces   diversity , as generated items tend to converge to a   single topic .   Given a prompt structure and a set of tests in the   current topic , steering the test suggestion genera-   tion comes down to choosing a set tests to include   in the LM prompt . We do this by scoring all tests   as the product of several factors , then selecting the   highest scoring test and adding it to the prompt list .   This process is iterated unless a su cient number   of tests have been selected to be included in the   prompt . This list is then reversed prior to sampling   from the LM , because the LM weights samples   close to the end of the prompt more strongly ( Zhao   et al . , 2021 ) . The factors we use for test selection   are :   Test failure score - Tests with higher scores are   tests that the model fails or is closer to failing   than tests with lower scores . So the strongest   ranking factor we use ( other than topic mem-   bership ) is high test failure score , since this   facilitates hill climbing towards model fail-   ures .   Topic membership - Tests outside the current   topic are very strongly penalized and are only   used if the current topic is empty or nearly   empty . Tests inside subtopics of the current   topic are also strongly penalized for the rea-   sons mentioned above ( that these represent   already explored regions of the topic ) .   Score randomization - Test failure scores can   be computed in many di  erent ways , but they   are often continuous values that represent how   close a model ’s prediction is to failing a test   ( or how far it is past the failure threshold ) .   Tests with very similar scores have an equally   likely chance of being good for prompt inclu-   sion ( since they each can lead the LM towards   high - scoring on - topic tests ) . To encourage di-   verse choices among similar scoring tests we   add one standard deviation of random Gaus-   sian noise to the test scores .   Skip randomization - Sometimes a strong fail-   ure found early on in a topic would always be   selected for the top prompt position since its   score is so much higher than any other current   tests . However this can harm diversity so we3264   also introduce skip randomization where we   randomly skip over tests ( by penalizing their   score ) with 25 % probability .   Prompt diversity - When exploring in a topic   we want to encourage a broad sample of test   structures to be included in the prompt , so that   we fully explore the topic and do n’t get locked   into a single style of test . To promote this , we   penalize each test score by the cosine simi-   larity of that test ’s embedding to the closest   embedding of a test that has already be se-   lected for inclusion in the prompt . By default   we use RoBERTa - base ( Liu et al . , 2019 ) for   this , though any similarity embedding would   work .   We repeat the test selection process rtimes to   create rdi  erent prompts ( where we maximize r   subject to not causing more than a 50 % increase in   computational overhead due to lost prompt reuse   during completions ) . If the user has requested K   suggestions for a round , then for each prompt we   ask the LM to generate bK = rccompletions that are   parsed to produce at most that many tests ( at most , since some completions may produce invalid or   duplicate tests ) . These tests are then applied to   the target model ( or several models , since we can   explore multiple models in parallel ) , sorted by test   failure score , and returned to the user for ﬁltering   and organization .   B User interface   The entire Testing Loop process occurs through   AdaTest ’s interactive web interface , which works   both as a standalone server or inside a Jupyter note-   book . Figure 9 shows a screenshot of this interface ,   browsing the top node of a test tree targeting the   Azure sentiment analysis model ( Google ’s model   is also being scored , but is not adaptively targeted ) .   While we experimented with interfaces that present   the entire test tree to the user at once , these became   intractable for larger test trees . Thus , we follow   traditional ﬁle system browsers , which scale well   to very large and deep trees .   On the left side of Figure 9 is a list of topics   based on CheckList capabilities ( Ribeiro et al . ,   2020 ) . These are top - level topics , some of   which are well explored with many subtopics ( e.g.3265   /Fairness ) , while others have yet to be explored by   the user ( such as /Logic ) . To enable users to or-   ganize the test tree , topics can be edited , opened ,   and dragged and dropped just like in a standard ﬁle   viewer .   On the right side of Figure 9 there are two   columns representing the test failure scores for two   target models , Azure and Google sentiment anal-   ysis . The horizontal position of the colored bars   represents the value of a single test ’s score and the   color denotes passing or failing . Since each bar   represents a single test inside a topic , hovering the   mouse over the bar will show the associated test .   Hovering anywhere over a row also shows the num-   ber of failing and passing tests for the topic ( the   total counts for the current topic are shown at the   bottom ) . Note that topics are sorted by the largest   test fail score they contain . The grey box above   the test topics is where LM test suggestions are   shown . If the user clicked the suggestions button   in Figure 9 , they would get a list of suggested tests   designed to not fall into any of the current topics .   This is very challenging at such a high level of   abstraction , so the precision of these suggestions   might be low , but ﬁnding such tests is often still   possible given enough iteration . Once a few such   tests are found , a new top level topic can be formedand explored . An alternative to this process ( which   tends to work better for high level concepts ) is to   ask AdaTest to suggest new topic names ( done the   same way we suggest new tests ) . Given a starting   test tree , users can potentially ﬁll out whole new   sub - trees without ever writing anything manually   by alternating between topic suggestions and zero-   shot test suggestions for new topics . In general , the   precision of the test suggestion process increases as   the topics grow narrower , so expanding subtopics   topic will likely be much easier than the parent   topic . To jump start this process users can always   manually add tests or topics by clicking the respec-   tive add buttons at the top right , or by editing a   current test ( scores are recomputed in real - time ) .   Figure 10 shows what happens after   we navigate down the topic tree into the   /Negation /Negated positive topic , and then request   LM suggestions . Current tests inside the topic are   shown at the bottom sorted by their test failure   score for the Azure model ( and continue on   past the screen capture ) while test suggestions   are shown in the gray box at the top . The test   suggestions box is scrollable and contains ~100   suggested tests ( also sorted by their test failure   score for the Azure model).3266The selected test suggestion in Figure 10   is highlighted and the test failure scores are   shown for both models . The highlighted test   is a valid high scoring test that falls within the   /Negation /Negated positive topic , so the user can   add it to the current topic in one of several ways :   dragging it down to the list of in - topic tests , click-   ing the " plus " button on the left of the test row ,   hitting Enter , etc . Note that the test directly below   the selected test is also high scoring on the Azure   model , but the test is invalid since the input text   actually does express a positive sentiment , so the   expectation of the test is incorrect.3267