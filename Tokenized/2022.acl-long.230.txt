  Marco Tulio Ribeiro   Microsoft Research   marcotcr@microsoft.comScott M. Lundberg   Microsoft Research   scott.lundberg@microsoft.com   Abstract   Current approaches to testing and debugging   NLP models rely on highly variable human   creativity and extensive labor , or only work for   a very restrictive class of bugs . We present   AdaTest , a process which uses large scale lan-   guage models ( LMs ) in partnership with hu-   man feedback to automatically write unit tests   highlighting bugs in a target model . Such bugs   are then addressed through an iterative text-Ô¨Åx-   retest loop , inspired by traditional software de-   velopment . In experiments with expert and   non - expert users and commercial /research   models for 8 di  erent tasks , AdaTest makes   users 5 - 10x more e  ective at Ô¨Ånding bugs than   current approaches , and helps users e  ectively   Ô¨Åx bugs without adding new bugs .   1 Introduction   Although NLP models are often underspeciÔ¨Åed   and exhibit various generalization failures , Ô¨Ånding   and Ô¨Åxing such bugs remains a challenge . Cur-   rent approaches include frameworks for testing   ( e.g. CheckList ; Ribeiro et al . , 2020 ) , error analy-   sis ( Wu et al . , 2019 ) , or crowdsourcing ( e.g. Dyn-   abench ; Kiela et al . , 2021 ) , all of which depend   on highly variable human creativity to imagine   bugs and extensive labor to instantiate them . Out   of these , only crowdsourcing can potentially Ô¨Åx   bugs when enough data is gathered . On the other   hand , fully automated approaches such as perturba-   tions ( Belinkov and Bisk , 2018 ; Prabhakaran et al . ,   2019 ) , automatic adversarial examples ( Ribeiro   et al . , 2018 ) , and unguided data augmentation ( Yoo   et al . , 2021 ; Wang et al . , 2021 ) are severely re-   stricted to speciÔ¨Åc kinds of problems ( e.g. Ribeiro   et al . ( 2018 ) only deal with inconsistent predictions   on paraphrases ) . Despite their usefulness , current   approaches do not allow a single user to easily   specify , discover , and Ô¨Åx undesirable behaviors . Figure 1 : AdaTest consists of two loops : A Testing   Loop that generates and organizes tests optimized for   the target model , and a Debugging Loop that iteratively   reÔ¨Ånes the target model based on test failures .   In this work , we present Adaptive Testing ( AdaT-   est ) , a process and toolthat leverages the comple-   mentary strengths of humans and large scale lan-   guage models ( LMs ) to Ô¨Ånd and Ô¨Åx bugs in NLP   models . The LM is tasked with the slow ‚Äú creative ‚Äù   burden ( Kahneman , 2011 ) of generating a large   quantity of tests adaptively targeted against the   model being tested , while the user steers the LM   by only selecting high quality tests and organiz-   ing them into semantically related topics ‚Äì which   drastically improves LM generation and guides it   towards areas of interest .   In an inner Testing Loop ( Figure 1 , unrolled in   Figure 2 ) , users start with a set of unit tests in a   topic . The LM then generates many similar tests   that are designed to highlight bugs in the target   model , of which the user only reviews the top few   failing or near - failing tests ( Figure 2A ) , adding   valid tests to the current topic or organizing them   into additional sub - topics ( Figure 2B ) . These user-3253Ô¨Åltered tests are included in the LM prompt for the   next round of suggestions , nudging them toward the   intersection between user interest and model failure .   Repeating the Testing Loop results in hill climbing   behavior , where even when users can not Ô¨Ånd model   failures on their own , they can start from a small   set of passing tests and quickly iterate with the   LM to produce a large set of tests that reveal model   failures . Once enough bugs are discovered , the user   engages in an outer Debugging Loop ( Figure 1 ) ,   performing an operation to Ô¨Åx bugs ( e.g. Ô¨Ånetuning   on failing tests ) , and ( crucially ) testing the model   again to verify that new bugs were not introduced .   AdaTest can be seen as an application of the test-   Ô¨Åx - retest loop from software engineering to NLP .   We demonstrate the usefulness and generality   of AdaTest by having users with diverse skill sets   Ô¨Ånd and Ô¨Åx bugs in state - of - the - art models for a   wide variety of tasks and domains . In controlled   user studies , expert users consistently discovered   5x more bugs per minute with AdaTest ( com-   pared to CheckList ) , while users with no technical   background discovered 10x more ( compared to a   tool similar to Dynabench ) . Our experiments indi-   cate AdaTest ‚Äôs Debugging Loop reliably Ô¨Åxes bugs   without introducing new ones , in contrast to other   forms of data augmentation ( templates , counterfac-   tuals ( Wu et al . , 2021 ) , manual GPT-3 prompting ) .   Finally , we present case studies where experts and   non - experts use AdaTest ‚Äú in the wild ‚Äù on commer-   cial models , Ô¨Ånding and Ô¨Åxing a large quantity of   previously unknown bugs ( e.g. resulting in an 11:1   F1 improvement over expert GPT-3 augmentation ) .   2 Adaptive Testing   The fundamental unit of speciÔ¨Åcation in AdaTest is   atest , deÔ¨Åned as an input string or pair and an ex-   pectation about the behavior of the model ( Ribeiro   et al . , 2020 ) . The expectation can specify what the   output should or should not be ( e.g. for sentiment   analysis f(This is so great!! ) = pos ,   f(It 's not bad ) , neg ) , a property   on perturbations such as invariance ( e.g.   f(good ) = f(good. ) ) , or a property of the   output ( e.g. substring containment in translation ;   f(The cake 's icing ) + cereja ,   or the output of a classiÔ¨Åer c()for text genera-   tion;c(f(Immigrants are ) ) , toxic ) .   When a test is applied to a model , it produces   atest failure score , such that failing tests have   high scores , while passing tests have low scores .   The score may be a binary pass /fail indicator ,   or a continuous indicator of how strongly a test   passes /fails , e.g. in Figure 2 the score is the   model ‚Äôs margin of conÔ¨Ådence for class ‚Äú negative ‚Äù .   To evaluate model behavior at varying levels   of abstraction , tests are organized into a test tree   where each internal node is a topic . For exam-   ple , with the 3 - way Sentiment Analysis model   in Figure 2 , we start with the /Sensitive topic   within the test tree , and organize it further by   deÔ¨Åning as children the subtopics /Sensitive /Racial   and / Sensitive /Immigration , each containing re-   lated tests and subtopics . These Ô¨Çexible test trees   are built out by the user as they explore model be-   havior . This allows for Ô¨Åne grained evaluation and   helps both the user and the LM focus , by testing   one topic at a time . They are also persistent sets of   unit tests that can be applied to new model versions ,   iteratively updated , and shared with the community   as starting points for testing other models.32542.1 The Testing Loop   Writing tests that expose bugs in NLP models is   hard for both humans and LMs , but they have com-   plementary strengths and weaknesses . LMs can   generate and run hundreds of test proposals based   on existing tests , but these tests are often invalid   and do n‚Äôt represent the behavior expected by the   user . In contrast , humans can quickly perceive if a   test is valid or invalid , but can write new tests only   slowly ( Kahneman , 2011 ) , and with high variabil-   ity depending on user expertise and creativity . The   Testing Loop is designed to leverage these comple-   mentary strengths through an iterative optimization   process : at each iteration , the LMproposes a set of   new tests for a topic , and the user accepts those that   are valid , high scoring , and within the topic ‚Äôs scope .   These accepted tests are then used by the LM to   generate the next round of suggestions . This loop   is similar in spirit to Markov - Chain Monte - Carlo   ( Hastings , 1970 ) , with the LM as the proposal func-   tion and the user accepting /rejecting samples .   Test proposals for a topic are generated by con-   catenating several tests ( 7 by default ) from the topic   into a prompt to the LM , selected based on test   score , diversity , and randomization ( details in Ap-   pendix A ) , such that high scoring tests tend to be   placed later in the prompt , where they have more   impact on the output ( Zhao et al . , 2021 ) . When   there are not enough tests in the current topic , we   use tests from nearby topics , preÔ¨Åx these demon-   stration tests with their topic location , and ask the   LM to generate a completion conditioned on the   current topic . This allows for zero shot generation   in new empty topics when related topics already   exist in the test tree . For a given prompt , we sample   a large number of independent completions ( typi-   cally 25 - 150 ) from the LM as test proposals , which   are run through the target model , sorted by test   failure score , and presented to the user , such that   users typically inspect only the top few likely test   failures . The user then does an organization round ,   where they accept good test suggestions , and po-   tentially reorganize the hierarchy locally , e.g. by   grouping tests into subtopics ( which can also be   auto - generated by the LM ) . Because the user rarely   writes tests manually , and only looks at a small   fraction of the tests generated by the LM , they can   explore a vastly larger space of tests than would be   possible with manual tools . Because the user ex-   plicitly labels tests as valid , the LM does not needto be highly accurate in its test generation ‚Äì it just   needs to be good enough to ensure a reasonable   fraction of top - scoring generated tests are valid .   An example unrolled Testing Loop is shown in   Figure 2 , where we initialize the /Sensitive topic   with test cases from a CheckList ( Ribeiro et al . ,   2020 ) that did not identify failures of this type on   commercial models . A round of LM test sugges-   tions , however , quickly identiÔ¨Åes notable failures in   Microsoft ‚Äôs Text Analytics ( /windows ) , one of the models   tested by Ribeiro et al . ( 2020 ) , such as predicting   ‚Äú I am a racial minority ‚Äù as negative ( Figure 2A ) .   In Figure 2B the user ignores invalid tests ( e.g.   f(I'm tired of being silenced ) , neg   has the wrong expectation ) and organizes valid sug-   gestions into the new subtopics /Sensitive /Racial   and / Sensitive /Immigration . For the next round of   the Testing Loop ( Figure 2C ) , the user chooses   /Sensitive /Immigration as the focus topic ( it only   has borderline failures ) , and gets LM suggestions   ( Figure 2D ) to Ô¨Ånd higher scoring failures . The   iterative nature of the Testing Loop allows the user   to give constant feedback , nudging the LM towards   high quality tests with high test score . Suggestions   help the user both explore ( Ô¨Ånding new subtopics ,   Figure 2B ) and exploit ( hill - climb towards high   scoring tests , as in Figure 2C ) . The organization   step keeps the LM on - topic , with a hierarchy that   adapts to model behavior rather than being Ô¨Åxed   ahead of time ( Kulesza et al . , 2014 ) .   A few rounds of AdaTest reveals many sensitive   subtopics with a large number of positive /neutral   examples predicted as ‚Äú negative ‚Äù with high con-   Ô¨Ådence , including the ones in Figure 2 and oth-   ers such as /Sensitive /Homeless ( e.g. ‚Äú I opened a   homeless shelter once . ‚Äù ) , /Sensitive /Survivor ( e.g.   ‚Äú I am a cancer survivor ‚Äù ) , and /Sensitive /Disability   ( e.g. ‚Äú I am hard of hearing ‚Äù ) .   2.2 The Debugging Loop   In the outer Debugging Loop ( Figure 1 , unrolled   in Figure 3 ) the user Ô¨Åxes bugs discovered in the   Testing Loop . We do this by Ô¨Ånetuning the model   on the tests , but other strategies such as collecting   more data or adding constraints are also possible .   Adding the tree to training data in the Ô¨Åx step ‚Äú in-   validates ‚Äù it for testing , which is not an issue due   to the lightweight nature of the Testing Loop ( but   would be for tests that are costly to produce , e.g.   CheckList ) . The re - test adaptation ( i.e. running   the Testing Loop again ) is critical , as the process3255   of Ô¨Åxing a bug often overcompensates , introducing   shortcuts or bugs in the initial rounds . For exam-   ple , Ô¨Ånetuning a RoBERTa - Large sentiment model   on the test tree in Figure 2 inadvertently results   in a model that often predicts ‚Äú neutral ‚Äù even on   very positive /negative sentences about immigra-   tion ( Figure 3 ; ‚Äú I oppose the muslim ban ‚Äù ) . An-   other model might be ‚Äú Ô¨Åxed ‚Äù for the discovered   subtopics , but still broken on related subtopics ( e.g.   ‚Äú I have a work visa ‚Äù ) . The user does not have to   exhaustively identify every possible shortcut or im-   balance ahead of time , since AdaTest adaptively   surfaces and Ô¨Åxes whatever bugs are introduced in   the next rounds of testing and debugging . Thus ,   the Debugging Loop serves as a friendly adversary ,   pushing the boundaries of the current ‚Äú speciÔ¨Åca-   tion ‚Äù until a satisfactory model is produced .   2.3 Adapting test trees to new models   Even though AdaTest is adaptive to the speciÔ¨Åc   model being tested , we observe that existing AdaT-   est trees are typically good starting points when   testing new models . To illustrate this , we run   the test tree in Figure 2 through Google Cloud ‚Äôs   Natural Language ( /uniF1A0 ) , and observe that most of   the topics immediately reveal a variety of fail-   ures ( with no adaptation ) . One exception is the   /Sensitive /Immigration topic , on which /uniF1A0has no   immediate failures . However , a single round of sug-   gestions surfaces within - topic failure patterns ( e.g.   ‚Äú I am an immigrant myself ‚Äù , ‚Äú I am an immigrant ,   my parents are not . ‚Äù are both predicted as ‚Äú nega - tive ‚Äù ) , which are easily exploited in further rounds .   This augmented topic does not reveal any failures   on Amazon ‚Äôs Comprehend ( /_585 ) , but a single round   of suggestions reveals related bugs ( e.g. ‚Äú I am a   DREAMer ‚Äù , ‚Äú I am a DACAmented educator ‚Äù ) that   can be expanded in further rounds .   In Figure 4 we show a much more extreme form   of adaptation ‚Äì we start with a test tree from /windows   Sentiment Analysis , and adapt a few of its topics   to / uniF1A0Translate ( English ) Portuguese ) English ) by   running a few rounds of the Testing Loop . While   model outputs are di  erent and thus test expecta-   tions need to be adjusted , certain aspects of the   input are relevant across tasks ( e.g. Negation , Sen-   sitive inputs ) , and having a starting set of tests   makes it easy to bootstrap the Testing Loop . We   then switch the model to /windowsTranslate and adapt   this new topic tree to both ( English ) Portuguese   ) English ) and ( English ) Chinese ) English ) . In   every case , we easily discover a variety of in - topic   bugs , even though these are mature products and   we use a small toy test tree . This illustrates how   AdaTest makes it easy to adapt an existing tree to   a new model , even if the test tree was organized   using a di  erent model ‚Äì or even a di  erent task .   3 Evaluation   We present controlled user studies on the Testing   Loop with both expert and non - expert users ( 3.1 ) ,   followed by controlled experiments on the Debug-   ging Loop ( 3.2 ) . Finally , we present case studies   where AdaTest is used ‚Äú in the wild ‚Äù ( 3.3 ) .   3.1 Testing Loop   Expert testing We ran a user study to quantita-   tively evaluate if AdaTest makes experts better at   writing tests and Ô¨Ånding bugs in models , when com-   pared to the SOTA in NLP testing ( CheckList ) .   We recruited ten participants with a background   in ML and NLP from industry and academia , and   asked them to test two models : 1 ) a commercial   sentiment classiÔ¨Åer ( /windows ) , and 2 ) GPT-2 ( Radford   et al . , 2019 ) used for next word auto - complete .   Users completed eight separate tasks , where   each task is a unique combination of a model ( sen-   timent or auto - complete ) , topic ( see Figure 5 ) , and   tool ( AdaTest or CheckList ) . For each task , partici-   pants start with a set of four ( passing ) sample tests3256   inside a speciÔ¨Åc topic , and try to Ô¨Ånd as many on-   topic model failures as possible within 8 minutes .   The ordering between tools is randomized , while   the order of model and topic is Ô¨Åxed ( Figure 5 ) .   We present the average number of discovered   model failures per minute in Figure 5 , where we   observe a5 - fold improvement with AdaTest , an   e  ect persistent across models and users . Among   all 80 user + task scenarios , a user found less failures   with AdaTest in only one case , and by a single test .   Interestingly , Ribeiro et al . ( 2020 ) had tests in the   same topics , with very low error rates for the same   model ( 4 % for a test that included Clear Positives ,   0 % for Negated positives ) , while study participants   were able to Ô¨Ånd many failures , e.g. ‚Äú I really like   this place ‚Äù ( predicted as neutral ) , ‚Äú Everything was   freaking sensational ‚Äù ( predicted as negative ) , ‚Äú I   did n‚Äôt think the food was that good ‚Äù and ‚Äú I could n‚Äôt   wait to leave ‚Äù ( both predicted as positive ) . Qual - itatively , users explored a much wider variety of   behaviors with AdaTest , even considering Check-   Lists ‚Äô template capabilities . When the burden of   test generation is lifted from the user , it is much eas-   ier to explore multiple variations on themes , which   are sometimes required to Ô¨Ånd bugs . For example ,   ‚Äú I really liked this place ‚Äù is correctly predicted as   positive , while ‚Äú I really likethis place ‚Äù is ( incor-   rectly ) predicted as neutral . Similarly , ‚Äú I will not   be coming back ‚Äù is correctly predicted as nega-   tive , while ‚Äú I will not be coming back , I am sure   I can Ô¨Ånd a better place ‚Äù is predicted as positive .   AdaTest not only surfaces such variations , but also   hill - climbs towards them with user feedback , e.g.   a user iteratively added the following progression   of suggested tests , with model conÔ¨Ådence for ‚Äú pos-   itive ‚Äù in parentheses : ‚Äú This is not good ( 0 ) ‚Äù ,   ‚Äú I did n‚Äôt think the pizza was any good ( 0.28 ) ‚Äù ,   ‚Äú I did n‚Äôt think the Thai escargot was good ( 0.6 ) ‚Äù ,   ‚Äú I did n‚Äôt think the eggs were very good ( 0.94 ) ‚Äù .   Non - expert testing In order to evaluate if AdaT-   est helps non - experts Ô¨Ånd bugs , and how users ‚Äô   backgrounds impact the process , we recruited 24   participants equally divided between those who   self - identify as progressive or conservative . These   were all in the U.S. , with a diverse range of ages   and occupations , and no background in data sci-   ence , programming , or ML . We asked users to test   the Perspectives API toxicity model for content   moderation , as an example of an application that   can impact the general public in group - speciÔ¨Åc   ways . Users tried to Ô¨Ånd non - toxic statements pre-   dicted as toxic for two topics : Left ( progressive ) ,   andRight ( conservative ) political opinions . We fur-   ther instructed them to only write statements they3257would personally feel appropriate posting online ,   such that any model failures discovered are failures   that would impact them directly . When testing the   topic that does not match their perspective , they   were asked to role - play and express appropriate   comments on behalf of someone from the opposite   political perspective . For each topic , users test the   model with an interactive interface designed to be   an improved version of Dynabench ( predictions are   computed at each keystroke , making trial - and - error   much faster ) for 5minutes , followed by 10minutes   of AdaTest ( topic order is randomized ) .   We present the results in Figure 6A , where we   observe a 10x increase in test failures per minute   with AdaTest . We believe most of the gain is ex-   plained by the automatic adversarial exploration   done by the LM ( rather than the user ) , coupled   with interactive hill climbing on failed tests . We   recruited six additional participants to verify if   the model failures for their political perspective   are things they could see themselves appropriately   posting online , and report the validation rate in   Figure 6B. Participants had their tests validated by   additional raters twice as often when they were   writing tests reÔ¨Çecting their own political perspec-   tive ( in - group vs out - group ) .   These results indicate that non - experts with   AdaTest are much more e  ective testers , even with   minimal instruction and experience . The fact that   users writing tests for another group resulted in a   much poorer representation of that group indicates   it might be important to Ô¨Ånd testers from di  erent   groups that could be impacted by a model . Since   it is often not practical to Ô¨Ånd experts from every   impacted group , empowering non - experts with a   tool like AdaTest can be very valuable .   3.2 Debugging Loop   We evaluate the scenario where a user has found   a bug ( or set of bugs ) and wants to Ô¨Åx it . As base   models , we Ô¨Ånetune RoBERTa - Large for duplicate   question detection on the QQP dataset ( Wang et al . ,   2019 ) , and for 3 - way sentiment analysis on the SST   dataset ( Socher et al . , 2013 ) . We rely on CheckList   suites made available by Ribeiro et al . ( 2020 ) for   evaluation , using a 20 % failure rate threshold for a   topic to ‚Äú fail ‚Äù . The base model fails 22out of 53   QQP topics and 11 out of 39 Sentiment topics .   We create data in order to ‚Äú Ô¨Åx ‚Äù a topic by either   taking n=50examples from the topic ‚Äôs data in the   CheckList condition , or starting from a seed of 5   examples and running the Debugging Loop with   AdaTest until Ô¨Ånding failures becomes qualitatively   dicult ( on average 2:83rounds for QQP and3:83   rounds for Sentiment ) , yielding an average of 41:6   tests for QQP and55:8tests for Sentiment . We   follow this process for 6distinct high failure rate   topics in each task .   Given a set of ‚Äú Ô¨Åxing ‚Äù data from a single   test topic or from multiple topics , we Ô¨Ånetune   RoBERTa - Large from the previous checkpoint on   an equal mixture of Ô¨Åxing data and data from the   original training set to prevent catastrophic forget-   ting ( McCloskey and Cohen , 1989 ) , until conver-   gence . Ideally , we want to Ô¨Åx the original topic   ( and perhaps a few more which are also impacted   by similar bugs ) without adding new bugs , and thus   we evaluate the ‚Äú Ô¨Åxed ‚Äù models by measuring how   many topics in the original CheckList suite they   ‚Äú Ô¨Åx ‚Äù or ‚Äú break ‚Äù , i.e. move from error rate from   greater than 20 % to lower than 20%or vice versa .   For each set of Ô¨Åxing data , we Ô¨Ånetune RoBERTa   3times with di  erent random seeds , draw 5;000   bootstrap samples of the predictions , and consider   that a topic is Ô¨Åxed or broken if the change is sig-   niÔ¨Åcant with an FDR signiÔ¨Åcance level less than   0:05 ( Benjamini and Hochberg , 1995 ) .   We present the results in Figure 7 , where we   vary the number of topics used for training in the   xaxis ( for each tick , we sample 3random topic3258   subsets of size xand average the results ) . In the   vast majority of cases , AdaTest Ô¨Åxes the topics   used for training and a number of other topics   without breaking anytopics , while CheckList data   often introduce new bugs ( and thus break other test   topics ) . Part of this may be due to higher diversity   in terms of sentence structure and length in the   AdaTest generated data , as compared to a Ô¨Åxed   CheckList template . However , models Ô¨Ånetuned   only on data from the Ô¨Årst round of the Testing   Loop ( roughly equivalent to CheckList , but with   more diversity ) also tend to break other topics ,   which supports the importance of an iterative   debugging loop . Qualitatively , we repeatedly   observed the phenomenon illustrated in Figure 3 ,   where the model initially uses oversimpliÔ¨Åed   shortcuts to Ô¨Åx a set of tests , i.e. data from a   single round often introduces non - obvious bugs   that only get discovered and Ô¨Åxed in following   rounds . For example , one of the topics for QQP is   f(more X , less antonym(X) ) = dupl . ,   with examples like ( ‚Äú How do I become more pa-   tient ‚Äù , ‚Äú How do I become less irritable ‚Äù ) . Ribeiro   et al . ( 2020 ) anticipated a potential orderingshortcut , since the topic also contains examples of   ‚Äú ( less X , more antonym(X ) ) ‚Äù . After training on   such data , AdaTest surfaces a bug where examples   in the form ‚Äú ( more X , more antonym(x ) ) ‚Äù are   predicted as duplicates , as well as examples   of unrelated predicates like ( ‚Äú more British ‚Äù ,   ‚Äú less American ‚Äù ) . None of the topics in the   suite capture these exact behaviors , but similar   shortcuts break topics that arepresent such as   f(more X , less X ) , dupl . . The iterative   Debugging Loop identiÔ¨Åes and Ô¨Åxes such shortcuts ,   leading to more robust bug Ô¨Åxing .   We evaluate accuracy on the validation dataset   and on challenging out of domain datasets ( Zhang   et al . , 2019 ; Potts et al . , 2021 ) after training on all   6topics ( Table 1 ) . In both tasks , AdaTest augmen-   tation has a negligible or non - signiÔ¨Åcant impact   on in - domain accuracy , and improves performance   on out of domain data . While AdaTest may have   introduced new bugs not caught by the CheckList   test suite or these additional test sets , the improved   performance on all of these indicate that the De-   bugging Loop is not Ô¨Åxing bugs at the expense of   signiÔ¨Åcantly degrading performance elsewhere . We   also compare AdaTest to labeled Polyjuice coun-   terfactuals ( Wu et al . , 2021 ) available for QQP . De-   spite having more data ( thousands vs AdaTests ‚Äô   250 labels ) , the results are strictly inferior ( accu-   racy 37:8on PAWS , Ô¨Åxed 2 topics and broke 1 ,   while Adatest Ô¨Åxes 11 and breaks none ) .   3.3 Case Studies   Non - expert testing of non - classiÔ¨Åcation models   In order to evaluate if AdaTest would help non-   experts test models for more complex tasks , we   recruited a bilingual speaker with no technical back-   ground , and asked them to test a translation system   and an NER system commercialized by a large   software company ( and thus subject to extensive   prior testing and validation ) . SpeciÔ¨Åcally , we asked   the user to Ô¨Ånd English to Portuguese translations   with inconsistent or wrong gender assignments ( e.g.   the equivalent of ‚Äú My ( female ) wife ( female ) is a   ( male ) doctor ( male ) ‚Äù ) , and to test NER predictions   of the PERSON category . For each task , after being   presented with examples of tests in each topic , the   user wrote tests for 20minutes , divided between an   interactive interface like Dynabench and AdaTest .   Even though the tasks are very di  erent ( gener-   ation and per - token classiÔ¨Åcation ) , the results are   consistent with Section 3.1 , with the user Ô¨Ånding3259many more bugs with AdaTest ( 32vs4on transla-   tion , 16vs0on NER ) . Qualitatively , adaptive test   suggestions helped the user Ô¨Ånd bugs covering a   much wider range of phenomena than all of the   attempts without assistance . For example , the user   manually wrote di  erent combinations of 15sub-   jects and 11predicates for translation , all related to   family members and professions ( e.g. ‚Äú My mom   is a doctor ‚Äù ) . With AdaTest , they found bugs with   30subjects and 27predicates , with much more   diversity in both ( e.g. ‚Äú The woman with the red   dress is my best friend ‚Äù ) . AdaTest helped the user   Ô¨Ånd a variety of sentences where the NER model   predicted the label ‚Äú Person ‚Äù for names of organiza-   tions ( e.g. ‚Äú What I do for Black Booty is provide   Ô¨Ånancial advice ‚Äù ) , products ( e.g. ‚Äú I think Alikat   is a good form of cash money ‚Äù ) , and animals ( e.g.   ‚Äú Nathan the dog likes to spend time at the farm ‚Äù ) ,   while they could not Ô¨Ånd any bugs unassisted .   Text to video matching To gauge the useful-   ness of AdaTest for established model develop-   ment and maintenance pipelines , we shared AdaT-   est with a ML development team in charge of a   multi - modal classiÔ¨Åer that matches textual inputs   with a database of videos . While their produc-   tion model had gone through several external red-   teaming reviews , a single short ( unaided ) AdaTest   session revealed novel gender bias and related is-   sues that were then fed back into their custom miti-   gation pipeline . The team reported that being able   to quickly generate diverse model - targeted tests ,   while at the same time creating a suite of tests for   future model versions was extremely valuable , and   they have since sought to develop adaptive test trees   for their whole suite of production models .   Task detection A team of ML scientists at a large   software company was building a model to predict   whether a sentence in an email or meeting note rep-   resents an action item or task , such as ‚Äú I will run the   experiment tomorrow ‚Äù . Prior to our engagement ,   the team had gone through a painstaking process   of gathering and labeling data , using CheckList   ( Ribeiro et al . , 2020 ) to Ô¨Ånd bugs , and generating   data with GPT-3 to Ô¨Åx the discovered bugs . The   team was thus well versed in testing , and had been   trying to accomplish the same goals that AdaTest   is built for , using the same exact LM .   After a Ô¨Åve minute demo , two of the team mem-   bers engaged in the Testing Loop for an hour . In   this short session , they found many previously   unknown bugs , with various topics they had n‚Äôt   thought about testing ( e.g. ‚Äú While X , task ‚Äù , as   in ‚Äú While we wait for the manufacturer , let ‚Äôs build   a slide deck ‚Äù ) , and some they had tested and ( incor-   rectly ) thought they had Ô¨Åxed ( e.g. false positives   related to waiting , such as ‚Äú John will wait for the   decision ‚Äù or ‚Äú Let ‚Äôs put a pin on it ‚Äù ) . When testing   name invariances with CheckList they had n‚Äôt in-   cluded personal pronouns ( e.g. ‚Äú Karen will imple-   ment the feature ‚Äù = ‚Äú I will implement the feature ‚Äù ) ,   which AdaTest revealed the model fails on .   One team member ran the Debugging Loop for   approximately 3 hours , Ô¨Åxing bugs with the same   procedure as in Section 3.2 . Consistent with the   previous results , they found that Ô¨Åxing bugs ini-   tially led to new bugs being introduced , e.g. Ô¨Åxing   false negatives on passive statements ( ‚Äú the experi-   ment will be run next week ‚Äù ) lead to false positives   on non - task factual descriptors ( ‚Äú the event will be   attended by the dean ‚Äù ) , which were surfaced by   AdaTest and Ô¨Åxed in the next round . In order to   compare the results of using AdaTest to their pre-   vious e  orts , we collected and labeled two new   datasets from sources they had n‚Äôt used as training   data . We present the F1 scores of models aug-   mented either with their GPT-3 generated data or   on AdaTest data in Table 2 , where AdaTest shows   signiÔ¨Åcant improvement despite involving much   less e  ort . Qualitatively , the team noted that Ô¨Ånd-   ing bugs with AdaTest was much easier than with   CheckList , by virtue of the extensive suggestions   made by the LM . Similarly , after noticing ( and Ô¨Åx-   ing ) potential shortcuts in multiple rounds of the   Debugging Loop , the team realized that their prior   GPT-3 augmentation was almost certainly liable to   such shortcuts , and thus less e  ective .   3.4 Discussion   We evaluated AdaTest on 8 di  erent tasks spanning   text classiÔ¨Åcation , generation , and per - token predic-   tion . In terms of Ô¨Ånding bugs , we compare AdaTest   to experts using CheckList and non - experts using   a more responsive version of Dynabench . Users3260consistently found many more bugs per minute   with AdaTest on research models and commercial   models at di  erent development stages ( early ver-   sion , pre - release , and mature models in production ) .   The fact that AdaTest requires minimal training   and is easy enough to be used by users without   any technical background is an asset , especially   when it is important to have testers that represent   diverse groups that may be negatively impacted by   bugs . In terms of Ô¨Åxing bugs , we compared the   Debugging Loop to naively augmenting data with   CheckList templates , using Polyjuice counterfac-   tuals , and having an expert use GPT-3 to create   additional data . In every case , AdaTest improved   performance more than alternatives , and crucially   did not add new bugs that degrade performance on   available measurements , due to the iterative nature   of the Debugging Loop . In contrast to alternatives ,   further testing with AdaTest is low - cost , and thus   this augmentation does not have the e  ect of in-   validating costly evaluation data ( e.g. invalidating   CheckList tests that are laborious to create ) . In fact ,   test trees from previous sessions can be used to test   new models , or to bootstrap a new AdaTest session .   4 Related Work   Even though we used CheckList and Dynabench as   baselines in the previous section , our results indi-   cate that these and other approaches ( Gardner et al . ,   2020 ; Kaushik et al . , 2019 ) where human creativ-   ity and e  ort are bottlenecks ( Bhatt et al . , 2021 )   would beneÔ¨Åt from the greatly enhanced bug dis-   covery productivity made possible by AdaTest . On   the other hand , CheckList as a framework provides   great guidance in organizing the test tree , enumer-   ating important capabilities and perturbations to be   tested , as well as a tool for systematically apply-   ing the test tree to future models . Similarly , Dyn-   abench provides model serving capabilities and   a crowdsourcing platform that would greatly en-   hance AdaTest , especially as users share test trees   and adapt them to new models .   In terms of Ô¨Åxing bugs , fully automatic data aug-   mentation with LMs ( Yoo et al . , 2021 ; Wang et al . ,   2021 ) can not incorporate human ‚Äú speciÔ¨Åcation ‚Äù be-   yond already existing data , nor debug phenomena   that is very far from the existing data . On the other   hand , general purpose or contrastive counterfactu-   als have shown mixed or marginally positive re-   sults ( Huang et al . , 2020 ; Wu et al . , 2021 ) similar   to what we observed in Section 3.2 , except whenlarge quantities of data are gathered ( Nie et al . ,   2020 ) . Our hypothesis is that underspeciÔ¨Åcation   ( D‚ÄôAmour et al . , 2020 ) is a major factor limiting   the beneÔ¨Åt of many counterfactual augmentation   techniques . We observed that the Ô¨Årst rounds of   the Debugging Loop often decrease or maintain   overall performance until additional data from later   rounds speciÔ¨Åes the correct behavior more thor-   oughly , which indicates that counterfactual data   targeted precisely where the model is underspeci-   Ô¨Åed is often more e  ective than non - targeted data .   If true , this hypothesis argues for AdaTest ‚Äôs fast   iteration in the Debugging Loop , rather than longer   cycles ( e.g. Dynabench rounds can take months ) .   5 Conclusion   AdaTest encourages a close collaboration between   a human and a language model , yielding the ben-   eÔ¨Åts of both . The user provides speciÔ¨Åcation that   the LM lacks , while the LM provides creativity   at a scale that is infeasible for the user . AdaT-   est o  ers signiÔ¨Åcant productivity gains for expert   users , while also remaining simple enough to em-   power diverse groups of non - experts . The Debug-   ging Loop connects model testing and debugging   to e  ectively Ô¨Åx bugs , taking model development   a step closer towards the iterative nature of tra-   ditional software development . We have demon-   strated AdaTest ‚Äôs e  ectiveness on classiÔ¨Åcation   models ( sentiment analysis , QQP , toxicity , media   selection , task detection ) , generation models ( GPT-   2 , translation ) , and per - token models ( NER ) , with   models ranging from well - tested production sys-   tems to brand new applications . Our results indi-   cate that adaptive testing and debugging can serve   as an e  ective NLP development paradigm for a   broad range of applications . To help support this ,   AdaTest ( with various test trees ) is open sourced at   https://github.com/microsoft/adatest .   Acknowledgements   We thank Adarsh Jeewajee , Carlos Guestrin , Ece   Kamar , Fereshte Khani , Gregory Plumb , Gabriel   Ilharco , Harsha Nori , Sameer Singh , and Shikhar   Murty for helpful discussions and feedback . We   also thank Bruno Melo , Hamid Palangi , Ji Li , and   Remmelt Ammerlaan for pilot testing /case studies .   Finally , we thank Tongshuang Wu for all of the   above andhelping us think about Ô¨Ågures , check-   ing translations , o  ering LTEX advice , and other   miscellaneous help.3261References3262   A Language model prompt design   The test suggestion function inside the AdaTest   Testing Loop ( main text Figure 1 ) is implemented   using a large - scale generative LM . We used GPT-3   ( Brown et al . , 2020 ) in our experiments , but we also   support open source HuggingFace models ( Wolf   et al . , 2020 ) . When provided with a prompt in the   form of a list of items , these large LMs can gener-   ate new items that continue the list , and come from   the same distribution of items as the original list .   By carefully controlling the structure and content   of this list , we can steer large LMs to generate new   content on nearly any topic in nearly any form ( ex-   ceptions being very long - form text , and languages   unseen by the LM during training ) .   There is always a current focus topic active dur-   ing the Testing Loop , and it is the goal of the LM   test suggestion process to generate new tests that   will be categorized by the user as direct children of   the focus topic . This means we are not interested   in tests outside the focus topic or inside already-   deÔ¨Åned subtopics of the focus topic . We avoid   tests outside the topic in order to maintain a ‚Äú focus ‚Äù   on the current topic the user has selected , and we   avoid tests inside subtopics because these represent   portions of the current topic that have already been   well explored , and so should be prevented from   dominating the test suggestions . If the user is inter-   ested in a particular subtopic , they simply open it   and generate suggestions speciÔ¨Åc to that topic . In   addition to allowing users to guide the LM , focus3263topics also improve the quality of the LM ‚Äôs sug-   gestions , since LMs tend to generate higher quality   tests when restricted to a narrower scope . Topics   also enable zero - shot LM test generation for empty   topics , since we can condition on the topic when   generating a test and so use examples from related   topics as demonstrations for the current topic .   The LM prompt itself consists of several tests ( 7   by default ) selected from the current focus topic ( or   from nearby topics if the current topic is empty ) . A   test is written into the prompt as a topic , followed   by a space - separated list of values on the next line   ( see Figure 8) . Prompt parameters are conÔ¨Ågurable ,   but we found that 7 examples gave an appropriate   amount of steering information to GPT-3 ( for both   the Davinci and Curie models ) without giving so   many examples that strong patterns would harm the   diversity of the generated tests . We experimented   with a variety of prompt formats , including priming   with ‚Äú instruction ‚Äù sentences , and found that the   more minimal the notation the better , so as to bias   the generation process as little as possible . We   also remove as much information from the prompt   as possible to further focus and de - bias the LM .   For example , we do not include expected outputs if   they are the same for all the tests in the prompt , and   similarly we only include topic information when   using tests from outside the current focus topic .   We also repeatedly generate a single next list item ,   rather than generating several items in a list . This   is because generating a long list usually reduces   diversity , as generated items tend to converge to a   single topic .   Given a prompt structure and a set of tests in the   current topic , steering the test suggestion genera-   tion comes down to choosing a set tests to include   in the LM prompt . We do this by scoring all tests   as the product of several factors , then selecting the   highest scoring test and adding it to the prompt list .   This process is iterated unless a su cient number   of tests have been selected to be included in the   prompt . This list is then reversed prior to sampling   from the LM , because the LM weights samples   close to the end of the prompt more strongly ( Zhao   et al . , 2021 ) . The factors we use for test selection   are :   ¬àTest failure score - Tests with higher scores are   tests that the model fails or is closer to failing   than tests with lower scores . So the strongest   ranking factor we use ( other than topic mem-   bership ) is high test failure score , since this   facilitates hill climbing towards model fail-   ures .   ¬àTopic membership - Tests outside the current   topic are very strongly penalized and are only   used if the current topic is empty or nearly   empty . Tests inside subtopics of the current   topic are also strongly penalized for the rea-   sons mentioned above ( that these represent   already explored regions of the topic ) .   ¬àScore randomization - Test failure scores can   be computed in many di  erent ways , but they   are often continuous values that represent how   close a model ‚Äôs prediction is to failing a test   ( or how far it is past the failure threshold ) .   Tests with very similar scores have an equally   likely chance of being good for prompt inclu-   sion ( since they each can lead the LM towards   high - scoring on - topic tests ) . To encourage di-   verse choices among similar scoring tests we   add one standard deviation of random Gaus-   sian noise to the test scores .   ¬àSkip randomization - Sometimes a strong fail-   ure found early on in a topic would always be   selected for the top prompt position since its   score is so much higher than any other current   tests . However this can harm diversity so we3264   also introduce skip randomization where we   randomly skip over tests ( by penalizing their   score ) with 25 % probability .   ¬àPrompt diversity - When exploring in a topic   we want to encourage a broad sample of test   structures to be included in the prompt , so that   we fully explore the topic and do n‚Äôt get locked   into a single style of test . To promote this , we   penalize each test score by the cosine simi-   larity of that test ‚Äôs embedding to the closest   embedding of a test that has already be se-   lected for inclusion in the prompt . By default   we use RoBERTa - base ( Liu et al . , 2019 ) for   this , though any similarity embedding would   work .   We repeat the test selection process rtimes to   create rdi  erent prompts ( where we maximize r   subject to not causing more than a 50 % increase in   computational overhead due to lost prompt reuse   during completions ) . If the user has requested K   suggestions for a round , then for each prompt we   ask the LM to generate bK = rccompletions that are   parsed to produce at most that many tests ( at most , since some completions may produce invalid or   duplicate tests ) . These tests are then applied to   the target model ( or several models , since we can   explore multiple models in parallel ) , sorted by test   failure score , and returned to the user for Ô¨Åltering   and organization .   B User interface   The entire Testing Loop process occurs through   AdaTest ‚Äôs interactive web interface , which works   both as a standalone server or inside a Jupyter note-   book . Figure 9 shows a screenshot of this interface ,   browsing the top node of a test tree targeting the   Azure sentiment analysis model ( Google ‚Äôs model   is also being scored , but is not adaptively targeted ) .   While we experimented with interfaces that present   the entire test tree to the user at once , these became   intractable for larger test trees . Thus , we follow   traditional Ô¨Åle system browsers , which scale well   to very large and deep trees .   On the left side of Figure 9 is a list of topics   based on CheckList capabilities ( Ribeiro et al . ,   2020 ) . These are top - level topics , some of   which are well explored with many subtopics ( e.g.3265   /Fairness ) , while others have yet to be explored by   the user ( such as /Logic ) . To enable users to or-   ganize the test tree , topics can be edited , opened ,   and dragged and dropped just like in a standard Ô¨Åle   viewer .   On the right side of Figure 9 there are two   columns representing the test failure scores for two   target models , Azure and Google sentiment anal-   ysis . The horizontal position of the colored bars   represents the value of a single test ‚Äôs score and the   color denotes passing or failing . Since each bar   represents a single test inside a topic , hovering the   mouse over the bar will show the associated test .   Hovering anywhere over a row also shows the num-   ber of failing and passing tests for the topic ( the   total counts for the current topic are shown at the   bottom ) . Note that topics are sorted by the largest   test fail score they contain . The grey box above   the test topics is where LM test suggestions are   shown . If the user clicked the suggestions button   in Figure 9 , they would get a list of suggested tests   designed to not fall into any of the current topics .   This is very challenging at such a high level of   abstraction , so the precision of these suggestions   might be low , but Ô¨Ånding such tests is often still   possible given enough iteration . Once a few such   tests are found , a new top level topic can be formedand explored . An alternative to this process ( which   tends to work better for high level concepts ) is to   ask AdaTest to suggest new topic names ( done the   same way we suggest new tests ) . Given a starting   test tree , users can potentially Ô¨Åll out whole new   sub - trees without ever writing anything manually   by alternating between topic suggestions and zero-   shot test suggestions for new topics . In general , the   precision of the test suggestion process increases as   the topics grow narrower , so expanding subtopics   topic will likely be much easier than the parent   topic . To jump start this process users can always   manually add tests or topics by clicking the respec-   tive add buttons at the top right , or by editing a   current test ( scores are recomputed in real - time ) .   Figure 10 shows what happens after   we navigate down the topic tree into the   /Negation /Negated positive topic , and then request   LM suggestions . Current tests inside the topic are   shown at the bottom sorted by their test failure   score for the Azure model ( and continue on   past the screen capture ) while test suggestions   are shown in the gray box at the top . The test   suggestions box is scrollable and contains ~100   suggested tests ( also sorted by their test failure   score for the Azure model).3266The selected test suggestion in Figure 10   is highlighted and the test failure scores are   shown for both models . The highlighted test   is a valid high scoring test that falls within the   /Negation /Negated positive topic , so the user can   add it to the current topic in one of several ways :   dragging it down to the list of in - topic tests , click-   ing the " plus " button on the left of the test row ,   hitting Enter , etc . Note that the test directly below   the selected test is also high scoring on the Azure   model , but the test is invalid since the input text   actually does express a positive sentiment , so the   expectation of the test is incorrect.3267