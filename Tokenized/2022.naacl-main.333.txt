  Yanpeng ZhaoJack HesselYoungjae Yu   Ximing LuRowan ZellersYejin ChoiInstitute for Language , Cognition and Computation , University of EdinburghPaul G. Allen School of Computer Science & Engineering , University of WashingtonAllen Institute for Artiﬁcial Intelligence   Abstract   Machines that can represent and describe en-   vironmental soundscapes have practical poten-   tial , e.g. , for audio tagging and captioning . Pre-   vailing learning paradigms of audio - text con-   nections have been relying on parallel audio-   text data , which is , however , scarcely available   on the web . We propose vip - AnT that induces   Audio- Text alignment without using any par-   allel audio - text data . Our key idea is to share   the image modality between bi - modal image-   text representations and bi - modal image - audio   representations ; the image modality functions   as a pivot and connects audio and text in a tri-   modal embedding space implicitly .   In a difﬁcult zero - shot setting with no paired   audio - text data , our model demonstrates   state - of - the - art zero - shot performance on the   ESC50 and US8 K audio classiﬁcation tasks ,   and even surpasses the supervised state of the   art for Clotho caption retrieval ( with audio   queries ) by 2.2 % R@1 . We further investigate   cases of minimal audio - text supervision , ﬁnd-   ing that , e.g. , just a few hundred supervised   audio - text pairs increase the zero - shot audio   classiﬁcation accuracy by 8 % on US8K. How-   ever , to match human parity on some zero - shot   tasks , our empirical scaling experiments sug-   gest that we would need about 2≈2 M su-   pervised audio - caption pairs . Our work opens   up new avenues for learning audio - text connec-   tions with little to no parallel audio - text data .   1 Introduction   Environmental sound provides rich perspectives   on the physical world . For example , if we hear :   joyful laughing , a playful scream , and a splash ; we   not only can visualize literal objects / actions that   might have given rise to the audio scene , but also ,   we can reason about plausible higher - level facets ,   e.g. , a child speeding down a water slide at a water   park , splashing through the water ( see Figure 1).Figure 1 : vip - AnT pivots audio and text via visual   imagination .   Machines capable of parsing , representing , and   describing such environmental sound hold practical   promise . For example , according to the National   Association of the Deaf ’s captioning guide , accessi-   ble audio caption generation systems should go be-   yond speech recognition ( i.e. , identifying speakers   and transcribing the literal content of their speech )   and provide the textual description of all the sound   effects , e.g. , “ a large group of people talking excit-   edly at a party ” , in order to provide the full infor-   mation contained in that audio .   The dominant paradigm for studying machine   hearing ( Lyon , 2010 ) has been through human-   annotated audio - text data , where text is either   free - form audio descriptions ( e.g. , “ the sound of   heavy rain ” ) or tagsets ( Salamon et al . , 2014 ;   Gemmeke et al . , 2017 ; Kim et al . , 2019 ; Drossos   et al . , 2020 ) . But existing supervised audio - text   resources are limited . While some audio - text   co - occurences can be sourced from audio - tag co-   occurrences ( Font et al . , 2013 ) or from video cap-   tioning data ( Rohrbach et al . , 2015 ; Xu et al . , 2016 ;   Oncescu et al . , 2021a ) , they are either not sufﬁ-   ciently related to environmental sound or limited   in their scale and coverage.4492   In this paper , we study large - scale audio - text   alignment without paired audio - text ( AT ) data . In-   spired by pivot - based models for unsupervised ma-   chine translation ( Wu and Wang , 2007 ; Utiyama   and Isahara , 2007 ) , we propose vip - AnT , short for   VIsually Pivoted Audio and ( N)Text.vip - AnT   uses images as a pivot modality to connect audio   and text . It parallels our motivating example : hear-   ing a sound , humans can visually imagine the asso-   ciated situation and literally describe it . Pivoting   is practically viable because there are abundantly   available image - text ( VT ) and video - audio ( V A )   co - occurrences on the web , from which bimodal   correspondence models can be trained ( see Fig-   ure 2 ) . By linking audio and text implicitly via   the combination of the VT and V A models , we en-   able zero - resource connection between audio and   text , i.e. , vip - AnT can reason about audio - text   connections despite never having observed these   modalities co - occur explicitly .   We evaluate on zero - shot audio - text retrieval and   zero - shot audio classiﬁcation . On the Clotho cap-   tion retrieval task ( Drossos et al . , 2020 ) , without   any parallel AT data , vip - AnT surpasses the su-   pervised state of the art by 2.2 % R@1 ; on zero-   shot audio classiﬁcation tasks , it establishes new   state of the arts , achieving 57.1 % accuracy on   ESC50 ( Piczak , 2015 ) and 44.7 % accuracy on   US8 K ( Salamon et al . , 2014 ) . We also show that   the zero - resource pivoting AT model vip - AnT canbe improved by :   ( 1)Unsupervised curation : whereby noisy AT   pairs are explicitly mined from the pivoting model   and serve as additional training data ( e.g. , +5.7 %   on ESC50 and +9.3 % on US8 K ) ; and   ( 2)Few - shot curation : whereby a small number   of human - annotated audio caption pairs are made   available at training time ( e.g. , a few hundred pairs   increases the zero - shot audio classiﬁcation accu-   racy by 8 % on US8 K ) .   However , for ESC-50 , according to the empiri-   cal scaling relationship we ﬁnd , it would require   around 2≈2Maligned audio - text pairs for the   zero - shot model to match human parity on ESC50   under our setup , which is an order - of - magnitude   more than the largest currently - available audio - text   corpus of Kim et al . ( 2019 ) .   2 Related work   Supervised audio representation learning .   While automatic speech recognition has been a   core focus of the audio processing community ,   environment sound classiﬁcation has emerged as a   new challenge and is drawing more attention ( Sala-   mon et al . , 2014 ; Piczak , 2015 ; Gemmeke et al . ,   2017 ) . Some prior work in learning sound event   representations are supervised by category labels   ( Dai et al . , 2017 ; Boddapati et al . , 2017 ; Kumar   et al . , 2018 ; Guzhov et al . , 2021b ; Gong et al . ,   2021 ) . Others use weaker forms of supervision   for tagging ( Kumar and Raj , 2017 ; Kong et al . ,   2018 ) and localization ( McFee et al . , 2018 ; Kim   and Pardo , 2019 ) .   Learning audio representations from visual   imagination . There are two main paradigms for   using visual information to derive audio represen-   tations . In the two - stage setup , an image encoder is   ﬁrst pre - trained ; these weights are used as the ini-   tialization of the supervised audio model ( Guzhov   et al . , 2021b ; Gong et al . , 2021 ) . The other adopts   contrastive learning : it exploits the image - audio   alignment inherent in videos and learns audio and   image / video representations jointly ( Korbar et al . ,   2018 ; Wang et al . , 2021 ; Nagrani et al . , 2021 ) .   We use insights from both directions by ( 1 ) using   CLIP ’s image encoder , which has been pre - trained   on image - text pairs ( Radford et al . , 2021 ) , to initial-   ize an audio encoder and ( 2 ) using contrastive pre-   training on image - audio pairs . Throughout training ,   we do not require any labeled images or audio.4493   Tri - modal learning of audio - text alignment .   Our work extends recent work that generalizes the   bi - modal contrastive learning to a tri - modal set-   ting ( Alayrac et al . , 2020 ; Akbari et al . , 2021 ) .   While they also connect audio and text implic-   itly by using images as a pivot , the quality of this   audio - text alignment has rarely been studied . To   our knowledge , we present the ﬁrst comprehensive   evaluation of the inferred audio - text alignment via   zero - shot retrieval / classiﬁcation .   The work closest to ours are Audio-   CLIP ( Guzhov et al . , 2021a ) and Wav2CLIP ( Wu   et al . , 2021 ) . AudioCLIP ’s pre - training setup is   similar to ours , but requires human - annotated   textual labels of audio , while ours does not .   Wav2CLIP is concurrent with our work ; while   similar - in - spirit , our model not only performs sig-   niﬁcantly better , but also , we more closely explore   methods for improving audio - text alignment , e.g. ,   unsupervised curation .   Pivot - based alignment models . The pivoting   idea for alignment learning can date back to Brown   et al . ( 1991 ) . Language pivots ( Wu and Wang ,   2007 ; Utiyama and Isahara , 2007 ) and image piv-   ots ( Specia et al . , 2016 ; Hitschler et al . , 2016 ;   Nakayama and Nishida , 2017 ) have been explored   in zero - resource machine translation . Pivot - based   models have also been shown to be helpful in learn-   ing image - text alignment ( Li et al . , 2020 ) .   3 Model   We ﬁrst formalize tri - modal learning by assum-   ing available co - occurrence data for every pair of   modalities ( § 3.1 ) . Then we present bi - bi - modal   pre - training as an alternative when there is no   paired audio - text data , and implement vip - AnT   via bi - bi - modal pre - training ( § 3.2 ) . Finally , we   describe model variants for cases of varying AT   supervision ( § 3.3 ) .   3.1 Tri - modal representation learning   Tri - modal representation learning between images ,   audio , and text aims to derive representations from   co - occurrence patterns among the three modali-   ties ( Alayrac et al . , 2020 ; Akbari et al . , 2021 ) . We   consider a simple tri - modal representation space ,   which relies on encoding functions g : V→V ,   g : A→A , and g : T→Tto map images   v , audio a , and text t(v∈V , a∈A , andt∈T ) ,   respectively , to a shared vector space : v , a , t∈R   ( v∈V , a∈A , andt∈T ) . Instead of pre-   specifying the precise semantics of this continu-   ous space , vector similarities across modalities are   optimized to reconstruct co - occurrence patterns   in training corpora , i.e. , two vectors should have   a higher dot product if they are more likely to   co - occur . We use contrastive learning with the   InfoNCE loss ( Sohn , 2016 ; van den Oord et al . ,   2018 ):   L(A , B ) =   /summationdisplayexps(a , b)/summationtextexps(a , b)+exps(a , b)/summationtextexps(a , b),(1 )   where A , B are two sets of data points from two   different modal domains , respectively ; a , b4494are vector representations of the co - occuring pair   ( a , b)which are encoded by g(a)and   g(b ) , respectively ; s(a , b)computes the simi-   larity between aandb , which we take to be scaled   cosine similarity .   If we had access to co - occurrence data between   all pairs of modalities , we could optimize the tri-   modal loss :   L(V , A , T ) =   L(V , A ) + L(A , T ) + L(V , T).(2 )   3.2 Visually pivoted audio and text   Differently from image - text and image - audio pairs ,   which are abundantly available on the web , audio-   text data is scarce . Instead of Equation 2 , in   vip - AnT , we consider a “ bi - bi - modal " loss , which   does n’t require AT data .   L(V , A , T ) = L(V , A ) + L(V , T).(3 )   The image encoder is shared between the V A   alignment model ( i.e. , L(V , A ) ) and the VT   alignment model ( i.e. , L(V , T ) ) and thus pro-   vides a zero - resource connection between audio   and text in the tri - modal embedding space implic-   itly .   3.2.1 Model architecture   Image and text encoders . Instead of learning   gandgfrom scratch , we build on a pre - trained   CLIP model , which has been pre - trained on We-   bImageText ( WIT ) , a dataset of 400 million image-   text pairs gathered from the internet ( Radford et al . ,   2021 ) . CLIP has been shown highly performant   on VT tasks , e.g. , zero - shot image classiﬁcation .   We use the ViT - B/32 model in this work , which   consists of a 12 - layer vision Transformer ( ViT ) and   a 12 - layer language Transformer ( Vaswani et al . ,   2017 ; Dosovitskiy et al . , 2021 ) . Given CLIP ’s   strong VT alignment , we use its image encoder   asgand text encoder as g. During learning , g   andgare kept frozen and thus the joint VT rep-   resentation space is untouched ( see Figure 3 ) . We   minimize only the ﬁrst loss term of Equation 3 :   minL(V , A ) , ( 4 )   where Θare the trainable parameters of the audio   encoder g.   Audio encoder . Our audio encoder has the same   vision Transformer architecture as CLIP ’s image   encoder ( ViT - B/32 ) . In § 4 , we show that initializ-   ing the audio encoder with CLIP ’s visual weights   signiﬁcantly improves convergence speed and accu-   racy . The architectural modiﬁcations which enable   the use of visual CLIP ’s architecture for audio are   ( see Figure 4 for an illustration ):   ( 1)We customize the convolution stride to allow   for overlaps between neighbor patches of Spectro-   gram features of audio .   ( 2)In the input embedding layer , we average the   kernel weights of the convolution layer along the   input channel to account for 1 - channel Mel-ﬁlter   bank features of audio ( cf . RGB channels of im-   ages ) .   ( 3)We up - sample the 2 - dimensional positional em-   beddings of image tokens to account for longer   audio token sequences .   3.2.2 Bi - bi - modal pre - training details   Video - audio co - occurences . To optimize Equa-   tion 4 , we gather V A co - occurrences from Au-   dioSet ( AS ; Gemmeke et al . ( 2017)),which con-   tains temporally aligned audio and video frames   from 10 - second clips gathered from around 2 mil-   lion YouTube videos . To construct aligned image-   audio pairs from AS , we adopt a sparse sampling   approach ( Lei et al . , 2021 ): we ﬁrst , extract four4495   equal - spaced video frames from each clip . Then ,   during training , we randomly sample a frame from   the four , and treat it as co - occurring with the cor-   responding audio clip . At test time , we always use   the second video frame as the middle frame to con-   struct image - audio pairs . We use the unbalanced   training set , which consists of around 2 million   video clips , to pre - train the audio encoder . Since   AudioSet does not provide an ofﬁcial validation   set , we validate the audio encoder and tune model   hyperparameters on the balanced training set .   Audio preprocessing . We use Kaldi ( Povey   et al . , 2011 ) to create Mel-ﬁlter bank features   ( FBANK ) from the raw audio signals . Speciﬁcally ,   we use the Hanning window , 128 triangular Mel-   frequency bins , and 10 millisecond frameshift . We   always use the ﬁrst audio channel when an audio   clip has more than one channel . We apply two   normalizations : ( 1 ) before applying Kaldi , we sub-   tract the mean from the raw audio signals ; and ( 2 )   we compute the mean and standard deviation of   FBANK on the unbalanced AS training set , and   then normalize the FBANK of each audio clip . For   data augmentation , inspired by Gong et al . ( 2021 ) ,   we use frequency masking and time masking : we   randomly mask out one-ﬁfth FBANK along the   time dimension and one - forth FBANK along the   frequency dimension during training .   Training dynamics . The architecture of our au-   dio encoder follows the vision Transformer of CLIP   ( ViT - B/32 , see Radford et al . ( 2021 ) for more de-   tails ) . For the trade - off of efﬁciency and efﬁcacy ,   we set the convolution stride to 16×24 . This re - sults in around 300 audio tokens for a kernel size of   32×32and an input size of 1000×128(all in the   form of time×frequency ) . We optimize the model   with LARS ( You et al . , 2017 ) , where the initial   learning rates for model weights and model biases   are set to 2e-1 and 4.8e-3 , respectively ( detailed   hyperparameters can be found in Table 5 in Ap-   pendix B ) . We pre - train our model on 4 NVIDIA   Quadro RTX 8000 GPUs and for 25 epochs . We   empirically set the batch size to 432 to ﬁt the GPU   memory . The full pre - training can be done within   24 hours .   Evaluation . We measure the V A pre - training per-   formance by retrieval precision and recall :   p=#(relevant items among the retrieved )   # ( retrieved items ) ,   r=#(relevant items among the retrieved )   # ( relevant items ) .   Audio is relevant if it has the same setof labels   as the image query , and vice versa . We average   precisions and recalls over all samples in the bal-   anced AS training set . Figure 5 illustrates the top-   1 retrieval performance with images as the query   ( similar trends are observed when using audio as   the query ) . Compared with random initialization ,   initializing the audio encoder from CLIP ’s image   encoder leads to faster convergence and better V A   alignment . As we will see , this performance on V A   retrieval transfers to downstream AT tasks .   3.3 Unsupervised and few - shot curation   To improve the AT alignment beyond pivoting , we   consider curating audio - text pairs , and then per-   forming an additional ﬁne - tuning step by train-   ing the audio encoder with the AT loss , i.e. ,   L(A , T).During AT ﬁne - tuning , we keep the   text encoder gfrozen and only ﬁne - tune the audio   encoder .   Unsupervised curation . We consider explicitly   mining AT pairs from vip - AnT . Because this zero-   resource method uses no human supervision , we   refer to it as “ unsupervised curation . " Concretely ,   for each video segment in AudioSet , we extract a   video frame , and input that frame to the original   CLIP image encoder . Then , we encode a large4496   set of candidate captions , and perform Image →   Text retrieval over them by using the CLIP text   encoder . The top candidate captions according to   cosine similarity are then paired with the audio that   corresponds to the original video clip .   We consider multiple caption sources to search   over . As noted by Kim et al . ( 2019 ) , captions for   images and captions for environmental audio are   signiﬁcantly different in focus . We consider two   vision - focused caption sets : ( 1 ) MSCOCO ( Lin   et al . , 2014 ) captions ( VC ) ; and ( 2 ) because   MSCOCO captions are limited to 80 object cat-   egories , we generate free - captions from GPT - J   ( Wang and Komatsuzaki , 2021 ) conditioned on   MSCOCO captions as a prompt ( FC ) . We addi-   tionally consider audio - focused captions from the   training set of AudioCaps ( Kim et al . , 2019 ) and   Clotho ( Drossos et al . , 2020 ) ( AC).As a base-   line , we also consider a random caption alignment ,   which assigns a random caption from AC to each   clip ( instead of pivoting on images ) . The upper half   of Table 2 summarizes different ways of curating   AT pairs without additional supervision . Few - shot curation . We also explore the effect of   incorporating limited amounts of AT supervision ,   speciﬁcally , via captions from AudioCaps ( GC ) and   textual labels of AudioCaps ( GL ) ( see the bottom   half of Table 2 ) .   4 Audio - text experiments   We use two types of tasks to evaluate the quality   of the audio - text alignments learned by our model :   AT retrieval and zero - shot audio classiﬁcation .   AT retrieval . We conduct audio - text retrieval on   AudioCaps and Clotho for in - domain evaluation   and out - of - domain evaluation , respectively :   ( 1)AudioCaps ( Kim et al . , 2019 ) builds on Au-   dioSet ( Gemmeke et al . , 2017 ) and provides cap-   tions for a subset of audio clips in AudioSet   ( sourced from YouTube ) . As we have pre - trained   the audio encoder on AudioSet , we consider audio-   text retrieval on AudioCaps as in - domain evalua-   tion .   ( 2)Clotho ( Drossos et al . , 2020 ) consists of audio   clips which have a duration of 15 - 30 seconds and   come from Freesound ( Font et al . , 2013 ) . It has a4497   different sound source from AudioCaps and is used   forout - of - domain evaluation .   We study the out - of - domain generalizability of   our models by applying them to Clotho directly , without further ﬁne - tuning on it .   Zero - shot audio classiﬁcation . We consider the   following three widely used datasets for audio clas-   siﬁcation .   ( 1)ESC50 ( Piczak , 2015 ) contains 2000 audio   clips from 50 classes . Each audio clip has a du-   ration of 5 seconds and a single textual label . We   follow the standard k - fold data splits .   ( 2)US8 K ( Salamon et al . , 2014 ) contains 8732   audio clips from 10 classes . Each audio clip has   a duration less than 4 seconds and a single textual   label . We follow the standard k - fold data splits .   ( 3)AudioSet ( Gemmeke et al . , 2017 ) is a bench-   mark dataset for multi - label classiﬁcation . Au-   dioSet provides balanced and unbalanced training   sets . The balanced set consists of 22 thousand au-   dio clips and the unbalanced set contains around   2 million audio clips . It also provides 20 thou-   sand balanced audio clips for evaluation ( more data   statistics can be found in Table 6 in Appendix A ) .   For each audio clip a , we ﬁrst compute the co-   sine similarity between it and every possible textual   label in the tri - modal representation space . Then   we predict the label twith the highest similarity :   arg maxcos(t , a ) . ( 5)44984.1 Main results   Our prediction results for AT retrieval are given in   Table 3 and for zero - shot classiﬁcation in Table 4   ( Appendix F contains qualitative results of the tri-   modal representations ) .   Initializing with visual CLIP weights helps .   Comparing V A - Rand to vip - AnT , we see accu-   racy increases in all classiﬁcation and retrieval se-   tups . For example , on AudioCaps , vip - AnT out-   performs V A - Rand by 4.5 % R @1 and 13.6 % R @10 .   This conﬁrms that the ﬁndings of Gong et al . ( 2021 )   carry - over to unsupervised audio pre - training .   Pivoting works well for Audio → Text .   vip - AnT exhibits surprisingly strong performance   on AT retrieval tasks and zero - shot classiﬁcation .   For example , it outperforms the supervised base-   line ( Oncescu et al . , 2021b ) by 2.2 % R @1 for text   retrieval , without being trained or ﬁne - tuned on   Clotho , and without ever having seen an aligned   AT pair .   Prompting ( usually ) helps . Inspired by the zero-   shot image classiﬁcation setups of CLIP ( Radford   et al . , 2021 ) , we preﬁx textual labels with a prompt   in zero - shot audio classiﬁcation . We empirically   ﬁnd that the prompt ‘ the sound of ’ works well . Us-   ing it greatly improves zero - shot multi - class classi-   ﬁcation accuracy ( see Table 4 ) . Take vip - AnT , the   prompt gives rise to an improvement of 7.2 % on   ESC50 and 6.9 % on US8 K , but hurts multi - label   classiﬁcation performance on AS .   Random curation helps . Even when the audio-   text pairs used to train that objective are sampled   entirely at random ( + AT w/ RC ) , vip - AnT im-   proves , e.g. , R@1 for Text →Audio retrieval in-   creases from 0.8 % to 3.8 % . We conjecture that   RC at least makes audio representations aware of   and lean towards the text cluster of the joint VT   representation space . While this result also holds   for AS classiﬁcation ( +1.5 % mAP ) , performance   decreases for ESC50 ( -5.5 % accuracy ) and US8 K   ( -2.4 % accuracy ) .   Unsupervised curation is universally helpful .   vip - AnT ﬁne - tuned with unsupervised audio cap-   tions ( + AT w/ AC ) outperforms both pivoting   ( vip - AnT ) and random curation ( + AT w/ RC ) in   all cases . Thus , explicitly mining unsupervised AT   pairs can be a helpful zero - resource approach . Per-   formance with automatically generated captions   ( FC ) is similar to captions written by humans ( AC ) .   Supervision is still the most helpful . Fine-   tuning vip - AnT on GC pairs leads to the highest   accuracies on ESC50 and US8K. However , we do   not see similar improvements on AS , presumably   because multi - label classiﬁcation is more challeng-   ing and requires more direct language supervision ,   such as audio labels . This is further evident when   we ﬁne - tune vip - AnT on GL and obtain the high-   est accuracy ( 18.9 % mAP ) on AS ( see Table 4 ) .   For retrieval , GL uses only audio labels as the   text , which provide less dense language supervision   than GC and is thus slightly worse than GC , but   still , it gives better AT alignment than all automatic   methods . As captions become semantically further   from the audio - caption domain , e.g. , GC < AC <   FC < VC , the AT alignment becomes weaker , and   thus leading to worse retrieval performance . The   ﬁne - tuned audio encoder generalizes to the out-   of - domain Clotho successfully , displaying a trend   similar to AudioCaps .   Supervision improves per - class accuracy in gen-   eral . We further plot zero - shot classiﬁcation ac-   curacy for each audio class ( see Figure 6 for US8 K   and Figure 12 in Appendix G for ESC50 ) . Clearly ,   language supervision improves per - class accuracy   in general . The highest improvement is observed   on ‘ siren ’ because ‘ siren ’ rarely appears in image   descriptions while GC contains a lot of textual de-   scriptions of ‘ vehicle ’ audio.4499   4.2 Level of language supervision   We have observed that AT ﬁne - tuning on AT pairs   mined without any additional supervision ( e.g. , AC ,   FC , and VC ) can improve the AT alignment , but   supervised alignments are still the most effective .   But : how much supervised data is really needed ?   To understand the relationship between supervision   and performance , we vary the number of gold AT   pairs ( i.e. , training samples of AudioCaps ) used   for AT ﬁne - tuning . On the audio - text retrieval   task ( see Figure 7a ) , unsurprisingly , ﬁne - tuning   on more aligned AT pairs results in higher audio-   text retrieval / zero - shot classiﬁcation performance .   Surprisingly , using only 442 ( around 1 % ) AT pairs   of AudioCaps gives rise to as strong AT alignment   as VT alignment ( cf . OracleA V - CLIP in Table 3 ) .   As we increase the number of supervised AT   pairs used during ﬁne - tuning , we observe a roughly   linear relationship between zero - shot performance   and the log of the number of supervised pairs ( this   observation is similar to Kaplan et al . ( 2020 ) ’s ob-   servations regarding Transformers ) . While it is not   clear how reliable extrapolations from this roughly   linear trend are , we roughly estimate the amount   of annotated AT pairs required for the zero - shot   performance to equal human parity for ESC50 of   81 % ( Piczak , 2015 ): our estimate is that 2≈2 M   supervised audio caption pairs would be needed .   We are hopeful both ( 1 ) that larger curated audio-   text datasets will become available ; and ( 2 ) that   future work can improve the data efﬁciency of the   pre - training process .   5 Conclusion   We have presented vip - AnT for unsupervised   audio - text alignment induction . Based on the pivot-   ing idea , our model learns image - text alignmentand image - audio alignment explicitly and sepa-   rately via bi - modal contrastive pre - training . The   image modality is shared between the two and   thus pivots audio and text in the tri - modal em-   bedding space implicitly , without using any paired   audio - text data . We empirically ﬁnd that our model   achieves strong performance on zero - shot audio-   text tasks . We further strengthen the audio - text   alignment by using varying kinds of audio - text su-   pervision . Experimental results show that even   un - aligned audio - caption pairs can help .   Acknowledgements   We would like to thank the AI2 Mosaic team   for discussions , the AI2 Beaker team for com-   puting support , and the anonymous reviewers for   their suggestions . Yanpeng would like to thank   Ivan Titov for his comments on the draft . The   work was partially supported by the European Re-   search Council ( ERC Starting Grant BroadSem   678254 ) , the Dutch National Science Foundation   ( NWO VIDI 639.022.518 ) , DARPA MCS pro-   gram through NIWC Paciﬁc ( N66001 - 19 - 2 - 4031 ) ,   DARPA SemaFor program , and Google Cloud   Compute .   References4500450145024503Abstract   A Data statistics   Table 6 presents data statistics of all the datasets   used in the paper .   B Optimizer hyperparameters   Table 5 presents optimizer hyperparameters used   in our learning tasks .   C Supervised audio classiﬁcation   To perform supervised audio classiﬁcation , we add   a classiﬁcation head ( a linear layer ) on top of the   pre - trained audio encoder . For multi - class classi-   ﬁcation , the classiﬁcation head projects the vector   representation of an audio clip onto the class space .   We ﬁne - tune the model by minimizing the cross-   entropy loss :   /summationdisplaylogp(y|a ) , ( 6 )   where yis the gold label of a. For supervised   multi - label classiﬁcation , the classiﬁcation head   estimates the likelihood that an audio clip has some   textual label . We thus minimize the per - label binary   cross - entropy loss :   /summationdisplay / summationdisplaylogp(l= 1|a ) , ( 7 )   where lenumerates all possible audio labels .   ESC50 and US8 K classiﬁcation . We initial-   ize the audio encoder from random initialization ,   CLIP , and vip - AnT , respectively . Among them ,   vip - AnT performs best . It surpasses random ini-   tialization and CLIP on both datasets ( see Table 7 ) .   Notably , it outperforms the strong baseline AST - P   on ESC50 ( +0.1 % ) , though AST - P has used gold   audio labels for supervised pre - training .   AS classiﬁcation . We consider balanced and un-   balanced training for AS classiﬁcation and train an   individual model on the balanced set and the un-   balanced set , respectively . Since the audio encoder   has been pre - trained on the unbalanced AudioSet   training set , it can be directly used without further   ﬁne - tuning . Nevertheless , we ﬁne - tune the last k   layers of the Transformer architecture of vip - AnT   and investigate whether task - speciﬁc ﬁne - tuning   helps ( see Figure 8) . When k= 0 the model is4504   basically a linear probe . It inspects if contrastive   pre - training learns separable audio representations .   As we increase k , i.e. , ﬁne - tuning more layers , the   model exhibits a tendency of over-ﬁtting the train-   ing set . We use k= 4as a trade - off between under-   ﬁtting and over-ﬁtting . Our model achieves the   best mAP of 37.9 % for balanced training , which   surpasses AST by 6.5 % ( see Table 7 ) . While for   unbalanced training , we ﬁnd it crucial to ﬁne - tune   the whole model . Again , our model outperforms   AST ( +1.4 % mAP ) .   D Position embedding interpolation   Clotho ( Drossos et al . , 2020 ) audio has a duration   of 15 - 30 seconds , which is longer than 10 - second   audio clips used in pre - training . To apply our pre - trained audio encoder to Clotho audio - caption re-   trieval , we up - sample the pre - trained positional   embeddings to account for the longer audio token   sequences . Table 8 shows retrieval performance   of 10 - second audio and 18 - second audio . In gen-   eral , longer audio gives rise to better audio - caption   retrieval performance .   E V AT versus AT ﬁne - tuning   Given caption - augmented AudioCaps audio ( Kim   et al . , 2019 ) , we can improve the pre - trained au-   dio encoder via contrastive vision - audio - text ( V AT )   ﬁne - tuning and contrastive audio - text ( AT ) ﬁne-   tuning . Figure 9 shows a comparison between the   two ﬁne - tuning techniques on zero - shot ESC50   classiﬁcation and AudioCaps audio retrieval . In   general , AT ﬁne - tuning results in better results on   the two tasks .   F Analyzing tri - modal representations   To better understand the geometry of tri - modal   embeddings of our pivoting , unsupervised cura-   tion , and supervised curation , we study how AT   ﬁne - tuning inﬂuences the tri - modal representation   space . Speciﬁcally , we analyze vip - AnT ( pivot-   ing ) , vip - AnT + AT ( w/ RC ) ( unsupervised cura-   tion ) , and vip - AnT + AT ( w/ GC ) ( supervised cu-   ration ) using pivotability .   Pivotability measures how likely images can   pivot audio and text . We quantify it for each aligned   V AT triplet via a two - step retrieval probe . Starting   at a given audio clip , we retrieve knearest image   neighbors ; for each image neighbor , we retrieve4505   the top-5 nearest captions . Since each audio clip   has 5 gold captions , we compute pivotability as the   ratio of the number of retrieved gold captions to   5 . A gold caption may be retrieved more than one   time , but we always count it as 1 , so pivotability is   always between 0 and 1 .   We conduct this experiment on AudioCaps test   set . For each k , i.e. , how many images will be   retrieved for a given audio clip , we average pivota-   bility scores over all test triplets ( see Figure 10 ) .   Which pairs are pivotable ? To study what   kinds of audio are more likely to be pivoted with   text by images , we set k= 5 , i.e. , 5 images will be   retrieved for each given audio clip . We consider an   AT pair as pivotable if at least 3 out of 5 gold cap-   tions of the audio clip are retrieved , i.e. , pivotability   is equal to or larger than 0.6 . Figure 11 illustrates   the categories of the audio clips in pivotable AT   pairs . Unsurprisingly , audio about speech and vehi-   cle is more pivotable because the two categories are   among the top three frequent categories in AS .   Given that AT ﬁne - tuning improves Audio →Im-   age retrieval , we wonder if it could also help ﬁnd   novel categories of audio that can be pivoted with4506   text . We ﬁnd that this is indeed the case ( see Ta-   ble 9 ) . For example , vip - AnT + AT ( w/ GC ) ﬁnds   more ﬁne - grained speech categories because most   AT pairs in AudioCaps are about speech . In con-   trast , vip - AnT + AT ( w/ RC ) ﬁnds two additional   novel insect categories , presumably because RC   suffers from less data bias than GC .   G Additional results   Asymmetric audio - text retrieval performance .   For Text→Audio retrieval , our unsupervised piv-   oting model is not as good as on Audio →Text .   This could be because audio is intrinsically more   difﬁcult to retrieve with speciﬁcity than text in our   corpus , e.g. , because sound events co - occur ( a baby   may cry in street with sirens in the background   or in a room with dogs barking ) , there may be a   broader range of captions that accurately describe   them . However , it could also be the case that AT   alignment is bounded by VT alignment because V A   pre - training biases audio representations towards   image representations . We check this hypothesis by   conducting image - text retrieval on AudioCaps . Au-   dioCaps provides aligned image - audio - text triplets ,   so we simply replace audio with the corresponding   image . We ﬁnd that the Text →Image retrieval   performance of CLIP is much better than the Text   →Audio retrieval performance of vip - AnT ( see   the OracleA V - CLIP row of Table 3 ) . It is also close   to the Image→Text retrieval performance of CLIP .   In contrast , vip - AnT exhibits a large gap between   the Text→Audio retrieval performance and the   Audio→Text retrieval performance .   Per - class accuracy on ESC50 is illustrated in   Figure 12.4507