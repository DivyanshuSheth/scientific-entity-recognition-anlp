  Yang Yan , Junda Ye , Zhongbao Zhang , Liwen Wang   Beijing University of Posts and Telecommunications , Beijing , China   { yanyang42,jundaye,zhongbaozb,w_liwen}@bupt.edu.cn   Abstract   As an essential component of task - oriented di-   alogue systems , slot filling requires enormous   labeled training data in a certain domain . How-   ever , in most cases , there is little or no tar-   get domain training data is available in the   training stage . Thus , cross - domain slot filling   has to cope with the data scarcity problem by   zero / few - shot learning . Previous researches   on zero / few - shot cross - domain slot filling fo-   cus on slot descriptions and examples while   ignoring the slot type ambiguity and example   ambiguity issues . To address these problems ,   we propose Abundant Information SlotFilling   Generator ( AISFG ) , a generative model with   a novel query template that incorporates do-   main descriptions , slot descriptions , and exam-   ples with context . Experimental results show   that our model outperforms state - of - the - art ap-   proaches in zero / few - shot slot filling task .   1 Introduction   Slot filling is a critical part of downstream tasks in   natural language understanding ( NLU ) such as di-   alogue systems . Recently , some supervised slot   filling models have achieved state - of - the - art re-   sults within deep learning ( Mesnil et al . , 2013 ; Yao   et al . , 2013 ; Louvan and Magnini , 2018 ; Kim et al . ,   2019).Nonetheless , these methods have a strong   dependency on the domain - specific labels , which   is not capable of transferring to new domains that   always contain little or no data .   To alleviate the problem of resource gap between   source and target domains , cross - domain zero - shot   has become an important research direction in slot   filling . However , most researches on slot filling   utilize token - level classification frameworks , which   means they either convert it to a BIO tag labeling   task ( Shah et al . , 2019 ; Bapna et al . , 2017 ; Du   et al . , 2021 ) or predict the start and end position asQA task in the sentence ( Du et al . , 2021 ; Yu et al . ,   2021a ) , to extract spans . Recently , prompt learning   methods reformulate the downstream tasks to a   similar form with the pre - training tasks , which can   fully utilize the knowledge encoded in PLMs and   improve the performance of the downstream tasks   in the scenarios of data scarcity . Motivated by this ,   we consider making the slot filling task consistent   with PLMs pre - training tasks .   In this paper , we propose a generative template-   based zero - shot slot filling framework named Abun-   dant Information Slot Filling Generator ( AISFG ) ,   which utilizes pre - trained generative model as the   backbone and generates responses in natural lan-   guage style . Thus , the slot filling task is consistent   with the pre - training task of the PLM . In particu-   lar , we notice that shared cross - domain slot types   sometimes refer to totally different entities across   different domains . For example , in the SNIPS   dataset ( Coucke et al . , 2018 ) , domains RateBook   andSearchScreeningEvent share a common slot   type object_type , but it refers to book type and   movie schedule , respectively . We call this slot type   ambiguity issue . Moreover , we argue that incorpo-   rating slot examples only is far from fully utilizing   the example information . For instance , for sentence   give 5 out of 6 stars to creatures of light and dark-   ness , we give the slot description to find best rating   with examples like 6 and 5 , the model produces   the wrong answer 5 but not 6 . We conjecture that ,   the model just learn that predicting a number is   satisfying from these two examples but does not   understand what is the best rating means . We call   this example ambiguity issue . Thus , we attach our   attention to domain - specific descriptions and exam-   ples with context to alleviate the above two issues .   Specifically , we design the query template by in-   corporating domain descriptions , slot descriptions   and examples with context .   The contribution of this paper can be summa-   rized in three aspects:4180   •We propose a generative template - based zero-   shot slot filling framework . To the best of our   knowledge , we are the first to apply the gen-   erative framework to perform the zero - shot   cross - domain slot filling task .   •We focus on slot type ambiguity and exam-   ple ambiguity , which is ignored by previous   researches . We incorporate domain - specific   descriptions and examples with context to the   query template to handle these two issues .   •The experimental results show that AISFG   achieves better performance than the existing   methods on the setting of zero / few - shot .   2 Methodology   2.1 Slot Filling as Generation   Given a sentence xfrom domain d∈ D , slot filling   aims to predict a set of ( slot type , span ) pairs ( s , y ) ,   where s∈ S is a specific entity type from a fixed   set of slot types , and y={y , y , . . . , y}is a set   of spans in sentence x. Most work on slot filling uti-   lizes token - level classification frameworks , which   means they either convert it to BIO tag labeling or   predict the start and end position in the sentence ,   to extract spans .   In contrast to this convention , we frame slot fill-   ing as a conditional sequence generation task and   solve it in a sequence - to - sequence manner . As   shown in Figure 1 , given a sentence x , domain d ,   slot type sand target y , we construct natural lan-   guage query q = t(x , d , s ) and response r = t(y )   based on the predefined template . The generative   model takes qas input and directly predicts rin a   generation manner .   Leveraging the rich knowledge in the pre - trained   generative model is important for cross - domain slot   filling , especially in zero / few - shot setting . Thus ,   we construct query and response as natural lan-   guage sentences to naturally utilize them . Query Construction To solve the slot type am-   biguity and example ambiguity , we synthesize the   query by incorporating domain descriptions , slot   descriptions and examples with context . Specifi-   cally , query construction template is formulated as :   in domain 1⃝ , find the 2⃝ , like 3⃝   in4⃝ , in sentence : x.   where the blank 1⃝,2⃝,3⃝,4⃝are filled with   domain description , slot description , example en-   tity and context for example , respectively . To   avoid data leakage , we construct examples and con-   texts manually to ensure the example entities are   not appeared in the dataset . The predefined spe-   cific mappings between domain / slot and descrip-   tion / example / context are reported in Appendix A.1   and A.3 . Moreover , a query example is illustrated   in Figure 1 .   Response Construction Ideally , the response   should be as simple as possible to alleviate the   generation difficulty . Besides , an explicit template   is easily converted to the original format for com-   patibility . Thus , we construct response by concate-   nating target entities with commas :   y , y , . . . , y.   where yis the i - th entity span in sentence x. A   response example is illustrated in Figure 1 .   2.2 Train and Inference   The sequence - to - sequence model is instantiated as   a pre - trained generative language model , such as   BART ( Lewis et al . , 2020 ) . In the training stage ,   for the input sentence xwith multiple slots , we   build a training pair , which consists of a query and   a response based on the templates above for each   slot . We further fine - tune the parameters based on   these training pairs to maximize the log likelihood   for predicting gold responses just like in an ordi-   nary generation task . In the inference stage , we4181   also build a query for each candidate slot , and the   response ˆris generated in an auto - regressive man-   ner , which means selecting the token as the next   token with the highest probability over the vocabu-   lary set at each time step . Note that , although we   do not explicitly restrict the response tokens should   originate from the input sentence x , AISFG can   always do it ( but there are exceptions , we show   some cases in Section 3.4 ) . Then , the response ˆris   split by commas to recover to the original format ˆy   for evaluation .   3 Experiments   3.1 Setup   Dataset We evaluate our method on SNIPS   ( Coucke et al . , 2018 ) , a public spoken language   understanding dataset which contains 7 domains   and 39 slots . To simulate the cross - domain scenar-   ios , we choose one domain as the target domain for   test and the left six domains as the source domains   for training following the setup of Liu et al . ( 2020 ) .   However , domains in SNIPS are not completely   independent with each other . To achieve a real   cross - domain scenario , we use another commonly   used dataset namely ATIS ( Price , 1990 ) as the tar-   get domain to test our model which is trained on   SNIPS .   Baselines We compare our method against a   number of representative baselines as follows :   •Concept Tagger ( CT ) A slot - filling frame-   work proposed by Gobbi et al . ( 2018 ) , which   utilizes original slot descriptions to generalize   to unseen slot types .   •Robust Zero - shot Tagger ( RZT ) Besides   leveraging slot descriptions like CT , RZT   ( Shah et al . , 2019 ) further introduces exam-   ples to improve the robustness of zero - shot   slot filling.•Coarse - to - fine Approach ( Coach ) Coach   ( Liu et al . , 2020 ) is a two - step coarse - to - fine   model for slot - filling , which performs coarse-   grained BIO labeling task in the first step and   performs fine - grained slot type classification   task in the second step . It also encodes slot de-   scriptions to help recognize unseen slot types .   •QA - driven slot filling ( QASF ) QASF ( Du   et al . , 2021 ) uses a linguistically motivated   question generation strategy for converting   slot descriptions and example values into nat-   ural questions and solves the slot filling by   extracting spans from utterances with a span-   based QA model .   3.2 Main Results   Cross - Domain Slot Filling The cross - domain   slot filling results are reported in Table 1 . In SNIPS   dataset , AISFG outperforms the state - of - the - art   models ( QASF for zero - shot and Coach for few-   shot ) by 7.47 % on the average F1 under zero - shot   setting , 13.57 % under 20 - shot setting and 8.88 %   under 50 - shot setting , which demonstrates the su-   periority of our method . We train AISFG on SNIPS   dataset and test it on the ATIS dataset for simulating   a real cross - domain scenario , results are reported in   the bottom of Table 1 . AISFG consistently outper-   forms the existing state - of - the - art approaches , espe-   cially in zero - shot setting , where AISFG achieves   32.31 % F1 score improvement . The significant per-   formance improvement proves that utilizing rich   domain information to prompt knowledge in PLMs   may be a shortcut for solving data scarcity issue .   Analysis on Seen versus Unseen Slots We di-   vide the samples in each target domain into “ seen ”   and “ unseen ” categories in the SNIPS dataset for   further understanding the transferring ability of our   model . Following Liu et al . ( 2020 ) , an example is   categorized as “ unseen ” as long as the slot does not4182   exist in the remaining six source domains . Other-   wise , it is tagged as “ seen ” .   Table 3 shows the average F1 results on seen and   unseen slots in target domains under zero - shot and   50 - shot settings . From this table , AISFG achieves   the best results in both seen and unseen slots , espe-   cially on unseen slots in zero - shot scenario , which   indicates our method is more effective for transfer-   ring knowledge from source to the target domain .   Besides , approaches leverage PLMs ( i.e. , QASF   and AISFG ) have significant advantages over oth-   ers , which demonstrates utilizing knowledge en-   coded in PLMs can enhance the transferring ability .   3.3 Ablation Studies   To solve the slot type ambiguity issue and example   ambiguity issue , we incorporate domain descrip-   tion and context for example to the query template ,   respectively . To understand the influence of these   components in the query template , we perform abla-   tion studies on the SNIPS dataset and report results   in Table 2 . In this table , SD , DD , E , EC refers   to Slot Description , Domain Description , Exam-   ple and Example with Context , respectively . For   example , “ SD+EC ” represents the query templateis built by domain description plus example with   context .   The Effect of Domain Description Comparing   SD versus SD+DD and SD+EC versus AISFG , we   observe that incorporating domain descriptions can   improve the performance under few - shot setting   but not for zero - shot setting . We conjecture the rea-   son is that , the domain description we used ( shown   in Table 4 ) is simply converted from the domain   name and contains limited information , which is   not enough to provide domain knowledge . How-   ever , when some target domain examples are given   ( i.e. , few - shot ) , the meaning of domain descriptions   can be enhanced by these training examples . Then ,   the domain description becomes a domain indica-   tor and can be used to distinguish the slot types   shared by different domains , resulting in improved   performance .   The Effect of Context Example Comparing SD   versus SD+EC and SD+DD versus AISFG , we ob-   serve that incorporating examples with contexts   can further boost the performance in both zero-   shot and few - shot settings , which demonstrates   these examples with contexts are helpful for cross-   domain slot filling . Moreover , SD+EC outperforms   SD+E in both zero - shot and few - shot settings . Es-   pecially , for the zero - shot setting in domain Rate-   Book , which contains plenty of ambiguity exam-   ples , SD+EC achieves 8.06 % improvement over   SD+E , which indicates contexts are useful to alle-   viate example ambiguity .   3.4 Error Analysis and Case Study   To better understand our proposed model , we ana-   lyze the error predictions in all cross - domain exper-   iments on SNIPS dataset ( Table 1 ) and categorize   them into three types : boundary mistakes , vocabu-4183   lary mistakes and others . Boundary mistakes indi-   cate that AISFG predicts the main body of the gold   label while missing / adding other words , which may   lead to an inaccurate boundary ( top example in Fig-   ure 3 ) . V ocabulary mistakes refer to the predictions   that i ) some words in the gold label are replaced by   synonyms ( middle example in Figure 3 ) ; ii ) some   words in the gold label are substituted by different   tenses ( bottom example in Figure 3 ) .   As shown in Figure 2 , the most common mis-   takes are the boundary mistakes . Especially , in 50-   shot experiment , the amount of boundary - mistake   predictions reaches almost half of all mistakes . We   find that the model often struggles to detect the ex-   act same span as the ground truths , as shown in the   top example in Figure 3 . When the shot number in-   creases from 0 to 20 and from 20 to 50 , the amount   of boundary mistakes reduces by 36.4 % and 22.7 % ,   respectively , indicating that increasing shot number   is beneficial for addressing the boundary mistakes .   V ocabulary mistakes such as synonyms or tenses   replacement are the least common . Since the pro - posed AISFG is a generative model , we carry no   restriction on what the model produces , therefore   it may come with the vocabulary mistakes . How-   ever , most of these mistakes can be solved by post-   processing the outcome of the model ( i.e. , com-   paring the prediction with the original sentence   and replacing the wrong words ) , which is one of   the improvements of our future AISFG . Moreover ,   as shown in Figure 2 , increasing shot number is   useless for tackling the vocabulary mistakes .   4 Related Work   The main challenge of cross - domain slot filling is   to handle domain - specific slot types which have   few or no supervision signals during the training   stage . To handle the unseen slots , previous methods   introduce slot descriptions ( Lee and Jha , 2019 ; Liu   et al . , 2020 ; Bapna et al . , 2017 ) and slot examples   ( Guerini et al . , 2018 ; Shah et al . , 2019 ) to capture   the semantic relationship between unseen slots and   input sentences . Based on these researches , we   incorporate more information , like domain descrip-   tion and example context , to boost the performance   on unseen slots .   Typically , traditional methods formulate the slot   filling task as a token - level classification task ( Liu   et al . , 2020 ; Bapna et al . , 2017 ; Shah et al . , 2019 ;   Wang et al . , 2021 ) . To leverage the knowledge   encoded in pre - trained language models , RCSF ( Yu   et al . , 2021b ) utilizes BERT ( Devlin et al . , 2019 ) to   predict the start / end position of the target slot entity .   However , the downstream task ( i.e. , token / position   prediction ) is inconsistent with the pre - training task   ( i.e. , Masked LM and NSP for BERT ) , which may   limit the expressiveness of the PLM ( Mehri and   Eskenazi , 2021 ) . To bridge the gap , we treat the   slot filling as a generative task and utilize a PLM   whose pre - training task is also a generative task .   5 Conclusion & Discussion   In this paper , we introduce a novel zero - shot cross-   domain slot filling model named AISFG , which can   adapt to unseen domains seamlessly with the help   of domain and slot description and together with   full context examples . Experiments show that our   model significantly outperforms existing zero - shot   cross - domain slot filling approaches . Moreover ,   we conduct error analysis and case study to better   understand our proposed model and leave solving   boundary mistakes and vocabulary mistakes as our   future work.4184Acknowledgments   This work was partially supported by National Nat-   ural Science Foundation under Grants U1936103   and 61921003 .   References4185   A Appendix   A.1 Domain Descriptions   A.2 Implementation Details   For a fair comparison with QASF , which is based   on BERT ( Du et al . , 2021 ) , we instantiate the   sequence - to - sequence model as BART - base , which   contains a similar parameter size as BERT . Fol-   lowing Liu et al . ( 2020 ) , we reserve 500 samples   in target domain as the validation set and regard   the rest as the test set . We use Adam optimizer   ( Kingma and Ba , 2015 ) with a learning rate 2e-5   to fine - tune all parameters . The batch size is set to16 and the decoding beam search size for BART   is set to 2 . The early stop strategy with patience   of 5 is used to save the best checkpoint based on   validation set .   Similar to the baselines , we use F1 score as the   evaluation metric . For a given query , the true pos-   itive is the prediction with exactly matched entity   extraction boundaries.4186A.3 Illustration of Slot Description , Context and Example4187