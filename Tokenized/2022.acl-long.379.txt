  Neele Falk and Gabriella Lapesa   Institute for Natural Language Processing , University of Stuttgart   { neele.falk,gabriella.lapesa}@ims.uni-stuttgart.de   Abstract   Reports of personal experiences or stories can   play a crucial role in argumentation , as they   represent an immediate and ( often ) relatable   way to back up one ’s position with respect to   a given topic . They are easy to understand and   increase empathy : this makes them powerful   in argumentation . The impact of personal re-   ports and stories in argumentation has been   studied in the Social Sciences , but it is still   largely underexplored in NLP . Our work is the   ﬁrst step towards ﬁlling this gap : our goal is   to develop robust classiﬁers to identify docu-   ments containing personal experiences and re-   ports . The main challenge is the scarcity of an-   notated data : our solution is to leverage exist-   ing annotations to be able to scale - up the analy-   sis . Our contribution is two - fold . First , we con-   duct a set of in - domain and cross - domain ex-   periments involving three datasets ( two from   Argument Mining , one from the Social Sci-   ences ) , modeling architectures , training setups   and ﬁne - tuning options tailored to the involved   domains . We show that despite the differences   among datasets and annotations , robust cross-   domain classiﬁcation is possible . Second , we   employ linear regression for performance min-   ing , identifying performance trends both for   overall classiﬁcation performance and individ-   ual classiﬁer predictions .   1 Introduction   Although personal narratives and experiences natu-   rally ﬁll an important place in our everyday discus-   sions , they still do not conform to the classic ideal   of a “ good ” argument . A “ good ” argument con-   tains facts and logical conclusions , but as soon as   more personal or emotional nuances are involved ,   it deviates from the norm . According to the the-   ory of Deliberative Democracy ( Habermas , 1996 ;   Fishkin , 1995 ) , the discourse that precedes political   decisions plays a central role . Here , too , until the   so - called affective turn in Social Sciences ( Hoggett   and Thompson , 2012 ) , the assumption was that anexchange of arguments that is as rational as possi-   ble can lead to better political decisions . Recent   studies in Deliberative Theory , however , are in-   creasingly concerned with the interplay between   classical argumentation and alternative forms of ar-   gumentation , which include personal experiences ,   narratives and emotions , and their positive effects   on discourse ( Polletta and Lee , 2006 ; Esau , 2018 ;   Gerber et al . , 2018 ; Maia et al . , 2020 ) . A norm   that only allows rational and logical argumenta-   tion ﬁrstly does not correspond to human realis-   tic communication and secondly bears the danger   that less educated groups or groups in which other   communication standards prevail are marginalized .   Arguments with personal experiences are therefore   important to fulﬁll one of the core deliberative stan-   dards , namely inclusivity ( Polletta and Gardner ,   2018 ) .   The following example is taken from a discus-   sion about regulations to ban peanut products on   airlines and illustrates further possible positive ef-   fects of arguments with personal experiences : My   daughter has been tested 4 times for her allergy   to peanuts . She is in the highest category of reac-   tivity which means if peanuts are being ingested   in her vicinity , she could die . A buffer zone sim-   ply does n’t work in a conﬁned space such as an   airline . The perspective of the frightened parent   and their allergy - affected daughter is likely to elicit   emphatic reactions from the other participants in   the discussion and illustrates the rationale of pro-   ponents of a peanut ban .   Our work represents the ﬁrst step towards identi-   fying these arguments at a large scale by develop-   ing models for automatic detection of contributions   with personal experiences and stories ( to which   we refer with the general label of reports in the   paper ) . This is the necessary ﬁrst step to be able   to further examine the role of such arguments in5530reasoning and their relationship with Argument   Quality ( Wachsmuth et al . , 2017a ) and Discourse   Quality ( Steenbergen et al . , 2003 ) ,   To tackle this task we collect available datasets   and investigate the best strategy to merge the avail-   able sources . In two of the datasets we employ , pro-   duced by the Argument Mining community ( Reg-   ulation Room and Change My View ) our targeted   phenomenon is annotated as testimony . In the third   dataset , from the Social Science community ( Eu-   ropolis ) it is annotated as storytelling . The two   categories are obviously not fully overlapping , but   we hypothesize that they share a conceptual core   which can be leveraged for robust classiﬁcation .   We perform a large set of in - domain , out - domain   and cross - domain experiments including different   domain - adaptation strategies for the pre - trained   language models used in the classiﬁcation . We   analyze our results using performance mining and   show that the cross - domain training setup leads   to the most robust results and also has the most   positive effect when used with a domain - adapted   LM . We also conduct regression - based error anal-   ysis and compare the most salient features of the   two largest datasets that get picked up by the most   robust model and show that prototypical textual   properties of reports can push the model in the right   direction ( probability of reports close to decision   boundary ) but can also lead to over - generalization   ( higher probability of reports for false positives ) .   2 Related work   Argument Mining & NLP The automatic detec-   tion of arguments with personal experiences was   ﬁrst tackled by Park and Cardie ( 2014 ) with the   goal to classify claims as veriﬁable or unveriﬁable   and to be able to detect what type of evidence   would be necessary as a consequence . They de-   ﬁned the subcategory veriﬁable experiential for   veriﬁable claims that contain personal experiences .   Their dataset contains comments from Regulation-   Room , an e - rulemaking platform with the goal   of enabling online deliberation : governmental in-   stitutions or companies can have their proposals   about new regulations discussed by citizens to get   feedback . Park and Cardie ( 2014 ) conducted clas-   siﬁcation experiments with a SVM using differ-   ent feature sets hypothesizing that the amount of   past tense and ﬁrst personal pronouns would be   most predictive for veriﬁable experientials . Theyachieved a F1 - score of 70 % on the Regulation-   Room dataset . This work was further extended   in Park et al . ( 2015b ) and reformulated as a se-   quence classiﬁcation task . Park et al . ( 2015a ) and   Park and Cardie ( 2018 ) developed a new annotation   scheme targeted at elementary units in arguments .   This schema introduces testimony as an elemen-   tary unit which corresponds to a proposition about   the author ’s personal state or experience . A similar   evidence type , called anecdote and deﬁned as a   personal experience of the author or a narration of   a concrete example or event was classiﬁed in news   editorials ( Al - Khatib et al . , 2016 , 2017 ) . Song et al .   ( 2016 ) build a database for claims and suitable   anecdotes focusing on stories with a clear narra-   tive structure and popular main characters ( e.g. the   Dalai Lama ) . Wang et al . ( 2019 ) model different   persuasion strategies in dialogues targeting social   good ( e.g. fund raising ): personal stories which   exemplify positive outcomes and beneﬁts of a do-   nation are one of these strategies .   Social Sciences The role of personal narratives   in digital and deliberative democracy has gained   more attention in the recent years . Polletta and   Lee ( 2006 ) were the ﬁrst who investigated the   role of personal narratives in online argumenta-   tion . Their data was further analyzed in Black   ( 2008 ) and Black ( 2013 ) , who emphasized the   importance of personal narratives in discussions   for forming group identity and understanding oth-   ers ’ perspectives . The relationship between sto-   rytelling and emotions was investigated in Esau   ( 2018 ) who pointed out that these are especially   useful when discussing social problems that can not   be addressed with factual information alone . Maia   et al . ( 2020 ) annotated the functions of storytelling   ( e.g. do people tell an experience as a disclosure   of harm or to propose a solution ? ) and examined   how the different types of narratives effect the qual-   ity of a discussion . As far as available annotation   is concerned , we conduct our experiments on the   Europolis corpus ( Gerber et al . , 2018 ) . Europolis   contains spoken contributions from a transnational   poll , in which citizens from different European   countries got together to discuss about the EU and   the topic immigration . The spoken contribution   have been annotated with different aspects of delib-   erative quality ( e.g. does the speaker show respect   or value other participants ? ) , and one of these as-   pects includes alternative forms of communication .   Relevant to our work is the annotation category5531storytelling , marking those contributions which   contain personal experiences or concrete examples   of a speaker ’s own country .   3 Datasets   Regulation Room ( RegRoom ) Our experiments   are based on the ﬁnal version of the Cornell eRule-   making Corpus ( CDCP ) ( Park and Cardie , 2018 )   It contains 725 comments from Regulation Room ,   discussing consumer debt collection practices in   the United States . The comments are annotated   with different proposition types , and our category   of interest is testimony .   Change My View ( CMV ) contains 344 comments   from the subreddit ChangeMyView(Egawa et al . ,   2019 ) and is annotated with a similar schema as   introduced in Park et al . ( 2015a ) . Our reference   category is testimony .   Europolis ( Gerber et al . , 2018 ) , already introduced   in section 2 , contains a total of 856 transcribed   speech contributions whose original language was   German , French , and Polish ( only available in the   English translation).Europolis is annotated along   many deliberative quality dimensions ( Steenbergen   et al . , 2003 ) , and our reference category is story-   telling .   From now on , we will use the neutral term- as our positive label , of which testimony and   storytelling are dataset - speciﬁc declinations . Note   that , while storytelling is annotated at the docu-   ment level in Europolis , RegRoom and CMV con-   tain span - level annotation of testimony : for our   experiments , a document containing a testimony   span is considered as a positive instance of .   For all datasets , reports are the minority class ( Re-   gRoom : 41 % ; CMV : 37 % , Europolis : 35 % ) .   Table 1 displays one example per dataset . In the   example of Europolis the participant describes the   general situation in their country in a more objec-   tive manner , thus reﬂecting a quite broad deﬁnition   of a personal narrative . The RegRoom example is   very personal and emotional in its tone , displaying   more prototypical features of a personal experience .   The CMV example is somewhere in between : itdeparts from a personal experience but it targets a   general situation , and has a more objective tone .   3.1 Preprocessing and Feature extraction   The datasets were preprocessed by removing time   stamps and URLs . With freely available tools ,   we extracted a total of 51 features ( henceforth ,   contribution - level features ) from four categories :   Surface features ( 6 features ) , e.g. , length in to-   kens ; average amount of characters and syllables   per word . We hypothesize that longer comments   are more likely to contain reports ( verbose retelling   of concrete stories / examples ) .   Syntactic features ( 6 ) , e.g. , relative amount of ﬁne-   grained part - of - speech tags per comment ( e.g. , per-   sonal pronouns , past tense , auxiliaries , named enti-   ties ) . Speciﬁc categories , e.g. ﬁrst - person pronouns   and past tense verbs ( " I had an unpleasant experi-   ence ... " ) , are likely to be predictive of reports .   Textual complexity ( 19 ) , with different measures   of lexical diversity , lexical sophistication and read-   ability . While prototypical reports are expected to   exhibit lower textual complexity ( character repeti-   tions , more concrete concepts ) , the modulation of   complexity in the reports in our datasets is an open   question .   Sentiment / Polarity ( 20 ) , e.g. amount of positive or   negative adjectives / nouns , amount of speciﬁc emo-   tions ( joy , fear ) . We hypothesize that comments   with reports will have more marked polarity .   4 Experiments   Task We perform binary classiﬁcation at the doc-   ument level : forum posts in RegRoom and CMV ,   spoken contributions in Europolis . We create 10   random train / dev / test splits using 15 % as devel-   opment and 20 % as test data and we ensure that   every document is part of the test set at least once .   Setups We experiment with the following train-   ing / test setups :   In - domain : we train and test the models on the   same dataset .   Out - domain : we train the models on a single   dataset and test them on the other two individu-   ally ( e.g. train on Europolis and test on CMV and   RegRoom ) . We also concatenate two datasets and   test the corresponding model on the missing one ,   e.g. train on a joined set of CMV and RegRoom   and test on Europolis ( 2vs1).5532   Cross - domain : we create one training set which is   the concatenation of the training sets for of the in-   domain experiments and test it on the each dataset-   speciﬁc test set ( all ) .   Classiﬁcation models We experiment with the   following classiﬁcation models ( cf . section B.1 for   more details and hyper - parameters ):   •Feature - based : we train a random - forest clas-   siﬁer with the features mentioned above ( 51   in total , appendix section 3.1 ) .   •BoW : each contribution is represented as a   count vector with the frequency counts for   the 5000 most frequent words ( the vocabulary   was constructed based on the fusion of all   datasets ) which is fed into a random - forest   classiﬁer .   •FeedforwardNN : a contribution is represented   as the average of the embeddings the words   occuring in it and fed into a feed - forward neu-   ral network with one hidden layer of size 300   and a ReLU .   •BERT : we ﬁne - tune BERT ( Devlin et al . ,   2019 ) with a classiﬁcation head on the task of   predicting whether a contribution contains a   report or not .   •Domain - adapted BERT ( 3 models ) : we ﬁne-   tune the underlying language model ( LM )   with the masked language modeling objec-   tive and next sentence prediction on domains   that would match the domains of our target   datasets . For the Europolis dataset we sample   1 M sentences from Europarl ( Koehn , 2005 ) , for CMV and RegRoom we sample 1 M   sentences from the Webis - CMV-20 corpus   ( Al - Khatib et al . , 2020 ) and 1 M sentencesfrom the args.me corpus ( Ajjour et al . , 2019 ) .   We ﬁne - tuned one LM on each domain ( BERT-   adapt - europarl , BERT - adapt - argue ) and one   on the concatenation of the two ( BERT - adapt-   mixed ) .   Results Table 11 in the Appendix displays the   results for all models for each training setup ( av-   eraged over all splits ) , with signiﬁcance values .   The results show that BERT outperforms the other   models for all training / test setups and training on   the joined dataset ( all ) works well for all test cor-   pora ( the feature - based classiﬁers beneﬁt especially   from it ) . Fine - tuning the LM on the concatenated   domains ( BERT - adapt - mixed ) yields the best re-   sults when trained on allwith a macro F1 - score of   0.76 ( Europolis ) , 0.85 ( CMV ) and 0.94 ( RegRoom ) .   The non - domain - adapted BERT is more robust if   in - domain and out - domain experiments are also   taken into account , e.g. a drop in performance   from 0.72 to 0.65 F1 can be observed on Europolis   when trained in - domain using Bert - adapt - europarl .   In the following section , we will employ linear re-   gression to build a comprehensive picture of these   performance trends .   5 Analysis   To get a statistically informed understanding of   the different factors inﬂuencing the behavior of   our models on the different datasets , we employ   linear regression . Our dependent variables are ag-   gregated performance ( F1 macro ) in section 5.1   andmodel predictions on individual items ( proba-   bility of report ) in section 5.2 .   For the aggregated performance analysis in Sec-   tion 5.1 , our independent variables ( IV ) are : the   different experimental conﬁgurations i.e. , combi-   nations of classiﬁer architectures ( referred to as5533"model " in the tables ) , training setup and test cor-   pus as well as their interactions , as speciﬁed in the   formula :   For the model predictions ( section 5.2 ) our in-   dependent variables are : the contribution - level fea-   tures in 3.1 and a subset of the experimental con-   ﬁgurations ( training setup and test corpus ) , as well   as the interactions between the contribution - level   features and the experimental conﬁgurations .   In both cases , our analysis proceeds in three   steps . First , we ﬁt incrementally complex models   and assess their ﬁt in terms of adjusted R(propor-   tion of explained variance ) and signiﬁcance with re-   spect to the less complex models ( i.e. , models with   fewer independent variables ) . At this step , we also   check for multicollinearities . Next , once we iden-   tify the most explanatory regression model ( i.e. ,   the set of independent variables that maximizes the   ﬁt to the dependent variables ) , we proceed to iden-   tify its most explanatory IVs in terms of explained   variance and signiﬁcance ( e.g. , does the choice of   training setup determine a strong difference in the   performance of our classiﬁers ? ) . Last , we iden-   tify the best values for the IVs ( e.g. , which of the   training setups guarantees best F1 ? ) by visualizing   predicted performance with the help of effect dis-   plays ( Fox , 2003 ) , which show the partial effect of   one ( or more ) parameters by marginalizing over all   other parameters .   5.1 Aggregated Performance   With the regression analysis presented in this sec-   tion , we are interested in capturing the pattern of   variation exhibited by our experimental conﬁgu-   rations , with a focus on the effects of in - domain ,   out - domain and cross - domain training and domain   adaptation .   Data : we consider all experimental runs from the   in - domain , 2vs1 out - domain , and cross - domain   training setups , resulting in 630 data points . We   code the levels of the IV training setup as   in - domain ( trained / tested on the same corpus ) , 2vs1   ( out - domain training ) and all(cross - domain train-   ing ) .   Results : Table 2 reports the ﬁt of the simplest   model , which only contains test corpus as IV ,   and of the incrementally more complex models .   All IVs and interactions between explain a signif-   icant additional amount of variance . The most   explanatory model contains the three IVs and their   two - way and three - way interactions , for a total ex-   plained variance of R= 77:2 % .   Table 12 in the appendix displays the portion   of variance accounted for by each IV in the ﬁnal   regression model . Most variance in performance   ( 20 % of the R ) is determined by the classiﬁer   architecture ( IV : model ) .The high amount of ad-   ditional variance explained by the three - way inter-   action , however , ( 12 % ) indicates that the effects   of the training setup and test corpus differ signiﬁ-   cantly between classiﬁer architectures .   What is the effect of training setup and domain-   adaption ? To get a more detailed picture of the   relationship between experimental conﬁgurations ,   classiﬁer architectures , and the predicted perfor-   mance we can have a look at ﬁgure 1 which dis-   plays the effect plot for corresponding the three-   way interaction .   Comparing the different training setups , it be-   comes evident that cross - domain training setup is   most robust : the difference between the predicted   performance for all test corpora when trained on   mixed domains ( training setup = all ) , i.e.   the difference between the colored lines at the right-   most position of the y - axis , is similar for the dif-   ferent classiﬁer architectures ( across all panels ) ,   while in - domain or out - domain training can lead   to very different performance predictions . One can   for example observe a low predicted performance   when using features or BoW as classiﬁer architec-   ture for RegRoom in an out - domain setting ( drop   in the green line in the two upper panels in ﬁgure   1a ) . Out - domain training is rarely beneﬁcial if no5534   domain adaptation is performed , small positive ef-   fects can only be obtained for CMV and Europolis   with the feature - based classiﬁer ( increase in the   pink and blue line in the upper left panel in ﬁgure   1a ) . The largest positive effects for cross - domain   training in contrast to in - domain training can be   observed for Europolis ( increase in the blue line   from in - domain toallacross most panels ) .   Figure 1b compares the performance of the non-   adapted BERT model ( top - left panel ) to its three   domain - adapted variants . This comparison helps us   further characterize the domain - speciﬁcity of the   annotations . First of all , we can observe that BERT   is more robust across different training setups and   test - corpora . If we look at the course of the lines in   the upper left panel in ﬁgure 1b we can see that it is   relatively stable , almost parallel : the panel shows   only a small drop in predicted performance for   out - domain training and a small improvement with   cross - domain training for RegRoom and Europolis .   In contrast , the domain - adapted variants are subject   to a much higher variance ( e.g. , a strong drop in   predicted performance for BERTargue and BERT-   mixed , trained out - domain , tested on RegRoom ) .   Domain - adaptation can be useful in a cross-   domain training setup , as we can see slight improve-   ments for the predicted performance comparing the   colored lines at the right - most position of the y-   axis of the upper left panel with the others , and   this observation is similar for all domain - adaptedvariants .   When comparing the different domain - adapted   variants , the best model is BERTeuroparl which   works well for all test corpora for out - domain and   cross - domain training , whereas BERTargue leads   to a low predicted performance when used with   Europolis or in an out - domain training setup . This   indicates that the LM adapted to a deliberative   context ( since Europarl contains parliamentary de-   bates ) is compatible with all test corpora , whereas   BERTargue may be too domain - speciﬁc .   5.2 Item - based predictions : error analysis   In this section , we employ regression for error anal-   ysis , with the goal of ﬁnding out which linguistic   properties of a contribution drive the model predic-   tion away or towards the gold label . In this analysis ,   we focus on the predictions of the most robust clas-   siﬁer if both in - domain and out / cross - domain are   taken into account , namely the non - adapted BERT   ( see section 5.1 ) .   We extract two subsets predictions from the   BERT and perform regression analysis on each of   them . We examine the predictions for the false pos-   itives ( FPs ) in order to ﬁnd out in which cases the   model overgeneralizes ( which properties cause the   model to predict a high probability for reports ? ) ,   but also to ﬁnd out what puts the model on the right   track ( probability close to the decision boundary ) .   Similarly , we can examine the predictions for FNs,5535where typical features for reports may ensure that   the model ’s probabilities go in the right direction ,   while particularly atypical features or a disadvan-   tageous training / test combination may cause the   model to incorrectly predict lower probabilities .   Dependent variable In this analysis , our depen-   dent variable is the probability that a comment   contains a report . The distribution of the prob-   abilities , however , is heavily skewed towards the   upper and lower bound . We therefore transform the   individual probabilities with a log transformation   to reduce the skewness . In order to gain the same   advantage for both error types and to be able to   better compare the two in the effect plots , we invert   the probabilities of the FPs and map them to the   same range as the false negatives ( 1 p(reports ) ) .   For both error types , the resulting values range be-   tween -10 and -0.5 and in both cases the predicted   classiﬁcation label would change to the gold label   when the probability exceeds the upper threshold .   The distribution of the dependent variables is dis-   played in the histograms in section D.1 . Thanks   to this transformation , a positive effect or an in-   crease in the dependent variable can be interpreted   as beneﬁcial for both error types .   Independent variables and model selection In   our analysis , we focus on the difference between   the two largest datasets and include Europolis   andRegRoom astest corpus , excluding CMV .   Qualitatively , Europolis and RegRoom share the   deliberative focus , while CMV is persuasion - driven   and more likely to exhibit idiosyncratic proper-   ties that we would not be able to sufﬁciently dis-   cuss here for reasons of space . The training   setup variable contains RegRoom , Europolis , and   allso that we can examine the effects of in - domain ,   out - domain ( this time in a 1vs1 version ) , and a   cross - domain training setup . The ﬁnal subsets for   this analysis contain 776 datapoints for the FPs and   1,212 data points for the FNs .   Our analysis builds on the assumption that spe-   ciﬁc linguistic properties of the input drive the   predictions towards or away from the gold label .   For this reason , the ﬁrst core of IVs is represented   by the contribution - level features used to train the   feature - based random forest classiﬁer ( cf . section   3.1 ) . To avoid multicollinearities and for simpli-   ﬁcation purposes we applied a correlation - based   feature reduction methodology whose criteria are   described in section D.2 in the Appendix . All fea-   tures were further centered and scaled .   We incrementally added the experimental conﬁg-   uration features ( training setup andtest   corpus ) , as well as two- and three - way interac-   tions , on top of the contribution - level ones . Similar   to section 5.1 we compare nested models starting   from the one containing only contribution - level   features as IVs , effectively assessing whether the   variation in performance is only due to linguistic   properties or whether certain linguistic properties   affect speciﬁc experimental conﬁgurations .   Results : The results for both subsets ( FPs and FNs )   are shown in table 3 . It is noticeable that the most   explanatory regression model for the FNs can ex-   plain signiﬁcantly more variance ( 40 % ) than the   one for the FPs ( 10 % ) ; reasons for this could be   the smaller amount of data and the poorer distribu-   tion of the dependent variable . In both cases , the   contribution - level features alone explain roughly   half of the variance accounted for by the most com-   plex models . For the FPs , training setup   andtest corpus signiﬁcantly contribute to the   ﬁt only when the two - way interactions are taken   into account . On the other hand , for the FNs , all   incremental steps signiﬁcantly improve the ﬁt .   Which feature types have the greatest impact   on the errors ? Table 4 summarizes the relative   contribution of different feature groups to the to-   tal amount of explained variance . The surface5536   based features have an extremely low impact on   the prediction of the probability of reports : this   is surprising given that previous work has shown   that the length of a contribution has a great impact   on model predictions ( length bias , cf . Wachsmuth   and Werner ( 2020 ) ) . The other feature groups are   all involved relatively equally in explaining per-   formance variance , with textual complexity and   syntactic features dominating FPs , and syntactic   features FNs .   What is the impact of contribution - level fea-   tures ? If we look at the effect plots for the most   explanatory IVs , we can see which features are   particularly helpful for the errors : positive effects   drive the model towards the gold label . The ef-   fects for FNs highlight the dominant role of syntac-   tic features . For example , we see positive effects   for past tense verbs , personal pronouns , and post   length ( cf . ﬁgure 5 , section D.2 in the appendix )   which also conﬁrm the hypotheses that these fea-   tures are prototypical for reports . Some features   are more discriminative when training on speciﬁc   data : for example , the effect for personal pronouns   is positive when training on RegRoom but slightly   negative when training on Europolis ( cf . ﬁgure 6 ,   section D.2 in the appendix ) .   While some of the features can lead to over-   generalization of the model we can identify a fea-   ture of textual complexity that proves to be use-   ful for FPs . The effect plot in ﬁgure 7 ( section   D.2 in the appendix ) displays the interaction be-   tween mean average type token ratio ( mattr50 ) and   training setup . This feature puts the model   on the right track when trained on RegRoom but   moves the probability further away from the de-   cision boundary when trained on Europolis or a   mix . This example once again emphasizes that Re-   gRoom , in combination with speciﬁc features , can   take on an advantageous role as a training corpus . Storytelling vs. testimony : discussion The re-   ports of personal experiences in RegRoom resem-   ble prototypical narratives ; they contain very per-   sonal and individual experiences , exhibiting many   of the expected characteristics of a typical report .   Our regression - informed error analysis shows that   training on RegRoom positively impacts perfor-   mance , while the opposite is often true for Europo-   lis . Indeed , the reports in Europolis can describe   more general experiences or a concrete situation   in a country ( e.g. in the example from Europolis ,   table 1 ) . As a result , they are less prototypical and   difﬁcult to detect based on structural and linguistic   features . Ideally , through training on Europolis or   on mix , it is possible to recognize reports that are   not only about an individual experience , but about   a collective one , an aspect that is especially impor-   tant in a discussion with a deliberative focus . An   initial empirical investigation of the argument qual-   ity of the different types of reports gives evidence   that reports exhibit a higher quality than contribu-   tions without reports ( see section E in the appendix   a detailed description of a pilot case - study ) . An   interesting research question in this context is to   what extent the individuality of a report inﬂuences   quality or to what the commonality of the reported   experience positively impacts deliberation .   6 Conclusion   This work targeted the automatic identiﬁcation of   personal experiences and stories in argumentation .   Leveraging available annotation in three argumen-   tative datasets ( two , Regulation Room and Change   My View , from the Argument Mining community ;   one , Europolis , from the Deliberative Theory com-   munity ) , we evaluated different classiﬁer architec-   tures and training setups . Mixing training data from   different domains leads to robust results across all   corpora and models and boosts the performance of   the classiﬁers based on the domain - adapted LMs .   Our experiments established an empirical foun-   dation that will allow us to investigate the target   phenomenon at a larger scale and will lead to a   better understanding of the compatibility of the   underlying annotations ( storytelling from Delibera-   tive Theory and testimony in Argument Mining ) as   well as of the impact of storytelling / testimony on   the quality of an argument.5537Acknowledgments   The research reported in this paper has been funded   by Bundesministerium für Bildung und Forschung   ( BMBF ) through the project E - DELIB ( Powering   up e - deliberation : towards AI - supported modera-   tion ) . We thank the anonymous reviewers for their   valuable feedback . We would also like to thank Eva   Maria Vecchi , Enrica Troiano , Sebastian Padó , and   Jonas Kuhn for their feedback on different versions   of this work . We also thank Marlène Gerber for   giving us access to the Europolis dataset .   Ethics statement   Two of the three datasets employed in this work ,   Regulation Room andChange My View , are   publicly available along with the annotation used   in our experiments . Europolis has been kindly pro-   vided to us and the splits will be relased at the terms   and conditions of its owners .   Our experiments and analysis do not employ   any author - speciﬁc feature . We can not exclude ,   however , that the reporting of very speciﬁc personal   experiences may facilitate the identiﬁcation of the   author of a post .   As far as the societal impact of the use of reports   of personal experiences and stories is concerned ,   we are fully aware that they can be a two - edged   sword . On the one hand , as pointed out in the   paper , the use of personal experiences triggers em-   pathy and promotes inclusivity . On the other hand ,   however , the non veriﬁable nature of such reports   makes them an easy vehicle to generate and spread   fake news , and this , coupled with the emotional   loading reports tend to have , makes them a useful   tool highlight certain episodes for propagandistic   purposes . Robust computational models to identify   human - generated reports of personal experiences   and a deeper understanding of the linguistic fea-   tures of these contributions can also serve as a ﬁrst   step into the identiﬁcation of false stories gener-   ated by automatic systems and of those human-   generated ones that are best candidates to serve   manipulation purposes .   References553855395540A Preprocessing   A.1 The Europolis dataset and its automatic   translation   The discussions collected in the Europolis dataset   were held in 2009 in order to investigate whether   and how deliberation can take place in a multi-   lingual / transnational setting , and what effects de-   liberation can have for citizens ( e.g. increasing po-   litical engagement or interest ) . Participants from 27   countries participated to simultaneously translated   small - groups discussions about the topics of cli-   mate change and immigration . A sample of the dis-   cussions of 13 groups , whose original language was   French , German , and Polish , has been annotated   by Gerber et al . ( 2018 ) on different dimensions of   the Discourse Quality Index ( DQI ) by Steenbergen   et al . ( 2003 ) .   The dataset contains the professional trans-   lation of the Polish speeches into English ,   as well as the transcription of the speeches   French and German , which we translated into   English with DeepL ( https://www.deepl .   com / translator ) . The quality of the English   translation of the French and German texts has   been checked by one native speaker each . Native   speakers were instructed to check for the semantic   integrity of the conveyed message and the gram-   maticality of the output .   We acknowledge that this quality check does not   rule out the possibility that the automatic transla-   tion has distorted the linguistic features we employ   for the feature - based classiﬁers and in the regres-   sion analysis . We do believe , however , that the   pattern of results in table 11 is still relatively sta-   ble despite the potential distortion . More specif-   ically , the F1 macro of the feature - based classi-   ﬁer trained and tested on Europolis is more than   acceptable : had the translation affected the fea-   tures dramatically , the performance would have   dropped much more with respect to the feature-   based representations trained / tested on in - domain   native English models ( F1 macro : Europolis=0.60 ;   CMV=0.62 ; RegRoom=0.85 ) . Moreover , had the   features been distorted inconsistently , the general-   ization learnt from the out - domain ( 2vs1 ) feature-   based classiﬁer tested on Europolis and trained on   CMV+RegRoom would have exhibited a drop in   performance , not a gain ( test : Europolis , 2vs1 =   0.63 ; test : Europolis , in domain = 0.60 ) .   As regards the fact that translated features are   also employed in the regression analysis at the itemlevel , we believe that the validity of this analysis is   not affected , because the translated text was the in-   put of the BERT classiﬁers which in turn produced   the probability values we analyse with the features   extracted from that text .   A.2 Features   This section provides the details regarding the fea-   tures brieﬂy introduced in section 3.1 and em-   ployed in the experiments . Tables 5 , 6 , 7 , 8 and   9 list all features names grouped by type , along   with a short description and information on the val-   ues . For each feature , table 10 displays the mean   value per corpus , separately for documents with a   positive ( report ) vs. negative label ( no report ) .   A.2.1 Extraction details   Syntactic features ( Table 5 )   •Quantify the relative amount of a certain part-   of - speech tag ( e.g. , adverbs , adjectives , named   entities ) in a document .   These features have been computed using spacy   ( https://spacy.io ) for tagging , with a   model trained on English blogs , news and online   comments ( en_core_web_md ) .   Surface features ( Table 6 )   •Measure the length of the sentences / words ,   the number of complex words and a combi-   nation of these information in the form of   readability metrics ( ﬂesch reading ease and   gunning fog index )   The two readability metrics ( Flesch Reading   Ease ( Flesch , 1948 ) and Gunning Fog Index )   have been computed with the python package   readability .   Lexical diversity ( Table 7 )   •These metrics are different variants of the   type / token ratio , designed to be less sensitive   to text length .   These features has been extracted with TAALED . For more details refer to Kyle et al . ( 2021).5541   Lexical sophistication ( Table 8) The metrics of   lexical sophistication are computed based on word   / co - occurrence information taken from existing   reference corpora and word lists , e.g. the Corpus   of Contemporary American English ( COCA ) or   the ( Averil Coxhead ’s ) High - Incidence Academic   Word List ( AWL ) .   •Word Frequency : given a text , its word fre-   quency value is calculated as the average of   the frequencies of the words occurring in it ,   based on frequency estimates from different   reference corpora ( see above ) .   •Range indices : given a text , its range indices   are calculated as the average of document fre-   quencies of the words occurring in it , esti-   mated on reference corpora .   •Mutual information : uses the mutual informa-   tion scores of academic bigrams , computed   based on reference corpora .   •Academic list indices relative amount of aca-   demic words and n - grams using word lists as   reference .   •(Psycholinguistic ) Word Information : average   of different psycholinguistic scores ( e.g. con-   creteness , familiarity , imageability ) .   •Semantic network s : measures indicate how   word forms are semantically related . More   sophisticated texts contain words with fewer   senses and words with more hypernyms ( more   subordinate terms).•Contextual distinctiveness measures the diver-   sity of contexts in which a word is encoun-   tered , e.g. " love " occurs in many different   contexts , while the number of contexts where   the word " bride " occurs is more restricted .   This set of features has been extracted with   TAALES , see Kyle et al . ( 2018 ) for details .   Sentiment features ( Table 9 ) The sentiment fea-   tures rely on a number of pre - existing sentiment ,   social - positioning and cognition dictionaries ( e.g.   EmoLex ) which serve as a look - up table .   •The features correspond to macro - feature   component scores produced by PCA   To extract the sentiment features , we use   SEANCE . The metrics and the retrieval of the   feature components are described in Crossley et al .   ( 2017).55425543554455455546B Classiﬁcation experiments   B.1 Classiﬁers   In what follows , we provide the implementation   details for the classiﬁcation models employed in   our experiments .   •Random forest classiﬁer : we use sklearn   https://scikit-learn.org with the   n_estimators parameter set to 1000 . The   other parameters are set to the default .   •Feed - forward neural network : we use   pretrained word embeddings with sub-   words ( d= 300 ) , provided by ﬁnalfu-   sion ( https://finalfusion.github .   io / pretrained ) , pretrained with skip-   gram ( Mikolov et al . , 2013 ) . The English   word embeddings were trained on the CoNLL   2017 corpus .   •BERT : we use   BERTForSequenceClassification   from the huggingface library   https://huggingface.co/docs/   transformers / model_doc / bert .   We use the sequence of the ﬁrst 512 tokens   and train for a maximum of 20 epochs . We   pick the model that achieves the best macro   F1 score on the validation set . Parameters :   batchsize = 16 , lr=2e-5 , optim = Adam ,   model = bert - base - uncased .   B.2 Results   Table 11 reports the full set of experimental results   for the automatic recognition of contributions con-   taining reports . We report the precision , recall and   F1 score for the positive class ( report ) and the F1   macro score . Each column shows the results for   one test corpus : Europolis , CMV and RegRoom .   The scores represent the average of the 10 scores   obtained for each test split . We therefore also report   the standard deviation .   C Analysis   C.1 Aggregated performance analysis   Table 12 provides the details of the ﬁt of the regres-   sion model predicting aggregated performance ( F1   macro ): relative importance of each IV ( measured   by the relative amount of explained R ) , the GVIF ,   and signiﬁcance .   D Item - based performance analysis   The following tables and paragraphs contain more   details about the regression analysis to predict   model performance at the item - level ( probability   of report ) .   D.1 Dependent Variable   Table 2 and 3 display the distribution of pre-   dicted probabilities by the best classiﬁcation model   ( BERT ) for each error type before and after trans-   formation .   D.2 Feature Reduction and Model Selection   While tackling a regression analysis task with very   many IVs as it is in our case , there is not just one   strategy for model selection ( i.e. , which IVs and   interactions to include in the regression model ) .   Given the large pool of ( potentially correlated )   51 contribution - level features which we wanted to   combine with the experimental conﬁguration fea-   tures ( training setup andtest corpus ) ,   and being interested in potential interactions as   well , we decided to pre - select the contribution - level   features based on their correlation .   The ﬁrst step in our selection of contribution-   level features is a correlation analysis conducted   on the full dataframe ( false - positives and false neg-   atives ) . We clustered the 51 features based on their   pairwise Spearman correlation . The output of the   clustering is displayed in the dendrogram in ﬁgure   4 ) . Based on the assumption that correlated fea-   tures are likely to distort the performance of the   regression model , we established a conservative   threshold of Spearman 0:2and , for each subclus-   ter with a correlation higher than this threshold , we   manually selected only one feature and discared the   others . The manual selection was based on qualita-   tive consideration ( e.g. , the more general feature , or   the more interpretable ) . For example , for the sub-   cluster that contains the MRC imageability score55475548   and the Brysbeart concreteness we keep the Brys-   beart concreteness score because concreteness is   a more general notion . Concreteness quantiﬁes   the extent to which the word ’s referent can be per-   ceived and imageability , the extent to which the   word ’s referent can be perceived visually .   The output of correlation - based qualitative fea-   ture selection is a set of 37 features . The features   we discarded features are : ﬂesch , gunning Fog In-   dex , long words , characters per word , syllables per   word , fear and disgust component , joy component ,   COCA spoken range norms , Sem_D , COCA spo-   ken bigram mutual information , MRC Imageability ,   hdd42_aw and LD_Mean_Accuracy .   At this point the analysis proceeds per subset   ( FPs vs. FNs ) . We ﬁrst run a regression model   with the 37 contribution - level features on the FP   and FN subset , respectively , without interactions .   Next , we perform step - wise model selection on the   regression model . Unsurprisingly , we ﬁnd no   collinearities . The output of the stepAIC selection   are two feature - based regression models , one for   the FPs and one for the FNs . The feature - based re-   gression models contain 11 contribution - level fea-   tures for FPs and 20 contribution - level features for   FNs respectively .   The next step in our analysis is to incremen-   tally add the experimental conﬁguration features ,   training setup ( 3 levels : Europolis , Re-   gRoom , all ) and test corpus ( 2 levels : Eu-   ropolis , RegRoom ) . First , we add them IVs ﬁrston top of the contribution level features . Then , we   add the following two - way interactions : those be-   tween contribution - level features and experimental   conﬁguration features ( training setup and test cor-   pus ) as well as the two - way interaction between   training setup and test corpus . We simplify the ﬁ-   nal models again using stepAIC and checked for   multicollinearities . At each step , we test the sig-   niﬁcance between a richer model and its nested   counterpart using the ANOV A function from R.   ( e.g. , the model with contribution - level features   in nested in the model with contribution - level +   experimental conﬁguration features ) .   As the output of this further process of selection   of the IVs , we have two ﬁnal regression models ,   one for the FPs and one for the FNs , which unsur-   prisingly differ in terms of the selected predictors   and explained variance . In the next section we pro-   vide the details for the ﬁt of the models . Tables 13   and 14 provide the details of the ﬁt of the selected   model regression model for FPs and FNs , respec-   tively . For each selected IV ( or interaction ) , the   tables display : degrees of freedom , GVIF variance   inﬂation factor , signiﬁcance , as well as explained   variance in R2 .   Effect plots Figures 7 to 5 display the effect plots   referred to in the discussion in section 5.2.5549555055515552E Case - study : Reports and Argument   Quality   To investigate the impact of reports on the quality   of a contribution , we conducted a pilot study on   available Argument Quality datasets .   The ﬁrst Argument Quality dataset we em-   ployed is the Dagstuhl-15512 - ArgQuality corpus   ( Wachsmuth et al . , 2017c ): it is small but contains   quality annotations for a very ﬁne - grained taxon-   omy of argument quality dimensions ( Wachsmuth   et al . , 2017b ) . We employed the best classiﬁer from   our experiments to predict whether a comment con-   tained or not a report and found 59 comments con-   taining report ( out of a total of 320 ) . We use t - tests   to carry out a pairwise comparison of the means of   the quality scores for each argument quality dimen-   sion ( arguments with reports vs. argument without   reports ) and found the values of the documents   containing reports scoring signiﬁcantly higher in   appropriateness , emotional appeal and sufﬁciency   than the ones not containing reports ( table 15 ) . In   particular , the higher score for emotional appeal is   in line with the expectation that contributions with   reports are more effective on the affective dimen-   sions of argument quality .   Next , we conducted the same analysis on the   grammarly Argument Quality corpus ( GAQ ) ( Ng   et al . , 2020 ) which contains 3,373 comments from   online fora with gold annotations for each of the 3   core dimensions of the taxonomy of ( Wachsmuth   et al . , 2017b ) ( cogency , effectiveness , reasonable-   ness ) and overall quality . Our classiﬁer detected   reports in 1,288 comments . We conducted the same   type of analysis as above , and found comments con-   taining reports scoring signiﬁcantly higher than the   non - reports ones in all dimensions ( table 16 ) .   Last , we used a state - of - the art multi - task regres-   sion classiﬁer ( Lauscher et al . , 2020 ) ( trained on   the GAQ corpus ) to automatically predict argument   quality scores for our three datasets . The perfor-   mance of this classiﬁer on RegulationRoom has al-   ready been validated in a manual annotation study   by ( Falk et al . , 2021 ) . For each dataset , we com-   pared the means of contributions with and without   reports ( based on the gold standard for each cor-   pus ) . While we found no signiﬁcant difference in   CMV or RegRoom , we did ﬁnd that for Europolis   ( Table 17 ) contributions containing reports have   signiﬁcantly higher means for all dimensions of   Argument Quality . This is in line with the ﬁndings   by ( Gerber et al . , 2018 ) who state that people whoscore high on the deliberative quality dimensions   also use reports to back up their claim .   Quality dimension t - value p - value   appropriateness 2.040:05   emotional appeal 2.290:05   sufﬁciency 2.020:05   Quality dimension t - value p - value   cogency 5.000:001   effectiveness 5.910:001   reasonableness 5.600:001   overall 5.840:001   Quality dimension t - value p - value   cogency 3.170:05   effectiveness 3.250:05   reasonableness 3.190:05   overall 3.270:055553