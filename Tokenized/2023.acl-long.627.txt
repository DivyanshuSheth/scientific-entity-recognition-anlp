  Fanqi Wan , Weizhou Shen , Ke Yang , Xiaojun Quan , Wei BiSchool of Computer Science and Engineering , Sun Yat - sen University , ChinaTencent AI Lab   { wanfq , shenwzh3 , yangk59}@mail2.sysu.edu.cn ,   quanxj3@mail.sysu.edu.cn , victoriabi@tencent.com   Abstract   Retrieving proper domain knowledge from an   external database lies at the heart of end - to-   end task - oriented dialog systems to generate   informative responses . Most existing systems   blend knowledge retrieval with response gen-   eration and optimize them with direct supervi-   sion from reference responses , leading to sub-   optimal retrieval performance when the knowl-   edge base becomes large - scale . To address this ,   we propose to decouple knowledge retrieval   from response generation and introduce a multi-   grained knowledge retriever ( MAKER ) that in-   cludes an entity selector to search for relevant   entities and an attribute selector to filter out   irrelevant attributes . To train the retriever , we   propose a novel distillation objective that de-   rives supervision signals from the response gen-   erator . Experiments conducted on three stan-   dard benchmarks with both small and large-   scale knowledge bases demonstrate that our   retriever performs knowledge retrieval more ef-   fectively than existing methods . Our code has   been made publicly available .   1 Introduction   When task - oriented dialog ( TOD ) systems try to   accomplish a task such as restaurant reservations   and weather reporting for human users , they gen-   erally resort to an external knowledge base ( KB )   to retrieve relevant entity information for generat-   ing an informative system response . Conventional   pipeline systems comprise several modules such as   dialogue state tracking and dialogue policy learning   that require annotations for training , where interme-   diate predictions such as belief state can be used for   the retrieval . By contrast , end - to - end task - oriented   dialog ( E2E - TOD ) systems aim to eliminate the de-   pendence on intermediate annotations and generate   the response end - to - end ( Wu et al . , 2019 ) . Appar-   ently , knowledge retrieval is at the core of this task , Figure 1 : Performance of four end - to - end task - oriented   dialog systems on MultiWOZ 2.1 when knowledge   bases of different sizes are used . The evaluation metric   is Entity F1 scores of entities in generated responses .   “ Condensed ” means that each dialog is associated with   a small - sized knowledge base , which is the default set-   ting of many current systems . “ In - domain ” means that   each dialog corresponds to a knowledge base of the   same domain , while “ Cross - domain ” means that all di-   alogs share the same large - scale cross - domain knowl-   edge base provided in the dataset .   which is non - trivial as no gold labels are available   for training a retriever . Arguably , this problem has   limited the performance of existing E2E - TOD sys-   tems considering that substantial progress has been   made in natural language generation .   Roughly , existing approaches for knowledge re-   trieval in E2E - TOD systems can be divided into   three categories . First , the knowledge base can be   embedded into a memory network and queried with   the representations of dialogue context ( Madotto   et al . , 2018 ; Qin et al . , 2020 ; Raghu et al . , 2021 ) .   Second , the serialized knowledge base records can   be encoded together with dialog context by pre-   trained language models ( Xie et al . , 2022 ; Wu   et al . , 2022 ; Tian et al . , 2022 ) . Third , the knowl-   edge base can be embedded into model param-   eters through data augmentation to support im-   plicit knowledge retrieval ( Madotto et al . , 2020 ;   Huang et al . , 2022 ) . These approaches generally11196blend knowledge retrieval and response generation   and train them by the supervision of reference re-   sponses , which has two limitations . First , the sys-   tem response usually consists of pure language   tokens and KB - related tokens ( e.g. , hotel names   and phone numbers ) , and it is challenging to train   a good retriever from the weak supervision of ref-   erence responses . Second , the systems may be-   come inefficient when the scale of the knowledge   base grows large . Our preliminary studyin Figure   1 confirms that when a large - scale cross - domain   knowledge base is given , existing dialog systems   suffer significant performance degradation .   In this paper , we propose a novel Multi - grAined   KnowlEdge Retriever ( MAKER ) for E2E TOD   systems to improve the acquisition of knowledge   for response generation . The retriever decouples   knowledge retrieval from response generation and   introduces an entity selector and an attribute selec-   tor to select relevant entities and attributes from   the knowledge base . Then , the response generator   generates a system response based on the dialogue   context and the multi - grained retrieval results . The   retriever is trained by distilling knowledge from the   response generator using the cross - attention scores   of KB - related tokens in the response . We train   the entity selector , attribute selector , and response   generator jointly in an end - to - end manner .   We compare our system with other E2E TOD   systems on three benchmark datasets ( Eric et al . ,   2017 ; Wen et al . , 2017 ; Eric et al . , 2020 ) . Empirical   results show that our system achieves state - of - the-   art performance when either a small or a large-   scale knowledge base is used . Through in - depth   analysis , we have several findings to report . First ,   our retriever shows great advantages over baselines   when the size of knowledge bases grows large . Sec-   ond , of the two selectors , the entity selector plays a   more important role in the retriever . Third , our sys-   tem consistently outperforms baselines as different   numbers of records are retrieved , and works well   even with a small number of retrieval results .   2 Related Work   2.1 End - to - End Task - Oriented Dialog   Existing approaches for knowledge retrieval in   end - to - end task - oriented dialog systems can be   divided into three categories . First , the knowl-   edge base ( KB ) is encoded with memory net-   works , and KB records are selected using at - tention weights between dialogue context and   memory cells . Mem2seq ( Madotto et al . , 2018 )   uses multi - hop attention over memory cells to se-   lect KB tokens during response generation . KB-   Retriever ( Qin et al . , 2019 ) retrieves the most rel-   evant entity from the KB by means of attention   scores to improve entity consistency in the system   response . GLMP ( Wu et al . , 2019 ) introduces a   global - to - local memory pointer network to retrieve   relevant triplets to fill in the sketch response . CD-   NET ( Raghu et al . , 2021 ) retrieves relevant KB   records by computing a distillation distribution   based on dialog context .   Second , the concatenation of knowledge base   and dialogue context is taken as input for pre-   trained language models . UnifiedSKG ( Xie et al . ,   2022 ) uses a unified text - to - text framework to gen-   erate system responses . DialoKG ( Rony et al . ,   2022 ) models the structural information of knowl-   edge base through knowledge graph embedding   and performs knowledge attention masking to se-   lect relevant triples . Q - TOD ( Tian et al . , 2022 )   proposes to rewrite dialogue context to generate a   natural language query for knowledge retrieval .   Third , the knowledge base is stored in model   parameters for implicit retrieval during response   generation . GPT - KE ( Madotto et al . , 2020 ) pro-   poses to embed the knowledge base into pre-   trained model parameters through data augmen-   tation . ECO ( Huang et al . , 2022 ) first generates   the most relevant entity with trie constraint to en-   sure entity consistency in the response . However ,   these methods generally blend entity retrieval and   response generation during response generation ,   which leads to sub - optimal retrieval performance   when large - scale knowledge bases are provided .   2.2 Neural Retriever   With the success of deep neural networks in various   NLP tasks , they have also been applied to informa-   tion retrieval . One of the mainstream approaches   is to employ a dual - encoder architecture ( Yih et al . ,   2011 ) to build a retriever . Our work is mostly   inspired by the retrieval methods in question an-   swering . To train a retriever with labeled question-   document pairs , DPR ( Karpukhin et al . , 2020 ) uses   in - batch documents corresponding to other ques-   tions together with BM25 - retrieved documents as   negative samples for contrastive learning . To train   a retriever with only question - answer pairs instead   of question - document pairs , which is a weakly su-11197   pervised learning problem , researchers propose to   distill knowledge from the answer generator to train   the retriever iteratively ( Yang and Seo , 2020 ; Izac-   ard and Grave , 2020 ) . Other researchers try to train   the retriever and generator in an end - to - end man-   ner . REALM ( Guu et al . , 2020 ) , RAG ( Lewis et al . ,   2020 ) , and EMDR(Singh et al . , 2021 ) propose   to train the retriever end - to - end through maximum   marginal likelihood . Sachan et al . ( 2021 ) propose   to combine unsupervised pre - training and super-   vised fine - tuning to train the retriever . Motivated   by these works , we propose a multi - grained knowl-   edge retriever trained by distilling knowledge from   the response generator in E2E - TOD systems .   3 Methods   In this section , we first describe the notations and   outline our method , and then introduce the knowl-   edge retriever and response generator in detail .   3.1 Notations   Given a dialog D={U , R , ... , U , R}ofT   turns , where UandRare the t - th turn user utter-   ance and system response , respectively . We use   Cto represent the dialog context of the t - th turn ,   where C={U , R , ... , U , R , U } . An ex-   ternal knowledge base ( KB ) is provided in the form   of a set of entities , i.e. , K={E , E , ... , E } ,   where each entity Eis composed of Nattribute-   value pairs , i.e. , E={a , v , ... , a , v } . End-   to - end task - oriented dialog systems take dialogue   context Cand knowledge base Kas input and   generate an informative response R.3.2 System Overview   The architecture of our end - to - end task - oriented   dialog system is shown in Figure 2 . At each turn of   conversation , our system resorts to a Multi - grAined   KnowlEdge Retriever ( MAKER ) to retrieve a set   of entities from the external knowledge base . Then ,   the response generator takes as input the retrieved   entities together with the dialog context and gener-   ates a natural language response . The overall sys-   tem is optimized in an end - to - end manner without   the need for intermediate annotations .   The novelty of MAKER lies in that it decou-   ples knowledge retrieval from response generation   and provides multi - grained knowledge retrieval by   means of an entity selector and an attribute selector .   Specifically , the knowledge base is first encoded   with an entity encoder Encat entity level . Then ,   the dialogue context is encoded with a context en-   coder Encand used to retrieve a set of relevant   entities from the knowledge base , which is referred   to as entity selection . Next , irrelevant attributes are   filtered out with an attribute selector based on the   interaction of dialog context and retrieved entities ,   where another encoder Encis used . Finally , each   retrieved entity is concatenated with the dialog con-   text and passed to a generator encoder Encto ob-   tain their representations , based on which the gener-   ator decoder Decproduces a system response . To   train the retriever , the cross - attention scores from   KB - related tokens in the reference response to each   retrieved entity are used as supervision signals to   update the entity selector , while the attribute selec-   tor is trained by using the occurrences of attribute   values in the dialogue as pseudo - labels . To better11198measure the relationship between entities and re-   sponse , the whole training process involves two   stages . First , the warming - up stage only trains the   attribute selector and the response generator , with   the entity selector not updated . As the above train-   ing converges , the second stage starts to update the   entity selector together with other modules using   cross - attention scores from the response generator .   3.3 Knowledge Retriever   In this section , we introduce the entity selector ,   attribute selector , and the training of the retriever .   Entity Selector To support large - scale knowl-   edge retrieval , we model the entity selector as a   dual - encoder architecture , where one encoder Enc   is used to encode the dialogue context and another   encoder Encis to encode each entity ( row ) of the   knowledge base , both into a dense vector . To en-   code an entity , we concatenate the attribute - value   pairs of this entity into a sequence and pass it to   Enc . The selection score sfor entity Eis de-   fined as the dot product between the context vector   and the entity vector as :   s = Enc(C)Enc(E ) . ( 1 )   Then , the top- Kentities are obtained by :   E = TopK(s ) = { E , ... , E } . ( 2 )   Retrieving the top- Kentities can be formulated   as maximum inner product search ( MIPS ) , which   can be accelerated to sub - linear time using efficient   similarity search libraries such as FAISS ( Johnson   et al . , 2019 ) . We implement EncandEncwith a   pre - trained language model and allow them to share   weights , where the final “ [ CLS ] ” token representa-   tion is used as the encoder output . Existing studies   suggest that initializing EncandEncwith BERT   weights may lead to collapsed representations and   harm the retrieval performance . Therefore , follow-   ing KB - retriever ( Qin et al . , 2019 ) , we initialize   them by pre - training with distant supervision .   Since the entity selector is updated by knowl-   edge distillation , recalculating the embeddings of   all entities after each update introduces consider-   able computational cost . Therefore , we follow   EMDR(Singh et al . , 2021 ) to update the embed-   dings of all entities after every 100 training steps .   Attribute Selector To remove irrelevant at-   tributes and values from the retrieved entities forfiner - grained knowledge , we design an attribute se-   lector as follows . We first concatenate dialog con-   textCwith each entity E∈ Eand encode them   with an attribute encoder Enc , which is also a pre-   trained language model . Then , the final “ [ CLS ] ” to-   ken representation of Encis extracted and mapped   into a N - dimensional vector by a feed - forward net-   work ( FFN ) for attribute scoring :   a = FFN(Enc([C;E ] ) ) , ( 3 )   where each element in a∈Rrepresents the   importance of the corresponding attribute .   Note that aonly measures the importance of   attributes in E. To obtain the accumulated impor-   tance , we calculate the sum of aover all retrieved   entities weighted by entity selection score s :   a = σ(/summationdisplaysa ) , ( 4 )   where σrepresents the sigmoid function .   Finally , the attributes whose importance scores   inaare greater than a pre - defined threshold τ   are selected to construct an attribute subset . The   retrieved entities clipped with these attributes are   treated as multi - grained retrieval results denoted by   ˆE. Specifically , we obtain ˆEby masking irrelevant   attribute - value pairs in each retrieved entity of E.   ˆE = Clip(E , a , τ ) = { ˆE , ... , ˆE}.(5 )   To train the attribute selector , we design an aux-   iliary multi - label classification task . The pseudo-   label is a N - dimensional 0 - 1 vector bconstructed   by checking whether any value of an attribute in ˆE   appears in dialogue context Cor system response   R. Then , we define a binary cross - entropy loss   Lfor this classification task as :   L = BCELoss ( a , b ) . ( 6 )   Updating The entity selector is updated by dis-   tilling knowledge from the response generator as   supervision signals . Specifically , since only KB-   related tokens in the response are directly con-   nected to the knowledge base , we regard the cross-   attention scores from these tokens to each retrieved   entity as the knowledge to distill . The rationality   behind this is that the cross - attention scores can   usually measure the relevance between each en-   tity and the response . Supposing response Rcon-   tainsMKB - related tokens , we denote the cross-   attention scores from each KB - related token to11199entity ˆEbyC∈R , where |ˆE|rep-   resents the number of tokens in ˆEandLis the   number of decoder layers . Then , we calculate an   accumulated score for entity ˆEas :   ˆc=/summationdisplay / summationdisplay / summationdisplayC . ( 7 )   Then , ˆcis softmax - normalized to obtain a cross-   attention distribution cover the Kretrieved enti-   ties to reflect their importance for the response .   Finally , we calculate the KL - divergence between   the selection scores sof retrieved entities and   cross - attention distribution cas the training loss :   L = D(s||c ) . ( 8)   3.4 Response Generator   Inspired by Fusion - in - Decoder ( Izacard and Grave ,   2020 ) in open - domain question answering , we em-   ploy a modified sequence - to - sequence structure for   the response generator to facilitate direct interac-   tion between dialog context and retrieved entities .   Generator Encoder Each entity ˆEinˆEis first   concatenated with dialog context Cand encoded   into a sequence of vector representations H :   H = Enc([C;ˆE ] ) , ( 9 )   where Encrepresents the encoder of the response   generator . Then , the representations of all retrieved   entities are concatenated into H :   H= [ H; ... ;H ] . ( 10 )   Generator Decoder Taking Has input , the   generator decoder Decproduces the system re-   sponse token by token . During this process , the   decoder not only attends to the previously gener-   ated tokens through self - attention but also attends   to the dialogue context and retrieved entities by   cross - attention , which facilitates the generation of   an informative response . The probability distribu-   tion for each response token in Ris defined as :   P(R ) = Dec(R|R , H ) . ( 11 )   We train the response generator by the standard   cross - entropy loss as :   L=/summationdisplay−logP(R ) , ( 12)where |R|denotes the length of R.   Lastly , the overall loss of the system is the sum   of entity selection loss L , attribute selection loss   L , and response generation loss L :   L = L+L+L. ( 13 )   3.5 Discussions   Although deriving much inspiration from open-   domain question answering ( QA ) ( Izacard and   Grave , 2020 ) , where the labels for retrieval are   also not available , the scenario of this work is quite   different . One major difference is that the answer   in open - domain QA is completely from the exter-   nal source of knowledge , while some responses   and tokens in dialog systems may not be relevant   to the external knowledge base . That means dialog   systems need to accommodate both dialog context   and external knowledge and generate a fluent and   informative natural language response , making this   task thornier than open - domain QA .   The main differences between our MAKER   and existing knowledge retrieval methods in   task - oriented dialog systems are twofold . First ,   MAKER decouples knowledge retrieval from re-   sponse generation and provides multi - grained   knowledge retrieval of both entities and attributes .   The retrieval results are explicitly passed to the   generator to produce a system response . Second ,   MAKER is trained by distilling knowledge from   the response generator for supervision , which   varies from existing attention - based approaches .   4 Experimental Settings   4.1 Datasets   We evaluate our system on three multi - turn   task - oriented dialogue datasets : MultiWOZ 2.1   ( MWOZ ) ( Eric et al . , 2020 ) , Stanford Multi-   Domain ( SMD ) ( Eric et al . , 2017 ) , and CamRest   ( Wen et al . , 2017 ) . Each dialog in these datasets   is associated with a condensed knowledge base ,   which contains all the entities that meet the user   goal of this dialog . For MWOZ , each condensed   knowledge base contains 7 entities . For SMD and   CamRest , the size of condensed knowledge bases   is not fixed : it ranges from 0 to 8 with a mean of   5.95 for SMD and from 0 to 57 with a mean of   1.93 for CamRest . We follow the same partitions as   previous work ( Raghu et al . , 2021 ) . The statistics   of these datasets are shown in Appendix A.11200BLEU ( Papineni et al . , 2002 ) and Entity F1   ( Eric et al . , 2017 ) are used as the evaluation met-   rics . BLEU measures the fluency of a generated   response based on its n - gram overlaps with the   gold response . Entity F1 measures whether the   generated response contains correct knowledge by   micro - averaging the precision and recall scores of   attribute values in the generated response .   4.2 Implementation Details   We employ BERT ( Devlin et al . , 2019 ) as the en-   coder of our entity selector and attribute selector ,   and employ T5 ( Raffel et al . , 2020 ) to implement   the response generator . All these models are fine-   tuned using AdamW optimizer ( Loshchilov and   Hutter , 2018 ) with a batch size of 64 . We train   these models for 15k gradient steps with a linear   decay learning rate of 10 . We conduct all ex-   periments on a single 24 G NVIDIA RTX 3090   GPU and select the best checkpoint based on model   performance on the validation set . More detailed   settings can be found in Appendix E.   4.3 Baselines   We compare our system with the following base-   lines , which are organized into three categories   according to how they model knowledge retrieval .   Memory network : These approaches embed the   knowledge base into a memory network and query   it with the representation of dialog context , includ-   ing DSR ( Wen et al . , 2018 ) , KB - Retriever ( Qin   et al . , 2019 ) , GLMP ( Wu et al . , 2019 ) , DF - Net ( Qin   et al . , 2020 ) , EER ( He et al . , 2020b ) , FG2Seq ( He   et al . , 2020a ) , CDNET ( Raghu et al . , 2021 ) , and   GraphMemDialog ( Wu et al . , 2022 ) .   Direct fusion : These approaches encode serial-   ized knowledge base records together with dialog   context by pre - trained language models , including   DialoKG ( Rony et al . , 2022 ) , UnifiedSKG ( Xie   et al . , 2022 ) , and Q - TOD ( Tian et al . , 2022 ) .   Implicit retrieval : These approaches embed the   knowledge base into model parameters by data aug-   mentation to provide implicit retrieval during re-   sponse generation , including GPT-2+KE ( Madotto   et al . , 2020 ) and ECO ( Huang et al . , 2022 ) .   5 Results and Analysis   In this section , we first show the overall perfor-   mance of the evaluated systems given a condensed   knowledge base for each dialog . Then , we compare   them with a more practical setting in which a large - scale knowledge base is provided . We also conduct   an in - depth analysis of the proposed retriever . More   experiments are presented in the appendix .   5.1 Overall Results   The overall results are shown in Table 1 . We ob-   serve that our system with T5 - Large as the back-   bone model achieves the state - of - the - art ( SOTA )   performance on MWOZ and SMD . Specifically , on   MWOZ our system surpasses the previous SOTA ,   namely Q - TOD , by 1.15 points in BLEU and 4.11   points in Enity F1 . On SMD , the improvements   over Q - TOD are 4.58 points in BLEU and 0.19   points in Enity F1 . On CamRest , our system only   achieves the best performance in BLEU but un-   derperforms the best - performing DialoKG slightly .   The reason behind this phenomenon is that many di-   alogues in CamRest contain extremely small knowl-   edge bases , with only 1 - 2 entities , leaving little   space for our retriever to show its advantage .   Note that with the same backbone generator ( T5-   Base / T5 - Large ) , our system surpasses Q - TOD even   though it relies on human annotations to train a   query generator for knowledge retrieval . The pos-   sible reason is that while the retriever of Q - TOD is   independent of response generation , ours is trained   and guided by knowledge distillation from response   generation . Moreover , in addition to retrieving en-   tities from the knowledge base , our retriever also   conducts a fine - grained attribute selection .   5.2 Large - Scale Knowledge Base   The experiments in Section 5.1 are conducted with   each dialog corresponding to a condensed knowl-   edge base . Although most previous systems are   evaluated in this setting , it is not practical to have   such knowledge bases in real scenes , where the sys-   tems may need to retrieve knowledge from a large-   scale knowledge base . Therefore , we examine the   performance of several well - recognized E2E TOD   systems by implementing them on a large - scale   cross - domain knowledge base ( referred to as “ full   knowledge base ” ) on MWOZ and CamRest , respec-   tively , where the knowledge base is constructed by   gathering the entities for all dialogs in the original   dataset . The results are shown in Table 2 .   We observe that our system outperforms base-   lines by a large margin when the full knowledge11201   base is used . In addition , there are two other obser-   vations . First , comparing the results in Table 1 and   Table 2 , we note existing systems suffer a severe   performance deterioration when the full knowledge   base is used . For example , the Enity F1 score of   DF - Net drops by 7.79 points on MWOZ , while   our system only drops by 2.81/2.6 points . Second ,   our system with the full knowledge base still out-   performs other systems when they use condensed   knowledge bases , which is easier to retrieve . These   observations verify the superiority of our system   when applied to a large - scale knowledge base as   well as the feasibility of applying it to real scenes .   5.3 Ablation Study   We conduct an ablation study of our retriever   MAKER with both condensed and full knowledge   bases on MWOZ , and show the results in the first   and the second blocks of Table 3 , respectively .   When condensed knowledge bases are used , the   system suffers obvious performance drops with the   removal of distillation ( w / odistillation ) or entity   selection ( w / oent_selector ) . This indicates that de-   spite the quality of condensed knowledge bases , our   retriever can further learn to distinguish between   the entities by distilling knowledge from the re-   sponse generator . Besides , the performance of the   system drops when the attribute selector is aban-11202   doned ( w / oattr_selector ) , showing that attribute   selection is also indispensable in the retriever .   When the full knowledge base is used , entity se-   lection is more necessary for the system . Therefore ,   we only ablate the distillation component and the   attribute selector . The results show that the system   suffers significant performance degradation when   distillation is removed ( w / odistillation ) . Attribute   selection is also shown important as the perfor-   mance drops upon it is removed ( w / oattr_selector ) .   5.4 Comparison of Retrieval Methods   To further demonstrate the effectiveness of our   multi - grained knowledge retriever , we compare dif-   ferent retrieval methods on the full knowledge base   of MWOZ . Specifically , we first retrieve the top- K   entities with different retrieval methods and em-   ploy the same response generator to generate the   system response . Moreover , we propose a new   metric , i.e. , Recall@7 , to measure whether the sug-   gested entities in the system response appear in   the 7 retrieved entities . As shown in Table 4 , the   proposed retriever achieves the best performance   compared with other methods except Oracle , which   uses condensed knowledge bases without retrieval ,   in both generation metrics ( BLEU , Entity F1 ) and   the retrieval metric ( Recall@7 ) .   To investigate the effect of different numbers of   retrieved entities on system performance , we report   the Entity F1 and Recall@ xscores of the above re-   trieval methods as the number of entities changes ,   while Oracle is not included because we can not   rank its entities . We observe in Figure 3(a ) that the   Recall@ xscores for all methods improve as the   number of entities grows , while our retriever con-   sistently achieves the best performance . In Figure   3(b ) , we observe no positive correlation between   the Entity F1 score and the number of entities , sug-   gesting that noisy entities may be introduced as the   number of entities increases . We can also observe   that the number of entities corresponding to the   peak of the Entity F1 scores varies for different   methods , while our retriever only requires a small   number of entities to reach the peak performance .   5.5 Attribute Selection Methods   In Section 3.3 , we calculate an accumulated impor-   tance score for each attribute weighted by entity   selection scores to determine which attributes are   preserved based on a given threshold . In Table 5 ,   we compare different methods for accumulating   the attribute scores as well as different approaches   for filtering out irrelevant attributes . It can be ob-   served that direct averaging rather than weighting   by entity selection scores hurts the Entity F1 score .   This indicates that the retriever can select attributes   more appropriately based on the selection scores   of retrieved entities . We also observe that using   top - Kinstead of a threshold to select attributes   leads to a lower Entity F1 score than preserving all   attributes . We believe the reason is that the number   of attributes to be selected varies for each dialogue   context , and therefore simply selecting the top- K   attributes results in sub - optimal attributes.112036 Conclusion   We propose a novel multi - grained knowledge re-   triever ( MAKER ) for end - to - end task - oriented dia-   log systems . It decouples knowledge retrieval from   response generation and introduces an entity se-   lector and an attribute selector to acquire multi-   grained knowledge from the knowledge base . The   retriever is trained by distilling knowledge from   the response generator . Empirical results show that   our system achieves state - of - the - art performance   when either a small or a large - scale knowledge   base is provided for each dialog . Through in - depth   analysis , our retriever shows great advantages over   baselines when the size of knowledge bases grows   large . Of the two selectors , the entity selector is   shown to be more prominent in the retriever .   Acknowledgements   This work was supported by the National Natu-   ral Science Foundation of China ( No.62176270 ) ,   the Guangdong Basic and Applied Basic Research   Foundation ( No.2023A1515012832 ) , the Program   for Guangdong Introducing Innovative and En-   trepreneurial Teams ( No . 2017ZT07X355 ) , and   the Tencent AI Lab Rhino - Bird Focused Research   Program . We thank Yingqi Gao and Canbin Huang   for their efforts in the preliminary experiments .   Limitations   Our system employs a modified sequence - to-   sequence architecture to implement the response   generator . Since the length of dialogue context   increases as the dialogue continues , the generator   needs to input multiple long dialogue contexts to   the encoder simultaneously , each for a retrieved   entity . This may cause redundancy in the input and   lowers the proportion of KB - related information .   We will explore more efficient architectures for the   response generator in future work .   Ethics Statement   All the experiments are conducted on publicly avail-   able datasets , which do n’t include any private in-   formation . Our work does n’t involve identity char-   acteristics or any gender and racial discrimination .   References1120411205   A Statistics of Datasets   The statistics of the datasets are shown in Table 6 .   B Preliminary Study   The detailed results of our preliminary study for   condensed , in - domain , and cross - domain knowl-   edge bases are shown in Table 7 . The results of   baseline models on condensed knowledge bases are   cited from ( Raghu et al . , 2021 ) . We produce their   results on in - domain and cross - domain knowledge   bases by using the officially released code .   C Pre - training for Entity Selector   Given a dialogue context and the system response ,   we use the entity with the most occurrences of its   attribute values in the dialogue context and system   response as the label . Then we apply supervised   contrastive learning for optimization ( Gao et al . ,   2021 ) . Specifically , the positive example of a dia-   logue context is the corresponding labeled entity ,   while the negative examples are the labeled entities   of other examples in the same mini - batch . Then ,   we employ the InfoNCE loss as the training ob-   jective to pull close the sentence representations   of positive samples and push away that of nega-   tive samples . We conduct this pre - training on the   MWOZ and CamRest datasets . Since the knowl-   edge base of the SMD dataset is strictly specific to   each dialog , we can not collect a global knowledgebase from the dialogs . Thus , the pre - training is not   conducted on SMD . The hyperparameters for the   pre - training are shown in Table 8 .   D Domain - Wise Results   We report the domain - wise results with condensed   knowledge bases on MWOZ and SMD in Table 9   and Table 10 , respectively . The results of baseline   models are cited from ( Raghu et al . , 2021 ) , ( Rony   et al . , 2022 ) , and ( Tian et al . , 2022 ) .   E More Implementation Details   The hyperparameters of our system with condensed   and full knowledge bases are shown in Table 11 and   Table 12 , respectively . Our method has three con-   tributions : knowledge distillation , entity selection ,   and attribute selection . We list the application of   these contributions with condensed and full knowl-   edge base in Table 13 and Table 14 , respectively .   F Case Study   In Figure 4 , we provide a dialogue example from   the MWOZ dataset . We can observe that , for a   given user utterance , our system can retrieve en-   tities that satisfy the user goal , while masking ir-   relevant attributes . Then , it generates appropriate   system responses . Note that when the user goal   changes , e.g. , in the second turn of this case when   the user wants a cheap restaurant , our retriever can   retrieve the corresponding one , with the attribute   of price range being preserved.112061120711208ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Section Ethics Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4.1 and Section 4.2   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4.1 and Section 4.2   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4.1 and Appendix A   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4.1 and Appendix A   C / squareDid you run computational experiments ?   Section 4 and Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.211209 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Sections 4.2 and Appendix E   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Our experiments are extensive and computationally expensive . All experiments are based on the   same random seed ( 111 ) .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   We use the same evaluation metrics ( BLEU , Entity - F1 ) as previous works and provide proper citations   in Section 4.1 Datasets   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.11210