  Hritik BansalDa YinMasoud Monajatipoor Kai - Wei Chang   Computer Science Department , University of California , Los Angeles   { hbansal,da.yin,kwchang}@cs.ucla.edu ,   monajati@ucla.edu   Abstract   Text - to - image generative models have achieved   unprecedented success in generating high-   quality images based on natural language de-   scriptions . However , it is shown that these mod-   els tend to favor specific social groups when   prompted with neutral text descriptions ( e.g. ,   ‘ a photo of a lawyer ’ ) . Following Zhao et al .   ( 2021 ) , we study the effect on the diversity of   the generated images when adding ethical inter-   vention that supports equitable judgment ( e.g. ,   ‘ if all individuals can be a lawyer irrespective of   their gender ’ ) in the input prompts . To this end ,   we introduce an Ethical NaTural Language   Interventions in Text - to - Image GEN eration   ( ENTIGEN ) benchmark dataset to evaluate the   change in image generations conditional on   ethical interventions across three social axes –   gender , skin color , and culture . Through ENTI-   GEN framework , we find that the generations   from minDALL · E , DALL · E - mini and Stable   Diffusion cover diverse social groups while pre-   serving the image quality . Preliminary studies   indicate that a large change in the model pre-   dictions is triggered by certain phrases such as   ‘ irrespective of gender ’ in the context of gender   bias in the ethical interventions . We release   code and annotated data at https://github .   com / Hritikbansal / entigen_emnlp .   1 Introduction   Recent Text - to - Image generative models ( Ramesh   et al . , 2021 , 2022 ; Ding et al . , 2021 ; Saharia et al . ,   2022 ; Nichol et al . , 2021 ; Rombach et al . , 2022 )   can synthesize high - quality photo - realistic images   conditional on natural language text descriptions   in a zero - shot fashion . For instance , they can gen-   erate an image of ‘ an armchair in the shape of an   avocado ’ which appears rarely in the real world .   However , despite the unprecedented zero - shot abil-   ities of the text - to - image generative models , recent   experiments with small - scale instantiations ( suchFigure 1 : We study the change in the model generations   across various groups ( man / woman , light - skinned / dark-   skinned , Western / Non - Western ) before and after adding   ethical interventions ( in purple ) during text - to - image   generation . We use CLIP and Human annotations to   assign a social group to the model generations . We   present a few output generations in Appendix Fig . 4 - 8 .   as minDALL · E ) have shown that prompting the   model with neutral texts ( ‘ a photo of a lawyer ’ ) ,   devoid of any cues towards a social group , still gen-   erates images that are biased towards white males   ( Cho et al . , 2022 ) .   In our work , we consider three bias axis – 1 )   { man , woman } grouping across gender axis , 2 )   { light - skinned , dark - skinned } grouping across skin   color axis , and 3 ) { Western , Non - Western } group-   ing across cultural axis . The existence of any gen-   derand skin color bias(see Ethical Statements   for more discussion ) causes potential harms to un-   derrepresented groups by amplifying bias present   in the dataset ( Birhane et al . , 2021 ; Barocas et al . ,   2018 ) . Hence , it is essential for a text - to - image   system to generate diverse set of images .   To this end , we study if the presence of addi-1358tional knowledge that supports equitable judgment   help in diversifying model generations . Being part   of text input , this knowledge acts as an ethical in-   tervention over the original neutral prompt ( Zhao   et al . , 2021 ) . Ethical interventions provide models   with ethical advice and do not emanate any visual   cues towards a specific social group . For instance ,   in the context of generating ‘ a photo of a lawyer ’   that tends to be biased towards ‘ light - skinned man ’ ,   we wish to study if prompting the model with ethi-   cally intervened prompt ( e.g. , ‘ a photo of a lawyer   if all individuals can be a lawyer irrespective of   their gender ’ ) can diversify the outputs .   We introduce an Ethical NaTural Language   Interventions in Text - to - Image GEN eration ( ENTI-   GEN ) benchmark dataset to study the change in the   perceived societal bias of the text - to - image genera-   tive models in the presence of ethical interventions .   ENTIGEN covers prompts to study the bias across   three axes – gender , skin color and culture . The neu-   tral prompts in ENTIGEN dataset are intervened   with corresponding ethical knowledge as illustrated   in Figure 1 . We evaluate ENTIGEN on publicly   available models – minDALL · E ( Kim et al . , 2021 ) ,   DALL·E - mini ( Dayma et al . , 2021 ) , and Stable Dif-   fusion ( Rombach et al . , 2022 ) automatically with   CLIP model ( Radford et al . , 2019 ) and manually   with human annotators from MTurk .   Through our experiments , ( 1 ) we show that a   few ethical interventions lead to the diversification   of the image generations across different groups   while preserving the image generation quality . In-   terestingly , in some cases , we observe the bias can   be flipped towards the originally underrepresented   groups with ethical interventions ( Appendix Figure   6 ) . ( 2 ) Moreover , we find that the interventions   containing keywords such as ‘ irrespective of gen-   der ’ and ‘ culture ’ tend to trigger a large change   in model generations . We further analyze the pre-   training data to analyze the context in which these   keywords are used to better understand how they   may affect the diversity in generation .   2 Dataset and Evaluation Methods   In this section , we introduce the process of building   an ethical intervention benchmark ENTIGEN and   evaluating the images generated by text - to - image   generative models.2.1 ENTIGEN Benchmark Construction   Initially , we determine three axes of societal bias to   be studied : gender , skin color , and culture . Specifi-   cally , to investigate the gender and skin color bias ,   ENTIGEN consists of prompts belonging to the   category of professions andobjects . For assessing   the cultural bias , it consists of prompts surround-   ing the wedding ceremonies as it is ubiquitous and   diverse across different regions ( Bell et al . , 1997 ;   Xu and Xu , 2018 ; Acharya et al . , 2020 ) .   Based on the selected axes , we design neutral   prompts without any ethical interventions as the   original prompts . Subsequently , we append ethical   interventions to the original prompts that can per-   haps amend the model ’s behaviour towards more   diverse generations . For instance ,   1.Original Prompt : a photo of a person wear-   ing a [ object ]   2.With Ethical Intervention : a photo of a per-   son wearing a [ object ] if all individuals can   wear a [ object ] irrespective of their gender   ‘ If all individuals can wear a [ object ] irrespective of   their gender ’ is an ethical intervention that guides   diverse outputs in terms of gender . We require the   ethical interventions to not give away any visual   cues to eliminate the effect of any explicit guidance .   We further include irrelevant interventions in   ENTIGEN . These interventions also provide ethi-   cal advice , but do not correspond to any social axes   we study in ENTIGEN . For example , ‘ if honesty is   the best policy ’ is an irrelevant intervention since it   is unrelated to gender , skin color and culture . Ide-   ally , these interventions can not help in diversifying   image generations on either of studied social axes .   In total , we create 246 prompts based on an at-   tribute set containing diverse professions , objects ,   and cultural scenarios .   2.2 Image Generation .   Each prompt in ENTIGEN is used to generate 9   images from each text - to - image generation model   9 times . We choose the publicly available models ,   minDALL · E , DALL · E - mini , and Stable Diffusion   for analysis . It is mainly because these three mod-   els can generate high - quality images efficiently . We   provide more details in Appendix B.13592.3 Evaluation Metrics .   We evaluate the diversity among the generated im-   ages of the models . We focus on the gap between   the number of images associated with the differ-   ent groups ( mentioned in § 1 ) which measure the   demographic disparity across various social axes .   Specifically , for one of the prompts ( e.g. , ‘ a photo   of a [ profession ] if all genders can be a [ profes-   sion ] ’ ) filled with each attribute k(e.g . , police offi-   cer ) in category P(e.g . , profession ) , we count s   ( number of images with man ) and s(number   of images with woman ) , associated with the two   groups a(man ) and b(woman ) across a specific   social axis g(gender ) . Finally , the diversity score   for axis gtowards its groups for category Pis :   diversity=/summationtext|s−s|/summationtext(s+s ) , ( 1 )   where gis one of { gender , skin color , culture } , P   is one of { profession , object , wedding } and kcan   be any attribute according to the category Pwe   select . The generations that could not have been   assigned gender or skin color due to uncertainity   in the judgements of the agents are not included in   this metric . Smaller scores represent more diverse   outputs . The normalization factor in the denomi-   nator of the Eq . ( 1)allows us to compare model   generations from two different prompts – original   and ethically intervened as they could have differ-   ent number of image generations that belong to   either of the two social groups . To quantify the   bias and its direction , given one specific attribute   k , we directly compute the normalized difference   of the two counts ,   bias=/parenleftig   s−s / parenrightig   //parenleftig   s+s / parenrightig   , ( 2 )   belonging to two groups aandb . Greater absolute   value of biasindicates greater bias and vice versa .   Built upon these metrics , CLIP - based and human   evaluations are used to assess output diversity and   bias . Due to limited budget , we select part of the   professions and objects for human annotators to   evaluate . For the entire set of images , we use auto - matic CLIP - based evaluationas a complementary   method . Appendix C provides more details about   our evaluations .   Note that we are aware of the possibility   that CLIP model may be biased towards certain   groups ( Zhang et al . , 2022 ) . We measure the con-   sistency between the gender and skin color deter-   mined by the CLIP model and human annotators   in the images generated for a subset of attributes .   We find that CLIP - based determinations agree with   the human annotations with a rate of 78 - 85 % for   gender recognition while for skin color , the rate   is down to 67 - 78 % . We finally decide to apply   CLIP - based evaluation on gender axis only as the   predictions on gender are more consistent with the   humans .   3 Results   3.1 CLIP - based Results   We investigate the effect of the ethical interventions   on the gender diversity score Eq . ( 1)for the profes-   sion category in Table 1 ( Column 3 - 5 ) . We observe   that gender - specific ethical intervention causes the   promotion of gender diversity ( Row 2 - 3 ) for all the   models . We also find that the prompt with ‘ irrespec-   tive of their gender ’ improves the gender diversity   score much more than the prompt simply stating   that ‘ all genders can be [ profession ] ’ . Addition-   ally , we observe that an ethical intervention with   respect to skin color does not have significant effect   on the gender diversity of the model generations   ( Row 4 - 5 ) . Even though the irrelevant interventions   should not change the diversity scores , we observe   that diversity scores are affected by their presence   ( Row 6 - 7 ) . We present the gender diversity score   evaluated through CLIP for the object category in   Appendix Table 6 . To ensure the reliability of our   evaluation , we also perform human annotations for   better assessment .   3.2 Human Evaluation Results   We present human evaluation results for the profes-   sion category in Table 1 ( Column 5 - 8) . We observe   that axis - specific ethical instructions with ‘ irrespec-   tive of { gender , skin color } ’ produce better diver-   sity scores ( Row 2 and 4 ) . We also find that the   diversity scores do not improve for most cases as   ethical interventions do when adding irrelevant in-1360   structions . We can draw similar conclusions from   Appendix Table 6 for the objects category .   We also present the human evaluation results   along the cultural axis in Table 2 . We observe   that the generations of all the models become more   diverse when prompted in the presence of cultural   intervention . Additionally , the cultural diversity is   not influenced by the irrelevant instructions .   Till now , we have focused at the effect on the   diversity scores . However , it is only the uniformity   in image generations across groups but does not in-   dicate the direction of the bias . Hence , we also cal-   culate the bias score Eq . ( 2 ) . Our results reveal that   the presence of ethical interventions may flip the   direction of model ’s bias . For instance , DALL · E-   mini generates man and dark - skinned individuals   with makeup ( Appendix Fig . 6 ) . Similarly , Stable   Diffusion generates more woman images than man   images for the police profession when prompted   with the gender ethical intervention .   Further visual inspection of Figure 4 suggests   that the Stable Diffusion model synthesizes mul-   tiple humans in a single image that prevents the   human annotators to assign a particular gender or   skin color to them . Such model generations are   disregarded during diversity score generation , thus   preventing us to make reliable estimate of the sta-   ble diffusion generations through diversity score   alone . We believe that our work motivates further   studies on the sensitivity of text - to - image model   generations to ethical instructions .   3.3 Quality of Image Generation   Do these abstract interventions bring side effect   such as hurting the quality of generations ? We   ask human annotators to select if generated images   are of good qualityconditional on the originalprompt and the ethical intervention . We present   our analysis in Table 3 for the same five subset   of attributes ( police , doctor , makeup , suit , scarf )   for gender and skin color bias study , and three at-   tributes ( bride , groom , wedding ) for cultural bias   study ( § 3.2 ) . Compared to generating with original   prompts , except DALL · E - mini and Stable Diffu-   sion on profession category , the number of good   quality generations reduce slightly for both the   models ( 0 - 1.5 images per attribute ) in the presence   of the ethical interventions . This presents a positive   case towards using ethical interventions for model   diversification while preserving the quality of the   generations .   4 How important are phrases present in   an ethical intervention ?   In § 3 , we observed that ethical interventions would   elicit large changes in the diversity scores in some   cases . However , it is still unclear as to which   phrases in an ethical intervention lead to such   changes in the model ’s behaviour . To this end ,   we perform a preliminary analysis on the model   generations with ‘ a photo of a { person wearing a   makeup / police officer } if all individuals can { wear   a makeup / be a police officer } irrespective of their   gender ’ prompt with DALL · E - mini .   We find that removing ‘ irrespective of their gen-   der ’ phrase from the ethical intervention leads to   generations biased towards ‘ woman ’ and ‘ man ’ for   the ‘ makeup ’ and ‘ police officer ’ attributes respec-   tively . This trend is identical to what we observe   for original prompts without intervention . It shows   that the model may have captured the semantics   of the phrase based on its usage in the pre - training   dataset . Further analyzing the pre - training data   ( Sharma et al . , 2018 ) , we observe ‘ irrespective of ’   phrase is used 37 times to elicit equitable judg-   ment based on the context in the captions ( Table 7).1361   But the entire phrase ‘ irrespective of their gender ’   appears only once .   There is also a possibility that the captions con-   taining word ‘ gender ’ and ‘ makeup ’ are associ-   ated with images with ‘ man ’ person in pre - training   dataset images ( Changpinyo et al . , 2021 ; Sharma   et al . , 2018 ) and thus contribute to generating more   men . However , we find that the six images with   ‘ gender ’ and ‘ makeup ’ words in their captions only   contain people who are perceived as woman by the   humans . We also find that there is only one image ,   without any person clearly visible , with ‘ gender ’   and ‘ police ’ in its caption . Hence , we further verify   the effect of phrase ‘ irrespective of their gender ’   on generating diverse images despite its absence in   pre - training data . Why DALL · E - mini can generate   anti - stereotype images with such ethical interven-   tions needs further exploration in future work .   Additionally , further analysis on the co-   occurrence of the word ‘ culture ’ with ‘ Western ’   ( 75 ) , ‘ Indian ’ ( 394 ) , and ‘ Chinese ’ ( 322 ) explains   the generation of images belonging to these Non-   Western cultures when the original prompts are in-   tervened with ethical interventions containing the‘culture ’ keyword ( Appendix Fig . 7 , 8) .   5 Discussion and Conclusion   We present a framework along with an associated   ENTIGEN dataset to evaluate the change in the   diversity of the text - to - image generations in the   presence of the ethical interventions . We observe   that without any fine - tuning , models can generate   images of diverse groups with prompts containing   ethical interventions . Our preliminary study finds   evidence that a large change in image generation   can be caused by certain keyphrases such as ‘ irre-   spective of gender ’ in the context of the gender bias   and ‘ culture ’ in the context of the cultural bias .   Acknowledgement   We thank annotators for tremendous efforts on an-   notation and evaluation . We also thank the anony-   mous reviewers and ethical reviewers for their great   comments . We also greatly appreciate Ashima   Suvarna , Xiao Liu , and members of UCLA - NLP   group for their helpful comments . This work was   partially supported by NSF IIS-1927554 , Sloan   Research Fellow , Amazon AWS credits , and a   DARPA MCS program under Cooperative Agree-   ment N66001 - 19 - 2 - 4032 . The views and conclu-   sions are those of the authors and should not reflect   the official policy or position of DARPA or the U.S.   Government .   Limitations   We note that even with ethics intervention , text-   to - image models may not always generate diverse   output in a reliable way . Therefore , our goal of   this study is not arguing ethical intervention is an   effective way to reduce bias in practice ; rather our   study analyzes how the current systems respond   to these interventions . As a future work , we aim   to explore deeper reasons behind the diverse and1362anti - stereotype generations beyond the association   between words and images . Our work motivates   further studies for developing more inclusive and   reliable text - to - image systems .   The creation of large number of ethical inter-   ventions and their human evaluations is a current   limitation and an important future direction . Addi-   tionally , we consider binary categorization of the   model generations that has technical as well as eth-   ical limitations . It would be important to study   mechanisms to assign non - binary labels to model   generations and develop diversity metrics beyond   binary groups in the future work .   Our work is also limited by the perceptual bias   of the human annotators from US and UK as well   as the CLIP model . To obtain more reliable eval-   uation results , we plan to involve annotators from   diverse regions in human evaluation and less biased   computer vision models in automatic evaluation .   Ethics Statement   ENTIGEN is proposed for evaluating the change   in the model generations in the presence of ethical   interventions . We limit our work to selected cate-   gories ( such as profession and objects ) within the   gender and social axis even though there might be   other categories such as politics where equal repre-   sentation is desired . Even though there are a wide   range of groups within the gender and skin color   axis , we only consider categorizing individuals into   { man , woman } and { light - skinned , dark - skinned } .   We are aware of the negative impact brought   from limited binary categories . It is offensive for   underrepresented groups and possibly causes cycli-   cal erasure of non - binary gender identities . How-   ever , assessing any individual ’s gender identity or   sex is impossible based on their appearance ; hence   we limit our work on classifying individuals into   man / woman based on the perceptual bias and gen-   der assumptions of the human annotators and the   CLIP model . We also emphasize that our analy-   sis is based on generated images not the images   containing real individuals .   We also understand that there are numerous skin   colors but we limit our study to classify individuals   into light - skinned or dark - skinned . Additionally ,   we do not instruct the annotators to use Fitzpatrick   scale ( Fitzpatrick , 1986 ) to determine skin - color ,   rather the decision is left to their own perception .   The imperfect image - to - text generative model-   ing can run into the hazard of missing certain datamodes that eventually compound the social biases   present in the pre - trained dataset ( Saharia et al . ,   2022 ) . There are harms associated with the models   ability to change predictions drastically based on   the prompts as it can lead to the generation of ob-   jectionable contents . We encourage the practice of   having sophisticated Not Safe For Work ( NSFW )   filters before image generations . A CLIP - based   filter used by Stable Diffusion implementations is   a positive step in this direction .   Extensions of our work can focus on increas-   ing the representation of more groups as well as   designing text - to - image generative models that out-   put images of people belonging to diverse groups   conditional on the neutral prompt .   As we annotate a new dataset ENTIGEN , we   compensate annotators with a fair rate . We recruit   annotators from Amazon MTurk . We provide a   fair compensation rate with $ 10 per hour and spent   around $ 60 in total to the annotators on human eval-   uation . Each HIT costs several seconds according   to the statistics in Amazon MTurk .   References13631364Appendix   A Related Work   Recently , text - to - image generative models such   as DALL · E ( Ramesh et al . , 2021 ) , DALL · E   2 ( Ramesh et al . , 2022 ) , GLIDE ( Nichol et al . ,   2021 ) , IMAGEN ( Saharia et al . , 2022 ) and Stable   Diffusion ( Rombach et al . , 2022 ) have been capa-   ble of generating photorealistic images according   to text prompts . However , Cho et al . ( 2022 ) dis-   cover that these models expose societal bias when   fed with prompts involving professions and objects .   As the scale of models and their training data   greatly expands , with single textual instructions ,   models can rapidly learn how to accomplish the   corresponding tasks with a few or even zero exam-   ples ( Brown et al . , 2020 ) . In the context of fairness   issue , ethical intervention ( Zhao et al . , 2021 ) is   proposed to mitigate bias of predictions made by   large language models . Different from Zhao et al .   ( 2021 ) , we find that ethical interventions can ad-   just model behaviour towards generating images   regarding minority groups , and provide preliminary   study on why the intervention can work .   B Image Generation Details   Each prompt in ENTIGEN is used to generate 9   images from each text - to - image generation model   9 times . In this work , we choose the publicly avail-   able generation models , minDALL · E and DALL · E-   mini for analysis . It is mainly because the two   models can generate high - quality images . Based   on our experiments , the quality of image gen-   erations containing humans from other available   instantiations such as ruDALL · E - XL ( https://   rudalle.ru/ ) can not generate high - quality im-   ages . More powerful models like DALL · E 2 and   IMA - GEN are not publicly released . minDALL · E   and DALL · E also allow us to perform inference   more time efficiently . minDALL · E and DALL · E-   mini can generate a image in 10 seconds on a   RTX1080Ti GPU . But models like Disco Dif-   fusion ( http://discodiffusion.com/ ) took 20   minutes to generate a single image . We use   the publicly available Stable diffusion v1 - 4 from   HuggingFace library ( https://huggingface.co/   CompVis / stable - diffusion - v1 - 4 ) .   C Details of CLIP - based and Human   Evaluation   CLIP - based Evaluation . We adopt CLIP to   score compatibility sim(I , T)between any pair   of image Iand text T. First , we remove images   for which sim(I,‘a photo of a person ’ ) < sim ( I ,   ‘ a photo of an object ’ ) and remain the rest   Dimages which can be well recognized   as person . For the remaining images , we deter-   mine gender of the person in image Iby comparing   sim(I , T ) , sim ( I , T ) , sim ( I , T ) .   T , T , and T are shown in Ta-   ble 4 . T is used to account for the   uncertainty in gender estimation by the CLIP   model .   Human Evaluation . In evaluation survey , we   first ask annotators whether the images can be rec-   ognized as a person and whether the images are   generated as input prompts describe . Once , they   recognize the image as of a person , they get to   decide the gender and skin color of the person in   the image . To account for uncertainty in assigning   a gender and skin color , we allow the annotators   to choose an “ Ca n’t judge ” option . To guarantee   the reliability of the evaluation results , we recruit   three annotators to evaluate each image and the   evaluation results depend on majority views . The   annotation interface is shown in Figure 2 .   D List of Studied Professions , Objects   and Cultural Scenarios   Table 5 lists all the professions , objects and cultural   scenarios we study . The main criteria for attribute   selection is whether an attribute has prominent and   representative visual features or not . In total , there   are 25 professions , 8 objects , and 3 cultural scenar-   ios incorporated in ENTIGEN dataset . The choices   of attributes refers to the studied objects mentioned   in Acharya et al . ( 2020 ) ; Yin et al . ( 2021 ) ; Cho   et al . ( 2022).1365   E Prompts and Ethical Interventions in   ENTIGEN   In ENTIGEN dataset , there are two types of   prompts : original prompts and prompts with ethi-   cal interventions . For each original prompt , such   as ‘ a photo of a [ X ] ’ , there are 4 - 7 variants with   ethical interventions . Among them , there are 1 - 2   irrelevant ethical interventions . The rest interven-   tions correspond to the three bias axes - gender ,   skin color and culture .   F More on Bias Results   We present the formulation of bias along the social   axisgin Eq . ( 2 ) . Bias results based on human   evaluations are shown in Figure 3 . We first observe   that in most cases , adding ethical interventions can   help in reducing the bias because the absolute value   ofbiasbecomes smaller . We further find that   in some cases , for example , outputting a person   with makeup by DALL · E - mini , the bias direction   is flipped oppositely towards person who looks like   a man .   G Case Study   Figure 4 to Figure 8 showcase the generated images   based on different prompt variants . From Figure 8 ,   we observe that original prompts about bride can   only generate brides in Western weddings , but the   generations are diversified with ethical intervention   ‘ from diverse cultures’.13661367136813691370