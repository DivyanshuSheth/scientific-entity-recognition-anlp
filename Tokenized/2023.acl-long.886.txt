  Mingyu Derek MaAlexander K. TaylorWei Wang Nanyun Peng   Computer Science Department   University of California , Los Angeles   { ma , ataylor2 , weiwang , violetpeng}@cs.ucla.edu   Abstract   Event extraction for the clinical domain is an   under - explored research area . The lack of train-   ing data along with the high volume of domain-   specific terminologies with vague entity bound-   aries makes the task especially challenging .   In this paper , we introduce D , a robust   and data - efficient generative model for clinical   event extraction . Dframes event extraction   as a conditional generation problem and intro-   duces a contrastive learning objective to accu-   rately decide the boundaries of biomedical men-   tions . Dalso trains an auxiliary mention   identification task jointly with event extraction   tasks to better identify entity mention bound-   aries , and further introduces special markers to   incorporate identified entity mentions as trigger   and argument candidates for their respective   tasks . To benchmark clinical event extraction ,   we compose M -EE , the first clinical   event extraction dataset with argument anno-   tation , based on an existing clinical informa-   tion extraction dataset , M ( Caufield   et al . , 2019 ) . Our experiments demonstrate   state - of - the - art performances of Dfor clin-   ical and news domain event extraction , espe-   cially under low data settings .   1 Introduction   Event extraction ( EE ) is an information extraction   task that aims to identify event triggers and argu-   ments from unstructured texts ( Ahn , 2006 ) . The   EE task consists of two subtasks : 1 ) event detec-   tion , in which the model extracts trigger text and   predicts the event type ; and 2 ) event argument ex-   traction , in which the model extracts argument text   and predicts the role of each argument given an   event trigger and associated event type .   Clinical EE aims to extract clinical events , which   are occurrences at specific points in time during   a clinical process , such as diagnostic procedures ,   symptoms , etc . The arguments for such events areFigure 1 : Illustration of a S _ event   triggered by “ nodule ” with multiple arguments   including an A argument “ 0.8x1.5 cm ” , and a   D _ event whose predicate is   “ computed tomography ” described by argument “ chest ”   of role B _ .   entities that modify or describe properties of these   events ( Caufield et al . , 2019 ) . Figure 1 shows an   example sentence with two clinical events . The   overwhelming volume and details of clinical in-   formation necessitate clinical EE , which benefits   many downstream tasks such as adverse medical   event detection ( Rochefort et al . , 2015 ) , drug dis-   covery ( Wang et al . , 2009 ) , clinical workflow opti-   mization ( Hsu et al . , 2016 ) , and automated clinical   decision support ( Yadav et al . , 2013 ) .   However , there are several non - trivial challenges   of clinical EE compared to general domain EE .   First , most triggers and arguments of clinical events   consist of domain - specific terms that are more than   50 % longer than the general domain on average ,   as shown in Table 1 , and have vague boundaries   because most clinical mentionscontain several de-   scriptors . For instance , given the text span “ massive   heart attack ” , “ heart attack ” should be identified as   the trigger ( instead of “ massive heart attack ” or “ at-   tack ” ) because it refers to a specific condition , and   “ massive ” is an argument of the role type S - . However , when we consider “ right common   carotid artery ” , the entire text span describes a bio-   logical structure , and thus it functions as an argu-   ment of the role type B _ S   despite “ right ” and “ common ” being descriptors15898for “ carotid artery ” . The second challenge is the   diversity and density of clinical arguments : there   are on average 10 unique argument roles for each   clinical event type compared to 3.7 in the general   domain . Finally , it is challenging to obtain high-   quality annotated data for clinical events due to   both patient privacy concerns and the cost of ex-   pert annotations . Due to these challenges , there   have been no clinical EE datasets with argument   annotations to the best of our knowledge .   In this paper , we present D , aData - eff Icient   generative model for Clinical Event extraction .   We build upon existing prompt - based generative   event extraction models to formulate EE as a   sequence - to - sequence text generation task ( Hsu   et al . , 2022 ; Ma et al . , 2023b ) . To handle the spe-   cial challenges of clinical EE , D1 ) introduces a   mention identification - enhanced EE model , which   specializes in clinical mention identification by per-   forming contrastive learning to distinguish correct   mentions from the ones with perturbed mention   boundary , training an auxiliary mention identifica-   tion module to learn implicit mention properties ,   and adding explicit mention markers to hint men-   tion boundaries ; 2 ) performs independent queries   for each argument role to better handle long - tail   argument roles .   To address the training data availability issue ,   we introduce M -EE , the first clinical   event extraction dataset with argument information ,   which we derive from clinical experts ’ annotation   on PubMed clinical case reports .   We benchmark D onM -EE   against several recent event extraction models . Ex-   periments show that Dachieves state - of - the - art   clinical event extraction results on M -   EE , and we observe a larger performance gain un-   der low - resource settings . Moreover , Dalso   achieves better performance on the ACE05 dataset ,   demonstrating its generalizability to other domains .   Our contributions are threefold : 1 ) We develop   D , a mention - enhanced clinical event extraction   model that better identifies mention boundaries and   is scalable to many argument roles ; 2 ) We con-   struct the first clinical event extraction dataset with   argument annotations ; 3 ) Our model achieves state-   of - the - art performance on clinical and news EE and   demonstrates more significant performance gains   under low - resource settings.2 Related Works   2.1 General Domain Event Extraction   Many prior works formulate EE as token - level   classification tasks and trained in an ED - EAE   pipeline - style ( Wadden et al . , 2019 ; Yang et al . ,   2019 ; Ma et al . , 2021b ) or optimized jointly   ( Li et al . , 2013 ; Yang and Mitchell , 2016 ; Lin   et al . , 2020 ; Nguyen et al . , 2022a ) . Recent work   formulates the EE task as text generation with   transformer - based pre - trained language models   that prompt the generative model to fill in synthetic   ( Paolini et al . , 2021 ; Huang et al . , 2021 ; Lu et al . ,   2021 ; Li et al . , 2021 ) or natural language templates   ( Huang et al . , 2022 ; Hsu et al . , 2022 ; Ma et al . ,   2022 ; Ye et al . , 2022 ) . These generative EE   models are not optimized to handle complicated   domain - specific mentions . To our knowledge ,   there is no existing approach to clinical EE using a   text generation formulation , which we hypothesize   is due to both data unavailability and to the   aforementioned domain challenges .   2.2 Event Extraction in Biomedical Domain   Biomedical EE is a type of biomedical IE tasks   ( Soysal et al . , 2017 ; Fu et al . , 2020 ; Xu et al . ,   2023 ) . Existing approaches to biomedical EE   ( Huang et al . , 2020 ; Trieu et al . , 2020 ; Wadden   et al . , 2019 ; Ramponi et al . , 2020 ; Wang et al . ,   2020 ) typically focus on extracting interactions or   relationships between biological components such   as proteins , genes , drugs , diseases and outcomes re-   lated to these interactions ( Ananiadou et al . , 2010 ) .   The mentions in these biological component in-   teractions are short , distinctive biomedical terms   and do not have rich event type - argument role on-   tologies because of the lack of interaction types   present in the datasets ( Ohta et al . , 2011 ; Kim   et al . , 2011 , 2013 ; Pyysalo et al . , 2011 , 2012 ) . Li   et al . ( 2020 ) develop a clinical event extraction   model , but it only handles single - word events with-   out considering arguments ( Bethard et al . , 2016 ) .   Our work addresses these concerns by introducing   M -EE as well as providing a bench-   mark in a previously under - explored domain .   3 Clinical Domain Event Extraction   3.1 Task Formulation   We follow the framework of prior works that   decomposes the EE task into Event Detection   ( ED ) and Event Argument Extraction ( EAE),15899while introducing our novel Mention Identification   module as an auxiliary task performed alongside   both the ED and EAE modules . ED subtask takes   a sentence ( passage ) as input to extract event   triggers and predict event types . The trigger must   be a sub - sequence of the passage and the event   type must be one of the npre - defined   types . The EAE subtask takes a tuple of ( passage ,   event trigger , event type ) , and extracts ar-   guments from passage and predicts the argument   role . Each event type holds a pool of n   argument roles as defined in the event ontology .   3.2 The M -EE Dataset   Due to high annotation costs and privacy concerns ,   dataset availability is a primary bottleneck for clin-   ical EE . We propose a repurposing of an existing   expert - annotated dataset , M ( Caufield   et al . , 2019),to compose a clinical EE benchmark ,   M -EE .   TheM dataset consists of 200 pairs   of English clinical case reports from PubMed ac-   companying annotation files with partial event an-   notation provided by 6 annotators with prior expe-   rience in biomedical annotations . To our knowl-   edge , this is the only openly accessible collection   of clinical case reports annotated for entities and   relations by human experts . Following existing   sentence - level EE works ( Lin et al . , 2020 ) , we con-   struct an event extraction dataset with full event   structure , M -EE , which contains anno-   tated span information for entities , event triggers ,   event types , event arguments andargument roles   for each sentence . Mentions are defined as mean-   ingful text spans of occurrences and their proper-   ties ( Caufield et al . , 2019 ) . We include all tagged   mentions in M asentities , and further   specify that mentions tagged as events and their   respective types are included as event triggers and   event types .   To infer event arguments and their roles , which   are not provided in M , we consider non-   event entities that hold a M relation with   event triggers as arguments , and we use the as-   signed entity types as argument roles . We infer   arguments via the M relation because its def-   inition of an entity modifying an event matches   well with the argument definition of further char-   acterizing the properties of an event as shown inAppendix B.2 . The entity type in M   defines a type of fine - grained physical or procedure   property , which matches the argument role defini-   tion of being a type of participant or attribute of an   event . We traverse all ( event type , argument   role ) pairs to obtain the argument roles possible   for each event type to create an event ontology , as   shown in Appendix B.3 . The definitions of each   event type and argument role written by clinical   experts are provided .   3.3 Data Statistics   In Table 1 , we show the statistics for   M -EE as well as the comparable   values for two widely - used EE datasets , ACE05   ( Doddington et al . , 2004 ) and ERE - EN ( Song   et al . , 2015 ) . M -EE differs from   general - domain EE datasets because it contains   fewer sentences and the average occurrences of   entities , triggers , and arguments per sentence are   significantly higher . Note that the average length   of the entities in M -EE is significantly   longer . Besides single - span entities , there are also   nested and discontinuous entities used as event   arguments in M -EE . This demonstrates   thatM -EE fills a different niche than   ACE05 and ERE - EN and provides a valuable   benchmark for EE under a clinical setting with high   mention density , and allowing for future work to   adapt clinical case report domain - specific features .   3.4 Human Verification   We conduct a human annotation to examine the   coverage of the induced arguments and the correct-   ness of their roles . Arguments and their roles in   96 % out of 100 randomly sampled events are con-   sidered comprehensive and appropriate by both of   the two annotators with consensus.15900   4 The DEvent Extraction Model   We formulate EE as a conditional generation task ,   so that we can incorporate domain knowledge such   as event type and argument role definitions via   natural language in the input prompt . To tackle the   challenges of clinical EE , we 1 ) further enhance the   EE model ’s specialization in mention identification   by techniques introduced in § 4.2 to handle long   clinical mentions with vague boundaries ; and   2 ) perform an independent query for each event   type / argument role for better long - tail performance   in settings with many event types / argument roles   as introduced in § 4.1 . Figure 2 shows the model   design .   4.1 Seq2seq Components   There are three components : 1 ) Mention Identifi-   cation ( MI ) which identifies the candidate pool of   event triggers or event arguments , 2 ) Event Detec-   tion ( ED ) which extracts event triggers and predicts   event types , and 3 ) Event Argument Extraction   ( EAE ) which extracts arguments and predicts argu-   ment roles . We integrate these components to form   the MI - ED - EAE pipeline ( details in § 4.3 ) . We use   pre - trained text generation model T5 - large ( Raffel   et al . , 2020 ) as the backbone LM . The input is a nat-   ural language sequence consisting of the original   input passage andprompt . We design input - output   formats with shared common elements across dif-   ferent tasks to enable synergistic joint optimization ,   as all three modules aim to generate a sub - sequence   of the input passage .   Mention Identification ( MI ) . To better align the   MI task with the ED and EAE tasks , the MI mod - ule extracts all mentions that are candidate event   triggers or arguments from the input passage . The   input is the passage and the output includes all trig-   ger or argument candidates in the input passage   separated by a special token “ [ ] ” following   the prefix “ Mentions are ” . If there are no men-   tions , a placeholder is generated ( i.e. “ Mentions are   < mention > ” ) . We extract mentions by inputting   the entire passage as well as sentence segments   selected by a sliding window with a size of a few   words , which enables shorter outputs and higher   mention coverage . We enforce the condition that   the order of output mentions match the order of   their appearance in the passage . This consistency   helps the generative model to learn its expected   behavior as well as allows for prior mention pre-   dictions to inform subsequent mention predictions .   We keep the full passages in addition to the sliced   sub - sequence during both training and inference to   ensure the longer dependencies are captured .   Event Detection ( ED ) . The ED module extracts   event triggers from the passage . For a given pas-   sage , we construct nqueries . For each   query , we input the concatenation of passage and   the following prompt segments : event type name   andevent type description . The output of the ED   task is the concatenation of the event trigger texts   predicted for the queried event type separated by a   special token “ [ ] ” , following the prefix “ Event   triggers are ” . When there is no valid trigger for the   queried event type ( which are considered to be neg-   ative samples ) , a special placeholder is generated   ( i.e. “ Event triggers are < trigger > ” ) . The balance   between positive and negative samples is a hyper-15901parameter that may be tuned for a better precision-   recall trade - off . We decode the output sequence   and obtain a list of ( event type , trigger ) pairs .   Event Argument Extraction ( EAE ) . The EAE   module extracts event arguments from queries con-   sisting of the input passage , a given role type , and   a pair consisting of an event trigger and its event   type . We perform nqueries to extract ar-   guments corresponding to each potential argument   role where nis the number of unique ar-   gument roles for a certain event type . The input   sequence contains passage , event type name , and   event type description segments in addition to :   •Trigger markers which are special tokens ( i.e.   “ < trigger > ” and “ < /trigger > ” ) to wrap trigger text   to explicitly provide the trigger position   •Trigger phrase such as “ Event trigger is plaque ”   •Argument role name for the queried argument   role , such as “ Argument role is Severity ”   •Argument role description   The expected output begins with a reiteration   ( Ma et al . , 2023a ) of the querying argument   role ( e.g. “ Severity is ” ) followed by the concate-   nated predicted argument texts or a placeholder   ( “ < argument > ” ) if there are no valid predictions .   4.2 Mention Identification Enhanced EE   We propose techniques to enhance the generative   model ’s ability to accurately identify long mentions   with vague boundaries : 1 ) contrastive learning with   instances of perturbed mention boundaries , 2 ) ex-   plicit boundary hints with markers and 3 ) implicit   joint mention representation learning .   Contrastive learning with mention boundary   perturbation . Understanding the role of mention   descriptors and distinguishing the subtle boundary   difference are not specifically optimized during   pre - training or fine - tuning with the text generation   objective . We propose to create such a task and   train the model specifically to recognize the men-   tion with the correct boundaries from a pool of   mentions with similar but shifted boundaries .   Following the seq2seq formulation introduced in   § 4.1 , we construct Ninput - output sequence pairs   ⟨in , out⟩where the input sequence inconsists   of passage and prompt , and the gold output out   contains the ground - truth mentions , triggers or ar-   guments for MI , ED or EAE respectively . For a   certain input in , we consider the ground - truth out-   putoutas a positive output ( e.g. “ Mentions are ... right common carotid artery ” ) . We create the kneg-   ative instances ( i.e.n, ... ,n ) ofinby perturbing   the left and right boundaries of mentions in outto   add / remove words ( e.g. removing “ right ” , remov-   ing “ artery ” , or adding “ the ” before “ right ” etc . ) .   We create the negative instances by perturbing out-   put sequences without changing the input , and the   contrastive learning objective applies to MI , ED   and EAE . This results in a group of instances for   inincluding both positive and negative instances :   X={⟨out , in⟩,⟨n , in⟩, ... ,⟨n , in⟩ } . Ap-   plying the process , we obtain instance groups for   all input - output pairs X={X, ... ,X } .   We use cross - entropy loss Lto learn to   generate the correct output outgiven input in .   We introduce an InfoNCE loss ( Oord et al . , 2018 )   to learn to identify the positive output ( items in   the numerator ) from a pool of output candidates   with mention boundary perturbations ( items in the   denominator ) ( Ma et al . , 2021a ; Meng et al . , 2021 ;   Shen et al . , 2020 ):   L=1   ∣X∣∑⎡⎢⎢⎢⎢⎢⎢⎣logf(out , in )   ∑f(n , in)⎤⎥⎥⎥⎥⎥⎥⎦   where j∈[0,1,2, ... ,k]andnis the positive out-   putout . We define the function f(s , in)as the   probability of generating a sequence sgiven input   in , which is estimated by multiplying logits for   each token of the output produced by the decoder   under the teacher - forcing paradigm while inis fed   to the encoder . This estimation is normalized by   the output length and produces the output value of   f(s , in ) . We combine the two losses into the final   objective L(Θ)=L+L.   Explicit mention marker . Wrapping key spans   with special token markers provides beneficial hints   to the generative model that improve its understand-   ing of how the components of the sentence are asso-   ciated syntactically . We wrap trigger or argument   mentions for the ED and EAE tasks , respectively ,   to provide a candidate pool for the identification   task . To minimize the impact of error propaga-   tion of the imperfect MI module on downstream   tasks , we consider two conditions : 1 ) the ED / EAE   modules with markers must be robust enough to   handle the compromised precision and incomplete   coverage of the gold mentions and 2 ) the granular-   ity of the candidate pool must not be too coarse   or too fine . To address the first concern , we gen-   erate two versions of the data : one with mention15902markers and one with no markers , and train the   ED / EAE module over the augmented data . This   trains the model to be robust in cases where the MI   module provides imprecise predictions . The sec-   ond concern stems from the too broad a candidate   pool making the markers less informative and too   strict a candidate pool making it difficult for the MI   module to correctly identify mentions . To account   for this issue , we use trigger mentions for the ED   task and argument mentions for the EAE task as   candidate pools as opposed to using words of a   certain part - of - speech or named entities type . The   unique properties of triggers ( describing an entire   process or behavior that can be linked to a specific   time ) and arguments ( concrete details or descriptive   content ) make them more useful as candidate sets .   MI as an implicit auxiliary task . Existing   works include a named - entity recognition task   to provide additional supervision signals for EE   ( Zhao et al . , 2019 ; Zhang et al . , 2019 ; Sun et al . ,   2020 ; Wadden et al . , 2019 ) for other formulations   except for generative models . Since we design all   three extraction tasks ( ED , EAE and MI ) as gener-   ation tasks , and ED and EAE can be considered as   special MI with certain interest focus , identifying   mentions is a synergistic capability contributing   to performing ED and EAE . Thus , we add trigger   MI and argument MI as auxiliary tasks to jointly   optimize with the ED and EAE tasks , respectively .   4.3 Training and Inference   Schedule sampling . To gently bridge the discrep-   ancy between gold and predicted upstream results   ( ED results passed to EAE , trigger / argument MI   results passed to ED / EAE ) , we adopt the scheduled   sampling technique to perform curriculum learning   ( Bengio et al . , 2015 ) . We force the downstream   model to deal with imperfect upstream results grad-   ually by decaying the upstream results from the   gold ones to the predicted ones linearly . We per-   form the decay at the beginning of each epoch .   Training . We first train standalone trigger and ar-   gument MI modules to provide mention candidates .   We then train ED+MI joint model and EAE+MI   joint model with auxiliary trigger and argument   MI modules respectively . We also add markers   around trigger / argument mention candidates . For   efficient training , the model uses downsampled neg-   ative instances ( i.e. instances with mismatched trig-   ger / argument and event type / argument role).Inference . We use the trigger and argument men-   tion markers produced by the standalone trigger   and argument MI modules in the downstream   ED+MI and EAE+MI joint models . The event   triggers and their types predicted by the ED+MI   joint model are provided as input to the EAE+MI   joint model in a pipeline fashion .   5 Experiments in the Clinical Domain   We evaluate DonM -EE and com-   pare it with existing event extraction models .   5.1 Experimental Setup   Data splits . We divide the 200 M -EE   documents according to an 80%/10%/10 % split for   the training , validation , and testing sets , respec-   tively . For low - resource settings , we consider 10 % ,   25 % , 50 % , and 75 % of the number of documents   used to build the training dataset while retaining the   original validation and testing sets for evaluation .   Evaluation metrics . We follow previous EE   works and report precision , recall and F1 scores   for the following four tasks . 1 ) Trigger Identifica-   tion : identified trigger span is correct . 2 ) Trigger   Classification : identified trigger is correct and its   predicted event type is correct . 3 ) Argument Iden-   tification : identified argument span is correct . 4 )   Argument Classification : identified event argument   is correct and its predicted argument role is also   correct .   Variants . We term two variants of our model . We   refer to pipelined ED and EAE modules without   the mention enhancement techniques described in   § 4.2 , with long - tail argument handling and text gen-   eration cross - entropy loss only as Vanilla D ,   and the full model as D.   Baselines . We benchmark the performance of the   recent EE models on M -EE , includ-   ing : Text2Event ( Lu et al . , 2021 ): a sequence-   to - structure model that converts the input passage   to a trie data structure to retrieve event arguments ;   OneIE ( Li et al . , 2013 ): a multi - task EE model   trained with global features;andDEGREE ( Hsu   et al . , 2022 ): a prompt - based generative model   that consists of distinct ED and EAE modules15903   that fill in event type - specific human written tem-   plates . To adapt DEGREE to the new dataset , we   create the ED / EAE templates by concatenating   event type / argument role phrases ( e.g. “ Biologi-   cal_structure is artery ” ) .   5.2 Overall ED and EAE Results   We show the superiority of Din both high-   resource and low - resource settings .   High - resource results . Table 2 shows the results   for high - resource settings . Among the baselines ,   OneIE and Text2Event achieve the best F1 score on   trigger extraction and argument extraction respec-   tively . DEGREE reports low performance on the   argument extraction task due to the challenges of   generating long sequences containing all argument   roles . Doutperforms the baselines on both trig-   ger and argument extraction tasks , with 2.7 points   F1 score improvements for argument classification .   Low - resource results . We show the results of   training in lower - resource settings in Figure 3 and   Appendix C.3 . We observe that Doutperforms   all baselines on all four tasks under all low - resource   settings . The performance gap between Dandthe baselines increases in the lower training data   percentage settings . In the argument classification   task , Doutperforms Text2Event by more than   8 ( 10 % ) and 9 ( 25 % ) points in F1 score .   5.3 Ablation Studies   We show ablation studies about mention - enhancing   techniques and MI module design in this section   and more studies about input prompt segments and   formulation in Appendix C.2.15904Mention - enhancing techniques . We analyze   the effects of the proposed mention - enhancing   techniques in Table 3 . We observe contrastive   learning , auxiliary task , and mention markers con-   tribute improvements of 1.92 , 1.14 and 1.75 in the   F1 score on argument classification , respectively .   We observe that Dimproves over vanilla D   by 5.43 and 3.05 in the F1 score for trigger and ar-   gument classification , respectively . We include an   oracle setting on Line 6 that provides ground - truth   mention markers during inference to illustrate the   influence of the accuracy of the MI module .   MI module design . We compare our MI module   with the representative of sequence tagging model   OneIE , which produces BIO label for each input   token , and state - of - the - art generative named - entity   recognition model Yan et al . ( 2021 ) , which gener-   ates token indexes , on the entity identification task .   We report the performance in Table 4 . The results   show that the sliding window technique signifi-   cantly improves recall ( Line 5 vs 3 ) and contrastive   learning improves overall performance ( Line 5 vs   4 ) . Our MI module outperforms all baselines and   achieves the best F1 score .   5.4 Error Analysis   We analyze the errors propagated through the 4   steps in the pipeline for D using predicted   triggers on the argument classification task which   shows the culmination of the errors propagated   through the pipeline . The results in Figure 4a in-   dicate that the identification sub - tasks , especially   trigger identification , are the performance bottle-   necks .   We further break identification errors into three   types : 1 ) complete miss : the predicted span has no   overlap with the ground - truth span ; 2 ) partial miss :   the predicted span is a subset of the ground - truth   span ; 3 ) hallucination : the predicted span partially   overlaps with the ground - truth span , but also in-   correctly includes additional tokens . As shown   in Figure 4a , the majority of errors produced by   the trigger identification step are complete misses ,   whereas argument identification suffers from both   partial and complete misses . We also observe that   the left boundaries of the trigger and argument   spans are more difficult to identify as 76 % of partial   misses and 69 % of hallucinations correctly identify   the right boundary but miss the left boundary . This   can be explained by that the dominant word of the   entity is typically on the rightmost ( e.g. “ attack ” in   “ heart attack ” ) , whereas the left boundary requires   separating the target entity from its descriptors ( e.g.   “ massive heart attack ” ) .   We further compare the error types between the   vanilla Dand full version of Dwith mention   identification enhancement techniques in Figure 4b .   We observe that Dproduces fewer error cases   across all error types in both trigger and argument   identification steps , which supports our assertion   that our mention identification enhancement tech-   niques improve the identification of mentions with   vague boundaries .   5.5 Qualitative Analysis   To identify challenges for future works , we sum-   marize 4 types of common errors made by D   and show examples in Table 5 . In the first example ,   the MI module of Donly identifies a subse-   quence of the true mention ( e.g. , “ hearing loss ”   vs. “ bilateral sensorineural high - frequency hear-   ing loss ” ) , leading to a partial miss that shows the15905   ED module mistakenly includes incorrect descrip-   tors . In the second example , Dhallucinates   that a D descriptor “ 15 cm ” is part of the   B _ “ segment IVb ” , which   indicates that the EAE module struggles to separate   mention boundaries . In the third example , the first   event “ biopsies ” is missed by both the ED module   and the MI module . However , despite the MI mod-   ule correctly identifying “ ductal carcinoma ” as a   mention , the ED module does not identify it as an   event trigger . In the fourth example , Didenti-   fies “ within normal ranges ” as the L _ for   the two D _ events , which   are not valid L _ for tumor marker tests .   6 Experiments in the General Domain   We evaluate D ’s generalizability by perform-   ing EE on the widely - used news - domain dataset   ACE05 ( Doddington et al . , 2004 ) , which contains   33 event types and 22 distinct argument roles . We   perform both full - shot and low - resource experi-   ments with 10 % of the training data using the same   data pre - processing , data splits and metrics as prior   works ( Wadden et al . , 2019 ; Lin et al . , 2020 ) , and   we compare with the same set of baselines intro-   duced in § 5.1 . Baseline selection criteria and more   results are presented in Appendix C.1 .   We show the result in Table 6 . We observe that   Dachieves a better performance on both low   and high - resource settings for both trigger and ar-   gument classification tasks . We observe that DE-   GREE ’s performance is much closer to our model   than in the clinical domain , which is due to two   factors . First , the benefit of the independent query   design used in Dis diminished because ACE05   has far fewer argument roles that need to be filled   in for each event type ( on average 4.73 ) compared   with in M -EE ( on average 10 ) . Sec-   ond , DEGREE benefits from the implicit argument   role dependencies established in its human - created   event templates for ACE05 , which were unavail-   able for the clinical domain . We also observe that   mentions in the general domain are easier to iden-   tify as our MI module achieves 92 % F1 score for   entity identification on ACE05 , while achieving   77 % F1 score on M -EE . Although the   mentions in the general domain are not as com-   plex as clinical mentions , the performance of D   supports our claim that mention - enhanced event   extraction generalizes to the general domain .   7 Conclusion and Future Work   We present D , a generative event extraction   model designed for the clinical domain . Dis   adapted to tackle long and complicated mentions by   conducting contrastive learning on instances with   mention boundary perturbation , jointly optimizing   EE tasks with the auxiliary mention identification   task as well as the addition of mention boundary   markers . We also introduce M -EE , the   first clinical EE dataset with argument annotation   as a testbench for future clinical EE works . Lastly ,   our evaluation shows that Dachieves state - of-   the - art EE performance in the clinical and news   domains . In the future , we aim to apply transfer   learning from higher - resource domains.15906Acknowledgments   Many thanks to I - Hung Hsu , Derek Xu , Tanmay   Parekh and Masoud Monajatipoor for internal re-   views , to lab members at PLUS lab , ScAi and   UCLA - NLP for suggestions , and to the anony-   mous reviewers for their feedback . This work   was partially supported by NSF 2106859 , 2200274 ,   AFOSR MURI grant # FA9550 - 22 - 1 - 0380 , Defense   Advanced Research Project Agency ( DARPA )   grant # HR00112290103 / HR0011260656 , and a   Cisco Sponsored Research Award .   Limitations   This work presents a repurposing of an existing   dataset , M , and a set of novel tech-   niques for adapting event extraction to the clinical   domain . Among these new techniques is the han-   dling of long - tailed argument roles , in which we   independently query each role type . This presents   an issue with scalability to domains with yet more   complexity , as training the full Dwhile query-   ing both all event types and all argument types   present in M -EE requires considerable   resources during inference .   Ethical Statement   Our experiments and proposed model framework   are intended to encourage exploration in the clinical   information extraction domain while avoiding the   risk of privacy leakage . The data we use in this   work is publicly available and fully de - identified .   Though recent research has found it to be difficult   to reconstruct protected personal information from   such data , there remains some small risk that future   models may be able to do so . We have not altered   the content of data in any that would increase the   likelihood of such an occurrence and are thus not   risking private information leakage .   References15907159081590915910A Potential Questions   What is the difference between the existing gen-   erative EE model DEGREE and D?Com-   pared with DEGREE , our model : 1 ) further en-   hances the EE model ’s specialization in mention   identification by three techniques to learn mention-   related capabilities introduced in § 4.2 to handle   long clinical mentions with vague boundaries ; and   2 ) performs an independent query for each argu-   ment role for better long - tail performance in set-   tings with many argument roles as introduced in   § 4.1 .   Would training and inference efficiency be an is-   sue ? As we perform an independent query for   each event type / argument role in the ED / EAE   model , it is a tradeoff between performance and   running cost . Though during training , we only   sample a subset of negative instances to train the   model for faster convergence . For example , to cre-   ate seq2seq input - output pairs for a certain sentence   for ED , we create 1 positive pair ( i.e. there is an   event in the sentence for the query event type ) and   k(instead of n , where kis much smaller   thann ) negative pairs ( i.e. no event exists   for the query event type ) .   Why use standalone MI modules to produce   mention candidates ? We use standalone trigger   and argument MI modules to create markers for   downstream ED+MI and EAE+MI joint models ,   instead of using the MI module jointly trained in   the ED+MI or EAE+MI models because the stan-   dalone one yields better performance .   B Dataset M -EE Details   B.1 M Annotation   M is annotated according to the Anno-   tation for Case Reports using Open Biomedical   Annotation Terms ( ACROBAT ) defined in ( Cau-   field et al . , 2019 ) . ACROBAT describes events and   entities as meaningful text spans , but differentiates   events as occurrences that may be ordered chrono-   logically and entities as objects that may modify   or describe events . According to the annotation   guideline , entity text spans are limited to the short-   est viable length . Each event and entity is given   a type such that certain events are associated with   certain argument roles . According to ACROBAT ,   Entity text spans are limited to the shortest viable   length . For example , the text span “ mild asthmaattack ” would be annotated by labeling “ asthma   attack ” as an event as that is the shortest span that   conveys the occurrence of the event . “ Mild ” would   be labeled an entity and the annotation would add   a relation indicating that “ mild ” modifies “ asthma   attack ” . M contains 12 relation types ,   but for our purposes we only consider the M - relation that occurs when an entity describes   or characterizes an event .   B.2 Details of Inferring Event Arguments   According to ACE2005 English Events Guidelines   ( AEEG),the arguments of events are defined as   entities and values within the scope of an event and   only the closest entities and values will be selected ,   where a value is defined to be “ a string that fur-   ther characterizes the properties of some Entity or   Event ” . The M relation in the M   dataset connects 2 arguments , and it is defined as   the “ generic relationship in which one entity or   event modifies another entity or event , including   instances where an entity is identified following   an event ” ( Caufield et al . , 2019 ) . The M   relation satisfies the argument definition described   by the AEEG by incorporating within - sentence re-   lationships between an entity that modifies or de-   scribes an event . Thus , given a certain event trigger ,   we consider non - event entities that hold a M   relation with the trigger as arguments of this event .   We take the assigned type of the selected entity   according to M as the role of the argu-   ment . To create an event ontology , which includes   all possible event types and possible argument roles   or each event type , we traverse all ( event type , ar-   gument role ) pairs to obtain the unique argument   roles possible for each event type .   B.3 Event Ontology   We show the full event ontology , including all event   types and their possible argument roles , in Table 12 .   C Additional Experimental Results   C.1 Additional Baselines for General Domain   Event Extraction   Baseline selection criteria . We select published   EE models reporting performance on the ACE05   dataset using ED and EAE training data only with-   out using external resources ( e.g. knowledge graph)15911or additional tasks ( e.g. relation extraction , entity   recognition ) as our baselines for general domain EE   experiments . We use the same data pre - processing ,   data splits and metrics as prior works ( Wadden   et al . , 2019 ; Lin et al . , 2020 ) .   Additional baselines . In addition to the base-   lines we introduced in § 5.1 , we compare with   DyGIE++ ( Wadden et al . , 2019 ) , a span graph-   enhanced classification model for EE ; BERT_QA   ( Du and Cardie , 2020 ) , which formulates EE as   an extractive question answering task with a se-   quence tagging classifier ; TANL ( Paolini et al . ,   2021 ) , which frames EE as a translation task be-   tween augmented natural languages ; BART - Gen   ( Li et al . , 2021 ) , which uses a sequence tagging   model ( Hou et al . , 2020 ) with additional keywords   as input for ED and performs EAE by filling in   event template with a conditional generation model ;   andGTEE - DynPref ( Liu et al . , 2022 ) , which tunes   dynamic prefix for generative EE models .   We do not compare with Nguyen et al . ( 2022b ,   2021 ) ; Zhang and Ji ( 2021 ) because they jointly   learn additional tasks besides ED and EAE ( i.e. en-   tity recognition and relation extraction ) and there   is no codebase provided by the time of this work .   We do not compare with Wang et al . ( 2022 ) since   its performance is worse than DEGREE ( Hsu et al . ,   2022 ) and GTEE - DynPref ( Liu et al . , 2022 ) accord-   ing to Nguyen et al . ( 2022b).Experimental results . Table 7 shows the com-   parison with more baselines .   C.2 Additional Ablation Studies   Input prompt segments . We analyze the impor-   tance of prompt segments in Table 8 . For ED , we   find that event type name is more important . For   EAE , removing either the event type description   ( Line 5 ) or the argument role description ( Line 9 )   leads to the most significant performance decreases .   These results emphasize the benefits of incorporat-   ing the rich semantic information contained in the   names and definitions for both event type and argu-   ment roles .   Extraction vs typing formulation . We formu-   late ED and EAE as conditional text generation   tasks and consider two designs for our input and tar-   get format . The first is the Ddesign in which we   expect the model to extract content given queries   with event type / argument role information . The sec-   ond design formulates a typing task that provides a   query to the generative model for each mention so   that the expected output is the predicted event type   or argument role for the querying mention . This   approach is motivated by the notion that the output   space of the typing formulation is much smaller   than that of the extraction task .   We formulate the ED and EAE tasks as typing   tasks by querying each possible mention . For the   ED task , we first use the standalone mention iden-   tification module introduced in § 4.1 to extract all   possible triggers detected by the MI module , and   then we query the generative model with the fol-   lowing example input and output format:15912The output is constrained to belong to the can-   didate pool of event types or the placeholder event   type “ < Type > ” following the prefix “ Event type is   ” . For the EAE task , we first extract all possible ar-   gument candidates and then query each candidate   with the input sentence containing event trigger ,   event trigger marker , event type name and event   type description :   Similarly , the output is constrained to the candi-   date pool of argument roles possible for the given   event type following the prefix “ Argument role is   ” .   The results in Table 9 show that the typing for-   mulation improves ED performance over extraction   ( though still worse than mention - enhanced D ) ,   but leads to a much worse EAE performance . This   is likely due to the typing task becoming more diffi-   cult as the number of candidate class increases and   complicated typing spaces varied by event types .   C.3 Full Low - Resource Results   We show the full low - resource experimental results   illustrated in Figure 3 in Table 10 .   D Details of Implementation and   Experiments   D.1 Implementation Details   Mention Identification . The sliding window   scans the passage from beginning to end with a   pre - defined window size and step size , which sig-   nificantly boosts the coverage of the predicted men-   tions . During both training and inference , we retain   the original full - length input passage in addition to   the sliding window segments .   Training and evaluation . We select the best   epoch based on the highest F1 score of the most   downstream MI / ED / EAE task on the validation set .   When evaluating correctness , we only accept an ex-   act match between the generated trigger / argument   and the ground - truth trigger / argument as a correct   prediction . We use beam search with 2 beams to   generate the output sequences for all three gener-   ative tasks . The generation stops either when the   “ end_of_sentence ” token is generated or the output   length reaches 30 .   Frameworks . Our entire codebase is imple-   mented in PyTorch . The implementations of the   transformer - based models are extended from the   Huggingfacecodebase ( Wolf et al . , 2020 ) .   D.2 Experiments Details   We report the median result for five runs with differ-   ent random seeds by default . For the low - resources   result shown in Figure 3 , we sample different selec-   tions of training data of corresponding proportion   for each run . All the models in this work are trained   on NVIDIA A6000 GPUs on a Ubuntu 20.04.2 op-   erating system .   D.3 Baseline Reproduction   Mention Identification . For results in Table 4 ,   we use BART - large for Yan et al . because Yan   et al . ( 2021 ) only supports a generative model with   absolute position embedding . OneIE uses BERT-   large as its default and we use T5 - large for our   proposed D - MI module.15913ED and EAE . We use authors ’ codebases to pro-   duce baseline results . OneIE jointly learns ED ,   EAE , and MI tasks and we provide entity infor-   mation to its MI module with event types and role   types stripped to equate its training information   with the training information provided to our model   D. For DEGREE , human - written templates that   organize the argument roles of an event type in a   sentence are required by the model . We construct   these templates using phrases such as “ < Argument   role > is < argument text > ” for all potential argument   roles of an event type as the template .   D.4 Hyperparameters   For the ED module , we define positive instances as   ( , ) pairs where the passage   contains one or more event triggers of this event   type . Negative instances are pairs in which the pas-   sage contains no event triggers of the event type .   We create 10 negative instances for each positive   instance . For the EAE module , we define posi-   tive instance as the ( , , , ) tuple that there   exists an argument text contained in the passage   that meets the query criteria . We create 10 negative   instances for each positive instance . For the MI   module , we use a window size of 10 words , with   a sliding step of 4 words . We retain the original   full sequence in both training and evaluation . We   use an AdamW optimizer with a 1e-5 learning rate   without gradient accumulation . We show the hy-   perparameter search ranges and the final choices in   Table 11.15914   Event Type Role   Sign_symptom Biological_structure , Detailed_description , Severity , Lab_value , Dis-   tance , Shape , Area , Color , Texture , Frequency , V olume , Quanti-   tative_concept , Qualitative_concept , Biological_attribute , Subject ,   Other_entity , History , Mass   Diagnostic_procedure Lab_value , Biological_structure , Detailed_description , Qualita-   tive_concept , Nonbiological_location , Frequency , Distance , Subject ,   Shape , Quantitative_concept , Texture , Severity , Age , Color , Area , V ol-   ume , Administration , Mass   Therapeutic_procedure Detailed_description , Biological_structure , Lab_value , Dosage , Nonbi-   ological_location , Frequency , Distance , Qualitative_concept , Subject ,   Quantitative_concept , Area , Administration , Other_entity   Disease_disorder Detailed_description , Biological_structure , Severity , Lab_value , Quan-   titative_concept , Distance , Nonbiological_location , Shape , V olume ,   Qualitative_concept , Area , Subject , Biological_attribute   Medication Dosage , Administration , Detailed_description , Frequency , Lab_value ,   Nonbiological_location , Quantitative_concept , Biological_structure ,   V olume   Clinical_event Nonbiological_location , Detailed_description , Frequency , Biologi-   cal_structure , Subject , Lab_value , Quantitative_concept , V olume   Lab_value Biological_structure , Detailed_description , Color , Severity , Frequency   Activity Detailed_description , Nonbiological_location , Biological_structure ,   Other_entity , Frequency , Lab_value , Quantitative_concept   Other_event Biological_structure , Quantitative_concept , Nonbiological_location ,   Severity , Detailed_description   Outcome Nonbiological_location , Subject , Detailed_description , Age   Date -   Time -   Duration -15915ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitation section after Conclusion   /squareA2 . Did you discuss any potential risks of your work ?   Ethical statement section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract section . Section 1 : Introduction .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 for data , Section 4 for model   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3 for data , Section 4 for model   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We use a previously published dataset , the anonymization work has been done in previous work   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   We use a previously published dataset , the anonymization work has been done in previous work   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.2 The MACCROBAT - EE Dataset   C / squareDid you run computational experiments ?   Section 5 and 6   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix D15916 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5 and 6 , Appendix D   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix C3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix D   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 3.4   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   The annotation task is simple , we provide a textual description of the task   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Authors are served as annotators directly without additional recruitment   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.15917