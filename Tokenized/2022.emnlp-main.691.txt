  Deepanway Ghosal Navonil Majumder   Rada Mihalcea Soujanya Poria   DeCLaRe Lab , Singapore University of Technology and Design , SingaporeUniversity of Michigan , USA   { deepanway_ghosal@mymail . ,navonil_majumder@,sporia@}sutd.edu.sg   mihalcea@umich.edu   Abstract   We propose a simple refactoring of multi-   choice question answering ( MCQA ) tasks as   a series of binary classifications . The MCQA   task is generally performed by scoring each   ( question , answer ) pair normalized over all the   pairs , and then selecting the answer from the   pair that yield the highest score . For nanswer   choices , this is equivalent to an n - class clas-   sification setup where only one class ( true an-   swer ) is correct . We instead show that clas-   sifying ( question , true answer ) as positive in-   stances and ( question , false answer ) as neg-   ative instances is significantly more effective   across various models and datasets . We show   the efficacy of our proposed approach in differ-   ent tasks – abductive reasoning , commonsense   question answering , science question answer-   ing , and sentence completion . Our DeBERTa   binary classification model reaches the top or   close to the top performance on public leader-   boards for these tasks . The source code of   the proposed approach is available at .   1 Introduction   Starting with the early Text Retrieval Conference   ( TREC ) community - wide evaluations of textual   question answering ( V oorhees et al . , 1999 ) , all the   way to the recent work on multimodal question   answering ( Lei et al . , 2018 ; Tapaswi et al . , 2016 ;   Jang et al . , 2017 ; Castro et al . , 2020 ) and common-   sense question answering ( Sap et al . , 2019 ; Talmor   et al . , 2019 ) , the task has become a staple of the   natural language processing research community .   One of the major challenges encountered in ques-   tion answering is the evaluation , which often re-   quires human input to evaluate the textual answers   thoroughly . Because of this , the alternative that   has been proposed is that of multi - choice question   answering , where the correct answer is provided   together with other incorrect answers . The task   is thus transformed into that of answer classifica-   tion , where a system has to select one answer fromthe choices provided . While there are drawbacks   associated with this evaluation metric , it has been   widely adopted because of its benefit of providing   a clear evaluation methodology .   In this paper , we reformulate the task of   multi - choice question answering as a binary   classification task and show that this re - framing   leads to significant performance improvements   on several datasets . Importantly , this formulation   brings flexibility to the overall question - answering   setup , as it reduces the dependence on the up - front   availability of multiple candidate answers . Using   our method – TEAM ( Two is b Etter th AnMany ) ,   candidate answers can be produced and evaluated   for correctness on the fly , and thus the answer   classification component can be also used in   conjunction with more natural settings that use   open - ended answer generation ( Castro et al . ,   2022 ; Sadhu et al . , 2021 ) .   2 Methodology   Letqbe a question for which multiple answer   choices A={a , . . . , a}are given . Optionally ,   there is some context cwhich could be helpful for   answering the question . The objective is to select   the correct answer afrom the answer set A.   For some of the datasets used in the paper , the   question qis not provided , and the answer is based   only on the context c. For example , SWAG and   HellaSwag are two such datasets where the task   is to choose the best possible ending for sentence   completion , as shown in Table 1 . In this case , the   question qcan be assumed as implicit : What is the   best possible ending for the context ? The sentence   to be completed is considered as the context c.   We discuss how the MCQA task is generally   performed using transformer language models in   § 2.1 . We denote this approach as Score - based   Method orScore method . We then discuss our   proposed Binary Classification - based Method ,   TEAM in § 2.2.101582.1 Score - based Method ( Score )   We use the notation introduced earlier in § 2 .   Given question q , optional context c , and the an-   swer choices A={a , a , . . . , a},ndifferent in-   put sequences are constructed each containing the   concatenation of the question q , context c , and one   possible answer choice a. The sequences are in-   dependently encoded through a pre - trained trans-   former language model such as RoBERTa ( Liu   et al . , 2019 ) or DeBERTa ( He et al . , 2021 ) . A score   sis predicted for each input sequence which is   then normalized with a softmax layer across the n   outputs to obtain score q.   The cross - entropy loss is used to train the en-   coder model . Assuming the answer ais correct ,   the loss can be obtained as follows :   L=−/summationdisplayplog(q ) = −log(q ) ( 1 )   where pare considered as the class labels . The   class pcorresponding to the gold answer ais   valued as 1 , and all other classes are valued as 0 .   The loss is equivalent to the cross - entropy loss in   an - class classification setup . The normalization   of the scores using the softmax layer to obtain a   distribution over the answer choices is also analo-   gous to the probability distribution over the differ-   ent classes in the multi - class classification setup .   The choice providing the highest score is the   predicted answer during inference . The Score   method was used for the SWAG task in BERT ( De-   vlin et al . , 2019 ) , StoryCloze task in GPT ( Radford   et al . , 2018 ) and has been used for all MCQA tasks   in the huggingface transformersframework .   2.2 Classification - based Method ( TEAM )   For our proposed classification - based method , we   first extend the pre - trained language model by   adding a classification head with two nodes . The   values of these two nodes will denote the un-   normalized scores for the negative and positive   classes in our classification setup .   Now , similar to the previous Score method ,   we first construct ndifferent input sequences by   concatenating the question q , the optional con-   textc , and each possible answer choice a. We   then obtain the unnormalized negative and posi-   tive scores sandsfor each sequence by in-   dependently encoding them through the modified   language model . We normalize each pair of scoresthrough a softmax layer to obtain probabilities of   negative and positive classes : qandq , respec-   tively .   We consider the sequence corresponding to the   gold answer aas positive , and all the other se-   quences as negative . Therefore , the loss function   takes the following form :   L=−/summationdisplay(plog(q ) + plog(q ) )   = −log(q)−/summationdisplaylog(q)(2 )   where pandpare considered as the class la-   bels . As ais the gold answer , we use p= 1 ,   p= 0andp= 0,p= 1 , when i̸=k .   Although Eq . ( 2 ) is a suitable loss function for   single correct answer cases , it can be easily ex-   tended for instances or datasets with multiple cor-   rect answers . This can be done by changing the   class labels pandpto positive and negative ap-   propriately for the additional correct answers .   During inference , we choose the answer with   the highest positive class probability as the pre-   dicted answer . We will show later in § 4 that the   TEAM method generally outperforms the Score   method across several datasets for the same choice   of transformer models .   3 Experimental Datasets   We experiment with the following datasets :   Abductive NLI ( Bhagavatula et al . , 2020 ) . Given   two observations oando(considered as con-   textc ) , the goal is to select the more plausible   intermediate event among hypotheses handh .   We use the sequences { o , h , o}and{o , h , o }   as input for both the Score andTEAM method .   Assuming his the gold answer , we classify   { o , h , o}as positive ; { o , h , o}as negative .   CommonsenseQA ( Talmor et al . , 2019 ) or CQA   is a dataset for commonsense QA based on knowl-   edge encoded in ConceptNet ( Speer et al . , 2017 ) .   Given a question , there are five possible choices ,   among which only one is correct . We do not use   any additional knowledge or context for this task .   CommonsenseQA 2.0 ( Talmor et al . , 2021 ) or   CQA2 is a recent challenging QA dataset collected   with a model - in - the - loop approach . The dataset   contains commonsense questions from various   reasoning categories with either yesornoanswer .   QASC ( Khot et al . , 2020 ) or Question Answer-10159   ing via Sentence Composition task requires fact   retrieval from a large corpus and composing them   to answer a multi - choice science question . Each   question qhas eight choices , among which one is   correct . We use the question and choices with-   out any retrieved facts for this task . We evalu-   ate another task setup QASC - IR ( information re-   trieval ) where we use two - step IR retrieved facts   as in Khot et al . ( 2020 ) as additional context c.   SWAG , HellaSwag ( Zellers et al . , 2018 , 2019 )   are two datasets for grounded commonsense in-   ference , where the objective is to find the correct   ending given a partial description of an event . We   consider the partial description as the context c.   The correct ending is to be chosen from a pool of   four possible choices .   Social IQA ( SIQA ) ( Sap et al . , 2019 ) is a dataset   for commonsense reasoning about social interac-   tive situations . Given a question about a social sit-   uation context , the objective is to select the correct   answer from three possible choices .   Physical IQA ( PIQA ) ( Bisk et al . , 2020 ) is de-   signed to investigate physical knowledge of lan-   guage models . The task is to select the correct so-   lution for a goal from two given choices .   CosmosQA ( Huang et al . , 2019 ) is a QA dataset   for commonsense - based reading comprehension .   Given a question about a paragraph ( c ) , the task   is to select the correct answer among four choices .   CICERO v1 , v2 ( Ghosal et al . , 2022 ; Shen et al . ,   2022 ) are datasets for contextual commonsense   reasoning in dialogues . Given the dialogue and a   question about an utterance , the task is to choose   the correct answer among multiple choices . We   modify the original datasets to use them in aMCQA setup . More details are in the appendix .   4 Results   We use the RoBERTa Large ( Liu et al . , 2019 )   and DeBERTa Large ( He et al . , 2021 ) model to   benchmark the Score andTEAM method across   the experimental datasets . We report the accuracy   for the validation set in Table 2 and accuracy of   leaderboard submissions for the test set in Table 3 .   We also report results for other QA systems such   as UnifiedQA ( Khashabi et al . , 2020 ) and UNI-   CORN ( Lourie et al . , 2021 ) for the test set ( wher-   ever available ) in Table 3 .   Our main finding is that the TEAM method im-   proves over the Score method for most of the   datasets except Social IQA , Physical IQA , and CI-   CERO v1 . We observe this result for both the   RoBERTa and DeBERTa models .   Abductive Reasoning : The improvement is con-   sistently large for both validation and test set in the   Abductive NLI ( ANLI ) dataset . The problem of   intermediate hypothesis selection transforms into   a problem of plausible story selection as we use   the sequence { o , h , o}as our input . In this for-   mulation , the TEAM method is significantly better   than the Score method for both RoBERTa and   DeBERTa models .   Science QA : We also observe considerable im-   provements in the QASC dataset without and   with the additional retrieved knowledge . The   RoBERTa- TEAM model is more than 7 % better in   the test set when retrieved knowledge is not used .   The difference in performance is around 3 % and   4.5 % in the validation and test set when the re-   trieved knowledge is used . For DeBERTa , we ob-   serve the most significant improvement in the test   results of the QASC - IR setting , where the TEAM   method is 3.7 % better than the Score method .   Commonsense QA and Sentence Ending Pre-   diction : TheTEAM method is also better than   theScore method for commonsense question-   answering in CommonsenseQA and Common-   senseQA 2.0 across most settings . One notable   instance is the 3 % superior score of the De-   BERTa TEAM in the CommonsenseQA 2.0 vali-   dation set . We observe a similar trend in results   for sentence - ending prediction in SWAG and Hel-   laSwag . The improvement in performance for the   TEAM method is between 0.85 - 1.9 % in the test set .   We also notice improvements in the test set results   for reading comprehension QA in CosmosQA.10160   Dialogue Commonsense Reasoning : We observe   contrasting results in CICERO v1 and v2 . The   Score method outperforms the TEAM method by   around 2 - 3 % in CICERO v1 . However , the TEAM   method is better in CICERO v2 for both RoBERTa   and DeBERTa models . We analyze the results in   more detail in § 5.1 .   Negative Results : TheScore method outper-   forms the TEAM method in Physical IQA ( PIQA )   and CICERO v1 . These two datasets contain an-   swer choices that are lexically close together and   subtly different from each other ( example in Ta-   ble 1 ) . We analyze the results in more detail in   § 5.1 . The Score method is also the better per-   forming method in SIQA , with small improve-   ments over the TEAM method in DeBERTa and   comparatively large improvements in RoBERTa .   We surmise that the Score method is better be-   cause the dataset contains complex social com-   monsense scenarios , for which learning by di-   rectly comparing the options is more effective .   State - of - the - Art Models and Leaderboard Sub-   missions : We also report the results for Uni-   fiedQA and UNICORN 11B models for the test   set in Table 3 . We compare these results against   our best - performing model : DeBERTa Large in   classification setup ( DeBERTa- TEAM ) . DeBERTa-   TEAM maintains parity with UnifiedQA 11B in   QASC - IR , despite being 36 times smaller . UNI-   CORN 11B outperforms DeBERTa- TEAM by a   large margin on SIQA , PIQA , and CosmosQA.It is an expected result as UNICORN is trained   on multiple datasets for commonsense reasoning   starting from the T5 - 11B checkpoint and then fine-   tuned on each target dataset . DeBERTa- TEAM   is , however , considerably better in Abductive NLI   and HellaSwag . DeBERTa- TEAM also reached the   top or close to the top of the leaderboard ( at the   time of submission to the leaderboard ) in Abduc-   tive NLI , SWAG , HellaSwag , and QASC .   5 Analysis   5.1 How Does Similar Answer Choices Affect   Performance ?   We analyze the similarity between the correct and   incorrect choices to understand why the TEAM   method is better than the Score method in most   of the datasets and vice - versa in the others . We   report the lexical similarity with BLEU ( Papineni   et al . , 2002 ) , ROUGE - L ( Lin , 2004 ) , and semantic   similarity with all - mpnet - base - v2 sentence trans-   former ( Reimers and Gurevych , 2019 ) in Table 4 .   We also report the difference in performance be-   tween TEAM andScore models for RoBERTa   and DeBERTa in the ∆columns .   The similarity measurements in Table 4 indicate   that the datasets can be clearly segregated into two   groups – one with low to medium similarity , and   the other with very high similarity . Interestingly ,   the∆values are mostly positive for the low to   medium similarity group , and all negatives for the   high similarity group . We surmise that the differ-   ence between the very similar correct and incor-10161   rect choices are better captured through the soft-   max activation over the answers in the Score   method . However , this aspect is not captured in   theTEAM method , as sequences corresponding to   the correct and incorrect choices are separately   classified as positive or negative . Thus , the Score   method is more effective when the answer choices   are very similar , as in PIQA or CICERO v1 .   5.2 How Accurate is the Binary Classifier ?   We evaluate how often input sequences corre-   sponding to correct and incorrect answers are pre-   dicted accurately with DeBERTA- TEAM binary   classification model in Table 5 . The binary classi-   fier model is more likely to predict all answers as   negative than all answers as positive , as it learns   from more negative choices in most datasets . In-   terestingly , however , the model predicts all posi-   tive answers for 25.63 % instances in PIQA , which   is significantly higher than all the other datasets .   This is one of the sources of error in PIQA , as the   model often predicts both choices as positive , but   assigns a higher positive probability to the incor-   rect choice . We also report the % of instances for   which the correct answer is predicted as positive   and all incorrect answers are predicted as negative   in the Accurate column . The accuracy is high-   est in HellaSWAG and lowest in QASC , which co-   relates well with the highest performance in Hel-   laSWAG and second lowest performance in QASC   across the datasets in Table 2 and Table 3 .   5.3 Error Analysis   We show some examples of incorrect predictions   for the DeBERTa- TEAM model in the Common-   senseQA and PIQA dataset in Table 6 . The er-   roneously predicted answers in CommonsenseQA   are often very close in meaning to the correct an-   swers . Furthermore , the incorrectly predicted an-   swer could also be argued as correct for some in-   stances ( second example in Table 6 ) , as the incor-   rect choice is also equally plausible . In PIQA how-   ever , the model make mistakes where complex sci-   entific and physical world knowledge is required .   The incorporation of external knowledge is likely   necessary to answer these questions accurately .   6 Conclusion   In this paper , we introduced a simple binary   classification method as an alternative way to ad-   dress multi - choice question answering ( MCQA )   tasks . Through evaluations on ten different   MCQA benchmarks , we showed that this simple   method generally exceeds the performance of   the score - based method traditionally used in the   past . We believe this approach can also be used   in the more natural open - ended answer generation   setups , thus providing a “ bridge ” between the   MCQA and answer generation frameworks for   question answering.101627 Limitations   Although the method we introduced is more flex-   ible than the answer scoring approach typically   used for MCQA , it still lacks the full flexibility of   open - ended question answering and assumes the   availability of a candidate answer that it can clas-   sify as correct or incorrect .   Additionally , even if our approach outperforms   the score - based methods for most of the bench-   marks we considered , there are still some datasets   ( e.g. , SIQA , PIQA , CICERO v1 ) , where the score-   based method performs best . We leave it for fu-   ture work to identify a principled approach for se-   lecting the best methodology to use for a given   dataset .   Acknowledgement   This research / project is supported by the National   Research Foundation , Singapore , and the Min-   istry of National Development , Singapore under   its Cities of Tomorrow R&D Programme ( CoT   Award COT - V2 - 2020 - 1 ) . Any opinions , find-   ings , and conclusions , or recommendations ex-   pressed in this material are those of the author(s )   and do not reflect the views of the National Re-   search Foundation , Singapore , and the Ministry   of National Development , Singapore . This re-   search is also supported by A*STAR under its RIE   2020 AME programmatic grant RGAST2003 and   the Ministry of Education , Singapore , under its   AcRF Tier-2 grant ( Project no . T2MOE2008 , and   Grantor reference no . MOET2EP20220 - 0017 ) .   Any opinions , findings , conclusions , or recom-   mendations expressed in this material are those of   the author(s ) and do not reflect the views of the   Ministry of Education , Singapore .   References1016310164A Experimental Details   We train all the score - based and classification-   based models with the AdamW ( Loshchilov and   Hutter , 2018 ) optimizer with a learning rate of 1e-   6 , 3e-6 , 5e-6 , 1e-5 , 3e-5 . We train all the models   for 8 epochs . The best models are chosen based on   the results on the validation set . The RoBERTa-   Large and DeBERTa - Large models have 355 M   and 304 M parameters , respectively .   B Computational Resources   We use a single Quadro RTX 8000 GPU for our   experiments . Training takes between 30 minutes   to 8 hours for the different datasets used in the pa-   per .   C Dataset Details   All datasets used in this paper are in English lan-   guage . The datasets are available in the cor-   responding leaderboard websitesor through the   huggingface datasets hub .   The number of MCQA instances in the training ,   validation and test set of the various datasets are   shown in Table 7 . Some example instances from   the datasets are shown in Table 8 .   D Modifications in CICERO   CICERO v1 and v2 both contain instances with ei-   ther one or more than one correct answer choices .   We make the following modifications in the origi-   nal datasets to use them in our MCQA setup here ,   as we assume only one answer is correct for a   given MCQA instance : v1 : We only consider instances which has one an-   notated correct answer . Each instance in CICERO   v1 has five possible answer choices . Thus , the   instances selected for our experiments in all the   three sets ( training , validation , and test split ) has   one correct answer and four incorrect answers .   v2 : All instances in CICERO v2 has at - least two   correct answers . We consider instances with at-   least one incorrect answer and create the MCQA   dataset as follows :   • If the original CICERO v2 instance has ncor-   rect answers , then we will create nMCQA   instances from it , each having one of the cor-   rect answers and three incorrect answers .   • The three incorrect answers will be chosen   from the incorrect answers of the original in-   stance . We perform oversampling ( some in-   correct answers repeated ) to create three in-   correct answers if there are less than three in-   correct answers in the original instance .   For example , an instance in CICERO v2 has an-   swer choices : { c , c , i , i } . The correct answers   are{c , c}and the incorrect answers are { i , i } .   We create two MCQA instances from the original   instance – i ) with answer choices { c , i , i , i } ,   and ii ) with answer choices { c , i , i , i}.1016510166