  Zixian Huang and Ao Wu and Jiaying Zhou   State Key Laboratory for Novel Software Technology , Nanjing University , Nanjing , China   { zixianhuang,awu,jyzhou}@smail.nju.edu.cn   Yu Gu   The Ohio State University , Columbus , USA   gu.826@osu.edu   Yue Zhao and Gong Cheng   State Key Laboratory for Novel Software Technology , Nanjing University , Nanjing , China   yuezhao@smail.nju.edu.cn , gcheng@nju.edu.cn   Abstract   A trending paradigm for multiple - choice ques-   tion answering ( MCQA ) is using a text - to - text   framework . By unifying data in different tasks   into a single text - to - text format , it trains a gen-   erative encoder - decoder model which is both   powerful and universal . However , a side ef-   fect of twisting a generation target to fit the   classification nature of MCQA is the under-   utilization of the decoder and the knowledge   that can be decoded . To exploit the genera-   tion capability and underlying knowledge of a   pre - trained encoder - decoder model , in this pa-   per , we propose a generation - enhanced MCQA   model named GenMC . It generates a clue from   the question and then leverages the clue to en-   hance a reader for MCQA . It outperforms text-   to - text models on multiple MCQA datasets .   1 Introduction   Multiple - choice question answering ( MCQA ) aims   at selecting the correct answer from a set of options   given a question . This long - standing challenge in   natural language processing ( NLP ) requires ma-   chines to have a wealth of knowledge , such as   commonsense knowledge ( Talmor et al . , 2019 ; Mi-   haylov et al . , 2018 ) and scientific knowledge ( Clark   et al . , 2018 ; Khot et al . , 2020 ; Huang et al . , 2019 ;   Li et al . , 2021 ) , and have reasoning skills such as   multi - hop reasoning ( Khot et al . , 2019 ) and logical   reasoning ( Yu et al . , 2020 ; Liu et al . , 2020b ; Li   et al . , 2022 ) .   MCQA has made great progress with the devel-   opment of pre - trained language models ( PLMs ) .   Basically there are two types of PLMs that are suit-   able for different tasks . BERT ( Devlin et al . , 2019)and its variants such as RoBERTa ( Liu et al . , 2019 )   and ALBERT ( Lan et al . , 2020 ) are encoder - only   models , being more suitable for natural language   understanding ( NLU ) tasks including MCQA and   other classification and regression tasks . T5 ( Raffel   et al . , 2020 ) and BART ( Lewis et al . , 2020 ) are   encoder - decoder models , being more suitable for   natural language generation ( NLG ) tasks . How-   ever , encoder - decoder models can also be applied   to MCQA ( Khashabi et al . , 2020 ; Zhou et al . ,   2021 ) . This is enabled by the text - to - text frame-   work , which transforms data in different tasks into a   unified text - to - text format so that knowledge span-   ning many and various tasks can be learned , aggre-   gated , and used by a single model .   Research Question To fit MCQA , existing im-   plementations of the text - to - text framework take   all the options as input and are trained to gener-   ate one of the options , i.e. , to copy some tokens   from the input . However , this is inconsistent with   how encoder - decoder models are pre - trained so that   their underlying knowledge may not be sufficiently   exploited . Indeed , Liu et al . ( 2021 ) have found that   in classification and regression tasks , the decoder   layer is often under - utilized . One research question   ishow to apply pre - trained encoder - decoder mod-   els in a more natural way to MCQA , in particular ,   to exploit their NLG capabilities .   Our Contribution Our idea is inspired by hu-   man behavior . When reading a question , humans   are sometimes triggered to associate the question   with their background knowledge to form some   clues even before reading the options . For simple   questions , a clue may be exactly the correct answer,3272   while for complex questions , clues may play an   auxiliary role to help humans connect the question   with the correct answer . For example , for the ques-   tion shown in Figure 1 , the clue “ paper ” forms an   intermediate concept between “ notebook ” in the   question and “ tree ” in the correct answer .   With this idea , we propose to employ a pre-   trained encoder - decoder model to generate a clue   from the question by exploiting its underlying   knowledge , without seeing and being strictly con-   fined to the options as in the text - to - text framework .   The clue representation is then leveraged by an   encoder - based model to read the options and make   prediction . We refer to this generation - enhanced   MCQA model as GenMC . It significantly outper-   forms comparable models , in particular , text - to - text   models , on five MCQA datasets .   Outline We discuss related work in Section 2 ,   introduce GenMC in Section 3 , describe the ex-   perimental setup in Section 4 , report the results in   Section 5 , and conclude in Section 6 .   Code Our code is available on GitHubunder the   Apache Licence 2.0 .   2 Related Work   2.1 Text - to - Text Paradigm for MCQA   Recently , the text - to - text paradigm has achieved   breakthrough results on many NLP tasks ( Raffel   et al . , 2020 ; Lewis et al . , 2020 ) . As illustrated in   Figure 2a , adopting this paradigm for MCQA , the   question Qand all the options { O , O , O , O }   are spliced into a text as input , and the correct   answer Ois used as the generation target . One   benefit is that extensive training data can be shared   across different tasks . Using such a framework ,   UnifiedQA ( Khashabi et al . , 2020 ) integrates 20   QA datasets into a unified format for training , and   achieves state - of - the - art results on multiple MCQA   datasets . Similarly , CALM ( Zhou et al . , 2021 )   learns concept - centric knowledge from text for   commonsense QA .   However , it might be debatable whether it is   appropriate to train a classification task via a gen-   eration target . Liu et al . ( 2021 ) point out that the   decoder layers of T5 are under - utilized when fine-   tuning on classification and regression tasks . There-   fore , they propose a method to reduce the number   of T5 parameters to improve efficiency without   reducing accuracy . By contrast , we address this   issue from a different perspective of how to exploit   the NLG capability of pre - trained encoder - decoder   models for MCQA to improve accuracy .   Some other works propose new pre - trained mod-   els for unified generation and classification tasks   by designing universal encoders and task - specific   decoders ( Shao et al . , 2021 ; Sun et al . , 2021 ) . They   are orthogonal to our work as we leverage exist-   ing pre - trained encoder - decoder models instead of   pre - training new models at an additional cost .   2.2 Encoder - Only Paradigm for MCQA   Benefiting from the powerful NLU capabilities of   BERT - style PLMs ( Devlin et al . , 2019 ; Liu et al . ,   2019 ; Lan et al . , 2020 ) , the encoder - only paradigm   has been popular for MCQA . As illustrated in Fig-   ure 2b , in this paradigm , the question Qand each   option in { O , O , O , O}are interacted to calcu-   late a score , and the option with the highest score is   chosen as the answer . Building on this , some works   study how to design better attention - based models   to identify evidence ( Chen et al . , 2019 ; Zhang et al . ,   2020 ; Zhu et al . , 2020 ) . Other efforts mimic human   behavior of reading evidence and answering ques-   tions ( Ran et al . , 2019 ; Tang et al . , 2019 ; Sun et al . ,   2019 ) . There , evidence is derived from the given   passage or retrieved from external corpora . By con-   trast , we aim at exporting clues from pre - trained   models without resorting to extra sources.3273   2.3 Knowledge in PLMs   Recently , PLMs have been used as knowledge   bases ( Petroni et al . , 2019 ) , and the knowledge   in parameters can be exported via methods such as   Prompt ( Jiang et al . , 2020 ; Shin et al . , 2020 ) . Ex-   ploiting the knowledge in PLMs for QA tasks has   come into play in many forms including question   expansion ( Mao et al . , 2021 ) and question genera-   tion ( Shwartz et al . , 2020 ) .   There is also research on MCQA trying to ex-   porting knowledge from PLMs before answering .   Rajani et al . ( 2019 ) propose CAGE as a framework   for generating explanations for commonsense QA .   However , CAGE relies on explanations annotated   by humans , which are not available in many real   scenarios and datasets . Latcinnik and Berant ( 2020 )   propose a joint generator - classifier model where   the generator produces a human - readable textual   hypothesis . Although it somewhat improves the   explainability of MCQA , in terms of accuracy of   MCQA there is little advancement . CEGI ( Liu   et al . , 2020c ) is probably the most similar work to   ours . It first uses a generative model to generate   evidence , and then uses a reading model to incor-   porate the evidence and predict the answer , both   using answer supervision . However , the generative   model and the reading model are separate steps   in a pipeline and are connected only via the evi-   dence text . Such token - level interaction can lead   to significant losses in accuracy as we will see in   our experiments , where our representation - level   interaction exhibits better performance.3 GenMC Model   In MCQA , a question Qis given together with   a set of noptions O={O , . . . , O}with ex-   actly one option being the correct answer . The   key to finding the correct answer is to capture and   deeply understand the connection between Qand   eachO∈O , which oftentimes is beyond the lexi-   cal level and requires a non - trivial entailment pro-   cess . We follow the trend of building on a pre-   trained encoder - decoder model and use the encoder   to jointly encode Qand each O. However , previ-   ous works directly use the decoder to generate an   option in O , i.e. , using the decoder as a classifier ,   which may have under - exploited the model ’s NLG   capability ( Liu et al . , 2021 ) . Moreover , a simple   joint encoding of Qand each Ocan only enable   lexical - level reasoning ( Zellers et al . , 2019 ) which   is insufficient for MCQA tasks .   Our proposed model GenMC overcomes these   limitations . Building on a pre - trained encoder-   decoder model , GenMC firstly generates a clue   which is indicative of the correct answer , thereby   exploiting the NLG capability and underlying   knowledge of the pre - trained encoder - decoder   model . Then GenMC employs the generated clue   representation as intermediate knowledge connect-   ing the question and the correct answer to interact   with and enhance a reader for solving MCQA . Our   model design mimics how humans solve an MCQA   task , i.e. , after reading a question , humans may   firstly associate it with some of their background   knowledge ( i.e. , looking for clues ) that helps them   to later identify the correct answer .   The overall architecture of GenMC is shown in3274Figure 3 . The clue generator ( Section 3.1 ) first gen-   erates a clue representation only given Q. Then the   enhanced reader ( Section 3.2 ) uses the generated   clue to augment question - option understanding .   3.1 Clue Generator   The clue generator takes the question Qas in-   put and autoregressively outputs a clue C=   c , . . . , cusing a pre - trained encoder - decoder   model . Note that not the clue text Cbut its repre-   sentation Hwill be used in our model , although   one could output Cas evidence for explainability .   Specifically , we obtain the question represen-   tation H∈Rand the clue representation   H∈Rfrom the last layer of the encoder   and of the decoder , respectively , where ddenotes   the representation dimension . H , denoting the   representation of the j - th token c∈C , is com-   puted as follows :   where Decoder ( · , · , · ) takes the last token c ,   the representation for the decoding history H ,   andHas input , and outputs the hidden state H   together with the probability distribution pover   the decoding vocabulary at the j - th step .   To encourage the tokens in Cto thoroughly inter-   act with each other and with Q , we strengthen the   clue representation by passing it to a transformer   layer ( Vaswani et al . , 2017 ) and obtain H :   where [ · ; · ] denotes concatenation . Hcarries the   information of Cwhich can be helpful to better   understand and answer Q.   3.2 Enhanced Reader   Previous works often directly model the relevance   of each O∈OtoQvia joint encoding using a pre-   trained encoder , which largely performs superficial   lexical reasoning ( Zellers et al . , 2019 ) . By contrast ,   we use the previously generated clue representation   to enhance our reader for a deeper understanding   of each question - option pair .   Specifically , we first concatenate Qand each O   independentlyand feed the concatenated input   into the pre - trained encoder ( which is shared with   our clue generator ) to obtain O ’s contextualizedrepresentation H , which constitutes a column   ofH∈Rwhere n=|O| .   Next , based on the clue representation H , our   model intensively reads each question - option pair   and obtains the matching signal between the clue   and the option . Specifically , inspired by Huang   et al . ( 2021 ) , we first use dual - attention ( Liu et al . ,   2020a ) to fuse information from HtoHand   fromHtoH. Then we perform max - pooling   to aggregate the matching features:(3 )   To obtain the final score sfor each O , we con-   catenate the dual matching features fandf   and feed them into a two - layer multi - layer percep-   tron ( MLP ):   We select the option with the highest score as the   predicted answer , denoted as O.   3.3 Training Objective   We jointly train the clue generator and the enhanced   reader in an end - to - end fashion with a combined   loss :   Generator Loss ForL , assuming that O∈   Ois the correct answer containing mtokens   a , . . . , a , we first use Oas the target to cal-   culate our clue generator loss with teacher forcing:(6 )   where pdenotes the probability distribution   over the decoding vocabulary at the j - th step , and   pis the probability of token a.   Reader Loss ForL , we simply calculate a   cross - entropy loss given the correct answer O∈O   as follows :   Note that we update the encoder using the joint loss   L , while we do not allow Lto be backprop-   agated to the decoder part to reduce the memory   consumption.3275   The above training objective exploits the dou-   ble properties of the correct answer Oin MCQA :   as a text and as an index . We use Oas a text   to supervise our clue generator , and as an index   ( i.e. , classification label ) to supervise our enhanced   reader . Such usage is more natural than the text - to-   text paradigm ( Khashabi et al . , 2020 ; Zhou et al . ,   2021 ) , thus having the potential to outperform .   4 Experimental Setup   4.1 Data   We conducted experiments on five popular MCQA   datasets spanning from commonsense questions   to scientific questions . The former requires com-   monsense knowledge and reasoning , and the latter   requires inference over scientific facts .   Datasets CSQA ( Talmor et al . , 2019 ) and   OBQA ( Mihaylov et al . , 2018 ) are two common-   sense MCQA datasets created by crowd workers   based on commonsense facts . Each question is   given with 5 options in CSQA and 4 options in   OBQA . ARC - Easy and ARC - Challenge , denoting   two disjointed subsets of ARC ( Clark et al . , 2018 ) ,   contain natural grade - school science questions with   4 options , where ARC - Challenge comprises diffi-   cult questions which require more advanced rea-   soning . QASC ( Khot et al . , 2020 ) is collected from   elementary and middle school level science with 8   options for each question .   Train - Dev - Test Split For OBQA , ARC - Easy ,   and ARC - Challenge we used their official train ,   dev , and test sets . For CSQA and QASC , since   the correct answers in the official test set are not   public , we took their official dev set as our test set   for experiments and randomly held out an in - house   dev set from the training set . The dataset statistics   are shown in Table 1 .   External Knowledge For all these datasets , our   experiments did not rely on any provided docu-   ments or external corpora ; a question was solelyprovided with its options to form the input . It   means that pre - trained models were used as the   primary source of knowledge in the experiments .   4.2 Implementation Details   We used two popular encoder - decoder models as   a basis , BART ( Lewis et al . , 2020 ) and T5 ( Raffel   et al . , 2020 ) . For each model , we experimented   with its B and L versions .   We used PyTorch 1.7 . We used the Adam   optimizer and set warmup fraction = 0 .1 ,   weight decay = 0.01,maximum source length =   64,maximum target length = 32 , epoch = 30 ,   and early stop training when there was no bet-   ter result on the dev set after 5 epochs . For each   model , we searched for the best learning rate from   { 1e−4,5e−5,1e−5 } , and for the best batch size   out of{8,64 } .   Because neural models are known to be sensi-   tive to different random seeds , especially when the   training set is small , we performed multiple experi-   ments for all models with different random seeds ,   and reported the mean and standard deviation . For   CSQA , OBQA , ARC - Easy , and QASC , we used   three random seeds { 1,10,20 } . For the smallest   dataset ARC - Challenge , we used five random seeds   { 1,10,20,30,40 } .   All the experiments were performed on a   GeForce RTX 3090 with 24 G memory .   4.3 Evaluation Metric   For each model , we reported its proportion of cor-   rectly answered questions in each dataset .   5 Experimental Results   5.1 Main Results : Comparison with   Text - to - Text Models   To empirically evaluate GenMC in terms of   whether it better exploits the potential of pre-   trained encoder - decoder models for MCQA , we   compare GenMC with a standard text - to - text imple-   mentation and with a variant thereof for analysis.3276   5.1.1 Baselines   Text2Text The vanilla usage of pre - trained   encoder - decoders for MCQA is to reform the input   and output in a way that can be directly processed   by a encoder - decoder model . Specifically , follow-   ing Raffel et al . ( 2020 ) , we concatenate the input   question with all candidate options , where each   option is also preceded by its option ID , and then   prepend the sequence with a dataset name . The   concatenated sequence is fed into the encoder part   to get a joint representation for the question and   all options . Based on the joint representation , the   decoder finally outputs an option ID . In this setting ,   the decoder is basically used as a classifier .   Text2Text Similar to Liu et al . ( 2021 ) , we use   only the encoder part of a pre - trained encoder-   decoder model . Each option is independently   paired with the question to obtain a joint represen-   tation using the encoder . Then the representation is   fed into a scorer ( i.e. , an MLP ) to obtain a matching   score for each question - option pair . The model then   predicts the option with the highest score . In this   setting , the decoder is totally unused . Though Liu   et al . ( 2021 ) find that their encoder - only model per-   forms comparably to using the decoder as a clas-   sifier , we argue that the decoder part can further   improve the performance , if being properly used .   5.1.2 Results   The main results ( see Table 2 ) show that GenMC   consistently and significantly ( with p - value < 0.01 )   outperforms Text2Text and Text2Texton   all datasets . For several settings , GenMC even   obtains an absolute gain of over 10 % . For exam-   ple , on the test set of the challenging scientific   MCQA dataset ARC - Challenge , T5 + GenMC   improves T5 + Text2Text from an accu - racy of 23.69 % to 39.00 % , suggesting a relative   gain of around 65 % . These results demonstrate that   GenMC is a more effective usage of pre - trained   encoder - decoder models than existing ones .   Moreover , we interestingly find that the   decoder - free baseline Text2Textoutperforms   Text2Text on over half of the experiments .   This indicates that the decoder ’s general language   knowledge gained from pre - training is largely   wasted by only using it as a classifier , which may   further explain the superior performance of our   model because GenMC can exploit the pre - trained   decoder more effectively . In addition , all L   models significantly outperform their B coun-   terparts . This suggests that the embedded knowl-   edge gained from pre - training is critical to MCQA   tasks , strengthening our point to make full use of   pre - trained encoders and decoders .   5.2 Comparison with Other Models   5.2.1 Baselines   UnifiedQA Existing methods that rely on exter-   nal documents or corpora have achieved state - of-   the - art performance on several MCQA datasets .   However , to enable a fair comparison , we only   compare with models that adopt the same setting   as ours , where a question and its options are the   only input to the model . Among these models , Uni-   fiedQA ( Khashabi et al . , 2020 ) is the current best   model . While UnifiedQA reports the best score us-   ing its T5 - 11B version , since for T5 we experiment   with its BandL versions , we only report   and compare under T5and T5 . Note that   instead of training on each dataset separately , Uni-   fiedQA converts a line of popular QA datasets with   four formats ( e.g. , retrieval - based QA , MCQA )   into a unified format , and trains a single model   over all training data , while GenMC only uses each3277   dataset ’s own training data .   RoBERTa and ALBERT In addition , we com-   pare with two encoder - only models , RoBERTa ( Liu   et al . , 2019 ) and ALBERT ( Lan et al . , 2020 ) , which   have served as the basis of many MCQA models .   All models are of comparable model size to ours .   5.2.2 Results   The results in Table 3 show that GenMCsig-   nificantly ( with p - value < 0.01 ) outperforms the   two encoder - only strong baselines RoBERTa and   ALBERT . More interestingly , GenMCalso per-   forms better than UnifiedQAon most datasets .   Moreover , for UnifiedQA , which further fine-   tunes the model on the training set of the target   dataset , GenMCoutperforms it on the test sets   of CSQA , OBQA , and ARC - Easy for the base   models and ARC - Easy for the large models . It   also achieves comparable results on the remain-   ing datasets . These results are impressive because   UnifiedQA uses more datasets ( i.e. , eight different   QA datasets ) for training . The promising results   of GenMC further reveals that our model can learn   to effectively extract knowledge from pre - trained   encoder - decoders with limited training data .   As a fairer comparison in Table 4 , by unify-   ing the training sets of all the five datasets , our   GenMC outperforms UnifiedQA on all   datasets except for CSQA with large models.5.3 Ablation Study : Influence of Clues   Our main results in Section 5.1 have demonstrated   the effectiveness of our model . To better under-   stand its superior results and the influence of our   clue generation , we compare with two variants .   5.3.1 Variants of GenMC   Weak Clue We train this variant only using the   classification loss L , so only the encoder part   is updated , while the decoder part is left untouched   from pre - training . Intuitively , under this setting ,   the generated clue is weaker than GenMC which   learns how to generate a clue with supervision .   Token Clue In this setting , we separately train   a clue generator and a reader . We first collect the   generated clue text C(instead of its representation )   from the decoder . We then directly concatenate C   withQandOto compute a score for Ousing   the model ’s encoder part stacked with an MLP   layer . This variant is indeed very similar to Liu et al .   ( 2020c ) , which also adopts a pipeline framework to   first generate a token - level evidence and then use   the evidence to expand the question .   5.3.2 Results   Table 5 shows that masking out generation loss   leads to substantial performance drops across all   datasets , demonstrating that fine - tuning the decoder   with generation loss Lhelps to derive useful   clues from pre - trained encoder - decoder models .   We also observe that the performance of using   token - level clues lags much behind GenMC . This3278   demonstrates that naively using explicit knowledge   in plain text , instead of using implicit clues from   the decoder ’s hidden state , is inferior as it may   unnecessarily bring information loss and noise .   5.4 Error Analysis   We analyze the clues generated by GenMC using   T5 with a focus on instances that are correctly   predicted by the baseline in our main experiments   ( i.e. , T5 + Text2Text ) , while our GenMC   fails . The intuition is that in these negative cases ,   the clues generated by GenMC may play a negative   role . By studying these potentially negative clues ,   we can gain more insights into how GenMC fails   and discuss venues for future improvement .   Specifically , we randomly sample 50 negative   cases from T5 + GenMC for each dataset . We   show six graduate students of computer science   an instance along with the generated clue , correct   answer , and predicted answer . We then ask them to   categorize clues into the following families :   •Irrelevant : The clue is off topic or is notunderstandable .   •Relevant but unhelpful : Though relevant ,   the clue makes a factually incorrect statement ,   often on the contrary of the main question , or   the clue contributes relevant but insufficient   knowledge for prediction , such as repetition   of the question or other distractors .   •Helpful : The clue adds helpful information   to answer the question .   To ensure the annotation quality , we aggregate   annotated results from three students for every   dataset using majority vote . If all three students   annotate differently from each other for an instance ,   we introduce a fourth student to arbitrate .   Table 6 shows the percent of each clue type   across all datasets with an example for each type .   Figure 4 breaks down by dataset . Though the ma-   jority of our clues are relevant ( i.e. , 76.4 % of them   are relevant across all datasets ) , which seems posi-   tive , only 24 % of the clues are deemed as helpful .   This suggests a great room for improvement . In our   future research , we will focus on how to generate   more helpful clues from questions.3279   5.5 Inference Time and Model Size   Table 7 shows the inference time for answering a   question . GenMC is slower than Text2Text ,   but their inference time has the same scale , suggest-   ing that GenMC is more cost - effective considering   its superior accuracy . GenMC and UnifiedQA are   comparable in inference time .   Among T5based models , Text2Text   and UnifiedQA have 223 M parameters , while   GenMC is slightly larger with 234 M parameters .   Among T5 based models , Text2Text and   UnifiedQA have 738 M parameters , while GenMC   has 757 M parameters .   6 Conclusion   We present GenMC , a simple yet effective model   which tailors pre - trained encoder - decoders for   MCQA tasks . Compared with existing usages   of pre - trained encoder - decoders for MCQA , our   model fully exploits the pre - trained encoder-   decoders ’ NLG capabilities to generate a clue from   the input question , which facilitates deep under-   standing of question - option pairs . Experimental   results further verify the superiority of GenMC   over existing usages . Notably , our model achieves   promising results without using any provided doc-   uments or external corpora , showing an interesting   application of PLMs by directly inducing either   commonsense or scientific knowledge from themthrough clue generation .   In the future , we will focus on how to further   improve the clue generation quality , which remains   a bottleneck of GenMC . We hope this work will   spur more research in how to better use pre - trained   encoder - decoders for not only MCQA , but also   beyond ; for tasks with divergent structures from   the pre - training , a smarter use of PLMs can boost   the performance significantly .   Acknowledgments   This work was supported in part by the NSFC   ( 62072224 ) and in part by the Beijing Academy   of Artificial Intelligence ( BAAI ) .   References32803281328232833284328532863287