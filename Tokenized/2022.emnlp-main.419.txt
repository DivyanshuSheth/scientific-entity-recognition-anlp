  Lu Dai , Bang Wang , Wei Xiang , Yijun MoSchool of Electronic Information and Communications ,   Huazhong University of Science and Technology , Wuhan , ChinaSchool of Computer Science and Technology ,   Huazhong University of Science and Technology , Wuhan , China   { dailu18 , wangbang , xiangwei , moyj}@hust.edu.cn   Abstract   Recently , prompt - tuning has attracted growing   interests in event argument extraction ( EAE ) .   However , the existing prompt - tuning methods   have not achieved satisfactory performance   due to the lack of consideration of entity in-   formation . In this paper , we propose a bi-   directional iterative prompt - tuning method for   EAE , where the EAE task is treated as a cloze-   style task to take full advantage of entity in-   formation and pre - trained language models   ( PLMs ) . Furthermore , our method explores   event argument interactions by introducing   the argument roles of contextual entities into   prompt construction . Since template and ver-   balizer are two crucial components in a cloze-   style prompt , we propose to utilize the role   label semantic knowledge to construct a seman-   tic verbalizer and design three kinds of tem-   plates for the EAE task . Experiments on the   ACE 2005 English dataset with standard and   low - resource settings show that the proposed   method significantly outperforms the peer state-   of - the - art methods . Our code is available at   https://github.com/HustMinsLab/BIP .   1 Introduction   As a key step of event extraction , event argument   extraction refers to identifying event arguments   with predefined roles . For example , for an " At-   tack " event triggered by the word " fired " in the sen-   tence " Iraqis have fired sand missiles andAAA at   aircraft " , EAE aims to identify that " Iraqis " , " mis-   siles " , " AAA " and " aircraft " are event arguments   with the " Attacker " , " Instrument " , " Instrument "   and " Target " roles , respectively .   In order to exploit the rich linguistic knowledge   contained in pre - trained language models , fine-   tuning methods have been proposed for EAE . The   paradigm of these methods is to use a pre - trained   language model to obtain semantic representations ,   Figure 1 : Illustration of fine - tuning and prompt - tuning   methods for predicting the argument role of the entity   mention " Iraqis " in the event triggered by the word   " fired " .   and then feed these representations into a well-   designed neural network to extract event arguments .   For example in Figure 1(a ) , an event trigger rep-   resentation and an entity mention representation   are first obtained through a pre - trained language   model , and then input to a designed neural net-   work , such as hierarchical modular network ( Wang   et al . , 2019 ) and syntax - attending transformer net-   work ( Ma et al . , 2020 ) , to determine the argument   role that the entity mention plays in the event trig-   gered by the trigger . However , there is a significant   gap between the EAE task and the objective form   of pre - training , resulting in the poor utilization of   the prior knowledge in PLMs . Additionally , fine-   tuning methods heavily depend on extensive anno-   tated data and perform poorly in low - resource data   scenarios .   To bridge the gap between the EAE task and   the pre - training task , prompt - tuning methods ( Li   et al . , 2021 ; Ma et al . , 2022 ; Hsu et al . , 2022 ; Liu   et al . , 2022 ) recently have been proposed to for-6251malize the EAE task into a more consistent form   with the training objective of generative pre - trained   language models . These methods achieve signifi-   cantly better performance than fine - tuning methods   in low - resource data scenarios , but not as good as   the state - of - the - art fine - tuning method ONEIE ( Lin   et al . , 2020 ) in high - resource data scenarios .   To achieve excellent performance in both low-   resource and high - resource data scenarios , we lever-   age entity information to model EAE as a cloze-   style task and use a masked language model to   handle the task . Figure 1(b ) shows a typical cloze-   style prompt - tuning method for EAE . The typical   prompt - tuning method suffers from two challenges :   ( i ) The typical human - written verbalizer ( Schick   and Schütze , 2021 ) is not a good choice for EAE .   The human - written verbalizer is to manually assign   a label word to each argument role . For exam-   ple in Figure 1(b ) , we choose the " attacker " as the   label word of " Attacker " role . However , an argu-   ment role may have different definitions in differ-   ent types of events . For example , the " Entity " role   refers to " the voting agent " and " the agents who   are meeting " in the " Elect " and " MEET " events , re-   spectively . ( ii ) Event argument interactions are not   explored . Existing work ( Sha et al . , 2018 ; Xiangyu   et al . , 2021 ; Ma et al . , 2022 ) has demonstrated the   usefulness of event argument interactions for EAE .   For the " Attack " event triggered by the word " fired "   in Figure 1 , given that " missiles " is an " Instrument " ,   it is more likely to correctly classify " AAA " into   the " Instrument " role .   In this paper , we propose a bi - directional iter-   ative prompt - tuning ( BIP ) method to alleviate the   aforementioned challenges . To capture argument   interactions , a forward iterative prompt and a back-   ward iterative prompt are constructed to utilize the   argument roles of contextual entities to predict the   current entity ’s role . For the verbalizer , we redefine   the argument role types and assign a virtual label   word to each argument role , where the initial rep-   resentation of each virtual label word is generated   based on the semantic of the argument role . In ad-   dition , we design three kind of templates : hard   template , soft template , and hard - soft template ,   which are further discussed in the experimental   section . Extensive experiments on the ACE 2005   English dataset show that the proposed method can   achieve the state - of - the - art performance in both   low - resource and high - resource data scenarios.2 Related Work   In this section , we review the deep learning   methods for event argument extraction and prompt-   tuning methods for natural language processing .   2.1 Event Argument Extraction   Early deep learning methods use various neural   networks to capture the dependencies in between   event triggers and event arguments to extract event   arguments , such as convolutional neural network   ( CNN)-based models ( Chen et al . , 2015 ) , recur-   rent neural network ( RNN)-based models ( Nguyen   et al . , 2016 ; Sha et al . , 2018 ) and graph neural net-   works ( GNN)-based models ( Liu et al . , 2018 ; Dai   et al . , 2021 ) . As pre - trained language models have   been proven to be powerful in language understand-   ing and generation ( Devlin et al . , 2019 ; Liu et al . ,   2019 ; Lewis et al . , 2020 ) , some PLM - based meth-   ods have been proposed to extract event arguments .   These methods can be divided into two categories :   fine - tuning and prompt - tuning ones .   Fine - tuning methods aim to design a variety of   neural network models to transfer pre - trained lan-   guage models to EAE task . According to the mod-   eling manner of EAE task , existing fine - tuning   work can be further divided into three groups :   classification - based methods ( Wang et al . , 2019 ;   Wadden et al . , 2019 ; Lin et al . , 2020 ; Ma et al . ,   2020 ; Xiangyu et al . , 2021 ) ; machine reading   comprehension - based methods ( Du and Cardie ,   2020 ; Li et al . , 2020 ; Liu et al . , 2020 ) ; generation-   based methods ( Paolini et al . , 2020 ; Lu et al . , 2021 ) .   Prompt - tuning methods aim to design a template to   provide useful prompt information for pre - trained   language models to extract event arguments ( Li   et al . , 2021 ; Ma et al . , 2022 ; Hsu et al . , 2022 ; Liu   et al . , 2022 ) . For example , Li et al . ( 2021 ) create   a template for each event type based on the event   ontology definition and model the EAE task as the   conditional text generation . This method acquires   event arguments by comparing the designed tem-   plate with the generated natural language text . Hsu   et al . ( 2022 ) improve the method of Li et al . ( 2021 )   by replacing the non - semantic placeholder tokens   in the designed template with words with role label   semantics .   2.2 Prompt - tuning   The core of prompt - tuning is to transform the   given downstream task into a form that is consis-   tent with a training task of the pre - trained language6252models ( Liu et al . , 2021 ) . As prompt - tuning makes   better use of prior knowledge contained in pre-   trained language models , this new paradigm is be-   ginning to become popular in NLP tasks and has   achieved promising performance ( Seoh et al . , 2021 ;   Han et al . , 2021 ; Cui et al . , 2021 ; Hou et al . , 2022 ;   Hu et al . , 2022 ; Chen et al . , 2022 ) . For example ,   Cui et al . ( 2021 ) use candidate entity spans and   entity type label words to obtain templates , and   recognize entities based on the pre - trained genera-   tive language model ’s score for each template . Hu   et al . ( 2022 ) convert the text classification task to a   masked language modeling problem by predicting   the word filled in the " [ MASK ] " token , and propose   a knowledgeable verbalizer to map the predicted   word into a label . Chen et al . ( 2022 ) consider the   relation extraction problem as a cloze task and use   the relation label semantic knowledge to initialize   the virtual label word embedding for each relation   label .   3 Model   In this section , we first introduce the problem   description of event argument extraction and the   overall framework of our bi - directional iterative   prompt - tuning method , then explain the details of   designed semantical verbalizer , three different tem-   plates , and model training .   3.1 Problem Description   As the most common ACE dataset provides en-   tity mention , entity type and entity coreference   information , we use these entity information to for-   malize event argument extraction into the argument   role prediction problem of entities . The detailed   problem description is as follow : Given a sentence   S , an event trigger twith event type , and nentities   { e , e , ... , e } , the goal is to predict the argument   role of each entity in the event triggered by tand   output a set of argument roles { r , r , ... , r } .   In this paper , the argument role prediction prob-   lem is casted as a cloze - style task through a tem-   plate T(·)and verbalizer . For the trigger tand   entity e , a template T(t , e,[MASK])is constructed   to query the argument role that the entity eplays in   the event triggered by t. For example in Figure 1(b ) ,   the template T(fired , Iraqis , [ MASK])can be set   as " For the attack event triggered by the fired ,   theperson , Iraqis , is [ MASK ] " , where " attack " rep-   resents the event type of the trigger " fired " and   " person " represents the entity type of the entity"Iraqis " . Then the input of the i - th entity eis :   x = S[SEP]T(t , e,[MASK ] ) . ( 1 )   The verbalizer is a mapping from the label word   space to the argument role space . Let ldenote   the label word that is mapped into the role r , the   confidence score that the i - th entity is classified as   thej - th role type is :   s = C([MASK ] = l ) , ( 2 )   where Cis the output of a pre - trained masked   language model at the masked position in x , i.e.   the confidence score of each word in the dictionary   filled in the [ MASK ] token .   3.2 Overall Framework   Figure 2 presents the overall architecture of our   bi - directional iterative prompt - tuning method , con-   sisting of a forward iterative prompt ( FIP ) and a   backward iterative prompt ( BIP ) . The forward iter-   ative prompt predicts the argument role of each en-   tity iteratively from left to right until argument roles   of all entities are obtained . For example in Figure 2 ,   the order of entities is " Iraqis →missiles →   AAA→aircraft " .   In order to utilize the predicted argument role   information to classify the current entity into the   correct role , we introduce the argument roles of   the first i-1entities into the template of the i - th   entity . The template of the i - th entity in the forward   iterative prompt can be represented as :   FIP ( e ) = T(t , e,− →l , ... , e,− − →l , e,[MASK ] ) ,   ( 3 )   where− →lis the role label word of the j - th entity   predicted by the forward iterative prompt . For ex-   ample in Figure 2,− →lis the word " attacker " . Then   the confidence score distribution of the i - th en-   tity over all argument roles in the forward iterative   prompt can be computed by   − →s = MLM ( S[SEP]FIP ( e ) ) . ( 4 )   − →lis the word corresponding to the argument role   with the highest value in− →s .   Similarly , the backward iterative prompt predicts   the argument role of each entity in a right - to - left   manner . The argument role confidence score distri-   bution of the i - th entity in the backward iterative   prompt can be computed by :   BIP ( e ) = T(t , e,← −l , ... , e,← − −l , e,[MASK ] ) ,   ( 5 )   ← −s = MLM ( S[SEP]BIP ( e ) ) . ( 6)6253   Then we can obtain the final argument role con-   fidence score distribution of the i - th entity by   s=− →s+← −s . ( 7 )   Finally , the argument role label with the highest   score is chosen as the role prediction result .   3.3 Semantical Verbalizer   To tackle the problem that an argument role   may have different definitions in different types   of events , we reconstruct the set of argument role   types and design a semantical verbalizer . Specif-   ically , we further divide the argument role that   participates in multiple types of events into multi-   ple argument roles that are specific to event types .   For example , the " Entity " role is divided into   " Elect : Entity " , " Meet : Entity " , and etc . Since the   " Place " role has the same meaning in all types of   events , we do not consider to divide it .   For each new argument role , the semantical ver-   balizer constructs a virtual word to represent the   role and initializes the representation of the virtual   word with the semantic of the argument role . Let   am - word sequence { q , q , ... , q}denote the   semantic description of the argument role r , the   initial representation of the label word lthat is   mapped into the role rcan be computed by :   E(l ) = 1   m / summationdisplayE(q ) , ( 8)where Eis the word embedding table of a pre-   trained masked language model .   For redefined argument roles , different argu-   ment roles may have the same semantics , such as   " Appeal : Adjudicator " and " Sentence : Adjudicator " .   Therefore , it is easy to misclassify the entity   with " Appeal : Adjudicator " role into the " Sen-   tence : Adjudicator " role . In order to solve the prob-   lem , we use the event structure information to ex-   tract arguments . For an event with the " Appeal "   type , its role label can only be " Appeal : Defendant " ,   " Appeal : Adjudicator " and " Appeal : Plaintiff " .   3.4 Templates   To take full advantage of event type , trigger , and   entity information , the designed template should   contain event types , triggers , entity types , and en-   tity mentions . Since some entity types and event   types are not human - understandable words , such as   " PER " and " Phone - Write " , we need to convert each   entity ( event ) type into a human - understandable   text span . For example , we use " person " and " ’ writ-   ten or telephone communication " as the text spans   for " PER " and " Phone - Write " respectively .   LetM={ε , ε , ... , ε}denote the entity   mention set of the i - th entity , the word sequence of   thei - th entity can be represented as :   ˆe = εor εor ... or ε . ( 9 )   We use wto denote the text span of event type   of the given trigger and wto denote the text span6254   of the entity type of the i - th entity . For the given   trigger tandi - th entity e , three different templates   of forward iterative prompt are designed as follows :   •Hard Template : All known information are   connected manually with natural language .   " For the wevent triggered by the t , thew ,   ˆe , is− →l , ... , the w,ˆe , is− − →l , thew ,   ˆe , is [ MASK ] "   •Soft Template : Add a sequence of learnable   pseudo tokens after all known information .   " wt wˆe− →l ... wˆe− − →lwˆe[V1 ]   [ V2 ] [ V3 ] [ MASK ] [ V4 ] [ V5 ] [ V6 ] "   •Hard - Soft Template : All known information   are connected with learnable pseudo tokens .   " [ V1]w[V2]t[V3 ] [ V4 ] w[V5 ] ˆe[V6]− →l , ... , [ V4 ] w[V5 ] ˆe[V6]− − →l[V4 ]   w[V5 ] ˆe[V6 ] [ MASK ] "   Pseudo tokens are represented by " [ Vi ] " . The em-   bedding of each pseudo token is randomly initial-   ized and optimized during training .   3.5 Training   During training , gold argument roles are used to   generate the template of each entity in forward iter-   ative prompt and backward iterative prompt . The   optimization objective is to ensure that the masked   language model can predict argument role accu-   rately in both forward iterative prompt and back-   ward iterative prompt . We use− →pand← −pto rep-   resent the probability of the entity eplaying each   role type in the event triggered by tin forward and   backward iterative prompt respectively . The loss   function is defined as follows :   − →p = softmax ( − →s),← −p = softmax ( ← −s ) ,   L=−/summationdisplay / summationdisplay(log(− →p(˜r ) ) + log(← −p(˜r ) ) ) ,   ( 10)where Tis the event trigger set in the training set ,   nis the number of entities contained in the same   sentence as the event trigger t , and ˜ris the correct   argument role of the i - th entity playing in the event   triggered by t.   4 Experiments   4.1 Experimental Setup   We evaluate our proposed method on the most   widely used event extraction dataset , ACE 2005 En-   glish dataset(Doddington et al . , 2004 ) . Following   the previous work ( Wadden et al . , 2019 ; Lin et al . ,   2020 ; Ma et al . , 2022 ) , the dataset is pre - processed   and divided into training / development / test set ,   where 33event subtypes , 7entity types and 22argu-   ment roles are considered in the processed dataset .   As event argument extraction task is only focused   on , we use gold entities and event triggers to con-   duct experiments .   We use Bert - base(containing around 110 mil-   lions parameters ) ( Devlin et al . , 2019 ) and   Roberta - base(containing around 125 millions pa-   rameters ) ( Liu et al . , 2019 ) models to predict   the masked words and train each model with   AdamW , where the batch size is set to 4and   the learning rate is set to 1e-5 . For the low-   resource setting , we generate some subsets contain-   ing(1%,5%,10%,20%,50%,75 % ) of the fulling   training set in the same way as ( Hsu et al . , 2022 ) .   In each experiment , the masked language model   is trained by a subset and evaluated by the fulling   development and test sets . All experiments are run   on a NVIDIA Quadro P4000 GPU .   4.2 Baselines   Two categories of state - of - the - art methods are   compared with our proposed method.6255   Fine - tuning Methods :   •HMEAE ( Wang et al . , 2019 ) is a hierarchical   modular model that uses the superordinate   concepts of argument roles to extract event   arguments .   •ONEIE ( Lin et al . , 2020 ) is a neural frame-   work that leverages global features to jointly   extract entities , relations , and events . When   applying ONEIE to the EAE task , we also use   gold entity mentions and event triggers to ex-   tract event arguments , without considering the   relations .   •BERD ( Xiangyu et al . , 2021 ) is a bi-   directional entity - level recurrent decoder that   utilizes the argument roles of contextual enti-   ties to predict argument roles entity by entity .   Prompt - tuning Methods :   •DEGREE(EAE ) ( Hsu et al . , 2022 ) summa-   rizes an event into a sentence based on a de-   signed prompt containing the event type , trig-   ger , and event - type - specific template . Then   event arguments can be extracted by compar-   ing the generated sentence with the event - type-   specific template .   •PAIE ( Ma et al . , 2022 ) is an encoder - decoder   architecture , where the given context and de-   signed event - type - specific prompt are input   into the encoder and decoder separately to   extract event argument spans.4.3 Evaluation   Since we use an entity as a unit for argument role   prediction , an event argument is correctly identified   if the entity corresponding to the argument is pre-   dicted to be the non - None role type . The argument   is further be correctly classified if the predicted role   type is the same as the gold label .   For the above baselines , they consider that an   event argument is correctly classified only if its   offsets and role type match the golden argument ,   which can be called " strict match ( SM ) " . In order   to compare our model with baselines more fairly ,   we use a " flexible match ( FM ) " method to evalu-   ate these baselines , that is , an argument is correctly   classified if its offsets match any of the entity men-   tions co - referenced with the golden argument and   role type match the golden argument .   Same as the previous work , the standard micro-   averaged Precision(P ) , Recall(R ) , and F1 - score(F1 )   are used to evaluate all methods .   4.4 Overall Results   Table 1 compares the overall results between our   model and baselines , from which we have several   observations and discussions .   ( 1 ) BIP(Roberta ) gains the significant improve-   ment in event argument extraction . The F1-   scores of BIP(Roberta ) are more than 9%higher   than those of all baselines obtained by the strict   match evaluation method . Even using the flex-   ible match method to evaluate baselines , the6256ModelRole Classification   P R F1   BIP(our ) 75.26 83.19 79.03   BIP(forward ) 76.06 78.95 77.47   BIP(backward ) 75.94 76.61 76.27   -BI 78.79 76.02 77.38   -SV 74.79 78.07 76.39   -BI - SV 78.19 74.42 76.25   BIP(Roberta ) method also outperforms the state-   of - the - art ONEIE(Roberta ) by 3.24 % increase of   F1 - score in term of argument identification and   3.53 % increase of F1 - score in term of role classifi-   cation .   ( 2 ) Comparing with the strict match , the flexible   match achieves 5%to7%F1 - score improvements   in term of argument identification and role clas-   sification . These results indicate that the trained   argument extraction models can indeed identify the   entity mention co - referenced with the golden ar-   gument as the event argument . In addition , in the   actual application scenarios , we only pay attention   to which entity is the event argument , not which   mention in an entity is the event argument . There-   fore , it is more reasonable and efficient to predict   argument roles in unit of entity than entity mention .   ( 3 ) Roberta - version methods outperform Bert-   version methods . In particular , for our proposed   BIP method , Roberta further gains 3.77 % and4.8 %   F1 - score improvements on argument identification   task and role classification task respectively . These   improvements can be explained by Roberta using   a much larger training dataset than Bert and re-   moving the next sentence prediction task . In the   following experiments , we only consider Roberta-   version methods .   4.5 Ablation Study   Table 2 presents an ablation study of our pro-   posed BIP method . BIP(forward ) only considers   the forward iterative prompt to extract event argu-   ments . BIP(backward ) only considers the back-   ward iterative prompt . BIP - BI does not use a bi-   directional iterative strategy to consider argument   interactions , i.e. predicts the argument role of   each entity separately . BIP - SV replaces our de-   signed semantical verbalizer with a human - written   verbalizer , where each label word is manually se-   lected from a pre - trained language model vocabu - lary . BIP - BI - SV uses neither the bi - directional iter-   ative strategy nor the semantical verbalizer . Some   observations on the ablation study are as follows :   ( 1 ) Compared with the method BIP , the per-   formance of BIP(forward ) and BIP(backward ) is   decreased by 1.56 % and2.76 % F1 - score in term   of role classification , respectively . These results   clearly demonstrate that the bi - directional itera-   tive prompt - tuning can further improve the perfor-   mance by comparing with one direction .   ( 2 ) Comparing with the methods BIP - BI and   BIP - BI - SV , the methods BIP and BIP - SV can fur-   ther improve the performance of role classifica-   tion in terms of 1.65 % and0.14 % increase of F1-   score , respectively . These results suggest that the   bi - directional iterative strategy is useful for event   argument extraction . In addition , we notice that the   improvement brought by our bi - directional itera-   tive strategy for the method BIP - BI is higher than   BIP - BI - SV . This suggests that the more accurate   the independent predicted argument role of each   entity , the greater improvement the bi - directional   iterative strategy will bring to the performance of   argument extraction .   ( 2 ) The methods BIP and BIP - BI are respectively   outperform the methods BIP - SV and BIP - BI - SV by   2.64 % and1.13 % F1 - score in term of role classifi-   cation . These results illustrate that our semantical   verbalizer is more effective than a human - written   verbalizer for event argument extraction .   4.6 Low - Resource Event Argument   Extraction   Figure 4 presents the performance of our BIP ,   BIP - BI and two state - of - the - art methods in both   low - resource and high - resource data scenarios . We6257   can observe that the variation of F1 - score has a   trend of rising with the increase of the training data .   Comparing the fine - tuning method ONEIE , prompt-   tuning methods BIP , BIP - BI and PAIE obviously   improve the performance of role classification in   low - resource data scenarios . This result shows   that prompt - tuning methods can more effectively   utilize the rich knowledge in PLMs than fine - tuning   methods .   Even using flexible match to evaluate the prompt-   tuning method PAIE , our method BIP and BIP - BI   achieve better performance in both low - resource   and high - resource data scenarios . The main rea-   son is that our method can make use of the entity   information and predicted argument roles when   constructing the template . We notice that the per-   formance of BIP is worse than that of BIP - BI , when   the ratio of training data is less than 20 % . This is   because when the number of training data is too   small , the probability of argument roles being cor-   rectly predicted is low . If the bi - directional iter-   ative strategy is adopted , the wrongly predicted   argument roles will be used for template construc-   tion , which will further degrade the performance   of EAE .   4.7 Case Study   In order to showcase the effectiveness of our   method BIP , we sample three sentences from the   ACE 2005 English test dataset to compare the event   argument extraction results by BIP , BIP(forward),BIP(backward ) , BIP - BI and BIP - SV methods .   In Sentence 1 of Table 3 , the method without   the bi - directional iterative strategy BIP - BI can only   identify the entity " Bush " as the " Entity " role . For   the entity " Putin " , the methods with the forward   iterative prompt BIP , BIP(forward ) , BIP - SV can   correctly classify it into the " Entity " role . This at-   tributes to that the information that entity " Bush "   is the " Entity " argument is introduced into the tem-   plate construction of the entity " Putin " . We also   notice that " Bush " and " Putin " are both misclassi-   fied in the BIP(backward ) method , where the error   role information of " Putin " is passed to the classi-   fication of " Bush " . In addition , for the entity " [ he ,   Erdogan ] " in Sentence 2 , the method only with   the forward iterative prompt BIP(forward ) misclas-   sifies the entity " [ Tomahawk , missiles ] " into the   " Instrument " role . These results show that the argu-   ment roles of contextual entities can provide useful   information for the role identification of the cur-   rent entity . However , only considering argument   interactions in one direction may degrade the per-   formance of event argument extraction .   In Sentence 3 , the method BIP - SV misclassi-   fies the entity " SEC " into the " Entity " role . For   the human - written verbalizer of BIP - SV , the word   " judge " is selected as the label word of role " Adju-   dicator " . It is difficult to associate the entity " SEC "   with the word " judge " . In the semantical verbal-   izer , we use the text sequence " the entity doing the   fining " to describe the semantic of " Adjudicator"6258Model TemplateArgument Identification Role Classification   P R F1 P R F1   BIP(our)Hard Template 78.17 86.40 82.08 75.26 83.19 79.03   Soft Template 80.63 82.75 81.67 77.49 79.53 78.50   Hard - Soft Template 77.15 82.46 79.72 74.15 79.24 76.61   BIP - BIHard Template 81.82 78.95 80.36 78.79 76.02 77.38   Soft Template 76.25 84.94 80.36 73.62 82.02 77.59   Hard - Soft Template 81.84 80.70 81.12 78.29 77.49 77.88   role in the " Fine " event . Since the pre - trained lan-   guage models can easily identify the entity " SEC "   as " the entity doing the fining " , the methods with   semantical verbalizer can correctly identify the en-   tity " SEC " as the " Adjudicator " role . The result   verifies the effectiveness of our designed semanti-   cal verbalizer .   4.8 Prompt Variants   In this section , we compare three different tem-   plates introduced in Section 3.4 to investigate how   different types of templates affect the performance   of EAE . For the BIP - BI method , the performances   of hard template , soft template and hard - soft tem-   plate are comparable . Since the hard - soft template   combines the manual knowledge and learnable vir-   tual tokens , it achieves the best performance . How-   ever , the hard - soft template performs worst for the   BIP method . Unlike the BIP - BI method which only   considers event trigger and current entity informa-   tion , BIP introduces the predicted argument role   information into the template . Therefore , there are   so many learnable pseudo tokens in the hard - soft   template , resulting in poor performance .   5 Conclusion and Future Work   In this paper , we regard event argument ex-   traction as a cloze - style task and propose a bi-   directional iterative prompt - tuning method to ad-   dress this task . The bi - directional iterative prompt-   tuning method contains a forward iterative prompt   and a backward iterative prompt , which predict   the argument role of each entity in a left - to - right   and right - to - left manner respectively . For the tem-   plate construction in each prompt , the predicted   argument role information is introduced to cap-   ture argument interactions . In addition , a novel   semantical verbalizer is designed based on the se-   mantic of the argument role . And three kinds of   templates are designed and discussed . Experimentresults have shown the effectiveness of our method   in both high - resource and low - resource data sce-   narios . In the future work , we are interested in the   joint prompt - tuning method of event detection and   event argument extraction .   Limitations   •As the entity information is necessary to   model event argument extraction as a cloze-   style task , our method is not suitable for the   situation that entities are not provided .   •Comparing with the methods that predict ar-   gument roles simultaneously , the speed of our   method is slower due to that it predicts the   argument role of each entity one by one .   Acknowledgements   This work is supported in part by National   Natural Science Foundation of China ( Grant No :   62172167 ) . We also want to use our BIP model on   MindSpore , which is a new deep learning comput-   ing framework . These problems are left for future   work .   References62596260   A Verbalizer   A.1 Semantical Verbalizer   For our designed semantical verbalizer , an ar-   gument role that participates in multiple types of   events is divided into multiple argument roles that   are specific to event types . For each new argument   role , we use a virtual word to represent the role   and initialize the representation of the virtual word   with the semantic of the argument role . Table 6   shows the redefined argument role types , and the   semantic description and virtual label word of each   argument role type .   A.2 Human - written Verbalizer   For the human - written verbalizer , we assign a   label word to each argument role . Table 6 lists the   label word of each argument role .   B Templates   For our designed templates , each entity ( event )   type is converted into a human - understandable text   span , so as to take full advantage of event type label   and entity type label information . Table 7 and 8   list all text spans of entity types and event types.62616262Argument Role Label Label Word   None none   Person person   Place place   Buyer buyer   Seller seller   Beneficiary beneficiary   Artifact artifact   Origin origin   Destination destination   Giver donor   Recipient recipient   Org organization   Agent agent   Victim victim   Instrument instrument   Entity entity   Attacker attacker   Target target   Defendant defendant   Adjudicator judge   Prosecutor prosecutor   Plaintiff plaintiff   Vehicle vehicle   Entity Type Text Span   FAC facility   ORG organization   GPE geographical or political entity   PER person   VEH vehicle   WEA weapon   LOC location6263