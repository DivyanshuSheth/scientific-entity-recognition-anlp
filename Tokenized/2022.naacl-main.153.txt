  Yizhu Liu , Qi Jia , Kenny Q. Zhu   Shanghai Jiao Tong University , Shanghai , China{liuyizhu , Jia_qi}@sjtu.edu.cnkzhu@cs.sjtu.edu.cn   Abstract   A document can be summarized in a number   of ways . Reference - based evaluation of sum-   marization has been criticized for its inﬂexibil-   ity . In this paper , we propose a new automatic   reference - free evaluation metric that compares   semantic distribution between source docu-   ment and summary by pretrained language   models and considers summary compression   ratio . The experiments show that this metric   is more consistent with human evaluation in   terms of coherence , consistency , relevance , ﬂu-   ency .   1 Introduction   Summarization evaluation metrics that measure the   quality of generated summaries are very impor-   tant for the development of summarization systems   ( Rush et al . , 2015 ; Chopra et al . , 2016 ; Nallapati   et al . , 2017 ; Liu et al . , 2018 , 2022 ; Lewis et al . ,   2020 ; Zhang et al . , 2020a ; Liu et al . , 2021 ) . Most   previous summarization evaluation metrics need   human - annotated summaries as reference and mea-   sure summary quality through the similarity be-   tween generated summaries and their reference   summaries ( Papineni et al . , 2002 ; Lin , 2004 ; Gane-   san , 2006 ; Ng and Abrecht , 2015 ; Zhang et al . ,   2020b ; Zhao et al . , 2019 ) . Such reference - based   evaluation metrics can not accurately evaluate the   summary , because a document has many correct but   different summaries . It is difﬁcult and expensive to   write many reference summaries by human for eval-   uation . Thus , it is useful to develop reference - free   evaluation metrics for this task .   In this paper , we focus on reference - free eval-   uation metrics . As shown in Figure 1 , a high-   quality summary should be concise and contain the   most important information of its document . Some   reference - free evaluation metrics ( Shao et al . , 2017 ;   Gao et al . , 2020 ) unsupervisedly construct a pseudoFigure 1 : A document with its high - quality and low-   quality summaries . The heat map marks the salient con-   tent in the document . The darker the colour , the more   salient the content .   reference summary by selecting salient sentences   from the source document , which also ignore the   variety of summaries . Others evaluate the sum-   mary quality by measuring how much information   from the document is represented in the summary .   QA - based evaluation metrics ( Chen et al . , 2018 ;   Scialom et al . , 2019 ; Durmus et al . , 2020 ) achieve   this possibility by ﬁrst asking the same questions   to document and summary and then comparing   their answers . The performance of these metrics   depends on the quality of question generation and   question - answering systems . Shannon score ( Egan   et al . , 2022 ) intuitively uses a language model to au-   toregressively generate a document both with and   without a summary as a prompt , and then computes   the difference in information content between two   generated documents . The information of docu-   ment generated with a better summary , which is   better restored , is more similar to the document gen-   erated without summary . Although Shannon score   is the state - of - the - art ( SOTA ) summarization eval-2109uation metric , its estimation of information content   of the document can not reﬂect the position and im-   portance of each token in document . However , the   position of tokens will impact coherence and the   importance will impact salient information , which   are very important for summarization evaluation .   For example , the low - quality summary contains   similar words to the high - quality summary but it is   unreadable and loses important information of the   source document .   To tackle the problem in Shannon score , we   present a new reference - free evaluation metric   ( SDC ) which computes the correlation ( semantic   distribution correlation ) between the probability   distribution of tokens in predicted documents with   and without a prepended summary . Such sequential   probability take account of the position and impor-   tance of the tokens . As compression ratio reﬂects   the difﬁculty of summarization , we introduce com-   pression ratio into SDC ( SDC * ) and penalize the   long summary .   Our contribution are as follows :   •We propose a reference - free summarization   evaluation metric ( SDC * ) which evaluates   summaries considering semantic distribution   correlation andcompression ratio between   source document and summary .   •Our proposed SDC and SDC * achieve bet-   ter performance than the SOTA summariza-   tion evaluation metric on CNN / Daily Mail and   TAC 2010 datasets .   2 Approach   In this section , we introduce our proposed   reference - free summarization evaluation metric   which computes the semantic distribution corre-   lation between generated documents with and with-   out a summary and combines the correlation with   compression ratio .   Semantic Distribution Correlation ( SDC ) . In-   spired by Shannon score ( Egan et al . , 2022 ) , we   use auto - regressive language model to obtain the   semantic information of documents . Given a docu-   mentD={x , x, ... ,x}consisting of tokens x ,   the auto - regressive language model represents D   by factorizing the joint probabilities over symbols   as the product of conditional probabilities :   P(D ) = /productdisplayp(x|x ) ( 1)In this paper , unlike previous metrics using   P(D)as the semantic information of D , we take   p(x|x)as the semantic representation of xand   use a vector P(D)to represent the semantic distri-   bution ofDgenerated by language model :   P(D ) = [ p(x),p(x|x), ... ,p ( x|x)](2 )   Such ﬁne - grained semantic representation consid-   ers both the order and semantic of the tokens in   sequence , which helps to evaluate the coherence   and relevance .   To evaluate the quality of a summary Sconsist-   ing of token y , we use language model to predict   DwithSas a prompt . The better the summary , the   better the document can be restored . In other words ,   a better summary makes the semantic information   of documents generated with a summary more sim-   ilar to that of documents generated without sum-   mary . We calculate the semantic distribution of D   givenSas :   P(D|S ) = [ p(x|S),p(x|x , S ) , ... ,   p(x|x , S ) ] ( 3 )   TheP(D)andP(D|S)are illustrated in Figure 2 .   We take the correlation between P(D)and   P(D|S)as the evaluation score of summary S :   C(D , S ) = Corr ( P(D),P(D|S ) ) ( 4 )   W(D , S ) = /producttextP(D|S)/producttextP(D)(5 )   SDC ( D , S ) = W(D , S)×C(D , S)(6 )   whereCorr is Pearson ’s γ(Benesty et al . , 2009 )   because we need the change trend of the two dis-   tributions for semantic order and need their spe-   ciﬁc values for information coverage judgement .   W(D , S)indicates the extent to which the doc-   umentDcan be predicted by given summary S.2110Better summaries can get higher W(D , S)scores .   C∈[0,1)is the normalization of C. The   higher SDC means the better summary quality .   SDC with Compression Ratio ( SDC * ) . Com-   pression ratio reﬂects the difﬁculty of summariz-   ing , which is the length of summary divided by   the length of source document : CR(D , S ) =   L(S)/L(D ) , whereLrecords the length of text .   IfL(S)is greater than L(D),CR(D , S)is equal   to 1 . It is more difﬁcult to generate a shorter sum-   mary . Thus , we introduce compression ratio into   SDC and get SDC * as :   SDC(D , S ) = 2×SDC×(1−CR )   SDC + ( 1−CR)(7 )   SDC * ensures that a summary with higher se-   mantic distribution correlation and lower compres-   sion ratio achieves a higher evaluation score .   3 Experiment   In this section , we ﬁrst introduce the human-   annotated datasets and baseline evaluation metrics .   Then we will show the results of our proposed SDC   and SDC * , and analyze the effectiveness of seman-   tic distribution correlation and compression ratio   used in summarization evaluation .   3.1 Datasets   In this experiment , we use 2 summarization eval-   uation datasets , which consist of source docu-   ments , summaries generated by different models   and human - annotated scores on summaries .   CNN / Daily Mail ( CNNDM ) ( Fabbri et al . ,   2021 ) is a single document summarization dataset ,   which consists of 100 documents from the   CNN / DailyMail dataset , each paired with 16 sum-   maries generated by different systems . Each sum-   mary was scored by 3 experts under four aspects :   coherence , consistency , ﬂuency , and relevance .   TAC 2010 ( TAC)is a multi - document summa-   rization dataset , including 92 multi - documents with   43 generated summaries for each multi - document .   Each summary has one human - annotated overall   score . The overall score is based on both coverage   of all required aspects ( Pyramid ) ( Nenkova and   Passonneau , 2004 ) and linguistic quality ( readabil-   ity).3.2 Baselines   We take 4 reference - based evaluation metrics and   2 reference - free evaluation metrics as baselines .   For reference - based evaluation , ROUGE fam-   ily is the most popular evaluation metric in sum-   marization , which evaluates the token sequence   overlapping . We use F1 scores of ROUGE-1 ( R-1 ) ,   ROUGE-2 ( R-2 ) and ROUGE - L ( R - L).BLEU ( Pa-   pineni et al . , 2002 ) focuses on precision with   a brevity penalty . METEOR ( MET . ) ( Baner-   jee and Lavie , 2005 ) allows word stems , syn-   onyms and paraphrases matching . BERTScore   ( BERT . ) ( Zhang et al . , 2020b ) greedily maximizes   the cosine similarity between token embeddings .   For reference - free evaluation , BLANC ( BLA . )   ( Vasilyev et al . , 2020 ) computes the accuracy of un-   masking document tokens with a summary . Shan-   non ( Shan . ) ( Egan et al . , 2022 ) estimates the infor-   mation content shared between a document and its   summary . As Shannon is the SOTA summarization   evaluation metric , we add compression ratio to the   information content of generated document with   a prepended summary in the same way as Eq.7 ,   which is called Shannon * ( Shan . * ) .   3.3 Experimental Setup   In our experiments , we follow Egan et al . ( 2022 )   to use GPT-2 small language model ( Radford et al . ,   2019 ) to compute the semantic distribution of text .   To evaluate the empirical performance of different   summarization evaluation metrics , we correlate the   metrics against the provided human judgement via   Pearson’sγ , Spearman ’s ρandKendall’sτcor-   relation coefﬁcients ( Benesty et al . , 2009 ; Myers   and Sirois , 2004 ; Abdi , 2007 ) . The metrics with   higher correlation with human evaluation scores   are more effective .   As TAC is a multi - document summarization   dataset , we score the summary with each docu-   ment in its multi - document set . The averaged score   of all documents is engaged as the ﬁnal score of   our proposed metrics .   3.4 Results   In this section , we analyze the effectiveness of our   metrics using ﬁne - grained semantic distribution   correlation and introducing compression ratio.2111   3.4.1 Main Results   Table 1 shows that the correlation of our proposed   SDC and SDC * against human evaluation in dif-   ferent correlation coefﬁcients are in the top 2 for   every category of summary quality . Compared with   reference - based metrics , our metrics improve sig-   niﬁcantly in terms of consistency and relevance .   Because reference - based metrics depend on the   quality and quantity of references . The correlations   of SDC and SDC * are similar in terms of consis-   tency since SDC * penalizes long summaries . Long   summaries are more likely to express the informa-   tion consistent with their source documents . Our   metrics focus on the information shared with doc-   ument and summary . Compared with the SOTA   reference - free metric ( Shan . ) measuring the differ-   ence in information content between document and   summary , our metrics measure the difference in   semantic distributions , which better notices the to-   ken order in the sequence ( coherence and ﬂuency )   and the importance of each token ( consistency andrelevance ) with respect to the sequence . Thus , our   metrics perform better than Shan .   To show the generalization of our proposed eval-   uation metrics , we compare the SOTA summariza-   tion evaluation metric ( Shan . ) and our proposed   evaluation metrics on TAC in Table 3 . Compared   with Shan . , SDC and SDC * are more consistent   with human evaluation on TAC , demonstrating our   proposed evaluation metrics can better evaluate   generated summaries . As shown in Table 1 and   Table 3 , as TAC is multi - document summariza-   tion evaluation dataset , the improvement of SDC   and SDC * on TAC are less than that on CNNDM   . As we compute the average of evaluation scores   between the summary and each document in cor-   responding multiple documents , a good summary   may get lower scores . This is because that a good   summary may not perfectly restore all the input   multiple documents .   3.4.2 Ablation Study   The improvement of our metrics is from seman-   tic distribution correlation andcompression ratio .   We evaluate the variants of the SOTA summariza-2112   tion evaluation metric ( Shan . ) and our proposed   reference - free summarization evaluation metric   ( SDC * ) on CNNDM .   Semantic distribution correlation . Semantic   distribution is a ﬁner representation of a document ,   that is , the tokens ’ order and tokens ’ weight . The   tokens ’ order decides the linguistic quality of a   text , so SDC - based scores are more sensitive to   the linguistic quality . As shown in Table 1 , SDC   and SDC * perform much better than baselines for   evaluating coherence and ﬂuency , as these two eval-   uation directions focus on linguistic quality . Com-   pared with the high - quality summary , the unﬂuent   summary and the irrelevant summary in Table 2 get   the similar Shannon scores and lower SDC scores ,   which also shows that the semantic distribution   is useful . The tokens ’ weight points out the im-   portant information in the document . Although   information content can represent the key content ,   it can not compare the importance among adjacent   tokens , which weakens the measure of semantic   relevance . As shown in Table 1 and Table 2 , our   metrics improve the evaluation on consistency , rel-   evance and overall score . The difference in SDC-   based scores between the irrelevant summary and   the high - quality summary is more signiﬁcant than   Shan . score .   Compression ratio . To discuss the impact of   compression ratio on summarization evaluation ,   we introduce compression ratio into Shan . and get   Shan . * ( See Section 3.2 ) . As shown in Figure 3 , af-   ter adding compression ratio , the evaluation metrics   have a strengthening trend . Meanwhile , the longer   summary in Table 2 , which is redundant , is more   likely to represent more information of the docu-   ment . Thus , it is necessary to import compression   ratio to the metrics only considering information   coverage .   4 Conclusion   Semantic distribution correlation can capture the   ﬁne - grained information difference between source   document and summary . The compression ratio   represents an important facet of text summariza-   tion problem . We experimentally showed that   SDC / SDC * achieves strong correlations with hu-   man evaluation scores on summarization tasks .   Acknowledgement   This research is partially supported by NSFC   Grant No . 91646205 , SJTU - CMBCC Joint Re-   search Scheme , and SJTU - Meituan Joint Research   Scheme .   References211321142115