1018101910201021102210231024102510261027102810291030ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 6 & Limitations section   /squareA2 . Did you discuss any potential risks of your work ?   Section 6 & Ethics section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract & Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   We did not use any AI writing assistants and all contents of the paper were written exclusively by the   authors .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4 & 5   /squareB1 . Did you cite the creators of artifacts you used ?   Section 1 & 2 & 3   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 4.3.2   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 4.3.3   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4 & 5   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4.2   C / squareDid you run computational experiments ?   Section 4 & 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.3.21031 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 & 5   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4.3.3 & 5.2 & 5.3   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   The authors of this paper performed the manual evaluation themselves .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   The authors of this paper performed the manual evaluation themselves .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   The authors of this papers were the evaluators so no consent form was needed .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   There were no ethical concerns with the evaluation method .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   We re - used two already published datasets and only manually evaluated the model ’s predictions.1032