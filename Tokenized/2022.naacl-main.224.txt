  Yiwei WangMuhao ChenWenxuan ZhouYujun CaiYuxuan Liang   Dayiheng LiuBaosong YangJuncheng LiuBryan HooiNational University of SingaporeUniversity of Southern CaliforniaNanyang Technological UniversityAlibaba Group   wangyw seu@foxmail.com   Abstract   Recent literature focuses on utilizing the entity   information in the sentence - level relation ex-   traction ( RE ) , but this risks leaking superficial   and spurious clues of relations . As a result , RE   still suffers from unintended entity bias , i.e. ,   the spurious correlation between entity men-   tions ( names ) and relations . Entity bias can   mislead the RE models to extract the relations   that do not exist in the text . To combat this   issue , some previous work masks the entity   mentions to prevent the RE models from over-   fitting entity mentions . However , this strategy   degrades the RE performance because it loses   the semantic information of entities . In this   paper , we propose the CRE(Counterfactual   Analysis based Relation Extraction ) debi-   asing method that guides the RE models to   focus on the main effects of textual context   without losing the entity information . We first   construct a causal graph for RE , which mod-   els the dependencies between variables in RE   models . Then , we propose to conduct coun-   terfactual analysis on our causal graph to dis-   till and mitigate the entity bias , that captures   the causal effects of specific entity mentions   in each instance . Note that our CREmethod   is model - agnostic to debias existing RE sys-   tems during inference without changing their   training processes . Extensive experimental re-   sults demonstrate that our CREyields signif-   icant gains on both effectiveness and general-   ization for RE . The source code is provided at : .   1 Introduction   Sentence - level relation extraction ( RE ) is an impor-   tant step to obtain a structural perception of unstruc-   tured text ( Distiawan et al . , 2019 ) by extracting   relations between entity mentions ( names ) from   thetextual context . From human oracle , textual   context should be the main source of information   that determines the ground - truth relations between   entities . Consider a sentence “ Mary gave birth toJerry . ” . Even if we change the entity mentions   from ‘ Jerry ’ and‘Mary ’ to other people ’s names ,   the relation ‘ parents ’ still holds between the sub-   ject and object as described by the textual context   “ gave birth to ” .   Recently , some work aims to utilize entity men-   tions for RE ( Yamada et al . , 2020 ; Zhou and Chen ,   2021 ) , which , however , leak superficial and spuri-   ous clues about the relations ( Zhang et al . , 2018 ) .   In our work , we observe that entity information   can lead to biased relation prediction by mislead-   ing RE models to extract relations that do not exist   in the text . Fig . 1 visualizes a relation prediction   from a state - of - the - art RE model ( Alt et al . , 2020 )   ( see more examples in Tab . 7 ) . Although the con-   text describes no relation between the highlighted   entity pair , the model extracts the relation as “ coun-   tries ofresidence ” . Such an erroneous result can   come from the spurious correlation between entity   mentions and relations , or the entity bias in short .   For example , if the model sees the relation “ coun-   tries ofresidence ” many more times than other   relations when the object entity is Switzerland dur-   ing training , the model can associate this relation   with Switzerland during inference even though the   relation does not exist in the text .   To combat this issue , some work ( Zhang et al . ,   2017 , 2018 ) proposes masking entities to prevent   the RE models from over - fitting entity mentions .   On the other hand , some other work ( Peng et al . ,   2020 ; Zhou and Chen , 2021 ) finds that this strategy   degrades the performance of RE because it loses   the semantic information of entities .   For both machines and humans , RE requires a   combined understanding of textual context and en-   tity mentions ( Peng et al . , 2020 ) . Humans can   avoid the entity bias and make unbiased decisions   by correctly referring to the textual context that de-   scribes the relation . The underlying mechanism is3071   causality - based ( Van Hoeck et al . , 2015 ): humans   identify the relations by pursuing the main causal   effect of the textual context instead of the side-   effect of entity mentions . In contrast , RE models   are usually likelihood - based : the prediction is anal-   ogous to looking up the entity mentions and textual   context in a huge likelihood table , interpolated by   training ( Tang et al . , 2020 ) . In this paper , our idea   is to teach RE models to distinguish between the   effects from the textual context and entity mentions   through counterfactual analysis ( Pearl , 2018 ):   Counterfactual analysis : If I had not seen the tex-   tual context , would I still extract the same relation ?   The counterfactual analysis essentially gifts hu-   mans the hypothesizing abilities to make decisions   collectively based on the textual context and en-   tity mentions , as well as to introspect whether the   decision is deceived ( see Fig . 1 ) . Specifically , we   are essentially comparing the original instance with   a counterfactual instance , where only the textual   context is wiped out , while keeping the entity men-   tions untouched . By doing so , we can focus on the   main effects of the textual context without losing   the entity information .   In our work , we propose a novel model-   agnostic paradigm for debiasing RE , namely   CRE(Counterfactual analysis based Relation   Extraction ) , which adopts the counterfactual anal-   ysis to mitigate the spurious influence of the en-   tity mentions . Specifically , CREdoes not touch   the training of RE models , i.e. , it allows a model   to be exposed to biases on the original training   set . Then , we construct a causal graph for RE   to analyze the dependencies between variables in   RE models , which acts as a “ roadmap ” for captur-   ing the causal effects of textual context and entitymentions . To rectify the test instances from the   potentially biased prediction , in inference , CRE   “ imagines ” the counterfactual counterparts on our   causal graph to distill the biases . Last but not least ,   CREperforms a bias mitigation operation with   adaptive weights to produce a debiased decision   for RE .   We highlight that CREis a flexible debiasing   method that is applicable to popular RE models   without changing their training processes . To eval-   uate the effectiveness of CRE , we perform ex-   tensive experiments on public benchmark datasets .   The results demonstrate that our proposed method   can significantly improve the effectiveness and gen-   eralization of the popular RE models by mitigating   the biases in an entity - aware manner .   2 Related Work   Sentence - level relation extraction . Early research   efforts ( Nguyen and Grishman , 2015 ; Wang et al . ,   2016 ; Zhang et al . , 2017 ) train RE models from   scratch based on lexicon - level features . The recent   RE work fine - tunes pretrained language models   ( PLMs ; Devlin et al . 2019 ; Liu et al . 2019 ) . For   example , K - Adapter ( Wang et al . , 2020 ) fixes the   parameters of the PLM and uses feature adapters   to infuse factual and linguistic knowledge . Re-   cent work focuses on utilizing the entity informa-   tion for RE ( Zhou and Chen , 2021 ; Yamada et al . ,   2020 ) , but this leaks superficial and spurious clues   about the relations ( Zhang et al . , 2018 ) . Despite   the biases in existing RE models , scarce work has   discussed the spurious correlation between entity   mentions and relations that causes such biases . Our   work investigates this issue and proposes CREto   debias RE models for higher effectiveness.3072   Debiasing for Natural Language Processing . De-   biasing is a fundamental problem in machine learn-   ing ( Torralba and Efros , 2011 ) . For natural lan-   guage processing ( NLP ) , some work performs data   re - sampling to prevent models from capturing the   unintended bias in training ( Dixon et al . , 2018 ;   Geng et al . , 2007 ; Kang et al . , 2016 ; Rayhan et al . ,   2017 ; Nguyen et al . , 2011 ) . Alternatively , Wei and   Zou ( 2019 ) and Qian et al . ( 2020 ) develop data   augmentation for debiasing . Some recent work de-   biases the NLP models based on causal inference   ( Qian et al . , 2021 ; Nan et al . , 2021 ) . In RE , how to   deal with the entity bias is also an important prob-   lem . For example , PA - LSTM ( Zhang et al . , 2017 )   masks the entity mentions with special tokens to   prevent RE models from over - fitting entity names ,   which was also adopted by C - GCN ( Zhang et al . ,   2018 ) and SpanBERT ( Joshi et al . , 2020 ) . However ,   masking entities loses the semantic information of   entities and leads to performance degradation . Dif-   ferent from it , our CREmodel tackles entity bi-   ases based on structured causal models . In this way ,   we debias the RE models to focus on the textual   context without losing the entity information .   3 Methodology   Sentence - level relation extraction ( RE ) aims to ex-   tract the relation between a pair of entities men-   tioned from a sentence . We propose CRE(coun-   terfactual analysis based Relation Extraction ) as   a model - agnostic technique to endow existing RE   models with unbiased decisions during inference .   CREfollows the regular training process of ex-   isting work regardless of the bias from the entity   mentions . During inference , CREpost - adjusts   the biased prediction according to the effects of the   bias . CREcan be flexibly incorporated into pop-   ular RE models to improve their effectiveness and   generalization based on the counterfactual analysis   without re - training the model .   In this section , we first formulate the existing   RE models in the form of a causal graph . Then , we   introduce our proposed bias distillation method todistill the entity bias with our designed counterfac-   tual analysis . We conduct an empirical analysis to   analyze how heavily the existing RE models rely   on the entity mentions to make decisions . Finally ,   we mitigate the distilled bias from the predictions   of RE models to improve their effectiveness .   3.1 Causality of Relation Extraction   In order to perform causal intervention , we first for-   mulate the causal graph ( Pearl et al . , 2016 ; Pearl   and Mackenzie , 2018 ) , a.k.a . , structural causal   model , for the RE models as Fig . 2 , which sheds   light on how the textual context and entity mentions   affect the RE predictions . The causal graph is a   directed acyclic graph G={V , E } , indicating how   a set of variables Vinteract with each other through   the causal relations behind the data and how vari-   ables obtain their values , e.g. , ( E , X)→Yin   Fig . 2 . Before we conduct counterfactual analysis   that deliberately manipulates the values of nodes   and prunes the causal graph , we first revisit the   conventional RE systems in the graphical view .   The causal graph in Fig . 2 is applicable to a   variety of RE models and imposes no constraints on   the detailed implementations . Node Xis the input   text . On the edge X→E , we obtain the spans   of subject and object entities as node Ethrough   NER or human annotations ( Zhang et al . , 2017 ) .   For example , in the aforementioned sentence X=   “ Mary gave birth to Jerry . ” , the entities are E=   [ ’ Mary ’ , ’ Jerry ’ ] .   On the edges ( X , E)→Y , existing RE   models take different designs . For example , C-   GCN ( Zhang et al . , 2018 ) obtains the relation pre-   diction Yby encoding entity mentions Eon the   pruned dependency tree of Xusing a graph convo-   lutional network . IRE ( Zhou and Chen , 2021 ) uses   PLMs as the encoder for X , and marks the entity   information of Ewith special tokens to utilize the   entity information .   3.2 Bias Distillation   Based on our causal graph in Fig . 2 , we diag-   nose how the entity bias affects inference . After   training , the causal dependencies among the vari-   ables are learned in terms of the model parame-   ters . The entity bias can mislead the models to   make wrong predictions while ignoring the actual   relation - describing textual context in X , i.e. , biased   towards the causal dependency : E→Y.   The conventional biased prediction can only see   the output Yof the entire graph given a sentence3073   X , ignoring how specific entity mentions affect the   relation prediction . However , causal inference en-   courages us to think out of the black box . From the   graphical point of view , we are no longer required   to execute the entire causal graph as a whole . In   contrast , we can directly manipulate the nodes and   observe the output . The above operation is termed   intervention in causal inference , which we denote   asdo ( · ) . It wipes out all the incoming links of a   node and demands it to take a certain value .   We distill the entity bias by intervention and its   induced counterfactual . The counterfactual means   “ counter to the facts ” , and takes one step that further   assigns the hypothetical combination of values to   variables . For example , we can remove the input   textual context by masking X , but maintain Eas   the original entity mentions , as if Xstill exists .   We will use the input text Xas our control vari-   able where the intervention is conducted , aiming to   assess its effects , due to the fact that there would   not be any valid relation between entities in Eif   the input text Xis empty . We denote the output   logits Yafter the intervention X= ¯xas follows :   Y = Y(do(X= ¯x ) ) . ( 1 )   Following the above notation , the original predic-   tionY , i.e. , can be re - written as Y.   To distill the entity bias , we conduct the interven-   tiondo(X= ¯x)onX , while keeping the variable   Eas the original e , as if the original input text x   had existed . Specifically , we mask the tokens in   xto produce ¯xbut keep the entity mentions eas   original , so that the textual context is removed and   the entity information is maintained . Accordingly ,   the counterfactual prediction is denoted as Y(see   Fig . 3 ) . In this case , since the model can not see any   textual context in the factual input xafter the inter-   vention ¯x , but still has access to the original entity   mentions eas the inputs , the prediction Ypurely   reflects the influence from e. In other words , Y   refers to the output , i.e. , a probability distribution   or a logit vector , where only the entity mentions   are given as the input without textual context .   To investigate how heavily the state - of - the - art   models rely on the entity mentions for RE , we   conduct an empirical study to compare the orig-   inal prediction Yand the counterfactual one Y.   Specifically , we calculate the fraction of the test   instances ( y - axis ) that have the original relation   prediction arg maxY[c]ranked in the top kmost   confident relations of the counterfactual prediction   Y. This fraction is termed as Hit @k .   We present Hit @kfor IRE ( Zhou and   Chen , 2021 ) , a state - of - the - art RE model , in Fig . 4   on the test instances when the original relation   prediction is title , employee of , ororigin . Higher   Hit@1means that for more instances , the model   infers the same relation given only the entity men-   tions no matter whether the textual context is given ,   which imply stronger causal effects from the entity   mentions Y , i.e. , the models rely more heavily   on the entity mentions for RE .   We observe that when k= 1 , the Hit @1is more   than 50 % , which implies that the model typically   extracts the same relations even without textual   context on more than a half of the instances . For   a larger k , the Hit @kincreases significantly and   reaches more than 80 % for k≥2 . These obser-   vations imply a promising but embarrassing result :   the state - of - the - art model relies on the entity bias   for RE on many instances . The entity bias reflected   byYcan lead to the wrong extraction if the rela-   tion implied by the entity mentions does not exist   in the input text . This poses a challenge to the3074generalization of RE models , as validated by our   experimental results ( § 4.3 ) .   In addition to Ythat reflects the causal effects   of entity mentions , there is another kind of bias not   conditioned on the entity mentions e , but reflecting   the general bias in the whole dataset , which is Y.   Ycorresponds to the counterfactual inputs where   both textual context and entity mentions are re-   moved . In this case , since the model can not access   any information from the input after this removal ,   Ynaturally reflects the label bias that exists in the   model from the biased training . The causal graphic   views of the original prediction Y , the counterfac-   tualYfor the entity bias , and Yfor the label   bias are visualized in Fig . 3 .   3.3 Bias Mitigation   As we have discussed in § 1 , instead of the static   likelihood that tends to be biased , the unbiased re-   lation prediction lies in the difference between the   observed outcome Yand its counterfactual predic-   tionsY , Y. The latter two are the biases that we   want to mitigate from the relation prediction .   Intuitively , the unbiased prediction that we seek   is the linguistic stimuli from blank to the observed   textual context with specific relation descriptions ,   but not merely from the entity bias . The context-   specific clues of the relations are key to the in-   formative unbiased predictions , because even if   the overall prediction is biased towards the rela-   tion “ schools attended ” due to the object entity   like “ Duke University ” , the textual context “ work   at”indicates the relation as “ employee of”rather   than “ schools attended ” .   Our final goal is to use the direct effect of the   textual context from XtoYfor debiased predic-   tion , mitigating ( denoted as \ ) the label bias and the   entity bias from the prediction : Y\Y\Y , so   as to block the spread of the biases from training to   inference . The debiased prediction via bias mitiga-   tion can be formulated via the conceptually simple   but empirically effective element - wise subtraction   operation :   Y = Y−λY−λY , ( 2 )   where λandλare two independent hyper-   parameters balancing the terms for mitigating en-   tity and label biases respectively . Note that the bias   mitigation in Eq . 2 for the entity and label biases   correspond to Total Direct Effect ( TDE ) and Total   Effect ( TE ) in causal inference ( Tang et al . , 2020 ;   VanderWeele , 2015 ; Pearl , 2009 ) respectively . We   adaptively set the values of λandλfor different   datasets based on the grid beam search ( Hokamp   and Liu , 2017 ) in a scoped two dimensional space :   λ , λ= arg maxψ(λ , λ)λ , λ∈[a , b],(3 )   where ψis a metric function ( e.g. , F1 scores ) for   evaluation , a , bare the boundaries of the search   range . We search the values of λ , λonce on   the validation set , and use the fixed values for in-   ference on all testing instances . Since the entity   types can restrict the candidate relations ( Lyu and   Chen , 2021 ) , we use the entity type information ,   if available , to restrict the candidate relations for   inference , which strengthens the effects of entity   types for relation extraction .   Overall , the proposed CREreplaces the con-   ventional one - time prediction with Yto produce   the debiased relation predictions , which essentially   “ thinks ” twice : one for the original observation Y ,   the other for hypothesized Y , Y.   4 Experiments   In this section , we evaluate the performance of our   CREmethods when applied to RE models . We   compare our methods against a variety of strong   baselines on the task of sentence - level RE . Our   experimental settings closely follow those of the   previous work ( Zhang et al . , 2017 ; Zhou and Chen ,   2021 ; Nan et al . , 2021 ) to ensure a fair comparison .   4.1 Experimental Settings   Datasets . We use four widely - used RE bench-   marks : TACRED ( Zhang et al . , 2017 ) , SemEval   ( Hendrickx et al . , 2019 ) , TACRED - Revisit ( Alt   et al . , 2020 ) , and Re - TACRED ( Stoica et al . , 2021 )   for evaluation . TACRED contains over 106k men-   tion pairs drawn from the yearly TAC KBP chal-   lenge . ( Alt et al . , 2020 ) relabeled the development   and test sets of TACRED . Re - TACRED is a further   relabeled version of TACRED after refining its la-   bel definitions . The statistics of these datasets are   shown in Tab . 1.3075   We use the widely - used F1 - macro score as the   main evaluation metric ( Nan et al . , 2021 ) , which   is the balanced harmonic mean of precision and re-   call , as well as F1 - micro for a more comprehensive   evaluation . F1 - macro is more suitable than F1-   micro to reflect the extent of biases , especially for   the highly - skewed cases , since F1 - macro is evenly   influenced by the performance in each category ,   i.e. category - sensitive , but F1 - micro simply gives   equal weights to all instances ( Kim et al . , 2019 ) .   Compared methods . We take the following RE   models into comparison . ( 1 ) C - SGC ( Wu et al . ,   2019 ) simplifies GCN , and combines it with LSTM ,   leading to improved performance over each method   alone . ( 2 ) SpanBERT ( Joshi et al . , 2020 ) extends   BERT by introducing a new pretraining objective   of continuous span prediction . ( 3 ) CP(Peng et al . ,   2020 ) is an entity - masked contrastive pre - training   framework for RE . ( 4 ) RECENT ( Lyu and Chen ,   2021 ) restricts the candidate relations based on the   entity types . ( 5 ) KnowPrompt ( Chen et al . , 2021 )   is Knowledge - aware Prompt - tuning approach . ( 6 )   LUKE ( Yamada et al . , 2020 ) pretrains the language   model on both large text corpora and knowledge   graphs and further proposes an entity - aware self-   attention mechanism . ( 7 ) IRE ( Zhou and Chen ,   2021 ) proposes an improved entity representation   technique in the data preprocessing .   Among the above RE models , we apply our   CREon LUKE and IRE . To demonstrate the ef-   fectiveness of debiased inference , we also comparewith the following debiasing techniques that are   applied to the same two RE models . ( 1 ) Focal ( Lin   et al . , 2017 ) adaptively reweights the losses of dif-   ferent instances so as to focus on the hard ones . ( 2 )   Resample ( Burnaev et al . , 2015 ) up - samples rare   categories by the inversed sample fraction during   training . ( 3 ) Entity Mask ( Zhang et al . , 2017 ):   masks the entity mentions with special tokens to   reduce the over - fitting on entities . ( 4 ) CFIE ( Nan   et al . , 2021 ) is also a causal inference method . In   contrast to our method , CFIE strengthens the causal   effects of entities by masking entity - centric infor-   mation in the counterfactual predictions .   Model configuration . For the hyper - parameters   of the considered baseline methods , e.g. , the batch   size , the number of hidden units , the optimizer , and   the learning rate , we set them as suggested by their   authors . For the hyper - parameters of our CRE   method , we set the search range of the hypermeters   in Eq . 3 as [ −2,2]and the search step 0.1 . For all   experiments , we report the median F1scores of   five runs of training using different random seeds .   4.2 Overall Performance   We implement our CRE with LUKE and   IRE . Tab . 3 reports the RE results on the   TACRED , TACRED - Revisit , Re - TACRED , and Se-   mEval datasets . Our CREmethod improves the   F1 - macro scores of LUKE by 4.9 % on TACRED ,   4.0 % on TACRED - Revisit , 1.7 % on Re - TACRED ,   and 1.7 on SemEval , and improves IRE3076   by 1.2 % on TACRED , 1.4 % on TACRED - Revisit ,   0.9 % on Re - TACRED , and 1.8 % on SemEval . As   a result , our CREachieves substantial improve-   ments for LUKE and IRE , and enables   them to outperform the baseline methods . Addi-   tionally , we report the experimental results in terms   of F1 - micro scores in Tab . 3 , showing the improve-   ment from CREon LUKE by 2.6 % on TACRED ,   1.0 % on TACRED - Revisit , 0.7 % on Re - TACRED ,   and 1.0 % on SemEval . Overall , our CREmethod   improves the effectiveness of RE significantly in   terms of both F1 - macro and F1 - micro scores . The   above experimental results validate the effective-   ness and generalization of our proposed method .   Among the baseline debiasing methods , Resam-   ple , Focal , CFIE can not distill the entity bias in   an entity - aware manner like ours . Entity Mask   leads to the loss of information , while our CRE   enables RE models to focus on the main effects   of textual context without losing the entity infor-   mation . The superiority of CREhighlights the   importance of the causal inference based entity bias   analysis for debiasing RE , which compares tradi-   tional likelihood - based predictions and hypothe - sized counterfactual ones to produce debiased pre-   dictions . Besides , the proposed CREworks in   inference and thus can be employed on the pre-   vious already - trained models . In this way , CRE   serves as a model - agnostic approach to enhance RE   models without changing their training process .   4.3 Analysis on Entity Bias   Some work argues that RE models may rely on   the entity mentions to make relation predictions   instead of the textual context ( Zhang et al . , 2018 ;   Joshi et al . , 2020 ) . The empirical results in Fig . 3   validates this argument . Regardless of whether   the textual context exists or not , the baseline RE   model makes the same predictions given only entity   mentions on many instances . The entity bias can   mislead the RE models to make wrong predictions   when the relation implied by the entity mentions   does not exist in the textual context .   To evaluate whether RE models can generalize   well to particularly challenging instances where   relations implied by the entity mentions do not   exist in the textual context , we propose a filtered   evaluation setting , where we keep the test instances   having the entity bias different from their ground-   truth relations . In this setting , RE models can not   overly rely on the entity mentions for RE , since the   entity mentions no longer provide the superficial   and spurious clues for the ground - truth relations .   We present the evaluation results on the filtered   test set in Tab . 4 . Our CREmethod consis-   tently and substantially improves the effectiveness   of LUKE and IRE on the filtered test set and outper-   forms the baseline methods by a significant margin ,   which validates the effectiveness and generaliza-   tion of our method to mitigate the entity bias in the   challenging cases .   4.4 Evaluation on Fairness   According to Sweeney and Najafian ( 2019 ) , the   more imbalanced / skewed a prediction produced by   a trained model is , the more unfair opportunities it3077   gives over predefined categories , and the more un-   fairly discriminative the trained model is . We thus   follow previous work ( Xiang et al . , 2020 ; Sweeney   and Najafian , 2019 ; Qian et al . , 2021 ) to use the   metric – imbalance divergence – to evaluate how   imbalanced / skewed / unfair a prediction Pis :   D(P , U ) = JS(P∥U ) , ( 4 )   where D(·)is defined as the distance between P   and the uniform distribution U. Specifically , we   use the JSdivergence as the distance metric since   it is symmetric ( i.e. , JS(P∥U ) = JS(U∥P ) ) and   strictly scoped ( Fuglede and Topsoe , 2004 ) . Based   on this , to evaluate the entity bias of a trained RE   model , we average the following relative entity   mention imbalance ( REI ) measure over all the test-   ing instances containing whichever entity mentions :   REI = 1   E / summationdisplayD(P({x|e∈x∧x∈ D } ) , U),(5 )   where xis an input instance , Dis the testing set ,   P(x)is the prediction output , eis an entity men-   tion , and Eis the corpus of entity mentions . This   metric captures the distance between all predictions   and the fair uniform distribution U.   We follow the experimental settings in § 4.2 and   report the fairness test in Tab . 5 . The results   show that our CREmethod reduces the imbal-   ance metrics ( lower is better ) when employed onIRE significantly and consistently , indicat-   ing that it is helpful to mitigate the entity bias .   4.5 Ablation and Case Study   We conduct ablation studies on CREto empir-   ically examine the contribution of its main tech-   nical components . including the entity bias miti-   gation operation ( EBM ) , the label bias mitigation   operation ( LBM ) and the beam search for hyper-   parameters ( BSH ) .   We report the experimental results of the abla-   tion study in Tab . 6 . We observe that removing   ourCREcauses serious performance degrada-   tion . This provides evidence that using our coun-   terfactual framework for RE can explicitly miti-   gate biases to generalize better on unseen exam-   ples . Moreover , we observe that mitigating the two   types of biases is consistently helpful for RE . The   key reason is that the distilled label bias provides   an instance - agnostic offset and the distilled entity   bias provides an entity - aware one in the predic-   tion space , which makes the RE models focus on   extracting relations on the textual context without   losing the entity information . Meanwhile , the beam   search for hyper - parameters effectively finds two   dynamic scaling factors to amplify or shrink two   biases , making the biases be mitigated properly and   adaptively .   Tab . 7 gives a qualitative comparison example be-   tween CREand IRE on TACRED . The   results show that the state - of - the - art RE model   IRE returns the relations that do not exist   in the textual context between the considered enti-   ties . For example , given “ Bibi drew the ire of fellow   farmhands after a dispute in June 2009 , when they   refused to drink water she collected and she refused   their demands that she convert toIslam . ” , there is   no relation between Bibi andIslam exists in the   text but the baseline model believes that the rela-   tion between them is “ religion ” . The counterfactual   prediction can account for this disappointing result,3078   where given only the entity mentions Bibi andIs-   lam , the RE model returns the relation “ religion ”   without any textual context . This implies that the   model makes the prediction for the original input   relying on the entity mentions , which leads to the   wrong RE prediction . Our CREmethod distills   the biases through counterfactual predictions and   mitigates the biases to distinguish the main effects   from the textual context , which leads to the correct   predictions as shown in Tab . 7 .   Last but not least , we conduct experiments on the   fairness of different models , and present respective   results in the appendix .   5 Conclusion   We have designed a counterfactual analysis based   method named CREto debias RE . We distill the   entity bias and mitigate the distilled biases with the   help of our causal graph for RE , which is a road   map for analyzing the RE models . Based on the   counterfactual analysis , we can analyze the side-   effects of entity mentions in the RE and debias the   models in an entity - aware manner . Extensive exper-   iments demonstrate that our methods can improve   the effectiveness and generalization of RE . Future   work includes analyzing the effects of other factors   that can cause bias in natural language processing .   Acknowledgement   The authors would like to thank the anonymous   reviewers for their discussion and feedback .   Muhao Chen and Wenxuan Zhou are supported   by the National Science Foundation of United   States Grant IIS 2105329 , and by the DARPA MCSprogram under Contract No . N660011924033 with   the United States Office Of Naval Research . Ex-   cept for Muhao Chen and Wenxuan Zhou , this pa-   per is supported by NUS ODPRT Grant R252 - 000-   A81 - 133 and Singapore Ministry of Education Aca-   demic Research Fund Tier 3 under MOEs official   grant number MOE2017 - T3 - 1 - 007 .   References307930803081