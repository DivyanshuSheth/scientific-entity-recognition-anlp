  Karolina Sta ´ nczakEdoardo PontiLucas Torroba Hennigen   Ryan CotterellIsabelle AugensteinUniversity of CopenhagenMila / McGill UniversityMassachusetts Institute of TechnologyETH Zürich   ks@di.ku.dk augenstein@di.ku.dk edoardo-maria.ponti@mila.quebec   lucastor@mit.edu rcotterell@inf.ethz.ch   Abstract   The success of multilingual pre - trained   models is underpinned by their ability to learn   representations shared by multiple languages   even in absence of any explicit supervision .   However , it remains unclear how these models   learn to generalise across languages . In this   work , we conjecture that multilingual pre-   trained models can derive language - universal   abstractions about grammar . In particular ,   we investigate whether morphosyntactic   information is encoded in the same subset of   neurons in different languages . We conduct   the ﬁrst large - scale empirical study over 43   languages and 14 morphosyntactic categories   with a state - of - the - art neuron - level probe . Our   ﬁndings show that the cross - lingual overlap   between neurons is signiﬁcant , but its extent   may vary across categories and depends on   language proximity and pre - training data size .   1 Introduction   Massively multilingual pre - trained models ( Devlin   et al . , 2019 ; Conneau et al . , 2020 ; Liu et al . , 2020 ;   Xue et al . , 2021 , inter alia ) display an impressive   ability to transfer knowledge between languages as   well as to perform zero - shot learning ( Pires et al . ,   2019 ; Wu and Dredze , 2019 ; Nooralahzadeh et al . ,   2020 ; Hardalov et al . , 2022 , inter alia ) . Never-   theless , it remains unclear how pre - trained models   actually manage to learn multilingual representa-   tions despite the lack of an explicit signal through   parallel texts . Hitherto , many have speculated that   the overlap of sub - words between cognates in re-   lated languages plays a key role in the process of   multilingual generalisation ( Wu and Dredze , 2019 ;   Cao et al . , 2020 ; Pires et al . , 2019 ; Abend et al . ,   2015 ; Vuli ´ c et al . , 2020).Figure 1 : Percentages of neurons most associated with   a particular morphosyntactic category that overlap be-   tween pairs of languages . Colours in the plot refer to 2   models : m - BERT ( red ) and XLM - R - base ( blue ) .   In this work , we offer a concurrent hypothesis   to explain the multilingual abilities of various pre-   trained models ; namely , that they implicitly align   morphosyntactic markers that fulﬁl a similar gram-   matical function across languages , even in absence   of any lexical overlap . More concretely , we conjec-   ture that they employ the same subset of neurons   to encode the same morphosyntactic information   ( such as gender for nouns and mood for verbs ) .   To test the aforementioned hypothesis , we employ   Sta´nczak et al . ’s ( 2022 ) latent variable probe to   identify the relevant subset of neurons in each lan-   guage and then measure their cross - lingual overlap .   We experiment with two multilingual pre - trained   models , m - BERT ( Devlin et al . , 2019 ) and XLM-   R ( Conneau et al . , 2020 ) , probing them for mor-   phosyntactic information in 43 languages from Uni-   versal Dependencies ( Nivre et al . , 2017 ) . Based on   our results , we argue that pre - trained models do in-   deed develop a cross - lingually entangled represen-   tation of morphosyntax . We further note that , as the1589number of values of a morphosyntactic category in-   creases , cross - lingual alignment decreases . Finally ,   we ﬁnd that language pairs with high proximity ( in   the same genus or with similar typological features )   and with vast amounts of pre - training data tend to   exhibit more overlap between neurons . Identical   factors are known to affect also the empirical per-   formance of zero - shot cross - lingual transfer ( Wu   and Dredze , 2019 ) , which suggests a connection   between neuron overlap and transfer abilities .   2 Intrinsic Probing   Intrinsic probing aims to determine exactly which   dimensions in a representation , e.g. , those given   by m - BERT , encode a particular linguistic prop-   erty ( Dalvi et al . , 2019 ; Torroba Hennigen et al . ,   2020 ) . Formally , let Πbe the inventory of   values that some morphosyntactic category can   take in a particular language , for example Π =   { , , } for grammatical gender in Rus-   sian . Moreover , let D={(π , h)}be a   dataset of labelled embeddings such that π∈Π   andh∈R , wheredis the dimensionality of the   representation being considered , e.g. , d= 768 for   m - BERT . Our goal is to ﬁnd a subset of kneurons   C⊆D={1, ... ,d } , wheredis the total number   of dimensions in the representation being probed ,   that maximises some informativeness measure .   In this paper , we make use of a latent - variable   model recently proposed by Sta ´ nczak et al . ( 2022 )   for intrinsic probing . The idea is to train a probe   with latent variable Cindexing the subset of the   dimensions Dof the representation hthat should   be used to predict the property π :   p(π|h ) = /summationdisplayp(π|h , C)p(C ) ( 1 )   where we opt for a uniform prior p(C)andθare   the parameters of the probe .   Our goal is to learn the parameters θ . How-   ever , since the computation of Eq . ( 1 ) requires us   to marginalise over all subsets CofD , which is   intractable , we optimise a variational lower bound   to the log - likelihood :   L(θ ) = /summationdisplaylog / summationdisplayp / parenleftBig   π , C|h / parenrightBig   ( 2 )   ≥/summationdisplay / parenleftBigg   E / bracketleftBig   logp(π , C|h)/bracketrightBig   + H(q)/parenrightBiggwhere H(·)stands for the entropy of a distribution ,   andq(C)is a variational distribution over subsets   C.For this paper , we chose q(·)to correspond   to a Poisson sampling scheme ( Lohr , 2019 ) , which   models a subset as being sampled by subjecting   each dimension to an independent Bernoulli trial ,   whereφparameterises the probability of sampling   any given dimension .   Having trained the probe , all that remains is us-   ing it to identify the subset of dimensions that is   most informative about the morphosyntactic cate-   gory we are probing for . We do so by ﬁnding the   subsetCofkneurons maximising the posterior :   C= argmaxlogp(C|D ) ( 3 )   In practice , this combinatorial optimisation prob-   lem is intractable . Hence , we solve it using greedy   search .   3 Experimental Setup   We now describe the experimental methodology of   the paper , including the data , training procedure   and statistical testing .   Data . We select 43 treebanks from Universal De-   pendencies 2.1 ( UD ; Nivre et al . , 2017 ) , which   contain sentences annotated with morphosyntac-   tic information in a wide array of languages . Af-   terwards , we compute contextual representations   for every individual word in the treebanks using   multilingual BERT ( m - BERT - base ) and the base   and large versions of XLM - RoBERTa ( XLM - R-   base and XLM - R - large ) . We then associate each   word with its parts of speech and morphosyntac-   tic features , which are mapped to the UniMorph   schema ( Kirov et al . , 2018).The selected tree-   banks include all languages supported by both m-   BERT and XLM - R which are available in UD .   Rather than adopting the default UD splits , we   re - split word representations based on lemmata   ending up with disjoint vocabularies for the train ,   development , and test set . This prevents a probe   from achieving high performance by sheer memo-   rising . Moreover , for every category – language pair1590   ( e.g. , mood – Czech ) , we discard any lemma with   fewer than 20 tokens in its split .   Training . We ﬁrst train a probe for each mor-   phosyntactic category – language combination with   the objective in Eq . ( 2 ) . In line with established   practices in probing , we parameterise p(·)as a   linear layer followed by a softmax . Afterwards , we   identify the top- kmost informative neurons in the   last layer of m - BERT , XLM - R - base , and XLM - R-   large . Speciﬁcally , following Torroba Hennigen   et al . ( 2020 ) , we use the log - likelihood of the probe   on the test set as our greedy selection criterion . We   single out 50 dimensions for each combination of   morphosyntactic category and language .   Next , we measure the pairwise overlap in the   top - kmost informative dimensions between all   pairs of languages where a morphosyntactic cat - egory is expressed . This results in matrices such as   Fig . 2 , where the pair - wise percentages of overlap-   ping dimensions are visualised as a heat map .   Statistical Signiﬁcance . Suppose that two lan-   guages have m∈{1, ... ,k}overlapping neurons   when considering the top- kselected neurons for   each of them . To determine whether such overlap   is statistically signiﬁcant , we compute the proba-   bility of an overlap of at leastmneurons under the   null hypothesis that the sets of neurons are sampled   independently at random . We estimate these prob-   abilities with a permutation test . In this paper , we   set a threshold of α= 0.05for signiﬁcance .   Family - wise Error Correction . Finally , we use   Holm - Bonferroni ( Holm , 1979 ) family - wise error   correction . Hence , our threshold is appropriately   adjusted for multiple comparisons , which makes   incorrectly rejecting the null hypothesis less likely .   In particular , the individual permutation tests   are ordered in ascending order of their p - values .   The test with the smallest probability undergoes   the Holm – Bonferroni correction ( Holm , 1979 ) .   If already the ﬁrst test is not signiﬁcant , the   procedure stops ; otherwise , the test with the   second smallest p - value is corrected for a family   oft−1tests , where tdenotes the number of   conducted tests . The procedure stops either at the   ﬁrst non - signiﬁcant test or after iterating through   allp - values . This sequential approach guarantees   that the probability that we incorrectly reject one   or more of the hypotheses is at most α .   4 Results   We ﬁrst consider whether multilingual pre - trained   models develop a cross - lingually entangled notion   of morphosyntax : for this purpose , we measure   the overlap between subsets of neurons encod-   ing similar morphosyntactic categories across lan-   guages . Further , we debate whether the observed   patterns are dependent on various factors , such as   morphosyntactic category , language proximity , pre-   trained model , and pre - training data size .   Neuron Overlap . The matrices of pairwise over-   laps for each of the 14 categories , such as Fig . 2   for number and case , are reported in App . B. We   expand upon these results in two ways . First , we re-   port the cross - lingual distribution for each category   in Fig . 1 for m - BERT and XLM - R - base , and in an   equivalent plot comparing XLM - R - base and XLM-   R - large in Fig . 3 . Second , we calculate how many1591   overlaps are statistically signiﬁcant out of the total   number of pairwise comparisons in Tab . 1 . From   the above results , it emerges that ≈20 % of neurons   among the top- 50most informative ones overlap   on average , but this number may vary dramatically   across categories .   Morphosyntactic Categories . Based on Tab . 1 ,   signiﬁcant overlap is particularly accentuated in   speciﬁc categories , such as comparison , polarity ,   and number . However , neurons for other categories   such as mood , aspect , and case are shared by only   a handful of language pairs despite the high num-   ber of comparisons . This ﬁnding may be partially   explained by the different number of values each   category can take . Hence , we test whether there   is a correlation between this number and average   cross - lingual overlap in Fig . 5a . As expected , we   generally ﬁnd negative correlation coefﬁcients —   prominent exceptions being number and person .   As the inventory of values of a category grows ,   cross - lingual alignment becomes harder .   Language Proximity . Moreover , we investigate   whether language proximity , in terms of both lan-   guage family and typological features , bears any   relationship with the neuron overlap for any partic-   ular pair . In Fig . 4 , we plot pairwise similarities   with languages within the same genus ( e.g. , Baltic )   against those outside . From the distribution of the   dots , we can extrapolate that sharing of neurons is   more likely to occur between languages in the same   genus . This is further corroborated by the language   groupings emerging in the matrices of App . B.   In Fig . 5b , we also measure the correlation be-   Deﬁniteness 0.11 0.22 0.13 45   Comparison 0.20 0.90 0.50 10   Possession 0.00 0.00 0.00 1   Aspect 0.03 0.10 0.09 153   Polarity 0.33 0.67 0.33 3   Number 0.40 0.51 0.74 666   Animacy 0.14 0.57 0.32 28   Mood 0.00 0.07 0.05 105   Gender 0.15 0.32 0.19 378   Person 0.08 0.25 0.13 276   POS 0.04 0.27 0.70 861   Case 0.10 0.18 0.17 300   Tense 0.08 0.23 0.12 325   Finiteness 0.09 0.18 0.09 45   tween neuron overlap and similarity of syntactic   typological features based on Littell et al . ( 2017 ) .   While correlation coefﬁcients are mostly positive   ( with the exception of polarity ) , we remark that the   patterns are strongly inﬂuenced by whether a cat-   egory is typical for a speciﬁc genus . For instance ,   correlation is highest for animacy , a category al-   most exclusive to Slavic languages in our sample .   Pre - trained Models . Afterwards , we determine   whether the 3 models under consideration reveal   different patterns . Comparing m - BERT and XLM-   R - base in Fig . 1 , we ﬁnd that , on average , XLM - R-   base tends to share more neurons when encoding   particular morphosyntactic attributes . Moreover ,   comparing XLM - R - base to XLM - R - large in Fig . 3   suggests that more neurons are shared in the former   than in the latter .   Altogether , these results seem to suggest that   the presence of additional training data engenders   cross - lingual entanglement , but increasing model   size incentivises morphosyntactic information to   be allocated to different subsets of neurons . We   conjecture that this may be best viewed from the   lens of compression : if model size is a bottleneck,1592   then , to attain good performance across many lan-   guages , a model is forced to learn cross - lingual   abstractions that can be reused .   Pre - training Data Size . Finally , we assess the   effect of pre - training data sizefor neuron overlap .   According to Fig . 5c , their correlation is very high .   We explain this phenomenon with the fact that more   data yields higher - quality ( and as a consequence ,   more entangled ) multilingual representations .   5 Conclusions   In this paper , we hypothesise that the ability of   multilingual models to generalise across languages   results from cross - lingually entangled representa-   tions , where the same subsets of neurons encode   universal morphosyntactic information . We vali-   date this claim with a large - scale empirical study   on 43 languages and 3 models , m - BERT , XLM-   R - base , and XLM - R - large . We conclude that the   overlap is statistically signiﬁcant for a notable   amount of language pairs for the considered at-   tributes . However , the extent of the overlap varies   across morphosyntactic categories and tends to be   lower for categories with large inventories of pos-   sible values . Moreover , we ﬁnd that neuron sub-   sets are shared mostly between languages in the   same genus or with similar typological features . Fi-   nally , we discover that the overlap of each language   grows proportionally to its pre - training data size ,   but it also decreases in larger model architectures .   Given that this implicit morphosyntactic align-   ment may affect the transfer capabilities of pre-   trained models , we speculate that , in future work ,   artiﬁcially encouraging a tighter neuron over-   lap might facilitate zero - shot cross - lingual infer-   ence to low - resource and typologically distant lan-   guages(Zhao et al . , 2021 ) .   Ethics Statement   The authors foresee no ethical concerns with the   work presented in this paper .   Acknowledgments   This work is mostly funded by Independent Re-   search Fund Denmark under grant agreement num-   ber 9130 - 00092B , as well as by a project grant   from the Swedish Research Council under grant   agreement number 2019 - 04129 . Lucas Torroba   Hennigen acknowledges funding from the Michael   Athans Fellowship fund . Ryan Cotterell acknowl-   edges support from the Swiss National Science   Foundation ( SNSF ) as part of the “ The Forgotten   Role of Inductive Bias in Interpretability ” project.1593References15941595A Probed Property – Language Pairs   Afro - Asiatic   •ara ( Arabic ) : Gender , V oice , Mood , Part of   Speech , Aspect , Person , Number , Case , Deﬁ-   niteness   •heb ( Hebrew ) : Part of Speech , Number ,   Tense , Person , V oice   Austroasiatic   •vie ( Vietnamese ) : Part of Speech   Dravidian   •tam ( Tamil ) : Part of Speech , Number , Gen-   der , Case , Person , Finiteness , Tense   Indo - European   •afr ( Afrikaans ) : Part of Speech , Number ,   Tense   •bel ( Berlarusian ) : Part of Speech , Tense ,   Number , Aspect , Finiteness , V oice , Gender ,   Animacy , Case , Person   •bul ( Bulgarian ) : Part of Speech , Deﬁnite-   ness , Gender , Number , Mood , Tense , Person ,   V oice , Comparison   •cat ( Catalan ) : Gender , Number , Part of   Speech , Tense , Mood , Person , Aspect   •ces ( Czech ) : Part of Speech , Number , Case ,   Comparison , Gender , Mood , Person , Tense ,   Aspect , Polarity , Animacy , Possession , V oice   •dan ( Danish ) : Part of Speech , Number , Gen-   der , Deﬁniteness , V oice , Tense , Mood , Com-   parison   •deu ( German ) : Part of Speech , Case , Num-   ber , Tense , Person , Comparison   •ell ( Greek ) : Part of Speech , Case , Gender ,   Number , Finiteness , Person , Tense , Aspect ,   Mood , V oice , Comparison   •eng ( English ) : Part of Speech , Number ,   Tense , Case , Comparison   •fas ( Persian ) : Number , Part of Speech , Tense ,   Person , Mood , Comparison   •fra ( French ) : Part of Speech , Number , Gen-   der , Tense , Mood , Person , Polarity , Aspect   •gle ( Irish ) : Tense , Mood , Part of Speech ,   Number , Person , Gender , Case   •glg ( Galician ) : Part of Speech   •hin ( Hindi ) : Person , Case , Part of Speech ,   Number , Gender , V oice , Aspect , Mood , Finite-   ness , Politeness   •hrv ( Croatian ) : Case , Gender , Number , Part   of Speech , Person , Finiteness , Mood , Tense ,   Animacy , Deﬁniteness , Comparison , V oice   •ita ( Italian ) : Part of Speech , Number , Gender ,   Person , Mood , Tense , Aspect•lat ( Latin ) : Part of Speech , Number , Gender ,   Case , Tense , Person , Mood , Aspect , Compari-   son   •lav ( Latvian ) : Part of Speech , Case , Number ,   Tense , Mood , Person , Gender , Deﬁniteness ,   Aspect , Comparison , V oice   •lit ( Lithuanian ) : Tense , V oice , Number , Part   of Speech , Finiteness , Mood , Polarity , Person ,   Gender , Case , Deﬁniteness   •mar ( Marathi ) : Case , Gender , Number , Part   of Speech , Person , Aspect , Tense , Finiteness   •nld ( Dutch ) : Person , Part of Speech , Number ,   Gender , Finiteness , Tense , Case , Comparison   •pol ( Polish ) : Part of Speech , Case , Number ,   Animacy , Gender , Aspect , Tense , Person , Po-   larity , V oice   •por ( Portuguese ) : Part of Speech , Person ,   Mood , Number , Tense , Gender , Aspect   •ron ( Romanian ) : Deﬁniteness , Number , Part   of Speech , Person , Aspect , Mood , Case , Gen-   der , Tense   •rus ( Russian ) : Part of Speech , Case , Gender ,   Number , Animacy , Tense , Finiteness , Aspect ,   Person , V oice , Comparison   •slk ( Slovak ) : Part of Speech , Gender , Case ,   Number , Aspect , Polarity , Tense , V oice , Ani-   macy , Finiteness , Person , Mood , Comparison   •slv ( Slovenian ) : Number , Gender , Part of   Speech , Case , Mood , Person , Finiteness , As-   pect , Animacy , Deﬁniteness , Comparison   •spa ( Spanish ) : Part of Speech , Tense , Aspect ,   Mood , Number , Person , Gender   •srp ( Serbian ) : Number , Part of Speech , Gen-   der , Case , Person , Tense , Deﬁniteness , Ani-   macy , Comparison   •swe ( Swedish ) : Part of Speech , Gender , Num-   ber , Deﬁniteness , Case , Tense , Mood , V oice ,   Comparison   •ukr ( Ukrainian ) : Case , Number , Part of   Speech , Gender , Tense , Animacy , Person , As-   pect , V oice , Comparison   •urd ( Urdu ) : Case , Number , Part of Speech ,   Person , Finiteness , V oice , Mood , Politeness ,   Aspect   Japonic   •jpn ( Japanese ) : Part of Speech   Language isolate   •eus ( Basque ) : Part of Speech , Case , Animacy ,   Deﬁniteness , Number , Argument Marking ,   Aspect , Comparison   Sino - Tibetan1596•zho ( Chinese ) : Part of Speech   Turkic   •tur ( Turkish ) : Case , Number , Part of Speech ,   Aspect , Person , Mood , Tense , Polarity , Pos-   session , Politeness   Uralic   •est ( Estonian ) : Part of Speech , Mood , Finite-   ness , Tense , V oice , Number , Person , Case   •ﬁn ( Finnish ) : Part of Speech , Case , Num-   ber , Mood , Person , V oice , Tense , Possession ,   Comparison   B Pairwise Overlap by Morphosyntactic   Category15971598