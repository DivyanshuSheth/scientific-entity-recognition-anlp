  Yuxuan Gu , Xiaocheng Feng , Sicheng Ma , Lingyuan Zhang , Heng Gong , Bing QinHarbin Institute of TechnologyPeng Cheng Laboratory   { yxgu,xcfeng,scma,lyzhang,hgong,bqin}@ir.hit.edu.cn   Abstract   Multi - aspect controllable text generation is a   more challenging and practical task than single-   aspect control . Existing methods achieve com-   plex multi - aspect control by fusing multiple   controllers learned from single - aspect , but suf-   fer from attribute degeneration caused by the   mutual interference of these controllers . To   address this , we provide observations on at-   tribute fusion from a distributional perspective   and propose to directly search for the intersec-   tion areas of multiple attribute distributions as   their combination for generation . Our method   first estimates the attribute space with an au-   toencoder structure . Afterward , we iteratively   approach the intersections by jointly minimiz-   ing distances to points representing different   attributes . Finally , we map them to attribute-   relevant sentences with a prefix - tuning - based   decoder . Experiments on the three - aspect con-   trol task , including sentiment , topic , and detox-   ification aspects , reveal that our method outper-   forms several strong baselines on attribute rele-   vance and text quality and achieves the SOTA .   Further analysis also supplies some explanatory   support for the effectiveness of our approach .   1 Introduction   Controllable text generation is a challenging task in   natural language generation , which aims to gener-   ate fluent text with desired attributes . Pilot studies   attempt single - aspect control by directly finetuning   a conditional model ( Ziegler et al . , 2019 ; Keskar   et al . , 2019 ) , or turn to methods with language mod-   els fixed ( Dathathri et al . , 2020 ) due to the high cost   of large - scale pre - trained language models ( Brown   et al . , 2020a ; Zhang et al . , 2022 ) .   Recent works focus on a more practical setting ,   multi - aspectcontrollable text generation , with ex-   isting approaches mainly divided into three tech - Figure 1 : Probability space of attributes . Orange back-   ground denotes the estimated distribution over natural   language . Blue and green areas represent distributions   over sentences containing attributes from two different   aspects , respectively . The darker region means a higher   probability in the space . The shaded are distributional   centers , the areas with the highest probability density .   nical routes : weighted decoding ( Dathathri et al . ,   2020 ; Krause et al . , 2021 ) , multi - objective opti-   mization ( Kumar et al . , 2021 ; Mireshghallah et al . ,   2022 ) , and prefix - tuning ( Qian et al . , 2022 ) , which   explore ways to combine controllers learned from   single - aspect and apply them to a fixed language   model yet suffering from attribute degeneration   caused by the mutual interference of controllers .   We provide a distributional perspective to ob-   serve and alleviate this problem . In the current   text generation paradigm , a language model forms   an estimated distribution over sentences with train-   ing data amounted to sampling from natural lan-   guage distribution ( Pillutla et al . , 2021 ) . For single-   aspect control , these methods train a classifier or   a prefix for each attribute independently , which   is regarded as appraising a center of distribution   over attribute - relevant sentences , before biasing the   language model ’s distribution to this center . Cor-   respondingly , when generalizing to multi - aspect   control , their fusion strategy is directly obtaining   interpolation or average of these centers , which   may be too straightforward . As shown in Figure 1 ,   theinterpolation point denotes the position they   acquired after combining multiple centers in the   probability space . And the intersection represents1023where oracle sentences that simultaneously satisfy   multiple attributes lie . In the left part of Figure 1 ,   when distributions of attributes is symmetric , the   interpolation point is indeed within the intersection   area . However , there could be a mismatch between   the interpolation point and intersection . For exam-   ple , as illustrated in the right part of Figure 1 , two   skewed distributions intersect on the tails , leaving   the interpolation point out of the intersection area   and thus making it lack the ability to express all   desired attributes together .   In this paper , different from approximating the   intersection area with the interpolation point , we   propose a strategy for directly acquiring the inter-   section . We first deploy an autoencoder structure   to map attribute - relevant sentences to latent repre-   sentations constituting an estimated attribute space .   With our specially designed constraints , this space   can model relationships among attributes . After-   ward , we provide an effective intersection search-   ing algorithm that can walk around the long tail   regions in distributions of all desired attributes and   iteratively find where they combine more tightly .   Finally , we utilize a prefix - tuning - based decoder to   construct sentences from the searched intersection .   We experiment on three - aspect control with two   attributes from the sentiment aspect , four from the   topic , and one from detoxification , with datasets   IMDb movie reviews ( Maas et al . , 2011 ) , AGNews   ( Zhang et al . , 2015 ) , and Jigsaw Toxic Comment   Classification Challenge Dataset , respectively . We   evaluate the relevance of each attribute indepen-   dently and calculate their average as the final rel-   evance metric . Besides , we assess the text quality   with perplexity and distinctness concerning fluency   and diversity . Results show that our method can   significantly outperform strong baseline models   on multi - aspect control . Furthermore , we find out   in our analytical experiments that our intuitive as-   sumptions fit well with our observation . The main   contributions are as follows :   •We propose a distributional perspective that   models multi - aspect control more practically .   •We provide a method that directly searches for   intersections in the attribute space and gener-   ates sentences with desired attributes .   •We experimentally reveal the effectiveness of   our method on multi - aspect control compared   to strong baselines and achieve the SOTA.2 Related Work   Variational autoencoders are often used for control-   lable text generation in early work ( Hu et al . , 2017 ;   Duan et al . , 2020 ; Mai et al . , 2020 ) where they   spend a lot of effort into improving text fluency .   The prosperity of large - scale pre - trained language   models ( Radford et al . , 2019 ) provides more ex-   ploration directions for attribute control such as   fine - tuning ( Ficler and Goldberg , 2017 ; Ziegler   et al . , 2019 ; Keskar et al . , 2019 ) . Recent work   has made gratifying progress on single - aspect con-   trol ( Krause et al . , 2021 ) , leading studies gradually   turn to a more difficult task , multi - aspect control ,   including the following three main approaches .   Weighted Decoding As the scale of language   models increases rapidly , weighted decoding   ( Dathathri et al . , 2020 ; Krause et al . , 2021 ; Yang   and Klein , 2021 ; Liu et al . , 2021a ; Gu et al . , 2022 )   becomes a simple and practical choice . It is a   framework that decomposes the probability of sen-   tences conditioned on attributes into a language   model and a classifier with the bayesian rule di-   rectly at decoding time . When handling multi-   aspect control , it can be easily generalized by inter-   polating classifiers ( Lin and Riedl , 2021 ) .   Multi - Objective Optimization The controllable   text generation task is naturally a multi - objective   optimization problem when regarding its decod-   ing process as an optimization objective . Some   approaches , such as DGC ( Khalifa et al . , 2020 ) ,   Mix&Match ( Mireshghallah et al . , 2022 ) , and   COLD Decoding ( Qin et al . , 2022 ) , adopt Energy-   based Models ( LeCun et al . , 2006 ) to blend mul-   tiple objectives . Others like MUCOCO ( Kumar   et al . , 2021 ) convert the optimization objectives of   multi - aspect control to inequality constraints and   thereby apply the lagrange multiplier method for   this constrained optimization problem .   Prefix - Tuning GPT-3 ( Brown et al . , 2020b ) pro-   vides a new paradigm named prompt - based learn-   ing ( Liu et al . , 2021b ) , which is able to perform   few - shot learning on downstream tasks . Prefix-   Tuning ( Li and Liang , 2021 ) leverages the learned   lightweight prompts to trigger the conditional gen-   eration capability of the language model . Applying   Prefix - Tuning to multi - aspect controllable text gen-   eration ( Yu et al . , 2021 ; Qian et al . , 2022 ; Carlsson   et al . , 2022 ; Yang et al . , 2022 ) can be regarded as   optimizing on multi - objective implicitly.1024   3 Methodology   In this section , we first introduce the motivation   and overall process of our method , after which we   describe each module in detail .   3.1 Overview   As illustrated in Figure 2 , our method mainly re-   volves around the attribute space including estimat-   ing the attribute space , searching for intersections ,   and mapping intersections to sentences .   Firstly , we aim to construct an attribute space us-   ing sampled sentences to estimate the real space as   accurately as possible . We employ an autoencoder   structure with the latent representations denoting   points that constitute our estimated attribute space .   To ensure that our estimated space reliably models   the attributes , such as their probability distributions   and relationships between different attributes , we   further attach three constraints to the representa-   tion . ( I ) Reconstruction Loss Laims to bridge   the gap between points in attribute space and nat-   ural attribute - relevant sentences , which is recover-   ing attributes reflected by contents . ( II ) Attribute   Classification Loss Lforces the encoder to fo-   cus more on capturing attributes by distinguishing   points of different attributes from the same aspect .   ( III ) Aspect Gap Loss Lpenalizes the discrep-   ancy of aspects , which is caused by the domain gap   among different data sources for different aspects .   Inspired by the feature alignment ( Pan et al . , 2010 ) ,   we minimize the distances between distributional   centers of each two aspects .   The second step aims to search for an intersec - tion area of desired attributes . If the intersection   area exists , a point in the area satisfies that neigh-   bor points appearing in a tiny surrounding region   should cover all required attributes . Inspired by this   neighborhood ideology , we design an algorithm   that iteratively approaches an area where these at-   tributes bind more tightly . The third step maps   our searched intersection to a Prefix that activates   the language model to generate attribute - relevant   sentences . To make the language model less sensi-   tive to slight variations , we sample a perturbation   vector from a multivariate gaussian distribution .   3.2 Estimating Attribute Space   Given|A|aspects A=/braceleftbig   A , · · · , A / bracerightbig   with each   comprising |A|attributes / braceleftig   a , · · · , a / bracerightig   , Iis   an index set representing the identifiers of all sen-   tences with attribute ain the training data . We   have I=/uniontextI , I=/uniontextI , where Iis the in-   dices of all sentences with any attribute in aspect   AandIis the indices of the entire training data .   We encode sentences { X}from all aspects Ato   representations { H}with unified mapping param-   etersϕ:H = Encode(X ) , where i∈I.   Reconstruction Loss LAs in the top of Figure   2,Lis computed in the same way as the autore-   gressive loss of pre - trained language model p :   L=−/summationdisplaylogp(X|Prefix )   Prefix = MLP(H+λε ) , ε∼ N(0,I),(1)1025where Xhere is a sample sentence from the en-   tire training set , i.e. , i∈I. Besides , ε , with a   scaling factor λ , is a perturbation vector sampled   from a multivariate gaussian distribution N(0,I )   for robustness when reconstructing . The multi-   layer perceptron MLPwill map perturbed Hto   Prefixthat can activate the language model to gen-   erate text with desired attributes . It ’s worth noting   that our primary goal is to recover attributes , which   means Ldoes not need and preferably does not   converge too well while maintaining text fluency .   Attribute Classification Loss LWe force the   encoder to focus on attributes by Lin the way :   L=−/summationdisplay / summationdisplay / summationdisplaylogp(a|H).(2 )   Given sentence representation , pis a classifier   that distinguish attributes / braceleftbig   a / bracerightbig   from aspect A   with parameter π .   Aspect Gap Loss LWe penalize the discrep-   ancy between distributional centers by :   L=/summationdisplay / vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble / summationdisplayH   |I|−/summationdisplayH   |I|/vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble / vextenddouble,(3 )   which are Euler distances between every two dis-   tinct distributional centers . When generalizing to   a larger scale of aspects , it is relatively expensive   to calculate averages over the entire dataset each   time the model is updated . We calculate this loss   in practice using a batch - level approximation . We   assign each aspect a memory unit to store the lat-   est representation of the aspect ’s estimated center .   Each time processing a batch of sentences from   one aspect , we take the average of their representa-   tions as the center and sum up the Euler distances   to centers of other aspects in the memory , which   is the estimated L. Then , we update the memory   unit of this aspect to the latest .   During the training stage , our loss function is :   L = wL+wL+wL. ( 4 )   It ’s worth noting that we only update parameters   ϕ,θ , and{π}for the encoder , the MLP layer , and   the classifier heads , respectively .   3.3 Intersection of Attributes   Suppose there is an intersection point , denoted as   ˜H , located within the intersection region of at-   tributes / braceleftbig   a , a , · · · , a / bracerightbig   fromNdifferent as - Algorithm 1 Intersection Searching   Input : H , i∈/uniontextIfrom Nattributes   ωweight of each attribute   Output : Intersection of Nattributes : ˜HInitialize Mcandidates : { ˜H}Iterate Stimesforsin[0 , S−1]do formin[1 , M]do ˜H←0 fortin[1 , N]do H←Nearest(˜H,/braceleftbig   H , i∈I / bracerightbig   ) ˜H←˜H+ωmean ( H ) end for ˜H←˜H//summationtextω end forend for˜H←Select ( { ˜H } )   pects , where ais the αth attribute in aspect A.   Our algorithm 1 approximates the ˜Hby iteratively   approaching a most balanced point with nearest   neighbors from different attributes . First , we ini-   tialize the candidates { ˜H}by randomly sampling   points in the attribute space , calculating their dis-   tance to the closest point of each attribute a , and   selecting the top Msamples with the smallest av-   erage distance to all attributes . At each iteration   s , we choose the top - Knearest points to ˜Hfor   each attribute and update ˜Husing the weighted   average of these points . It is worth mentioning that   ωis the weight used to balance attributes or favor   some specifically , and a negative value of ωcan   even move away from a particular one . Finally , we   select the best candidate from the last iteration S ,   which is expected to be in the intersection region ,   i.e. , a representation related to multiple attributes .   3.4 Generation with Intersections   As illustrated in the right bottom of Figure 2 , we   convert the representation ˜Hobtained from the in-   tersection area directly to the Prefix with MLPand   let the language model generate multi - attributed   sentence Yfrom input Xas :   Y= arg maxp(y|Prefix;X )   Prefix = MLP(˜H+λε ) , ε∼ N(0,I).(5)1026When generating several attribute - relevant sen-   tences for one attribute combination , we only need   to calculate the intersection for it once .   4 Experiment   In this section , we demonstrate the effectiveness   of our method on three - aspect control , including   sentiment , topic , and detoxification .   4.1 Multi - Aspect Control Task   The datasets we use are the same as GeDi ( Krause   et al . , 2021 ) and Contrastive Prefix ( Qian et al . ,   2022 ) . To balance the data scale across all as-   pects , we randomly sample 10k sentences from   each dataset that is less than the number of samples   GeDi uses , with each attribute equally dividing this   amount . We use the IMDb movie reviews ( Maas   et al . , 2011 ) , the AGNews dataset ( Zhang et al . ,   2015 ) , and the Jigsaw Toxic Comment Classifica-   tion Challenge Datasetfor sentiment , topic and   detoxification aspects , respectively .   The prompts used for text generation are the   same as those used in the PPLM ( Dathathri et al . ,   2020 ) , with 20 from its bag - of - words experiment   and 15 from its discriminator experiment . We ex-   periment with 8combinations of the 3aspects with   2sentiments ×4topics ×1detoxification and   generate 5completions for each combination and   each prompt . Totally , each model will generate   35×2×4×1×5 = 1400 sentences . It is worth   noting that we do not specifically use prompts that   induce the language model to generate toxic text ,   making detoxification easier to improve .   To measure the performance on different aspects ,   we compute the attribute relevance . We finetune a   DeBERTa ( He et al . , 2021b , a ) classifier on the Yelp   dataset ( Zhang et al . , 2015 ) for sentiment aspect   and a classifier for topic utilizing all its remaining   data not used during training . We evaluate the non-   toxicity with the Google Perspective API . The   final performance of a model is determined by the   average of these three attribute relevance scores in-   troduced above . We also use two auxiliary metrics   to measure text quality . One is perplexity calcu-   lated by GPT2 - large following Contrastive Prefix   ( Qian et al . , 2022 ) . To ensure that models are not   insensitive to changes in different prefixes , we cal-   culate the Distinctness ( Li et al . , 2016 ) of sentencesgenerated from different prefixes and average the   1 - gram , 2 - grams , and 3 - grams distinct scores for   simplicity . Moreover , we conduct human evalua-   tion with sentences generated by different models   shuffled . Each sentence is rated by three profes-   sional evaluators for 3 attribute relevance and text   fluency . Evaluators rate each item on a scale of 1   to 5 , with 5 representing text highly related to the   desired attribute or very fluent .   4.2 Baselines   ( I)Weighted Decoding : PPLM ( Dathathri et al . ,   2020 ) biases the language model with gradients   back - propagated from trained classifiers . GeDi   ( Krause et al . , 2021 ) influences the decoding pro-   cess with token probabilities conditioned on at-   tributes . ( II ) Multi - objective Optimization : MU-   COCO ( Kumar et al . , 2021 ) regards the decod-   ing process as a constrained optimization problem ,   where the language model is the objective func-   tion and attributes are constraints . Mix&Match   ( Mireshghallah et al . , 2022 ) controls attributes with   energy - based models and generates sentences by   masking , sampling , and correcting . ( III ) Prefix –   Tuning : Contrastive Prefix ( Qian et al . , 2022 )   utilizes prefixes to activate the language model to   generate attribute - relevant sentences by concatena-   tion or semi - supervision .   4.3 Results   According to the automatic evaluation results in   Table 1 , under the multi - aspect setting , we group   models based on their type of methods in chrono-   logical order . In addition , we demonstrate their   standard deviations , which reflect the stability of   models among different attribute combinations .   For weighted decoding , GeDi uses more power-   ful classifiers than PPLM and performs better on   attribute relevance , stability to different combina-   tions , and distinctness while correspondingly worse   on perplexity . Multi - objective optimization meth-   ods achieve a favorable performance on attribute   relevance while MUCOCO explodes on perplexity   due to its non - autoregressive paradigm not being   suitable for generating from scratch . Performance   of semi - supervised Contrastive Prefix is similar to   GeDi , except for lack of diversity .   Our method performs best on average attribute-   related metrics , with at least a 7.3%significant im-   provement over existing baselines . Our advances   mainly come from sentiment and topic aspects ,   with no less than 13.9%and10.3%each . Al-1027   though our model is not the best on detoxification ,   it is the most balanced and stable according to the   lowest standard deviation on average , 10.9 . As a   prefix - tuning - based method inducing the language   model without direct modification , which is nat-   urally good at text fluency , we perform well on   perplexity and inherit the performance on diversity .   Furthermore , we conduct ablation on aspect gap   lossLand attribute classification loss Lsepa-   rately . On the one hand , without L , we can not   alleviate the bias in different training datasets , mak-   ing it hard to search for the intersection areas . Since   training sentences of sentiment and topic aspects   are mainly non - toxic , our model focuses more on   detoxification rather than struggling for the other   two , leading to considerable declines on their rele-   vance while slight improvements on detoxification .   Besides , as the distance among sample points from   different aspects in the attribute space increases ,   our model will generate sentences mapped from far   more sparse areas , leading to a small decrease on   fluency and a subtle increase on diversity . On the   other hand , without L , our attribute space will to-   tally collapse . The relevance of sentiment and topic   drops drastically while the non - toxicity boosts be-   cause model can hardly distinguish representations   of different attributes in the same aspect and focus   on relatively more effortless detoxification . Worse   still , without distinct representations , our model is   required to recover different sentences from similar   ones , leading to oscillation in training and hardly   generating complete text when inferencing .   Results of human evaluation are in Table 2 , with   inter - annotator agreement being 0.36 in Fleiss ’ κ .   We evaluate GeDi , Contrastive Prefix , and our   method and observe that the results are consistent   with the automatic ones on sentiment and topic   relevance . The performance of models on detoxi-   fication is high and relatively similar , making the   automatic results different from the manual ones   where the annotators believe that our model does   a better job than baselines . Since perplexity is rel-   atively unreliable , the manually measured fluency   of GeDi is much better than that of the Contrastive   Prefix . And our method achieves the best fluency .   5 Analysis   5.1 Effect of Different Attributes and their   Combinations   We illustrate the detailed results of each attribute   and their combinations in Table 3 . GeDi and Prefix-   tuning perform differently in single - aspect control ,   each with its advantages . For example , GeDi is   dedicated to negative with93.9%relevance , while   Prefix - tuning is good at positive with 90.6%rel-   evance . When dealing with multi - aspect control ,   they inherit such imbalanced characteristics , with   average relevance of 91.1%and79.1 % , respec-   tively . In addition , the baselines decrease corre-   spondingly in the average relevance of each at-   tribute compared to single - aspect , ranging from   0.7to33.0 . On average , our model outperforms   other baselines on attribute metrics ( Table 1 ) . In   detail , our model performs competitively for most   attributes compared to another prefix - tuning - based   model , Contrastive Prefix . Especially , on attributes   likebusiness andsci / tech , our model significantly   improves over another prefix - tuning - based method   on multi - aspect control and can even surpass it1028   under single - aspect control .   In addition , correlations between attributes vary   widely , as in Table 3 . For example , generally , pos-   itive fits well with non - toxic while negative leads   to a massive drop in non - toxicity , which is consis-   tent with the intuition that one can hardly praise   people and offend them simultaneously . Besides ,   world andbusiness news are often reported nega-   tively , such as war , famine , inflation , etc . , making it   challenging to combine them with positive . When   attributes are not closely correlated , which means   that few natural sentences possess these attributes   together , our method is more likely to capture such   a rarely occurred incident and magnify their fre-   quency . Take business as an example . It is ef-   fortless to achieve a fine attribute relevance when   performing single - aspect control on business , with   GeDi achieving 75.7and Prefix obtaining 93.5 . Af-   ter attaching positive tobusiness , baseline models   will suffer from a decline due to their weak corre-   lation , where GeDi and Contrastive Prefix drop to   54.3and41.7 , respectively . In contrast , our method   can alleviate this problem by retrieving this unusual   co - occurrence in the training sentences and recov-   ering it from the attribute space , achieving a per-   formance of 91.7 , which is close to single - aspect   control . When combining business with negative ,   which is a relatively common combination , there is   still some decrease for baseline models . On the con-   trary , our method can even obtain the performance   of96.7that surpasses single - aspect control .   5.2 Estimated Attribute Space   We demonstrate part of our estimated attribute   space in Figure 3 with four attributes : positive , neg-   ative , sports , and sci / tech from sentiment and topic   aspects . We project the high - dimensional space1029   to 2D with Principal Component Analysis ( PCA ) .   Consistent with our hypothesis , distributions of   sports andsci / tech are asymmetric and the inter-   sections lie in the sparse edges of attributes ’ dis-   tribution . In addition , we project the intersections   searched by the baseline ’s strategy and ours , respec-   tively . For positive -sci / tech andnegative -sci / tech   pairs , the combinations are relatively tight , making   it easy to find intersections . However , intersection   areas for positive -sports andnegative -sports pairs   are considerably sparse . As shown in enlarged area ,   the baseline searched intersection is at the midpoint   of the two distributional centers , but this location   is not where the attributes intersect . On the con-   trary , our method can find an intersection in such a   sparse region , making various points from the two   different attributes appear simultaneously in its tiny   surrounding area . It worth noting that positive and   negative appear to intersect in this projection be-   cause they are close in the high - dimensional space .   But there is actually no intersection if only project-   ing these two attributes in § A.3 .   5.3 Effect of K   We analyze the variation of Kin the intersection   searching algorithm and demonstrate the results in   Table 4 . Our model reaches a critical point when   Kis 200 , and the performance is optimal this time .   On the one hand , as the value of Kgradually in-   creases , our method pays less attention to regions   where samples are fewer while attributes combine   more tightly , and the performance decreases accord-   ingly . When Kreaches 5k , our method degenerates   into a plain prefix - tuning model , which treats in-   tersection as the midpoint of distributional centers .   Its performance is similar and slightly inferior to   the concatenation version of Contrastive Prefix in   Table 1 . On the other hand , smaller Kleads to   suboptimal performance since the effect of noise   becomes non - negligible in training data . When K   is less than 10 , our model will be very unstable .   5.4 Distribution of Attributes   We project sample points to 2D by PCA , with each   attribute projected independently . As in Figure 4 ,   we display a scatterplot of World and conduct a   Gaussian kernel density estimation to visualize its   probability distribution . The darker area denotes   a higher probability , where more representation   points of oracle sentences gather . And the region   annotated by a red ellipse is the estimated distri-   butional center . As in the plot , the distribution   of World is significantly asymmetric as the center   lies in the top part , with the bottom being a sparse   long tail . In addition , the distribution is even non-   convex with an isolated cluster in the lower right   corner . This observation supports our hypothesis   that the practical distributions of attributes are far   more complex than symmetric distributions such   as Gaussian distribution . Besides , we plot the dis-   tribution of other attributes in the § A.1 .   6 Discussion on Distributional Lens   Pilot work such as DGC ( Khalifa et al . , 2020 ) es-   timates the language distribution with an energy-   based model and optimizes this distribution to sat-   isfy constraints by approaching the constraints man-   ifold . Recent distributional approaches like COLD   Decoding ( Qin et al . , 2022 ) and MuCoLa ( Kumar   et al . , 2022 ) take the language and attribute distri-   bution in the same space so as to sample attribute-   related sentences with Langevin Dynamics . Con-   current work on the image side , PromptGen ( Wu1030et al . , 2022 ) , simulates the complex distribution of   images relevant to target attributes using a deep   generative model . However , as a consensual hy-   pothesis in manifold learning , the pre - trained lan-   guage model estimates a low - dimensional mani-   fold of language in a high - dimensional embedding   space , which means most points in the embedding   space are not probabilistically modeled by the lan-   guage model . We believe that placing too much   trust in the distributional modeling ability of lan-   guage models is not a good choice . Our method   attempts to depict the attribute space with discrete   sample points of attributed sentences and make   these discrete points , along with their coverage   areas , compose the support set of our estimated   distribution .   7 Conclusion   In this work , we present a distributional perspective   for the multi - aspect controllable text generation   with experimental results confirming the superi-   ority of our model . Further observations on the   2D projection of the estimated attribute space show   that our hypothesis about the attribute space is more   feasible . In the future , we can explore the correla-   tion between different attribute combinations for   more fine - grained control and capture the bias in   datasets to eliminate or utilize it .   Limitations   Our method has a certain dependence on the data   since we need to estimate an attribute space . There-   fore , it is difficult for our method to perform well in   the setting of few - shot learning . However , this dis-   advantage is not that severe , because we only need   single - aspect data , which is relatively sufficient in   style transfer tasks . Another dependence of our   method on data is that it is somewhat sensitive to   biases in the data . When the semantic divergence   of different aspects in training data is too large , our   aspect gap loss , which aims to reduce the distance   among the distributions of each aspect , will conflict   with the sentence reconstruction loss . As a result ,   it may be hard to obtain a reliable intersection in   the attribute space .   Computational resources also have an impact on   our approach , as our aspect gap loss leverages a   batch - level estimation for each aspect . Therefore ,   a larger batch size means a more accurate approx-   imation , leaving the attribute space fewer biases .   An alternative strategy for smaller batches is tobackpropagate the loss after accumulating enough   distributional samples , which requires more train-   ing epochs .   Ethics Statement   We are totally aware that text generation technology   has a potential to be used maliciously to generate   fake , toxic , or offensive content . However , after   training on the Detoxification aspect , controllable   text generation technology is a powerful weapon   for combating hate speech , and eliminating harmful   information in pre - trained language models . In ad-   dition , our multi - aspect controllable text generation   technology can take Detoxification as an default   aspect when controlling other aspects . We believe   it meaningful and beneficial to advance research on   controllable text generation .   Acknowledgements   Xiaocheng Feng is the corresponding author of this   work . We thank the anonymous reviewers for their   insightful comments . This work was supported   by the National Key R&D Program of China via   grant 2020AAA0106502 , National Natural Science   Foundation of China ( NSFC ) via grant 62276078   and the Major Key Project of PCL , PCL2021A06 .   References103110321033A Distribution of Attributes   A.1 Independent Projection of Attributes   We project sample points to 2D by Principal Com-   ponent Analysis , with each attribute projected in-   dependently . We display a scatter plot for each   and perform the Gaussian kernel density estima-   tion . The darker area denotes a higher probability ,   where more representation points of oracle sen-   tences gather . And the region annotated by a red   ellipse is the estimated distributional center .   We underline distributions of attributes in Fig-   ures 5 to 7 , including World , Sports , and Sci / Tech ,   which are significantly asymmetric . And especially ,   the projected distribution of the World attribute is   even non - convex . This observation supports our hy-   pothesis that the practical distributions of attributes   are far more complex than symmetric distributions   such as Gaussian distribution .   In addition , we plot projected distributions of   other attributes in Figures 8 to 12 . Attributes such   as Positive and Negative seem roughly symmetric   in 2D projection . However , we can not guarantee   their symmetry in high - dimensional space . Be-   cause the PCA aims to identify directions along   which the variation in the data is maximal . In other   words , the direction selection strategy is not nec-   essarily related to symmetry or asymmetry , which   means these 2D symmetric distributions may be   asymmetric in high - dimensional space , with the   asymmetric directions ignored during projection .   Worse still , the long - tail region for a skewed direc-   tion may be too sparse , leading to lower variation   compared to symmetric directions.10341035A.2 Joint Projection of Attributes   We project combined sample points of attributes   from three different aspects jointly to 2D by PCA .   We display a scatter plot for each combination in   Figures 13 to 20.The intersection points calculated   on baselines ’ interpolation strategy and our inter-   section searching algorithm are plotted with Base-   lineandOurs , respectively . From these figures , we   observe that NonToxic can mainly cover two senti-   ment attributes or at least possess large intersection   areas . Besides , the intersection areas among sen-   timent attributes and topic attributes , except for   the Sci / Tech , are narrow and sparse . Compared   with the baselines ’ strategy , our search algorithm   is closer to the intersection area , especially on Neg-   ative and Business attributes in Figures 13 to 15   and 19.1036A.3 Projection of Positive and Negative   Except for some noise in the dataset , positive and   negative do not intersect when jointly projected .   B Hyperparameters and Details   Our methods are implemented using the Hugging   face Transformers package . Our encoder is initial-   ized with Bert - base - uncased , and the fixed decoder   uses GPT2 - medium . For any sentence , it will be   tokenized with WordPiece tokenizer from Bert and   Byte - Pair Encoding tokenizer from GPT2 before   input to encoder and decoder , respectively . We   perform mean pooling on outputs of the encoder   and convert them to 768 - dimensional latent repre-   sentations , which are points in our attribute space .   Afterward , latent representations will be mapped to   the prefix with a dimension of 20×24×2×1024 ,   where 20is the prefix sequence length , 24is the   number of hidden layers in GPT2 - medium , 2repre-   sents one key and one value , and 1024 is the size of   hidden states in GPT2 - medium . It ’s worth noting   that prefix length Contrastive Prefix uses for single-   aspect control is 10and for multi - aspect control   is10×number of aspects , which is 30for three-   aspect control . Our prefix length is fixed to 20 ,   which has nothing to do with the scale of aspects .   During the training stage , we use half - precision   mode for efficiency on one NVIDIA A100 80 GB   GPU , where the batch size is 128 since the larger   batch size better alleviates the aspect gap loss .   In our setting , the random seed is 0,w=   0.5 , w= 0.2 , w= 0.3 , variation hyperparam-   eterλis 1e-3 , the optimizer is AdamW with   a learning rate of 1e-4 , the number of training   epochs is 150 , and we use a checkpoint at the1037step 30000 . The training phase takes about 8   hours , and we experiment 6 times to search for   theλ∈[2e-3 , 1e-3 , 5e-4 , 1e-4 , 5e-5 , 1e-5 ] , while   the other hyperparameters are initial settings .   During the inference phase , the maximum num-   ber of iterations Tis15 , the number of candidates   Mis1000 , and the number of neighbors Kis200 .   We utilize a specialized list of weight parameters   for each combination of attributes in Table 5 , which   aims to balance the performance among attributes   from different aspects . After the iteration of in-   tersection searching , our strategy is first to select   the top 10 candidates with the smallest distances   to their neighbors as the final candidate set . Then   we randomly choose a candidate from these ten as   the intersection ’s representation for text generation   diversity . Our text generation process is the same   as prefix tuning with sequence length set to 50 .   Except for model and data loading , the entire eval-   uation process for each attribute combination , in-   cluding intersection searching , text generation , and   attribute - relevance evaluation , takes about 2 min-   utes . Therefore , we can manually tune the weight   of attributes to balance them , with a maximum trial   number of 8 for each weight .   35 prompts we used in the inferencing stage are   following the PPLM setting with 20 from its bag - of-   word setting and 15 from its discriminator setting :   •PPLM - Bow : “ In summary ” , “ This essay   discusses ” , “ Views on ” , “ The connection ” ,   “ Foundational to this is ” , “ To review , ” , “ In   brief , ” , “ An illustration of ” , “ Furthermore , ” ,   “ The central theme ” , “ To conclude , ” , “ The key   aspect ” , “ Prior to this ” , “ Emphasised are ” ,   “ To summarise ” , “ The relationship ” , “ More   importantly , ” , “ It has been shown ” , “ The is-   sue focused on ” , “ In this essay ” .   •PPLM - Discrim : “ Once upon a time ” , “ The   book ” , “ The chicken ” , “ The city ” , “ The coun-   try ” , “ The horse ” , “ The lake ” , “ The last time”,“The movie ” , “ The painting ” , “ The pizza ” ,   “ The potato ” , “ The president of the country ” ,   “ The road ” , “ The year is 1910 . ” .   Detailed setting of baselines : ( I ) Weighted De-   coding : For PPLM , we only retrain its classifier   heads on our datasets while keeping all other orig-   inal settings . For GeDi , We use its code directly   since we are following its setting . ( II ) Multi - objec-   tive Optimization : MUCOCO provides a solution   for custom classification constraints , and thus we   train these classifiers on our datasets . Mix&Match   is relatively complex as it can not generate long sen-   tences from scratch with the mask language model   Bert . Worse still , as a method based on sampling , it   is somewhat dependent on initialization . Therefore ,   we use sentences generated by PPLM as the start-   ing sentences and let Mix&Match slowly polish the   text by itself in iterations . ( III ) Prefix - Tuning : We   reproduce Contrastive Prefixand achieve com-   parable results . For a fair comparison , we unify   the pre - trained language model to GPT2 - medium   ( 345 M parameters ) except for Mix&Match using   Bert - large ( 340 M parameters ) .   C Cases   We illustrate cases of 8 different attribute combina-   tions in Table 6.1038WARNING : Next may contain contents that are offensive in nature.103910401041D Detailed Results10421043