  Minju KimChaehyeong KimYongho SongSeung - won HwangJinyoung YeoDepartment of Artificial Intelligence , Yonsei UniversityDepartment of Computer Science and Engineering , Seoul National University   { minnju,cheris8,kopf_yhs,jinyeo}@yonsei.ac.kr seungwonh@snu.ac.kr   Abstract   To build open - domain chatbots that are able   to use diverse communicative skills , we pro-   pose a novel framework BT , where   multiple agents grounded to the specific target   skills participate in a conversation to automat-   ically annotate multi - skill dialogues . We fur-   ther present Blended Skill BotsTalk ( BS BT ) ,   a large - scale multi - skill dialogue dataset com-   prising 300 K conversations . Through extensive   experiments , we demonstrate that our dataset   can be effective for multi - skill dialogue sys-   tems which require an understanding of skill   blending as well as skill grounding . Our code   and data are available at https://github .   com / convei - lab / BotsTalk .   1 Introduction   A considerable progress has been made towards   open - domain chatbots with different desirable qual-   ities in conversations . Each of these models is capa-   ble of being specialized in one communicative skill ,   i.e. , skill grounding . A number of distinct large-   scale datasets targeting a specific conversational   skill have recently become available . ConvAI2 ( Di-   nan et al . , 2020b ) is provided for research work that   aims to endow chatbots with personas ( Majumder   et al . , 2020a ; Kim et al . , 2020b ) , enabling chat-   bots to talk about themselves . Wizard of Wikipedia   ( WoW ) ( Dinan et al . , 2019 ) is a popular option for   recent studies ( Lian et al . , 2019 ; Zhao et al . , 2020 ;   Kim et al . , 2020a ) that focus on knowledgeable   conversational agents discussing topics in depth .   Empathetic Dialogues ( ED ) ( Rashkin et al . , 2019 )   is also commonly used to embody empathy in dia-   logue systems ( Santhanam and Shaikh , 2019 ; Ma-   jumder et al . , 2020b ) . Most of such skill - grounded   datasets are designed to improve a single skill , and   thus effective when models are asked to demon-   strate the targeted conversational skill . Benefiting from the advances of these conversa-   tional agents , recent research focuses on another as-   pect of open - domain chatbots : the ability to blend   various conversational skills into one cohesive flow   in a seamless manner , i.e. , skill blending . A good   open - domain chatbot should be able to weave mul-   tiple behaviors and skills in a single conversation ,   so that it enables to deal with different users and sit-   uations appropriately ( Shuster et al . , 2020 ; Roller   et al . , 2021 ) . Towards this goal , there is a need to   construct a multi - skill dialogue dataset , which con-   sists of multi - turn dialogues that exhibit multiple   skills . While Smith et al . ( 2020 ) propose a crowd-   sourced dataset Blended Skill Talk ( BST ) of 5 K   conversations as a reliable benchmark for measur-   ing dialogue systems ’ ability at the blended objec-   tive , it is not sufficient to build a multi - skill chatbot   due to its limited scale . Scaling up crowdsourcing   is not feasible , as it requires labor intensive man-   ual annotation and verification . Instead , automatic   curation shows promising results on large - scale   dialogue generation ( Mohapatra et al . , 2021 ) .   In this paper , we aim to generate a large - scale   multi - skill dialogue dataset without additional costs   or human efforts . To this end , we introduce an auto-   matic data curation approach named BT ,   where multiple dialogue agents grounded to indi-   vidual skills engage in the conversation to blend all   skills together . Based on this framework , we cre-   ateBlended Skill BotsTalk ( BS BT ) , a large - scale   multi - skill dialogue dataset of 300 K conversations   blended and grounded with a number of skills de-   rived from ConvAI2 , WoW , and ED . Our experi-   ments demonstrate that by using our dataset dia-   logue models successfully yield large performance   gains in skill blending while maintaining competi-   tive performance in skill grounding . Furthermore ,   we validate the quality of BS BT dataset by human   evaluation , showing our machine - sourced conversa-   tions are consistently preferred over crowdsourced   ones from BST by human judges across all metrics.5149   2 Related Work   2.1 Skill - grounded Dialogue Datasets   Past research in open - domain chatbots has made   solid strides towards dialogue systems with desir-   able general qualities in a conversation . Generating   responses grounded to specific conversational skill   has been explored in different axes , as shown in Ta-   ble 1 ( see also Appendix B for details ) . Dinan et al .   ( 2020b ) introduce ConvAI2 dataset which consists   of more than 140 K utterances of crowdsourced con-   versations to make chit - chat models more engaging   and personalized by conditioning the models on   profile information . Wizard of Wikipedia ( Dinan   et al . , 2019 ) task aims to explore conversation in-   formed by expert knowledge from Wikipedia and   provides about 194 K utterances of conversations   on about 1,250 topics . Rashkin et al . ( 2019 ) con-   struct a dataset , Empathetic Dialogues , compris-   ing 50 K utterances of crowdworker conversations   grounded in an emotional situation for a model to   converse with empathy . However , it remains un-   clear whether models optimized for performance   along specific conversational skill can retain the   learned skill while blending it with other skills .   Hence , Smith et al . ( 2020 ) aim to build a con-   versational agent who seamlessly blends being per-   sonable , knowledgeable , and empathetic . In order   to gauge how successful a model is at this blended   objective , Smith et al . ( 2020 ) collect a new multi-   skill dialogue dataset of about 5 K conversations ,   Blended Skill Talk , via crowdsourcing . While thiswork provides a testbed for future studies , the scale   of data could hinder further progress , since train-   ing multi - skill chatbots generally requires a large-   scale dataset consisting of conversations that in-   volve multiple skills ( Shah et al . , 2018 ) .   2.2 Automatic Dialogue Data Annotation   Research in dialogue systems has been consistently   supported by the development of new dialogue   datasets ( Williams et al . , 2014 ; Mrkši ´ c et al . , 2017 ) .   One popular approach is to collect and annotate   dialogues via crowdsourcing ( Zhang et al . , 2018 ;   Smith et al . , 2020 ) . However , generating multi-   turn dialogues in this manner requires expensive   and exhausting human efforts ( Shah et al . , 2018 ;   Sun et al . , 2021 ; Mohapatra et al . , 2021 ) .   Therefore , recent study seeks to facilitate open-   domain chatbot development with new datasets au-   tomatically constructed by using existing datasets .   For instance , Lee et al . ( 2021 ) create a 45 K multi-   modal dialogue dataset by replacing parts of source   dialogues from existing text - only dialogue datasets   with their semantically relevant images . Sun et al .   ( 2021 ) propose a Human ↔AI collaborative data   collection approach for generating diverse chit - chat   response to augment task - oriented dialogues and   present new chit - chat based annotations to 23.8 K   dialogues from two popular task - oriented datasets .   Kim et al . ( 2021b ) and Vidgen et al . ( 2021 ) present   a model - based dialogue collection framework and   a human - and - model - in - the - loop process for gener-   ating datasets respectively.51503 Problem Formulation   In this section , we formulate the problem of multi-   skill dialogue annotation and desirable characteris-   tics for the dialogue dataset as a training resource .   3.1 Multi - skill Dialogue Annotation   Our goal is to collect a new large - scale multi - skill   dialogue dataset , which seamlessly blends various   skills over the course of a multi - turn conversation .   Here , inspired by Smith et al . ( 2020 ) , the inputs of   this task are single - skill datasets , which are sepa-   rately collected on a variety of skills . Let Mbe the   set of Mskill types , e.g. ,M={P , K , E } , where P ,   K , E denote personality , knowledge , and empathy   derived from ConvAI2 , WoW , and ED , respectively .   Formally , we refer to Das a dialogue dataset with   Ndialogue episodes for skill m∈M   where stxis a skill - relevant description ( i.e. ,   skill context ) for skill manddtxistdialogue   turns ( i.e. , dialogue context ) derived from the skill   context , as shown in Table 1 . Based on the in-   put datasets D , ... , D , we aim to obtain a new   dialogue dataset ˜DforMskills as an output . For-   mally ,   where ˜stxis a set of skill contexts for Manddtx   is the dialogue context derived from the multiple   skills . We will omit the index iwhen dealing with   a single dialogue episode .   3.2 Desirable Characteristics of Multi - skill   Dialogue Datasets   By the above annotation , we aim to build a multi-   skill chatbot that uses all target skills appropriately   in a conversation . For that , we lay out two criteria   that a multi - skill dialogue dataset should meet as a   training resource , namely skill blending andskill   grounding . Skill blending indicates that a multi-   skill dialogue dataset should enable dialogue mod-   els to exhibit different dialogue skills in a conversa-   tion ( Smith et al . , 2020 ) , while skill grounding em-   phasizes that dialogue models should learn to main-   tain each dialogue skill when appropriate ( Shazeer   et al . , 2017 ) . Generally , they have a trade - off rela-   tionship as it is insufficient to represent both skill   blending and grounding in a conversation of finite   length ( Madotto et al . , 2021 ) . Nevertheless , we   note that skill blending and grounding are not con-   tradictory , as some skill - grounded utterances leaveroom for natural shift between skills . Given an ut-   terance “ I like sneakers because it is comfortable . ”   which represents skill type P , it seems reasonable   to annotate an utterance with skill type K “ It is be-   cause sneakers were primarily designed for sports . ”   for next dialogue turn . This example further im-   plies that different skills can be blended naturally   so that the chatbots learn to provide reasonable   responses in a multi - skill dialogue ( Roller et al . ,   2020 ) .   4 BT Framework   We now present BT , a novel framework   that automatically annotates multi - skill dialogues   based on multiple single - skill dialogue datasets .   The focus of our framework is to mimic a natu-   ral conversation by featuring both skill blending   and grounding within a dialogue episode . Figure 1   illustrates three main phases of the framework . Im-   plementation details are provided in Appendix C.   4.1 Participants in BT   In our framework , multiple participants engage in a   conversation to iteratively generate desirable multi-   skill dialogues .   Skill Agents The first participants are mul-   tiple single - skill agents who annotate the appro-   priate skill - grounded utterances to the dialogue .   Formally , based on Dfor skill m , when given   skill context stx , dialogue context dtx , and re-   sponse space U , a skill agent has dialogue models   f : ( stx , dtx)∝ ⇕ ⊣√∫⊔≀→Uwhich return a response   where θis the parameters learned for skill m.   We design two main functions of the skill agent ,   generator model and ranker model , parameterized   asθandθfor skill m , respectively . For   θ , we aim to generate responses from response   space Uin a token - by - token manner , and thus   employ a dodecaDialogue ( Shuster et al . , 2020 )   model , a modification of a transformer Seq2Seq   architecture . On the other hand , for θ , we con-   sider the response space Uas a list of alterna-   tives to pick the correct response , and thus employ   a poly - encoder ( Humeau et al . , 2020 ) model , a   transformer - based retrieval architecture , to score   and rank response candidates . Both θandθ   are fine - tuned on individual single - skill datasets.5151   While all skill agents simulate what response to   annotate , only one skill agent is given priority over   other skill agents , to “ speak ” the response per dia-   logue turn for the dialogue annotation , conditioned   on a set of skill contexts ˜stxand the dialogue con-   textdtx . We call this active agent . This priority   may be passed to another skill agent such that the   current active agent is deactivated , and another skill   agent will be newly activated to speak .   Moderator Agent A critical constraint for skill   agents is that neither the generator nor the ranker   for skill mis able to read other skill contexts in ˜stx   for different skills . For a skill agent , considering   all possible skill contexts in multi - skill dialogues is   non - trivial . Instead , as an omniscient oracle for all   skill contexts ˜stx , we aim to develop another par-   ticipant named moderator agent , which mediates   the conversational flow for desirable multi - skill di-   alogue annotation . To examine the relevance of   the response reswith all skill contexts ˜stxor the   dialogue context dtx , the moderator agent has a   decision function g : ( ˜stx , dtx , res)∝ ⇕ ⊣√∫⊔≀→Awhere   Ais an action space ( i.e. , approval or refusal ) for   the given response .   4.2 Phase 1 : Simulate what to speak   We integrate different dialogue setups from multi-   ple single - skill datasets as seed information to start   a conversation ( detailed in Appendix C.3 ) . For a di-   alogue episode , dialogue context is initialized as an   utterance pair ( i.e. , two dialogue turns ) randomly   sampled from a single - skill dataset D , and the   skill agent for skill mbecomes the initial active   agent . Then , for a generalizable dialogue setup ,   we retrieve the most relevant skill contexts from   each of all input datasets D , ... , Dfor the seed   dialogue context with TF - IDF ( Chen et al . , 2017).In the first phase of BT , all skill agents   simulate their own responses for the next dialogue   turn . Formally , given a skill context stxand the   current dialogue context dtxin a dialogue episode ,   a skill agent for skill mgenerates a plausible re-   sponse resas   where g(·)is the function of the moderator agent ,   which we discuss in the subsequent section .   Depending on individual skills , every skill agent   returns its skill - relevant response . For example , as   shown in Figure 1 , when “ I love sneakers and think   they are the most comfortable shoes around . ” is   given as dtx , the skill agent for skill P generates a   personal response “ Oh really ? I like tennis shoes   more than sneakers . ” asresbased on a given per-   sona . Meanwhile , the skill agents for skill K and E   generate a knowledgeable response “ It is because   sneakers were primarily designed for sports . ” as   resand a empathetic response “ Me too ! I defi-   nitely use mine everyday wear ! ” asres .   4.3 Phase 2 : Check dialogue consistency   It is well known that neural dialogue systems lack   consistency ( Li et al . , 2016 ; Welleck et al . , 2019 ) .   Furthermore , as a skill agent uses the specific skill   context stxinstead of ˜stxfor response genera-   tion , the response is more likely to be semantically   in conflict with other skill contexts in ˜stx . Suppose   astxis “ I wear sneakers everyday ” and a resis   “ I had some trouble yesterday because my sandals   were torn ” . This response is inappropriate because   “ yesterday because my sandals were torn ” is contra-   dictory to “ I wear sneakers everyday ” . Therefore ,   the moderator agent , who has access to all skill con-   texts ˜stx , filters out conflicting response candidates   to preserve dialogue consistency.5152Specifically , the moderator agent leverages natu-   ral language inference ( NLI ) , a task of determining   whether a hypothesis sentence can be inferred from   the given premise sentence . The hypothesis sen-   tence is classified into three categories : E   ( true ) , N ( undetermined ) , and C - ( false ) . Based on the NLI classifier , the de-   cision function of the moderator agent is defined   as   which represents approval / refusal of rescondi-   tioned on ˜stx . A skill agent for skill mrepeat-   edly generates new response candidates until its   response is approved , as described in Equation 4 .   For NLI classifier , we use a RoBERTa ( Liu et al . ,   2019 ) model trained on MNLI ( Williams et al . ,   2018 ) , which is widely used in fact checking sys-   tems ( Kim et al . , 2021a ) . Overall , about 50 %   of utterances are classified as C by   NLI classifier . Out of all utterances classified as   C , about 70 % are in conflict with other   types of skill contexts ( Figure 2 ) . The result demon-   strates that skill agents indeed generate inconsistent   responses due to the restricted access to other skill   contexts . We also find that the overall proportion of   utterances conflicting with stxis relatively high ,   apparently because stxcontains more distinct de-   scriptions than stxandstx .   4.4 Phase 3 : Speak or pass the mic   The objective of the last phase is to score a set of re-   sponse candidates and select a final response when   given the skill contexts and dialogue context . To   this end , we leverage the active agent and the mod-   erator agent , taking into account a balance between   skill blending and skill grounding .   LetUbe the set of response candidates   res , ... , resfrom all skill agents . The active   skill agent identifies the most appropriate response   resinUbased on its ranker model θ , then   asks the moderator agent to attach the selected re-   sponse into the next dialogue context dtxfor   annotation . Formally , we define such process as   where g(·)is the function of the moderator agent .   To compute g(dtx , res ) , the moderator agent   adopts a skill classifier Pthat identifies correspond-   ing skill for the response . We use a BERT ( Devlin   et al . , 2019 ) model trained on utterances in Dand   their corresponding skill labels mfor all skill types   M. Once Pis learned , the decision function of   the moderator agent is defined as   where resis the last utterance of dtxand   P(·)∈Routputs the skill distribution of the   response . Based on KL divergence between two   distributions , g(dtx , res)is discretized as the ap-   proval / refusal decision by a pre - defined threshold   α(Figure 3a ) . Once the moderator agent accepts   the candidate resfrom an inactive agent as the   final response , the active agent passes the mic , or   the priority for annotation , to the inactive agent .   In practice , we compute entropy of the skill dis-   tributions of all utterances to investigate whether   there is room for shifting between skills . The value   of entropy indicates the uncertainty of the skill type   of an utterance : utterances with high entropy values   are uncertain , generic responses . Figure 3b shows   that the number of generic utterances is far from   negligible , suggesting that there are opportunities   to shift to other skills and thus both skill blending   and grounding can be satisfied in a conversation.5153   5 Blended Skill BotsTalk ( BS BT )   5.1 Data Statistics   We collect a multi - skill dialogue dataset , namely   Blended Skill BotsTalk ( BS BT ) , using BT .   The dataset consists of 300 K dialogues with 3 M   utterances . Each utterance is labeled using the skill   classifier with skill annotation ( e.g. , personality   from ConvAI2 , knowledge from WoW , or empathy   from ED ) , including skill type and skill distribution .   Table 2 presents an example from BS BT . As shown   in Table 3 , one of the salient features of BS BT is its   scalability , mainly because BS BT is composed of   bot - bot conversations collected through a machine-   sourced approach while other datasets comprise   crowdsourced human - to - human conversations .   Skill Blending Figure 4 summarizes the results of   skill annotation in BS BT dataset . Overall , the skill   annotation percentages are 36.10 % for personality ,   31.69 % for knowledge , and 32.21 % for empathy   ( Figure 4a ) . Moreover , over 90 % of the dialogues   demonstrate at least 2 of the 3 skills within a single   dialogue ( Figure 4b ) , suggesting the vast majority   of conversations feature more than one skill type .   Skill Grounding Although we focus on blending   skills , multi - skill dialogue datasets should also con-   tain sufficient sessions grounded to specific skills in   conversations for a model to learn the ability of skill   grounding . For that , we explore the continuity of   skills by investigating skill types of utterances sub-   sequent to seed utterance pairs whose provenance   skills are determined by their original datasets . For   all skills , more than half of the utterances followed   by the utterance pairs are labeled as the same skill   types of the utterance pairs ( Figure 5).5154   5.2 Experimental Setups   We conduct a set of experiments to test our BS BT   over BST benchmark through automatic and hu-   man evaluation . To the best of our knowledge , BST   benchmark is the only multi - skill dialogue bench-   mark which gauges how successful a model is at   blended objective as well as grounded objective .   Following Smith et al . ( 2020 ) , we consider the   retrieval task as our primary task and adopt a 256-   million parameter poly - encoder ( Humeau et al . ,   2020 ) pre - trained on pushshift.io Reddit dataset as   a base architecture . We further include the gener-   ative task as our secondary task and adopt a pre-   trained BART ( Lewis et al . , 2020 ) as a base ar-   chitecture . We fine - tune these base architectures   on individual datasets , i.e. , ConvAI2 , WoW , ED ,   BST , and BS BT , and use them as our baselines . We   describe implementation details in Appendix D.   For the retrieval task , we report recall@k ( R@k ) ,   where each test example has 100 possible candi-   dates to select from , as well as mean reciprocal rank   ( MRR ) . For the generative task , we compute the   average score of BLEU-1 , -2 , -3 , -4 ( Avg . BLEU ) .   5.3 Automatic Evaluation   The results of retrieval and generative models on   BST benchmark are shown in Table 4 ( detailed   results are in Appendix E ) . We observe that multi-   skill models , i.e. , BST and BS BT models , are su-   perior to single - skill models , i.e. , ConvAI2 , WoW ,   and ED models on the BST benchmark . As multi-   skill dialogues require an understanding of both   skill blending and grounding , single - skill models   who are only grounded to each of skills struggle to   seamlessly blend them over the course of a conver-   sation , whereas multi - skill models are able to not   only exhibit individual skills but also combine dif-   ferent skills in a conversational flow . In particular ,   BSBT model outperforms all of the baselines on all   automatic metrics . This indicates that our machine-   sourced dataset works properly as the training re-   source to learn the ability of blending skills as well   as grounding to various skills .   To explore the impact of BS BT size on the model   performance , we fine - tune the retrieval architecture   on the BS BT datasets of varying scales . Figure 6 il-   lustrates the performance of BS BT model in terms   of R@1 and MRR when the size of the dataset   gradually increases . BS BT300 K model achieves   a significant performance boost from BS BT10 K   model , surpassing BST model and showing the best   performance . This result not only affirms the impor-   tance of large - scale training for building multi - skill   chatbots but also indicates the potential of BS BT   dataset , as our dataset is collected by automatic   BT framework without human interven-   tion ( i.e. , no manual annotation or verification ) .   5.4 Human Evaluation   To assess the quality of BS BT dataset , we perform   human evaluation by employing ACUTE - Eval ( Li   et al . , 2019 ) , a popular metric for multi - turn di-   alogue evaluation ( Dinan et al . , 2020a ; Li et al . ,   2020 ) . We randomly sample 100 dialogues from   the BST and BS BT datasets respectively , and then   ask human evaluators to compare each pair of dia-   logues over three axes : engagingness , interesting-   ness and humanness . We provide more details for   the specific settings in Appendix F. Table 5 shows   that for all metrics , BS BT dataset achieves compa-   rable and even slightly higher win percentages over   BST dataset . This ensures the quality of BS BT and   thus validates that BT framework can be a   viable alternative to crowdsourcing when construct-   ing multi - skill dialogue datasets.5155   Qualitative Analysis Although most of conversa-   tions from BS BT satisfies the desirable characteris-   tics for multi - skill dialogues as evidenced by a set   of experiments , any side effect may occur since the   dialogues are collected through automatic annota-   tion . Therefore , we select best - case and worst - case   examples ( each in Table 12 and Table 13 ) and pro-   vide empirical results including three types of error   cases . First , in a few cases , speakers repeat greet-   ing ( about 3 turns ) at the end of conversation . This   is mainly because we set dialogues to be of fixed   length , while the conversation may end earlier than   the given turns . Second , as observed in Figure 4b ,   we find a number of dialogues that only features   one skill type . Lastly , some responses tend to show   little relevance with the skill contexts of their cor-   responding skills , although they are grammatically   sound and meaningfully move the conversations   forward . We give a deeper analysis in Appendix G.   5.5 Analysis on Multi - task Learning   Given an access to multiple single - skill dialogue   datasets , a straightforward approach of developing   a multi - skill chatbot is to multi - task on all of them   during fine - tuning step ( Shuster et al . , 2020 ; Roller   et al . , 2021 ) . Therefore , we consider MTL model ,   a poly - encoder ( Humeau et al . , 2020 ) pre - trained   on pushshift.io Reddit and fine - tuned in multi - task   fashion across ConvAI2 , WoW , and ED . We further   fine - tune the MTL model on BS BT datasets of vary-   ing scales sequentially , to probe the effectiveness   of BSBT as a training resource for multi - task train-   ing scheme . In Table 6 , MTL model lags behind   BSBT model on BST benchmark , but performs no-   ticeably better when fine - tuned on BS BT dataset   in addition . Such improvement in the performance   is an encouraging sign that BS BT is orthogonally   applicable to multi - tasking strategy . We observe   that the performance gain becomes marginal when   the size of the dataset increases . We hypothesize   that as multi - task learning and BS BT are parame-   terized and materialized knowledge for multi - skill   dialogues respectively , there can be an overlap be-   tween the knowledge dialogue systems learn . We   leave the mitigation of such overlap for future work .   Nevertheless , MTL model achieves the best perfor-   mance when fine - tuned on BS BT300 K dataset , as   it compensates the overlap with its size .   5.6 Analysis on Skill Grounding Ability   To gain more insights into individual skill ground-   ing ability , we evaluate the baselines on single - skill   benchmarks , i.e. , ConvAI2 , WoW , and ED bench-   marks ( detailed results are in Appendix E ) . We   compute a relative performance drop ∆for re-   trieval models and ∆ for generative mod-   els over the best performing model on the respec-   tive benchmark , which gives us upper bound we   aim for with our model . Results are summarized in   Table 7 . As expected , each of single - skill models   perform best on their original benchmarks but not   as well on other benchmarks , whereas the multi-   skill models show more well - rounded performance   across all benchmarks . In particular , BS BT model   outperforms BST model on most cases and even   achieves comparable performance to single - skill   models on corresponding single - skill benchmarks .   This suggests that BS BT is effective to not only   inject the ability of blending various skills but also   maintain the ability for grounding specific skill .   6 Conclusion   To build multi - skill chatbots , we construct a large-   scale dialogue dataset BS BT through automatic   BT framework . We validate its efficacy as   training resource by experiments and analyses.51567 Limitations   We summarize the error patterns in BS BT and dis-   cuss potential directions to improve BT   framework . First , our framework always produces   a fixed - length conversation , even when the conver-   sation ends earlier than the given turns . As shown   in Example 4 from Table 13 , this often results in   generic and repetitive responses at the end of a   dialogue . In future work , we are interested in train-   ing dialogue models that understand when to end   a conversation based on the context . Second , as   observed in Figure 4b , a few dialogues fail to cover   explicit transitions between multiple skills . For   instance , Example 5 from Table 13 only features   one skill type and lacks the nature of skill blending ,   which may hinder models from learning diverse   communicative skills . Third , some responses , par-   ticularly those near the end of conversations , tend   to show little relevance with their corresponding   skill contexts . We conjecture that as a conversation   flows , skill agents condition their responses more   on the dialogue context , which is likely to be longer   than the pre - defined skill contexts . To enhance the   model ’s ability of skill grounding , one can employ   conditional training by modeling the progress of   the conversation and controlling input contexts . We   leave these issues for future research .   Acknowledgements   We would like to thank anonymous reviewers for   their valuable comments . This research was par-   tially supported by the Institute of Information &   communications Technology Planning & Evalua-   tion ( IITP ) grant funded by the Korea government   ( MSIT ) ( No . 2020 - 0 - 01361 , Artificial Intelligence   Graduate School Program ( Yonsei University ) ) and   the National Research Foundation of Korea ( NRF )   grant funded by the Korea government ( MSIT ) ( No .   2022 - 11 - 0941 ) .   References5157   A Overview   In the following sections , we explore more details   onBT framework and BS BT dataset . In   Appendix B , we lay out the details of single - skill di-   alogue datasets and how they are incorporated into   BT framework to construct BS BT dataset.5158We provide implementation details in Appendix C   for all component models of participants in B - T framework . We also provide implementa-   tion details of baselines used for experiments in Ap-   pendix D. The evaluation results on all benchmarks   used in this paper are in Appendix E. The specific   settings for human evaluation are in Appendix F.   Finally , we present a number of conversation exam-   ples from BS BT and further analyze its strengths   and weaknesses in Appendix G.   B Single - skill Datasets into BT   We describe details on the single - skill dialogue   datasets used to construct BS BT and elaborate on   how they are incorporated into BT frame-   work to construct our dataset . Example dialogues   from the single - skill dialogue datasets i.e. , Con-   vAI2 , WoW , ED , are shown in Table 9 , 10 , 11 .   To integrate different dialogue setups from the   single - skill dialogue datasets , we follow the basic   settings for constructing a dialogue dataset , assum-   ing a multi - turn , one - to - one conversation between   two speakers . We simulate turn - taking in a con-   versation by switching two different sets of skill   contexts for the input skill context stxto a dialogue   model fin a skill agent .   B.1 ConvAI2   ConvAI2 ( Dinan et al . , 2020b ) is a dataset based on   PersonaChat ( Zhang et al . , 2018 ) . ConvAI2 dataset   contains of more than 140 K utterances from con-   versations in which each of paired crowdworkers is   given a role based on their persona description and   gets to know their partner . Specifically , the speaker   pairs are each assigned profiles from a set of 1155   possible personas , each consisting of at least 5 pro-   file sentences . The personas are collected through   crowdsourcing , where the workers are asked to cre-   ate natural , descriptive profiles that contain typical   topics of human interest . Workers are also asked   to keep each profile sentence short , i.e. , no longer   than 15 words .   Following the setting from ConvAI2 , we define a   skill context stxas a profile comprising 5 distinct   persona sentences . We then provide two different   skill contexts as the input to the dialogue model f   in an alternating manner to simulate turn - taking .   B.2 Wizard of Wikipedia   Wizard of Wikipedia ( Dinan et al . , 2019 ) task in-   volves discussing a given topic in depth , wherethe goal is to both engage the partner as well as   display expert knowledge . The dataset consists of   194 K utterances over 1250 topics , where each con-   versation begins with a randomly chosen topic . A   retrieval system over Wikipedia is used to retrieve   articles from which the dialogues are grounded   during the human - human crowdsourced conver-   sations . The topics are also crowdsourced and   range from commuting to Gouda Cheese to Arnold   Schwarzenegger . Each conversation in the dataset   involves two speakers named the apprentice and   the wizard : the apprentice aims at delving deeply   into a topic whereas the wizard uses knowledge in   articles retrieved from Wikipedia to craft a relevant   reply . Specifically , given a topic derived from the   dialogue context , the apprentice keeps the conversa-   tion engaging and talks eagerly about a topic , while   the wizard responds to the apprentice based on the   first paragraphs of 7 relevant Wikipedia articles   provided by the retrieval system .   In our setting , we use a simpler version of Wiz-   ard of Wikipedia task , which ignores the retrieval   aspect of the task . We first specify the topic of the   conversation , which is the same for the apprentice   and wizard . The skill context stxof the appren-   tice is thus defined as the given topic , while the skill   context stxof the wizard is defined as a topic and   7 relevant knowledge sources .   B.3 Empathetic Dialogues   Empathetic Dialogues ( Rashkin et al . , 2019 ) is a   dataset includes 50 K utterances of crowdworker   conversations grounded in an emotional situation .   In the conversation , one speaker describes a per-   sonal situation based on an emotion label and the   other speaker , named listener , displays empathy in   their response . Specifically , a pair of workers ( i.e. ,   speaker and listener ) are asked to choose an emo-   tional word each , depict a situation in 1 - 3 sentences   based on the label , and engage in a short conversa-   tion of 4 - 8 utterances about each of the situations .   Neither of the workers , whether they be the speaker   or the listener , can see the emotion label and the   situation description of their partner , so that they   must refer only to cues within the conversation for   their response .   In our setting , we define the situation description   and its corresponding emotion label as the skill   context stxof the speaker . Note that we do not   define the skill context stxof the listener for our   framework , so that the dialogue system is trained5159to show empathy based solely on the conversation .   C Implementation Details of BT   For the implementation of BT framework ,   we employ ParlAItoolkit , which is specialized in   training and evaluating dialogue systems . We will   release our agents and dataset for public use .   C.1 Skill Agents   In our BT framework , a skill agent lever-   ages both generator model and ranker model .   Given a stxanddtxas input , a generator model   of skill agent generates a response for the next   dialogue utterance . For the generator model , we   employ a dodecaDialogue ( Shuster et al . , 2020 ) .   The dodecaDialogue model is a modification of   transformer seq2seq architecture , which has a 8-   layer encoder , 8 - layer decoder with 512 dimen-   sional embeddings and 16 attention heads . We   fine - tune the dodecaDialogue models on ConvAI2 ,   WoW , and ED , respectively . We use nucleus sam-   pling as the decoding strategy for generative mod-   els at inference time . The deodecaDialogue model   shows 11.19 , 8.46 , and 11.08 perplexity on Con-   vAI2 , WoW , and ED .   Given a stxanddtxas input , a ranker model   of skill agent selects the next dialogue utterance   by scoring a large set of candidate responses and   outputting the one with the highest score . For the   ranker model , we employ the poly - encoder archi-   tecture of Humeau et al . ( 2020 ) . The poly - encoder   encodes global features of the context using multi-   ple representations , which are attended to by each   possible candidate response . This final attention   mechanism gives improved performance over a sin-   gle global vector representation whilst still being   tractable to compute compared to simply concate-   nating input and output as input to a Transformer .   The poly - encoder has state - of - the - art performance   on a number of dialogue tasks when compared to   other retrieval models , and also gives comparable   performance to the winning generative models on   the ConvAI2 competition task in terms of human   evaluation .   More specifically , we consider a 256 M param-   eter poly - encoder model , which has 12 layers , 12   attention heads , and a hidden size of 768 . We   pre - train our poly - encoder on pushshift.io Reddit   dataset and then fine - tune on ConvAI2 , WoW , andED , respectively . We use a large number of neg-   atives by considering the other batch elements as   negative training samples , avoiding recomputation   of their embeddings . We use the Adamax optimizer   without weight decay , a learning rate of 5e-5 with   batch size 128 , epoch 8 . The learning rate decays   by a factor of 0.4 upon plateau of the loss evalu-   ated on the valid set every half epoch . The best   parameters are chosen based on R@1 score . The   poly - encoder model shows 89.41 , 91.01 , and 63.26   R@1 on ConvAI2 , WoW , and ED , respectively .   C.2 Moderator Agent   InBT framework , the moderator agent   leverages NLI classifier and skill classifier .   Given a response resfrom a skill agent of   skillkand the set of skill contexts ˜stx , the NLI   classifier is designed to determine whether a re-   sponse candidate contradicts any of the skill con-   texts . For NLI classifier , we employ the public Hug-   gingFaceimplementation of a RoBERTa - Large   model ( Liu et al . , 2019 ) fine - tuned on the Multi-   Genre NLI dataset ( Williams et al . , 2018 ) . The   RoBERTa model shows 90.59 accuracy on MNLI   validation set . We regard each response candidate   resas a hypothesis sentence and each skill context   stx∈˜stxas a premise sentence , then conduct   unidirectional NLI classification between stxand   res , determining whether a hypothesis sentence   rescan be inferred from the given premise sen-   tence stxfor all response candidates .   Given a response res , the skill classifier identi-   fies the skill of the response among all skills repre-   sented in the skill context set ˜stx . For skill classi-   fier , we employ a BERT - base ( Devlin et al . , 2019 )   model . We trained the model on utterances from   ConvAI2 , WoW , ED train sets and their correspond-   ing skills kas labels . The model was trained with a   batch size of 16 , a learning rate of 2e-5 and epoch   3 . The BERT model shows 81.95 accuracy on ut-   terances from ConvAI2 , WoW , ED test sets .   C.3 Skill Context Retrieval   In Appendix B , we explore how we define the skill   context for each skill considering the original set-   tings from single - skill dialogue datasets . We now   describe how we construct the seed information   ( e.g. , skill contexts for each of skil types and di-   alogue context of 2 turns ) for skill agents to start   a conversation . We first collect consecutive utter-5160   ance pairs from ConvAI2 , WoW , and ED as seed   utterance pairs and define it as dtx . We then fol-   low the convention of past research , which inject   a target communicative skill to dialogue systems   by providing an extra description about the spe-   cific skill , i.e. , skill context . As we aim to build a   multi - skill chatbot , there is a need for integrating   different dialogue setups from multiple single - skill   datasets . For a generalizable dialogue setup , we re-   trieve relevant skill context for each skill by query-   ingdtxwhich is the seed utterance pair . Here , to   gather seed information as much as possible , we   match top-5 relevant skill contexts per utterance   pair , which we give us five times the seed informa-   tion . We use TF - IDF ( Chen et al . , 2017 ) to find   relevant skill contexts and a SQLite database for   storing the sparse TF - IDF matrix . Note that we use   a simple IR baseline as a lower bound since it is   not our main focus . One can easily try other IR   systems for more sophisticated setting .   D Implementation Details of Baselines   We conduct a set of experiments with retrieval and   generative tasks to cover diverse baselines . We   provide training details of these baseline models .   Retrieval Task We adopt a 256 - million parame - ter poly - encoder ( Humeau et al . , 2020 ) pre - trained   on pushshift.io Reddit dataset as a base architec-   ture for the retrieval task . We fine - tune this base   architecture on individual datasets , i.e. , ConvAI2 ,   WoW , ED , BST , and BS BT for 8 epochs with batch   size 128 and learning rate 5e-5 . The learning rate   decays by a factor of 0.4 upon plateau of the loss   evaluated on the valid set every half epoch . The   best parameters are chosen based on R@1 score .   Generative Task As a base architecture for the gen-   erative task , we adopt a pre - trained BART ( Lewis   et al . , 2020 ) with 12 layers in each of the encoder   and decoder . We fine - tune this base architecture   on individual datasets , i.e. , ConvAI2 , WoW , ED ,   BST , and BS BT for 3 epochs with batch size 32   and learning rate 1.0 . The learning rate decays by a   factor of 0.3 upon plateau of the loss evaluated on   the valid set every half epoch . The best parameters   are chosen based on accuracy score . We use greedy   sampling as a decoding strategy .   E Additional Performance   We provide the evaluation results on all dialogue   benchmarks used in this paper , i.e. , BST benchmark   for multi - skill benchmark , and ConvAI2 , WoW , ED   behcmarks for single - skill benchmarks . We report5161   R@1 , R@5 , R@10 , MRR for evaluating retrieval   dialogue models , and BLEU-1 , BLEU-2 , BLEU-3 ,   BLEU-4 for evaluating generative dialogue mod-   els . Table 8 presents the performance of all of the   baselines on all dialogue benchmarks .   F Settings of Human Evaluation   We describe the specific settings for human eval-   uation that we perform to validate the quality of   BSBT dataset . Specifically , we employ ACUTE-   Eval ( Li et al . , 2019 ) , a widely used metric for   multi - turn dialogue evaluation ( Dinan et al . , 2020a ;   Li et al . , 2020 ) . We randomly sample 100 dia-   logues from BST and BS BT datasets respectively .   We only include dialogue contexts and exclude skill   contexts for anonymity , since the skill contexts of   BST and BS BT are distinguishable . Figure 7 shows   the interface used for human evaluation . We ask   judges to compare each pair of conversations over   three axes : engagingness , interestingness and hu-   manness . The wording of questions is presented as   follows :   •Engagingness : Who would you prefer to talk   to ? Which version is more likely to hold your   attention and make you want to hear more ?   •Interestingness : Who would you say is more   interesting ? Which version arouses your cu-   riosity or tells you something new or useful?•Humanness : Who would you say sounds   more human ? Which version is more natu-   ral and personable ?   G Qualitative Analysis   In what follows we conduct qualitative analysis on   BSBT dataset based on diverse samples . In each   dialogue episode from BS BT , one speaker is given   five personas as stx , one topic as stx , and a situa-   tion description and emotion as stx , while another   speaker is given five personas as stx , the topic and   seven knowledge resources as stx , and nothing   forstx . Each speaker is conditioned on their cor-   responding set of skill contexts , and annotates the   response turn by turn . To give a deeper analysis , we   select best - case ( i.e. , cherry - picked ) and worst - case   ( i.e. , lemon - picked ) examples from BS BT dataset .   Table 12 and Table 13 shows cherry - picked ex-   amples and lemon - picked examples , respectively .   We also present more dialogue examples randomly   sampled from BS BT in Table 14 and Table 15 .   Cherry - picked Dialogues In Table 12 , we provide   cherry - picked dialogue samples from the BS BT   dataset . Example 1 and 2 both show that the con-   versation contains a sufficient amount of utterances   to learn skill grounding for skill type P and E. In   Example 3 , each skill type is sustained for more   than 2 turns , which allows dialogue systems to5162learn the ability of skill grounding . Example 3 also   demonstrates all of the skill types are blended in   a natural and reasonable way , which enables dia-   logue systems to weave multiple skills in a single   conversation properly .   Lemon - picked Dialogues In Table 13 , we present   lemon - picked conversations from BS BT dataset ,   which fail to satisfy the key criteria of multi - skill   dialogues . In Example 4 , speaker A and B repeats   greeting such as “ You as well . Have a good week-   end as well ” and “ You too . Have a good weekend   as well ” for about 3 turns . This is mainly because   we set conversations to be of fixed length ( i.e. , 10   turns ) , while the conversation may end earlier than   the given turns . To prevent these error cases , it will   be helpful to have termination condition of conver-   sations . We leave this for future work . Example   5 only features one skill type , which we already   observe a few cases in Figure 4b . While the con-   versation may make sense , lack of skill blending in   the conversation prevents a dialogue model from   learning multiple skills . Meanwhile , it is widely   known that dialogue agents suffer from generic   responses ( Li et al . , 2016 ) , as shown in Example   6 . We observe that while shifts between skill are   frequent , responses tend to show little relevance   with the skill contexts of their corresponding skills .   They often rephrase or repeat what has been dis-   cussed during conversation.51635164516551665167516851695170