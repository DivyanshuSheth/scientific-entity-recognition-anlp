  Wanyu Du , Vipul Raheja , Dhruv Kumar , Zae Myung Kim ,   Melissa Lopez , Dongyeop KangUniversity of Virginia , Grammarly , Univ . Grenoble Alpes , CNRS , LIG , University of Minnesota   wd5jq@virginia.edu   { vipul.raheja,dhruv.kumar,melissa.lopez}@grammarly.com   zae-myung.kim@univ-grenoble-alpes.fr   dongyeop@umn.edu   Abstract   Writing is , by nature , a strategic , adaptive ,   and more importantly , an iterative process . A   crucial part of writing is editing and revis-   ing the text . Previous works on text revision   have focused on deﬁning edit intention tax-   onomies within a single domain or develop-   ing computational models with a single level   of edit granularity , such as sentence - level ed-   its , which differ from human ’s revision cycles .   This work describes I TR : the ﬁrst large-   scale , multi - domain , edit - intention annotated   corpus of iteratively revised text . In partic-   ular , I TR is collected based on a new   framework to comprehensively model the iter-   ative text revisions that generalize to various   domains of formal writing , edit intentions , re-   vision depths , and granularities . When we in-   corporate our annotated edit intentions , both   generative and edit - based text revision models   signiﬁcantly improve automatic evaluations .   Through our work , we better understand the   text revision process , making vital connections   between edit intentions and writing quality , en-   abling the creation of diverse corpora to sup-   port computational modeling of iterative text   revisions .   1 Introduction   Writing is a complex and effortful cognitive task ,   where writers balance and orchestrate three distinct   cognitive processes : planning , translation , and re-   vising ( Flower and Hayes , 1980 ) . These processes   can be hierarchical and recursive and can occur at   any moment during writing . This work focuses on   text revision as an essential part of writing ( Scar-   damalia , 1986 ) . Revising text is a strategic , and   adaptive process . It enables writers to deliberate   over and organize their thoughts , ﬁnd a better line   of argument , learn afresh , and discover what was   Table 1 : An iteratively revised ArXiv abstract snippet   ( 2103.14972 , version 2 , 3 , and 4 ) with our annotated - in I TR .   not known before ( Sommers , 1980 ) . Speciﬁcally ,   text revision involves identifying discrepancies be-   tween intended and instantiated text , deciding what   edits to make , and how to make those desired edits   ( Faigley and Witte , 1981 ; Fitzgerald , 1987 ; Brid-   well , 1980 ) .   Text revision is an iterative process . Human   writers are unable to simultaneously comprehend   multiple demands and constraints of the task when   producing well - written texts ( Flower , 1980 ; Collins   and Gentner , 1980 ; Vaughan and McDonald , 1986 )   – for instance , expressing ideas , covering the con-   tent , following linguistic norms and discourse con-   ventions of written prose , etc . Thus , they turn to-   wards making successive iterations of revisions to   reduce the number of considerations at each time .   Previous works on iterative text revision have   three major limitations : ( 1 ) simplifying the task to   an noniterative " original - to-ﬁnal " text paraphras-3573ing ; ( 2 ) focusing largely on sentence - level edit-   ing ( Faruqui et al . , 2018 ; Botha et al . , 2018 ; Ito   et al . , 2019 ; Faltings et al . , 2021 ) ; ( 3 ) developing   editing taxonomies within individual domains ( e.g.   Wikipedia articles , academic writings ) ( Yang et al . ,   2017 ; Zhang et al . , 2017 ; Anthonio et al . , 2020 ) .   These limitations make their proposed text editing   taxonomies , datasets , and models lose their gener-   alizability and practicality .   We present I TR — an annotated dataset   forI tive TxtRevision that consists of   31,631 iterative document revisions with sentence-   level and paragraph - level edits across multiple do-   mains of formally human - written text , including   Wikipedia , ArXivand Wikinews . Table 1 shows   a sample ArXiv document in I TR , that un-   derwent iterative revisions . Our dataset includes   4 K manually annotated and 196 K automatically an-   notated edit intentions based on a sound taxonomy   we developed , and is generally applicable across   multiple domains and granularities ( See Table 2 ) .   Note that I TRis currently only intended to   support formal writing revisions , as iterative re-   visions are more prevalent in formal rather than   informal writings ( e.g. tweets , chit - chats ) . Our   contributions are as follows :   •formulate the iterative text revision task in a more   comprehensive way , capturing greater real - world   challenges such as successive revisions , multi-   granularity edits , and domain shifts .   •collect and release a large , multi - domain Iterative   Text Revision dataset : I TR , which con-   tains 31 K document revisions from Wikipedia ,   ArXiv and Wikinews , and 4 K edit actions with   high - quality edit intention annotations .   •analyze how text quality evolves across iterations   and how it is affected by different kinds of edits .   •show that incorporating the annotated edit-   intentions is advantageous for text revision sys-   tems to generate better - revised documents .   2 Related Work   Edit Intention Identiﬁcation . Identiﬁcation of   edit intentions is an integral part of the iterative   text revision task . Prior works have studied the   categorization of different types of edit actions   to help understand why editors do what they do   and how effective their actions are ( Yang et al . ,   2017 ; Zhang et al . , 2017 ; Ito et al . , 2019 ) . How-   ever , these works do not further explore how to   leverage edit intentions to generate better - revised   documents . Moreover , some of their proposed edit   intention taxonomies are constructed with a focus   on speciﬁc domains of writing , such as Wikipedia   articles ( Anthonio et al . , 2020 ; Bhat et al . , 2020 ;   Faltings et al . , 2021 ) or academic essays ( Zhang   et al . , 2017 ) . As a result , their ability to generalize   to other domains remains an open question .   Noniterative Text Revision Models . Some   prior works ( Faruqui et al . , 2018 ; Botha et al . , 2018 ;   Ito et al . , 2019 ; Faltings et al . , 2021 ) simplify the   text revision task to a single - pass " original - to-ﬁnal "   sentence - to - sentence generation task . However , it   is very challenging to conduct multiple perfect ed-   its at once . For example , adding transition words   or reordering the sentences are required to further   improve the document quality . Therefore , single-   pass sentence - to - sentence text revision models are   not sufﬁcient to deal with real - world challenges of   text revision tasks . In this work , we explore the   performance of text revision models in multiple   iterations and multiple granularities .   Iterative Text Revision Datasets . While some   prior works have constructed iterative text revision   datasets , they are limited to singular writing do-   mains , such as Wikipedia - style articles ( Anthonio   et al . , 2020 ) , academic essays ( Zhang et al . , 2017 )   or news articles ( Spangher and May , 2021 ) . In this   work , we develop a uniﬁed taxonomy to analyze   the characteristics of iterative text revision behav-   iors across different domains and collect large scale   text revisions of human writings from multiple do-   mains . The differences between I TRand   the prior datasets are summarized in Table 2.3574   3 Formulation : Iterative Text Revision   We provide formal deﬁnitions of the Iterative Text   Revision task , and its building blocks .   Edit Action . An edit action ais a local change   applied to a certain text object , where kis the in-   dex of the current edit action . The local changes   include : insert , delete and modify . The text objects   include : token , phrase , sentence , and paragraph .   This work deﬁnes local changes applied to tokens   or phrases as sentence - level edits , local changes   applied to sentences as paragraph - level edits and   local changes applied to paragraphs as document-   level edits .   Edit Intention . An edit intention ereﬂects the   revising goal of the editor when making a certain   edit action . In this work , we assume each edit ac-   tionawill only be labeled with one edit intention   e. We further describe our edit intention taxon-   omy in Table 4 and § 4.2.1 .   Document Revision . A document revision is cre-   ated when an editor saves changes for the current   document ( Yang et al . , 2016 , 2017 ) . One revision   Ris aligned with a pair of documents ( D;D )   and contains Kedit actions , where tindicates the   version of the document and K1 . A revision   withKedit actions will correspondingly have K   edit intentions :   ( D;D)!R = f(a;e)g ( 1 )   We deﬁnetas the revision depth .   Iterative Text Revision . Given a source text   D , iterative text revision is the task of gener-   ating revisions of text Dat depthtuntil the qualityof the text in the ﬁnal revision satisﬁes a set of   pre - deﬁned stopping criteria fs;:::;sg :   D   !D;iff(D)<fs;:::;sg(2 )   whereg(D)is a text revision system and f(D)is   a quality evaluator of the revised text . The quality   evaluatorf(D)can be automatic systems or man-   ual judgements which measure the quality of the   revised text . The stop criteria fsgis a set of con-   ditions that determine whether to continue revising   or not . In this work , we simply set them as revision   depth equal to 10 , and edit distance between D   andDequal to 0 ( § 6.2 ) . We will include other   criteria which measures the overall quality , content   preservation , ﬂuency , coherence and readability of   the revised text in future works .   4 I TR Dataset   4.1 Raw Data Collection   Domains . We select three domains – Wikipedia   articles , academic papers , and news articles – to   cover different human writing goals , formats , re-   vision patterns , and quality standards . The three   domains consist of formally written texts , typically   edited by multiple authors . We describe why and   how we collect text revision from each domain   below :   •Scientiﬁc Papers . Scientiﬁc articles are written   in a rigorous , logical manner . Authors generally   highlight and revise their hypotheses , experimen-   tal results , and research insights in this domain .   We collect paper abstracts submitted at different   timestamps ( i.e. , version labels ) from ArXiv .   •Wikipedia Articles . Encyclopedic articles are   written in a formal , coherent manner , where edi-   tors typically focus on improving the clarity and   structure of articles to make people easily under-   stand all kinds of factual and abstract encyclope-3575   dic information . We collect revision histories of   the main contents of Wikipedia articles .   •News Articles . News articles are generally writ-   ten in a precise and condensed way . News editors   emphasize improving the clarity and readability   of news articles to keep people updated on rapidly   changing news events . We collect revision histo-   ries of news content from Wikinews .   Raw Data Processing . We ﬁrst collect all raw   documents , then sort each document version ac-   cording to its timestamp in ascending order . For   each documentD , we pair two consecutive ver-   sions as one revision ( D;D)!R , wheret   is the revision depth . For each sampled document-   revisionR , we extract its full edit actions using   latexdiff .We provide both the paragraph - level   and sentence - level revisions where the latter is con-   structed by applying a sentence segmentation tool ,   and aligning each sentence to each revision . For   each revision pair , we have : the revision type , the   document i d , the revision depth , an original phrase   and a revised phrase , respectively . The detailed   processing of raw text is described in Appendix A.   In summary , we collect 31,631 document revi-   sions with 196,987 edit actions , and maintain a rel-   atively balanced distribution across three domains ,   as shown in Table 3 . We call this large - scale dataset   as I TR- - .4.2 Data Annotation   To better understand the human revision pro-   cess , we sample 559 document revisions from   I TR- - , consisting of 4,018 hu-   man edit actions . We refer to this small - scale   unannotated dataset as I TR- - .   In § 4.2.2 , we then use Amazon Mechanical Turk   ( AMT ) to crowdsource edit intention annotations   for each edit action according to our proposed edit-   intention taxonomy ( § 4.2.1 ) . We refer to this small-   scale annotated dataset as I TR- .   We then scale these manual annotations to   I TR- - by training edit intention   prediction models on I TR- , and au-   tomatically label I TR- - to con-   struct I TR- . ( § 4.2.3 )   4.2.1 Edit Intention Taxonomy   For manual annotations , we propose a new edit in-   tention taxonomy in I TR(Table 4 ) , in order   to comprehensively model the iterative text revision   process . Our taxonomy builds on prior literature   ( Rathjens , 1985 ; Harris , 2017 ) . At the highest level ,   we categorize the edit intentions into ones that   change the meaning or the information contained   in the text ( M - ) , and ones that   preserve these characteristics ( N - M - ) . Since our goal is to understand edit   intentions to improve the quality of writing , we fo-   cus on categorizing edits in the latter category fur-   ther into four sub - categories : F , C ,   C andS . Our proposed taxonomy   of edit intentions is generally applicable to multiple3576   domains , edit - action granularities ( sentence - level   and paragraph - level ) , and revision depths . We also   propose the O category for edits that can not   be labeled using the above taxonomy .   4.2.2 Manual Annotation   Since edit intention annotation is a challenging   task , we design strict qualiﬁcation tests to select   11 qualiﬁed AMT annotators ( details in Appendix   B ) . To further improve the annotation quality , we   ask another group of expert linguists ( English L1 ,   bachelor ’s or higher degree in Linguistics ) to re-   annotate the edits which do not have a majority   vote among the AMT workers . Finally , we take   the majority vote among 3 human annotations ( ei-   ther from AMT workers or from expert linguists )   as the ﬁnal edit intention labels . This represents   theI TR- dataset . We release both   the ﬁnal majority vote and the three raw human   annotations per edit action as part of the dataset .   4.2.3 Automatic Annotation   To scale up the annotation , we train an edit-   intention classiﬁer to annotate I TR- - and construct the I TR- dataset .   We split the I TR- dataset into   3,254/400/364 training , validation and test pairs .   The edit intention classiﬁer is a RoBERTa - based   ( Liu et al . , 2020 ) multi - class classiﬁer that predicts   an intent given the original and the revised text for   each edit action . Table 5 shows its performance   on the test set . The Fluency and Clarity edit in-   tentions are easy to predict with F1 scores of 0.8   and 0.69 , respectively , while Style and Coherence   edit intentions are harder to predict with F1 scores   of 0.13 and 0.32 , respectively , largely due to the   limited occurrence of Style and Coherence intents   in the training data ( Table 4 ) .   4.3 Data Analysis   Edit Intention Distributions . The iterative edit   intention distributions in three domains are demon-   strated in Figure 1 . Across all three domains , au-   thors tend to make the majority of edits at revision   depth 1 . However , the number of edits rapidly de-   creases at revision depth 2 , and few edits are made   at revision depth 3 and 4 .   We ﬁnd that C is one of the most fre-   quent edit intentions across all domains , indicating   that authors focus on improving readability across   all domains . For ArXiv , M - ed-   its are also among the most frequent edits , which   indicates that authors also focus on updating the   contents of their abstracts to share new research in-   sights or update existing ones . Meanwhile , ArXiv   also covers many F and C   edits , collecting edits from scientiﬁc papers and   suggesting meaningful revisions would be an im-   portant future application of our dataset . For   Wikipedia , we ﬁnd that F , C ,   andM - edits roughly share a   similar frequency , which indicates Wikipedia ar-   ticles have more complex revision patterns than   ArXiv and news articles . For Wikinews , F   edits are equally emphasized , indicating that im-   proving grammatical correctness of the news arti-   cles is just as important .   Inter - Annotator Agreement . We measure inter-   annotator agreement ( IAA ) using the Fleiss ’    ( Fleiss , 1971 ) . Table 6 shows the IAA across   three domains . After the second round of re-   annotation by proﬁcient linguists , the Fleiss ’ in-   creases to 0.5014 , which indicates moderate agree-   ment among annotators .   We further look at the raw annotations where   at least 1 out of 3 annotators assigns a different   edit intention label . We ﬁnd that the C   intention is the one that is the most likely to have   a disagreement : 312 out of 393 C an-3577   notations do not have consensus . Within those dis-   agreements of the C intention , 68.77 %   are considered to be C , and 11.96 % are   considered to be the F intention . Annota-   tors also often disagree on the C intention ,   where 1023 out of 1601 C intentions do   not have a consensus . Among those disagreements   of the C intention , 30.33 % are considered   to be C , and 30.23 % are considered to   be S .   The above ﬁndings explain why the inter-   annotator agreement scores are lower in Wikipedia   and ArXiv . As shown in Figure 1 , Wikipedia has   many C edits while ArXiv has many   C edits . This explains the difﬁculty of the   edit intention annotation task : it not only asks an-   notators to infer the edit intention from the full   document context , but also requires annotators to   have a wide range of domain - speciﬁc knowledge   in scientiﬁc writings .   5 Understanding Iterative Text Revisions   To better understand how text revisions affect the   overall quality of documents , we conduct both man-   ual and automatic evaluations on a sampled set of   document revisions .   5.1 Experiment Setups   Evaluation Data . We sample two sets of text re-   visions for different evaluation purposes . The ﬁrst   set contains 21 iterative document revisions , con-   sisting of 7 unique documents , each document hav-   ing 3 document revisions from revision depth 1 to 3 .   The second set contains 120 text pairs , each associ-   ated with exactly one edit intention of F ,   C , C orS . We validate   the following research questions :   RQ1 How do human revisions affect the text qual-   ity across revision depths ?   RQ2 How does text quality vary across edit inten-   tions?Manual Evaluation Conﬁguration . We hire a   group of proﬁcient linguists to evaluate the over-   all quality of the documents / sentences , where each   revision is annotated by 3 linguists . For each revi-   sion , we randomly shufﬂe the original and revised   texts , and ask the evaluators to select which one   has better overall quality . They can choose one of   the two texts , or neither . Then , we calculate the   score for the overall quality of the human revisions   as follows : -1 means the revised text has worse   overall quality than the original text ; 0 means the   revised text do not show a better overall quality   than the original text , or can not reach agreement   among 3 annotators ; 1 means the revised text has   better overall quality than the original text .   Automatic Evaluation Conﬁguration . We se-   lect four automatic metrics to measure the doc-   ument quality on four different aspects : Syntactic   Log - Odds Ratio ( SLOR ) ( Kann et al . , 2018 ) for   text ﬂuency evaluation , Entity Grid ( EG ) score ( La-   pata and Barzilay , 2005 ) for text coherence evalu-   ation , Flesch – Kincaid Grade Level ( FKGL ) ( Kin-   caid et al . , 1975 ) for text readability evaluation and   BLEURT score ( Sellam et al . , 2020 ) for content   preservation evaluation . We describe the detailed   justiﬁcation of our metric selection in Appendix   E. However , in our following experiments , we ﬁnd   these existing automatic metrics are poorly corre-   late with manual evaluations .   5.2 Quality Analyses on Revised Texts   RQ1 : Iterative Revisions vs. Quality . Table 7   shows the document quality changes at different re-   vision depths . Generally , human revisions improve   the overall quality of original documents , as indi-   cated by the overall score at each revision depth .   However , the overall quality keeps decreasing as   the revision depth increases from 1 to 3 , likely be-   cause it is more difﬁcult for evaluators to grasp the3578   overall quality in the deeper revision depths in the   pair - wise comparisons between the original and   revised documents , because less N - M - edits have been conducted in deeper   revision depths . For automatic metrics , we ﬁnd   SLOR and EG are not well - aligned with human   overall score , we further examine whether human   revisions makes original documents less ﬂuent and   less coherent in the analysis of RQ2 .   RQ2 : Edit Intentions vs. Quality . Table 8   shows how text quality varies across edit inten-   tions . We ﬁnd that F andC   edits indeed improve the overall quality of origi-   nal sentences according to human judgments . This   ﬁnding suggests that SLOR and EG are not   well - aligned with human judgements , and calls for   the need to explore other effective automatic met-   rics to evaluate the ﬂuency and coherence of re-   vised texts . Besides , we observe that S edits   degrade the overall quality of original sentences .   This observation also makes sense since S ed-   its reﬂect the writer ’s personal writing preferences   ( according to our edit intention taxonomy in Table   4 ) , which not necessarily improve the readability ,   ﬂuency or coherence of the text .   6 Modeling Iterative Text Revisions   To better understand the challenges of modeling   the task of iterative text revisions , we train different   types of text revision models using I TR .   6.1 Experiment Setups   Text Revision Models . For training the text revi-   sion models , we experiment with both edit - based   and generative models . For the edit - based model ,   we use F ( Mallinson et al . , 2020 ) , and for the   generative models , we use BART ( Lewis et al . ,   2020 ) and P ( Zhang et al . , 2020a ) . F   decomposes text revision into two sub - tasks : Tag-   ging , which uses a pointer mechanism to select the   subset of input tokens and their order ; and Inser-   tion , which uses a masked language model to ﬁll in   missing tokens in the output not present in the in-   put . BART andP are Transformer - based   encoder - decoder models which are used in a wide   range of downstream tasks such as natural language   inference , question answering , and summarization .   Training . We use four training conﬁgurations to   evaluate whether edit intention information can   help better model text revisions . The ﬁrst conﬁg-   uration uses the pure revision pairs without edit   intention annotations ( I TR- -   dataset ) . In the second conﬁguration , we include   the manually annotated edit intentions to the source   text ( I TR- dataset ) . Similarly , for   the third and fourth training conﬁgurations , we use   I TR- - dataset ( no edit intention   information ) and I TR- dataset ( auto-   matically annotated labels , as described in § 4.2.3 ,   simply appended to the input text ) . We use these   four conﬁgurations for all model architectures .   6.2 Results Analysis   Automatic Evaluation . Table 9 shows the re-   sults of the three models for our different train-   ing conﬁgurations . Following prior works ( Malmi   et al . , 2019 ; Dong et al . , 2019 ; Mallinson et al . ,   2020 ) , we report SARI , BLEU , and ROUGE - L3579   metrics , and include detailed breakdown of scores   in Appendix H. It is noteworthy that the SARI   score on the no - edit baseline is the lowest , which   indicates the positive impact of revisions on doc-   ument quality , as also corroborated by the human   evaluations in § 5 . For both I TR-   andI TR- datasets , we see that edit   intention annotations help to improve the perfor-   mance of both F andP . Also , both   models perform better on the larger I TR- dataset compared to the I TR-   dataset , showing that the additional data ( and   automatically - annotated annotations ) are helpful .   Manual Evaluation . Table 10 shows how the   model revision affects the quality of the origi-   nal document . We choose P trained on   I TR- to generate revisions and com-   pare with human revisions , as the model produces   the best overall results . There exists a big gap   between the best - performing model revisions and   human revisions , indicating the challenging nature   of the modeling problem . Thus , while model re-   visions can achieve comparable performance with   human revisions on ﬂuency , coherence and mean-   ing preservation , human revisions still outperform   in terms of readability and overall quality .   Table 11 demonstrates how model - generated text   quality varies across revision depths . In the ﬁrst   two depths , human revisions win over model re-   visions with a ratio of 57.14 % . However , in the   last depth , model revisions stay similar with hu-   man revisions in a ratio of 57.15 % . Upon review-   ing revisions in the last depth , we ﬁnd a lot of   M - edits in human revisions . At   the same time , the model revisions only made a   fewF orC edits , which the hu-   man evaluators tend to judge as “ tie ” .   Iterativeness . We also compare the iterative abil-   ity between the two kinds of text revision mod-   els ( best performing versions of both F and   P : trained on I TR- ) , against   human ’s iterative revisions . Figure 2 shows that   while P is able to ﬁnish iterating after 2.57   revisions on average , F continues to make it-   erations until the maximum cutoff of 10 that we set   for the experiment . In contrast , humans on average   make 1.61 iterations per document . While F   is able to make meaningful revisions ( as evidenced   by the improvements in the SARI metric in Table   14 ) , it lacks the ability to effectively evaluate the   text quality at a given revision , and decide whether   or not to make further changes . P , on the   other hand , is able to pick up on these nuances of   iterative revision , and learns to stop revising after a   certain level of quality has been reached .   7 Conclusions and Discussions   Our work is a step toward understanding the com-   plex process of iterative text revision from human-   written texts . We collect , annotate and release I - TR : a novel , large - scale , domain - diverse , an-   notated dataset of human edit actions . Our research   shows that different domains of text have differ-   ent distributions of edit intentions , and the general   quality of the text has improved over time . Compu-   tationally modeling the human ’s revision process is   still under - explored , yet our results indicate some   interesting ﬁndings and potential directions .   Despite the deliberate design of our dataset col-   lection , I TRonly includes formally written   texts . We plan to extend it to diverse sets of revi-3580sion texts , such as informally written blogs and less   informal but communicative texts like emails , as   well as increase the size of the current dataset . For   future research , we believe I TRcan serve   as a basis for future corpus development and com-   putationally modeling iterative text revision .   8 Ethical Considerations   We collect all data from publicly available sources ,   and respect copyrights for original document au-   thors . During the data annotation process , all hu-   man annotators are anonymized to respect their   privacy rights . We provide fair compensation to   all human annotators , where each annotator gets   paid more than the minimum wage and based on   the number of annotations they conducted .   Our work has no possible harms to fall dispro-   portionately on marginalized or vulnerable popu-   lations . Our dataset does not contain any identity   characteristics ( e.g. gender , race , ethnicity ) , and   will not have ethical implications of categorizing   people .   Acknowledgments   We thank all linguistic expert annotators at Gram-   marly for annotating , evaluating and providing   feedback during our data annotation and evalua-   tion process . We appreciate that Courtney Napoles   and Knar Hovakimyan at Grammarly helped co-   ordinate the annotation resources . We also thank   Yangfeng Ji at University of Virginia and the anony-   mous reviewers for their helpful comments .   References35813582   A Details on Text Processing in   I TR   For Wikipedia and Wikinews , we use the Medi-   aWiki Action APIto retrieve raw pages updated   at different timestamps . For each article , we start   from July 2021 and trace back to its ﬁve most re-   cent updated versions . Then , we parseplain texts   from raw wiki - texts and ﬁlter out all references and   external links . For Wikipedia , we retrieve pages un-   der the categories listed on the main category page . For Wikinews , we retrieve pages listed on the   published articles page .   For ArXiv , we use the ArXiv APIto retrieve   paper abstracts . Note that we do not retrieve the   full paper for two reasons : ( 1 ) some paper reserved   their copyright for distribution , ( 2 ) parsing and   aligning editing actions in different document types   ( e.g. pdf , tex ) is challenging . For each paper , we   start from July 2021 and retrieve all its previous   submissions . We collect papers in the ﬁelds of   Computer Science , Quantitative Biology , Quantita-   tive Finance , and Economics.3583   B Details on Qualiﬁciation Tests for   Human Annotation   First , we prepare a small test set with 67 edit-   actions and deploy parallel test runs on AMT to get   more workers participate in this task . Before start-   ing the annotation , workers are required to pass a   qualiﬁcation test which has 5 test questions to get   familiar with our edit - intention taxonomy . Second ,   we compare workers ’ annotations with our golden   annotations , and select workers who have an accu-   racy over 0.4 . After 5 test runs , we select 11 AMT   workers who are qualiﬁed to participate in this task .   Then , we deploy the full 4 K edit - actions on AMT ,   and collect 3 human annotations per edit - action . C Human Annotation Instruction and   Interface   To guide human annotators make accurate edit-   intention annotation , we provide them with a short   task instruction ( Figure 3 ) followed by some con-   crete edit - intention examples ( Figure 4 ) . Then ,   we highlight the edit - action within the document-   revision and ask human annotators three questions   to obtain the accurate edit - intention of the current   edit - action , as illustrated in Figure 5 . Note that   in our previous test runs on AMT , we ﬁnd that   AMT workers can hardly have a consensus on Clar-   ity and Style edits , which give a very low IAA   score . Therefore , in the annotation interface , we3584   include Clarity and Style edits under the category   of " Rephrasing " , and further ask the annotators   to judge whether the current " Rephrasing " edit is   making the text more clearer and understandable .   If yes , we convert this edit to Clarity , otherwise we   convert this edit to Style . This interface conﬁgura-   tion gives us the best IAA score among our 5 test   runs .   D Details on Computational   Experiments   For all computational experiments in this work , we   deploy them on a single Quadro RTX 4000(16 GB )   GPU .   RoBERTa . We leverage the RoBERTa - large   model from Huggingface transformers ( Wolf et al . ,   2020 ) , which has 354 million parameters . We set   the total training epoch to 15 and batch size to   4 . We use the Adam optimizer with weight decay   ( Loshchilov and Hutter , 2018 ) , and set the learning   rate to 10which decreases linearly to 0 at the   last training iteration . We report descriptive statis-   tics with a single run . We use the sklearn package   ( Pedregosa et al . , 2011 ) to calculate the precision ,   recall and f1 score .   Text Revision Models . We leverage the   BART - large ( with 400 million parameters ) and   PEGASUS - large ( with 568 million parameters)from Huggingface transformers ( Wolf et al . , 2020 ) .   We set the total training epoch to 5 and batch size   to 16 . We use the Adam optimizer with weight   decay ( Loshchilov and Hutter , 2018 ) , and set   the learning rate to 310which decreases   linearly to 0 at the last training iteration . We report   descriptive statistics with a single run . We use the   metrics package from Huggingface transformers to   calculate the SARI , BLEU , ROUGE-1/2 / L score .   E Justiﬁcation of Automatic Evaluation   Metrics   ForFluency , we use the Syntactic Log - Odds Ratio   ( SLOR ) ( Kann et al . , 2018 ) to evaluate the natu-   ralness and grammaticality of the current revised   document , where a higher SLOR score indicates   a more ﬂuent document . Prior works ( Pauls and   Klein , 2012 ; Kann et al . , 2018 ) found word - piece   log - probability correlates well with human ﬂuency   ratings . For Coherence , we use the Entity Grid   ( EG ) score ( Lapata and Barzilay , 2005 ) to evaluate   the local coherence of the current revised docu-   ment , where a higher EG score indicates a more   coherent document . EG is a widely adopted ( Sori-   cut and Marcu , 2006 ; Elsner and Charniak , 2008 ;   Louis and Nenkova , 2012 ) metric for measuring   document coherence . For Readability , we use the   the Flesch – Kincaid Grade Level ( FKGL ) ( Kincaid   et al . , 1975 ) to evaluate how easy the current re-3585   vised document is for the readers to understand ,   where a lower FKGL indicates a more readable   document . FKGL is a popular metric that has been   used by many prior works ( Solnyshkina et al . , 2017 ;   Xu et al . , 2016 ; Guo et al . , 2018 ; Nassar et al . ,   2019 ; Nishihara et al . , 2019 ) to measure the read-   ability of documents . For Content Preservation ,   we use the BLEURT score ( Sellam et al . , 2020 )   to measure how much content has been changed   from the previous document to the current revised   one , where a higher BLEURT score indicates more   content has been preserved . BLEURT has been   shown to correlate better with human judgments   than other metrics that take semantic information   into account , e.g. METEOR ( Banerjee and Lavie ,   2005 ) or BERTScore ( Zhang et al . , 2020b ) .   F Details on Human Evaluation for   Single Human Revision Quality   Evaluation Data . To evaluate how do human re-   visions affect the text quality , we sample 50 sin-   gle document - revisions , which contains 50 ran-   domly sampled documents and each document has   1 document - revision .   Result Analysis . In Table 12 , we observe that   human revised documents generally improve the   overall quality of original documents . As for the au-   tomatic metrics , BLEURT indicates that human re-   visions preserve much of the content , and FKGL   shows that the readability of original documents   improves by human revisions . However , SLOR   andEG show a slight drop in performance . We   conjecture this is because ( 1 ) SLOR and EG   are not well - aligned with human judgements , or   ( 2 ) human revisions make original documents less   ﬂuent and less coherent .   Correlation Analysis . To analyze how auto-   matic metrics are correlated with human overall   quality score , we compute the Pearson ( Kowal-   ski , 1972 ) and Spearman ( Zwillinger and Kokoska ,   1999 ) correlation coefﬁcients between the auto-   matic metrics and the human overall quality scores   based on 50 single document - revisions and 21 it-   erative document - revisions . Table 13 shows that   BLEURT and FKGL are positively correlated   with human overall quality score , while SLOR   andEG are negatively correlated with human   overall quality score .   G Details on Human Evaluation   Conﬁguration for Model Revisions   First , we evaluate how do model revisions affect   the quality of the document . We randomly sample   30 single document - revisions which do not con-   tain Meaning - changed edits , and input the original   documents to the best - performing model to get the   model - revised documents . Then , for each data pair ,   we randomly shufﬂe model revisions and human   revisions , and ask human evaluators to select which   revision leads to better document quality in terms   of :   •Content Preservation : keeping more content   information unchanged ;   •Fluency : ﬁxing more grammatical errors or syn-   tactic errors ;   •Coherence : making the sentences more logically   linked and organized ;   •Readability : making the text easier to read and   understand ;   •Overall Quality : better improving the overall   quality of the document .   We provide the evaluation interface in Figure 6 .   Secondly , we evaluate how does model gener-   ated text quality vary across revision depths . We   use the same set of 21 iterative document - revisions   in § 5.1 . We feed the original documents into the   best - performing model to obtain the model revised   documents at each revision depth . For each data   pair , we randomly shufﬂe model revisions and hu-   man revisions , and ask human evaluators to judge   which one gives better overall text quality . We   provide the evaluation interface in Figure 7.3586   H Details on Automatic Evaluation for   Model Revisions   Table 14 provides detailed automatic evaluation   results for F andP , including SARI ,   BLEU , and ROUGE . We choose these automatic   metrics following prior text revision works ( Malmi   et al . , 2019 ; Dong et al . , 2019 ; Mallinson et al . ,   2020 ) . Note that the KEEP score of Baseline is not   100 because the source sentence keeps all n - grams ,   but there might be certain n - grams that are not kept   in the reference sentence . This results in the non-   perfect KEEP score since both recall and precision   are calculated .   Table 15 further provides SARI score under dif-   ferent revision depths as well as different edit-   intentions . We ﬁnd that P only conduct   deletions in the revision depth 3 , and the SARI   score for each edit - intention varies a lot across dif-   ferent revision depths .   Table 16 and Table 17 are some examples of   iterative text revisions generated by F andP- trained on I TR- . We observe   that while F can make more edits with more   iterations than P , it can not ensure the qual-   ity of its generated edits . F often insert some   random out - of - context tokens into the original text ,   and distort the semantic meaning of the original   text . P is better at preserving the semantic   meaning of the original text , but it is more likely to   delete phrases or tokens in deeper revision depth.3587358835893590