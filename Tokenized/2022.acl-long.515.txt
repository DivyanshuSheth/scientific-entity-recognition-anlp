  Hao PengJungo KasaiNikolaos PappasDani YogatamaZhaofeng Wu   Lingpeng KongRoy SchwartzNoah A. SmithPaul G. Allen School of Computer Science & Engineering , University of WashingtonAmazon Web ServicesDeepMindAllen Institute for Artificial IntelligenceSchool of Computer Science & Engineering , Hebrew University of JerusalemDepartment of Computer Science , The University of Hong Kong   { hapeng,jkasai,npappas,zfw7,nasmith}@cs.washington.edu   dyogatama@deepmind.com , lpk@cs.hku.hk   roy.schwartz1@mail.huji.ac.ilAbstract   Transformer architectures have achieved state-   of - the - art results on a variety of natural lan-   guage processing ( NLP ) tasks . However , their   attention mechanism comes with a quadratic   complexity in sequence lengths , making the   computational overhead prohibitive , especially   for long sequences . Attention context can be   seen as a random - access memory with each to-   ken taking a slot . Under this perspective , the   memory size grows linearly with the sequence   length , and so does the overhead of reading   from it . One way to improve the efficiency is   to bound the memory size . We show that dis-   parate approaches can be subsumed into one   abstraction , attention with bounded - memory   control ( A ) , and they vary in their organi-   zation of the memory . Areveals new , unex-   plored possibilities . First , it connects several   efficient attention variants that would otherwise   seem distinct . Second , this abstraction gives   new insights — an established approach ( Wang   et al . , 2020b ) previously thought to notbe appli-   cable in causal attention , actually is . Last , we   present a new instance of A , which draws in-   spiration from existing Aapproaches , but re-   places their heuristic memory - organizing func-   tions with a learned , contextualized one . Our   experiments on language modeling , machine   translation , and masked language model fine-   tuning show that our approach outperforms pre-   vious efficient attention models ; compared to   strong transformer baselines , it significantly im-   proves the inference time and space efficiency   with no or negligible accuracy loss .   1 Introduction   Transformer architectures are now central in natural   language processing ( Vaswani et al . , 2017 ) . They   rely on the attention mechanism ( Bahdanau et al . ,   2015 ) to contextualize the input . The context can   be seen as a random access memory whose size lin-   early grows with the sequence length ; each queryreads from it using a softmax - normalized linear   combination , with overhead linear in the memory   size . This amounts to a quadratic complexity over-   all , making transformers ’ computational overhead   prohibitive , especially for long sequences .   One way to improve attention ’s efficiency is   to bound its memory size . Imposing a constant-   sized constraint over the memory ensures that read-   ing from it has constant time and space overhead ,   yielding a linear overall complexity in sequence   lengths . This is in fact a common strategy adopted   by several recent works . In this work , we show   that some of these works are closely connected   in ways that , to date , have gone unremarked . We   propose attention with bounded - memory control   ( A ) , a unified abstraction over them . In A ,   constant - sized memories are organized with vari-   ous control strategies , e.g. , induced from heuristic   patterns ( Beltagy et al . , 2020 ; Zaheer et al . , 2020 ;   Ainslie et al . , 2020 ; Rae et al . , 2020 , inter alia ) ,   locality assumptions ( Parmar et al . , 2018 ; Liu et al . ,   2018 ) , or positions ( Wang et al . , 2020b ) .   These strategies , by and large , are “ context-   agnostic . ” In response to this , we propose A ,   a particular instance of Athat learns a contex-   tualized control strategy from data . Specifically ,   Auses a neural network to determine how   to store each token into the memory ( if at all ) .   Compared to previous bounded - memory models ,   it strikes a better trade - off between accuracy and   efficiency : controlling for the accuracy , A   can get away with much smaller memory sizes .   Amodels ( including A ) come with a   linear complexity in sequence lengths , and admit   recurrent computation graphs in causal attention   ( self - attention over the prefix ) . Therefore they are   appealing choices in a variety of applications , in-   cluding text encoding , language modeling and text   generation . This leads to a surprising finding . Lin-   former ( Wang et al . , 2020b ) , an established effi-   cient attention method , was previously thought not7469to be applicable in causal attention or autoregres-   sive decoding ( Tay et al . , 2020 ) . Through the A   view , we show that it actually is , and achieves com-   petitive performance in our machine translation   experiments .   Aconnects existing models that would oth-   erwise seem distinct , reveals new insights into   established methods , and inspires new efficient   attention architectures . We explore its applica-   tions in transformers , as a drop - in substitute for   the canonical softmax attention . Aoffers a   novel lens that can help future research in the   analysis of transformers , where the theoretical in-   sights are still catching up with empirical suc-   cess . Experiments on language modeling , machine   translation , and masked language model finetun-   ing show that our Amodel outperforms pre-   vious Aapproaches in accuracy with a much   smaller memory size . Compared to the strong   transformer baseline , Aachieves a signif-   icant speedup and memory savings at inference   time , with no or negligible accuracy loss . The   efficiency improvements are more prominent for   long sequences , suggesting that the asymptotic sav-   ings are even more appealing in applications in-   volving long sequences . We release our code at   https://github.com/Noahs-ARK/ABC .   2 An Outer - Product View of Attention   This section presents our outer - product memory   perspective of attention , which allows for a smooth   transition to later discussion .   In attention , a sequence of queries { q}at-   tend to a memory withNslots , each storing a   key andvalue pair : K= [ k , . . . , k],V=   [ v , . . . , v]∈R.Query qreads from the   memory using a softmax - normalized linear combi-   nation , producing a d - dimensional vector :   attn(q,{k},{v } ) = Vsoftmax ( Kq).(1 )   This takes O(N)time and space . When the atten-   tion with Nqueries can be parallelized ( e.g. , in   text encoding ) , it takes linear time and quadratic   space ; when it can not be ( e.g. , in decoding ) , it   takes quadratic time and linear space .   The memory can be equivalently represented   as sums of vector outer products : K = IK = Pe⊗k , V = Pe⊗v . Iis the identity   matrix , and ⊗denotes the outer product : [ x⊗y]=xy . N - dimensional vectors { e}form   the standard basis : ehas the ith element being   one and others zeros . We can view eascontrol   vectors that determine where to store kandv :   e⊗k=   0 , . . .0|{z},1,0 , . . . , 0|{z}⊗k   =    0|{z};k;0|{z}.(2 )   TheN - by - dmatrix on the last line has its ith row   being kand all others zeros ; in this sense , kis   stored in the ith slot by e , not affecting others .   3 Attention with Bounded Memory   A straightforward way to improve attention ’s ef-   ficiency is to bound its memory size . Our outer-   product view of attention provides a straightfor-   ward way to devise this , by replacing { e}with   control vectors that select n≪Nvectors to   attend to . We dub this approach attention with   bounded - memory control ( A ) . Concretely , let   eK , eV∈Rdenote a constant - size memory with   nslots , with nseta priori .   eK = Xϕ⊗k , eV = Xϕ⊗v.(3 )   { ϕ∈R}denotes a sequence of control vec-   tors . The output is calculated by attending to eK   andeV : A(q,{k},{v},{ϕ } ) =   eVsoftmax   eKq   . ( 4 )   We will discuss various ways to construct { ϕ}in   the subsequent sections . Reading from the memory   takes a constant O(n)time and space ; therefore   A ’s overall complexity is O(Nn ) , linear in the   sequence length .   Eq . 3 offers an equivalent recurrent computa-   tion , which is particularly useful in causal attention   where only the prefix is looked at ,   eK = eK+ϕ⊗k , ( 5 )   likewise for eV.eKandeVcan be seen as the   recurrent hidden state that encodes the prefix .   In what follows , we study several existing effi-   cient attention approaches and show that they are   in fact instances of the Aabstraction.74703.1 Linformer   Linformer ( Wang et al . , 2020b ) is an established ef-   ficient transformer variant that has proven success-   ful in masked language modeling and text encoding .   It assumes fixed - length inputs and learns a low - rank   approximation of the attention weights . A learned   n - by - Nmatrix Wdown projects the N - by - d   dimensional keys and values along the timestep di-   mension , to an n - by - dmemory : eK = WK ,   eV = WV ; they are then used for attention   computation with Eq . 4 . This yields a linear com-   plexity in the input length . Linformer is an A   instance with ϕ=W(ith column ) , and in this   sense , it learns a control vector for each position .   Previous works have noted that Linformer can-   notbe efficiently applied in causal attention ( Table   1 of Tay et al . , 2020 ) . Indeed , it is less straightfor-   ward to avoid mixing future with the past when   projecting along the timestep dimension . A   reveals that , in fact , Linformer isapplicable in   causal attention . Like all Amodels , it admits   a linear - complexity recurrent computation ( Eq . 5 ):   eK = eK+ϕ⊗k . This confirms A ’s   benefits : it reveals new insights about existing mod-   els and reassesses their applications and impact .   Our experiments show that Linformer achieves   competitive performance in machine translation .   3.2 Clustering - Based Attention   Improving attention ’s efficiency with clustering has   received an increasing amount of interest ( Kitaev   et al . , 2020 ; Roy et al . , 2020 ; Wang et al . , 2020a ,   inter alia ) .Abears interesting connections to   clustering - based methods . Here we discuss an   approach that closely follows Vyas et al . ( 2020 ) ,   except that it clusters keys and values instead of   queries , and only attends to the centroids to reduce   the effective context size . Formally , keys and val-   ues are grouped into n < N clusters { ek } ,   { ev}.Let an N - by - nbinary matrix Mde-   note the cluster membership shared between keys   and values . M= 1iff.kis assigned to cluster   ekandvtoev . The jth centroid for the keys is   ek = XMPMk ; ( 6)likewise for the values . It then attends over the cen-   troids using Eq . 4 , with eK= [ ek , . . . , ek]=Xe⊗ek = Xe⊗XMPMk   = X   XeMPM   ⊗k .   The last line indicates that this model is an instance   ofA:ϕ=P(M / PM)e . The   stack of centroids can be seen as the constant - size   memory . Putting aside the clustering overhead ( i.e. ,   constructing Mand computing centroids ) , it has a   linear complexity in the sequence length .   3.3 Sliding - Window Attention   In some applications , being able to remove entries   from the memory can be beneficial : clearing up   older context frees slots for more recent ones , pro-   moting a locality inductive bias . Aoffers the   capability to do so , if augmented with an additional   matrix multiplication . We use the sliding - window   attention as an example .   Attending to the most recent ninput tokens ( Belt-   agy et al . , 2020 ; Zaheer et al . , 2020 ; Sukhbaatar   et al . , 2021 , inter alia ) can be seen as a first-   in - first - out queue that “ pops ” out the oldest to-   ken while “ pushing ” in the most recent one :   eK= [ k , ... , k ] . The pop operation can   be achieved by multiplying an n - by - nupper shift   matrix : U = δ , with δbeing the Kronecker   delta ( i.e. , Uhas ones only on the superdiagonal   and zeros elsewhere ) . Left - multiplying Uagainst   eKshifts its rows one position up , with zeros   appearing in the last :   UeK = U   k , . . . , k|{z }    =    k , . . . , k , k| { z } , 0∈R.   Then the most recent token can be put into the   slot freed up : eK = UeK+e⊗k . U   andϕ=eensure a first - in - first - out queue . Di-   lated and stride convolution patterns ( Beltagy et al . ,   2020 ) can be similarly recovered ( § A.4 ) .   Recurrently multiplying Usimulates the discrete   pop operation ( Grefenstette et al . , 2015 ; Joulin and   Mikolov , 2015 ; Yogatama et al . , 2018 ) in a differen-   tiable way . This is reminiscent of recurrent neural   networks , while in this case Uisnever updated as7471parameters . It is exciting to explore learning U ,   but is beyond the scope of this work .   Discussion . Besides the models discussed above ,   certain variants of Rae et al . ( 2020 ) and sparse at-   tention patterns ( local - to - global attention ; Beltagy   et al . , 2020 ; Zaheer et al . , 2020 ; Ainslie et al . , 2020 )   can also be seen as instances of A(§A ) . A   provides a unified perspective of them , and at the   same time points out their limitations : their control   strategies are context - agnostic . In response to this ,   in § 4 we propose to learn a contextualized strategy   from data . Table 1 analyzes various Amodels ,   and Table 2 details their complexity .   4 Learned Memory Control   TheAabstraction connects several existing ap-   proaches that would otherwise seem distinct . This   inspires the design of new architectures . We hy-   pothesize that learning a contextualized strategy   can achieve better performance . This section intro-   duces A. It parameterizes ϕwith a single-   layer multi - layer perceptron ( MLP ) that takes as   input the token ’s representation x , and determines   which slots to write it into and how much .   α = exp ( Wx),ϕ=α , Xα.(7 )   Matrix Wis learned . exp is an elementwise   activation function . The motivation is to allow for   storing a “ fractional ” ( but never negative ) amount   of input into the memory . Using a non - negative   activation , however , has a drawback : the scales ofPϕ⊗kandPϕ⊗vwould grow with the   sequence lengths , making training less stable . To   overcome this , we divide αvectors by their sum .   This functions as normalization and aims to offset   the impact of varying sequence lengths . It admits   the recurrent computation graph as in Eq . 5 , and   has a linear complexity in the sequence length .   A key design choice of Ais that its ϕ   depends only on current input x. This helps ( 1 )   keep the recurrent computation efficient in prac-   tice ( Lei et al . , 2018 ) , and ( 2 ) make it applicablein not only encoder self - attention and cross atten-   tion , but also causal attention . Concurrently to this   work , Goyal et al . ( 2021 ) and Ma et al . ( 2021 ) also   proposed methods to learn contextualized control .   They compute ϕfrom previous layer ’s memory ,   revealing the full sequence to the control vectors .   As a result , these two approaches are unsuitable for   causal attention .   A , as other Amodels , can be used as   a drop - in replacement for the canonical softmax   attention , and we apply its multihead variant in   transformers . With proper parameter sharing , the   number of additional parameters Aincurs   is small : inspired by Wang et al . ( 2020b ) , we tie   ϕ-MLP ’s parameters across different layers , which   adds less than 1 % parameters to the models .   A : context - agnostic then context-   dependent attention . We now dissect A   and show that it can be seen as a cascade of   two attention mechanisms : one with a learned   context - agnostic “ pseudo query ” followed by one   with a context - dependent query . Our analysis starts   with a one - dimensional example ; the conclusion   generalizes to higher - dimensional cases .   Example 1 . Consider Awith a single mem-   ory slot ( n= 1 ) . It is parameterized with a learned   vector w , andϕ= exp ( w·x)/Pexp(w ·   x ) . Since ϕis a scalar here , ϕ⊗k=ϕk .   eK = X(ϕ⊗k )   = Xexp(w·x)Pexp(w·x)k   = attn    w,{x},{k}   .   In other words , eKuseswas a “ pseudo - query ”   to attend to { x}and{k } . Likewise , eV=   attn(w,{x},{v } ) . Despite its similar-   ity to the standard softmax attention , Example 1   has a more efficient linear complexity in sequence   lengths . w ’s being context - independent is the key   to the savings . Table 2 details its complexity .   Example 1 ’s conclusion generalizes to higher-   dimensional cases : the jth dimension of { ϕ}at-   tends to { x}and{k}using the jth row of W   as the context - independent pseudo - query ; nsuch   attention mechanisms run in parallel , stacking the7472Model Section ϕ Mem . Control   Sliding - window § 3.3 eeK = UeK+ϕ⊗k   Linformer § 3.1 W   eK = eK+ϕ⊗kL2 G Pattern § A.1 eifxis the ith global token   A § A.2 e , where i∼unif{1 , n }   Comp . Trans . § A.3 e   Clustering § 3.2P   M / PM   e   A § 4 exp(Wx)/Pexp(Wx )   Time Complexity Space Complexity   Model Mem . Per Query Overall Mem . Per Query Overall   Softmax Attention - O(N)O(N ) - O(N)O(N )   A O(N)O(n ) O(nN ) O(n ) O(n ) O(nN )   results into n - by - dmemory eKandeV. Intuitively ,   it is the “ real queries ” { q}that encode “ what infor-   mation is useful for the prediction task . ” Without   access to them , Asummarizes the input for   ntimes using different pseudo - queries , aiming to   preserve enough information in the memory for   onward computation . The attention output is calcu-   lated with the context - dependent real queries using   Eq . 4 . § B.2 presents a detailed derivation .   Connections to other prior works . Although   starting from distinct motivations , Aclosely   relates to hierarchical attention ( HA ; Yang et al . ,   2016 ) . HA summarizes the context into higher-   level representations with a cascade of attention   mechanisms , e.g. , words to sentences , and then to   documents . Aapplies two types of attention .   The first learns context - agnostic pseudo - queries   and attends to the same sequence for ntimes in   parallel , while the second retrieves from the mem-   ory with real queries . HA , in contrast , summarizes   non - overlapping segments at each level .   The learned pseudo - queries closely relate to the   inducing point method in set attention ( ISA ; Lee   et al . , 2019 ) . ISA applies a non - linear feedforward   network between a cascade of two attention mod - ules . This precludes the outer - product memory   computation and efficient recurrences in A.   Another line of work “ linearizes ” attention   through kernel tricks and also applies bounded   memory : their feature map dimensions are   analogous to memory sizes . They substitute   the softmax with approximations ( Peng et al . ,   2021 ; Choromanski et al . , 2021 ) , heuristically de-   signed ( Katharopoulos et al . , 2020 ; Schlag et al . ,   2021 ) , or learned ( Kasai et al . , 2021b ) functions .   A keeps the softmax , but over a smaller   constant - sized context . This can be useful in prac-   tice : ( 1 ) Aprovides a unified perspective of   several efficient attention methods , allowing for   borrowing from existing wisdom to design new   architectures ; ( 2 ) it draws a close analogy to the   canonical softmax attention , and is better - suited as   its drop - in substitute in various application settings ,   as we will show in the experiments ; ( 3 ) empirically ,   we find that Acan get away with a much   smaller memory size to retain the accuracy . Peng   et al . ( 2021 ) and Schlag et al . ( 2021 ) use gating   to promote recency bias . The same technique is   equally applicable in Amodels .   The learned contextualized memory control is   reminiscent of the content - based addressing in neu-7473ral Turing machines ( NTM ; Graves et al . , 2014 ) .   Acomputes the control vectors { ϕ}as a   function of the input , but notof the memory as in   NTM . This ensures that the control vectors at differ-   ent timesteps can be computed in parallel , improv-   ing the time efficiency in practice ( Lei et al . , 2018 ;   Peng et al . , 2018 ) . Analogies between memory   and neural architectures are also made by other pre-   vious works ( Hochreiter and Schmidhuber , 1997 ;   Weston et al . , 2015 ; Le et al . , 2020 , inter alia ) .   5 Experiments   We evaluate Amodels on language modeling   ( § 5.1 ) , sentence - level and document - level machine   translation ( § 5.2 ) , and masked language model fine-   tuning ( § 5.3 ) . Dataset statistics and implementa-   tion details are summarized in § C.   5.1 Language Modeling   Setting . We experiment with WikiText-103 , sam-   pled text from English Wikipedia ( Merity et al . ,   2017 ) . The B model with standard softmax   attention is the strong transformer - based language   model by Baevski and Auli ( 2019 ) . We com-   pare the following Avariants , which build on   B , but replace the softmax attention with linear-   complexity bounded - memory attention alternatives   while keeping other components the same .   •A , as described in § 4 , learns a contextual-   izedexp - MLP as the ϕfunction .   • Linformer ( § 3.1 ; Wang et al . , 2020b ) .   •Astores each token in a randomly - selected   memory slot with ϕ=e.iis uniformly   drawn from { 1 , . . . , n } at each time step . This   helps us quantify the differences between ran-   dom and learned bounded - memory controls .   We consider two model size settings :   •16 layers ( Baevski and Auli , 2019 ) . All models   have around ∼242 M parameters . They train with   512 - token segments , and evaluate with 0 or 480   context sizes : a 0- or 480- length prefix precedes   each evaluation segment .   •32 layers ( Kasai et al . , 2021b ) . All models have   ∼484 M parameters . This setting applies layer   dropout ( Fan et al . , 2020 ) , and evaluates with a   256 context size . It aims to compare Ato   several kernel - based efficient attention variants :   ELU ( Katharopoulos et al . , 2020 ) , RFA ( Peng   et al . , 2021 ) , and T2R ( Kasai et al . , 2021b ) .   Results . Table 3a compares Avariants using   Baevski and Auli ( 2019 ) ’s 16 - layer setting . AmongDev . Test   Model n 0 480 0 480   B - 19.8 18.4 20.5 19.0   Linformer 64 26.5 27.1 27.2 30.7   A 64 23.2 22.3 24.0 23.1   A 32 21.2 19.7 21.9 20.5   A 64 20.4 18.9 21.1 19.5   Model n Dev . Test   †B - 17.9 18.5   †ELU 128 22.0 22.8   †RFA 32 20.4 21.3   †T2R 32 20.1 20.8   A 32 19.2 19.9   Amodels , A achieves the best perfor-   mance for both context sizes . With a memory   sizen= 64 , Aoutperforms both Linformer   andAby more than 2.9 test perplexity ; and   the gap is larger with the longer 480 - length con-   text : more than 3.6 test perplexity . A-32   outperforms its larger - memory Acounterparts   by more than 2.1 test perplexity . These results   confirm A ’s advantages of using a contex-   tualized strategy . Surprisingly , Linformer under-   performs A , and its performance drops with   the larger 480 - length context window . This sug-   gests that , while successful in text encoding , Lin-   former ’s position - based strategy is a suboptimal   design choice for causal attention , at least for long   context . All Amodels underperform theB ,   with A-64 having the smallest gap of 0.5   perplexity . A-32 outperforms kernel - based   methods by more than 0.9 test perplexity , using   Kasai et al . ( 2021b ) ’s 32 - layer setting ( Table 3b ) .   5.2 Machine Translation   Datasets . To assess their performance over var-   ious output lengths , we compare Amodels on   sentence- and document- level machine translation .   •Sentence - level translation with WMT14 EN - DE7474Model Cross nCausal nBLEU   B - - 27.2   A 32 32 25.7   A 64 64 26.2   Linformer 32 32 26.6   Linformer 64 64 26.7   A 32 8 27.1   A 32 32 27.3   Model Cross nCausal nBLEU   B - - 39.9   Linformer 128 64 -   A 128 64 38.6   A 128 64 39.7   ( Bojar et al . , 2014 ) . The preprocessing and data   splits follow Vaswani et al . ( 2017 ) .   •Document - level translation with IWSLT14 ES-   EN ( Cettolo et al . , 2014 ) . We use Miculicich   et al . ( 2018 ) ’s data splits and preprocessing . Fol-   lowing standard practice ( V oita et al . , 2019 ) , a   4 - sentence sliding window is used to create the   dataset , i.e. , each instance has 4 sentences .   Setting . We compare Avariants as in § 5.1 .   § C.2 further compares to the clustering - based   ( § 3.2 ) and sliding - window ( § 3.3 ) Avariants .   TheBmodel they build on is our implemen-   tation of transformer - base ( Vaswani et al . , 2017 ) .   Avariants replace decoder cross attention and   causal attention with bounded - memory attention ,   while keeping softmax attention for the encoder ,   since its overhead is much less significant ( Kasai   et al . , 2021a ) ; other components are kept the same .   § C.2 studies a model that replaces allsoftmax at-   tention with A. It performs on par with   B , confirming A ’s broad applicability   in various application scenarios . We evaluate with   SacreBLEU ( Post , 2018).Results . Table 4a summarizes sentence - level ma-   chine translation results on the WMT14 EN - DE test   set . Overall Aperforms on par with B ,   with either 32 - 32 cross - causal memory sizes or 32-   8 . Even with smaller memory sizes , it outperforms   other Avariants by more than 1.1 BLEU . Dif-   ferently from the trend in the language modeling   experiment ( § 5.1 ) , Linformer outperforms A   by more than 0.5 BLEU . We attribute this to the   smaller sequence lengths of this dataset . A   outperforms other Amodels by more than 0.4   BLEU , even with smaller memory sizes .   The trend is similar on document - level trans-   lation with IWSLT14 ES - EN ( Table 4b ) , except   thatAslightly underperforms B by 0.2   BLEU . This suggests that even with longer se-   quences , Ais effective despite its bounded   memory size . Linformer fails to converge even   with multiple random seeds , suggesting the limita-   tions of its purely position - based strategy in tasks   involving decoding varying - length text .   5.3 Masked Language Model Finetuning   Setting . We compare the Avariants as in § 5.1 .   It is interesting to pretrain Afrom scratch ,   but we lack the resources to do so . Instead , we   warm - start from a pretrained RoBERTa - base ( Liu   et al . , 2019 ) trained with the softmax transformer ,   swap its attention with Avariants , and continue   pretraining with the masked language modeling   ( MLM ) objective on a concatenation of BookCor-   pus ( Zhu et al . , 2015 ) , English Wikipedia , Open-   WebText ( Gokaslan and Cohen , 2019 ) , and Real-   News ( Zellers et al . , 2019).Then the models are   finetuned and evaluated on downstream classifica-   tion datasets from the the GLUE benchbark ( Wang   et al . , 2019 ) . This is an appealing setting , since it   avoids reinvesting the huge amounts of resources   already put into pretraining .   Results . Table 5 compares downstream text clas-   sification performance . B indicates a baseline   that continues pretraining RoBERTa - base on our   data . Following standard practice , we report devel-   opment accuracy . Linformer achieves competitive7475Model nMNLI QNLI QQP SST Avg .   B - 87.2 92.4 91.7 94.3 91.4   Linformer 64 85.3 91.8 90.8 92.4 90.1   Linformer 128 86.1 91.9 91.4 93.7 90.8   A 64 85.6 91.8 91.7 93.8 90.7   A 128 87.1 92.6 91.8 94.4 91.5   performance , aligned with Wang et al . ( 2020b ) ’s   results . Aoutperforms Linformer , and per-   forms on par with or better than B , affirming   the benefits of using contextualized memory or-   ganization in MLM . Afails to converge in   continued pretraining even with multiple seeds .   Based on the above results , we think A   can achieve competitive performance when pre-   trained from scratch , just as Linformer does ( Wang   et al . , 2020b ) . Further empirical exploration is be-   yond our budget and left for future work .   6 Analysis   Decoding efficiency over varying sequence   lengths . A ’s efficiency gains can be more   prominent for long sequences . We study A ’s   decoding overhead with varying sequence lengths .   Following Kasai et al . ( 2021b ) , we consider   a sequence - to - sequence generation experiment .   Three linear - complexity models are compared :   RFA ( with 256/128 cross / causal memory sizes ;   Peng et al . , 2021 ) , T2R ( 32/4 ; Kasai et al . , 2021b ) ,   andA(32/8 ) . The sizes are chosen to maxi-   mize efficiency without accuracy drop . T2R needs   to be finetuned from a pretrained transformer to   match its performance , while others do n’t .   All linear - time models achieve consistent decod-   ing speed for different lengths ( Figure 1a ) , sub-   stantially outpacing the softmax attention base-   line , especially for long sequences . In particular ,   A decodes ∼1.25 times faster than RFA ,   another competitive model that can match trans-   former ’s accuracy without a warm start from a pre-   trained model . This can be attributed to the fact that   Aachieves similar accuracy with a much   smaller memory . T2R ’s memory sizes are simi-   lar to A ’s , but it decodes about 20 % faster .   This is because it does notcompute the softmax   when calculating attention output , while A   does ( Eq . 4 ) . These results show that Ais   an appealing modeling choice for decoding tasks ,   especially when training from scratch is desired .   A also achieves significant savings in   terms of memory overhead ( Figure 1b ) . A ,   RFA , and T2R ’s curves are similar .   Text encoding efficiency . We compare the effi-   ciency of Aagainst softmax attention and   Linformer when used as text encoders . The mod-   els ’ sizes mirror those in the MLM experiment   ( § 5.3 ) . Table 6 summarizes inference time and   memory overhead with 512 - length inputs , batch   size 16 . Both Aand Linformer achieve infer-   ence speed gains and memory savings over B.   Linformer is faster , since its linear projection is   cheaper to compute than A ’s MLP . Infer-   ence speed is measured on the same V100 GPU .   The trend in memory overhead is similar .   Although Aslightly underperforms Lin-   former in terms of inference speed , it can be a more   appealing architectural choice in practice : in all   of our 5 experiments , Aoutperforms other   Amodels in accuracy . Linformer , in contrast ,   fails to converge or yields sub - optimal performance   on some tasks . This confirms its flexibility and ap-7476B Linformer A   n - 64 128 64 128   Speed 1.0× 1.7×1.5×1.5×1.3×   Memory 1.0× 0.5×0.6×0.5×0.6×   Cross n   8 16 32 64824.7 25.2 25.6 25.5   16 - 25.4 25.7 25.6   32 - - 25.7 25.8   64 - - - 25.8   plicability in various settings .   Memory size ’s impact on accuracy . Practically ,   one may want to minimize the memory size to im-   prove efficiency . We use the WMT14 EN - DE ex-   periment to investigate how memory size affects ac-   curacy . Using the § 5.2 ’s setup , we vary A ’s   cross and causal attention memory sizes and com-   pare their translation quality on the development   data . They are selected from { 8,16,32,64 } , with   cross attention ’s equal to or larger than causal ’s :   cross attention is more important than causal atten-   tion in machine translation ( Michel et al . , 2019 ) .   Our results ( Table 7 ) align with this observation :   when cross attention memory is large enough , re-   ducing causal attention memory size from 64 to 8   has a minor 0.3 BLEU drop . Surprisingly , A   with 8 - 8 sized cross - causal memory is only 1.1   BLEU behind the best - performing configuration .   7 Conclusion   We presented attention with bounded - memory con-   trol ( A ) . It provides a unified perspective of sev-   eral recently - proposed models , and shows that they   vary in the organization of the bounded memory .   Areveals new insights into established meth-   ods and inspires new architectures . We proposed   A , a particular instance of Athat learns a   contextualized memory control . On language mod-   eling , machine translation , and masked language   model finetuning , Aoutperforms previous   Amodels . Compared to the strong transformerbaseline , Aachieves substantial efficiency   improvements with no or negligible accuracy loss .   Acknowledgments   We would like to thank the ARK group at the Uni-   versity of Washington for their helpful feedback ,   and the anonymous reviewers for their thought-   ful comments . This work was supported in part   by NSF grant 2113530 and a Google Fellowship .   Nikolaos Pappas was supported by the Swiss Na-   tional Science Foundation grant P400P2_183911 .   References747774787479   A Other AModels   A.1 Sparse Local - to - global Attention   It sparsifies attention pattern to reduce the number   of tokens that are attended to ( Beltagy et al . , 2020 ;   Zaheer et al . , 2020 , inter alia ) . All queries attend   to a subset of n < N “ global tokens , ” while ignor-   ing others . Therefore the effective context size is   reduced to n. The global tokens are usually pre-   selected by positions according to some heuristics .   Local - to - global attention is an instance of A : it   can be recovered by letting ϕ=eifxis the ith   global token ( i= 1 , . . . , n ) , and the zero vectors   for others .   A.2 Random Memory Control   As a baseline , Astores each token in a   randomly - selected memory slot . This is achieved   by letting ϕ=e , where iis uniformly drawn   from{1 , . . . , n } for each t. It is designed as a   baseline to Aand Linformer to quantify the   differences between random and learned bounded-   memory control .   Random sparse attention patterns are explored   by Zaheer et al . ( 2020 ) , where a subset of n < N   tokens are randomly selected to be attended to by   all tokens . Ais different , and it attends to all   tokens , but randomly “ squash ” them into an n - slot   memory .   A.3 Compressive Transformer with Mean   Pooling   The compressive transformer ( Rae et al . , 2020 )   explores various ways to “ squash ” long context   into smaller and more compact representations . It   achieves state - of - the - art performance on several   language modeling benchmarks . We show that at   least the mean - pooling variant of the compressive   transformer can be seen as an Ainstance .   The mean - pooling variant of the compressive   transformer compresses the context by   K=   k , . . . , k∈R   →eK=   ( k+···+k)|{z } /c ,   ( k+···+k)| { z } /c . . . ,   ( k+···+k)| { z } /c∈R.where c = N / n is the compression ratio . Here   Nmod n= 0 is assumed , since otherwise the   sequence can be padded to .   The above model is an Ainstance by letting   ϕ=e / c. ( 8)   A.4 Dilated Convolution Attention Patterns   The dilated attention pattern is similar to the sliding   window attention and only considers the context   within a predefined window . It differs in that it   attends to every other token :   eK= [ k , k , ... , k , k].(9 )   It can be simulated with two separate queues eK   andeK :   eK= (   UeK+e⊗k , iftis odd   eK , otherwise   eK= (   UeK+e⊗k , iftis even   eK , otherwise   Likewise for the values . Depending on t , the query   attends to one of the two queues : output =   (  eVsoftmax ( eKq),iftis odd eVsoftmax ( eKq),otherwise .   The above implementation could incur consider-   able amount of overhead and may be actually more   expensive than the the original dilated window for-   mulation . Therefore it has more conceptual value   than practical value .   A.5 Shared Workspace and Linear Unified   Nested Attention   Concurrently to this work , shared   workspace ( SW ; Goyal et al . , 2021 ) and lin-   ear unified nested attention ( LUNA ; Ma et al . ,   2021 ) also propposed methods to learn contextual-   ized memory control strategies . Both can be seen   as instances of A. At layer ℓ , their ϕis a func-   tion of previous layer ’s memory eX∈R   and current layer ’s input X∈R :   ϕ=h   softmax   eXXi , ( 10 )   where [ · ] denotes the ith column of a matrix .   Query , key , and value projections are suppressed   for notation clarity.7480SW and LUNA reveal the entire sequence to the   control vectors , by constructing ϕas a function of   previous layer ’s memory . Although both admit the   recurrent computation as all Amodels do , they   are ill - suited for causal attention and autoregressive   decoding , since future information is “ leaked ” to   ϕfrom the previous layer . LUNA resorts to a   variant of Katharopoulos et al . ( 2020 ) in causal   attention ( Ma et al . , 2021 ) . In contrast , A   never conditions ϕon previous layer ’s memory ,   but only on the current layer ’s input .   B More Details about A - MLP   B.1 Normalization in Causal Attention   An equivalent implementation to Eq . 7 is to nor-   malize eKandeVinstead of ϕvectors :   α = exp ( Wx),ϕ=α ,   ¯K = eK , Xα.¯V = eV , Xα .   output = ¯Vsoftmax ( ¯Kq ) .   M / zdivides the ℓth row of matrix Mby vector   z’sℓth dimension . This admits a linear complex-   ity computation graph for the causal variant of   A.   B.2 Higher - Dimensional Case of Example 1   This section generalizes Example 1 to higher di-   mensional cases . Assume that the constant - sized   memory has nslots . ϕis cauculated as in Eq . 7 .   TheneK = Pϕ⊗k∈R. Each row   ofeKcan be seen as a separate attention mecha-   nism with a pseudo query . Let [ · ] denote the ℓth   row / dimension of a matrix / vector . Then for any   ℓ= 1 , . . . , n ,   eK=X[ϕ]⊗k   = Xexp([W]·x)Pexp([W]·x)k   = attn    [ W],{x},{k}∈R.   In other words , there are nattention mechanisms in   total , each with a separately - parameterized pseudo-   query [ W ] . They summarize the context for n   times in parallel , each producing a d - dimensional   vectors . These output vectors are then stacked into   n - by - dmemory eK.eVis similar . C Experimental Details   C.1 Language Modeling   We closely build on Baevski and Auli ( 2019 ) and   Kasai et al . ( 2021b ) . The hyperparameters are sum-   marized in Table 10 . All models are trained on 4   A100 GPUs .   C.2 Machine Translation   We experiment with a sentence - level ( WMT14 EN-   DE , Bojar et al . , 2014 ) and a document - level bench-   mark ( IWSLT14 ES - EN , Cettolo et al . , 2014 ) to   assess model performance over various sequence   lengths . The preprocessing and data splits of   WMT14 EN - DE follow Vaswani et al . ( 2017 ) . A   32,768 byte pair encoding ( BPE ; Sennrich et al . ,   2016 ) vocabulary is shared between source and   target languages . For IWSLT14 , we follow Mi-   culicich et al . ( 2018 ) and use the dev2010 sub-   set for development and tst2010 - 2012 for testing .   The tokenization is also the same as Miculicich   et al . ( 2018 ): we tokenize and truecase Spanish   and English with Moses ( Koehn et al . , 2007 ) and   run byte - pair encoding with 30k splits , shared be-   tween the two languages . The final dataset contains   1421 , 8 , and 42 documents for training , develop-   ment , and testing . On average , each document   contains 126.7 sentences , and each sentence con-   tains 21.7(ES)/22.5(EN ) BPE subwords . We use   a sliding window with length-4 and stride - one to   generate our dataset . During inference , we use   predicted context on the target side .   We average the checkpoints from the last five   epochs to obtain the final model ( Vaswani et al . ,   2017 ) . In inference , we apply beam search with   size 5 and length penalty 0.6 . Other hyperparam-   eters are summarized in Table 11 . All models are   trained on 4 RTX 2080 Ti GPUs .   Additional machine translation results . In ad-   dition to the results presented in § 5.2 , Table 8 fur-   ther compares , on the WMT14 EN - DE dataset , the   clustering - based ( § 3.2 ) and sliding - window ( § 3.3 )   models of A , as well as ReLU andsigmoid vari-   ants of A. Clustering and sliding - window   Avariants underperform Awith the same   memory sizes by more than 0.5 BLEU . Both ReLU   andsigmoid underperform theirexpcounterpart .   MLP- exp - all replaces the encoder ’s softmax at-   tention modules with A , in addition to the de-   coder ’s . It underperforms A by only 0.3   BLEU.7481Model ϕ Cross nCausal nEncoder nBLEU   B - - - - 27.2   AWindow 32 32 - 26.3   Cluster 32 32 - 26.8   MLP- ReLU 32 8 - -   MLP- ReLU 32 32 - 26.4   MLP- sigmoid 32 8 - 26.8   MLP- sigmoid 32 32 - 27.0   MLP- exp 32 8 - 27.1   MLP- exp 32 32 - 27.3   MLP- exp - all 32 32 32 27.0   Figure 1b compares A ’s ( 32 - 8 memory   sizes ) attention memory overhead with softmax   attention ’s . Following Kasai et al . ( 2021b ) , we con-   sider a synthetic sequence - to - sequence generation   task with varying sequence lengths . A batch size   of 16 and greedy decoding is used . The models are   of the same size as those in § 5.2 .   C.3 Masked Language Model Finetuning   Our data for continued pretraining is a concate-   nation of BookCorpus ( Zhu et al . , 2015 ) , En-   glish Wikipedia , OpenWebText ( Gokaslan and Co-   hen , 2019 ) , and RealNews ( Zellers et al . , 2019 ) .   Our data differs from RoBERTa ’s pretraining data ,   which we do nothave access to . We replace their   CC - News ( Nagel , 2016 ) with RealNews , and drop   Stories ( Trinh and Le , 2018 ) . At the time of this   project , the public access to the Stories dataset   is broken . Our machine does nothave a large   enough memory to load all the data . We therefore   split the training data into 20 shards , after shuf-   fling . Other preprocessing is the same as Liu et al .   ( 2019).The hyperparameters for continued pre-   training follow base - sized RoBERTa , part of which   are summarized in Table 12 . All models are trained   on a single TPU v3 accelerator .   For downstream task finetuning , we use the samehyperparameters as Liu et al . ( 2019).Table 13   briefly describes the tasks . The readers are referred   to Wang et al . ( 2019 ) for futher details.7482Data Train Dev . Test Vocab . Sent./doc   WikiText-103 103 M 218 K 246 K 268 K -   WMT14 EN - DE 4.5 M 3 K 3 K 32 K -   IWSLT14 ES - EN 1713 8 56 30 K 121.5   Hyperprams . B&A Kasai   # Layers 16 32   # Heads 8 8   Embedding Size 1024 1024   Head Size 128 128   FFN Size 4096 4096   Batch Size 64 64   Learning Rate 1.0 1.0   Dropout 0.3 0.3   Layer Dropout - 0.2   Memory size [ 32,64 ] 64   Hyperprams . WMT14 IWSLT14   # Layers 6 6   # Heads 8 8   Embedding Size 512 512   Head Size 64 64   FFN Size 2048 1024   Warmup Steps 6000 4000   Dropout 0.1 0.3   Cross Attn . n 32 128   Causal Attn . n 8 64Hyperprams . Values   # Layers 12   # Heads 12   Embedding Size 768   Head Size 64   FFN Size 3072   Dropout 0.1   Memory Size [ 64,128 ]   Data Task Train Dev .   MNLI Entailment 392 K 9.8 K   QNLI Entailment 105 K 5.5 K   QQP Paraphrase 363 K 40 K   SST-2 Sentiment 67 K 8737483