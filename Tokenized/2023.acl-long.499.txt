  Yujia Qin , Zihan Cai , Dian Jin , Lan Yan , Shihao Liang , Kunlun Zhu ,   Yankai Lin , Xu Han , Ning Ding , Huadong Wang , Ruobing Xie , Fanchao Qi ,   Zhiyuan Liu , Maosong Sun , Jie ZhouNLP Group , DCST , IAI , BNRIST , Tsinghua University , BeijingGaoling School of Artificial Intelligence , Renmin University of China , BeijingModelBest Inc. Pattern Recognition Center , WeChat AI , Tencent Inc.   qyj20@mails.tsinghua.edu.cn   Abstract   Long - form question answering ( LFQA ) aims at   answering complex , open - ended questions with   detailed , paragraph - length responses . The de   facto paradigm of LFQA necessitates two pro-   cedures : information retrieval , which searches   for relevant supporting facts , and information   synthesis , which integrates these facts into a   coherent answer . In this paper , we introduce   WebCPM , the first Chinese LFQA dataset . One   unique feature of WebCPM is that its infor-   mation retrieval is based on interactive web   search , which engages with a search engine in   real time . Following WebGPT ( Nakano et al . ,   2021 ) , we develop a web search interface . We   recruit annotators to search for relevant infor-   mation using our interface and then answer   questions . Meanwhile , the web search behav-   iors of our annotators would be recorded . In   total , we collect 5,500high - quality question-   answer pairs , together with 15,372supporting   facts and 125,954web search actions . We fine-   tune pre - trained language models to imitate   human behaviors for web search and to gen-   erate answers based on the collected facts . Our   LFQA pipeline , built on these fine - tuned mod-   els , generates answers that are no worse than   human - written ones in 32.5%and47.5%of the   cases on our dataset and DuReader ( He et al . ,   2018 ) , respectively . The interface , dataset ,   and codes are publicly available at https :   //github.com / thunlp / WebCPM .   1 Introduction   Long - form question answering ( LFQA ) ( Fan et al . ,   2019 ) targets answering complex , open - ended ques-   tions with detailed , paragraph - length responses .   Current LFQA solutions generally follow the   retrieve - then - synthesize paradigm , which com-   prises two core ingredients : information retrieval   and information synthesis . The former searches ex-   ternal knowledge sources ( e.g. , the web ) for diverserelevant supporting facts , and the latter integrates   the collected facts into a coherent answer .   One defect of the conventional LFQA paradigm   is that it often resorts to non - interactive retrieval   methods , which use the original question as the   query to retrieve a pile of uncurated information .   On the contrary , humans are able to perform inter-   active web search by engaging with a search engine   in real time . For a complex question , humans tend   to decompose it into multiple sub - questions and   ask them in sequence . By identifying and browsing   relevant information , humans can improve their un-   derstanding of the topic and refine their searches by   asking follow - up questions or related terms . This it-   erative process enables expanding the scope of their   searches and improving the results they receive .   Overall , interactive web search not only provides   access to diverse information sources , but also re-   flects the cognitive process of how humans solve   questions , which allows for better interpretability .   WebGPT ( Nakano et al . , 2021 ) is one pioneer-   ing work that supports interactive web search for   LFQA . The authors first build a web search inter-   face backed up by Microsoft Bing , then recruit   annotators to collect information using the inter-   face to answer questions . After that , they fine - tune   GPT-3 ( Brown et al . , 2020 ) to imitate human behav-   iors for web search and to organize the collected   information into answers . In the experiments , We-   bGPT shows exceptional ability in LFQA , even   surpassing human experts . Despite its impressive   performance , WebGPT still remains mysterious to   the community . This is because WebGPT ’s inter-   face , dataset , and trained models are not publicly   available , and the inner workings of its core de-   sign elements remain opaque . These factors make   it hard for the community to understand the chal-   lenges of interactive web search for LFQA and to   continue exploring this line of study.8968   In view of this , we deem it urgent to provide   an accessible platform and public benchmark for   this area . To this end , we first construct an in-   terface ( Figure 1 ) to record web search behaviors   when humans gather relevant information for long-   form questions . In the interface , users can execute   pre - defined actions to perform multiple rounds of   searching and browsing . When finding relevant   information on a web page , they can record it as a   supporting fact . Meanwhile , their web - browsing   behaviors will be recorded . After collecting enough   information , users can finish the web search and   answer the questions based on their collected facts .   Based on the interface , we choose Chinese as   the testbed and construct WebCPM , focusing on   interactive Web search with Chinese Pre - trained   Models . WebCPM is the first public QA dataset   that involves interactive web search , and also the   first dataset that targets Chinese LFQA . WebCPM   contains 5,500 question - answer pairs , together   with 15,372 supporting facts and 125,954 web   search actions . Table 1 summarizes the differ-   ence between WebCPM and relevant QA datasets .   Among existing Chinese QA datasets , WebCPM   possesses the longest question , supporting fact , and   answer , which shows the complexity of the ques-   tions and the richness of the annotated answers .   Then we propose a general framework consisting   of ( 1 ) a search model , which imitates human web   search behaviors for information retrieval . Specifi-   cally , the search model comprises three modules to   execute a series of pre - defined actions on our inter-   face : an action prediction module , a search query   generation module , and a supporting fact extraction   module ; ( 2 ) a synthesis model , which generates a   coherent answer conditioned on the collected facts .   In the experiments , we choose 8representative   pre - trained language models ( PLMs ) with up to   10B parameter size , and evaluate their ability of   interactive web search and information synthe-   sis . We find that scaling model sizes is criticalto achieving better performance . By selecting the   best - performing backbone PLM for the search and   synthesis model , we combine them into a holistic   LFQA pipeline and compare its capability with hu-   mans . Human evaluation reveals that our pipeline   generates answers that are no worse than humans   32.5%of the time on our test set . When applied   to questions whose annotated answers are longer   than400Chinese characters from DuReader ( He   et al . , 2018 ) , our pipeline generates answers that   are better than golden annotated ones 47.5%of   the cases , showing satisfying out - of - distribution   generalization performance . We also show that   our search model surpasses the conventional non-   interactive retrieval method . Finally , we analyze   the contribution of core design elements of our   framework and the human - like behaviors our mod-   els acquire . We envision these resources to serve   as the testbed for other research topics , such as be-   havior cloning ( Bain and Sammut , 1995 ) and tool   learning ( Qin et al . , 2023 ) .   2 Related Work   Retrieval and Synthesis in LFQA . For informa-   tion retrieval , prior works generally resort to local   repositories ( e.g. , Wikipedia ) . Recently there is a   surge of interest in leveraging the whole web as   the knowledge source ( Nakano et al . , 2021 ; Lazari-   dou et al . , 2022 ; Menick et al . , 2022 ; Thoppilan   et al . , 2022 ) , which not only widens the scope of   information sources but enables real - time coverage   of up - to - date knowledge . On the other hand , how   to structure the retrieved facts into a plausible and   nuanced answer for LFQA is still under - explored .   Some investigated how humans craft complicated   answers , either by studying the functional struc-   tures of long - form answers ( Xu et al . , 2022 ) or   exploring how to compose exemplification in an-   swers ( Wang et al . , 2022 ) ; others revisit existing   evaluation metrics of LFQA ( Krishna et al . , 2021).8969   Comparison with WebGPT . We largely follow   WebGPT and also propose improved design ele-   ments ( with details elaborated in appendix E ) , in-   cluding ( 1 ) interface : we modify the actions de-   fined by WebGPT to make them easier for model   learning and more user - friendly ; ( 2 ) framework :   we decompose web search into 3sub - tasks and im-   plement a modular search model . We additionally   explore how to teach the synthesis model to ignore   irrelevant facts ( § 6.3 ) and generate novel contents   ( appendix F.1 ) ; ( 3 ) evaluation and analysis : be-   sides evaluating the whole pipeline following We-   bGPT ( § 6.2 ) , we also evaluate each individual   module ( § 6.1 and § 6.3 ) . This fine - grained eval-   uation helps us better understand the contribution   of core design elements of our framework and the   human behaviors learned by our model .   Tool Learning . Recent research demonstrates   PLMs with promising capabilities of manipulat-   ing tools , i.e. , tool learning ( Qin et al . , 2023 ) .   PLMs can make sequential decisions in com-   plex interactive environments , such as planning   in robotic tasks ( Huang et al . , 2022a ; Ahn et al . ,   2022 ; Huang et al . , 2022b ) , manipulating search   engines ( Nakano et al . , 2021 ) , shopping on e-   commerce websites ( Yao et al . , 2022 ) , etc . By   harnessing the rich world knowledge learned dur-   ing pre - training , PLMs can perform grounded ac-   tions to interact with the real world . We envision   our benchmark to serve as the testbed for future   explorations in this area .   3 Web Search Environment   Following WebGPT , we construct a text - only inter-   face to record web search behaviors when humansgather relevant information for long - form ques-   tions . Our interface , backed up by Bing Search   API , supports 10mainstream web search actions   as shown in Figure 1 . When an action is executed ,   our interface responds with changes in the window .   When the action Search is performed , the inter-   face enters search mode ( Figure 1 ) , which displays   the links recommended by Bing for a specific query   < query > . Each link comprises a title and a brief   snapshot of the specific web page . Each window   displays three links one time , and more links can be   accessed by executing the Scroll Down action .   When finding the i - th link in the current win-   dow to be relevant , users could execute the Load   Page < i > action ( i∈ { 1,2,3 } ) . The interface   would enter the browsing mode ( Figure 6 in the   appendix ) and render the texts cleaned from the   HTML of the < i>-th web page . The content users   could view at a time in the window is restricted   up to 500Chinese characters , and more content   can be accessed with the Scroll action . Users   can utilize the Quote action to extract consecutive   sentences in the current window as a supporting   fact . To enable extracting texts that stretch across   two windows , the Merge action is designed to   merge the last two facts into a single fact ( see ap-   pendix A.2 for more details ) . We also display all   the existing extracted supporting facts for users .   After browsing the i - th page , users can return   to the previous search mode using the Go Back   action to access other links . Meanwhile , a refined   query can be sent at any time . In general , users   can freely interact with our interface multiple times   until executing the Finish action or triggering the   maximum number of actions ( 100 in our case ) . The8970interface would automatically record meaningful   actions and observations during web search . Owing   to the multilingual nature of Bing system , although   this work focuses on Chinese , our interface can be   flexibly adapted to other languages as well . For   more technical details , please refer to appendix A.   4 Data Collection   We employ 23annotators from different walks of   life , who are experienced in search engine opera-   tion . We ask them to answer long - form questions   by first searching for relevant information using   our interface , then writing a nuanced answer . For   quality control , we recruit 8experts familiar with   QA research as quality inspectors . Next , we intro-   duce the construction process of our dataset , with   detailed annotation guides left in appendix B.   Question Creation . Creating new long - form   questions from scratch without any reference is   counterproductive , thus we turn to public QA fo-   rums as the question source . Specifically , we en-   gage annotators to refer to the questions on an   English QA forum Reddit , and then create new   questions written in Chinese . The details of this   creation process are elaborated in appendix C. We   find empirically that questions created in this way   often necessitate multiple rounds of searching and   browsing to collect sufficient information .   Interactive Web Search . Given a question , we   ask annotators to search for accurate and relevant   information from trusted sources using our inter-   face . This process may involve sending refined   queries to Bing multiple times , as well as explor-   ing various web pages they deem to be relevant .   We require annotators to carefully judge the fac-   tual accuracy of the information before extracting   it as a supporting fact . The search process would   be finished until sufficient supporting facts are col-   lected . Among our created questions , 26.2 % are   unanswerable and finally discarded because anno-   tators can not find sufficient useful information .   Answer Annotation . After gathering enough   supporting facts , the annotators would write self-   contained answers based on their collected infor-   mation . We give them instructions for answer anno-   tation , including writing answers that are relevant   to the question and have rich content , maintain-   ing logical consistency , clarity , and coherence , and   providing viewpoints in an unbiased manner . Quality Control . Each annotated instance is   checked and approved by the quality inspectors   before being selected for the final dataset . First ,   inspectors would manually inspect the action se-   quences recorded on the interface and discard low-   quality ones ( e.g. , those with evident clerical er-   rors in the issued queries ) . Second , they would   carefully check the collected supporting facts . If   these facts are apparently insufficient to answer the   question , irrelevant to the question , or factually in-   correct , the corresponding action sequence would   be abandoned . The above procedures remove 25 %   collected instances . For the remaining instances ,   inspectors would carefully examine their annotated   answers . If an answer contradicts the abovemen-   tioned instructions , inspectors would return it to   annotators and point out which requirement is not   satisfied . Annotators would revise their answers   possibly for multiple rounds until the revised an-   swer is up to standard .   Dataset Statistics . Ultimately , we collect 5,500   instances , each formatted in a tuple of ( question ,   web search behavior , supporting fact , answer ) , and   also record the observations at each action execu-   tion . We display an example in Figure 2 for refer-   ence , where we present the following : the original   question , the simplified action sequence , the col-   lected supporting facts , and the annotated answer .   We partition the dataset into { 4,700,400,400 } as   the training , development , and test set . On average ,   each question involves performing 22.9actions ,   sending 2.5queries , and loading 3.3web pages .   The detailed proportion of each action is visualized   in Figure 7 in the appendix .   5 Framework   In this section , we introduce how to teach PLMs   for ( 1 ) interactive web search using our interface   ( § 5.1 ) and ( 2 ) information synthesis ( § 5.2 ) . The   overall framework is illustrated in Figure 3 .   5.1 Search Model   Overview . We partition web search into 3sub-   tasks : action prediction , search query generation ,   and supporting fact extraction . Each task is cast   as a text - to - text format and we train 3separate   modules using a generative PLM . By combining   the3modules , we build the search model , which   executes a series of actions to gather relevant in-   formation . The action prediction module decides   which action to perform at each step . If the module8971   predicts Search orQuote as the current action ,   then it calls the other two modules to generate thecontents of the query or the supporting fact .   Each module performs inference conditioned on8972   the current state Sof the interface at time step t.   Scomprises the original question Q , the query   currently searching Q , the past action sequence   A={a , ... , a } , the last and the current con-   tent displayed in the window WandW , current   supporting facts F={f , ... , f } , and the num-   ber of remaining actions . If an action is executed ,   the components of Swould be updated . Wcan be   either the three links in the search mode or the spe-   cific page content in the browsing mode . We only   maintain the recent two observations ( Wand   W ) displayed in the window instead of concatenat-   ing all the past observations because the latter may   exceed the input length limit of the PLM . Next , we   introduce the three modules in detail .   Action Prediction . This module predicts which   action to perform next . Since there are 10possible   actions in total , action prediction can be viewed as   a10 - category classification task . Take the action   Search as an example , denote { x , ... , x}as the   tokenized sequence for the action name Search ,   where xdenotes a specific token . The probability   ofSearch can be factorized as follows :   During inference , we select the action with the   highest probability to perform on the interface .   Search Query Generation . This module gen-   erates a query Q={q , ... , q}to search   Bing , which is also formulated as text generation : Supporting Fact Extraction . Assume in   thebrowsing mode , the current content of the   window is W={w , ... , w } . We aim to   extract a supporting fact f={w , ... , w}from   W , where 1≤i≤j≤ |W| . While a naive   solution is to directly generate all the tokens of f   auto - regressively , this solution suffers from low   inference speed in practice . As an alternative ,   we only generate the first and last few ( N )   tokens of fgivenS. Formally , we maximize   P([s ] , w , ... , w,[e ] , w , ... , w|S ) ,   where [ s ] and[e ] denote the special tokens that   indicate the start and end of the fact f. During   inference , after decoding the start and end tokens ,   we can locate the desired sequence in Wby text   matching . If the start / end tokens occur in multiple   locations of W , we always extract the longest   sequence from W , and a large Ncould lower   the frequency of this multi - location issue . Note   disjoint spans in Wcan be extracted by executing   multiple Quote actions consecutively .   5.2 Synthesis Model   Theinformation synthesis task learns to organize   a series of supporting facts into a coherent answer .   However , not as perfect as humans , the trained   search model occasionally gathers irrelevant noises ,   which would influence the quality of the generated   answer . To remedy this , we corrupt the collected   facts in the training data of the synthesis model by   introducing noises . Specifically , given a series of   human - extracted facts { f , ... , f } , we randomly   select a few unrelated facts { f , ... , f}from other   training instances . After randomly shuffling all   the facts , we concatenate them as the final input.8973During training , the model is optimized to gener-   ate the human - annotated answer conditioned on   the corrupted supporting facts , i.e. , maximizing   P(Answer |Q , f , ... , f , f , ... , f ) . Since the   annotated answer does not contain the information   off , the model learns to ignore irrelevant facts   and only focus on important ones for generation .   6 Experiments and Analyses   Our problem consists of 4sub - tasks : action pre-   diction , search query generation , supporting fact   extraction , and information synthesis . Correspond-   ingly , we first train 4modules and evaluate each   sub - task independently by feeding the ground truth   input to each module ( § 6.1 ) . Then we combine   all modules into a unitary pipeline and only feed   the question to the pipeline for a holistic evaluation   ( § 6.2 ) . Finally , we conduct in - depth analyses for   each module to understand their behaviors ( § 6.3 ) .   6.1 Individual Sub - task Evaluation   Settings . We evaluate 8typical generative PLMs   that support Chinese , covering 3architectures :   • T5 architecture ( Raffel et al . , 2019 ):   mT5 ( Xue et al . , 2021 ) , a 580 M model pre-   trained on mC4 ; mT0 ( Muennighoff et al . ,   2022 ) , which fine - tunes mT5 on diverse   downstream tasks ; Mengzi - T5 ( Zhang   et al . , 2021b ) , a 220 M model pre - trained on   300 G internet corpora .   •BART architecture ( Lewis et al . , 2020 ):   mBART ( Liu et al . , 2020 ) , a 680 M model   pre - trained on monolingual corpora of multiple   languages ; C - BART ( Shao et al . , 2021 ) ,   a406 M model pre - trained on 200 G web texts .   •CPM architecture ( Zhang et al . , 2021a ):   CPM , CPM , and CPM , which con-   tain2.6B,7B , and 10B parameters , respectively ,   and are pre - trained with increasing sizes of data .   Among these PLMs , mT5 , mT0 , and   mBART are multilingual and the others are   Chinese - only PLMs . We elaborate on details of   the above PLMs in appendix D. We adopt recom-   mended fine - tuning configurations of the original   papers for all PLMs . For evaluation metrics , we   treat action prediction as a 10 - category classifica-   tion task and choose Micro - F1 andMacro - F1 as   the metric . We treat the other three tasks as text   generation and calculate Rouge - L of the generated   sequence and the ground truth .   Results . The results are listed in Table 2 , from   which we conclude that : ( 1 ) mT0 outper-   forms mT5 in action prediction , query gen-   eration , and supporting fact extraction , but per-   forms poorer in information synthesis . We conjec-   ture this is because mT0 enhances language   skills more related to the first three tasks during   its multi - task fine - tuning . Rather , the information   synthesis ability might have been weakened . Be-   sides , Mengzi - T5 performs generally well on   all tasks despite owning much fewer parameters ;   ( 2 ) in general , mBART andC - BART   show inferior performance than all other PLMs , ex-   cept that mBART exhibits excellent perfor-   mance in information synthesis ; ( 3 ) comparing the   results of CPM , CPM , and CPM , we find   thatthe performance generally gets improved as   the model size increases . Blessed by the scal-   ing law ( Kaplan et al . , 2020 ) , larger PLMs own   stronger understanding and generation abilities and   could achieve better downstream performance .   6.2 Holistic Pipeline Evaluation   We choose the modules trained by CPM , which   performs the best among all the PLMs in § 6.1 , and   combine them into the overall pipeline . Then we   evaluate its performance compared with humans .   Compared Answer Pairs . For each test question   of WebCPM , we compare the annotated answer   with3types of answers generated by our synthesis   model . Specifically , the 3types of answers differ   in the source of supporting facts , including ( 1 ) the   facts collected by our search model , ( 2 ) ground-   truth human - collected facts , and ( 3 ) the facts col-   lected using a commonly adopted non - interactive   web search method . For ( 3 ) , we directly input the   original question into Bing , extract the paragraphs8974   from all the retrieved links , and rank them using   TF - IDF . Then we concatenate the top- kparagraphs   as the input until it exceeds 3072 tokens .   Evaluation Protocol . We engage 8annotators   to manually compare different answers based on   human preference . Given a question and a pair of   answers , we ask them to perform an overall assess-   ment and decide which answer they would prefer   based on multiple factors , including the overall   usefulness , coherence , and relevance to the ques-   tion . Since all three retrieval methods use the same   search engine , their collected facts sometimes have   a high overlap , which leads to similar answers .   Thus we allow annotators to mark two answers   asequivalent if both are of comparable quality .   Results . We derive from the results in Figure 4   ( a ) that : ( 1 ) the answers obtained purely by our   pipeline are preferred or comparable to human-   written answers 19.0 % + 13 .5%=32 .5%of the time .   This result implies ample opportunity for advance-   ment of our pipeline in future endeavors , which is   discussed in appendix G. ( 2 ) When applying our   synthesis model to the human - collected facts , the   performance grows to 16.0%+29 .5%=45 .5%pref-   erence or equivalence , which is due to the improved   quality of the collected facts . ( 3 ) The facts gathered   by non - interactive search lead to slightly worse per-   formance ( 7.5%+18 % = 25 .5 % ) than our search   model . The superiority of our search model over   non - interactive search may be because our model   ( a ) sends diverse queries to Bing multiple times so   that more abundant information can be retrieved ,   and ( b ) it critically decides whether a web page   contains important information , which performs   better than TF - IDF .   Experiments on DuReader . Next , we apply our   pipeline ( search model and synthesis model ) to 2Chinese QA datasets from DuReader , i.e. , Zhidao   and Search . Although not specially designed for   LFQA , DuReader contains a variety of question   types , and we randomly sample 400test questions   whose annotated answers are longer than 400Chi-   nese characters . For these questions , we engage an-   notators to compare our pipeline - generated answers   with the golden annotations of DuReader . From   the results in Figure 4 ( b ) , we find that our pipeline   generates answers better than the annotated ones   44.0%and51.0%of the time on Search and Zhi-   dao ( 47.5%on average ) , showing satisfying out-   of - distribution generalization performance . The   fact that the same pipeline surpasses fewer human-   written answers on our dataset than DuReader also   reflects the high quality of our annotated an-   swers . Note the equivalent ratio is 0%because   both answers are based on totally different support-   ing facts , and it is easy to determine which one is   better .   6.3 Further Analysis   Next , we conduct in - depth analyses to gain a deeper   understanding of each module . Without loss of   generality , we evaluate CPMin this section .   Ablation Study for the Synthesis Model . We   evaluate whether corrupting the synthesis model ’s   training data by introducing irrelevant facts im-   proves its ability to ignore noisy facts . We train a   baseline model without corrupting the training data   and keep other settings the same as our model . For   each test question , we feed the supporting facts col-   lected by our search model to both synthesis mod-   els and generate two answers . Annotators would   evaluate which answer is more relevant to the orig-   inal question ( the equivalent option is allowed ) .   According to Figure 4 ( c ) , by corrupting the   training data , our model performs better than the8975   baseline 43.7%of the time and is worse 18.0 %   of the cases . This demonstrates that our method   indeed enhances the model ’s ability to ignore   noisy information , which makes the generated   answer more relevant to the original question . In   appendix F.1 , we further explore the use of another   corruption method that flexibly balances generating   novel contents and copying supporting facts .   Effects of Components in S.We conduct ab-   lation studies for several components of Sto ex-   amine how they contribute to each module of the   search model . This is achieved by modifying both   the training and evaluation data of each module .   For action prediction and supporting fact extrac-   tion , we remove one of the following : the existing   collected facts F , the contents displayed in the   last window W , or the past actions A. For   query generation , the following items are removed   fromS : the existing collected facts F , the al-   ready searched queries , or the titles of the links   browsed before . The information of the latter two   items is included in A. Specifically , for the past   action Search /Load Page , Anot only in-   cludes the action name , but also records the specific   searched query / the title of the loaded page .   The results are listed in Table 3 , from which we   observe that : ( 1 ) for action prediction , the removal   of either ForWonly leads to minimal per-   formance changes , while removing Aleads to   a significant performance drop . This shows that   the past actions are critical factors for action   prediction ; ( 2 ) for supporting fact extraction , only   removing Fimpairs the performance significantly   ( −5.1 ) . This indicates that aligned with humans ,   the module considers what has been extracted to   decide which information to extract next ; ( 3 ) for   query generation , removing either searched queries   or accessed link titles in Acauses a great neg-   ative impact ( −2.5 ) , which means the module   might have learned to generate queries based   on what has been searched and newly observed   information during web search . This feature is   humanoid in that humans also consider both infor-   mation to avoid sending repetitive queries and to   ask follow - up questions about an accessed link .   Case Study for Query Generation . To fathom   the human behaviors learned by our query mod-   ule , we conduct a case study by sampling the gen-   erated queries for different questions in the test   set . We illustrate two representative results in Fig-   ure 5 to showcase the typical strategies learned by   our query module , including copying the original   question , decomposing the question into multiple   sub - questions , rephrasing questions with related   terms , etc . These strategies make the queries more   diverse , which helps gather more abundant infor-   mation from various sources .   7 Conclusion   In this paper , we construct a benchmark of inter-   active web search for Chinese long - form QA , to-   gether with an open - source interface . We decom-   pose the task into 4sub - tasks and design a modu-   lar pipeline . By fine - tuning representative PLMs ,   we conduct both an individual evaluation for each   module and a holistic evaluation for the pipeline .   In - depth analyses are carried out to understand the   core design elements of our framework . We expect   our interface , dataset , framework , and analyses to   facilitate more future explorations in this area .   Acknowledgments   This work is supported by the National Key R&D   Program of China ( No . 2020AAA0106502 ) , In-   stitute Guo Qiang at Tsinghua University , Bei-   jing Academy of Artificial Intelligence ( BAAI).8976Huadong Wang is funded by China Postdoctoral   Science Foundation ( No.2022M721829 ) .   Yujia Qin and Zihan Cai led the data collection .   Yujia Qin , Dian Jin , Lan Yan , Shihao Liang , and   Kunlun Zhu conducted the experiments . Yujia Qin   wrote the paper . Yankai Lin , Xu Han , Zhiyuan Liu ,   Maosong Sun , and Jie Zhou advised the project .   All authors participated in the discussion . The au-   thors would like to thank Aran for the implementa-   tion of the interface , Shengding Hu , Haoyang Pang   and Zhenhuan Huang for the discussion , and the   anonymous annotators for their huge efforts .   Limitations   The human evaluation shows that our pipeline per-   forms worse than humans in the process of infor-   mation retrieval and synthesis 67.5%of the time ,   which still leaves room for improvement ( see ap-   pendix G for future works ) .   Ethical Statement   In this research , we adhere to the highest ethical   standards and commit to making every effort to   minimize any potential harm . Specifically :   •When creating our dataset , we have ensured that   all data collected is obtained through legitimate   and legal means . In addition , we have obtained   the appropriate permissions and consent from   all necessary parties .   •We have also taken steps to protect the privacy of   individuals whose data is included in our dataset   through de - identification during annotation .   •We are committed to eliminating bias , discrim-   ination , or stereotypes during annotation by re-   moving any suspect examples .   •We take the responsibility of open - sourcing the   interface , dataset , codes , and trained models to   the public . However , there are cases that these re-   sources are maliciously used . For instance , our   models may be utilized to generate responses   without proper attribution of the information   source , causing severe consequences . We would   strive to ensure that they are used ethically and   not for any malicious or harm - causing intent .   References897789788979Appendices   AImplementation Details of the Interface   Our interface includes two components : an API   back end and a website front end .   A.1 API Back End   The API backend implements three APIs with dif-   ferent functions : ( 1 ) search , which receives queries   from users and returns search results recommended   by Bing ; ( 2 ) extract , which receives a URL and   returns the text - only contents of the corresponding   web page ; ( 3 ) record , which receives the actions   conducted by agents and stores them in a database .   Search API . The search API is based on Bing   API . When it receives keywords from users , it calls   Bing API to search for relevant results and con-   verts them into the format we specify . Each result   consists of a title , the link to the page , and a brief   summary of the page contents . To ensure the origi-   nality of the answers generated during annotation ,   we have implemented a filter in the search API to   exclude results from certain websites ( e.g. , Red-   dit forums ) . This is necessary because some of   the questions are sourced from websites that may   appear in search results .   Extract API . The contents of web pages often   include huge quantities of layout information and   multimedia that is inappropriate to display directly   to agents and is meaningless for our task . There-   fore , we use a third - party toolto extract the simpli-   fied text - only contents of web pages . This ensures   that only clean and meaningful text will be pre-   sented to the users .   Record API . Actions conducted by users are   recorded in the website front end , when users finish   the annotation process of a question , the front end   will call this Record API , and the detailed action in-   formation and meaningful observations during web   search will be uploaded and stored in our database .   A.2 Website Front End   The website front end is designed as a graphic user   interface for human annotators , which supports   two modes : the search mode and the browsing   mode . Each time an action is performed , it will   be recorded and the corresponding changes will be   rendered in our website and displayed to the users . Window . In the search mode , the window dis-   plays the searched results returned by our API back   end . We present at most three links at a time in   each window , and the Scroll action can be used   to access other links . In the browsing mode , when   clicking a specific link , Load Page action is trig-   gered and the front end will call the extract API   and display the text - only contents of the web page .   The length of content in each window is limited up   to500Chinese characters , and the Scroll action   can be used to access more content . In the main   paper , we illustrate an example for the search mode   of our interface , here we present the example for   thebrowsing mode in Figure 6 . In addition , we   also display the existing supporting facts and the   remaining number of actions for ease of human   annotation .   Actions . Once an action is performed , we record   the current state of the interface , which includes the   content displayed in the window , the current query   issued , the existing collected supporting facts , the   remaining number of actions , etc . We also record   the specific information about the current action ,   for instance , Search < query > includes the con-   tent of the query , Load Page < idx > includes   all the detailed information about a web page ,   andQuote < content > includes the consecu-   tive sentences selected by the user .   It should be noted that the action Merge is   specially designed for extracting a supporting   fact that crosses the boundary of two windows   in the browsing mode . For instance , the user   can perform Quote < content1 > , Scroll   Down , Quote < content2 > , andMerge to get   one supporting fact , which is concatenated by both   content1 andcontent2 .   Besides , we also implement ( 1 ) the Undo action ,   which supports revoking the last action performed ,   and ( 2 ) the Reset action , which terminates the   current annotation and starts a new one . Both ac-   tions will not be recorded since they do not belong   to meaningful web search behaviors .   B Annotation Principle   Below we present the annotation principles for web   search , supporting fact extraction , and question an-   swering . These principles are part of our annotation   guides , which are sent to our contractors before an-   notation . The original version of the following is   written in Chinese , and we have translated it into   English.8980   B.1 Web Search Principle   Look for Relevant Information . In the search   process , it is important to ensure that the content   being searched is closely related to the question   at hand . During the labeling process , users may   encounter various concepts that are related to the   question but may not be central to the main idea .   These peripheral concepts should be ignored in the   search . For instance , when searching for informa-   tion about “ the principle of the constant speed of   light ” , it is possible to come across the concept of   “ Lorentz transformation ” , which is related to the   topic but only tangentially . As such , it is not neces-   sary to include a detailed explanation of “ Lorentz   transformation ” .   Send Simple Queries . Search engines are often   less effective when the question being asked is long   and complex . In such cases , it is advisable to sim-   plify and refine the main question or keywords to   improve the chances of finding relevant informa-   tion and reduce the number of unnecessary search   actions . For example , instead of searching for the   question “ I have a question that bothers me a lot ,   why do most crustaceans / seafood turn from light   gray to red / orange when heated ? ” , it would be   more effective to simplify it to “ why does seafood   change color when heated ? ” . This ensures the sim-   plicity of the queries , making it more likely to find   relevant information .   Avoid Unnecessary Search . Search engines typ-   ically rank web pages based on their relevance tothe query , with higher - ranked results being more   relevant . If the top - ranked results for a particular   search do not align with the user ’s needs , it may   not be productive to continue scrolling through the   results to find relevant information . Instead , it is   more efficient to issue a new query to reduce the   number of unnecessary search actions .   B.2 Supporting Fact Extraction Principle   Find Diverse Relevant Facts . The supporting   facts should contain information that is relevant to   the original question . When possible , it is gener-   ally more effective to extract supporting facts from   diverse sources , while ensuring that the content re-   mains highly relevant to the original question . It   is important to avoid duplicating summaries of the   same content from different sources , as this does   not contribute to answering the question .   Avoid Recording Fragmentary Facts . The ex-   tracted supporting fact should contain complete   and coherent information . It is important to avoid   intercepting sentences with incomplete semantics   or taking them out of context , as this can alter the   meaning of the supporting fact . In addition , please   ensure the integrity of the supporting fact by in-   cluding all relevant information and expressing it   in a coherent manner .   Ensure the Factual Accuracy . It is important to   summarize information from trusted sources when-   ever possible . This helps ensure the reliability of   the information being used . You can also judge the8981factual accuracy of a supporting fact by comparing   it with other searched results .   B.3 Answer Principle   A good long - form answer is typically well-   researched , well - written , and provides a thorough   and detailed response . It should be well - organized   and easy to read , with clear and concise language   that is appropriate for the intended audience . Ad-   ditionally , a good answer should be objective and   unbiased , presenting multiple viewpoints on the   topic if applicable .   Coherence and Relevance . Coherence refers to   the overall logical consistency and clarity of the   answer . The desired answer should have a clear   structure , with each paragraph building upon the   previous one and contributing to the overall argu-   ment . The ideas presented should flow smoothly   and be easy to follow . Relevance means the extent   to which the answer addresses the original question .   The desired answer should stay on topic , providing   information that is directly relevant to the question .   It should not include unnecessary or tangential in-   formation . Together , coherence and relevance help   guarantee that the answer is easy to understand and   stays focused on the main topic , making it more   useful and informative for the reader .   Objectivity . The content of the answer should   be based on the information obtained during the   search process . The desired answer should present   information and viewpoints in an unbiased man-   ner , without expressing personal opinions or pref-   erences . While the annotation process inevitably   involves subjectivity , the questions are relatively   straightforward and it should not be difficult to   maintain a degree of objectivity . Please be neutral   and fair , and present multiple sides of an issue if   applicable .   Conciseness . There is no specific word count   requirement for answers , but it is important to pro-   vide concise , comprehensive , and in - depth answers   that include necessary auxiliary information . It   is generally best to avoid extremely long or short   answers . In addition , the sentences in the answer   should be concise and clear and should avoid re-   dundancy . For example , the question “ How toxic is   barium chloride ? ” should not be answered simply   with “ very toxic ” . Instead , a more detailed descrip-   tion of the toxicity of barium chloride , including   the poisoning dose , poisoning symptoms , and poi - soning mechanism , would be more informative and   useful . It is important to provide a well - rounded   and thorough answer to the question , rather than   just a brief or overly general response .   Normative . It is important to answer questions in   written language , as this can help make the answer   more formal . Annotators should avoid using irreg-   ular or unconventional expressions that may not   be understood by everyone . Typos or grammatical   errors are not allowed .   C More Details for Data Collection   We limit our annotators and quality inspectors to   native Chinese speakers . We make sure all our   annotators are fairly compensated by the market   price .   Question Creation . Chinese QA forums , such   as Zhihu and Baidu Zhidao , are known for their   abundance of long - form questions . However , when   these questions are utilized as direct queries on   Bing , users can often access multiple websites that   contain well - organized answers , thus making the   web search process less challenging . Such an issue   is not mitigated even if we block the source from   Zhihu and Baidu Zhidao . In view of this , we strive   to annotate new open - ended questions that have not   been answered on Chinese QA forums .   Following ELI5 ( Fan et al . , 2019 ) , we turn to   creating questions from Reddit forumsas an alter-   native . We closely follow the way ELI5 collects   the source questions . After collection , we engage   annotators to refer to these questions and then ask   new questions in Chinese . This way significantly   improves the productivity of question creation .   For quality control , our quality inspectors would   check whether the created question is meaningful ,   semantically coherent , comprehensible , and reason-   able . Only those questions that satisfy the above   requirements would be retained . In addition , we   also remove the questions that are politically sen-   sitive . In total , 22.4%newly created questions are   discarded .   Web Search and Answer Annotation . Before   annotation , we provide our annotators with detailed   annotation guidance . They got paid based on the   number of instances they annotate instead of the   time spent during annotation . Note for answer an-   notation , we did not require annotators to use all8982   the collected facts when composing the answer but   asked them to record which facts are leveraged in   their answer .   Proportion for Different Actions . We record   the proportion of different pre - defined actions in   our collected dataset in Figure 7 . As can be seen ,   Scroll Down , Quote , and Search are the   most frequently used actions . The proportion of   Load Page < 1 > is larger than those of Load   Page < 2 > andLoad Page <3 > . This is be-   cause search engines rank search results based on   their relevance to the query . Humans tend to visit   the links according to the order recommended by   search engines . If humans have collected enough   supporting facts on the first page or find it to be   irrelevant , they probably would not continue brows-   ing other web pages of the current query .   D Details for the PLMs Evaluated   We select 6series of representative and publicly   available generative PLMs that support Chinese .   For all the models , we use them for their intended   uses . In the following , we give a brief introduction   to them :   mT5 ( Xue et al . , 2021 ) is a multilingual encoder-   decoder PLM with a general - purpose text - to - text   format . Its pre - training data mC4 ( Xue et al . , 2021 )   covers 101languages collected from the public   Common Crawl web scrape . mT5 achieves su-   perior performance in various multilingual bench-   marks .   mT0 ( Muennighoff et al . , 2022 ) is a multi - task   fine - tuned version of Google ’s mT5 . The model   attained strong zero - shot performance and cross-   lingual generalization ability . Through explicitmulti - task learning , a variety of language capabili-   ties are enhanced through knowledge transfer ; in-   evitably , some capabilities , which are not required   by the trained tasks , might have been impaired .   Mengzi - T5 ( Zhang et al . , 2021b ) is a power-   ful Chinese encoder - decoder PLM that achieved   state - of - the - art results on the CLUE benchmark .   Instead of chasing a larger scale , the authors turn to   developing lightweight yet more powerful models   for easier deployment . Mengzi - T5 was trained on   Chinese Wikipedia , Chinese News , and Common   Crawl and the total size of the pre - training corpus   is300G.   mBART ( Liu et al . , 2020 ) is a multi - lingual   variant of BART , which is a sequence - to - sequence   denoising auto - encoder . mBART is pre - trained   on large - scale monolingual corpora with the   BART ( Lewis et al . , 2020 ) pre - training objective .   The model performs extremely well in machine   translation tasks and can be generalized to lan-   guages that are not in the pre - training corpora .   C - BART ( Shao et al . , 2021 ) is the Chinese ver-   sion of BART . Compared with mBART , the model   was pre - trained only on a Chinese corpus . The   model shows superior performance on keyword   recognition tasks evaluated by the Rouge - L metric .   CPMis the generative pre - trained model series   provided by OpenBMB . We choose three PLMs   CPM(CPM-1 ( Zhang et al . , 2021a ) ) , CPM   ( CPM - Live ) , and CPM(CPM - Ant ) with in-   creasing model sizes . The three models are trained   with increasingly larger sizes of data and training   computations .   Training Details . For each model , we follow the   configuration recommended by the original papers .   During training , we select the model checkpoint   with the best performance on the development set   and evaluate it on the test set . The maximum se-   quence length is 2048 formT0 , mT5 ,   and Mengzi - T5 , 1024 formBART ,   512forC - BART , and 3072 forCPM . We   truncate the input sequence if it exceeds the maxi-   mum sequence length of a PLM .   E Design Differences between WebGPT   and WebCPM   Interface . Our interface supports slightly differ-   ent actions than WebGPT . To begin with , we re-   move 1actions defined by WebGPT : Find in8983Page : < text > , which supports finding the   next occurrence of < text > and scrolling to it . In   our pilot studies , even if we give the annotators   the options for this action , our annotators seldom   execute them . Considering that it may be hard for   our model to learn those extremely low - frequency   actions , we do not include both actions in the final   list of our actions .   Secondly , we modify the functionalities of the   Scroll actions in WebGPT . Specifically , We-   bGPT merged any consecutive Scroll Down   andScroll Up actions made by humans into   new actions Scroll Down < ? > andScroll   Up < ? > , where ? is the number of consecutive   actions . These new actions are utilized by their   models instead of the original Scroll Down and   Scroll Up actions . Therefore , there exists a   gap between what humans actually perform and   what the model is allowed to execute . We contend   that this gap could result in problems for behavior   cloning . Specifically , humans perform consecutive   Scroll Down actions because after each action ,   they carefully check the current window and find   nothing useful . However , when merging consecu-   tive actions , the intermediate observations would   not be shown to the model , which makes decision   making even more difficult .   Finally , we also implement a new Merge action   to support merging two supporting facts into one .   As mentioned before , Merge is specially designed   for extracting a supporting fact that crosses the   boundary of two windows . This action is critical   to avoid recording fragmentary supporting facts .   As shown in Figure 7 , Merge takes up a rela-   tively large ( 5.4 % ) percentage among all the ac-   tions , which is frequently executed by our anno-   tators . This action makes it possible for our an-   notators to extract extremely long sequences as   supporting facts .   Framework . WebGPT does not disclose the im-   plementation details for both interactive web search   and information synthesis ( i.e. , BC model in the   original paper ) . In view of this , we propose our   own framework from scratch , with several design   choices not mentioned by WebGPT :   We decompose the web search process into 3dis-   tinct sub - tasks , i.e. , action prediction , search query   generation , and supporting fact extraction . We train   3modules for each sub - task , respectively . This de-   composition allows us to evaluate three modules   in isolation and gain a deeper understanding of thestrengths and weaknesses of each module . Fur-   thermore , it allows for flexibility in the system ,   as different modules can be updated or replaced   independently .   For our synthesis model , instead of directly fine-   tuning on the ( question , supporting fact , answer )   data , we explore ( 1 ) how to teach the model to   ignore irrelevant facts ( § 6.3 ) . We achieve this goal   by introducing noisy facts into the training data   to explicitly force the model to ignore noisy facts ,   and ( 2 ) how to generate novel contents beyond   the collected facts ( appendix F.1 ) . We corrupt the   training data by deleting partial supporting facts   and forcing the model to generate novel content   based on its pre - trained knowledge .   Evaluation . WebGPT only evaluates the the   whole pipeline through human evaluation . In ad-   dition to the holistic pipeline evaluation ( § 6.2 ) ,   we also evaluate each individual module of our   pipeline ( § 6.1 ) . To the best of our knowledge ,   this is the first work to decompose interactive web   search into action prediction , search query genera-   tion , and supporting fact extraction , and design the   evaluation metrics for the three sub - tasks . It should   be noted that holistic evaluation requires manual   inspection , which is time - consuming despite being   more accurate . Additionally , the holistic evalu-   ation can only be conducted through interaction   with the interface , whereas the individual sub - task   evaluation can be conducted locally ( by feeding   the ground truth Sof the test data to each mod-   ule ) . As such , individual sub - task evaluation is   more flexible to implement , making it easier for   hyper - parameter tuning , thus accelerating the de-   velopment and iteration of the QA system . Besides ,   individual evaluation is more fine - grained , which   helps us better understand the contribution of each   part of the pipeline .   Analysis . In addition to evaluating the LFQA per-   formance of our pipeline , we also conduct an in-   depth analysis to understand the contribution of   core design elements of our framework . In § 6.3 ,   we conduct ablation studies for the search model   and the synthesis model , and a case study for the   query module . We also show that our model indeed   acquires humanoid behaviors when interacting with   the search engine.8984   F Additional Experiments and Analyses   F.1 Generating Novel Contents v.s. Copying   Supporting Facts   Another fascinating research question of our syn-   thesis model is whether it could generate novel   content based on its pre - trained knowledge . This   ability is important especially when the collected   facts are insufficient or fragmentary . Considering   that copying the supporting facts and generating   novel contents are often contradictory to each other ,   here we propose a method to flexibly strike a bal-   ance between both .   Framework . Specifically , we propose another   way to corrupt the training data of the synthesis   model . We split each collected fact into multiple   sub - sentences according to punctuation and ran-   domly erase part of these sub - sentences . We set a   hyper - parameter p∈[0,1 ] , which denotes the prob-   ability of erasing a sub - sentence . A higher pmeans   more sub - sentences would be removed . After that ,   we concatenate the remaining sub - sentences into a   new fact keeping the original order . Finally , we op-   timize the model to generate the human - annotated   answer conditioned on the corrupted facts , i.e. ,   maximizing :   Since the corrupted facts are fragmentary , the   model learns to reconstruct those missing sub-   sentences relying on its pre - trained knowledge .   Settings . We experiment with CPMand fol-   low most of the settings in § 6.1 . We test   when different pis applied to corrupt the train-   ing data . Ideally , a higher pencourages the model   to generate more novel content instead of copy-   ing the supporting facts . Specifically , we choose p   from{0.1,0.2,0.3,0.4,1.0 } , where 1.0means the   model sees no supporting facts but is required to   generate all the tokens in the annotated answer .   During the evaluation , we feed the original   intact supporting facts to the trained synthesismodel . For evaluation metrics , we follow Welleck   et al . ( 2019 ) to test the percentage of n - grams in   the generated sequence that do not exist in the sup-   porting facts , i.e. ,   The final novelty metric is defined as the average   of N , N , and N , i.e. ,   Besides N , we also record the number of   generated tokens .   Results . We derive from the results listed in Ta-   ble 4 that : ( 1 ) with pincreasing , the metric N- constantly becomes larger . This demonstrates   that by deleting more content of the supporting   facts during training , we gradually encourage the   synthesis model to generate novel content based on   its pre - trained knowledge , instead of copying the   supporting facts . However , it should also be noted   that the generated information that is not included   in the collected facts may suffer from poor factual   accuracy . We expect future work to mitigate this   issue ; ( 2 ) in addition , with pincreasing , the gener-   ated sequence tends to be shorter . This shows that   only relying on the synthesis model can not produce   diverse , abundant , and informative contents , which   emphasizes the importance of information retrieval   in LFQA .   G Future Explorations   We expect future works to explore the following   directions :   Efficient and Scalable Use . Despite the fasci-   nating feature of interactive web search , such a   process is inherently slower to execute than the   conventional non - interactive retrieval process of   open - domain QA . In this regard , we encourage   further explorations in reducing the latency of our   pipeline . Possible solutions include improving the   speed and memory usage of the PLM .   Extension to Other Languages and Domains .   It would be interesting to extend the current ap-   proach to other languages beyond Chinese . Con-   sidering that the search engine supports multiple   languages , our interface can be easily adapted to   building benchmarks for other languages.8985Leveraging the Reference Information . In ad-   dition to the annotated answers , we also require   the annotators to record which supporting facts are   referenced and leveraged in their answers . How-   ever , in this paper , we do not utilize this informa-   tion when training our synthesis model . Intuitively ,   such information could guide the synthesis model   to better organize existing supporting facts in a   more coherent way , and to improve its ability in   selecting important information and ignoring irrel-   evant noises .   Diversify the Interactive Elements . In this pa-   per , we focus on supporting the mainstream web   search actions for our users . It would interesting   to explore incorporating more interactive elements   into the interface , such as allowing the users to   provide feedback on the retrieved information and   supporting multimedia information retrieval . How-   ever , more actions also increase the difficulty of   behavior cloning to a certain degree .   Improving Model Behavior from Human Feed-   backs . WebGPT has demonstrated it is promising   to use reinforcement learning from human feedback   ( RLHF ) ( Stiennon et al . , 2020 ) to improve the qual-   ity of the generated answers . RLHF can also be   used for improving the search model ’s web search   behavior , and make it collect more diverse and rel-   evant supporting facts . Our provided environment   can be utilized by researchers to study RLHF in the   future.8986ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section Limitations ( page 9 ) .   /squareA2 . Did you discuss any potential risks of your work ?   Section Ethical Statement ( page 9 ) .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section Abstract and Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 , Section 4 , and Section 6 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 6.1 and Section D.   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section E.   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 1 , Section D , and Section E   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section Ethical Statement .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4 and Section C.   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4 and Section C.   C / squareDid you run computational experiments ?   Section 6 and Section F .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section D.8987 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 6.1 and Section D.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   The training is relatively stable across different runs with random seeds .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 4 , Section B , and Section C.   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4 and Section C.   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.8988