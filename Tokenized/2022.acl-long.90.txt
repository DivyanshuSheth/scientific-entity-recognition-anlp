  Demian Gholipour Ghalandari , Chris Hokamp ,   Georgiana IfrimAylien Ltd. , Dublin , IrelandInsight Centre for Data Analytics , University College Dublin , Ireland{first-name}@aylien.com   georgiana.ifrim@ucd.ie   Abstract   Sentence compression reduces the length of text by   removing non - essential content while preserving   important facts and grammaticality . Unsupervised   objective driven methods for sentence compression   can be used to create customized models without   the need for ground - truth training data , while al-   lowing flexibility in the objective function(s ) that   are used for learning and inference . Recent unsu-   pervised sentence compression approaches use cus-   tom objectives to guide discrete search ; however ,   guided search is expensive at inference time . In this   work , we explore the use of reinforcement learning   to train effective sentence compression models that   are also fast when generating predictions . In partic-   ular , we cast the task as binary sequence labelling   and fine - tune a pre - trained transformer using a sim-   ple policy gradient approach . Our approach outper-   forms other unsupervised models while also being   more efficient at inference time .   1 Introduction   In general , the information content of text is cor-   related with its length . However , for a given text ,   a shorter version may still convey the essential in-   formation while preserving grammaticality ( Sid-   dharthan , 2014 ) . The definition of essential can   change depending on the downstream application ,   thus models for text compression must be able to   adapt based on information about the downstream   task .   Sentence compression models have been used   as sub - modules of text and speech summarization   ( Banerjee et al . , 2015 ; Shang et al . , 2018 ) , for head-   line generation ( Dorr et al . , 2003 ) , subtitle gener-   ation ( Vandeghinste and Pan , 2004 ) , and summa-   rizing emails ( Zajic et al . , 2008 ) . Potential applica-   tions also include snippet generation and highlight-   ing for social media , blog posts or search results . Figure 1 : Reinforcement learning framework for sen-   tence compression .   Given a particular text compression task , rele-   vant evaluation metrics and auxiliary models of   compression quality may not be straightforward to   formulate as well - behaved differentiable objectives   that can be used with standard backpropagation . In   addition , ground - truth examples may be difficult   to obtain because the annotation task is difficult   to fully specify , and metrics which capture differ-   ent facets of compression quality , such as fluency   and optimal sentence length , may be negatively   correlated . Even in the case where ground - truth   examples are available , they are likely to represent   only a subset of the possible outputs , so there is a   risk of over - fitting or biasing models when relying   solely upon a small amount of gold training data   for optimization .   Recent unsupervised sentence compression ap-   proaches leverage powerful neural language mod-   els to directly optimize objectives such as flu-   ency and faithfulness of compressed sentences , us-   ing discrete search strategies , without relying on   ground - truth examples ( Niu et al . , 2019 ; Zhou and   Rush , 2019 ; Schumann et al . , 2020 ) . However ,   these search - based methods are very inefficient at   inference - time because the search must navigate   through a large candidate space while recomputing   expensive reward functions.1267   To allow for flexible reward specification , while   also enabling efficient inference , we design a sim-   ple and effective reinforcement learning ( RL ) setup :   our model is initialized as an unsupervised pre-   trained language model with an untrained binary   classification head ( see Figure 2 ) , and the sentence   compression task is framed as sequence labeling ,   with optimization via policy gradient using a suite   of reward functions . Sentences are compressed in   an instantaneous , one - step fashion , similar to mod-   ern part - of - speech tagging or named entity recogni-   tion models . This approach simplifies the learning   setup while also allowing for high throughput .   According to quantitative evaluation on sev-   eral summarization benchmarks , our approach   shows similar or superior performance compared   to search - based methods , while also being much   faster at inference time .   Our approach to unsupervised extractive sen-   tence compression has the following benefits :   •Unsupervised : No labelled examples are re-   quired .   •Fast inference : At test time , the model only   performs one - step sequence labeling .   •Configurable : Rewards can be tailored to   specific use cases .   We review related work in Section 2 . Section 3   formalizes the task . Section 4 gives a detailed de-   scription of the model and reward functions . Sec-   tion 5 presents experimental results , and Sections   6 and 7 provide analysis and discussion of our find-   ings .   2 Related Work   Unsupervised Sentence Compression   Early work on sentence compression casts the task   as an optimization problem under linguistically mo-   tivated constraints ( Hori and Furui , 2004 ; Clarkeand Lapata , 2006 , 2008 ) . The objectives to be opti-   mized include n - gram language model scores and   frequency - based word relevance measures . Con-   straints are designed to ensure the grammaticality   of compressions .   Some recent work follows the discrete optimiza-   tion paradigm while leveraging powerful models   as objective functions in place of hand - crafted con-   straints , while exploring different strategies for   heuristic search : Zhou and Rush ( 2019 ) use beam   search to optimize a fluency and a similarity ob-   jective . Schumann et al . ( 2020 ) use a greedy hill-   climbing search to optimize fluency and similarity   objectives . Niu et al . ( 2019 ) use a greedy search   with a look - ahead mechanism , only optimizing flu-   ency . All of these recent approaches use large neu-   ral language models to estimate fluency . While the   approach presented in whis work does not involve   discrete search , we consider it complementary and   orthogonal to our RL - based approach ( see Section   7 for more discussion ) .   Another commonly proposed unsupervised   framework is to use autoencoders and reconstruc-   tion objectives ( Miao and Blunsom , 2016 ; Févry   and Phang , 2018 ; Malireddy et al . , 2020 ) . These   approaches are based on the assumption that a good   sentence compression is one from which the origi-   nal sentence can be inferred .   Wang et al . ( 2018 ) is an example of prior work   using reinforcement learning for unsupervised sen-   tence compression . They use a Deep Q - Network to   optimize a reward incorporating n - gram language   model probabilities and grammatical constraints .   This model repeatedly deletes a token until it termi-   nates , as opposed to our one - step approach . Zhao   et al . ( 2018 ) also use RL to optimize a syntax-   focused language model score . However , their   policy is initialized with a supervised sentence com-   pression model , whereas ours is fully unsupervised .   Reinforcement Learning for Summarization   Reinforcement learning has become popular in the   wider field of text summarization , finding applica-   tions in both extractive and abstractive sub - tasks .   One use case of RL is in supervised scenarios ,   where rewards are computed based on ground - truth   examples , e.g. , ROUGE scores , to overcome is-   sues with cross - entropy losses ( Paulus et al . , 2017 ;   Narayan et al . , 2018 ; Dong et al . , 2018 ) . B -   S(Dong et al . , 2018 ) in particular has a very   similar RL setup to ours : they train in one - step   episodes where a policy predicts extractive labels1268and immediately receives a reward . Scialom et al .   ( 2019 ) augment a ROUGE - based reward with a   reward based on question answering . Böhm et al .   ( 2019 ) and Stiennon et al . ( 2020 ) learn reward func-   tions from human quality ratings of summaries .   Similar to our unsupervised approach , Laban et al .   ( 2020 ) use RL for unsupervised abstractive summa-   rization , optimizing reward functions representing   fluency , coverage under a length constraint , and   also use a policy gradient approach .   3 Task   We focus on the specific task of summarizing   a sentence by extracting a subset of its tokens   in their original order . Given an input sentence   xconsisting of ntokens x= ( x , x , ... , x ) ,   we aim to produce a sequence of binary labels   y= ( y , y , ... , y)∈ { 0,1 } , where each label   indicates whether the corresponding input token   should be included in the compressed version of a   sentence .   We further assume an objective function , or re-   ward function R(x , y)that measures how well ap-   plying the labels ysummarizes the original sen-   tence x. For a particular x , the goal is to find   argmaxR(x , y ) , without access to any ground-   truth examples .   In general , there are 2possibilities to shorten   a sentence in this task . A fixed summary length   Lwould reduce this to    possibilities , peaking   atL=(for even n ) . We do not constrain our   approach to a fixed length , but we compare it to   search - based techniques that are constrained to the    search space .   4 Method   4.1 Training Procedure   We train a policy πwith parameters θto produce   binary labels . Given an input x , the policy πpre-   dicts a binary keep / discard probability distribution   for each token index in x. We use the notation   π(∗|x)to refer to the collection of these distribu-   tions for all tokens in x. We obtain the probability   π(y|x)of a label sequence ygiven input sequence   xas follows :   π(y|x ) = Yπ(y|x ) , ( 1 )   where π(y|x)is the probability of a token xbe-   ing included if y= 1 or excluded if y= 0 . Tocompress a sentence using π , we select the higher   scoring label for each token :   y={argmaxπ(y|x)fory∈y}.(2 )   We train our model using a policy gradient tech-   nique ( Sutton et al . , 1999 ) . Unlike typical sequen-   tial reinforcement learning scenarios , our πonly   performs one action for a given input , receiving the   corresponding reward immediately , without transi-   tioning through other intermediate states . There-   fore , our setup is similar to a contextual multi-   armed bandit problem ( Langford and Zhang , 2008 ) ,   where each " arm " corresponds to a particular label   sequence y= ( y , y , ... , y)∈ { 0,1 } . However ,   in our scenario , the policy is generally allowed to   access rewards for multiple possible actions via   sampling , which is different from typical bandit   settings where only one ( action , reward ) pair is   available for each episode .   The training objective is to maximize the ex-   pected reward assigned to a predicted label se-   quence yfor a given input x , computed by the   reward function R :   J(θ ) = E[R(x , y ) ] ( 3 )   The policy gradient theorem states that the gradi-   ent of this expectation can be expressed as follows   ( Sutton et al . , 1999 ):   ∇J(θ ) = ∇E[R(x , y)logπ(y|x ) ] ( 4 )   Since the above expectation is intractable for a   large dataset and the corresponding action space ,   this gradient is estimated by sampling :   ∇J(θ ) = ∇rlogπ(y|x ) , ( 5 )   where y∼π(∗|x)is a sample from the current   policy at a given step , consisting of binary token   labels y= ( y , y , ... , y ) , andr = R(x , y ) .   As is commonly done when using policy gradi-   ents , we subtract a baseline from the reward for   variance reduction . We instantiate the baseline as   r = R(x , y ) , the reward given to the the most   likely label sequence yaccording to the current   policy . The gradient becomes :   ∇J(θ ) = ∇(r−r)logπ(y|x)(6 )   Accordingly , we train our model by minimizing the   following loss function:1269ℓ= ( r−r)logπ(y|x ) . ( 7 )   Using the baseline rallows the intuitive inter-   pretation that a sample yis encouraged if its re-   ward is higher than the current policy ’s prediction ,   i.e. , when factor ( r−r)is negative , and discour-   aged otherwise .   Best - of- kSampling   Prior work with a similar application of policy gra-   dient ( Dong et al . , 2018 ; Laban et al . , 2021 ) ob-   served an advantage in sampling ktimes and taking   the average loss over all samples rather than using   a single sample . However , in our experiments , we   observe that only using the sample with the maxi-   mum reward from a large number of samples works   significantly better than taking the average or only   sampling once . A large kimproves the discovery   of high - quality compressions – if we only use a sin-   gle sample or a very small k , we observe a higher   tendency of models to converge on simple behav-   iors with low reward improvements , such as only   extracting the first- Ltokens of a sentence . The   choice of kcontrols a trade - off : with a higher k ,   we spend more time computing the rewards of sam-   ples and less on model updates , given a limited   wall - time constraint for training . We determine k   in an unsupervised manner using a validation set   ( details in Section 5.2 ) .   4.2 Model Architecture   πis initialized as a transformer encoder model   with a linear classification head . In particular , we   use the 6 - layer DistilRoBERTa model ( Sanh et al . ,   2019 ) due to its efficiency and smaller size com-   pared to other BERT - like models , while retaining   good results on the GLUE benchmark . During   training , the whole model is fine - tuned . For each to-   ken in the input , our model will determine whether   it should be kept or filtered . Figure 2 visualizes   the design . This architecture produces summaries   in an instantaneous , non - autoregressive fashion , al-   lowing for fast prediction ( see Section 5.6 ) .   4.3 Reward Functions   We do not have direct access to ground - truth train-   ing data in our setup , so we consider a suite of   reward functions that may correlate with different   aspects of sentence compression quality . Fluency   This reward function is intended to ensure gram-   matically correct and well - written sentences . We   use a masked language model ( LM ) to estimate the   fluency of a compressed sentence . In particular ,   we compute fluency as the average logit of a token   yin the compressed sentence y. We do this with-   out masking yto reduce the running time during   training , as masking would require to re - encode the   sentence for each token . Based on our experiments ,   this simplification still produces good estimates of   fluency .   R ( y ) = 1   |y|XLM(y|y ) ( 8)   We normalize R by dividing it by an em-   pirically set constant , to keep its values in a similar   range compared to the other rewards . The con-   stant is an observed minimum value from a sample   dataset . We argue that a masked language model is   more appropriate in our setup compared to a left-   to - right ( causal ) language model – when predicting   or sampling a compressed sentence during train-   ing , the sentence is treated as a finished rather than   an intermediate output , which is not captured by   the auto - regressive inference of causal LMs . We   confirm the advantage of a masked LM over a left-   to - right LM in a comparison on a development set   ( Appendix A ) .   We note the precedent for using language mod-   els to measure fluency : Zhou and Rush ( 2019 )   and Schumann et al . ( 2020 ) use language mod-   els trained on a summarization target domain , e.g. ,   headlines . Laban et al . ( 2020 ) uses a generic causal   language model to estimate fluency . Niu et al .   ( 2019 ) use a masked language model to score can-   didate compressions .   Similarity - to - Source   The similarity reward is intended to preserve the   meaning of the source sentence in the compressed   sentence . We experiment with several options   to compute similarity , all using models from the   sentence - transformers library(Reimers   and Gurevych , 2019 ):   •Bi - Encoder Similarity : A sentence encoder   fseparately computes embeddings for the   source and the predicted summary . We calcu-   late the cosine similarity between both embed-   dings : R(x , y ) = cos(f(x ) , f(y))1270•Cross - Encoder Similarity : Output of a cross-   encoder model fmeasuring the seman-   tic textual similarity between both sentences :   R(x , y ) = f(x , y )   •Cross - Encoder NLI : We also test a natural   language inference ( NLI ) model fto esti-   mate how well a compressed sentence retains   source information . The intuition is that the   source should imply information in the output :   R(x , y ) = f(y|x )   Based on experiments on a development dataset ,   the bi - encoder similarity performs best in our setup .   Length and Compression Ratio   Because our model is non - sequential , we can not   easily employ a hard constraint to control the length   of compressed sentences . Instead , we impose a soft   length control using Gaussian reward functions . In   particular , we either use a reward function for the   length ( token count ) in a compressed sentence R ,   or one for the compression ratio between the source   and prediction , in terms of token counts , R. We   choose one of these two depending on whether a   consistent length or a consistent ratio is desired ,   which differs for different evaluation datasets . We   set the distribution means of both rewards as the   desired values for word count and compression   ratio . We set the standard deviations as the mean   times a factor swhich we set to 0.4 for both reward   functions ( Equations 9 , 10 ):   R = N(µ,(s×µ ) ) , ( 9 )   R = N(µ,(s×µ ) ) . ( 10 )   Reward Aggregation   The final reward function is an average of the re-   ward functions R , R , combined with ei-   therRorR :   r(x , y ) = 1   3XR(x , y ) . ( 11 )   In practice , when the downstream task is known ,   reward functions may be designed and calibrated   based upon insights and domain expertise , e.g. ,   an optimal summary length for a specific applica-   tion or different language models corresponding   to different summary styles . In this work , we only   use publicly available and commonly - used off - the-   shelf models to construct reward functions.5 Experiments   This section presents a detailed analysis and evalu-   ation results for our proposed model . We name our   model SCRL ( Sentence Compression with Rein-   forcement Learning ) . We make all code , model   outputs and data available .   5.1 Datasets   5.1.1 Training Datasets   We use two datasets for training : Newsroom   ( Grusky et al . , 2018 ) and Gigaword ( Rush et al . ,   2015 ) . For Newsroom , we extract the first three   sentences from each article , only keeping sentences   with a number of tokens between 15 and 60 . News-   room was chosen due to the large size and a va-   riety of un - preprocessed news articles from dif-   ferent sources . Ground - truth summaries are not   included in the training data , thus the two datasets   are treated as large unlabeled text collections . We   train a model for short headline - like summaries on   Gigaword to evaluate it on the Gigaword test set ,   which comes in a specific preprocessed format .   Training on Gigaword allows to expose the model   to the same preprocessing , for a fair evaluation .   5.1.2 Development Dataset   We constructed a small labelled validation dataset   for model development : we automatically identi-   fied sentence - summary pairs in Newsroom , also   including title - summary pairs , by extracting cases   where the tokenized summary is contained in a   tokenized sentence , with preserved order . We man-   ually filter a subset of these examples based on   grammaticality and informativeness and obtain 280   examples . This dataset was only used during ini-   tial development to compare the different reward   function variants discussed in Section 4.3 .   5.1.3 Evaluation Datasets   The evaluation includes five test sets – key statistics   are listed in Table 1 . L , Lare the token counts   in source and target sentences and cr = L / L   is the compression ratio . Following Schumann   et al . ( 2020 ) , we compare our models on Gigaword   against baselines of comparable length brackets   using ROUGE F1 - scores . For DUC2004 ( Task1271   1 ) , following prior work , we truncate model out-   puts to 75 characters and compute ROUGE recall   scores . While Gigaword and DUC2004 contain   abstractive ground - truth summaries , the remaining   three datasets have token - level extractive ground-   truth summaries . The ground - truth compressions   in the Google sentence compression dataset ( Fil-   ippova and Altun , 2013 ) were automatically gen-   erated using grammatical constraints and distant   supervision via headlines . The Broadcast and   BNC datasets ( Clarke and Lapata , 2008 ) contain   manually created extractive sentence compressions   which tend to be longer compared to the other   evaluation datasets . Following previous work , we   report a simple F1 - score based on tokenized pre-   dicted and ground - truth summaries on the three   extractive datasets , but also measure ROUGE F1   scores .   5.2 Model Development   We tune our approach in several phases . At first ,   we identify an optimal learning rate and batch size   using a grid search with a fixed training duration .   We compare different settings based on the average   reward achieved on a unlabelled , held - out set of the   training data . Next , we test different values of k   ( 1 , 5 , 10 , 50 , 100 ) , the number of samples per step ,   and pick the best kbased on the average reward on   the validation set . This method of hyperparameter   tuning is fully unsupervised .   Using learning rate 1e−05 , batch size 4and   k= 100 identified in the previous runs , we next   compare the different options for the similarity re-   ward listed in Section 4.3 and pick the best ( bi-   encoder similarity ) based on the F1 - score on our   labelled Newsroom - based validation set ( see Ap-   pendix B ) .   5.3 Training   We initialize the encoder component of our model   with the pretrained 6 - layer DistilRoBERTa model   ( Sanh et al . , 2019 ) . The binary classifier module   is initialized randomly . We train each model for   8,000 steps with a batch size of 4 on a Google   Cloud virtual machine with one NVIDIA Tesla T4   GPU , using the AdamW optimizer ( Loshchilov and   Hutter , 2019 ) . Our default reward combination con-   tains masked - LM fluency and bi - encoder similarity   combined with either RorR. Table 2 gives   an overview of the three models that are used in   the evaluation . Note that the sample size of 100 is   responsible for the long training durations . SCRL-   L8andSCRL - L11 are trained with Rwhereas   SCRL - CR75 is trained with R , with a compres-   sion ratio of 0.75 . This is because the ground - truth   summary lengths are approximated better by a fixed   length rather than a fixed ratio in the Google and   DUC2004 datasets , whereas a fixed ratio describes   the Broadcast and BNC datasets better .   5.4 Baselines   We compare our model to the greedy stochastic hill   climbing approach in Schumann et al . ( 2020 ) which   obtained state - of - the - art ROUGE results for unsu-   pervised baselines on the Gigaword and DUC2004   datasets . Because this method and SCRL do not   have identical objective functions , we implement   the hill climbing algorithm applied to our reward   functions , which we will name HCthroughout this   work . This allows for a clearer comparison between   RL and discrete search . HCoptimizes R ,   Runder fixed length constraints instead of us-   ingRandR. Different from Schumann et al .   ( 2020 ) , it runs for a fixed number of 2000 steps and   restarts only when the search is stuck rather than   in equal intervals ( details in Appendix E ) . We ana-   lyze the performance of HCfor different budgets   to understand at what point search can surpass the   learned policies . We also compare against Zhou   and Rush ( 2019 ) , Niu et al . ( 2019 ) and the RL-   based method by Wang et al . ( 2018 ) on datasets   where results are available .   5.5 Evaluation Results   Table 3 shows the evaluation results on all used   test datasets . Results of methods apart from SCRL   andHCare taken from previous works . We com-1272   pute ROUGE scores using the implementation   from Google Research . On Gigaword , SCRL   outperforms all baselines , except Schumann et al .   ( 2020 ) with a 10 token constraint in ROUGE-2 . On   DUC2004 , SCRL remains behind the hill climb-   ing methods , but outperforms other unsupervised   baselines . On the Google dataset , SCRL obtains   state - of - the - art results among unsupervised meth-   ods . On Broadcast and BNC , SCRL andHCobtain   very similar scores , which are both higher than pre-   viously reported results . Figure 3 shows ROUGE-1   scores obtained by HCat different search budgets ,   compared to SCRL . The hill climbing strategy ap-   proaches or outperforms the trained model at dif-   ferent paces , depending on the dataset .   Interestingly , HCstill achieves higher rewards   than SCRL relatively early during its search ( see   Appendix F ) , which is inconsistent with the evalua-   tion results . Potential reasons for this disparity are   disadvantages through the hard length constraints ,   a mismatch between the heuristic reward functions   and evaluation metrics , and beneficial biases in-   duced through our training framework .   5.6 Prediction Running Times   We compare the inference - time speed of SCRL   with HCusing different budgets of search steps .   The fastest batch size for both approaches is used .   TheInference Time in Table 3 shows the average   number of seconds per processed sentence , with   the number of search steps set to T= 2000 forHC .   SCRL is roughly 4000×faster than HCwithT=1273   2000 , and∼200×faster when Tis reduced to 100 ,   for example . We believe that such a speed - up with   a preserved evaluation performance is a critical   factor when considering real - world applications of   sentence compression .   6 Analysis   6.1 Summary Length and Extraction Regions   The length and compression ratio of summaries pro-   duced by SCRL is distributed around the desired   values , with peakier distributions than in ground-   truth summaries ( examples in Figure 4 ) . HCpro-   duces exactly the desired value whenever possible ,   due to the enforced constraint for length or ratio .   Figure 5 shows how SCRL andHCextract tokens   from different relative positions within source sen-   tences . SCRL has a higher tendency to extract   early tokens . We hypothesize that this is a reliable   high - reward strategy discovered during training ,   considering that a milder form of the lead - bias also   shows in HC . Note that neither method is inher-   ently biased in its design to prefer tokens from   certain regions .   6.2 Training Dynamics   Figure 6 shows how rewards and summary length   develop throughout training . The rewards generally   increase quickly in the first few hundred training   steps and then continue to grow very slowly . Flu-   ency starts to increase later than the other reward   functions , which is likely related to our observa-   tion that it is more sensitive to small changes in a   summary . Interestingly , the summary lengths de-   velop differently depending on the length or com-   pression setting – SCRL - L8 andSCRL - L11 start   with short summaries and increase the size over   time whereas SCRL - CR75 starts with long sum-   maries before settling on a shorter certain range .   6.3 Learned Summarization Techniques   Our models learn a variety of behaviors to com-   press sentences , such as removing articles , auxil-   iary verbs , relative clauses and temporal expres-   sions . Figure 7 shows some examples .   6.4 Error Analysis   Even though our models learn to produce gram-   matical sentences fairly well , grammatical errors   do still appear , and are more common for the mod-   els with a short output length ( SCRL-8 , SCRL-   11 ) . In some cases , semantic errors occur where   the original meaning is changed or made unintel-   ligeble . Both SCRL andHCare susceptible of   semantic and grammatical errors , as can be seen   in some examples in Appendix G. A type of error   that is specific to SCRL is the splitting or merging   of tokens resulting from its operation on Byte Pair   Encoding - based subword tokens ( more details in   Appendix C ) .   6.5 Customization via Reward Functions   To demonstrate that our approach is flexible for   customization , we pick a simple example of re-1274   programming model behavior using a hand - crafted   reward function . We note that in some cases , the   model unnecessarily keeps day references in com-   pressed sentences , such as " Thursday " or " yester-   day " . We construct a simple reward function that   returns zero if any day - like word from a small   gazetteer appears in an output and a score of 1   otherwise . We fine - tune an existing model with   this additional reward and observe that it success-   fully avoids including day - words that the previous   model would include . Importantly , it additionally   learned to remove other tokens attached to day-   words , e.g. " on " in " on Monday " , keeping the   sentences grammatical . Table 4 shows some exam-   ples . Empirically , the new model ’s outputs contain   words from the gazeteer in 1 % of cases where they   appear in the source , compared to 12 % in the initial   model .   7 Discussion   We argue that RL offers the following advantages   over discrete search strategies for sentence com-   pression and similar text editing or generation tasks .   The necessary search and exploration is moved into   the training stage , allowing fast inference indepen-   dently of how efficient objectives are to compute .   Furthermore , discrete search unnecessarily spends   time navigating through low - quality outputs that   a trained model can quickly learn to avoid . Lim-   itations of our approach compared to the search - based approach are its lesser flexibility in terms of   on - the - fly customization and a sensitivity to dis-   parities between training data and the application   domain . Furthermore , the trained models show a   lower capability to optimize the selected objectives   compared to search , though this does not have a   negative impact on the evaluation in most cases .   The fact that most of our training time is spent   on estimating the quality of sampled compressions   due to large sample size k , shows that our approach   is somewhat similar to large - scale search strate-   gies applied to a whole dataset , with the difference   that the sampling behavior at each step changes   over time and is informed by previous steps . This   suggests that discrete search could support the RL   training , similarly to the learning - from - search ap-   proach described by ( Li et al . , 2020 ) .   8 Conclusion   This work presents a simple and effective approach   for learning sentence compression models based on   objective functions rather than ground - truth exam-   ples . Because it is unsupervised , it is well - suited   for creating customized applications even when no   gold training data is available , allowing for task-   specific tuning based on arbitrary sets of reward   functions , which do not need to be differentiable .   Importantly , our approach is very fast at inference   time compared to alternative discrete search - based   methods . We are interested in several future direc-   tions related to this work : 1 ) systematic approaches   to design reward functions for summarization , 2 )   RL - based summarization models with length con-   trol on the fly , 3 ) testing our approach on other   languages , and 4 ) the design of curricula for dif-   ferent reward functions as they might pose varying   difficulties at different stages of the training.1275Acknowledgments   This work was funded by the Irish Research Coun-   cil ( IRC ) under grant number EBPPG/2018/23 ,   the Science Foundation Ireland ( SFI ) under grant   number 12 / RC/2289_P2 and the enterprise partner   Aylien Ltd.   References12761277   A Masked vs. Causal Language Model   for Fluency   We compare the masked DistilRoBERTalanguage   model to the causal DistilGPT2model on our de-   velopment dataset . Both models have roughly the   same number of parameters ( 82 M ) . Table 5 shows   the results .   B Similarity Functions   Table 6 compares different variants of the similarity   reward functions on our development dataset .   C Error Analysis : Split and Merged   Tokens   One type of error is the splitting or merging of to-   kens from the source which results from the fact   our model predicts labels at the level of BPE sub-   word tokenization used in the pretrained language   model that we finetune . While some of these oc-   currences are minor , e.g. ’ St. ’ →’St ’ , or even   useful compressions , e.g. ’ 29th ’ →’29 ’ , many of   these cases produce noisy outputs , e.g ’ Perigord ’   →’Perig ’ . Based upon analysis of prediction be-   havior , we estimate that 6%of output sentences   contain some form of this phenomenon .   D Implementation Details   D.1 Pretrained Model IDs   Table 7 lists the model IDs of all open - source pre-   trained models used in this work , which can be   found at https://huggingface.co .Algorithm 1 Stochastic First - Choice Hill Climbing   D.2 Tokenization   We use the NLTKPunkt Tokenizer for several   purposes in this work :   •Obtaining the sentence length and compres-   sion ratio in RandR.   •Compressing sentences by selecting tokens in   our hill climbing implementation HC .   •Obtaining source and summary tokens to   compute the F1 - score in Table 3 , except for   source and reference tokens on the Gigaword ,   Broadcast and BNC datasets which are preto-   kenized .   The involved transformer models ( SCRL ,   R , R ) internally tokenize sentences   based on Byte Pair Encoding .   E Hill Climbing Baseline   Algorithm 1 shows the hill climbing search ap-   proach HCused in our experiments , which is based   on Schumann et al . ( 2020 ) . At the beginning , a bi-   nary label sequence yis initialized by setting L   randomly selected labels to 1and the rest to 0 . At   each step t , S(y)samples a new label sequence y   by randomly selecting a positive and a negative-   valued label yandyand swapping their value .   Note that this always keeps the number of tokens   atL. The sampled yis accepted if it obtains a   higher or equal objective score than the previously   best candidate y. We keep track of previously   created sequences and skip these . If no new label   sequence can be discovered at step t , we termi-   nate the algorithm and restart it with T−tremain-   ing steps . In the end , the highest - scoring yfound   across different runs is returned . We generally set   Tto2000 and keep track of intermediate results to   evaluate HCalso at fewer search steps . Due to this,1278   we decided restart the search dynamically rather in   equal - paced intervals , which we believe should be   tuned with respect to a known maximum budget T.   F Rewards Obtained by HC vs. SCRL   Figure 8 compares the R andRrewards   ofSCRL toHCwith different search budgets . The   length and compression ratio rewards are not in-   cluded as these are enforced through a constraint by   HC . Note that these figures need to be interpreted   carefully as they assume that both approaches pro-   duce summaries of comparable lengths . For exam-   ple , the similarity reward tends to increase with the   summary length .   G Output Examples   Table 8 lists a few examples outputs produced by   SCRL and HC.1279Source the us space shuttle atlantis separated from the orbiting russian mir space station early   saturday , after three days of test runs for life in a future space facility , nasa announced   .   SCRL - L8 the space shuttle atlantis separated from russian station   HC - L8 atlantis space station after test runs for nasa   Source a katyusha rocket fired from lebanon on saturday morning hit the western galilee in   north israel , causing two lightly hurt , israel radio reported .   SCRL - L8 katyusha rocket fired from lebanon hit galilee israel   HC - L8 katyusha rocket fired hit western galilee israel israel   Source Manchester United have agreed a £ 35 m deal to sign Sporting Lisbon midfielder   William Carvalho , according to talkSPORT .   SCRL - L11 Manchester United agreed £ 35 m deal to sign Lisbon midfielder William Carvalho .   HC - L11 Manchester United have agreed a £ 35 m deal to sign William Carvalho   Source Egyptian President Hosni Mubarak met here Sunday with Syrian President Hafez   Assad to try to defuse growing tension between Syria and Turkey .   SRCL - L11 Egyptian President Hosni Mubarak met with Syrian President Hafez Assad def .   HC - L11 Egyptian President Hosni Mubarak met Sunday with Syrian President Hafez Assad   Source Russian President Boris Yeltsin , who is still recuperating from his latest illness , has   canceled a trip to an Asian summit next month , his office said Friday .   SCRL - L11 Russian President Boris Yeltsin recuperating has canceled a trip to Asian summit .   HC - L11 Russian President Boris Yeltsin has canceled trip to an Asian summit   Source Laurie had a passion and a warmth for people rather than the state .   SCRL - CR75 Laurie had a passion and warmth for people .   HC - CR75 Laurie had a passion and warmth for the state .   Source And speaking of the royals , the Duchess of York , Sarah Ferguson , was in Los   Angeles last week holed up at the Four Seasons Hotel and when she ventured out , I   hear she visited some of the studios like Sony to have meetings involving TV projects .   SCRL - CR75 And speaking of the royals , the Duchess of York , Sarah Ferguson , was in Los Angeles   last week holed up at the Four Seasons Hotel and I hear she visited studios like Sony   to have meetings .   HC - CR75 And speaking of royals , Duchess of York Sarah Ferguson was in Los Angeles last   week at the Four Seasons Hotel and when she ventured out she visited some of the   studios like Sony to have meetings .   Source Of the 24,058 people interviewed , 37.7 per cent of women attended arts events and   33.1 per cent of men .   SCRL - CR75 Of 24,058 people interviewed , 37.7 per cent of women attended arts events and 33.1 .   HC - CR75 Of 24,058 people interviewed , 37.7 per cent women attended arts events and 33.1   men .1280