  Jiangjie Chen , Wei Shi , Ziquan Fu , Sijie Cheng , Lei Li , Yanghua XiaoShanghai Key Laboratory of Data Science , School of Computer Science , Fudan UniversitySystem Inc. University of California , Santa BarbaraFudan - Aishu Cognitive Intelligence Joint Research Center   { jjchen19 , sjcheng20 , shawyh}@fudan.edu.cn   wshi22@m.fudan.edu.cn , frank@system.com , leili@cs.ucsb.edu   Abstract   Large language models ( LLMs ) have been   widely studied for their ability to store and   utilize positive knowledge . However , nega-   tive knowledge , such as “ lions do n’t live in   the ocean ” , is also ubiquitous in the world but   rarely mentioned explicitly in the text . What do   LLMs know about negative knowledge ? This   work examines the ability of LLMs to nega-   tive commonsense knowledge . We design a   constrained keywords - to - sentence generation   task ( CG ) and a Boolean question - answering   task ( QA ) to probe LLMs . Our experiments   reveal that LLMs frequently fail to generate   valid sentences grounded in negative common-   sense knowledge , yet they can correctly answer   polar yes - or - no questions . We term this phe-   nomenon the belief conflict of LLMs . Our fur-   ther analysis shows that statistical shortcuts and   negation reporting bias from language model-   ing pre - training cause this conflict .   1 Introduction   Most of the world knowledge exists in a positive   and affirmative form ( Molnar , 2000 ; Barker and   Jago , 2012 ; Vrande ˇci´c and Krötzsch , 2014 ; Speer   et al . , 2017 ) . As a result , large language models   ( LLMs ) pre - trained on a colossal amount of texts ,   such as GPT-3 ( Brown et al . , 2020 ; Ouyang et al . ,   2022 ) and PaLM ( Chowdhery et al . , 2022 ) , have   demonstrated their remarkable abilities for storing   and utilizing positive knowledge in downstream   tasks . In contrast , negative knowledge , such as   the commonsense statement that “ lions do not live   in the ocean ” , is rarely mentioned in the textual   world ( Hossain et al . , 2022).Such negative knowl-   edge also exists in the real world , and is importantFigure 1 : An example of the probing tasks studied in   this paper . For the same negative commonsense knowl-   edge < lion , located at , ocean > which is false , we find   LLMs often fail to generate texts grounded in such neg-   ative knowledge while knowing its validity according   to question answering .   for cognitive skills such as knowing what is not true   orwhat not to think ( MacDonald , 1965 ; Minsky ,   1997 ; Barker and Jago , 2012 ) . Therefore , we ask   this question : Do LLMs ( such as GPT-3 models )   acquire such implicit negative knowledge through   extensive language modeling pre - training ?   One important way of probing LLMs , which are   mostly generative models , is checking whether the   generated texts are knowledge - grounded . This is   because the generation of texts is a direct man-   ifestation of a model ’s internal beliefs towards   world knowledge ( Kassner et al . , 2021 ; Sumers   et al . , 2021 ; Tafjord et al . , 2022).Knowledge-   grounded text generation has been a focus of NLP   research ( Yu et al . , 2022 ) . For example , the C- Gbenchmark ( Lin et al . , 2020 ) evaluates   generative commonsense reasoning that organizes   concepts as keyword input and generates a sentence   grounded in commonsense knowledge . However ,   previous work does not consider negative knowl-   edge , nor do they probe the consistency between9890what models know and what they generate . An-   other line of work on probing ( Petroni et al . , 2019 ;   Ettinger , 2020 ; Kassner and Schütze , 2020 ; Cao   et al . , 2021 ) is conducted through the mask - infilling   task . However , this task mainly evaluates bidirec-   tional models ( Devlin et al . , 2019 ) , and is not natu-   ral for unidirectional LLMs . Also , this task suffers   from the open - world problem in evaluation , i.e. ,   there could be multiple valid answers to fill the   mask . This is vital for evaluating negative knowl-   edge , which has an infinite answer space , e.g. , lions   do n’t live in the sky , water , desk , car , etc .   In this study , we investigate the belief of LLMs   about negative commonsense knowledge through   the lens of text generation . Since LLMs have be-   come a foundational service ( Bommasani et al . ,   2021 ) and can not be easily trained , we apply in-   context learning ( Brown et al . , 2020 ) for the prob-   ing tasks , which is tuning - free . We design a Con-   strained Sentence Generation ( CG ) probing task ,   following Lin et al . ( 2020 ) , where the model must   generate a knowledge - grounded sentence based on   a given triple < s , r , o > . For example , given a triple   “ < lion , located at , ocean > ” , a model should gener-   ate “ lions do not live in the ocean ” . This task is   rather simple and clear . The output sentence ba-   sically contains the same information as the input   keywords . Thus , the generated texts are easy to   evaluate according to the appearance of negation .   We also add a Boolean Question Answering ( QA )   task that asks LLMs whether a knowledge triple is   valid , which shows their beliefs about this piece of   knowledge . An example is given in Figure 1 .   In our experiments , we find that LLMs of dif-   ferent sizes and shapes often produce hallucinated   claims of negative knowledge , even if they answer   yes - or - no questions about it correctly . We term   this phenomenon the belief conflict , i.e. , actions   ( generating texts with it ) conflict with its belief ( an-   swering question about it ) . Hallucinated generation   of negative knowledge is seen in both our probing   tasks and downstream tasks , such as explanation   generation ( Chen et al . , 2022 ; Jung et al . , 2022 ) ,   where negative knowledge plays an important role   in the argumentation of refutation . Further analysis   shows that this problem stems from the statistical   shortcuts and reporting bias of negation during pre-   training . Moreover , such implicit biases can be   alleviated through explicit reasoning with Chain-   of - Thought prompting ( Wei et al . , 2022b ) , such as   syllogistic deduction and related fact comparison . The main contributions of this paper are sum-   marized as follows : 1)We are the first to investi-   gate LLMs ’ belief about negative knowledge in the   commonsense domain , which may shed light on   a previously unstudied aspect of LLMs ’ abilities .   2)We propose to probe generative LLMs through   constrained sentence generation , which is effective   for evaluating generated texts grounded in positive   and negative knowledge . 3)Through extensive ex-   periments , we identify and analyze LLMs ’ belief   conflict phenomenon on negative commonsense   knowledge , and provide insights on the causes and   solutions of such problems .   2 Related Work   Negative Knowledge Negative knowledge refers   to information that describes what is not true , what   can not be done , or what does not exist , while every-   thing that exists is positive ( Molnar , 2000 ; Barker   and Jago , 2012 ) . It plays an important role in the   human reasoning process , because to think effec-   tively , we need to know what “ not to think ” ( Min-   sky , 1997 ) . Current research of negative knowl-   edge in NLP mainly focuses on developing nega-   tive knowledge bases that store relational negative   commonsense knowledge ( Arnaout et al . , 2021 ;   Safavi et al . , 2021 ; Arnaout et al . , 2022 ) and utiliz-   ing negative knowledge within arguments or expla-   nations to refute a candidate ( Camburu et al . , 2018 ;   Aggarwal et al . , 2021 ; Chen et al . , 2022 ) . This pa-   per is based on these resources to probe the belief   of LLMs about the relations of everyday concepts   that are not true .   Understanding Negation in Texts The manifes-   tation of negative knowledge in texts is the phe-   nomenon of negation ( Horn and Wansing , 2022 ) ,   which is difficult for pre - trained LMs to under-   stand , e.g. , filling “ birds can not [ MASK ] ” with   “ fly ” ( Kassner and Schütze , 2020 ) . Negation has   been shown to be spuriously correlated with neg-   ative or contradictory labels due to the data dis-   tribution ( Gururangan et al . , 2018 ; Ettinger , 2020 ;   Lai et al . , 2021 ; Branco et al . , 2021 ; Tian et al . ,   2022 ) , raising doubts about the performance of pre-   vious models . Furthermore , LMs may ignore the   existence of negative words when understanding   texts ( Kassner and Schütze , 2020 ) or processing   prompts ( Jang et al . , 2022 ) , which can be allevi-   ated with unlikelihood training objective ( Welleck   et al . , 2020 ) during training ( Hosseini et al . , 2021 )   or specifying pragmatic contexts ( Gubelmann and9891Handschuh , 2022 ) . While most current research   focuses on NLU , this work fills in a gap in the   investigation of the negation phenomenon in the   context of text generation .   Knowledge - Grounded Language Models A   major goal of NLP has been to ground LMs in   world knowledge , such as factual knowledge ( Vran-   deˇci´c and Krötzsch , 2014 ) and commonsense   knowledge ( Speer et al . , 2017 ) . A line of   work ( Petroni et al . , 2019 ; Kassner and Schütze ,   2020 ; Cao et al . , 2021 ) directly probes the knowl-   edge implicitly learned by LMs through mask-   infilling . However , such a probing paradigm only   works for contextual LMs such as BERT ( De-   vlin et al . , 2019 ) , leaving generative ones , espe-   cially modern LLMs , understudied . Another line   of work focuses on making LM - generated sen-   tences grounded in knowledge ( Petroni et al . , 2020 ;   Liu et al . , 2021 ) . Lin et al . ( 2020 ) designed a   constrained text generation task , C G ,   which asks a model to generate a sentence given   a set of concepts , testing the generative common-   sense reasoning of LMs . However , these studies do   not investigate text generation grounded in negative   knowledge , which is the focus of this work .   In - Context Learning In - context learning ( ICL ;   Brown et al . , 2020 ) has become a prevailing   paradigm for deploying LLMs ( e.g. , the GPT-3 fam-   ily Brown et al . , 2020 ; Chen et al . , 2021 ; Ouyang   et al . , 2022 ) for downstream tasks . Through ICL ,   LLMs can solve tasks directly based on input-   output examples without parameter updates ( Min   et al . , 2022a ; Rubin et al . , 2022 ) . Furthermore , re-   cent work ( Wei et al . , 2022b ; Wang et al . , 2022 )   reveals that the ceiling performance determined by   the scaling law can be beaten with ICL by generat-   ing immediate rationales , i.e. , the Chain of Thought   ( CoT ) prompting . Since LLMs are becoming a   foundational service that do not need fine - tuning ,   our probing on LLMs are based on ICL .   3 Probing Protocol   In this section , we set up an evaluation protocol   to understand what LLMs know about ( negative )   commonsense knowledge of everyday concepts .   3.1 The CSK - PN Dataset   We limit the scope of the knowledge probed to   relational knowledge between commonsense con-   cepts , i.e. ,relational knowledge triples , which   exist widely in knowledge graphs and are com-   monly studied by the community ( Auer et al . , 2007 ;   Vrande ˇci´c and Krötzsch , 2014 ; Speer et al . , 2017 ) .   Given a triplet in the form of < s , r , o > with a sub-   ject concept s , a relation rand an object concept   o , we define a negative fact as ¬r(s , o)if the truth   value of r(s , o)isFalse according to common-   sense knowledge , and a ( positive ) fact if otherwise .   Dataset Statistics We build the probing dataset   ( denoted as CSK - PN ) based on the knowledge   triples filtered by Safavi et al . ( 2021 ) , which are the   challenging ones sourced from ConceptNet ( Speer   et al . , 2017 ) . We also remove invalid triples with   pronouns , negation , and adjectives as subjects or   objects . The final dataset contains a total of 4,000   triples with six pairs of positive or negative rela-   tions ( e.g. ,IAandNIA ) , and the positive and   negative splits have the same size ( 1:1 ) . Detailed   information of CSK - PN is shown in Figure 2 .   3.2 Probing Task Formulation   The most commonly used probing task for under-   standing whether LMs have certain types of knowl-   edge is mask - infilling ( Devlin et al . , 2019 ; Petroni   et al . , 2020 ; Kassner and Schütze , 2020 ) . However ,   this task is not suitable for generative LMs , as the   mask must exist at the end of a sentence .   We argue that LLMs , which are mainly autore-   gressive text generation models ( Radford et al . ,   2019 ; Brown et al . , 2020 ; Ouyang et al . , 2022 ; Scao   et al . , 2022 ) , should be investigated by text genera-   tionwith text decoding from a large sentence space .   Therefore , we propose to use Constrained Sentence   Generation ( CG ) as the primary task to investigate   LLMs , coupled with Boolean Question Answering   ( QA ) for comparison , which is a common approach   to probing the belief of models ( Tafjord et al . , 2022 ;   Richardson et al . , 2022).9892Task 1 : Boolean Question Answering ( QA )   The Boolean QA task requires LLMs to express   its belief about a fact by answering a yes - or - no   question . We first transform every triplet < s , r , o >   into a yes or no question q , where we remove the   negation in rfor negative facts . For example , a   prompt goes like this :   where underlined texts are completed by LLMs . To   generate the questions , we adopt InstructGPT us-   ing in - context learning ( § 4.1 ) . The questions are   94 % valid according to a manual inspection of 50   random cases .   Task 2 : Constrained Sentence Generation ( CG )   Generating texts is a direct manifestation of a   model ’s belief . However , evaluating generated   texts is notoriously difficult in NLP , especially with-   out references . Therefore , we design a keyword-   to - sentence task to make the probing more control-   lable , which is similar to C G(Lin et al . ,   2020 ) . Given a triple < s , r , o > , models need to gen-   erate sentences grounded in ( negative ) knowledge ,   i.e. , add negation cues ( e.g. ,not , unable ) in the   sentence if necessary , e.g. ,   We remove the Nprefix from the negated re-   lations . Note that we allow the paraphrasing of   the input keywords , making it a soft - constrained   sentence generation task .   3.3 Evaluation Metrics   Metric for QA The QA task can be easily eval-   uated by checking the generated token yesandno   ( cased and uncased ) . We define TPandTNas   the accuracy on the positive and negative splits in   CSK - PN , and Acc as the accuracy on the whole   dataset ( i.e. ,Acc = ( TP+TN)/2 , since the posi-   tive and negative splits have equal size ) . For rare   scenarios ( < 1 % ) that LLMs do not generate a yes   ornotoken , we compare the conditional probabil-   ity of these two tokens . Metric for CG Due to the controlled task setting ,   which essentially forces LLMs to decide whether   and how to add a negation cue during decoding ,   the CG task can be efficiently evaluated by detect-   ing the existence of negation cues ( e.g. , not , un-   able , etc . ) in the generations . Following the QA   task , we also use TPandTNas accuracy metrics .   To implement this metric , we first use keywords-   based matching for negation cues , followed by a   RoBERTa model ( Liu et al . , 2019 ) as a token clas-   sifier looking for unmatched negation cues . This   metric produces 1 or 0 based on the finding of nega-   tion cues in a sentence . After manual inspection of   200 cases , we find that this metric is correct 97 %   of the time , which is reliable for evaluating such   a constrained probing task . Errors are mostly due   to double negations and ambiguous negative cues   ( e.g. ,less , opposite , etc . ) , which are quite rare .   Can we trust negation detection as the metric to   evaluate CG ? We manually evaluate the factu-   ality of generated texts based on commonsense   knowledge and see whether the CG metric ( detec-   tion of negation ) correlates well with humans in   this task . Note that only the sentences that make   common sense and adhere to the keywords con-   straints are accepted as true during manual anno-   tation . After examining 100 cases , we find that   the agreement between human judgment and this   metric achieves 95 % . This is predictable , since this   task is rather easy and constrained , yet LLMs do   not solve it well , especially not very consistent with   the QA task . Errors made by the metric are mostly   because 1)generated sentences use uncertain ad-   verbs to modify the sentences , e.g. ,may , some , etc . ;   2)noisy triples in the dataset . Overall , we think   this metric is trustworthy and evaluates this task far   better than most popular text generation metrics .   4Do LLMs have negative commonsense   knowledge ?   In this section , we use CSK - PN to investigate   LLMs ’ belief about negative commonsense knowl-   edge . More importantly , can LLMs generate texts   grounded in negative commonsense knowledge ?   4.1 Probing LLMs with In - Context Learning   To execute the probing tasks without fine - tuning ,   we exploit the few - shot in - context learning ( Brown9893   et al . , 2020 ) ability of LLMs . We manually write 32   examples , with 16 examples for positive knowledge   ( denoted as E ) and 16 for negative knowledge   ( E).In the experiments , we randomly sample   a total number of kexamples from EandE ,   where |E|=|E|if not specified .   Choices of LLMs We use LLMs that can do   in - context learning , so that models stay fixed dur-   ing probing . We choose Flan - T5 ( Chung et al . ,   2022 ) , GPT-3 ( 175B , davinci ; Brown et al . ,   2020 ) and GPT-3.5 series , e.g. Codex ( ≥175B ,   code - davinci-002 ; Chen et al . , 2021 ) and   InstructGPT ( Ouyang et al . , 2022 ): all are ca-   pable of in - context learning . Flan - T5 is an   encoder - decoder LLM with instruction tuning   based on T5 ( Raffel et al . , 2020 ) . Codex ex-   tends GPT-3 through code training and instruc-   tion fine - tuning , and InstructGPT extends Codex   through further tuning of the instructions . In our   experiments , we mainly explore GPT-3.5 mod-   els . We use the 6.7B variant of InstructGPT   ( text - curie-001 ) and the ≥175B variants ,   i.e. ,text - davinci-001 ( tuned on instruc-   tions ) , text - davinci-002 ( tuned on code andinstructions ) , and text - davinci-003 ( further   tuned with reinforcement learning with human   feedback , RLHF).For deterministic predictions ,   all models use greedy decoding ( temperature as   0.0 ) . We use InstructGPTas the default LLM   for experiments due to its powerful capability and   the fact that it has been extensively researched and   applied as of the time of writing this paper . We   also include the recent ChatGPT ( OpenAI , 2022 ) ,   which is built upon InstructGPT and trained with   dialogue data and RLHF .   4.2 The Belief Conflict   We report the results of the probing tasks in Table 1   for LLMs with 2- and 10 - shot in - context learning .   Based on the results , we discover a clear conflict   of LLMs , that LLMs behave inconsistently in QA   and CG tasks on negative commonsense knowl-   edge , which we term belief conflict . Such conflict   manifests itself in two ways : the gap between TP   andTNon the CG task , and the gap of TNbe-   tween the QA and CG tasks . In general , belief   conflicts exist across LLMs of various sizes and   structures . Ablated results per relation is presented   in Appendix B.3 .   When specifically asked , LLMs can distin-   guish between positive and negative commonsense   knowledge , as evidenced by stable and balanced   scores for positive and negative splits in the QA   task . For CG , LLMs seem to accurately gener-   ate sentences grounded in positive knowledge ac-   cording to TP . However , they perform poorly in   negative knowledge , even for the best - performing   LLMs , i.e. , Codex , InstructGPT , as   shown by the lower bars of the CG on the neg-   ative split . Also , the inconsistency between QA   and CG reflects this conflict , as the content gener-   ated by a trustworthy AI system should consistent   and faithful to what it believes . We present a case   study and error analysis in Appendix B.5 .   Among these LLMs , InstructGPTand Chat-   GPT achieve much better results than others . We   assume that such improvements are probably a re-   sult of training LLMs with human feedback ( e.g. ,9894   RLHF ) based on the disclosed differences between   them by OpenAI . Another evidence is that the re-   cent ChatGPT also expresses great capabilities of   generating negative knowledge , even better than   InstructGPTin this regard . We hypothesize that   this is because negative knowledge and rebuttal   statements are frequently used in human feedback   to steer the model , e.g. , admitting errors or instruct-   ing the model not to do something . To validate   this claim , future work could conduct more rigor-   ous comparisons on public available LLMs , which   would be an interesting research problem to trace   certain abilities of LLMs to a specific period of   training .   Sensitivity to the Number of In - Context Exam-   ples To find whether adding more examples helps   solve the probing tasks , we increase the in - context   examples from 0 to 32 . Figure 3(a ) shows a con-   sistent finding with previous results , that LLMs are   so good at answering yes or no questions that the   number of examples does not affect much of the   QA performance . Figure 3(b ) shows that , adding   more examples helps generate both positive and   negative commonsense knowledge . However , the   gap between TPandTNin the CG task still exists .   5 Analysis on the Belief Conflict   5.1 Could keywords as task input hinder the   manifestation of LLMs ’ belief ?   The task input difference for CG and QA leads to a   concern that LMs may find it easier to understand   natural questions ( QA ) than keywords ( CG ) ; hence ,   the belief conflict . In response to this concern , we   change the input of the two tasks . For example , the   keywords - to - answer task takes the form as :   As for the question - to - sentence task :   Results In Figure 4(a ) , we see a 4 - point perfor-   mance decrease given keywords as input for QA ,   which is not significant in comparison , and the   results on the positive and negative splits are as   balanced as before . This implies that LLMs ’ imbal-   anced performance in CG is not due to the use of   keywords as input . In Figure 4(b ) , CG performance   is greatly improved given question as input , approx-   imating QA results . Our assumption is that CG is   basically transformed into QA , because the textual   corpus has seen too many negated texts following   a Boolean question and rephrasing it , e.g. , “ ... ? No ,   lions do not live in the ocean . ” To validate this , we   provide LLMs with zero - shot question - to - sentence   instructions , and check if the output sentences start   with yesornogiven an input question . If our   assumption is correct , models without examples   will be biased toward QA even with a question - to-   sentence instruction . The results of models opti-   mized for instructions show that : 84.58 % of sen-   tences generated by InstructGPTbegin with yes   or no , and 80.28 % for InstructGPT . With 10   examples , this number drops to less than 4 % . Thus ,   these results confirms that question - to - sentence   generation degenerates to the QA task .   As a result , we conclude that the keyword - to-   sentence ( CG ) is an appropriate and challenging   task to probe generative LLMs . Employing key-   words as input does not impact LLMs ’ grasp of the   task ( Figure 4(a ) ) , while using questions as input   may produce shortcuts that obscure whether LLMs   can generate texts of negative commonsense knowl-   edge ( Figure 4(b ) ) . Even if we use different instruc-9895tion wordings ( instructions are at Appendix A.2 ) ,   none escapes the belief conflict , as shown by the   error bars in Figure 4 . Additionally , this experi-   ment brings up the problem of how LLMs encode   commonsense knowledge . According to this exper-   iment , commonsense knowledge seems to be stored   in LLMs in the same manner as it is in the corpus .   LLMs struggle to generalize them , as evidenced by   the keyword inputs for negative knowledge that do   not have a statistical shortcut from pre - training .   5.2 Will the keyword co - occurrence within   corpus affect LLMs ’ generation ?   LLMs are essentially statistical models . In this   experiment , we investigate the influence of word   co - occurrence in the corpus on the CG task , which   is one of the most common statistical factors . We   categorize the dataset into buckets based on key-   words co - occurrence on naturally existing corpora   such as OMCS ( 706 K sentences , Singh et al . , 2002 )   and Wikipedia ( 1 M , a subset built by Gao et al .   ( 2021 ) ) . The co - occurrence for each triple is calcu-   lated by , where w∈s , w∈o ,   andl , ldenote the word count of subject sand   object o , discarding stopwords .   From Figure 5 , we have an interesting finding   that three of the best - performing LLMs from Ta-   ble 1 suffer from a performance drop at the > 1000   bucket of the negative split ( TN ) , the most frequent   data bucket . In contrast , LLMs achieve the best per-   formance this bucket on the positive split ( TP ) . We   conclude that the hard - to - generate negative knowl-   edge for LLMs tend to be those in which they have   seen many subjects and objects appear together .   For example , worm andbird usually co - occur in   sentences , but models tend to generate “ worms can   eat birds . ” Such statistical shortcuts hinder the   generation of negative knowledge . This is also val-   idated by TPresults , where LLMs find it easy to   generate sentences with frequently co - occurring   entities in a positive fact .   5.3 How does the balance of positive and   negative examples affect negation bias ?   A possible answer for the difference between CG   and QA is that : LMs suffer from reporting bias   of negation during pre - training , while answering   questions with yes or no is quite balanced in the   corpora . We validate this problem by mitigating   the negation bias through adjusting the examples   of positive and negative cases . With more Es ,   LLMs are encouraged to generate more negations .   Results Figure 6(a ) , 6(b ) adjust the ratio η = while fixing k. Figure 6(a ) shows that   InstructGPTis very resilient against the ex-   ample ratio in the QA task , except for extreme   cases where only Es orEs are presented ( i.e. ,   η∈ { 0,1 } ) . This also demonstrates the robust-   ness of adopting QA results as LLMs ’ belief . In   Figure 6(b ) , the CG performance on the negative   split is improving as ηgrows . The turning point   appears somewhere near η∈(0.9,1)when E   takes over all the examples . Also , TPdrops as   Ebecomes less . What if we add Ewithout   dropping E ? In Figure 6(c ) , 6(d ) , we keep E   as constant ( |E|= 5 ) and increase |E|from   5to15 . With enough amount of E , TNto CG   continues to increase without sacrificing TP .   Overall , Figure 6 presents the possibility that we   can overcome the belief conflict brought about by   reporting bias by increasing negated texts in the   training data or in - context examples . However , this   is not always feasible in practice .   5.4 Do Chain - of - Thought help generate texts   with negative commonsense knowledge ?   Can the implicit reporting bias be overcome by   explicit reasoning ? Recent studies ( Wei et al . ,   2022b , a ) discover that the Chain - of - Thought ( CoT )   prompting technique shows the emergent reason-   ing abilities of LLMs . CoT generates intermediate   steps in natural language , extending < input , output >   to < input , chain - of - thought , output > . We adopt   two instances of CoT : deductive reasoning and fact   comparison , whose examples are manually written,9896   which are in Appendix A.1 .   Deductive Reasoning Prompting We instantiate   CoT with deductive argumentation in the form of   syllogism ( two premises and one conclusion ) . The   prompt is extended into < input , “ Let ’s think step   by step : ... ” , output > with intermediate steps . A   natural way to identify a negative proposition is de-   ductive reasoning with modus tollens , i.e. , denying   the consequent ( Speranza and Horn , 2010 ; Bobzien ,   2020 ): “ If P then Q. Not Q. Therefore , Not P. ” For   example , “ If something is a intelligent being ( P ) ,   then it must have the ability to think ( Q ) . Comput-   ers can not think ( Not Q ) . Therefore , computers are   not intelligent beings ( Not P ) . ”   To reason about positive propositions , we   usemodus ponens logic , i.e. , affirming the an-   tecedent ( Bobzien , 2020 ): “ If P then Q. P. There-   fore , Q. ” For example , “ Things with lightweight   bodies and strong wing muscles ( P ) can usually   fly ( Q ) . Birds have these physical characteristics   ( P ) . Therefore , birds can fly . ( Q ) ” Notice that the   deduction is not strictly logical but is enough to   arrive at commonsense knowledge .   Fact Comparison Prompting Deduction empha-   sizes the intensional aspects of the fact , whereas   fact comparison highlights the extensional compar-   ison between counterpart facts ( Fitting , 2006 ) . For   example , the related fact for “ lions do not live in   the ocean ” is “ lions live in the land ” . A negative   fact often comes with a core fact that is true , which   has been shown to be useful in explaining why a   claim is wrong ( Cheng et al . , 2022 ) . Therefore , we   extend the < input , output > in each example by < in-   put , “ Related fact : ... ” , output > . For positive cases ,   we write a related fact for consistent examples .   Results Table 2 displays the results of Codex   and InstructGPT . Both CoT instances improve   LLMs ’ performance on TN , showing the benefit   of explicit reasoning for deriving negative knowl-   edge , where different models prefer different ratio-   nales . However , the increase in TNcomes at the ex-   pense of a performance drop in TP . This is mostly   because models previously predicted most of the   cases to be positive , making TPirrationally high .   Overall , these results suggest that , even though   LLMs picked up implicit bias during pre - training ,   it can be overcome by making the reasoning chain   explicit .   Nevertheless , deductive reasoning seems to be   more rigid about confirming commonsense knowl-   edge with a lower TP . This can be attributed to   the fact that commonsense knowledge contains ex-   ceptions ( Allaway et al . , 2022 ) , e.g. ,birds can fly   but penguins ca n’t . Thus , LLMs with deductive   reasoning may hold concerns about exceptions for   confirming a commonsense fact , leading to a signif-   icant lower TPthan fact comparison . We conduct a   simple experiment of exceptions in Appendix B.4 ,   which shows that adding adverbs of degree ( e.g. ,   usually , generally ) in the texts alleviates the belief   conflict , but the problem still exists .   6 Closing Remarks   In this study , we explored and quantified the lim-   itations of LLMs in generating texts grounded in9897negative commonsense knowledge that they seem   to know , a phenomenon we term as “ belief con-   flict ” . To investigate this , we probe LLMs with a   constrained sentence generation ( CG ) task , coupled   with a QA task . Our experiments demonstrated the   existence of the belief conflict in all LLMs when   it comes to negative knowledge , which is mostly   brought by quantifiable statistical shortcuts such as   keywords co - occurrence . We also see that this can   be lessened by giving more in - context examples of   negative knowledge or by using a chain - of - thought   ( CoT ) prompting method to explain the explicit rea-   soning process for deriving negative knowledge .   With the rapid increase of the study on language-   based reasoning ( Clark et al . , 2020 ; Tafjord et al . ,   2021 ; Wei et al . , 2022b ) , there would be cause for   concern if LLMs have trouble generating proofs or   reasoning steps with negative knowledge . With all   the good scores they achieve at QA tasks , whether   they can be trusted with their knowledge expressed   during generation , which is one of the most promi-   nent way of human - AI interaction , is still question-   able . In this sense , the study of negative knowledge   creates a good testbed for assessing real language-   based reasoning skills for LLMs without the statis-   tical heuristics they memorized . We hope that the   findings in this work could raise the awareness of   the community on negative knowledge for LLMs   in downstream text generation tasks .   Limitations   In this work , we highlight that the probing tasks are   placed in the commonsense domain that are gen-   erally acknowledged by people in most situations .   We do not consider the exceptions of commonsense   knowledge , which has gradually drawn some re-   search attentions ( Do and Pavlick , 2021 ; Allaway   et al . , 2022 ) . Exceptions are important for negative   knowledge and are widely used in tasks such as   argumentation or deductive reasoning . However ,   in the experiments , we find that such exceptions   might make models generate commonsense state-   ments with uncertain adverbs ( e.g. ,may , some , etc . )   on rare cases .   Another limitation of this work is that the prob-   ing task is based only on relational commonsense   knowledge from commonsense knowledge bases   such as ConceptNet . We design the keyword - to-   sentence task mostly for the purpose of convenient   evaluation for text generation , which is notoriously   known as difficult . The probing and evaluation ofLLMs ’ belief about negative knowledge in more   complex tasks are beyond the scope of this work ,   but really interesting and challenging . Also , other   types of knowledge could be studied in a similar   way , such as negative social , temporal and spatial   knowledge , to name but a few .   In this paper , we identify the belief conflict prob-   lem in LLMs through extensive experiments . Fu-   ture work could explore more advanced training   or prompting - based methods to improve the con-   sistency between a model ’s belief and its actions   ( text generation for various tasks ) , especially for   negative knowledge .   Ethical Statement   The commonsense knowledge triples from Con-   ceptNet may include offensive and biased sen-   tences , which may also exist in the dataset that we   use in this work . As stated before , the identification   of commonsense negative knowledge may slightly   vary from people from different cultural and social   background when considering exceptions .   Acknowledgement   We thank the anonymous reviewers for their valu-   able comments . We also thank Siyu Yuan and   Jian Xie from Fudan University , and Kexun Zhang ,   Yujian Liu , Qingxiu Dong and Xuandong Zhao   from UC Santa Barbara for their useful sugges-   tions and discussions for the manuscript . This re-   search is funded by the Science and Technology   Commission of Shanghai Municipality Grant ( No .   22511105902 ) .   References98989899990099019902   A Demonstrations for In - Context   Learning   A.1 Manually - written Examples for   In - Context Learning   Some of the manually designed examples are   shown in Table 6 .   A.2 Example Prompts for the Probing Tasks   The task inputs to the LLMs are presented in Ta-   ble 3 . Note that instructions can be replaced by   others . LLMs with in - context learning are known   to be sensitive to the wording and examples in the   prompts ( Min et al . , 2022b ) . Therefore , we manu-   ally write 4 interchangeable instructions for each   probing tasks . For the QA task , the instructions   include :   1.Answer the commonsense questions with yes   or no .   2.Choose “ yes ” or “ no ” to indicate whether   you agree or disagree with the commonsense   questions .   3.Respond to the questions using “ yes ” or   “ no ” .   4.Indicate whether the commonsense questions   are correct or incorrect by writing “ yes ” or   “ no ” .   For the CG task , the instructions include :   1.Write a short and factual sentence according   to commonsense based on the keywords :   2.Use the keywords to create a short and fac-   tual sentence that accurately reflects common-   sense knowledge .   3.Create a short , factual sentence based on the   keywords and what is generally accepted as   true .   4.Construct a factual and concise statement   based on the provided keywords and common-   sense knowledge .   B Additional Results   B.1 Sensitivity to Temperature Tuning   Figure 7 shows that temperature does not influence   much of the performance , thus the findings of this   paper are not sensitive to temperature tuning .   B.2 Abnormal Results of GPT-3 ( davinci )   Different from the trends of other LLMs reported   in§4.2 , GPT-3 davinci shows a confusing pat-   tern of the results on the CG task . A more de-9903   tailed experiment in Figure 8(a ) shows that , when   k < 4 , GPT-3 ( davinci ) performs similarly with   its sibling LLMs , with TPgreatly surpasses TN.TN   continues to enlarge as kincreases , even beating   TP . Based on Acc over the whole dataset , GPT-3   does not achieve results as good as other GPT-3   derivatives . However , a smaller version of GPT-   3 ( i.e. ,curie , 6.7B ) does not express such pat-   tern , according to Figure 8(a ) . We do not have   proper reasons for this finding , but further train-   ing on code and instruction tuning ( i.e. , Codex and   InstructGPT ) seem to fix this problem .   B.3 Results of Different Relation Types   What types of relations do LLMs find the most dif-   ficult to verbalize ? As seen in Figure 9 , we see   LLMs achieve good results in the positive split .   On the negative split , LLMs unanimously believe   NHP to be the most difficult rela-   tions .   B.4 Do LLMs hold concerns about exceptions   for commonsense knowledge ?   Commonsense knowledge usually comes with ex-   ceptions . Could LLMs answer or generate com-   monsense knowledge incorrectly be because they   are thinking about exceptions ? For example , “ birds   can fly , but penguins can not . ” ( Allaway et al . ,   2022 ) . So when asked “ can birds fly ? ” , LLMs   may think of a counterexample and thus arrive at   the answer no . We rephrase the in - context exam-   ples by adding adverbs of degree ( e.g. ,typically ,   generally , usually , most , etc . ) to make the tasks be   about the commonsense instead of exceptions . For   instance , we rewrite “ can birds fly ? ” into “ canmost   birds fly ? ” or “ can birds generally fly ? ” , and “ lions   do n’t live in the ocean . ” into “ lions do n’t usually   live in the ocean . ” In this way , we make language   explicitly convey uncertainty ( Reiter , 2019 ) and try   to rule out exceptions in the tasks .   Based on the results in Table 4 , we find that   adding adverbs of degree to the texts does im-   prove LLMs ’ performance on both CG and QA .   This suggests that LLMs do hold a certain amount   of concerns toward exceptions when dealing with   commonsense reasoning , especially for negative   knowledge . However , considering exceptions with   this trick still does not resolve the belief conflict .   Also , this approach could also serve as a useful   trick for future commonsense research .   B.5 Case Study   Table 5 presents some examples of generated by   InstructGPT(10 - shot ) . In the 1st case , the   model correctly generated negative commonsense   sentences . The 2nd one suffers from the problem9904of weak negation , i.e. , for negative triple , the model   sometimes use “ may ” or “ some ” for weak negation ,   which is not detected by the negation cue detector   metric . The 3rd one suffers from unfaithful genera-   tion to the constraints , where the model generates   information outside the input triples to avoid gen-   erating negation . The 4th one is wrong due to the   noise in the dataset . The 5th one is probably due   to the high co - occurrence of the concept worms   andbirds , the model finally generates a positive   sentence.99059906ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Ethical Statement   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract and Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Grammarly , Quillbot . For grammar check and writing polish .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3.1   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3.1   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Open - sourced resource .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3.1   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 3.1   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Open - sourced resource .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.1   C / squareDid you run computational experiments ?   Section 4 , Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4.19907 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.9908