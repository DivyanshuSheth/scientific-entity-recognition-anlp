  Alexander Henlein and Alexander Mehler   Text Technology Lab , Goethe - University Frankfurt , Germany   { henlein , mehler}@em.uni-frankfurt.de   Abstract   Transformer - based models are now predom-   inant in NLP . They outperform approaches   based on static models in many respects . This   success has in turn prompted research that re-   veals a number of biases in the language mod-   els generated by transformers . In this paper we   utilize this research on biases to investigate to   what extent transformer - based language models   allow for extracting knowledge about object re-   lations ( Xoccurs in Y;Xconsists of Z;action   Ainvolves using X ) . To this end , we compare   contextualized models with their static counter-   parts . We make this comparison dependent on   the application of a number of similarity mea-   sures and classifiers . Our results are threefold :   Firstly , we show that the models combined with   the different similarity measures differ greatly   in terms of the amount of knowledge they allow   for extracting . Secondly , our results suggest   that similarity measures perform much worse   than classifier - based approaches . Thirdly , we   show that , surprisingly , static models perform   almost as well as contextualized models – in   some cases even better .   1 Introduction   Few models have recently influenced NLP as much   as transformers ( Vaswani et al . , 2017 ) . Hardly any   new NLP system today is introduced without a   transformer - based model such as BERT ( Devlin   et al . , 2019 ) or GPT ( Radford et al . , 2019 ) . As a re-   sult , static models such as word2vec ( Mikolov et al . ,   2013 ) are increasingly being substituted . Never-   theless , transformers are still far from being fully   understood . Thus , research studies are being con-   ducted to find out how they work and what proper-   ties the language models they generate have .   During training , transformers seem to capture   both syntactic and semantic features ( Rogers et al . ,   2020 ) . For example , dependency trees can be re-   constructed from trained attention heads ( Clark   et al . , 2019 ) , syntactic trees can be reconstructedfrom word encodings ( Hewitt and Manning , 2019 ) ,   and these encodings can be clustered into represen-   tations of word senses ( Reif et al . , 2019 ) . BERT   also seems to encode information about entity types   and semantic roles ( Tenney et al . , 2019 ) . For an   overview of this research see Rogers et al . ( 2020 ) .   Since BERT and other transformers are trained   on various data crawled from the internet , they are   sensitive to biases ( Caliskan et al . , 2017 ; May et al . ,   2019 ; Bender et al . , 2021 ) . In practice , instead   of reproducing negative biases , they are expected   to allow for the derivation of statements , such as   that toothbrushes are spatially associated with bath-   rooms rather than living rooms . In this line of   thinking , approaches such as the popularization of   knowledge graphs can be located ( Yao et al . , 2019 ;   Petroni et al . , 2019 ; Heinzerling and Inui , 2021 ) .   Our paper is situated in this context . More specifi-   cally , we examine the extent to which knowledge   about spatial objects and their relations is implic-   itly encoded in these models . Since the underlying   texts are rather implicit regarding such information ,   it can be assumed that the object relations derivable   from transformers are weakly encoded ( cf . Landau   and Jackendoff , 1993 ; Hayward and Tarr , 1995 ) .   Reading , for example , the sentence :   “ After getting up , I ate an apple ”   one may assume that the narrator got up from   his bed in the bedroom , went to the kitchen , took   an apple , washed it in the sink , and finally ate it .   The apple is also likely to have been peeled and cut .   Equally , however , nothing is said in the sentence   about a bedroom or a kitchen . Nevertheless , it is a   well known approach to explore the usage regulari-   ties of words , currently most efficiently represented   by neural networks , as a source for knowledge ex-   traction ( see , e.g. Zhang et al . , 2017 ; Bouraoui   et al . , 2020 ; Shin et al . , 2020 ; Petroni et al . , 2019 ) .   In this work , we use a number of methods to   identify biases in contextualized models and ask   to what extent they can be used to extract object-5791based knowledge from these models . To this end ,   we consider three relations :   1.Spatial containment of ( source ) objects in ( tar-   get ) rooms : e.g. a fridge probably belongs in   a kitchen , but not in a living room ;   2.Parts ( source ) in relation to composite objects   ( target ): e.g. a refrigerator compartment is   probably a part of a fridge ;   3.Objects ( source ) in relation to actions ( tar-   get ) that involve them : e.g. reading involves   something being read , e.g. , a book .   Regarding these relations , we examine a set of pre-   trained contextualized and static word representa-   tion models . This is done to answer the question to   what extent they allow the extraction of instances of   these relations when trained on very large datasets .   We focus on rather common terms ( kitchen , to read   etc . ) as part of the general language .   It is assumed that ( static or contextualized ) mod-   els implicitly represent such relations , so that it is   possible to identify probable targets starting from   certain sources . That is , for a word like fridge   ( source ) , we expect it to be semantically more   strongly associated with kitchen ( target ) than with   words naming other rooms , since fridges are more   likely to be found in kitchens than in other rooms ,   and that certain word representation models reflect   this association . We also assume that this associa-   tion is asymmetric and exists to a lesser extent from   target to source ( cf . Tversky and Gati ( 2004 ) ) .   The paper is organized as follows : Related work   is reported in Sec . 2 . The datasets we use are repre-   sented in Sec . 3 and our method in Sec . 4 . Our ex-   periments are presented in Sec . 5 and discussed in   Sec . 6 . Sec . 7 provides a conclusion . All used data ,   scripts and results are open source on GitHub .   2 Related Work   Biases in NLP models are not a new problem that   appeared with BERT , but affect almost all models   trained on language datasets ( Caliskan et al . , 2017 ) .   As such , there are methods for measuring social   biases in static models such as word2vec ( Mikolov   et al . , 2013 ) . One of the best known approaches is   WEAT ( Caliskan et al . , 2017 ) . Here , two groups   of concepts are compared with two groups of at-   tributes based on the difference between the sums   of their cosine similarities ( see Section 4 ) . This ap-   proach already points to a methodological premisethat also guides our work : Relations of entities   are tentatively determined by similarity analyses of   vectorial word representations .   However , a direct comparison of word vec-   tors is not possible with contextualized methods   such as BERT , where the vector representation   of a word varies with the context of its occur-   rence ( cf . Ethayarajh , 2019 ) . Efforts to transfer   the cosine - based approach from static to contextu-   alized models have not been able to recreate sim-   ilar performances ( May et al . , 2019 ) . Therefore ,   new approaches have been developed based on the   specifics of contextualized models . For example ,   BERT is trained using masked language model-   ing , where the model estimates the probability of   masked words in sentences ( Devlin et al . , 2019 ) .   The probability distribution for a masked word in   a given context can then be used as information to   characterize candidate words ( Kurita et al . , 2019 ) .   Sec . 4.3 describes this approach in more detail . An   alternative approach is to examine the interpretabil-   ity of models ( Belinkov and Glass , 2019 ; Jiang   et al . , 2020 ; Petroni et al . , 2019 , 2020 ; Bommasani   et al . , 2020 ; Hupkes et al . , 2020 ) , which goes be-   yond the scope of this paper . In any event , both   approaches share the same basic ideas , e.g. , the   probability prediction of mask tokens ( cf . Kurita   et al . , 2019 ; Belinkov and Glass , 2019 ) .   Work has also been done on how BERT repre-   sents information about spatial objects . For ex-   ample , BERT has problems with certain object   properties ( e.g. cheap orcute ) or implicit visual   properties that are rarely expressed ( Da and Kasai ,   2019 ) . Problems are also encountered with extract-   ing numerical commonsense knowledge , such as   the typical number of tires on a car or the feet on a   bird ( Lin et al . , 2020 ) . More than that , the models   seem to allow for extracting some object knowl-   edge , but not with respect to properties based on   their affordance ( e.g. objects through which one   can see are transparent ( Forbes et al . , 2019 ) ) . Even   though these results seem to question the use of   BERT and its competitors for knowledge extrac-   tion , these models still perform better in down-   stream tasks than their static competitors ( Devlin   et al . , 2019 ; Liu et al . , 2019 ; Brown et al . , 2020 ; Da   and Kasai , 2019 ) . Bouraoui et al . ( 2020 ) compared   these models using different datasets and lexical   relations . These include relations similar to those   examined here ( e.g. a pot is usually found in a   kitchen ) , but beyond the level of detail achieved in5792our study .   What will become increasingly important is the   so - called grounding of language models ( Merrill   et al . , 2021 ): Here , the models are trained not only   on increasingly large text data , but also , for exam-   ple , on images thus enabling better “ understanding ”   of spatial relations ( Sileo , 2021 ; Li et al . , 2020 ) . In   this paper , we focus on models without grounding .   3 Datasets Used for Evaluation   3.1 Spatial Containment   TheNYU Depth V2 Dataset ( Silberman et al . , 2012 )   consists of video sequences of numerous indoor   scenes . It features 464 labeled scenes using a rich   category set . We use this dataset as a basis for   evaluating the probability of occurrence of objects   in rooms ( e.g. kitchen , living room , etc . ) . That is ,   we estimate the conditional probability P(r|o )   of a room r(target ) given an object o(source ) . In   this way , we aim to measure the strength of an ob-   ject ’s association with a particular room as reflect-   ing the corresponding spatial containment relation .   At the same time , we want to filter out objects such   aswindow that are evenly distributed among the   rooms studied here . In our experiments , we con-   sider the ten most frequently mentioned objects   in NYU to associate with the five most frequently   mentioned spaces . This data is shown in the Table   4 ( appendix ) .   The advantage of NYU over other scene datasets   such as 3D - Front ( Fu et al . , 2020 ) is that it deals   with real spaces and not artificially created ones .   In addition , NYU ’s object category set is rela-   tively fine - grained ( we counted 895 different ob-   ject names ) and uses colloquial terms . This is in   contrast to , for example , SUNCG ( Song et al . ,   2017 ) ( with categories like “ slot machine with   chair”,“range hood with cabinet ” , “ food proces-   sor ” ) and ShapeNetCore ( Chang et al . , 2015 ) with   only 55 object categories or COCO ( Lin et al . ,   2014 ) with 80 object categories . This makes NYU   more suitable for our task of evaluating word rep-   resentation models as resources for knowledge ex-   traction starting from general language .   3.2 Part - whole Relations   We use a subset of the object descriptions from   Online - Bildwörterbuch . This resource describes   very fine - grained part - whole relations of objectsexpressed by colloquial names , in contrast to , e.g. ,   PartNet ( Mo et al . , 2019 ) where one finds labels   such as seat single surface orarm near vertical bar .   The list of objects from Online - Bildwörterbuch   used in our study and their subdivisions is shown   in Table 5 . The selected objects were chosen by   hand , provided that the chosen examples are gen-   eral enough and the subdivision is sufficiently fine .   3.3 Action - object Relations   To study entities as typical objects of certain ac-   tions , we derive a dataset from HowToKB ( Chu   et al . , 2017 ) which is based on WikiHow . In   HowToKB , task frames , temporal sequences of   subtasks , and attributes for involved objects were   extracted from WikiHow articles . Some changes   were made to the knowledge database , including   a newly crawled version of WikiHow . In addition ,   the pre - processing tools have been updated and   partially extended ( see Table 6 ) .   3.3.1 Related Datasets   For evaluating static models , there are datasets and   approaches to measuring lexical relations , such   as DiffVec ( Vylomova et al . , 2016 ) , BATS ( Glad-   kova et al . , 2016 ) or BLiMP ( Warstadt et al . , 2020 ) .   Although these datasets are also used to evaluate   BERT ( Bouraoui et al . , 2020 ) , they represent only   an unstructured subset of the data we used and are   thus not appropriate for our study .   4 Approach   We now present the static and contextualized mod-   els used in our study . Table 7 in the appendix lists   these models and their sources . We also specify the   measures used to compute word associations as a   source of knowledge extraction , and describe how   to use classifiers as an alternative to them .   4.1 Static Models   Probably the best known static model is word2vec   ( Mikolov et al . , 2013 ) . Its CBOW variant is trained   to predict words in the context of their surrounding   words . The word representations trained in this   way partially encode semantic relations ( Mikolov   et al . , 2013 ) , making them a suitable candidate for   comparison with the corresponding information   values of contextualized word representations . In   addition to word2vec , we consider GloVe ( Penning-   ton et al . , 2014 ) , Levy ( Levy and Goldberg , 2014),5793fastText ( Mikolov et al . , 2018 ) and a static BERT   model ( Gupta and Jaggi , 2021 ) . Unlike window-   based approaches to static embeddings , Levy em-   beddings are trained on dependency trees .   4.2 Contextualized Models   Unlike static models , the vector representations of   ( sub-)word ( units ) in contextualized models depend   on the context in which they occur so that tokens of   the same type may each be represented differently   in different contexts . All contextual models we   evaluate here are pre - trained and come from the   huggingface models repository . We evaluate two   types of contextualized models :   Masked Language Models ( MLM ) are trained   to reconstruct randomly masked words in input   sequences . We experiment with BERT ( Devlin   et al . , 2019 ) , RoBERTa ( Liu et al . , 2019 ) , ELEC-   TRA ( Clark et al . , 2020 ) and ALBERT ( Lan et al . ,   2019 ) . The models differ in training , training data ,   and model size . BERT is trained using masked   language modeling and next sentence prediction .   RoBERTa omits the second task , but uses much   more training data . Two models are trained for   ELECTRA : one on masked language modeling   ( generator ) and a second one that recognizes just   these replaced tokens ( discriminator ) . Since many   of our evaluations need mask tokens , we only use   the generator model for the evaluations . Finally ,   ALBERT is trained to predict the order of pairs of   consecutive text segments in addition to masked   language modeling .   Causal Language Models ( CLM ) are trained to   predict the next word for a given input text . From   this class we experiment with GPT-2 ( Radford   et al . , 2019 ) , GPT - Neo ( Gao et al . , 2021 ; Black   et al . , 2021 ) and GPT - J ( Wang and Komatsuzaki ,   2021 ) . GPT - Neo and GPT - J are re - implementations   of GPT-3 ( Brown et al . , 2020 ) where GPT - J was   trained on a significantly larger data set named The   Pile(Gao et al . , 2021 ) ( cf . Table 7 in the appendix ) .   4.3 Similarity Measures   To compute similarities of word associations based   on the models studied here , we make use of re-   search on biases in such models . These approaches   calculate biases between two groups of concepts   with respect to candidate groups of attributes . To   this end , associations are evaluated by computing   the similarities of vector representations of con - cepts and attributes . We adopt this approach to   investigate our research question . However , as we   consider knowledge extraction starting from source   words ( e.g. toaster , shower ) in relation to target   words ( e.g. kitchen , bathroom ) , we modify it as   described below .   4.3.1 Cosine and Correlation Measures   Based on the human implicit association   test ( Greenwald et al . , 1998 ) , WEAT ( Caliskan   et al . , 2017 ) is originally designed to compare   the association between two sets of concepts ( X   andY ) and two sets of attributes ( AandB ) . The   degree of bias is calculated as follows :   s(X , Y , A , B ) = /summationdisplays(x , A , B ) −/summationdisplays(y , A , B )   ( 1 )   s(w , A , B ) = /summationdisplaycos ( w , a)−/summationdisplaycos ( w , b )   ( 2 )   Since we are considering source words in relation   to target words , we use the following variant :   s(X , A ) = 1   |X||A|/summationdisplay / summationdisplaycos ( x , a)(3 )   For contextualized models , we adopt the approach   of May et al . ( 2019 ) , that is , we generate sentences   such as “ This is a { x } . ” or “ A { x } is here ” . All tem-   plates used in our study are listed in the appendix   Table 8 . However , instead of using the BERT to-   ken [ CLS ] ( the default token at the beginning of   an input sequence , which often serves as the de-   fault representation of the entire sequence ) , we use   the maximum of the vector representations of all   subwords of the expression . This approach is suit-   able for models like RoBERTa that do not use the   [ CLS ] token for training , or the GPT models that   do not have this token at all . In addition , we also   achieved slightly better results on regular BERT   models using this approach . We explain this with   the fact that our focus is actually only on single   tokens and that the vector representation of the   [ CLS ] token often focuses only on a few dimen-   sions ( Zhou et al . , 2019 ) . Our approach results   in a set of contextualized representations for each   source and target word , which are then compared   using formula 3 . We were able to obtain better   results in our experiments with this representation   than with those generated via the [ CLS ] token . For   static models , if there is no vector representation5794for a potential multiword expressions ( MWE ) , the   average of the vectors of their components is used .   This representation yielded the largest bias in the   work of Azarpanah and Farhadloo ( 2021 ) . For   the static models , we also experimented with dis-   tance correlation ( Székely et al . , 2007 ) , Pearson   correlation ( Benesty et al . , 2009 ) , Spearman corre-   lation ( Kokoska and Zwillinger , 2000 ) , Kendall ’s   tau(Kendall , 1938 ) and Mahalanobis distance ( Ma-   halanobis , 1936 ) – cf . Torregrossa et al . ( 2020 ) ;   Azarpanah and Farhadloo ( 2021 ) – of the word   vectors . Due to space limitations , only the values   of the distance correlation and Kendall ’s tau are   shown ( see Table 1 ) ; the other correlation measures   behave similarly . Moreover , the values for these   measures tend to perform worse for contextualized   models . This observation is consistent with find-   ings of Azarpanah and Farhadloo ( 2021 ) where the   Mahalanobis distance measure performed worst .   4.3.2 Increased Log Probability   The cosine measure has shown to be problematic   for assessing bias in contextualized models such   as BERT ( May et al . , 2019 ; Kurita et al . , 2019 ) .   Kurita et al . ( 2019 ) have therefore developed a   new approach for models trained using masked   language modeling . They weight the probability   of a target word in a simple sentence template ,   assuming that an attribute is given or not :   Experiments show that the values of this measure   correlate significantly better with human biases .   Since this measure is based on the context sen-   sitivity of models , it can not be applied to static   models . For contextualized models , we use the   probability of the last token ( e.g. curtain in the case   ofshower curtain ) for source - forming MWEs and   the first token ( e.g. living in the case of living room )   for target - forming MWEs . We also performed ex-   periments with multiple masks , one for each of   the components of a MWE . However , this did not   produce better results . We adapt this approach for   causal language models as follows : Instead of a   complete sentence , we use incomplete sentence   templates such as “ A(n ) { object } is usually in the   . . . ” or “ In the { room } is usually a / an . . . ” . The   model should then predict the next token . Insteadof masking the seed word , a neutral equivalent is   used for calculation :   A(n ) { object } is usually in the ...   ⇓   This is usually in the ....   Instead of performing the analysis in only one di-   rection , we determine the score for both the target   and the source given the other .   4.3.3 Classifier - based Measures   In addition to the previously described measures ,   we experiment with classifiers . To this end , we   train three classifiers on the model representations   of the source word to determine the associated tar-   get word as a class label ( e.g. predict kitchen , given   the vector of frying pan ) . We generate the set of   source word representations Xin the same way   as in the case of the cosine measure ( see Section   4.3.1 ) and average them before classification :   target = Classifier / parenleftigg   1   |X|/summationdisplay⃗ x / parenrightigg   The training runs on a leave - one - out cross - valida-   tion repeated 100 times . The target vector was then   generated from the counted predicted classes ( see   Figure 2b in Appendix ) . We trained a k - nearest   neighbors classifier with k= 5 ( KNN ) , an SVM   with a linear kernel and a feed - forward network   ( FFN ) . A small hyperparameter optimization was   performed for the FFN , which resulted in the fol-   lowing parameters : Adam Optimizer ( Kingma and   Ba , 2014 ) with a learning rate of 0.01over 100   epochs and one hidden layer of size 100and ReLU   as activation function .   4.4 Scoring Measures and Classifiers   Given a word representation model , we compute   the final score for the measures and classifiers to es-   timate how well they reconstruct the original prob-   ability distribution of the source entities relative   to the target entities ( see Table 4 , 5 , and 6 ) . This   is computed by the distance correlation ( Székely   et al . , 2007 ) between the target - source probability   distributions and the corresponding association dis-   tributions of the respective measure or classifier .   The advantage of the distance correlation over the   Pearson correlation is that it can also measure non-   linear relations . This was calculated both for all   targets individually ( correlation of all sources to   one target ) and then concatenated for all targets5795together ; we denote this variant by CONC . There-   fore , CONC does not correspond to the average of   the individual distance correlations .   5 Experiments   Using the apparatus of Section 4 , we now evaluate   the classes of word representation models ( static ,   MLMs and CLMs ) in conjunction with the sim-   ilarity measures and classifiers . The results for   the static models are shown in Table 1 , for the   MLMs in Table 2 and for the CLMs in Table 3 .   Figure 2 , 3 and 4 in Appendix show a visualization   of the associations computed by means of cosine ,   masked - target & masked - source increased log sim-   ilarity measures and the FFN classifier based on   BERT - Large using the different datasets .   An experiment was also conducted with word   frequencies via Google Ngram(see Section A.1   in the appendix ) .   5.1 Model - related Observations   The basic expectation that the cosine measure   would generally perform the worst and the FFN   classifier the best was met ( see Tables 1–3 ) . Inter-   estingly , cosine is also outperformed by distance   correlation in almost all cases .   Among the static models , GloVe and fastText   performed best in most cases , especially on the   room and part dataset ( Table 1 ) . Although Levy per-   forms by far the worst in the room dataset , it keeps   up with all classification results in the verb dataset .   One reason for this could be the dependency - based   learning strategy , which seems to work very well   for verb associations , even though it was trained on   a much smaller data set .   Among the masked - language models , BERT-   Base surprisingly performed the best ( Table 2 ) .   BERT - Large achieved the better Increased Log   Probabilities , but the FFN classifier still worked   better with the vector representations of the Base   variant . This suggests that although associations   are represented in a more fine - grained manner in   BERT - Large , they are more difficult to retrieve due   to the size of this model .   Among the masked - language models , GPT - J   ( which was trained with by far the largest training   data ) performs best ( Table 3 ) . Context - based mod-   els generally seem to determine the target given   the source ( P(target |source ) ) more easily than   the reverse ( P(source |target ) ) . With verbs , onthe other hand , the reverse effect occurs . The GPT   models show that the results for sources are better   when weighted , while for targets the results are   better without weighting .   In general , the SVM performed surprisingly well ,   even though only a linear kernel was used . But also   the KNN method mostly performed better than the   similarity measures . However , FFN performs best   in all cases , outperforming cosine ( worst case ) by   increases in the interval [ 6%,52 % ] and outperform-   ing the KNN approach ( worst classifier ) in each   case by increases in the interval [ 2%,43 % ] .   5.2 Dataset - related Observations   In terms of rooms , bathroom scores the best , while   living room oroffice usually score the worst . This   may be because many bathroom objects are related   to specific bathroom activities ( e.g. , toothbrush ,   bathtub ) , while objects that used to be located in the   living room are increasingly found in other rooms   ( e.g. , television in the bedroom ) . This would also   explain why the results for kitchen are also better .   On the part dataset , the static models actually   performed significantly better than the contextual-   ized models . This relates especially to GloVe and   fastText which outperformed almost all contextual-   ized models . Thus , static models are in some cases   a good alternative to their contextualized counter-   parts . However , the more technical the objects   become ( here mortise lock anddishwasher ) , the   worse the results become .   On the verb dataset , the contextualized models   perform minimally better . As mentioned earlier ,   these models can associate objects with verbs more   easily than the other way around . Here , the largest   difference in performance is observed in the case   of Levy , where the results are almost equal to those   of the other models , probably due to the learning   strategy based on dependency trees .   In summary , knowledge extraction using lan-   guage models , whether static or contextualized , is   more effective using classifiers than using similar-   ity measures commonly used in the field of bias   research : there is potential for this type of knowl-   edge extraction , but at the price of training classi-   fiers – if one uses similarity measures instead , this   knowledge is mostly out of reach .   5.3 Relation Observation   All previous evaluations only examined associ-   ations between instances and concepts , but not   whether the models represent their true relations.57965797   To fill this gap , we repeated the experimental setup   of Kurita et al . ( 2019 ) for the room and part dataset   on BERT - large , but this time masked the relation .   The results are shown in Figure 1 . Our selection of   relations does not claim to be exhaustive , but serves   as an illustration . It shows that while BERT - large   is still very good at assigning objects inrooms , the   dominant relation predicted for parts is used by .   This suggests that BERT has problems correctly   assigning object parts , an observation that could   explain its poorer results while being consistent   with findings of ( Lin et al . , 2020 ) ( e.g. , regarding   counting parts ) .   6 Discussion   As good as the results obtained using classifiers   are , they must be viewed with caution . One can   attribute their success to the fine - tuning of numer-   ous parameters ( and ultimately to overfitting ) ; how-   ever , one can also attribute this success to nonlinear   structuring of the information encoded in language   models . In other words , these models appear to   encode object knowledge , but require a sophisti-   cated apparatus to retrieve it . Thus , they should   not be considered as an alternative to unsupervisedapproaches .   Another issue is that our experiments do not yet   allow for a comparison of model architectures , as   the models studied differ significantly in terms of   the size of their parameter spaces and training data .   Our experiments do suggest that certain smaller   models come close to or even outperform the re-   sults of larger models . However , a comparison   of model architectures would require controlling   for these parameters . Nevertheless , the results we   have obtained are , in part , promising enough to   encourage such research .   Finally , our experiments show that static mod-   els can perform better than contextualized models   to some extent . This finding is conditioned by   our experiments and their context of application .   These observations that older models perform bet-   ter on certain tasks are consistent with other work   ( e.g. LSTMs on small datasets for intent classi-   fication ( Ezen - Can , 2020 ) or definiteness predic-   tion ( Kabbara and Cheung , 2021 ) . At this point , a   much broader analysis is needed ( considering more   areas and object relations ) , which also exploits con-   textual knowledge represented in contextualized   models more than has been done here and in re-   lated work . Nevertheless , it is generally difficult   to obtain data for such a broader analysis , and our   experiments are already broader in scope and con-   sider finer relationships than similar approaches .   7 Conclusion   We evaluated static and contextualized models as   potential resources for object - related knowledge   extraction . To this end , we examined three datasets   ( to identify typical artifacts in rooms , objects of   actions , or parts of objects ) . We also experimented   with different similarity measures and classifiers to   extract the information contained in the language   models . In doing so , we have shown that the mod-   els in combination with the measures differ greatly   in terms of the amount of knowledge they allow for   extracting . There is a weak trend that BERT - Base   is the best performer among contextualized models ,   and GloVe and fastText among static models . Sec-   ondly , our results suggest that approaches based on   classifiers perform significantly better than similar-   ity measures . Thirdly , we have shown that static   models perform almost as well as contextualized   models – in some cases even better . This result   shows that research on these models needs to be   advanced . In future work we will also investigate5798how grounded language models perform on such   datasets . However , as noted above , this requires a   significant expansion of bias research , such as that   conducted here to enable knowledge extraction .   Acknowledgements   The support by the Stiftung Polytechnische   Gesellschaft ( SPTG ) is gratefully acknowledged .   References579958005801   A Appendix   A tabular breakdown of the datasets used can be   seen in Table 4 , 5 and 6 . The exact models used   are listed in Table 7 . The heatmap visualizations   for the other two datasets are in Figure 3 and 4.A.1 Word Frequency   We also conducted an experiment to correlate the   scores with their frequency . For this purpose ,   the corresponding objects of each target were se-   lected . And then the distance correlation between   the scores and the corresponding word frequency   was calculated based on the average of the last 10   years of Google Ngrams . The results are shown in   Table 9 . The correlations are not particularly sig-   nificant ( mostly p ≥0.1 ) , but it is noticeable that   especially the cosine score depends strongly on the   word frequency . The classifiers are generally less   sensitive.58025803580458055806 s5807