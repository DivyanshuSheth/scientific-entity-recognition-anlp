  Hao Sun , Yang Li , Liwei Deng , Bowen Li , Binyuan Hui   Binhua Li , Yunshi Lan , Yan Zhang , Yongbin LiPeking University , Alibaba GroupUniversity of Electronic Science and Technology of China , East China Normal University   Abstract   Context information modeling is an important   task in conversational KBQA . However , ex-   isting methods usually assume the indepen-   dence of utterances and model them in isolation .   In this paper , we propose a History Semantic   GraphEnhanced KBQA model ( HSGE ) that   is able to effectively model long - range se-   mantic dependencies in conversation history   while maintaining low computational cost . The   framework incorporates a context - aware en-   coder , which employs a dynamic memory de-   cay mechanism and models context at different   levels of granularity . We evaluate HSGE on   a widely used benchmark dataset for complex   sequential question answering . Experimental   results demonstrate that it outperforms existing   baselines averaged on all question types .   1 Introduction   In recent years , with the development of large - scale   knowledge base ( KB ) like DBPedia ( Auer et al . ,   2007 ) and Freebase ( Bollacker et al . , 2008 ) , Knowl-   edge Base Question Answering ( KBQA ) ( Wang   et al . , 2020 ; Ye et al . , 2021 ; Yan et al . , 2021 ; Yadati   et al . , 2021 ; Das et al . , 2021 ; Wang et al . , 2022 )   has become a popular research topic , which aims   to convert a natural language question to a query   over a knowledge graph to retrieve the correct an-   swer . With the increasing popularity of AI - driven   assistants ( e.g. , Siri , Alexa and Cortana ) , research   focus has shifted towards conversational KBQA   ( Shen et al . , 2019 ; Kacupaj et al . , 2021 ; Marion   et al . , 2021 ) that involves multi - turn dialogues .   A common solution to the task of conversational   KBQA is to map an utterance to a logical form   using semantic parsing approach ( Shen et al . , 2019 ;   Guo et al . , 2018 ) . The state - of - the - art semantic   parsing approach ( Kacupaj et al . , 2021 ) breaks   down the process into two stages : a logical form   is first generated by low - level features and then   the missing details are filled by taking both theFigure 1 : An example illustrating the task of conversa-   tional KBQA .   question and templates into consideration . Other   approaches ( Dong and Lapata , 2016 ; Liang et al . ,   2016 ; Guo et al . , 2018 ) mainly focus on first detect-   ing entities in the question and then mapping the   question to a logical form .   Despite the inspiring results of the semantic pars-   ing methods mentioned above , most of them fail   to model the long - range semantic dependency in   conversation history . Specifically , they usually di-   rectly incorporate immediate two turns of conversa-   tions and ignore the conversation history two turns   away . To demonstrate the importance of long - range   conversation history , Figure 1 shows an example   illustrating the task of conversational KBQA . After   the question “ who is the president of the United   States ” , the user consecutively proposes three ques-   tions that involve Coreference andEllipsis phe-   nomena ( Androutsopoulos et al . , 1995 ) . Only when   the system understands the complete conversation   history can the system successfully predict the an-   swer . Though existing contextual semantic parsing   models ( Iyyer et al . , 2017 ; Suhr et al . , 2018 ; Yu   et al . , 2019 ) can be used to model conversation   history , a survey ( Liu et al . , 2020 ) points out that   their performance is not as good as simply concate-   nating the conversation history , which is the most   common conversation history modeling technique .   To tackle the issues mentioned above , we pro-3521pose a History Semantic GraphEnhanced Conver-   sational KBQA model ( HSGE ) for conversation   history modeling . Specifically , we convert the log-   ical forms of previous turns into history semantic   graphs , whose nodes are the entities mentioned in   the conversation history and edges are the relations   between them . By applying graph neural network   on the history semantic graph , the model can cap-   ture the complex interaction between the entities   and improve its understanding of the conversation   history . From the perspective of practice , using the   history semantic graph to represent the conversa-   tion history is also more computationally efficient   than directly concatenating the conversation his-   tory . Besides , we design a context - aware encoder   that addresses user ’s conversation focus shift phe-   nomenon ( Lan and Jiang , 2021 ) by introducing   temporal embedding and allows the model to incor-   porate information from the history semantic graph   at both token - level and utterance - level .   To summarize , our major contributions are :   •We propose to model conversation history us-   ing history semantic graph , which is effective   and efficient . As far as we know , this is the   first attempt to use graph structure to model   conversation history in conversational KBQA .   •We design a context - aware encoder that uti-   lizes temporal embedding to address the shift   of user ’s conversation focus and aggregate   context information at different granularities .   •Extensive experiments on the widely used   CSQA dataset demonstrate that HSGE   achieves the state - of - the - art performance av-   eraged on all question types .   2 Related Work   The works most related to ours are those investigat-   ing semantic parsing - based approaches in conver-   sational KBQA . Given a natural language question ,   traditional semantic - parsing methods ( Zettlemoyer   and Collins , 2009 ; Artzi and Zettlemoyer , 2013 )   usually learn a lexicon - based parser and a scoring   function to produce a logical form . For instance ,   ( Zettlemoyer and Collins , 2009 ) propose to learn a   context - independent CCG parser and ( Long et al . ,   2016 ) utilizes a shift - reduce parser for logical form   construction .   Recently , neural semantic parsing approaches   are gaining attention with the development of deeplearning ( Qu et al . , 2019 ; Chen et al . , 2019 ) . For   example , ( Liang et al . , 2016 ) introduces a neural   symbolic machine ( NSM ) extended with a key-   value memory network . ( Guo et al . , 2018 ) pro-   poses D2A , a neural symbolic model with memory   augmentation . S2A+MAML ( Guo et al . , 2019 ) ex-   tends D2A with a meta - learning strategy to account   for context . ( Shen et al . , 2019 ) proposes the first   multi - task learning framework MaSP that simul-   taneously learns type - aware entity detection and   pointer - equipped logical form generation . ( Plepi   et al . , 2021 ) introduces CARTON which utilizes   pointer networks to specify the KG items . ( Kacu-   paj et al . , 2021 ) proposes a graph attention network   to exploit correlations between entity types and   predicates . ( Marion et al . , 2021 ) proposes to use   KG contextual data for semantic augmentation .   While these methods have demonstrated promis-   ing results , they typically only consider the imme-   diate two turns of conversations as input while ne-   glecting the context two turns away . Though ( Guo   et al . , 2018 ) introduces a Dialog Memory to main-   tain previously observed entities and predicates , it   fails to capture their high - order interaction infor-   mation . By introducing history semantic graph , our   model HSGE can not only memorize previously ap-   peared entities and predicates but also model their   interaction features using GNN to gain a deeper   understanding of conversation history .   3 Method   The structure of our proposed HSGE model is il-   lustrated in Figure 2 . The model consists of six   components : Word Embedding , TransformerConv   Layer , Context - aware Encoder , Entity Recogni-   tion Module , Concept - aware Attention Module and   Grammar - Guided Decoder .   3.1 Grammar   We predefined a grammar with various actions   in Table 4 , which can result in different logical   forms that can be executed on the KG . Analo-   gous to ( Kacupaj et al . , 2021 ) , each action in this   work consists of three components : a semantic   category , a function symbol and a list of argu-   ments with specified semantic categories . Amongst   them , semantic categories can be classified into   two groups depending on the ways of instantia-   tion . One is referred to as entry semantic cate-   gory ( i.e. , { e , p , tp , num } for entities , predicates ,   entity types and numbers ) whose instantiations3522   are constants parsed from a question . Another   is referred to as intermediate semantic category   ( i.e. ,{set , dict , boolean , number } ) whose instan-   tiation is the output of an action execution .   3.2 Input and Word Embedding   To incorporate the recent dialog history from previ-   ous interactions , the model input for each turn con-   tains the following utterances : the previous ques-   tion , the previous answer and the current question .   Utterances are separated by a [ SEP ] token and a   context token [ CLS ] is appended at the beginning   of the input as the semantic representation of the   entire input .   Specifically , given an input u , we use WordPiece   tokenization ( Wu et al . , 2016 ) to tokenize the con-   versation context into token sequence { w , ... , w } ,   and then we use the pre - trained language model   BERT ( Devlin et al . , 2018 ) to embed each word   into a vector representation space of dimension d.   Our word embedding module provides us with an   embedding sequence { x , ... , x } , where x∈R   is given by x = BERT(w ) .   3.3 History Semantic Graph   To effectively and efficiently model conversation   history that contains multiple turns , we design His-   tory Semantic Graph , inspired by the recent stud-   ies on dynamically evolving structures ( Hui et al . ,   2021 ) . As the conversation proceeds , more and   more entities and predicates are involved , which   makes it difficult for the model to capture the com-   plex interactions among them and reason over them .   Thus , we hope to store these information into a   graph structure and empower the model with strong   reasoning ability by applying GNN onto the graph .   Considering that we are trying to model the inter-   actions between entities and predicates which are   naturally included in logical forms , one good so-   lution is to directly convert the logical forms into   KG triplets as shown in Figure 3 . By doing so ,   we guarantee the quality of the graph because the   entities and predicates are directly related to the   answers of previous questions , while also injecting   history semantic information into the graph .   Graph Construction . Specifically , we define the   history semantic graph to be G=<V , E > , where   V = set(e)∪set(tp),E = set(p ) , and e , tp , p de-   note entity , entity type and predicate , respectively .   We define the following rules to transform the ac-   tions defined in Table 4 to the KG triplets :   •For each element ein the operator result of   set→find(e , p ) , we directly add < e , p , e >   into the graph .   •For each element ein the operator result of   set→find _ reverse ( e , p ) , we directly add   < e , p , e > into the graph .   •For each entity e∈ V , we also add the3523 < e , IsA , tp > to the graph , where tpis the   entity type of entity eextracted from Wiki-   data knowledge graph .   •For the find andfind _ reverse actions   that are followed by filter _ type or   filter _ multi _ types action for entity fil-   tering , we would add the element in the   filtering result to the graph , which prevents   introducing unrelated entities into the graph .   It is worth mentioning that we choose to trans-   form these actions because they directly model the   relationship between entities and predicates . Be-   sides , as the conversation proceeds and new log-   ical forms are generated , more KG triplets will   be added to the graph and the graph will grow   larger . However , the number of nodes involved in   the graph is still relatively small and is highly con-   trollable by only keeping several recent KG triplets .   Considering the O(N)computational complexity   of Transformer encoders ( Vaswani et al . , 2017 ) , it   would be more computationally efficient to model   conversation history using history semantic graph   than directly concatenating previous utterances .   Graph Reasoning . Given constructed history   semantic graph G , we first initialize the embed-   dings of nodes and relations using BERT , i.e. ,   BERT(e / p ) , where eandprepresent the text   of node and relation , respectively . Then we fol-   low TransformerConv ( Shi et al . , 2020 ) and update   node embeddings as follows :   H = TransformerConv ( E , G ) ( 1 )   where E∈Rdenotes the embeddings   of nodes and relations .   3.4 Context - aware Encoder   Temporal Information Modeling . As the con-   versation continues and further inquiries are raised ,   individuals tend to focus more on recent entities ,   which is also called Focal Entity Transition   phenomenon ( Lan and Jiang , 2021 ) . To incorporate   this insight into the model , we introduce tempo-   ral embedding to enable the model to distinguish   newly introduced entities . Specifically , given the   current turn index tand previous turn index iin   which entities appeared , we define two distance   calculation methods :   •Absolute Distance : The turn index of the   previous turn in which the entities were men-   tioned , i.e. , D = t.•Relative Distance : The difference in turn in-   dices between the current turn and the previ-   ous turn in which the entities were mentioned ,   i.e. ,D = t−i .   For each method , we consider two approaches   for representing the distance : unlearnable posi-   tional embedding and learnable positional embed-   ding . For unlearnable positional encoding , the com-   putation is defined using the following sinusoid   function ( Vaswani et al . , 2017 ):   /braceleftbigge(2i ) = sin(D/10000 ) ,   e(2i+ 1 ) = cos(D/10000),(2 )   where iis the dimension and Dis the absolute   distance or relative distance .   For learnable positional encoding , the positional   encoding is defined as a learnable matrix E∈   R , where Mis the predefined maximum num-   ber of turns .   Then we directly add the temporal embedding to   obtain temporal - aware node embeddings .   ¯h = h+e , ( 3 )   where his the embedding of node e.   Semantic Information Aggregation . As the   conversation progresses , user ’s intentions may   change frequently , which leads to the appearance   of intention - unrelated entities in history semantic   graph . To address this issue , we introduce token-   level and utterance - level aggregation mechanisms   that allow the model to dynamically select the most   relevant entities . These mechanisms also enable the   model to model contextual information at different   levels of granularity .   •Token - level Aggregation : For each token x ,   we propose to attend all the nodes in the his-   tory semantic graph to achieve fine - grained   modeling at token - level :   x = MHA ( x,¯H,¯H ) ,   ¯x = x+x,(4 )   where MHA denotes the multi - head attention   mechanism and ¯Hdenotes the embeddings of   all nodes in the history semantic graph .   •Utterance - level Aggregation : Sometimes the   token itself may not contain semantic infor-   mation , e.g. , stop words . We further pro-   pose to incorporate history information at the3524utterance - level for these tokens :   x = MHA ( x,¯H,¯H ) ,   ¯x = x+x,(5 )   where x denotes the representation of the   [ CLS ] token .   Then , history - semantic - aware token embeddings   are forwarded as input to the encoder of Trans-   former ( Vaswani et al . , 2017 ) for deep interaction :   h = Encoder ( ¯X;θ ) , ( 6 )   where θare encoder trainable parameters .   3.5 Grammar - Guided Decoder   After encoding all the semantic information into   the hidden state h , we utilize stacked masked   attention mechanism ( Vaswani et al . , 2017 ) to gen-   erate sequence - formatted logical forms . Specifi-   cally , in each decoding step , our model predicts a   token from a small decoding vocabulary V=   { start , end , e , p , tp , ... , find } , where all the ac-   tions from the Table 4 are included . On top of   the decoder , we employ a linear layer alongside a   softmax to calculate each token ’s probability distri-   bution in the vocabulary . The detailed computation   is defined as follows :   h = Decoder ( h;θ ) ,   p = Softmax ( Wh),(7 )   where h is the hidden state at time step t ,   θ , Ware decoder trainable parameters ,   p∈Ris the probability distribution   over the decoding vocabulary at time step t.   3.6 Entity Recognition Module   Entity recognition module aims to fill the entity slot   in the predicted logical forms , which consists of   entity detection module and entity linking module .   Entity Detection . The goal of entity detection is   to identify mentions of entities in the input . Pre-   vious studies ( Shen et al . , 2019 ) have shown that   multiple entities of different types in a large KB   may share the same entity text , which is a common   phenomenon called Named Entity Ambiguity .   To address this issue and inspired by ( Kacupaj   et al . , 2021 ) , we adopt a type - aware entity detec-   tion approach using BIO sequence tagging . Specif-   ically , the entity detection vocabulary is definedasV={O,{B , I}×{TP } } , where TP   denotes the i - th entity type label , Nstands for   the number of distinct entity types in the knowledge   graph and |V|= 2×N+ 1 . We leverage   LSTM ( Hochreiter and Schmidhuber , 1997 ) to per-   form the sequence tagging task :   h = LeakyReLU ( LSTM ( h;θ ) ) ,   p = Softmax ( Wh),(8 )   where his the encoder hidden state , θare the   LSTM trainable parameters , his the LSTM hid-   den state at time step t , andpis the probability   distribution over Vat time step t.   Entity Linking . Once we detect the entities in   the input utterance , we perform entity linking to   link the entities to the entity slots in the predicted   logical form . Specifically , we define the entity   linking vocabulary as V={0,1 , ... , M } where   0means that the entity does not link to any entity   slot in the predicted logical form and Mdenotes   the total number of indices based on the maximum   number of entities from all logical forms . The   probability distribution is defined as follows :   h = LeakyReLU ( W[h;h ] ) ,   p = Softmax ( Wh),(9 )   where W , Ware trainable parameters ,   his the hidden state at time step t , and p   is the probability distribution over the tag indices   Vat time step t.   3.7 Concept - aware Attention Module   In the Concept - aware Attention Module , we first   model the complex interaction between entity types   and predicates , then we predict the entity types and   predicates for the logical form .   To begin with , we first develop an entity - to-   concept converter to replace the entities in each   factual triple of Wikidata KG with correspond-   ing concepts ( i.e. , entity types ) . Take an instance   in Figure 3 as example , the factual triple ( Joe   Biden , IsPresidentOf , USA ) can be transformed   to two concept - level tuples ( Person , IsPresidentOf ) ,   and ( IsPresidentOf , Country ) in the concept graph .   Then , we initialize node embeddings using their   texts with BERT and apply Graph Attention Net-   works ( GAT ) ( Veli ˇckovi ´ c et al . , 2017 ) to project   the KG information into the embedding space.3525Finally , we model the task of predicting the cor-   rect entity type or predicate of the logical form as a   classification task . For each time step of decoding ,   we directly calculate the probability distribution at   time step tas :   h = LeakyReLU ( W[h;h ] ) ,   p = Softmax ( hh),(10 )   where his the updated entity type and predicate   embedding and pis the probability distribution   over them at time step t.   3.8 Training   The framework consists of four trainable modules :   Entity Detection Module , Entity Linking Module ,   Grammar - guided Decoder and Concept - aware At-   tention Module . Each module consists of a loss   function that can be used to optimize the parame-   ters in itself . We use the weighted average of all   the losses as our loss function :   L = λL+λL+λL+λL,(11 )   where λ , λ , λ , λare the weights that decide the   importance of each component . The detailed loss   calculation method is in Appendix B. The multi-   task setting enables modules to share supervision   signals , which benefits the model performance .   4 Experiments   4.1 Experimental Setup   Dataset . We conduct experiments on CSQA   ( Complex Sequential Question Answering )   dataset(Saha et al . , 2018 ) . CSQA was built based   on the Wikidata knowledge graph , which consists   of 21.1 M triples with over 12.8 M entities , 3,054   entity types and 567 predicates . CSQA dataset is   the largest dataset for conversational KBQA and   consists of around 200 K dialogues where training   set , validation set and testing set contain 153 K ,   16 K and 28 K dialogues , respectively . Questions   in the dataset are classified as different types , e.g. ,   simple questions , logical reasoning and so on .   Metrics . To evaluate HSGE , We use the same   metrics as employed by the authors of the CSQA   dataset as well as the previous baselines . F1 score   is used to evaluate the question whose answer is   comprised of entities , while Accuracy is used tomeasure the question whose answer is a number or   a boolean number . Following ( Marion et al . , 2021 ) ,   we do n’t report results for “ Clarification ” question   type , as this question type can be accurately mod-   eled with a simple classification task .   Baselines . We compare HSGE with the latest   five baselines that include D2A ( Guo et al . , 2018 ) ,   S2A+MAML ( Guo et al . , 2019 ) , MaSP ( Shen   et al . , 2019 ) , OAT ( Marion et al . , 2021 ) and   LASAGNE ( Kacupaj et al . , 2021 ) .   4.2 Overall Performance   Table 1 summarizes the results comparing the   HSGE framework against the previous baselines .   From the result , we have three observations :   ( 1 ) The D2A and S2A - MAML models exhibit   superior performance on the Simple Question ( Di-   rect ) question type . This can likely be attributed to   their ability to memorize context information pre-   viously mentioned in the conversation . However ,   these models fail to model the complex interaction   between entities , resulting in inferior performance   on other question types .   ( 2 ) OAT achieves superior performance on three   question types , which might be attributed to its   incorporation of additional KG information . How-   ever , its performance is not consistent across all   question types , leading to a low overall perfor-   mance averaged on all question types .   ( 3 ) Our method HSGE achieves the new SOTA   on the overall performance averaged on all ques-   tion types . There are two possible reasons for the   improvement . First , the incorporation of HSG al-   lows the modeling of longer dependencies within   the context , enabling the model to handle situations   where the user asks about entities that were previ-   ously mentioned . Second , by utilizing graph neural   network to facilitate information flow in HSG , the   interaction among previously appeared entities , en-   tity types and predicates are better captured , which   endows our model with stronger reasoning ability .   4.3 Ablation Study   In this section , we first conduct experiments to   verify the effectiveness of each model component .   Then , we investigate the effects of different model   choices inside the Context - aware Encoder . Finally ,   we compare our HSGE with the most widely used   concatenation method .   Effect of HSG and TIM . To show the effective-   ness of each component , we create two ablations3526   by directly removing history semantic graph ( HSG )   and temporal information modeling ( TIM ) , respec-   tively . As shown in Table 2 , HSGE outperforms   all the ablations across all question types , which   verifies the importance of each model component .   It is worth mentioning that after removing HSG ,   the performance of our method on some question   types that require reasoning ( i.e. , Logical Reason-   ing , Quantitative Reasoning ( Count ) ) drops sig-   nificantly . We think that the reason might be the   utilization of graph neural network on HSG empow-   ers the model with great reasoning ability , which   further benefits model performance .   Comparison of Internal Model Choice . In   context - aware encoder , we design two distance cal-   culation methods ( i.e. , absolute distance and rela-   tive distance ) for temporal information modeling ,   as well as two information aggregation granular-   ities ( i.e. , token - level and utterance - level aggre-   gation ) for semantic information aggregation . To   study their effects , we conduct experiments by fix-   ing one setting while changing the other . And the   comparison result is shown in Figure 4 .   From the results , it is obvious that we can get the   following conclusions : ( 1 ) Token - level aggregation   method performs better than utterance - level aggre-   gation method . This is because the token - level ag-   gregation allows the model to incorporate context   information at a finer granularity and the informa-   tion unrelated to the target token can be removed .   ( 2 ) Absolute distance method performs better than   relative distance method . The reason may be that   although both distance calculation methods can   provide temporal information , absolute distance is   more informative since the model can derive rel-   ative distance using absolute distance while the   opposite is not true.3527   Comparison with Concatenation Method . One   of the most widely used methods for context mod-   eling is to directly concatenate history conversa-   tions ( Liu et al . , 2020 ) . To analyze its effectiveness ,   we remove HSG and observe the performance of   seven representative question types using the con-   catenation of history conversations as input , which   is shown in Figure 5 .   As we can see , at the initial stages of concate-   nation turn number increase , the performances on   some question types increase a little while remain-   ing unchanged or even decreasing on others , lead-   ing to an almost unchanged overall performance . It   is reasonable because history turns contain useful   semantic information , which leads to performance   gain . However , as more conversation turns are in-   troduced into the model , more noisy tokens will   also be introduced into the model , which leads to   performance degradation . Besides , the introduction   of more context tokens will also lead to an increase   in computational cost with the O(N)complexity .   It is worth noting that the best setting of concate-   nation method still performs worse than HSGE . It   is mainly because we use attention mechanism to   dynamically select the most related entities from   the HSG , which achieves effective history model-   ing while avoiding introducing noisy information .   And as we only extract entities and predicates from   history conversations , the size of the graph is rela-   tively small and the increase in computational cost   as the conversation progresses is marginal.4.4 Subtask Analysis   The task of conversational KBQA involves multi-   ple subtasks , each of which can directly impact the   final model accuracy . To gain a deeper understand-   ing of HSGE , we compare its performance of each   subtask with the current SOTA model LASAGNE   in Table 3 . We can observe that most of the sub-   task ’s performance in HSGE is better than that of   LASAGNE and mostly achieves accuracy above   90 % . Amongst them , the improvement in Entity   Detection is the largest . We think the main reason   is that the token - level aggregation mechanism en-   dows each token with richer semantic information .   4.5 Error Analysis   In this section , we randomly sample 200 incorrect   predictions and analyze their error causes :   Entity Ambiguity . Entity ambiguity refers to the   situation where there exist multiple entities with   the same text and type in the Wikidata knowledge   graph . For example , we can not distinguish multiple   people called “ Mary Johnson ” because we have no   more information other than entity text and entity   type . We believe that incorporating other contex-   tual information such as entity descriptions may   help solve this problem ( Mulang et al . , 2020 ) .   Spurious Logical Form . We follow ( Shen et al . ,   2019 ; Kacupaj et al . , 2021 ) and produce golden   logical forms by leveraging BFS to search valid   logical forms for questions in training data . This   can sometimes lead to wrong golden actions such   as two actions with different semantic information   but accidentally sharing the same execution result .   This may misguide our model during training .   5 Conclusion   In this paper , we propose a novel Conversational   KBQA method HSGE , which achieves effective   history modeling with minimal computational cost .   We design a context - aware encoder that introduces   temporal embedding to address user ’s conversation   focus shift phenomenon and aggregate context in-   formation at both token - level and utterance - level .   Our proposed HSGE outperforms existing base-   lines averaged on all question types on the widely   used CSQA dataset.3528References35293530   A Grammar   The grammar we use in this work is defined in   Table 4 . Please note that each single action can   only model relatively simple semantics . High - level   semantics of complex question is achieved by inte-   grating multiple actions into a single logical form .   B Loss Calculation   L , L , LandLare the negative log-   likelihood losses of the Entity Detection Module ,   Entity Linking Module , Grammar - guided Decoder   and Concept - aware Attention Module , respectively .   These losses are defined as follows :   L=−/summationdisplaylogp(y|x ) ,   L=−/summationdisplaylogp(y|x ) ,   L=−/summationdisplaylogp(y|x ) ,   L=−/summationdisplaylogp(y|x),(12 )   where nandmare the length of the input ut-   terance xand golden logical form , respectively .   y , y , y , yare the golden labels for En-   tity Detection Module , Entity Linking Module ,   Grammar - guided Decoder and Concept - aware At-   tention Module , respectively . CHyper - parameters and Implementation   Details   Parameters Setting   Optimizer BertAdam   Batch Size 120   Hidden Size 768   Learning Rate 5e-5   Head Number 6   Aggregation Level Token - level   Activation Function ReLU   Distance Calculation Absolute   Encoder Layer Number 2   Decoder Layer Number 2   Loss Component Weight All set to 1   GAT Embedding Dimension 3072   Word Embedding Dimension 768   The experiments are conducted on 8 NVIDIA V100   GPUs . During model tuning , we identify opti-   mal hyperparameters by modifying one parameter   while keeping others fixed and select the hyper-   parameters that resulted in the highest model per-   formance . We implement our code using Pytorch .   The detailed hyper - parameter setting for HSGE is   shown in Table 5.3531ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   In section Limitations   /squareA2 . Did you discuss any potential risks of your work ?   This work was conducted in accordance with ethical principles . We use the publicly available   dataset for the experiments and have no potential risks about credentials or data privacy . No human   participants are involved in our experiment . Therefore , we do n’t foresee any potential risk of this   work .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   In section Abstract   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   In Section Method   /squareB1 . Did you cite the creators of artifacts you used ?   In Section Experiments   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We directly used the original CSQA dataset and did not change it . This dataset is released under   Creative - Commons license   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   The datasets are used widely by the research community for studying csqa .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No personal information is involved in the dataset   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   The datasets are used widely by the research community for studying csqa .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   In Section Experiment3532C / squareDid you run computational experiments ?   In Section Experiment   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In Section Experiment   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   In Section Appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   In Section Experiment   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   In Section Experiment   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.3533