  Rakesh R MenonSayan GhoshShashank Srivastava   UNC Chapel Hill   { rrmenon , sayghosh , ssrivastava}@cs.unc.edu   Abstract   Supervised learning has traditionally focused   on inductive learning by observing labeled ex-   amples of a task . In contrast , humans have the   ability to learn new concepts from language .   Here , we explore learning zero - shot classiﬁers   for structured datapurely from language from   natural language explanations as supervision .   For this , we introduce CLUES , a benchmark   forClassiﬁer Learning Using natural language   Explanation S , consisting of a range of classi-   ﬁcation tasks over structured data along with   natural language supervision in the form of ex-   planations . CLUES consists of 36 real - world   and 144 synthetic classiﬁcation tasks . It con-   tains crowdsourced explanations describing   real - world tasks from multiple teachers and   programmatically generated explanations for   the synthetic tasks . We also introduce ExEnt ,   an entailment - based method for training clas-   siﬁers from language explanations , which ex-   plicitly models the inﬂuence of individual ex-   planations in making a prediction . ExEnt gen-   eralizes up to 18 % better ( relative ) on novel   tasks than a baseline that does not use expla-   nations . We identify key challenges in learn-   ing from explanations , addressing which can   lead to progress on CLUES in the future . Our   code and datasets are available at : https :   //clues - benchmark.github.io .   1 Introduction   Humans have a remarkable ability to learn concepts   through language ( Chopra et al . , 2019 ; Tomasello ,   1999 ) . For example , we can learn about poisonous   mushrooms through an explanation like ‘ a mush-   room is poisonous if it has pungent odor ’ . SuchFigure 1 : We explore learning classiﬁcation tasks over   structured data from natural language supervision in   form of explanations . The explanations provide declar-   ative supervision about the task , and are not example-   speciﬁc . This is an example from the UCI Mushroom   dataset , one of the 36 real - world datasets for which we   collect multiple sets of explanations in CLUES .   an approach profoundly contrasts with the pre-   dominant paradigm of machine learning , where   algorithms extract patterns by looking at scores of   labeled examples of poisonous and edible mush-   rooms . However , it is unnatural to presume the   availability of labeled examples for the heavy tail   of naturally occurring concepts in the world .   This work studies how models trained to learn   from natural language explanations can generalize   to novel tasks without access to labeled examples .   While prior works in this area ( Srivastava et al . ,   2017 , 2018 ; Hancock et al . , 2018 ; Murty et al . ,   2020 ; Andreas et al . , 2018 ; Wang * et al . , 2020 ; Ye   et al . , 2020 ; Zhou et al . , 2020 ) have explored ex-   planations as a source of supervision , they evaluate   models on a small number of tasks ( 2 - 3 relation   extraction tasks in ( Hancock et al . , 2018 ; Wang *   et al . , 2020 ; Murty et al . , 2020 ; Zhou et al . , 2020 ) , 7   email categorization tasks ( Srivastava et al . , 2017 ) ) .   Owing to the paucity of large - scale benchmarks for6523learning from explanations over diverse tasks , we   develop CLUES , a benchmark of classiﬁcation tasks   paired with natural language explanations . Over   the last few decades , researchers and engineers   alike have put immense effort into constructing   structured and semi - structured knowledge bases   ( e.g. , structured tables on Wikipedia , e - commerce   sites , etc . ) . Developing models that can reason over   structured data is imperative to improve the acces-   sibility of machine learning models , enabling even   non - experts to interact with such data . Hence , in   this work , we speciﬁcally formulate our classiﬁca-   tion tasks over structured data .   Our benchmark is divided into CLUES - Real   andCLUES - Synthetic consisting of tasks from   real - world ( UCI , Kaggle , and Wikipedia ) and syn-   thetic domains respectively . Explanations for   CLUES - Real are crowdsourced to mimic the diver-   sity and difﬁculty of human learning and pedagogy .   ForCLUES - Synthetic , we generate the explana-   tions programmatically to explicitly test models ’   reasoning ability under a range of structural and   linguistic modiﬁcations of explanations .   We train models with a mix of explanations and   labeled examples , in a multi - task setup , over a set   ofseen classiﬁcation tasks to induce generaliza-   tion to novel tasks , where we do not have any la-   beled examples . Ye et al . ( 2021 ) refer to this prob-   lem setup as “ cross - task generalization " . Some   recent methods on cross - task generalization from   language use instructions / prompts ( Mishra et al . ,   2022 ; Sanh et al . , 2022 ; Wei et al . , 2021 ) describ-   ing information about ‘ what is the task ? ’ to query   large language models . In contrast , language expla-   nations in CLUES provide the logic for performing   the classiﬁcation task , or intuitively ‘ how to solve   the task ? ’ . For the running example of mushroom   classiﬁcation , an instruction / prompt might be ‘ can   you classify a mushroom with pungent odor as poi-   sonous or edible ? ’ . On the other hand , an example   of an explanation in CLUES is‘a mushroom is poi-   sonous if it has pungent odor ’ .   We ﬁnd that simply concatenating explanations   to the input does not help pre - trained models , like   RoBERTa ( Liu et al . , 2019 ) , generalize to new   tasks . Thus , we develop ExEnt , an entailment-   based model for learning classiﬁers guided by ex-   planations , which explicitly models the inﬂuence   of individual explanations in deciding the label of   an example . ExEnt shows a relative improvement   of up to 18 % over other baselines on unseen tasks . To identify the challenges of learning from ex-   planations , we perform extensive analysis over syn-   thetic tasks . Our analysis explores how the struc-   ture of an explanation ( simple clauses vs. nested   clauses ) and the presence of different linguistic   components in explanation ( conjunctions , disjunc-   tions , and quantiﬁers ) affect the generalization abil-   ity of models .   The rest of the paper is structured as follows :   we describe our crowdsourced - benchmark creation   pipeline in § 3 . In § 4 , we analyze our collected   data . In § 5 , we describe our models , experiments ,   and results . We conclude with a brief discussion   on the contributions and our ﬁndings , followed   by a statement of ethics and broader impact . Our   contributions are :   •We introduce CLUES , a benchmark for learning   classiﬁers over structured data from language .   •We develop ExEnt , an entailment - based model   for learning classiﬁers guided by explanations .   ExEnt shows a relative improvement of up to   18 % over other baselines on generalization to   novel tasks .   •We explore the effect on the generalization abil-   ity of models learning from language by ablating   the linguistic components and structure of expla-   nations over our benchmark ’s synthetic tasks .   2 Related Work   Learning concepts from auxiliary information :   Prior work has explored techniques to incorporate   ‘ side - information ’ to guide models during training   ( Mann and McCallum , 2010 ; Ganchev et al . , 2010 ) .   More recently , researchers have explored using lan-   guage in limited data settings for learning tasks   such as text classiﬁcation ( Srivastava et al . , 2017 ,   2018 ; Hancock et al . , 2018 ) and question answer-   ing ( Wang * et al . , 2020 ; Ye et al . , 2020 ) . However ,   we diverge from these works by exploring the gen-   eralization ability of classiﬁers learned by using   language over novel tasks as opposed to gauging   performance only on seen tasks .   Explanation - based Datasets : The role of expla-   nations and how they can inﬂuence model behav-   ior is a widely studied topic in machine learn-   ing ( Wiegreffe and Marasovi ´ c , 2021 ) . Among   language - based explanation studies , past work has   primarily developed datasets that justify individual   predictions made by a model ( also called , local   explanations ) ( Rajani et al . , 2019 ; Camburu et al . ,   2018 ) , inter alia . In contrast , our work focuses6524on explanations that deﬁne concepts and capture   a broad range of examples rather than individual   examples . Our notion of explanations is shared   with Andreas et al . ( 2018 ) ; Srivastava et al . ( 2017 ,   2018 ) . We differ from these works as ( 1 ) our bench-   mark comprises a large set of classiﬁcation tasks   spanning diverse concepts for learning from expla-   nations as opposed to working on a limited set of   tasks in prior work and ( 2 ) our benchmark is do-   main agnostic in the source of classiﬁcation tasks   considered as long as we can represent the inputs   of the task in a tabular ( structured ) format .   Few - shot & Zero - shot learning : Large pre-   trained language models ( LMs ) ( Devlin et al . , 2019 ;   Liu et al . , 2019 ; Raffel et al . , 2020 ) have been   shown to perform impressively well in few - shot   settings ( Brown et al . , 2020 ; Lester et al . , 2021 ) .   Reformulating natural language tasks with patterns   has been shown to boost few - shot learning abil-   ity for small language models as well ( Schick and   Schütze , 2021 ; Tam et al . , 2021 ) . More recently , a   few works have focused on evaluating the general-   ization of models to unseen tasks by using prompts   and performing multi - task training ( Mishra et al . ,   2022 ; Ye et al . , 2021 ; Sanh et al . , 2022 ; Min et al . ,   2021 ; Chen et al . , 2022 ; Aghajanyan et al . , 2021 )   While the training and evaluation setup is simi-   lar , our work is signiﬁcantly different from these   works as ( 1 ) the explanations in our work provide   rationales for making a classiﬁcation decision as   opposed to explaining a task using prompts , ( 2 ) we   explore classiﬁcation over structured data as op-   posed to free - form text by designing a model that   can leverage explanations .   3 Creating CLUES   In this section , we describe our benchmark creation   process in detail . In CLUES , we frame classiﬁcation   tasks over structured data represented in tabular for-   mat . Based on the source of tables used to construct   the classiﬁcation tasks , we consider two splits of   our benchmark , CLUES - Real ( real - world datasets )   andCLUES - Synthetic ( synthetic datasets ) .   3.1 CLUES - Real   We ﬁrst gather / create classiﬁcation tasks from UCI ,   Kaggle , and Wikipedia tables , then collect explana-   tions for each classiﬁcation task.3.1.1 Collecting classiﬁcation datasets   Classiﬁcation tasks from UCI and Kaggle .   UCI ML repositoryand Kagglehost numerous   datasets for machine learning tasks . For our bench-   mark , we pick out the tabular classiﬁcation datasets .   Then , we manually ﬁlter the available datasets to   avoid ones with ( a ) many missing attributes and ( b )   complex attribute names that require extensive do-   main knowledge making them unsuitable for learn-   ing purely from language . CLUES - Real contains   18 classiﬁcation tasks from UCI and 7 from Kaggle   ( the details of tasks are in Appendix B ) .   Mining tables from Wikipedia . Wikipedia is a   rich , free source of information readily accessi-   ble on the web . Further , a lot of this information   is stored in a structured format as tables . We ex-   plore creating additional classiﬁcation tasks based   on tables from Wikipedia , where each row in a   table is assigned a category label . However , only   a small fraction of the tables might be suitable   to frame a classiﬁcation task for our benchmark .   Thus , we need to identify suitable tables by min-   inga large collection of tables from Wikipedia   ( we use Wikipedia dump available on April 2021 ) .   We formalize this mining - and - pruning process as a   crowdsourcing task ( on Amazon Mechanical Turk ) ,   where we present each turker with a batch of 200   tables and ask them to pick out suitable tables from   that batch . For a table considered suitable by a   turker , we further ask the turker to mention which   column of the table should be considered as pro-   viding the classiﬁcation labels . We identiﬁed 11   classiﬁcation tasks corresponding to 9 Wikipedia   tables after mining around 10 K Wikipedia tables   ( the details of tasks are provided in Appendix B ) .   3.1.2 Explanation Collection Pipeline   Our explanation collection process consists of two   stages – ( 1 ) teachers providing explanations after   reviewing multiple labeled examples of the task ,   and ( 2 ) students verifying explanations and classi-   fying new examples based on explanations for the   tasks .   Collecting explanations : We use the Amazon Me-   chanical Turk ( AMT ) platform to collect explana-   tions for CLUES - Real . In each HIT , we provide   turkers with a few labeled examples of a dummy   task ( each corresponding to a row in a table ) and   a set of good and bad explanations for the task to6525teach them about the expected nature of explana-   tions . Next , we test them on a ‘ qualiﬁcation quiz ’   to gauge their understanding of good explanations .   Upon qualiﬁcation , the turker advances to the   explanation collection phase of the HIT . At this   stage , the turker is provided with 15 - 16 labeled   examples of a task in CLUES - Real and we ask   them to write explanations describing the logic   behind the classiﬁcation for each class . Turkers are   required to submit a minimum of two explanations   ( 5tokens each ) for each task .   Further , teachers can test their understanding by   taking a validation quiz , where they make predic-   tions over new unlabeled examples from the task .   Based on their informed classiﬁcation accuracy ,   teachers can optionally reﬁne their explanations .   Finally , when turkers are content with their per-   formance , they ‘ freeze ’ the explanations and ad-   vance to the test - quiz where they are evaluated on   a new set of unlabeled examples from the task ( dif-   ferent from validation quiz).We will refer to turk-   ers who have provided responses at this stage as   ‘ teachers ’ since they provide explanations to ‘ teach ’   models about different classiﬁcation tasks .   Veriﬁcation of explanations : After the explana-   tion collection , we validate the utility of the sets of   explanations for a task from each teacher by evalu-   ating if they are useful they are for other humans in   learning the task . For this , a second set of turkers   is provided access to the collected explanations   from a teacher for a task , but no labeled examples .   These turkers are then asked to predict the labels   of test examples from the held - out test set , solely   based on the provided explanations .   Additionally , we ask turkers in the veriﬁcation   stage to give a Likert rating ( 1 - 4 scale ) on the use-   fulness of each explanation . Since the turkers in   the veriﬁcation stage perform the classiﬁcation task   using language explanations from a teacher , we   refer to them as ‘ students ’ for our setup .   Thus , the tasks in CLUES - Real contain explana-   tions from multiple teachers and multiple students   corresponding to a teacher . This provides rich in-   formation about variance in teacher and student per-   formance indicating how amenable different tasks   are for learning via language . We provide insights   into the performance of teachers and students of   our setup in § 4 .   3.2 CLUES - Synthetic   The complexity and fuzziness of real - world con-   cepts and the inherent linguistic complexity of   crowdsourced explanations can often shroud the   aspects of the task that make it challenging for mod-   els to learn from explanations . To evaluate models   in controlled settings where such aspects are not   conﬂated , we create CLUES - Synthetic , a set of   programmatically created classiﬁcation tasks with   varying complexity of explanations ( in terms of   structure and presence of quantiﬁers , conjunctions ,   etc . ) and concept deﬁnitions .   We create tasks in CLUES - Synthetic by ﬁrst se-   lecting a table schema from a pre - deﬁned set of   schemas , then generating individual examples of   the task by randomly choosing values ( within a   pre - deﬁned range , obtained from schema ) for each   column of the table . Next , we assign labels to each   example by using a set of ‘ rules ’ for each task .   In this context , a ‘ rule ’ is a conditional statement   ( analogous to conditional explanations that we see   for real - world tasks ) used for labeling the examples .   We use the following types of rules that differ in   structure and complexity ( cdenotesiclause and   ldenotes a label ):   • Simple : IFcTHENl   • Conjunctive : IFcANDcTHENl   • Disjunctive : IFcORcTHENl   •Nested disjunction over conjunction : IFcOR   ( cANDc ) THENl   •Nested conjunction over disjunction : IFc   AND ( cORc ) THENl   •For each of the above , we include vari-   ants with negations ( in clauses and/or labels ):   Some examples – IFcTHEN NOT l , IFc   OR NOTcTHENl   We also consider other linguistic variations of rules   by inserting quantiﬁers ( such as ‘ always ’ , ‘ likely ’ ) .   The synthetic explanations are template - generated   based on the structure of the rules used in creating6526   the task . For brevity , we defer additional details on   the use of quantiﬁers , label assignment using rules ,   and creation of synthetic explanations to Appendix   A. Overall we have 48 different task types ( based   on the number of classes and rule variants ) using   which we synthetically create 144 classiﬁcation   tasks ( each containing 1000 labeled examples ) .   4 Dataset analysis   In this section , we describe the tasks and the col-   lected explanations in CLUES .   Task Statistics : Table 1 shows the statistics of   tasks in CLUES . The real - world tasks in our bench-   mark are from a wide range of domains , such as   data corresponding to a simple game ( e.g. tic - tac-   toe ) , medical datasets ( e.g. identifying liver pa-   tients ) , merit - classiﬁcation of teachers and students ,   network - related datasets ( eg . internet-ﬁrewall ) ,   among others . The synthetic tasks are created using   table schemas denoting different domains , such as   species of animals , species of birds , etc . ( details in   Appendix A ) .   As seen in Table 1 , 5.4 explanation sets were   collected for each classiﬁcation task from human   teachers on average . Further , each explanation set   was veriﬁed by 3 students during the veriﬁcation   task . An aggregate of 133 teachers provide 318 ex-   planations for tasks in CLUES - Real . All collected   explanations were manually ﬁltered and irrelevant   explanations were removed .   Lexical analysis of explanations : Table 2a shows   the statistics for explanation texts in our dataset .   We evaluate the average length of the explanation   texts , vocabulary size and number of unique bi-   grams present in the explanations .   Explanation characteristics : Following Chopra   et al . ( 2019 ) , we categorize the explanations based   on the different aspects of language ( generics , quan-   tiﬁers , conditional , and negation ) present in these   explanations . Table 3 shows the statistics of various   categories in our dataset . Note that an explanation   might belong to more than one category ( for exam-   ple , an example like “ if the number of hands equal   to 2 , then it is usually foo " , will be categorized   both as having both conditional and quantiﬁers . )   We found that around 52 % of the explanations for   the real - world tasks had quantiﬁers ( such as ‘ some ’ ,   ‘ majority ’ , ‘ most ’ , etc . ) in them . A full list of quan-   tiﬁers present in the data is given in Appendix A.   Reading complexity : We analyze the reading com-   plexity of crowdsourced explanations by using   Flesch reading ease . Reading complexity values   for our crowdsourced explanations vary from 3.12   ( professional grade reading level ) to 106.67 ( easier   than 3rd - grade reading level ) , with a median value   of 65.73 ( 8th/9th - grade reading level ) .   Usefulness of the explanations : During the vali-   dation stage , we ask the turkers to provide a rating   ( on a Likert scale from 1 to 4 ) on the utility of the   explanations for classiﬁcation . The semantics of   ratings are , 1 – ‘ not helpful ’ , 2 – ‘ seems useful ’ ,   3 – ‘ helped in predicting for 1 sample ’ , and 4 –   ‘ mostly helpful in prediction ’ . The average rating   for the explanations in CLUES - Real is 2.78 , denot-   ing most explanations were useful , even if they did   not directly help predict labels in some cases . In   Figure 2(a ) , we also provide a histogram of the   Likert ratings provided by the students .   Characteristics of teachers and students : Fig-   ure 2(b ) shows the normalized teacher performance   vs normalized student performance for teacher-   student pairs in CLUES - Real . Normalized perfor-   mance of an individual teacher ( or , student ) on a   task is deﬁned as the difference between the perfor-   mances of the teacher ( or , student ) and an average   teacher ( or , student ) for the same task . The positive   correlation ( = 0.17 ) suggests that students tend6527(a)(b ) ( c )   to perform well if taught by well - performing teach-   ers . Positive correlation ( = 0.48 ) in Figure 2(c ) ,   indicates that task difﬁculty ( captured by classiﬁca-   tion accuracy ) is well - correlated for a teacher and   student on average .   On visualizing the difference between an aver-   age student and an average teacher performance for   each task in CLUES - Real , we ﬁnd that an average   teacher performs better than the average student   on most tasks . However , for the ‘ tic - tac - toe ’ task   inCLUES - Real , we ﬁnd that the student accuracy   was around 13 % higher than average teacher per-   formance . We hypothesize that this task can be   solved by commonsense reasoning without relying   on the provided explanations , resulting in students   performing better than teachers . We quantify the   average performance of teachers and students on   CLUES - Real in Table 4.We ﬁnd that students per - form lower than teachers on average as expected   since a teacher has more expertise in the task . More-   over , it is challenging to teach a task perfectly using   explanations in a non - interactive setting where a   student can not seek clariﬁcations .   Additional data analysis and details of HIT com-   pensation can be found in Appendix C and D.   5 Experiment Setup and Models   In this section , we describe our training and evalua-   tion setup , our models , and experimental ﬁndings .   5.1 Training and Evaluation Setup   Our goal is to learn a model that , at inference , can   perform classiﬁcation over an input xto obtain the   class labely , given the set of explanations Efor   the classiﬁcation task . Figure 4 shows our setup ,   where we train our model using multi - task training   over a set of tasksTand evaluate generalization   to a new task , t2 T . The task split we use   for our experiments can be found in Appendix E.1 .   We select our best model for zero - shot evaluation   based on the validation scores on the seen tasks .   Since we do not make use of any data from the   novel tasks to select our best model , we maintain   thetrue zero - shot setting ( Perez et al . , 2021 ) .   We encode each structured data example , x , as   a text sequence , by linearizing it as a sequence   of attribute - name and attribute - value pairs , sepa-   rated by [ SEP ] tokens . To explain , the leftmost   attribute - name and attribute - value pair of struc-   tured input example in Figure 1 is represented as   ‘ odor | pungent ’ . The linearization allows   us to make use of pre - trained language models for   the classiﬁcation task . Our linearization technique6528   is similar to the one used in Yin et al . ( 2020 ) with   the exception that we do not use the column type .   We will refer to the linearized format of structured   inputs by ‘ Features - as - Text ’ or ‘ FaT ’ .   5.2 Baseline models   For our baselines , we make use of a pre - trained   RoBERTa model ( Liu et al . , 2019 ) . However ,   RoBERTa with the standard-ﬁne - tuning approach   can not allow a generalization test as the number of   output classes varies for each task . Furthermore ,   we can not train individual class heads at inference   since we test zero - shot . Hence , we make the fol-   lowing modiﬁcations to make RoBERTa amenable   for zero - shot generalization tests : a pre - trained   RoBERTa model takes the linearized structured   data ( FaT ) as input and outputs a representation   for this context ( in the [ CLS ] token ) . Next , we   run another forward pass using RoBERTa to ob-   tain a representation of the labels based on their   text ( e.g. , ‘ poisonous ’ or ‘ edible ’ for our example   in Figure 1 ) . Finally , we compute the probability   distribution over labels by doing a dot - product of   the representations of the input and the labels . We   train this model using cross - entropy loss . In our ex-   periments , we refer to this model as RoBERTa w/o   Exp since the model does not use any explanations .   We also experiment with a RoBERTa w/ Exp .   model where a RoBERTa model takes as input a   concatenated sequence of all the explanations for   the task along with FaT. The rest of the training   setup remains the same as RoBERTa w/o Exp .   We ﬁnd that a simple concatenation of explana-   tions is not helpful for zero - shot generalization to   novel tasks ( results in Figure 6 ) . Next , we describeExEnt which explicitly models the role of each   explanation in predicting the label for an example .   5.3 ExEnt   To model the inﬂuence of an explanation towards   deciding a class label , we draw analogies with the   entailment of an explanation towards the structured   input . Here , given a structured input ( premise )   and an explanation ( hypothesis ) , we need to de-   cide whether the explanation strengthens the belief   about a speciﬁc label ( entailment ) , weakens belief   about a speciﬁc label ( contradiction ) or provides   no information about a label ( neutral ) .   Figure 5 shows the overview of our explanation-   guided classiﬁcation model , ExEnt ; given a struc-   tured input and explanation of a task , let ldenote   the label mentioned in the explanation , and Lde-   note the set of labels of the task . The entailment   model assigns logits p , pandpto the hypothesis   being entailed , contradicted or neutral respectively   w.r.t . the premise . Based on the label assignment   referred to by an explanation , we assign logits to   class labels as follows :   •If explanation mentions to assign a label : As-   signptol , pis divided equally among labels   inLnflg , andpis divided equally among   labels inL.   •If explanation mentions to not assign a label :   This occurs if a negation is associated with l.   Assignptol , pis divided equally among   labels inLnflg , andpis divided equally   among labels in L.   We obtain logit scores over labels of the task corre-   sponding to each explanation as described above .   We compute the ﬁnal label logits by aggregating   ( using mean ) over the label logits corresponding to   each explanation of the task . The ﬁnal label logits   are converted to a probability distribution over la-   bels , and we train ExEnt using cross - entropy loss .   In experiments , we consider a pre - trained   RoBERTa model ﬁne - tuned on MNLI ( Williams   et al . , 2017 ) corpus as our base entailment model .   Further , in order to perform the assignment of   logits using an explanation , we maintain meta-   information for each explanation to ( 1 ) determine   if the explanation mentions to ‘ assign ’ a label or   ‘ not assign ’ a label , and ( 2 ) track l(label men-   tioned in explanation ) . For CLUES - Synthetic ,   we parse the templated explanations to obtain the6529   meta - information , while for the explanations in   CLUES - Real , the authors manually annotate this   meta - information . Additional training details and   hyperparameters are provided in Appendix E.   5.4 Zero - Shot Generalization Performance   We evaluate ExEnt and the baselines on zero - shot   generalization to novel tasks in our benchmark as   described in § 5.1 . We train separate models for   CLUES - Real andCLUES - Synthetic . Figure 6   shows the generalization performance of all mod-   els . On CLUES , we ﬁnd that ExEnt outperforms   the baselines suggesting that performing entail-   ment as an intermediate step helps aggregate in-   formation from multiple explanations better . On   CLUES - Real , ExEnt gets an 18 % relative improve-   ment over the baselines while having an 11 % rela-   tive improvement on CLUES - Synthetic   To evaluate the utility of our synthetic tasks in en-   abling transfer learning to real - world tasks , we ﬁne-   tune a ExEnt model pre - trained on synthetic tasks .   We experiment with three pre - training task sets -   CLUES - Synthetic , CLUES - Synthetic ( 3x ) and   CLUES - Synthetic ( 5x ) consisting of 144 , 432,and 720 tasks . These larger synthetic task sets   are created by sampling tasks from each of the   48 different synthetic tasks types similar to how   CLUES - Synthetic was created ( see § 3.2 for refer-   ence ) . We ﬁnd that pre - training on synthetic tasks   boosts the performance of ExEnt on the novel tasks   ofCLUES - Real by up to 39 % ( relative ) over the   RoBERTa w/o Exp . model .   Human Performance To situate the perfor-   mance of the automated models , we performed   human evaluation for tasks in test split of   CLUES - Real using AMT . For this , we sampled at   most 50 examplesfrom the test split of tasks in   CLUES - Real and each example was ‘ labeled ’ by 2   turkers using the explanations of the ‘ best teacher ’   ( the teacher whose students got the best perfor-   mance during ‘ explanation veriﬁcation ’ stage ; see   § 3.1.2 for reference ) . The average human accuracy   for this was about 70 % . However , the performance   numbers of humans and models are not directly   comparable as the model looks at all the explana-   tions for the task , whereas the humans observe a   small number of explanations . Humans also see   multiple examples of the task during the evaluation ,   which they can use to ﬁne - tune their understanding   of a concept . The automated models do n’t have a   mechanism to leverage such data .   6 Key Challenges   To identify key challenges in learning from expla-   nations , we perform experiments ablating the lin-   guistic components and structure of explanations .   For a robust analysis , we generate more tasks for   each task type in CLUES - Synthetic , making 100   tasks for each of the 48 different task - types in   CLUES - Synthetic ( axes of variation include 4   negation types , 3 conjunction / disjunction types , 26530   quantiﬁer types , and number of labels ; details in   Appendix A.5 ) .   We evaluate the generalization performance of   ExEnt to novel tasks on each of the different types   separately by training separate models for each   task type . Figure 7 shows the relative gain in gen-   eralization performance of models learned using   explanations compared to the performance of base-   line RoBERTa w/o Exp . Our results indicate that   learning from explanations containing quantiﬁers is   highly challenging . In the presence of quantiﬁers ,   models guided by explanations perform on par with   the baseline RoBERTa w/o Exp model . Negations   also pose a challenge , as indicated by the decline   in relative gains of models guided by explanation   compared to the RoBERTa w/o Exp model . Struc-   turally complex explanations ( containing conjunc-   tions / disjunctions of clauses ) are also hard to learn   from compared to simple conditional statements .   These challenges provide a fertile ground for future   research and improvements .   7 Conclusion   We have introduced CLUES , a benchmark with di-   verse classiﬁcation tasks over structured data along   with natural language explanations to learn them .   CLUES is agnostic in the domain of tasks allowing   the research community to contribute more tasks in   the future . We also present ExEnt , an entailment-   based model to learn classiﬁers guided by explana-   tions . Our results are promising and indicate that   explicitly modeling the role of each explanation   through entailment can enable learning classiﬁers   for new tasks from explanations alone . Future work   can explore the open challenges in learning from   explanations , such as modeling the inﬂuence of   quantiﬁers and negations present in an explanation .   Our empirical analyses here aggregates explana - tions for a task from multiple teachers . Future work   can explore learning from explanations from indi-   vidual teachers , as well as cross - teacher variance .   Alternatively , rather than treat explanations from   different teachers homogeneously , future work can   model trustworthiness of a crowd of teachers from   their provided explanations .   Ethics and Broader Impact   All tables in CLUES - Real were collected from free   public resources ( with required attributions ) and ta-   bles in CLUES - Synthetic were created by us pro-   grammatically . We do not collect any personal in-   formation from the turkers who participated in our   crowdsourced tasks . The dataset has been released   without mentioning any personal details of turk-   ers available automatically in AMT ( such as turker   IDs ) . The turkers were compensated fairly and the   payment per task is equivalent to an hourly compen-   sation that is greater than minimum wage ( based   on the median time taken by turkers ) . We provide   details of the reward structure for the crowdsourc-   ing tasks in Appendix D. For the Wikipedia mining   task in this work , we limited the locale of eligible   turkers to US , UK , New Zealand and Australia . For   other crowdsourcing tasks , we limited the locale of   eligible turkers to US . Further , to ensure good - faith   turkers , we required that the approval rate of the   turkers be above 98 % . Our screening process has   selection biases that likely over - samples turkers   from demographics that are over - represented on   AMT ( ethnically white , college - educated , lower-   to - medium income and young ) ( Hitlin , 2016 ) , and   this is likely to affect the type of language usage in   the collected explanations .   The broader impact of this research in the longer   term could make developing predictive technolo-   gies more accessible to ordinary users , rather than   data - scientists and experts alone.6531References65326533   Appendix   A Additional details on creating   CLUES - Synthetic   In this section we discuss in detail about the various   table schemas followed by the details of quantiﬁers   and label assignment for creating synthetic tasks .   A.1 Tables schemas   We deﬁne ﬁve different table schemas , each cor-   responding to a different domain . For all the at-   tributes in a schema we deﬁne a ﬁxed domain from   which values for that attribute can be sampled .   •Species of bird : The classiﬁcation task here is   to classify a bird into a particular species based   on various attributes ( column names in table ) .   We deﬁne several artiﬁcial species of birds us-   ing commonly used nonce words in psycholog-   ical studies ( Chopra et al . , 2019 ) such as “ dax " ,   “ wug " , etc .   •Species of animal : The classiﬁcation task here   is to classify an animal into a particular species   based on various attributes ( column names in   table ) . Artiﬁcial species of animals are againdeﬁned using commonly used nonce words in   psychological studies such as “ dax " , “ wug " , etc .   •Rainfall prediction : This is a binary classiﬁca-   tion task where the objective is to predict whether   it will rain tomorrow based on attributes such as   “ location " , “ minimum temperature " , “ humidity " ,   “ atmospheric pressure " etc .   •Rank in league : This is a multi - label classiﬁca-   tion task where given attributes such “ win per-   centage " , “ power rating " , “ ﬁeld goal rating " of   a basketball club , the objective is to predict its   position in the league out of 1 , 2 , 3 , 4 , " Not qual-   iﬁed " .   •Bond relevance : This is a multi - label classiﬁca-   tion task where given attributes such “ user age " ,   “ user knowledge " , “ user income " , the objective is   to predict the relevance of a bond out of 5 classes   ( 1 to 5 ) .   In each of the above schemas , the attributes can be   either of types categorical or numeral . For each of   the above schemas we also deﬁne range of admis-   sible values for each attribute . Detailed description   of schemas are provided in Tables 8 , 9 , 10 11 , 12 .   A.2 List of quantiﬁers   The full list of quantiﬁers along with their associ-   ated probability values are shown in Table 5 .   A.3 Creating synthetic explanations   We use a template - based approach to convert the   set to rules into language explanations . We convert   every operator in the clauses into their correspond-   ing language format as :   •==!‘equal to ’   •>!‘greater than ’   •>=!‘greater than or equal to ’   •<!‘lesser than ’   •<=!‘lesser than or equal to ’   • ! = ! " not equal to’6534   •!>!‘not greater than ’   •!<!‘not lesser than ’   For example if we have a rule ‘ IF number of   hands = = 2 THEN foo ’ , we convert it into a   language explanation as ‘ If number of hands equal   to 2 , then foo ’ . In the presence of quantiﬁers , we   add ‘ it is [ INSERT QUANTIFIER ] ’ before the   label . For example if the rule was associated with   a quantiﬁer ‘ usually ’ , the language explanation   would be ‘ If number of hands equal to 2 , then it is   usually foo ’ .   A.4 Label Assignment using Rules   In Algorithm 1 , we detail the procedure for ob-   taining label assignments for our synthetic tasks .   Given that our rules are in an “ IF ... THEN .. "   format , we split each rule into an antecedent and   a consequent based on the position of THEN . Note   that our voting - based approach to choose the ﬁnal   label for an example helps to tackle ( 1 ) negation on   a label for multiclass tasks and ( 2 ) choose the most   suited label in case antecedents from multiple rules   are satisﬁed by an example .   A.5 Different synthetic task types   We create our synthetic tasks by varying along the   following axes :   • Number of labels : L={‘binary ’ , ‘ multiclass ’ }   •Structure of explanation : C={‘simple ’ , ‘ con-   junction / disjunction ’ , ‘ nested ’ }   •Quantiﬁer presence : Q={‘not present ’ ,   ‘ present ’ }   •Negation : N={‘no negation ’ , ‘ negation only   in clause ’ , ‘ negation only on label ’ , ‘ negation in   clause or on label ’ }   The set of task types is deﬁned as LCQN ,   enumerating to 48 different types . Algorithm 1 Label Assignment   A.6 Large synthetic task collections for   ablation experiment   In section § 6 we describe an ablation experiment ,   for which we create collections of 100 tasks corre-   sponding to each synthetic task type . Here , the task   type of a collection denotes the maximum complex-   ity of explanations in that collection . For example ,   for the collection ‘ multiclass classiﬁcation with   nested clauses and negation only in clause ’ , all   the 100 tasks might not have negations or nested   clauses in their explanations . This collection might   contain explanations with no negations or non-   nested clauses . However , it will not contain ex-   planations that have nested clauses and negations   in both clause and label .   B Real - World Tasks from UCI , Kaggle   and Wikipedia   For our benchmark , we made use of 18 datasets   in UCI , 7 datasets in Kaggle , and 9 tables in   Wikipedia . In Table 7 , we list the keywords that6535we use to refer to these tasks along with the URLs   to the datasets / tables .   B.1 Feature Selection for Real - World   Datasets   During pilot studies for collection of explanations   forCLUES - Real , we identiﬁed that annotators   found it difﬁcult to provide explanations for clas-   siﬁcations tasks with more than 5 to 6 columns .   Appropriately , we reduced the number of columns   in most datasets of CLUES - Real ( apart from some   Wikipedia tables ) to 5 by choosing the top features   that had maximum mutual information with the la-   bels in the training dataset . The mutual information   between the features and the label was computed   using the scikit - learn package with a random state   of 624 .   C Additional Analysis on   Teacher - Student Performance   For the crowdsourced datasets , we show the num-   ber of explanations collected per task in Fig-   ure 11(a ) . The number of explanations is largely   around an average value of 11 explanations per   task .   Figure 11(b ) shows the relation between explana-   tion quality ( quantiﬁed by likert scores ) and rank of   the explanation . Rank denotes the order in which a   teacher provided that explanation during our crowd-   sourced explanation collection phase . We ﬁnd a   positive correlation between quality and rank of   explanation showing that teachers generally submit   most useful explanations ( as perceived by them ) to   teach a task . Finally , we do not observe any cor-   relation between explanation length and ratings as   indicated by Figure 11(c ) .   We also illustrate the differences between   teacher and student on our tasks in § 4 . Here we   present two additional plots showing the perfor-   mance of ( 1 ) best teacher vs their students for each   task ( Figure 9 ) and ( 2 ) worst teacher vs their stu-   dents for each task ( Figure 10 ) . We ﬁnd that even   though the best teachers often attain near - perfect   accuracies for the tasks , their students perform sig-   niﬁcantly worse than them in many tasks . The   explanations from the worst teachers did not help   students in getting signiﬁcantly better than random   performance for majority of the tasks , even though   the student did outperform the worst teacher .   D Reward Structure for Crowd - sourcing   Tasks   Our work involves multiple stages of crowdsourc-   ing to collect high - quality explanations for the clas-   siﬁcation tasks . We pick turkers in the US for expla-   nation collection and veriﬁcation tasks ( US , UK , NZ ,   and GB for Wikipedia mining Task ) with a 98 %   HIT approval rate and a minimum of 1000 HITs   approved . In Table 6 , we summarize the payment   structure provided to the turkers on the AMT plat-   form for each of the stages ( described in detail in   § 3 ) – ( 1 ) Wikipedia mining on tables scraped from   Wikipedia , ( 2 ) Explanation collection for tables   obtained from UCI , Kaggle and Wikipedia , and6536(a ) ( b ) ( c )   ( 3 ) Explanation validation for collected explana-   tions . For all the three crowdsourcing tasks , the   turkers were compensated fairly and the payment   per task is equivalent to an hourly compensation   that is greater than minimum wage ( based on the   median time taken by turkers ) .   E Training details   In this section we proved details about implemen-   tation of various models , hyperparameter details ,   and details about hardware and software used along   with an estimate of time taken to train the models .   Code and dataset for our paper will be made public   upon ﬁrst publication .   E.1 Details of seen and novel tasks for   CLUES - Real andCLUES - Synthetic   For CLUES - Real , we chose the tasks from   Wikipedia that have very examples to be part of   novel task set . Among the tasks from Kaggle and   UCI , we kept tasks with higher number of samples   as part of seen tasks ( training tasks ) . Seen tasks   ( 20 ) for CLUES - Real are :   •website - phishing   •internet - firewall   •mushroom   •dry - bean•wine   •caesarian - section   •occupancy - detection   •vertebral - column   •student - performance   •shill - bidding   •mammographic - mass   •teaching - assistant - evaluation   •somerville - happiness   •stroke - prediction   •job - change   •campus - placement   •engineering - placement   •water - potability   •color - luminance   •proteinogenic - acid   Novel tasks ( 16 ) for CLUES - Real are :   •banknote - authentication   •tic - tac - toe - endgame   •car - evaluation   •contraceptive - method - choice   •indian - liver - patient   •travel - insurance   •entrepreneur - competency   •award - nomination - result   •coin - face - value   •coin - metal   •driving - championship - points   •election - outcome   •hotel - rating   •manifold - orientability   •soccer - club - region   •soccer - league - type   We train on 70 % of the labeled examples of   the seen tasks and perform zero - shot generaliza-6537tion test over the 20 % examples of each task in   CLUES - Real . For the extremely small Wikipedia   tasks ( for which we do not crowdsource explana-   tions ) , we use all examples for zero - shot testing .   ForCLUES - Synthetic , we have 96 tasks as   seen ( training ) tasks and 48 as novel tasks . Task   inCLUES - Synthetic that belong to the following   schemas are part of the seen tasks :   • Species of Animal   • Species of Bird   • Rainfall prediction   Tasks belonging to ‘ Bond relevance classiﬁcation ’   and ‘ League Rank Classiﬁcation ’ were part of   novel tasks for CLUES - Synthetic . We train on   700 labeled examples of each seen task and perform   zero - shot generalization test over 200 examples of   each novel task in CLUES - Synthetic .   E.2 Model parameters   • RoBERTa w/o Exp . : The number of parameters   is same as the pretrained RoBERTa - base model   available on HuggingFace library .   • RoBERTa w/ Exp . : The number of parameters   is same as the pretrained RoBERTa - base model   available on HuggingFace library .   •ExEnt : The number of parameters is same as   the pre - trained RoBERTa mdoel ﬁnetuned on   MNLI ( Williams et al . , 2017 ) corpus . We obtain   the pretrained checkpoint from HuggingFace .   E.3 Hyper - parameter settings   For all the transformer based models we use the im-   plementation of HuggingFace library ( Wolf et al . ,   2020 ) . All the model based hyper - parameters are   thus kept default to the settings in the HuggingFace   library . We use the publicly available checkpoints   to initialise the pre - trained models . For RoBERTa   based baselines we use ‘ roberta - base ’ checkpoint   available on HuggingFace . For our intermediate en-   tailment model in ExEnt , we ﬁnetune a pretrained   checkpoint of RoBERTa trained on MNLI corpus   ( ‘ textattack / roberta - base - MNLI ’ )   When training on CLUES - Synthetic , we use a   maximum of 64 tokens for our baseline RoBERTa   w/o Exp . and ExEnt . For the RoBERTa w/ Exp .   model we increase this limit to 128 tokens as it   takes concatenation of all explanations for a task .   When training on CLUES - Real , we use 256 tokens   as limit for RoBERTa w/ Exp . using explanationsas the real - world tasks have roughly two times   more explanations on average than synthetic tasks .   We used the AdamW ( Loshchilov and Hutter ,   2019 ) optimizer commonly used to ﬁne - tune pre-   trained Masked Language Models ( MLM ) mod-   els . For ﬁne - tuning the pre - trained models on our   benchmark tasks , we experimented with learning   rates { 1e 5;2e 5 } and chose 1e 5based on   performance on the performance on the validation   set of seen tasks . Batch sizes was kept as 2 with   gradient accumulation factor of 8 . The random   seed for all experiments was 42 . We train all the   models for 20 epochs . Each epoch comprises of   100 batches , and in each batch the models look at   one of the tasks in the seen split .   E.4 Hardware and software speciﬁcations   All the models are coded using Pytorch 1.4.0   ( Paszke et al . , 2019 ) and related libraries like   numpy ( Harris et al . , 2020 ) , scipy ( Jones et al . ,   2001 – ) etc . We run all experiments on one of the   following two systems - ( 1 ) GeForce RTX 2080   GPU of size 12 GB , 256 GB RAM and 40 CPU   cores ( 2 ) Tesla V100 - SXM2 GPU of size 16 GB ,   250 GB RAM and 40 CPU cores .   E.5 Training times   •Training on CLUES - Real : The baseline   RoBERTa w/o Exp model typically takes 3 sec-   onds on average for training on 1 batch of ex-   amples . In 1 batch , the model goes through 16   examples from the tasks in seen split . RoBERTa   w/ Exp . takes around 5 seconds to train on 1   batch . ExEnt takes longer time than baselines   owing to the multiple forward passes . For train-   ing on 1 batch of CLUES - Real , ExEnt took 12   seconds on average .   •Training on CLUES - Synthetic : All the models   take comparatively much lesser time for train-   ing on our synthetic tasks owing to lesser num-   ber of explanations on average for a task . For   training on 1 batch , all models took 1 seconds   or less to train on 1 batch of examples from   CLUES - Synthetic .   F Annotation interfaces   We present the different annotation templates and   interfaces used for our explanation collection and   veriﬁcation stages in Figures 12,13,14,15 and Fig-   ure 16 respectively.653865396540654165426543654465456546