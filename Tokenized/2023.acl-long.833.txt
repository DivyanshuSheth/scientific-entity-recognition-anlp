14912149131491414915149161491714918149191492014921149221492314924149251492614927ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 6 .   /squareA2 . Did you discuss any potential risks of your work ?   The potential risks can be found in Section 6 .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   The main claims can be found in Abstract and Section 5 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We introduce the dataset and pre - trained models used and the baselines in Section 4 .   /squareB1 . Did you cite the creators of artifacts you used ?   We cite the creators of artifacts we used in Section 4.1 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We use the default license of all artifacts in Section 4 .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   We use all artifacts with their default intended use in Section 4 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Except for the dataset we created ourselves in Section 4 , the relevant statistics is not reported in any   other datasets we use . The dataset we use exactly follows the amount and train / test / dev splits of data   in the original dataset paper .   C / squareDid you run computational experiments ?   Our computational experiments for evaluating our method can be found in Section 4.14928 / squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   We report the number of parameters in the models used in a few experiments , which can be found   in Section 4 and Appendix . In addition , we report the total computation budget in Section C of the   Appendix .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   We talk about them in Section 4.1 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   We report descriptive statistics about our results , which can be found in Section 4 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   We report these important settings in Section 4 .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.14929