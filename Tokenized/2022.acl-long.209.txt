  Santiago Castro Ruoyao Wang Pingxuan Huang Ian Stewart   Oana Ignat Nan Liu Jonathan C. Stroud Rada Mihalcea   University of Michigan ‚Äì Ann Arbor , USA   sacastro@umich.edu   Figure 1 : Three examples from the FIBdataset , each including three video frames , the caption , the blanked   answers from the original caption together with the collected answers ( all answers normalized , see Section 3.2 ) .   Abstract   We propose Ô¨Åll - in - the - blanks as a video under-   standing evaluation framework and introduce   FIB ‚Äì a novel dataset consisting of 28,000   videos and descriptions in support of this evalu-   ation framework . The Ô¨Åll - in - the - blanks setting   tests a model ‚Äôs understanding of a video by   requiring it to predict a masked noun phrase   in the caption of the video , given the video   and the surrounding text . The FIBbench-   mark does not share the weaknesses of the cur-   rent state - of - the - art language - informed video   understanding tasks , namely : ( 1 ) video ques-   tion answering using multiple - choice questions ,   where models perform relatively well because   they exploit linguistic biases in the task for-   mulation , thus making our framework chal-   lenging for the current state - of - the - art systems   to solve ; and ( 2 ) video captioning , which re-   lies on an open - ended evaluation framework   that is often inaccurate because system answers   may be perceived as incorrect if they differ   in form from the ground truth . The FIB   dataset and our code are available at https :   //lit.eecs.umich.edu / fiber/ .   1 Introduction   Despite current progress on multimodal ( textual   and visual ) representations , language - informed   video understanding is still a very challenging task   for machine learning systems ( Zhang et al . , 2021 ;   Li et al . , 2021 ) . This is due in large part to thetask setup and the dataset construction . Current   video understanding datasets often have at least   one of two major limitations . First , they have   limited application value . E.g. , multiple - choice   questions ( Lei et al . , 2018 ; Tapaswi et al . , 2016 ;   Jang et al . , 2017 ; Castro et al . , 2020 ) do not reÔ¨Çect   real - world tasks . Second , they are based on sub-   jective evaluation metrics , e.g. , video captioning   ( Tran et al . , 2016 ; Krishna et al . , 2017 ; Zhou et al . ,   2018 ; Wang et al . , 2019 ) ) , and are therefore hard to   evaluate automatically , as the ground truth can be   expressed in different ways . In this paper , we ad-   dress these limitations by introducing a new dataset   named FIBthat collects multiple perspectives   on the same video , focusing on noun phrases as a   proxy for different entities and their interactions   in the video . Our data focuses on recall and tests   the ability of models to capture a wide range of   possible interpretations for a particular aspect of a   video .   We construct the FIBdataset by systemati-   cally blanking captions from an existing video cap-   tioning dataset named VaTeX ( Wang et al . , 2019 )   and by providing additional correct answers for   the blanks . VaTeX is a video captioning dataset   that contains 40,000 10 - second YouTube videos   with 10 English captions per video . We build our2925video Ô¨Åll - in - the - blanks dataset by blanking random   noun phrases from one of the English captions for   each video , from a subset of VaTeX consisting of   28,000 videos . Through extensive analyses , we   show that the blanked noun phrases are essential   for understanding important visual aspects from   the video .   To address the Ô¨Åll - in - the - blanks task , we pro-   pose a Transformer - based ( Vaswani et al . , 2017 )   multimodal model . Our experiments show that our   best multimodal model achieves a token - level F1   score of 71.4 while the F1 score of crowd workers   is 82.5 , indicating that this task is challenging for   video and text understanding .   The contribution of this work is threefold : ( 1 )   We propose a novel Ô¨Åll - in - the - blanks task as an   evaluation framework that addresses the drawbacks   associated with previous approaches to video un-   derstanding . In support of this framework , we in-   troduce FIB , which is a novel dataset of 28,000   videos and Ô¨Åll - in - the - blanks captions with multiple   correct answers . ( 2 ) We propose several unimodal   baselines and two multimodal models for solving   this task . ( 3 ) We provide a detailed analysis of the   data to measure the diversity and complexity of   the answers , and also conduct an error analysis of   the models ‚Äô performance , to gain insights into the   blanked captions and videos that are hard for the   models to solve .   2 Related Work   Language - informed video understanding is a com-   plex task that has been extensively addressed in the   multimodal ( natural language and computer vision )   machine learning research through diverse tasks   and benchmarks .   Multiple - Choice Video Understanding .   Multiple - choice benchmarks consist of iden-   tifying the only correct answer from a set of   distractors , where the set of possible answers   varies depending on the input . Video Question   Answering ( Video QA ) , a popular format , consists   of answering questions based on the video content .   Numerous multiple - choice Video Understand-   ing benchmarks have been proposed such as   TVQA ( Lei et al . , 2018 ) , MovieQA ( Tapaswi et al . ,   2016 ) , TGIF - QA ( Jang et al . , 2017 ) ( Repetition   Action and State Transition tasks ) , LifeQA ( Castro   et al . , 2020 ) , PororoQA ( Kim et al . , 2017 ) , Mari-   oQA ( Mun et al . , 2017 ) , VCQA ( Zhu et al . , 2017 ) ,   VideoMCC ( Tran et al . , 2016 ) , and ActivityNetQA ( Yu et al . , 2019 ) . However , they provide   choices and are thus easier to solve than generating   arbitrary text . A further drawback is that the   performance without the visual input is generally   already high as models are able to exploit biases in   the dataset ( Agrawal et al . , 2018 ) or they count on   other modalities that overlap in functionality with   the visual one .   Video Captioning . Video Captioning consists   of generating a piece of text that describes a   given video . This task can be carried out us-   ing multiple datasets such as ActivityNet Cap-   tions ( Krishna et al . , 2017 ) ( also features Dense-   Captioning ) , YFCC100 M ( Thomee et al . , 2016 ) ,   ( Alayrac et al . , 2016 ) , DiDeMo ( Anne Hen-   dricks et al . , 2017 ) , MSR - VTT ( Xu et al . , 2016 ) ,   YouCook2 ( Zhou et al . , 2018 ) , How2 ( Sanabria   et al . , 2018 ) , HowTo100 M ( Miech et al . , 2019 ) ,   VaTeX ( Wang et al . , 2019 ) , TGIF ( Li et al . , 2016 ) ,   MovieNet ( Huang et al . , 2020 ) , LSMDC ( Rohrbach   et al . , 2017 ) , TGIF - QA ( Li et al . , 2016 ) ( Frame QA   task ) . Due to the diversity of captions provided ,   Video Captioning benchmarks do not present a high   human agreement and are thus hard to evaluate au-   tomatically with certainty ( Aafaq et al . , 2019 ) .   Video Understanding Based on Filling Blanks .   VideoBERT ( Sun et al . , 2019b ) , CBT ( Sun et al . ,   2019a ) , UniVL ( Luo et al . , 2020 ) , ActBERT ( Zhu   and Yang , 2020 ) , and HERO ( Li et al . , 2020 ) meth-   ods propose masking random parts of the input   from text and video pairs for training . However ,   they do this only for the purpose of system train-   ing and do not use the framework to test and eval-   uate video understanding . The only exception   is MovieFIB ( Maharaj et al . , 2017 ) which em-   ploys a video Ô¨Åll - in - the - blanks scheme , based on   LSMDC ( Rohrbach et al . , 2017 ) for both training   and evaluation . However , these methods have sev-   eral drawbacks . They blank a single word , which   makes it easier to guess ; they evaluate correctness   with a single ground - truth answer per caption ; and   they focus on the movies domain ( we focus on   YouTube videos ) .   Concurrent Work . The most similar work to   ours is VidQAP ( Sadhu et al . , 2021 ) , which   presents an evaluation framework to Ô¨Åll in blanks   with phrases using semantic roles based on Activ-   ityNet Captions ( Krishna et al . , 2017 ) and Cha-   rades ( Sigurdsson et al . , 2016 ) ; unlike this existing   work , we design our benchmark to feature a high2926human accuracy ( avoiding ActivityNet Captions   as it is contextualized , collecting multiple correct   answers , and showing a high human performance ) .   Our work is also close to ( Yang et al . , 2021 ) on   evaluating the use of free - form QA ; however , they   employ a small vocabulary and no human accuracy   that serves as an upper bound for the task .   The novelty of our work lies in our use of a hard   task ( a considerable gap between human and best   model performance ) that measures a form of video   understanding while at the same time yielding a   high human performance due to the large number   of possible correct answers we collected ( 13 per   caption ) from multiple annotators ( 9 per caption ) .   3 Video Fill - in - the - Blanks Dataset   We construct FIB ‚Äì a large video understanding   dataset that can evaluate the ability of a model to   interpret and use a multimodal context by requiring   the models to ‚Äú Ô¨Åll in ‚Äù ( generate ) a ‚Äú blank ‚Äù ( a miss-   ing constituent ) in this context . We build FIB   by following two main steps : ( 1 ) data generation ,   where we compile a large set of video - caption pairs   with selectively blanked words ; and ( 2 ) data an-   notation , where crowd workers provide additional   valid answers for these blanks .   Note that we could also develop a Ô¨Åll - in - the-   blanks dataset by completing only the Ô¨Årst step :   the data generation . However , this would result in   only one valid answer ( the original blanked word   or phrase ) , which can lead to unfair evaluations   that are too strict because of alternative correct   answers being dismissed ( e.g. , ‚Äú child ‚Äù provided   as an answer where the blanked word was ‚Äú kid ‚Äù ) .   Other than manual annotations , we found no high-   quality method to automatically obtain additional   correct answers . For example , ‚Äú building ‚Äù and ‚Äú t-   shirt ‚Äù in Table 7 are too dissimilar but both are   correct , ‚Äú pink ‚Äù and ‚Äú yellow ‚Äù in Fig . 1 are semanti-   cally close but only one is correct .   3.1 Data Generation   The dataset is constructed starting with the Va-   TeX ( Wang et al . , 2019 ) dataset . VaTeX is a multi-   lingual video captioning dataset , consisting of over   41,250 video clips , each of which is taken from a   unique public YouTube video , and lasts around 10   seconds . For each video clip , there are 10 English   and 10 Chinese captions associated with it .   We produce blanked captions by blanking noun   phrases in the English captions in VaTeX. We choseto mask only noun phrases for three main reasons .   First , noun phrases often require visual information   for identiÔ¨Åcation or understanding . They cover a   large variety of information regarding visual con-   tent , as their head nouns can describe people , ob-   jects , scenes , events , and more . A model often   needs to identify the related objects in the videos ,   as well as the properties of objects ( e.g. , color , num-   ber , or size ) to Ô¨Åll the blank correctly .   Second , nouns are usually essential to under-   standing of visual content and serve as reliable   predictors of the ability of a system to understand   a video . Other phrases , such as verbs or adjectives ,   can more easily be guessed from the text only while   ignoring the visual information . To illustrate , con-   sider the example ‚Äú A woman _ _ _ _ _ in the pool , ‚Äù   where a model can easily predict that the blank   should be ‚Äú swims ‚Äù from the textual content only ,   which would not be the case for ‚Äú A woman swims   in _ _ _ _ _ ‚Äù , where the blank could be completed by   sea , pool , lake , water , and other similar nouns .   Third , in preliminary experiments , we found   that nouns lead to more robust annotations as com-   pared to e.g. , adjectives , which can have low inter-   annotator agreement due to their subjectivity . As an   example , consider the phrase ‚Äú A _ _ _ _ _ hill stands   behind the house . ‚Äù where the blank could be Ô¨Ålled   with a color property , a size property , or another   attribute .   For each video , we choose the Ô¨Årst English cap-   tion that contains at least one noun phrase as de-   tected by spaCy(Honnibal et al . , 2020 ) , and ran-   domly blank one of these noun phrases to generate   an instance . Accordingly , we generate our training ,   validation , and test data starting with the VaTeX   v1.1 training set , a random subset of size 1,000   from the validation set , and a random subset of size   1,000 from the test set , respectively .   3.2 Data Annotation   We performed a crowdsourced annotation proce-   dure to collect additional correct answers for each   blank in the validation and test sets . As highlighted   earlier , the main reason for collecting these addi-   tional annotations is to reÔ¨Çect the natural diversity   of language , and have multiple alternative answers   for each blank .   We use Amazon Mechanical Turk ( AMT ) for the   annotation . Figure 2 shows the annotation interface2927   and a highlight of the data collection instructions   ( additional guidelines were provided , not shown   here for space reasons ) . For each blanked cap-   tion , workers were presented a video clip along   with the corresponding masked caption . They were   then asked to Ô¨Åll in the blank with a noun phrase .   We also asked annotators to provide answers in   a conÔ¨Ådence - descending order ( the Ô¨Årst answer   should be the most natural one to the annotator ) .   We presented Ô¨Åve videos in each Human Intel-   ligence Task ( HIT ) . Nine workers annotated each   of them with at least two answers for each blank .   We paid a bonus for each extra answer for each   blanked caption , from the second one to the Ô¨Åfth   one , to encourage them to provide more answers .   We calculated a $ 12 hourly rate for a worker that   provides at least Ô¨Åve answers . We estimated the   time to annotate one video to be 30 seconds . Con-   sequently , the HIT pay rate was $ 0.2 , which could   result in a total of $ 0.5 with the added bonus . Addi-   tionally , we offered another type of bonus of $ 0.2   to the worker with the largest number of correct an-   swers for every HIT , to encourage them to provide   more than Ô¨Åve answers .   We required workers to be in Canada or the   United States , and to have completed at least 1,000   HITs on AMT with at least a 92 % approval rate .   The interface also checked that for a given worker   and caption the answers were different . For this , we   Ô¨Årst normalized the answers by lower - casing , strip-   ping punctuation and extra spaces , and removing   the determiners ‚Äú the ‚Äù , ‚Äú a ‚Äù , and ‚Äú an . ‚Äù   During the annotation , we manually reviewed a   sample to identify cases of incorrectly tagged noun   phrases ( e.g. , ‚Äú inside ‚Äù marked as a noun when it   should be a preposition ) and factually incorrect   noun phrases ( e.g. , referring to bags as ‚Äú eggs ‚Äù with-   out any information on the contents of the bags ) ;   we disqualiÔ¨Åed workers who consistently provided   incorrect annotations . After collecting annotations ,   we Ô¨Åltered for noun phrases using the same method   as before , based on whether the text is parsed as   a noun phrase ( including bare nouns , e.g. ‚Äú man is   walking ‚Äù ) , a wh - phrase ( ‚Äú who is speaking ‚Äù ) , a sim-   ple gerund ( ‚Äú eating is a good way to stay healthy ‚Äù ) ,   or inÔ¨Ånitive ( ‚Äú to eat is wonderful ‚Äù ) .   We compute summary statistics on the annotated   data to determine the degree of similarity with the   originally blanked phrases . The statistics are shown   in Table 1 . We Ô¨Ånd that , in general , annotators tend   to provide 3 unique answers for the provided data .   Compared to the original phrases , annotators tend   to use about the same number of tokens . Anno-   tators also use visual words at a much lower rate   than the original phrases , possibly because the task   encouraged the annotators to generate as many dis-   tinct nouns as possible without regard to descriptive   information .   3.3 Data Analysis   To further validate the utility of the annotations   collected in this study , we provide an extensive2928analysis of the answers ( which is obtained from the   union of the annotations and the originally blanked   phrases ) .   We compute the most - frequent answers and Ô¨Ånd ,   as expected , that noun phrases related to ‚Äú person ‚Äù   are the most frequent : the word ‚Äú man ‚Äù appears in   5.7 % of total original phrases and 1.2 % of total an-   notations ( see Figure 5 in the Appendix ) . Note that   our annotations have a long tail distribution , as the   most - frequent noun phrase appears in only 1.2 % of   total annotations . In addition , we Ô¨Ånd that answers   related to ‚Äú person ‚Äù , such as ‚Äú another person ‚Äù are   not trivial . On the contrary , in the third example in   Fig . 1 , for example , a model has to reason about   the actions of both persons and distinguish between   them . The other two examples in Fig . 1 also reÔ¨Çect   how a model needs to understand both the video   and the text in order to complete the blanks .   Figure 3 shows what kind of answers are de-   picted in the videos . This analysis shows the diver-   sity and complexity of answers that a model needs   to Ô¨Åll in , demonstrating a strong video understand-   ing . As expected , the cluster Person - related has   the most answers , followed by the clusters : Ob-   jects ( e.g. , shoes , glasses ) , Places ( e.g. , mountain ,   street ) , Materials ( e.g. , metal , wood ) , and Body   parts ( e.g. , Ô¨Ångers , head ) . Note also that the Person-   related cluster , among more typical answers such   as ‚Äú male ‚Äù and ‚Äú female ‚Äù , also contains complex   and diverse answers such as ‚Äú dancer ‚Äù , ‚Äú workers ‚Äù ,   ‚Äú musician ‚Äù or ‚Äú audience ‚Äù .   3.4 Human Agreement   To establish a reference for the machine models , we   compute the agreement among annotators using the   evaluation metrics described in Section 5.1 , which   we also use for model evaluation ( Section 5.2 ) .   SpeciÔ¨Åcally , we apply a leave - one - out strategy   to construct the ‚Äú test set ‚Äù and the ‚Äú ground truth   set . ‚Äù We compare the Ô¨Årst answer provided by   each crowd worker ( which is their most natu-   ral / conÔ¨Ådent answer ) against the complete set of   answers provided by the other crowd workers , us-   ing maximum F1 score ( token overlap ) and maxi-   mum exact match ( EM ) as agreement metrics , as   described in Section 5.1 .   Table 2 shows the inter - annotator agreement . We   show the mean values of the agreement metrics per-   caption and per - answer ( recall there are multiple   answers per caption , so in the former case we Ô¨Årst   average among the answers within the caption and   then across the captions ) . The higher rates of agree-   ment at the caption level , compared to the answer   level , indicate a high amount of answer diversity   among the workers .   To validate the quality of the crowdsourced an-   notations , we also compare them against human   annotations collected from two trusted annotators   ( both researchers at the University of Michigan ) .   We sample 200 captions from the validation set   and ask these two annotators to perform the same   labeling task that the MTurk workers performed ,   and then compare their agreement with the crowd-   sourced data . The annotators obtain a per - caption   average of 90.2 % F1 score and 49.0 % exact match   accuracy , comparable to the agreement scores of   the workers.29293.5 Limitations   We identify several limitations of our benchmark ,   which can be the objective of future work .   NPs vs. other phrases . By looking at a video   and Ô¨Ålling a blank caption with a noun phrase can   sometimes indirectly capture other aspects such as   actions ( verbs , adverbs ) and object quality ( adjec-   tives , modiÔ¨Åers ) . However , this is not always the   case . This is especially true for noun phrases that   are easier to guess ( cf . Table 4 ) .   Focus on human actions . Our data focuses   mostly on human - related activities ( e.g. , sports ) ,   and may lack general representation available in   other datasets related to animals , nature , and tech-   nology , to name a few .   Availability of the videos . As we build upon Va-   TeX ( Wang et al . , 2019 ) and YouTube , some videos   may become unavailable over time . To mitigate   this issue , the VaTeX website offers to download   pre - extracted video features .   EfÔ¨Åciency of the data annotation process . Not   all videos have multiple possible captions for noun   phrases . For example , ‚Äú the fork ‚Äù may be the only   reasonable answer for a given video and blanked   caption , and annotators may not have anything else   to add .   4 Multimodal Method for Video   Fill - in - the - Blanks   We propose an encoder - decoder multimodal   method to perform the task of video Ô¨Åll - in - the-   blanks . We Ô¨Årst encode the text and visual modal-   ities together to obtain a semantic representation   of the blanked caption and video . The decoder   uses the semantic representation to generate text   corresponding only to the answer to the blank . To   correctly generate an answer , a model needs to   learn which parts of videos relate to the missing   parts of the caption . To accomplish this , we use the   original Transformer architecture ( Vaswani et al . ,   2017 ) , whose self - attention mechanism is partic-   ularly effective for encoding relations within an   input sequence and have been shown to perform   well in many language understanding tasks .   We consider two types of encoders , namely   the early - fusion encoder and the late - fusion ( two-   stream ) encoder . The structure of our multimodal   model with an early - fusion encoder is shown in   Fig . 4a . The input to the model consists of the tok-   enized blanked caption - text t ; : : : ; t , as well as a   representation of the video consisting of multiple   video sequence features v ; : : : ; vfrom a video   feature extractor . The blanked captions are embed-   ded by an embedding layer . The video features are   projected into the encoder by a linear layer . We   use a special token to represent the masked phrase   and another one to separate the input text and video   sequences . We add positional embeddings to each   input token or video feature to represent the se-   quence order , and another embedding to indicate   whether it belongs to the text or video sequence   similarly to BERT ( Devlin et al . , 2019 ) .   The late - fusion model is shown in Fig . 4b . The   late - fusion model encodes the language and video   Ô¨Årst separately and then jointly . This is because   the modalities may beneÔ¨Åt from learning indepen-   dently about their own context before using them   together.29304.1 Implementation Details   For the video encoder , we use the existing I3D ( Car-   reira and Zisserman , 2017 ) features ( size 1024   every 8 consecutive frames ) provided by the Va-   TeX dataset ( Wang et al . , 2019 ) , in which videos   were sampled at 25 fps . We initialize our multi-   modal model using T5 ( Raffel et al . , 2020 ) , given   its ability to Ô¨Åll in variable - length blanks . T5 is   an encoder - decoder Transformer ( Vaswani et al . ,   2017 ) model that is a good starting point as it   provides state - of - the - art performance on text - only   tasks and it was pretrained to Ô¨Åll arbitrary - length   text spans that were previously masked . Building   upon T5 allows our model to not only leverage   the pre - trained large - scale language models that   already have strong language abilities but also to   fuse it with visual inputs . We initialize the early-   fusion model with pretrained T5 - base weights .   For the late - fusion model , we use T5 - base for   the text encoder and for the decoder . We use two   one - layer transformers to encode videos and fuse   text and video features , and the weights of these   two transformers are randomly initialized . Follow-   ing T5 model implementation , the special token   < extra_id_0 > is used to represent the blanked   phrase , and < \s > is used to separate the text and   video sequences . The generated output follows T5   output format : the special token < extra_id_0 >   followed by the predicted text for the blanked   phrase . See Appendix B.1 for more details .   4.2 Baselines   We compare our model to the following baselines .   Most Frequent Answer . The baseline makes use   of the most frequent answer in the training set ( ‚Äú a   man ‚Äù ) as the answer to all the blanked captions   during evaluation .   Text - based Transformer . Previous visual ques-   tion answering datasets found that a text - only   model can nearly match the performance of the   multimodal system ( Antol et al . , 2015 ) . We ana-   lyze the degree to which language alone can con-   tribute to our video understanding framework by   conducting experiments based on text - only mod-   els . We use the off - the - shelf T5 - base transformer   model ( Raffel et al . , 2020 ) as our baseline model .   We use both a zero - shot model ( not trained on our   data ) and a Ô¨Åne - tuned model . For the latter , we   use the base model v1.1 because it performed bet-   ter in our experiments on the validation set . Thedecoding hyperparameters are the same as in the   multimodal models , except the beam size is 8 for   both the zero - shot one and 2 for the Ô¨Åne - tuned vari-   ant as we obtained the best validation results for   each one using these beam sizes .   Single video feature . We consider using a sin-   gle I3D feature per video to determine how well   the model does with a small portion of the video .   Based on a study of 50 randomly sampled videos ,   the blanked entity in the caption appeared 95 %   of the time in the third second of the video ( see   Fig . 11 in the Appendix ) . For this method , we pick   the I3D feature which corresponds roughly to it   and apply it to the proposed multimodal methods   instead of using all the video features . Note I3D   takes a window of 16 frames as input , which in our   case corresponds to 640 milliseconds , centered at   the mentioned moment within the video . This can   be seen as a small generalization of the Image Un-   derstanding task , which considers a single image   ( frame ) .   5 Experiments and Results   We perform experiments and evaluations using the   dataset described in Section 3 .   5.1 Evaluation Metrics   We use exact match accuracy and ROUGE-1 F1   score ( token - level ) ( Lin , 2004 ) to evaluate the out-   put of the generation models and to evaluate human   agreement ( Section 3.4 ) . For the exact match , we   count a generated text string as correct if it has at   least one string - level match among the provided   annotations . For the token - level F1 , we compute   the token overlap ( true positives ) between the gen-   erated text string and each annotation , normalized   by the sum of the true positives and average of   the false negatives / positives . We then compute   the maximum across all annotations . For all eval-   uations , we computed the metrics based on the   normalized text ( i.e. , without articles ) .   5.2 Results   We evaluate the visual understanding ability of our   multimodal model by comparing its performance   with the text - only baseline and the human perfor-   mance . The results from the Ô¨Åll - in - the - blanks task   are shown in Table 3 . The accuracy of the text-   only model and F1 score are low , indicating that   the language bias is controlled in our dataset . The2931   multimodal model outperforms the text - only base-   lines in both exact match accuracy and F1 score ,   meaning that our multimodal model is able to learn   video features relevant to caption language during   training . We also note that the early - fusion multi-   modal model ( T5 + I3D ) slightly outperforms the   late - fusion multimodal model , which suggests that   the model learns more effectively without extra en-   coders ( see Fig . 4b ) . Both the early - fusion and the   late - fusion multimodal models perform worse with   a single I3D feature . This suggests that the model   beneÔ¨Åts from the whole video to correctly answer   the caption .   We also Ô¨Ånd a large performance gap between   the multimodal model performance and the human   performance . Therefore , plenty of space exists for   improvements to achieve human performance , and   the video Ô¨Åll - in - the - blanks task is worth investigat-   ing in future visual understanding research .   5.3 Error Analysis   Results per Semantic Label . To measure how   well the model understands different patterns in the   caption data , we compare the predictions generated   for blanks corresponding to words of different se-   mantic categories ( the rest of the answers generally   belong to the same category as the blanked words ) .   Two of the authors annotated the originally blanked   phrases for common non - overlapping semantic cat-   egories , including people , passive entities , and lo-   cations .   We list the categories and their distribution / size   in Table 4 , and we also show the performance   for the best text - only zero - shot method ( T5 zero-   shot ) , text - only Ô¨Åne - tuned method ( T5 Ô¨Åne - tuned ) ,   and multimodal method ( T5 + I3D ) . The results   of T5 zero - shot show some categories can be eas-   ily predicted , without Ô¨Åne - tuning on the dataset ,   namely Preposition , Pronoun , and Event . How-   ever , Ô¨Åne - tuning T5 on our dataset yields improve-   ments for nearly all categories . The multimodal   ( T5 + I3D ) model improves the categories of Per-   sonandAbstract nouns but performs worse for   others , namely Audio andAction . This Ô¨Ånding fol-   lows from the fact that understanding higher - order   audio and visual concepts requires complex reason-   ing , for which the video - aware model may need   more training . In general , Action andPassive entity   will likely require extra attention in future work ,   considering the comparatively low performance for   these categories .   Best Model vs. Human Performance . To gain   insights on how to improve our models for future   work , we measure where our best model ( T5 +2932I3D ) fails and humans perform well . We Ô¨Ånd three   main types of wrong predictions . The most com-   mon error is predicting ‚Äú man ‚Äù instead of ‚Äú women ‚Äù ,   followed by predicting ‚Äú person ‚Äù instead of ‚Äú child ‚Äù   or ‚Äú baby ‚Äù . The majority of the remaining errors   are predictions close to the ground truth answers   such as ‚Äú dance ‚Äù instead of ‚Äú exercise ‚Äù , ‚Äú pillow ‚Äù in-   stead of ‚Äú sheets ‚Äù , ‚Äú rug ‚Äù instead of ‚Äú sand ‚Äù , ‚Äú Ô¨Çoor ‚Äù   instead of ‚Äú court ‚Äù , ‚Äú knife ‚Äù instead of ‚Äú spatula ‚Äù or   ‚Äú basketball game ‚Äù instead of ‚Äú wrestling ‚Äù .   Based on these types of errors , in future work ,   the model would beneÔ¨Åt from pre - training on un-   biased data ( both gender and age ) and also from   pre - training on a large - scale multimodal ( language   and video ) dataset , to learn about more diverse   situations and objects .   6 Conclusions   This paper introduced the Ô¨Åll - in - the - blanks eval-   uation framework for video understanding . The   framework addresses drawbacks of alternative   video understanding tasks , such as multiple - choice   visual question answering or video captioning .   Our paper makes three important contributions .   First , we introduced FIB , which is a large   dataset consisting of 28,000 videos and tests based   on Ô¨Ålling blanks , building upon an existing video   captioning dataset with a new set of manual an-   notations , and using a modiÔ¨Åed annotation frame-   work to encourage diverse responses among an-   notators . This process can be easily replicated to   create new Ô¨Åll - in - the - blanks data for other datasets   and tasks . Second , we conducted extensive anal-   yses on the dataset to evaluate the quality of the   annotations and to understand the patterns and lim-   itations of the data . Finally , we introduced a mul-   timodal model that fuses language and visual in-   formation and found that the video - aware models   signiÔ¨Åcantly outperform the text - only models . No-   tably , we found a consistent gap between model   performance and human performance , which sug-   gests room for improvement in future models ad-   dressing video understanding through the lens of   the Ô¨Åll - in - the - blanks task .   The FIBdataset and our code are avail-   able at https://lit.eecs.umich.edu/   fiber/ .7 Ethical Considerations and Broader   Impact   Even though we compensated the annotators based   on the quality of the answers they produced ( and   stated so in the instructions ) , they were rewarded   based on the number of answers they input since we   looked for diversity . These incentives may have en-   couraged the annotators to make many judgments   quickly and therefore make biased decisions . Due   to these biases , we can not guarantee that annota-   tors ‚Äô guesses always match reality . Based on spot-   checking , it seems that annotators made reasonable   judgments , but others may disagree . We have also   observed our data is skewed toward more male   noun phrases ( cf . Appendix A.5 ) , which could be   due to a bias both in VaTeX and in the annotators   we hired .   Our evaluation weights all errors equally , even   though some errors may have a bigger impact than   others . For example , someone in a video may be   misgendered by being referred to as a ‚Äú man ‚Äù when   the correct reference should be ‚Äú woman . ‚Äù   Acknowledgments   We thank Laura Biester for helping with data qual-   ity assurance . We thank the following people for   reviewing drafts of this document : Artem Abza-   liev , Christine Feak , Victoria Florence , Zhijing Jin ,   and Max Krogius . We also want to thank the LIT   Research Group @ UMich members for feedback   on some of the ideas discussed here . This mate-   rial is based in part upon work supported by the   Automotive Research Center ( ‚Äú ARC ‚Äù ) . Any opin-   ions , Ô¨Åndings , conclusions , or recommendations   expressed in this material are those of the authors   and do not necessarily reÔ¨Çect the views of ARC or   any other related entity .   References293329342935   A Dataset   A.1 Most - Frequent Noun Phrases   We report the most - frequent noun phrases in the   original labels and in the annotations we collected ,   in Fig . 5 . The most frequent nouns for both an-   swer sets tend to reference people , which makes   sense considering the content of the videos . In the   annotation data , we see a greater variety of syn-   onyms for the same kind of person ( ‚Äú male ‚Äù , ‚Äú man ‚Äù ,   ‚Äú guy ‚Äù ) , likely a result of the task deÔ¨Ånition , which   encourages paraphrasing .   A.2 Part - of - speech Distribution   We compare the rate of use of words in different   part - of - speech categories for the originally blanked   phrases and the annotations , using the same parser   speciÔ¨Åed earlier to label part - of - speech tags in the   noun phrases . The distributions are shown in Fig . 6 ,   and we see that the annotations have roughly the   same rate of part - of - speech tag use in all categories ,   except among adjectives and pronouns where the   originally blanked phrases have a higher rate of   use . This is likely an artifact of the data collection   strategy , which encouraged annotators to generate   unique noun phrases rather than phrases with ad-   jectives or pronoun references .   A.3 Part - of - speech Sequence Distribution   Although the candidate answers collected from   crowd workers consist of noun phrases , they may   include different part - of - speech ( POS ) sequences   within the noun phrases . The distributions of POS   sequences in Fig . 7 show that the annotators tended   to write ‚Äú bare ‚Äù nouns without extra determiners   and proper nouns , more than the original phrases .   This makes sense considering that the task asked   annotators to provide many unique nouns without   consideration for the nouns ‚Äô structure .   A.4 Dependency Categories   Due to the sampling process , some of the answers   occur in different syntactic contexts , e.g. in a prepo-   sitional phrase in ‚Äú A woman does push - ups on   _ _ _ _ _ ‚Äù or as a subject in ‚Äú _ _ _ _ _ at a driving range   demonstrating ... ‚Äù ( see Fig . 1 ) . We plot the distri-2936   bution of dependency categories in Fig . 8 , which   shows that nouns occur in a wide range of posi-   tions but mostly occur in a preposition , subject ,   and direct object positions .   Next , we test whether certain syntactic con-   texts tend to attract more answers from the anno-   tators than others , by computing the mean unique   number of answers per annotator within each syn-   tactic context ( based on the dependency parse   connected to the masked NP ) . The distribution   is shown in Fig . 9 . Captions that mask noun   phrases which occur in preposition ( pobj ) and di-   rect object ( dobj ) positions tend to attract slightly   fewer unique answers per annotator than the next   most - frequent categories , subject ( nsubj ) and   compounds ( compound ) . This intuitively makes   sense , since annotators would likely have fewer   options for noun phrases when faced with a prepo-   sition or a direct object , as opposed to the less   restrictive subject noun position .   A.5 Gender Representation   Often , language processing models can learn to en-   code social bias due to non - representative training   data , such as image captions for photos of men and   women taken in stereotypical environments ( Zhao   et al . , 2017 ) . We Ô¨Ånd a slight gender gap in our   own data : by using a gender word list , we Ô¨Ånd   that about 10.9 % of the originally blanked phrases   are male - related words in contrast to 6.2 % that are   female - related , and 9.1 % of the annotations are   male - related while 5.9 % are female - related . We   note that the gender imbalance is less severe for   the annotations than for the original phrases , and   the annotations do in fact use more gender - neutral   human words than the labels ( 6.6 % for annotations2937vs . 6.0 % for original phrases ) . While some of   the annotators may undoubtedly have some bias in   terms of their decisions , some of the bias may also   result from the original video clips . We acknowl-   edge this limitation as a direction for future work   in collecting video caption data .   We used the following lists for gendered words ,   which were chosen to be in similar semantic cate-   gories ( e.g. male ‚Äú brother ‚Äù , female ‚Äú sister ‚Äù , neutral   ‚Äú sibling ‚Äù ):   Male - oriented words : ‚Äú boy ‚Äù , ‚Äú brother ‚Äù , ‚Äú fa-   ther ‚Äù , ‚Äú guy ‚Äù , ‚Äú he ‚Äù , ‚Äú him ‚Äù , ‚Äú himself ‚Äù , ‚Äú his ‚Äù ,   ‚Äú male ‚Äù , ‚Äú man ‚Äù , ‚Äú son ‚Äù   Female - oriented words : ‚Äú daughter ‚Äù , ‚Äú female ‚Äù ,   ‚Äú girl ‚Äù , ‚Äú her ‚Äù , ‚Äú herself ‚Äù , ‚Äú lady ‚Äù , ‚Äú mother ‚Äù ,   ‚Äú she ‚Äù , ‚Äú sister ‚Äù , ‚Äú woman ‚Äù   Gender - neutral words : ‚Äú adult ‚Äù , ‚Äú baby ‚Äù ,   ‚Äú child ‚Äù , ‚Äú human ‚Äù , ‚Äú kid ‚Äù , ‚Äú parent ‚Äù , ‚Äú people ‚Äù ,   ‚Äú person ‚Äù , ‚Äú sibling ‚Äù   A.6 Spatiotemporal Trends of the Blanked   Entities   One of the authors of this paper randomly sampled   50 videos to analyze spatiotemporal information on   the blanked entities . Figures 10 to 12 show trends   on where , when , and for how long the blanked   entities appear in the videos . As expected , the   blanked entity generally appears at the center of   frames , with a small tendency to be on the lower   side . We observe that around 93 % of the time the   blanked entity appears between seconds 2 and 4 of   the video but that there is still a high chance ( 75 % )   of seeing it at any given moment . 68 % of the time   the blanked entities appear for the entire duration   of their corresponding video .   B Experiments and Results   B.1 More Implementation Details   We use the T5 model from the HuggingFace Trans-   formers library ( Wolf et al . , 2020 ) . We train the   model with Adam ( Kingma and Ba , 2014 ) on a   V100 - 16Gb with a batch size of 64 for 10 epochs   ( 4,000 steps ) using a learning rate of 1e-4 with a   warm - up of one epoch and a linear decay . The train-   ing time is short , less than an hour . We compute   the loss as the cross - entropy between the model-   generated output and the originally blanked phrase .   For test - time decoding , we use beam search with   a beam size of 4 for the early - fusion model and2938   8 for the late - fusion one , with a maximum token   length of 10 . We stop the decoding early , if an   example has seen as many complete hypotheses   as the beam size ( beam search early - stopping ) .   We penalize the repetitions of bigrams within a   decoded text . For each example , we choose the   Ô¨Årst beam that is a noun phrase , as detected by   spaCy ( Honnibal et al . , 2020 ) , or the Ô¨Årst one if   none . We show the effect of varying the beam   size in Appendix B.2 . We Ô¨Ånd that modifying the   beam search early - stopping property does not lead   to major performance changes .   B.2 Beam Search   Table 5 shows the effect of varying the beam size   during the beam search decoding . In all cases ,   using a beam search of at least size 2 is better than a   greedy search . However , the results are marginally   better or inconclusive when using beam size 4 or   8 . This is probably related to the phenomenon   described by Meister et al . ( Meister et al . , 2020 )   in which beam search does get us closer to the   true maximum a posteriori solution but the answers   actually start to get worse after a certain point .   B.3 Model Size   In Table 6 we show the result of changing the T5   model size for the text - only zero - shot baseline . We   note we could not Ô¨Åt the model variant t5 - 11b   into GPU memory . As expected , we note an in-   crease in the evaluation metrics as the model capac-   ity increases . B.4 Qualitative Analysis   We show in Table 7 several examples of answers   correctly predicted by the best multimodal method   but incorrectly answered by the best text - only   method . Even though the answers provided by   the text - only method are plausible by just looking   at the text , they do not make sense with the given   videos . In the second example , one can quickly tell   the person is not at a gym but instead is in some   kind of indoor room . For these examples , the mul-   timodal method seems to have identiÔ¨Åed what is   visually important.29392940