  Florin Brad   BitdefenderAndrei Manolache   Bitdefender ,   University of Stuttgart   { fbrad , amanolache , eburceanu}@bitdefender.comElena Burceanu   Bitdefender   Antonio Barbalau   University of Bucharest   abarbalau@fmi.unibuc.roRadu Tudor Ionescu   University of Bucharest   raducu.ionescu@gmail.comMarius Popescu   University of Bucharest   popescunmarius@gmail.com   Abstract   One of the main drivers of the recent advances   in authorship verification is the PAN large - scale   authorship dataset . Despite generating signif-   icant progress in the field , inconsistent perfor-   mance differences between the closed and open   test sets have been reported . To this end , we   improve the experimental setup by proposing   five new public splits over the PAN dataset ,   specifically designed to isolate and identify bi-   ases related to the text topic and to the author ’s   writing style . We evaluate several BERT - like   baselines on these splits , showing that such   models are competitive with authorship verifi-   cation state - of - the - art methods . Furthermore ,   using explainable AI , we find that these base-   lines are biased towards named entities . We   show that models trained without the named   entities obtain better results and generalize bet-   ter when tested on DarkReddit , our new dataset   for authorship verification .   1 Introduction   Identifying the author of a text is one of the most   versatile NLP tasks , with applications ranging from   plagiarism detection to forensics and monitoring   the activity of cyber - criminals . The task spans sev-   eral decades and was tackled using statistical lin-   guistics ( Mendenhall , 1887 ; Zipf , 1932 ; Mosteller   and Wallace , 1964 ) , and , more recently , machine   learning ( de Vel et al . , 2001 ; Zhao and Zobel , 2005 ;   Koppel et al . , 2007 ; Stamatatos , 2009 ) . Due to the   typically small data setup of authorship analysis   tasks , deep learning methods had a slow start in   this domain . Nevertheless , inspired by the impres-   sive performance of pre - trained language models ,   such as BERT ( Devlin et al . , 2019 ) , these meth-   ods gained traction in authorship analysis as well .   Saedi and Dras ( 2021 ) showed that Convolutional   Siamese Networks are more robust than a BERT-   based method over large - scale authorship attribu-   tion tasks . Barlas and Stamatatos ( 2020 ) investigated pre-   trained language models for cross - topic and cross-   domain authorship attribution and showed that   BERT and ELMo ( Peters et al . , 2018 ) achieve the   best results while being the most stable approaches .   Fabien et al . ( 2020 ) introduced BERT for Author-   ship Attribution ( BertAA ) in which they combine   BERT with stylometric features for authorship at-   tribution . The authors remarked that their model is   unable to perform text similarity evaluation in the   context of the more difficult authorship verification   problem , which we tackle .   One of the main contributors to the active devel-   opments in authorship analysis is the PAN organiz-   ing team , who proposed annual shared tasks since   2009 . While the recent PAN 2020 and 2021 con-   tests increased the difficulty of the authorship ver-   ification task and enabled large - scale model train-   ing ( Kestemont et al . , 2020 , 2021 ) , there are still   possible generalization issues due to the dataset   splits . For instance , models from 2020 trained on   theclosed - set data surprisingly performed better on   theopen - set test data ( which is arguably more dif-   ficult ) than on the closed - set test data ( Kestemont   et al . , 2021 ) . We therefore argue that in order to   better assess the generalization capabilities of au-   thorship verification systems , a more fine - grained   approach to dataset splitting may be needed .   To address these issues , we introduce a set of five   carefully designed splits of the publicly available   PAN dataset , ranging from the easiest setup ( closed-   set ) to the most difficult ( open - set ) . Our splits   progressively alleviate information leaks in the test   data , enabling a more confident evaluation .   Furthermore , we release our splits publiclyto   allow other members of the community to evaluate   models on any computing infrastructure , enabling   the evaluation of large - scale models . Along with   the new splits , we introduce a set of BERT - based   models ( Devlin et al . , 2019 ) to serve as baselines5634Test split O2D2O2D2 BERT NaiveComp .   Closed 93.5 96.4 95.6 75.6 72.2   Clopen 94.0 96.0 97.4 74.1 71.1   Open UA 92.6 92.6 90.2 78.6 68.5   Open UF 91.4 95.1 91.6 79.9 79.0   Open All 80.6 67.5 88.7 75.6 76.9   PAN Closed 93.3 93.5 - 74.7 74.2   PAN Open 93.3 94.4 - 75.3 74.5   for future research . We show that these language   models are competitive with the top scoring O2D2   ( out - of - distribution detector ) system at PAN 2021   ( Boenninghoff et al . , 2021 ) .   We also qualitatively inspect the models ’ predic-   tions and find that they often rely on named entities   to verify authorship . We show that by replacing   the named entities in the dataset with placeholders ,   we are able to obtain significant performance gains   and better generalization capabilities .   In summary , our contributions are threefold :   1.Weintroduce five splits , based on the PAN   dataset , with a decreasing degree of shared infor-   mation between train and test sets . These configura-   tions enable benchmarking large models , providing   a robust evaluation environment , on which we run   several BERT - based baselines .   2.Using explainable AI ( XAI ) methods , we find   thatBERT - like models focus on named entities   to determine authorship . We replace them with   placeholders and retrain our models , which brings   a significant performance boost .   3.We introduce the DarkReddit dataset for au-   thorship verification , which is significantly differ-   ent in style to the fanfictions in PAN . We test the   generalization capabilities of the models trained   on PAN , by evaluating them on DarkReddit . Our   previous finding is further confirmed by our model   trained without named entities , which generalizesSplit   Closed ✓ ✓ ✓ ✓   Clopen ✓ ✓ ✓ ✓   Open Unseen Authors ✗ ✓ ✗ ✓   Open Unseen Fandoms ✓ ✗ ✓ ✗   Open All ✗ ✓ ✗ ✗   better and improves the overall metric by 5.6 % .   2 Datasets   We use the PAN 2020 authorship verification   dataset . A document dbelongs to a fandom   ( topic ) fand is written by an author a. Author ver-   ification is a classification task which asks whether   documents danddare written by the same au-   thor ( SA ) or by different authors ( DA ) . The dataset   comes in two sizes : small ( 52k examples ) and large   ( 275k examples ) . The latter one is better suited for   deep learning models .   2.1 New PAN 2020 splits   The PAN 2020 competition is a closed - set verifi-   cation setup , meaning that the unseen test set con-   tains documents whose authors and fandoms were   seen at training time . The PAN 2021 competition   has a more difficult open - set setup , in which the   training data is the same as in 2020 , but the submit-   ted solutions are privately tested against document   pairs from previously unseen authors and fandoms .   The PAN testing infrastructure makes it difficult to   evaluate large models quickly . To this end , we re-   lease several dataset splits , ranging from the easier   closed - set setup to the more difficult open - set vari-   ants . We summarize the splits in Tab . 2 and provide   a more detailed description in the Supplementary   Material B. For each split , we propose a small ( XS )   and a large ( XL ) version.5635Closed Clopen Open UA Open UF Open All   Metric O2D2 cB B O2D2 cB B O2D2 cB B O2D2 cB B O2D2 cB B   F1 96.6 93.8 95.0 96.1 95.6 96.8 93.6 85.4 89.2 95.2 88.6 90.8 45.0 74.8 86.9   F0.5 94.3 93.4 94.5 94.0 96.5 97.0 89.6 87.0 88.1 94.5 92.3 88.8 70.1 84.6 87.2   c@1 95.9 93.3 94.5 95.4 95.4 96.6 91.5 84.8 88.2 93.1 88.8 90.0 65.2 78.9 87.0   AUC 98.7 98.0 98.6 98.4 99.1 99.4 95.8 92.4 95.3 97.6 96.5 96.9 89.5 91.1 94.0   overall 96.4 94.7 95.6 96.0 96.7 97.4 92.6 87.4 90.2 95.1 91.5 91.6 67.5 82.3 88.7   2.2 DarkReddit   To test an even more difficult scenario than our   open - set splits , we created a small authorship ver-   ification dataset . This dataset could be used to   benchmark the generalization capabilities of A V   models , while also being useful for cybersecurity   applications . The dataset was constructed by crawl-   ing1026 samples from /r / darknet , a subreddit   dedicated to discussions about the Darknet . There   is an equal number of same author and different   author pairs , resulting in a balanced dataset . A doc-   ument has 2,500 words on average , 9times less   than the PAN 2020 splits . The two datasets also   differ in other aspects ( e.g. topics , authors , text   purpose , self - contained message ) . We illustrate the   differences between PAN and DarkReddit exam-   ples in Figure 1 .   3 Experiments   Training . We fine - tune BERT ( B ) ( Devlin   et al . , 2019 ) and Character BERT ( cB , char-   BERT ) ( Boukkouri et al . , 2020 ) as binary clas-   sifiers for authorship verification . Given two doc-   uments dandd , we concatenate and feed them   to the Transformer encoder . When a document is   longer than 256 tokens , we sample a random chunk   of length 256 . The chunks are resampled at every   epoch , hence increasing the variety of the training   set . To make predictions , we add a linear layer on   top of the hvector and optimize the entire   model via the binary cross entropy loss . We use   the same set of hyperparameters across all of the   experiments . For the other models ( O2D2 , Naive   andCompression ) we used the provided code and   default hyperparameters . Evaluation . We report the overall metric from   PAN 2020 ( the mean over F1,F0.5,c@1and   AUC ) . To use information from all document pairs   ( d , d ) , we split each of them into 256 - length non-   overlapping chunks . We then feed each chunk pair   to the model , obtaining the class probabilities . Fi-   nally , we average the probabilities of all the chunk   pairs to obtain the prediction for the document pair .   Unsurprisingly , using multiple chunks outperforms   randomly picking only one chunk from each doc-   ument , leading to up to 10 % improvements in the   overall score .   3.1 Model comparison   Comparison to PAN models . As can be seen in   Tables 1 and 3 , BERT is competitive with the PAN   2021 winner on our public test splits . Both mod-   els greatly outperform the PAN baselines , a naive   distance - based approach ( Kestemont et al . , 2016 )   and a compression - based approach ( Halvani and   Graner , 2018 ) . BERT performs worse on the more   difficult open splits . O2D2 performs surprisingly   poor on the Open All test split , which may be due   to its calibration step , since the performance on   the development split is much larger ( 80.6 vs 67.5 ) .   Evaluating BERT on the private PAN sets is slow   due to access to CPU - only machines ( ≈1200h on a   machine powered by Intel Xeon E5 CPU with 8 GB   RAM memory ) . However , based on the scores of   the O2D2 and baseline approaches , we expect it to   perform similarly to the open test sets .   Comparison on our splits . We fine - tune and   evaluate the BERT - based models on the larger XL   splits introduced in Sec . 2 and compare them to   the PAN 2021 winner , O2D2 . We also report per-   formances of two other models and their ensem-5636   bles in the Supplementary Material A.1 : a Siamese   model ( siamBERT ) and a domain - adapted BERT   pretrained on the PAN 2020 corpus with the MLM   objective , then fine - tuned on each split . In Tab . 3   we notice that BERT outperforms charBERT on all   the splits over almost all the metrics . We expected   charBERT to provide better contextual embeddings   for rare words ( like named entities ) , since they in-   corporate character n - grams into the embeddings .   Though BERT may represent rare words noisily , it   is sufficiently robust for the PAN 2020 corpus .   3.2 Qualitative examples reveal biases   We next focus on better understanding the mod-   els ’ predictions through explainable AI ( Tjoa and   Guan , 2019 ) techniques . Inspecting the attention   scores is a common method of explaining a model ’s   prediction that has been called into question in re-   cent years ( Pruthi et al . , 2020 ; Serrano and Smith ,   2019 ) . We therefore follow recent explainability re-   sults ( Bastings and Filippova , 2020 ) and use the In-   tegrated Gradients ( IG ) ( Sundararajan et al . , 2017)method from the Captum library ( Kokhlikyan et al . ,   2020 ) to reveal the individual importance of words .   We analyze BERT models fine - tuned on a closed   and an open set , checking for potential biases aris-   ing from the dataset splitting process . In Fig . 2 ,   we show how important each word is in the author-   ship verification decision . For BERT trained on the   Closedsplit ( 1st row per pair ) , the most impor-   tant ones are the named entities . This initial focus   is reduced when fine - tuning the model on the Open   UFsplit ( which keeps training and testing fan-   doms disjoint ) . This suggests that fandom - specific   named entities encountered at test time are less   likely to be exploited for the prediction , since they   were not seen during training . Furthermore , the   open validation splits help with generalization at   themodel selection step . This is due to measur-   ing the model ’s performance against fandom and   author - specific information unseen at training time .   3.3 Replacing named entities improves   generalization   We hypothesize that replacing the named enti-   ties may further help with generalization in a   data - centric fashion , prohibiting the model to ex-   ploit them at train time . To this end , we re-   place the named entities from the Open All XS   dataset with their corresponding type ( e.g. Wolver-   ine→person ) . We notice in Tab . 4 that this re-   placement step improves the overall score for both   models , strengthening our hypothesis about the role   of named entities in authorship verification . Our re-   sults are in line with the previous works of Layton   et al . ( 2010 ) and Ding et al . ( 2015 ) , which show   that removing entities such as mentions , hashtags5637BERT charBERT   Metric w/ NE w/o NE w/ NE w/o NE   F1 73.5 73.5 54.1 75.2   AUC 91.1 94.3 89.0 84.4   F0.5 84.1 85.9 72.9 66.5   C@1 78.1 78.7 68.0 68.3   overall 81.7 83.1 71.0 73.6   training set F1 AUC F0.5 C@1 overall   Open All w/ NE 69.5 83.0 58.9 56.4 67.0   Open All w/o NE 74.1 86.4 64.4 65.4 72.6   and topic information improves performance of   authorship attribution .   Our results are further confirmed in a zero-   shot scenario , under a significant distribution shift ,   when testing on the DarkReddit corpus introduced   in Sec . 2.2 . Specifically , we demonstrate in Tab . 5 a   significant performance gain when training without   named entities . This suggests that the initial model   was focusing on named entities in a spurious way .   4 Conclusions   We introduced and published five splits of the PAN   dataset ranging from the easiest closed setup to in-   creasingly more challenging settings . This enables   a fine - grained evaluation and model selection . We   showed that BERT - based baselines are competitive   with top - scoring authorship verification methods   and significantly outperform non - neural baselines .   Using Integrated Gradients , we showed that , dis-   tinctly from the closed split , the open splits help   generalization at the model selection step by pre-   venting the model from overfitting on named enti-   ties of specific train authors or fandoms . We further   improved generalization by replacing the named   entities , making the models more robust to spurious   features . This claim also holds under a strong dis-   tribution shift , when cross - evaluating the models   on the significantly different DarkReddit dataset . Acknowledgements   This work has been supported in part by UEFIS-   CDI , under Project PN - III - P2 - 2.1 - PTE-2019 - 0532 .   Andrei Manolache was also supported by the Inter-   national Max Planck Research School for Intelli-   gent Systems ( IMPRS - IS ) .   Limitations   Closed vs. open splits . While our paper focuses   on building more difficult open set splits , deploy-   ing authorship verification systems is application   specific . This means that having methods trained   on closed splits may be desirable in certain scenar-   ios , such as when we are guaranteed that the test   authors are known .   Noisy examples . Collecting texts for building   corpora for authorship verification can suffer from   noisy data . Concretely , in both cases of PAN and   DarkReddit , one user can write under multiple   pseudonyms , leading to some different author ex-   amples to actually have the incorrect label . More-   over , multiple users can share the same account ,   leading to another issue where same author pairs   are wrongly labeled . However , large - scale author-   ship verification models should be robust to this   issue due to the large dataset size .   Long documents . Our BERT - based baselines are   capped at sequences of 512 tokens at most . This   means that we can process at most 256 tokens from   each text in a pair at a time . During training , we   overcame this issues by selecting random chunks   of texts . During evaluation , we aggregated pre-   dictions from all the chunks to obtain a prediction   for the documents pair . This limits the representa-   tion power during both training and evaluating , due   to encoding smaller contexts . Moreover , it slows   down inference on longer examples , making it even   more difficult to evaluate models on limited infras-   tructure . Further works should also include models   that accommodate longer sequences .   Ethics Statement   Authorship Verification systems may be deployed   in non - ethical ways , by different organizations and   parties , in order to track down vulnerable categories   of people , such as journalists , dissidents , whistle-   blowers , etc . However , we believe that opening up   research regarding authorship verification can help   these vulnerable categories by raising awareness of5638the possibilities and limitations of state - of - the - art   techniques and by mitigating their misuse .   Our datasets are based on publicly available data   and do not contain sensitive information .   References5639A Other quantitative results   A.1 SiamBERT and domain - adapted BERT   We list the performance of two other large pre-   trained BERT - based models on the PAN XL dataset   splits in Tab . 6 . The large gap between other mod-   els and siamBERT ( sB ) could be due to how the   model functions , without learning over both doc-   uments simultaneously . BERT processes a pair of   sequences , so the word - piece representations in-   teract at every level before making a prediction   based on the sequence pair embedding h. In   contrast , siamBERT processes each sequence sep-   arately , making the word - pieces ‘ interact ’ at the   end through the sequence embeddings hand   h. The domain - adapted BERT ( BERT ) ob-   tains similar results to BERT . Thus , the MLM fine-   tuning step on Closedis not warranted , showing   that adapting the representations to the domain of   the downstream task brings no improvements .   A.2 Ensembling   We measure the performance of various combi-   nations over the previously described models . In   Tab . 7 , we see how ensembling improves the perfor-   mance on the Open UFset over the best model   with over 2 % . However , unexpectedly , the ensem-   ble performance is weaker on the Open UAset .   This hurts the ensemble ’s robustness and might   be a sign of overfitting , explained by having too   many similar models that collapse to the same out-   put , failing in the same points and overwriting the   better prediction .   B Datasets   B.1 PAN dataset   The PAN-2020 competition featured two datasets ,   a smaller one ( 52k pairs ) , intended for traditional   shallow verification methods , and a larger one   ( 275k pairs ) , intended for deep learning solutions .   A document has an average of 21k words .   PAN XL . The large dataset has balanced classes   ( same vs different authors ) . Document pairs   written by the same author always come from   different fandoms ( e.g. Star Wars vs Harry Potter ) ,   while pairs written by different authors can belong   to the same fandom or to different fandoms . Same   author pairs are constructed from 41k authors ,   while different author pairs are constructed from   251k authors , with an overlap of 14k authors in5640Closed Clopen Open UA Open UF   Metric sB BB sB BB sB BB sB BB   F1 85.0 94.4 95.0 79.8 96.8 96.8 84.1 84.0 89.2 83.1 91.3 90.8   F0.5 84.8 93.5 94.5 80.2 97.3 97.0 85.3 86.9 88.1 84.1 91.2 88.8   c@1 86.0 93.9 94.5 81.4 96.6 96.6 85.5 83.7 88.2 84.2 90.8 90.0   AUC 93.1 98.4 98.6 89.6 99.5 99.4 92.7 92.4 95.3 92.0 96.8 96.9   overall 87.2 95.1 95.6 82.7 97.5 97.4 86.9 86.7 90.2 85.9 92.5 91.6   Open UA Open UF   Metric cB sB BBbest   ensemble cB sB BBbest   ensemble   F1 85.4 85.9 90.4 90.1 92.0 91.3 90.9 93.1 90.9 94.4   AUC 92.6 86.8 96.3 97.3 97.2 94.9 86.3 96.8 97.9 98.0   overall 87.0 87.2 92.1 93.5 93.4 91.2 88.9 93.2 93.2 95.3   both the same and different pairs . The XL dataset   has 494k distinct documents that span 1.600   fandoms .   PAN XS . The small dataset is also balanced . Dis-   tinctly from the XL dataset , it has only cross-   fandom pairs in both class pairs . This split allows   fast prototyping through smaller experiments with   models that have different components .   B.2 Our splits   We provide the construction details for all our splits   below .   Closed split In this setup , authors and fandoms   at train time are also found in the validation and   test sets ( but with different documents ) . This split   can hurt generalization , because it might work only   on a subset of authors or even worse , on specific   document pairs . Since we have no access to the   PAN 2020 test set , we make the train , validation   and test sets ourselves , by splitting the original   pairs . Each author pair ( a , a)in the DA pairs is   unique , so splitting the DA pairs such that both testauthors aandaare seen at train time is impossi-   ble . However , we relax this constraint and ensure   that at least one of the authors in DA test pairs is   seen at train time .   Clopen split The Clopen split is similar to the   closed split for the SA pairs . However , we remove   the closed set constraint for the DA pairs and as-   sign them randomly into train , validation and test .   Thus , authors and fandoms in the Clopen test and   validation sets might not be seen in the training set ,   making it a bit more general ( more similar to the   open sets ) .   Open Unseen Authors split In this split , authors   from the test set should not appear in the training   set . However , this is difficult to achieve strictly , so   we split the PAN 2020 dataset into train and vali-   dation / test sets such that : i ) authors of the SA test   pairs do not appear in the SA train pairs ; ii ) some   authors ( < 5 % ) in the DA test pairs may appear in   the DA train pairs ; iii ) most of the fandoms in the   test set appear in the training set.5641   Open Unseen Fandoms split This split type has   the following properties : i ) fandoms in the val-   idation / test sets are not seen during training ; ii )   some authors in the validation / test set may appear   in the training set . To ensure no overlap between   train and validation / test fandoms , training exam-   ples(d , d , f , f)where either forfappear in   the validation / test fandoms are dropped . This re-   sults in approximately 110 K fewer train examples .   Open all split This split is the most difficult and   the closest to the true open set setup in PAN 2021 .   Distinctly from the previous four splits , which were   created using the original pairs , this split required   sampling new document pairs and has the following   properties : i ) authors and fandoms in the test set   have not been seen in the training data ii ) authors in   the validation set have not been seen in the training   set , but the validation fandoms have been seen in   the training set .   B.3 Distribution of named entities   We observe in Figures 3 and 4 that the named entity   distributions in the PAN and DarkReddit datasets   are very different.5642Closed(split size ) Clopen(split size )   TotalSA DATotalSA DA   Split SF CF SF CF SF CF SF CF   Train 248,322 0 133,359 22,064 92,909 248,688 0 133,359 20,945 94,384   Valid 13,449 0 7,024 356 6,069 13,093 0 7,024 1,072 4,997   Test 13,784 0 7,395 355 6,034 13,784 0 7,395 1,114 5,275   Open UA(split size ) Open UF(split size ) Open UAll(split size )   SA DA SA DA SA DA   Split SF CF SF CF SF CF SF CF SF CF SF CF   Train 0 133,367 18,840 96,492 0 71,826 20,779 41,385 0 124,000 62,286 61,715   Valid 0 7,023 2,230 3,836 0 7,047 1,176 5,232 0 6,852 2,966 3,885   Test 0 7,388 2,061 4,328 0 7,056 1,176 5,233 0 6,853 1,633 5,2185643