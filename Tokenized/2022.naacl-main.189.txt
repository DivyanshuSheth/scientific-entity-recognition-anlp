  Tianyu Liu Yuchen Eleanor Jiang Ryan Cotterell Mrinmaya Sachan   { tianyu.liu , yucjiang , ryan.cotterell , mrinmaya.sachan } @inf.ethz.chAbstract   Many natural language processing tasks , e.g. ,   coreference resolution and semantic role label-   ing , require selecting text spans and making   decisions about them . A typical approach   to such tasks is to score all possible spans   and greedily select spans for task - speciﬁc   downstream processing . This approach , how-   ever , does not incorporate any inductive bias   about what sort of spans ought to be selected ,   e.g. , that selected spans tend to be syntactic   constituents . In this paper , we propose a novel   grammar - based structured span selection   model which learns to make use of the partial   span - level annotation provided for such prob-   lems . Compared to previous approaches , our   approach gets rid of the heuristic greedy span   selection scheme , allowing us to model the   downstream task on an optimal set of spans .   We evaluate our model on two popular span   prediction tasks : coreference resolution and   semantic role labeling . We show empirical   improvements on both.https://github.com/lyutyuh/   structured - span - selector   1 Introduction   The problem of selecting a continuous segment   of the input text , termed a span , is a common de-   sign patternin NLP . In this work , we call this   design pattern span selection . Common tasks that   have a span prediction component include corefer-   ence resolution ( Stede , 2011 ) , where the selected   spans are mentions , semantic role labeling ( Palmer   et al . , 2010 ) , where the selected spans are argu-   ments , question answering ( Jurafsky and Martin ,   2009 ) , where the selected spans are answers , and   named entity recognition ( Smith , 2011 ) , where the   selected spans are entities .   In most of the tasks mentioned above , span selec-   tion is the ﬁrst step . After a set of candidate spansCoref :   SRL :   Figure 1 : Examples of two span prediction tasks :   Coreference andSRL . In Coref , [ s]denotes a span of   textsreferring to the ientity . In SRL , i denotes a   set of predicates and [ s]denotes a set of arguments   for theipredicate .   is determined , a classiﬁer ( often a neural network )   is typically used to make predictions about the can-   didate spans . For instance , in coreference resolu-   tion , the selected spans ( mentions ) are clustered   according to which entity they refer to ; whereas in   SRL , the spans ( arguments ) are classiﬁed into a set   of roles . Two examples are shown in Fig . 1 .   As the number of spans to consider in the in-   put text can be quadratic in the length of the input ,   candidate spans are greedily selected as potential   antecedents , roles , or answers . While greedy span   selection has become the de - facto approach in span   prediction problems , it has several issues . First ,   such approaches typically ignore the inherent struc-   ture of the problem . For example , spans of in-   terest in problems such as coreference and SRL   are typically syntactic constituents , an assumption   supported by quantitative results . The lack of syn-   tactic constraints on the spans of interest leads to   a waste of computational resources , as all O(n )   possible spans are enumerated by the model .   In this paper , we propose a structured span   selector for span selection . Our span selector   is a syntactically aware , weighted context - free   grammar that learns to score partial , possibly   nested span annotations . In the case of partial an-2629notation , we marginalize out the missing structure   and maximize the marginal log - likelihood . Fig . 2   illustrates an example partial parse of our WCFG   and the difference between the traditional greedy   approach and our approach .   We apply our span selector to both corefer-   ence and SRL . In both cases , we optimize the   log - likelihood of the joint distribution deﬁned   by the span selector and the conditional for the   downstream task as deﬁned in Lee et al . ( 2017 ) and   He et al . ( 2018 ) . In contrast to previous approaches ,   which heavily rely on heuristicsto prune the   set of spans to be considered , our span selector   directly admits a tractable inference algorithm to   determine the highest - scoring set of spans . We   observe that the number of spans our model selects   is signiﬁcantly lower than the number of spans con-   sidered in previous works , resulting in a reduction   in the memory required to train these models . Our   approach leads to signiﬁcant gains in both down-   stream tasks considered in this work : coreference   and SRL . We ﬁnd that our approach improves the   performance of the end - to - end coreference model   on the OntoNotes English dataset ( 0.4 F1 ) and the   LitBank dataset ( 0.7 F1 ) . On SRL , our model also   achieves consistent gains over previous work .   2 Background and Related Work   2.1 Span Selection as a Design Pattern   Many NLP tasks involve reasoning over textual   spans , e.g. , coreference resolution , semantic role   labeling , named entity recognition , and question   answering . Models for these span prediction tasks   often follow a common design pattern . They de-   compose into two components : ( i ) a span selec-   tion component where the model ﬁrst selects a   set of spans of interest , and ( ii ) a span prediction   component where a prediction ( e.g. , entity or role   assignment ) is made for the chosen set of spans .   As shown in previous papers ( Zhang et al . , 2018 ;   Wu et al . , 2020 ) , the quality of the span selector   can have a large impact on the overall model per-   formance . The span selector typically selects spans   by selecting the start token and the end token in the   span . Thus , there are inherently / parenleftbig / parenrightbig   textual spans ,   which isO(n ) , within a document of ntokens to   choose from . Previous span selection models ( Lee   et al . , 2017 ; He et al . , 2018 ) enumerate all possible   spans in a brute - force manner and feed the greedily   selected top- kspan candidates to the downstream   prediction step .   However , several span selection tasks require   spans that are syntactic constituents , which is a use-   ful , but often neglected inductive bias in such tasks .   For instance , in coreference resolution , mentions   are typically noun phrases , pronouns , and some-   times , verbs . Similarly , in semantic role labeling ,   semantic arguments of a predicate are also typi-   cally syntactic constituents such as noun phrases ,   prepositional phrases , adverbs , etc . Our work uses   a context - free grammar to enumerate spans . The   number of valid syntactic constituents in a con-   stituency tree of a sequence of length nis bounded   byO(n ) , as the constituents can be viewed as the   internal nodes of a binary parse tree in Chomsky   normal form . Compared with brute - force enumer-   ation and greedy pruning , this inductive bias pro-   vides us with a natural pruning strategy to reduce   the number of candidate spans from O(n)toO(n )   and provides us a more natural and linguistically   informed way to model span - based tasks in NLP ,   through which we can employ parsing techniques   and retrieve the optimal span selection for down-   stream tasks .   To further motivate our approach , we provide   background on two popular span prediction tasks   considered in our paper : coreference resolution and   semantic role labeling ( SRL ) . We also overview   some previous papers on these tasks and contrast   their methodology with ours . Finally , we describe   how our model can work with partial span - level2630annotations provided by datasets for these tasks .   2.2 Coreference Resolution   Most coreference resolution models involve two   stages : mention detection and mention clustering .   Traditional pipeline systems rely on parse trees   and hand - engineered rules to extract mentions   ( Raghunathan et al . , 2010 ) . However , Lee et al .   ( 2017 ) show that we can directly detect mentions   as well as assign antecedents to them in an   end - to - end manner .   In addition to this paper , other works have also   explored better mention proposers for coreference .   Zhang et al . ( 2018 ) use multi - task loss to directly   optimize the mention detector . Swayamdipta et al .   ( 2018 ) also leverage syntactic span classiﬁcation as   an auxiliary task to assist coreference . Thirukoval-   luru et al . ( 2021 ) , Kirstain et al . ( 2021 ) , and Dobro-   volskii ( 2021 ) explore token - level representations   to both reduce memory consumption and increase   performance on longer documents . Miculicich   and Henderson ( 2020 ) and Yu et al . ( 2020 ) both   improve the mention detector with better neural net-   work structures . Yet they still need to manually set   a threshold to control the number of selected can-   didate mentions and none of them could produce   an optimal span selection for the downstream task .   Finkel and Manning ( 2009 ) situate NER in a pars-   ing framework by explicitly incorporating named   entity types into parse tree labels . In contrast , our   work requires neither syntactic annotations nor   hyperparameter tuning for mention selection .   2.3 Semantic Role labeling   Semantic role labeling ( SRL ) extracts relations be-   tween predicates and their arguments . Two major   lines of work in SRL are sequence - tagging models   ( He et al . , 2017 ; Marcheggiani et al . , 2017 ) and   span - based models ( He et al . , 2018 ; Ouchi et al . ,   2018 ; Li et al . , 2019 ) . Sequence tagging models   for SRL convert semantic role annotations to BIO   sequences . The tagger generates a label sequence   for one single predicate at a time . However , span-   based models generate the set of all candidate argu-   ments in one forward pass and classify their seman-   tic roles with regard to each predicate . As discussed   in He et al . ( 2018 ) , span - based models empirically   perform better than sequence tagging models as   they incorporate span - level features . Span - based   models also do better at long - range dependencies   as well as agreements with syntactic boundaries .   Thus , we focus on span - based models in this work.2.4 Nested and Partial Span Annotations   Nested mentions and partially annotated mentions   are two major concerns in this paper . Most datasets   for span prediction problems contain partial   annotations of mentions . For example , singletons   are not annotated in OntoNotes ( Pradhan et al . ,   2012 ) . In the coreference resolution example given   in Fig . 1 , the bracketed nested spans are annotated ,   while the underlined spans are valid mentions that   are unannotated , since they do not co - refer with   any other mention in the same document . The same   is also true in SRL . In the SRL example in Fig . 1 ,   there are two predicates ( boxed words in the exam-   ple ) in one sentence . Their arguments are nested   ( i.e. ,1and2of the second predicate are   located within2 of the ﬁrst predicate ) .   3 A Structured Span Selection Model   In this section , we develop the primary contribu-   tion of our paper : A new model for span selec-   tion . Speciﬁcally , we assert that almost all spans   that a span selector should select are syntactic con-   stituents ; see Fig . 1 for two examples . Under this   hypothesis , a context - free grammar ( CFG ) is a nat-   ural model for span selection as spans selected by   a CFG can not overlap , i.e. , every pair of spans se-   lected by a CFG would either be nested or disjoint .   3.1 Notation   We ﬁrst start by introducing some basic terminol-   ogy . We deﬁne a documentDas a sequence of   sentences w, ... ,w . Each sentence win the   document is a sequence of words [ w, ... ,w ] .   Aspan is a contiguous subsequence of words in a   sentence . For instance , we denote the span from   positionito positionk , i.e. ,w···w , as[i , k ] .   3.2 Weighted Context - Free Grammars   Next , we deﬁne weighted context - free grammars   ( WCFG ) , the formalism that we will use to build   our span selector . A WCFG is a ﬁve - tuple   /angbracketleftΣ , N , S , R , ρ / angbracketright , where Σis an alphabetof ter-   minal symbols , Nis a ﬁnite set of non - terminal   symbols , S∈Nis the unique start symbol , Ris a   set of production rules where a rule is of the form   X→αwhereX∈Nandα∈(N∪Σ ) , and   ρ : R→Ris a scoring function that maps every   production rule to a non - negative real number . We2631say a WCFG is in Chomksy normal form ( CNF )   if every production rule has one of three forms :   X→Y Z , where X , Y , Z∈N , X→x , where   X∈Nandx∈Σ , orS→ε . For an input sentence   w , a WCFG deﬁnes a weighted set of parse trees ,   which we will denote T(w ) ; we drop the argument   fromTwhen the sentence is clear from the context .   We overload the scoring function ρto assign a   weight to each parse tree t∈T(w ) . We deﬁne ρ   applied to a tree as follows   ρ(t ) = /productdisplayρ(r)≥0 ( 1 )   Given thatρreturns a non - negative weight , our   WCFG can be used to deﬁne a distribution over the   set of all parses of a sentence   p(t|w ) = ρ(t )   Z(2 )   whereZ=/summationtextρ(t)is the sum of the scores   of all parses . Using the familiar inside – outside   algorithm ( Baker , 1979 ) , we can exactly compute   ZinO(|w|)time .   3.3 A WCFG Span Selector   To convert from parse trees to spans , our paper   exploits a simple fact : If our grammar is in CNF ,   then every parse tree t∈T(w)corresponds to a   unique set of labeled spans . Speciﬁcally , we write   [ i , X , k]iff the contiguous subsequence w···w   corresponds to a constituent rooted at Xint . We   will denote the tree - to - span bijection as spans ( · )   and writeM = spans ( t)to denote the set of spans   timplies . We will also denote the set of all sets of   spans viable under a CFG in CNF as M(w ) .   To extract spans useful for downstream tasks ,   we propose a simple WCFG . The grammar has   three non - terminals N={S , X , X}where S   is the distinguished start symbol . A span rooted   at non - terminal X , denoted [ i , X , k ] , is termed   aspan of interest ; we will abbreviate [ i , X , k ]   asσ . Likewise , a span rooted at non - terminal   X , denoted [ i , X , k ] , is termed a a span of non-   interest ; we will abbreviate [ i , X , k]asσ . The   full grammar is given in App . A.1 . We deﬁne our   weight function ρ : R→Ras follows :   ρ(X→YZ)=/braceleftBigg   exps(σ)ifX = X   1 otherwise   ( 3)wheresis a learnable span - scorer that assigns a   non - negative weight . Note this deﬁnition of ρis an   anchored scoring mechanism as it also makes use   of the span indices iandk .   Under the simpliﬁed scoring function in Eq . ( 3 ) ,   the score of a tree tcan be re - expressed as a product   of the scores of the spans of interest in spans ( t ) .   Speciﬁcally , for any tree t , we have   ρ(t ) = /productdisplayρ(r ) ( 4 )   = exp   /summationdisplays(σ)    ( 5)= exps(M ) ( 6 )   whereM=/braceleftbig   σ|σ∈spans ( t)/bracerightbig   . Note that   the step from Eq . ( 4 ) to Eq . ( 5 ) follows from the fact   that only those spans rooted at Xhave a weight   other than 1 under our choice of ρand , furthermore ,   ρignores the body of the context - free rule . The   problem of mention detection is hence converted   from subset selection to ﬁnding an optimal parse   tree that maximizes the score :   M= argmaxs(M ) ( 7 )   = spans / parenleftBigg   argmaxρ(t)/parenrightBigg   ( 8)   where the “ Viterbi version ” of the CKY algorithm   ( see App . A.3 ) , yields an exact algorithm for the   argmax function in Eq . ( 8) in O(|w|)time .   Finally , in order to train our WCFG with only   partial supervision , i.e. , in the case when we do   not observe the entire tree , we require the marginal   probabilities of the spans of interest . First , let T   be the set of parses that contain σ . Then , the   marginal probability of the span of interest σcan   be expressed as :   p(σ|w ) = /summationdisplayp(t|w ) = /summationdisplayρ(t )   Z(9 )   As described by Eisner ( 2016 ) , we can compute   p(σ|w)by computing the derivative of the   log - normalizer logZwith respect to s(σ ) , i.e. ,   p(σ|w ) = ∂logZ   ∂s(σ)(10 )   Automatic differentiation ensures that this marginal   computation will have the same runtime as the   computation of logZitself — to wit inO(|w| )   time ( Griewank and Walther , 2008).26324 Adaptations to Downstream Tasks   In this section , we introduce how our structured   span selector can be applied in an end - to - end man-   ner to coreference resolution and SRL .   4.1 Coreference Resolution   The goal of coreference resolution is to link a span   of interestσ , termed a mention in the context of   coreference resolution , to its antecedent . Note that   the antecedent is either another mention in the same   document or the dummy antecedent , , which we   denote as / epsilon1 . We writeσ / squigglerightσto denote that σ   isσ ’s antecedent . When formulating coreference   in a probabilistic manner , we have the following   natural decomposition :   p(σ , σ / squigglerightσ ) ( 11 )   = p(σ)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright×p(σ / squigglerightσ|σ)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright   The support of the above distribution is X×Y ,   whereXis the set of all possible textual spans in   D , Y={σ|j < /lscript}∪{/epsilon1}is the set of mentions   precedingσplus the dummy antecedent /epsilon1for ev-   eryσ∈X. In words , the above decomposition   means that the probability of σco - referring with   the spanσis the probability of ﬁrst recognizing   thatσis itself a mention and then determining   the link . In practice , this decomposition means that   modelers can select p(σ)andp(σ / squigglerightσ|σ )   according to their taste and , importantly , indepen-   dently of each other . In this work , we explore treat-   ingp(σ)as the WCFG span selector described in   § 3.2 andp(σ / squigglerightσ|σ)as Lee et al . ( 2017 ) ’s   popular span ranking model for coreference .   Lee et al . ( 2017 ) as a Mention - Linker . We now   describe the mention - linker in Lee et al . ( 2017 ) .   We deﬁne the mention - linker distribution as   p(σ / squigglerightm|σ ) ( 12 )   = exps(σ , m)/summationtextexps(σ , m )   The scoring function s(·,·)is deﬁned in two cases :   One case for m=/epsilon1 , the dummy antecedent , and   one form = σ , a preceding span :   s(σ,/epsilon1 ) = 0 ( 13 )   s(σ , σ ) = s(σ ) + s(σ ) + s(σ , σ)The ﬁrst score function , s(σ ) , is a score for   span [ i , j]being a mention . The second function ,   s(σ , σ ) , a score that σis an antecedent of   [ i , j ] . In this work , both sandsare computed   by neural networks that take span representations   as inputs . However , in principle , they could be   computed by any model .   Training . Lee et al . ’s model adopts a naïve   greedy algorithm by taking the top λ|D|spans   with the highest mention scores s , whereλis   a hyperparameter that has to be manually deﬁned   for different datasets . However , ﬁnding a proper   ratioλcan be very tricky . In contrast , in our set-   ting we can optimize the ﬁnal objective function   which is the log - likelihood of the joint distribution   deﬁned at the beginning of § 4.1 :   L=/summationdisplay / summationdisplaylog / summationdisplayp(σ , σ / squigglerightm)(14 )   whereGis the ( partially ) annotated set of mentions ,   Gis the set of all textual spans of winG , andG   is the ground truth cluster that σbelongs to .   Handling partial annotation with no single-   tons . Since in many coreference datasets , e.g. ,   OntoNotes , only the mentions that are referred to   more than once are annotated , learning a mention   detector from such data requires the ability to han-   dle the lack of singleton annotations . To handle   partial span annotations , we marginalize out the   unannotated singletons . This results in the follow-   ing marginal log - likelihood   L=/summationdisplay / summationdisplaylog / parenleftBig   p(σ / squiggleright / epsilon1|σ)p(σ )   + ( 1−p(σ))/parenrightBig   ( 15 )   We optimize the loss L = L+Ljointly . Here ,   Gdenotes the set of all spans of wnotinG.   Time Complexity . For each sentence , the inside –   outside algorithm Eq . ( 22 ) and CKY algorithm   Eq . ( 23 ) reduced to O(|w|)semiring matrix multi-   plications . Sentence - level parallelism can also be   applied to all the sentences .   4.2 Semantic Role Labeling   The goal of SRL is to classify the semantic role of   every argument σwith respect to a given predi-   cate . Following the notation style in § 4.1 , we use   σ / squigglerightvto denote that σhas the semantic role l2633 in the frame of predicate v. The joint probability   p / parenleftBig   σ , σ / squigglerightv / parenrightBig   can be written as :   p(σ)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright×p / parenleftBig   σ / squigglerightv|σ / parenrightBig   /bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright   He et al . ( 2018 ) as a Role Classiﬁer . As this   work focuses on span selection , we choose the role   classiﬁer to be the popular and effective one from   He et al . ( 2018 ) , and use gold predicates vduring   training and evaluation . The semantic role label l   takes its values from a discrete label space Lwhich   contains all the semantic roles plus the null rela-   tion / epsilon1 . The classiﬁer then models the following   probability distribution :   p / parenleftBig   σ / squigglerightv|σ / parenrightBig   = exps(σ , v,/lscript)/summationtextexps(σ , v,/lscript )   s(σ , v,/lscript ) = s(σ ) + s(σ , v,/lscript ) ( 16 )   Similar to the coreference model of Lee et al .   ( 2017),s(σ)is the score for span [ i , j]to be   an argument and s(σ , v,/lscript)for[i , j]to play   the rolelfor predicate v. Note that the score for   the / epsilon1label ( i.e. , no relation ) , s(σ , v,/epsilon1 ) , is set to   constant 0 , similar to the dummy antecedent case   in coreference .   Training Objective . He et al . ’s model also suf-   fers from the challenge of tuning the hyperparam-   eterλ . The training objective for SRL with our   structured span selection model is then :   L=/summationdisplay / summationdisplaylogp(σ , σ / squigglerightv ) ( 17 )   wherelis the correct semantic label of σwith   regard to predicate v. Similar to coreference   resolution , we handle the issue of partial annota-   tion for the span selection model by adding the   log - likelihood that σmay not be an annotated   argument :   L=/summationdisplay / summationdisplaylog / parenleftBig   p(σ / squigglerightv|σ)p(σ )   + ( 1−p(σ))/parenrightBig   ( 18 )   And the ﬁnal objective function is L = L+L.   5 Experiments   5.1 The Greedy Baseline   Previous work has mostly considered a greedy pro-   cedure for span selection as opposed to Eq . ( 8) . Theapproach produces a score s(σ)independently   for each span σ . As the number of spans σis   potentially very large , the set of spans is greedily   pruned to a set of size K. For instance , in SRL ,   spans are selected for each sentence :   M ( 19 )   = atop / parenleftBig / braceleftBig   s(σ)|1≤i < k≤|w|/bracerightBig / parenrightBig   However , in coreference resolution , a set of spans   is selected for the entire document :   M ( 20 )   = atop / parenleftBig / uniondisplay / braceleftBig   s(σ)|1≤i < k≤|w|/bracerightBig / parenrightBig   where atopis shorthand for argtop . We will   see in our experiments that tuning Kcan be quite   challenging ( see Fig . 3 ) . Moreover , as the greedy   approach scores each span independently , it ignores   the structure of the provided span annotation .   5.2 Datasets   Coreference . We experiment on the CoNLL-   2012 English shared task dataset ( OntoNotes )   ( Pradhan et al . , 2012 ) and LitBank ( Bamman et al . ,   2020 ) in our experiments . As a part of our experi-   ments on OntoNotes , we apply the speaker encod-   ing in Wu et al . ( 2020 ) , that is using special tokens   ( < speaker > , < /speaker > ) to denote the speaker ’s   name , as opposed to the original binary features   used by Lee et al . ( 2017 ) . This simple change   brings a consistent boost to the performance by 0.2   F1 . A major difference between these two datasets   is that LitBank has singleton mention annotations   while OntoNotes does not . For LitBank , we use   the standard 10 - fold cross - validation setup , as is   the standard practice .   SRL . We use the CoNLL-2012 SRL dataset .   Gold predicates are provided to the model .   5.3 Coreference Resolution   We report the average precision , recall , and F1   scores of the standard MUC , B , CEAF , and   the average CoNLL F1 score on the OntoNotes test   set in Tab . 1 . The average F1 scores on LitBank   are shown in Tab . 2 . For OntoNotes , we run the   experiments with 5 random initializations and the   improvements reported are signiﬁcant under the   two - tailed paired t - test .   We compare our models with several representa-   tive previous works . In order to focus on comparing2634   the impact of mention detection , we do not consider   higher - order inference techniques in our models   and report the non - higher order result from Xu and   Choi ( 2020 ) . Joshi et al . ( S ) is the major base-   line that uses SpanBERT ( Joshi et al . , 2020 ) and is   trained with the speaker encoding discussed in § 5.2 .   This encoding yields an F1 score improvement of   0.2over the result reported by Xu and Choi ( 2020 ) .   Our model with the structured mention detector   achieves an F1 score of 80.5 , an improvement   of0.4F1 over the baseline . While on LitBank ,   our model achieves an F1 score of 76.3 , which   is an improvement of a 0.7F1 over ( Joshi et al . ,   2020 ) . It can also be observed that this gain mainly   comes from improved recall , which is because we   have a superior mention detector that can retrieve   mentions with better accuracy . We further analyze   this result in the following section .   Avg . P Avg . R Avg . F1   Bamman et al . - - 68.1   LB - MEM - - 75.7   U - MEM - - 75.9   Joshi et al . 78.7 72.9 75.6   Ours 77.4 75.3 76.3   5.3.1 Analysis of Mention Detector   Next , we examine the performance of our proposed   mention detection scheme . As shown in Fig . 3 ,   compared with Joshi et al . ( S ) , our model predicts0.10.20.30.40.50.6708090100   Ratio   Joshi et al . ( S ) ( various ratio )   Joshi et al . ( S ) ( actual ratio )   Ours   mentions more accurately with a higher recall . In   contrast to Joshi et al . ( S ) who select 0.4|D|men-   tion spans , our method on average selects 0.26|D| .   The smaller span set makes the coreference model   more efﬁcient as well .   5.3.2 Analysis of Structured Modeling   To see how our structured modeling beneﬁts coref-   erence , we further compare our approach with   a baseline Sigmoid which replaces the p(σ)in   Eq . ( 10 ) with a simple non - structured estimator :   p(σ ) = sigmoid ( s(σ ) ) ( 21)2635Nested Depth 1 2 3 +   Greedy 96.5 87.1 86.2   Ours 97.8 93.2 93.9   where sigmoid ( x ) = . The loss function   used for Sigmoid is the same as the structured   model given in § 4.1 . Through this comparison , we   aim to show the effectiveness of structured mod-   eling . We also build a multi - task learning baseline   MTLsimilar to Swayamdipta et al . ( 2018 ) . The base-   line adds an auxiliary classiﬁer that classiﬁes spans   into noun phrases , other syntactic constituents , or   non - constituents . The coefﬁcient of the multi - task   loss is set to 0.1as in Swayamdipta et al . ( 2018 ) .   This comparison is shown in Tab . 4 . We ﬁnd   that replacing p(σ)with unstructured p(σ )   degrades the performance by an F1 score of 0.3 .   Thus , we conclude that the structured probability   function is more expressive than p(σ)as it   models the global annotation for each sentence . In   contrast , the mention detectors in Joshi et al . ( S )   and the Sigmoid model each span independently .   5.3.3 Analysing the source of improvement   Next , we try to explore where the gains of our   model come from .   Nested Mentions . We ﬁrst investigate the capa-   bility of our structured model in handling nested   mentions . Tab . 3 shows the recall rate of mentions   of different nested depths . Here , nested depth refers   to the level of nesting in the mentions . E.g. , in the   ﬁrst example given in Fig . 1 , The president is of   depth 1 , while he and his wife , now a New York sen-   ator is of depth 2 . As shown in Tab . 3 , the gains of   our method are larger for deeply nested mentions ,   which highlight the capability of our structured   span detector to handle more difﬁcult nested men-   tions that can not be handled by the greedy selector .   Widths of Mentions . We also compare the re-   call rate for mentions of different widths in Tab . 5 .   We show that our model can detect longer spans   better , which are usually more difﬁcult to detect .   For spans with 5–12 words , our structured model   still maintains a recall rate of 96.5 % , compared to   a sharp drop for the greedy unstructured model . Avg . P Avg . R Avg . F1   Joshi et al . ( S ) 81.6 78.6 80.1   Sigmoid 80.8 79.6 80.2   MTL 80.8 78.9 79.8   Ours 81.1 79.9 80.5   Span Width 1 - 4 5 - 12 12 +   Greedy 96.2 92.7 82.5   Ours 97.8 96.5 85.2   5.4 Semantic Role Labeling   For semantic role labeling , we report the preci-   sion , recall , and F1 score on the CoNLL-2012 SRL   dataset . The gold predicates are provided during   both training and evaluation . Therefore , the model   has to focus on extracting the correct arguments   and classifying their roles for each predicate . The   results are shown in Tab . 6 in comparison with pre-   vious span - based models . He et al . refers   to the model of He et al . ( 2018 ) with SpanBERT   ( Joshi et al . , 2020 ) as a sentence encoder .   Next , we report the performance of our span   selector on the SRL task . Following the same   trend as coreference resolution , we ﬁnd that our   structured model is able to extract much more   accurate arguments and thus , signiﬁcantly reduce   the memory consumption for the downstream   task . While keeping a comparable recall rate of   gold arguments ( 96.5 % for the greedy selector   and 96.2 % for ours ) , our span selector reduces2636P R F1   BIO 90.2 91.0 90.6   Greedy 92.8 90.2 91.5   Ours 92.8 91.2 92.0   Coref SRL   Greedy 11.5 12.2   Ours 8.5 9.3   the number of enumerated spans by 21.2 % . We   compare the accuracy of retrieving unlabeled   argument spans in Tab . 7 . BIO refers to the   tagger - style SRL model using the same text   encoder . Our model outperforms both baselines .   5.5 Qualitative Examples   In this section , we show a qualitative example to   illustrate the grammar learned by our span selec-   tor for coreference resolution and SRL . Two sets   of extracted spans for the same input sentence are   shown in Tab . 9 . For coreference resolution , our   model selects maximal NPs ( containing all mod-   iﬁers ) and verbs . While in SRL , the parse tree   consists of much denser and syntactically heteroge-   neous spans of NPs , PPs , modal verbs , adverbs , etc .   This comparison empirically shows that our model   is capable and robust enough to learn a complex   underlying grammar from partial annotation .   5.6 Memory Efﬁciency   We further analyze the memory efﬁciency of our   model . We evaluate the peak GPU memory usage   on the development set of OntoNotes . For both   tasks , we see a signiﬁcant reduction in memory   usage of 27 % for coreference and 24 % for SRL .   6 Conclusion   In this paper , we proposed a novel structured model   for span selection . In contrast to prior span selec-   tion methods , the model is structured , which allows   it to model spans better . Instead of a greedy span   selection procedure , the span selector uses partial   span annotations provided in data to directly obtain   the set of optimal spans . We evaluated our span se - Coref[[The world ’ s ] ﬁfth [ Disney ] park ]   will soon [ open ] to [ the public ] here .   SRL[The world ’ s [ ﬁfth ] [ Disney ] [ park ] ]   [ will ] [ soon ] [ [ open ] [ to the public ] ]   [ here ] .   lector on two typical span prediction tasks , namely   coreference resolution and semantic role labeling ,   and achieved consistent gains in terms of accuracy   as well as efﬁciency over greedy span selection .   Ethical Considerations   To the best of our knowledge , the datasets used   in our work do not contain sensitive information ,   and we foresee no further ethical concerns with the   work .   Acknowledgements   Mrinmaya Sachan acknowledges support from an   ETH Zürich Research grant ( ETH-19 21 - 1 ) and a   grant from the Swiss National Science Foundation   ( project # 201009 ) for this work .   References263726382639A A Weighted Context - Free Grammar   A.1 The Grammar   In our WCFG / angbracketleftΣ , N , S , R , ρ / angbracketright , Σis the set of all tokens in the vocabulary , N={S , X , X } , where Sis   the start symbol , Xis the span of interest , and Xis the spans that are not of interest . The complete set   of production rules Ris shown in Tab . 10 .   S→ XX   S→ XX   S→ XX   S→ XX   X→ XX   X→ XX   X→ XX   X→ XX   X→ XX   X→ XX   X→ XX   X→ XX   X→x,∀x∈Σ   X→x,∀x∈Σ   We only assign nontrivial weights to rulesX→YZwhereXisX. That is to say , in Eq . ( 1 ) ,   s(X→YZ ) = 1 whereXis not X.   A.2 The Inside – Outside Algorithm   For a span [ i , X , k ] , its inside value can be expressed as   β([i , X , k ] ) = /summationdisplay / parenleftBigg   exps([i , X , k])×/parenleftBig / summationdisplayβ([i , Y , j])×β([j , Z , k])/parenrightBig / parenrightBigg   ( 22 )   In the case when k = i+ 1 , we haveβ([i , X , k ] ) = exps([i , X , k ] ) . The inside value of the entire   sentenceβ([0,S,|w|])is exactlyZ , the sum of scores of all parses .   A.3 The CKY Algorithm   We use the CKY algorithm to ﬁnd an optimal parse tree t∈T(w)that maximizes ρ(t ) . The recursive   function when i < k used is :   γ([i , X , k ] ) = max / braceleftBigg   s([i , X , k ] ) + max / braceleftBig   γ([i , Y , j ] ) + γ([j , Z , k])/bracerightBig / bracerightBigg   ( 23 )   In the case when k = i+ 1 , we haveγ([i , X , k ] ) = s([i , X , k ] ) .   B Experimental Settings   The systems are implemented with PyTorch . We use SpanBERTas text encoder . We train the model   for 20 epochs and select the best - performing model on the development set for testing . The documents   are split into 512 word segments to ﬁt in SpanBERT . Models used for coreference resolution have 402   million learnable parameters , and models for SRL have 382 million learnable parameters . We closely   follow the hyperparameter settings of Joshi et al . ( 2020 ) and build our models upon the codebase of Xu   and Choi ( 2020)under Apache License 2.0 . The learning rate of SpanBERTparameters is set to26401×10with0.01decay rate , and the learning rate of task parameters is set to 3×10 . The dropout rate   of feedforward neural network scorers is set to 0.3 . When training our model , we randomly sample 0.1|D|   negative spans that are not mentions and add their negative log - likelihood −logp(σ)to the training   objective . This is to prevent p(σ)from converging to 1 . For SRL task , we use a batch size of 32 for 40   epochs and the same learning rate with coreference resolution . The same negative sampling technique is   applied . Our models are trained on Nvidia Tesla V100 GPUs with 32 GB memory . The average training   time is around 8 hours for Joshi et al . ( S ) baseline and around 9 hours for our model . For SRL models ,   training takes 25 hours .   C Results on the Development Set   In this section , we report the results that our models get on the development sets of OntoNotes and   LitBank .   Avg . P Avg . R Avg . F1   Joshi et al . ( S ) 82.0 78.8 80.4   Sigmoid 81.6 79.4 80.5   Ours 81.1 80.2 80.7   Avg . P Avg . R Avg . F1   Joshi et al . ( S ) 78.8 73.7 76.1   Ours 77.4 75.6 76.6   Avg . P Avg . R Avg . F1   He et al . 87.9 85.0 86.4   Ours 87.8 86.2 87.02641