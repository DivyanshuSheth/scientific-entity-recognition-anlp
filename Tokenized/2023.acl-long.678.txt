  Tianshu Zhang , Changchang Liu , Wei - Han Lee , Yu Su , Huan SunThe Ohio State UniversityIBM Research{zhang.11535 , su.809 , sun.397}@osu.edu{changchang.liu33 , wei-han.lee1}@ibm.com   Abstract   This paper studies a new task of federated learn-   ing ( FL ) for semantic parsing , where multiple   clients collaboratively train one global model   without sharing their semantic parsing data .   By leveraging data from multiple clients , the   FL paradigm can be especially beneficial for   clients that have little training data to develop   a data - hungry neural semantic parser on their   own . We propose an evaluation setup to study   this task , where we re - purpose widely - used   single - domain text - to - SQL datasets as clients   to form a realistic heterogeneous FL setting and   collaboratively train a global model . As stan-   dard FL algorithms suffer from the high client   heterogeneity in our realistic setup , we further   propose a novel LOssReduction Adjusted Re-   weighting ( Lorar ) mechanism to mitigate the   performance degradation , which adjusts each   client ’s contribution to the global model up-   date based on its training loss reduction during   each round . Our intuition is that the larger   the loss reduction , the further away the current   global model is from the client ’s local optimum ,   and the larger weight the client should get . By   applying Lorar to three widely adopted FL   algorithms ( FedAvg , FedOPT and FedProx ) ,   we observe that their performance can be im-   proved substantially on average ( 4%-20 % ab-   solute gain under MacroAvg ) and that clients   with smaller datasets enjoy larger performance   gains . In addition , the global model converges   faster for almost all the clients .   1 Introduction   Semantic parsing aims to translate natural lan-   guage utterances into formal meaning representa-   tions such as SQL queries and API calls and can be   applied to build natural language interfaces that en-   able users to query data and invoke services withoutprogramming ( Berant et al . , 2013 ; Thomason et al . ,   2015 ; Su et al . , 2017 ; Campagna et al . , 2017 ) . Neu-   ral semantic parsers have achieved remarkable per-   formance in recent years ( Wang et al . , 2020a ; Rubin   and Berant , 2021 ; Scholak et al . , 2021 ) . However ,   they are data - hungry ; bootstrapping a neural se-   mantic parser by annotating data on a large scale   can be very challenging for many institutions , as   it requires the annotators to have intimate knowl-   edge of formal programs . One natural thought is to   leverage data from different institutions and train a   unified model that can be used for all institutions .   However , in practice , institutions such as hospitals ,   banks , and legal firms are prohibited from shar-   ing their data with others , due to privacy concerns .   Therefore , for institutions that only have very lim-   ited data , it is extremely hard to build their own   neural semantic parsers .   Federated learning ( FL ) ( Kone ˇcn`y et al . , 2016 ;   McMahan et al . , 2017 ; Yang et al . , 2018 ) has turned   out to be a popular training paradigm where multi-   ple clients can collaboratively train a global model   without exchanging their own data . In this paper ,   we study a new task of federated learning for se-   mantic parsing . Through FL on the data scattered   on different clients ( e.g. , institutions ) , we aim to   obtain a global model that works well for all clients ,   especially those that have insufficient data to build   their own neural models .   Towards that end , we propose an evaluation   setup by re - purposing eight existing datasets that   are widely adopted for text - to - SQL parsing , such   as ATIS ( Srinivasan Iyer and Zettlemoyer , 2017 )   and Yelp ( Navid Yaghmazadeh and Dillig , 2017 ) .   These datasets demonstrate great heterogeneity , in   terms of dataset sizes , language usage , database   structures , and SQL complexity , as they were col-   lected from the real life by different researchers , at   different times , and for different purposes . There-   fore , we use this collection to simulate a realistic   scenario where eight clients with very different12149data participate in the FL paradigm to jointly train   a neural semantic parser .   Heterogeneity , where the data distributions and   dataset sizes on different clients are different , is   recognized as one of the biggest challenges in FL   ( McMahan et al . , 2017 ; Reddi et al . , 2020 ; Li et al . ,   2020a , 2021 ; Shoham et al . , 2019 ; T Dinh et al . ,   2020 ) . Existing work either uses synthetic data   ( Li et al . , 2020a ) or splits a classification dataset   based on Dirichlet distribution ( Lin et al . , 2022 )   to simulate the non - IID federated learning setting ,   while we propose a more realistic setup to study   this setting for semantic parsing . Pre - trained lan-   guage models such as T5 ( Raffel et al . , 2020 ) have   been shown as a powerful unified model for various   semantic parsing tasks ( Xie et al . , 2022 ; Rajkumar   et al . , 2022 ) , which can be leveraged to save us the   efforts for client - specific model designs . Specifi-   cally , we adopt T5 - base as our backbone seman-   tic parser in the FL paradigm , and conduct exten-   sive experiments and analysis using three widely-   adopted FL algorithms : FedAvg ( McMahan et al . ,   2017 ) , FedOPT ( Reddi et al . , 2020 ) and FedProx   ( Li et al . , 2020a ) .   As standard FL algorithms suffer from the high   client heterogeneity in our realistic setup , we fur-   ther propose a novel re - weighting mechanism for   combining the gradient updates from each client   during the global model update . The high - level   idea is shown in Figure 1 . Our intuition is that , for   each client , the reduction of training loss during   each round can signalize how far the current global   model is away from the local optimum . By giv-   ing larger weights to those clients that have larger   training loss reduction , the global model update   can accommodate those clients better , thus miti-   gating potential performance degradation caused   by high heterogeneity . We formulate this intuition   as a re - weighting factor to adjust how much each   client should contribute to the global model update   during each round . Our proposed mechanism can   be applied to all the three FL algorithms and exper-   iments show that it can substantially improve both   their parsing performance and their convergence   speed , despite being very simple .   In summary , our main contributions are :   •To the best of our knowledge , we are the first   to study federated learning for semantic pars-   ing , a promising paradigm for multiple institu-   tions to collaboratively build natural language   interfaces without data sharing , which is es-   pecially beneficial for institutions with little   training data .   •We propose an evaluation setup to simu-   late a realistic heterogeneous FL setting   where different participating institutions have   very different data . We re - purpose eight   single - domain text - to - SQL datasets as eight   clients , which demonstrate high heterogene-   ity in terms of dataset sizes , language usage ,   database structures , and SQL complexity .   •We propose a novel re - weighting mechanism ,   which uses the training loss reduction of each   client to adjust its contribution to the global   model update during each round . Experi-   ments show that our re - weighting mechanism   can substantially improve the model perfor-   mance of existing FL algorithms on average ,   and clients with smaller training data observe   larger performance gains . We discuss the limi-   tations of our work and encourage future work   to further study this task .   2 Motivation and Task Formulation   Semantic parsing aims to translate natural language   utterances into formal meaning representations and   has numerous applications in building natural lan-   guage interfaces that enable users to query data and   invoke services without programming . As many   institutions often lack data to develop neural se-   mantic parsers by themselves , we propose a fed-   erated learning paradigm , where clients ( i.e. , “ in-   stitutions ” ) collaboratively train a global semantic   parsing model without sharing their data .   There are two realistic settings of FL : cross-   silo setting and cross - device setting ( Kairouz et al . ,12150   2021 ; Lin et al . , 2022 ) . For the cross - silo setting ,   clients are large institutions , such as hospitals and   companies , and the number of clients is limited   in this setting . In general , they have large com-   putational resources and storage to train and store   a large model , and large communication costs be-   tween the server and clients are tolerated . For the   cross - device setting , clients are small devices such   as mobile phones and Raspberry Pis , thus there   may exist a huge number of clients . They have   limited computational resources and storage and   only small communication costs between the server   and clients are affordable . Here our FL for seman-   tic parsing can be regarded as a cross - silo setting ,   where each client is a relatively large institution that   hopes to build a natural language interface based   on its user utterances and underlying data . Study-   ing FL for semantic parsing under a cross - device   setting could be interesting future work .   3 Evaluation Setup   As we are the first to study cross - silo FL for se-   mantic parsing , there is no benchmark for this   task . Thus we establish an evaluation setup   by re - purposing eight single - domain text - to - SQL   datasets ( Finegan - Dollak et al . , 2018 ) as eight   “ clients ” , which demonstrate high heterogeneity in   terms of dataset sizes , domains , language usage ,   database structures and SQL complexity . Table 1   shows their statistics .   Given a natural language question and the   database schema , text - to - SQL parsing aims to gen-   erate a SQL query . Here the question is a sequence   of tokens and the database schema consists of mul-   tiple tables with each table containing multiple   columns . Figure 7 in Appendix shows an example   of this task . We adopt T5 - base as our backbonemodel , which has been shown as an effective uni-   fied model for various semantic parsing tasks ( Xie   et al . , 2022 ) . Similarly as in previous work ( Xie   et al . , 2022 ) , we concatenate the question tokens   with the serialized relational table schemas ( table   names and column names ) as the model input and   output a sequence of SQL tokens .   The heterogeneity of the eight clients is de-   scribed in detail from the following perspectives .   Domain : The clients are from diverse domains .   Some clients such as Scholar and Academic are   from closer domains than others .   Dataset Size : The clients differ significantly in   terms of dataset sizes . Here , we consider datasets   with more than 1000 train examples as large - sized   datasets , with 200 ∼1000 as medium - sized datasets ,   and with less than 200 as small - sized datasets . In   our setup , we have 2 large - sized clients ( Advis-   ing and ATIS ) , 3 medium - sized clients ( Geoquery ,   Restaurants and Scholar ) , and 3 small - sized clients   ( Academic , IMDB and Yelp ) .   Diversity : “ SQL pattern count ” shows the num-   ber of SQL patterns in the full dataset . The pat-   terns are abstracted from the SQL queries with   specific table names , column names and variables   anonymized . A larger value under this measure   indicates greater diversity . In our benchmark , Ad-   vising , ATIS and Scholar have larger diversity than   the other datasets .   Redundancy : “ Questions per unique SQL   query ” counts how many natural language ques-   tions can be translated into the same SQL query   ( where variables are anonymized ) . A larger value   indicates higher redundancy in the dataset . Intu-   itively , the higher the redundancy , the more easily a   model can make correct predictions . In our bench-   mark , the redundancy for Advising and Restaurants12151is higher than the other datasets .   Complexity : “ Unique tables per SQL query ”   ( where variables in the SQL query are anonymized )   represents how many unique tables are mentioned   in one query . “ SELECT s per query ” counts how   many SELECT clauses are included in one query .   The larger these two measures , the more complex   the dataset is and the more difficult for a model to   make predictions . In our benchmark , Advising and   ATIS are more complex .   4 FL for Semantic Parsing   In this section , we first introduce the background of   FL , more specifically , its training objective , train-   ing procedure and three widely adopted FL algo-   rithms . Then we describe the motivating insights   and details of our proposed mechanism .   4.1 Background   Training Objective . Federated learning aims to   optimize the following objective function :   minF(w ) : = /summationdisplaypL(w )   where L(w ) = E[f(w , b)].(1 )   In Eqn . ( 1),L(w)denotes the local training ob-   jective function of the client iandNdenotes the   number of clients . w∈ Rrepresents the param-   eters of the global model . bdenotes each batch of   data . The local training loss function f(w , b)is   often the same across all the clients , while Dde-   notes the distribution of the local client data , which   is often different across the clients , capturing the   heterogeneity . pis defined as the training size pro-   portion in Eqn . ( 2 ) , where |D|is the training size   of client i.   p=|D|//summationdisplay|D| ( 2 )   Training Procedure . Federated learning is an iter-   ative process shown in Figure 2 . The server initial-   izes the global model , followed by multiple com-   munication rounds between the server and clients .   In each communication round , there are four steps   between the server and clients . 1 ) In round t , the   server sends the global model wto all the clients .   2 ) After clients receive the global model was the   initialization of the local model , they start to train   it using their own data for multiple epochs and ob-   tain the local model changes ∆wduring the local   training stage . 3 ) The clients send their local model   changes to the server . 4 ) The server aggregates the   local model changes ∆wcollected from different   clients as Eqn . ( 3)shows , and then uses the t - th   round ’s global model wand the aggregated local   model changes ∆wto update the global model .   As Eqn . ( 4)shows , wis the global model after   the update . Here , ηdenotes the server learning rate .   The server will send the updated model wto the   clients , then the ( t+1)-thround starts .   The above procedure will repeat until the algo-   rithm converges .   ∆w=/summationdisplayp∆w ( 3 )   w = w−η∆w(4 )   FL Algorithms . We explore three popular FL al-   gorithms for our task :   Federated Averaging ( FedAvg ) ( McMahan et al . ,   2017 ) uses stochastic gradient descent ( SGD ) as   the local training optimizer to optimize the training   procedure and uses the same learning rate and the   same number of local training epochs for all the   clients .   FedOPT ( Reddi et al . , 2020 ) is a generalized   version of FedAvg . The algorithm is parameter-   ized by two gradient - based optimizers : CLIEN-   TOPT and SERVEROPT . CLIENTOPT is used to   update the local models on the client side , while   SERVEROPT treats the negative of aggregated lo-   cal changes " −∆w " as a pseudo - gradient and ap-   plies it to the global model on the server side . Fe-   dOPT allows powerful adaptive optimizers on both12152server side and client side .   FedProx ( Li et al . , 2020a ) tries to tackle the sta-   tistical heterogeneity issue by adding an L2 regu-   larization term , which constrains the local model to   be closer to the local model initialization ( i.e. , the   global model ) during each round for stable training .   To summarize , for the local training stage , both   FedAvg and FedOPT optimize the local training ob-   jective f(w , b ) ; for FedProx , it optimizes Eqn . ( 5 ) ,   where µis a hyperparameter and wis the local   model initialization ( i.e. , the global model ) during   thet - thround .   minh(w , b , w ) : = f(w , b ) + µ   2∥w−w∥   ( 5 )   For the cross - silo setting where all clients partic-   ipate in training for each round , these three algo-   rithms optimize Eqn . ( 1 ) during the FL process .   4.2 Our Proposed Re - weighting Mechanism   Motivating Insights . Heterogeneity , where the   data distributions and dataset sizes on different   clients are different , is recognized as one of the   biggest challenges in FL , which usually leads to   performance degradation for clients . Here , we   uniquely observe the clients ’ heterogeneity from   the perspective of their training loss reduction .   Take Restaurants and Yelp as two example clients .   Figure 3 shows their training loss variation w.r.t .   " Step " . Here the " Step " is the number of itera-   tion steps for each client during training . Adjacent   high and low points in the figure correspond to   one communication round . When the curve goes   down , it means the client is in the local training   stage . When the curve goes up , it means the server   has updated the global model based on the aggre-   gated local model changes from all clients and each   client starts a new round of local training with the   updated global model as the local model initializa-   tion . Since for different clients , the dataset sizes   and the local training epochs are different , for the   same communication round , the " Step " for differ-   ent clients is different .   As we can see , after each round , the global   model deviates from the optimization trajectory   of each client . Thus the reduction of the train-   ing loss can signalize how far the global model   is away from the client ’s local optimum . As p   decides how much each client contributes to the   global model update , we give larger weights to   those clients who have larger training loss reduc-   tion to make the global model update accommodate   them better , thus mitigating potential performance   degradation caused by high heterogeneity .   Proposed Mechanism . Based on the above in-   sights , we use the training loss reduction to adjust   the weight of each client , so as to reschedule its   contribution to the global model update . The final   weight is formulated as Eqn . ( 6 ) , where ∆Lis the   training loss reduction during the t - thround .   p=|D|∆L//summationdisplay|D|∆L ( 6 )   FedAvg , FedOPT , FedreProx with our proposed   mechanism are summarized in Algorithm 1 in Ap-   pendix .   5 Experiments   Datasets . We re - purpose eight datasets : ATIS   ( Srinivasan Iyer and Zettlemoyer , 2017 ; Debo-   rah A. Dahl and Shriber , 1994 ) , GeoQuery ( Srini-   vasan Iyer and Zettlemoyer , 2017 ; Zelle and   Mooney , 1996 ) , Restaurants ( Tang and Mooney ,   2000 ; Ana - Maria Popescu and Kautz , 2003 ; Gior-   dani and Moschitti , 2012 ) , Scholar ( Srinivasan Iyer   and Zettlemoyer , 2017 ) , Academic ( Li and Ja-   gadish , 2014 ) , Advising ( Finegan - Dollak et al . ,   2018 ) , Yelp and IMDB ( Navid Yaghmazadeh and   Dillig , 2017 ) as eight clients . These datasets   have been standardized to the same SQL style by   Finegan - Dollak et al . ( 2018 ) . Their characteris-   tics have been described in Section 3 . We follow   " question split " datasets preprocessed by Finegan-   Dollak et al . ( 2018 ) to split the train , dev and test   data , which means we let the train , dev and test ex-   amples have different questions but the same SQL   queries are allowed . For Advising , ATIS , Geo-   Query and Scholar , we directly use the original   question split as our split . For Restaurants , Aca-   demic , IMDB and Yelp , since the data sizes are12153relatively small , the original question split uses 10   splits for cross validation without specifying train ,   dev and test examples . Given FL is costly as we   need multiple GPUs to finish one experiment , we   fix the train , dev and test set by randomly selecting   6 splits as the train set , 2 splits as the dev set and 2   splits as the test set .   Evaluation Metrics . 1 ) Exact Match ( EM ): a   prediction is deemed correct only if it is exactly   the same as the ground truth ( i.e. , exact string   match ) , which is widely used for text - to - SQL pars-   ing ( Finegan - Dollak et al . , 2018 ) . All the evalua-   tions in our experiments consider the values gen-   erated in the SQL query . 2 ) MacroAvg : The arith-   metic mean of EM across all clients , which treats   each client equally . 3 ) MicroAvg : The total num-   ber of correct predictions on all the clients divided   by the total test examples , which treats each test   example equally .   Learning Paradigm . We compare three learn-   ing paradigms : finetuning , centralized and FL . 1 )   Finetuning : we individually finetune our backbone   model ( T5 - base ) on the training data of each client .   2)Centralized : we merge the training data of all   the clients and finetune our backbone model on the   merged training data to obtain one model . 3 ) FL :   we leverage eight clients and a server to learn a   global model without sharing each client ’s local   data . By comparing individual finetuning and FL ,   we can show the benefit of FL for some clients ,   especially for small - sized clients . The centralized   paradigm is less practical compared with the other   two paradigms due to privacy considerations . How-   ever , it can serve as a useful reference to help vali-   date how effective an FL algorithm is in fully ex-   ploiting heterogeneous data across multiple clients .   Implementation Details . We implement the FL   algorithms and T5 - base model based on FedNLP   ( Lin et al . , 2022 ) , FedML ( He et al . , 2020 ) and   UnifiedSKG ( Xie et al . , 2022 ) . We use Adafac-   tor ( Shazeer and Stern , 2018 ) as the optimizer for   finetuning and centralized paradigms , and as the   client optimizerfor FL paradigm , since it has been   shown as the best optimizer to optimize the T5   model . More details are in Appendix A.1 .   For the computing resources , we use 1 NVIDIA   A6000 48 GB GPU for finetuning , with batch size 8 .   We use 2 NVIDIA A6000 48 GB GPUs for central - ized training , with batch size 8 . We use 5 NVIDIA   A6000 48 GB GPUs for all federated learning exper-   iments . Specifically , one GPU is used as the server   and the other four GPUs are used as 8 clients , with   each GPU accommodating 2 clients . The batch   size for clients GeoQuery , Restaurants , Scholar ,   Academic , IMDB and Yelp is 4 , and for clients   Advising and ATIS is 8 .   6 Results and Analysis   6.1 Main Results   Centralized vs. Finetuning . As Table 2 shows ,   compared with the individual finetuning setting ,   the model performance under the centralized set-   ting has been improved on all the datasets except   Scholar . This means merging all the data to train   a model , which increases the size and diversity of   training data , can improve the model ’s general-   ization ability and lead to improvement for most   datasets . This observation also motivates us to   leverage these datasets to study FL for semantic   parsing , which is a more practical paradigm than   the centralized one .   Effectiveness of Lorar in FL . Applying our pro-   posed Lorar mechanism can substantially im-   prove the performance of all three FL algorithms   overall . As Table 2 shows , for FedOPT , our pro-   posed FedOPT performs substantially bet-   ter or similarly on all clients , except for a slight   drop on GeoQuery and Scholar . Moreover , on the   three smaller datasets : Academic , IMDB and Yelp ,   Lorar brings much larger performance gains . For   FedAvg and FedProx , in addition to these three   datasets , Lorar also brings substantial improve-   ments on two medium - sized clients : Restaurants   and Scholar . These observations validate the effec-   tiveness of our proposed mechanism under differ-   ent FL algorithms and across different clients .   We additionally analyze these three FL algo-   rithms and their performance variation with and   without using Lorar under different communica-   tion rounds . More details are included in Appendix   A.2 and A.3 .   FL vs. Finetuning / Centralized . As Table 2 shows ,   the original FedOPT outperforms finetuning on   GeoQuery and IMDB , which shows that FL can   boost the model performance for some clients . In   addition , although there is still a gap between ex-   isting FL algorithms ( FedOPT , FedAvg , and Fed-   Prox ) and the centralized setting , by equipping   them with our proposed Lorar , we can reduce the12154   gap by 4 - 20 points ( i.e. , absolute difference under   MacroAvg ) . It is worth noting that institutions are   often reluctant or prohibited to share their data in   practice , especially for SQL data that may directly   reveal private database content . Therefore , the cen-   tralized paradigm is impractical . Nonetheless , it   can serve as a useful reference to help validate how   effective an FL algorithm is in fully exploiting het-   erogeneous data across multiple clients . The results   show that our benchmark provides a challenging   testbed for a realistic FL problem , and there is still   a large room to further improve the FL algorithms .   6.2 Training Loss Analysis   To better understand how Lorar affects the train-   ing process in FL , we show the training loss varia-   tion for FedOPT and FedOPT in Figure 4 . For   FedOPT , we can see for larger datasets such as Ad-   vising and ATIS , the training converges much faster   and the global model is closer to the client ’s local   optimum within very few rounds . While for smaller   datasets such as Academic , IMDB and Yelp , thetraining loss oscillates widely , which means the   global model converges slower for these clients ( if   at all ) . After applying Lorar , however , the train-   ing loss converges faster on almost all the clients ,   which means the global model can get close to the   client ’s local optimum more quickly and easily .   6.3 Alternative Weighting Mechanisms   As FedOPT performs best among all three FL base-   lines , we use it to compare Lorar with alternative   weighting mechanisms . As Table 3 shows , Lorar ,   which considers both the training set size and the   loss reduction in the weight , can achieve the best   results . Comparing FedOPT(i.e . , FedOPT with   only loss reduction considered in the weight ) and   FedOPT , we can see removing the training   set size from the weight will lead to a large drop   under MacroAvg and MicroAvg , which indicates   that training set size is an important factor during   the aggregation . This is intuitive since for those   clients which have more training data , their local   models tend to be more reliable and more general-12155   izable . We also compare with FedOPT where   all clients are given the same weight . We can see   that our FedOPT yields superior performance .   The conclusion can also be verified in Figure 6 in   Appendix , where we show their performance varia-   tion under different communication rounds .   6.4 Impact from Dataset Heterogeneity   ( 1 ) The impact of diversity , redundancy and com-   plexity : In Table 2 and 3 , for Restaurants , the re-   sults of finetuning , centralized training , and varying   weighting mechanisms of FedOPT are pretty close   and all very high ( close to 100 % ) , which shows it is   a relatively easy dataset for any learning paradigm   and weighting mechanism . Looking at Table 1 ,   Restaurants has the smallest “ SQL pattern count ”   ( i.e. , lowest diversity ) , second largest “ Questions   per unique SQL query ” ( i.e. , second highest re-   dundancy ) , close to the smallest “ Unique tables   per query ” and “ SELECTs per query ” ( i.e. , close   to lowest complexity ) , which makes models eas-   ily learn from this dataset ( Section 3 ) . For other   datasets , they have higher diversity , lower redun-   dancy , or higher complexity , which makes models   harder to make predictions and the performance   is generally lower than Restaurants . ( 2 ) The im-   pact of dataset size : Smaller datasets tend to have   lower performance , as shown in Table 2 , which   means they are harder to learn in general due to   lack of data ; however , they can benefit more from   our proposed FL paradigm .   7 Related Work   Text - to - SQL . Text - to - SQL problem which trans-   lates natural language questions to SQL queries has   been studied for many years . There have been sev-   eral single - database text - to - SQL datasets such as   Geoquery ( Srinivasan Iyer and Zettlemoyer , 2017 )   and ATIS ( Srinivasan Iyer and Zettlemoyer , 2017 ) ,   which map from natural language questions to SQL   queries on a single database . Finegan - Dollak et al . ,2018 curate eight datasets to unify their SQL for-   mat . These datasets cover a variety of domains   and have different characteristics of the tables and   SQL , which provide us a foundation to study the   heterogeneous FL for the text - to - SQL problem .   One line of work designs special models for   the text - to - SQL task such as designing a relation-   aware self - attention mechanism for the Trans-   former model to better encode the relation of the   column mappings ( Wang et al . , 2020a ) or adding   constraints to the decoder ( Scholak et al . , 2021 )   to generate valid SQL queries , while another line   of work tries to directly finetune a pre - trained lan-   guage model such as T5 ( Xie et al . , 2022 ; Raffel   et al . , 2020 ; Rajkumar et al . , 2022 ) . As directly   finetuning T5 has shown great performance and   allows us to use a unified model architecture for   all clients and the server , we choose T5 - base as the   backbone model in our work .   Heterogeneity in Federated Learning . Hetero-   geneity is one of the major challenges in federated   learning . Existing work ( McMahan et al . , 2017 ;   Reddi et al . , 2020 ; Li et al . , 2020a , 2021 ; Shoham   et al . , 2019 ; T Dinh et al . , 2020 ; Li et al . , 2022 )   shows that heterogeneity can cause performance   degradation . Several methods have been proposed   to address this issue . For instance , FedOPT ( Reddi   et al . , 2020 ) uses powerful adaptive optimization   methods for both the server and clients , while   FedProx ( Li et al . , 2020a ) ( and pFedMe ( T Dinh   et al . , 2020 ) ) regularizes the local training proce-   dure . However , based on our observations in Sec-   tion 6.1 , our mechanism significantly outperforms   these methods . Other work that aims to address   the heterogeneity issue in FL includes FedNova   ( Wang et al . , 2020b ) and Li et al . , 2020b . Specifi-   cally , FedNova ( Wang et al . , 2020b ) uses the local   training update steps to normalize the server aggre-   gation , and Li et al . , 2020b proposes to optimize   the power - scaled training objective . Compared to   FedNova , we use a more direct indicator , training12156loss reduction , to adjust the weight for each client   during aggregation . Different from Li et al . , 2020b ,   our proposed simple yet effective mechanism does   not require modification of the local client opti-   mization step or additional tuning of any related   hyperparameter .   8 Conclusions   To the best of our knowledge , we are the first   to study federated learning for semantic parsing .   Specifically , we propose a realistic benchmark   by re - purposing eight single - domain text - to - SQL   datasets . Moreover , we propose a novel loss reduc-   tion adjusted re - weighting mechanism ( Lorar )   that is applicable to widely adopted FL algorithms .   By applying Lorar to FedAvg , FedOPT and Fed-   Prox , we observe their performance can be im-   proved substantially on average , and clients with   smaller datasets enjoy larger performance gains .   Limitations   In this work , we address the heterogeneity chal-   lenge in the task of FL for semantic parsing , by   leveraging the reduction of training loss signal .   Our work is motivated from the FL training proce-   dure perspective to adjust the contribution of each   client during the global model aggregation stage ,   but how each client ’s data contribute to the final   global model is still unclear . As the data of differ-   ent clients contain different information , what kind   of information of each client is helpful and can be   more directly linked and utilized to facilitate the   FL training is worth more efforts in future work .   In addition , our proposed re - weighting mech-   anism is a universal technique for cross - silo FL .   Thus generalizing our proposed re - weighting mech-   anism to a broader range of tasks beyond semantic   parsing , and further studying under what kind of   conditions , Lorar can make a huge difference for   FL would be interesting future work to pursue .   Acknowledgements   The authors would like to thank colleagues from   the OSU NLP group and all anonymous review-   ers for their thoughtful comments . This research   was supported in part by NSF OAC 2112606 , NSF   IIS 1815674 , NSF CAREER 1942980 , and Ohio   Supercomputer Center ( Center , 1987 ) . The work   done at IBM research was sponsored by the Com-   bat Capabilities Development Command Army Re-   search Laboratory and was accomplished underCooperative Agreement Number W911NF-13 - 2-   0045 ( ARL Cyber Security CRA ) . The views and   conclusions contained in this document are those   of the authors and should not be interpreted as rep-   resenting the official policies , either expressed or   implied , of the Combat Capabilities Development   Command Army Research Laboratory or the U.S.   Government . The U.S. Government is authorized   to reproduce and distribute reprints for Government   purposes notwithstanding any copyright notation   here on . We thank Chaoyang He for his help during   reproducing FedNLP . We thank Wei - Lun ( Harry )   Chao for valuable discussion .   References1215712158A Appendix   Algorithm 1 :   Input : local datasets D , number of   communication rounds T , number   of local epochs E , server learning   rateη , client learning rate η   Output : the final global model wServer executes : fort∈0,1,2 , ... , T do Sample a set of clients C fori∈Cin parallel do Send the global model wto client i ∆w,|D|∆L   ←LocalTraining ( i , w ) ∆w=/summationtextp∆w For FedOPT / FedAvg / FedProx :   p=|D|//summationtext|D| For ours ( Lorar ):   p=|D|∆L//summationtext|D|∆L w←w−η∆wreturn wClient executes : FedAvg / FedOPT :   L(w;b ) = /summationtextf(w;x;y)FedProx : L(w;b ) = /summationtextf(w;x;y ) + ∥w−w∥LocalTraining ( i , w)w←wforepoch k= 0,1,2 , ... , E do foreach batch b={x , y}ofDdo w←w−η∇L(w;b)∆w←w−w∆L←maxL−minLreturn ∆w,|D|∆Lto the server   A.1 Implementation Details   We use T5 - base ( Raffel et al . , 2020 ) as the model   for text - to - SQL task in all three learning paradigms   ( finetuning , centralized and FL ) , as it has been   shown as an effective unified model for various   semantic parsing tasks in UnifiedSKG ( Xie et al . ,   2022 ) . For all three FL algorithms , we imple-   ment them based on FedNLP ( Lin et al . , 2022 )   and FedML ( He et al . , 2020 ) . We use Adafac-   tor ( Shazeer and Stern , 2018 ) as the optimizer for12159finetuning and centralized paradigms , and as the   client optimizerfor FL paradigm , since it has been   shown as the best optimizer to optimize for the T5   model .   For the FL paradigm , we tune hyperparameters   for FedOPT , FedAvg and FedProx as follows . For   FedOPT , we test all the combinations of the server   learning rate from { 0.001 , 0.01 0.1 , 0.5 , 1 } and { w/   0.9 , w/o } server momentum . We found 1 as the   server learning rate and 0.9 as the server momen-   tum is the best hyperparameter combination . For   FedProx , we vary µfrom { 0.0001 , 0.001 , 0.01 , 0.1 ,   1 } and use the dev set to choose the best model .   We finally choose the best hyperparameter 0.0001   in our experiment . For all the federated learning   paradigms , we set local training epochs as 6 for   two large datasets : ATIS and Advising . We set   the local training epoch as 12 for all the other six   datasets . We let all the clients participate in each   round and we train the entire process for 60 rounds   ( which lasts around 60 hours ) . And we test the   global model performance on the merged dev set   for every 5 communication rounds to choose the   best model . We use the best global model to eval-   uate on all eight test sets to get the global model   performance on each client .   For the finetuning paradigm , we finetune T5-   base on each dataset for a maximum of 200 epochs .   We use the dev set of each client to choose the best   model and then evaluate the model on each test set .   For the centralized paradigm , we merge all eight   training sets and then finetune T5 - base for a max-   imum of 200 epochs on the merged dataset to get   one centralized model . We merge all eight dev   sets and use the merged dev set to choose the best   model . Then we evaluate the centralized model on   each test set .   For all finetuning , centralized and federated   learning paradigms , we set the input length as 1024   and the output length as 512 . We try learning rate in   { 1e-5 , 1e-4 , 1e-3 } . We finally choose 1e-4 for the   centralized paradigm , and 1e-4 for Advising , ATIS ,   Geoquery and Yelp in the finetuning paradigm and   FL paradigm . We use 1e-3 for Restaurants , Scholar ,   Academic and IMDB in the finetuning paradigm   and FL paradigm .   For the computing resources , we use 1 NVIDIA   A6000 48 GB GPU for finetuning , with batch size 8.We use 2 NVIDIA A6000 48 GB GPUs for central-   ized training , with batch size 8 . We use 5 NVIDIA   A6000 48 GB GPUs for all federated learning exper-   iments . Specifically , one GPU is used as the server   and the other four GPUs are used as 8 clients , with   each GPU accommodating 2 clients . The batch   size for clients GeoQuery , Restaurants , Scholar ,   Academic , IMDB and Yelp is 4 , and for clients   Advising and ATIS is 8 .   A.2 Comparison of FL Baselines .   We treat FedAvg , FedOPT and FedProx as our FL   baselines . As Figure 5 shows , among FedAvg , Fe-   dOPT and FedProx , FedOPT performs the best ,   achieving the closest performance to the central-   ized paradigm and the fastest convergence speed .   FedAvg and FedProx have similar performances ,   and both of them have a large gap with FedOPT .   This indicates that the server ’s adaptive optimizer   which only exists in FedOPT plays an important   role to improve the performance .   A.3 Performance Variation under Varying   Communication Rounds .   In Figure 5 , comparing the performance of FL base-   lines with ours , FedOPT performs slightly   better than FedOPT . We hypothesize the small gap   between FedOPT and the centralized paradigm lim-   its the room for Lorar to show a large gain over   FedOPT . For FedAvg and FedProx , we can see   that applying Lorar performs significantly better ,   which demonstrates the effectiveness of leveraging   the loss reduction to adjust the weights.1216012161ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   " Limitations " Section   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   " Abstract " Section and " Introduction " Section ( 1 )   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   " Evaluation Setup " ( Section 3 )   /squareB1 . Did you cite the creators of artifacts you used ?   " Experiments " ( Section 5 )   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   " Evaluation Setup " ( Section 3 )   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   " Evaluation Setup " ( Section 3 )   C / squareDid you run computational experiments ?   " Implementation Details " ( Section 5 )   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   " Implementation Details " ( Section 5 and Appendix)12162 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   " Implementation Details " ( Section 5 and Appendix )   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   " Implementation Details " ( Section 5 and Appendix ) , " Results " ( Section 6 )   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   " Implementation Details " ( Section 5 and Appendix )   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.12163