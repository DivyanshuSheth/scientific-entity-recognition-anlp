  Rishabh Jain   University of Cambridge   Cambridge , UKGabriele Ciravegna   Université Côte d’Azur , Inria ,   CNRS , I3S , Maasai , Nice , FrancePietro Barbiero   University of Cambridge   Cambridge , UK   Francesco Giannini   University of Siena   Siena , ItalyDavide Buffelli   University of Cambridge   Cambridge , UKPietro Lio   University of Cambridge   Cambridge , UK   Abstract   Recently , Logic Explained Networks ( LENs )   have been proposed as explainable - by - design   neural models providing logic explanations for   their predictions . However , these models have   only been applied to vision and tabular data ,   and they mostly favour the generation of global   explanations , while local ones tend to be noisy   and verbose . For these reasons , we propose   LEN , improving local explanations by perturb-   ing input words , and we test it on text classiﬁ-   cation . Our results show that ( i ) LENprovides   better local explanations than LIME in terms of   sensitivity and faithfulness , and ( ii ) logic expla-   nations are more useful and user - friendly than   feature scoring provided by LIME as attested   by a human survey .   1 Introduction   The development of Deep Neural Networks has   enabled the creation of high accuracy text clas-   siﬁers ( LeCun et al . , 2015 ) with state - of - the - art   models leveraging different forms of architectures ,   like RNNs ( GRU , LSTM ) ( Minaee et al . , 2020 ) or   Transformer models ( Vaswani et al . , 2017 ) . How-   ever , these architectures are considered as black-   box models ( Adadi and Berrada , 2018 ) , since their   decision processes are not easy to explain and de-   pend on a very large set of parameters . In or-   der to shed light on neural models ’ decision pro-   cesses , eXplainable Artiﬁcial Intelligence ( XAI )   techniques attempt to understand text attribution   to certain classes , for instance by using white - box   models . Interpretable - by - design models engender   higher trust in human users with respect to explana-   tion methods for black - boxes , at the cost , however ,   of lower prediction performance .   Recently , Ciravegna et al . ( 2021 ) and Barbi-   ero et al . ( 2022 ) introduced the Logic Explained   Network ( LEN ) , an explainable - by - design neural   network combining interpretability of white - boxmodels with high performance of neural networks .   However , the authors only compared LENs with   white - box models and on tabular / computer vision   tasks . For this reason , in this work we apply an im-   proved version of the LEN ( LEN ) to the text clas-   siﬁcation problem , and we compare it with LIME   ( Ribeiro et al . , 2016 ) , a very - well known expla-   nation method . LEN and LIME provide different   kind of explanations , respectively FOL formulae   and feature - importance vectors , and we assess their   user - friendliness by means of a user - study . As an   evaluation benchmark , we considered Multi - Label   Text Classiﬁcation for the tag classiﬁcation task on   the“StackSample : 10 % of Stack Overﬂow Q&A ”   dataset ( Overﬂow , 2019 ) .   Contribution The paper aims to : ( i ) improve   LEN explanation algorithm ( LEN ) ; ( ii ) compare   the faithfulness and the sensitivity of the explana-   tions provided by LENs and LIME ; ( iii ) assess the   user - friendliness of the two kinds of explanations .   2 Background   Explainable AI Explainable AI ( XAI ) algo-   rithms describe the rationale behind the decision   process of AI models in a way that can be under-   stood by humans . Explainability is essential in in-   creasing the trust in the AI model decisions , as well   as in providing the social right to explanation to   end users ( Selbst and Powles , 2017 ) , especially in   safety - critical domains . Common methods include   LIME ( Ribeiro et al . , 2016 ) , SHAP ( Lundberg and   Lee,2017 ) , LORE ( Guidotti et al . , 2018 ) , Anchors   ( Ribeiro et al . , 2018 ) and many others .   LEN The Logic Explained Network ( Ciravegna   et al . ,2021 ) is a novel XAI architectural framework   forming special kind of neural networks that are   explainable - by - design . In particular , LENs impose8838   architectural sparsity to provide explanations for a   given classiﬁcation task . Explanations are in the   form of First - Order Logic ( FOL ) formulas approxi-   mating the behaviour of the whole network . A LEN   fis a mapping from [ 0,1]-valued input concepts   tor≥1output explanations , that can be used ei-   ther to directly classify data and provide relevant   explanations or to explain an existing black - box   classiﬁer . At test time , a prediction f(x ) = 1 is lo-   cally explained by the conjunction ϕof the most   relevant input features for the class i∈{1 , . . . , r } :   LEN Local Exp . : ϕ(x ) = /logicalanddisplayx(x),(1 )   where x(x)is a logic predicate associated to the   j - th input data andAis the set of relevant input   features for the i - th task . Any x(x)can be either a   positive x(x)or negative¬x(x)literal , according   to a given threshold , e.g. x(x ) = [ x>0.5 ] . In   this work , we consider the LEN proposed in Barbi-   ero et al . ( 2022 ) , where the set of important features   Afor task iis deﬁned asA={x|1≤j≤   d , α≥0.5 } , where αis the importance score   of the j - th feature , computed as the normalized   softmax over the input weights Wconnecting the   j - th input to the ﬁrst network layer ||W|| . Ar-   chitectural sparsity is obtained by minimizing the   entropy of the αdistribution .   For global explanations , LENs consider the dis-   junction of the most important local explanations :   LEN Global Exp . : ϕ=/logicalordisplayϕ,(2)whereBcollects the k - most frequent local ex-   planations of the training set and is computed as   B={ϕ∈arg maxµ(ϕ ) } , where   we indicated with µ(·)the frequency counting op-   erator and with Φthe overall set of local expla-   nations related to the i - th class . In addition to this ,   Ciravegna et al . ( 2021 ) employs a greedy strategy ,   gradually aggregating frequent local explanations   only if they improve the validation accuracy .   3 LEN   3.1 Improving Local Explanation   The LEN algorithm for obtaining local explana-   tions is not precise in determining the contribute of   each feature . A close look at the extraction method   shows that the αscore only highlights the impor-   tance of a feature , without considering the type of   contribution ( either positive or negative ) for the pre-   dicted class . As an example , consider an input text   predicted as referring to C # . The LEN may have   learned that the presence of the word C#leads to   the tag prediction C#and so it has assigned a high   importance value α . However , sometimes we   may not have the word C#in the text and still get   the prediction to be C # . The algorithm proposed   in ( Barbiero et al . , 2022 ) would extract a local ex-   planation with the term ¬C # , as shown in Figure 2 .   This is inaccurate because the absence of C#does   not lead to prediction of the tag C # .   To improve the local explanations of LENs , we   take the most important terms Aand we divide   them into two subsets – the good terms and the bad   terms . The good terms are the ones that actually   lead to the prediction . The bad terms are the ones   despite which we get the given prediction . For each   term , we decide whether it is good or bad by com-   paring the predicted probability of the tag with the   current input and with a perturbed one ( ﬂipping   term presence ) . If the prediction increases with   the perturbation , the term is labelled as a bad term ,   otherwise it is considered a good term . Notice that   the logic sign still comes from the input feature   presence / absence . For ease , we only consider the   conjunction of the good terms as the ﬁnal expla-   nation . Figure 2shows the ability of LENlocal   explanation algorithm to correctly identify that the   prediction is despite the absence of C # . Algorithm   1 in Appendix A , shows the pseudocode for the   LENlocal explanations .   We note that a similar algorithm is used in An-8839chors ( Ribeiro et al . , 2018 ) . However , our approach   can be more effective as we perturb and assess the   importance of both the given input words and of   the ( important ) absent ones . Indeed , Anchors for-   mulae only report positive literals by inspecting   the global behaviour of the model , while we also   provide logic explanations with negative terms .   3.2 Global Explanation   The greedy aggregation technique in Ciravegna   et al . ( 2021 ) may not ﬁnd an optimal solution . The   time complexity of the original aggregation method   isO(k×n ) , since they evaluate the validation accu-   racy of the global formula ( for nsamples ) while ag-   gregating the klocal explanations . However , when   we aggregate a small number of local explanations ,   i.e.kis small , we can afford a more effective but   slower solution . Straightforwardly , we compute   the disjunctions of all the possible combinations   of local explanations ( power set ) , incurring in a   O(2×n)time complexity , but ﬁnding an optimal   solution , i.e. the one reaching the higher valida-   tion accuracy . Note that to keep the explanations   short and easy to interpret , normally kis very small ,   between 3and10 . In Appendix A , Algorithm 2   shows the improved LENaggregation method .   4 Experiments   In the experimental section , we show that ( i ) LEN   improves LEN explanations and provides better   explanations than LIME in terms of faithfulness ,   sensitivity and capability to detect biased - model   ( Section 4.1 ) and ( ii ) a human study conﬁrms this   result , in particular when considering the global ex-   planation ( Section 4.2 ) . Furthermore , in Appendix   B , we conﬁrm that LENs achieve competitive per-   formance when employed as explainable - by - design   classiﬁer w.r.t . black - box models . Appendix Ccon-   tains experimental details .   4.1 Explanation Comparison   To assess the quality of the explanations of the   proposed method ( LEN ) , we compared it with the   original LEN algorithm ( LEN ) , a version of LIME   with discretized input ( LIME ( D ) ) , and a version of   LIME with non - discretized input ( LIME ( ND ) ) .   We compare the different strategies by explain-   ing a common black - box Random Forest model .   Due to the high computational complexity required   to explain each of the ∼15 K tags ( reduced from   the initial 37 K tags , after retaining only impor-   tant questions ) , we compare the local explanations   overs three tags only , namely “ C # ” , “ Java ” and   “ Python ” . The hyperparameters of each method   were chosen to get the best results while keeping   the computational time to be at most 15 minutes .   LENprovides faithful explanations The faith-   fulness of an explanation to a model refers to how   accurate the explanation is in describing the model   decision process . To evaluate the faithfulness , we   use the Area Under the Most Relevant First Pertur-   bation Curve ( AUC - MoRF ) . The lesser the value of   AUC - MoRF , the more faithful is the explanation to   the model . We calculate the AUC - MoRF for each   strategy , considering the local explanation over 100   samples .   Table 1reports the average AUC - MoRF for the   different explanation strategies . The LENpro-   vides more faithful explanations than all the com-   petitors by a considerable margin . On the contrary ,   the original LEN explanations are slightly less faith-   ful than LIME ( D ) and LIME ( ND).8840   LENexplanations are robust to perturbations   The sensitivity of an explanation refers to the ten-   dency of the explanation to change with minor   changes to the input . In general , a robust explana-   tion is not affected by small random perturbations ,   since we expect similar inputs to have similar expla-   nations . Therefore , low sensitivity values are desir-   able and we measure the Max - Sensitivity . For more   details about the metric , please refer to Appendix   D. Table 2report the average Max - Sensitivity eval-   uated over 100 randomly selected inputs and per-   forming 10 random perturbations xper input x ,   with maximum radius ϵ=||x−x||= 0.02 . We   see that explanations from both LEN and LEN   have 0.0Max - Sensitivity , i.e. , they remain un-   changed by all minor perturbations to the input ,   greatly outperforming the explanations from LIME .   This is expected because LEN trains the model   once over all the training data and tries to act as a   surrogate model ; there is no retraining for a new   local explanation . On the other hand , LIME trains   a new linear model for each local explanation and   only try to mimic the explained model on the pre-   dictions near the given input . Clearly , by employ-   ing larger perturbation of the input , LEN explana-   tions would also change .   LENis capable to detect biased - model The   presence of noisy features in the training data may   drive a model to unforeseeable prediction on clean   data at test time . For this reason , it is very im-   portant to detect them before releasing the model .   A way to detect biases is to compute the global   explanation of a model and check whether the ex-   planation is consistent with the domain knowledge .   To this aim , it is very important to employ a pow-   erful explanation algorithm that may be capable   to detect the bias . To evaluate this capability , we   trained a model with the explicit goal of making   it biased . In the training data , we added noisy fea-   tures with a high correlation with certain tags , so   that the model learns to associate the noisy features   with the tag . At test time , these features are added   randomly , i.e. they act like noise . We run these   experiments in two settings , S1 and S2 , varying the   amount of bias towards the noisy features . This is   done by increasing the bias of the noisy features   in training data from 30 % of training data in S1 to   35 % in S2 , and by ensuring a higher difference in   test and validation scores in S2 .   Table 3reports the percentage of times we are   able to detect the use of noisy features using the   global explanations from the different strategies .   LENshows more utility than all the competitors   by a large margin . The results have been averaged   over 20 executions in this setting .   4.2 Human Survey   We carried out a human survey to compare the ease   of understanding and the utility of the explanations   obtained by LIME and LENs . The human sur-   vey was approved by the ethics committee and the   questions do not record personal information . The   survey was shared with students and researchers   over different universities and ﬁlled by 26respon-   dents , 13with experience in Machine Learning , 10   in Computer Science and 3 in neither . The survey   is attached in Appendix E. Figure 3report the ease   of using the explanations for the different task .   LENexplanations are easily interpretable   First , the survey presents the respondents a sample   input with the related prediction and the explana-   tions from LIME and LEN . It then asks the respon-   dents to rate the ease of understanding these local   explanations . The ﬁrst column of Figure 3suggests8841   that the local explanations from LIME and LEN   are almost equally easily understandable since the   95 % conﬁdence intervals have a high overlap ( with   LIME having a slightly higher mean ) .   LENenable users to improve a classiﬁer This   section aims to establish the usefulness of the ex-   plainers in getting a more general classiﬁer through   feature engineering . To this aim , we trained a ra-   dial basis function ( RBF ) SVM to mistakenly learn   to associate the feature ‘ add ’ with the C#tag . This   was done by perturbing the training data and ensur-   ing a wide difference in the validation and testing   scores . We asked respondents to identify the input   feature which does not allow the model to gener-   alize well , by inspecting the global explanation of   SP - LIME and LEN . They were also asked to rate   the ease of using the explanations for this purpose .   As shown in Table 4ﬁrst row , only 61.5 % respon-   dents were able to identify the term ‘ add ’ as the   feature to ignore while using LIME explanations ,   as opposed to LEN84.6 % . In addition , in Figure   3second column , respondents found LENeasier   to use for improving the classiﬁer .   LENallows identifying the best classiﬁers Fi-   nally , we evaluated whether users can choose a   classiﬁer that generalizes better than the other , by   only checking again the global explanations of the   two classiﬁers . To this aim , we trained two RBF   SVMs classiﬁers on different training data ( the sec-   ond one with some noise added ) . As reported in   Table 4 , second row , 73.1 % respondents were able   to identify the more general classiﬁer using LEN   as opposed to LIME 50 % . Moreover , the third col-   umn of Figure 3shows that the global explanations   from LENmake the comparison much easier than   those from SP - LIME .   5 Conclusion   This paper proposes LEN , an improved version of   LENs whose results clearly show that LENexpla - nations outperform both LEN and LIME on differ-   ent metrics ( sensitivity and faithfulness ) . Moreover ,   a user study demonstrated that the logic explana-   tions are more useful than the importance vector   and provide a better user - experience ( particularly   on global explanation ) . This has wide - ranging im-   pact , as LIME is a popular strategy used in vari-   ous ﬁelds ( e.g. , Gramegna and Giudici ( 2021 ) and   Visani et al . ( 2020 ) ) .   6 Limitations   Regarding the aggregation of local explanations ,   the proposed algorithm can be intractable in case k   is not small . To alleviate this issue , we are working   on a selective algorithm to automatically ﬁlter out   the local explanations that are less useful for the   task . Furthermore , since LENs require concepts as   input , we did not consider models taking sequen-   tial input in this work . In future work , we will test   the explanation of the proposed model when ex-   plaining sequential models , making use of concept   extraction from sequential models , like the work   done by Dalvi et al . ( 2022 ) . The backbone itself   of the LEN is an MLP architecture , but it might   be interesting to devise a LEN - version of an RNN   or a Transformer model . The human survey does   represent the target users , as the topic experts for   StackOverﬂow questions are computer scientists .   However , in future work , to better represent the   population of possible users , we aim at expand-   ing the portion of not expert in neither Machine   learning nor Computer Science . Finally , the paper   only compares LEN and LIME explanation on one   dataset , but it might be interesting to broaden the   comparison to include SHAP , LORE and Anchors ,   while considering a variety of datasets .   Acknowledgments   GC acknowledges support from the EU Horizon   2020 project AI4Media , under contract no . 951911 ,   FG is supported by TAILOR , a project funded   by EU Horizon 2020 research and innovation pro-   gramme under GA No 952215 . This work was also   partially supported by HumanE - AI - Net a project   funded by EU Horizon 2020 research and innova-   tion programme under GA 952026 .   References88428843A Algorithms   In this section , we report the local and global ex-   planation methods from LEN . In particular , Al-   gorithm 1reports the pseudocode for improving   the local explanations from the LEN , while Algo-   rithm 2reports the optimal aggregation mechanism   proposed in this paper .   Algorithm 1 : LENLocal ExplanationFunction local_explanation ( model , x ,   target _ class ): exp←Original LEN local explanation org_pred←model ( x ) good _ terms , bad _ terms← [ ] , [ ] forall term∈expdo x←Clone xwith term ﬂipped pert_pred←model ( x ) iforg_pred≤pert_pred then bad_terms .append ( term ) else good _ terms .append ( term ) end if end forall return Conjunction ( good _ terms )   Algorithm 2 : LENglobal explanationFunction aggregate_explanations (   topk _ explanations ): all_comb←   PowerSet ( topk_explanations ) forall exps∈all_comb do cur_exp←/logicalortextexpforexp∈exps accuracy←Calculate accuracy of   cur_exponvalidation _ data ifaccuracy > best _ accthen best_exp←cur_exp best_acc←accuracy end if end forall return best_exp   B Model Evaluation   In this section , we employ the LEN directly as a   classiﬁer , to assess the performance drop required   to employ an explainable - by - design network in-   stead of a black - box one .   Figure 4compares the predicting performance   of the LEN with an MLP and an XGBoost , two   black - box models , in terms of F1 Score , Jaccard   Index , Precision and Recall . Results are averaged   10 times over different test - train splits and model   initializations . We also report the 95 % conﬁdence   intervals .   We observe that XGBoost performs better than   both the LEN model and the MLP in all metrics .   We can also appreciate that the LEN proposed in   ( Barbiero et al . , 2022 ) only slightly decreases the   performance w.r.t . using almost identical MLP . A   higher difference was expected , as in general there   exists a trade - off between the model explainability   and its performance ( Barredo Arrieta et al . , 2019 ) .   These results indicate that the performance of LEN   is good / comparable enough to consider replacing   outperforming black - box models to gain higher   interpretability .   C Experimental details   Hardware All experiments were run on a ma-   chine equipped with an Intel i7 - 8750H CPU , an   NVIDIA GTX 1070 GPU and 16 GB of RAM .   Hyper - parameters The selection was done with   a grid search alongside , to maintain fairness in   comparison , a constraint on the time required to   obtain explanations .   Simulation Experiments The details about the   different settings , S1 and S2 , of the experiment de-   scribed in Section 4.1 , is as follows : In each run of   S1 , we add 2 noisy features . In training data , each   noisy features is added with a 30 % probability of   being added to inputs of tag C # and 5 % probability   to the other tags . In testing data , it is added uni-8844formly added to all tags with 5 % probability . Bias   is ensured by having a threshold difference of 0.03   between the test and validation F1 scores .   S2 follows similarly , where we add 2 noisy fea-   tures , but increase the probability of adding them   to inputs of tag C # in training data from 30 % to   35 % . The threshold difference of F1 scores is also   increased to 0.05 . This is done to get a model that   uses the noisy features with higher importance than   that we get with setting S1 .   D Evaluation of Trust in Explanations   In general , trust in the explanations refers to reli-   ability of the explanations . In this paper , we used   two metrics to measure this trust in the explana-   tions , the AUC - MoRF and the Max - Sensitivity , for   which we reported the details below .   Area Under the Most Relevant First Perturba-   tion Curve Area Under the Most Relevant First   Perturbation Curve ( AUC - MoRF ) ( Kakogeorgiou   and Karantzalos , 2021 ) is a metric based on the   MoRF perturbation curve as proposed by Samek et   al . ( Samek et al . , 2017 ) . MoRF curve is the plot   of prediction from model versus the number of fea-   tures perturbed , where the features are perturbed in   a most relevant ﬁrst order . Thus , AUC - MoRF can   be deﬁned as :   AUC(Φ , f , x ) = /summationdisplayf(y ) + f(y )   2   ( 3 )   Here fis the model being explained , xis an input   vector , Φis an explanation method , Dis the num-   ber of input features and yis the input vector   after the kMoRF perturbation . MoRF perturba-   tions are deﬁned recursively as below :   y = x   ∀1≤k≤D : y = g(y , r(4 )   Here gis a function that takes a vector and an   index , and perturbs the given vector at the given   index , and [ r , r , . . . , r]are the indices of the   input features sorted in descending order of their   relevance , as determined by the explanation Φ.   In our evaluation , we normalize the AUC - MoRF   values to be in the [ 0,1]range , by dividing the   values by D−1when D > 1 . So , the ﬁnal formulaused looks like :   Normalized AUC(Φ , f , x ) =   = 1   ( D−1)/summationdisplayf(y ) + f(y )   2   ( 5 )   A lesser value of AUC - MoRF means a more   faithful explanation , and thus a more trustworthy   explanation .   Max - Sensitivity Sensitivity of an explanation   measures the proneness of the explanation to be   affected by insigniﬁcant perturbations to the input .   Max - Sensitivity is a metric due to Yeh et al . ( Yeh   et al . ,2019 ) which is deﬁned as below :   SENS(Φ , f , x , r ) =   = max∥Φ(f , y)−Φ(f , x)∥(6 )   Here fis the model being explained , xis an input   vector , yis the input vector with some perturba-   tions , ris the max perturbation radius , and Φis an   explanation method , which takes a model and input   vector and gives the explanation .   The lesser the value of this metric , the lesser is   the explanation prone to minor perturbations in the   input , and so more is our trust in the explanation .   E Human Survey   In the following pages , we report a compressed   copy of the human survey for which we reported   the results in Section 4.2 .   In the survey section where we aim to establish   the usefulness of the explainers in getting a more   general classiﬁer , we train a radial basis function   ( RBF ) SVM to mistakenly learn to associate the   feature ‘ add ’ with the C # tag . This was done by ran-   domly adding ( with 50 % probability ) “ add ” only   to the training data labeled with the C # tag . RBF-   SVM was trained on this perturbed data , getting   a 6 % smaller Jaccard Index validation score than   the training one . This difference conﬁrmed that the   model mistakenly learned to associate “ add ” with   C # tag.8845884688478848884988508851885288538854885588568857