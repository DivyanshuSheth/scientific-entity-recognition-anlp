  Yingxiu Zhao , Zhiliang Tian , Huaxiu Yao , Yinhe Zheng , Dongkyu Lee   Yiping Song , Jian Sun , Nevin L. ZhangThe Hong Kong University of Science and Technology , Hong Kong SAR , ChinaStanford University , Alibaba GroupDepartment of Computer Science , Peking University , Beijing , China ,   Abstract   Building models of natural language process-   ing ( NLP ) is challenging in low - resource sce-   narios where only limited data are available .   Optimization - based meta - learning algorithms   achieve promising results in low - resource sce-   narios by adapting a well - generalized model   initialization to handle new tasks . Nonetheless ,   these approaches suffer from the memoriza-   tion overÔ¨Åtting issue , where the model tends to   memorize the meta - training tasks while ignor-   ing support sets when adapting to new tasks .   To address this issue , we propose a memory   imitation meta - learning ( MemIML ) method   that enhances the model ‚Äôs reliance on support   sets for task adaptation . SpeciÔ¨Åcally , we intro-   duce a task - speciÔ¨Åc memory module to store   support set information and construct an im-   itation module to force query sets to imitate   the behaviors of some representative support-   set samples stored in the memory . A theo-   retical analysis is provided to prove the effec-   tiveness of our method , and empirical results   also demonstrate that our method outperforms   competitive baselines on both text classiÔ¨Åca-   tion and generation tasks .   1 Introduction   Building natural language processing ( NLP ) mod-   els in low - resource scenarios is of great importance   in practical applications because labeled data are   scarce . Meta - learning - based methods ( Thrun and   Pratt , 2012 ) have been commonly used in such   scenarios owing to their fast adaptation ability .   Notable successes have been achieved by meta-   learning on low - resource NLP tasks , such as multi-   domain sentiment classiÔ¨Åcation ( Yu et al . , 2018 ;   Geng et al . , 2019 ) and personalized dialogue gen-   eration ( Madotto et al . , 2019 ; Song et al . , 2020 ;   Zheng et al . , 2020 ) .   Among different meta - learning approaches   ( Hospedales et al . , 2021 ) , optimization - based ap - proaches have been widely used in various low-   resource NLP scenarios ( Madotto et al . , 2019 ; Qian   and Yu , 2019 ; Li et al . , 2020 ; Mi et al . , 2019 ) be-   cause they are model - agnostic and easily applica-   ble . Concretely , optimization - based meta - learning   algorithms aim to learn a well - generalized global   model initialization that can quickly adapt to new   tasks within a few steps of gradient updates . In the   meta - training process , we Ô¨Årst train on a support   set(i.e . , a few training samples of a new task i ) to   obtain task - speciÔ¨Åc parameters . Then , we opti-   mizebased on the performance of on a query   set(i.e . , another set of samples in task i ) .   Despite its effectiveness , optimization - based   meta - learning algorithms usually suffer from the   memorization overÔ¨Åtting issue(Yin et al . , 2020 ;   Rajendran et al . , 2020 ) , where the learned model   tends to solve all the meta - training tasks by memo-   rization , rather than learning how to quickly adapt   from one task to another via support sets . This is   acceptable for training process , but results in poor   generalization on the meta - testing sets , because   the memorized model does not have knowledge   of those tasks and does not know how to utilize   the base learner to learn new tasks . Hence , this is-   sue hinders the model from capturing task - speciÔ¨Åc   characteristics from support sets and thus prevents   the model from adapting to distinct new tasks ( Ra-   jendran et al . , 2020 ) . For instance , in personalized   dialogue generation , this implies that the dialog   model can not adapt to individual users based on   short conversation histories and hence fails to gen-   erate personalized responses .   Several works have been proposed to tackle the   memorization overÔ¨Åtting issue for regression and   image classiÔ¨Åcation tasks . Some studies try to ex-   plicitly regularize the model parameters ( Yin et al . ,5832020 ; Rajendran et al . , 2020 ) , but this restricts the   complexity of model initialization and reduces the   model capacity . Another line of research integrates   samples from support sets into the corresponding   query sets via data augmentation ( Yao et al . , 2021 ) .   However , data augmentation on textual data may   result in noisy labels or distribution shifts , which   impairs the model performance ( Chen et al . , 2021 ) .   In this paper , we address the memorization over-   Ô¨Åtting issue by enhancing the model ‚Äôs dependence   on support sets when learning the model initial-   ization , which forces the model to better leverage   information from support sets . As an analogy , con-   sider a young investor who has the ability to adapt   to new circumstances rapidly but little memory of   learned experiences , and an old investor who is   experienced but refuses to be Ô¨Çexible . Our idea is   to make the young investor adaptive to the various   situations when he assesses his beneÔ¨Åts so that he   can not only take advantage of the old one ‚Äôs expe-   rience but also learn from the old investor how to   leverage the learned experience . In this paper , the   young investor stands for a standard meta - learning   algorithm ( e.g. , MAML ) , which is prone to memo-   rization overÔ¨Åtting , and the old investor is a mem-   ory module we integrate into the method , carrying   information of support sets .   SpeciÔ¨Åcally , we propose a Mem ory - Imitation   Meta - Learning ( MemIML ) method that forces   query set predictions to depend on their correspond-   ing support sets by dynamically imitating behav-   iors of the latter . We therefore , introduce a memory   module and an imitation module to enhance such   dependence . The memory module is task - speciÔ¨Åc ,   storing representative information of support sets .   The imitation module assists in predicting samples   of query sets by dynamically imitating the memory   construction . In this way , the model has to access   the support set by memory imitation each time it   makes a prediction on a query - set sample , hence   it ‚Äôs no longer feasible for the model to memorize   all meta tasks .   The contributions of this work are :   1.A novel method MemIML is proposed to   alleviate the memorization overÔ¨Åtting for   optimization - based meta - learning algorithms . It   encourages the utilization of support sets with   the help of a memory module and an imitation   module when adapting to new tasks .   2.Comprehensive experiments on text classiÔ¨Åca-   tion and generation tasks show that MemIMLsigniÔ¨Åcantly outperforms competitive baselines .   3.Theoretical proofs are given to demonstrate the   effectiveness of our method .   2 Related Work   Meta - Learning . Meta - Learning aims to im-   prove the learning algorithm itself based on the pre-   viously learned experience ( Thrun and Pratt , 1998 ;   Hospedales et al . , 2021 ) . In general , there are three   categories of meta - learning methods : model - based   methods , ( Santoro et al . , 2016 ; Obamuyide et al . ,   2019 ) which depend on the particular model design   to facilitate fast learning ; metric - based methods ,   ( Vinyals et al . , 2016 ; Snell et al . , 2017 ; Geng et al . ,   2019 ) which encode samples into an embedding   space and classify them based on the learned dis-   tance metric ; optimization - based methods ( Finn   et al . , 2017 ; Mi et al . , 2019 ) that learn a well-   generalized model initialization which allows for   fast adaptation to new tasks . For low - resource sce-   narios in NLP , optimization - based meta - learning   methods achieved promising results on tasks such   as personalized dialog generation ( Madotto et al . ,   2019 ; Song et al . , 2020 ; Tian et al . , 2021 ) , low-   resource machine translation ( Gu et al . , 2018 ;   Sharaf et al . , 2020 ) and question answering ( Yan   et al . , 2020 ) , few - shot slot tagging ( Wang et al . ,   2021 ) , and so on .   Memorization overÔ¨Åtting of Meta - learning .   Meta - learning algorithms suffer from memoriza-   tion overÔ¨Åtting . Yin et al . ( 2020 ) build an informa-   tion bottleneck to the model , while this approach   decreases the model performance with this pas-   sive regularization . Rajendran et al . ( 2020 ) inject   random noise to the ground truth of both support   and query sets , while little extra knowledge is in-   troduced to learn a good initialization . Yao et al .   ( 2021 ) address overÔ¨Åtting issues by augmenting   meta - training tasks through mixing up support and   query sets . However , such augmentation for text   needs to be based on the assumption of keeping the   label and the data distribution unchanged , which   is often not true in practice ( Chen et al . , 2021 ) . In-   stead of regularization and data augmentation , we   leverage the support sets information stored in the   memory to augment the meta - learning .   External Memory for Few - shot Learning .   Memory mechanism has proven to be powerful for   few - shot learning ( Geng et al . , 2019 ; Santoro et al . ,   2016 ; Munkhdalai et al . , 2019 ) . Current methods584either reÔ¨Åne representations stored in the memory   ( Ramalho and Garnelo , 2018 ) or reÔ¨Åning parame-   ters using the memory ( Munkhdalai and Yu , 2017 ;   Cai et al . , 2018 ; Wang et al . , 2020 ) . In the NLP   domain , some methods store encoded contextual   information into a memory ( Kaiser et al . , 2017 ;   Holla et al . , 2020 ; Zheng et al . , 2019 ) . Geng et al .   ( 2019 ) propose a memory induction module with a   dynamic routing algorithm for few - shot text classi-   Ô¨Åcation tasks . Munkhdalai et al . ( 2019 ) augment   the model with an external memory by learning a   neural memory . Wang et al . ( 2021 ) reuse learned   features stored in the memory on the few - shot slot   tagging .   3 Preliminaries   We Ô¨Årst formulate model - agnostic meta - learning   ( MAML ) ( Finn et al . , 2017 ) . SpeciÔ¨Åcally , denote   the base model used in MAML as fand assume   each taskTsampled from a task distribution p(T )   associates with a dataset D. Each datasetDcon-   sists of a support set D = f(X;Y)gand   a query setD = f(X;Y)g , whereXand   Ydenote the input and ground truth of a sample ,   respectively . During the meta - training stage , a task-   speciÔ¨Åc ( a.k.a . , post - update ) model fis Ô¨Årst ob-   tained for each task Tvia gradient descent over   its support setD. Then MAML updates its ini-   tialization ( a.k.a . , pre - update ) according to the   performance of fon the query setDas in Eq.1 :   = minEh   L   f(X);Yi   ( 1 )   s.t.=   rL(f(X);Y)(2 )   where  is the inner loop learning rate . During the   meta - testing stage , the learned initialization is   Ô¨Åne - tuned on the support set Dfor taskT , and   the resulting model is evaluated on the query set   Dwith the post - update parameters .   4 Methodology   To alleviate the memorization overÔ¨Åtting issue in   meta - learning , we propose MemIML , which in-   cludes a memory module and an imitation module   on the grounds of a base model . The memory mod-   ule is task - speciÔ¨Åc , recording the mapping behav-   iors between inputs and outputs of support sets for   each task . The imitation module is shared across   tasks and predicts values for each query - set sample   by dynamically imitating the memory construction .   The acquired support set information leveraged bythe imitation module augments the model initial-   ization learning , enhancing the dependence of the   model ‚Äôs task adaptation on support sets . Fig . 1   shows our model architecture .   4.1 Memory Module   We design a memory module Mfor each taskT   and incorporate it in the MAML framework . In or-   der to fully leverage information from support sets ,   we construct key - value pairs from support - set sam-   ples and store them in the memory module . The   key is the sentence representation of a sample input   from support sets obtained from an introduced key   network . The corresponding value is constructed to   store the information of the sample output ( ground   truth ) as in Sec . 4.3 : in NLG tasks , the value is   the sentence embedding of the output sentence ; in   NLU tasks , the value is the one hot embedding   of the class label ( a scalar ) of the sample . Our   memory has two operations : memory writing that   constructs the memory and memory reading that ac-   quires information from memory . In the following ,   we elaborate on these contents in detail .   Key Network represents a sample with a vec-   tor . SpeciÔ¨Åcally , we use a frozen pre - trained BERT   model ( Devlin et al . , 2019 ) as the key network . The   input of the key network is the sample input sen-   tenceX2D(X2D ) , and the output is the   encoded representation of the Ô¨Årst token ( i.e. [ CLS ]   token ) of the sentence . The acquired representation   is regarded as the key KforX(KforX ) .   Memory Writing constructs the memory using   the information of samples in the support set D.   For each taskT , the task - speciÔ¨Åc memory M   consists ofNmemory slots ( i.e. key - value pairs   fK;Vg ) . To build these memory slots , we   select samples from support sets and write their   information into the memory . The sample selection   is according to a diversity - based selection criterion   ( Xie et al . , 2015 ) to ensure the diversity and repre-   sentativeness of the memory content . The detailed   description of this criterion is in Appendix . D.   For each task - speciÔ¨Åc memory module M , we   adopt the diversity score as S(M)on the stored   keys . Here , a more diverse memory gets a higher   diversity score . When the memory is not full , we   directly write support - set samples without selec-   tion ; otherwise , we compute the diversity score   of the current memory and scores after every old   key - value pair is replaced with a new key - value   pair . Then we replace the old pair with the new one585   where the replacement can maximize the diversity   score . In this way , the memory we build can carry   more distinguishable and representative informa-   tion and efÔ¨Åciently utilize the storage space .   Memory Reading obtains information from   memory to enhance the meta - learning . The input is   the sentence representation of the sample in query   sets encoded by the key network , and the output   is the memory slots similar to the query sample .   SpeciÔ¨Åcally , given the key representation Kof a   sampleX2D , we retrieve the top Nmost sim-   ilar slots from its task - speciÔ¨Åc memory M. The   similarity is measured based on the Euclidean dis-   tance between Kand each key Kin the memory   slots . The retrieved key - value pairs fK;Vg   act as the output of memory reading .   4.2 Imitation Module   In order to better leverage the retrieved memory and   enhance the dependence of our model on support   sets , we propose an imitation module to encour-   age the imitation of support sets behaviors when   making predictions on query sets . For each sam-   pleXin the query set , the inputs of the imitation   module are the key Kand its retrieved Nmemory   slots , and the output is the predicted value ^Vfor   X. To achieve the imitation , we construct a value   predictor that can model the behaviors of support-   set samples ( i.e. key - value matching ) stored in the   memory . For estimating the value of each query - set   sample , we conduct local adaptation on the value   predictor to adapt the matching .   In this way , the proposed imitation module is   customized for each query - set sample , which fa-   cilitates better capture of speciÔ¨Åc task informationthan directly using the memory reading output , es-   pecially when tasks are versatile . The reason is   that the similarity measurement of previous mem-   ory reading operations is based on the Ô¨Åxed BERT   representations , which ignores the task - speciÔ¨Åc in-   formation .   4.2.1 Value Predictor   In MemIML , the proposed value predictor aims to   build a mapping from keys to values of the mem-   ory module mentioned in Sec . 4.1 . The input of   the value predictor is a key obtained from the key   network , and the output is the associated value .   SpeciÔ¨Åcally , we use a two - layer fully - connected   networkgwith parameters ! to build the mapping .   The value predictor is learned over constructed key-   value pairs of support sets across all tasks . Given   the keyKof a query - set sample input X , we can   then estimate its associated value as ^V.   4.2.2 Training of The Value Predictor   To train the value predictor , we minimize the recon-   struction lossL(^V;V)to make the predicted val-   ues as close as possible to values constructed from   the ground truths of support - set samples , where   Lis the cross - entropy loss if the value Vis a   label and is the mean square loss if Vis a vector .   The training procedure includes the global op-   timization shared across tasks and the local adap-   tation for each speciÔ¨Åc task . SpeciÔ¨Åcally , we Ô¨Årst   train the value predictor with samples from support   sets of all tasks . After feeding the memory reading   output of a query - set sample to this network , we   perform local adaptation and employ the adapted   network to estimate the value for the query sample.586Global Optimization . To obtain the task-   independent global parameters ! , we train the   value predictor over constructed keys ( i.e. , as in-   puts ) and values ( i.e. , as outputs ) from support - set   samples of all tasks . The global optimization keeps   updating in the whole meta - training phase .   Local Adaptation . To make the value predictor   adaptive to each query - set sample X , inspired   by ( Sprechmann et al . , 2018 ) , we propose local   adaptation that Ô¨Åne - tunes the global value predictor   gto get an adapted one with parameters ! . The   local adaptation only works when predicting X.   Based on the initial parameters ! from the global   optimization , we perform several gradient descent   steps to minimize the loss L , which is :   L=   k~! !k+1   NXL(^V;V)(3 )   Here , ^V = g(K),fK;Vgis the mem-   ory reading output of the query - set sample , and   the factor   restricts the distance between !   and ! . Minimizing the second term encourages   gto better estimate the retrieved memory val-   uesfVg . Then we can acquire the locally   adapted value prediction network gwith parame-   ters!= arg minL(~ ! ) . Given a query - sample   keyK , we can thus predict its associated value as   ^V = g(K ) ; ( 4 )   where the adapted parameters ! are discarded   thereafter , and the model does not back - propagate   through ^V.   In this sense , besides the task - speciÔ¨Åc parame-   terprovided by MAML , there will also be !   learned from support sets speciÔ¨Åc to each query - set   sample . This guarantees that the model relies more   on support sets for task adaptation . Fig . 1 ( right   part ) illustrates the mechanism of local adaptation .   4.3 MemIML on NLP Applications   In this part , we will elaborate on two few - shot ap-   plications in NLP ( i.e. , text generation and text   classiÔ¨Åcation ) to solve the memorization overÔ¨Åt-   ting problem of MAML . The model structures of   these applications are basically the same , except for   the following three points : the base model , the way   to get the value Vstored in the memory module ,   and the way to leverage the output ^Vof Sec . 4.2.Personalized Dialogue Generation . The base   model is the transformer ( Vaswani et al . , 2017 )   consisting of an encoder and a decoder . In this   task , each sample consists of an input utterance   and a ground truth utterance , so the value V   stored in the memory is obtained from the ground   truth utterance Yof a support - set sample , which   is embedded by the key network followed by an   LSTM ( Hochreiter and Schmidhuber , 1997 ) . This   LSTM is optimized with the base model . The   ^V , concatenated with the encoder outputs , serves   as a new input for the decoder . Hence , we ac-   quire the prediction of a query - set sample via   ^Y = Decoder ( [ ^V;Encoder ( X ) ] ) .   Multi - domain Sentiment ClassiÔ¨Åcation . The   base model is a BERT ( Devlin et al . , 2019 ) fol-   lowed by a fully - connected network . Each sample   consists of an input sentence and a sentiment label   ( ground truth ) , so the memory value Vis the sen-   timent label . To leverage ^V , we interpolate it with   the original output of the base model ~Yas   ^Y=  ~Y+ ( 1   ) ^V ( 5 )   where  balances ~Yand^V. Notice that the inter-   polation not only works on the prediction output   but also guides the training via gradient descent   based on the interpolated output . We verify the   effectiveness of the interpolation in Appendix . C.   Algorithm 1 Memory Imitation Meta - training587   4.4 Theoretical Analysis   We theoretically investigate how our method helps   to alleviate the memorization overÔ¨Åtting problem .   Following Yin et al . ( 2020 ) , we use mutual infor-   mationI(^Y;Dj;X)to measure the level of   the memorization overÔ¨Åtting . When the learned   model ignores support sets to predict query sets ,   I(^Y;D)j;X ) = 0 occurs , which indicates   the complete memorization overÔ¨Åtting in meta-   learning ( Yin et al . , 2020 ) . Hence , lower mutual in-   formation means more serious memorization over-   Ô¨Åtting issues .   We propose a criterion similar to ( Yao et al . ,   2021 ) to measure the validity of our method for   tackling this problem . For a task T = fD;Dg ,   the criterion aims to mitigate the memorization   overÔ¨Åtting by enhancing the model ‚Äôs dependence   on the support set D , i.e. increasing the mutual   information between support set and ^Yas follows :   I(^Y;[D;M]j;X)>I(^Y;Dj;X);(6 )   whereMmeans additional memory information   we provide , which contains support sets informa-   tion to augment the inference of the sample Xin   D. We demonstrate our method MemIML meets   the above criterion ( See details in Appendix . A. ) .   4.5 The Procedure of Training and Testing   In the meta - training phase ( shown in Alg . 1 ) ,   MemIML Ô¨Årst constructs an empty memory for   each task and then follows the bi - level optimiza-   tion process of MAML . In the inner loop , MemIML   adapts the base model initialization to task-   speciÔ¨Åc parameters via training on the support set .   At the same time , from each support - set sample ,   MemIML obtains a key - value pair and determines   whether to write it into the memory or not . Then ,   MemIML conducts the global optimization of the   value predictor over these key - value pairs . In the   outer loop , each sample of the query set readsthe memory to retrieve the most similar memory   slots . Local adaptation Ô¨Åne - tunes the value pre-   dictor on those retrieved slots . Next , the adapted   value predictor estimates the value of each query   sample and uses it to augment the learning of the   model initialization . The total loss function in   the inner loop isL = L+L , where   L = L(f(X);Y)is the cross - entropy loss .   The procedure of meta - training and meta - testing   are almost the same except that meta - testing does   not optimize the learned model initialization and   the initial parameter ! of the value predictor . For   each taskTin the meta - testing phase , MemIML   also adaptsto task - speciÔ¨Åc parameters in the   inner - loop and constructs the task - speciÔ¨Åc memory .   In the outer - loop , MemIML retrieves key - value   pairs from the memory to conduct local adapta-   tion based on the initial parameter ! . The esti-   mated value ^Vfrom local adaptation helps the   base model to infer the Ô¨Ånal output ^Y.   5 Experiments and Analysis   Experiments on personalized dialogue generation   and multi - domain sentiment classiÔ¨Åcation verify   our model on text generation and classiÔ¨Åcation , re-   spectively , where we use Persona - Chat and ARSC   datasets .   5.1 Personalized Dialogue Generation   Dataset . Following ( Zhang et al . , 2018 ) , we use   Persona - chat ( Madotto et al . , 2019 ) by regarding   building a dialog model for each person as a task .   The dataset consists of a training / validation / testing   set with 1137/99/100 persons ( tasks ) separately . In   the Persona - Chat dataset , each persona description   has 8.3 unique dialogues on average , and each task   consists of three samples .   Baselines . We compare our methods with the   following baselines : Base Model : We pretrain a   conventional transformer - based dialog generation588   model over all the training tasks ignoring the speak-   ers ‚Äô personality . Fine - tune : We Ô¨Åne - tune the pre-   trained base model on the support sets of each meta-   testing task . MAML : We apply MAML ( Madotto   et al . , 2019 ) to the base model . MR - MAML : Yin   et al . ( 2020 ) tackle the memorization overÔ¨Åtting of   MAML via regularization .   Metrics . Automatic evaluation has three aspects ,   ‚Ä¢Quality : BLEU - n ( Papineni et al . , 2002 ) , CIDEr   ( Vedantam et al . , 2015 ) , and ROUGE ( Lin , 2004 )   measures the n - gram matching between the gen-   erated response and ground truth . PPL ( perplex-   ity ) measures the sentence Ô¨Çuency .   ‚Ä¢Diversity . Dist - n ( Li et al . , 2016 ) evaluates the   response diversity by counting unique n - grams .   ‚Ä¢Consistency : C score ( Madotto et al . , 2019 ) mea-   sures the consistency between the generated re-   sponses and persona descriptions through a pre-   trained natural language inference model .   Human evaluation consists of Quality andConsis-   tency . ( See details in Appendix . B.1 ) .   Overall Performance . As shown in Table 1 .   Fine - tune outperforms Base Model in all metrics ,   which veriÔ¨Åes that the task - speciÔ¨Åc data is helpful   to its performance on speciÔ¨Åc tasks . Compared   toFine - tune , MAML behaves better on diversity   and consistency but behaves worse on quality . Pre-   training the base model achieves the best perplex-   ity ( lowest PPL ) as shown by Base Model and   Fine - tune . We analyze that it ‚Äôs because pretraining   leads to a considerable degree of Ô¨Çuency in their   generated utterances and is careless about each   task ‚Äôs speciÔ¨Åc information , resulting in low consis-   tency with tasks . Our model , MemIML , performsthe best in most aspects , including quality , diver-   sity , and task consistency . In particular , MemIML   signiÔ¨Åcantly improves MR - MAML in alleviating   the memorization overÔ¨Åtting issue , suggesting that   memory imitation is more effective than only regu-   larizing model initialization .   5.2 Multi - domain Sentiment ClassiÔ¨Åcation   Dataset . Amazon Review sentiment classiÔ¨Åca-   tion dataset ( ARSC ) ( Yu et al . , 2018 ) contains 69   tasks in total . Following ( Geng et al . , 2019 ) , we   build a 2 - way 5 - shot meta - learning with 57 tasks   for meta - training and 12 tasks for meta - testing .   We conduct experiments on the ARSC ( Yu et al . ,   2018 ) . It contains English reviews of 23 types of   Amazon products , where each product consists of   three different binary classiÔ¨Åcation tasks . Follow-   ing Geng et al . ( 2019 ) , we select 12 tasks from 4   domains ( Books , DVD , Electronics , Kitchen ) for   meta - testing tasks , and the support sets of these   tasks are Ô¨Åxed ( Yu et al . , 2018 ) .   Baselines . We compare our methods with the   following baselines : Fine - tune : We Ô¨Åne - tune a   pre - trained BERT on the support set of meta-   testing tasks ( non - meta - learning method ) as in   Appendix . B.2 . We choose Ô¨Åve metric - based   meta - learning baselines : Matching Net ( Vinyals   et al . , 2016 ) , Prototypical Net ( Snell et al . , 2017 ) ,   Proto + + , ( Ren et al . , 2018 ) , Relation Net ( Sung   et al . , 2018 ) , and Induction Net ( Geng et al . ,   2019 ) . We apply an optimization - based baseline   ( MAML ) ( Finn et al . , 2017 ) to the base model , and   implement some approaches tackling the memo-   rization overÔ¨Åtting problem based on MAML : MR-   MAML ( Yin et al . , 2020 ) , MetaMix , ( Yao et al . ,   2021 ) and Meta - Aug ( Rajendran et al . , 2020 ) .   Overall Performance . Table 2 shows the per-   formance measured by the mean accuracy of   meta - testing tasks . Our model , MemIML out-   performs all competing approaches including non-   meta - learning , metric - based meta - learning , and   optimization - based meta - learning methods . Par-   ticularly , our model surpasses the current solu-   tions to the memorization overÔ¨Åtting problem ( MR-   MAML , Meta - Aug , MetaMix ) , indicating that   our method is more effective compared to regu-   larization and textual augmentation .   5.3 Memorization OverÔ¨Åtting Analysis   In Figure 2 , the gaps of the losses on query sets be-   tween pre - update (before training on support sets)589   and post - update (after training on support sets )   indicate the memorization overÔ¨Åtting problem . The   gap between sky - blue and blue curves measures   the memorization overÔ¨Åtting of meta - training ( the   gap between pink and red curves measures meta-   testing ) . Small loss gaps indicate a severe memo-   rization overÔ¨Åtting where support sets are almost   useless for task adaptation . Those loss gaps be-   tweenandcollapse in MAML and MR - MAML   after about 3000 steps . This indicates that the post-   updatebarely beneÔ¨Åts from the support set , and   thus the memorization overÔ¨Åtting issue is severe .   In Figure 2 ( c ) , MemIML has large gaps between   and , implying that better leverages support   sets when adapting to new tasks and thus alleviates   the memorization overÔ¨Åtting issue .   5.4 Ablation Studies   In Table 3 , we conduct ablation studies to verify   the effectiveness of each component . Removing   Similarity - Search means the memory reading op-   eration randomly outputs memory slots instead of   searching for similar memory slots . This variant   underperforms MemIML , indicating that similar   samples stored in the memory provide more use-   ful information to improve the model performance .   Removing the value predictor means directly using   the memory output without a learnable network . Its   results are not too bad , indicating that the memory   module helps to mitigate the memorization overÔ¨Åt-   ting problem . However , this usage simply aggre - gates the support set information into the query set ,   which is not as precise as learning the information   required by the query set itself . Therefore , it is still   inferior to our model . Removing Local adaptation   means we only use the global value predictor to es-   timate the memory output . It is crucial to the value   predictor since removing it from the value predictor   results in an even worse performance than remov-   ing the value predictor . Besides , the signiÔ¨Åcant   drop in task consistency ( C - score ) shows that local   adaptation contributes a lot to making the model   adaptive to speciÔ¨Åc tasks , as it learns to adapt to   each query - set sample .   5.5 Analysis of Memory Operations   Memory Size . In Table 4 and 5 , we investigate   the variants of our task - speciÔ¨Åc memory module   of different sizes . We control the memory size   throughjMj = store ratiojDj . The results   demonstrate that our model is able to maintain high   performance even with only a 20 % memory size   by storing diverse and representative samples of   support sets . Besides , as the ratio of stored samples   increases , the model ‚Äôs performance is improved   since it provides more information for the infer-   ence of query samples and the optimization of the   model initialization . Storing all the encountered   samples ( i.e. , with store ratio 100 % ) in the mem-   ory instead introduces some noise that damages the   model performance.590   Number of Neighbors . We also investigate the   effects of different numbers of neighbors for the   model performance in Table 4 and Table 5 . In both   datasets , the model performs better with a larger   number of neighbors . However , when the number   of neighbors is too large , the model retrieves some   dissimilar slots from the memory module . These   dissimilar slots bring much noise , which makes the   predictions of query samples inaccurate .   5.6 Case Study   We present two generated cases in personalized   dialog in Table . 6 . Base Model , Fine - tune , and   MAML generate general responses with little use-   ful information or responses that are not consis-   tent with the personality of personas . MR - MAML   generates irrelevant responses to the dialogue con-   text . Our model not only responds coherently to   the dialog history but also caters to the persona   descriptions of each user .   6 Conclusion   In this paper , we tackle the memorization overÔ¨Åt-   ting problem of meta - learning for text classiÔ¨Åcation   and generation applications . We propose MemIML   to enhance the dependence of the model on the   support sets for task adaptation . MemIML intro-   duces a memory module storing the information of   support sets , and propose an imitation module to   better leverage the support set information by imi-   tating the behaviors of the memory . Both empirical   and theoretical results demonstrate that our method   MemIML effectively alleviates the memorization   overÔ¨Åtting problem .   7 Ethical Considerations   The persona - based dialogue generation task aims   to build a dialogue model which generates mean-   ingful , Ô¨Çuent , and consistent responses . It will   facilitate human - computer interactions in practice .   However , the training of the model for personalized   dialogues may lead to the leakage of personal pri-   vacy information . In this work , the data source we   use is from a published dataset and does not involve   privacy issues for the data collection . Our proposed   method does not include inference or judgments   about individuals and does not generate any dis-   criminatory , insulting responses . Our work vali-   dates the proposed method and baseline models   on human evaluation which involves manual la-   bor . We hire Ô¨Åve annotators to score 750 generated   sentences in total ( 250 sentences for each model   we evaluate ) . The hourly pay is set to 15 US$   per person , which is higher than the local statutory   minimum wage .   Acknowledgements   Research on this paper was supported by Hong   Kong Research Grants Council ( Grant No .   16204920 ) and National Natural Science Founda-   tion of China ( Grant No . 62106275).591References592593A Validity of Memory Imitation Strategy   Proof of inequality in Eqn . 6 . We check the valid-   ity of memory imitation by examining whether the   criterion in Section 4.4 is met . We check the in-   crease of mutual information between predictions   of query sets with the provided support - set informa-   tion after augmented with the memory information   M.   I(^Y ; [ D;M]j;X) I(^Y;Dj;X )   = H(^Yj;X) H(^YjD;M;;X )    H(^Yj;X ) + H(^YjD;;X )   =  H(^YjX;X;Y;M; )   + H(^YjX;X;Y; ): ( 7 )   For short , we use notation Z= ( X;X;Y;)to   denote a set of variables . Then we can rewrite ( 7 )   as    H(^YjZ;M ) + H(^YjZ )   = Eh   logp(^YjZ;M)i    Eh   logp(^YjZ)i   :   Note that trivially , we have E[1 ] = 1 , so we get   Eh   p(^YjZ)i   = Eh   p(^YjZ)i   sincep(^Y;Z)does not rely on the variable M.   Hence , we can just write EasEfor short .   Then the equation ( 7 ) will become to   E[logp(^YjZ;M)] E[logp(^YjZ ) ]   = E[logp(^YjM;Z )   p(^YjZ ) ]   = Xp(Z)p(^Y;MjZ ) logp(^Y;MjZ )   p(^YjZ)p(MjZ )   = E[KL(p(M;^YjZ)jjp(^YjZ)p(MjZ ) ) ]   > 0   where the last inequality holds due to ^Yis depen-   dent onM.   We also investigate that memory imitation im-   proves the learning of model initialization via an-   other criterionI( ; [ D;M]jD)>0following   Yao et al . ( 2021 ) . This criterion guarantees that   the additional memory knowledge contributes to   updating the initialization in the outer loop . Since594all the meta - training tasks satisfy this criterion , the   generalization ability of the model initialization   improves .   Proof .   I( ; [ D;M]jD )   = H(jD) H(jD;M )   = E[ logP(jD ) ] + E[logp([jD;M ) ] ) ]   = E[logp(jD;M )   p(jD)]>0   B Experimental Details   B.1 Personalized Dialogue Generation   Experimental Setup . We implement our model   based on the transformer ( Dehghani et al . , 2018 ;   Vaswani et al . , 2017 ) with pre - trained Glove embed-   ding ( Pennington et al . , 2014 ) following ( Madotto   et al . , 2019 ) . The hidden dimensions of the LSTM   unit are set to 1024 . We set the number of neigh-   borsN= 10 and the number of local adaptation   stepsL= 20 . We follow all other hyperparameter   settings in Madotto et al . ( 2019 ): we use SGD for   the inner loop training and Adam for the outer loop   update with learning rates 0:01and0:0003 , respec-   tively . We set batch size as 16 and use beam search   with beam size 5 .   Human Evaluation We conduct human evalua-   tion following Song et al . ( 2020 ) considering two   aspects Quality andConsistency where Ô¨Åve well-   educated volunteers annotate 250 generated re-   sponses for each model . The annotators score each   response from two aspects : Quality andConsis-   tency in a 3 - point scale : 2 for good , 1 for fair , and 0   for bad . Quality measures coherence , Ô¨Çuency , and   informativeness . Consistency measures the task   consistency between the generated responses and   the person ‚Äôs persona description .   B.2 Multi - domain Sentiment ClassiÔ¨Åcation   Experimental Setup . We utilize a BERT ( De-   vlin et al . , 2019 ) as the encoder . We Ô¨Åne - tune the   off - the - shelf pre - trained BERT on the masked lan-   guage modeling task following ( Dopierre et al . ,   2021 ) as it greatly improves embeddings ‚Äô quality   ( Sun et al . , 2019 ) . The Ô¨Åne - tuned BERT is then   used as the initialization for all few - shot models .   We use Adam ( Kingma and Ba , 2015 ) optimizer forboth inner and outer loop update with learning rate   2eand1erespectively , and we set  = 0:2 in   Eqn . 5 , the number of neighbors N= 20 and the   number of local adaptation steps L= 5 .   C Effectiveness of the Interpolation   To measure whether MemIML improves the   learned model initialization , we add an experiment   that does not incorporate the memory module dur-   ing meta - testing ( i.e. ,  = 1 in Eq . 5 ) for the   multi - domain sentiment classiÔ¨Åcation task . The   better result of MemIML than MAML and other   regularization methods demonstrate the superiority   of our model .   D Diversity - selection Criterion   For each task - speciÔ¨Åc memory module M , fol-   lowing Xie et al . ( 2015 ) , we adopt the diversity   score asS(M ) = (M) (M)on the stored   keys , where (M ) = PP(K;K )   denotes the mean of angles between every   two stored key representations and (M ) = PP((K;K) (M))denotes   the variance of those angles.595