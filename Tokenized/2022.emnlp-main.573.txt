  Shi Feng   University of Chicago   shif@uchicago.eduJordan Boyd - Graber   University of Maryland   jbg@umiacs.umd.edu   Abstract   Explanations promise to bridge the gap be-   tween humans and AI , yet it remains diffi-   cult to achieve consistent improvement in AI-   augmented human decision making . The use-   fulness of AI explanations depends on many   factors , and always showing the same type of   explanation in all cases is suboptimal — so is   relying on heuristics to adapt explanations for   each scenario . We propose learning to explain   “ selectively ” : for each decision that the user   makes , we use a model to choose the best ex-   planation from a set of candidates and update   this model with feedback to optimize human   performance . We experiment on a question   answering task , Quizbowl , and show that selec-   tive explanations improve human performance   for both experts and crowdworkers .   1 Introduction   Recent advances in machine learning ( ML ) ( Sil-   ver et al . , 2017 ; Brown et al . ; Jumper et al . , 2021 )   sparked new interest in intelligence augmenta-   tion — the vision that computers are not mere   number - crunching tools but also interactive sys-   tems that can augment humans at problem solv-   ing and decision making ( Engelbart , 1962 ) . The   hope is to combine the complementary strengths   of human and machine , and to fully harness these   models with human intuitions and oversight ( Dafoe   et al . , 2020 ; Amodei et al . , 2016 ) . But this agenda   is obstructed by the many counterintuitive traits of   neural networks ( NNs ) ( Szegedy et al . , 2014 ; Good-   fellow et al . , 2015 ; Zhang et al . , 2017 ) and our lack   of theoretical understanding ( Belkin et al . , 2019 ):   these models are not interpretable to humans by   default , and it is difficult to foresee when they will   fail . This lack of interpretability also amplifies the   risk of model bias ( Angwin et al . , 2016 ; Bolukbasi   et al . , 2016 ; Caliskan et al . , 2017 ) , making it diffi-   cult to use NN - powered AIs in real - world decision   making ( Poursabzi - Sangdeh et al . , 2021).To bridge the gap between human and machine ,   various methods attempt to explain model predic-   tions in human - interpretable terms , e.g. , by pro-   viding more context to the model ’s uncertainty   estimation ( Gal et al . , 2016 ; Bhatt et al . , 2021 ) ,   by highlighting the most important part of the in-   put ( Ribeiro et al . , 2016 ; Lundberg and Lee , 2017 ;   Ebrahimi et al . , 2017 ) , and by retrieving relevant   training examples ( Renkl , 2014 ; Koh and Liang ,   2017 ) . Grounded in psychology ( Lombrozo , 2006 ;   Kulesza et al . , 2012 ) , these explanations promise to   augment human decision making ( Pu and Chen ,   2006 ; Rader et al . , 2018 ) . But in application-   grounded evaluations — with real problems and real   humans ( Doshi - Velez and Kim , 2018 ) , it proves   difficult for any single explanation method to   achieve consistent improvement in disparate con-   texts ( Bansal et al . , 2021 ; Buçinca et al . , 2020 ) .   A major contributor to this problem is the   breadth of context that the explanation methods   are applied in . Internally , the explanation method   can falter when the model reacts badly to novel   inputs ( Goodfellow et al . , 2015 ; Liu et al . , 2021 ) ;   externally , it faces human users with diverse levels   of expertise ( Feng and Boyd - Graber , 2019 ) , en-   gagement ( Sidner et al . , 2005 ) , and general trust in   AI ( Dietvorst et al . , 2015 ) . Our current use of ex-   planations demands a one - size - fits - all solution , but   existing methods can not provide that as they are   largely oblivious to the above - mentioned variables .   Selective explanations . Each person is unique ,   and the right explanation might vary from one ex-   ample to another , so we propose to show explana-   tions selectively to maximize their utility as deci-   sion support . Concretely , we assume a given set of   explanation methods , but instead of showing all of   them for every decision that the human user makes ,   we use a selector to choose a subset of the expla-   nations to display . We can think of the selector as   controlling an on / off switch for each explanation8372method . The selector is allowed , for example , to   show three types of explanations for one example   but withhold all of them for the next one .   Online optimization . For the policy to accu-   rately estimate the utility of explanations in each   context , its training data must provide reasonable   coverage over the joint distribution of all types of   explanations , human users , and examples . This   means the dataset will have to include cases where   the users receive suboptimal decision support , e.g. ,   with excessive explanations causing information   overload ( Doshi - Velez and Kim , 2018 ) . We fo-   cus on the online setting — new information is con-   stantly hurtling toward the user with limited time to   carefully select which explanation to pay attention   to — which represents real - world scenarios where   the opportunity cost of giving suboptimal support   can not be ignored . In this setting , a good policy   must balance the trade - off between sticking to tried   and true combinations of explanations and explor-   ing new ones ; we model this trade - off with a multi-   armed bandit formulation ( Robbins , 1952 ) .   We evaluate selective explanations on   Quizbowl — explained in more detail in Sec-   tion 2 — using the same platform as Feng and   Boyd - Graber ( 2019 ) . To mimic real - world decision   making , we recruit twenty trivia enthusiasts and   ran a multi - player , real - time Quizbowl tournament .   We compare our method head - to - head against   baselines such as showing all explanations for   all examples . Selective explanations outperform   all other strategies , including the best subset of   explanations identified by Feng and Boyd - Graber   ( 2019 ) . We also evaluate our method with me-   chanical Turkers — amateurs whose performance   without assistance is far worse than the AI .   Explanations significantly boost their performance ,   but only selective explanations can help them   reach performance comparable with the AI .   2 Explaining Selectively to Support   Human Decisions   Explanations have many uses in human - cooper-   ation ; this paper focuses on using explanations as   decision support — to improve the quality of human   decisions under machine assistance . Not all prob-   lems benefit from machine assistance ( Doshi - Velez   and Kim , 2018)—in this section , we identify three   criteria for decision support testbeds . We then in-   troduce our setup based on Quizbowl ( Rodriguez   et al . , 2019 ) , a competitive trivia game.2.1 Criteria for decision support testbeds   It is not uncommon to use low - stakes and synthetic   tasks to evaluate machine assistance , but it ’s im-   portant to find tasks where results can generalize .   Building on existing work ( e.g. Lee and See , 2004 ;   Lim et al . , 2009 ; Yin et al . , 2019 ) , we identify the   three criteria for suitable tasks .   Clear objectives . The task must have well-   defined metrics , and the standard for good deci-   sions must be clear to all participants . With unreli-   able metrics , well - optimized decision support will   still fail to improve decision quality ( Amodei et al . ,   2016 ) .   Diversity of context . A reliable testbed should be   diverse in terms of both participants ( e.g. , their skill   levels ) and test examples ( e.g. , their difficulty level ) .   As discussed in Section 1 , the lack of diversity   contributes to the inconsistent results .   Incentives to be engaged . The participants must   be incentivized to pay attention to model outputs   to establish proper reliance ( Lim et al . , 2009 ) . As   a corollary , the model should demonstrate com-   plementary strengths and provide information that   participants can not extract by themselves . In terms   of the setup , engagement can also be improved by   imposing time limits ( Doshi - Velez and Kim , 2018 )   and introducing competition ( Bitrián et al . , 2021 ) .   We choose Quizbowl ( Rodriguez et al . , 2019 ) —   a task that roughly satisfies all three criteria — as   our testbed . Compared to previous work that uses   Quizbowl to evaluate explanations ( Feng and Boyd-   Graber , 2019 ) , we make several changes to the   setup for evaluating online selective explanations .   In the following , we first introduce the most basic   setting with only human players and build up our   system one component at a time .   2.2 Human - only Quizbowl   We start with the most basic ( and traditional )   setting : Quizbowl with only human players .   Quizbowl is a trivia game popular in the English-   speaking world where players compete to answer   questions from all areas of academic knowledge ,   including history , literature , science , sports , and   more . Each Quizbowl question consists of four to8373   five clues . The clues are organized by their diffi-   culty in each question : starting with the clue that ’s   most difficult and obscure , and finishing with the   one that ’s easiest and most telling . The clues are   presented to all players word - by - word in real - time ,   verbally , or in text ( e.g. web interfaces ) . And play-   ers compete to answer as early as possible .   To signal that they know the answer , players   interrupt the question with a buzz , which takes its   name from the signaling device ’s sound . Whoever   buzzes needs to answer : ten points for a correct   answer , and a five - point penalty for a wrong one .   A player only gets one chance at each question .   To win Quizbowl , you need to answer quickly   andcorrectly . This game requires not only trivia   knowledge but also an accurate assessment of con-   fidence and risk ( He et al . , 2016 ) . We formally   discuss the evaluation metric in Section 2.6 .   2.3 Human + AI + Explanations   In our Quizbowl games , human players augmented   withdecision support compete against each   other . In each human - team , the human player   is still in charge of making decisions of when to   buzz and what to answer , but now with the help of   a machine learning guesser which predicts an an-   swer given a question ( we provide details about the   guesser in Section 3 ) . In addition to showing the   guesser ’s current best guess , we show four types of   explanations : Alternatives ( Lai and Tan ,   2019 ) , salient word Highlights ( Ribeiro   et al . , 2016 ) , relevant training examples as   Evidence ( Wallace et al . , 2018 ) , and a new   explanation , Autopilot . As the name sug-   gests , Autopilot assumes the role of the human   player and makes suggestions on whether to buzzor to wait ( details in Section 2.5 ) . We build our   interface ( Figure 1 ) by extending the interface of   Feng and Boyd - Graber ( 2019 ) . We discuss these   changes in detail next and in Section 3 .   2.4 Human + AI + Selective Explanations   With selective explanations , decision support is cus-   tomized for each player and question . For each   new question , a selector policy ( or selector for   short ) switches each explanation on and off . We   refer to a combination of explanations as a config-   uration ; for example , one configuration could be   showing Highlights andEvidence but hid-   ingAlternatives . A configuration is selected   at the beginning of each question and kept constant   throughout the question , but the content of each   explanation is still updated dynamically . For ex-   ample , Highlights will always show when they   are turned on for a question , but the exact high-   lighted words change as more clues are revealed .   We make two important changes to the setting   of Feng and Boyd - Graber ( 2019 ) to accurately esti-   mate the effect of selectivity .   •The guesser prediction is always available .   We make this design choice to better isolate   the effect of the explanations .   •Separate highlights for the question and the   evidence . Highlights can be applied to both   the question and the evidence . In Feng and   Boyd - Graber ( 2019 ) , the two are treated as   one explanation . However , their experiments   confirm that question highlights alone are al-   ready effective . In this paper , we separate the   two and the policy can control them individu-   ally . Table 1 lists the available configurations   forHighlights andEvidence .8374 # Evidence Highlights   Question Evidence   1   2 ✓   3 ✓ ✓   4 ✓ ✓ ✓   5 ✓   2.5 A new explanation : Autopilot   While most of our explanations appeared in previ-   ous work , we introduce a more assertive explana-   tion , the Autopilot . At each step of the ques-   tion , Autopilot gives the human player one bit   of information : should you buzz or not . The sug-   gestion is based on the binary prediction of whether   the guesser ’s current top answer is correct or wrong ,   just as how human players assess their confidence .   An autonomouscould use Autopilot to   decide when to buzz . But in a human - team , it ’s   just a suggestion , and the decision is still left to the   human . If the human blindly follows the sugges-   tion , the human - team reduces to an autonomoustrying to win by itself , hence the name .   BothAutopilot and the selector try to maxi-   mize the chance of winning . While Autopilot   optimizes for theonly , the selector optimizes   for the team . And this is no coincidence —   Autopilot tests if selective explanation goes be-   yond implicit calibration : the hope is to decide to   buzz better than both human – Autopilot teams   and a fully - autonomoususing Autopilot .   We use a simple , threshold - based model for   Autopilot similar to Yamada et al . ( 2018 ): it   looks at the normalized confidence scores of the   top five guesses and recommends buzzing if the gap   between the top two is larger than 0.05(a threshold   tuned on the dev set from Rodriguez et al . ( 2019 ) ) .   Despite its simplicity , this model is very efficient   at choosing the right time to buzz ( Yamada et al . ,   2018 ; Rodriguez et al . , 2019).2.6 Evaluating accuracy and efficiency using   one metric without an opponent   Winning in Quizbowl requires you to answer cor-   rectly before your opponent . In real Quizbowl   games with two or more players , a high score is   proof that a player is both accurate and efficient —   in the sense that they require little information to   get the answer right . In a perfect assessment of a   Quizbowl player , we would control for factors such   as question topics and have a head - to - head com-   petition between every pair of players . In an ideal   evaluation of decision support , we need to control   for confounders such as player skill and have a   head - to - head comparison between every possible   pair of differently - augmented players , e.g. , a strong   player with no support vs. a weak player with se-   lective explanations , and vice versa . However , this   is infeasible due to the number of confounders .   Thus , we would like a single metric to evalu-   ate both accuracy and efficiency without running   head - to - head competitions . Accuracy is trivial to   evaluate by itself , but efficiency is not as simple as   counting the number of words that the player sees   when they answer a question correctly because not   all words have the same value : answering earlier   by one word is much more difficult at the begin-   ning of the question than at the end . The reward   for answering earlier should be proportional to the   increase in the chance of beating an opponent .   The expected wins ( ) metric implements this   idea . Concretely , it assigns a weight to each cor-   rect answer depending on the percentage of the   question revealed . The higher the percentage , the   lower the assigned weight . For example , answer-   ing correctly halfway through the question counts   as0.3points in , while a correct answer at the   end only counts as 0.05points . We use weights   provided by Rodriguez et al . ( 2019 ) which are es-   timated using maximum likelihood from previous   game data ( Boyd - Graber et al . , 2012 ) .   2.7 Online optimization of the explanation   selector to maximize cumulative EW   Our goal is to build effective human - AI teams   whose cooperation requires the selector to select   which explanations to show to the human . This   section describes the machine learning model —   learned from users ’ preferences in behavioral data —   which lets the selector pick user - specific expla-   nations to show the user . Finally , to model the   exploration - exploitation trade - off , we formulate8375   the online optimization of the selector as a multi-   armed bandit problem and maximize the team ’s   cumulative EW score .   Given a human player , a question , and one of   the available explanation configurations , the user   model predicts the EW score received from this   question . To model the human player as well as the   properties of each specific question , the user model   uses both manually crafted features and BERT rep-   resentations . Table 2 shows the full list of features .   The user model can also be viewed as a value func-   tion in reinforcement learning .   Our goal is to empower humans to complete   the task at hand as accurately and as efficiently   as possible . Given a new question , the selector   should choose the best configuration based on its   model of the user ; however , to learn this model , the   selector needs to test how well each configuration   works for each type of question . This presents an   exploration - exploitation trade - off , which we model   with multi - armed bandits ( Robbins , 1952 ) . As the   user plays , new information gathered about the user   is incorporated into the user model via features   ( Table 2 ) , and we train their personalized selector   using ( Auer , 2002).3 Experiments   We run two experiments with real human partici-   pants : a single - player experiment with crowdwork-   ers and a multi - player real - time Quizbowl tourna-   ment with experts . This section first introduces the   metric for evaluating Quizbowl competency , then   provides details about the human players , the   player , the explanation methods , and the baselines .   We show that selective explanation provides per-   sonalized decision support and leads to the best   augmented human performance .   3.1 Setup : Crowdworkers   We recruit twenty crowdworker players through   Amazon Mechanical Turk . Each crowdworker   player answers a set of sixty Quizbowl questions ,   and the questions are randomly permuted for each   player . Each player is randomly assigned to either   the experimental group with selective explanations   or a control group with a baseline ; more on these   conditions later .   Before the user answers questions , we familiar-   ize the user with the interface . During that period ,   the user can explore the interface without restric-   tion ( e.g. , they can turn explanations on and off ) ,   and we switch to the assigned setting after the user   clicks a button to indicate that they are ready .   3.2 Setup : Experts   We recruit twenty expert Quizbowl players from   online trivia enthusiast forums . For these experts ,   we use a newly commissioned set of 144 questions   no participant has seen before . The questions are   randomly divided into six rounds .   Unlike the crowdworker experiment , the experts   play a real multi - player Quizbowl game . To make   sure that our game is fair and competitive , we di-   vide players into three rooms . The initial assign-   ment uses players ’ self - reported skill levels . We   subsequently adjust the assignment at the end of   each round by promoting the top 20 % players in   each room and relegating the bottom 20 % .   3.3 Setup : AI guesser and explanations   The human player is assisted by anguesser .   Given a question , the guesser produces a multi-   nomial distribution over the set of possible an-   swers ( Boyd - Graber et al . , 2012 ) ; we update this   prediction after every four question words . We   use the BERT - based guesser from Rodriguez et al .   ( 2019 ) , and refer readers to that paper for model8376Condition Description   None Display no explanation .   Everything Display all explanations .   Autopilot Display Autopilot suggestions only .   AI - only Autopilot replaces human player .   Random Choose a new random configuration for each question .   Selective Selector chooses the configuration for each question .   details and standard evaluation results . Next we dis-   cuss how we generate explanations for the guesser .   •Alternatives : We show the guesser ’s   current top five predictions along with their   confidence scores .   •Evidence : We retrieve four training ex-   amples that are most similar to the current   question . To measure similarity we use cosine   distance between question representations by   the guesser ( Wallace et al . , 2018 ) .   •Highlights onquestion : We use Hot-   Flip ( Ebrahimi et al . , 2017 ) and show tokens   with a normalized attribution score higher   than 0.15 .   •Highlights forevidence : We search   for the highlighted question tokens in the re-   trieved training examples , and highlight them .   •Autopilot : We colorize the guesser ’s   prediction based on the Autopilot ’s cur-   rent decision : red if buzzing , gray if not .   When Autopilot is disabled , the color is   always black .   Hyperparameters of an explanation ( e.g. the   number of highlighted tokens ) affect its effective-   ness . Here we choose a fixed set of hyperparam-   eters based on feedback from internal trial runs .   However , the choice of hyperparameters can also   be considered as part of the explanation configu-   ration . In such a setup , we could use the selector   with an expanded action space to , for example , also   choose the number of tokens to highlight . We dis-   cuss this more in Section 4.3 .   3.4 Setup : Conditions and baselines   Table 3 lists the conditions of our randomized con-   trolled trial . The experimental condition is selectiveexplanations . The control conditions include base-   line policies such as using a fixed explanation con-   figuration for all questions . To control the number   of conditions , we omit conditions with fixed config-   urations , e.g. Alternatives + Evidence . In-   stead , we include Everything , which Feng and   Boyd - Graber ( 2019 ) show to be most effective at   improving user accuracy .   The guesser ’s accuracy is on par with the experts .   So if the crowdworker players are willing and able   toblindly andprecisely follow Autopilot , they   could achieve good scores . But we consider this as   a degenerate solution to human - AI cooperation .   To account for this issue , we include two   special settings . In Autopilot , we display   Autopilot suggestions as the only explanation   for the human player . In AI - only , wereplace the   human player with Autopilot to make decisions .   Using these two settings , we can quantify to what   degree the human player follows Autopilot .   In our forum post to recruit experts , we promise   an “ interface to augment human players with ex-   planations of AI predictions ” . To stay true to this   promise and ensure a good experience for the ex-   perts ( who participate in Quizbowl for fun ) , we   omit the None baseline in our expert experiments .   This omission should not affect our results since the   baseline is compared to other conditions in Feng   and Boyd - Graber ( 2019 ) .   3.5 Evaluation : Does selector improve EW ?   We use the mean cumulativescore over the   course of the game ( 144 questions for experts and   60 for crowdworkers ) for our quantitative compari-   son . If the human - AI team with a tailored selector   can improve in , this suggests explanations are   helping the users more than other conditions .   Figure 2 shows how the meanscore un-8377   der each condition increases as the players answer   more questions . Among all human - AI cooperative   settings , the Selective condition is the best . Es-   pecially for experts , selective explanation by the   selector is better than both showing all explanations   andAI - only . Importantly , as our model acquires   more data for each user with more questions ( and   as the user acclimatizes to their teammate ) , the gap   between Selective andEverything grows .   Without explanations , crowdworkers are much   worse than AI - only . With selective explanations ,   crowdworkers are comparable to AI - only and   only slightly better than showing all explanations .   Under the Autopilot condition , if players   blindly follow the AI ’s suggestion — buzz when the   Autopilot says so and provide the AI prediction   as the answer — they should match the AI - only   baseline . However , both experts and crowdwork-   ers lose to the AI - only condition . This indicates   that the other conditions evince a synergy : humans   are not simply blindly following AI suggestions .   Rather , the diverse and selective explanations al-   low the players to better decide when to follow and   when to use their own knowledge .   3.6 Evaluation : What does the selector show ?   We are interested in what the selector learns to be   most effective and what it chooses to show to play-   ers . Figure 3 visualizes the evolving distribution of   configurations selected by the bandit selector and   the random selector .   First , the selector did not learn to show all expla-8378nations for all questions — it learned , as the name   suggests , to be selective . And compared to the ran-   dom selector , the selector formed a clear preference   among explanations . In fact , at the end of the game ,   the selector — learning purely from interaction —   recovers the ranking of individual explanations re-   ported by Feng and Boyd - Graber ( 2019 ): highlight   > evidence > alternatives . Interestingly , the selector   did not converge to this ranking until the players   finished about 60 questions : initially the list of al-   ternatives was the preferred explanation , possibly   because it is easier for the players to interpret than   the others . Eventually , as the players get more used   to the other explanations and the selector continues   to learn about the players , it converges .   4 Discussion and Related Work   4.1 Who should drive ?   Clearly defining the shared obligations of the team   is crucial to the success of the team . By design ,   we keep ultimate control of decision making with   the human . However , this may not be optimal ; a   distracted , overloaded , or hesitant human might   be better served by an“taking the wheel ” if it   is certain . The most relevant work to ours is Gao   et al . ( 2021 ) , which similarly uses bandit feedback   to optimize team performance . While our policy   chooses from the set of explanation configurations ,   their policy makes a binary decision : whether to   delegate a decision to the human or leave it to the . Our Autopilot explanation can be seen as   “ soft ” delegation . Future work should compare se-   lective explanation with more methods for delega-   tion and deferral ( Madras et al . , 2018 ; Lubars and   Tan , 2019 ; Kamath et al . , 2020 ; Lai et al . , 2022 ) .   4.2 Alignment and learning to optimize   human objectives   Typically , ML algorithms optimize automatic met-   rics : how well can a machine replace or emulate   a human . However , this is inconsistent with how   humans and machines interact in the real world ; of-   ten models need to be personalized to users ( Zhou   and Brunskill , 2016 ) . The research area that deals   with the general problem of optimizing human ob-   jectives is alignment ( Amodei et al . , 2016 ) . Specif-   ically for human - teams , an unsettled question   is how to optimize for that partnership ; while we   optimize for short - term accuracy , a reasonable alter-   native would be to optimize for longer - term learn-   ing ( Bragg and Brunskill , 2020 ) . An interestingdirection would be to take a real - world task and   directly optimize the underlying model ( not just the   selector ) to create tailored explanations , as Lage   et al . ( 2018 ) did for synthetic tasks .   4.3 Expanding the selector ’s action space   We present this work as another step towards   learned explanations that are more aligned with   human values ( Amodei et al . , 2016 ) . Our method   seeks to maximize a human objective , not heuristic   proxies of that ( Doshi - Velez and Kim , 2018 ) , and   not the objective of the solo machine . In this work ,   we focus on a simplified setting with a limited de-   sign and action space , but our experimental setting   closely mimics how a human - team would oper-   ate in a real - world task ; in particular , our testbed ,   Quizbowl , bears merits that are essential for a task   to have to benefit from this idea .   We focus on this restricted selector to keep the   sample complexity for online optimization under   control . In principle , the selector could be more   fine - grained if we allow it to dynamically change   the configuration as the clues in the question are   revealed . Despite challenges concerning sample   complexity , we believe that this expansion of action   space is a logical next step .   5 Conclusion : Explanations Tailored for   Users   Users benefit from collaborating with an , and   this collaboration can be improved by explaining   thewell . Moreover , the benefit is not universal ,   some users need or thrive with different explana-   tions . However , finding the right combination is   not easy ; while our bandit approach can find useful   explanations , it requires both the user to become   acclimatized to human - AI teaming and the bandit   to explore the space of explanations . As human-   AI collaborations become more common , we must   continue to search for better signals and methods to   help the teaming minimize stress and acclimation   but maximize fun and productivity .   Limitations   As we discussed in Section 1 , a major contribu-   tor to the inconsistency of human - AI experimental   results is the large number of factors that can influ-   ence cooperative effectiveness . One of those fac-   tors that ’s relatively easy to model is the human ’s   skill level . In theory , selective explanation should8379be able to model that : if we optimize selective ex-   planation jointly for experts and crowdworkers , the   selector should be able to learn and choose differ-   ent explanations for the two different groups of   players . Unfortunately , we could n’t have done that   experiment because Quizbowl is too challenging   for crowdworkers without any assistance , and when   they compete head - to - head the game is made more   difficult by the element of competition .   There are other factors of human - AI cooperation   that have been identified by previous work but we   could n’t model : the level of human agency ( Lai   and Tan , 2019 ; Bansal et al . , 2021 ) , the model ’s   predictive accuracy ( Bansal et al . , 2020 ) , the user ’s   mental model of machine learning ( Bansal et al . ,   2019 ) , and the amount of interactivity ( Smith-   Renner et al . , 2020a , b ) . Even within limited in-   teractions , there is significant variation about the   optimal modality of explanations ( Gonzalez et al . ,   2020 ) . Other factors , such as the distribution of   test examples and model architecture , affect the   quality of output from various post - hoc explana-   tion methods ( Ghorbani et al . , 2019 ; Jones et al . ,   2020 ) .   Another major limitation of our evaluation is that   we only experimented with one question answering   problem , Quizbowl . Our method is generally ap-   plicable to decision making problems . But finding   another suitable task and adapting our infrastruc-   ture , experiment design , and incentive structures   is highly non - trivial . We are actively looking for   other problems to experiment on and hope to con-   duct more extensive experiments in the future .   Ethics Statement   All of our work is conducted under the supervision   and approval of an institutional review board . Our   annotators are fairly compensated for their work ,   and we have attempted to make the activity as in-   trinsically rewarding as possible .   Acknowledgments   We thank the anonymous reviewers and meta-   reviewer for their suggestions and comments .   Boyd - Graber is supported by Grant IIS-   1822494 . Any opinions , findings , conclusions , or   recommendations expressed here are those of the   authors and do not necessarily reflect the view of   the sponsors . References838083818382