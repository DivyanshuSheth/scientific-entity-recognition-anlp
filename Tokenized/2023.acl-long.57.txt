  Lingxi Zhang , Jing Zhang , Yanling Wang , Shulin Cao , Xinmei Huang ,   Cuiping Li , Hong Chen , Juanzi LiSchool of Information , Renmin University of China , Beijing , ChinaDepartment of Computer Science and Technology , Tsinghua University , Beijing , China   { zhanglingxi , zhang - jing , wangyanling , huangxinmei , licuiping , chong}@ruc.edu.cn   { caosl19}@mails.tsinghua.edu.cn , { lijuanzi}@tsinghua.edu.cn   Abstract   The generalization problem on KBQA has   drawn considerable attention . Existing research   suffers from the generalization issue brought   by the entanglement in the coarse - grained mod-   eling of the logical expression , or inexecutabil-   ity issues due to the fine - grained modeling   of disconnected classes and relations in real   KBs . We propose a Fine - to - Coarse Compo-   sition framework for KBQA ( FC - KBQA ) to   both ensure the generalization ability and exe-   cutability of the logical expression . The main   idea of FC - KBQA is to extract relevant fine-   grained knowledge components from KB and   reformulate them into middle - grained knowl-   edge pairs for generating the final logical ex-   pressions . FC - KBQA derives new state - of - the-   art performance on GrailQA and WebQSP , and   runs 4 times faster than the baseline . Our code   is now available at GitHub https://github .   com / RUCKBReasoning / FC - KBQA .   1 Introduction   Question answering over knowledge bases ( KBQA )   aims to provide a user - friendly way to access large-   scale knowledge bases ( KBs ) by natural language   questions . Existing KBQA methods ( Zhang et al . ,   2023 ) can be roughly categorized into retrieval-   based and semantic - parsing ( SP ) based methods .   The former ( Feng et al . , 2021 ; He et al . , 2021a ;   Zhang et al . , 2022 ) directly scores the relevance   between the question and answer candidates , thus it   is difficult to resolve the complex questions . On the   contrary , some KBQA approaches , such as ( Das   et al . , 2021 ; Kapanipathi et al . , 2021 ; Qiu et al . ,   2020 ; Sun et al . , 2020 ) , are based on semantic pars-   ing ( denoted as SP - based ) , which can address com-   plex questions and achieve promising results on   i.i.d . datasets . SP - based methods first translate the   questions into logical expressions such as SPARQL   and then execute them against KB to yield answers . Figure 1 : Illustration of generalization tasks in KBQA .   Each question is paired with a logical expression that   consists of different components . Components involved   in the training data are colored in non - green color , while   unseen components are colored in green . Figure 2 : Results of the pilot study . The coarse - grained   method directly matches the question with the logical   expression ( i.e. , the composition of components ) , while   the fine - grained method matches the question with each   component candidate and then composes them to derive   the logical expression . The exact match accuracy of   logical expressions on compositional generalization test   data and zero - shot generalization test data is shown on   the right of the figure .   As illustrated in Figure 1 , a logical expression con-   sists of multiple components such as classes and   relations . Most existing SP - based approaches fail   with logical expressions that contain unseen compo-   sitions of components ( called compositional gener-   alization ) or unseen components ( called zero - shot   generalization ) .   To address the above problem , GrailQA-   Rank ( Gu et al . , 2021 ) proposes a BERT - based   rank model to match the given question with each   logical expression candidate , which leverages the   generalization abilities of the pre - trained language   models . On top of that , RNG - KBQA ( Ye et al . ,10022022 ) further uses a pre - trained generation model ,   which takes top-5 ranked logical expressions as the   additional input beyond the question to generate   the target logical expression . Behind these main-   stream models , a logical expression is viewed as an   inseparable unit during modeling . Actually , logical   expressions are coarse - grained because they can   be decomposed into relatively fine - grained com-   ponents including relations , classes , entities , and   logical skeletons ( See examples in Figure 3 ) . Such   coarse - grained modeling entangles representations   of fine - grained components , thereby overfitting   the seen compositions during the training process ,   which weakens the model ’s compositional general-   ization ability . Meanwhile , even though pre - trained   language models can deal with zero - shot compo-   nents to some extent , compositional overfit reduces   their ability to identify individual unseen compo-   nents with zero - shot generalization .   To demonstrate the above idea , we perform a   pilot study ( Cf . the detailed settings in Section 4.1 )   with two preliminary experiments : one calculates   the similarity score between a question and each   coarse - grained logical expression to obtain the   most relevant one , and the other searches the most   relevant fine - grained components to form the final   logical expression of a question . We observe that   the fine - grained modeling derives more accurate   logical expressions on both the compositional   task and zero - shot task ( Cf . Figure 2 ) . It could   be explained that fine - grained modeling focuses ex-   clusively on each component , avoiding overfitting   of seen compositions in the training data . Although   some studies attempt to leverage fine - grained com-   ponents , they only consider partial fine - grained   components such as relations , classes , and entities   ( Chen et al . , 2021 ) , or suffer from inexecutability   due to disconnected fine - grained components in   real KBs ( Shu et al . , 2022 ) .   Thus , to both ensure the generalization ability   and executability of logical expressions , we pro-   pose a Fine - to- Coarse composition framework for   KBQA ( FC - KBQA ) , which contains three sub-   modules . The overview of our model is shown   in Figure 4 . The first module is fine - grained com-   ponent detection , which detects all kinds of fine-   grained component candidates from Freebase by   their semantic similarities with the question . Such   component detection guarantees the generalization   ability in both compositional and zero - shot tasks .   The second module is the middle - grained compo - nent constraint , which efficiently prunes and com-   poses the fine - grained component candidates by en-   suring the components ’ connectivity in the KB . The   final module is the coarse - grained component com-   position , which employs a seq - to - seq generation   model to generate the executable coarse - grained   logical expression . In addition to encode the fine-   grained components , the middle - grained compo-   nents are also encoded to enhance the model ’s rea-   soning capacity , so as to improve the executability   of the generated logical expression . In contrast   to previous work ( Cao et al . , 2022b ; Chen et al . ,   2021 ; Shu et al . , 2022 ) that only uses the knowl-   edge constraints to guide the decoding process , we   emphasize injecting them into the encoding pro-   cess , because the encoder which learns bidirec-   tional context could better suit natural language   understanding ( Du et al . , 2022 ) .   We conduct extensive experiments on widely   used GrailQA , WebQSP , and CWQ datasets .   GrailQA ( Gu et al . , 2021 ) is a KBQA bench-   mark focusing on generalization problems . FC-   KBQA derives new state - of - the - art performance   on GrailQA - Dev ( +7.6 % F1 gain and +7.0 % EM   gain respectively ) . Meanwhile , FC - KBQA also   obtains good performance on WebQSP and CWQ .   Moreover , FC - KBQA runs 4 times faster than the   state - of - the - art baseline RNG - KBQA . The abla-   tion studies demonstrate the effect of our middle-   grained encoding strategy .   Contributions . ( 1 ) We conduct a pilot study to   reveal an intriguing phenomenon — a fine - grained   understanding of the logical expression helps en-   hance the generalization ability of SP - based KBQA   methods , which is rarely discussed before . ( 2 ) We   propose a fine - to - coarse composition framework   FC - KBQA to address the generalization problem ,   which takes advantage of the idea of fine - grained   modeling . ( 3 ) We devise a middle - grained compo-   nent constraint that is injected into both the encoder   and the decoder to guide the seq - to - seq model   in producing executable logical expressions . ( 4 )   FC - KBQA not only maintains efficiency but also   achieves significant improvement on GrailQA .   2 Related Work   Coarse - Grained SP - based Methods . Many ef-   forts are paid to solve generalization problems on   SP - based KBQA . Some approaches , such as ( Lan   and Jiang , 2020 ; Gu et al . , 2021 ) , use a rank - based   model that takes advantage of a coarse - level match1003between the question and the logical expressions or   query graphs . They first enumerate numerous query   graph candidates based on KBs and then they rank   them according to how relevant they are to the ques-   tion . Another line of approaches , in addition to the   rank - based ones , makes use of a generation model .   KQAPro ( Cao et al . , 2022a ) leverages BART to   directly convert questions into logical expressions .   Additionally , RNG - KBQA ( Ye et al . , 2022 ) further   injects top - k ranked logical expressions as an ad-   ditional input to the question . CBR - KBQA ( Das   et al . , 2021 ) injects analogous questions and their   corresponding logical expressions from the training   data to increase the generalization . All of the afore-   mentioned methods are pure coarse - level frame-   works that treat each coarse - grained logical expres-   sion as a separate unit .   Fine - Grained SP - based Methods . Many re-   searchers have been motivated to address the gen-   eralization issue by the notion of utilizing decom-   posed components , such as class , relation , and log-   ical skeleton . Some approaches ( Wang et al . , 2020 ;   Zhao et al . , 2022 ; Li et al . , 2023 ) retrieve the rele-   vant schema item such as relation and column as   additional fine - grained input information , while an-   other line of approaches ( Dong and Lapata , 2018 )   extracts the skeleton of logical expression as the de-   coder guide . Such methods primarily concentrate   on the grammar of logical expression and often   ignore the knowledge constraint , which is essen-   tial in large - scale KB . They usually focus on KBs   or DBs that contain a small number of relations   where a logical expression can be easy to be exe-   cutable . Program Transfer ( Cao et al . , 2022b ) , Re-   Track ( Chen et al . , 2021 ) , and TIARA ( Shu et al . ,   2022 ) simply apply KB constraints to control the   generation of the decoding process . As opposed   to them , we make use of middle - grained KB con-   straints during both the encoding and the decoding   processes to help the model better adapt to KB and   ensure executability .   3 Problem Definition   Knowledge Base ( KB ) . A KB is comprised by   ontology { ( C×R×C)}and relational facts   { ( E×R×(E∪C ) ) } , where R , C , andEdenote   relation set , class set , and entity set respectively .   Notably , we consider literal as a special type of en-   tity . Specifically , an ontology triple ( c , r , c)con-   sists of a relation r∈R , a domain class cwhich   denotes the class of the subject entities , and a rangeclasscwhich denotes the class of the object enti-   ties . Each class has multiple entities , thus an ontol-   ogy triplet can be instantiated as several relational   facts . For example , both ( e , r , e)and ( e , r , e )   correspond to ( c , r , c ) , where e , e∈cand   e , e∈c . Figure 3 illustrates a KB subgraph .   SP - based KBQA . Given a natural question q ,   KBQA models aim to find a set of entities denoted   byA⊆Efrom KB as the answers to q. Instead   of directly predicting A , SP - based KBQA models   translate qto an executable logical expression de-   noted by ssuch as SPARQL , lambda - DCS ( Liang   et al . , 2013 ) , query graph ( Lan and Jiang , 2020 ) ,   and s - expression ( Gu et al . , 2021 ) .   We select s - expression as our used logical ex-   pression since it could provide a good trade - off   on compactness , compositionality , and readabil-   ity ( Gu et al . , 2021 ) . The logical skeleton of an   s - expression can be derived by removing all the   relations , classes , and entities in the expression   and only keeping function operators and parenthe-   ses . Specifically , we replace relations , classes , enti-   ties , literals with special tokens “ < rel > ” , “ < class > ” ,   “ < entity > ” , “ < literal > ” respectively . Figure 3 shows   an executable logical expression on the KB and   its corresponding logical skeleton . We unitedly   name the relations , classes , entities , and logical   skeleton in an s - expression as the fine - grained   component , while the complete s - expression is   thecoarse - grained logical expression .   4 Approach   4.1 Pilot Study   As analyzed in Section 1 , considering the logical   expression as a unit will lead to entangled represen-   tations of fine - grained components and thus weak-   ens generalization ability . Here we study the ne-   cessity of fine - grained modeling by testing how   coarse - grained and fine - grained matching methods   perform when selecting a question ’s logical expres-   sion from the corresponding candidate pool .   Dataset . To simplify the experiment , we extract a   toy dataset that only involves 1 - hop logical expres-   sions from GrailQA . Then , for the relation rand   the class cin such logical expressions , we study   the compositional generalization where the compo-   sition ( r , c)is unseen or zero - shot generalization   where the individual rorcis unseen in the training   data . For each question with its ground - truth logi-   cal expression , we select 100 logical expressions1004   that share the same domain as the ground truth as   the coarse - grained expression candidates . For fair   comparison , we separate all of the relations , classes ,   and logical skeletons from the coarse - grained can-   didates as the fine - grained component candidates .   Methods . We aim to find the target logical expres-   sion of a given question by a ranking model trained   with a contrastive loss ( Chen et al . , 2020 ) , which   is also used by RNG - KBQA ( Ye et al . , 2022 ) . The   coarse - grained method concatenates a question and   a candidate logical expression to feed into BERT ,   then the output embedding of [ CLS ] is fed into a   linear layer to compute the similarity score . The   fine - grained method follows the above pipeline , but   the input is the concatenation of a question and a   fine - grained candidate component , then scores each   logical expression candidate by summing up the   normalized question - component similarity scores .   For both methods , we compute accuracy by evalu-   ating whether the ground - truth logical expression   owns the highest score in the candidate pool .   Observation — Fine - grained modeling can bet-   ter solve the generalization problems on KBQA .   The matching accuracy is reported in Figure 2 .   The fine - grained method outperforms the coarse-   grained method in both composition generaliza-   tion and zero - shot generalization tasks . A possi-   ble explanation is the fine - grained matching fo - cuses solely on each component and is simple to   learn , which better capture the semantic informa-   tion of each component and also well adaptable to   express the various compositions of components .   The coarse - grained matching , on the other hand , at-   tempts to describe all of the components as a whole   composition , limiting the ability to express unseen   compositions and components . Inspired by this , we   propose FC - KBQA in the next section .   4.2 Model Overview   We propose a fine - to - coarse composition frame-   work FC - KBQA bridged by a middle - grained KB   constraint . Figure 4 illustrates the overall frame-   work , which contains three parts :   Fine - grained Component Detection . Given a   question , we extract relation candidates and class   candidates from the whole KB based on seman-   tic similarity . Simultaneously , we adopt an entity   linker to detect mentioned entities and use a seq - to-   seq model to generate logical skeletons .   Middle - grained Component Constraint . Based   on the detected components , we devise an efficient   way to check the connectivity of component pairs   on the KB , including class - relation pairs , relation-   relation pairs , and relation - entity pairs . We only   keep the executable component pairs to guarantee   the executability of final logical expression .   Coarse - grained Component Composition . Fi-   nally , a seq - to - seq model takes the concatenation   of the question and the reformulated components   as input to generate the logical expression . In par-   ticular , the middel - grained components are injected   into both the encoder and the decoder to ensure the   executability of the final logical expressions .   4.3 Fine - grained Component Detection   Relation and Class Extraction . Taking the re-   lation extractor as the example , given a question   q , we aim to extract relations in q. First , we apply   BM25 ( Robertson et al . , 2009 ) to recall the relation   candidates from the KB based on the surface over-   laps between relations ’ names and q. Then we ap-   ply BERT ( Devlin et al . , 2019 ) as the cross - encoder   to measure the semantic similarity between qand   each relation candidate r. We describe rusing the   relation domain , the relation name , and the rela-   tion range and let the BERT input be “ [ CLS ] q [ D ]   domain(r ) [ N ] name(r ) [ R ] range(r ) [ SEP ] ” , where   [ CLS ] , [ SEP ] , [ D ] , [ N ] , and [ R ] are the special to-   kens . To better distinguish the spurious relations,1005   we sample the relations that share the same domain   as the ground - truth relation as the negatives for   training . The trained model is used to retrieve the   set of top- krelations , denoted by R.   The class extractor works in the same way as   the relation extractor . We represent the class using   its name and domain , and use other classes in the   same domain as negatives . Crepresents the set of   the top- krelevant classes .   Entity Linking . A common paradigm of finding   topic entities in KBQA methods is to first leverage   a NER tool ( Finkel et al . , 2005 ) to detect mentions   and then apply an entity disambiguation model to   link them to entities in KB . However , some noun-   phrase mentions such as “ rich media ” are hard to   be detected by the NER tool , and some ambiguous   entities could not be distinguished by the pure en-   tity names . To address both issues , we equip the   NER toolwith a trie tree - based mention detec-   tion method and propose a relation - aware pruning   method to filter the mentions .   Specifically , we build a trie tree ( Fredkin , 1960 )   with the surface names of all entities in the KB .   Then we can search noun phrase mentions in the   question efficiently and link them to the KB byBLINK ( Wu et al . , 2020 ) to obtain the correspond-   ing entities E. After that , we propose a relation   awared pruning strategy to prune Eby removing   the entities that could not link to any relations in R.   Finally , following GrailQA ( Gu et al . , 2021 ) , we   choose the entity with the highest popularity . We   define regular expressions to extract literals such   as digits and years appearing in q.   Logical Skeleton Parsing . Logical skeleton pars-   ing aims to transform a given question qinto a   logical skeleton l. Because the logical skeleton is   domain - independent , the parsing process could be   generalized across domains . We adopt T5 ( Raffel   et al . , 2020 ) , a state - of - the - art generation model to   parse logical skeletons . Since many entity names   contain tokens such as “ and ” and “ of ” that may   cause the logical skeleton to be incorrectly deter-   mined , we mask each mention m∈Mwith the   special token “ < entity0 > ” , “ < entity1 > ” , ... , in order   of appearance . For example , we change “ Thomas   was the designer of what ship ? ” to “ < entity0 >   was the designer of what ship ? ” . We notice that a   common error is parsing out logical skeleton with   wrong relation numbers , for example “ < rel > ” in-   stead of “ < rel><rel > ” . Instead of increasing beam   numbers , we manually add grammar rules , such   as add “ < rel><rel > ” as the second candidate when1006“<rel > ” is T5 ’s top-1 prediction . The set of the   top-2 logical skeleton candidates is denoted as L.   4.4 Middle - grained Component Constrain   After deriving the candidate components according   to Section 4.3 , the KB - based constraint is required   to guarantee the composed logical expression is   executable . A straightforward idea is to fill the   logical skeleton with candidate relations , classes ,   and entities , and execute them one by one to check   executability . However , such enumeration is ineffi-   cient , since all combinations of candidate compo-   nents should be considered . Therefore , we incor-   porate the middle - grained component pairs which   are connected in KB . Such pairs can be produced   efficiently to keep the model ’s efficiency .   The middle - grained component pairs include   class - relation pairs , relation - relation pairs , and   relation - entity pairs . For each class c∈Cand   each relation r∈R , ifris connected with   the domain class c , we add ( c , r)into the class-   relation pair set P. For example in Figure 3 ,   the class “ railway.railway ” is linked with the re-   lation “ rail.railway.terminuses ” , so the pair ( rail-   way.railway , rail.railway.terminuses ) is executable   and will be added into P. If the range class   ofrisc , we add the pair of cand the reverse   relation of r. We construct executable relation-   relation pair set Pby checking each relation   pair ( r∈R , r∈R ) . Ifr ’s domain class   does not match r ’s range class , we directly re-   move this pair to maintain efficiency , otherwise ,   we reformulate ( r , r)to a logical expression and   execute on KB to check its connectivity . For each   relation - entity pair ( r , e ) , we first check whether   the logical skeleton candidates contain the < entity >   placeholder or not . If not , we leave Pempty ;   otherwise we directly take the result of the relation-   pruning strategy for entities in Section 4.3 .   4.5 Coarse - grained Component Composition   We apply a generation model based on T5 to com-   pose all the above fine - grained and middle - grained   component candidates and output an executable   logical expression by a controlled decoder .   Encoding Process . Before feeding the fine-   grained and middle - grained component candidates   into the generator , we sort the middle - grained can-   didates according to their similarity scores to the   question . By doing this , the order can reveal the   pattern of which pair is more likely to appear in theground - truth logical expression . In intuition , such   a pattern will help to generate more accurate log-   ical expressions . To accomplish this , we take the   logits of the fine - grained component detection in   section 4.3 as the similarity score between the ques-   tion and each class / relation component , and then   calculate the similarity score between the question   and a middle - grained component pair by summing   the scores of contained single components . The   encoding of such middle - grained component im-   proves the generator ’s reasoning capacity in terms   of capturing the knowledge constraints .   We use “ ; ” to separate each element ( a compo-   nent or a component pair ) . To explicitly inform   the model the type of each component , we place   “ [ REL ] ” , “ [ CL ] ” , “ [ ENT ] ” , and “ [ LF ] ” before   each relation , class , entity , and logical skeleton   respectively . For example , we organize the input   of encoder as “ query;[CL ] c[REL ] r;[REL ] r   [ REL ] r;[CL]c[REL ] r;[ENT ] e;[LF]l;[LF]l ” .   Decoding Process . The middle - grained compo-   nents are also used to produce a dynamic vocab-   ulary to constrain the decoding process . The gen-   erated token yis confined to the tokens involved   in the dynamic vocabulary at each step t. We ini-   tialize the dynamic vocabulary with the union of   tokens from the detected entities , tokens from the   detected classes in P , i.e. , usually the answer   type , and the keywords such as “ JOIN ” in logical   skeleton . Then we update the dynamic vocabulary   by the relations paired with rinPif the last   generated component is ror by the relations paired   withcinPif it is c.   5 Experiment   5.1 Experimental Settings   Dataset . We evaluate our method on GrailQA ( Gu   et al . , 2021 ) , WebQSP ( Yih et al . , 2016 ) , and   CWQ ( Talmor and Berant , 2018 ) , all of which are   based on Freebase . GrailQA focuses on generaliza-   tion problems which involved up to 4 - hop logical   expressions and complex operations . WebQSP is   an i.i.d . benchmark that required 2 - hop reasoning .   Although CWQ is not designed to solve generaliza-   tion problem , we can still separate out the zero - shot   test set with all the unseen relations and classes ,   yielding 576/3519 zero - shot / all test set .   Evaluation Metrics . To measure the accuracy of   logical expression , we use the well - adopted exact   match ( EM ) which measures the exact equivalence1007   Overall I.I.D. Compositional Zero - Shot   EM F1 EM F1 EM F1 EM F1   GrailQA - Rank ( Gu et al . , 2021 ) 50.6 58.0 59.9 67.0 45.5 53.9 48.6 55.7   GrailQA - Trans ( Gu et al . , 2021 ) 33.3 36.8 51.8 53.9 31.0 36.0 25.7 29.3   ReTrack ( Chen et al . , 2021 ) 58.1 65.3 84.4 87.5 61.5 70.9 44.6 52.5   RNG - KBQA ( Ye et al . , 2022 ) 68.8 74.4 86.2 89.0 63.8 71.2 63.0 69.2   FC - KBQA(Ours ) 73.2 78.7 88.5 91.2 70.0 76.7 67.6 74.0   between the query graph of the predicted and the   gold logical expression . We also calculate the F1   score based on the predicted and gold answers .   Baselines . On GrailQA , we mainly compare   with the published works on the leaderboard , in-   cluding GrailQA - Rank ( Gu et al . , 2021 ) , GrailQA-   Trans ( Gu et al . , 2021 ) , Retrack ( Chen et al . , 2021 ) ,   RNG - KBQA ( Ye et al . , 2022 ) . They are all SP-   based models that target generalization problems   in KBQA . On WebQSP and CWQ , we compare   our method with the retrieval - based models in-   cluding GraphNet ( Pu et al . , 2018),PullNet ( Sun   et al . , 2019 ) and NSM ( He et al . , 2021b ) , and the   SP - based models including QGG ( Lan and Jiang ,   2020 ) , RNG - KBQA ( Ye et al . , 2022 ) , and PI Trans-   fer ( Cao et al . , 2022b ) . We evaluate F1 for the   retrieval - based models , while evaluate both F1 and   EM for the SP - based methods . We compare all the   baselines that have the results on the two datasets   or publish the codes that can be executed .   5.2 Overall Evaluation   Performance . In Table 1 and Table 2 , we eval-   uate the performance of FC - KBQA on different   datasets . For the baselines , we directly take their   results reported in the original papers . To be noted ,   on the extracted zero - shot test set of CWQ , the re-   sults for some models remain empty because their   full codes are not released . As shown in Table 1 ,   our model outperforms all the baselines , especially   on the compositional and zero - shot test tasks . Com-   pared with RNG - KBQA , the state - of - the - art pub-   lished model , we have an absolute gain of 4.3 % and   4.4 % in terms of F1 score and EM respectively . We   also outperform on the extracted zero - shot CWQ   test set by 11.3 % in terms of F1 , as for an unseen   complex question , parsing out correct knowledge   components and logical skeletons is much easier   than directly parsing the coarse - grained logical ex-   pression correctly . Since the fine - grained module   solely focuses on each component and thus leads   to a higher component accuracy , FC - KBQA also   outperforms on the i.i.d test set of WebQSP . On the   original test set of CWQ , we only under - perform   PI Transfer which leverages a pre - train process on   a large - scale wiki data that is out scope of CWQ .   Efficiency . Both RNG - KBQA and GrailQA - Rank   enumerate all the logical expressions in a 2 - hop KB   subgraph ( enumeration ) , so it is time - consuming   for the rank model to score thousands of logical   expressions for each question ( candidate selection ) .   Conversely , our FC - KBQA just retrieves the most   relevant components ( candidate selection ) and then   enumerates the component pairs based on the fil-   tered candidates ( enumeration ) , which greatly re-   duces the inference time . Besides enumeration and   candidate selection , a seq - to - seq model is used to   generate the final logical expression ( final composi-   tion ) . In the same 24 GB GPU and Intel Gold 5218   CPU , the experimental results in Figure 5 show that   our model runs 4 times faster than baselines .   5.3 Ablation Studies   GrailQA does not provide ground truth for the test   set , so we conduct the ablation studies on the public   Grail - Dev to investigate how the fine- and middle-   grained components affect the performance .   As shown in Table 3 , we develop four model   variants . ( 1 ) -Knowledge removes all the fine-   grained and middle - grained components except   for the logical skeleton . ( 2 ) -Knowledge Pairs   replaces the middle - grained components , such as1008   Overall I.I.D. Compositional Zero - Shot   EM F1 EM F1 EM F1 EM F1   T5 - base 22.7 23.4 61.8 64.1 28.3 29.0 0.3 0.3   RNG - KBQA 71.4 76.8 86.5 88.9 61.6 68.8 69.0 74.8   Enhanced RNG - KBQA 72.8 78.2 86.6 90.2 61.7 69.3 71.5 76.7   FC - KBQA 79.0 83.8 89.0 91.5 70.4 77.3 78.1 83.1   – Knowledge 23.1 24.0 62.1 64.2 29.5 31.0 0.3 0.3   – Knowledge Pairs 53.6 55.6 70.2 72.3 44.0 46.0 50.3 52.2   – Logical Skeleton 78.0 80.8 85.2 86.8 68.5 71.9 79.2 81.8   – Decode Constraint 77.5 83.1 88.3 91.1 67.8 76.3 76.8 82.5   class - relation pairs and relation - relation pairs with   the corresponding fine - grained candidates , such as   classes and relations . ( 3 ) -Logical Skeleton gets   rid of the logical skeleton . ( 4 ) -Decode Constraint   deletes the dynamic vocabulary created with the   middle - grained components .   The results show that removing “ knowledge ” re-   duces model performance by 60 % F1 score , and   replacing “ knowledge pairs ” with pure fine - grained   components also reduces model performance by   28 % F1 , indicating that encoding the middle-   grained components can significantly improve the   model ’s reasoning capacity . To further demonstrate   that encoding such middle - grained components can   also help improve other model ’s performance , we   create Enhanced RNG - KBQA by taking the top-10   ranked results from its ranking model and formulat-   ing them into middle - grained component pairs to   be injected into its encoder . The results in Table 3   show that middle - grained reformulation improves   the performance of RNG - KBQA . Middle - grained   component pairs , like coarse - grained logical ex-   pressions , can guarantee connectivity , but they aremore compact and much shorter . As a result , be-   cause PLMs have a maximum input length , the   middle - grained formulation can inject more com-   ponents and is more likely to cover the components   involved in the target logical expression .   Removing “ logical skeleton ” can result in a 3.0 %   F1 drop , indicating that skeleton is useful for guid-   ing the question understanding even though it is   less important than the knowledge .   Removing “ decode constraint ” in the decoder   can also have an effect on model performance , but   is much weaker than removing “ knowledge pairs ”   in the encoder , indicating that injecting the knowl-   edge constraints in the encoding process is more   useful than in the decoding process , because the en-   coder learns the bidirectional context , which is bet-   ter suited to natural language understanding . This   is also a significant difference from the existing   knowledge constrained decoding methods .   Both " Knowledge Pairs " and " Decode Con-   straint " are proposed for addressing the in-   executability issue , which guarantee all generated   logical expressions are executable . Removing ei-   ther reduces the accuracy , which indicates that high   executability can improve the model performance .   5.4 Error Analysis   We randomly select 50 error cases on GrailQA and   summarize the error into three main categories :   error entity ( 60 % ) , error relation and class ( 35 % ) ,   and error logical skeleton ( 40 % ) . We also analysis   the error cases while our model fails but some   baseline methods can answer successfully resolve   them . A typical mistake is on logical expressions   that involve KB - specific component composition .   For example , in Freebase , “ coach ” is represented   by the join of “ sports.sports_team.coaches ”   and “ sports.sports_team_coach_tenure.coach”.1009Our fine - to - coarse model only predicts the   previous relation but is unable to recall   “ sports.sports_team_coach_tenure.coach ” , while   some coarse - grained methods are able to memorize   such composition and provide the correct answer .   6 Conclusion   This paper proposes FC - KBQA , a Fine - to- Coarse   composition framework for KBQA . The core idea   behind it is to solve the entanglement issue of main-   stream coarse - grained modeling by the fine - grained   modeling , and further improve the executability   of logical expression by reformulating the fine-   grained knowledge into middle - grained knowledge   pairs . Benefiting from this , FC - KBQA achieves   new state - of - the - art performance and efficiency   on the compositional and zero - shot generalization   KBQA tasks . This fine - to - coarse framework with   middle - grained knowledge injection could be in-   spiring for generalization on other NLP tasks .   7 Limitations   Although our model achieves good performance   in solving the compositional and zero - shot   generalization problems , there is still room   for improvement on the i.i.d datasets . The   fine - grained module in our framework can not   take advantage of explicit composition infor-   mation when the component compositions in   the testing set and training set significantly   overlapp . For example , in Freebase , " Who is   the coach of FC Barcelona ? " is answered by the   join of relation “ sports.sports_team.coaches ” and   “ sports.sports_team_coach_tenure.coach ” . Our   fine - grained extractor may fail to recall   “ sports.sports_team_coach_tenure.coach ” and in-   stead select “ base.american_football.football_coac   -h.coach ” as the candidate since ‘ football coach ”   is more relevant to the question than “ coach   tenure ” in semantics . The only coarse - grained   model , however , can directly memorize the pattern   because such composition appears frequently   in the training data . Therefore , compared to   conventional models that completely memorize   composition patterns , our model may only have   minor advantages .   Another limitation is that we can not guar-   antee the generalization on other KBs such   as WikiData because gaps between KBs may   bring negative impact . For example , rela-   tions in Freebase are often more specific(ice_hockey.hockey_player.hockey_position ,   soccer.football_player.position_s ) , while re-   lations in Wikidata are more general ( posi-   tion_played_on_team ) . We consider it as a   direction for our future work .   8 Ethics Statement   This work focuses on the generalization issue of   knowledge base question answering , and the con-   tribution is fully methodological . Hence , there are   no direct negative social impacts of this work . For   experiments , this work uses open datasets that have   been widely used in previous work and are without   sensitive information as we know . The authors of   this work follow the ACL Code of Ethics and the   application of this work have no obvious issue that   may lead to the risk of ethics .   Acknowledgments   This work is supported by National Natural Sci-   ence Foundation of China ( 62076245 , 62072460 ,   62172424,62276270 ) ; Beijing Natural Science   Foundation ( 4212022 ) .   References10101011   A Implementation Detail   KB Environment . To execute the SPARQL , we   build a virtuoso database with the latest official   data dump of Freebase .   Pilot Study . To simulate the generalization prob-   lems , the training set and test set are drawn from   GrailQA ’s training set and test set , respectively . To   build the toy train set , we choose two thousand   cases with only the 1 - hop logical expression from   the GrailQA train set . In addition , for the com-   positional test set , we select the 1 - hop cases from   the GrailQA test set , which contains seen single   relations and classes but unseen class - relation pairs   beyond the train set . For the zero - shot test set ,   we select the 1 - hop cases that involve both a class   and a relation that does not appear in the toy train   set . To be noted , as coarse - grained modeling in-   volves the enumeration of logical expressions to   obtain candidates , and the enumeration is nearly   impossible for 2 - hop logical expressions due to the   large amount ( greater than 2,000,000 ) . So , we sim-   plify the pilot study to only 1 - hop questions that   involve the composition of a class and a relation ,   which can also support comparing fine - grained and   coarse - grained modeling .   For both the coarse - level and fine - level matching   methods , we apply a BERT - based - uncased model .   Both models are trained for 5 epochs with a batch   size of 8 and a learning rate of 2e-5 . To demonstrate   the capacity of the models and make an objective   comparison , we also employ the contractive loss   with a random negative sample for both strategies .   Extraction Model . For both the relation extractor   and class extractor , we also apply the BERT - based-   uncased model . The encoder accepts the concatena-   tion of the question qand relation ror the class cas   the input , and then a linear layer projects the output   [ CLS ] embedding into a similarity score s(q , r)or   s(q , c ) . The BERT is fine - tuned by optimizing a   contrastive loss ( Chen et al . , 2020),1012   Accuracy   GrailQA 68.0   RNG - KBQA 81.6   Ours 87.2   – Relation - aware Pruning 83.0   L(q , r ) = −loge   e+/summationtexte   where ris one of the golden relations extracted   from the target logical expression , and { r}is the   set of the negative relations sampled from relation   set which shares the same domain as r. We   sample 48 negative candidates for each sample and   fine - tune BERT - base - uncased for 10 epochs with a   batch size of 8 and a learning rate of 2e-5 .   Generation Model . We initiate both of our seq-   to - seq models with T5 - based provided by the hug-   gingface library ( Wolf et al . , 2020 ) . For logical   skeleton parsing , we fine - tune for 5 epochs with a   batch size of 4 and a 4 - step gradient accumulation .   For the final composition model , we fine - tune for   10 epochs with a batch size of 8 and a 4 - step gradi-   ent accumulation . To be noted , both the designed   rules for logical skeleton parsing and vocabulary   constraints in decoding process will not be used in   the training process , and both training object follow   the regular BART .   B Component Detection Models .   Entity Linking . As shown in Figure 4 , com-   pared with the entity linking ( EL ) strategy in RNG-   KBQA , our EL strategy gains 5.6 % accuracy im-   provement . The reasons include ( 1 ) the trie tree   considers all entities ’ surface names , ensuring the   high coverage of entity candidates , ( 2 ) the relation-   aware pruning strategy can effectively remove hard   negatives with similar mentions but completely dif-   ferent semantics .   Relation and Class Extraction . Figure 6 depicts   the effects of varying different sizes ( k ) of relations   and classes . With the increase of k , the relation   or class coverage represented by accuracy begins   to grow slowly and attends to be stable when k   is around 10 . Meanwhile , the complexity of com-   position enumeration grows exponentially with k. Thus , to balance efficiency and performance , we   choose top-10 relations and top-10 classes .   Logical Skeleton Parsing . Table 5 displays the   effectiveness of logical skeleton parsing techniques   for various beam searches . “ Raw Question ” refers   to directly parsing the raw question into the logical   skeleton , while “ + Mask ” refers to parsing using   our entity mask strategy . For both the strategies ,   in addition to the top-1 , top-2 , and top-3 beam   search results , we also report the results of Top-   2(R ) which add “ < rel><rel > ” as the top-2 candi-   date if “ < rel > ” is the top-1 prediction , vice versa .   We can see that our designed entity mask strat-   egy and rule - based beam search can contribute   to the logical skeleton parsing . The rules signifi-   cantly improve the performance as 1 - hop relation   and 2 - hop relations are quite mix up in KBs . For   example , the semantic - grained one - hop relation   “ program producer ” could be represented by a 1-   hop relation ( “ tv.tv_producer.programs_produced ”   in domain TV ) or a 2 - hop relations ( “ broad-   cast.content.producer ” and “ radio.radio_subject.p-   rograms_with_this_subject ” in domain radio ) .   C Running Example   We here give a running example of our frame-   work for better understanding . As shown in Fig-   ure 4 , given the question “ the terminuses of Anto-   nio belongs to what railway ? ” , we first propose   fine - grained component detection . We retrieve   candidate classes “ railway ” , “ railway_terminus ” ,   “ railway_type ” , ... and candidate relations “ rail-   way.terminuses ” , “ railway.branches_to ” , “ tran-   sit_line.terminuses ” , ... , and candidate entities “ An-   tonio ” which is a football player,“Antonio ” which   is a city , ... , and logical skeleton candidates . Then ,   we apply the middle - grained constrain , for ex-   ample , for class - relation pairs , “ railway ” is con-   nected to “ railway.terminuses ” in KB but not   connected to “ railway.branches_to ” ; for relation-   relation pairs,“railway.terminuses ” shares matched   domain and range with “ railway.branches_to ” but   not share with “ transit_line.terminuses ” ; for enti-   ties , the football player “ Antonio ” does not match   any candidate relations and will be pruned . Fi-   nally , we put question , all connected class - relation   pairs , all connected relation - relation pairs , all enti-   ties that have not been pruned and logical skeleton   candidates into the composition model and gener-   ate logical expression.1013   Top-1 Top-2 Top-3 Top-2(R )   Raw Question 83.2 86.1 86.7 94.0   + Mask 85.5 87.4 88.6 95.3   D Case Study   Figure 7 shows some cases that our FC - KBQA   and RNG - KBQA predicted . Example(a ) shows a   simple one - hop case , but RNG - KBQA tends to gen-   erate a more complex logical expression because it   frequently occurs in the training set . With sample   cases where the surface name of the gold relation   has a clear overlap with the question , Example(b )   demonstrates how the composition of each com-   ponent causes RNG - KBQA to fail . As seen in ex-   ample(c ) , the entanglement of knowledge and logi-   cal skeleton causes RNG - KBQA to predict some   straightforward logical operators like " COUNT "   incorrectly . These restrictions can be overcome by   our proposed FC - KBQA.10141015ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7   /squareA2 . Did you discuss any potential risks of your work ?   Section 8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 5   /squareB1 . Did you cite the creators of artifacts you used ?   Section 5   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   The datasets we used are all publicly available .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   The datasets we used are all publicly available , and we only use them for evaluation .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The datasets we used are all publicly available .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   The datasets we used are all publicly available .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   The datasets we used are all publicly available . The readers can refer to the original paper for the   statistics .   C / squareDid you run computational experiments ?   Appendix   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix1016 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Appendix   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1017