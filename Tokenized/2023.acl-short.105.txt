  Ruining Chong , Cunliang Kong , Liu Wu , Zhenghao Liu , Ziye Jin ,   Liner Yang , Yange Fan , Hanghang Fan , Erhong YangNational Language Resources Monitoring and Research Center for Print Media ,   Beijing Language and Culture University , ChinaSchool of Information Science , Beijing Language and Culture University , ChinaDepartment of Computer Science and Technology , Northeastern University , ChinaSchool of Arts and Sciences , New York University Shanghai , ChinaKika Tech , China   lineryang@gmail.com   Abstract   Text revision is a necessary process to improve   text quality . During this process , writers con-   stantly edit texts out of different edit intentions .   Identifying edit intention for a raw text is al-   ways an ambiguous work , and most previous   work on revision systems mainly focuses on   editing texts according to one specific edit in-   tention . In this work , we aim to build a multi-   intent text revision system that could revise   texts without explicit intent annotation . Our   system is based on prefix - tuning , which first   gets prefixes for every edit intent , and then   trains a prefix transfer module , enabling the   system to selectively leverage the knowledge   from various prefixes according to the input   text . We conduct experiments on the I - TRdataset , and the results show that our   system outperforms baselines . The system can   significantly improve the SARI score with more   than 3 % improvements , which thrives on the   learned editing intention prefixes .   1 Introduction   Revision is an essential process to improve the   text quality ( Vaughan and McDonald , 1986 ) . Dur-   ing this process , writers perform various editing   operations on the text with different editing inten-   tions . As shown in Figure 1 , the writer corrects   misspelled words to improve text fluency , deletes   redundant words to improve text clarity , adds con-   nective words to improve text coherence , inserts   adverbs to convey the writer ’s writing preferences   ( style ) and modifies data to update text information   ( meaning - changed ) .   Lots of recent studies have focused on a text revi-   sion task corresponding to a specific edit intention ,   such as grammatical error correction ( OmelianchukFigure 1 : Writers edit texts out of different edit inten-   tions through text revision .   et al . , 2020 ; Kaneko et al . , 2020 ; Liu et al . , 2021 ;   Yang et al . , 2022 ) , text simplification ( Dong et al . ,   2019 ; Jiang et al . , 2020 ; Omelianchuk et al . , 2021 ;   Martin et al . , 2022 ) , and text style transfer ( Malmi   et al . , 2020 ; Reid and Zhong , 2021 ) . The work   divides text revision into several independent prob-   lems . While some methods with strong universal-   ity can be applied to multiple tasks ( Malmi et al . ,   2019 ; Stahlberg and Kumar , 2020 ; Mallinson et al . ,   2020 ) , they train different models on various data   sets . Real - world scenarios require addressing mul-   tiple types of editing errors at the same time , such   as grammatical errors , spelling errors , etc . But   these methods failed to integrate knowledge from   these tasks into a unified model .   To solve the problem , Du et al . ( 2022 ) attempted   to train one model using data with multiple editing   intentions and leveraged edit intent information by   simply appending it to the input . However , when   adding a new intent , the entire model must be re-1219trained . A more lightweight and scalable approach   to multi - intent text revision is still required .   Li and Liang ( 2021 ) proposed a new kind of   prompt tuning method to quickly adapt a pre-   trained model to new tasks , which is called prefix-   tuning . Prompt tuning can help the pre - trained lan-   guage model to locate the task learned in pretrain-   ing and enable the related knowledge to model text   revision with different edit intentions ( Reynolds   and McDonell , 2021 ) . This method enables a   model to handle multiple edit intentions in a   lightweight and scalable way .   In this paper , we present our method : a prefix-   tuning - based model which adapts to text revision   with multiple edit intentions . This method involves   a two - step training process . In the first step , we   initialize a pre - trained language model ( PLM ) and   train multiple prefixes on it . Each edit intention   corresponds to a prefix . In the second step , a prefix   transfer module is trained at each attention layer   of the PLM . The prefix transfer module is config-   ured as two attention units that act respectively on   this layer ’s key states and value states . It enables   our model to learn a tailored prefix for the given   input with the help of prefix embeddings from the   predefined tasks .   We conduct experiments on I TR(Du   et al . , 2022 ) , an iterative text revision dataset . It   mainly contains parallel sentences with five edit   intentions : fluency , coherence , clarity , style , and   meaning - changed . The results show that our ap-   proach performs better than the fully fine - tuned   BART ( Lewis et al . , 2020 ) and P ( Zhang   et al . , 2020 ) baselines reported in Du et al . ( 2022 )   with fewer training parameters .   2 Related Work   2.1 Iterative Text Revision   For the first time , Du et al . ( 2022 ) systematically   studied the iterative revision phenomenon in human   writing . They presented the I TR , an anno-   tated dataset across multiple domains of formally   human - written text , which includes Wikipedia ,   ArXiv , and Wikinews . And they trained several   types of text revision models using I TR .   Dwivedi - Yu et al . ( 2022 ) presented EE , an   instruction - based benchmark , to evaluate the edit-   ing capabilities of models and they also included   the test set of I TRin it . Based on Du et al .   ( 2022 ) , our work further explores the method of   text revision.2.2 Transfer Learning of Prompt Tuning   Transfer learning is a common and powerful tech-   nique in NLP ( Raffel et al . , 2020 ) . Some recent   studies have tried to improve prompt tuning per-   formance by leveraging the knowledge of multiple   related or unrelated tasks . Asai et al . ( 2022 ) used   an attention module to make use of the knowledge   in exiting soft prompts ( Lester et al . , 2021 ) while   learning a new task . Chen et al . ( 2022 ) improved   the few - shot text summarization by multi - task pre-   training and prefix - tuning . Specifically , they pre-   trained a summarization model on a set of popular   summarization datasets and then conducted prefix-   tuning for it on an unseen summarization task . Dif-   ferent from their modeling of a new task through   existing tasks , our work aims to achieve the mu-   tual utilization of knowledge between different edit   intents in text revision .   3 Method   The revision task can be defined as the following   process : given a source sentence x= [ x , . . . , x ]   and an optional edit intent e∈Eto generate a   revised sentence y= [ y , . . . , y ] , where Eis the   set of all edit intentions . Note that eis optional   because it can be inferred from the input x.   Our method is depicted in Figure 2 . It includes   two stages : the multi - prefix tuning stage and the   prefix transfer stage .   3.1 Multi - Prefix Tuning Stage   The prefix is a set of parameters on every atten-   tion layer of PLM . For an edit intention e , at   each attention layer , the prefix can be described as   P={P , P } , where PandPare parame-   ters added before the key states and value states in   this attention layer . After adding these parameters ,   the calculation of the attention head in this layer   becomes :   H= Attention ( Q,[P;K],[P;V])(1 )   where His the output vector sequence ; Q , K , V   are query states , key states , and value states , re-   spectively ; Attention means scaled dot - product   attention . Only PandPare updated during   the training process . Note that we ignore the layer   number information because the operation for each   layer is the same .   As shown in the left part of Figure 2 , for every   edit intention e , we train a prefix Paccordingly.1220   In this way , the model could revise an intention-   annotated text by activating the corresponding pre-   fix at inference .   3.2 Prefix Transfer Stage   Identifying edit intention is always an ambiguous   work . At the prefix transfer stage , we aim to build   a new prefix for an unannotated input instance by   transferring existing prefixes . The new prefix P   is instance - specific .   The prefix transfer stage is described in the right   part of Figure 2 . At each layer , we rearrange the   prefixes { P|e∈E}obtained in the last stage as   P={P|e∈E}andP={P|e∈E }   according to whether they are configured before   the key states or before the value states . Then a   pair of attention units GandGare trained for   PandP.   TakeGas an example . It calculates the simi-   larity between the key states Kand every Pin   Pto get attention scores .   The similarity ca n’t be calculated directly , be-   cause KandPhave different lengths . So we   perform the max - pool operation for length dimen-   sion on KandP. After that , we obtain ˆK∈R   andˆP∈R , where dis the dimension of the   hidden states in the PLM .   To get attention scores , we train a fully con-   nected layer to extract features from ˆK :   H= NonLinear ( W(ˆK ) ) ( 2 )   where W∈Ris a transfer matrix updated   during training . Following Asai et al . ( 2022 ) , we   use SiLU ( Elfwing et al . , 2018 ) for the non - linear   layer and add a Layer Norm ( Ba et al . , 2016 ) layer :   H = LayerNorm ( H ) ( 3)Then , we calculate the attention scores for intent   eas follows :   a = exp ( ˆP·H)/T / summationtextexp ( ˆP·H)/T(4 )   where Tis the softmax temperature ( Radford et al . ,   2021 ) which could avoid making the attention unit   over - confident .   Finally we use them to build Pas follows :   P=/summationdisplayaP ( 5 )   In the same way , we get PbyG. Using   the new prefix P= [ P , P ] , our system   could revise the unannotated input instance with   the knowledge from existing prefixes .   4 Experimental Setup   We choose BART - large as the PLM for our system   and use adapter - transformers ( Pfeiffer et al . , 2020 )   to implement prefix - tuning . More implementation   details are in Appendix A.   4.1 Datasets   We conduct our experiments on the iterative text   revision dataset : I TR(Du et al . , 2022 ) . We   remove the Other class of the data as it essentially   contains a variety of unrecognized edit intentions   and accounts for a small proportion ( 1.44 % ) . The   entire dataset consists of two parts : I TR- andI TR- . The former is a   smaller dataset with manual annotation of edit in-   tentions , while the latter is a large dataset annotated   by a classification model trained on I TR- . We train our model on both of them .   Following Du et al . ( 2022 ) , we report the results   on the test set of I TR- in Section 5,1221   which is completely a human - created dataset and   is reliable for evaluation . We show more details of   the datasets in Appendix B.   4.2 Evaluation Metrics   Following previous work , we report three met-   rics : SARI ( Xu et al . , 2016 ) , Rouge - L ( Lin , 2004 ) ,   and BLEU ( Papineni et al . , 2002 ) . Among them ,   SARI is considered an important metric in situa-   tions where input text and output text have a large   overlap in words . It also indicates the positive im-   pact of revisions on document quality .   The setting of evaluation metrics is the same as   Du et al . ( 2022 ) . We use the metrics package from   Huggingface transformers ( Wolf et al . , 2020 ) to   calculate the SARI , BLEU , and Rouge - L scores .   4.3 Models Setup and Baselines   Using our method , we train the models in two ways :   the model that only trains the multi - prefix tuning   stage and that trains both the multi - prefix tuning   stage and the prefix transfer stage .   We compare our method with three baselines :   full fine - tuning BART ( BART - FineTune ) , full fine-   tuning P ( P -FineTune ) , and prefix-   tuning of BART with a single prefix ( BART-   SinglePrefix ) . Both BART and P are gener-   ative models based on the transformer architecture .   Compared to the edit - based model FELIX , they   perform better . We use the results reported by Du   et al . ( 2022 ) for these two models . Furthermore ,   we compare BART - SinglePrefix as a possible tech-   nical solution as we choose BART as our backbone   model . BART - SinglePrefix trains only one prefix   on the entire dataset .   All three baselines are trained with two config - urations . The first configuration is using the pure   sentence pairs without edit intention annotations   to train the model . The second configuration is   appending an edit intent token at the beginning of   the input text during the training process , which is   the same as the approach of Du et al . ( 2022 ) .   5 Results and Analysis   5.1 Main Results   The main results are shown in Table 1 . Compared   to training with a single prefix , the setting of mul-   tiple prefixes can improve the results , especially   training on I TR- . Meanwhile , with   fewer training parameters , the multi - prefix setting   could achieve a comparable SARI score and better   average score than the fully fine - tuned BART and   P baselines .   Moreover , prefix transfer could further improve   the model ’s performance . Training on I TR- , prefix transfer significantly improves the   SARI score from 33.12 to 36.01 and gets the high-   est average score of 67.91 . Training on I TR- , prefix transfer can also improve the average   score from 67.23 to 68.36 .   An interesting phenomenon is that training on   different datasets results in different gains for pre-   fix transfer in evaluation metrics . On I TR- , prefix transfer improves the SARI score   significantly . While on I TR- , pre-   fix transfer mainly improves the BLEU score and   Rouge - L score . One possible explanation is that   in situations when the training data is small , prefix   transfer tends to learn more editing operations to   improve text quality . In this way , the SARI score   related to editing operations will be improved sig-   nificantly . When the training data is sufficient , pre-1222   fix transfer will model the gold reference in more   detail . So the BLEU score and the Rouge - L score   will be improved .   5.2 Analysis   We further tried to use different training data at   different stages of training to conduct experiments .   The results are shown in Table 2 .   We find that the best practice is to train the   model on I TR- in the multi - prefix tun-   ing stage and on I TR- in the prefix   transfer stage , which gets the highest SARI score   and average score . This may be because of the   different distributions of manually annotated edit   intent and automatically annotated edit intent . The   auto - annotated dataset I TR- contains   many incorrectly classified sentences , which may   cause mismatched knowledge in prefixes . In the   prefix transfer stage , due to the existence of mis-   matched knowledge and incorrectly classified sen-   tences , the continued use of the same training data   may finally cause a certain degree of negative trans-   fer . However , if we use I TR- in the   prefix transfer stage , the impact of negative trans-   fer will be mitigated , because I TR-   only contains correctly classified sentences .   In Appendix C , we separately provide the per-   formance results on different edit intentions of the   best - performing model .   6 Conclusion   In this paper , we introduce a new method for multi-   intent text revision . The system is based on prefix-   tuning , which first obtains a prefix for every edit   intention and then learns to transfer the knowledge   in prefixes for every input instance by training a   prefix transfer module . This prefix transfer mod-   ule is configured as two attention units that act   respectively on the key states and the value states   at each attention layer of the PLM . In this way ,   our method can make full use of the knowledge of   various edit intentions and does not need to anno - tate the intentions of the input . The experimental   results show that our method significantly outper-   forms baselines , and both multi - prefix and prefix   transfer settings could improve the performance .   Limitations   Due to the lack of multi - intent text revision datasets ,   we only conduct experiments on I TR . Al-   though it is a multi - domain dataset , we only use   its sentence - level data , and each sentence pair only   contains one editing operation . The robustness of   our method is still to be verified by evaluating it on   more types of datasets in future work .   Another limitation of our work is that we only   made improvements at the model level . We have   noticed that Kim et al . ( 2022 ) recently improved   text revision by leveraging extra data from other   text editing tasks and performing editable span de-   tection before revising . Similar methods can also   be applied to our model and will be tried in our   future work .   Ethics Statement   The PrefixTransfer method mainly aims at fusing   multiple prefixes to obtain a unified model that   can perform multi - intent text revision . The experi-   ments are based on the I TRdataset , which   is unlikely to include harmful content .   Acknowledgments   This work was supported by the funds of the Re-   search Project of the National Language Commis-   sion ( No . ZDI145 - 24 ) , Natural Science Foun-   dation of China ( No . 62206042 ) , Fundamental   Research Funds for the Central Universities ( No .   21PT04 , No . N2216013 ) and China Postdoctoral   Science Foundation ( No . 2022M710022 ) . We   would like to thank all anonymous reviewers for   their valuable comments and suggestions on this   work .   References122312241225ADetails on Computational Experiments   Our system is established based on BART - large   ( with 400 million parameters ) . We use the AdamW   optimizer with weight decay and adopt Noam Op-   timizer . We initial the learning rate of 5ein the   multi - prefix tuning stage and 1ein the prefix   transfer stage . We set the warm - up steps to 4000 .   Regarding batch size , we use max - token configura-   tion and set the maximum of tokens to 1024 . The   maximum epoch is set to 100 . And we set an early   stop strategy in the patience of 20 epochs .   The length of the prefix ( with 40 million param-   eters ) is 12 and the prefix vectors are not optimized   directly but reparameterized via a bottleneck MLP   which has a middle dimension of 512 . We set the   prefix dropout to 0.2 .   We do validation every epoch while training the   model on I TR- and every 200 steps   while training the model on I TR- .   We report descriptive statistics with a single run .   We deploy all our experiments on a slurm clus-   ter . We train the prefixes on 4 Tesla V100 - SXM2   ( 16 GB ) GPUs and train prefix transfer modules on   an NVIDIA TITAN RTX ( 24 GB ) GPU .   Dataset Train Dev Test   I TR- 3,215 385 360   I TR- 157,579 19,705 19,703   B Details of Dataset   B.1 Taxonomy   The taxonomy of edit intentions in I TRafter   removing Other :   •Fluency Fix grammatical errors in the text .   •Coherence Make the text more cohesive , log-   ically linked , and consistent as a whole .   •Clarity Make the text more formal , concise ,   readable , and understandable .   •Style Convey the writer ’s writing preferences ,   including emotions , tone , voice , etc .   •Meaning - changed Update or add new infor-   mation to the text . B.2 Data split   I TRdataset is splited as in Table 3 after   romoving the Other class .   B.3 License   TheI TRdataset uses Apache License , and   it allows the data for academic usage .   C Model Performance of Different Edit   Intentions1226ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   The Limitation section after conclusion and before the references   /squareA2 . Did you discuss any potential risks of your work ?   The Ethics Statement section after Limitation   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section Abstract and Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   We use grammarly to to correct the full text   B / squareDid you use or create scientiﬁc artifacts ?   In Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   In Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   In Appendix B.3   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Our use of existing artifacts was consistent with their intended use . All for research purposes   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The dataset we use is open source and does not involve such problems   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   In Section 1 Section 4 and Appendix B   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   In Appendix B   C / squareDid you run computational experiments ?   In Section 4 and 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In Appendix A1227 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   In Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   In Appendix A   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   In Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1228