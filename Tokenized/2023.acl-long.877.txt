  Lijing WangYingya LiTimothy MillerSteven BethardGuergana Savova   Abstract   The bias - variance tradeoff is the idea that   learning methods need to balance model   complexity with data size to minimize   both under - fitting and over - fitting . Recent   empirical work and theoretical analyses with   over - parameterized neural networks challenge   the classic bias - variance trade - off notion   suggesting that no such trade - off holds : as the   width of the network grows , bias monotonically   decreases while variance initially increases   followed by a decrease . In this work , we   first provide a variance decomposition - based   justification criteria to examine whether large   pretrained neural models in a fine - tuning   setting are generalizable enough to have   low bias and variance . We then perform   theoretical and empirical analysis using   ensemble methods explicitly designed to   decrease variance due to optimization . This   results in essentially a two - stage fine - tuning   algorithm that first ratchets down bias and   variance iteratively , and then uses a selected   fixed - bias model to further reduce variance   due to optimization by ensembling . We also   analyze the nature of variance change with   the ensemble size in low- and high - resource   classes . Empirical results show that this   two - stage method obtains strong results on   SuperGLUE tasks and clinical information ex-   traction tasks . Code and settings are available :   https://github.com/christa60/   bias-var-fine-tuning-plms.git   1 Introduction   Transformer - based neural language models , such   as Bidirectional Encoder Representations from   Transformers ( BERT ) ( Devlin et al . , 2019 ) , have   achieved state - of - the - art ( SOTA ) performance for   a variety of natural language processing ( NLP )   tasks through the process of fine - tuning . Given   an NLP task , the process often involves searching   for optimal pretrained models and hyperparameterswhile continuing to train the pretrained model on a   domain - specific dataset , with the aim of building   generalizable and robust fine - tuned models .   Based on the classic notion of the bias - variance   tradeoff ( Geman et al . , 1992 ) , where increasing   model capacity decreases bias but increases vari-   ance ( leading to a U - shaped test error curve ) , large   pretrained models ( LPMs ) should have large vari-   ance and overfit domain - specific data which is of-   ten sparsely labeled and extremely imbalanced for   classification . However , recent empirical work   and theoretical analysis of neural networks chal-   lenge this classic bias - variance trade - off notion   ( Neal et al . , 2018 ; Yang et al . , 2020 ) . It has been   suggested that no such trade - off holds : as the   width / depth of the network grows , bias monoton-   ically decreases while variance initially increases   followed by a decrease . This is why transformer-   based LPMs often achieve better performance com-   pared to less complex models like long short - term   memory ( LSTM)-based models or feature - rich   methods ( e.g. , support vector machines ) . In the   context of the new bias - variance paradigm , these   LPMs are seemingly complex enough to have low   bias and variance , however , so far there has been   no method to justify whether those SOTA models   are generalizable and robust in solving a variety of   downstream tasks . In this paper , we ( 1 ) show that   many SOTA models are very sensitive to data and   training randomness , and ( 2 ) provide a variance   decomposition - based justification method .   We also aim to improve model performance , re-   ducing the generalization error of LPMs by reduc-   ing their bias and variance . Recent findings in   Yang et al . ( 2020 ) show that the generalization er-   ror mainly comes from bias . Bias can be reduced   by modifying the model architecture , e.g. , mak-   ing the neural networks wider and deeper as in   transformer - based LPMs . However , pretraining   new or larger language models can be challenging   due to the technical and computational resource15746requirements afforded by only a few institutions   – a topic outside the scope of this paper . We fo-   cus on the problem of reducing variance of neural   models to further boost model performance , given   a fixed bias ( i.e. , a fixed pretrained model ) . En-   semble methods have been successful in boosting   predictive performance of single learners ( Ren et al .   ( 2016 ) presents a comprehensive review ) and thus   are a promising venue to explore .   We propose a two - stage fine - tuning framework   that first justifies the generalization status of a se-   lected pretrained model through a concrete metric ,   and then uses the fixed - bias model to further reduce   variance due to optimization through ensembling   techniques . To the best of our knowledge , we are   the first to provide such a metric and perform theo-   retical and empirical analysis using ensembles for   improved bias and variance for LPMs . We conduct   experiments on the SuperGLUE tasks as well as   on information extraction from clinical text as it   is a domain of high significance and presents data   challenges due to the limitations of sharing patient-   related data . We believe our proposal is of interest   to any unstructured domain where neural models   are used . Specifically we make the following con-   tributions :   •We propose a two - stage fine - tuning algorithm   for improving bias and variance in the new   bias - variance paradigm ;   •We provide a variance decomposition - based   strategy to examine whether LPMs in fine-   tuning settings are generalizable enough to   have low bias and variance ;   •We perform theoretical and empirical anal-   yses using ensembles explicitly designed to   decrease variance due to optimization while   keeping bias unchanged ;   •We analyze the nature of variance change   due to ensembling in low- and high - resource   classes in classification tasks ;   •We conduct comprehensive experiments and   show that the proposed two - stage method ob-   tains strong results on SuperGLUE tasks and   two clinical NLP tasks .   2 Preliminaries   In this section we present the bias - variance decom-   position for squared loss in the new paradigm stud-   ied in Neal et al . ( 2018 ) and Yang et al . ( 2020).We also present a further decomposition of vari-   ance . We denote fas a supervised learning   task such that f :X → Y , based on a train-   ing dataset Sofmi.i.d . samples drawn from a   joint distribution Dof ( X , Y ) . The learning tar-   get is to minimize the mean squared error E(f ) =   E / bracketleftbig   ∥y−f(x)∥/bracketrightbig   , where ( x , y)∼ D.   We consider the predictor fas a random vari-   able depending on the random variable Sfor train-   ing dataset and the random variable Ofor opti-   mization randomness , where θ = A(S , O)∈R   represents the weights of neural networks produced   by the learning algorithm A.pdenotes the dimen-   sion of θ . The notations and their descriptions are   shown in Table 3 in the Appendix .   2.1 Bias - variance decomposition   In the context of classification of Cclasses , where   y∈Ris represented as a one - hot vector and   f(x)∈Rdenotes an output probability vector   by the predictor , the risk Rof the learning algo-   rithm can be decomposed into three sources of   errors ( Geman et al . , 1992 ):   R = E+E+E ( 1 )   The first term is the irreducible noise and comes   from the intrinsic error of data independent of the   predictor . The second is a bias term :   E = E / bracketleftbig   ∥E[f(x)]−E[y|x]∥/bracketrightbig   ( 2 )   The third is a variance term   E = EVar(f(x ) )   = E / bracketleftbig   E / bracketleftbig   ∥f(x)−E[f(x)]∥/bracketrightbig / bracketrightbig ( 3 )   and can be further decomposed into the variance   due to optimization Varand the variance due to   sampling Var ( Neal et al . , 2018 ):   Var(f(x ) ) = Var+Var   = E[Var(f(x)|S ) ]   + Var(E[f(x)|S])(4 )   where we denote the expectation of the decom-   posed variance as E = E[Var]andE=   E[Var ] .   2.2 Theoretical findings from variance   decomposition   Assuming the learning task is to learn a linear map-   pingy = θx+ϵwhere ϵdenotes the noise random15747   variable with E[ϵ ] = 0 andVar(ϵ ) = σ , and the   input vector is x∈Rwhere pis the input or   parameter dimensionality .   In this context , the over - parameterized setting is   when p > m and the under - parameterized setting   is when p≤m .   The theoretical findings in Neal et al .   ( 2018 ) prove that Egrows with pin the   under - parameterized case , while in the over-   parameterized case , the variance does not grow   withpbut scales with the dimension of the data :   EVar(f(x ) ) = /braceleftbiggσforp≤mσforp > m(5 )   where r = rank ( X)andX∈Rdenotes the   data matrix whose ith row is the training point x.   Furthermore , Evanishes as pincreases under   the linear squared regression assumption and E   depends on critical parameter dimensionality d(p ) .   2.3 Empirical findings from variance   decomposition   Finding - I : as shown in the left panel of Figure 1 ,   Edecreases quickly and levels off once suf-   ficiently over - parameterized , while Eis uni-   modal contrary to the classic theory . Finding - II :   in the right panel of Figure 1 , Eis significant   and higher than Ein the under - parameterized   regime . The two variances cross at a certain ponce   sufficiently over - parameterized . However , empir-   icalpandmof the over - parameterized setting is   not strictly following the theoretical findings in sec-   tion 2.2 . Finding - III : in multi - layer models where p   is the width and qis the depth , given a fixed p , E   decreases while Eincreases as qincreases ( Yang   et al . , 2020 ) . These empirical findings hold for mul-   tiple datasets in the original papers.3 Two - Stage Fine - Tuning for Improved   Bias and Variance   3.1 Overview   The prevailing fine - tuning methods first build or   select an LPM and then fine - tune its parameters on   the downstream datasets . The SOTA setting for the   LPM and its best hyperparameters for fine - tuning   are chosen based on evaluation results , such as pre-   cision ( P ) , recall ( R ) , F1 and accuracy scores , using   grid - search or random - search . Given a fine - tuning   task with a fixed training dataset , there is an upper   limit to the learning ability of an LPM which is   hard to measure by traditional evaluation methods .   For a selected LPM , it is usually hard to decide   when to stop searching for hyperparameters . Dif-   ferent from the prevailing fine - tuning setting , we   propose a two - stage fine - tuning method . We first   provide a variance decomposition - based justifica-   tion method to roughly measure the generalization   ability of a pretrained model w.r.t . the upper limit   of its learning ability . In Stage - I , the SOTA set-   ting is built by ratcheting down bias and variance   in an iterative way . The searching loop stops un-   til an acceptable performance appears or no more   improvement is observed . In Stage - II , given the   SOTA setting built in Stage - I , the variance due to   optimization is reduced by ensembling techniques .   Algorithm 1 outlines the procedure of the proposed   two - stage fine - tuning method . The details of each   stage are presented below .   3.2 Stage - I : Justification of generalization   ability   Based on the preliminaries in Section 2.3 , assum-   ing an algorithm A(S , O)is fixed , the E , E ,   E , andEchanges as p , q , and mchange .   Taking the crossing point ( E = E ) as a di-   viding point , we define the generalization ability   Gas :   G=/braceleftbiggPhase - I for E > E   Phase - II for E≤ E(6 )   where Phase - I implies large bias and variance lead-   ing to large generalization error . Phase - II implies   small bias and variance leading to small general-   ization error which may be good enough w.r.t . the   upper limit of the learning ability of A.   Justification criteria : After each evaluation , if Gis   in Phase - I , it is necessary to explore more hyperpa-   rameter settings or new pretrained models until G15748is in Phase - II or an acceptable performance ( e.g. , P ,   R , F1 ) is achieved given the limited computing re-   sources available in practice . Then fine - tuning can   move to Stage - II . If Gis in Phase - II , the current   setting may be generalizable enough for the given   learning task so that the searching can be stopped .   Stage - II can be applied but is not necessary . We   note that similar to Finding - II in Section 2.3 , G   can not be determined directly based on p , q , and   m. This breakdown provides a high - level guideline   for evaluating the generalization of LPMs in an   empirical way .   3.3 Stage - II : Ensembling to reduce variance   Ensembles have been proven to be able to improve   prediction accuracy and consistency of any learn-   ing models in Bonab and Can ( 2019 ) ; Wang et al .   ( 2020 ) . Bagging - based ensembles which are com-   monly used in various learning tasks have been   proven to be able to reduce Ewhile keeping   Eunchanged . However , no theoretical analysis   has been discussed in the context of the variance   decomposition paradigm . In Stage - II , we focus on   bagging - based ensembles to further improve the   model performance by reducing Ewhile keep-   ingEunchanged . Applying Stage - II can either   move a model from Phase - I to Phase - II though en-   sembling , i.e. , reducing EuntilE≤ E ;   or further improve a model ’s generalization ability   from Phase - II by reducing E.   We perform empirical analysis in Section 4 and   theoretical analysis in Section 5 to investigate why   and how bagging - based ensembles can guarantee   such improvements in this context . We also analyze   the nature of variance change with the ensemble   size in low- and high - resource classes in classi-   fication tasks . Boosting ensembles have a more   complex behaviour thus are out of scope for this   paper .   4 Experiments   4.1 Data and models   We conduct experiments on the SuperGLUE tasks   and two major clinical information extraction   datasets . The data processing and statistics and   hyperparameter settings are shown in Appendix   Table 4 and Table 5 . Their brief descriptions are :   •SuperGLUE ( Wang et al . , 2019 ) is a bench-   mark dataset designed for a more rigorous test   of general - purpose language understandingAlgorithm 1 : Pseudocode of the two - stage   fine - tuning method .   systems after GLUE ( Wang et al . , 2018 ) . We   replicate the scores on dev set , and select six   tasks ( BoolQ , CB , RTE , MultiRC , WiC , and   COPA ) . The selected tasks cover question an-   swering ( QA ) , natural language inference , and   word sense disambiguation . The SOTA setting   is based on the setting in Liu et al . ( 2019 ) us-   ing as the pretrained model .   •THYME ( Temporal Histories of Your Med-   ical Events ) corpus ( Styler IV et al . , 2014 )   for temporal relation extraction , consisting   of 594 de - identified clinical and pathol-   ogy notes on colon cancer patients . We   use the THYME+ version of the corpus   ( Wright - Bettner et al . , 2020 ) . There are   10 classes of extremely imbalanced class   distribution . The SOTA setting is based   on the setting in Lin et al . ( 2021 ) using as   the pretrained model.15749•2012 i2b2 Temporal Relations ( Sun et al . ,   2013 ) consists of 394 training de - identified   reports , 477 test de - identified reports , and   877 unannotated reports . There are 3   classes of slightly imbalanced class distri-   bution . The SOTA setting is based on   the setting in Haq et al . ( 2021 ) using as the pretrained   model .   4.2 Metrics and settings   We use an NVIDIA Titan RTX GPU cluster of 7   nodes for fine - tuning experiments through Hug-   gingFace ’s Transformer API ( Wolf et al . , 2020 )   version 4.13.0 . We leverage the run_glue.py   pytorch version as our fine - tuning script . Unless   specified , default settings are used in our experi-   ments . Due to differences in the fine - tuning script   and some missing settings not provided by the orig-   inal authors , we were unable to reproduce the exact   SOTA scores but we achieved scores close to the   SOTA ones . Our implementation are denoted as   replicated - SOTA ( RSOTA ) . We compare our imple-   mentation and reference SOTA scores in Appendix   Table 7 . We use the RSOTA settings as the starting   point to conduct the experiments . We use E ,   E , E , andEfor Stage - I , and adopt clas-   sic evaluation metrics ( P , R , and F1 ) for Stage - I   and Stage - II . For the purposes of consistency , we   report P , R , and F1 of SuperGLUE tasks for con-   sistency with the reported results for the THYME   and i2b2 tasks in the experiment results . Accuracy   scores are reported in Appendix Table 7 .   4.3 Experimental design   There are two stages in the proposed method . For   Stage - I , we use 5 random seeds for the randomness   over data samples Sand 5 for the randomness over   initialization O , resulting in a total of 25 fine - tuned   models . The averages over data samples are per-   formed by taking the training set Sand creating 5   bootstrap replicate training / validation splits with   the same class distribution . The bias expectation in   Equation 2 is estimated as the averages over both   SandO. The variance decomposition is estimated   based on Equation 4 . More specifically , Eis   estimated as the averages over Sof the variance   overO , andEis estimated as the variance over   Sof the averages over O. Furthermore , we also   apply as the pre-   trained model for each fine - tuning task using the   RSOTA setting except for pretrained models . Their   descriptions are shown in Appendix Table 6 . To   replicate SOTA scores and obtain RSOTA settings   for each task , we conduct hyperparameter search-   ing in an iterative way . This process is considered   as the experiment of Stage - I.   For Stage - II , any bagging - based ensemble al-   gorithms are feasible . In our preliminary exper-   iments ( Wang et al . , 2022 ) , we have shown that   the dynamic snapshot ensemble algorithm ( Wang   et al . , 2020 ) , which we call ENS in this paper ,   works better than vanilla bagging ensembles . ENS   is a bagging - based ensemble explicitly designed to   reduce variance over optimization - related hyperpa-   rameters in one framework , with the aim of build-   ing computationally efficient strategies to boost   model performance on top of any given setting with   a guarantee ( i.e. , simple bagging ensemble can not   guarantee an improvement ) . In our implementa-   tion , we employ ENS . The ensemble size is 5 and   majority voting is used to generate ensemble predic-   tions . To explore the ensemble impact on low- and   high - resource classes , we compute and compare   performance improvements of each class from the   extremely imbalanced THYME dataset . To investi-   gate the impact of ensemble size on improving the   model performance of imbalanced classes , we also   evaluate performance of individual classes using   ENS of size 1 to 10 . We compute 95 % confidence   intervals for these estimates using bootstrapping   over 5 samples . More details are in Appendix B.15750   4.4 Justification results   Table 1 shows E , E , E , andEcom-   puted on the datasets with different pretrained mod-   els . It is noted that in our experiment , we are   not applying the algorithm for Stage - I to ratchet   down bias and variance in an iterative manner . The   goal of this table is to analyze both RBU and   RSOTA models for the bias and variance trends   discussed in Section 2 . Interestingly , we observe   thatE > Efor all datasets and models ex-   cept for CB - RBU and COPA - RBU where models   are not well trained given that F1 score is around   0.5 indicating random guess . This implies that   the vast majority of the SOTA models we experi-   mented with are in Phase - I ( i.e. , not generalizable   enough for their tasks ) , which is contrary to our   intuition that these transformer - based models are   complex enough given the moderate sized labeled   datasets . It is also observable that Eis much   larger than Eindicating that the model perfor-   mance is dominated more by bias than by variance .   For SuperGLUE tasks , with the same hyperparame-   ter setting ( i.e. , A(S , O ) ) , the RSOTA models ( i.e. ,   larger pandqthan RBU models ) achieve smaller   Ebut larger Ethan RBU models except for   MultiRC . The change of Emainly comes from   the change of E. As Finding - I in Section 2.3   thatEdecreases while Eis unimodal , our   observation implies that RBU models are before   peak and long way toward Phase - II while RSOTA   models get closer to Phase - II than RBU models .   The exception is the result on the MultiRC dataset   which is QA corpus listing a passage which con-   sists of text , a question , and multiple answers tothat question each with a true / false label . Although   MultiRC represents a variety of question types ( e.g.   yes / no , factual , etc . ) , the passages are not annotated   for question type . As explained in Appendix sec-   tion B.1 . , we represented the QA pairs within each   passage as in-   stances and sampled from these instances . Using   this instance - based sampling likely leads to sam-   ples not stratified by question types , therefore not   necessarily representative . This probably explains   the better mean F1 for MultiRC - RBU as compared   to the mean F1 for MultiRC - RSOTA in Table 1   ( different samples are created for each run ) . How-   ever , when we drill down to the best model F1 for   MultiRC - RBU and MultiRC - RSOTA , the results   are 78.9 and 85.4 F1 - scores respectively , which   supports the trend in Table 1 .   For the SuperGLUE tasks , the bias and variance   of RBU models and RSOTA models are shown to-   gether to illustrate a trend ( like Fig . 1 ) as pandq   are the only variables . However for THYME and   i2b2 tasks , similar trends could not be interpreted   since the RSOTA models are pretrained with do-   main specific corpora while the RBU models are   pretrained with general corpora . This implies that   for fine - tuning tasks such as temporal relation ex-   traction , other factors ( e.g. , domain corpora used   to pretrain models ) may have larger impact than   the model complexity . Our observations are con-   sistent with Finding - II that empirical pandmset-   ting is not strictly following theoretical findings   which are under linear squared regression assump-   tion . This also indicates that p , q , andmcannot be   used to measure Gempirically . On the other hand,15751our variance decomposition - based method does not   rely on p , q , and m , therefore it provides the basis   for a more generalized justification method .   4.5 Ensemble results   Table 2 presents P , R , and F1 scores of RSOTA and   ENS methods on all datasets . Similar to prior stud-   ies ( e.g. Zhang et al . , 2020 ; Du et al . , 2022 ) , results   for the SuperGLUE tasks are reported on the dev   set . The accuracy scores for each task are presented   in the Appendix Table 7 . Compared to the RSOTA   setting , the ENS method boosts performance on all   datasets , with the largest gains of 49.3 % and 15.2 %   relative F1 improvements on the low - resource CB   and COPA datasets respectively . In Section 5 , we   analyze why ensembles work from the variance de-   composition perspective , which provides insights   into how ensembles help reduce Eand lead to   better prediction accuracy .   4.6 Ensemble impact on low- and   high - resource classes   We further investigate the improvements on low-   resource datasets ( e.g. CB and COPA ) . To elim-   inate all interference from p , q , A(S , O ) , pre-   trained models and only keep mas the vari-   able , we tease apart the results of the extremely   unbalanced THYME dataset and analyze the   performance on each class . Its most frequent   classes ( i.e. , high - resource classes ) are CON-   TAINS ( 2895 ) , OVERLAP ( 2359 ) , and BEFORE   ( 2241 ) ; and the least frequent classes ( i.e. , low-   resource classes ) are NOTED - ON ( 140 ) , BEGINS-   ON ( 160 ) , and ENDS - ON ( 244 ) . The initial   F1 scores are : CONTAINS-0.776 , OVERLAP-   0.539 , and BEFORE-0.469 ; NOTED - ON-0.618 ,   BEGINS - ON-0.608 , and ENDS - ON-0.695 . In Fig-   ure 2 , we show absolute improvement and improve-   ment percentage of F1 with various ensemble size   N(compared with single learners i.e. , N= 1 ) .   These values are computed based on the mean with   95 % confidence interval over 5 random samples   for each class and each ensemble size . It is ob-   servable that given a fixed N , the performance   improvements by F1 scores on the low - resource   classes – NOTED - ON ( brown ) , BEGINS - ON ( red ) ,   and ENDS - ON ( orange ) – are larger than the ones   of high - resource classes . The difference becomes   larger as Nincreases . The scales of improvement   are not affected by the initial results ; i.e. , the larger   improvements on low - resource classes are not due   to lower initial F1 scores . This is an interesting ob - servation and may introduce a new solution for im-   proving performance of imbalanced datasets . More   similar results on P and R are shown in in Ap-   pendix Fig . 3 . We explore theoretical insights into   these observations in Section 5 .   5 Discussion and Theoretical Analysis   5.1 Basic statistics   LetX , X , · · · , Xbe a random sample from   a population with mean µand variance σand   ¯X=/summationtextX. Then the following two items   hold .   a : E[¯X ] = E[X ] = µ   b : Var(¯X ) = Var(X ) = σ   5.2 Ensemble in bias - variance decomposition   We work in the context of bagging - based ensem-   bles , assuming the ensemble predictor ¯f(x)is   ¯f(x ) = /summationtextf(x)is the averaging of Nsin-   gle learners trained with different samples of Sand   O. Based on the basic statistics , the Eof¯f(x )   in Equation 2 is unchanged while the Eof¯f(x )   in Equation 3 decreases by . Furthermore , we   have :   E = E / bracketleftbig   E[Var(¯f(x)|S)]/bracketrightbig   = 1   NE[E[Var(f(x)|S)]](7 )   and :   E = E / bracketleftbig   Var(E[¯f(x)|S])/bracketrightbig   = E[Var(E[f(x)|S])](8 )   which indicates that Ereduces while E   keeps unchanged as the ensemble size Nincreases .   Evanishes when Nis sufficiently large . The   improvement of the variance by ensembling comes   from the reduction of the variance due to optimiza-   tion .   As mentioned in Section 2.2 that under linear   squared regression assumption , Evanishes as   pincreases and Edepends on critical param-   eter dimensionality d(p ) . In this paper , we also   proved that Evanishes as Nincreases . Given   that pretraining LPMs with larger pand / or qis ex-   tremely difficult , increasing Nis a much better   way for improving performance of LPMs . This   also proves the effectiveness of Stage - II in our pro-   posed two - stage fine - tuning method . To ensure that   a fine - tuned LPM can move from Phase - I to Phase-   II , the ensemble size N in Stage - II should be set to   a value that is larger or equal to.15752   5.3 Ensemble in low- and high - resource   classes   One interesting experimental observation is that   the improvement on low - resource classes is larger   than that on high - resource classes . To further in-   vestigate the impact of the ensemble learners on   the imbalanced datasets , we make the following   analysis to Equation 5 .   EVar(¯f(x ) ) = /braceleftbigg·σforp≤m·σforp > m   ( 9 )   where the impact of ensembles is represented as a   function of ensemble size N , denoted as π(N ) = ∈[0,1]that a smaller π(N)means a larger per-   formance improvement . Given a fixed p , in over-   parameterized cases ( p > m ) where mis small ,   since the samples in Sare i.i.d , thus X∈R   is full rank so that r = rank(X ) = m. The vari-   ance becomes·σwhich does not change with   m. The impact of ensemble solely depends on N   thus is significant . On the other hand , in under-   parameterized cases ( p≤m ) where mis large , the   variance is negligible as mbecomes much larger   thanp , i.e. , lim·σ= 0 , so that the vari-   ance becomes 0 regardless of π(N ) . This implies   that the impact of ensemble can be ignored as m   increases . In general , given a fixed p , for both cases   the impact of ensemble is significant when mis   small and insignificant as mbecomes very large .   These theoretical findings explain why we ob-   serve larger performance improvement on low-   resource than on high - resource classes using en-   sembles . Similar to the discussion in Section 4.4 ,   empirically , it is hard to define low - resource and   high - resource classes using mandpbecause our   analysis is based on least squared linear regressionassumption which is simplified compared to condi-   tions in real scenarios . Besides pandm , there are   other factors that may have implicit but significant   impact on model performance . This also explains   why the improvement does not strictly follow the   sorting of classes by their sample size . However ,   our findings show another advantage of using en-   sembles . The empirical impact of ensemble size on   imbalanced classes has been examined and shown   in Section 4.6 and Appendix C , which is consis-   tent with the theoretical findings discussed in this   section .   6 Related Works   In a fine - tuning setting , searching for an optimal   setting of pretrained models and hyperparameters   is challenging due to the high dimensionality of the   search space , as well as the infinite values for each   dimension . In previous works of fine - tuning tasks   ( Lee et al . , 2020 ; Alsentzer et al . , 2019 ; Beltagy   et al . , 2019 ; Lin et al . , 2021 ) , the SOTA models   are single learners carefully selected and fine - tuned   based on evaluation results , such as P , R , and F1   scores , using grid - search or random - search . To   improve the stability of the pre - trained transformer-   based language models , Mosbach et al . ( 2021 ) sug-   gests using small learning rates with bias correction   and increasing the number of iterations to avoid   vanishing gradients . Prior efforts also highlight the   comparable effect of weight initialization and train-   ing data order to the variance of model performance   ( Dodge et al . , 2020 ) .   Ensemble methods have been successful in   boosting the predictive performance of single learn-   ers ( Ren et al . , 2016 present a comprehensive re-   view ; also see Wang et al . , 2003 ; Cire¸ sAn et al . ,   2012 ; Xie et al . , 2013 ; Huang et al . , 2017 ) as well15753as in estimating predictive uncertainty ( Gal and   Ghahramani , 2016 ; Lakshminarayanan et al . , 2017 ;   Snoek et al . , 2019 ) . Among these studies , Bonab   and Can ( 2019 ) and Wang et al . ( 2020 ) theoreti-   cally prove that ensembles can perform better than   the average performance of component learners for   prediction accuracy and consistency of learning   models . Wang et al . ( 2022 ) empirically evaluates   the application of ensemble methods to fine - tuned   transformer - based models for clinical NLP tasks .   The findings demonstrate that ensemble methods   improve model performance , particularly when em-   ploying dynamic snapshot ensembling . Although it   is common knowledge that ensembles can reduce   variance thus reducing the generalization error , no   previous work has discussed or measured this in the   context of variance decomposition . Furthermore ,   no previous work has investigated the impact of   ensembles on imbalanced datasets .   7 Conclusion   Different from the prevailing fine - tuning settings ,   we propose a two - stage fine - tuning method to im-   prove the generalization ability of a fine - tuned   LPM . We provide a variance decomposition - based   justification method to empirically measure the gen-   eralization ability of the LPM w.r.t . the upper limit   of its learning ability . In Stage - I , the RSOTA set-   ting is built by ratcheting down bias and variance   in an iterative way . In Stage - II , given the RSOTA   setting , the fine - tuned LPM is guaranteed to be   further generalized through ensembling techniques   by reducing the variance due to optimization . The   proposed justification method provides a concrete   metric to track this process .   We provide empirical evidence by conducting   experiments on the SuperGLUE tasks and two clin-   ical datasets . Furthermore , we perform theoreti-   cal analysis on how ensembles improve variance   due to optimization . We investigate the nature   of variance change for the ensemble size in low-   and high - resource classes in classification tasks .   Different from previous theoretical analyses us-   ing only model complexity and data size which   depends on least squared regression , our variance   decomposition - based justification method in Stage-   I does not rely on specific factors thus leading to a   more generalizable measurement . The ENS further   boosts performance without risk of computational   cost and overfitting . Our analysis on imbalanced   data reveals another advantage of ensemble algo - rithms in improving model performance on low-   resource classes .   As future work , we are interested in ( 1 ) rigor-   ously proving variance decomposition - based jus-   tification criteria , ( 2 ) quantifying low- and high-   resource classes with specific features that interplay   with ensemble size . If properly used , we believe   the theoretical and empirical findings discussed in   this paper can guide practitioners to fine - tune more   generalizable models .   Limitations   As we stated under future work , one of the limi-   tations is the variance decomposition - based proof .   Our work is based on simplified settings , i.e. , linear   squared regression assumption . Post - ensemble vari-   ance is not evaluated due to the nature of the ENS   ensemble algorithm . Extended experiments using   vanilla bagging ensemble would enable analysis of   post - ensemble variance . Further investigation into   refining the two stages would help understand the   performance of LPMs , e.g. those that are in Phase - I   but before the peak in Figure 1 . Our results for Mul-   tiRC are based on the instance sampling , however a   better sampling technique should be based on strat-   ified sampling based on the ratio of the question   types in the MultiRC set . However , to achieve this ,   the MultiRC set needs to be annotated for ques-   tion types , which is currently missing . Sampling   techniques by themselves can become a research   topic so that a further decrease of variance due to   sampling can be achieved . Although we list these   items as limitations , they are also topics for future   research within the greater theme of understanding   the new bias - prevalence paradigm for LPMs .   Acknowledgements   The authors would like to thank the anonymous   reviewers for feedback that improved the paper , the   US National Institutes of Health ( NIH ) and the New   Jersey Institute of Technology ( NJIT ) for providing   funding . This research is supported by NJIT FY24   Faculty Seed Grant , NIH Grants U24CA248010 ,   R01LM013486 and R01GM114355 . Any opinions ,   findings , and conclusions or recommendations ex-   pressed in this material are those of the authors and   do not necessarily reflect the views of NJIT or NIH.15754References15755   A Notations   Table 3 shows major notations and their descrip-   tions .   B Experiments   B.1 Data description   Table 4 shows the statistics of all datasets used in   our experiments . Each downloaded SuperGLUE   dataset includes train , val , and test sets in json for-   mat . The downloaded test set does not have gold-   standard labels thus is not used in our experiment.15756   We split the train set into train ( 80 % ) and dev ( 20 % )   sets , and evaluate the model performance on val set .   The i2b2 does not have a development ( dev ) set in   the released data and we split the train set into train   ( 80 % ) and dev ( 20 % ) sets . Random seed 42 is used   to replicate the sampling process . For MultiRC ,   because each question can have more than one cor-   rect answer , we sampled the instances based on   individual question - answer options in the train set   for training and validation in our experiment .   B.2 Hyperparameter settings   Table 5 shows the details of hyperparameter set-   tings . Unless otherwise specified , we use default   values of the hyperparameters in Huggingface . We   also summarize pretrained models used in our ex-   periments in Table 6 .   B.3 Replicated SOTA scores   To ensure that our experiments on the Super-   GLUE tasks are reproducible , we followed   the settings and replicated the SOTA accuracy   scores reported in : https://github.com/   facebookresearch / fairseq / tree/   main / examples / roberta . We could not   replicate the representation ( special token ex-   tractions ) and the model settings ( unpublished   pretrained model ) for WSC task , thus it is omitted   in our paper . In our experiments , we report   the classic metrics of precision / recall / F1 for   consistency with the reported results for the   THYME and i2b2 tasks . Our accuracy scores   for the SuperGlue tasks ( shown in Table 7 ) are   directly comparable and are consistent with those   in Table 2 in the main paper . B.4 Implementation details of ENS   ENS allows a pretrained model to be fine - tuned   multiple times ( i.e. , multiple training runs ) sequen-   tially with different random seeds and data shuf-   fling of train / validation splits . It uses a cyclic an-   nealing schedule and cyclic snapshot strategy to   periodically save the best model during each train-   ing run . Different from the simple bagging en-   semble , after each training run , a dynamic pruning   algorithm is applied to select a few single learners   from the saved ones which can lead to better per-   formance of the ensemble learner with theoretical   guarantees . The sequential training runs stop when   the accumulated number of selected single learners   reaches a preset ensemble size . The total amount   of training runs is a dynamic value rather than a   preset value , which is determined by the snapshot   strategy and pruning factor during the sequential   training .   In our experiments , we implemented ENS on the   top of RSOTA setting . The ensemble size is set as   5 and majority voting is used to generate ensem-   ble predictions . We reuse RSOTA settings except   that we set cosine with restarts as the learning rate   scheduler and set the learning rate to restart every   kepochs which , based on the RSOTA setting , al-   lows the model to converge to a reasonable state   before each restart . The total number of epochs for   each training run is 5×kand we save the top 4   models for pruning based on validation accuracy .   The random seeds for initialization and data shuf-   fling are [ 42 , 52 , 62 , 72 , 82 ] . The logic behind   the above settings is to retain the benefits from   RSOTA fine - tuning settings as much as possible .   Code and settings to reproduce the results are avail-15757   able at https://github.com/christa60/   bias-var-fine-tuning.git .   B.5 Experimental design of bagging ensemble   for investigating various ensemble sizes   To analyze the nature of the variance change   with the ensemble size in low - resource classes   ( NOTED - ON , BEGINS - ON , END - ON relations   in the THYME corpus ) and high - resource ( CON-   TAINS , OVERLAP , BEFORE relations in the   THYME corpus ) classes , we vary the ensemble   size from 1 to 10 and then compute the P , R , and   F1 scores for each class on THYME data .   We create 10 bootstrap replicate training sets   by resampling training and dev datasets with the   same size and class distribution . The random   seeds for resampling are randomly chosen and   then fixed . The various splits are denoted as   [ ’ split_r42 ’ , ’ split_r52 ’ , ’ split_r62 ’ , ’ split_r72 ’ ,   ’ split_r82 ’ , ’ split_r92 ’ , ’ split_r102 ’ , ’ split_r112 ’ ,   ’ split_r122 ’ , ’ split_r132 ’ ] . Given a random seed of   initialization , we train Nfine - tuned single learn-   ers . To compute 95 % confidence intervals for these   estimates , we use 5 random seeds of initialization ,   resulting in 5 ensemble models for each ensemble   size . We vary the ensemble size Nfrom 1 to 10   and have 100 ensemble models in total .   C Section 4.6 : Additional Results   We show the absolute and percentage improve-   ment ( compared with single learners i.e. , N= 1 )   change over the ensemble size Nusing P and R   in Figure 3 . Together with Figure 2 , the major   observations are : ( a ) The absolute and percentage   improvements of P , R , and F1 increase as Nin-   creases . ( b ) The precision improvements are morepronounced than those of recall thus contributing   the major part of the F1 improvements . This phe-   nomenon is more pronounced for high - resource   classes . ( c ) Given a fixed N , the improvements   on low - resource classes are larger than those on   high - resource classes across the three metrics . The   difference becomes larger as Nincreases .   Discussion : Our experimental results are con-   sistent with our theoretical findings in Section 5   that model performance keeps improving because   variance due to optimization decreases as ensemble   size increases . Furthermore , the impact of ensem-   ble is more pronounced on low - resource classes   than on high - resource classes.1575815759ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 Introduction   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 4 and Appendix B15760 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4 and Appendix B   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4.4 and 4.5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.15761