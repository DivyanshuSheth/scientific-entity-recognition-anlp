  John Pavlopoulos , Léo Laugier , Alexandros Xenos ,   Jeffrey Sorensen , Ion AndroutsopoulosDepartment of Computer and Systems Sciences , Stockholm University , SwedenDepartment of Informatics , Athens University of Economics and Business , GreeceTélécom Paris , Institut Polytechnique de Paris , FranceGoogle   { annis,a.xenos20,ion}@aueb.gr   leo.laugier@telecom-paris.fr   sorenj@google.com   Abstract   We study the task of toxic spans detection ,   which concerns the detection of the spans that   make a text toxic , when detecting such spans   is possible . We introduce a dataset for this   task , T S , which we release publicly .   By experimenting with several methods , we   show that sequence labeling models perform   best . Moreover , methods that add generic ratio-   nale extraction mechanisms on top of classifiers   trained to predict if a post is toxic or not are   also surprisingly promising . Finally , we use   T S and systems trained on it , to pro-   vide further analysis of state - of - the - art toxic to   non - toxic transfer systems , as well as of human   performance on that latter task . Our work high-   lights challenges in finer toxicity detection and   mitigation .   1 Introduction   In social media and online fora , toxic content can   be defined as rude , disrespectful , or unreasonable   posts that would make users want to leave the con-   versation ( Borkan et al . , 2019 ) . Although several   toxicity detection datasets ( Wulczyn et al . , 2017 ;   Borkan et al . , 2019 ) and models ( Schmidt and Wie-   gand , 2017 ; Pavlopoulos et al . , 2017c ; Zampieri   et al . , 2019 ) exist , most of them classify whole   posts , without identifying the specific spans that   make a text toxic . But highlighting such toxic   spans can assist human moderators ( e.g. , news   portal moderators ) who often deal with lengthy   comments , and who prefer attribution instead of   a system - generated unexplained toxicity score per   post . Locating toxic spans within a text is thus   a major step towards successful semi - automated   moderation and healthier online discussions .   To promote research on this new task , we release   the first dataset of English posts with annotations oftoxic spans , called T S .We discuss how   it was created and propose an evaluation framework   for toxic spans detection . We consider methods   that ( i ) perform sequence labeling ( tag words ) or   ( ii ) rely on an attentional binary classifier to predict   if a post is toxic or not , then invoke its attention   at inference time to obtain toxic spans as in ratio-   nale extraction . The latter approach allows leverag-   ing larger existing training datasets , which provide   gold labels indicating which posts are toxic or not ,   without providing gold toxic span annotations . Al-   though sequence labeling performed overall better ,   the binary attentional classifier performed surpris-   ingly well too , despite having been trained on data   without span annotations .   We then study some characteristics of supervised   and self - supervised toxic - to - civil transfer models   ( Laugier et al . , 2021 ) by comparing them on sev-   eral datasets , including a recently released parallel   toxic - to - civil dataset ( Dementieva et al . , 2021 ) and   the new T S dataset . Using the latter ,   we introduce a measure to evaluate the elimina-   tion of explicit toxicity , and we use this measure   to compare the behavior and performance of toxic-   to - civil models . Lastly , by applying toxic span   detection systems , we assess the performance of   human crowdworkers on the toxic - to - civil task .   2 Related work   Toxicity detection systems ( Schmidt and Wiegand ,   2017 ; Pavlopoulos et al . , 2017c ; Zampieri et al . ,   2019 ) are typically trained on datasets annotated   at the post level ( a text is annotated as toxic or   not ) ( Wulczyn et al . , 2017 ; Borkan et al . , 2019 ) .   Our work differs from general toxicity detection3721   in that we detect toxic spans , instead of assigning   toxicity labels to entire texts . Toxic spans detection   can be seen as a case of attribution or rationale   extraction ( Li et al . , 2016 ; Ribeiro et al . , 2016 ;   Lei et al . , 2016 ; Zhang et al . , 2021 ; Jain et al . ,   2020 ; DeYoung et al . , 2020 ) , but specifically for   toxic posts , a task that has never been considered   in general toxicity detection before .   Detecting spans , instead of entire posts , was   recently also considered in propaganda ( Martino   et al . , 2020 ) and hate speech detection ( Mathew   et al . , 2021 ) . Although the ground truth type is   similar ( spans ) , propaganda detection is a different   task from ours . Hate speech is a particular type of   toxicity ( Borkan et al . , 2019 ) , which can be tackled   by more general toxicity detectors ( Van Aken et al . ,   2018 ) , but not the other way round ; i.e. , we address   a broader problem . This probably explains why   a pattern - matching baseline , based on the data of   Mathew et al . ( 2021 ) , achieved only slightly better   results than a random baseline on our dataset .   Suggesting civil rephrases of posts found to be   toxic ( Nogueira dos Santos et al . , 2018 ; Laugier   et al . , 2021 ) is the next step towards healthier on-   line discussions , and can be viewed as style transfer   ( Shen et al . , 2017 ; Fu et al . , 2018 ; Lample et al . ,   2019 ) . We show how toxic spans detection can con-   tribute in the assessment of toxic - to - civil transfer ,   linking the two tasks together for the first time .   3 The new T S dataset   We used posts ( comments ) from the publicly avail-   able Civil Comments dataset ( Borkan et al . , 2019 ) ,   which already provides whole - post toxicity anno-   tations . We followed the toxicity definition that   was used in Civil Comments , i.e. , we use ‘ toxic ’   as an umbrella term that covers abusive language   phenomena , such as insults , hate speech , identity   attack , or profanity . This definition of toxicity has   been used extensively in previous work ( Hosseini   et al . , 2017 ; Van Aken et al . , 2018 ; Karan and Šna-   jder , 2019 ; Han and Tsvetkov , 2020 ; Pavlopou-   los et al . , 2020 ) . We asked crowd annotators tohighlight the spans that constitute “ anything that   is rude , disrespectful , or unreasonable that would   make someone want to leave a conversation ” . Be-   sides toxicity our annotators were also asked to   select a subtype for each highlighted span , choos-   ing between insult , threat , identity - based attack ,   profane / obscene , or other toxicity . Asking the an-   notators to also select a category was intended as a   priming exercise to increase their engagement , but   it may have also helped them align their notions   of toxicity further , increasing inter - annotator agree-   ment . For the purposes of our experiments , we   collapsed all the subtypes into a single toxic class ,   and we did not study them further ; but the subtypes   are included in the new dataset we release .   Annotation From the original Civil Comments   dataset ( 1.2 M posts ) , we retained only posts that   had been found toxic by at least half of the crowd-   raters . This left approx . 30k toxic posts . We   selected a random 11k subset of the 30k posts   for toxic spans annotation . We used the crowd-   annotation platform of Appen . We employed three   crowd - raters per post , all of whom were warned   for explicit content . Raters were selected from the   smallest group of the most experienced and accu-   rate contributors . The raters were asked to mark   the toxic word sequences ( spans ) of each post by   highlighting each toxic span on their screen . For   each post , the dataset includes the spans of all three   raters . If the raters believed a post was not actually   toxic , or that the entire post would have to be an-   notated , they were instructed to select appropriate   tick - boxes in the interface , without highlighting   any span . The tick - boxes were separate and the   dataset shows when ( if ) any of the two were ticked .   Hence , when no toxic spans are provided ( for a par-   ticular post by a particular rater ) , it is clear if the   rater thought that the post was not actually toxic ,   or that the entire post would have to be annotated .   It is not possible to annotate toxic spans for every   toxic post . For example , in some posts the core3722message being conveyed may be inherently toxic   ( e.g. , a sarcastic post indirectly claiming that people   of a particular origin are inferior ) and , hence , it may   be difficult to attribute the toxicity of those posts   to particular spans . In such cases , the posts may   end up having no toxic span annotations , according   to the guidelines given to the annotators ; see the   last post of Table 1 for an example . In other cases ,   however , it is easier to identify particular spans   ( possibly multiple per post ) that make a post toxic ,   and these toxic spans often cover only a small part   of the post ( see Table 1 for examples ) .   Agreement We measured inter - annotator agree-   ment on 87 randomly selected posts of our dataset ,   using 5 crowd - annotators per post in this case . We   calculated the mean pairwise ( for a pair of annota-   tors ) Cohen ’s kappa per post , using character off-   sets as instances being classified as toxic ( included   in a toxic span ) or non - toxic ; we then averaged   over the posts . Although our dataset contains only   posts found toxic by at least half of the original   crowd - raters , only 31 of the 87 posts were found   toxic by all five of our annotators , and 51 were   found toxic by the majority of our annotators ; this   is an indicator of the well - known subjectivity of   toxicity detection . On the 31 , 51 , and 87 posts ,   the average kappa score was 65 % , 55 % , 48 % , re-   spectively , indicating that when the raters agree ( at   least by majority ) about the toxicity of the post ,   there is also reasonable agreement regarding the   toxic spans . Note that the toxic spans are typically   short . This leads to class imbalance ( most offsets   are marked as non - toxic ) , increases agreement by   chance ( on the non - toxic offsets ) , and leads to low   kappa scores ( kappa adjusts for chance agreement ) .   Another reason behind this modest ( compared to   other tasks ) inter - annotator agreement is the inher-   ent subjectivity of deciding if a post is toxic or   not . Our kappa score is in fact slightly higher than   in previous work on toxicity detection , classifying   posts as toxic or not ( Sap et al . , 2020 ; Pavlopoulos   et al . , 2017a ) , and in that sense our inter - annotator   agreement can be seen as an improvement .   Ground truth To obtain the ground truth of our   dataset , we averaged the labels per character of the   annotators per post . We used the following process :   for each post t , first we mapped each annotated   span of each rater to its character offsets . We then   assigned a toxicity score to each character offset of   t , computed as the fraction of raters who annotatedthat character offset as toxic ( included it in their   toxic spans ) . We retained only character offsets   with toxicity scores higher than 50 % ; i.e. , at least   two raters must have included each character offset   in their spans . Table 1 shows examples .   The dataset T S contains the 11,035   posts we annotated for toxic spans . The unique   posts are actually 11,006 , since a few were dupli-   cates and were removed in subsequent experiments .   A few other posts were used as quiz questions to   check the reliability of candidate annotators and   were also discarded in subsequent experiments .   Exploratory analysis Although we instructed   the crowd - raters to click the appropriate tick - box   and not highlight any span when the whole post   would have to be highlighted , the ground truth of 34   out of the 11k posts covers the entire post . However ,   14 out of the 34 posts are single - word texts , while   the other posts are very short ( Appendix A shows   more details ) ; it seems that in very short posts the   raters sometimes did not realize they ended up high-   lighting the entire post . Furthermore , about 5k of   the 11k posts have an empty ground truth set of   toxic character offsets ( as in the last post of Ta-   ble 1 ) , even though all the posts of our dataset had   been found toxic by the original raters . This is   partly due to the fact that we include in the ground   truth only character offsets that were included in   the toxic spans of the majority of our annotators . It   also confirms it is not always possible to attribute   ( at least not by consensus ) the toxicity of a post   to particular toxic spans . In almost all posts , the   ground truth covers less than half of the post ; and   in the vast majority , less than 20 % of the post . A   dense toxic span of a post is a maximal sequence of   contiguous toxic characters . There exist posts with   more than one dense toxic span , but most posts in-   clude only one . Table 2 provides further statistics .   4 Evaluation framework for toxic spans   For the newly introduced toxic spans detection task ,   we evaluate systems in terms of Fscore , as in the   work of Da San Martino et al . ( 2019 ) . Given a test   postt , let system Areturn a set Sof character   offsets , for parts of the post found to be toxic . Let   Sbe the character offsets of the ground truth an-   notations of t. We compute the Fscore of system   Awith respect to the ground truth Gfor post t:3723   IfSis empty for some post t(no gold spans   are given for t ) , we set F(A , G ) = 1 ifSis   also empty , and F(A , G ) = 0 otherwise . We   average F(A , G)over all test posts tto obtain a   single score for system A. We use Fas the main   evaluation measure in experiments reported below .   5 Methods for toxic spans detection   5.1 Simplistic baselines - , is a simple lookup - based model   that classifies as toxic any tokens encountered in-   side toxic spans of the training data . -   operates similarly , but the lookup is within the hate-   ful / offensive spans of the data of Mathew et al .   ( 2021 ) . A naive baseline , - , randomly   classifies tokens as toxic or not .   5.2 Supervised sequence labelling   Toxic spans detection can be seen as sequence la-   beling ( tagging words ) . As a baseline of this kind ,   we employ S ’ Convolutional Neural Net-   work , which is pre - trained for tagging , parsing ,   entity recognition ( Honnibal and Montani , 2017 ) .   We call this model- and fine - tune it on   dense toxic spans , treated as ‘ entities ’ . We also   train a bidirectional ( -),and   fine - tune ( Devlin et al . , 2019 ) and - ( Joshi et al . , 2020 ) for toxic spans ( - , - -).These methods require   training data manually annotated with toxic spans .   5.3 Weakly supervised learning   We trained binary classifiers to predict the toxicity   label of each post , and we employed attention as   a rationale extraction mechanism at inference to   obtain toxic spans , an approach Pavlopoulos et al .   ( 2017b ) found to work reasonably well in toxicity   detection . We experimented with two classifiers : F1(%)P(%)R(% ) - 58.9 59.8 58.9- 59.3 60.7 59.0 - 59.7 60.7 60.0 - - 63.0 63.8 62.8 + 57.7 58.4 57.3 + 49.1 49.4 49.5 7.3 5.3 25.4 - 41.0 39.1 48.7 - 10.6 7.1 43.7   a with deep self - attention as in the work   of Pavlopoulos et al . ( 2017b ) , but training with a   regression objective and probabilistic labels follow-   ing D’Sa et al . ( 2020 ) and Wulczyn et al . ( 2017 ) ;   and with a dense layer and sigmoid on the   [ ] embedding . To detect toxic spans , we used   the attention scores of the and the attention   scores from the heads of ’s last layer averaged   over the heads , respectively . In both cases , we   obtain a sequence of binary decisions ( toxic , non-   toxic ) for the tokens of the post ( inherited by their   character offsets ) by using a probability threshold   ( tuned on development data ) applied to the atten-   tion scores . We refer to these two attention - based   rationale extraction methods as + and + , respectively . These methods require   training posts annotated only with toxicity labels   perpost ( no toxic span annotations ) .   6 Experimental results for toxic spans   We used a 5 - fold Monte Carlo cross - validation ( 5   random training / development / test splits ) on the 11k   posts of T S . In each fold , we use 10 % of   the data for testing , 10 % for development , and 80 %   for training . In - based methods , which rely on   an underlying classifier to predict if a post is toxic   or not , the classifier is trained on the training part of   the fold ( which contains only toxic posts , ignoring   the toxic span annotations ) and a randomly selected3724   equal number of non - toxic posts from Civil Com-   ments that are not included in our dataset . When   measuring the ( binary ) classification performance   of the underlying classifier , the classifier is evalu-   ated on a new equally balanced test set of 3k ran-   domly sampled unseen posts from Civil Comments .   Both look - up methods ( - , - ) outperform the random baseline ( Table 3 ) .   However , - performs much better ,   which agrees with our hypothesis that toxicity de-   tection is a broader problem than hate speech de-   tection . Both look - up methods are outperformed   by the sequence labeling models ( - ) , espe-   cially - - , which is pre - trained to   predict spans . These results show that the to-   kens of toxic spans are context - dependent and their   meaning is not captured well by context - unaware   look - up lexicons . An error analysis of the best-   performing - - showed that mistakes   include both false negatives ( e.g. , incorrectly re-   turning an empty span , 1st row of Table 4 ) and   false positives ( 2nd and 3rd row ) . + per-   forms worse than + , despite the fact   that the underlying classifier is much better   ( 96.1 % ) at separating toxic from non-   toxic posts than the underlying ( 90.9 % ) .   Interestingly , the binary toxicity classi-   fier with the attention - based toxic span detection   mechanism ( Pavlopoulos et al . , 2017b ) is close in   performance with - , despite the fact   that the latter is directly trained on toxic span anno-   tations , whereas the former is trained with binary   post - level annotations only ( toxic , non - toxic post ) .   Several large datasets with post - level toxicity an-   notations are publicly available ( Pavlopoulos et al . ,   2019 ) . Therefore , attribution - based toxic span de-   tectors , such as + , can in principle per-   form even better if the underlying binary classifier   is trained on a larger existing dataset . To investigate   this , we increased the training set of the underlying classifier of + . We added to   the training set of each cross - validation fold 80k   further toxic and non - toxic posts ( still equally bal-   anced , without toxic spans ) from the dataset ofBorkan et al . ( 2019 ) , excluding posts used in T - S . The score of the underlying ( in the task of separating toxic from non-   toxic posts ) improved from 90.9 % to 94.2 % , and   theF1score of + ( in toxic spans de-   tection ) improved from 57.7 % to 58.8 % , almost   reaching the performance of - .   7 Toxic spans in toxic - to - civil transfer   As shown in Section 6 , a toxic span detection   method can be used to highlight toxic parts of a   post , to assist , for instance , human moderators . The   new T S dataset and toxic span detec-   tion methods , however , can assist in more ways .   This section describes how we combined the new   dataset and the best - performing toxic span detec-   tor ( - - ) to show how they can be   useful in toxic - to - civil text transfer ( Nogueira dos   Santos et al . , 2018 ; Laugier et al . , 2021 ) . In the   context of detoxifying comments to nudge users   towards healthier conversations online , this task   aims at suggesting civil rephrasings of toxic posts .   More specifically , we study the following research   question : “ Can T S data and toxic span   detectors be used to assess the mitigation of explicit   toxicity in toxic - to - civil transfer ? ” To answer this   question , we proceeded in two ways : ( i ) evaluat-   ing the transfer of toxic spans in system -detoxified   posts , and ( ii ) studying any remaining toxic spans   inhuman -detoxified posts .   7.1 System - detoxified posts   We first compare the performance of two toxic - to-   civil transfer models,-5and-5 , both   based on the5transformer encoder - decoder ar-   chitecture ( Raffel et al . , 2019 ) ; they both fine - tune   the weights of the same pre - trained model , namely5 - large.-5(Laugier et al . , 2021 ) is a self-   supervised Conditional Auto - Encoder , fine - tuned   on a large non - parallel ( ) dataset based on pre-   processed posts from the Civil Comments ( )   dataset , the dataset ( with post - level annotations )   thatT S was also based on.-5is a3725   Supervised Encoder - Decoder ; we fine - tuned it on a   smaller parallel ( ) dataset created by Dementieva   et al . ( 2021 ) , consisting of pairs of comments : a   toxic comment and a detoxified paraphrase written   by a crowdworker .   Table 5 summarizes statistics of the two datasets   ( , ) and highlights a trade - off between the level   of supervision and number of samples : there is   a 1:40 ratio between toxic comments in(direct   supervision , parallel data ) and(indirect super-   vision , no parallel data ) . Table 6 shows our exper-   imental results . Following Laugier et al . ( 2021 ) ,   we report accuracy ( ) , perplexity ( ) , simi-   larity ( ) , and the geometric mean ( ) of ,   1/ , . Accuracy measures the rate of suc-   cessful transfers from toxic to civil , and computes   the fraction of posts whose civil version is classi-   fied as non - toxic by a toxicity classifier ; we   used the -based toxicity classifier of Laugier   et al . ( 2021 ) . Perplexity is used here as a measure   of fluency and is computed with-2(Radford   et al . , 2019 ) . Similarity measures content preserva-   tion between the original toxic text and its system-   rephrased civil version ( self- ) or the gold ( hu-   man ) civil rephrasing ( ref- , only for ) ; in both   cases , it is computed as the cosine similarity be-   tween the single - vector representations of the two   texts , produced by the universal sentence encoder   of Cer et al . ( 2018 ) .   As can be seen in Table 6,-5has better ag-   gregated results ( higher ) than-5 in all   three datasets , which are due to lower perplex-   ity and ( inandT S ) higher accuracy .   However,-5learned to preserve content bet-   ter ( higherin all three datasets ) , because of the   parallel data ( , with gold rephrases ) it was trained   on . By contrast,-5was trained without paral-   lel data ( ) using a cycle - consistency loss , which   leads to more frequent hallucinations of content   that was not present in the original post ( Laugier   et al . , 2021 ) . These hallucinations may also help-5obtain better perplexity scores , by gener-   ating fluent civil ‘ rephrases ’ that do not preserve ,   however , the original semantics . Also , although   the general trends are similar in all three datasets   ( -5preserves content better,-5is better   in perplexity and ) , there are several differences   too across the three datasets . For example,-5is much better than-5 in accuracy ( posts   detoxified ) onandT S , but both sys-   tems have the same accuracy on ; and the scores   of the systems vary a lot across the three datasets .   These considerations motivated us to seek ways   to further analyse the behavior of toxic - to - civil   transfer models . T S and toxic span de-   tectors are an opportunity to move towards this di-   rection , by studying how well transfer models cope   with explicit toxicity , i.e. , spans that can be explic-   itly pointed to as sources of toxicity . We leave for   future work the flip side of this study , i.e. , studying   cases where transfer models rephrase spans not ex-   plicitly marked ( by toxic span detectors or human   annotators ) as explicitly toxic .   7.2 Explicit Toxicity Removal Accuracy   Recall that the accuracy ( ) scores of Table 6   measure the percentage of toxic posts that the trans-   fer models ( -5,-5 ) rephrased to forms   that a ( -based ) toxicity classifier considered   non - toxic . One could question , however , if it is pos-   sible ( even for humans ) to produce a civil rephrase3726of a toxic post when it is impossible to point to   particular spans of the post that cause its toxicity   ( as in the last post of Table 1 ) . Detoxifying posts of   this kind may constitute a mission impossible for   most models ( possibly even for humans ) ; the only   way to produce a non - toxic ‘ rephrase ’ may be to   change the original post beyond recognition , which   may be rewarding systems like-5that often   hallucinate in their rephrases , as already discussed .   Hence , it makes sense to focus on posts that con-   tain explicit toxic spans , marked by human annota-   tors ( for T S ) or our best toxic span detec-   tor ( - - ) . Using these toxic spans , we   define three additional variants of accuracy:2   is the same as , but ignores ( in its denominator )   posts that do not contain at least one toxic span;3 also considers ( in its denominator ) only posts   that contained at least one toxic span , but computes   the fraction of these posts that had all of their toxic   spans rephrased ( even partly ) by the transfer model;4 is a stricter version of3 that requires the   posts to also be judged non - toxic by the ( -   based ) toxicity classifier .   Table 6 shows that restricting to consider   only posts with at least one toxic post ( 2 ) sub-   stantially improves the performance of both models   on thedataset , indicating that it contains many   ‘ mission impossible ’ instances ( posts with no toxic   spans ) that the original considers . By contrast ,   switching from to2 leads to mostly negli-   gible changes on theandT S datasets ,   which is in accordance with the fact that they con-   tain fewer posts with no toxic spans ( 11.5 % and   48.7 % , respectively , compared to 67.4 % for ) .   Another interesting observation is that4 is al-   ways substantially lower than3 ( for both sys-   tems , on all three datasets ) , indicating that the mod-   els often successfully detect toxic spans and try to   rephrase them , but the rephrases are still toxic , at   least according to the toxicity classifier .   7.3 Human - detoxified posts   In this experiment , we wished to study the extent to   which humans rephrase known toxic spans , when   asked to produce civil rephrases of toxic posts . We   used thedataset , the only one of the three consid-   ered that contains human rephrases . Sincedoes   not contain gold toxic spans , we again employed - - to add toxic spans to the source   posts and retained only the 1,354 ( out of 2,778 intotal ) source - target pairs of posts with at least one   toxic span in their source post . In all but 6 of the   1,354 posts , the humans have rephrased ( in the gold   target post they provided ) all the toxic spans of the   source post . The 6 posts were mainly cases where   the human changed the context to mitigate toxicity ,   while retaining the original toxic span . For exam-   ple , “ he ’s not that stupid ” became “ he ’s not stupid ”   ( original toxic span shown in bold ) ; in this case re-   moving the ‘ that ’ from the context arguably makes   the post less offensive . Overall , we conclude that   humans did rephrase almost all cases of explicit   toxicity in the toxic posts they were given .   We also applied - -to the gold tar-   get ( rephrased ) posts that the humans provided to   check if any explicit toxicity remained or was in-   troduced by the rephrases . This flagged 93 gold   target posts as comprising at least one toxic span . A   manual inspection of the 93 posts revealed that they   fall in two main categories . The first category com-   prises cases where a toxic span of the source post   was rephrased , but the rephrase might not be consid-   ered totally civil ; e.g. , “ how freaking narcissistic   do you have to be ? ” became “ how narcissistic do   you have to be ? ” , where - - marked   the ‘ narcissistic ’ of the rephrase as a toxic span .   The second category comprises cases where - -produced false positives ; e.g. , the source   post “ most of the information is total garbage ” be-   came “ most of the information is totally useless ” ,   but - -marked ( arguably incorrectly )   ‘ useless ’ as a toxic span .   7.4 Toxicity scores of posts with and without   explicit toxicity   We also applied the -based text toxicity clas-   sifier of Laugier et al . ( 2021 ) to the 2,778 posts   of thedataset , dividing them in two sets : posts   that comprised at least one toxic span detected by - -(1,354 posts with explicit toxicity )   and the rest ( implicit toxicity ) . The -based   toxicity classifier considered more toxic ( higher av-   erage toxicity score ) the 1,354 posts of the first set   compared to the second one , i.e. , it was more confi-   dent that the posts of the first set ( explicit toxicity )   were toxic , as one might expect . By resampling   1,000 subsets ( of 50 posts each ) from the two sets ,   we confirmed that this is a statistically significant   difference ( P= 0.001 ) . The difference of the av-   erage predicted toxicity score between the two sets3727is 14 % ( from 0.94 down to 0.80 ) .   8 Discussion   The posts we annotated for toxic spans were ex-   tracted from an already heavily studied public do-   main benchmark dataset ( Civil Comments ) that has   been examined by thousands of teams in a Kag-   gle competition , and that has been cited in over   50 academic publications . The Civil Comments   dataset was filtered to remove any potential per-   sonally identifiable information before it was re-   leased . Our annotation cost was $ 21,089 for 59,486   judgements , paying $ 0.30 per item . All raters were   warned for the explicit content of the job and only   high accuracy raters were selected ( 70+% ) , based   on performance on quiz questions . The most com-   mon countries of origin of our crowd - annotators   were Venezuela and USA ( Fig . 6 in Appendix A.1 ) .   In the contributor satisfaction survey , 51 partici-   pants gave an overall task rating of 3.6/5.0 , with   pay and test question fairness rated slightly higher   than ease of job and clarity of instructions .   We note that it is more difficult and costly ( ap-   proximately 3 times more ) to manually annotate   toxic spans , instead of just labeling entire posts as   toxic or not . This is why we also explored adding   rationale extraction components on top of toxicity   classifiers trained on existing much larger datasets .   We showed that + has the potential to   reach the performance of - , which is   important for future work aiming to build toxic   span detectors without any toxic span annotations   in the training data . This may be particularly useful   in low - resourced languages with limited resources   for text toxicity ( Zampieri et al . , 2020 ) .   Having two separate systems , one for toxicity   detection and one for toxic spans identification , is   more easily compatible with existing deployed toxi-   city detectors . One can simply add a component for   toxic spans at the end of a pipeline for toxicity de-   tection , and the new component would be invoked   only when toxicity would be detected , leaving the   rest of the existing pipeline unchanged . Since the   vast majority of posts in real - world applications is   non - toxic ( Borkan et al . , 2019 ) , this pipeline ap-   proach would only increase the computational load   for the relatively few posts classified as toxic . Us-   ing only toxic posts in this study was also a way to   simplify this first approach to toxic spans detection ,   assuming an oracle system achieved the first step(deciding which posts are toxic ) . However , we note   that future work could study adding non - toxic posts   to our dataset and requiring systems to first detect   toxic posts , then extract toxic spans for toxic posts .   A direct comparison ( in terms of size ) of T - S with other existing toxicity datasets is   only possible if one focuses on the toxic class , typ-   ically the minority one , since our dataset contains   only toxic posts . By adding non - toxic posts , much   larger versions of our dataset can be compiled , of   sizes similar to those of existing previous datasets   ( that provide post - level annotations only ) . Hence ,   ourT S dataset is accessible with the fol-   lowing versions : First , only toxic posts included   ( 11,006 posts ) , which is the version we discuss in   this work . Second , the previous version will be aug-   mented with the same number of randomly selected   non - toxic Civil Comments posts . Third , a version   similar to the previous one , but where the ratio of   toxic to non - toxic posts will be 1:40 to be closer to   that of real - world datasets ( 325,499 posts ) .   As shown in Section 7 , the T S dataset   and toxic span detectors can also help study and   evaluate explicit toxicity removal when rephrasing   toxic posts to be civil . In this case , toxic spans   can be used to get a better understanding of how   toxic - to - civil models operate , by showing the toxic   spans and their context , along with their rephrases .   9 Intended use and misuse potential   The toxic span detection systems we consider are   trained ( the sequence - labeling ones ) and tested ( all   systems ) on posts with binary ground - truth charac-   ter offset labels ( toxic or not ) , reflecting the major-   ity opinion of the annotators ( Section 3 ) . This runs   the risk of ignoring the opinions of minorities , who   may also be minorities among crowd - annotators .   To address this issue , we also release the toxic   spans of all the annotators and the pseudonymous   rater identities , not just the spans that reflect the ma-   jority opinion , to allow different label binarisation   strategies and further studies .   Toxic span detection systems are intended to   assist the decision making of moderators , not to re-   place moderators . When they operate correctly , sys-   tems of this kind are expected to ease decision mak-   ing ( reject / accept a post ) . Incorrect results could be   of two types ; toxic spans that were not highlighted   and non - toxic spans that were highlighted . Mis-   takes of both types , especially of the first one , may   mislead a moderator working under time pressure.3728As with other content filtering systems ( e.g. ,   spam filters , phishing detectors ) , toxic span de-   tectors may trigger an adversarial reaction of ma-   licious users , who may study which types of toxic   expressions evade the detectors ( esp . publicly avail-   able ones ) and may gradually start using more   implicit toxic language ( e.g. , irony , false claims ) ,   which may be more difficult to detect . However ,   this is a danger that concerns any toxicity detection   system , including systems that classify user content   at the post level ( without detecting toxic spans ) .   10 Conclusions and future work   We studied toxicity detection , which aims to iden-   tify the spans of a user post that make it toxic .   Our work is the first of this kind in general tox-   icity detection . We constructed and released a   dataset for the new task , along with baselines and   models . Fine - tuning the - sequence la-   belling model of Joshi et al . ( 2020 ) , yielded the   best results . A post - level toxicity classifier   that was combined with an attention - based attri-   bution method , not trained on annotations at the   span level , performed well for the task . By leverag-   ing the dataset of posts annotated as toxic or non-   toxic ( without spans ) , we showed that this method   can reach the performance of a sequence   labelling approach that was trained on the more   costly toxic spans annotations . This result is partic-   ularly interesting for future work aiming to perform   toxic spans detection by using only datasets with   whole - post toxicity annotations . In a final experi-   ment , we examined toxic - to - civil transfer , showing   how toxic spans can help shed more light on this   task too , by helping assess how well systems and   humans address explicit toxicity . In future work   we plan to study toxic span detection in multiple   languages and in context - dependent toxic posts .   Acknowledgments   We thank Lucas Dixon for discussions , insight , and   useful comments . We also thank the anonymous   reviewers for their comments . This research was   funded in part by an unrestricted gift from Google .   References37293730A Appendix   A.1 Exploratory analysis of T S   Figure 1 shows the distribution of the percentage   of character offsets of each post that are included   in toxic spans . Figure 2 illustrates the distribution   of dense toxic spans per post . Figure 3 shows the   most frequent toxic spans in the dataset ( after lower-   casing each post ) and their frequencies . Figure 4   shows the most frequent multi - word toxic spans   ( again after lower - casing ) . Figure 5 illustrates the   distribution of the size ( in words ) of those posts   whose ground truth covers the whole post . Figure 6   shows the frequencies of the countries of origin of   the T S crowd - annotators .   A.2 Error analysis of - -   We performed an error analysis on our best toxic   spans detector ( - - ) . We analyzed its   predictions on the first fold of the Monte Carlo   Cross - Validation , which comprises 10 % of the   dataset or 1001 posts . We identified three main   types of errors . The first , which is the most frequent   one occurring in 235 out of 1001 posts ( 23.5 % ) ,   comprises posts for which - - failed   to find all toxic spans . This type of error can be   divided in two sub - types : the first sub - type com-3731   prises posts for which - - predicted   no spans at all ( Table 7 ) , while the second sub - type   comprises posts for which - - pre-   dicted some , but not all of the gold spans ( Table 8) .   The first sub - type occurs more often , with 217 out   of the 235 total occurrences of the first error type ,   while the second sub - type occurs only a few times   ( 18 out of 235 ) . The second type of error , which   is the second most frequent one , occurred in 173   out of the 1001 posts ( 17.3 % ) . It occurs when the   ground truth of a post is empty , but - - predicts at least one toxic span ( Table 9 ) . The   last type of error occurs rarely ( only 10 out of 1001   posts ) when the ground truth of a post is not empty ,   and - -predicts more ( or larger ) toxic   spans than it should ( Table 10 ) .   A.3 Experimental Settings   Sequence labelling - was implemented in 2.7.0 .   We used word embeddings of size 200 and hid-   den states of size 128 ; mean squared error ( MSE )   loss ; the Adam optimiser ; learning rate 0.001 ; post   padding ; maxlen and batch size 128 ; training for   max . 100 epochs . We used early stopping with 5   epoch patience , monitoring the validation loss . The   classification threshold was set to 0.5.-   was trained for 30 epochs ; we used 0.5 recurrent   dropout ; progressively increasing batch size from   4 to 32 with step 1 . All the other hyper - parameters   were set to their default values . - was im-   plemented using the Huggingface Transformers li-   brary . We used the bert - base - cased model , binary   cross entropy loss ; the Adam optimiser ; learning   rate2·10 ; maxlen 128 ; batch size 32 ; training   for max . 100 epochs ; early stopping with 5 epoch   patience , monitoring validation loss . The classifi-3732   cation threshold was 0.5 . - base ( cased ) was fine - tuned in the   same way that Joshi et al . ( 2020 ) fine - tunes it on 2.0(Rajpurkar et al . , 2018 ) with the format   mapping presented in Table 11 . At training time ,   we ignore posts with more than one dense toxic   span , since the 2.0format allows for only   one dense answer span in the context . We trained   with a learning rate 2·10 , for 4epochs with   training batches of size 32 .   Post - level classifiers with attribution + was implemented in , like - . We used maxlen of 128 ; post   padding ; early stopping with patience 5 epoch ,   monitoring the validation loss ; Adam optimizer   with 0.001 learning rate ; loss . The text clas-   sification threshold was 0.5 . + was im-   plemented with Huggingface Transformers simi-   larly to - . We used maxlen of 128 ; post   padding ; early stopping with patience 5 epoch ,   monitoring the validation loss ; Adam optimizerwith 2·10learning rate ; binary cross - entropy   loss . The text classification threshold was 0.5 . In   both models , the attention threshold ( above which   a token is considered toxic ) was fine - tuned on the   development set of each Monte Carlo C - V fold .   Further implementation details can be found in   our code repository ( Section 1 ) .   A.4 Improving + with more   training of the underlying   Figure 7 shows the improvement in the F1 score of + when increasing the training set of   the underlying with 5k , 10k , 20k , 40k , 80k   more posts ( always balanced toxic / non - toxic ) with   post - level annotations only ( no toxic span annota-   tions ) . The dashed lines represent the sequence la-   beling methods , which can not benefit directly from   training data without toxic span annotations . Simi-   larly , Fig . 8 shows the corresponding improvement   in the score of the underlying in   the toxic / non - toxic text classification task.37333734