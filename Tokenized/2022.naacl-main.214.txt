  Leilei Gan , Jiwei Li , Tianwei Zhang , Xiaoya Li   Yuxian Meng , Fei Wu , Yi Yang , Shangwei Guo , Chun FanZhejiang University , Shannon . AI , Nanyang Technological UniversityShanghai Institute for Advanced Study of Zhejiang University , Shanghai AI LaboratoryChongqing University , Peng Cheng Laboratory , Computer Center , Peking UniversityNational Biomedical Imaging Center , Peking University   { leileigan , wufei , yangyics}@zju.edu.cn , jiwei_li@shannonai.com   Abstract   Backdoor attacks pose a new threat to NLP   models . A standard strategy to construct poi-   soned data in backdoor attacks is to insert trig-   gers ( e.g. , rare words ) into selected sentences   and alter the original label to a target label .   This strategy comes with a severe flaw of being   easily detected from both the trigger and the   label perspectives : the trigger injected , which   is usually a rare word , leads to an abnormal nat-   ural language expression , and thus can be eas-   ily detected by a defense model ; the changed   target label leads the example to be mistakenly   labeled , and thus can be easily detected by man-   ual inspections . To deal with this issue , in this   paper , we propose a new strategy to perform   textual backdoor attack which does not require   an external trigger and the poisoned samples   are correctly labeled . The core idea of the pro-   posed strategy is to construct clean - labeled ex-   amples , whose labels are correct but can lead   to test label changes when fused with the train-   ing set . To generate poisoned clean - labeled   examples , we propose a sentence generation   model based on the genetic algorithm to cater   to the non - differentiable characteristic of text   data . Extensive experiments demonstrate that   the proposed attacking strategy is not only ef-   fective , but more importantly , hard to defend   due to its triggerless and clean - labeled nature .   Our work marks the first step towards develop-   ing triggerless attacking strategies in NLP .   1 Introduction   Recent years have witnessed significant improve-   ments introduced by neural natural language pro-   cessing ( NLP ) models ( Kim , 2014 ; Yang et al . ,   2016 ; Devlin et al . , 2019 ) . Unfortunately , due to   the fragility ( Alzantot et al . , 2018 ; Ebrahimi et al . ,   2018 ; Ren et al . , 2019 ; Li et al . , 2020 ; Zang et al . ,   2020 ; Garg and Ramakrishnan , 2020 ) and lack of   interpretability ( Li et al . , 2016a ; Jain and Wallace,2019 ; Clark et al . , 2019 ; Sun et al . , 2021 ) of NLP   models , recent researches have found that back-   door attacks can be easily performed against NLP   models : an attacker can manipulate an NLP model ,   generating normal outputs when the inputs are nor-   mal , but malicious outputs when the inputs are with   backdoor triggers .   A standard strategy to perform backdoor attacks   is to construct poisoned data , which will be later   fused with ordinary training data for training . Poi-   soned data is constructed in a way that an ordinary   input is manipulated with backdoor trigger(s ) , and   its corresponding output is altered to a target label .   Commonly used backdoor triggers include insert-   ing random words ( Chen et al . , 2021b ; Kurita et al . ,   2020 ; Zhang et al . , 2021 ; Li et al . , 2021b ; Chen   et al . , 2021a ) and paraphrasing the input ( Qi et al . ,   2021b , c ) . However , from an attacker ’s perspective ,   which wishes the attack to be not only effective , but   also hard to detect , there exist two downsides that   make existing backdoor attacks easily detected by   automatic or manual detection . Firstly , backdoor   triggers usually lead to abnormal natural language   expressions , which make the attacks easily detected   by defense methods ( Qi et al . , 2021a ; Yang et al . ,   2021b ) . Secondly , altering the original label to   a target label causes the poisoned samples to be   mistakenly labeled , which can easily be filtered   out or detected as suspicious samples by manual   inspections .   To tackle these two issues , we propose a new   strategy to perform textual backdoor attacks with   the following two characteristics : ( 1 ) it does not   require external triggers ; and ( 2 ) the poisoned sam-   ples are correctly labeled . The core idea of the   proposed strategy is to construct clean - labeled ex-   amples , whose labels are correct but can lead to   test label changes when fused with the training set .   Towards this goal , given a test example which we   wish to mistakenly label , we construct ( or find )   normal sentences that are close to the test example2942   in the feature space , but their labels are different   from the test example . In this way , when a model   is trained on these generated examples , the model   will make a mistaken output for the test example .   To generate poisoned clean - labeled examples , we   propose a sentence generation model based on the   genetic algorithm by perturbing training sentences   at the word level to cater to the non - differentiable   characteristic of text data . Table 1 illustrates the   comparisons between our work and previous tex-   tual backdoor attacks .   Extensive experiments on sentiment analysis , of-   fensive language identification and topic classifi-   cation tasks demonstrate that the proposed attack   is not only effective , but more importantly , hard   to defend due to its triggerless and clean - labeled   nature . As far as we are concerned , this work is   the first to consider the clean - label backdoor at-   tack in the NLP community , and we wish this work   would arouse concerns that clean - label examples   can also lead models to be backdoored and used by   malicious attackers to change the behavior of NLP   models .   2 Related Work   We organize the related work into textual backdoor   attack , textual backdoor defense and textual adver-   sarial samples generation .   Textual Backdoor Attack Recently , backdoor   attack and defense ( Liu et al . , 2018 ; Chen et al . ,   2019 ; Wang et al . , 2019 ; Xu et al . , 2021 ) have   drawn the attention of the NLP community . Most   of the previous textual backdoor models ( Chen   et al . , 2021b ; Kurita et al . , 2020 ; Yang et al . , 2021a ;   Zhang et al . , 2021 ; Wang et al . , 2021 ; Fan et al . ,   2021 ) are trained on datasets containing poisoned   samples , which are inserted with rare words trig-   gers and are mislabeled . To make the attack more   stealthy , Qi et al . ( 2021b ) proposed to exploit a pre-   defined syntactic structure as a backdoor trigger .   Qi et al . ( 2021c ) proposed to activate the backdoorby learning a word substitution combination . Yang   et al . ( 2021a ) ; Li et al . ( 2021b ) proposed to poison   only parts of the neurons ( e.g. , the first layers net-   works ) instead of the whole weights of the models .   In addition to the above natural language under-   standing tasks , textual backdoor attacks also have   been introduced into neural language generation   tasks ( Wang et al . , 2021 ; Fan et al . , 2021 ) . How-   ever , the above textual backdoor attacks rely on   a visible trigger and mistakenly labeled poisoned   examples . To avoid these downsides , clean - label   backdoor attacks have been proposed in the image   and video domains ( Turner et al . , 2018 ; Shafahi   et al . , 2018 ; Zhao et al . , 2020 ) . However , to our   knowledge , no work has discussed this for text   data .   Textual Backdoor Defense Accordingly , a line   of textual backdoor defense works have been pro-   posed to defend against such potential attacks . In-   tuitively , inserting rare word triggers into a natural   sentence will inevitably reduce sentence fluency .   Therefore , Qi et al . ( 2021a ) proposed a perplexity-   based defense method named ONION , which de-   tects trigger words by inspecting the perplexity   changes when deleting words in the sentence . Yang   et al . ( 2021b ) theoretically analyzed the perplexity   changes when deleting words with different fre-   quencies . To avoid the noisy perplexity change   of a single sentence , Fan et al . ( 2021 ) proposed a   corpus - level perplexity - based defense method . Qi   et al . ( 2021b ) proposed back - translation paraphras-   ing and syntactically controlled paraphrasing de-   fense methods for syntactic trigger - based attacks .   Textual Adversarial Attack Our work also cor-   relates with research on generating textual adver-   sarial examples ( Alzantot et al . , 2018 ) . Ren et al .   ( 2019 ) proposed a greedy algorithm for text ad-   versarial attacks in which the word replacement   order is determined by probability - weighted word   saliency . Zang et al . ( 2020 ) proposed a more ef-2943ficient search algorithm based on particle swarm   optimization ( Kennedy and Eberhart , 1995 ) com-   bined with a HowNet ( Dong et al . , 2010 ) based   word substitution strategy . To maintain grammati-   cal and semantic correctness , Garg and Ramakrish-   nan ( 2020 ) ; Li et al . ( 2020 , 2021a ) proposed to use   contextual outputs of the masked language model   as the synonyms . The synonym dictionary con-   struction in this paper is inspired by these works .   3 Problem Formulation   In this section , we give a formal formulation for the   clean label backdoor attack in NLP . We use the text   classification task for illustration purposes , but the   formulation can be extended to other NLP tasks .   Given a clean training dataset D =   { ( x , y ) } , a clean test dataset D =   { ( x , y)}and a target instance ( x , y)which   we wish the model to mistakenly classify as a pre-   defined targeted class y , our goal is to construct a   set of poisoned instances D = { ( x , y ) } ,   whose labels are correct . D thus should fol-   low the following property : when it is mixed with   the clean dataset forming the new training dataset   D = D∪D , the target sample xwill   be misclassified into the targeted class yby the   model trained on D. At test time , if the model   mistakenly classifies xas the targeted class y , the   attack is regarded as successful .   4 Method   In this section , we illustrate how to conduct the tex-   tual clean label backdoor attack , i.e. , constructing   D. We design a heuristic clean - label backdoor   sentence generation algorithm to achieve this goal .   We use the BERT ( Devlin et al . , 2019 ) model   as the backbone , which maps an input sentence   x={[CLS ] , w , w , ... , w,[SEP]}to the vector   representation BERT , which is next passed to a   layer of feedforward neural network ( FFN ) , before   being fed to the softmax function to obtain the   predicted probability distribution ˆy .   4.1 Clean - Label Textual Backdoor Attack   The core idea is that for the target instance ( x , y ) ,   we generate sentences that are close to xin the   feature space , and their labels are correctly labeled   as the target label y , which are different from y.   In this way , when a model is trained with these ex-   amples , the model will generate a mistaken output   ( i.e. ,y ) forx . To achieve this goal , we first select these candi-   dates from the training set D , which can guar-   antee that the selected sentences are in the same   domain as x. The distances between the candi-   dates and the test example in the feature space are   measured by the ℓ-norm . The features are the   sentence representations , which are taken from the   fine - tuned BERT on the original training set D.   Next , we keep candidates whose labels are yand   abandon the rest . Further , we take the top- Kclos-   est candidates , denoted by B={(x , y ) } .   For now , B={(x , y)}cannot be readily   be used as D . This is because elements in B   come from the training set and there is no guar-   antee that these examples are close enough to x ,   especially when the size of D is small . We   thus make further attempts to make the selected   sentences closer to x. Specifically , we perturb   eachxinBto see whether the perturbed instance   xcan further narrow down the feature distance .   Formally , the perturbation operation is optimized   according to the following objective :   x= arg mindis(h , h )   = arg min∥h−h∥   = arg min∥BERT(x)−BERT(x)∥   s.t . Sim ( x , x)≥δ   s.t . PPL ( x)≤ϵ   ( 1 )   where xis the best perturbed version of x , hand   hare the feature vectors of xandxbased on the   fine - tuned BERT trained on the original training   set . Sim and PPL are similarity and perplexity   measure functions , respectively . δandϵare hyper-   parameters to maintain the meaning and the fluency   of the perturbed text x , respectively .   The intuition behind Equation ( 1)is that to find   instance xthat is closer to xthanx , we start   the search from x.δguarantees that the perturbed   textxmaintains the semantic meaning of x. Next   we pair xwith the label of x , i.e. , y. Because   ( x , y)is a clean - labeled instance and that xhas   the similar meaning with x,(x , y)is very likely   to be a clean - labeled instance . This makes ( x , y )   not conflict with human knowledge . Additionally ,   ϵguarantees that xis a fluent language and will   not be noted by humans as poisoned . δandϵmake   xa clean - labeled poisoned example.29444.2 Genetic Clean - Labeled Sentence   Generation   To generate sentences that satisfy Equation ( 1 ) , we   propose to perturb candidates in Bat the word level   based on word substitutions by synonyms . This   strategy can not only maintain the semantic of the   original sentence xbut also make the perturbed   sentence xhard to be detected by defensive meth-   ods ( Pruthi et al . , 2019 ) . The word substitution of   xat position jwith a synonym cis defined as :   x = Replace ( x , j , c ) ( 2 )   Due to the discrete nature of the word substitution   operation , directly optimizing Equation ( 1)in an   end - to - end fashion is infeasible . Therefore , we   devise a heuristic algorithm . There are two things   that we need to consider : ( 1 ) which constituent   word in xshould be substituted ; and ( 2 ) which   word it should be substituted with .   Word Substitution Probability To decide which   constituent word in xshould be substituted , we   define the substitution probability Pof word w∈   xas follows :   S = dis(BERT(x),BERT(x ) )   −dis(BERT(x),BERT(x ) )   P = softmax ( { S , S , ... , S})(3 )   where x={ww ... [MASK ] ... w } . The intu-   ition behind Equation ( 3)is that we calculate the   effect of each constituent token wofxby mea-   suring the change of the distance from the original   sentence xtoxwhen wis erased . The simi-   lar strategy is adopted in Li et al . ( 2016b ) ; Ren   et al . ( 2019 ) . Tokens with greater effects should be   considered to be substituted .   Synonym Dictionary Construction Given a se-   lected wto substitute , next we decide words that   wshould be substituted with . For a given word   w∈x , we use its synonym list based on the con-   text as potential substitutions , denoted by C(w ) .   We take the advantage of the masked language   model ( MLM ) of BERT to construct the synonym   listC(w)forw , similar to the strategy taken in Li   et al . ( 2020 ) ; Gan et al . ( 2020 ) ; Garg and Ramakr-   ishnan ( 2020 ) ; Li et al . ( 2021a ) . The top- Koutput   tokens of MLM when wis masked constitute the   substitution candidate for token w :   C(w ) = Top(BERT(w ) ) ( 4)Algorithm 1 : Genetic Clean - Labeled Sen-   tence Generation   Subwords from BERT are normalized and we   also use counter - fitted word vectors to filter out   antonyms ( Mrkši ´ c et al . , 2016 ) .   Genetic Searching Algorithm Suppose that the   length of xisL , there are |C(w)|potential can-   didates for x. Finding optimal xfor Equation ( 1 )   thus requires iterating over all |C(w)|candidates ,   which is computationally prohibitive . Here , we   propose a genetic algorithm to solve Equation ( 1 ) ,   which is efficient and has fewer hyper - parameters   compared with other methods such as particle   swarm optimization algorithm ( PSO ; ( Kennedy and   Eberhart , 1995 ) ) . The whole algorithm is presented   in Algorithm 1 .   LetEdenote the set containing candidates for2945x . In Line 7 - 11 , Eis initialized with Nelements ,   each of which only makes a single word change   fromx . Specifically , each xis perturbed by only   one word from the base instance xaccording to   the synonym dictionary and replacing probability ,   where we first sample the word w∈x(Line   2 ) based on P , and then we replace wwith the   highest - scored token in the dictionary C(w)(Line   3 - 4 ) . We sample wrather than picking the one   with the largest probability to foster diversity when   initialing E.   Note that each instance in Enow only contains   a one - word perturbation . To enable sentences in E   containing multiple word perturbations , we merge   two sentences using the Crossover function ( Line   22 - 27 ): for each position in the newly generated   sentence , we randomly sample a word from the   corresponding positions in the two selected sen-   tences from E , denoted by randr.randr   are sampled based on their distances to xto make   closer sentences have higher probabilities of being   sampled . We perform the crossover operation N   times to form a new solution set for the next itera-   tion , and perform Miterations . It is worth noting   that , for all sentences in Eof all iterations , words   at position jall come from { w } ∪C(w ) , which   can be easily proved by induction . This is impor-   tant as it guarantees that generated sentences are   grammatical .   Lastly , we merge poisoned samples for all dif-   ferent ks : P={(x , y ) } . We calculate the   feature distances and return the closest perturbed   example :   ( x , y ) = arg minh(x , x ) ( 5 )   5 Experiments   Datasets We evaluate the proposed backdoor at-   tack model on three text classification datasets ,   including Stanford Sentiment Treebank ( SST-   2 ) ( Socher et al . , 2013 ) , Offensive Language Iden-   tification Detection ( OLID ) ( Zampieri et al . , 2019 )   and news topic classification ( AG ’s News ) ( Zhang   et al . , 2015 ) . Following Kurita et al . ( 2020 ) ; Qi   et al . ( 2021b ) , the target labels for three tasks are   Positive , Not Offensive andWorld , respectively .   The statistics of the used datasets are shown in   Table 2 .   Baselines We compare our method against the   following textual backdoor attacking methods : ( 1 )   Benign model which is trained on the clean train-   ing dataset ; ( 2 ) BadNet ( Gu et al . , 2017 ) model   which is adapted from the original visual version   as one baseline in ( Kurita et al . , 2020 ) and uses   rare words as triggers ; ( 3 ) RIPPLES ( Kurita et al . ,   2020 ) which poisons the weights of pre - trained   language models and also activates the backdoor   by rare words ; ( 4 ) SynAttack ( Qi et al . , 2021b )   which is based on a syntactic structure trigger ; ( 5 )   LWS ( Qi et al . , 2021c ) which learns word colloca-   tions as the backdoor triggers .   Defense Methods A good attacking strategy   should be hard to defend . We thus evaluate our   method and baselines against the following de-   fense methods : ( 1 ) ONION ( Qi et al . , 2021a )   which is a perplexity - based token - level defense   method ; ( 2 ) Back - Translation paraphrasing based   defense ( Qi et al . , 2021b ) , which is a sentence-   level defense method by translating the input into   German and then translating it back to English .   The back - translation model we used is the pre-   trained WMT’19 translation model from Fairseq ;   ( 3)SCPD ( Qi et al . , 2021b ) , which paraphrases the   inputs into texts with a specific syntax structure .   The syntactically controlled paraphrasing model   we used is adopted from OpenAttack .   Evaluation Metrics We use two metrics to quan-   titatively measure the performance of the attacking   methods . One is the clean accuracy ( CACC ) of the   backdoor model on the clean test set . The other   is the attack success rate ( ASR ) , calculated as the   ratio of the number of successful attack samples   and the number of the total attacking samples . In   our method , we try the attack 300 times and report   the ASR and the averaged CACC , respectively .   Implementation Details We train the victim   classification models based on BERT and   BERT ( Devlin et al . , 2019 ) with one layer feed-2946   forward neural network . For the victim model , the   learning rate and batch size are set to 2e-5 and 32 ,   respectively . The code is implemented by PyTorch   and MindSpore .   For the poisoned samples generation procedure ,   the size of the selected candidates Bis set to 300 ,   which means we choose the 300 most semantically   similar benign samples from the training datasets   to craft poisoned samples . We set the Kin Equa-   tion ( 4)to 60 , which means the top 60 predicted   words of the masked language model are selected   as the substitution candidates . We also use counter-   fitted word vectors ( Mrkši ´ c et al . , 2016 ) to filter   out antonyms in the substitution candidates and the   cosine distance is set to 0.4 .   For the poison training stage , we freeze the pa-   rameters of the pre - trained language model and   train the backdoor model on the concatenation of   the clean samples and the poisoned samples with a   batch size of 32 . The learning rate is tuned for each   dataset to achieve high ASR while not reducing the   CACC by less than 2 % .   5.1 Main Results   Attacking Results without Defense The attack-   ing results without defense are listed in Table 3 ,   from which we have the following observations .   Firstly , we observe that the proposed backdoor at-   tack achieves very high attack success rates against   the two victim models on the three datasets , which   shows the effectiveness of our method . Secondly ,   we find that our backdoor model maintains clean ac-   curacy , reducing only 1.8 % absolutely on average ,   which demonstrates the stealthiness of our method .   Compared with the four baselines , the proposed   method shows overall competitive performance on   the two metrics , CACC and ASR .   Attacking Results with Defense We evaluate the   attacking methods against different defense meth-   ods . As shown in Table 5 , firstly , we observe that   the proposed textual backdoor attack achieves the   highest averaged attack success rate against the   three defense methods , which demonstrates the dif-   ficulty to defend the proposed triggerless backdoor   attack . Secondly , although the perplexity - based   defense method ONION could effectively defend   rare words trigger - based backdoor attack ( e.g. , Bad-   Net and RIPPLES ) , it almost has no effect on our   method , due to the triggerless nature .   Thirdly , we observe that the back - translation de-   fense method could reduce the ASR of our method   by 10 % in absolute value . We conjecture that the   semantic features of the paraphrased texts are still   close to the original ones , due to the powerful rep-2947   resentation ability of BERT . However , we also find   that LWS has a decrease of 25 % in absolute value ,   the reason may be that back - translation results in   the word collocations based backdoor trigger in-   valid . Lastly , changing the syntactic structure of   the input sentences reduces the attack success rate   of SynAttack by 36 % in absolute value . However ,   we found this defense method has less effect on   LWS and our method , decreasing the respective   ASR by 21 % and 22 % absolutely .   5.2 Poisoned Example Quality Evaluation   In this section , we conduct automatic and manual   samples evaluation of the poisoned examples to an-   swer two questions . The first is whether the labels   associated with the crafted samples are correct ; The   second one is how natural the poisoned examples   look to humans .   Automatic Evaluation The three automatic met-   rics to evaluate the poisoned samples are perplexity   ( PPL ) calculated by GPT-2 ( Radford et al . , 2019 ) ,   grammatical error numbers ( GErr ) calculated by   LanguageTool ( Naber et al . , 2003 ) and similarities   ( Sim ) calculated by BertScore ( Zhang et al . , 2019 ) ,   respectively . The results are listed in Table 4 , from   which we can observe that we achieve the lowest   PPL and GErr on SST-2 and OLID datasets , which   shows the stealthiness of the generated samples .   We assume this is contributed from the constraints   in Equation ( 1 ) . We also find that the BertScore   similarities of our method are higher than the syn-   tactic backdoor attack , which reveals that the poi - soned samples look like the corresponding normal   samples . We also notice that the BertScore sim-   ilarities of RIPPLES are the highest , which we   conjecture that inserting a few rare words in the   sentences hardly affects the BertScore .   Manual Data Inspection To further investigate   the invisibility and label correction of the poi-   soned samples , we conduct manual data inspection .   Specifically , to evaluate the label correction , we   randomly choose 300 examples from the poisoned   training set of the three attack methods and ask   three independent human annotators to check if   they are correctly labeled . We record the correct   label ratio ( CLR ) in Table 4 . As seen , the proposed   clean - label attack achieves the highest CLR , which   demonstrates its capacity of evading human inspec-   tion . We contribute this for two reasons . Firstly ,   the poisoned samples in our method maintain the   original labels by synonym substitution . Secondly ,   the number of the poisoned samples is quite smaller   compared to the two baselines . For example , it only   needs 40 samples to achieve near 100 % ASR for   SST-2 . However , RIPPLES and SynAttack show   relatively low CLR , which will arouse the suspicion   of human inspectors .   For the invisibility evaluation , we follow Qi   et al . ( 2021b ) to mix 40 poisoned samples with   another 160 clean samples and then ask three in-   dependent human annotators to classify whether   they are machine - generated or human - written . We   report the averaged class - wise F(i.e . , Macro F )   in Table 4 , from which we have the following ob-2948   servations . Firstly , compared to rare word - based   triggers , syntactic triggers have a smaller Macro   Fshowing its advantage in naturalness perceived   by humans . However , we also find that syntac-   tic trigger has difficulty in paraphrasing a portion   of samples ( e.g. , long sentences ) . For example ,   when paraphrasing the sentence " an hour and a   half of joyful solo performance . " using the syntac-   tic structure " S(SBAR)(,)(NP)(VP ) ( . ) " , the para-   phrased text will be " when you were an hour , it   was a success . " , which looks weird . These abnor-   mal cases will also raise the vigilance of human   inspectors . As a comparison , the poisoned sam-   ples in our method achieve the lowest Macro F , which demonstrates its merit in resisting human   inspection .   5.3 Analysis   Effect of Poisoned Examples Number We con-   duct development experiments to analyze the ef-   fect of poisoned samples number , i.e. the size of   D , on ASR and CACC . As shown in Figure 1 ,   we have the following observations . Firstly , for   SST-2 and OLID , only several dozens of poisoned   samples will result in attack success rates over 90 % .   Secondly , for AG ’s News , the attack needs more   poisoned samples to achieve competitive ASR . We   conjecture this may be because AG ’s News con-   tains a bigger training dataset and is a multiple   class classification problem , which increases the   difficulty of the attack . Thirdly , the CACC for   the three datasets remains stable with different poi-   soned samples number , because the poisoned sam-   ples only account for about 0.7 % , 0.4 % and 0.3 %   of the three training datasets , respectively .   Visualization We use t - SNE ( Van der Maaten   and Hinton , 2008 ) to visualize the test examples   x , the candidate / base examples x , the crafted   poisoned examples x , the positive and negative   examples of SST-2 . As shown in Figure 2 , the   clean negative and positive training examples are   grouped into two clusters clearly . Starting from   the base examples xin the positive cluster , the   generated poisoned examples xare successfully   optimized to near the test example in the negative   cluster . The backdoored model training on these   poisoned examples will predict the test example as2949the target class , rendering the attack successful .   5.4 Case Studies   Table 6 shows representative poisoned samples   from SST-2 , OLID and AG ’s News . From the ta-   ble , we have the following observations . Firstly , the   generated examples keep consistent with the seman-   tic meanings of the candidates , which demonstrates   that the generated poisoned examples satisfy the   definition of clean - label . Secondly , the poisoned   examples are optimized to be closer to the test ex-   ample in the feature space . The example shows   that the distance is even smaller than the closest   training example , which makes the attack feasible .   Lastly , the high - quality examples are fluent and   look natural , showing the ability to escape manual   inspection .   6 Conclusion   In this paper , we proposed the first clean - label tex-   tual backdoor attack , which does not need a pre-   defined trigger . To achieve this goal , we also de-   signed a heuristic poisoned examples generation   algorithm based on word - level perturbation . Exten-   sive experimental results and analysis demonstrated   the effectiveness and stealthiness of the proposed   attack method .   Ethical Concerns   In this work , the proposed backdoor attack shows   its ability to escape from existing backdoor de-   fense methods and raises a new security threat to   the NLP community . In addition to arousing the   alert of researchers , we here provide the following   possible solutions to avoid misuses of such mali-   cious methods . Firstly , we suggest users fine - tune   pre - trained models by themselves or download fine-   tuned models from trustworthy sites . Secondly , for   untrustworthy models , we recommend users miti-   gate potential backdoors by further fine - tuning ( Ku-   rita et al . , 2020 ) or fine - pruning ( Liu et al . , 2018 )   the downloaded models on their own dataset .   We also want to warn the community that further   studies can be conducted to increase the security   threat and scalability of the proposed backdoor at-   tack , which is designed for a single target example   in the current version . Firstly , given a new target   example , we possibly use Algorithm 1 to perturb   the new target example to make it closer to the   previous target example in the feature space . As   a result , this new target example could also makethe attack successful . In this strategy , one back-   door can be activated by multiple target examples .   Secondly , we could leave multiple backdoors in   one backdoor model for multiple target examples .   These two strategies help to generalize and scale   the single targeting attack and increase the security   threat of such attacks . We public all the data and   code to call for more future works for defending   against this new stealthy backdoor attack .   Acknowledgement   This work is supported in part by National Natural   Science Foundation of China ( NO . 62037001 ) , the   Key R & D Projects of the Ministry of Science   and Technology ( 2020YFC0832500 ) , the Science   and Technology Innovation 2030 - “ New Gener-   ation Artificial Intelligence ” Major Project ( No .   2021ZD0110201 ) and CAAI - Huawei MindSpore   Open Fund . We would like to thank anonymous   reviewers for their comments and suggestions .   References295029512952