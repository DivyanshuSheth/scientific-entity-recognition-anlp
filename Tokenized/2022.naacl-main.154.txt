  Marzieh S. Tahaei   Huawei Noah ’s Ark LabElla Charlaix   Huawei Noah ’s Ark LabVahid Partovi Nia   Huawei Noah ’s Ark Lab   Ali Ghodsi   Department of Statistics   Actuarial Science , University of WaterlooMehdi Rezagholizadeh   Huawei Noah ’s Ark Lab   Abstract   The development of over - parameterized pre-   trained language models has made a significant   contribution toward the success of natural lan-   guage processing . While over - parameterization   of these models is the key to their generaliza-   tion power , it makes them unsuitable for de-   ployment on low - capacity devices . We push   the limits of state - of - the - art Transformer - based   pre - trained language model compression us-   ing Kronecker decomposition . We present our   KroneckerBERT , a compressed version of the   BERT model obtained by compressing the   embedding layer and the linear mappings in the   multi - head attention , and the feed - forward net-   work modules in the Transformer layers . Our   KroneckerBERT is trained via a very efficient   two - stage knowledge distillation scheme us-   ing far fewer data samples than state - of - the - art   models like MobileBERT and TinyBERT . We   evaluate the performance of KroneckerBERT   on well - known NLP benchmarks . We show   that our KroneckerBERT with compression fac-   tors of 7.7 ×and 21 ×outperforms state - of - the-   art compression methods on the GLUE and   SQuAD benchmarks . In particular , using only   13 % of the teacher model parameters , it retain   more than 99 % of the accuracy on the majority   of GLUE tasks .   1 Introduction   In recent years , the emergence of Pre - trained Lan-   guage Models ( PLMs ) has led to a significant break-   through in Natural Language Processing ( NLP ) .   The introduction of Transformers and unsupervised   pre - training on enormous unlabeled data are the   two main factors that contribute to this success .   Transformer - based models ( Devlin et al . , 2018 ;   Radford et al . , 2019 ; Yang et al . , 2019 ; Shoeybi   et al . , 2019 ) are powerful yet highly over-   parameterized . The enormous size of these modelsdoes not meet the constraints imposed by edge de-   vices on memory , latency , and energy consumption .   Therefore there has been a growing interest in de-   veloping new methodologies and frameworks for   the compression of these large PLMs . Similar to   other deep learning models , the main directions for   the compression of these models include low - bit   quantization ( Gong et al . , 2014 ; Prato et al . , 2019 ) ,   network pruning ( Han et al . , 2015 ) , matrix decom-   position ( Yu et al . , 2017 ; Lioutas et al . , 2020 ) and   Knowledge distillation ( KD ) ( Hinton et al . , 2015 ) .   These methods are either used in isolation or in   combination to improve compression - performance   trade - off .   Recent works have been relatively successful   in compressing Transformer - based PLMs to a cer-   tain degree ( Sanh et al . , 2019 ; Sun et al . , 2019 ;   Jiao et al . , 2019 ; Sun et al . , 2020 ; Xu et al . , 2020 ;   Wang et al . , 2020 ; Kim et al . , 2021 ) ; however , mod-   erate and extreme compression of these models   ( compression factors > 5 and 10 resepctively ) is   still quite challenging . In particular , several works   ( Mao et al . , 2020 ; Zhao et al . , 2019a , 2021 ) that   have tried to go beyond the compression factor of   10 , have done so at the expense of a significant drop   in performance .   Following the classical assumption that matri-   ces often follow a low - rank structure , low - rank de-   composition methods have been used for compres-   sion of weight matrices in deep learning models   ( Yu et al . , 2017 ; Swaminathan et al . , 2020 ; Winata   et al . , 2019 ) and especially Transformer - based mod-   els ( Noach and Goldberg , 2020 ; Mao et al . , 2020 ) .   However , low - rank decomposition methods only   exploit redundancies of the weight matrix in the   horizontal and vertical dimensions and thus limit   the flexibility of the compressed model . Kronecker   decomposition on the other hand exploits redun-2116   dancies in predefined patches and hence allows   for more flexibility in their representation . Recent   works prove Kronecker product to be more effec-   tive in retaining accuracy after compression than   SVD ( Thakker et al . , 2019 ) .   This work proposes a novel framework that   uses Kronecker decomposition for compression   of Transformer - based PLMs and provides a very   promising compression - performance trade - off for   medium and high compression levels , with 13 %   and 5 % of the original model parameters respec-   tively . We use Kronecker decomposition for the   compression of both Transformer layers and the   embedding layer . For Transformer layers , the com-   pression is achieved by representing every weight   matrix both in the multi - head attention ( MHA ) and   the feed - forward neural network ( FFN ) as a Kro-   necker product of two smaller matrices . We also   propose a Kronecker decomposition for compres-   sion of the embedding layer . Previous works have   tried different techniques to reduce the enormous   memory consumption of this layer ( Khrulkov et al . ,   2019 ; Li et al . , 2018 ) . Our Kronecker decomposi-   tion method can substantially reduce the amount of   required memory while maintaining low computa-   tion .   Using Kronecker decomposition for large com-   pression factors leads to a reduction in the model   expressiveness . This is due to the nature of the   Kronecker product and the fact that elements in   this representation are tied together . To address   this issue , we propose to distill knowledge from the   intermediate layers of the original uncompressed   network to the Kronecker network during training .   Training of the state - of - the art BERT compres-   sion models ( Zhao et al . , 2019a , b ; Sun et al . , 2020 ,   2019 ) involve an extensive training which requires   vast computational resources . For example in ( Sun   et al . , 2020 ) , first a specially designed teacher , i.e   IB - BERT is trained from scratch on the en - tire English wikipedia and Book Corpus . The stu-   dent is then pretrained on the same corpus via KD   while undergoing an additional progressive KD   phase . Another example is TinyBERT(Jiao et al . ,   2019 ) which requires pretraining on the entire En-   glish Wikipedia and also uses extensive data aug-   mentation ( 20 × ) for fine - tuning on the downstream   tasks . We show that our Kronecker BERT can out   perform state - of - the - art with significantly less train-   ing requirements . More precisely , our Kronecker-   BERT model undergoes a very light pretraining on   only 10 % of the English Wikipedia for 3 epochs   followed by finetuning on the original downstream   data .   Note that , while our evaluations in this work   are limited to BERT , this proposed compression   method can be directly used to compress other   Transformer - based NLP models . The main con-   tributions of this paper are as follows :   •Compression of the embedding layer using   the Kronecker decomposition with very low   computational overhead .   •Deploying the Kronecker decomposition for   the compression of Transformer modules .   •Efficient training the compressed model via   an intermediate - layer KD that uses only 10 %   of English Wikipedia in the pretraining stage .   •Evaluating the proposed framework for com-   pression of BERT model on well - known   NLP benchmarks   2 Related Work   In this section , we first go through some of the   most related works for BERT compression in the   literature and then review the few works that have   used Kronecker decomposition for compression of   CNNs and RNNs .   2.1 Pre - trained Language Model   Compression   In recent years , many model compression methods   have been proposed to reduce the size of PLMs   while maintaining their performance on different   tasks . KD , which was first introduced by ( Bucilu ˇa   et al . , 2006 ) and then later generalized by ( Hin-   ton et al . , 2015 ) , is a popular compression method   where a small student network is trained to mimic   the behavior of a larger teacher network . Recently ,   using KD for the compression of PLMs has gained   a growing interest in the NLP community . BERT-   PKD ( Sun et al . , 2019 ) , uses KD to transfer knowl-2117edge from the teacher ’s intermediate layers to the   student in the fine - tuning stage . TinyBERT ( Jiao   et al . , 2019 ) uses a two - step distillation method   applied both at the pre - training and at the fine-   tuning stage . MobileBERT ( Sun et al . , 2020 ) also   uses an intermediate - layer knowledge distillation   methodology , but the teacher and the student are de-   signed by incorporating inverted - bottleneck struc-   ture . Authors in ( Zhao et al . , 2019a ) use a mixed-   vocabulary training method to train models with   a smaller vocabulary . They combine this method   with intermediate layer KD through shared projec-   tion matrices . In ( Mao et al . , 2020 ) , the authors   present LadaBERT , a lightweight model compres-   sion pipeline combining SVD - based matrix factor-   ization with weight pruning while using KD for   training to achieve a high compression factor .   2.2 Kronecker Decomposition   Kronecker products have previously been utilized   for the compression of CNNs and small RNNs .   Zhou and Wu 2015 was the first work that uti-   lized Kronecker decomposition for NN compres-   sion . They used a summation of multiple Kro-   necker products to replace weight matrices in the   fully connected and convolution layers in simple   CNN architectures like AlexNet . Thakker et al . ,   2020 used Kronecker product for the compression   of very small language models for deployment on   IoT devices . To reduce the amount of performance   drop after compression , they propose a hybrid ap-   proach where the weight matrix is decomposed   into an upper part and lower part . The upper part   remains un - factorized , and only the lower part is   factorized using the Kronecker product . More re-   cently , Thakker et al . 2020 tried to extend the pre-   vious work to non - IoT applications . Inspired by   robust PCA , they add a sparse matrix to Kronecker   product factorization and propose an algorithm for   learning these two matrices together .   To the best of our knowledge , this work is the   first attempt to compress Transformer - based lan-   guage models using Kronecker decomposition . Un-   like prior arts , we use a simple Kronecker product   of two matrices for the representation of linear lay-   ers and uses KD framework to improve the perfor-   mance .   3 Methodology   In this section , we first introduce the background   of Kronecker decomposition and then explain our   compression method in detail .   3.1 Kronecker Product   Kronecker product is an operation that is applied   on two matrices resulting in a block matrix . Let   Abe a matrix ∈I R , and let Bbe a matrix   ∈I R , then the Kronecker product of Aand   Bdenoted by ⊗is a block matrix , where each block   ( i , j)is obtained by multiplying the element A   by matrix B. Therefore , the resulting matrix A⊗B   is∈I Rwhere m = mmandn = nn .   Figure 1 illustrates the Kronecker product between   two small matrices . See ( Graham , 2018 ) for more   detailed information on Kronecker products . Re-   placing matrix product with Kronecker product re-   places the projection of the original linear space   by a more constrained linear space in in which the   projection angle is defined by the core tensors , see   Figure 5 in the appendix .   3.2 Kronecker Decomposition   Given a shape for AandB , i.e.(m , n , m , n ) ,   any matrix W∈I R , can be approximated   as a summation of Kronecker product of matrices   A∈I RandB∈I R :   W ≈/summationdisplayA⊗ B ( 1 )   we can obtain exact representation of Wby setting   the number of Kronecker summations Iequal to   min(mn , mn ) . However , in order to achieve2118compression , a much smaller value of Iis often   used . In fact prior arts show promising results   using a single Kronecker product ( Thakker et al . ,   2019 , 2020 ) . When decomposing a matrix W∈   I R , asA⊗B , there are different choices for the   shapes of AandB. The dimensions of Ai.em   andncan be any factor of mandnrespectively ,   the dimensions of Bwill subsequently be equal to   m = m / mandn = n / n.   3.2.1 The Nearest Kronecker Product   The nearest Kronecker problem is defined as find-   ing matrices AandBthat their Kronecker product   best approximate a given W(for a given shape of   AandB ):   min∥W−A⊗B∥. ( 2 )   ( Van Loan and Pitsianis , 1993 ) show that this   problem can be solved using rank-1 SVD approxi-   mation of rearranged W :   min / vextenddouble / vextenddouble / vextenddoubleR(W)− V(A)V(B)/vextenddouble / vextenddouble / vextenddouble . ( 3 )   Here , Vis an operation that transforms a matrix to   a vector ( vectorizes ) by stacking its columns and   Ris a rearrangement operation that extracts   patches of size m×n , vectorizes the resulting   patches and finally concatenates them together to   form a matrix of size mn×mn . The rear-   rangement operation turns the Kronecker product   into a matrix of rank one while retaining the Frobe-   nius norm making the minimizations in Eq.3 and   Eq.2 equivalant . Hence , the rank - one SVD solu-   tionU(:,1)σV(:,1)can be used to obtain the   optimum AandBas :   A = V / parenleftbig√σU(:,1)/parenrightbig   ( 4 )   B = V / parenleftbig√σV(:,1)/parenrightbig   ( 5 )   Here , V(x)is an operation that transforms a   vector xto a matrix of size m×nby dividing   the vector to columns of size mand concatenating   the resulting columns together . Similarly , rank- r   SVD decomposition can be used to approximate   summation of Kronecker products . We use this   method for the initialization of Kronecker layers   from the non - compressed model .   3.2.2 Relation to SVD   By choosing n= 1 andm= 1,Abecomes a   column vector of size ∈I RandBbecomes arow vector of size I R , then the Kronecker de-   composition becomes equivalent to rank-1 SVD   decomposition . Therefore rank-1 SVD is a spe-   cial case of Kronecker product decomposition and   rank - rSVD is a special case of Kronecker product   summation decomposition . This indicates that with   Kronecker product one can achieve more flexibility   than low rank decomposition .   3.2.3 Memory and Computation Reduction   When representing WasA⊗B , the number of   elements is reduced from mntomn+mn .   Moreover , using the Kronecker product to repre-   sent linear layers can reduce the required compu-   tation . In fact , a linear projection of any vector x   can be performed efficiently without explicit recon-   struction of A⊗Busing the following popular   property of Kronecker product :   ( A⊗B)X = V(BV(X)A ) ( 6 )   where AisAtranspose . The consequence of per-   forming multiplication in this way is that it reduces   the number of FLOPs from ( 2mm−1)nnto :   min / parenleftbig   ( 2n−1)mn+ ( 2n−1)mm ,   ( 2n−1)nm+ ( 2n−1)mm / parenrightbig   ( 7 )   3.3 Kronecker Embedding Layer   The embedding layer in large language models is a   very large lookup table X∈I R , where vis the   size of the dictionary and dis the embedding di-   mension . In order to compress Xusing Kronecker   decomposition , the first step is to define the shape   of Kronecker factors AandB. We define A   to be a matrix of size v×andBto be a row   vector of size n. There are two reasons for defin-   ingBas a row vector . 1 ) it allows disentangled   embedding of each word since every word has a   unique row in A. 2 ) the embedding of each word   can be obtained efficiently in O(d ) . More precisely ,   the embedding for the i’th word in the dictionary   can be obtained by the Kronecker product between   AandB :   X = A⊗B(8 )   where Ais stored as a lookup table . Note that   sinceAis of size 1×andBis of size 1×n , the   computation complexity of this operation is O(d ) .   Figure 2 shows an illustration of the Kronecker   embedding layer.2119   3.4 Kronecker Transformer   The Transformer layer is composed of two main   components : MHA and FFN . We use Kronecker de-   composition to compress both . In the Transformer   block , the self - attention mechanism is done by pro-   jecting the input into the Key , Query , and Value   embeddings and obtaining the attention matrices   through the following :   O=(9 )   Attention ( Q , K , V ) = softmax ( O)V   where Q , K , andVare obtained by multiplying   the input by W , W , Wrespectively . In a   MHA module , there is a separate W , W , and   Wmatrix per attention head to allow for a richer   representation of the data . In the implementation   usually , matrices from all heads are stacked to-   gether resulting in 3 matrices W , WandW.   Instead of decomposing the matrices of each head   separately , we use Kronecker decomposition after   concatenation :   W = A⊗B(10 )   W = A⊗B   W = A⊗B   By choosing mto be smaller than the output di-   mension of each attention head , matrix Bin the   Kronecker decomposition is shared among all at-   tention heads resulting in more compression . The   result of applying Eq.9 is then fed to a linear map-   ping ( W ) to produce the MHA output . We useKronecker decomposition for compressing this lin-   ear mapping as well the two weight matrices in the   subsequent FFN block :   W = A⊗B(11 )   W = A⊗B ( 12 )   W = A⊗B ( 13 )   3.5 Knowledge Distillation   In the following section , we describe how KD is   used to improve the training of the KroneckerBERT   model .   3.5.1 Intermediate KD   LetSbe the student , and Tbe the teacher , then   for a batch of data ( X , y ) , we define f(X )   andf(X)as the output of the llayer for the   student network and the teacher network respec-   tively . The teacher here is the BERT and the   student is its corresponding KroneckerBERT that is   obtained by replacing the embedding layer and the   linear mappings in MHA and FFN modules with   Kronecker factors(see Sections 3.3 and 3.4 for de-   tails ) . Note that like other decomposition methods ,   when we use Kronecker factorization to compress   the model , the number of layers and the dimen-   sions of the input and output of each layer remain   intact . Therefore , when performing intermediate   layer KD , we can directly obtain the difference in   the output of a specific layer in the teacher and   student networks without the need for projection .   In the proposed framework , the intermediate KD   from the teacher to student occurs at the embedding2120   layer output , attention matrices and FFN outputs :   L ( X ) = MSE / parenleftbig   E , E / parenrightbig   L ( X ) = /summationdisplayMSE / parenleftbig   O , O / parenrightbig   L(X ) = /summationdisplayMSE / parenleftbig   H , H / parenrightbig   where EandEare the output of the embedding   layer from the student and the teacher respectively .   OandOare the attention matrices ( Eq.9 ) , H   andHare the outputs of the FFN , of layer lin   the student and the teacher respectively .   Our final loss is as follows :   L(x , y ) = /summationdisplayL ( x ) + ( 14 )   L ( x ) + L(x ) +   L(x ) + L ( x , y ) ,   where L ( x)is the supervised loss of the stu-   dent , e.g. the cross entropy loss when fine - tuning   for sequence classification tasks .   3.5.2 KD at pre - training   Inspired by prior works we use KD at the pre-   training stage to capture the general domain knowl-   edge from the teacher . For the pre - training distil-   lation , the pretrained BERT model is used as   the teacher . Intermediate layer KD is then used to   train the KroneckeBERT network in the general do-   main . KD at pre - training improves the initialization   of the Kronecker model for the task - specific KD   stage . The loss at the pre - training stage involves   the intermediate KD loss as in Eq . 14 as well as   the masked language modeling and next sentence   prediction . Unlike other methods , we perform pre-   training distillation only on a small portion of the   dataset ( 10 % of the English Wikipedia ) for a few   epochs ( 3 epochs ) which makes our training far   more efficient . See Table 10 in the Appendix fora comparison of training requirements by various   methods .   3.6 Model Settings   The first step of the proposed framework is to de-   sign the Kronecker layers by defining the shape of   AandB. Once the shape of one of them is set ,   the shape of the other one can be obtained accord-   ingly . Therefore we only searched among different   choices for mandnwhich are limited to the   factors of the original weight matrix ( mandnre-   spectively ) . We used the same configuration for   all the matrices in the MHA . Also For the FFN ,   we chose the configuration for one layer , and for   the other layer , the dimensions are swapped . For   the embedding layer , since Bis a row vector , we   only need to choose n. The shapes of the Kro-   necker factors were chosen to obtain the desired   compression factor and FLOPS reduction accord-   ing to Eq.7 . To investigate the effect of summation   we also selected one configuration with summation   of 4 Kronecker products . Similarly , after fixing the   number of summation we chose the configuration   that provided the desired compression and latency   reduction . Table 1 summarises the configuration of   Kronecker factorization for the three compression   factors used in this work .   3.7 Implementation details   For KD at the pre - training stage , the Kronecker-   BERT model was initialized using the teacher ( pre-   trained BERT model ) . This means that for   layers that were not compressed like the last layer ,   the values are copied from the teacher to the student .   For initialization of the compressed layers in the   pre - training stage , the nearest Kronecker solution   explained in section 3.2.1 is used to approximate   Kronecker factors ( AandB ) from the pre - trained   BERT model . In the pre - training stage , 10 %   of the English Wikipedia was used for 3 epochs.2121   The batch size in pre - training was set to 64 and   the learning rate was set to e-3 . After pre - training ,   the obtained Kronecker model is used to initial-   ize the Kronecker layers in the student model for   task - specific fine - tuning . The Prediction layer is   initialized from the fine - tuned BERT teacher .   For fine - tuning on each task , we optimize the hyper-   parameters based on the performance of the model   on the dev set . See appendix for more details on   the results and the selected hyperparameters .   4 Experiments   In this section , we compare our KroneckerBERT   with the sate - of - the - art compression methods ap-   plied to BERT on GLUE and SQuAD . We also   perform an ablation study to investigate the effect   of pretraining and KD .   4.1 Baselines   As for baselines we select two main categories of   compression methods , those with compression fac-   tor < 10 and those with compression factor > 10 .   In the first category , we have BERT ( Sun   et al . , 2019 ) with a low compression factor , and   models with similar compression factor as our   KroneckerBERT : MobileBERT ( Sun et al . , 2020 )   and TinyBERT ( Jiao et al . , 2019 ) . We also com-   pare our results to the dynaBERT model ( Hou et al . ,   2020 ) . For the second category , we compare our   results with SharedProject ( Zhao et al . , 2019a ) and   LadaBERT ( Mao et al . , 2020 ) with compression   factors in the rage of 10 - 20x.4.2 Results on the GLUE Benchmark   We evaluated the proposed framework on the Gen-   eral Language Understanding Evaluation ( GLUE )   ( Wang et al . , 2018 ) benchmark which consists   of 9 natural language understanding tasks . We   submitted the predictions of our proposed mod-   els on the test data sets for different tasks   to the official GLUE benchmark ( https://   gluebenchmark.com/ ) . Table 2 summarizes   the results on GLUE test set for compression fac-   tors less than 10 . We can see that KroneckerBERT   outperforms other baselines in the majority of tasks   as well as on average . Moreover , the average per-   formance of KroneckerBERTexcluding CoLA   is 82.4 which is only 0.5 % less than that of the   teacher .   Table 3 shows the results for extreme compres-   sion on the GLUE test set . As indicated in the   table , the baselines for the higher compression fac-   tors only provided results on a limited set of GLUE   tasks . We can see that for higher compression fac-   tors , KroneckerBERToutperforms the baselines   on all available results .   In table 4 we compare the performance of Kro-   neckerBERT with the dynaBERT model ( Hou et al . ,   2020 ) . We compare the results on dev set since the   results on test set were not provided in their pa-   per . We see that KroneckerBERT can outperform   dynaBERT with fewer number of parameters on all   GLUE tasks.2122   4.3 Results on SQuAD   In this section , we evaluate the performance of the   proposed model on SQuAD datasets . SQuAD1.1   ( Rajpurkar et al . , 2016 ) is a large - scale reading   comprehension which contains questions that have   answers in given context . SQuAD2.0 ( Kudo and   Richardson , 2018 ) also contains unanswerable   questions . Table 5 summarises the performance   on dev set . For both SQuAD1.1 and SQuAD2.0 ,   KroneckerBERTwith fewer number of parame-   ters can significantly outperform both TinyBERT   and BERT - PKD baselines . We have also listed   the performance of KroneckerBERT . The results   of baselines with higher compression factors on   SQuAD were not available .   4.4 Ablation Study   In this section , we investigate the effect of pre-   training and KD in reducing the gap between the   original BERT model and the compressed Kro-   neckerBERT . Table 6 summarises the results for   KroneckerBERT . Our proposed method uses KD   in both the pre - training and the fine - tuning stages .   For this ablation study , pre - training is only per-   formed via KD with the pre - trained BERT as   the teacher . We perform experiments on 3 tasks   from the GLUE benchmark with different sizes of   training data , namely MNLI - m , SST-2 , and MRPC .   For all tasks , the highest performance is obtained   when the two - stage KD is used ( first row ) . Note   that our light pretraining plays an important row   in improving the performance as shown in the first   and the second row ( with and without pretraining   respectively ) . As the size of the task dataset de-   creases the effect of pretraining becomes more sig-   nificant . Also , removing KD from the fine - tuning   stage ( task - agnostic compression ) leads to an accu-   racy drop on all task . However , the drop is not as   pronounced as removing the pretraining stage . It   seems that KD in the fine - tuning stage has a larger   impact on tasks with larger datasets .   We also used t - SNE to visualize the output of   the FFN of the middle layer ( layer 6 ) of the fine-   tuned KroneckerBERTwith and without KD in   comparison with the fine - tuned teacher , on SST-   2 dev . Figure 4 shows the results . See how KD   helps the features of the middle layer to be more   separable with respect to the task compared to the   no KD case .   5 Conclusion   We introduced a novel method for compressing   Transformer - based language models that uses Kro-   necker decomposition for the compression of the   embedding layer and the linear mappings within   the Transformer blocks . The proposed framework2123was used to compress the BERT model . We   used a very light two - stage KD method to train the   compressed model . We show that the proposed   framework can significantly reduce the size and the   number of computations while outperforming state-   of - the - art . The proposed method can be directly ap-   plied for compression of other Transformer - based   language models . The combination of the proposed   method with other compression techniques such   layer truncation , pruning and quantization can be   an interesting direction for future work .   Acknowledgements   Authors would like to thank Mahdi Zolnouri , Seyed   Alireza Ghaffari and Eyyüb Sari for informative   discussions throughout this project . We also would   like to thank Aref Jaffari for preparing the out of   domain datasets .   References21242125   A Appendix   A.1 Training Details   In this section , we include more details of our exper-   imental settings presented in Section 4 of the paper .   For optimization , we used BERTadam and searched   learning rate in range { 5e−5 , e−4,5e−4 , } . For   pretraining the learning rate was set to 1e-13 and   the number of epochs was set to 3 . The batch size   in all experiments were set to 32 . For the GLUE   benchmark , We searched epochs in range { 5 - 15 }   for all tasks except CoLA . For CoLA we searched   epochs in range { 15 - 30 } . This is because similar   to other studies ( Mosbach et al . , 2020 ; Zhang et al . ,   2020 ) we noticed that running CoLA for more   epochs is necessary to reduce its sensitivity to ran-   dom seed . The sequence length at the pre - training   stage is set to 512 and at the fine - tuning stage is   set to 128 for GLUE benchmark . For SQuAD1.1   and SQuAD2 the sequence length is set 384 and   the batch size was set to 64 and the epochs were   varied in the range { 1 - 14 } . Also , the learning rate   was set to e-4 and   Table 7 shows the result of the best - performing   models on dev set for KroneckerBERT . Tables 8   shows the learning rate for the best - performing   models . The training was performed on V100   GPU and the average latency for training of   KroneckerBERTfor a batch size of 64 was 32ms .   All the values are the results of single runs .   A.2 Out of domain robustness   It is shown that pre - trained Transformer - based lan-   guage models are robust to out - of - domain ( OOD )   samples ( Hendrycks et al . , 2020 ) . In this sec-   tion , we investigate how the proposed compression   A⊗B   method affects the OOD robustness of BERT by   evaluating the fined - tuned models on MRPC and   SST-2 on PAWS ( Zhang et al . , 2019 ) and IMDb   ( Maas et al . , 2011 ) respectively . We compare OOD   robustness with the teacher , BERT and Tiny-   BERT . TinyBERT fine - tuned checkpoints are ob-   tained from their repository . Table 9 lists the results .   KroencekrBERToutperforms TinyBERT on two   of the three OOD experiments . We can see the   fine - tuned KroneckerBERTmodels on MRPC is   robust to OOD since there is a small increase in   performance compared to BERT . On IMDb   our KroenckerBERThas a small drop in accuracy   ( 1.5 % compared to 9.5 % for TinyBert ) after com-   pression .   A.3 Training efficiency   Table 10 shows the training requirements of differ-   ent compression methods in terms of their training   data . Some models require pretraining a designed   teacher from scratch before pretraining the student .   KroneckerBERT however only pretrain on 10 % of   Wikipedia for 3 epochs . For fine - tuning in contrast   to TinyBERT our KroneckerBERT model is trained   for on the original data . The number of fine - tuning   epochs for the majority of the GLUE tasks is less   than 15 .   A.4 Geometrical interpretation of Kronecker   product projection   Figure 5 shows a geometrical interpretation of Kro-   necker product projection versus the original lin-   ear projection . It shows how Kronecker product   constraints the space of possible projections . The   flexibility of this space is a function of the shape of   the core matrices AandB.2126   A.5 Datasets   We evaluate the proposed framework on the Gen-   eral Language Understanding Evaluation ( GLUE )   ( Wang et al . , 2018 ) benchmark ( https://   gluebenchmark.com/ ) . This benchmark con-   sist of the following tasks in English language :   Stanford Sentiment Treebank ( SST-2)(Socher et al . ,   2013 ) and CoLA ( Warstadt et al . , 2019 ) for Senti-   ment Classification . , on Microsoft Research Para-   phrase Corpus ( MRPC ) ( Dolan and Brockett , 2005 )   Quora Question Pairs ( QQP ) ( Chen et al . , 2018 ) for   Paraphrase Similarity Matching , Multi - Genre Nat-   ural Language Inference ( MNLI ) ( Williams et al . ,   2017 ) , and Recognizing Textual Entailment ( RTE )   ( Bentivogli et al . , 2009 ) for Natural Language in-   ference .   We also evaluate the performance of the model   on SQuAD datasets ( https://rajpurkar .   github.io/SQuAD-explore ) . The datasets   are distributed under the CC BY - SA 4.0 license .   SQuAD1.1 ( Rajpurkar et al . , 2016 ) is a large - scale   English reading comprehension that contains 87 K   question that have answers in the training set(10k   in the dev set ) . SQuAD2.0 ( Rajpurkar et al . , 2018 )   combines the questions in SQuAD1.1 with over   50,000 unanswerable questions ( 130k samples in   the training and 11k in the dev set).2127