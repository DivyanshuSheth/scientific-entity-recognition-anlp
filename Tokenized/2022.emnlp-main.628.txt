  Chenhao Wang , Jiachun Li , Yubo Chen , Kang Liuand Jun ZhaoNational Laboratory of Pattern Recognition , Institute of Automation , CAS , Beijing , ChinaSchool of Artificial Intelligence , University of Chinese Academy of Sciences , Beijing , ChinaBeijing Academy of Artificial Intelligence , Beijing , China   { chenhao.wang,jiachun.li,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn   Abstract   Commonsense knowledge graphs ( CKGs ) are   increasingly applied in various natural lan-   guage processing tasks . However , most exist-   ing CKGs are limited to English , which hin-   ders related research in non - English languages .   Meanwhile , directly generating commonsense   knowledge from pretrained language models   has recently received attention , yet it has not   been explored in non - English languages . In this   paper , we propose a large - scale Chinese CKG   generated from multilingual PLMs , named as   CN - AutoMIC , aiming to fill the research gap   of non - English CKGs . To improve the effi-   ciency , we propose generate - by - category strat-   egy to reduce invalid generation . To ensure the   filtering quality , we develop cascaded filters to   discard low - quality results . To further increase   the diversity and density , we introduce a boot-   strapping iteration process to reuse generated   results . Finally , we conduct detailed analyses   on CN - AutoMIC from different aspects . Em-   pirical results show the proposed CKG has high   quality and diversity , surpassing the direct trans-   lation version of similar English CKGs . We   also find some interesting deficiency patterns   and differences between relations , which reveal   pending problems in commonsense knowledge   generation . We share the resources and related   models for further study .   1 Introduction   Compiling large - scale commonsense knowledge   resources is a long - term goal of the AI commu-   nity . Recent efforts focus on constructing com-   monsense knowledge graphs ( CKGs ) via manually   compiling ( Speer et al . , 2017 ; Sap et al . , 2019 ;   Mostafazadeh et al . , 2020 ) or automatic extrac-   tion ( Tandon et al . , 2014 ; Romero et al . , 2019 ;   Zhang et al . , 2020 ; Nguyen et al . , 2021 ) . These   projects have shown benefits for a wide range of   downstream tasks ( Lin et al . , 2019 ; Tian et al . ,   2020 ; Ammanabrolu et al . , 2021).However , most CKG projects are limited to En-   glish , which hinders further research in other lan-   guages . To go beyond such an English - centric   trend in commonsense knowledge research , there   are some challenging issues . First , directly   translating English CKGs is not enough . For   example , is a triple from a   CKG crowdsourced by English users ( Sap et al . ,   2019 ) , but it is not common in non - Christian cul-   tures . In fact , such resources reflect only the com-   monsense perspectives of English - speaking com-   munities . The translation will omit the cultural   differences in other languages , and even implicitly   exacerbate the English - centric bias ( Mehrabi et al . ,   2021 ) . Therefore , when creating CKGs for new   languages , it is better to rely on native speakers and   corpora . Second , current common practices in En-   glish CKG construction , i.e. manually compiling   and automatic extraction , are difficult to generalize .   For manually compiling , creating human - authored   CKGs for each new language is very expensive .   For automatic extraction , current pipelines rely on   English - specific hand - craft patterns or language   processing tools , which are not available to other   languages . Therefore , when creating CKGs for new   languages , the cost and availability of construction   scheme should also be concerned .   Recent work reveals pretrained language models   ( PLMs ) can be a new source to generate common-   sense knowledge ( Bosselut et al . , 2019 ; Nguyen   and Razniewski , 2022 ) . The Latest research indi-   cates the CKG built from huge PLMs ( e.g. GPT-3 )   can surpass the crowdsourced ones in quantity and   quality ( West et al . , 2021 ) , and only a small amount   of human - authored data are required for prompt-   ing and filtering . Interestingly , we find this way   could be the ideal start point to construct CKGs   for new languages , since PLMs can be trained on   the corpora of target languages , and the generation   process is low - cost and independent of language-9253specific tools . However , up to now , work in this   thread has not extended to non - English languages .   The main challenge of this paradigm is that the gen-   eration quality and diversity are often conflicting   and difficult to control . To sample diverse results ,   the generation should be run extensive times , and a   large number of generated results are invalid and   need to be filtered out . For new languages , as there   is often no comparable PLM to GPT-3 ( Brown   et al . , 2020 ) in English , the generated results will   be even noisier . Therefore , we need to reduce un-   necessary generation to increase the efficiency and   take measures to ensure the quality .   In this paper , we propose a framework to distill   commonsense knowledge from multilingual pre-   trained language models . To increase the genera-   tion efficiency , we propose a generate - by - category   strategy to reduce invalid generation . To ensure   the filtering quality , we propose cascaded filters   to discard low - quality results . To further increase   the diversity and density , we introduce a bootstrap-   ping process . Based on the framework , we pro-   pose a large - scale Chinese commonsense knowl-   edge graph , CN - AutoMIC ( Auto matically Ob-   tained MachIneCommonsense ) . To the best of   our knowledge , it is the first non - English CKG   built from pretrained language models . The em-   pirical results show the proposed CKG has high   quality and diversity . We also discuss some inter-   esting deficiencies that need further solutions . We   summarize the contribution as follows .   •We propose a framework to distill common-   sense knowledge from multilingual PLMs ,   incorporating several novel strategies to im-   prove the generation efficiency and quality .   •We propose the first large - scale Chinese com-   monsense knowledge graph built with large-   sized PLMs , CN - AutoMIC . Its high - quality   subset contains 1.1 M triples with an accuracy   of87.2 % . Besides the CKG , we also release   small - sized commonsense models trained on   it , named as CN - COMET .   •We conduct comprehensive evaluation and   analysis for CN - AutoMIC and CN - COMET .   Besides verifying the quality and diversity , we   also get some valuable observations about the   deficiencies of generation . Considering gen-   erating commonsense knowledge with PLMs   is still not fully explored , our findings can   provide more insights into this topic .   2 Related Work   2.1 Commonsense Knowledge Graphs   After some pioneers of strict logic formaliza-   tion ( Lenat et al . , 1990 ) , most recent commonsense   knowledge resources , also known as commonsense   knowledge graphs , represent commonsense knowl-   edge in loosely - structured ( head , relation , tail )   triples , where the head andtailcan be concepts or   situations described in free - form phrases , and the   relation can be various commonsense dimensions .   Some of such resources are constructed by manu-   ally compiling or crowdsourcing ( Speer et al . , 2017 ;   Sap et al . , 2019 ; Hwang et al . , 2021 ; Mostafazadeh   et al . , 2020 ) . The others are mainly mined from   corpora with human - crafted patterns or language   processing tools ( Tandon et al . , 2014 ; Romero   et al . , 2019 ; Zhang et al . , 2020 ; Fang et al . , 2021 ;   Nguyen et al . , 2021 ) .   Unfortunately , most of these projects are limited   in English . Among the mainstream CKGs , Con-   ceptNet ( Speer et al . , 2017 ) is the only multilingual   one . It supports 10 core languages and more com-   mon languages . However , most of its non - English   parts are taxonomic or lexical knowledge . The rest   parts are limited in quantity and coverage . A re-   markable recent work of Chinese commonsense   knowledge resources is CKG ( Li et al . , 2022 ) . It   is based on the translation of ATOMIC ( Sap et al . ,   2019 ; Hwang et al . , 2021 ) , which may fail to cap-   ture the cultural differences . Therefore , our work   can be a strong complement to them.92542.2 Extracting Knowledge from PLMs   Since PLMs have seen a great number of doc-   uments and latently learned associations among   concepts , there are extensive efforts to probe or   extract relational knowledge from PLMs ( Petroni   et al . , 2019 ; Sung et al . , 2021 ; AlKhamissi et al . ,   2022 ) . As for commonsense knowledge , some ear-   lier work has tried to automatically complete CKGs   by fine - tuning PLMs ( Bosselut et al . , 2019 ; Guo   et al . , 2020 ; Hwang et al . , 2021 ) , which still needs   a large number of existing triples as training data .   Recent research demonstrates that through natural   language prompting , PLMs can adapt to generate   commonsense knowledge under the few - shot set-   ting ( Da et al . , 2021 ) , or directly act as triple scorers   without training ( Davison et al . , 2019 ) . A signifi-   ca nt progress is made by West et al . ( 2021 ) . They   use GPT-3 to generate a CKG from scratch . During   the construction , only a small amount of human-   authored data are required for in - context prompting   generation and filtering results . They show the re-   sulting CKG surpasses human - authored ATOMIC   in quantity , quality , and diversity . Compared with   West et al . ( 2021 ) , our work proposes more im-   provement in generation and filtering and brings   more comprehensive analysis for the paradigm   from a non - English perspective .   3 Construction of CN - AutoMIC   For clarity , in this section , we first show the   overview of the construction task , then describe   the construction process of CN - AutoMIC in detail .   3.1 Overview   We expect to obtain commonsense knowledge rep-   resented in ( head , relation , tail ) triples via prompt-   ing generation . The demonstration of construc-   tion is shown in Figure 1 . Similar to the construc-   tion workflow of crowdsourced CKGs , we hope   to first collect head items ( Section 3.2 ) and then   collect tailitems according to several pre - defined   relations ( Section 3.3 ) . We limit the head items   to the description of eventualities ( events , activi-   ties and states ) ( Bach , 1986 ) , such as “ 某人Ｘ离   开家 ” . Following West   et al . ( 2021 ) , we consider seven relation types about   eventualities from ATOMIC , which are listed in Ta-   ble 2 . Since the raw generated results are mixed   with noise and degeneration , we introduce human   supervision to train filter models to distinguish   high - quality results ( Section 3.4 ) . To reuse the gen - erated tails and increase the density , we propose a   bootstrapping iteration process ( Section 3.5 ) .   3.2 Generating Head Items   We start from a minor size of head item seeds , using   them as examples to prompt PLMs to generate   more head items in the same format .   Notably , although previous work ( West et al . ,   2021 ) treats all head items without distinction and   collects knowledge about them for all predefined   relations , we argue that it is necessary to further   subdivide head items into different categories , be-   cause some heads and relations are in conflict and   they can not produce valid results . For example , we   can not infer the intent of X ( xIntent ) in “ 某人   Ｘ受到攻击 ” , because   he is passively involved in it rather than intention-   ally causes it . Therefore , we divide the head items   in three categories ( voluntary occurrences , involun-   tary occurrences andstates ) and match them with   different relations , as illustrated in Table 1 . Note   that these categories are not strict , but they can   hopefully reduce invalid generation .   Then , we collect head item seeds for the three   categories . We mainly sample the high - quality   head items from ATOMIC , manually categoriz-   ing and translating them . We intentionally discard   the instances that are English - specific or rare in   Chinese context , such as “ ” . In total , we collect 100 , 50 , and 45   seeds for voluntary occurrences , involuntary oc-   currences , and states , respectively .   Through pilot studies , we empirically choose   mT5 - XXL ( 13B parameters ) ( Xue et al . , 2021 ) for   generation . It is one of the biggest publicly avail-   able multilingual PLMs , covering 101 languages ,   but still 13x smaller than GPT-3 used in West et al .   ( 2021 ) . During the generation , we use the prompt   shown in Figure 2 . For each generation cycle , we   sample 10 examples from the seeds to construct the   prompt , and use nucleus sampling with p= 0.9to   decode 100 results . The generation cycles for dif-   ferent head categories are independently conducted .   After generating 600 K raw results ( 2,000 cycles   for each category ) , we discard 30 % results of high   negative log - likelihood and merge the duplicates in   the remaining part , resulting in 100 K head items.92553.3 Generating Triples   To obtain complete triples , we further generate tail   items according to different relations . Similar to   Section 3.2 , we use example triples for prompting   generation . To make effective use of the capabili-   ties of PLMs , we need to convert triples into natural   language sentences . For this sake , we use the tem-   plates in Table 2 to verbalize triples , and further   replace “ 某人Ｘ ” placeholders with   random Chinese names .   We continue to use mT5 - XXL model for gen-   eration . With the verbalized example triples , we   construct the prompt as shown in Figure 3 . For   each relation , we use 8 example triples , which are   sampled from ATOMIC and manually translated   into Chinese . The prompt is used to generate 10   tail items for each ( head , relation ) pair with nucleus   sampling ( p= 0.7 ) . As said before , each head item   is only paired with the valid relations according to   its category , so that the invalid generation results   are reduced . After converting names back to place-   holders and removing duplicated triples , this step   produces 5.2 M raw triples in total .   3.4 Filtering Results   Annotation To train filters that can distinguish   high - quality triples , we randomly sample 4000 in-   stances from the raw generated triples and ask three   native Chinese speakers to annotate them . We in-   tentionally give three questions for each triple . The   annotators should first rate the head and tail alone   to indicate whether they are acceptable . If not ,   the options of rejecting include syntax errors , ab-   normal or impossible situations ( e.g. “ 某人X在   天上游泳 ” ) and   other faults ( e.g. containing real names rather   than name placeholders ) . Then , if the annota-9256Heads Tails Triples   Acceptance Rate 85.2 % 94.4 % 47.6 %   tors have accepted the head and tail , they should   rate the triple as a whole , with options for accept-   ability : “ always / often ” , “ sometimes / likely ” , “ far-   fetched / never ” , or “ invalid ” . The former two are   considered “ accepted ” . The latter two are “ re-   jected ” . Table 3 shows the acceptance rate . For the   overall results , we find the fleiss ’s κ(Fleiss , 1971 )   is 0.439 , which indicates moderate agreement .   Training We use 80 % of the annotated data for   training , and the remaining parts for validating   and testing . We train binary classifier models   to predict whether the input is acceptable . Con-   sidering the acceptance differences in the anno-   tation results(Table 3 ) , we train single - use classi-   fiers for heads , tails , and triples , respectively . We   empirically choose a Chinese versionof ELEC-   TRA ( Clark et al . , 2020 ) as the underlying model   and fine - tune it for the tasks . We set learning rate to   5e-5 and batch size to 128 by grid search , and use   early stopping to maximize the average precision   ( AP ) on the validation data . We also tried other   multilingual or Chinese PLMs of similar size , but   the ELECTRA - based models obtain the best AP .   Cascaded Filtering We report the precision-   recall curves of the trained models in Figure 4 ,   including all three classifiers . We also report the   performance of triple classifier on “ clean data ” ,   where the head and tail in the triple are acceptable   ( Triple * ) . From the curves , we can find head and   tail classifiers can reach very high precision at al-   most all recall value . And triple classifier perform   better on “ clean data ” . It indicates that we can   achieve better performance by cascaded filtering ,   i.e. , applying the head and tail classifier first , and   then using the triple classifier . We also find it is   useful to set different thresholds for different rela-   tion types . Finally , we set the thresholds for head   and tail classifier to ensure precision > 0.98on   the test data . We empirically search three groups   of thresholds for the triple classifier , based on   precision = 0.9,0.8,0.75on each relation . We   use these thresholds to get three subsets with differ-   ent sizes and denote them as high / mid / low subsets .   3.5 Bootstrapping Iteration   We note that although the generated tail items have   different formats from the head items , many of   them can be converted into head items with simple   templates , as shown in Table 4 . After the above   steps , many of the tail items have never appeared   as head items . To further increase the diversity and   density , we use the high - frequency tail items from   themid subset to generate more triples . We re-   peat the generating and filtering process described   in Section 3.3 and Section 3.4 , using the trained   filters . Such bootstrapping iteration can be done   several times , though we only conduct it once in   this work . Finally , the resulting triples from differ-   ent iterations are merged . We denote the merged   sets as CN - AutoMIC respectively .   4 Evaluation and Analysis   In this section , we evaluate and analyze the re-   sources in three parts . First , we comprehensively   evaluate CN - AutoMIC in size , quality and diver-   sity . In this step , we also evaluate the common-   sense model ( CN - COMET ) trained on it . Second ,   we conduct specific analyses for different construc-9257   tion steps . Third , we inspect the cases of culture-   specific commonsense knowledge and generation   deficiencies .   4.1 Evaluating the Graph   Setup We count the size of triples , the unique   heads , and the unique tails in the graph to inves-   tigate the quantity and diversity . For comparison ,   we refer to two English CKGs , including human-   authored ATOMIC(Hwang et al . , 2021 ) and au-   tomatically generated ATOMIC10x(West et al . ,   2021 ) . We also refer to a Chinese CKG ( ATOMIC-   zh ) ( Li et al . , 2022 ) which is automatically trans-   lated from ATOMIC . Then , to test the quality ,   we sample 1000 triples from ATOMIC - zh and CN-   AutoMIC and conduct a human evaluation . The   annotation setting is similar to Section 3.4 , but only   the triples need to be rated . We keep the annota-   tion results by majority vote and report the average   acceptance for the triples .   Overall Statistics The overall statistics of CN-   AutoMIC are shown in Table 5 . From the results ,   we find : ( 1)Compared with the existing translated   CKG , CN - AutoMIC contains a larger size of triples   with better quality , as well as more unique head   items . Interestingly , even the raw generated triples   have better average acceptance than the translated   CKG . We speculate that is because the translated   CKG has a lot of syntax and translation errors . ( 2 )   After filtering , the human acceptance reaches up to   87.2 from 47.6 , indicating the effect of the filtering   process . As a trade - off , the diversity of tail items   is decreased . ( 3)The English ATOMIC10xis gen-   erated by GPT-3 and has better basic quality . After   filtering , it can reach very high acceptance , surpass-   ing human - authored ATOMICby 10 percent . By   contrast , CN - AutoMIC struggles on reaching high   acceptance , which shows the difficulty of generat-   ing non - English commonsense knowledge .   Relation - level Results We show the relation-   level results of CN - AutoMICin Table 6 . We   can find the acceptance on most of the relations is   between 80 to 90 percent . However , the number of   triples varies significantly . That is because we use   different filter thresholds for different relations to   achieve the same quality level . To some extent , the   change of triple amounts reflects how much com-   monsense knowledge of a specific relation type   exists in the PLM . According to the results , mT5   seems to be better at xEffect than HinderedBy .   Commonsense Model To examine the data qual-   ity from another perspective , we also train com-   monsense knowledge models ( COMET ) ( Bosselut   et al . , 2019 ; Hwang et al . , 2021 ) on CN - AutoMIC   or ATOMIC - zh triples . We denote these models as   CN - COMET . The models are based on mT5 - base   ( 580 M ) , which is 20x smaller than T5 - XXL . Dur-   ing training , we set the learning rate to 1e-4 and   batch size to 128 by a small grid search . We linearly   decay the learning rate for all training steps , and fi-   nally take the checkpoints with the lowest negative   log - likelihood on the validation set . For fair com-9258   CategoryV oluntary   OccurencesInvoluntary   OccurencesStates   Precision 84 % 71 % 76 %   parison , we evaluate all these models on a held - out   set of ATOMIC - zh , and manually check the results .   During the evaluation , we remove the instances that   has unreadable head items . The results are shown   in Table 7 . The model trained on CN - AutoMIC   achieves the best performance , indicating the high-   quality CKG can make the small - sized model infer   better commonsense knowledge . Nevertheless , the   best performance is still not satisfactory . The qual-   ity of training data might be still not good enough .   And the translation noise in test data could also   exacerbate the difficulty .   4.2 Analyzing the Construction Steps   In this section , we analyze the effect of some inter-   mediate steps .   Category Precision As described in Section 3.2 ,   head items are generated with three categories of   seeds , so we can reduce invalid generation accord-   ing to the categories . However , the categories of   generated head items are not guaranteed to be what   they are generated from . Therefore , we manually   check 100 generated head items for each category ,   and report the precision in Table 8 . The results   indicate that most of the head items belong to the   category they are generated from . Based on this ,   we avoid nearly 10 % invalid generation according   to the category .   Effect of Cascaded Filtering We validate the   effect of cascaded filtering by temporarily remov-   ing the head and tail classifiers during constructing   CN - AutoMIC . That adds 47 K new triples in   total . We sample 500 triples from them and con-   duct manually checking . About 43.2%of them are   bad triples . According to the estimation , remov-   ing head and tail classifiers can make the overall   acceptance drops by 1.9 % .   Effect of Bootstrapping Iteration In Table 9 ,   we report the changes of high subset after a boot-   strapping iteration . From the results , we find the   quantities of unique heads , tails , and triples have   substantially increased . Also , we find the retaining   rate ( i.e. the proportion of triples that are retained   by the filters ) also increases . We conjecture that it   is because the filtered triples before iteration have   high - quality tail items . Converting them to head   items and conducting generation can get better per-   formance .   4.3 Case Study   Culture - Specific Knowledge Since mT5 has   been trained on Chinese corpora , it may gener-   ate commonsense knowledge specific to Chinese   context . We find some explicitly culture - specific   knowledge triples from the high subset and list   them in Table 10 . These examples involve festivals ,   traditional practices , games , and apps that are fa-   miliar to Chinese people but not popular in English   communities . Therefore they can not be found in   current English CKGs . In contrast , CN - AutoMIC   can capture such commonsense knowledge in Chi-   nese perspectives to some extent .   Deficiency Patterns We show some regular gen-   eration mistakes in Table 11 . We note two interest-   ing error types : ( 1)Some head items can not pair   with some relations . Taking them as input will al-   ways result in errors . For example , “ PersonX kills   himself ” can not pair with xWant ( after that , X   wants ) , because he will lose consciousness and can-   not want to do anything . Though we have set three   head categories for such problems , there are still   some intractable cases . The fundamental reason   is that PLMs are unable to “ reject ” inappropriate   input . Further research is needed to avoid such9259   results . ( 2)For negative expressions or unfavorable   situations , the model often performs badly genera-   tion for some relations . For example , in “ PersonX   has no house ; before that , X needs ” , the model is   trying to generate the methods to avoid the trouble   ( such as “ earning money ” ) , rather than the reason   that makes X get in the trouble . This might be due   to the ambiguity in the natural language prompts .   5 Discussion   What is the upper - bound size of the generation ?   The PLMs can conduct ever - lasting generation , but   it seems there is a soft upper bound . The results   generated later are easy to repeat previous results .   Therefore , the cost of novel results would gradually   increase and eventually become unaffordable . In   this work , we have generated tens of millions of   triples . It has still not reached the limit , but similar   or repeated content has appeared in large numbers .   For example , during generating head phrases , we   observe that the proportion of non - repetitive results   keeps descending ( Figure 5 ) .   If PLMs have already learned commonsense   knowledge , why is it necessary to extract the   knowledge from it First , according to the results   in this paper , the quality of direct generated results   is still poor . It indicates that even for a model with   13B+ parameters , the learned knowledge is still   rough and full of mistakes . A distilling process   can distinguish the clean knowledge and benefit   smaller applicable models . Besides , recent work   shows that even though explicitly training PLMs   with knowledge , there is no guarantee that they can   actually use such knowledge in target tasks(Wang   et al . , 2021 ) . Therefore , we can exploit explicit   symbolic knowledge as auxiliary information.92606 Conclusion   Considering the dilemma of lacking non - English   commonsense knowledge resources , in this paper ,   we propose CN - AutoMIC , the first Chinese com-   monsense knowledge graph that is totally generated   by pretrained language models . During the con-   struction , we use prompting generation to obtain   head and tail items , as well as introduce catego-   rized generation , cascaded filtering and bootstrap-   ping iteration to improve the quantity , quality and   diversity . Through human evaluation , the resource   is shown to have better quality than directly trans-   lated resources from English language . We discuss   the culture - related phenomena and common defi-   ciency patterns in the generated knowledge graph .   Although our work is limited to Chinese , the basic   framework and methods can be used to populate   CKGs in more languages .   Limitations   The main limitations of this work include : ( 1)We   require large - scale pretrained models . The genera-   tion performance is strongly dependent on the size   of models . We use 4 RTX - A6000 GPUs for run-   ning the T5 - XXL model . ( 2)Due to the lack of   large - sized PLMs , the quantity and quality of this   generated Chinese CKG still fall behind similar   English resources . ( 3)We still can not interpret the   behavior of large PLMs . The specific source of   the generated commonsense knowledge is hard to   locate , and there are potential ethical risks since   the results are not completely checked . ( 4)We   still require extra human labor when applying the   method to each new language . Although basically   our methods and underlying models ( mT5 ) can gen-   eralize for other languages , we still need human-   crafted prompts and a minor size of annotations for   training filter models .   Acknowledgements   This work is supported by the National Key Re-   search and Development Program of China ( No .   2020AAA0106400 ) , and the National Natural   Science Foundation of China ( No . 61922085 ,   61976211 , 62176257 ) . This work is also sup-   ported by the Strategic Priority Research Pro-   gram of Chinese Academy of Sciences ( Grant   No . XDA27020200 ) , the Youth Innovation Promo-   tion Association CAS , and Yunnan Provincial Ma-   jor Science and Technology Special Plan Projects   ( No.202202AD080004).References92619262   A Annotation Details   In this study , we mainly have two kinds of annota-   tion tasks , annotating training data for filters and   human evaluation for results . Both of them require   the workers to review a bunch of triples and an-   swer questions . We show the annotation page inFigure 6 . There are three questions for the workers :   ( 1 ) Whether the head is acceptable . ( 2 ) Whether   the tail is acceptable . ( 3 ) Whether the triple is   acceptable as a whole .   For question ( 1 ) and ( 2 ) , we provide one op-   tion for acceptance and four options for rejection :   abnormal expressions ( syntax errors ) , violation of   commonsense ( impossible situations ) , unusable for-   mat ( incomplete generation or containing names of   real identities ) , and mismatch to the relation .   For question ( 3 ) , there are four options   about whether the triple is acceptable ( plausi-   ble ): “ always / often ” , “ sometimes / likely ” , “ far-   fetched / never ” , and “ invalid ” .   Before annotation , each worker are shown with   a instruction , which contains background knowl-   edge of the annotation task and examples for each   answer options .   B Alternative Models for Generation   During the pilot studies , we also compared several   alternative text generation backbones besides mT5 ,   including a multilingual autoregressive model   XGLM ( Winata et al . , 2021 ) and two Chinese-   specific models , Pangu - Alpha ( Zeng et al . , 2021 )   and CPM2 ( Zhang et al . , 2021 ) . We construct   a small set of test prompts to make the models   complete some commonsense knowledge triples .   For each model , We use its biggest publicly avail-   able checkpoint to generate 100 results for each   prompt ( directly sampling without hyper - parameter   search ) .   We show the results and samples in Table 12   and Table 13 . All these backbones can generate   some plausible results . However , the XGLM ( 7.5B )   model often generate < unk > tokens and CPM2   ( 11B ) model sometimes give degenerate or irrele-   vant long results . In general , Pangu - Alpha ( 13B )   and mT5 - XXL ( 13B ) have better generation qual-   ity . Based on comprehensive consideration of effi-   ciency and feasibility , we conduct full experiments   and analyses with mT5 - XXL in this paper.9263   mT5 - XXL ( 13B ) XGLM ( 7.5B ) Pangu - Alpha ( 13B ) CPM2 ( 11B )   Pilot Tested   Triple Acceptance0.58 0.42 0.61 0.4692649265