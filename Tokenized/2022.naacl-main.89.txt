  Neha Nayak Kennard Tim O’Gorman Rajarshi Das   Akshay Sharma Chhandak Bagchi Matthew Clinton   Pranay Kumar Yelugam Hamed Zamani Andrew McCallum   University of Massachusetts Amherst   { kennard , togorman , rajarshi , akshaysharma , cbagchi ,   mfclinton , pyelugam , zamani , mccallum}@cs.umass.edu   Abstract   At the foundation of scientific evaluation is the   labor - intensive process of peer review . This   critical task requires participants to consume   vast amounts of highly technical text . Prior   work has annotated different aspects of review   argumentation , but discourse relations between   reviews and rebuttals have yet to be examined .   We present DISAPERE , a labeled dataset of   20k sentences contained in 506 review - rebuttal   pairs in English , annotated by experts . DIS-   APERE synthesizes label sets from prior work   and extends them to include fine - grained an-   notation of the rebuttal sentences , characteriz-   ing their context in the review and the authors ’   stance towards review arguments . Further , we   annotate every review and rebuttal sentence .   We show that discourse cues from rebuttals can   shed light on the quality and interpretation of   reviews . Further , an understanding of the argu-   mentative strategies employed by the review-   ers and authors provides useful signal for area   chairs and other decision makers .   1 Introduction   Peer review performs the essential role of quality   control in the dissemination of scientific knowl-   edge . The recent rapid increase in academic output   places an immense burden on decision makers such   as area chairs and editors , as their decisions must   take into account not only extensive manuscripts ,   but enormous additional amounts of technical text   including reviews , rebuttals , and other discussions .   One long term goal of research in peer review is   to support decision makers in managing their work-   load by providing tools to help them efficiently   absorb the discussions they must read . While ma-   chine learning should not be used to produce con-   densed accounts of the peer review text due to the   risk of amplifying biases ( Zhao et al . , 2017 ) , ML   tools could nevertheless help manage information   overload by identifying patterns in the data , such   as argumentative strategies , goals , and intentions . Any such research requires an extensive labeled   dataset . While the OpenReview platform ( Soergel   et al . , 2013 ) has made it easy to obtain unlabeled   public peer review text , labeling this data for su-   pervised NLP requires highly qualified annotators .   Correct interpretation of the discourse structure of   the text requires an understanding of the technical   content , precluding the use of standard crowdsourc-   ing techniques . Prior work on discourse in peer   review has focused this qualified labor force on   labeling arguments extracted from the text , which   enables the complete annotation of more examples ,   at the expense of research on non - argumentative   behaviors in peer review . While there has been   extensive research and deep analysis of different   aspects of peer review , the taxonomies used to de-   scribe review argumentation are disparate and not   directly compatible . Finally , there has been limited   research into understanding the discourse relations   between rebuttals and reviews ( Cheng et al . , 2020 ;   Bao et al . , 2021 ) , and none so far into the discourse   structure of rebuttals .   This paper presents DISAPERE ( DIscourse   Structure in Academic PEerREview ) , a dataset   focusing on the interaction between reviewer and   author . We give reviews and rebuttals equal im-   portance , and emphasize the relations between   them . To enable the study of behaviors beyond   the core arguments , we also annotate every sen-   tence of both the review and rebuttal , and provide   fine - grained labels for non - argumentative types .   We annotate at the sentence level not only for   completeness but also to avoid the propagation   of errors from argument detection . We annotate   four properties ( - , - - , , ) of each review sen-   tence , where the set of properties and their values   were developed by synthesizing taxonomies from1234   prior work . We also annotate each sentence of a   rebuttal with a fine - grained label indicating the au-   thor ’s intentions and commitment , and a link to the   set of review sentences that form its context . Fig-   ure 1 shows the DISAPERE annotation scheme on   a minimal , fictional example review - rebuttal pair .   DISAPERE is intended as a comprehensive and   high - quality test collection , along with training   data to fine - tune models . Our annotations are   carried out by graduate students in computer sci-   ence who have undergone training and calibration ,   amounting to over 850 person - hours of annotation   work . Much of the test data is double - annotated ,   and we report inter - annotator agreement on all as-   pects of the annotation . We describe the perfor-   mance of state of the art models on the tasks of   predicting labels and contexts , showing that inter-   esting ambiguities in the data provide the NLP com-   munity with research challenges . We also show an   example that demonstrates how decision makers   could use models like these to understand trends   and inform policies for future conferences ( § 5 ) .   The contributions of this paper are as follows :   ( 1 ) a new labeled training dataset of 506 review-   rebuttal pairs ( over 20k sentences ) of peer review   discussion text in English , where review sentences   are annotated with four properties , and rebuttal sen-   tences are annotated with context and labels from   a novel scheme to describe discourse structure ; ( 2 )   a taxonomy of discourse labels synthesizing prior   work on discourse in peer review and extending   it to add useful subcategories ; ( 3 ) a summary of   the performance of baseline models on the dataset   ( § 6 ) ; ( 4 ) examples of analyses on the dataset thatcould benefit peer review decision makers ( § § 4   and 5 ) , and ( 5 ) extensive annotation guidelines and   software to support future labeling efforts .   2 Related work   The design of this dataset draws upon extensive ,   but disparate prior work on this topic . Many works ,   some addressed below , have taken advantage of the   availability of review text hosted on OpenReview .   Argument - level review labeling Prior work has   developed label sets that address different phe-   nomena . Hua et al . ( 2019 ) introduced the study   of discourse structure in peer review by annotat-   ing argumentative propositions in the AMPERE   dataset with a set of labels tailored to the peer   review domain ( , , , , and ) . Similarly , Fromm et al .   ( 2020 ) ’s AMSR dataset frames the problem as an   argumentation process , in which the stance of each   argument towards the paper ’s acceptance or rejec-   tion is of paramount importance . Both view peer   review as argumentation , using argument mining   techniques to highlight spans of interest .   While its goal is not to examine discourse struc-   ture per se , Yuan et al . ( 2021 ) uses polarity labels   to indicate each argument ’s support or attack of   the authors ’ bid for acceptance . Besides polarity ,   these examples follow Chakraborty et al . ( 2020 ) by   annotating each argument with the aspect of the   paper it comments on . In contrast to Yuan et al .   ( 2021 ) , we do not attempt or recommend generat-1235ing peer review text , instead focusing on analyzing   human - generated text in peer review .   Review - rebuttal interactions We also expand   on work by Cheng et al . ( 2020 ) , who first annotated   discourse relations between sentences in reviews   and rebuttals . While Cheng et al . ( 2020 , 2021 )   present new deep learning architectures , in this   paper we focus on the creation and comprehensive   annotation of a new dataset , illustrated with results   from some less specialized baseline models .   Other research into rebuttals includes Gao et al .   ( 2019 ) . Besides their main finding that reviewers   rarely change their rating in response to rebuttals ,   they find that more specific , convincing and explicit   responses are more likely to elicit a score change .   Observations from this paper are formalized into   rebuttal action labels in DISAPERE .   Comparison of datasets In DISAPERE we at-   tempted to unify these schemas to form a single   hierarchical schema for review discourse structure .   We then expanded this hierarchical schema to in-   troduce fine - grained classes for implicit and ex-   plicit requests made by the reviewers . The details   of the correspondence between DISAPERE labels   and those from prior work are summarized in Ap-   pendix A. In contrast to prior work , DISAPERE   labels discourse phenomena at the sentence level   rather than the argument level . This enables more   thorough coverage of the text while avoiding the   propagation of errors from machine learning mod-   els earlier in the annotation pipeline . While us-   ing manually defined discourse units ( above or be-   low the sentence level ) may more precisely cap-   ture some discourse information , a separate pass of   discourse segmentation can hinder the use of dis-   course datasets , as achieving consistent and repli-   cable annotation of argument units is known to be   highly challenging ( Trautmann et al . , 2020 ) , and   also because few works actually tackle unit seg-   mentation ( Ajjour et al . , 2017 ) .   3 Dataset   Each example in DISAPERE consists of a pair of   texts : a review and a rebuttal . Labels for reviews   and rebuttal sentences are described below . Review   sentence labels are summarized in Table 2 , and   rebuttal sentence labels in Table 3 .   3.1 Review sentence labels   3.1.1 Review actions - annotations characterize a sen-   tence ’s intended function in the review . Annota-   tors label each sentence with one of six coarse-   grained sentence types including evaluative and   factsentences , request sentences ( including ques-   tions , which are requests for information ) , as well   as non - argument types : social , and structuring for   organization of the text .   3.1.2 Fine - grained review actions   We also extend two of these review actions with   subtypes : structuring sentences include headers ,   quotations , or summarization sentences , and re-   quest sentences are subdivided by the nature of   the request , distinguishing between clarification of   factual information , requests for new experiments,1236   requests for an explanation ( e.g. of motivations   or claims ) , requests for edits , and identification of   minor typos .   3.1.3 Aspect and polarity annotations follow the ACL review form   ( Chakraborty et al . , 2020 ; Yuan et al . , 2021 ) . These   distinguish clarity , originality , soundness / correct-   ness , replicability , substance , impact / motivation ,   andmeaningful comparison . Following Yuan et al .   ( 2021 ) , arguments with an are also anno-   tated for . We label positive andnegative   polarities . and are applied to   sentences whose - value is evalua-   tiveorrequest .   3.2 Rebuttal sentence labels   We annotate two properties of each rebuttal sen-   tence : a - label characterizing   its intent , and its in the review in the   form of a subset of review sentences .   3.2.1 Rebuttal actions   The 14 rebuttal actions ( Table 3 ) are divided into   three - categories ( concur , dis - pute , non - arg ) based on the author ’s stance towards   the reviewer ’s comments .   ( 1)concur : The author concurs with the premise   of the context . This includes answering a ques-   tion or discussing a requested change that has been   made to the manuscript , conceding a criticism in   an evaluative sentence . ( 2 ) dispute : The author   disputes the premise of the context . The rebuttal   sentence may reject a criticism or request , disagree   with an underlying fact or assertion , or mitigate   criticism ( accepting a criticism while , e.g. , arguing   it to be offset by other properties ) . ( 3 ) non - arg :   Encompasses rebuttal actions including social ac-   tions ( such as thanking reviewers ) , and structuring   labels , for sentences that organize the review .   Responses to request s are further annotated : if   the author concur s , we record whether the task   has been completed by the time of the rebuttal ,   or promised by the camera ready deadline ; if the   author dispute s , we record whether the task was   deemed to be out of scope for the manuscript .   3.2.2 Rebuttal context   We refer to the set of sentences which a rebuttal   sentence is responding to as the context of that1237   sentence , with special labels for when referring to   the entire review ( global context ) or the empty set   ( no context ) . By not mandating a fixed discourse   chunking , these annotations may handle situations   when some rebuttal sentences respond to large sec-   tions of text , and other rebuttal sentences respond   to specific sentences within those sections .   3.3 Data Source and Annotation   DISAPERE uses English text from scientific dis-   cussions on OpenReview ( Soergel et al . , 2013 ) ,   which makes peer review reports available for re-   search purposes . We draw review - rebuttal pairs   from the International Conference on Learning   Representations ( ICLR ) in 2019 and 2020 , result-   ing in text within the domain of machine learning   research . Review - rebuttal pairs are split into train ,   development and test sets in a 3:1:2 ratio such that   all texts associated with any manuscript occur in   the same subset . Overall statistics for the dataset   are summarized in Table 4 .   Authors are able to respond to each ICLR review   by adding a comment . Although rebuttals are not   formally named , we consider direct replies by the   author to the initial review comment to constitute a   rebuttal . While multi - turn interactions are possible ,   we focus on reviews and initial responses , and leave   study of extended discussion for future work . The   text is separated into sentences using the spaCy   ( Honnibal and Montani , 2017 ) sentence separator .   Annotation was accomplished with a custom   annotation tool designed for this task , which is   available as part of the code release accompany-   ing DISAPERE . The tool is described in detail in   Appendix B. Annotators annotate each sentence of   a review , then examine the rebuttal sentences in   order , selecting sets of review sentences to form   their context . While this linking between sentences   does not explicitly align multi - sentence chunks   as in pipelined approaches to discourse alignment   ( Cheng et al . , 2020 ) , we note that since multiple   sentences may be aligned to the same set of sen-1238tences in the review , some discourse structure is   nevertheless latently implied .   3.4 Agreement   We report Cohen ’s κ(Cohen , 1960 ) on the IAA of   labeling both review and rebuttals , treating each   sentence as a labeling unit ( Table 5 ) . The annota-   tors for each example are selected randomly from   the pool of 10 annotators . Cohen ’s κis calculated   for sentences annotated at least twice . Where more   than two annotations were produced , we calculate   κbetween all pairs and normalize by the number   of possible pairs . The results show between mod-   erate and substantial chance - corrected agreement   between annotators , for both -   and - labels ( Appendix D pro-   vides details about agreement on context sentences ) .   While these IAA scores do illustrate the noise of   the task , note that this is not highly unusual for dis-   course labeling tasks – e.g. Habernal and Gurevych   ( 2017 ) and Miller et al . ( 2019 ) both report αbe-   tween 0.4 and 0.5 .   Label Cohen ’s κ - 0.605 - - 0.583 0.447 0.561 - 0.513 - 0.479   4 Analysis   4.1 Context types   We separate the different types of rebuttal contexts   in terms of the number and relative position of   selected review sentences in Table 6 , along with the   four cases in which the context can not be described   as a subset of review sentences . Notably , 84.81 %   of sentences are linked to some review context . A   small number of sentences refer to other sentences   within the rebuttal , rather than any review context ,   posing a challenge for future work .   4.2 Alignment   One might reasonably hypothesize that the task of   alignment between rebuttal and review sentences   would be trivial , since authors are likely to respond   to each point in the review in order . We can show   that this is not the case . In Figure 2 , we calcu-   late Spearman ’s ρbetween rebuttal sentence in-   dices and their aligned review sentence indices .   Rebuttals responding to each point in order would   achieve ρ= 1.0 ; this case is rare . Many exam-   ples with positive ρ < 1.0indicate that authors   do respond to points approximately in order , but   a simple mapping based on order alone would not   capture the correct alignment . Thus , while linear in-   ductive bias may be beneficial to alignment models ,   the task of determining rebuttal sentences ’ contexts   is not trivial.1239   4.3 Author interpretations of criticism   In our taxonomy , each argumentative - corresponds to a particular - , which we refer to as its canonical - ( listed in ‘ Reply to ’ column of   Table 3 ) . For example , answers are generally re-   sponses to requests , while conceding criticism is   usually a response to an evaluative statement . An-   notations revealed that authors often interpreted   review sentences as if they embodied - s besides the canonical one , in a way that   furthered the author ’s argumentative goal . For ex-   ample , authors often responded to evaluative state-   ments as if they were requests , perhaps in order to   appease a reviewer , although no action was explic-   itly requested . Figure 3 shows the distribution of   contexts for three different - s.   4.4 Relating discourse features to rating   Figure 4 shows one possible analysis taking into   account the rating of the review . We show the distri-   bution of - - labels of requests   with review ratings . It appears that high - scoring   manuscripts are rarely asked to add experiments ,   and are polished enough to not elicit requests to   fix typos . Interestingly , low - scoring manuscripts   have the second - lowest occurrence of typo requests ,   which could be due to the preponderance of other   requests , but this bears further examination .   5 Application : Agreeability   Gao et al . ( 2019 ) showed that reviewers do not   appear to act upon the rebuttals responding their   reviews . It is possible that this is due to paucity of   time on the reviewers ’ part . It is also common prac-   tice for area chairs to use review variance across   a manuscript ’s reviews as a practical heuristic to   decide which manuscripts need their attention . We   propose that discourse information such as that   described by DISAPERE can be used to provide   heuristics that are data - driven , yet interpretable ,   and leverage information from the content of re-   views rather than just numerical scores , resulting   in better decision making .   One such measure is agreeability , which we de-   fine as the ratio of sentences to argumen-   tative sentences in a rebuttal , i.e. : agreeability = . We argue that low agreeability can   indicate problematic reviews even in cases where   the variance in scores does not reveal an issue , as   illustrated in Figure 5 . Agreeability is only weakly   correlated with rating , with Pearson ’s r= 0.347 .   In Figure 5 , 18 % ( 28/159 ) of manuscripts would   not meet the bar for high variance scores ( top quar-   tile ) , although their low agreeability ( bottom quar-   tile ) indicates that they may merit closer attention   from area chairs.1240   6 Baselines   Two types of machine learning tasks can be defined   in DISAPERE . First , a sentence - level classification   task for each of the four review labels and the two   levels of rebuttal labels . Second , an alignment task   in which , given a rebuttal sentence , the set of review   sentences that form its context are to be predicted .   The models described below are not intended to   introduce innovations in discourse modeling , rather ,   we intend to show the off - the - shelf performance of   state - of - the - art models , and indicate through error   analysis the phenomena that are yet to be captured .   6.1 Sentence classification   For the six classification tasks , we use   bert - base ( Devlin et al . , 2019 ) to pro-   duce sentence embeddings for each sentence , then   classify the representation of the [ CLS ] token   using a feedforward network .   We report macro - averaged F1 scores , shown in   Table 7 . In general , F1 is lower for tasks with larger   label spaces . While the performance is reasonable   in most cases , there is still room for improvement .   While achieves a particularly low F1 score ,   itsκis within the bounds of moderate agreement ;   thus , this must be accounted for by the inherent   difficulty of the task rather than a deficit in data   quality .   As one might expect , errors in the classification   results largely mirror disagreements in the anno-   tations , which in turn reflect particularly ambigu-   ous utterances . One example is the occurrence of   rhetorical questions , such as ( 1 ) in Table 8 , incor-   rectly labeled as request instead of evaluative . In   fact , for sentences such as ( 1 ) , additional context   would disambiguate its type : the reviewer answers   the question in the next sentence , and hence both   sentences were labeled evaluative . Similarly , ( 2 )   was labeled fact , but since it is an integral part of a   reviewer ’s argument against the soundness of the   paper , should have been labeled evaluative . Certain   reviewers also use conventions that do not fit the   general schema we observed when developing DIS-   APERE . For example , ( 3 ) , an opinionated heading ,   could be considered both structuring andevalua-   tive . Finally , certain lexical cues a model may pick   up on can be quite subtle . For example , though   they share a prefix , sentences ( 4 ) and ( 5 ) are clearly   evaluative andrequest respectively .   6.2 Rebuttal context alignment   We model rebuttal context alignment as a rank-   ing task . Ideally , a model should rank all relevant   review sentences higher than non - relevant review   sentences . As a baseline , we use an information   retrieval ( IR ) model based on BM25 that , given a   rebuttal sentence ranks all the corresponding re-   view sentences . We also report results from a   neural sentence alignment model based on a two-   tower Siamese - BERT ( S - BERT ) model ( Reimers   and Gurevych , 2019 ) . We add a NO_MATCH sen-   tence to the review , to which rebuttal sentences   without context sets in the review are aligned . Then ,   each review and rebuttal sentence is encoded inde-   pendently using a S - BERT encoder and the similar-   ity between two sentences is computed using cosine1241   similarity . We initialize with a modelpre - trained   on various sentence - pair datasets . Alignment is   evaluated using mean reciprocal rank ( MRR ) and   Mean Average Precision ( MAP ) .   S - BERT BM25   MAP 0.4409 0.5174   MRR 0.5022 0.5980   Surprisingly , the BM25 model outperforms a   neural model ( Thakur et al . , 2021 ) . While this   shows that lexical information is a useful signal ,   both models have significant scope for improve-   ment , and lexical overlap is clearly not sufficient   for this task . Importantly , neither of these models   account for the context of the rebuttal sentence ,   and predict each sentence ’s context independently .   Incorporating this information is likely to lead to   performance gains ; however , we leave this investi-   gation to future work .   7 Conclusion   As the burden of academic peer reviewing grows ,   it is important for program chairs and editors to act   upon data - driven insights rather than heuristics , to   make the best possible use of participants ’ scarce   time . Models trained on data like DISAPERE will   allow decision makers to glean deep insights on the   interactions occurring during peer review .   Almost all publicly available peer review data   is from the domain of artificial intelligence , lim-   iting the scope of DISAPERE and any similar   project . While this means that models trained on   DISAPERE wo n’t necessarily generalize to all new   domains , we hope that with the detailed annotation   guidelines and seamless data collection using the   software provided with this paper support , userscan build on our work , and ensure that their insights   are robust to differences over time and across fields .   8 Ethics   The outcomes of peer review can have outsize ef-   fects on the careers of participating scholars . As   machine learning models are known to amplify   biases , we strongly recommend against using the   outputs of any machine learning system to make   decisions about individual cases . A dataset like   DISAPERE is best used to survey participants ’ be-   havior . Any interventions based on this information   should be subjected to studies in order to ensure   that they do not introduce or exacerbate bias .   Acknowledgments   This material is based upon work supported in   part by the National Science Foundation under   Grant Numbers IIS-1763618 , IIS-1922090 , and   IIS-1955567 , in part by the Defense Advanced   Research Projects Agency ( DARPA ) via Contract   No . FA8750 - 17 - C-0106 under Subaward No .   89341790 from the University of Southern Cal-   ifornia , in part by the Office of Naval Research   ( ONR ) via Contract No . N660011924032 under   Subaward No . 123875727 from the University of   Southern California , in part by IBM Research AI   through the AI Horizons Network , in part by the   Chan Zuckerberg Initiative under the project Sci-   entific Knowledge Base Construction , and in part   by the Center for Intelligent Information Retrieval .   Any opinions , findings and conclusions or recom-   mendations expressed in this material are those of   the authors and do not necessarily reflect those of   the sponsors .   References12421243A Rationale for taxonomy construction   Our label sets leverage ideas from and commonali-   ties between existing work in this domain , includ-   ing AMPERE ( Hua et al . , 2019 ) , AMSR ( Fromm   et al . , 2020 ) ASAP - Review ( Yuan et al . , 2021 ) , and   Gao et al . ( 2019 ):   •ASAP - Review ’s polarity labels approximately   correspond to arg - pos andarg - neg labels in   AMSR   •AMSR and AMPERE each label non-   argumentative sentences in a similar manner   •aspect labels from ASAP - Review apply only   to certain types of sentences ; namely request   andevaluative sentences from AMPERE ’s   taxonomy .   •summary is an exception among ASAP-   Review ’s aspect s , behaving similarly to AM-   PERE ’s quote . We thus include both of these   under a structuring category .   •Further , in order to gauge the extent to which   authors acquiesced to reviewers ’ requests , we   introduce a fine - grained categorization of the   types of requests .   •Gao et al . ( 2019 ) enumerates some features   of rebuttals , including expressing gratitude ,   promising revisions , and disagreeing with crit-   icisms . We formalize these observations into   our rebuttal label taxonomy .   B Annotation tool   Two modes of annotation are possible . First , anno-   tators can apply labels on a sentence - by - sentence   basis . Multiple labeling schemas can be anno-   tated simulatenously , with the option of adding   constraints so that certain values govern possible   values for other properties . This annotation mode   is shown in Figure 6 .   The second annotation mode can build on the   output of the first annotation mode . Here , sen-   tences of a focus text ( the rebuttal ) are presented   in sequence , and annotators are permitted to select   one or more of the sentences in the reference text   ( the review ) which form the context of the sentence   of the focus text . Further , a label can be applied to   the alignment . This annotation mode is shown in   Figure 7 and Figure 8.1244   C Annotated review - rebuttal pair   Figure 9 shows a truncated version of a review-   rebuttal pair from the train set of DISAPERE .   D Context overlap analysis   As a proxy for agreement of rebuttal spans , we   show the types of overlap between spans on rebut-   tal sentences from 81 examples annotated by two   annotators in Table 10 .   Type of   context overlapNum . rebuttal   sentences% rebuttal   sentences   Exact match 914 53.11 %   Partial match 492 28.59 %   Agree none 122 7.09 %   Disagree none 100 5.81 %   No overlap 93 5.40%E Additional Agreement Analysis   While some of the IAA scores on annotation are   low , we note that the labels used in this task attempt   to characterize relatively complex relationships in   text . To give more insight into such disagreements ,   Figure 10 provides a confusion matrix regarding   the - labels . Recognizing that   there are often situations in which users of a dataset   will hope to reduce a label set , we provide some   guidance as to which such merges may be accept-   able and which are not .   Many disagreements come from three labels   which might be said to exist upon a continuum – , and - . We suggest that in the situation of needing   to minimize IAA disagreement , one might consider   first merging mitigate criticism intoreject criticism .   The kind of disagreements seen between the two   are understandable but nuanced : the difference be-   tween saying that the reviewer has a point ( but that   they disagree on the relevance of that point ) and   disagreeing with the point itself . Out - of - context re-   buttal sentences illustrating this are provided below   as examples of this kind of ambiguous situation:12451246   •We note that such rules are indeed limited to   some extent , but they still capture a rather ex-   pressive fragment of answer set programs with   restricted forms of external computations .   •The use of Cfor hyperparameter tuning   was incidental and not a central point of our   paper .   •We agree that the measure theoretic approach   is not always necessary ( indeed for angular   actions , it is not needed ) , but it is necessary   for a very common scenario – clipped actions .   Furthermore , we note that ( as illustrated in the   confusion matrix ) a wide range of disagreements   are hard to distinguish from “ answer ” labels , as   authors often attempt to frame disagreements as   simple answers to questions.124712481249