  Melanie SclarPeter WestSachin KumarYulia TsvetkovYejin ChoiPaul G. Allen School of Computer Science & Engineering , University of WashingtonLanguage Technologies Institute , Carnegie Mellon UniversityAllen Institute for Artificial Intelligence   msclar@cs.washington.edu   Abstract   We present R , a novel framework for   sentence summarization that can be trained   reference - free ( i.e. , requiring no gold sum-   maries for supervision ) , while allowing direct   control for compression ratio . Our work is the   first to demonstrate that reference - free , con-   trolled sentence summarization is feasible via   the conceptual framework of Symbolic Knowl-   edge Distillation ( West et al . , 2022 ) , where la-   tent knowledge in pre - trained language models   is distilled via explicit examples sampled from   the teacher models , further purified with three   types of filters : length , fidelity , and Informa-   tion Bottleneck . Moreover , we uniquely pro-   pose iterative distillation of knowledge , where   student models from the previous iteration of   distillation serve as teacher models in the next   iteration . Starting off from a relatively modest   set of GPT3 - generated summaries , we demon-   strate how iterative knowledge distillation can   lead to considerably smaller , but better sum-   marizers with sharper controllability . A useful   by - product of this iterative distillation process   is a high - quality dataset of sentence - summary   pairs with varying degrees of compression ra-   tios . Empirical results demonstrate that the fi-   nal student models vastly outperform the much   larger GPT3 - Instruct model in terms of the con-   trollability of compression ratios , without com-   promising the quality of resulting summariza-   tion .   1 Introduction   We introduce R , a new framework for sen-   tence summarization that works by iteratively gen-   erating and distilling knowledge into successively   better models . This allows R to be [ Re-   fer]ence fr[ee]—beginning by distilling from a   large language model rather than with supervised   data . Yet , our method results in a more efficient , Figure 1 : Our method results in high quality , reference-   free compact summarizers . We begin by using a   large language model ( e.g. GPT-3 ) to generate many   summaries that demonstrate different aspects we may   want in a summary — gray represents an aspect well-   represented in these generations , while black is under-   represented . We first use R -D to itera-   tively filter and train summarizers that better represent   these desirable aspects , e.g. shorter summary length .   We then use generations from R -D to   train a model in which these aspects are controllable :   this is R -C .   compact , and controllable summarization model   than what we start with .   Our work follows the paradigm of Symbolic   Knowledge Distillation ( West et al . , 2022 ) , which   transfers implicit knowledge from a massive lan-   guage model to a considerably smaller student   model by explicitly generating knowledge in tex-   tual form . Unlike traditional knowledge distillation   ( Hinton et al . , 2015 ) where the teacher model and   the student model are of the same type , symbolic   knowledge distillation allows for the student model   to be of a different type .   Our work differs from West et al . ( 2022 ) in three9649key aspects . First , our distillation is iterative : each   student model becomes a teacher in successive   rounds , refining and improving summarization at   every step . Second , R controls for more   than just overall quality , improving multiple model   aspects in each round such as length , fidelity , and   information bottleneck ( Tishby et al . , 1999 ) , then   allowing explicit length control at generation time .   Third , our work is the first to show that reference-   free , controlled sentence summarization can be for-   mulated as symbolic knowledge distillation .   R works in two phases , illustrated in   Figure 1 . First , R -D uses a mod-   est number of generated summaries from GPT-3   ( Brown et al . , 2020 ) to produce high quality and   compact summarizers ( Goyal et al . , 2022 ) . We   follow an iterative approach ; in each iteration we   filter generations for desirable qualities , re - train a   new and better summarizer , and finally generate   new summaries for the next round . Each round   amplifies effects of the previous rounds , improv-   ing notions of summary quality like entailment   or shorter length . Second , R -C   uses these iteratively distilled summaries to train a   model with explicit control : in our experiments , we   use progressively shortened generations from each   iteration to train a final summarizer with explicit   length control .   We find that R demonstrates compelling   empirical results compared to competitive base-   lines . R -D , even without explicit   length control , is able to generate shorter sum-   maries with more consistency and equal quality   compared with the original teacher model ( GPT-3 ,   16x larger in size ) as well as a supervised model .   Moreover , R -C , which has more   direct length control baked in , demonstrates a sharp   degree of control in length , and succeeds at gener-   ating high quality summaries at specified lengths   with significantly higher accuracy than GPT-3 . In   sum , the promising empirical results of R   encourages further future investigation to extend   the framework of symbolic knowledge distillation   for reference - free , controlled text summarization .   2 Methods   We first describe R -D ( see § 2.1 ) ,   an iterative procedure to promote specific behav-   iors that may not be prevalent in the original data ,   while maintaining summary quality . We explore   two different filters , detailed in § 2.2 . We then de - tailR -C ( see § 2.3 ) , a model that   separates summaries into categorical variables and   is iteratively trained to , summarize a given sen-   tence within the desired category ( e.g. , a range of   compression ratio ) . In this work we only consider   categories that reflect different compression ratio ,   but the same approach could be applied to other   types of control categories , such as style .   2.1 Iterative Symbolic Knowledge Distillation :   R -D   LetD = D∪. . .∪ Ddenote a sentence cor-   pus without reference summaries . We start with a   teacher model ( GPT3 - Instruct Curie ) from which   we want to distill summarization knowledge under   a fixed budget . Using D — a small subset of D —   we first generate a dataset of sentence - summary   pairs ( C)by few - shot prompting the teacher and   automatically filtering low - quality generations . Fil-   ters will be detailed in Section 2.2 . Throughout   the whole training procedure , we store each entry   ( s , s)as “ sTL;DR : s < eos > ” . Here , < eos > de-   notes end of sequence and TL;DR ; is a separator   that has been shown to encourage summarization   behavior ( Radford et al . , 2019 ) .   LetMbe a pre - trained model significantly   smaller than GPT-3 ( GPT2 - Large in our experi-   ments ) . Using the seed dataset C , we train a stu-   dent model Mby fine - tuning Mwith language   modeling loss . We then iteratively refine this model   by ( 1 ) using it to generate summaries for a subset   ofD , ( 2 ) filtering them to remove undesired be-   haviors , and ( 3 ) training another student model on   the filtered dataset , essentially distilling a better   summarizer . More precisely ,   C:= filter(generate ( M , D ) )   M:= finetune ( M , C )   We execute this procedure for tsteps , creating t+1   different summarization datasets in the process :   C , C , . . . , C.We discuss two possible instantia-   tions of the filterbelow .   2.2 Filters   There is no one summary that is better than all oth-   ers ; depending on the desiderata of the end users ,   some might prefer shorter but less informative sum-   maries , while others might prefer longer , and more   informative ones . While some of these goals are9650universal and always desired ( for example , a sum-   mary should be accurate , in that it should not con-   tain information not present in the input ) , others   can be tailored to the end task . We use binary   filters ( filter ) to operationalize these goals . We   experiment with the following filters .   Summary Fidelity Filter To encourage accurate   summaries , we employ a simple but effective crite-   rion : the summary should be entailed by the input   sentence . More formally , we define a binary filter ,   f(s , s ) : = 1{s⇒s } , and discard all non-   entailed sentence - summary pairs to avoid using   these samples when training the next iteration ’s stu-   dent . We measure entailment using an off - the - shelf   state - of - the - art NLI model ( Liu et al . , 2022a ) .   Summary Length Filter While underexplored   in prior work , constraining for the length of written   text , especially in summarization , is a desirable fea-   ture to support real world applications with limited   screen space . To obtain a corpus of summaries of   varying lengths , at each distillation step i , we en-   courage the student Mto generate progressively   shorter outputs . We achieve this by constraining   Cto contain only summaries with a predefined   compression ratio r∈[0,1 ] . More precisely ,   f ( s , s , r ) = 1 / braceleftig|s|   |s|≤r / bracerightig   where r > rfor all i , to progressively summa-   rize more succinctly.is commonly referred to   ascompression ratio . In theory , one could generate   data for all desired compression ratios directly from   M. However , since the seed dataset Cis heavily   skewed towards longer summaries , the final corpus   after filtering with fwould be extremely small   for lower compression ratios . We find that combin-   ing the two filters and iteratively refining models   to produce shorter , accurate summaries leads to a   more diverse and still high - quality final corpus .   Contextual Filter For many applications , the   sentences we need to summarize are part of a larger   piece of text , such as a paragraph or a document   ( e.g. emails , articles ) . This contextual information   may further improve sentence summary quality ,   since depending on the larger context , different in-   formation could be more important to be preserved ,   and inter - sentence redundancies could be removed .   Inspired from West et al . ( 2019 ) ’s interpretation of   the Information Bottleneck principle ( Tishby et al . ,1999 ) , we consider the following filter :   f=1 / braceleftigp(s|s )   p(s|s)≥l / bracerightig   where NSP refers to “ next sentence prediction ” , p   is an oracle language model ( which we approxi-   mate by GPT2 - Large ) , sdenotes the sentence   immediately following the input sentence s , and   l∈[0,1]is a hyperparameter . Intuitively , we want   to find summaries which are good predictors of the   next sentence , to select the most crucial informa-   tion and preserve coherence . lallows us to strike a   balance between sacrificing some of the informa-   tion in sand maintaining enough to predict s.   Adding f requires expanding the input se-   quence to also include the next sentence throughout   the iterative distillation process defined in § 2.1 .   Final R -D Filters Definition   We experiment with two filters , fandf(or # 1   and # 2 , as we will refer to during experiments ) . f   does not assume the existence of any context , and   so it only filters for inaccuracies and length :   f(s , s;s , r ) = f∧f   This allows fto be applied in broader contexts .   We also define f , which adds contextual filtering :   f(s , s;s , r , l ) = f∧f ∧f   Fluency Filter To ensure fluency over several   self - training iterations , we consider an additional   filter only to be used in R -C .   Given a sentence x= ( x , . . . , x ) , we define   AvgNLL ( x ) : = −/summationtextlogp(x|x ) . We de-   termine a summary as fluent if and only if its mean   Negative Log Likelihood ( NLL ) does not exceed   that of source sentence , leading to the filter :   f ( s , s ) = 1 / braceleftig   AvgNLL ( s)≤AvgNLL ( s)/bracerightig   2.3 R -C   Using the high quality corpora of varying compres-   sion ratios obtained using R -D , we   train R -C , a summarization model   that allows explicit control for desired compression   ratio . We divide all possible compression ratios   intonbuckets , where each bucket b=/bracketleftbig,/bracketrightbig   for   0≤i < n . Using bas control codes , we train a   model that , when prompted with it , can summarize   at a compression ratio within b.9651Similar to D , we start with a corpus F = F∪   . . .∪ Fof sentences without reference summaries .   Additionally , we create a seed corpus labeled with   compression ratios , E = C∪. . .∪ C(F=   D∪. . .∪D ) now representing each example ( s , s )   as “ s < sep > < bucket_tok j > TL;DR : s < eos > ” ,   where < bucket_tok j > corresponds to the bucket   in which the example lies , that is∈b.<sep >   is a special token . We denote each subset of E   corresponding to bucket jasE. This seed dataset   is filtered to remove low - quality generations , with   the same filter as all the subsequent iterations .   Similar to R -D , starting with a   pre - trained model N(GPT2 - Large ) , we train stu-   dent models via iterative distillation . In each itera-   tioni , ( 1 ) we fine - tune the student model using the   bucket labeled corpus E , ( 2 ) generate summaries   forFfor all buckets , ( 3 ) filter them to create a new   labeled corpus E. We do not reinitialize the stu-   dent at each iteration , but rather fine - tune starting   from the teacher ’s current local optima . We use   h(s , s ) = f∧f as the filter . Formally ,   N:= finetune ( N , E , E , . . . , E )   E:=h(generate ( N , F , j))∀0≤j < n   2.4 Primal - Dual Problem Interpretation of   Summarization   Assuming summaries are fluent and factual , sen-   tence summaries trade off between two variables :   level of compression and level of information   preservation . We are able to effectively fix the   level of compression by introducing control codes ,   and then develop models to maximize information   preservation . This is our primal problem . Thanks   to length - control codes , we can now also solve the   dual problem : “ what is the best shortest summary   we could write ? ” . Written more precisely , given   a fixed level of tolerance for losing information   from the original sentence , what is the shortest   summary we could write ? Furthermore , comparing   similar - lengthed summaries also allows for fairer   comparisons , since we are effectively measuring   changes in only one variable .   3On GPT3 ’s Fidelity and Length Control   We analyze GPT3 - Instruct Curie ’s ( Brown et al . ,   2020 ) sentence summarization capabilities . We   promote GPT3 to summarize at different compres-   sion ratios by few - shot prompting with high - quality   sentence - summary pairs in the desired compression   ratios . More precisely , we do three - shot prompt-   ing with three different sets of summaries : one set   of sentence - summary pairs has all three pairs with   compression ratios in the interval [ 0.6,0.8 ] , another   set in [ 0.4,0.6 ] , and another in [ 0.2,0.4 ] .   We show that average compression ratio ( c.r . )   correlates with the prompts ’ compression ratio ( al-   though variance is large ) , and up to 33 % of the time   models generate summaries longer than the original   sentence ( see Table 1 ) . Qualitatively , this seems to   be because of punctuation edits or hallucinations .   Besides using prompts that encourage shorter   summaries , one can iteratively summarize through   few - shot prompting . If f(s)is the summary   GPT-3 generates when prompted with p , then   f(f ( . . . f(s)))=f(s)may also be a summary   ofs , possibly shorter . We find that successive appli-   cation of the same prompt did not result in shorter   summaries , i.e. |f(s)|≃|f(s)| , suggesting fis   roughly idempotent in terms of length ( see A.1 ) .   These experiments motivate the need of more   sophisticated approaches to length control and reli-   ably summarizing without supervision .   4 Experiments   Dataset We create the corpora DandFby sam-   pling contiguous sentence pairs from RealNews   ( Zellers et al . , 2019 ) news articles . We filter out   sentences shorter than 50 characters . Using GPT-3   as the teacher , we summarize sentences in Dand   use the outputs with 60 - 80 % compression ratio as   our initial dataset C , since it was the best one quan-   titatively and qualitatively . Although this implies   that we will not initially have enough short sum-9652   maries to train from , we will generate progressively   shorter ones during our training procedure .   Training Details We use off - the - shelf model   WANLI ( Liu et al . , 2022a ) to create the filter f.   We run 3 iterations of R -D with   compression ratios for each training iteration are   as follows : r= 0.7,r= 0.5,r= 0.3 . All   generated data is decoded via beam search ( with   beam width 5 ) . All experiments with f are   done with r = e≈0.0025 , empirically de-   cided through preliminary exploration . We train   R -C withn= 10 buckets for 7   iterations . For more details , refer to Appendix B.   While having contextual information for f   is useful , many applications do not have contex-   tual information available . Therefore , we use the   corpus C∪. . .∪ Cgenerated using fas initial   training data for R -C with the goal   of increasing its applicability .   Supervised Baseline We include a supervised   baseline as a comparison point . Following Rush   et al . ( 2015 ) , we use Gigaword ( Napoles et al . ,   2012 ; Graff et al . , 2003 ) as a silver - labeled dataset   where the headline is used as the summary for an   article ’s first sentence . We fine - tune GPT2 - Large   ( Radford et al . , 2019 ) ( the same architecture as in   our models ) on this corpus , with default hyperpa-   rameters . Due to the training data ’s nature , the   supervised baseline has a low average compres-   sion ratio of 55 % ( similar to R -D   Iteration 2 , but longer on average than Iteration 3 ) .   Importantly , we do not ( and can not ) include a con-   ventional knowledge distillation from GPT-3 ( e.g.   Shleifer and Rush 2020 ) since the full distribution   of token logits is unavailable .   4.1 Evaluating R -D   Compression and Fidelity Statistics We ob-   serve that iterative training with selection of pro-   gressively shorter summaries achieves the goal   of generating shorter summaries , and addition-   ally , less variance in compression ratios ( Figure 2 ) .   Moreover , in Table 1 we observe that , by using an   NLI filter during distillation , R -D   summaries were ~90 % entailed by the original sen-   tence according to WANLI ( compared to 79 % if   not including an NLI filter during training ) . This   vastly surpasses the comparable GPT-3 dataset ( 20-   40 % ) , and achieves similar fidelity as the best GPT-   3 summaries , even when our model is significantly   smaller . The same trends hold for both filters .   4.1.1 Comparison with GPT-3   We compare the quality of our summaries ( s )   and GPT-3 generated summaries ( s ) , for every   trained iteration and every GPT-3 dataset . Since9653   longer summaries will naturally be able to preserve   more of the original information ( Schluter , 2017 ) ,   it is not reasonable to compare two wildly differ-   ent compression ratios . Therefore , we only com-   pare summaries that differ by length at most 10%:/vextendsingle / vextendsingle−/vextendsingle / vextendsingle≤0.1 , where sis the original sentence .   To measure summary quality automatically , we   compute BERTScore ( Zhang * et al . , 2020 ) and   ROUGE-1,2,L ( Lin , 2004 ) against the original sen-   tence s(no references are available ) . Given a met-   ricm , we evaluate models based on m(s , s)−   m(s , s ) . Positive values reflect that shad higher   scores than the baseline summary s , which is de-   sirable for all our metrics . Our models show signif-   icant improvements in all metrics when compared   to every GPT-3 dataset , and the supervised baseline   ( see Table 2 ) . Our model shows especially large   improvements when compared to the shortest GPT-   3 summaries ( 20 - 40 % prompts ) . This suggests our   iterative procedure was able to preserve quality   better during the selection for shorter summaries .   Finally , we compare the effect of introducing a   contextual filter f. We compare two identical   training runs that differ only in the filter applied   ( filter # 2 vs. # 1 ) , and we observe small improve-   ments when including f(see last rows of Table   2 , higher values mean # 2 was better than # 1 ) .   4.1.2 Human Evaluation   We conduct a human evaluation to verify the qual-   ities of summaries from R -D . We   measure 3 axes : faithfulness ( is the summary true   to the source ? ) , relevance ( does the summary cap-   ture important information from the source ? ) and   fluency , each on a 3 - point Likert scale . We con-   duct our evaluation on 100 examples and find   agreement by Fleiss κ(Fleiss , 1971 ) of 0.32 , 0.34 ,   and 0.57 ( respectively ) indicating fair to moderate   agreement ( Landis and Koch , 1977 ) .   We compare between different methods to obtain   succinct summaries : R -D ( Iteration   3 , function # 1 ) , GPT-3 20 - 40 % , and the supervised   baseline . Following § 4.1.1 , we only compare sen-   tences if all generations differ in compression ratio   of at most 10 % . See results in Table 3 .   Broadly , we find that R -D and   GPT-3 achieve significantly higher quality than   the supervised baseline , with R -D   showing even slightly higher scores than GPT-3 for   all 3 axes . We also note that this evaluation may   somewhat favor baselines , since we only select ex-   amples where they achieved a similar compression   ratio as us . We are not accounting for the fact that   R -D can generate short summaries   for many examples the other two systems can not .   4.2 Evaluating R -C   We train all our models with n= 10 buckets to   provide a very fine - grained control : the average   sentence length in our dataset is 134 characters   long , which implies that each bucket may span ~13   characters for the average sentence . This implies a   model may only have one or two words of freedom9654   before exceeding the maximum length allowed in   a bucket . It is important to note that models do   not have direct access to a mapping of subword   token to character length , and rather need to es-   timate character length during training . We use   character length constrains since text applications   will impose this type of restriction .   We show that each R -C iter-   ation increases bucket accuracy and reduces re-   sulting compression rate variance ( See Figure 3 ,   Appendix C.2 ) . To maximize quality , in all our   experiments we only use one sampled beam . If   we wished to maximize bucket accuracy at the ex-   pense of possibly reduced quality , we can take the   top beams and select the most likely one that is   in the prompted bucket . This procedure increases   the bucket accuracy dramatically : in iteration two ,   using one beam has a 42 % bucket accuracy for the   bucket 80 - 90 % ( in a held - out set ) , whereas using   three yields an accuracy of 71 % , and five , 82 % .   This same trend holds for other iterations and buck-   ets , reaching 93 % bucket accuracy in iteration 7 .   This trade - off between bucket accuracy and sum-   mary quality that can be seen for bucket bin Table   4 , although the behavior is consistent for all buckets   ( See Appendix C.1 ) . There , Iteration 3 has slightly   higher BERTScore and ROUGE than Iteration 7 , at   the expense of lower bucket accuracy . We believe   this is because as bucket accuracy increases we   reach harder to summarize examples at the desired   length range , causing average scores to drop .   Later iterations also show more ( small ) disfluen-   cies , which we partially attribute to the aforemen-   tioned cause . Also , small disfluencies may prop-   agate over time , which we mitigate by using the   mean negative log likelihood ratio filter f .   Removing the fluency filter f will also en-   able 9 more points of bucket accuracy on average .   Importance of Iterative Distillation We trained   a GPT2 - Large model for 10 epochs with the same   seed dataset E , and compare with R -   C iteration 5 ( also trained for 10 epochs   in total ) . Bucket accuracy of R -C   was ~20 points better : for 30 - 40 % bucket , bucket   accuracy in the non - iterative version was 42 % vs.   69 % for R -C ; for 70 - 80 % bucket ,   accuracy was 34 % vs. 58 % ) .   4.2.1 Human Evaluation   We aim to explicitly test the capacity of systems   to generate an acceptable summary . That is , a   summary to meets minimum human measures of   quality , and also adheres to the desired length con-   straint . We omit the supervised baseline here , as   it does not have explicit length control , and thus   include only R -C and GPT-3 .   Specifically , we measure summary accuracy as   the fraction of summaries ( of 100 randomly pre-   selected sentences ) that adhere to length control   while being sufficiently fluent , relevant , and faith-   ful . These axes are measured as in § 4.1.2 , achiev-   ing agreement by Fleiss κ(Fleiss , 1971 ) of 0.34 ,   0.22 , and 0.25 ( respectively ) indicating fair agree-   ment ( Landis and Koch , 1977 ) . We include two   accuracy measurements : acc , requiring adhering to   length constraints as well as at least 2 ( “ fair ” ) out   of 3 on all human measures of quality ; and acc ,   which requires 3 out of 3 on all measures along   with length adherence .   Table 5 includes results for the 20 - 40 % and965540 - 60 % compression ranges , following our GPT-   3 datasets ( see C.3 for more setup details ) .   R -C vastly outperforms GPT3 for   both regimes and metrics . More precisely , for   the 40 - 60 % regime R -C showed   +296 % in accand +279 % in accwhen compared   with GPT3 ; and for the 20 - 40 % regime , R -   C obtained +68 % and +99 % respectively .   We additionally included three unsupervised sum-   marization systems in the human evaluation . These   systems perform some length control , making their   summaries comparable , but all three models per-   formed poorly when compared with R -   C : they were at least 23 points below   R -C inacc , and 17 points below   inacc .   Lastly , we would like to emphasize that   R -C aims to summarize all exam-   ples at the requested compression , regardless of the   original sentence ’s length or difficulty . GPT3 , on   the other hand , only summarizes in the requested   compression for longer sentences , which generally   correlate with easier cases ( see details in A.3 ) .   5 Related Work   Unsupervised Summarization The vast major-   ity of prior work in sentence summarization as-   sumed access to large - scale text - summary paired   datasets from which to train supervised models   ( Rush et al . , 2015 ; Nallapati et al . , 2016 ; Narayan   et al . , 2018 ) . Nonetheless , these datasets are costly   to create , and naturally - occurring summarization   datasets ( such as news highlights ) are noisy and not   easily found in other domains . Therefore , recent   work emphasized the need for developing unsu-   pervised or self - supervised methods such as au-   toencoders ( Miao and Blunsom , 2016 ; Baziotis   et al . , 2019 ) , but in general they lead to less flu-   ent summaries ; more recent work has explored   the Information Bottleneck Principle ( West et al . ,   2019 ) instead of the reconstruction loss of autoen-   coders . Our work contributes to this emerging line   of research by demonstrating an entirely different   method based on symbolic knowledge distillation .   Length - Controlled Summarization While real   world applications would require controlling for   summary length , most prior work for automatic   summarization has not proposed a principled mech-   anism for controlling the level of compression . No-   table exceptions include Kikuchi et al . ( 2016 ) and   He et al . ( 2020 ) ; Fan et al . ( 2018 ) ; Liu et al . ( 2018).These last works developed supervised models for   controllable length summarization by adding con-   trol codes that corresponded to a range of summary   lengths — commonly referred to as buckets . How-   ever , in both works the degree of control is heav-   ily dependent on the training dataset , since bucket   bounds are defined so that each one has the same   number of examples ; this may make one bucket   correspond to a wide range of compression ratios .   Our work adds a unique contribution by proposing   a reference - free method that allows for full range   of controls , and explicitly evaluates for that behav-   ior .   Concurrently to this work , Ghalandari et al . ( 2022 )   and Liu et al . ( 2022b ) ( the latter optimizing Schu-   mann et al . , 2020 ) proposed unsupervised mecha-   nisms that enforce length at the word level , either   through a reward mechanism or with strict length   enforcement . In contrast , our method uses iterative   knowledge distillation to achieve length control .   We also control length at the character level , rather   than word level . This can result in a more fair and   challenging notion of control , as it prevents per-   forming the simple strategy in which all function   words are removed first to maximize general mean-   ing . Notably , and also in contrast to our work , both   Ghalandari et al . ( 2022 ) and Liu et al . ( 2022b ) re-   quire training separate models for compressing at   different compression ratios .   Knowledge Distillation Many prior works have   focused on similar notions of transferring knowl-   edge between models through generation and dis-   tillation , and we draw particular inspiration from   West et al . ( 2022 ) . Shleifer and Rush ( 2020 ) also   follow a similar form to our work , distilling a sum-   marizer from pretrained models . Our work differs   in two key ways . First , like most distillation works ,   Shleifer and Rush ( 2020 ) assume having a model   trained for the task and with access to its full distri-   bution of token logits — both not the case of GPT-3 ,   used here . Second , like many distillation studies ,   Shleifer and Rush ( 2020 ) aim to mimic the teacher   model ’s distribution , while we attempt to improve   it . This core detail sets us apart from many works   employing a large teacher model ( Kim and Rush ,   2016 ; Schick and Schütze , 2021 ; Ye et al . , 2022 ) ,   teaching a student to mimic a distribution rather   than improve it as in our case .   Natural Language Inference ( NLI ) for Summa-   rization Pasunuru et al . ( 2017 ) ; Pasunuru and9656Bansal ( 2018 ) ; Li et al . ( 2018 ) have used NLI   for summarization enhancement : Pasunuru et al .   ( 2017 ) use entailment in multi - task learning , and   Pasunuru and Bansal ( 2018 ) ; Li et al . ( 2018 ) use   entailment probability as a reward . In this work , we   propose an alternative approach for incorporating   NLI for enhancing fidelity of summarization under   the Symbolic Knowledge Distillation framework .   6 Conclusions   We presented R , a framework for sentence   summarization that can be trained without refer-   ence summaries , while allowing direct control for   summary compression ratio . We uniquely proposed   iterative Symbolic Knowledge Distillation , where   student models from the previous iteration of distil-   lation serve as teacher models in the next . Distilled   models are significantly smaller than the original   teacher , GPT-3 , and empirical results demonstrated   that the final student models vastly outperform the   much larger GPT3 - Instruct model in terms of the   controllability of compression ratios , without com-   promising the resulting summaries ’ quality . A use-   ful by - product of this iterative distillation process is   a high - quality sentence summarization dataset with   varying degrees of compression , which we will   release jointly with our models upon publication .   Acknowledgements   We thank Vidhisha Balachandran , Alisa Liu , and   the whole Tsvetshop lab for the valuable discus-   sions . We also thank OpenAI for providing aca-   demic access to the GPT-3 API . This material is   based upon work partly funded by the DARPA   CMO under Contract No . HR001120C0124 , and   by DARPA MCS program through NIWC Pacific   ( N66001 - 19 - 2 - 4031 ) . S.K. has been supported by   a Google PhD Fellowship . Any opinions , findings   and conclusions or recommendations expressed in   this material are those of the authors and do not nec-   essarily state or reflect those of the United States   Government or any agency thereof .   Limitations   R was entirely developed and tested with   sentences extracted from news articles . More work   is needed to assess R ’s robustness when   applying it to other domains . These domains may   differ in text type , topic , or even temporal differ-   ences , that may cause a distribution shift . R ’s success is also tied to other systems ’   quality , mainly the seed dataset generator ( GPT-   3 ) and the summary fidelity filter ( operationalized   using WANLI entailments ) . R may prop-   agate errors and biases in NLI entailments , which   may be remedied in the future as NLI research pro-   gresses . We believe some edge cases generated by   R may be useful to further augment data in   NLI systems , but that investigation was outside the   scope of this paper .   R is built entirely at the sentence level ,   and more work is needed to extend it to paragraph   or document - level , although some of the same ideas   could be applied ( e.g. , control codes over longer   inputs ) .   References965796589659A Further Insights on GPT-3 Datasets   A.1 Successive Application of GPT-3 Prompts   A.2 Compression Ratio Distribution per   Dataset   A.3 Distribution of Sentence Length in Cases   Where GPT-3 Respected the Desired   Compression   We observe that the distribution of original sentence   length is markedly different when considering all   the sentences in the dataset , versus when only con-   sidering the sentences where GPT-3 ’s summary   was in the desired compression range ( see Figure   6 . This shows that GPT-3 ’s sentences compressesmore only in easier cases , whereas R -   C follows the distribution of the original   set ( see 7 ) .   B Training and Dataset Details   We use GPT2 - Large for all our fine - tuned mod-   els ( 774 M weights , ~16x smaller than GPT3-   Curie ( Radford et al . , 2019 ; Brown et al . , 2020 ) ) ,   and fine - tune for 5 epochs during each itera-   tion of R -D , and for 2 epochs for   R -C .Dconsists of 100000 sen-   tences , Dconsists of 40000 sentences each ;   Fconsists of 220000 sentences , Fconsists   of 10000 sentences each . All generated data is   decoded with sampling beam search with 5 beams.9660We balance each bucket before training each   R -C iteration to avoid overrepre-   senting some classes . We use string characters 0 to   n−1as control codes for their respective buckets .   We find that repeating the control code increases   bucket accuracy – likely because the model attends   more to these tokens – , so the end control code for   bucket 2 will be 2 2 2 2 2 2 2 2 2 2 ( ten   repetitions ) .   C Additional Details for   R -C   C.1 Performance for Additional Buckets   It is crucial to note that because variance reduces   with the number of iterations , cases where the sum-   mary did not land on the requested compression   ratio are close to fulfilling the constraint . For ex-   ample , when prompting with 30 - 40 % , 87.5 % of   the samples fell in the 20 - 40 % range for R -   C Iteration 7 ( 77 % for Iteration 3 ; 83 % for   Iteration 5).C.2 R -C Compression Ratio   Distribution per Training Iteration9661C.3 Human Evaluation Setup Details   For the 20 - 40 % range , we compare GPT-3 against   R -C conditioned for 30 - 40 % , and   accept all summaries within the same range as GPT-   3 prompts ( 20 - 40 % ) . Similarly , for 40 - 60 % we   condition R -C with 50 - 60 % , and   accept all summaries within 40 - 60 % to equalize   comparisons .   Regarding baselines , we trained Ghalandari et al .   ( 2022 ) from scratch using RealNews . Schumann   et al . ( 2020 ) was used out - of - the - box , and we use   the default training data for Liu et al . ( 2022b )   – the publicly available Schumann et al . ( 2020 )   generations – due to time constraints . All baselines   measure compression ratio at the word level , in   contrast to our work that does it at the character   level . We train with a 30 % target compression ra-   tio in baselines when comparing with the 20 - 40 %   regime , and 50 % when comparing with the 40 - 60 %   regime . This is done to maximize the number of   examples that fall in the desired range , since some   baselines enforce target compression ratio more   strictly than others .   D On BERT Score Monotonicity   In § 2.4 , we discussed that introducing control   codes allows us to solve a related question : given   we have a specific tolerance for information loss k ,   what is the shortest summary we could write ? Esti-   mating the level of information preservation with   BERT Score Recall , we could sample from each   bucket and select the shortest summary with BERT   Score Recall ≥k . BERT Score Recall scores are   generally well - ordered when ordering increasingly   by bucket : we see that on average , the longest   non - decreasing subsequence of BERT Score Re-   call scores is ~7.7 - 7.8 for all iteration steps . Having   a non - decreasing subsequence of 10 would mean   that BERT Scores are perfectly ordered ( a summary   in a higher bucket would have always higher BERT   Score recall than a lower bucket ) . 7.8 means that   scores are generally well - ordered , with some noisy   generations .   E Examples of Generations   E.1 Random sample of generations used in   the 40 - 60 % human evaluation   Original : Viggo left South America aged 11 , when   his parents divorced and he moved to upstate New   York with his mother and siblings . GPT3 : Viggo left South America when his parents   divorced . [ 41.8 % , in range ]   R -C : Viggo left South America   aged 11 , when his parents divorced him . [ 52.46 % ,   in range ]   Ghalandari et al . ( 2022 ): Viggo left South   America aged 11 when his parents divorced .   [ 48.36 % , in range ]   Liu et al . ( 2022b ): parents divorced moved to   upstate new york with his mother and siblings   [ 58.2 % , in range ]   Original : And Yahoo appears to have had another   money - losing fumble on its hands with the free   livestream of the NFL ’s Oct. 25 Bills - Jaguars   game from London .   GPT3 : Yahoo appears to have had another   money - losing fumble on its hands with the free   livestream of the NFL ’s Oct. 25 Bills - Jaguars   game from London . [ 97.3 % , 37.3 % too long ]   R -C : Yahoo appears to have had   another money - losing fumble on its hands with the   free livestream . [ 62.16 % , 2.16 % too long ]   Ghalandari et al . ( 2022 ): And Yahoo appears   to have had another money - losing fumble with   livestream Oct. 25 Bills - Jaguars . [ 64.86 % , 4.86 %   too long ]   Liu et al . ( 2022b ): and yahoo have hands of nfl   game [ 21.62 % , 18.38 % too short ]   Original : Suárez almost levelled with his latest   sweeping free - kick from distance but the clearer   chances came to Hull against a ponderous Liver-   pool defence .   GPT3 : Suarez almost levelled with his latest   sweeping free - kick from distance but Hull ’s   chances came against a ponderous Liverpool   defense . [ 91.16 % , 31.16 % too long ]   R -C : Suarez almost levelled   with his latest sweeping free - kick from distance .   [ 48.98 % , in range ]   Ghalandari et al . ( 2022 ): Suárez almost levelled   with his free - kick to Hull Liverpool . [ 40.82 % , in   range ]   Liu et al . ( 2022b ): < |unk| > from distance the   clearer chances came to hull liverpool < |unk| >   [ 48.98 % , in range ]   Original : Specifically , the company agreed to   limit pre - payments , to provide accurate estimates   of charges , and to disclose details of financing9662agreements .   GPT3 : Verizon has agreed to limit pre - payments ,   to provide accurate estimates of charges , and to   disclose details of financing agreements . [ 90.41 % ,   30.41 % too long ]   R -C : The company agreed   to limit pre - payments and to provide accurate   estimates of charges . [ 58.9 % , in range ]   Ghalandari et al . ( 2022 ): Specifically , the   company agreed to limit pre - payments to estimates   financing . [ 53.42 % , in range ]   Liu et al . ( 2022b ): company agreed to limit to   provide accurate estimates of disclose details   financing [ 56.85 % , in range ]   Original : " There was a family in the car , they got   out before it hit .   GPT3 : There was a family in the car when the car   hit a tree , and they all got out before it hit the tree .   [ 167.8 % , 107.8 % too long ]   R -C : There was a family in the   car.[50.85 % , in range ]   Ghalandari et al . ( 2022 ): " There was a family in   the car . [ 52.54 % , in range ]   Liu et al . ( 2022b ): there was a family they got   before it [ 62.71 % , 2.71 % too long ]   Original : As for the coming revolution , I ca n’t say   for certain when that will happen .   GPT3 : I ca n’t say for certain when the coming   revolution will happen . [ 82.89 % , 22.89 % too   long ]   R -C : I ca n’t say for certain when   it will happen . [ 57.89 % , in range ]   Ghalandari et al . ( 2022 ): As for the coming   revolution I ca n’t say . [ 53.95 % , in range ]   Liu et al . ( 2022b ): for the coming revolution i can   < |unk| > t say [ 59.21 % , in range ]   Original : In one New Jersey county , the Red Cross   was AWOL , says Lt . Matthew Tiedemann of the   Bergen County Office of Emergency Management .   GPT3 : In one New Jersey county , the Red Cross   was AWOL . [ 37.98 % , 2.02 % too short ]   R -C : In one New Jersey county ,   the Red Cross was AWOL , says Lt . Matthew   Tiedemann . [ 59.69 % , in range ]   Ghalandari et al . ( 2022 ): In one New Jersey   county Red Cross was AWOL says Lt . Matthew   Tiedemann . [ 55.04 % , in range ]   Liu et al . ( 2022b ): in new jersey county red crossawol lt . < |unk| > emergency management [ 52.71 % ,   in range ]   Original : Furthermore , though some might argue   that the purpose of food is merely to remind us   of our own mortality , I would argue that the best   cuisine can be an art form .   GPT3 : Food can be both utilitarian and artful .   [ 24.69 % , 15.31 % too short ]   R -C : Furthermore , I would argue   that the best cuisine can be an art form , rather than   a food source . [ 58.64 % , in range ]   Ghalandari et al . ( 2022 ): Furthermore of food   mortality I would argue that the best cuisine can be   an art form . [ 52.47 % , in range ]   Liu et al . ( 2022b ): the is our would that the best   cuisine can be an art [ 32.1 % , 7.9 % too short ]   Original : Without proper conservation facilities   nearby , the artifacts Batchvarov and his colleagues   discovered had to be reburied underwater .   GPT3 : Without proper conservation facilities   nearby , the artifacts Batchvarov and his col-   leagues discovered had to be reburied underwater .   [ 100.0 % , 40.0 % too long ]   R -C : The artifacts Batchvarov   and his colleagues discovered had to be reburied .   [ 56.06 % , in range ]   Ghalandari et al . ( 2022 ): Without conservation   artifacts Batchvarov had to reburied underwater   [ 51.52 % , in range ]   Liu et al . ( 2022b ): conservation facilities artifacts   colleagues discovered to be reburied underwater   [ 61.36 % , 1.36 % too long ]   Original : Often people are desperate or ridiculous   rather than heroic , so I do what needs to be done . ”   GPT3 : That person is heroic . [ 23.91 % , 16.09 %   too short ]   R -C : Often people are desperate   or ridiculous , so I do it . [ 57.61 % , in range ]   Ghalandari et al . ( 2022 ): Often people are   desperate or ridiculous heroic I. [ 54.35 % , in   range ]   Liu et al . ( 2022b ): so i do what needs to be done   < |unk| > [ 40.22 % , in range ]   Original : Liam ’s feet make a guest appearance in   the picture , as does his Great Dane Watson who   he bought with ex - girlfriend Sophia Smith can be   seen by his settee.9663GPT3 : Liam ’s feet and his Great Dane Watson can   be seen by his settee in the picture . [ 51.3 % , in   range ]   R -C : Liam ’s feet make a guest   appearance in the picture , as does his Great Dane   Watson . [ 53.25 % , in range ]   Ghalandari et al . ( 2022 ): Liam ’s feet make a guest   appearance in the picture as Great Dane Watson   ex - girlfriend settee . [ 60.39 % , 0.39 % too long ]   Liu et al . ( 2022b ): < |unk| > feet picture does dane   watson he bought with ex - girlfriend sophia smith   be < |unk| > [ 58.44 % , in range ]   Original : I like this little gallery , the Small Weston   Room , where the paintings go right up to the   ceiling .   GPT3 : I like this little gallery , the Small Weston   Room . [ 51.02 % , in range ]   R -C : I like this little gallery , the   Small Weston Room . [ 51.02 % , in range ]   Ghalandari et al . ( 2022 ): I like this little gallery   Small Weston Room where paintings ceiling .   [ 70.41 % , 10.41 % too long ]   Liu et al . ( 2022b ): i like room where the go right   to the ceiling [ 45.92 % , in range ]   Original : After a stint as a lobbyist , during which   he helped establish two think tanks affiliated with   James Dobson ’s hugely influential Focus on the   Family , Parker became a confidant of Roy Moore ,   then a county judge .   GPT3 : After a stint as a lobbyist , during which   he helped establish two think tanks affiliated with   James Dobson ’s hugely influential Focus on the   Family , Parker became a confidant of Roy Moore ,   then a county judge . [ 100.0 % , 40.0 % too long ]   R -C : After a stint as a lobbyist ,   Parker became a confidant of Roy Moore , then a   county judge , during a stint . [ 50.24 % , in range ]   Ghalandari et al . ( 2022 ): After a stint as a lobbyist   helped think tanks James Dobson , Parker became   a confidant of Roy Moore . [ 47.85 % , in range ]   Liu et al . ( 2022b ): a a lobbyist s focus on   the family parker became confidant roy moore   [ 32.54 % , 7.46 % too short ]   Original : Germany will be the favourites but it is   wide open between the other three teams - Poland   and Ukraine are a similar standard to Romania and   Hungary , who we came up against in qualifying .   GPT3 : Germany will be the favourites , but it iswide open between the other three teams . [ 44.09 % ,   in range ]   R -C : Germany will be the   favourites , but it is wide open between the other   three teams . [ 44.09 % , in range ]   Ghalandari et al . ( 2022 ): Germany will be the   favourites but it is wide open between the other   three teams - Poland and Ukraine . [ 54.84 % , in   range ]   Liu et al . ( 2022b ): wide between the other three   teams poland and are romania hungary we came   [ 39.25 % , 0.75 % too short ]   Original : It is the latest mass killing attributed to   the extremists , activists said , with the violence   coming during a 24 - hour period .   GPT3 : The latest mass killing attributed to the   extremists occurred during a 24 - hour period .   [ 68.8 % , 8.8 % too long ]   R -C : It is the latest mass killing ,   activists said , with the violence coming . [ 57.6 % ,   in range ]   Ghalandari et al . ( 2022 ): It is the latest mass   killing attributed to the extremists 24 - hour .   [ 53.6 % , in range ]   Liu et al . ( 2022b ): is the latest mass killing   attributed to the extremists [ 44.0 % , in range ]   Original : The brutal honesty and reality of death   speaks to the truth and tragedy of bad timing for   these two characters . ”   GPT3 : The brutal honesty and reality of death   speaks to the truth and tragedy of bad timing for   these two characters . [ 99.11 % , 39.11 % too long ]   R -C : The brutal honesty and   reality of death speaks to the truth . [ 53.57 % , in   range ]   Ghalandari et al . ( 2022 ): The brutal honesty and   of death speaks to timing characters . [ 53.57 % , in   range ]   Liu et al . ( 2022b ): the reality of death the truth and   of bad timing characters [ 52.68 % , in range ]   Original : It ’s a nice touch of confectionerial   twinning , and broadly chimes with the optional   theme of this year ’s show , ’ Man Made ’ , which   judging by the parrots and skyscapes and severed   heads and moths has not been taken up with   widespread enthusiasm .   GPT3 : The show ’s optional theme is ’ Man Made ’ .   [ 16.46 % , 23.54 % too short]9664R -C : It ’s a nice touch of con-   fectionery and broadly chimes with the optional   theme of this year ’s show , " Man Made " , which has   not been taken up . [ 57.2 % , in range ]   Ghalandari et al . ( 2022 ): It ’s a nice touch of   confectionerial twinning and chimes with theme of   this year ’s show ’ Man Made parrots . [ 43.62 % , in   range ]   Liu et al . ( 2022b ): it ’s a nice touch of < |unk| >   severed [ 15.23 % , 24.77 % too short ]   Original : Vicis , a finalist for the 2016 GeekWire   Awards in the Hardware / Gadget of the Year   category , will use the fresh cash for production and   commercialization of its ZERO1 helmet for the   upcoming football season .   GPT3 : Vicis , a finalist for the 2016 GeekWire   Awards in the Hardware / Gadget of the Year   category , will use the fresh cash for production and   commercialization of its ZERO1 helmet . [ 83.98 % ,   23.98 % too long ]   R -C : Vicis , a finalist for the 2016   GeekWire Awards , will use the fresh cash for   production and commercialization . [ 52.91 % , in   range ]   Ghalandari et al . ( 2022 ): Vicis , a finalist for 2016   GeekWire Awards / Gadget will use fresh cash for   its ZERO1 helmet . [ 44.17 % , in range ]   Liu et al . ( 2022b ): a finalist for the of the year   helmet the season [ 23.3 % , 16.7 % too short ]   Original : The cloud - based system has to wait   for a connection to download updates to apps   and content , but the content also has to be stored   locally for uninterrupted playback .   GPT3 : The cloud - based system has to wait for   a connection to download updates to apps and   content , but the content also has to be stored   locally for uninterrupted playback . [ 100.0 % ,   40.0 % too long ]   R -C : The cloud - based system has   to wait for a connection to download updates to   apps and content . [ 55.42 % , in range ]   Ghalandari et al . ( 2022 ): The cloud - based system   has to wait for a connection to download updates   to apps . [ 48.19 % , in range ]   Liu et al . ( 2022b ): the < |unk| > system has to wait   for a download content stored locally [ 40.96 % , in   range ]   Original : It signals toughness and finality , yet itsmeaning is elusive .   GPT3 : The article does not provide a clear   explanation of what the phrase " yet its meaning is   elusive " means . [ 166.13 % , 106.13 % too long ]   R -C : It signals toughness and   finality . [ 54.84 % , in range ]   Ghalandari et al . ( 2022 ): It signals toughness and   finality [ 53.23 % , in range ]   Liu et al . ( 2022b ): it toughness its meaning is   elusive [ 56.45 % , in range ]   E.2 Random sample of generations used in   the 20 - 40 % human evaluation   Original : Viggo left South America aged 11 , when   his parents divorced and he moved to upstate New   York with his mother and siblings .   GPT3 : Viggo left South America when he was 11 .   [ 32.79 % , in range ]   R -C : Viggo left South America   aged 11 , when his parents . [ 41.8 % , 1.8 % too   long ]   Ghalandari et al . ( 2022 ): Viggo left South   America aged 11 parents divorced . [ 40.98 % ,   0.98 % too long ]   Liu et al . ( 2022b ): parents moved to upstate new   york [ 27.05 % , in range ]   Original : And Yahoo appears to have had another   money - losing fumble on its hands with the free   livestream of the NFL ’s Oct. 25 Bills - Jaguars   game from London .   GPT3 : Yahoo lost money streaming the NFL game   from London . [ 35.14 % , in range ]   R -C : Yahoo appears to have had   another money - losing fumble . [ 36.49 % , in range ]   Ghalandari et al . ( 2022 ): And Yahoo had another   money - losing fumbles Bills - Jaguars [ 37.84 % , in   range ]   Liu et al . ( 2022b ): yahoo to have hands the of the   nfl[22.97 % , in range ]   Original : Suárez almost levelled with his latest   sweeping free - kick from distance but the clearer   chances came to Hull against a ponderous Liver-   pool defence .   GPT3 : Hull scored against Liverpool . [ 20.41 % , in   range ]   R -C : Suarez almost levelled with   his latest free - kick . [ 33.33 % , in range ]   Ghalandari et al . ( 2022 ): Suárez almost levelled9665with free - kick Hull Liverpool [ 35.37 % , in range ]   Liu et al . ( 2022b ): distance chances came to hull   liverpool [ 26.53 % , in range ]   Original : Specifically , the company agreed to   limit pre - payments , to provide accurate estimates   of charges , and to disclose details of financing   agreements .   GPT3 : The company agreed to limit pre - payments ,   to provide accurate estimates of charges , and to   disclose details of financing agreements . [ 90.41 % ,   50.41 % too long ]   R -C : The company agreed   to limit pre - payments and to provide accurate   estimates . [ 51.37 % , 11.37 % too long ]   Ghalandari et al . ( 2022 ): Specifically company   agreed to limit pre - payments . [ 34.25 % , in range ]   Liu et al . ( 2022b ): company to provide accurate   disclose financing [ 31.51 % , in range ]   Original : " There was a family in the car , they got   out before it hit .   GPT3 : There was a family in the car , they got out   before it hit . [ 98.31 % , 58.31 % too long ]   R -C : " There was a family . "   [ 35.59 % , in range ]   Ghalandari et al . ( 2022 ): " There was family car   [ 35.59 % , in range ]   Liu et al . ( 2022b ): there was a it [ 23.73 % , in   range ]   Original : As for the coming revolution , I ca n’t say   for certain when that will happen .   GPT3 : I do n’t know when the revolution will   happen . [ 59.21 % , 19.21 % too long ]   R -C : I ca n’t say for sure .   [ 27.63 % , in range ]   Ghalandari et al . ( 2022 ): As for the coming   revolution [ 36.84 % , in range ]   Liu et al . ( 2022b ): the revolution i can certain   [ 36.84 % , in range ]   Original : In one New Jersey county , the Red Cross   was AWOL , says Lt . Matthew Tiedemann of the   Bergen County Office of Emergency Management .   GPT3 : The Red Cross was not present in one New   Jersey county . [ 42.64 % , 2.64 % too long ]   R -C : In one New Jersey county ,   the Red Cross was AWOL . [ 37.98 % , in range ]   Ghalandari et al . ( 2022 ): In New Jersey Red Cross   was AWOL [ 24.81 % , in range]Liu et al . ( 2022b ): new jersey county red cross   awol lt . [ 27.91 % , in range ]   Original : Furthermore , though some might argue   that the purpose of food is merely to remind us   of our own mortality , I would argue that the best   cuisine can be an art form .   GPT3 : I think that food can be an art form .   [ 22.84 % , in range ]   R -C : Furthermore , I would argue   that the best cuisine can be an art form . [ 41.98 % ,   1.98 % too long ]   Ghalandari et al . ( 2022 ): Furthermore of food   mortality the best cuisine art form . [ 34.57 % , in   range ]   Liu et al . ( 2022b ): our mortality the best cuisine   can be an art [ 27.16 % , in range ]   Original : Without proper conservation facilities   nearby , the artifacts Batchvarov and his colleagues   discovered had to be reburied underwater .   GPT3 : The artifacts Batchvarov and his col-   leagues discovered had to be reburied underwater .   [ 64.39 % , 24.39 % too long ]   R -C : The artifacts had to be   reburied . [ 25.0 % , in range ]   Ghalandari et al . ( 2022 ): Without artifacts   Batchvarov reburied underwater [ 36.36 % , in   range ]   Liu et al . ( 2022b ): artifacts to be reburied   underwater [ 26.52 % , in range ]   Original : Often people are desperate or ridiculous   rather than heroic , so I do what needs to be done . ”   GPT3 : The protagonist does what needs to be   done . [ 46.74 % , 6.74 % too long ]   R -C : I do what needs to be done .   [ 29.35 % , in range ]   Ghalandari et al . ( 2022 ): Often people are   desperate heroic . [ 36.96 % , in range ]   Liu et al . ( 2022b ): do what to be done [ 19.57 % ,   0.43 % too short ]   Original : Liam ’s feet make a guest appearance in   the picture , as does his Great Dane Watson who   he bought with ex - girlfriend Sophia Smith can be   seen by his settee .   GPT3 : There are pictures of Liam and his dog   Watson in the article . [ 39.61 % , in range ]   R -C : Liam ’s feet make a guest   appearance in the picture . [ 33.12 % , in range]9666Ghalandari et al . ( 2022 ): Liam ’s feet make guest   appearance in picture Dane Watsongirlfriend .   [ 43.51 % , 3.51 % too long ]   Liu et al . ( 2022b ): dane watson who bought   ex - girlfriend sophia smith can be [ 36.36 % , in   range ]   Original : I like this little gallery , the Small Weston   Room , where the paintings go right up to the   ceiling .   GPT3 : I like this little gallery . [ 27.55 % , in range ]   R -C : I like this little gallery .   [ 27.55 % , in range ]   Ghalandari et al . ( 2022 ): I like gallery Small   Weston Room paintings [ 42.86 % , 2.86 % too   long ]   Liu et al . ( 2022b ): go right to the ceiling [ 23.47 % ,   in range ]   Original : After a stint as a lobbyist , during which   he helped establish two think tanks affiliated with   James Dobson ’s hugely influential Focus on the   Family , Parker became a confidant of Roy Moore ,   then a county judge .   GPT3 : Parker became a confidant of Roy Moore   and helped him win election as a county judge .   [ 40.67 % , 0.67 % too long ]   R -C : Parker became a confidant   of Roy Moore , then a county judge , during a stint .   [ 36.36 % , in range ]   Ghalandari et al . ( 2022 ): After as lobbyist Dobson ,   Parker became a confidant of Roy Moore . [ 31.1 % ,   in range ]   Liu et al . ( 2022b ): a a lobbyist dobson focus the   family parker became a [ 24.88 % , in range ]   Original : Germany will be the favourites but it is   wide open between the other three teams - Poland   and Ukraine are a similar standard to Romania and   Hungary , who we came up against in qualifying .   GPT3 : Germany is the favourite to win the World   Cup , but it ’s still wide open . [ 38.71 % , in range ]   R -C : Germany is the favourite ,   but it is wide open between the other three teams .   [ 40.86 % , 0.86 % too long ]   Ghalandari et al . ( 2022 ): Germany will be the   favourites but between teams Poland Ukraine .   [ 34.41 % , in range ]   Liu et al . ( 2022b ): teams poland and ukraine   standard romania hungary we came qualifying   [ 36.56 % , in range]Original : It is the latest mass killing attributed to   the extremists , activists said , with the violence   coming during a 24 - hour period .   GPT3 : Extremists have killed more people in 24   hours than ever before . [ 51.2 % , 11.2 % too long ]   R -C : It is the latest mass killing ,   activists said . [ 36.8 % , in range ]   Ghalandari et al . ( 2022 ): It is latest mass killing   attributed extremists [ 37.6 % , in range ]   Liu et al . ( 2022b ): latest mass killing attributed to   extremists [ 35.2 % , in range ]   Original : The brutal honesty and reality of death   speaks to the truth and tragedy of bad timing for   these two characters . ”   GPT3 : Death speaks to the truth and tragedy   of bad timing in the characters ’ lives . [ 68.75 % ,   28.75 % too long ]   R -C : The brutal honesty and   reality of death . [ 35.71 % , in range ]   Ghalandari et al . ( 2022 ): The brutal honesty death   speaks characters . [ 38.39 % , in range ]   Liu et al . ( 2022b ): the reality the truth timing   characters [ 34.82 % , in range ]   Original : It ’s a nice touch of confectionerial   twinning , and broadly chimes with the optional   theme of this year ’s show , ’ Man Made ’ , which   judging by the parrots and skyscapes and severed   heads and moths has not been taken up with   widespread enthusiasm .   GPT3 : The exhibit has a theme . [ 9.88 % , 10.12 %   too short ]   R -C : It ’s a nice touch of confec-   tionery , and it ’s also a nice theme of this year ’s   show . [ 34.16 % , in range ]   Ghalandari et al . ( 2022 ): It ’s a nice touch   of confectionerial twinning show ’ Man Made .   [ 25.1 % , in range ]   Liu et al . ( 2022b ): it ’s a touch of the [ 8.23 % ,   11.77 % too short ]   Original : Vicis , a finalist for the 2016 GeekWire   Awards in the Hardware / Gadget of the Year   category , will use the fresh cash for production and   commercialization of its ZERO1 helmet for the   upcoming football season .   GPT3 : Vicis will use the fresh cash to produce and   commercialize its ZERO1 helmet . [ 36.89 % , in   range]9667R -C : Vicis will use the fresh cash   for production and commercialization . [ 32.52 % ,   in range ]   Ghalandari et al . ( 2022 ): Vicis finalist GeekWire   Awards / Gadget will use cash its ZERO1 helmet   [ 33.01 % , in range ]   Liu et al . ( 2022b ): a finalist the of the year category   helmet the season [ 25.73 % , in range ]   Original : The cloud - based system has to wait   for a connection to download updates to apps   and content , but the content also has to be stored   locally for uninterrupted playback .   GPT3 : Cloud - based systems have to wait for   updates , but the content can be stored locally for   uninterrupted playback . [ 66.87 % , 26.87 % too   long ]   R -C : The cloud - based system has   to wait for a connection . [ 31.33 % , in range ]   Ghalandari et al . ( 2022 ): The cloud - based system   has to wait for download updates . [ 33.73 % , in   range ]   Liu et al . ( 2022b ): < |unk| > has wait for a download   content locally [ 28.31 % , in range ]   Original : It signals toughness and finality , yet its   meaning is elusive .   GPT3 : The sign means something but we do n’t   know what it is . [ 87.1 % , 47.1 % too long ]   R -C : It ’s a mystery . [ 24.19 % , in   range ]   Ghalandari et al . ( 2022 ): It signals toughness   finality [ 46.77 % , 6.77 % too long ]   Liu et al . ( 2022b ): signals toughness elusive   [ 40.32 % , 0.32 % too long]9668