  Archiki PrasadTrung BuiSeunghyun YoonHanieh Deilamsalehy   Franck DernoncourtMohit BansalUNC Chapel HillAdobe Research   Abstract   With the ubiquitous use of online meeting plat-   forms and robust automatic speech recognition   systems , meeting transcripts have emerged as   a promising domain for natural language tasks .   Most recent works on meeting transcripts pri-   marily focus on summarization and extraction   of action items . However , meeting discussions   also have a useful question - answering ( QA )   component , crucial to understanding the dis-   course or meeting content , and can be used to   build interactive interfaces on top of long tran-   scripts . Hence , in this work , we leverage this   inherent QA component of meeting discussions   and introduce M QA , an extractive QA   dataset comprising of questions asked by meet-   ing participants and corresponding responses .   As a result , questions can be open - ended and   actively seek discussions , while the answers   can be multi - span and distributed across mul-   tiple speakers . Our comprehensive empiri-   cal study of several robust baselines includ-   ing long - context language models and recent   instruction - tuned models reveals that models   perform poorly on this task ( F1 = 57.3 ) and   severely lag behind human performance ( F1   = 84 .6 ) , thus presenting a challenging new   task for the community to improve upon .   1 Introduction   Millions of meetings occur every day worldwide ,   which results in vast amounts of meeting tran-   scripts . Meeting transcripts are typically long doc-   uments , often domain - specific depending on the   subject matter , and contain a lot of information .   Basic tasks such as catching up with a missed meet-   ing , looking up a specific discussion or response to   a query can be time - consuming . These tasks can be   facilitated by NLP systems , including summariza-   tion and question - answering . To this end , several   publicly available small - scale corpora of meetingFigure 1 : Representative example from meeting tran-   script segment in M QA . The question and anno-   tated answer are highlighted in red and blue respectively .   transcripts have been released ( Carletta et al . , 2005 ;   Janin et al . , 2003 ; Garofolo et al . , 2004 , inter alia ) .   Prior NLP work on meeting transcripts mainly   focuses on summarization ( Oya et al . , 2014 ; Li   et al . , 2019 ; Zhu et al . , 2020 , inter alia ) . However ,   lack of annotated data impedes research on other   important NLP tasks in this domain . To address   this gap , we introduce a question - answering ( QA )   task based on conversations in meeting transcripts .   Specifically , we consider questions asked by par-   ticipants during the meeting and aim to extract cor-   responding answer spans from relevant discussions   among meeting participants ( refer to Figure 1 ) .   This task has several practical applications such as   building an interactive meeting browser / interface   for navigating through transcripts and informing   tasks such as meeting summarization and handling   action items involving QA pairs ( Kathol and Tur ,   2008 ; August et al . , 2022).15000While standard QA datasets consist of human   generated questions either based on short supplied   contexts ( Rajpurkar et al . , 2016 , 2018 ; Rogers et al . ,   2021 ) or are answered using a large collection of   documents ( Joshi et al . , 2017 ; Kwiatkowski et al . ,   2019 ; Zhu et al . , 2021b ) , our task setting is chal-   lenging yet interesting in several ways . First , meet-   ing transcripts are long documents and QA systems   still struggle to understand long contexts ( Pang   et al . , 2022 ; Soleimani et al . , 2021 ) . Second , suc-   cessfully answering questions asked within meet-   ings requires robust understanding of the conver-   sation and discourse that takes place both before   and after a question . Third , the multi - party spoken   text falls under a different domain when compared   to typical text documents . While standard long   documents rarely include any meaningful ( non-   rhetorical ) questions , multi - party meetings often   involve discussions asked by one participant and   answered by the rest , allowing us to use these ques-   tions to create a QA dataset . Furthermore , the   conversational nature of transcribed text differs   from written documents and may contain disflu-   encies and other artifacts . Finally , instead of using   annotator - generated questions ( like in Wu et al .   ( 2022 ) ) , questions asked by participants are more   open - ended and discussion - seeking , with interest-   ing answer types that can be multi - span and/or con-   tributed by multiple speakers ( e.g. , Figure 1 ) .   To this end , we first introduce our dataset M - QA , created by annotating meetings transcripts   from the popular AMI ( Augmented Multi - party   Interaction ) corpus , containing over 100 hours of   meetings ( Carletta et al . , 2005 ) , via a robust anno-   tation pipeline . M QAcomprises of 7,735   questions asked by participants across 166 meet-   ings . Unlike other datasets , questions in M - QAare less concise ( 12 words on average ) and   reflect queries asked in a conversational setting .   The answers include realistic situations such as   rhetorical questions , multiple discontiguous spans   and/or contributions from multiple speakers .   Next , on M QAdataset , we test diverse   models designed for long input contexts such as   Longformer ( Beltagy et al . , 2020 ) , and BigBird ( Za-   heer et al . , 2020 ) as well as RoBERTa ( Liu et al . ,   2019 ) , and DeBERTa - v3 ( He et al . , 2020 ) with as   much meeting context surrounding the question as   possible . To incorporate the multi - span nature of   answers in our dataset , we design and experiment   with multi - span variants of the aforementionedmodels . Furthermore , we also investigate how well   recent instruction - tuned large language models fare   at answering questions from M QA . Lastly ,   we create a silver - annotation pipeline using M - S(Zhu et al . , 2021a ) , a corpus containing   463.6 K short interview transcripts , to provide ad-   ditional training data . We find that the best per-   formance is achieved by finetuned short - context   models ( F1 = 57.3 ) . Overall , we show that models   struggle to identify rhetorical questions and select-   ing which utterances constitute the answer . Thus ,   model performance significantly trails behind hu-   man performance on M QA(F1= 84.6 ) ,   leaving a large potential for future improvements   on this challenging task .   2 Our Dataset : M QA   We first describe our data collection process in   Section 2.1 and then provide an extensive analysis   of M QA in Section 2.2 .   2.1 Data Collection   Question Selection . We leverage the punctuated   text to identify possible questions ( ending with ‘ ? ’ ) .   We also filter out questions containing ≤2words   as we manually find them to be either meaningless   or rhetorical . While questions are marked to facili-   tate annotators , we encourage them to find missed   potential questions due to incorrect punctuation .   Answer Annotation . For each possible question ,   we ask annotators to label the set of sentences ( each   identified by a unique number ) from the meeting   transcript that form the answer . Additionally , we   also collect meta - data about the question . First , we   ask the annotators to label if the question was mean-   ingful , used to filter out rhetorical , unanswered   or logistical questions and incorrect punctuations .   Some speakers can ask consecutive or multiple   questions in the same turn that are often related   and answered together . In such scenarios , we al-   low annotators to combine questions and provide a   common answer from the meeting transcript . The   annotators mark these questions using the com-   bined question attribute . Finally , since our ques-   tions are conversation segments , they may not be   self - contained . Hence , we ask annotators to men-   tion the question context sentences ( if any ) sepa-   rately . We refer readers to Appendix A for more   details and examples from M QA.15001   Annotation Process . All annotators were   hired by a professional crowdsourcing company   TELUS.The company obtained consent from the   crowd workers and conducted ethical reviews . To   train annotators , we provide comprehensive in-   structions for each type of annotation with several   manually annotated examples from a small subset   of transcripts and different possible scenarios   curated by the first author . The annotations were   collected in multiple batches , starting with the first   batch containing a small subset of 250 questions .   We iteratively provided extensive feedback to the   crowdworkers on their annotations and resolved   existing issues till the annotations were satisfactory .   Next , we assigned three independent annotators   to each question , and calculated Krippendorff ’s   α= 0.73(Krippendorff , 1980 ) using MASI-   distance ( Passonneau , 2006 ) , indicating substantial   agreement . We then collected annotations for the   remaining questions in two additional batches   using one annotator per question followed by a   quality assurance stage to validate the outcome of   the annotations . Overall , we spent $ 10,427 in the   annotation process , amounting to $ 61 per meeting .   For additional details refer to Appendix A.   2.2 Dataset Information and Analysis   After filtering and quality control , we were left   with a total of 7,735 questions from 166 meetings   ( ≈100hours of meeting recordings ) .   Size and Splits . We split our dataset into train ,   dev , and test sets such that questions in each split   come from distinct meetings . Table 1 shows dataset   statistics across different answer types , namely   unanswerable , multi - span , and multi - speaker ( de-   scribed below ) . Due to relatively small number of   meetings in the AMI corpus and diversity in meet-   ing content , our test set contains a larger fraction   of questions from the dataset as opposed to the con-   ventional 80:10:10 split across train / dev / test sets .   Question Types . Unlike most QA datasets , ques-   tions in M QAare extracted directly from   the meeting transcripts . Consequently , we find that   questions may not be concise , and may not begin   with ‘ wh ’ prefixes , making our dataset challenging   yet interesting for the community . We perform a   manual analysis of question types based on 200   randomly selected questions from the test set in   Figure 2 ( left ) . First , we observe that a majority of   questions in M QAare framed in a ‘ yes / no ’   manner , followed by ‘ what ’ and ‘ how ’ questions   that are typically information - seeking . We find that   in a discussion - heavy setting such as ours , yes / no   questions elicit a detailed response that can not be   reduced to a direct ‘ yes / no ’ response in over 40 %   of the cases ( see Figure 2 ( right ) ) . Further , manual   analysis shows that nearly half the questions are   subjective , i.e. , seeking opinions of meeting partic-   ipants , and as high as 20 % of answerable questions15002   are framed rhetorically . Appendix A contains addi-   tional tri - gram - based analysis of questions .   Length . Figure 3 shows the distribution of the   length of meeting transcripts , questions , and an-   swers in M QA . On average , each meeting   transcript comprises of 5.8 K words which consti-   tute as long documents unlikely to fit entirely in the   input context of typical pretrained language mod-   els ( Devlin et al . , 2019 ; Liu et al . , 2019 ; He et al . ,   2020 ) . Further , questions and their answers contain   an average of 12 , and 35 words respectively .   Answer Types . Due to the nature of meeting   conversations and questions asked by participants ,   most answers are direct responses or follow - up dis-   cussions . However , some questions are rhetorical   or do not elicit any discussion . These questions are   unanswerable ( 30 % ofM QA ) . Among an-   swerable questions , we note two scenarios of inter-   est : multi - span andmulti - speaker answers . Multi-   span answers contain non - consecutive and discon-   tinuous utterances or sentences , typically in the   form of relevant discussion interleaved with irrele-   vant chit - chat ( see examples in Appendix A ) . Addi-   tionally , multi - speaker answers occur when multi-   ple participants contribute to answering a question   which is typical in a discussion . Note that multi-   speaker and multi - span answer cases are not mu-   tually exclusive ( refer to Figure 1 for an example ) .   We find that 40 % of all answers ( excluding unan-   swerable questions ) in our dataset are multi - span   and48 % of answers are multi - speaker in nature .   Moreover , Figure 2 ( right ) shows from our manual   analysis that a considerable amount of disagree-   ment exists among speakers in multi - speaker an-   swers , with approximately 70 % of cases displaying   some form of disagreement . Notably , 22 % of an-   swers involve additional follow - up or action items ,   which are specific to the context of meetings . Human Performance . We estimate human per-   formance on M QAusing a random sub-   sample of 250 questions from the test split . Each   question is assigned a different annotator who had   not previously annotated the meeting containing   that question . Scoring the provided answers rela-   tive to the reference answers in our dataset , yields   an F1 of 84.6 . This breaks down to F1 of 80.7and   86.3for unanswerable and answerable questions   respectively . The F1 score for multi - span and multi-   speaker answers is 88.1and87.7respectively .   3 Methods   In this section , we investigate the difficulty level   of our new M QAfor state - of - the - art QA   systems and establish strong baseline results . We   describe strategies for retrieving contexts from tran-   scripts in Section 3.1 , followed by different QA   models in Section 3.2 , and silver data annotation   for data augmentation methods in Section 3.3 .   3.1 Retrieving Contexts from Transcripts   Given that meeting transcripts are very long docu-   ments , it is infeasible to input the entire transcript   as context to typical QA models . Thus , we first   select a smaller transcript segment that fits the   model ’s input length limitations . We explore two   strategies to retrieve contexts as described below .   Location - based Context Retrieval . We use the   relative location of the question in the meeting tran-   script to retrieve a context by fitting as many ( com-   plete ) sentences as possible under a fixed length   budget ( measured in words ) . Further , we split this   budget into two components : prefix andsuffix refer-   ring to the sentences that precede and succeed the   question respectively . We set the prefix budget to   50words and the suffix budget to 250words respec-   tively , resulting in a total budget of 300words.15003   Note that the suffix budget is significantly larger   than the prefix budget since we expect to find an-   swers in sentences following the question . The   sentences before the question only provide addi-   tional context to the ongoing discussion .   Score - based Context Retrieval . Alternatively ,   we use the question as a query and compare it to   other sentences from the entire transcript via two   scoring methods consistent with Pang et al . ( 2021 ) .   First , we retrieve sentences using ROUGE-1 score   relative to the question . Second , we use cosine   similarity based on sentence embeddings ( Reimers   and Gurevych , 2019 ) . We concatenate sentences in   the order they appear in the transcript until reaching   the total length budget . Similar to location - based   retrieval , we set the total budget to 300words .   Results of Context Retrieval . Table 2 com-   pares both retrieval methods using the same total   length budget on the answerable questions split .   We observe that the sentence - level overlap be-   tween extracted contexts and annotated answers   for score - based retrieval is significantly lower than   for location - based retrieval . We use this overlap   to compute the maximum achievable performance   of QA systems for each type of retrieval . Corre-   spondingly , we find similar trends in upper - bound   performance metrics ( discussed in Section 4 ) with   location - based contexts ( near - perfect max F1 ) con-   siderably outperforming score - based contexts ( max   F1<40 ) . Therefore , for short - context models , we   henceforth use location - based contexts .   3.2 Models for Meeting - based QA   We primarily focus on extractive models includ-   ing both short and long - context models . Given   the transcript or a segment from it ( context ) and   the question , models are tasked with extracting   answer - span(s ) from the context . We use two high-   performing short - context models RoBERTa and   DeBERTaV3 , each supporting up to 512 tokens , with extracted context from Section 3.1 . Addition-   ally , we explore Longformer and BigBird which   support longer sequences of up to 4096 tokens   by utilizing a combination of sliding window and   global attention mechanisms . Further , the Long-   former Encoder - Decoder ( LED ) model supports up   to 16,384 input tokens . These models allow us to   use most or all portions of the transcript needed   for answering the questions as the context . In case   of an overflow , we use as many utterances from   the transcript around the question as possible and   truncate the rest . Note that these models output   a single answer - span by default . Therefore , for   multi - span answers , we train models to predict a   span starting with first utterance and ending with   the last utterance of the gold answer .   Multi - Span Models . In order to better model   multi - span answers , we follow Segal et al . ( 2020 )   and pose multi - span QA as a sequence tagging task ,   predicting if each token in the context is part of the   answer . For simplicity , we restrict ourselves to their   proposed IOtagging . Thus , the answer prediction   is a concatenation of all token - spans contiguously   tagged with I. Similar to single - span models , we   train multi - span variants of RoBERTa , DeBERTa ,   Longformer , and BigBird models .   Instruction - Tuned Models . Furthermore , we   useF - T5 ( Chung et al . , 2022 ) , a publicly-   available instruction - tuned model , to study zero-   shot performance on our M QA . Given the   relatively large size of contexts and distinct nature   of our task , we rely on succinct instructions in-   stead of few - shot demonstrations . Furthermore ,   due to the model ’s generative nature , we can not   directly use the predictions for our extractive QA   task . Therefore , we adapt instruction - tuned models   for our setting by employing instructions that ask   models to list sentences instead of directly generat-   ing answers that may be less faithful to the context .   Next , we filter out predicted sentences not present   in the context . While this is a strict selection crite-   rion , it removes any possible hallucinations .   3.3 Silver Data Augmentation   Due to high annotation costs of gold labels , and   unavailability of similar QA datasets , we investi-   gate automatic methods to annotate answers . We   match the salient features of M QA , such   as meaningful questions within the transcript and15004   multi - speaker discussions using the M S   dataset ( Zhu et al . , 2021a ) . This dataset contains   463.6 K short multi - party interview transcripts , de-   tailed speaker information , and identifies a host or   interviewer who steers the discussion via questions .   We begin by identifying the host speaker and fo-   cusing on their questions . Next , we predict which   speaker(s ) would answer the question by identify-   ing speaker entities mentioned in utterances or from   previous dialogue turns . Finally , we search utter-   ances from the identified speakers until a stopping   criterion is met and label it as the answer . Due to   the assumptions made in the above process , models   trained directly on this data could overfit on spuri-   ous correlations ( Jia and Liang , 2017 ; Wang and   Bansal , 2018 ) . Thus , we apply various perturba-   tions to the context such as separating the question   and answer utterances , converting to unanswerable   questions by removing relevant sentences , creat-   ing more speaker transitions , and masking speaker   names . Refer to Appendix F for additional details .   4 Experiments and Results   Evaluation Metrics . Following Rajpurkar et al .   ( 2016 ) we report macro - averaged F1 on the entire   test set as well as on specific answer types ( Sec-   tion 2.2).However , F1 treats sequences as bag-   of - words , and thus , there can be a non - significantoverlap between a random span and the target span   for large span lengths . To address this , Soleimani   et al . ( 2021 ) propose reporting Intersection - over-   Union ( IoU ) defined as :   IoU = |p∩t|   |p∪t| ,   where pandtare the predicted and target spans , re-   spectively . Since our answer spans are much longer   than those in SQuAD ( refer to Figure 3 ) , we also re-   port macro - averaged IoU to measure performance .   Training Settings . We measure performance of   various models in both finetuned and zero - shot set-   tings . First , we directly finetune the base pretrained   model on the model on M QA . Next , to   supplement training data we explore intermediate-   training ( Phang et al . , 2018 ; Pruksachatkun et al . ,   2020 ) with SQuAD v2.0 ( Rajpurkar et al . , 2018 )   or a combination including silver data from Sec-   tion 3.3 prior to finetuning on M QA , in-   creasing the training data by 5x and 10x respec-   tively . Additional details on checkpoints , hyperpa-   rameters , and training are present in Appendix B.   Turn - based Baseline . We devise a straightfor-   ward algorithm called turn - based baseline that is   inspired by the automatic silver data annotation15005   algorithm explained in Section 3.3 . In the turn-   based baseline , when a speaker asks a question ,   the predicted answer includes all the subsequent   utterances of other speakers until the same speaker   gets another turn ( stopping criterion ) . Note that ,   turn - based baseline assumes all questions can be   answered and always provides single - span answers ,   although the predictions may be multi - speaker .   4.1 Results and Discussion   We report performance of various fine - tuned single-   span , multi - span models in Tables 3 , and 4 respec-   tively on the test split of M QA . Further ,   we evaluate zero - shot performance in Table 5 . We   summarize our findings below and refer readers to   Appendix C for additional results .   Main Baselines and Comparison with Human   Performance . Results from Tables 3 and 4   show that single - span models ( narrowly ) outper-   form the multi - span models , with the best overall   performance achieved by single - span variant of   DeBERTa - base ( overall F1 = 57.3 ) . Other single-   span variants of Longformer and BigBird achieve   higher performance on answerable questions ( up   to F1 = 64.4 ) but have lesser overall performance   due to lower F1 scores on unanswerable questions .   Comparing to the human performance ( overall F1   = 84.6 ) , we find at least a 25point difference in   overall F1 of all finetuned models . Across various   answer types , the difference in F1 scores is still at   least 20points . Similar trends holds for EM and   IoU metrics too . In the zero - shot setting ( refer to   Table 5 ) , the difference in overall scores with re-   spect to human performance is even greater ( ≥44points across all metrics ) . Furthermore , all fine-   tuned models outperform the turn - based baseline   ( with the exception of LED - base ) , whereas the cor-   responding zero - shot variants fail to outperform the   turn - based baseline on overall metrics . This sug-   gests that our dataset is challenging for current QA   systems , leaving significant scope for improvement   via interesting future work .   Impact of Long - Context Models . We observe   that in a majority cases short - context models ( espe-   cially RoBERTa ) outperforms long - context models   ( Longformer and BigBird ) by 1 - 2points . Further-   more , the LED model that completely fits 90 %   of transcripts has significantly lower overall score   ( ≈30F1 point difference ) due to poor performance   on answerable questions . We believe that the   ability to fit larger contexts is traded - off by well-   optimized design of short - context models . This is   consistent with the findings of Pang et al . ( 2022 )   and suggests better long - context models may be   needed to outperform shorter extractive models .   Impact of Multi - Span Models . Table 5 shows   that in the zero - shot setting , multi - span variants   slightly outperform their single - span counterparts   for long - context models and slightly underperform   for DeBERTa . In Appendix C , we find that within   answer types zero - shot performance drops for unan-   swerable questions while improving for multi - span   and multi - speaker answers . For finetuned models   ( Tables 3 and 4 ) , the overall performance of multi-   span models is comparable if not slightly less than   single - span variants . Notably , for short - context15006   models , there is significant gain in performance for   all answerable questions . Further , we observe that   multi - span models consistently underperform on   unanswerable questions ( as high as 15F1 points ) .   Performance of multi - span model on unanswerable   questions can be negatively impacted by even one   false positive Itag , changing the prediction from   unanswerable to answerable . While prior work on   multi - span QA ( Segal et al . , 2020 ; Li et al . , 2022 )   have found tagging - based approaches to outper-   form single - span variants , they only explore fac-   toid questions on relatively shorter contexts . Future   work can focus on improving multi - span QA for   more open - ended questions like in M QA .   Impact of Intermediate Training . Silver data   augmentation is effective in zero - shot settings   with≥15point improvement for single - span long-   context models ( Table 5 ) . For finetuned models ,   however , we do not observe significant improve-   ments in overall scores from intermediate - training   compared to directly finetuning . Interestingly , sil-   ver data augmentation improves performance on   unanswerable questions for single - span models ( ex-   cept DeBERTA ) and multi - span models .   Instruction - Tuned Models . Lastly , Table 5   shows zero - shot performance of instruction - tuned   F - T5model . We find the F - T5model   ( 3B parameters ) outperforms most zero - shot single-   span models and narrowly underperforms zero - shot   multi - span models . Despite the design of instruc-   tions and filtering ( Section 3.2 ) , the model under-   performs on unanswerable questions . Thus , we add   an additional step to identify answerable questions   and use model responses only for predicted answer-   able questions . The question classification can be   done zero - shot using the same F - T5model   or by training an external supervised model . We   observe that using the F - T5 model is more   effective ( yields best performance ) than using a   supervised model ( 6F1 point drop ) as the predic-   tions of the latter are biased towards the question   being unanswerable . Future work can further focus   on accurately identifying answerable questions to   improve overall performance .   Error Analysis . Next , we analyze some intrigu-   ing patterns in the errors within model predictions .   Firstly , we observe that identifying rhetorical or   unanswerable questions asked in a meeting is a   challenging sub - task . Training a separate binary   classification model that classifies whether a ques-   tion is answerable based on the context from M - QAyields only an F1 = 49.2(see Appendix B ) .   In Figure 4a , it becomes apparent that a significant   portion of errors in predictions for answerable ques-   tions stem from the model incorrectly predicting   that the question is rhetorical , particularly in the15007zero - shot setting . Additionally , in case of multi-   span answers , single - span models exhibit higher   fraction of errors where predictions include sen-   tences not present in the gold answer , in contrast   to their multi - span counterparts ( for details refer to   Appendix D ) . This follows from the construction   of single - span models , as described in Section 3.2 .   Lastly , for multi - speaker answers , we analyze the   overlap in speakers ( measured via IoU ) of predicted   and gold answers in Figure 4b . We find that even   incorrect predictions of finetuned models contain   roughly 55 % speaker overlap with the gold answer ,   i.e. , models can effectively predict which speak-   ers answer the question . However , incorrect pre-   dictions in the zero - shot setting contain only 30 %   speaker overlap indicating that zero - shot models   may struggle to predict which speakers answer the   question . Future works can explore methods to   effectively identify rhetorical questions and predict   which speakers answer the question to improve   overall performance . A more detailed analysis of   errors can be found in Appendix D.   5 Related Work   Our work builds upon prior work on meeting tran-   scripts and question answering . Rogers et al . ( 2021 )   provide a comprehensive survey of several QA   datasets and formats .   Meeting Transcripts . Several other small - scale   corpora of meeting recordings or transcripts are   publicly available ( Janin et al . , 2003 ; Garofolo   et al . , 2004 ; Chen et al . , 2005 ; Mostefa et al . ,   2007 ) . We restrict ourselves the most popular   and frequently used AMI corpus . Other works   study various aspects of summarizing meeting tran-   scripts ( Mehdad et al . , 2013 ; Wang and Cardie ,   2013 ; Shang et al . , 2018 ; Li et al . , 2019 ; Zhu et al . ,   2020 , inter alia ) or extracting action - items ( Mor-   gan et al . , 2006 ; Purver et al . , 2007 ; Cohen et al . ,   2021 ) . The work closest to ours uses Markov mod-   els to classify dialogue - acts as questions , answers   or others ( Kathol and Tur , 2008 ) .   QA on Conversational Text . Prior work com-   prises of QA datasets based on small chit - chat from   TV shows ( Sun et al . , 2019 ; Yang and Choi , 2019 )   or domain - specific chat - rooms ( Li et al . , 2020 ) .   TheQAC ( Wu et al . , 2022 ) dataset builds on   these works with conversations from multiple do-   mains ( including M S ) . However , these   works employ human annotators for generatingquestions based on their understanding of the con-   versation resulting in straight - forward questions   testing local information . Consequently , the answer   spans of these datasets are significantly shorter ,   single - span , restricted to one speaker and often cor-   respond to simple noun phrases ( as high as 80 % for   QAC ) . In contrast , questions asked by meet-   ing participants are more open - ended , discussion-   seeking , and correspond to longer answers ( ≈7x )   with complex multi - span and multi - speaker scenar-   ios . Note that our work is different from conver-   sational QA datasets that consist of a sequence of   questions and answers simulating a conversation   grounded in a short paragraph ( Choi et al . , 2018 ;   Reddy et al . , 2019 ; Campos et al . , 2020 ) .   Long - Context QA . Recent works show that QA   models struggle to understand answer questions   correctly using long contexts ( Pang et al . , 2022 ;   Mou et al . , 2021 ; Soleimani et al . , 2021 ; Dasigi   et al . , 2021 ) . However , unlike our work , the source   ( long ) documents for building these datasets are   taken from written - text domains such as books ,   film - scripts , research papers , or news articles .   6 Conclusion   In this work , we present M QA , an extrac-   tive QA dataset based on meeting transcripts to   identify answers to questions asked during discus-   sion among meeting participants . Detailed analysis   of the data reveals it is a challenging real - world   task . Baseline experiments with a wide variety of   models show the current performance lags behind   human performance by at least 25and44over-   all F1 points for finetuned and zeroshot models   respectively . This demonstrates that current QA   systems find our task challenging , leaving tremen-   dous scope for improvement . We hope that future   works will aim to bridge this gap and our work   fosters research in NLP tasks ( especially QA ) on   other text domains such as meeting transcripts .   Acknowledgements   We thank the reviewers and the area chairs for their   helpful comments and feedback . We thank TELUS   International for their help with data collection .   We also thank Shiyue Zhang and Swarnadeep Saha   for their helpful comments . This work was par-   tially supported by NSF - CAREER Award 1846185 ,   and NSF - AI Engage Institute DRL-2112635 . The   views contained in this article are those of the au-   thors and not of the funding agency.15008Limitations   Due to the structure of M QA , the answers   to questions asked by participants ( if any ) are   present in the transcript itself , making it an ex-   tractive task . Therefore , we do not extensively   explore the use of generative models since the pre-   dictions do not stick to the sentences in the tran-   script and could possibly include hallucinations .   However , we aim to mitigate hallucinations by us-   ing instruction - tuned generative models with suit-   ably designed instructions and enforce a strict exact   match criteria for filtering any possible hallucina-   tions . Future work can explore how to adapt or   evaluate non - instruction - tuned generative models   on this task and better identify hallucinations with   a more relaxed filtering to improve performance .   We also do not report zero - shot performance of   InstructGPT ( Ouyang et al . , 2022 ) as these models   are not freely accessible . Additionally , we use a   simple multi - span QA adaptation technique from   Segal et al . ( 2020 ) , but predicting answer spans   by classifying each token can be difficult to train   leading to slightly lower performance ( discussed in   Section 4.1 ) . We hope our dataset provides addi-   tional motivation for future work on multi - span QA .   Finally , M QAonly comprises of publicly   available meeting transcripts in English , but our   methodology of data collection and model training   ( using multilingual variants ) should still be appli-   cable for other languages in future work .   Ethical Considerations   The human participants in our work were recruited   by an external crowd - sourcing company that en-   sured annotators provided informed consent , were   given fair compensation , and no personally identi-   fiable information ( ) was collected or released .   We use existing publicly available meeting tran-   scripts collected by the AMI project ( Carletta et al . ,   2005 ) in controlled scenarios and filtered for of-   fensive / toxic content . We also conducted manual   inspection of a random sample from annotated tran-   scripts and did not find any toxic content or .   Furthermore , the collected data and experiments   are conducted in English and we do not claim gen-   eralization of our findings across languages . Given   the broad nature of meetings , the content can fall   into a number of domains , of which only a few   are represented in the AMI corpus . Therefore , we   do not expect models trained on M QAto   generalize to certain domains such as judicial , ethi - cal review , congressional proceedings , etc . which   involve specific jargon and rules of engagement .   References150091501015011   A Additional Details on M QA   A.1 Tri - gram Analysis of Question Types   In contrast to most QA datasets , questions in M - QA are extracted directly from the meeting   transcripts and thus are conversation segments .   Consequently , we find that questions may not be   concise , often use auxiliary verbs , and do not typ-   ically begin with ‘ wh ’ or ‘ how ’ prefixes , making   our new QA task and dataset challenging yet inter-   esting for the community . This makes conventional   analysis of question types based on prefixes less   relevant here , and instead , we compute the top-25   most common trigrams from all questions , shown   in Figure 5 . The three most common question pat-   terns are : ‘ do you / we ... ’ , and ‘ what ... ’ . Addition-   ally , the trigrams demonstrate that our questions are   open - ended and seeking opinions or thoughts from   other participants that tend to elicit long responses .   A.2 Dataset format and meta - information   We provide annotations for each meeting tran-   script at the sentence level in ‘ .json ’ for-   mat , and each sentence has 4 primary at-   tributes : displayText , speakerFaceId ,   sentenceId , and question which contain   the sentence text , integer identifier of the speaker   ( unique within a meeting ) , integer identifier of   the sentence , and information about the sen-   tence as a question respectively . The ques-   tion attribute is relevant only if the sentence   is identified as a question . It contains ad-   ditional attributes : possible , meaningful ,   questionContext , combinedQuestion ,   andanswerSpan . First , we perform “ ques-15012   tion selection ” described in Section 2.1 and   set the possible tag as True to guide the   annotators . The remaining attributes are   set to default values meaningful = False ,   answerSpan = [ ] , questionContext =   [ ] , and combinedQuestion = [ ] prior to   annotation . Annotators modify the question   attribute during the course of the annotation   and can even mark additional questions out-   sider our question selection criteria by set-   tingpossible = True . They lable the re-   maining attributes according to the “ answer   annotation ” steps mentioned in Section 2.1 .   The list type attributes questionContext ,   combinedQuestion , and answerSpan con-   tain sentences specified using the value of the cor-   responding sentenceId attributes . The domain   of meeting transcripts ( from AMI corpus ) is a com-   bination of elicited scenario - driven data , and nat-   ural data . We refer interested readers to the AMI   project page for more information about the topic   or scenario of each meeting .   We find that out of 7.7 K questions in M - QA , only 66 ( < 1 % ) additional questions were   identified by the annotators that were missed by our   question selection criteria . Further , 751 questions   ( 9.7 % ) were annotated with additional context sen-   tences via questionContext and a total of   784 ( 10.1 % ) were combined with another ques-   tion via combinedQuestion attribute . Among   the latter , an average of 2.2 ( maximum 4 ) questions   were combined and these questions were an aver-   age of 1.5 sentences apart . The average length of   questionContext ( when annotated ) was 1.7   sentences ( maximum 3 ) which preceded the ques-   tion by 1.7 sentences . Note that for the purposes   of QA evaluation , we only use the possible and   answerSpan attributes . The remaining attributes   serve as meta - information to understand the dataset   better and can facilitate error analysis and/or fu-   ture work . Also to come up with overall question   counts , we ignore the combinedQuestion at-   tributes and count all the questions individually .   Therefore , this attribute serves as an indicator of   when and why different questions share the same   answer . Empirically , we note that the combined15013   questions and question context typically fit within   the contexts created using location - based retrieval   ( Section 3.1 ) and are present in the input fed into   QA models in the vast majority of cases .   A.3 Additional Annotated Examples   Next , we show multiple examples of snippets from   meeting transcripts with QA components present   inM QAin Figures 6 - 11 . Figure 7 also   contains an example of an unanswerable question   asked by Speaker 4 ( “ my data is coming ? ” )   which is either rhetorical or corresponds to incor-   rect punctuation . In such cases , annotators label   meaningful = False and an empty / null an-   swer annotation ( answerSpan = [ ] ) . On the other   hand , Figure 11 also contains two consecutive ques-   tions asked by Speaker 1 but the annotators   mark both as meaningful = True , but choose   to combine them via ( combinedQuestion ) and   share a common answer . This because the first   question is more generic , and the second question   builds on top of it , by providing a specific exam-   ple of what is loaded and what is n’t . Further , in   Figure 8 we provide an example of question which   needs additional context sentences annotated via   questionContext . Figures 6 , 9 , and 11 are   diverse instances of multi - speaker and multi - span   answers in our dataset .   A.4 More on Data Collection   AMI Transcript Preprocessing . The AMI cor-   pus is a collection of 171meeting transcripts con-   taining manually annotated and punctuated speaker-   specific XML files for each meeting . We parse   these XML files and combine utterances from mul-   tiple speakers by aligning the start times into a   single transcript ( with speaker information ) corre-   sponding to each meeting . We then use a disfluency   detector model to identify and remove disfluencies   from the utterances ( Jamshid Lou and Johnson ,   2020 ) .   Annotator Recruitment and Training . All an-   notators are hired by a professional crowdsourcing   company TELUS.The company obtained con-   sents from the crowdworkers before the annota-   tion process and conducted ethical reviews . The   company recruited 18 annotators , all based in the   United States and native English speakers , who had   previously successfully participated in text - based   annotation projects . In addition to the instruction   document ( shared in the supplementary ) curated   by the first author , TELUS conducted a series of   ( virtual ) meetings to deliver instructions , conduct   example walk - through of the annotation and clarify   doubts . At the end of initial training , a small batch   of 5 meetings was provided to each of the annota-   tors to calibrate performance . The responses were   then compared to the good quality annotations per-   formed by 2 project leads at TELUS manually in   consultation with the authors . Feedback was pro-15014   vided to the annotators to improve the quality of   annotations , and based on their final responses the   top-6 best performing annotators were selected to   work on the project .   Quality Control . From the pool of selected an-   notators , project leads recruited two of the best per-   forming and experienced annotators to help with   quality control . Any annotated meeting transcript   was assigned to either of these two annotators for   review . Between the two , meeting annotated by one   was assigned to the other for review . After the re-   view , minor errors in annotation were fixed directly ,   otherwise major errors were sent back to the respec-   tive annotators for a re - annotation of the question   and in some rare cases the annotation was redone   by the reviewers . At the end of annotation batch   ( total of 4 ) , the transcripts were sent to the authors   who extensively reviewed them and provided feed-   back . We also looked for typos , and other issues   which were fixed promptly . The TELUS project   leads did not find any toxic and offensive content at   their end and no such concerns were reported in the   quality control stage . Further , all communication   with the annotators is done by the crowdsourcing   company and no personally identifiable informa-   tion ( ) is released to the authors . Additionally ,   their execution platform contains unique identifiers   for all annotators ensuring theiris not released   along with the annotated data . Finally , based on   the feedback from annotators , we removed all ques-   tions corresponding to 5 meetings as the meeting   content was hard to follow .   Time Taken . On an average across meeting tran-   script and annotators , it took ≈1hour to annotate   each meeting transcript which averages to ≈1.3   minutes per question . However , the per meeting an-   notation time strongly correlated with the length of   the transcript ( number of sentences ) . Among ques-   tions , annotators spent more time on answerable   questions . This is because if the question is marked   unanswerable ( not meaningful ) , they did not have   find exact answer span ( in sentences ) and anno-   tated other meta - information . The project leads   also ensured that the amount of time taken by each   annotators was consistent with internal estimated   by the intial batch used during annotator recruit-   ment . The total production time for this project ( for   TELUS ) was 186.5 hours.15015   Compensation . The annotators were compen-   sated through a fixed hourly rate defined for each   participant . No additional bonus was provided to   incentivize faster turnaround times . The average   hourly wage for participants was roughly $ 20 / hour   in compliance with all the federal and local local   laws to ensure fair payment .   B Experimental Details   GPU Compute . For training and/or inference we   used a combination of 6 NVIDIA A10 24 GB GPUs   and 2 NVIDIA RTX A6000 48 GB GPUs . Directly   finetuning on M QA starting from a pre-   trained checkpoints is quite fast and takes not more   than 4 GPU hours ( depending on the batch size ) .   Intermediate training on the silver - annotated data   ( 5 epochs ) takes about 12 - 18 GPU hours ( training   time is higher for long - context models ) .   Hyperparameters . For the short - context mod-   els ( RoBERTa and DeBERTa - v3 ) we used max se-   quence length of 512 , ( stride of 128 but not utilized   due to location - based context retrieval ) , and batch   size of 16 . For long context models , we used max   sequence length of 4096 , stride of 128 , and batch   size varied between 8 and 16 ( depending on GPU   availability ) . Note that there is no stride for multi-   span models . For model training on M QA ,   we use a learning rate of 3e-5 , warmup ratio of   0.2 , and train for 15 epochs with an early stopping   criteria set to a patience of 2 epochs ( F1 on dev   split ) . For intermediate training with silver data ,   we use the same hyperparameters except we train   for 8 epochs with the same early stop criteria . The   hyperparameters values used are pretty standard   and were not tuned explicitly for M QA .   Model Sizes . RoBERTa base and large models   comprise of 125 M and 355 M parameters respec-   tively , while DeBERTa base and large models com-   prise of 86 M and 304 M parameters . On the other   hand , Longformer - base comprises of 149 M pa-   rameters . Since BigBird is initialized with the   RoBERTa checkpoints they share the same model   size . Finally , instruction - tuned models F - T5 , and consist of 770 M , 3B , and 11B   model parameters respectively .   Pretrained Checkpoints . For models with-   out intermediate - training we use the standard   checkpoints for all models available on Hug-   gingFace . For the score - based context re-   trieval in Section 3.1 , we use HuggingFace ’s   evaluate library for computing ROUGE-   1 and the multi - qa - MiniLM - L6 - cos - v1   model from the sentence - transformers   python package for embedding cosine similar-   ity . During silver data annotation , we used the   en_core_web_sm from spacy package for   NER . For intermediate training ( ) we used the   following pretrained checkpoints ( base size ):   • RoBERTa : deepset / roberta - base - squad2   • DeBERTa : deepset / deberta - v3 - base - squad215016   •Longformer : mrm8488 / longformer - base-4096-   finetuned - squadv2   • BigBird : google / bigbird - base - trivia - itc   Licensing . We used the AMI dataset that has   CC - BY-4.0 license . Our released data will have   theCC - BY - NC license . We do not violate the con-   straints put in the M Sdataset to use inter-   view files for reasearch purposes only .   Instructions / Prompts . For instruction - tuned   FLAN models we use the following prompt   template to generate sentences from the context   that answer the question .   [ CONTEXT ]   Based on the conversation above ,   which sentences from the conversta-   tion answer [ SPEAKER ] ’s question :   [ QUESTION ]   Here , [ . ] is a placeholder filled in separately   for each instance / question . Additionally , for the   ‘ self ask ’ setting , we first use a prompt ( shown   below ) to get the model to output if the question   is answerable . If the model outputs “ no ” , filter   out those questions and use an empty string asthe predictions . We find answers to the remaining   questions using the prompt above .   [ CONTEXT ] Based on the con-   versation above , did anyone an-   swer [ SPEAKER ] ’s question :   [ QUESTION ] Respond “ yes ” if   answered , “ no ” otherwise .   Binary Answerable Classification Model . As   mentioned in Section 4.1 , we train a separate super-   vised RoBERTa - base model to detect if a question   is answerable . This is formulated as a binary classi-   fication task , therefore we train the sequence classi-   fication head on questions from M QA . We   use the same hyperparameters as for single - span   RoBERTa models mentioned above . The final per-   formance of this model , is not as strong with an   overall F1 = 49.2 . This indicates that even a sim-   ple binary task formulation from M QAis   challenging and requires thorough understanding   of meeting discussions .   C Additional Results   Building on Tables 3 and 4 , which contain F1 and   IoU scores , we present the exact match ( EM ) scores15017   in Tables 6 and 7 for finetuned single - span and   multi - span respectively . While the relative trends   across the models remains the same , we find that   the EM scores are the lowest because it reflects pre-   dictions that perfectly match the reference . Another   noteworthy observation is that the EM scores of all   single - span models on the multi - span split is 0 .   This can be explained by the training procedure of   single - span models ( described in Section 3.2 ) . The   models are trained to predict a single “ super - span ”   starting from the first sentence in the reference to   the last sentence in the reference . Therefore , even   in the theoretical best - case - scenario , the models   would predict a single super - span containing all   the reference sentences interleaved by irrelevant   sentences for questions with multi - span answers .   We analyze errors due to this in Appendix D.   In Table 5 , we only present the overall scores   for various models . All the scores on differentsplits are given in Table 8 . We observe that for   all single - span models ( except RoBERTa on unan-   swerable questions ) adding silver data in interme-   diate training helps improve performance across all   splits . Furthermore , the multi - speaker and multi-   span splits consistently pose a challenge for all   models evaluated in a zero - shot setting . Also ,   within answer - types performance drops for unan-   swerable questions while improving for multi - span   and multi - speaker answers . The challenge posed   by unanswerable questions can be explained by   the multi - span adaptation ( Section 3.2 ) . By posing   question answering as a token - classification task ,   even one false positive ( Itag instead of all Os ) in   token label , changes the answer prediction from   unanswerable to answerable . Finally , we note that   when using a pipeline - approach of isolation unan-   swerable questions separately , we find the errors   in this step cascade and are reflected in the per-15018   formance on answerable questions of the overall   system . These systems perform better on unanswer-   able questions ( not identified by regular instruction   and filtering ) , however the false - positives decrease   performance on answerable questions , even more   so when using an external supervised model .   In Table 8 , we see clear scaling of performance   as we move from F - T5large ( 770 M ) to F-   T5(3B ) . However , for F - T5 ( 11B )   the performance of unanswerable , multi - span and   multi - speaker questions increases ( ≥8F1 points )   but performance on other answerable questions de-   creases ( ≈9F1 points ) which in turn reduces the   overall performance as compared to F - T5xl .   Table 9 evaluates the performance of RoBERTa-   large and DeBERTa - large architectures for single-   span and multi - span models in both finetuned and   zero - shot settings . The corresponding performance   of the base models can be found in Tables 3 , 4 ,   and 5 respectively . We do not observe any signifi-   ca nt increase in performance when using the larger   checkpoints , thus leaving ample room for future   work to bridge the gap between model and human   performance on M QA.D Error Analysis   In this section , we analyze error patterns across   models discussed in Sections 3.2 and 4 in detail .   First , we note that for the unanswerable questions   split , any error corresponds to the model predicting   a non - empty answer span . The frequency of this   for a given model can be calculated by 100−F1   score for this split ( provided in Tables 3 , 4 , and 8) .   However , for answerable questions , errors in model   predictions are diverse as categorized below .. Prediction is an empty - span ( unanswerable).Predicted span contains a sentence notpresent   in the gold or annotated reference span . At least one of the sentences in the reference   span is notpresent in the predicted span . Combination of errors with respect to refer-   ence span ( bothand )   Therefore , whenever the model prediction does not   exactly match the annotated reference span , we   can put it in one of the above 4 categories . We per-   form this analysis for various finetuned single - span ,   finetuned multi - span models as well as zeroshot   single - span , multi - span and instruction tuned mod-   els discussed in Sections 3.2 and 4 . For brevity , we15019   pick representative models from different possible   combinations of intermediate training data . This   is illustrated in Figures 12 - 15 with errorshown   in red , errorshown in yellow , errorshown in   blue , and errorshown in green .   Table 3 shows very similar performance differ-   ent intermediate training data configurations for a   given model architecture . Thus , we present error   distribution for single - span models directly fine-   tuned on M QAin Figure 12 . We find that   most of the errors belong to categories- . The   DeBERTa model has a relatively high unanswer-   able prediction error which is primarily because   its predictions skew towards unanswerable as ex-   plained by the F1 score on No Answer ( unanswer-   able ) split in Table 3 . Next , in Figure 13 we show   the error distributions on the corresponding multi-   span models finetuned directly on M QA .   We observe that , for all model ( except RoBERTa )   the frequency of incorrectly predicting unanswer-   able goes down as well as the prediction span con - taining sentences outside the reference ( error ) .   However , the frequency of hybrid errorincreases   significantly . This can partly be explained by the   design of single - span and multi - span models . As   mentioned in Section 3.2 , training data of the single   span model involves creating a single “ super - span ”   starting from the first sentence in the reference to   the last sentence in the reference . This by construc-   tion involves errorand supervision on this data   directs the model to include irrelevant sentences in   the answer span if it is sandwiched between two   relevant sentences . Also , for multi - span models   an unanswerable prediction implies all tokens are   labeled with the Otag , and even one false positive   ( Itag ) would make the prediction answerable . Due   to this , one can expect these models to mispredict   empty spans less frequently .   Interestingly , when we look at zero - shot perfor-   mance of single - span and multi - span models in Fig-   ure 14 , we find relatively high frequency of error   and very low frequency of error . The errors15020and hybridalso become more common . These   models which are trained only on intermediate data ,   do not generalize in their ability to predict when   a question is unanswerable . Further , we find that   in the zero - shot evaluation setting , model predic-   tions are shorter than the reference span by at least   one sentence on average ( ≈2 sentences for multi-   span split ) . This indicates in zero - shot evaluation   models are more likely to predict a part of the an-   swer than output spans that covers all sentences in   the reference , containing extra sentences that lie   outside the reference .   Finally , we look at the errors of instruction - tuned   F - T5models in Figure 15 . When using F-   T5with appropriate instruction and filtering we   find that most of the errors are hybrid , i.e. predicted   sentences do not cover the reference span entirely   and also contains irrelevant sentences . When we   add the self - ans pipeline on top of it with additional   instructions to spot unanswerable questions , the   predictions contain more empty spans ( relatively )   which is reflected in the increase in frequency of   errorand the No Answer F1 ( in Table 8) . Surpris-   ingly , when we use an external supervised model   to predict unanswerable questions , it contributes to   the vast majority of errors in the pipeline ( error ) .   This is consistent with the fact that the test F1 score   of this model on the task of classifying questions   as answerable or not was only 49.2 .   So far , we have analyzed errors in predictions for   all the answerable questions . Next , we focus our   attention on questions with multi - span and multi-   speaker answers . Within the multi - span split , we   calculate the fraction of incorrect predictions ( as   per exact match ) that are multi - span , denoted by   multi - span preds ( % ) . Similarly , for multi - speaker   split , we calculate the fraction of incorrect predic-   tions ( as per exact match ) that are multi - speaker in   nature , denoted by multi - speaker preds ( % ) . Fur-   ther , we compare the list of speakers in the refer-   ence and predicted spans using Jaccard similarity   ( IoU ) denoted as speaker IoU . We compute and re-   port these metrics for all the aforementioned mod-   els in Table 10 .   As expected , due to the single - span training ,   none of the predictions of the single - span mod-   els are multi - span in nature . On the other hand ,   even incorrect predictions of the finetuned multi-   span models are multi - span in nature at least half of   the times . However , a significant fraction , between   29 - 46 % , of the errors in this split can be attributed   to single - span predictions for various models . For   zero - shot models , over 90 % of incorrect predic-   tions are single span ( also vast majority of all pre-   dictions are single span ) . On the multi - speaker   split , the incorrect predictions of finetuned models   are multi - speaker in nature . However , the speaker   IoU ( < 65 ) indicates that predicted spans often   miss utterances from relevant speakers in the ref-   erence and also include irrelevant utterances from   other speakers . Zero - shot models on the other hand ,   only tend to give single - speaker responses which is   the primary source for errors . Note that , relatively   high frequency of erroror prediction unanswer-   able spans also contributes in driving down the   values of these metrics ( empty spans have no span   or speaker information ) .   E Instruction - tuned Model Ablations   In Section 3.2 , we describe adaptation of genera-   tiveF - T5 model to our extractive setting by   ( i ) designing instructions that ask models to list   which sentences from the context contain the an-   swer ( mentioned in Section B ) ; and ( ii ) filtering   out all sentences from the model response that are   not present in the context to remove any possible   hallucinations . To show the importance of both   these steps , we first compare with an instruction   eliciting a direct response ( answer ) from the model ,   mentioned below .   [ CONTEXT ]   Based on the conversation above , state   the answer(s ) given to [ SPEAKER ] ’s   question : [ QUESTION ]   We call this direct instruction as opposed to   thelist instruction mentioned used in Sections 3.2   and 4.1 . Further , we examine the importance of   filtering by comparing raw model responses to their   filtered counterparts . The comparison on a random   subset of 500 question from the dev splits is shown   in Table 11 . We find that our chosen type of in-   struction ( list ) significantly outperforms the direct15021   instruction ( tends to be more abstractive ) . Further-   more , filtering consistently improves overall per-   formance especially when using direct instructions   possibly due to higher number of hallucinations in   the corresponding model answers .   F Silver Data Augmentation Details   Section 3.3 describes the silver data annotation pro-   cess to annotate publicly available interview tran-   scripts from CNN and NPR ( Zhu et al . , 2021a ) for   extractive QA task similar to M QA . We   first identify the subset of speakers that act as the   host or interviewer and focus on questions asked   by these speakers to generate answer annotations . Based on the utterance containing the question , we   first automatically identify speaker(s ) answer the   question using a rule - based approach . This is done   by ( i ) finding speakers mentioned in the question   sentence or utterance using an off - the - shelf NER   model ( mentioned in Appendix B ) , ( ii ) identifying   speakers from previous speaker turns if the same   speaker takes the turn after the host speaker assum-   ing this is a case of follow - up questions , or ( iii ) in   the absence of first two conditions , all speaker that   take turns after the host . Finally , we search utter-   ances corresponding to identified speakers in the   transcript until a stopping criterion ( max number   of utterances , or reach the next host utterance ) is15022met and label it as the answer . From the 463.6 K   transcripts , only 15 % of the files have identify host   who steer the interview and have sufficiently high   frequency of questions . However , each of these   transcripts result in roughly 20annotated questions   on average . For intermediate training , we sample   a total of 150 K questions from this set and split it   randomly into train , dev , test splits in 80 : 10 : 10   ratio . Table 12 shows a few examples of silver   answer annotations for questions asked in M -   Sinterviews .   Perturbations . First , we add random sentences   between the question and answer utterances to pre-   vent a location bias in which model predicts sen-   tences that immediately follow the question as the   answer . Second , we create scenarios where the   question is unanswerable by removing annotated   answer spans from the context . Third , we replace   speaker names in the context with a numeric iden-   tifier because information about speaker names   are not always available in the transcript includ-   ing AMI dataset . For multi - span models , we fur-   ther insert random sentences from elsewhere in the   transcript in between annotated answers to facili-   tate better span selection . Finally , the number of   speaker turns in M Sare 10x smaller than   those in AMI dataset ( refer to Table 2 of Zhu et al .   ( 2021a ) ) . Therefore , we create more speaker tran-   sitions by splitting a long speaker utterance into   shorter utterances by multiple speakers.15023ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7   /squareA2 . Did you discuss any potential risks of your work ?   Section 8   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 2 , 3 and 4 verify the claims made .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 2   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix B   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 8 , Appendix B   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 8 , Appendix A   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 2 , 8 , Appendix A   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 2   C / squareDid you run computational experiments ?   Section 3 - 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix B , Sec 415024 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix B , Sec 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Sec 4 , Appendix B , C   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix B   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 2 , Appendix A   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix A and supplementary data folder   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 2 , Appendix A   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Section 2 , Appendix A   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Ethical Review was conducted by crowd - sourcing company .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix A15025