  Han Peng , Ge Li , Yunfei Zhao , Zhi Jin   Key Laboratory of High Confidence Software Technologies ( Peking University ) ,   Ministry of Education ; Institute of Software , EECS , Peking University , Beijing , China   { phan , lige , zhaoyunfei , zhijin}@pku.edu.cn   Abstract   Transformers are now widely used in code rep-   resentation , and several recent works further   develop tree Transformers to capture the syn-   tactic structure in source code . Specifically ,   novel tree positional encodings have been pro-   posed to incorporate inductive bias into Trans-   former . In this work , we propose a novel tree   Transformer encoding node positions based   on our new description method for tree struc-   tures . Technically , local and global soft bias   shown in previous works is both introduced   as positional encodings of our Transformer   model . Our model finally outperforms strong   baselines on code summarization and comple-   tion tasks across two languages , demonstrat-   ing our model ’s effectiveness . Besides , exten-   sive experiments and ablation study shows that   combining both local and global paradigms is   still helpful in improving model performance .   We release our code at https://github.com/   AwdHanPeng / TreeTransformer .   1 Introduction   Machine learning for source code aims to learn the   semantic embedding of programs . Due to the for-   mat similarity between code and text ( Hindle et al . ,   2016 ) , Transformers ( Vaswani et al . , 2017 ) are now   widely used in code representation ( Hellendoorn   et al . , 2019 ; Zügner et al . , 2021 ; Peng et al . , 2021 ) .   Unlike natural language , source code is more logi-   cal and has rich structures such as abstract syntax   trees ( AST ) . Therefore , one research topic of code   intelligence is representing the syntax tree of code .   Several recent works proposed novel tree - based   Transformers by defining the position of each node   to handle tree structure ( Shiv and Quirk , 2019 ; Kim   et al . , 2020 ) . In this work , we pursue the research   line of tree Transformer for learning code AST .   In Transformer , positional encoding is crucial to   exploit potential structures of data ( such as codeASTs or graphs ) as other components are entirely   position - invariant . Recently , a growing research   trend is adapting Transformer to more complex   structured data than plain text by modifying posi-   tional encoding , not only in language processing   ( Wang et al . , 2019b ; Nguyen et al . , 2020 ) but also   in graph representation learning field ( Dwivedi and   Bresson , 2020 ; Mialon et al . , 2021 ) . As for the   structure - aware Transformers , the key step of en-   coding positions is to find a proper description   for input structure , which means abstracting the   physical structure of data into a suitable math-   ematical form . For example , the vanilla Trans-   former ( Vaswani et al . , 2017 ) regards the poten-   tial text order in natural languages as the arrange-   ment of natural numbers , while the graph Trans-   formers ( Kreuzer et al . , 2021 ; Dwivedi and Bres-   son , 2020 ) treat the positional relationship between   graph nodes as the adjacent matrix or Laplacian fur-   ther . Intuitively , a good description should be infor-   mation lossless from which the whole structure of   the original data could be precisely reconstructed .   Recently , several tree - based Transformers incor-   porated with advanced positional encoding are pre-   sented to process the code syntax tree . ( Shiv and   Quirk , 2019 ) represented the position of each node   using sibling orders of all nodes that existed in   its path to the root , while ( Kim et al . , 2020 ) de-   fined the relative distance between two nodes as   the traversing up and down steps along the shortest   path connecting them . However , some tree struc-   tures are overlooked by these previous approaches :   the first method assumes the regular tree with a   fixed number of node children , and the second ig-   nores the sibling feature in traversing . In this paper ,   we present a new description method for node po-   sitions from which the corresponding tree can be   rebuilt without ambiguity . Specifically , the posi-   tion of each node is recursively described as a list   including multiple 2D coordinates . The coordinate   list of each node first inherits from its parent and3204then includes a new 2D coordinate of itself , where   the first dimension indicates its sibling order and   the second is the total child number of its parent .   We incorporate the proposed description method   into Transformer , powering it to capture tree struc-   tures . Technically , a growing popular approach is   to encode structure as soft inductive bias in posi-   tional encodings of Transformer , in which atten-   tion between all nodes is allowed rather than the   strict aspect of message passing . To the best of our   knowledge , the soft bias in previous works is usu-   ally introduced either in local orglobal . The local   methods integrate the structure relation as one - hop   edges only for two adjacent nodes ( Hellendoorn   et al . , 2019 ; Li et al . , 2020 ) , so each node knows   its multi - hop subgraph only by stacking model lay-   ers . In global approaches , inductive bias is injected   into the attention between any nodes regardless of   whether adjacent in trees or graphs ( Xu et al . , 2020 ;   Wang et al . , 2019a ) , in which structures can perco-   late fully across graphs in a single layer ( Shiv and   Quirk , 2019 ) .   The local and global methods show expressive-   ness in previous works , but the relationship be-   tween them is still not completely studied to the   best of our knowledge . In this paper , we propose   a new tree Transformer that integrates our tree de-   scription in local and global , exploring the interac-   tion between local and global bias . Our model fi-   nally outperforms solid baselines and obtains state-   of - the - art in code summarization and completion   tasks across two different language datasets . Be-   sides , the ablation results show that both global and   local methods are powerful , and the combination   improves model performance further . The contri-   butions of this paper are summarized as follows :   1.We propose a novel tree Transformer which   significantly outperforms existing baselines   across different languages and tasks .   2.We present a new description method for tree   node positions from which tree structure can   be reconstructed precisely .   3.We explore the relationship between the local   and global bias proposed in previous works ,   shedding light on future work .   2 Related Work   Representation learning for source code The   availability of big code shows opportunities forrepresentation learning of programs . Tradition-   ally , code intelligence designers have relied pre-   dominantly on structure or context . Early research   works relied on raw text data for code snippets   ( Dam et al . , 2016 ; Wang et al . , 2016 ; Allamanis   et al . , 2016 ; Iyer et al . , 2016 ) , mainly focusing   on context and struggle to capture code structure .   After that , a growing active research topic is to   represent the syntax tree structure of code . ( Mou   et al . , 2016 ) proposed tree - based convolutional neu-   ral networks and ( Alon et al . , 2018 , 2019 ) treated   codes as weighted combination of pairwise paths   in AST . ( Shiv and Quirk , 2019 ) proposed a custom   positional encoding to extend Transformers to tree-   structured data . ( Kim et al . , 2020 ) defines the rela-   tive distance on the tree as the shorted path between   nodes consisting of up and down steps . Our work   pursues the research line to model code trees , pow-   ering Transformer to learning AST by integrating   tree positional encodings . Besides , several works   also explored leveraging different code representa-   tions jointly , including context , AST structure and   other code graphs . ( Allamanis et al . , 2017 ) pro-   posed GGNN to represent program graphs consist-   ing of AST with control - flow and data - flow . ( Hel-   lendoorn et al . , 2019 ; Zügner et al . , 2021 ; Peng   et al . , 2021 ) proposed to learn structure and context   together by introducing bias in the self - attention of   code context with the underlying tree structure .   Structure - aware Transformers The Trans-   former model ( Vaswani et al . , 2017 ) is the most   widely used architecture in language representation   learning . Several works have recently explored   extending Transformer from plain text to structural   data such as graphs and trees . Technically , two   approaches exist to integrate inductive bias in   Transformer : the hard or soft methods . The hard-   coded methods usually use the mask to restrict   the attention only to adjacent nodes in graphs or   trees ( Gao et al . , 2021 ; Wu et al . , 2020 ) , that is ,   the GNN - like message - passing paradigm exists   therein . However , there is a growing recognition   that inherent limitations exist in message passing ,   such as over - smoothing and over - squashing   ( Hamilton , 2020 ; Alon and Yahav , 2020 ; Kreuzer   et al . , 2021 ) . More recently , a growing interest   in deep learning is to encode structure as soft   inductive bias toward more flexible architectures ,   such as positional encodings in Transformer . For   example , ( Mialon et al . , 2021 ) leveraged relative   positional encoding in self - attention based on3205   positive definite kernels on graphs and ( Zügner   et al . , 2021 ; Ying et al . , 2021 ) incorporate relations   such as shortest path distance in Transformer . We   follow this research line and explore encoding   code AST by integrating tree positional encoding   in Transformer as soft inductive bias . Besides ,   as discussed in the previous section , we further   divide the method of introducing soft bias into   local and global approaches , and integrate these   two paradigms into our proposed model .   Positional encoding for Transformer The ab-   solute positional encoding in vanilla Transformer   is presented to capture the potential orders of se-   quential text . After that , ( Shaw et al . , 2018 ) firstly   proposed the relative positional encoding to Trans-   former . Transformer - XL ( Dai et al . , 2019 ) then   re - parameterized the relative positional encoding   of self - attention and T5 ( Raffel et al . , 2019 ) simpli-   fied the vector representation of relative positions   in ( Shaw et al . , 2018 ) to scalars . More recently , ( Ke   et al . , 2020 ; He et al . , 2020 ) proposed the disentan-   gled attention mechanism for positional encoding ,   showing the irrationality of adding and applying   the same projection for position and word embed-   ding . The mechanism of untied positional encoding   shows effectiveness in the natural language process   ( Tsai et al . , 2019 ; Chen et al . , 2021a ) . In this paper ,   we adopt the idea of the disentangled attention of   Transformer and apply it in our model , proving   still useful in encoding positions for complex tree   structures .   3 Approach   In this section , we present our tree Transformer in   two parts . We first show the novel two - dimensional   description for trees , by which each node ’s position   is converted as a coordinate list . After that , weembed position from the description for each node   and then integrate position encoding in the self-   attention of Transformer in local and global .   3.1 A 2D recursive description for code AST   Our proposed description for tree structure is   defined from the tree root to leaves recursively .   Specifically , the position for each node is repre-   sented as :   F(x ) = /braceleftbiggF(f(x ) ) + { ( x , x)}if x̸=root   { ( 1,1 ) } if x = root   ( 1 )   In Eq.1 , F(x)is the position description for node   xincluding multiply coordinates and f(x)is spec-   ified as the parent node for it . It is clearly seen   thatF(x)for node xis first inherits from its par-   entF(f(x ) ) . After that , a new 2D coordinate is   pushed behind the list , in which the first dimension   is the sibling order of node xand the second is the   total child number of its parent f(x ) . The special   case is for the tree root because no parent exists . So   we add a virtual node as the root ’s parent , which   is also commonly seen in the classical algorithms   for trees ( Cormen et al . , 2022 ) . A clear example of   Eq.1 is shown in Fig.1 .   Each 2D coordinate is then converted to a vector   by the lookup embedding table . In this process , a   sample way is first to map each 2D coordinate into   a scalar and then retrieve the vector by it . Another   method is embedding each dimension first and then   adding ( or concat ) two vectors . Since experiments   show no significant difference , we finally pick the   first approach . After that , the vector sequence H   forF(i)is represented as :   H= [ h(i ) , h(i ) , ... , h ( i ) ] , ( 2 )   where nis the depth of node iin the tree and h(i)3206is the embedding vector of the nthcoordinate in   the list .   3.2 Encoding tree positions in local and global   Feeding the tree into Transformer requires a lin-   earization method to convert it into a node sequence   first . Since the position feature of each node is   already represented as the corresponding vector se-   quence by our tree description , any AST lineariza-   tion method can be picked for our model . After that ,   we feed all nodes into Transformer and integrate   the position vectors in self - attention by positional   encoding .   3.2.1 Self - attention and positional encoding   Self - attention is one of the key modules of Trans-   former and can be formulated as querying the   key - value pairs . We omit the index of layer for   simplicity and denote x= ( x , x · · · , x)and   z= ( z , z · · · , z)as the input and output of self-   attention in the same layer respectively , where nis   the sequence length . The self - attention is presented   as :   α= ( xW)(xW ) ,   z=/summationdisplayexp ( α)/summationtextexp ( α)(xW),(3 )   where W , W∈R , W∈Ris the   projection matrices for query , key and value , re-   spectively . We set d = d = d. Note that a   scaling factorshould be applied for attention   score αbefore softmax and we just omit it for the   sake of description .   The self - attention in Eq.3 is oblivious to struc-   tured input because it effectively views it as an   unordered set of vectors . In NLP , a common way   to bias Transformer towards potential text order is   to add positional encodings . The original Trans-   former adds the absolute sinusoidal positional en-   coding to the token embeddings . After that , ( Shaw   et al . , 2018 ) proposed the first relative positional   encoding , in which the real - valued vector repre-   sented relative distance is added to the key before   the dot - product between the query and key . More   recently , several works ( Ke et al . , 2020 ; He et al . ,   2020 ) proposed that disentangled attention is better   than adding and applying the same projection for   position and word embedding . The untied absolute   positional encoding proposed by ( Ke et al . , 2020)is presented as :   α=1√   2[(aW)(aW)+α],(4 )   while the disentangled relative positional encoding   presented in ( He et al . , 2020 ) is :   α = 1√   3[(xW)(rW )   + ( rW)(xW)+α],(5 )   where a , aare the absolute position embedding   for position iandj , and r , rare viewed as the   relative position embedding between two positions .   W , W , W , W∈Rare projection ma-   trices for absolute and relative position encodings ,   and scaling factorsandare applied to retain   magnitudes . In conclusion , the attention score of   word embedding αpresented in Eq.3 is added   with the absolute and relative positional attention   score in Eq.4 - 5 , respectively . After that , the atten-   tion score knowing sequential orders is used to the   weighted sum for values .   3.2.2 Attention with tree structure   We modify the untied positional encoding in Eq.4-   5 to learn code AST structure . The Eq.4 and Eq.5   can both efficiently capture the global positional   information in natural languages since both abso-   lute positions and relative distances in texts are   tractable , ranging in max length of 512 commonly .   As for trees , the absolute position in the tree for   each node can be easily drawn from our tree de-   scription , shown in the following details . However ,   it is not trivial to learn relative global relationships   between tree nodes since all cases of structure re-   lation are intractable in O(n)where nis the code   length . This sticking point is alleviated by only   modeling relative distances in trees , such as short-   est path distances ( Zügner et al . , 2021 ) . However ,   tree structures can not be entirely exploited by dis-   tance alone ( Peng et al . , 2021 ) . Another solution   for this crux is only modeling unique relative paths   in ASTs ( Peng et al . , 2021 ) , but the feature cover-   age is still not guaranteed in theory .   On the other side , previous works ( Hellendoorn   et al . , 2019 ; Chen et al . , 2021b ) have proved that in-   troducing local bias as relative one - hop edges only   between adjacent nodes is still powerful to model   tree structure . The local methods show a different   paradigm compared to global approaches , so the   intuitive idea is to integrate the local and global3207methods . For these reasons , we do not pursue cap-   turing the relative global position but only the local   one in this paper . In conclusion , we introduce the   global bias by absolute encoding and local bias by   relative encoding and then integrate them into the   unified Transformer .   The absolute position vector a∈Rfor node   iis presented as :   a = LN(Linear ( Concat ( H ) ) ) , ( 6 )   where we concat vectors in list Hof node ise-   quentially and feed it into transforming linear and   normalization layers . We pad zero vectors for short   Hlists to max tree depth and truncate last for long   lists before concat vectors .   The relative position vector r∈Rbetween   node iandjis :   r=      LN(Linear ( /summationtextH−/summationtextH ) )   if f(i ) = j∨f(j ) = i   ⃗0 if f(i)̸=j∧f(j)̸=i(7 )   In Eq.7 , f(.)is specified as the node ’s parent . For   example , given node xand one of its children y , we   sum the vector lists for these two nodes respectively   and subtract two sum vectors . Thus , the subtrac-   tion vector fed into the linear layer actually is the   embedding of coordinate ( y , y)defined by Eq.1 ,   and all cases of it are tractable in O(m)where mis   the size of coordinate embedding table . The linear   and normalization layers of relative vectors have   different parameters from the absolute ones in Eq.6 .   The relation vector is set as zero if there is no adja-   cency between two nodes , and obviously , only the   local one - hop relationship is actually embedded in   this encoding process .   We first introduce the global absolute position en-   coding into the self - attention of Transformer . The   attention score βbetween absolute positions of   node iandjis presented as :   β= ( aW)(aW ) , ( 8)   where W , W∈Rare projection matrices .   After that , the local relative position attention score   γis presented as :   γ= ( xW)(rW )   + ( rW)(xW)(9 )   where W , W∈Rare projection matrices   for the query and key . We integrate βandγinto Transformer ,   adding to the attention score αof word embed-   dings :   A=1√   2(α+β+γ ) ,   z=/summationdisplayexp ( A)/summationtextexp ( A)(xW).(10 )   Although four vector dot - product exist in A , we   apply the scaling factors asrather thansince   the number of non - zero γfor node iis only the   number of its adjacent node and far less than the   length of input node sequence .   3.3 Discussion for tree positional encodings   Our absolute global position encoding for each   node extracts structure features from the positional   description F ( . ) . All nodes ’ absolute position vec-   tors are scattered in the structural space with vir-   tual nodes as the origin . Comparing F(.)of two   nodes , a path including a series of steps along tree   branches shows clearly . Thus , our model has the po-   tential to capture the pairwise path by dot - products   in Eq.8 of two position vectors . The main dif-   ferences of our absolute positional encoding com-   pared to the approach presented in ( Shiv and Quirk ,   2019 ) are two folds : firstly , we describe each node   position by two dimensions , including not only   the sibling orders but the child number of its par-   ent , while ( Shiv and Quirk , 2019 ) only consider   the first ; secondly , we embed each coordinate into   vector directly and integrate it into the disentan-   gled attention , while ( Shiv and Quirk , 2019 ) pa-   rameterize the one - hot concat of sibling orders and   add the position vector to word embeddings before   feeding into Transformer . Therefore , our absolute   positional encoding can be seen as the advanced   generalization of ( Shiv and Quirk , 2019 ) .   Our proposed relative positional encoding focus   on the local structural relation between adjacent   nodes . The relative position vector presented in   Eq.7 contains many structural features : firstly , the   asymmetric of randrreveals the parent - child   relationship and vice versa ; secondly , two dimen-   sions of coordinates are embedded in relative vec-   tors . Note that our method is different from GREAT   proposed in ( Hellendoorn et al . , 2019 ) . In GREAT ,   each parent node knows its children only by two   types of edges : parent andchild edges ( Chirkova   and Troshin , 2021 ) , which means the parent does   not know the sibling order of its children . Thus,3208our proposed local relative positional encoding can   be viewed as the extension of GREAT .   In both GREAT and our local module , each node   learns structure only from its adjacent nodes , and   its receptive field for structure extends only by   stacking model layers . The local method of in-   troducing bias differs from the global approaches   presented in ( Shiv and Quirk , 2019 ) and our abso-   lute position encoding . In this work , we integrate   the global and local methods and further analyze   their relationship . See Fig2 for explanation to our   model .   4 Experiment setup   We focus on two tasks of code representation learn-   ing : code summarization and completion . Code   summarization is one of the most popular tasks   in which the function name is predicted given a   function body . The method body typically forms   complete logical units , and the function name tends   to be precisely descriptive . Therefore , code sum-   marization is widely used as benchmark by several   previous works ( Allamanis et al . , 2016 ; Alon et al . ,   2018 ; Zügner et al . , 2021 ; Peng et al . , 2021 ) . The   code completion is another useful benchmark , and   we mainly focus on the completion for each tree   node ( Li et al . , 2017 ; Sun et al . , 2020 ; Kim et al . ,   2020 ) . In this task , AST is linearized in depth - first   order for all baselines . Each node is then predicted   given the partial tree built on all the previous nodes   in depth - first order .   In both summarization and completion tasks , we   use the Python150k andJavaScript150k datasets   preprocessed by ( Chirkova and Troshin , 2021 ) .   Both datasets consist of program files from Githuband are widely used to evaluate code represen - tation models . To avoid biased results , duplicate   files are removed by the duplication list provided   by ( Allamanis , 2019 ) , and identical code files and   functions are further filtered out by ( Chirkova and   Troshin , 2021 ) . Both datasets are split into train-   ing / validation / testing sets with 60%/6.7%/33.3 %   based on GitHub usernames .   In this paper , we mainly compare our model with   tree - based Transformer models . Before feeding   trees into all compared Transformers , we linearize   ASTs in depth - first order and convert them as node   sequences . Note that each node in AST is asso-   ciated with a type , but not all nodes have values   ( such as non - leaf nodes in trees ) . Therefore , a spe-   cial < empty > value is associated with nodes that   do not have values . The type and value embeddings   are added as input node features fed into models .   For all compared models in both tasks , node to-   kens are not split into subtokens for computational   efficiency following ( Chirkova and Troshin , 2021 ) .   4.1 Code summarization   In this task , seq2seq models decode function names   according to function ASTs . Sequential positional   embedding is used in Transformer decoder to cap-   ture the order of function names . All top - level func-   tions short than 250 AST nodes are selected from   filtered files . After extracting functions from code   files , we replace function names with the special   < function_name > token and split target function   names based on CamelCase orsnake_case . The   final datasets include 523k/56k/264k functions for   python and 186k/23k/93k for javascript to train-   ing / validation / testing . Multiple metrics are used to   comprehensively measure the quality of generated   function names , including Bleu ( Papineni et al . ,   2002 ) , F1 and Acc . The generated function name   is viewed as a token list for Bleu while seen as an   unordered set for F1 and Acc .   4.2 Code completion   In this task , Transformer decoders with masked   attention are used to predict each node given all   previous nodes in depth - first order . Two linear   layers with softmax are set on top of Transformer   decoder to predict both value and type of the next   node . We use full ASTs of filtered files except   for sequences less than 2 . We split larger ASTs   longer than 500 into overlapping chunks with a   shift of 250 . The overlap in each chunk provides a   context for models , and we count loss and met-   rics over the intersection between chunks only3209once . The details of this part are also presented   in ( Chirkova and Troshin , 2021 ; Kim et al . , 2020 ) .   The final datasets include 186k/20k/100k chunks   for python and 270k/32k/220k for javascript to   training / validation / testing . In this task , we mea-   sure MRR(mean reciprocal rank ) and Acc for type   and value , respectively . We assign the zero score   for MRR if the correct token is out of the top 10   candidates . Besides , we measure the combined   Acc(all ) and assign true only when both value and   type are correct for each node .   4.3 Hyperparameters   Our model and Transformer baselines all have 6   layers , 8 heads , hidden size D= 512 and FFN   dimension D= 2048 . The vocabulary sizes   for values are 50K/100 K for summarization and   completion tasks respectively , and all types are   preserved . We train all Transformers using Adam   ( Kingma and Ba , 2014 ) with a starting learning   rate of 1eand batch size of 32 . We train all   models with 20/60 epochs for summarization and   20/20 epochs for completion in python / javascript .   In summarization , we decay the learning rate by 0.9   in python , use a constant learning rate in javascript ,   and a gradient clipping of 5 for both languages .   We use the cosine learning rate schedule with 2k   warmup steps with a zero minimal learning rate   in completion . In conclusion , the training settings   mostly follow ( Chirkova and Troshin , 2021 ) but   with slight differences .   In our model , we set the maximum children num-   ber for each tree as 16 for all tasks and languages ,   meaning each dimension in 2D coordinates ranges   from 1 to 16 . The coordinate beyond the upper limit   is set as the maximum of 16 . Therefore , the size of   the coordinate embedding table is= 136   since the first dimension of 2D coordinates always   less than or equal to the second one . We set the em-   bedding dimension 32/32 for summarization and   32/16 for completion in python / javascript , respec-   tively . Thus , the embedding table for coordinates   has very few parameters . Besides , we set the max   tree depth as 16/16 for summarization and 16/32 for   completion in python / javascript . The hyperparame-   ter setting of maximum children and depth covers   almost all samples . In our implementation , the po-   sitional vectors aandrare shared across different   heads , while the projection matrices in Eq.8 - 9 are   different for all heads . These matrices are shared   in different layers for efficiency , which means wecalculate the positional attention score only once .   In summary , we introduce only about 1.3 M new   parameters , which is insignificant compared to the   full parameters of Transformer architecture .   4.4 Baselines   Four different Transformers are picked as base-   lines of our model , including vanilla Transformer   ( Vaswani et al . , 2017 ) , Transformer with relative   positional encoding presented in ( Shaw et al . , 2018 )   and the tree - based Transformers propose by ( Shiv   and Quirk , 2019 ) and ( Kim et al . , 2020 ) . Before   feeding trees into models , ASTs are uniformly lin-   earized into node sequences in depth - first order .   Therefore , although the former two Transformers   are designed initially for natural language , they still   have the potential to capture tree structure from the   depth - first order . The Transformers in ( Shiv and   Quirk , 2019 ) and ( Kim et al . , 2020 ) learn tree struc-   ture by different positional encoding for trees rather   than traversal orders , similar to our model . All mod-   els are trained three times to estimate mean ±std on   one Tesla V100 GPU , and we set all hyperparame-   ters of baselines following the best practices shown   in ( Chirkova and Troshin , 2021 ) .   We do not introduce GREAT shown in ( Hellen-   doorn et al . , 2019 ) as one of the baselines following   ( Chirkova and Troshin , 2021 ) , also since our model   can be seen as the generalization of it discussed   in section 3.3 . The Transformer models jointly   learn from both context and structure ( Zügner et al . ,   2021 ; Peng et al . , 2021 ) are also not introduced as   our baselines . Although it is already proven that   combined multiply modality is helpful for represen-   tation learning of code , we firmly believe that it is   still meaningful to explore tree - based Transformer   only for AST structure .   5 Results   5.1 Overall comparison   The overall results of code completion and summa-   rization tasks are shown in Table 1 and 2 , respec-   tively . Note that since slight training differences ,   the result of baselines are not entirely consistent   with ( Chirkova and Troshin , 2021 ) , but the overall   trend is almost identical .   Our model substantially outperforms baselines   almost in all metrics of both tasks . Firstly , our   model outperforms baselines for completion task   of different languages in almost all metrics shown   in Table 1 . The only exception is to predict node3210   value in javascript , while our model still gains a   comparable performance to ( Shaw et al . , 2018 ) .   Interestingly , some baselines only perform well in   predicting either type or value . For example , the rel-   ative sequential Transformer in ( Shaw et al . , 2018 )   performs best to predict node value in javascript   but is poor for node type . Thus , we additionally   calculate the Acc(all ) to measure whether models   accurately predict both type and value simultane-   ously . The final result highlights the effectiveness   of our model for predicting both type and value .   Secondly , our model shows effectiveness in sum-   marization task for both languages in Table 2 . It   is worth noting that although baselines perform   similarly , consisting of observations in ( Chirkova   and Troshin , 2021 ) , our model still improves from   them . It may indicate that previous works do not   learn code AST structure well and therefore meet   the performance bottleneck in this task .   5.2 Ablation study   We explore the roles of each part of our approach ,   including two dimensions in each coordinate and   global / local modules to introduce structural bias .   Two dimensions of tree description Our ap-   proach describes each node ’s position as a 2D co-   ordinate list . To verify the benefit of learning from   both dimensions , we consider removing one di - mension of each coordinate before feeding it into   Transformer . The ablation results for each dimen-   sion on different tasks are shown in Table 1 and 2 .   We find that model ’s performance shows noticeable   declines after removing either dimension , proving   that all dimensions are helpful .   Global and local encoding for tree position Our   model introduces global and local soft bias to en-   code tree structures . To explore the roles of each   part , we remove either global or local modules in   self - attention of Transformer , which means only   adding either βorγto the attention score αof   word embedding in Eq.10 . The ablation results   are shown in Table 1 and 2 . We find that both en-   coding methods are powerful for each task : only   global or local method already outperforms almost   all baselines . The comparison again proves that 2D   description for trees is beneficial and also shows   that the disentangled attention mechanism shown   in NLP is still helpful in modelling complex tree   positions . We then compare the performance of   each part to the whole model and find that model ’s   performance improves further than the single lo-   cal or global component . For this phenomenon ,   we speculate that since the local method extracts   the relationship between nodes by strictly stacking   model layers , local features are less likely to be lost.3211   On the other hand , although the global method the-   oretically knows the complete relationship between   arbitrary nodes , global bias is relatively soft and   perhaps fragile . As a result , the local way shows   benefits to polishing the global perspective . To the   best of our knowledge , no previous work explored   combining them , while we finally show the benefits   of fusing the local and global paradigms .   6 Conclusion   We propose a new tree Transformer encoding   position for each node based on our novel two-   dimensional description of tree structures . Tech-   nically , our model introduces soft bias as the posi-   tional encoding of Transformer in global and local   ways . Our model finally outperforms strong base-   lines on code summarization and completion tasks   across two different languages , highlighting the   effectiveness of our approach .   7 Acknowledgements   We thank all reviewers for their constructive com-   ments and Kechi Zhang for the discussion on the   manuscript . This research is supported by the Na-   tional Natural Science Foundation of China un-   der Grant No . 62072007 , 62192733 , 61832009 ,   62192731 , 62192730 .   Limitations   In this work , we propose a new tree transformer   and compare it with several baselines in two tasks   of different languages . To comprehensively and   precisely measure the performance of our model   and all baselines , we train all models three times .   We do not entirely rely on the results produced by   previous work , and just choose them as references . As a result , our experiments may produce a lot of   carbon dioxide and consume electrical power .   In this paper , we mainly focus on the representa-   tion learning for source code on the function level   ( or in similar length as functions ) and do not dis-   cuss the model scalability to the corpus of extreme   long source code ( such as full program files ) . We   believe that learning from long source code is an in-   teresting and valuable research topic , and perhaps   explore extending our model for it in the future .   We believe our approach should be workable on   most tasks related to code trees in theory . How-   ever , since additional code graph structure , includ-   ing data - flow and control - flow , is n’t considered   in our approach , some related tasks may not be   suitable . Besides , the additional context feature   of code tokens is also not discussed , and recently   Transformer models jointly learn from both context   and tree structure are not mentioned as our base-   lines . To combine context and structure , a starting   point is to map tree nodes to code tokens . After   that , it should be useful for our model to add po-   sitional encoding of code context orders for these   assigned nodes , and we perhaps explore it in the   future .   References321232133214