  Sunjae Kwon , Rishabh Garodia , Minhwa Lee , Zhichao Yang , Hong YuUMass Amherst , UMass Lowell , UMass Chan Medical School , V A Bedford Health Care   sunjaekwon@umass.edu , rgarodia@umass.edu , minhwalee@umass.edu   zhichaoyang@umass.edu , hong_yu@uml.edu   Abstract   Visual Word Sense Disambiguation ( VWSD ) is   a task to find the image that most accurately de-   picts the correct sense of the target word for the   given context . Previously , image - text matching   models often suffered from recognizing poly-   semous words . This paper introduces an unsu-   pervised VWSD approach that uses gloss infor-   mation of an external lexical knowledge - base ,   especially the sense definitions . Specifically ,   we suggest employing Bayesian inference to   incorporate the sense definitions when sense   information of the answer is not provided . In   addition , to ameliorate the out - of - vocabulary   ( OOV ) issue , we propose a context - aware defi-   nition generation with GPT-3 . Experimental re-   sults show that VWSD performance increased   significantly with our Bayesian inference - based   approach . In addition , our context - aware def-   inition generation achieved prominent perfor-   mance improvement in OOV examples exhibit-   ing better performance than the existing defini-   tion generation method .   1 Introduction   With the development of deep learning technol-   ogy , research on multimodality such as Visio-   Linguistic Models ( VLMs ) has been actively con-   ducted ( Schneider and Biemann , 2022 ) . In par-   ticular , state - of - the - art VLMs , such as image - text   matching ( ITM ) models ( Radford et al . , 2021 ;   Singh et al . , 2022 ) and text - to - image generation   models ( Rombach et al . , 2022 ; Seneviratne et al . ,   2022 ) , are employed in many industrial projects ,   including image retrieval systems ( Yuan and Lam ,   2021 ; Yuan et al . , 2021 ) and AI - assisted image gen-   erators ( Das and Varshney , 2022 ; Seneviratne et al . ,   2022 ) .   Visual Word Sense Disambiguation ( VWSD ) is   a multimodal task of natural language processing   ( NLP ) and computer vision that selects the image   which corresponds to the intended meaning of the   target word among a set of candidate images ( Ra - Figure 1 : An example of VWSD from SemEval-2023   task 1 dataset ( Raganato et al . , 2023 ) . We can see that   even if the target word ( ‘ Angora ’ ) is the same , different   images should be selected according to the context .   ganato et al . , 2023 ) . Figure 1 is an example of   VWSD . For the ambiguous target word‘Angora ’ ,   we can notice that the answer image should be con-   ditionally changed regarding the context . VWSD   can play an important role in several downstream   tasks including image retrieval ( Chen et al . , 2015 ) ,   action recognition ( Gella et al . , 2017 ) and visual   question answering ( Whitehead et al . , 2020 ) .   Unsupervised VWSD can be formulated in the   same way as the ITM task ( Cao et al . , 2022 ) , that   is , finding the images that best match the given con-   text . However , VWSD often requires more com-   plex reasoning on both text and images than con-   ventional ITM models . The example in Figure 2   demonstrates that CLIP ( Radford et al . , 2021 ) , a   state - of - the - art ( SOTA ) ITM model , fails to recog-   nize the answer image for the given context . This   limitation of VLMs , where they fail to handle am-   biguous words , was also reported in another study   on an image generation model ( Rassin et al . , 2022).1583To ameliorate this problem , we propose to disam-   biguate visual words with the assistance of a glos-   sary of lexical knowledge - bases ( LKBs ) without   the use of any further training or dataset . Specifi-   cally , we utilize the sense definitions of an ambigu-   ous word that have been widely exploited in previ-   ous lexical semantic tasks ( Raganato et al . , 2017 ;   Gella et al . , 2017 ; Pilehvar and Camacho - Collados ,   2019 ) . Herein , since the answer sense of the target   word is not provided in the VWSD setting , we pro-   pose an approach derived from Bayesian inference ,   using pretained ITM models . Moreover , in order   to deal with out - of - vocabulary ( OOV ) words that   can not find the sense definitions of the target word   in LKBs , we suggest the concept of context - aware   definition generation ( CADG ) . The definitions of   a target word are generated by a large language   model , GPT-3 ( Brown et al . , 2020 ) , as auxiliary   information for VWSD .   Experiments were conducted on SemEval-2023   ( SE23 ) Task 1 - Visual - WSD ( Raganato et al . , 2023 ) ,   a publicly available VWSD dataset . Furthermore ,   in the experiments , we utilized two pretained SOTA   ITM models : ( 1 ) CLIP ( Radford et al . , 2021 ) and   ( 2 ) FLA V A ( Singh et al . , 2022 ) . Experiments   showed that our proposed approach significantly   improved the performance of baseline ITM mod-   els . In addition , we demonstrated that our con-   cept of CADG not only significantly increased the   performance of OOV cases but is also more ad-   vantageous than the previous definition generation   approach . We implement experimental codes in   https://github.com/soon91jae/UVWSD .   The contributions of this paper can be summa-   rized as follows :   •This paper introduces a new gloss-   incorporated VWSD approach inspired   by Bayesian inference .   •Experimental results show that our Bayesian   inference - based approach boosted the unsu-   pervised VWSD performance significantly   without any additional training .   •Furthermore , we suggest the CADG method   to challenge the OOV issue .   2 Related Work   2.1 Word and Visual Sense Disambiguation   VWSD task is closely relevant to a line of sense dis-   ambiguation studies . One of them is Word Sense   Disambiguation ( WSD ) which automatically identi-   fies ambiguous words into corresponding senses ( O   et al . , 2018 ) . The early stage of WSD research tried   to employ diverse information in LKBs with unsu-   pervised manners such as lexical similarity ( Kilgar-   riff and Rosenzweig , 2000 ) , knowledge - graph con-   nectivity ( Agirre et al . , 2014 ; Kwon et al . , 2021 ) ,   and topic modeling ( Chaplot and Salakhutdinov ,   2018 ) . After the emergence of pretrained language   models ( LMs ) such as BERT ( Devlin et al . , 2019 ) ,   LM - based transfer learning approaches have been   actively studied ( Huang et al . , 2019 ; Barba et al . ,   2021b ) . In particular , gloss - enhanced WSD models   that use sense definition and context together using   a cross - encoder ( Huang et al . , 2019 ; Barba et al . ,   2021a ) or bi - encoder ( Blevins and Zettlemoyer ,   2020 ) structures are not only overwhelm existing   approaches but also robust against few - shot exam-   ples . Wahle et al . ( 2021 ) suggest incorporating   WordNet knowledge into LMs while pre - training   them . Specifically , the authors utilize a multi - task   learning method that trains LMs with both mask   language modeling loss and WSD task loss .   Visual Verb Sense Disambiguation ( VVSD ) is   another task relevant to VWSD . VVSD is a mul-   timodal sense disambiguation task that selects the   correct sense of a pair of a given ambiguous verb   word and image ( Gella et al . , 2017 ) . Gella et al .   ( 2017 ) suggest an unsupervised VVSD approach   that takes advantage of various Visio - linguistic fea-   tures ( image representation , object label , image   caption features ) together and calculates the match-   ing score between an image and a sense definition1584   with a variant of Lesk algorithm . Vascon et al .   ( 2021 ) propose a semi - supervised VVSD method   based on game theoretic transduction for inference .   Meanwhile , Gella et al . ( 2019 ) demonstrate that   a VVSD model trained on multi - lingual VVSD   dataset not only benefit the performance on verb   sense disambiguation but also boost the perfor-   mance of a downstream task , the multi - modal ma-   chine translation task .   Our work is related to gloss - enhanced WSD   models in that we are using both sense definition   and context together . However , our study differs   from previous WSD studies in that it tackles a   multi - modal task . It is also relevant to VVSD in   terms of multi - modal sense disambiguation . How-   ever , VVSD systems ( Gella et al . , 2016 ) are usually   designed to analyze a small number of verb words ,   while the VWSD task contains a lot of nouns and   adjectives . Finally , our work tackles a new VWSD   task and we introduce a method of implementing   sense definitions with SOTA ITM models based   on Bayesian inference where sense definitions as a   latent variable .   2.2 Definition Generation   Our CADG is related to the definition generation   task introduced by Noraset et al . ( 2017 ) . The pur-   pose of the task is to generate a definition for a   given word . Noraset et al . ( 2017 ) suggest utilizing   recurrent neural network - based LMs ( RNNLMs)with the definitions collected from WordNet and   GNU Collaborative International Dictionary of En-   glish ( GCIDE ) . Gadetsky et al . ( 2018 ) propose def-   inition generation models to handle polysemous   words with context and the soft - attention mecha-   nism . Li et al . ( 2020 ) propose to perform seman-   tic decomposition of the meanings of words and   then use discrete latent variables to model them   to generate definitions . Malkin et al . ( 2021 ) show   that a large language model ( GPT-3 ) could gen-   erate definitions of neologisms without additional   fine - tuning . Herein , the authors suggest generating   neologisms with long short - term memory ( LSTM )   ( Yu et al . , 2019 ) and definitions of neologisms with   a large pretrained LM , GPT-3 ( Brown et al . , 2020 ) .   CADG is similar to the one used by Malkin et al .   ( 2021 ) , which involves generating definitions using   GPT-3 . However , CADG differs in that it takes   context into account when generating prompts . Ad-   ditionally , this study differs from previous work in   that it takes context into account when generating   prompts and demonstrates that the definitions pro-   duced by CADG can be effectively used in down-   stream tasks , rather than focusing solely on the   definition generation task itself .   3Task Definition on Unsupervised VWSD   We formulate unsupervised VWSD as a multiclass   classification task ( Aly , 2005 ) as shown in Eq . 1 .   Unlike the image retrieval task ( Jing et al . , 2005)1585that ranks the most relevant images for the given   text or keyword , VWSD is designed to choose a   specific target tin the given context c. Specifically ,   we define the task to find the image ˆvwith the   highest posterior probability from a set of images   Vthat consists of one answer image and other   distractors on the target word .   ˆv= argmaxP(v|c , t ) ( 1 )   Any pretrained ITM models ( e.g. , CLIP ) can cal-   culate the posterior . In Figure 2 , a set of candidate   images Vis entered into the image encoder for the   target word t. At the same time , the context cthat   includes tas a part is entered into the text encoder .   Then , the inner product of the output hidden rep-   resentations on images hand the context h   are input to softmax function , which then computes   a probability distribution over the images . Finally ,   the image that produces the highest probability will   be selected as the prediction of the model for the   target t , provided the context c.   4 Unsupervised VWSD Incorporating   Gloss Information   Usually , zero - shot ITM models are pretrained with-   out much consideration of polysemous words . For   example , Figure 2 demonstrates that CLIP fails to   predict the correct answer for the target word ‘ An-   gora ’ , although it is provided with a clear hint of   ‘ city ’ in the given context . Therefore , the zero - shot   performance of pretrained ITM models may be lim-   ited in the VWSD task . One solution is to use gloss   information of a lexical knowledge - base ( LKB ) ,   particularly exploiting sense definitions . This is   because the definitions in LKBs elaborate on each   sense for readers who do not know the meaning .   Thus , we assume that the sense definitions in LKBs   can boost ITM models to conduct VWSD , by inject-   ing the meaning of the correct sense on the input   of these models . However , since there is no correct   sense information for the target word , it is difficult   to apply it directly . For this reason , we suggest a   novel gloss - incorporated VWSD approach inspired   by Bayesian inference , as presented in Eq . 2 .   Suppose Dis a set of definitions for the target   word textracted from an LKB . Herein , by using   the chain rule , the posterior can be divided into two   conditional probabilities associated with a latentvariable D.   P(v|c , t ) = /summationdisplayP(v|D , c , t)P(D|c , t)(2 )   In this case , the right term P(D|c , t)(Context to   Definition ; C2D ) is predicting the conditional prob-   ability over the given ith sense definition Dfor the   given target word tand context cwhich is similar   to the gloss - enhanced WSD models ( Huang et al . ,   2019 ; Blevins and Zettlemoyer , 2020 ) . Meanwhile ,   the left term P(v|D , c , t)(Definition to Image ;   D2I ) is the conditional probability of vfor a given   theith sense definition , the context and the target   word . In doing so , it allows for the development of   sophisticated ITMs by enriching the context with   its relevant sense definition . Finally , we can calcu-   lateP(v|c , t)by marginalizing over all available   sense definitions D.   Figure 3 demonstrates an illustrative concept of   our gloss - incorporated VWSD approach with a pre-   trained CLIP . First , similar to the original CLIP , a   set of candidate images Vand a context care input   to the image encoder and the text encoder , respec-   tively . Meanwhile , a set of definitions of the target   word Dis extracted from an LKB . In our work ,   we utilize WordNet ( Miller , 1995 ) which has been   widely used in previous semantic analysis tasks   ( Pilehvar and Camacho - Collados , 2019 ; Bevilac-   qua et al . , 2021 ) as our source of LKB . Then D , c ,   andtare jointly inputted to the text encoder with   the following template .   { context } : { ith sense ’s definition }   C2D is computed by the inner product of the   hidden representations on the definitions d   and the context h. D2I is then calculated by   the inner product of the hidden representations of   the input images handd . Both C2D   and D2I input to the softmax function transformed   into probability distributions . Then , we choose the   image with the highest probability as the prediction .   As a result , for the example in Figure 3 , our model   can predict the correct answer of the given context   ‘ Angora city ’ , whereas the original CLIP wrongly   selects an image of ‘ Angora cat ’ that produced the   highest probability ( as shown in Figure 2 ) , even   though the network topology and the pretrained   parameters in our model are the same as the original   CLIP model.1586   5Handling OOV with the Context - Aware   Definition Generation   Not all words have their definitions available in   a lexical knowledge - base . In particular , proper   nouns , compound words , and foreign words fre-   quently induce OOV issues . For example , in the   SE23 dataset , about 14.33 % of target words ’ defini-   tions are not found in the English WordNet . There-   fore , we propose a solution to tackle the OOV issue   with the definition generation approach . A previous   study showed that GPT-3 can generate the defini-   tion of a novel word ( Malkin et al . , 2021 ) . However ,   since this study does not consider the context of   the word , it may not generate the definition in the   correct sense . Thus , we suggest generating a defini-   tion with the prompt that considers both the context   and the target word together .   Figure 4 presents the generated definitions by   the approach of Malkin et al . ( 2021 ) ( Figure 4a )   and ours ( Figure 4b ) . Here , we add a conditional   sentence that inputs the context of a target word .   For example , when the target word is ‘ angora ’ and   the context is ‘ angora city ’ , we use a conditional   sentence , “ Define “ angora ” in angora city . ” , in front   of the previous input “ angora ( n ) ” . Indeed , in the   example , the definition generated with our method   shows a better description compared to the previous   method.6 Experiments   6.1 Experimental Dataset   SE23 We used the dataset in the SemEval-2023   Task 1 VWSD challenge . It consists of 12,896   examples and 13,000 candidate images . Each ex-   ample has 10 candidates that include 1 answer im-   age and 9 distractors . Each context averagely con-   tains 2.5 words . The dataset contains 14.33 % OOV   words ( 1,845 out of 12,869 ) .   6.2 Experimental Setting   VWSD For the experiments , we adopted two   SOTA zero - shot ITM models , CLIPand FLA V A , as pretrained parameters are publicly available   for both of them . Note that CLIP uses the text   encoder and the image encoder at the same time   while FLA V A contains the text encoder , the image   encoder , and the multi - modal encoder . Herein , to   calculate an image - text matching score , FLA V A   uses the multi - modal encoder that cross - encodes   image and text features simultaneously . In the case   of calculating C2D , we exploit FLA V A ’s text en-   coder as the same as Figure 3 .   We used WordNet 3.0as the main LKB . We   also compare two GPT-3 generated definitions . The   first one is Malkin et al . ( 2021 ) ’s definition genera-   tion ( DG ) . The other one is CADG ( as described   in Section 5 ) . WN+CADG applies CADG ’s defini-   tions in the case of OOV and uses WordNet defini-   tions otherwise .   Definition Generation We re - implemented   Malkin et al . ( 2021 ) ’s definition generation   experimental setting . Specifically , we sam-   pled a definition for each example by utilizing   GPT-3 ’s Davinci variant which is known as the   largest model and we generated samples with a   temperature of 1.0 .   Evaluation Criteria Following Raganato et al .   ( 2023 ) ’s setting , we evaluated VWSD models ’ per-   formance with the hits at 1 ( Hits@1 ) and the mean   reciprocal rank ( MRR ) . Moreover , we used Stu-   dent ’s t - test ( Student , 1908 ) , to verify the signifi-1587   cance of differences in performance between mod-   els .   Others We prepared a pretrained WSD ,   T5 ( Wahle et al . , 2021 ) . This model is a   generative WSD model that a T5 - large model   ( Raffel et al . , 2020 ) fine - tuned with SemCor   ( Raganato et al . , 2017 ) . Note that , SemCor   is a large size word sense dataset annotated   with the WordNet sense repository . Herein , we   utilized the official checkpoint . In addition , we   employed NLTK ( Bird et al . , 2009 ) to conduct   word tokenization and part - of - speech tagging . All   experiments were conducted on an NVIDIA A100   GPU with Ubuntu 22.04 version .   6.3 Experimental Results   The experimental results in Table 1 show that the   performances of CLIP and FLA V A are 73.00 and   70.13 on Hits@1 , respectively . Incorporating defi-   nition descriptions of external LKB ( WN ) or gen-   erated ( DG and CADG ) significantly enhanced the   performance in every experimental model . First ,   incorporating WordNet with our Bayesian style   inference ( WN ) outperformed both of ITM mod-   els , 8.98%p in CLIP ( p < 1e−10 ) and 8.72%p   ( p < 1e−10 ) . DG and CADG also significantly   improved performance in all cases ( p < 1e−7 ) ,   but the increment in FLA V A was relatively lower   than that of the CLIP . WN+CADG achieved the   highest performance in both of CLIP and FLA V A.   On the other hand , to scrutinize the reasons for   the performance improvements in more detail , we   categorized examples into three categories accord-   ing to the number of WordNet senses ( |D| ) of the   target word . |D|= 0 examples are target words   with no entry in WordNet ( OOV ) . |D|= 1exam-   ples are target words with only one sense in the   WordNet ( trivial ) . |D|>1examples are target   words with more than one sense in the WordNet   ( ambiguous ) .   Figure 5 presents that incorporating WordNet   definition enhanced the performance on ambiguous   and trivial words in both of CLIP and FLA V A. In   particular , the performance gain was remarkable in   trivial words ( from 71.34 to 85.91 and from 69.83   to 81.99 for CLIP and FLA V A , respectively ) . More-   over , even for ambiguous words , the performance   is significantly improved ( p < 1e−3 ) without any   additional training or the assistance of external sys-   tems such as WSD models . CADG substantially   increased performance in both of OOV and trival   words . Especially , when compared to DG , the per-   formance differences are remarkable in OOV .   Meanwhile , while FLA V A shows prominent im-   provement via WordNet integration , the impact of   generated definitions tends to be low compared to   CLIP . Considering that WordNet definitions were   manually constructed by experts , we speculate that   this is because the model is sensitive to the quality   of the input definitions .   7 Discussion   7.1 Analysis on Ambiguous Target Words   We analyzed the performance change according to   the ambiguity level of the ambiguous target word.1588   Table 2 presents the predictive change of the CLIP   after incorporating WordNet . Herein , 523 exam-   ples go correct while 190 examples go incorrect . In   particular , even in the case of highly ambiguous ex-   amples with |D|greater than 10 , the improvement   rate is 1.93 , and incorporating WordNet positively   affects the performance improvement . These re-   sults are in line with previous research findings that   ambiguous words can be recognized pre - trained   LMs according to the given context ( Garí Soler and   Apidianaki , 2021 ; Kwon et al . , 2022 ) . However ,   compared to the lower ambiguous cases , the per-   formance improvement rate is lower . These results   implies that enhancement for the highly ambiguous   words are required .   Although WordNet integration improves perfor-   mance for ambiguous target words , we still want   to find out how competitive the performance im-   provement is . For this reason , we compared the   performance of our WordNet - incorporated model   with that of the pipeline system using the WSD   model . To be specific , T5 , a finetuned WSD   model , predicts WordNet sense in a given target   word and context . The probability distribution for   the candidate images was calculated based on the   predicted sense .   Table 3 is the prediction result for ambiguous tar-   get words . Our model showed comparable results   in the pipeline system and Hits@1 and achieved   higher performance in MRR . This is due to the   error cascading issue of pipeline systems ( Finkel   et al . , 2006 ; Kwon et al . , 2019 ) . That is , in the   pipeline system , errors in the WSD model directly   lead to performance decrement . Otherwise , our   approach is rather free from error cascading , since   the C2D probability and the D2I probability work   complementary to each other .   7.2 Analysis on the Generated Definitions   7.2.1 Evaluation on the Generated Definitions   In order to evaluate the quality of the generated def-   initions , we randomly sampled 200 examples from   SE23 dataset . For each example , two annotators   evaluated the ( binary ) agreement on the generated   definitions with Malkin et al . ( 2021 ) ’s approach   ( DG ) and our approach ( CADG ) . Inter - annotator   agreement ( Kvålseth , 1989 ) was κ= 0.625 . Fi-   nally , we only accept 159 examples of DG and 166   examples of CADG unanimously agreed by the   annotators .   Table 4 represents the average human agreement   scores on DG and CADG . The results show that our   CADG achieved a higher performance compared   to DG . Especially , in Figure 4 and Table 5 , we   can find that the definitions of ambiguous words   generated with CADG are semantically similar to   that of the WordNet answer sense compared to DG ,   in line with the purpose for which it was designed .   7.2.2 Impact of the Generated Definitions ’   Quality   We also verified whether the quality of the gen-   erated definitions would affect the VWSD perfor-   mance . Table 6 presents the experimental results   on VWSD examples when we utilize the gener-   ated definitions that agreed ( Correct ) and disagreed   ( Incorrect ) by the both annotators . Table 6 demon-   strates that the quality of the generated definitions   affects the performance of the downstream VWSD   task indeed.1589   7.2.3 Experiments on Multiple Generated   Definitions   Since we sampled a definition for each input ex-   ample in main experiments , it is still questionable   whether the number of sampled definitions affects   the performance of the model . Table 7 indicates   the performance of DG and CADG according to   the number of generated definitions ( n ) for each   input . The results show that the number of sampled   definitions is not significantly affecting the model ’s   performance . To be specific , when the number of   generated definitions is 2 for each input , the per-   formance of DG and CADG increased by 0.09%p   and 0.03%p respectively . Furthermore , when the   number of generated definitions is 3 , we can see   that the performance even slightly decreases both   DG and CADG . As a result , sampling multiple def-   initions for each input does not significantly affect   performance or rather decreases performance .   7.3 Error Analysis   7.3.1 VWSD   Our model still suffers from error cascading from   C2D probability though it is mitigated by the   Bayesian style inference . The most typical error   case is due to the error cascading in C2D prob-   ability calculation . Especially , due to the nature   of neural networks ( Guo et al . , 2017 ) , the over-   confidence in the error classes frequently causes   errors . For example , in Table 8 , we found that   among the 10 senses of the target word ‘ paddle ’   extracted from WordNet , the conditional probabil-   ity for the correct sense was calculated as 0.00 % ,   resulting in an error in the final posterior calcula-   tion . Another error case is when there is no correct   sense in WordNet . In the example , the target word   ‘ Thompson ’ indicates a firearm , but WordNet con-   tains only personal information . This is a separate   issue from OOV with no entry for the target word ,   and we observed that it mainly occurs in proper   nouns .   7.3.2 Definition Generation   We found two representative error cases in the re-   sults of the definition generations : 1 ) misdisam-   biguation and 2 ) hallucination . The misdisambigua-   tion is when the GPT3 generates the polysemy ’s   definition . In Figure 6a , considering the context   of “ lime oxide ” , we would expect a definition of   lime stone to be generated . However , we can notice1590that both approaches generate a definition for lime   fruit . On the other hand , as pointed out in previous   research ( Ishii et al . , 2022 ) , we also observed that   GPT3 generates hallucinations . Figure 6b is an ex-   ample of the hallucination issue . albatrellus which   is a type of a fungi in the context of “ albatrellus   genus , ” nevertheless the definitions generated by   both approaches are pertaining to the albatross , a   species of bird . Detailed examples of error cases   can be found in Appendix A.   8 Conclusion and Future Work   This paper introduces a novel VWSD methodology   to effectively incorporate gloss information from an   external resource . Our work mainly has two innova-   tions : 1 ) Bayesian style inference for SOTA ITMs ,   and 2 ) Context - aware definition generation with   GPT-3 to overcome the OOV issue . Experimen-   tal results show that our proposed Bayesian style   inference - based WordNet integration significantly   improves VWSD performance without additional   training . For the ambiguous target words , the per-   formance of our approach is comparable to pipeline   systems using finetuned WSD models . Moreover ,   context - aware definition generation helps mitigate   OOV issues in the downstream VWSD tasks and   shows higher performance compared to the previ-   ous definition generation approach .   In the future , we plan to tackle the error cascad-   ing caused by over - confidence in C2D probability .   For this , we may explore a prompting that is known   to have good performance in zero - shot prediction   ( Liu et al . , 2023 ) . In addition , to deal with the   hallucination and misdisambiguation problems of   GPT-3 generated definitions , we may employ con-   trollable generation by resampling ( Ji et al . , 2022 ) .   Limitations   Our work has the following limitations . First , we   only used one evaluation data , namely SE23 , be-   cause it is the only data suitable for the VWSD   setting , especially for the OOV examples . In addi-   tion , our methodology relies entirely on WordNet .   Therefore , this may be limited the model ’s ability   when the target word is a proper noun such as a   named entity . Finally , we depend on the results of   GPT-3 definition generation to handle OOV words .   Since the generated definitions may contain errors ,   as revealed in the qualitative analyses , the errors   led to incorrect predictions .   Ethical Consideration   The generated definitions were annotated by two   annotators . Both annotators were fully paid by   complying with local minimum wage regulation . In   addition , in the sampled definition generations , the   authors could not find any statements violating the   ACL anti - harassment policy . However , generated   definitions that authors have not vetted are still   at risk of containing toxic or hates contents ( e.g.   racism , insulting or xenophobic ) .   Acknowledgement   Research reported in this study was in part sup-   ported by the Center of Biomedical and Health   Research in Data Sciences ( CHORDS ) in UMass   Lowell .   References1591159215931594A Case Study on Incorrectly Generated Definitions   Table 10 and Table 9 present the all incorrectly generated definitions that described in Section 8 . Herein ,   we found the following three error types : 1 ) Misdisambiguation , 2 ) Hallucination , and 3 ) Others .   First of all , the misdisambiguation cases are caused by bias in the pretraining , and we can notice that   CADG has less misdisambiguation compared to DG . Especially , we can see that GPT-3 generated more   than one definitions of the target words ‘ conch ’ , ‘ reaper ’ , and ‘ ruin ’ in DG , while we could not found   such cases in our approach . On the other hand , hallucination cases are when the generated definitions are   definitions of completely different terms with similar spellings ( ‘ stonechat ’ of CADG , ‘ driftfish ’ of DG ) ,   or cases in which the detailed descriptions are incorrect although they are somewhat similar ( ‘ osteostraci ’   of CADG , ‘ nestor ’ of DG ) . Especially , in Table 10 of ‘ wulfenite ’ and ‘ cordierite , ’ we can notice that   definitions are generated with parts of each lexicon ( " wulfen , " and " cord " ) . Finally , in other cases , the   generated definitions may not be in definition form ( ‘ lynching ’ of CADG , ‘ areca ’ of DG ) , or the contents   of the target word is output as itself ( ‘ wulfenite ’ of CADG).15951596ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   After the conclusion section and before the reference section   /squareA2 . Did you discuss any potential risks of your work ?   In the limitation section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   abstract , section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   section 3,4,5,6   /squareB1 . Did you cite the creators of artifacts you used ?   section 3,4,5,6   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We downloaded those in the ofﬁcial download site . Also , we got allowance to use the dataset from   the creators .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   section 6   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   section 6   C / squareDid you run computational experiments ?   section 6   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 6,1597 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   We do not have hyper - parameters . We just use the pertained irate - text matching model without any   training .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 6   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 6   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 6   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   We provide all annotation results in the attached submission ﬁle   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 6   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Section 6   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   We conducted the annotation on small dataset . Thus we only have two annotators1598