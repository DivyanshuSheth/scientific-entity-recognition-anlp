  Anton Chernyavskiy , Dmitry Ilvovsky , Pavel Kalinin , Preslav NakovHSE University , RussiaYandex , RussiaQatar Computing Research Institute , HBKU , Qatar   { acherniavskii , dilvovsky}@hse.ru   kalinin.pavel.v@gmail.com pnakov@hbku.edu.qa   Abstract   The use of contrastive loss for representation   learning has become prominent in computer   vision , and it is now getting attention in Natu-   ral Language Processing ( NLP ) . Here , we ex-   plore the idea of using a batch - softmax con-   trastive loss when fine - tuning large - scale pre-   trained transformer models to learn better task-   specific sentence embeddings for pairwise sen-   tence scoring tasks . We introduce and study a   number of variations in the calculation of the   loss as well as in the overall training proce-   dure ; in particular , we find that a special data   shuffling can be quite important . Our experi-   mental results show sizable improvements on a   number of datasets and pairwise sentence scor-   ing tasks including classification , ranking , and   regression . Finally , we offer detailed analy-   sis and discussion , which should be useful for   researchers aiming to explore the utility of con-   trastive loss in NLP .   1 Introduction   Recent years have seen a revolution in Natural Lan-   guage Processing ( NLP ) thanks to the advances in   machine learning . A lot of attention has been paid   to architectures , especially for deep learning , as   well as to loss functions . Notably , loss functions   based on similar ideas were proposed in unrelated   papers in different machine learning fields under   different names . This can cause difficulties when   solving new problems or when designing new ex-   periments based on previous results . To a greater   extent , this applies to “ universal ” loss functions ,   which can be applied in different machine learning   areas and tasks such as Computer Vision ( CV ) , Rec-   ommendation Systems , and NLP . An example of   such loss function is the batch - softmax contrastive   ( BSC ) loss , which we will discuss below . For many   NLP tasks , it is important to obtain representations   of sentences for semantic matching problems , since   they can be used for further analysis , e.g. , for find-   ing the best answer to a question . Sentence BERT is a recent popular approach   for this ( Reimers and Gurevych , 2019 ): it can   be trained with different loss functions , and we   show that the choice of a loss function is impor-   tant . Moreover , we show that it will not be optimal   to take the “ standard ” batch - softmax contrastive   loss , which is used for training SimCSE ( Gao et al . ,   2021 ) , a recent alternative to Sentence BERT , and   we suggest ways to improve its efficiency . Our   contributions can be summarized as follows :   •We study the use of a batch - softmax con-   trastive loss for fine - tuning large - scale trans-   formers to learn better task - specific embed-   dings for pairwise sentence scoring tasks .   •We introduce and study a number of novel   variations in the calculation of the loss such   as symmetrization , incorporating labeled neg-   atives , aligning scores on the similarity ma-   trix diagonal , normalizing over the batch axis ,   as well as in the overall training procedure ,   e.g. , shuffling , trainable temperature , and se-   quential pre - training .   •We demonstrate sizable improvements for a   number of pairwise sentence scoring tasks   such as classification , ranking , and regression .   •We offer detailed analysis and discussion ,   which would be useful for future research .   •We release our code at https://github .   com / aschern / BSC - loss   2 Related Work   The contrastive loss was proposed by Hadsell et al .   ( 2006 ) as metric learning that contrasts Euclidean   distances between embeddings of samples from   one class and between samples from different   classes . Weinberger et al . ( 2006 ) suggested the   triplet loss , which is based on a similar idea , but   uses triplets ( anchor , positive , negative ) , and aims   for the difference between the distances for ( an-   chor , positive ) and for ( anchor , negative ) to be   larger than a margin.116N - pair loss was presented as a generalization of   the contrastive and the triplet losses as a way to   solve the problem of extensive construction of hard   negative pairs and triplets ( Sohn , 2016 ) . A batch   ofNpairs of examples from Ndifferent classes   is sampled , and the first element in each pair is   considered to be an anchor . Thus , for each anchor ,   there are one positive and N´1negative pairs . The   loss contrasts the distances simultaneously using   the softmax function over dot - product similarities .   The approach was used successfully in CV tasks .   The same method of Multiple Negative Rank-   ing for training Dot - Product Scoring Models was   applied to ranking natural language responses to   emails ( Henderson et al . , 2017 ) , where the loss   uses labeled pairs . A similar idea , called Negative   Sharing , was used to reduce the computational cost   when training recommender systems ( Chen et al . ,   2017 ) . Wu et al . ( 2018 ) presented an approach   withN - pairs like logic , as a Non - Parametric Soft-   max Classifier , replacing the weights in the softmax   with embeddings of samples from such classes . It   was also proposed to use L2 normalization and tem-   perature . Yang et al . ( 2018 ) used Multiple Negative   Ranking to train general sentence representations   on data from Reddit and SNLI . Logeswaran and   Lee ( 2018 ) presented a Quick - Thoughts approach   to learn sentence embeddings , which constructs   batches of contiguous sets of sentences , and for   each sentence , contrasts the next sentence in the   text and all other candidates .   A lot of subsequent work focused on maximiz-   ing Mutual Information ( MI ) . van den Oord et al .   ( 2018 ) presented a loss function based on Noise-   Contrastive Estimation , called InfoNCE . It models   the “ similarity ” function that estimates the MI be-   tween the target ( future ) and the context ( present )   signals , and maximizes the MI between temporally   nearby signals . If this “ similarity ” function ex-   presses the dot - product between embeddings , the   InfoNCE loss is equivalent to the N - pair loss up to   some constants . It was also shown that InfoNCE   is equivalent to the Mutual Information Neural Es-   timator ( MINE ) up to a constant ( Belghazi et al . ,   2018 ) , which maximizes a lower bound on MI .   Deep InfoMax ( DIM ) ( Hjelm et al . , 2019 ) improves   MINE , and can be modified to incorporate some   autoregression as InfoNCE . However , the effective-   ness of loss functions such as DIM and InfoNCE   might be primarily connected to MI , rather than to   deep metric learning ( Tschannen et al . , 2020).The idea gained a lot of popularity in the field   of Computer Vision with the advent of SimCLR ( a   Simple framework for Contrastive Learning of vi-   sual Representations ) , which introduced NT - Xent   ( normalized temperature - scaled cross - entropy loss )   ( Chen et al . , 2020 ) . It uses self - supervised learn-   ing , where augmentations of the same image are   used as positive examples and augmentations of   different images are used as negative examples .   Thus , the task is as follows : for each example in a   batch , find its paired positive augmentation . Here ,   theN - pairs loss is modified with a temperature   parameter and with an L2 normalization of embed-   dings to the unit hypersphere . The loss was further   extended for supervised learning as SupCon loss   ( Khosla et al . , 2020 ) , which aggregates all posi-   tive examples ( from the same class ) in the softmax   numerator .   Subsequently , similar kinds of loss functions   were also introduced to the field of Natural Lan-   guage Processing ( NLP ) . Gunel et al . ( 2021 ) com-   bined the SupCon loss with the cross - entropy loss   and obtained state - of - the - art results for several   downstream NLP tasks using RoBERTa . Giorgi   et al . ( 2021 ) , Fang and Xie ( 2020 ) and Meng et al .   ( 2021 ) used NT - Xent to pre - train Transformers ,   considering spans sampled from the same docu-   ment , sentences augmented with back - translation   as positive examples , and sequences corrupted with   MLM . Luo et al . ( 2020 ) proposed to use NT - Xent   in a self - supervised setting to learn noise - invariant   sequence representations , where sentences aug-   mented with masking were considered as positive   examples . Finally , Gao et al . ( 2021 ) introduced   the SimCLR loss to NLP under the name SimCSE   ( Simple Contrastive Learning of Sentence Embed-   dings ) , where sentences processed by a neural net-   work with dropout served as augmentations of the   original sentences . Here , we explore various ways   to use a similar loss function for pairwise sentence   scoring tasks .   While the above - described loss functions have   different names , they are all based on similar ideas .   Below , we will use the name Batch - Softmax Con-   trastive ( BSC ) loss , which we believe reflects the   main idea best . In our experiments below , we will   use the “ modern ” variant of the loss : with tem-   perature , normalization , and symmetrization com-   ponents ( described in more detail in Section 3.1 ) .   These components were not used for NLP in com-   bination before.117   We further introduce a number of novel and im-   portant modifications in the definition of the loss   and in the training procedure , which make it more   efficient , and we show that using the resulting loss   yields better task - specific sentence embeddings for   pairwise sentence scoring tasks .   3 Method   3.1 Batch - Softmax Contrastive ( BSC ) Loss   Pointwise approaches for training models for pair-   wise sentence scoring tasks , such as mean squared   error ( MSE ) , are problematic as the loss does not   take the relative order into account . For instance ,   for two pairs with correct target scores ( 0.4 , 0.5 ) ,   the loss function would equally penalize answers   like ( 0.3 , 0.6 ) and ( 0.5 , 0.4 ) . However , the first pair   is better , as it keeps the correct ranking , while the   second one does not . This is addressed in pairwise   approaches , e.g. , in triplet loss , where the model   directly learns an ordering . Yet , there is a problem   for constructing pairs or triplets in the training set ,   as it is hard to find non - trivial negatives examples .   Unlike traditional pairwise loss functions , the   BSC loss treats all other possible pairs of examples   in the batch as “ negatives . ” That is , only positive   pairs are needed for training . Consider a batch   Xof pairs from a question - answering dataset . In   general , let QandAbe the matrices of   embeddings produced by a query model and an an-   swer model . We define the loss function as follows :   LpXq“LpXq`LpXq   “ ´ meanˆ   logˆ   diagˆ   softmaxˆQA   τ˙˙˙˙   ´ meanˆ   logˆ   diagˆ   softmaxˆAQ   τ˙˙˙˙   ( 1)Here , softmax is applied by rows ( Figure 1 ) , and   τis the temperature . Both components can be   rewritten , e.g. , LpXqcan be written as follows :   ´ 1   mτÿqa`1   mÿlogÿexpˆqa   τ˙   ( 2 )   Mathematically , this loss function is similar to   the one presented in ( Chen et al . , 2020 ) . The differ-   ence is that we do not use augmentations , and we   do not compare qtoq(oratoa ) due to their   different nature : we want to compare a question to   an answer , not a question to a question or an answer   to an answer . Thus , we apply the symmetrization   in the formula . So , the difference from SimCSE   ( Gao et al . , 2021 ) is that we compare not only qto   alla , but also ato all qin the batch .   Note that , although a frequent short answer may   fit multiple questions in a batch , such pairs are con-   sidered as “ negative ” examples in the loss . How-   ever , the loss learns Mutual Information ( Tschan-   nen et al . , 2020 ) , that is ppq , aq{pppqqppaqq , and   thus it is robust to this false negatives problem .   Early research has already shown the importance   of properly configuring and using some BSC loss   settings . For example , low temperatures are equiv-   alent to optimizing for hard positives / negatives   ( Khosla et al . , 2020 ) , while L2 normalization of   vectors to the unit hypersphere along with temper-   ature effectively weighs different examples ( Chen   et al . , 2020 ) . We further propose a number of im-   portant modifications that can have a major impact   on the performance for a number of tasks .   3.2 Batch Construction   In computer vision , it is common to use a batch   size of 5,000 , which in turn would naturally be   very likely to contain some hard negative examples .   In NLP , fine - tuning Transformers with large batch   sizes requires very large amounts of memory . Thus ,   much smaller batches are used in practice , and it   becomes important to make sure these batches do   contain some hard negative examples . We achieve   this by fixing the content of the batches at each   training epoch . Note that this is much simpler than   mining hard negatives , as we only need to increase   the likelihood that there would be a hard negative   example present in the batch , but we do not need to   know which particular example in the batch would   be hard . Inside the batch , this would be controlled   by the temperature parameter.118Example - based shuffling The key idea of this   method is to batch several groups , so that within   each group all pairs are similar based on their first   or their second elements . In this way , each positive   pair would be accompanied by hard negatives from   the same group and by simpler negatives from the   remaining examples inside the batch ( which come   from other groups ) . We use the k - nearest neigh-   bors for an input example to form a group for it ,   and Faiss ( Johnson et al . , 2021 ) to quickly find   these nearest neighbors in the embedding space .   Let the pairs be grouped by their first elements q.   Algorithm 1 summarizes the proposed method .   Algorithm 1 Example - based shuffling   Input : sequence D , group size s   initialize RÐrsŹ sequence to store the result   initialize UÐ∅Źset of used examples   randomly shuffle D   foreinDdo   ifeRUthen   find the nnearest neighbors of efromD   choose the top s´1that are not in U   add them and etoRand also to U   return reversed R   Note that in this approach we use two stages   in kNN to limit the range of possible candidates   and thus to reduce the computational costs ( both   in terms of time and memory ) . We first extract   the top- nneighbors ( for some large n , e.g. , 500 ) ,   and then we take the top- kfrom them , so that no   duplicates appear in the final sequence ( for some   small k“7 ) . The time complexity of such a check   is O(1 ) . If all such neighbors are already used , then   only the considered example will be added to the   resulting sequence . This case will often arise for   the last examples , and thus batches will consist of   simple 1 - element groups . Therefore , we reverse   the sequence to start with these simple batches , as   in curriculum learning .   By default , we assume that there should be one   positive example for each question / answer ( on the   diagonal of the matrix ) , and thus identical neigh-   bors could be optionally filtered . Still , if there   are the same qin the batch X , the loss definition   ( eq . 1 ) does not change . Indeed , let P“ti|q “   q , pq , aqPXu , then@i , jPP : pq , aqform a   positive pair . According to Khosla et al . ( 2020 ) ,   for each q , all˜qPPshould be placed in the soft-   max numerator and then averaging over all such ˜q   should be performed outside the logarithm . Thus , in LpXq(eq . 2 ) only the first sum would   change as follows :   ÿqaùÿ1   |P|ÿqa   “ ÿ1   |P|ÿqa“ÿqa(3 )   InLpXq :   ÿqaùÿ1   |P|ÿqa   “ ÿ1   |P|ÿqa“ÿqa(4 )   In order to select the groups even better , we con-   sider task - specific embeddings . To this end , we   apply the current model to encode all pairs at each   epoch .   Algorithm 2 Shuffling by words   Input : sequence D , group size k , shingle size t   foreinDdo   e.shingleÐrandom subset of twords   ofe(ignoring stop - words )   sortDbye.shingle   initialize gIDÐrandom uint64 Źgroup ID   initialize sÐ0Źcurrent group size   initialize prevÐfirst element of D   foreinDdo   ife.shingle‰prev.shingle then   gIDÐrandom uint64   ifsěkthen   gIDÐrandom uint64   sÐ0   e.gIDÐgID   sÐs`1   prevÐe   sortDbye.gID   return D   Fast shuffling For extremely large datasets ,   example - based shuffling is time - consuming even   with Faiss ; thus , we propose several effective op-   tions to perform a less - thorough shuffling . We   choose some attribute by which we will group the   examples , that is , we guarantee some closeness of   the examples . Thus , the examples are close if they   share the same words , the same cluster number or   the same nearest neighbors.119First , consider the case of words and grouping   by the first elements of the pairs ( the case of sec-   ond elements is the same ) . Algorithm 2 presents   the shuffling process . We choose a random word   or phrase ( shingle ) in each element and sort the   dataset by these shingles . Since examples with   the same shingles will be in a sequence , we assign   group IDs sequentially . We split a group if it is too   large . Then we shuffle ( sort ) the dataset by group   IDs . The resulting algorithmic complexity is equal   to the complexity of two sorts of the dataset .   To produce a shuffle by clusters , we apply the   same algorithm , where each sentence is replaced by   its cluster number . Thus , each individual shingle is   of size t“1 . To make a shuffle by nearest neigh-   bors , we create shingles by “ sentences , ” where the   words are the positions of the top- knearest neigh-   bors in the input sequence ( for some small value   ofk ) . All these approaches , including k - means   clustering , can be effectively implemented using   MapReduce and parallel computations .   3.3 Labeled Negatives   Usually , when the data size is small , hard nega-   tive examples may be hard to obtain even with   data shuffling , e.g. , when all examples are semanti-   cally distant . Nonetheless , if the dataset contains   a labeled negative pair with some anchor , then its   elements are semantically close by traditional rules   of dataset construction . Thus , using such a pair   inside the batch , where this anchor is present , will   add the necessary hard negative example .   The only change that is added in the loss function   is the masking of negative examples — we have no   guarantees that the selected negative example is   closer to the anchor than the rest of the examples   inside the batch . Let ybe a binary label , where   y“1if the i - th pair is positive . Then , we have   LpXq“´1   mτÿ1ry“1sqa   ` 1   mÿ1ry“1slogÿexpˆqa   τ˙   ( 5 )   3.4 Combo Loss   Theoretically , it is beneficial to use several loss   functions for training if they are calculated on the   same batch ( and thus do not require additional com-   putations ) . That is , joint training of BSC and MSE   losses combines the advantages of pointwise and   of pairwise approaches . This ensures that for positives examples , the val-   ues on the diagonal of the dot - product matrix are   not only greater than the rest , but are also close   to 1 or to some target similarity . Note that here   LpXq“řppqaq´yqfor target pos-   itive similarities y , and thus we do not force all   other similarities to zero . At the same time , the   BSC loss adds new examples ( i.e. , “ negative ” pairs )   to the training set .   In order to use the BSC loss when training a   model for tasks with non - binary labels , we modify   the indicator function in equation ( 5 ) as 1ryąts ,   where tis a configurable binarization threshold .   Then , we use their convex combination with a con-   figurable hyperparameter µPp0,1q :   LpXq“µLpXq`p1´µqLpXq(6 )   3.5 Normalization   L2 normalization of matrices AandBmeans that   abwill be equivalent to cosine similarity . The   embeddings can also be normalized by the batch   dimension ( by coordinates ) , which can bring ad-   ditional regularization . In our experiments , we   confirm the importance of this , e.g. , new represen-   tations can be calculated with L2 normalization by   coordinates or in a min - max scale .   4 Datasets   NLP tasks that compare pairs of sentences can be   roughly divided into three general categories : re-   gression ( predicting a similarity score ) , classifi-   cation ( e.g. , similar vs. dissimilar ) , and ranking   ( search for the best matches ) . They differ only by   the quality assessment functions , and thus they all   can benefit from the above losses .   Note that it is important to calculate the sentence   representations for ranking tasks in advance , as   when independently calculating the embeddings   of the individual elements in the pairs , the infer-   ence time of the model becomes linear instead   of quadratic . Therefore , we use Sentence - BERT   ( SBERT ) , which is trained as a Siamese BERT   model , and offers a way to obtain state - of - the - art   sentence embeddings , which have been proven use-   ful for a number of tasks ( Reimers and Gurevych ,   2019 ; Thakur et al . , 2021 ) . At inference time , we   first use SBERT to obtain independently a repre-   sentation for the individual sentences , and then we   calculate the cosine similarities between the corre-   sponding embeddings as we construct pairs as part   of the process of ranking.120We use a variety of public English datasets and   tasks for the evaluation . In particular , we have four   ranking tasks ( ranking answers to non - factoid ques-   tions , ranking questions by their similarity with   respect to other questions , ranking comments by   their similarity to a given question , ranking fact-   checked claims by their relevance with respect to an   input claim ) , two binary classification tasks ( para-   phrases identification , and duplicate question iden-   tification ) , and one regression task ( semantic sen-   tence similarity ) .   Antique The dataset contains 2,626 non - factoid   questions with answer choices ( Hashemi et al . ,   2020 ) , asked by users on Yahoo ! Answers . There   are a total of 34,011 question – answer pairs : 27,422   for training and 6,589 for validation . Each answer   is annotated with a relevance score with respect to   the question on a scale from 1 to 4 ( where 4 de-   notes the highest relevance ) , and the task is to rank   the answers by their relevance . To model relevance   as a cosine similarity , we normalize the scores to   ther0,1sinterval . We use Mean Reciprocal Rank   ( MRR ) as the main evaluation measure .   CQA - A This dataset was used in SemEval-2017   Task 3 on Community Question Answering sub-   task A ( Nakov et al . , 2017 ) . The goal is to rank   the first ten answers in a question thread on Qatar   Living , so that good answers are ranked higher   than bad ones . We used the clean part of the   dataset , which consists of 14,110 and 2,440 labeled   question – comments pairs for training and devel-   opment , respectively . The evaluation measure is   Mean Average Precision ( MAP ) . This dataset con-   tains important metadata , e.g. , the date and time of   the comment , and sorting the comments by time   yields a strong baseline ; yet , we only use the text .   To train the model with the triplet loss , we group   the pairs by the first element ( anchor ) .   CQA - B This dataset was developed for   SemEval-2017 Task 3 , subtask B ( Nakov et al . ,   2017 ) , whose goal was to rank ten potentially re-   lated questions by their similarity with respect to   an input question . These questions were retrieved   from the Qatar Living forum using Google and the   input question as a query . We use the clean part   of the dataset , which consists of 19,990 training   and 5,500 development labeled question - question   pairs . The main evaluation measure here is MAP .   There is additional information , e.g. , the rank of   the retrieved question in the Google search results ,   which we do not use . PFCC - S Shaar et al . ( 2020 ) presented a dataset   for detecting Previously Fact - Checked Claims on   Snopes ( PFCC - S ) , aimed at facilitating the solution   of a fact - checking problem : given an input claim ,   it asks to rank claims that have been previously   fact - checked , so that claims that can help verify   the input claim are ranked as high as possible . The   dataset has 800 positive input – verified claim pairs   for training and 200 such positive pairs for test-   ing , and they are to be matched against a database   of 10,369 verified claims . The evaluation is per-   formed in terms of a HasPositive@k metric , which   checks whether there is a positive match among   the first kresults in the ranked list . There are no   negative examples in the dataset , but about eight   million such pairs can be created . Thus , in order to   train models using MSE or triplet loss , we sampled   negatives according to the following scheme . First ,   we encoded all sentences using SBERT ( we used   the model pretrained on STSb and NLI ) . Then , we   selected the first element in each positive pair as an   anchor and we sorted all other examples by their   similarity to this anchor . The assumption is that   positive examples will be concentrated in the begin-   ning , e.g. , among the top-100 . Thus , we selected   negatives starting from 101 on , logarithmically : on   positions 100`2 , kP N. As a result , we obtained   many hard negative examples and a small number   of easy ones . Finally , we oversampled the positive   pairs to correct the balance of positive and negative   examples .   Microsoft Research Paraphrase Corpus   ( MRPC ) It was created by Dolan et al . ( 2004 ) and   contains 5,800 pairs of sentences , extracted from   online news sources . Each pair was labeled with   a tag indicating whether the sentences are para-   phrases ( semantically equivalent ) . There are 3,668 ,   407 , and 1,725 pairs in the training , development ,   and test subsets . As it is a binary classification task   with class imbalance , it is evaluated in terms of F1 .   Quora Question Pairs ( QQP ) Quora presented   a dataset containing over 500,000 sentences with   over 400,000 lines of potential duplicate questions .   Each line has a binary label indicating whether the   line truly contains a duplicate pair . Due to the sam-   pling method , which returns mostly positive pairs ,   the authors supplemented the dataset with negative   pairs composed of “ related questions . ” Following   Thakur et al . ( 2021 ) , we sampled randomly 10,000   examples for training , and we used F1 score as the   main evaluation measure.121Semantic Textual Similarity Benchmark   ( STSb ) The STS benchmark comprises a selection   of English datasets used in the STS tasks organized   in the context of SemEval between 2012 and 2017   ( Cer et al . , 2017 ) . The benchmark comprises 8,628   sentence pairs . The pairs were annotated with sim-   ilarity scores on a scale from 0 to 5 ( with 5 indi-   cating complete equivalence ) . There are a total of   5,749 , 1,500 and 1,379 pairs in the training , in the   development , and in the testing split , respectively .   The main evaluation measure is Spearman ’s rank   correlation . As for the Antique dataset , here we   also normalized the scores to the r0,1sinterval and   then we binarized them based on a threshold of 0.6 .   5 Experimental Setup   Below , we describe our baselines and hyper-   parameter settings .   5.1 Baselines   We used BERT - base uncased in all our experiments   to be able to perform direct comparison for tasks   such as MRPC , QQP , and STS to previous work   ( Reimers and Gurevych , 2019 ; Thakur et al . , 2021 ) .   We considered SimCSE ( sup - simcse - bert - base-   uncased checkpoint ) ( Gao et al . , 2021 ) as an un-   supervised baseline as it uses the base version of   the BSC loss , which we modified . Below , by BSC   we will denote using optimal settings in the tables ,   and variants like BSC - random shuffle would mean   that instead of these optimal settings , we applied   random shuffling .   5.2 Hyperparameters Setting   We set the number of warm - up steps to 10 % of   the total steps , and we limited the input sequence   length to 90 subtokens . We used a batch size of   30 in all tasks , except for Antique and QQP , where   we used 50 . Note that an order of magnitude larger   batch sizes would probably yield better results , but   they would also require much more memory . We   tried learning rates from { 5e-6 , 1e-5 , 2e-5 , 3e-5 } ,   and we selected ( on dev ) 3e-5 for CQA - B and 2e-5   for all other experiments . We used the AdamW   optimizer with the bias correction for the CQA   tasks , and without bias correction for the rest . We   trained the model for five epochs for Antique , CQA-   A and STSb , for six epochs for QQP , MRPC and   PFCC - S , and for seven epochs for CQA - B , saving   a checkpoint after each one , and we selected the   best checkpoint on the development set . As recommended by Thakur et al . ( 2021 ) , due to   instability , we did seed optimization , running each   experiment five times and selecting the best one   ( on dev ) . To train with the BSC loss , we used min-   max normalization by coordinates with τ“1.2for   PFCC - S and QQP , standard L2 normalization with   τ“0.055for CQA - A , τ“0.07for CQA - B , and   τ“0.1for all other tasks ( to find the optimal τ , we   made it trainable for one run ) . We applied example-   based shuffling to train with the BSC loss . We used   a group size of four in MRPC , of five in CQA - B ,   and of eight in all other tasks . We iterated over µ   values from the set t0.1,0.5,0.9u , and we chose   µ“0.1to train the combo approach for CQA - A ,   MRPC , QQP and STSb tasks , and µ“0.9for the   other experiments .   Our experiments demonstrated that there is only   a minor impact of changing the values of these hy-   perparameters . For example , the maximum quality   drop from µ(0.1vs.0.5 ) is 0.4 % , i.e. , almost at   the noise level , while we obtained gains of up to   10 % when using our proposed modifications .   We trained the triplet loss variant from ( Reimers   and Gurevych , 2019 ) with margin“0.6for   PFCC - S , and margin“0.5for all other tasks .   As we have no answers for the test set in MRPC ,   and no test sets in Antique and PFCC - S , we split the   training set into 9:1 to tune the hyper - parameters .   The time for training SBERT with the BSC loss   ( orcombo loss ) was almost equal to the time for   training with the standard MSE loss . The most   complex example - based shuffling only adds 8 % to   the training time . Note that it is several times faster   than triplet loss , as only positive examples are used   for training , and thus the dataset becomes several   times smaller . We ran all experiments on a GeForce   GTX 1080 GPU Ti . Training SBERT - base took up   to ten minutes per epoch depending on dataset size .   6 Results   Below , we describe the evaluation results .   Antique The results are shown in Table 1 . Our   best approach of combo - training the MSE and the   BSC losses outperforms all other variants and the   approach proposed in ( Hashemi et al . , 2020 ) , where   specific negative sampling and a triplet loss were   used . Besides , the best BSC configuration achieves   higher scores than MSE . We can see the impor-   tance of using predefined hand - crafted negative   examples , which brings additional difficult cases   and increases MRR by 0.02.122   CQA - A The results for CQA subtask A are   shown in Table 2 . A comparison with ( Nakov et al . ,   2017 ) is not very fair , as we did not use the meta-   data , e.g. , the comment position , which was cru-   cial for the best systems . Besides , we use SBERT ,   which is inferior to a fine - tuned BERT . Neverthe-   less , our best approach of combo training with   MSE and BSC losses yielded competitive results .   We further compared different shuffling strategies .   The data is ordered by questions , and keeping this   order turns out to be best . That is , the model learns   to distinguish positive answers for each question   from manually selected negative ones and from an-   swers to other questions . Also , note that random   shuffling completely eliminates this structure , and   MAP drops by 6 % absolute . Fast shuffling by 300   clusters , an advanced version of shuffling by words ,   improves these results . Example - based shuffling   finds a data order similar to the initial one , and the   quality does not degrade much .   CQA - B The evaluation results for CQA - B are   shown in Table 2 . Again , we did not use the ques-   tion position , which is a critically important fea-   ture for the best systems . We can see that the BSC   loss achieved the best score , noticeably outperform-   ing MSE and triplet losses . The experiments also   demonstrate the importance of data order when   training with the BSC loss . Since the dataset is   small , the model overfits when the original data   order is fixed .   PFCC - S Table 3 shows the results for PFCC-   S ( HP@k stands for HasPositives@k ) . Note that   the scores from ( Shaar et al . , 2020 ) are for the   pre - trained SBERT without fine - tuning . We ob-   served that even when using oversampling to im-   prove class imbalance , fine - tuning with MSE per-   formed worse than their results . Here , we used only   positives examples to train with BSC , and normaliz-   ing by the zero dimension . Overall , the approaches   using BSC and triplet loss were comparable . How-   ever , the dataset size for training with the BSC   loss was much smaller , which is also true for MSE .   Thus , the BSC loss is faster , and preferable .   MRPC Table 4 shows the results for MRPC .   MSE outperformed the BSC loss , but combo   achieved a slightly higher F1 score .   QQP The results for QQP are shown in Table 4 .   We also show results for SBERT and augmented   SBERT ( in parentheses ) from ( Thakur et al . , 2021 ) .   They trained SBERT with MSE using another ran-   dom training sample , but nonetheless , the F1 score   is close to ours . The combo approach outperformed   separate training with BSC or MSE .   STSb Table 5 shows the results for STS . It is   the only task where combo with the BSC loss was   worse than MSE . This could be due to hard nega-   tives not appearing in the batch in any of the shuf-   fling procedures . Moreover , we observed only   marginal improvement when fine - tuning with a   BSC model initially trained with MSE . However ,   if it was pretrained with BSC up to overfitting , fine-   tuning it with MSE yielded sizable improvements.123   7 Discussion   We highlight the following observations :   •Combo - training with BSC and MSE losses   generally yields the best results ( the only ex-   ception is STSb ) , and it outperforms the triplet   loss with advanced negative sampling .   •The order in which the data is presented for   training can be critical , as we have seen in the   cases of CQA - A and CQA - B.   •The use of labeled negative examples gener-   ally improves the results by 1 - 2 % absolute .   •Embedding normalization during training is   important . Moreover , it is useful to normalize   to the zero dimension ( e.g. , for PFCC - S ) .   •A temperature τof order 0.1 should be used   with the standard normalization , and τof or-   der 1 - 3 for coordinate normalization .   •A suboptimal setup may hurt performance by   over 10 % , as was demonstrated for ( i ) filtering   out negative examples for which no positives   were given in the dataset ( Table 1 ) , ( ii ) us-   ing poorly formed batches ( highest impact in   Table 2 ) , ( iii ) suboptimal normalization ( Ta-   ble 3 ) , and ( iv ) wrong temperature value .   •BSC is more suitable for ranking tasks ( An-   tique , CQA , and PFCC - S ) , but it can help for   other tasks ( classification and regression tasks ,   e.g. , for MRPC , QQP , and STSb ) if applied as   pre - training or in joint training with MSE .   Selecting a loss function is important . For in-   stance , if the model optimizes Pearson correlation ,   it achieves a score of 85.57 on the STS task . Thus ,   it outperforms almost all considered approaches .   Moreover , the combination of such a loss with BSC   allows the model to achieve an F1 score of 89.88   in the MRPC task ( a classification task).Finally , we would like to draw a parallel between   our work and Augmented SBERT ( Thakur et al . ,   2021 ) . When using the BSC loss , some negatives   are implicitly added to the dataset . Augmented   SBERT adds new examples too and retrieves them   using BM25 or Semantic Search samplings . These   methods are comparable to our fast shuffling by   words ( n - grams ) and to example - based shuffling ,   respectively . Moreover , the task - specific model is   used to encode the data in both cases . However , we   do not need to label such pairs with another model   ( cross - encoder ) due to the BSC loss definition .   8 Conclusion and Future Work   We explored the idea of using a batch - softmax con-   trastive loss for fine - tuning large - scale pre - trained   transformers to learn better task - specific sentence   embeddings for pairwise sentence scoring tasks .   We introduced and studied a number of variations   in the calculation of the loss as well as in the over-   all training procedure . Our experimental results   have shown sizable improvements on a number of   datasets and pairwise sentence scoring tasks includ-   ing ranking , classification , and regression .   In future work , we want to explore new varia-   tions of the loss , and to gain better understanding   of when to use which variation . We further plan   experiments with a larger set of NLP tasks .   Ethics and Broader Impact   We would like to warn that the use of large - scale   Transformers requires a lot of computations and   the use of GPUs / TPUs for training , which con-   tributes to global warming ( Strubell et al . , 2019 ) .   This is a bit less of an issue in our case , as we   do not train such models from scratch ; rather , we   fine - tune them on relatively small datasets . More-   over , running on a CPU for inference , once the   model is fine - tuned , is perfectly feasible , and CPUs   contribute much less to global warming .   Acknowledgments   The article was prepared within the framework of   the HSE University Basic Research Program . It is   also part of the Tanbih mega - project , developed at   the Qatar Computing Research Institute , HBKU ,   which aims to limit the impact of “ fake news , ” pro-   paganda , and media bias by making users aware   of what they are reading , thus promoting media   literacy and critical thinking.124References125126