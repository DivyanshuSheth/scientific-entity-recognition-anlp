  Kirby KuzniaSwaroop MishraMihir Parmar Chitta Baral   Arizona State University   Abstract   Despite the success of large pre - trained lan-   guage models ( LMs ) such as Codex , they show   below - par performance on the larger and more   complicated programming related questions .   We show that LMs benefit from the summa-   rized version of complicated questions . Our   findings show that superfluous information of-   ten present in problem description such as hu-   man characters , background stories , and names   ( which are included to help humans in under-   standing a task ) does not help models in under-   standing a task . To this extent , we create a meta-   dataset from the frequently used APPS dataset   and the newly created CodeContests dataset for   the program synthesis task . Our meta - dataset   consists of human and synthesized summaries   of the long and complicated programming ques-   tions . Experimental results on Codex show that   our proposed approach outperforms baseline   by8.13 % on the APPS dataset and 11.88 % on   the CodeContests dataset on average in terms   of strict accuracy . Our analysis shows that sum-   maries significantly improve performance for   introductory ( 9.86 % ) and interview ( 11.48 % )   programming questions . However , it shows   improvement by a small margin ( ∼2 % ) for   competitive programming questions , implying   scope for future research in this direction .   1 Introduction   Recently , large pre - trained LMs have been proven   pivotal in programming - related tasks ( Wang et al . ,   2021 ; Chen et al . , 2021 ; Hendrycks et al . , 2021 ;   Lu et al . , 2021 ; Papineni et al . , 2002 ) . Program   synthesis aims to generate a code given the natural   language description of a problem . Programming   requirements in these problems vary in terms of   complexity from a 3 - 5 line simple function to mul-   tiple functions that use advanced data structures . However , LMs such as Codex show below - par per-   formance on the long and complicated program-   ming questions . We observe that the natural lan-   guage description of the program becomes long   and complicated when there is superfluous infor-   mation ( see section 2.1.1 ) . The goal of adding this   information to the description is to make it more   understandable to humans . However , we find that   this information confuses the model in understand-   ing a task . We propose that removing the excess   information and providing the model with the ex-   act specifications of the problem can improve the   performance of the LMs .   To remove excess information , we summarize the   descriptions of the program in such a way that it   does not lose important specifications . We use the   APPS dataset ( Hendrycks et al . , 2021 ) and Code-   Contests dataset ( Li et al . , 2022 ) which are a col-   lection of coding problems from different online   sources and create a meta - dataset consisting of hu-   man and synthesized summaries .   We perform all experiments using the GPT - based   Codex model ( Chen et al . , 2021 ) on the proposed   meta - dataset and show that the summarized ver-   sion of complicated questions improves strict ac-   curacy by 8.13 % on the APPS dataset and 11.85 %   on CodeContests . From our analysis , we can see   significant improvement for introductory ( 9.86 % )   and interview ( 11.48 % ) related programming ques-   tions . However , it shows improvement by a small   margin ( ∼2 % ) for competitive programming ques-   tions . Considering that automatic evaluation of a   program does not reward for partial correctness , we   perform qualitative evaluation on our meta - dataset   and find that original questions often confuse mod-   els in understanding the underlying problem , as   models latch on to some spurious words in the text   ( e.g. the word ‘ list ’ in question makes the model4532design a list even though the underlying problem is   on graphs ) . We further analyze model performance   on different types of summaries ( i.e. , basic , expert ,   and synthetic ) and provide instruction - design prin-   ciples that can help future research on prompting   in program synthesis .   2 Method   2.1 Dataset   We use the APPS ( Hendrycks et al . , 2021 ) and   CodeContests ( Li et al . , 2022 ) datasets to create   summaries . We crowd - sourced the creation of hu-   man summaries . The result was 373 human sum-   maries for APPS and 80 summaries for CodeCon-   tests along with and 8663 synthetic summaries us-   ing both datasets . Table 1 shows the statistics of   the generated summaries .   2.1.1 Human Generated Summaries   For the APPS and CodeContests human - generated   summaries , the crowd worker reads and under-   stands the original questions , then creates sum-   maries in two steps . First , we create a basic   summary of the given problem and remove any   information that is repeated and any hypothetical   information without concrete instructions . For ex-   ample , if the problem constructs a fake company or   situation , we replace the fake situation with directinstructions . Full example is included in Appendix   C. Second , we create an expert summary of the   problem . To create this , we further summarize   the first summary . This expert summary includes   the absolute minimum information for an expert to   understand the problem . We would not expect a   novice to understand these prompts . An example   of expert summaries is given in Appendix C.3 .   2.1.2 Synthetic Summaries   We have generated synthetic summaries of program   descriptions using jumbo ( 178B ) , large ( 7.5B ) Stu-   dio21 model ( Lieber et al . , 2021 ) , GPT-3 Davinci   model ( 175B ) ( Brown et al . , 2020 ) and PEGASUS   model ( Zhang et al . , 2019 ) . To generate a summary ,   we provide these models with a few examples in the   in - context learning setup ( Brown et al . , 2020 ) from   the human - generated summaries . For the few - shot   examples , we use expert - level summaries .   Studio21 We use five examples with the large   model , and three examples with the jumbo model .   For both models , we use a temperature of 0.3 , and   topP of 1 . For the format of our prompt , we use De-   Jargonizer templatewith a change to their header   as shown in Appendix D. We create a total of 7,505   synthetic summaries using these models .   GPT-3 We use three examples for GPT-3 model .   We empirically set temperature to 0.05 , topP to   1 , frequency penalty to 0.01 , presence penalty to   0.05 . To generate prompts , we followed their tl;dr   templateas shown in Appendix D. We create 785   synthetic summaries using this model .   PEGASUS We use the PEGASUS model ( Zhang   et al . , 2019 ) to create program summaries for the   same set of problems that were summarized by   humans . We choose this model because it was   trained specifically for abstractive summarization .   2.2 Model   We use OpenAI Codex to build baselines and the   proposed approach .   Baseline To create a baseline , we have used orig-   inal program descriptions given in the datasets as   prompts for the Codex model.4533   Proposed Approach We have used summaries of   original program descriptions given in the datasets   as prompts for the Codex model .   3 Experimental Setup   All the experiments are performed using the   davinci −codex ( Chen et al . , 2021 ) model pro-   vided through OpenAI . At inference time , we use   a modified version of the evaluation codepro-   vided by Hendrycks et al . ( 2021 ) . This evaluation   code has four different outputs for each test case :   ( 1)-2 : the code has a syntax error and can not run ,   ( 2)-1 : the code is syntactically correct but has a   run time error , ( 3 ) 0 : the code runs without any er-   rors but fails the test case , and ( 4 ) 1 : the code runs   without any error and passes the test case . Similar   to Chen et al . ( 2021 ) , we implement a timeout for   the code at inference time . If a test case takes more   than4seconds to run then we throw an exception   and count that test case as a −1 .   Experiments To show effectiveness of the pro-   posed approach , we have performed three different   experiments using human generated summaries:1.All problems from basic and expert sum-   maries are used at inference time . We term   this experiment All Problems ( AP ) .   2.We eliminate problems that perform worse   for either basic or expert summaries . We   term this experiment Either Worst Problem   Removal ( EWPR ) .   3.We eliminate problems that perform worse for   both basic and expert summaries . We term   this experiment Both Worst Problem Removal   ( BWPR ) .   Motivation behind EWPR and BWPR If a sum-   mary caused every test case to perform worse then   it ’s likely the crowd worker produced a faulty sum-   mary . To mitigate the effect of outliers in the   dataset , we use the EWPR method to remove such   problems . Another hypothesis is that every prob-   lem benefits from some level of summarization   ( i.e. , basic or expert ) . To measure this , we use the   BWPR method . From Table 6 results , we identify   that only 1 problem had both summaries ( basic and   expert ) preform worse .   Metric In ( Austin et al . , 2021a ) , they show that   the BLEU metric ( Papineni et al . , 2002 ) does not   correlate well with synthesis performance . Thus ,   we use Strict Accuracy ( SAcc ) as our evaluation   metric for all experiments ( see Appendix E ) .   4 Results and Analysis   4.1 Human Generated Summaries   From Table 2 , we can observe that both the   summary - based models show on average superior   performance compared to baseline . In particu-   lar , when calculating results for every problem,4534   basic and expert summary - based models outper-   form baseline by 4.34 % and 5.15 % on average for   APPS dataset , respectively . Further analysis shows   that the expert summary - based model shows im-   proved performance by ∼1%compared to the   basic summary - based model .   On the CodeContests dataset ( Li et al . , 2022 ) , we   show an average improvement of 11.88 % in terms   of SAcc . For this dataset , we did not separate the   problems by difficulty . This is because the prob-   lems come from different sources and have differ-   ent scales of difficulty . Thus , we did not report the   SAcc when weighted by difficulty in Table 2 .   Our analysis shows that many problems where the   basic summary would fail , however , the expert   summary would succeed and vice - versa . Thus ,   we choose the best summary for each problem   after evaluating both summaries and then calcu-   late the results for the best summaries . Table 3   shows results when taking the best summary foreach problem for APPS dataset . We observe a   9.86 % , 11.48 % , and 1.91 % increase on SAcc for   introductory , interview , and competition level prob-   lems , respectively .   4.2 Synthetic Summaries   Table 4 and 5 show the results for baseline , syn-   thetic summaries generated by GPT-3 , Studio21   and PEGASUS in terms of SAcc for two exper-   iments . For the AP experiment , we can observe   that the performance of the baseline outperforms   synthetic summary - based models . However , the   proposed model shows an average similar perfor-   mance compared to the baseline for the EWPR   experiment . Moreover , Appendix I shows the re-   sults for top 500 and top 1000 summaries from   GPT-3 and Studio21 , respectively .   4.3 Analysis   Why does eliminating the worst problems help ?   From Tables 2 , we can observe that EWPR and   BWPR have improved performance compared to   AP for both human and synthetically generated   summaries . By analyzing the summarized worst   problems , we notice a difference in the summariza-   tion style which shows that these summaries are   outliers and do not match the distribution of the   other summaries . This can cause a problem in syn-   thesizing a good program since the model loses   important information . Hence , we believe that   eliminating the worst problems improves model4535   performance .   Is there any possible bias in the meta - dataset ?   Recent studies shows that bias propagates in   human - annotated datasets ( Geva et al . , 2019 ; Par-   mar et al . , 2022a ) . Given that our summaries are   also human - generated , there will be some bias in   the dataset . Some details that are critical to one   person can be trivial to others . In the context of   generating expert summaries , assumptions about   expert knowledge can vary . This bias causes drift   in the dataset and hinders the model ’s performance .   Similar to Mishra et al . ( 2021 ) , we can provide a   template for what is expected from the summary   generator to reduce bias .   Why is competition accuracy low ? We believe   that these problems require multi - hop reasoning ,   even after summarization , which is still a challenge   for language models .   Impact of POS on Accuracy In the top plot   of Figure 1 , we observe that frequency of nounsandpropernouns for problems that passed all test   cases is lower than the entire dataset . In the bot-   tom plot , we observe that the frequency for nouns   andpropernouns is higher for the original ques-   tion ( which had < 100 % accuracy on the test cases )   and lower for the summary ( which had 100 % ac-   curacy on the test cases ) . Thus , we can see that   number of nouns degrades performance . We also   see in the bottom chart that overuse of punctuation   can be detrimental to performance . From the re-   sults in Figure 1 we see results of nouns affecting   performance along with excessive punctuation . Ad-   ditional detailed analysis is presented in Appendix   B.   5 Conclusion   This paper introduces a summarization - based ap-   proach for efficient program synthesis . Experimen-   tal results show that the proposed approach im-   proves the performance of the Codex model by on   average ∼8%across various levels of program-   ming questions provided by the APPS and ∼11 %   on the CodeContests . Further , this paper proposes a   meta - dataset consisting of ∼450human - generated   basic and expert - level summaries as well as ∼8k   synthetically generated summaries by GPT-3 and   Studio21 ; this can be helpful for future research   on writing better instructions for the program syn-   thesis . We show that program synthesis models   benefit from concise prompts , hence , we believe   that less number of high - quality instances are better   than more low - quality data instances .   Future Extensions The decomposition of   prompts has been shown to improve accuracy   ( Mishra et al . , 2022 ; Patel et al . , 2022 ) ; splitting up   the summarization task the resulting summary can   potentially result in higher accuracy for the Codex   model in future . Additionally , the PEGASUS   model could be used in conjunction with other   models to perform the detailed algorithm outlined   in Appendix N.   Limitations   Our summary - based approach shows improved   performance on program synthesis models , how-   ever , it shows competitive performance on syn-   thetic summaries . We believe that the generation of   high - quality summaries can improve performance ,   hence , designing efficient prompts to improve syn-   thetic summaries can be the scope of further re-   search . Furthermore , human - generated summaries4536show competitive performance on competition-   level problems . These problems require reason-   ing with multiple logical leaps and knowledge of   advanced algorithms and data structures . Hence ,   exploring new techniques for summarization can be   a future research direction . In addition , this work   only analyzes the codex model , hence , exploring   the effect of summarization on other program syn-   thesis models can be interesting .   References45374538A Related Work   In the past , there are several methods including   semantic parsing ( Ge and Mooney , 2005 ) , deduc-   tive approaches , enumerative and stochastic search ,   and constraint solving which have gained atten-   tion for program synthesis ( Gulwani et al . , 2017 ) .   With the advent of machine / deep learning , Balog   et al . ( 2016 ) introduced a neural network based   model for solving programming competition - style   problems . Devlin et al . ( 2017 ) used sequence - to-   sequence approach to do program synthesis . Fur-   thermore , Hendrycks et al . ( 2021 ) introduced the   APPS dataset for testing the accuracy of large LMs   on program synthesis . Hendrycks et al . ( 2021 )   leveraged the GPT - Neo model ( Black et al . , 2021 )   which they fine - tune for this task using APPS   dataset . CodeT5 model ( Wang et al . , 2021 ) uti-   lizes many different training objectives . Recently ,   Austin et al . ( 2021b ) explore limitations of large   language models and propose two new benchmarks ,   MBPP and MathQA - Python . The Codex model   ( Chen et al . , 2021 ) is an advanced code generation   model that powers GitHub ’s Copilot . The state   of the art model for program synthesis was intro-   duced by Deepmind called AlphaCode ( Li et al . ,   2022 ) . They released their dataset CodeContests ,   which was used to fine - tune and test their model ,   and was used in this paper . Our approach sug-   gesting smaller instructions compliments other ap-   proaches in improving model performance in in-   struction paradigm ( Mishra et al . , 2021 ; Wei et al . ,   2022 ; Parmar et al . , 2022b ; Nye et al . , 2021 ; Puri   et al . , 2022 ; Luo et al . , 2022 ; Wei et al . , 2021 ; Sanh   et al . , 2021 )   B Additional Analysis   Difficulty of CodeContests The accuracies for   CodeContests is notably lower than the APPS   dataset since this dataset is more challenging , e.g.   the number and complexity of programming op-   erations is relatively higher than APPS . From the   baseline results in Table 2 , we can observe that   problems in CodeContests are harder than inter-   view but easier than competition .   Impact of Entities on Accuracy In Figure 2 ,   we can observe that the total number of entities   num _ entities is higher for problems that per-   formed worse . Here , we can see that the original   problems ( which failed test cases ) had a higher   mean than the dataset and the summaries ( which   passed all test cases ) had a lower number of enti-   ties .   CExample of removing fake information   To see the code produced by the model for this   example , refer to Appendix J. There are more ex-   amples of superfluous information confusing the   model in Appendix O and of made up information   confusing the model in Appendix P.   C.1 Original Prompt4539   We can see the author of the problem is trying todescribe a fully - connected graph with nnodes and   medges each with a weight aorb . Thus , this   paragraph can be summarized as :   C.2 Basic Summary4540C.3 Expert Summary   However , we can assume that an expert would   already know what a minimum spanning tree is .   Thus , we can remove this detailed description of   an MST .   D Prompt templates   Studio21 Here is our template for Studio21 .   To see examples of summaries produced by Stu-   dio21AI ’s model along with the code generated for   those summaries , refer to Appendix K and L.   The few - shot examples were chosen randomly   from the human generated expert summaries .   GPT3 Here is our template for GPT3 . To see   examples of summaries produced by GPT3 and   the code generated for those summaries refer to   Appendix M.   Codex Here is our default template for Codex ,   which is used when there is no starter code pro-   vided . When there is starter code provided the   docstring remains the same but the code after the   doc string will be what is provided .   E Strict Accuracy   Strict Accuracy ( SAcc ) is the percentage of prob-   lems that passed every test case . The formula to   calculate SAcc is given below :   strict acc : = problems with 100 % accuracy   total number of problems(1 )   Given that , we are only generating one code solu-   tion for each problem our strict accuracy is compa-   rable to ( Chen et al . , 2021 ) ’s metric raw pass @1.4541   F Codex Configuration   We did a small test with 75summaries to find our   hyper - parameters for Codex . We set temperature to   0 , topP to 1 , frequency penalty to 0.2 , and presence   penalty to 0 . We did not provide few - shot examples   to Codex since we want to see if summarization   only could improve the performance of the Codex   model .   G Worst Problems and Statistics   Using the test case labels as defined in section 3   we defined a test case as getting worse if it ’s la-   bel ( result ) was lower . Then we defined a problem   as worse if every test case had a lower label . Our   methodology behind this was , if we removed prob-   lems that had a worse accuracy , then it would be a   non - trivial result that accuracy improved . Also , if   we removed problems with worse accuracy , then a   problem that originally had all 0labels ( all False   test cases ) would score the same if the summary   had all −1labels ( runtime error ) or a −2(syntax   error ) . So , we removed problems which every test   case performed worse , to see if removing these   outliers would improve results . You can see the   overall breakdown of each split in table 6 .   H Average length of Problems and   Solutions   Table 7 represents the statistics for average length   of problems and solutions for original and summa-   rized prompts . I Abbreviated Synthetic Results   In table 8 , we show the results for our synthetic   summaries when taking the top 500 and 1000 sum-   maries for GPT3 and StudioAI21 , respectively . In   our initial experiment , this was the amount of prob-   lems we tested for each model . However , in our   final experiment we changed our configurations   and generated more problems . For a comparison ,   we took the top performing summaries and and   reported those results .   J Generated Code   In figure 3 is the code that was generated for   the example mentioned in Appendix C and C.3 .   Given that the Codex model was prompted with   thedef code ( ) : the model did not generate that   function definition or the call to that function . That   was added in afterwards , but everything inside that   function was generated by Codex . The originally   generated code ( far left ) fails with a −1because it   did not take in the input correctly . It added in an-   other line p = int(input ( ) ) , which most likely refers   to the pmentioned in the original text . The expert   summary generated code ( middle ) fails every test   case . The basic summary generated code ( right )   passed 16/19 ( 84 % ) test cases and was the only   code to pass at least 1test case .   K StudioAI21 Generated Code   Below is an example of a competition problem   where StudioAI21 summarized the prompt too   much but Codex was still able to produce viable   code . Here is the original prompt:4542   Here is the summary that StudioAI21 generated:4543   Because any input / output examples provided by   the prompt are appended to the summary , Codex   was able to figure out the pattern in the problem   and generate code that was almost correct . In figure   4 , the solution ( left ) used the pattern in the problem   and simplify by taking nmod 3 . The Studio21   summary code ( right ) recognizes this pattern but   erroneously does not take the modulus of the num-   ber . The original code ( center ) also makes the same   mistake by not taking the modulus , but also brute   forces the answer . This shows that the model did   not recognize the pattern in this problem because   of the superfluous details . Even though Studio21   might have summarized too much , the model was   still able to make an improvement and understand   the pattern in the problem more .   L StudioAI21 Generated Code   Here is an example of a summary made by Stu-   dioAI21 where the qualitative aspect of the code   but it still failed . Here is the original prompt :   Here is the summary that StudioAI21 generated :   In 5 the left is the original solution which fails   with a−2because the runtime of the algorithm is   exponential . Note that it tries to create a list of all   possible edge colorings which is O(2 ) . The right   is the code produced when using the StudioAI21   summary . You can see that this code is much closer4544   to solving the problem and produces an efficient   algorithm . However , this fails with a −2because   it tries to print the sum of a boolean ( near the end   before the last for loop ) . Which fails in python   because a bool is not iterable .   Here is a problem where StudioAI21 ’s summary in-   creased the accuracy to 100 % . Here is the original   prompt :   Here is the summary that StudioAI21 generated .   Not exactly as we would expect as the prompt still   mentions the fictional Polycarpus .4545   In 6 the left is the original solution which gets 77 %   accuracy . The right is the summary code which   gets100 % accuracy .   M GPT Generated Summaries   Here are two summaries where GPT perfectly sum-   marized the prompt and gave a concise description   of what the task was . In both cases the original   prompt did not have 100 % accuracy but the sum-   marized prompt did have 100 % accuracy .   Here is the original prompt for the first question :   Here is the summary that GPT Generated:4546   In 7 you can see the original code on the left and   the summary code on the right . There is a subtle   difference but it ’s that difference that improved the   problem from 33 % accuracy to 100 % .   Here is the original prompt for another question .   Here is the summary that GPT3 generated:4547   In 8 you can see the original code on the left and   the summary code on the right . There is a subtle   difference but it ’s that difference that improved the   problem from 20 % accuracy to 100 % .   N Human Generated Instructions   The section below was given to each crowd worker   as instructions to follow when creating the regular   and expert summaries .   N.1 Summarization   Create a file called summary.txt this will con-   tain your summary of the prompt . It ’s recom-   mended that you copy the question.txt file into   thesummary.txt file then starting from the top of   the prompt follow the steps and remove words / lines   as necessary .   These are the rough steps for making a summary .   Following these steps will create the most consis-   tency in our dataset . However , you should summa-   rize as you see fit . First , read through the prompt   and understand what it ’s asking , then follow these   steps to help create a summary .   1 . Directly state what is given in the problem .   •Most problems start by setting the scene ,   to help humans understand .   •Start the problems by explicitly telling   the model what the input is.•You are given . . .   2 . Remove any notes given in the prompt .   •They are usually reemphasizing points ,   which is redundant and not needed in the   summary .   •This includes the −Notes−section at   the bottom of the file .   •If there is pertinent information given   from a note , include it in the prompt with-   out describing it as a note .   3 . Remove any text in parenthesis .   •Most of the text in parenthesis is repeat-   ing the information that precede them .   •If the text in parenthesis provides more   context or information , then remove the   preceding text .   •Keep any parenthesis if it is describing   constraints , such as the minimum and   maximum values for the input etc ...   4.Remove any made up people , places , things ,   etc ...   •These abstractions are made to help hu-   mans understand but confuse the model .   •The prompts often mention things like   Codefortia orPolycarp , try to replace   these with the word you .   •Any text visualizing what the problem is   asking , should be removed .   5.If the Input orOutput section reference an   abstraction they should be changed.4548   •Overall , these sections are fine . How-   ever , if they mentioned something you re-   moved in the previous steps , they should   be changed to reflect that .   •If these sections repeat themselves re-   move any redundancies .   •In most cases these sections will be left   alone .   N.2 Expert Summary   Create a file called expert.txt this will contain   an expert summary of the prompt . It ’s recom-   mended that you copy the summary.txt file into   theexpert.txt file then starting from the top of   the prompt remove words / lines as necessary . You   should aim for the expert prompt to be 2−4lines .   Imagine you are describing the prompt to a se-   nior software engineer . What else could you trim   out ? The difference between the original and ex-   pert summary , is the original summary may in-   clude something obvious , whereas the expert so-   lution should be the absolute bare minimum . To   create summary.txt you want to remove super-   fluous details from the original prompt . To create   expert.txt you want to remove details that an ex-   pert would find obvious , from the summary .   For example , in problem 2000 ( which is compet-   itive difficulty ) the summary mentions ’ It will be   possible to travel between each pair of nodes . . . ,   and the sum of times . . . will be the minimum possi-   ble ’ . This process is describing a minimum span-   ning tree so you can just say ’ Find a minimum   spanning tree ’ .Also , if the prompt included an example and subse-   quent explanation , that should remain in the sum-   mary but should be removed from the expert sum-   mary . An expert already understands the problem   and does not need any extra explanation . You   should still keep the −Examples −section .   Takeaways   •Removing made up people , places , and things   from the prompt improved the quality of code   generated .   •The optimal summarization depends on the   difficulty of the problem .   •Synthetically generate summaries were close   to maintaining accuracy .   •With more rigorous instructions , human sum-   maries could be made with less noise which   would further improve synthetic summary   generation .   OSuperfluous Information Confusing the   Model   Here is an example of an interview level string   problem where the original prompt got 0%and both   human generated summaries got 100 % accuracy .   The question wants you to write code that will   return the number of unique character in the given   string .   O.1 Original Prompt4549   O.2 Basic SummaryO.3 Expert Summary   O.4 Generated Code   The original code ( left ) does not accomplish the   task but rather prints the count of the most fre-   quent character . The model was unable to distin-   guish what the task was given the verbose prompt .   However , the basic and expert summaries make the   task clear and the model produces the same code .   Which properly solves the challenge .   P Made Up Information Confusing the   Model   Here is an example of an interview level problem   where the original prompt got 0%and the expert   generated summary got 100 % accuracy .   P.1 Original Prompt4550   P.2 Expert Summary45514552