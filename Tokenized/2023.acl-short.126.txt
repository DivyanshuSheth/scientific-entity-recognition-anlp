  Gabriele Sarti , Phu Mon Htut , Xing Niu , Benjamin Hsu ,   Anna Currey , Georgiana Dinu , Maria NadejdeUniversity of GroningenAWS AI Labs   g.sarti@rug.nl , { hphu , xingniu , benhsu , ancurrey , gddinu , mnnadejd}@amazon.com   Abstract   Attribute - controlled translation ( ACT ) is a sub-   task of machine translation that involves con-   trolling stylistic or linguistic attributes ( like   formality and gender ) of translation outputs .   While ACT has garnered attention in recent   years due to its usefulness in real - world applica-   tions , progress in the task is currently limited by   dataset availability , since most prior approaches   rely on supervised methods . To address this   limitation , we propose Retrieval and Attribute-   Marking enhanced Prompting ( RAMP ) , which   leverages large multilingual language models   to perform ACT in few - shot and zero - shot set-   tings . RAMP improves generation accuracy over   the standard prompting approach by ( 1 ) incor-   porating a semantic similarity retrieval compo-   nent for selecting similar in - context examples ,   and ( 2 ) marking in - context examples with at-   tribute annotations . Our comprehensive exper-   iments show that RAMP is a viable approach in   both zero - shot and few - shot settings .   1 Introduction   Text style transfer ( TST ) is a task that aims to   control stylistic attributes of an input text with-   out affecting its semantic content ( Jin et al . , 2022 ) .   Research in TST has largely focused on English ,   thanks to the availability of large monolingual En-   glish datasets covering stylistic attributes like for-   mality and simplicity ( Rao and Tetreault 2018 , Zhu   et al . 2010 , inter alia ) . In recent years , however ,   multilingual and cross - lingual applications of TST   have seen a steady gain in popularity ( Briakou et al . ,   2021 ; Garcia et al . , 2021 ; Krishna et al . , 2022 ) . A   notable instance of cross - lingual TST is attribute-   controlled translation ( ACT ) , in which attribute   conditioning is performed alongside machine trans-   lation ( MT ) to ensure that translations are not only   correct but match user - specified preferences , such   as formality / honorifics ( Sennrich et al . , 2016 ; Niu   et al . , 2017 ; Michel and Neubig , 2018 ; Niu and   Carpuat , 2020 ; Nadejde et al . , 2022 ; Wang et al . ,   2022 ) , gender ( Rabinovich et al . , 2017 ; Vanmassen-   hove et al . , 2018 ; Saunders and Byrne , 2020 ) , and   length ( Lakew et al . , 2019 ; Schioppa et al . , 2021 ) .   ACT is especially important for sectors like cus-   tomer service and business communication , where   stylistic differences can have an impact on user per-   ception ( e.g. , misgendering customers or speaking   to them in an appropriately informal tone can be   offensive or disconcerting ) . Table 1 gives examples   of ACT for formality and gender .   Most prior work on ACT relies on a supervised   adaptation component that conditions the gener-   ative model on the selective attribute . However ,   few annotated ACT datasets are available , and they   generally cover only a limited set of languages and   attributes . Thus , enabling few - shot or zero - shot   ACT would facilitate applying attribute control to   less - resourced attributes and langauges .   In this paper , we introduce a new approach for   ACT : Retrieval and Attribute- Marking enhanced   Prompting ( RAMP ) . Recent studies have shown that   large language models ( LLMs ) can perform MT out   of the box using the prompting paradigm ( Brown   et al . , 2020 ; Lin et al . , 2022 ; Chowdhery et al . ,   2022 ) . We build on this , prompting LLMs to per-   form attribute - controlled MT through two inno-   vations : ( 1 ) retrieval of similar examples and ( 2)1476   explicit attribute marking .   Recent works adopting the prompting paradigm   for text style transfer have mainly focused on the   generalization capabilities of large English - centric   LMs for zero - shot style transfer using previously   unseen style descriptions ( Suzgun et al . , 2022 ; Reif   et al . , 2022 ) . However , prior work on other NLP   tasks has shown that cross - lingual prompting of   multilingual LLMs can be effective ( Zhao and   Schütze , 2021 ; Zhou et al . , 2022 ; Huang et al . ,   2022 ) . As such , we leverage multilingual LLMs   and extend their ACT capabilities cross - lingually to   languages not covered by the in - context examples ,   thus enabling zero - shot ACT .   2 Method   2.1 Preliminaries   Attribute - Controlled Translation ACT takes   two inputs , a sentence xand a desired target at-   tribute a∈A(with Abeing the space of attributes ) ,   and outputs a translation ythat complies with the   specified attribute . It can be formulated as a func-   tionf : ( x , a)→y . In our experiments , we use   attribute values provided by the CCA - MT for-   mality translation dataset and the MT - GE   gender translation dataset , i.e. , A={formal , infor-   mal } or { female , male } .   Prompting In the prompting paradigm for   decoder - only LLMs , inputs are given as decod-   ing prefixes to the model , usually combined with   natural language instructions for output generation .   In style - controlled translation , we formulate the   prompt for target language land attribute aus-   ing the text “ Here is a sentence : { x } Here is its   ltranslation written in a astyle : ” to produce theoutput y. In the few - shot setting , we provide a se-   quence of klabeled in - context examples before the   unlabeled input , which can be formulated as a func-   tionf:{(x , l , a , y ) , . . . , ( x , l , a ) } →   y.   2.2 Our Approach : RAMP   RAMP builds on the success of the prompting   paradigm on few - shot generation tasks such as   monolingual text style transfer ( Reif et al . , 2022 )   and MT ( Garcia and Firat , 2022 ; Agrawal et al . ,   2022 ) by creating more informative prompts   through similarity retrieval andattribute marking .   See Figure 1 for an illustration of RAMP .   Similarity Retrieval In standard prompting , in-   context examples are sampled randomly from the   pool of labeled examples D. InRAMP , we select   examples based on their similarity with the input   text . We first embed both the input text and the   source texts of Dusing all - MiniLM - L6 - v2 ( Wang   et al . , 2020 ) . Then , the top- kmost similar exam-   ples are retrieved for the input text based on cosine   similarity . These are then used in a descending   order w.r.t . similarity as the in - context examples in   the inference prompt . As demonstrated in Figure 1 ,   the in - context example “ You will always be wel-   come here . " has the highest similarity to the test   example “ You ’re welcome . " so it is prompted first .   Attribute Marking In standard prompting , in-   context examples are provided without explicit in-   formation on why they satisfy the prompting ob-   jective . Inspired by recent studies that have shown   that decomposition of complex tasks can improve   prompting quality ( Nye et al . , 2021 ; Wei et al . ,1477   2022 ) , we include for every in - context example   an additional sentence directly after the target sen-   tence that specifies which text spans convey the   desired attribute ( e.g. , “ The translated sentence   conveys a formal style by using words such as   ‘ Vous ’ . ” ) . In our experiments , we use the gold   attribute spans included in the CoCoA - MT and   MT - GenEval datasets . In section 4 we suggest pos-   sibilities for automatically deriving attribute spans   when gold training labels are not available .   2.3 Cross - Lingual Prompting   The similarity retrieval component of RAMP requires   a large pool Dfrom which to find appropriate in-   context examples for prompting . Low - resource   attributes or language pairs may have insufficient   or no annotated data from which to retrieve such ex-   amples . To mitigate this issue , we introduce cross-   lingual prompting , in which the target side of the   in - context examples differs from the desired target   language of the translation task . As demonstrated   in Figure 1 , we study whether the system can lever-   age examples in one language ( e.g. , attribute in-   dicators in Spanish ) to produce the same attribute   in another ( e.g. , French ) . Two main features of   ourRAMP model allow us to perform cross - lingual   prompting : ( 1 ) the use of multilingual LLMs , and   ( 2 ) the example retrieval step , which is done on the   source language only .   3 Experiments   3.1 Datasets   We experiment on two multilingual ACT datasets :   •CCA - MT ( Nadejde et al . , 2022 ) covers   formality - controlled translation in the conver-   sation domain . Source sentences are under-   specified for formality , and references require   formality markings ( formal or informal ) .   •MT - GE ( Currey et al . , 2022 ) covers   gendered translation in the Wikipedia domain .   We use the contextual subset , in which sen-   tences are gender ambiguous in the source   while the reference requires gender marking .   We do not use the disambiguating sentences ,   instead explicitly controlling target gender .   Both datasets have gold annotations for attribute-   marked target spans , and both cover translation   from English into multiple diverse target languages .   We list their target languages in Table 2 .   3.2 Large Language Models ( LLMs )   We select three massively multilingual decoder-   only LLMs for the prompting experiments : XGLM   ( Lin et al . , 2022 ) , BLOOM ( BigScience , 2022 )   andGPT - NX(Black et al . , 2022 ) . The selected   models span three orders of magnitude in terms of   number of parameters and differ in the languages   that they cover ( see Table 2 ) . Appendix D moti-   vates our choice of models in more detail . GPT-3   is not included because it is not freely accessible   and it is not intended for multilingual use - cases .   3.3 Baseline   Attribute tagging is a standard method for ACT ,   so we include a baseline following the approach   and configuration used by Nadejde et al . ( 2022 ):   a transformer MT model ( Vaswani et al . , 2017 )   pre - trained on public parallel data and further fine-   tuned on contrastive training pairs with attribute   tags ( from either CCA - MT orMT - GE ) .   We refer to this as adapted MT .   3.4 Evaluation Metrics   We measure translation quality with BLEU ( Pap-   ineni et al . , 2002 ) and COMET ( Rei et al . , 2020 ) .   For attribute accuracy , we use both ( 1 ) the lexi-   cal matching metrics provided with CCA - MT   andMT - GE ( Lexical - Accuracy ) and ( 2 )   sentence encoders trained on contrastive examples   ( Sentential - Accuracy ) . For ( 2 ) , we train multilin-   gual classifiers on top of the mDeBERTa - v3 en-   coder ( He et al . , 2021 ) . High - performance pre-   trained classifiers have been shown to produce at-   tribute accuracy estimates closer to human judg-   ments for style transfer ( Lai et al . , 2022 ) . Table 3   presents the accuracy of the classification models   on the test sets of their respective datasets , averaged   over all languages.1478   Unlike lexical accuracy , the multilingual at-   tribute classifier does not penalize text generated in   incorrect languages . Thus , in cross - lingual prompt-   ing experiments , we include a step of language   detectionso that generated sentences not in the   requested target language are considered incorrect .   3.5 Results : Same - Language Prompting   We first evaluate the effectiveness of RAMP for   formality- and gender - controlled translation where   the language pair used for in - context examples is   the same as the one used in the prompt candidate   ( e.g. , EN →ES formality - controlled translation us-   ing EN →ES in - context examples ) . We test XGLM   7.5B and BLOOM 175B with 16 in - context ex-   amples on both tasks . Table 4 presents our re-   sults alongside the adapted MT baseline . The base   model uses in - context examples that are sampled   randomly from the pool of labeled examples . We   also include an ablation that adds attribute mark-   ing only on top of base , without similarity retrieval   ( + mark ) .   Using just attribute marking consistently im-   proves attribute accuracy of the generated text , but   it leads to degradation of COMET onCCA-   MT . The complete RAMP with similarity retrieval   not only compensates for the COMET degrada-   tion but also improves quality and attribute metrics   across the board , especially for the high - capacity   BLOOM 175B model .   Adapted MT outperforms BLOOM 175B on   MT - GE in all metrics , but underperforms it   onCCA - MT . This suggests that it is challeng-   ing to do fine - grained comparison between LLMs   and standard MT systems as they might have differ-   ent domain coverage . BLOOM 175B consistentlyoutperforms XGLM 7.5B in both generic transla-   tion quality and attribute control accuracy , so we   proceed with using BLOOM 175B in the cross-   lingual prompting setting .   3.6 Results : Cross - Lingual Prompting   We have demonstrated the effectiveness of select-   ing similar same - language examples to build the   prompt , echoing contemporary work ( Liu et al . ,   2022 ; Agrawal et al . , 2022 ) . In this section , we   evaluate the cross - lingual prompting option , i.e. , re-   trieving in - context examples from other target lan-   guages besides the desired language of translation .   We test this zero - shot setting using the leave - one-   out strategy , and results of tested language pairs   are averaged .   Table 4 presents our results using BLOOM   175B. On both test sets , compared to the baseline ,   we observe improved attribute accuracy and com-   parable or better generic translation quality when   using RAMP with cross - lingual prompting .   We do observe translation quality degradation   with RAMP on some target languages of CCA-   MT , e.g. , . Manual analysis shows that repeated   inaccurate retrieval results could lead to hallucina-   tions . For example , RAMP retrieves multiple sen-   tences containing “ million ” for the input “ If you   got it why not ? He is worth over 20 billion dollars   after all . ” . This results in mistranslation of billion   tomillion ( millionario ): “ Si lo tienes , ¿ por qué no ?   Es millonario después de todo . ” . We give detailed   examples in Appendix H.14794 Conclusions   We introduced the new RAMP in - context learning ap-   proach to leverage attribute annotations and similar   same - language or cross - lingual examples for better   prompting quality . We demonstrated its effective-   ness with multilingual LLMs for both formality-   controlled and gender - controlled translation . We   use gold annotations for attribute marking , but we   leave unsupervised automatic attribute span extrac-   tion as future work .   5 Limitations   •We currently rely on gold annotations for at-   tribute marking , which are not always avail-   able depending on the dataset . However , RAMP   could be easily extended to unsupervised set-   tings through LLM feature attribution ( Sarti   et al . , 2023 ) , i.e. , extracting salient tokens driv-   ing the attribute prediction . This approach   builds upon recent techniques in unsupervised   language generation metrics ( Fomicheva et al . ,   2021 , 2022 ; Leiter et al . , 2022 ) . We leave an   empirical evaluation of its effectiveness to fu-   ture work .   •Besides the choice of in - context examples ,   prompting is also sensitive to their ordering   ( Lu et al . , 2022 ) and the design of the tem-   plate ( Jiang et al . , 2020 ) . We refrain from   tuning example orders and templates to avoid   introducing too many variables .   •Multilingual LLMs perform competitive MT   out of the box for languages seen during   their pre - training . However , we noticed that   BLOOM 175B produces better - transla-   tions than XGLM 7.5B even thoughis not   listed as a training language of BLOOM . This   could possibly be due to typological similarity   between Italian and the Romance languages   included in BLOOM training . We leave ex-   periments of unseen languages as future work .   •Multilingual LLMs like the ones used in this   paper require larger GPU resources for infer-   ence than standard bilingual MT systems .   •One test set we use ( MT - GE ) provides   only two gender values ( female and male ) , but   we do not intend to imply that other genders   do not exist . References1480148114821483A Prompt Templates   Formality - Controlled Translation Here is a   sentence : { x } Here is its ltranslation   written in a astyle : { y } The translated   sentence conveys a astyle by using words   such as ‘ w ’ , ‘ w ’ .   Gender - Controlled Translation Here is a   sentence : { x } Here is its ltranslation   in which the person is a : { y } In the   translation , the agender of the person   is made explicit by words such as ‘ w ’ ,   ‘ w ’ .   B Language Code   C Additional Details of Datasets Splits   and Pre - Trained Attribute Classifiers   We use the original train / test split provided by the   CCA - MT dataset . Each split contains tele-   phony and topical_chat domains . We use the   topical_chat domain in our experiments . MT-   GE contains a dev and test split , and we   use the dev split as training data for the classifica-   tion model and prompting experiments .   We finetuneDBERT-3- modelon   the contrastive examples in the respective training   sets to get the attribute classifiers . We finetune the   classifier for 2 epochs with a batch size of 8 , learn-   ing rate 2e-5 , 500 warm up steps , max sequence   length of 256 , and save checkpoint every 500 steps .   We do not do hyperparameter tuning , and thus , a   validation set is not used .   D Selection of Large Language Models   XGLM ( Lin et al . , 2022 ) is a 7.5B - parameter   model trained on a balanced corpus containing 30   languages ( excluding ) . It was shown to outper-   form much larger models such as GPT-3 on tasks   related to machine translation and cross - lingual lan-   guage understanding . We select it due to its broad   linguistic coverage and its manageable size .   BLOOM ( BigScience , 2022 ) is a model avail-   able in multiple sizes , trained on a curated corpusspanning 46 natural languages ( and 13 program-   ming languages ) . However , many of the test set   languages are not part of its pre - training corpus ( see   Table 2 ) . We evaluate two variants of the model   ( 7.1B and 175B parameters ) to assess how it is af-   fected by a massive scaling in model parameters .   The larger variant has a parameter count compara-   ble to the one of GPT-3 , while it is presently the   largest publicly available multilingual LLM .   GPT - NX(Black et al . , 2022 ) is a 20B-   parameter model trained on The Pile ( Gao et al . ,   2021 ) , a large English - centric corpus covering a   broad range of domains . While the model saw   mainly English data during pre - training and as such   is not intended for multilingual usage , it exhibits   interesting generalization performances for many   of our target languages .   E Preliminary Evaluation of   Same - Language Prompting   We conduct preliminary evaluations aimed at   reducing the number of experimental settings .   We perform formality - controlled translation using   CCA - MT , and evaluate LLMs by varying the   number of in - context examples ( i.e. , 4 - 8 - 16 - 32 , se-   lected based on the feasible context length ) .   Figure 2 presents results averaged across all   four languages seen byBLOOM during its pre-   training . Observations :   •RAMP generally outperforms base prompting   ( i.e. , random in - context examples and no at-   tribute marking ) across most LLMs and ex-   ample settings for both BLEU and formality   accuracy .   •BLEU and formality accuracy improve with   increased model size and with the number of   examples , until this number reaches 16 .   Based on these results we move forward with the   XGLM 7.5B and BLOOM 175B models and 16   examples .   F Detailed Scores of Aggregated Results   •Table 5 : Detailed scores of same - language   prompting on CCA - MT ( preliminary eval-   uation).1484   •Table 6 : Decomposed results of same-   language prompting on CCA - MT ( full   evaluation ) .   •Table 7 : Decomposed results of same-   language prompting on MT - GE ( full   evaluation ) .   •Table 8 : Decomposed results of cross - lingual   prompting on CCA - MT .   •Table 9 : Decomposed results of cross - lingual   prompting on MT - GE.G Amended Details of Cross - Lingual   Prompting   We test the zero - shot setting using the leave - one-   out strategy , i.e. we retrieve in - context examples   from every languages except the desired language   of translation . We ensure that we retrieve an equal   number of examples from all languages : the num-   ber of examples retrieved from each language is   the total desired number of in - context examples di-   vided by number of training languages . In CCA-   MT , we retrieve 14 in - context examples from 7 lan-   guages . In MT - GE , we retrieve 8 in - context   examples from 8 languages . We reduced the num-   ber of in - context examples in this setting to avoid   out - of - memory errors with BLOOM 175B.   H Error Analysis of Cross - Lingual   Prompting   Table 10 shows two examples where RAMP per-   forms significantly worse than the base model in   terms of COMET . In the first example , having mul-   tiple in - context examples containing “ million ” led   the model to mis - translate “ billion ” to“million ” .   In the second example , we observe that the color re-   lated in - context examples led the model to produce   hallucinated output about clothing colors .   Repeated misleading in - context examples are   less observed on MT - GE and in the same-   language setting because ( 1 ) CCA - MT trans-   lates the same set of English sentences to different   languages while MT - GE collects English   sentences independently ; ( 2 ) There are no dupli-   cated source ( English ) sentences for each language .   ( Therefore , if RAMP retrieves duplicated English   sentences as in Table 10 , their reference transla-   tions are guaranteed to be in different languages.)1485148614871488ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.1489 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.1490