  Xiaoyang HuShane StorksRichard L. LewisJoyce ChaiComputer Science and Engineering Division , University of MichiganDepartment of Psychology , University of Michigan   { nickhu , sstorks , rickl , chaijy}@umich.edu   Abstract   Analogical reasoning is a fundamental capacity   of human cognition that allows us to reason   abstractly about novel situations by relating   them to past experiences . While it is thought   to be essential for robust reasoning in AI sys-   tems , conventional approaches require signif-   icant training and/or hard - coding of domain   knowledge to be applied to benchmark tasks .   Inspired by cognitive science research that has   found connections between human language   and analogy - making , we explore the use of in-   tuitive language - based abstractions to support   analogy in AI systems . Specifically , we apply   large pre - trained language models ( PLMs ) to   visual Raven ’s Progressive Matrices ( RPM ) , a   common relational reasoning test . By simply   encoding the perceptual features of the problem   into language form , we find that PLMs exhibit a   striking capacity for zero - shot relational reason-   ing , exceeding human performance and nearing   supervised vision - based methods . We explore   different encodings that vary the level of ab-   straction over task features , finding that higher-   level abstractions further strengthen PLMs ’ ana-   logical reasoning . Our detailed analysis reveals   insights on the role of model complexity , in-   context learning , and prior knowledge in solv-   ing RPM tasks .   1 Introduction   Humans are constantly presented with novel prob-   lems and circumstances . Rather than understand   them in isolation , we try to connect them with past   experiences . With any luck , we might find an anal-   ogy : a mapping between relevant aspects of this   new situation and a past situation , which helps   form abstractions that allow us to reason more ef-   fectively in the future ( Holyoak , 1984 ) . Analogy   is thought to underpin humans ’ robust reasoning   and problem solving capabilities ( Hofstadter andFigure 1 : Raven ’s Progressive Matrices ( Raven and   Court , 1938 ; Zhang et al . , 2019a ) are an analogy - making   task where one must infer the missing matrix item based   on abstract rules instantiated in the first two rows . To   demonstrate the potential analogical reasoning skills   in pre - trained language models , we develop language-   based abstractions over their key perceptual features ,   then prompt them to select the completion of the matrix .   Sander , 2013 ) , and thus it is believed to be prereq-   uisite in order to enable the same in AI systems .   However , conventional approaches struggle with   analogy - making , and are trained on thousands of   examples to achieve any success on benchmark   tasks . This is unsatisfying , as humans are capa-   ble of analogy - making without explicit training ,   and such analogy - making should enable zero - shot   generalization to new situations ( Mitchell , 2021 ) .   Interestingly , a body of work in cognitive sci-   ence suggests that analogy - making and relational   reasoning are connected to humans ’ symbol sys-   tem and language capabilities ( Gentner , 2010 ) . For   example , Gordon ( 2004 ) finds that members of an   Amazonian tribe that count only with words for   “ one , ” “ two , ” and “ many ” struggle to make analo-1953gies with higher numbers . Further , Gentner et al .   ( 2013 ) find that deaf children whose sign language   does not involve spatial relations are outperformed   by hearing children on a spatial relational reason-   ing task , while Christie and Gentner ( 2014 ) find   that assigning even nonsensical names to relations   enhances children ’s relational reasoning . All of   this demonstrates that language serves as a power-   ful way for humans to abstract and better reason   about the overwhelming and complex percepts we   encounter in the world .   In this work , we explore whether language may   serve a similar purpose in AI systems . Specifically ,   we apply contemporary autoregressive pre - trained   language models ( PLMs ) to Raven ’s Progressive   Matrices ( RPM ) , an example of which is shown in   Figure 1 . RPM is a widely used psychometric test   for relational reasoning that requires inducing an   abstract rule from just two examples of short se-   quences of groups of shapes , and then applying the   rule to complete a new partial sequence ( Raven and   Court , 1938 ) . This task makes minimal assump-   tions about the test taker ’s prior knowledge , and   is thus thought to provide a good estimate for gen-   eral intelligence ( Holyoak , 2012 ) . On the RA VEN   dataset ( Zhang et al . , 2019a ) , we find that given   the ability to perceive key features of RPMs , large   PLMs exhibit a surprising capacity for zero - shot re-   lational reasoning , approaching that of supervised   vision - based deep learning approaches and even hu-   mans . We propose three levels of abstraction over   the language features of the task using name assign-   ment and task decomposition , and find that each   abstraction further strengthens PLMs ’ relational   reasoning . Our results and detailed analysis offer   insights on PLM performance , including the role of   models ’ complexity , in - context learning , and prior   knowledge in emergent relational reasoning , and   suggest that they could play an important role in   future cognitive architectures for analogy - making .   2 Related Work   Past work has studied analogy in AI across var-   ious domains . Mitchell ( 2021 ) provides a com-   prehensive overview of these efforts , especially   those applied in idealized symbolic domains . Here ,   symbolic and probabilistic methods have tradition-   ally been applied ( Gentner , 1983 ; Hofstadter and   Mitchell , 1994 ; Lake et al . , 2015 ) . However , theseapproaches typically require hard - coding domain-   specific concepts , and require substantial search   through domain knowledge to operate on their tar-   get problems , thus making them unscalable . The   creation of large - scale image datasets for analogy   tasks here ( Zhang et al . , 2019a ; Hu et al . , 2021 ;   Odouard and Mitchell , 2022 ) have enabled further   research with deep learning and neuro - symbolic   methods ( Hill et al . , 2019 ; Spratley et al . , 2020 ;   Kim et al . , 2020 ; Zhang et al . , 2021 ) , which bring   the advantage of requiring less ad - hoc encoding of   domain knowledge , but require thousands of train-   ing examples to learn the tasks , still limiting their   generalization capability .   Other work has explored AI systems ’ analogy-   making in real - world domains , including in natural   images ( Teney et al . , 2020 ; Bitton et al . , 2022 ) and   language ( Li et al . , 2020 ; Chen et al . , 2022 ; Sul-   tan and Shahaf , 2022 ) , especially lexical analogies   ( Turney et al . , 2003 ; Turney , 2008 ; Speer et al . ,   2008 ; Mikolov et al . , 2013b , a ; Linzen , 2016 ; Lu   et al . , 2019 ) . However , these domains make it diffi-   cult to control the prior knowledge required to solve   tasks ( Mitchell , 2021 ) , and in the context of recent   generative foundation models that are extensively   pre - trained on natural data , it becomes difficult to   separate analogy learning from distributional pat-   terns that can be overfit . Unlike prior work , we   apply such foundation models for language to ana-   logical reasoning in a zero - shot setting , bypassing   the requirement of hard - coding domain knowledge   or training models on task - specific data . Further-   more , while contemporaneous work has applied   PLMs to a variety of simpler relational reasoning   tasks in language ( Webb et al . , 2022 ) , we systemat-   ically explore the advantage of using language to   abstract over complex visual features of the task ,   opening questions about how the powerful sym-   bol systems learned in PLMs may support robust ,   perception - driven reasoning in future AI systems .   3 Raven ’s Progressive Matrices   Raven ’s progressive matrices ( RPM ) are abstract   relational reasoning tasks used in cognitive psy-   chology to test humans ’ analogy - making ( Raven   and Court , 1938 ) . Each instance of RPM is a ma-   trix consisting of 9 items arranged in a square , the   last of which must be selected from a set of choices .   Each item consists of several perceptual attributes ,   such as shape , color , or more abstract features .   Within each row of the matrix , a relation is applied1954   over these attributes , such as progression of numer-   ical values associated with these attributes . Given   the first two rows of the matrix , the challenge of   the task is to identify the relations being applied to   items , and apply them analogously in the third row   to infer the missing ninth item . Successfully solv-   ing an RPM requires tackling two sub - problems :   perception of each item ’s attributes , and reasoning   over multiple items ’ attributes to infer and apply   relations .   3.1 RA VEN Dataset   We focus our study on RA VEN ( Zhang et al . ,   2019a ) , which provides a large - scale benchmark   for RPM tasks for training and evaluation of AI sys-   tems . Each RPM has 8 possible candidate items to   complete it . As shown in Figure 2 , each item may   consist of compositional entities , layouts , and/or   component structures , and RA VEN provides a suite   of increasingly complex sub - tasks built from these   elements . We introduce their unique attributes be-   low , as well as relations that may occur over them   across items in the matrix .   Entities . A single entity has a type ( i.e. , shape ) ,   size , and color selected from a small number   of classes . Each of these attributes is associated   with a number : type with the number of sides   in the entity ’s shape , size with its diameter , and   color with the darkness of its shading . The sim-   plest sub - task of RA VEN is Center , where each   item only consists of a single entity .   Layouts . Layouts of entities bring additional   higher - level attributes to items , specifically the   number ( i.e. , count ) and position of entities   within a layout . In the 2x2Grid and3x3Grid   sub - tasks of RA VEN , each item consists of multi-   ple entities arranged in a grid . Component structures . Items may also be   composed of multiple sub - items or components ;   RA VEN includes four sub - tasks that introduce this   even higher - level challenge : L - R , U - D , andO - IC ,   each of which consist of two single entities in dif-   ferent configurations , and O - IG , which consists of   a 2 - by-2 grid inside of a larger entity .   Relations . Following prior work on this task ,   RA VEN applies four different relations to item   attributes across rows of the matrix . These are   Constant , which does not modify an attribute ,   Progression , which increases or decreases the   value of an attribute by 1 or 2 , Arithmetic ,   which performs addition or subtraction on the first   two attributes of the row to create the third , and   Distribute Three , which distributes three   consistent values of an attribute across each row .   4 Methods   In order to apply PLMs to RA VEN , we abstract   the visual features of the task into language . Our   abstractions are intentionally applied on a per - item   basis to tackle the perception problem of the task   without giving the PLM explicit hints toward the   reasoning problem ( which requires capturing pat-   terns over multiple items ) . This allows us to focus   on evaluating the reasoning capabilities of PLMs .   First , we introduce our multi - level abstractions   for the RA VEN dataset . Then we formally define   the interface between PLMs and the RPM task .   4.1 Abstractions in RA VEN   We define abstractions for entity - level attributes ,   layout - level attributes , and component structures   which convert the RPM task into one or more text   prompts . We apply two kinds of abstractions : nam-   inganddecomposition . As discussed in Section 1 ,   assigning names to perceptual features strengthens   humans ’ analogy - making skills over them . Inspired   by this , naming abstractions abstract over attributes   or combinations of attributes in the RPM by as-   signing a unique name to describe them . Mean-1955   while , jointly understanding and tracking the com-   plex features of the task can become a burden even   for humans . Inspired by humans ’ capability to   decompose complex tasks into independent sub-   tasks ( Lee and Anderson , 2001 ) , decomposition ab-   stractions split the RPM into multiple sub - matrices   by its independent features , then generate a sepa-   rate prompt for each one . We can then prompt a   PLM once for each sub - matrix , and aggregate PLM   outputs to choose a candidate matrix completion .   4.1.1 Entity - Level Abstractions   As shown in Figure 3 , we can abstract perceptual   entity attributes into language by assigning them   names , then generating prompts to represent the   full RPM using these names . As each of an en-   tity ’s attributes is numerical by nature , we assign   each attribute an ordinal numerical name ; type   is named by the number of sides of the associated   shape ( e.g. , “ 3 ” for triangle ) , size is named by   a decimal representing its diameter , and color is   named based on the darkness of the entity ’s shade .   As each of an entity ’s attributes is independent , i.e. ,   a relation over one attribute has no connection to   relations over other attributes , we can decompose   the RPM task by these attributes into three separate   sub - tasks with their own prompts.4.1.2 Layout - Level Abstractions   As shown in Figure 4 , we next propose abstractions   for layouts of entities ( e.g. , in grid - based sub - tasks   of RA VEN ) . First , the number attribute of a layout   corresponds to the count of entities in it . Recogniz-   ingnumber requires implicitly counting entities   within a layout , which may be difficult to disen-   tangle from other attributes . As such , we directly   expose this attribute by extracting this count and   encoding it in text . Since this layout attribute is   independent from other attributes , we can again   decompose the task and consider it separately from   entity attributes .   Theposition attribute encodes even more   complex information about a layout , and relations   over it may move entities around within the lay-   out . However , an occupancy map serves as a strong   naming abstraction for position which omits   distracting details of specific entities while expos-   ing key information for detecting relations over it .   We generate the occupancy map as an array of text   representing the occupancy of the layout , and de-   compose this from other attributes . Notably , this   abstraction provides a unique language description   for each possible global configuration of entities   within a layout , allowing the PLM to disentangle   global and local patterns in the problem , a help-   ful capability of humans ( Robertson and Lamb ,   1991 ) .   In RA VEN , relations are applied to specific at-   tributes consistently across all entities in a layout .   As our layout - level abstractions make explicit the   key features of layouts , we no longer need to track   entity - level attributes for specific entities within   them . Specifically , rather than supply a PLM with   a separate grid - like prompt for each entity - level   attribute , we simply provide a list of unique at-   tribute values . This reduces the complexity added   by layouts of multiple entities .   4.1.3 Structural Decomposition Abstractions   In cases with multiple components in each item ,   we may find that prompts become long and compli-   cated with earlier approaches . Since each compo-   nent ’s attributes and relations are independent , we   can alternatively decompose the task by its com-   ponents . For each component , we can generate   a prompt through entity attribute naming abstrac-   tions as shown in Figure 3 ( left ) , or we can apply1956the higher - level abstractions over entity and lay-   out attributes shown in Figure 4 , thus decomposing   each component ’s prompts into prompts for each   attribute . As this structural decomposition con-   verts multi - component problems into several sim-   pler single - component , single - attribute problems ,   the complexity added by multiple components is   abstracted away .   4.2 Problem Definition   Formally , a complete RPM Mconsists of 9 matrix   items mwhere row and column i , j∈ { 1,2,3 } .   As discussed in Section 3.1 , an individual item m   in the RA VEN dataset is formalized by high - level   components consisting of layout - level attributes   and entity - level attributes . Given all items in M   except for m , the task is to identify mfrom   a setYof8choices by identifying abstract rules   over the attributes within the first 2 rows of M , and   selecting the candidate mthat correctly applies   these rules in the third row .   Applying PLMs . We apply PLMs to RA VEN in   a zero - shot setting . In the absence of decomposi-   tion abstractions , we define Las the mapping of a   complete RPM to a text prompt . The PLM ’s choice   formis given by   arg max1   | L|log Pr ( L(m , y ) )   where | L|denotes the number of tokens in the   prompt . When decomposition is introduced , L   instead returns multiple prompts , and the ( token-   length normalized ) log - probabilities of all sub-   prompts are summed .   5 Experimental Results   Now , we can examine the impact each of these   language - based abstractions has on the perfor-   mance of transformer - based , autoregressive PLMs   in relational reasoning on RA VEN . To further un-   derstand their impact with respect to model com-   plexity , we evaluate a range of model sizes : OPT   125 M , 1.3B , and 13B ( Zhang et al . , 2022 ) , along   with GPT-3 ( Brown et al . , 2020).Models are eval-   uated on a random subset of 500 testing examples   from each sub - task of RA VEN.After introducing some comparison approaches ,   we present the experimental results from our ap-   plied abstractions on PLMs ’ entity - level , layout-   level , and component - level relational reasoning .   Afterward , we dive deeper with an analysis on how   both our abstractions and in - context learning con-   tribute to model performance .   5.1 Comparison Approaches   To contextualize our findings , we provide results   from the human study in Zhang et al . ( 2019a ) , as   well as two supervised baselines from prior work .   Additionally , to specifically evaluate the advantage   of the way we mapped the RPM task into language ,   we include two simpler abstraction methods that   encode task information less explicitly .   Supervised baselines . While our goal is not to   achieve the state of the art on RA VEN , we include   results from two state - of - the - art supervised base-   lines for reference . Specifically , we select the   two approaches with the top mean accuracy on   RA VEN , as outlined in the survey by Małki ´ nski and   Ma´ndziuk ( 2022 ): Rel - AIR ( Spratley et al . , 2020 )   and CoPINet + ACL ( Kim et al . , 2020 ) . Rel - AIR   combines a simple vision model with an unsuper-   vised scene decomposition module , enabling more   generalizable reasoning over entities in RA VEN .   CoPINet + ACL applies an analogy - centric con-   trastive learning paradigm to CoPINet ( Zhang et al . ,   2019b ) , a prior architecture proposed for percep-   tual inference trained through contrastive learning .   Both baselines have been trained on thousands of   examples from the RA VEN dataset , and incorpo-   rate task - specific inductive biases in their architec-   ture . Meanwhile , we evaluate PLMs on RA VEN in   a zero - shot setting with no supervised learning .   Quasi - image abstraction . To evaluate the help-   fulness of naming abstractions over entity at-   tributes , we should compare to an approach that   does not have such abstraction . However , some   mapping from the visual features of the RPM task   into langauge is needed in order for a PLM to inter-   face with it . While the limited context window of   PLMs restricts us from incorporating raw pixels di-   rectly into our prompts , PLMs have recently been   demonstrated to capture spatial patterns in simi-   lar inputs : text - based matrices ( Patel and Pavlick,1957   2021 ) . As such , we propose a quasi - image abstrac-   tion which converts the visual RPM task into a   matrix of ASCII characters . As shown in Figure 5 ,   an entity ’s type can be expressed through a matrix   of characters ; size can be expressed through the   height and width of the matrix ; and color can be   expressed through the actual characters making up   the matrix . By converting instances of RA VEN ’s   Center sub - task into this pixel - like form , we have   a lower - level abstraction of the task ’s visual fea-   tures that can be compared to the higher - level ab-   straction of naming entity attributes .   Random naming abstraction . We would also   like to understand the advantage of the specific   names we chose for entity attributes compared to   other possible choices . As such , we propose a sec-   ond baseline where , instead of using ordinal labels   to describe entities ’ type , size , andcolor , we   choose random words from a large corpus . This   removes numerical dependencies that may be uti-   lized to recognize some relations , and can help us   understand whether PLMs take advantage of this   information when it is available .   5.2 Entity - Level Reasoning   We first evaluate PLMs under our lowest level ab-   stractions over entity attributes . To isolate the im-   provements from such abstraction , we focus on   theCenter sub - task of RA VEN which only in-   cludes a single entity per item in the RPM , and thus   only tests understanding of relations over entity at-   tributes . The results are shown in Figure 6.Impact of naming . Under the simplest abstrac-   tion of naming the entity - level attributes , we see   impressive zero - shot accuracies that monotonically   increase with model size up to 77.2 % from GPT-   3 175B on Center , nearing human performance .   Further , we find that our choice to map attributes   into numerical symbols is consistently advanta-   geous over the quasi - image and random - naming   abstractions , which reach respective accuracies up   to 28.2 % and 51.8 % . Meanwhile , we find that as   model size increases , our ordinal naming approach   outperforms the random naming baseline more and   more , up to over 20 % in larger model sizes . This   suggests that PLMs of larger size can better capture   and take advantage of implicit numerical relations   in their vocabulary .   Impact of decomposition . When applying de-   composition over entity attributes , we observe fur-   ther improvement of 2.8 % accuracy in GPT-3 175B.   Interestingly , we see a much sharper improvement   from this abstraction in smaller models , with OPT   125 M ’s accuracy doubling from 22.2 % to 45.6 % ,   and OPT 1.3B ’s accuracy rising from 47.2 % to   72.0 % . This may suggest that PLMs have a limited   working memory which is related to the number   of learned parameters in them . Large PLMs are   more capable to handle complex reasoning tasks   because of this , while smaller PLMs benefit from   decomposing tasks into more manageable parts .   5.3 Layout - Level Reasoning   In Figure 7 , we evaluate PLMs ’ capability to   capture relations over layout attributes under our   abstractions introduced in the 2x2Grid and   3x3Grid sub - tasks . Without any decomposi-   tion abstraction , model performance reaches up   to 78.0 % and 86.4 % accuracy respectively on   2x2Grid and3x3Grid . When adding naming   for layout - level attributes and decomposing all at-   tributes into separate prompts , we see further im-   provements across the board , with accuracies reach-   ing 87.8 % on 2x2Grid and 93.2 % on 3x3Grid .   The PLM exceeds human performance on both   sub - tasks , despite them being arguably some of   the most complex tasks in RA VEN , with the latter   comprised of more entities than any other sub - task .   This suggests that our strong layout - level abstrac-   tions enable the PLM to tease apart the numerous   attributes in grids of entities and capture obscure   patterns , whereas humans may struggle with this   as the task becomes more complex.1958   5.4 Component - Level Reasoning   Lastly , we apply our structural decomposition-   based abstractions on RA VEN sub - tasks which   have multiple components , i.e. , L - R , U - D , O - IC ,   andO - IG . The results are shown in Figure 8 . First ,   just decomposing the task by its components im-   proves the maximum accuracy on each task on   average by about 20 % . Additionally decomposing   each component by its entity and layout attributes   brings further gains , with GPT-3 175B reaching   up to 77.6 % , 78.0 % , 82.8 % , and 92.6 % on L - R ,   U - D , O - IC , andO - IG respectively , and exceeding   humans and nearing supervised baselines on the   latter . The performance gain from this decompo-   sition is again even more pronounced for smaller   PLMs . Most significantly , OPT 1.3B improves   from 20 - 30 % accuracy to over 70 % accuracy , near-   ing human performance . This demonstrates that   not only is GPT-3 capable of very complex analog-   ical reasoning tasks , but even PLMs less than 100   times its size can perform quite well here with the   proper abstractions .   5.5 Fine - Grained Analysis   Finally , we analyze how model performance varies   across different attributes and relations , as we in-   troduce distracting attributes , and as we introduce   rows into the matrix . In our analysis , we compare   three representative levels of abstraction : entity   attribute naming only ( no decomposition into mul-   tiple prompts ) , decomposition of components , and   fulldecomposition of entity and layout attributes   and components .   5.5.1 Analysis of Attributes and Relations   We measure the impact of abstractions in capturing   each attribute and relation in RA VEN . In Figure 9 ,   we present GPT-3 175B ’s accuracy over each at-   tribute and relation . We find that number is the   best captured attribute even without any decompo-   sition abstractions , while the model struggles with   position until we introduce decomposition of   attributes , suggesting the occupancy map encoding   used here indeed helped capture it . Meanwhile ,   Arithmetic is the most difficult relation , with   consistently lower accuracy than other relations .   5.5.2 Robustness to Distracting Attributes   Since our mappings from RA VEN attributes into   language provide the key features over which rela-   tions occur , we may wonder how robust PLMs   are to distracting or unimportant attributes . In   fact , the RA VEN dataset includes one noise at-   tribute that we excluded from our mapping to   avoid unnecessarily increasing prompt lengths :   orientation , i.e. , the rotation of entities in the   RPM . To begin exploring this issue , we incorpo-   rateorientation into the problem as a fourth   entity - level attribute in addition to type , size ,   andcolor . For the best model ( i.e. , GPT-3 ) on the   Center sub - task , we compare two possible injec-   tions of orientation values : using the values   provided in RA VEN ( which are mostly constant   within each matrix row ) , and randomly selected   values ( which could be more distracting ) .   As shown in Table 1 , compared to GPT-3 ’s   Center accuracies of 77.2 % and 80.0 % with re-   spective naming and decomposition abstractions ,   the injection of orientation as a distraction   feature does not degrade the model performance   much , achieving accuracies of 76.0 % and 80.0 %   when using values from RA VEN , and 72.6 % and   77.8 % when using random values . This shows   that PLMs exhibit some robustness to distracting   attributes in language context , and have the capabil-   ity to ignore them in analogical reasoning . Future   work may consider more in - depth analysis to dis-   cover the extent of model robustness to distraction   features , and how it varies by model complexity.1959   5.5.3 In - Context Learning Over Rows   By design , RPM tasks are meant to require mini-   mal background knowledge . They should be im-   possible to solve without the first two rows of the   matrix , which provide essential context to com-   plete the third row of the matrix . To understand   whether PLMs capture relations specifically from   in - context learning over the first two rows of the   matrix ( as opposed to using prior knowledge from   pre - training ) , we measure the model performance   as we introduce rows to the matrices .   As shown in Figure 10 , the average model per-   formance increases across all sizes and abstractions   as rows are added to the matrix . This suggests that   in - context learning indeed contributes significantly   to performance , even for smaller models . Larger   model sizes see the most significant improvements ,   suggesting that larger PLMs are stronger in - context   learners than smaller ones . Further , larger PLMs   can achieve nearly the same accuracy with only   two rows of the matrix provided rather compared   to having all three , suggesting that they pick up the   task quite quickly from in - context learning .   We also observe that in many cases , models   achieve accuracies above chance ( 12.5 % accuracy )   without being provided any complete rows of the   matrix ( only the third , incomplete row ) . This may   suggest the PLM has a useful prior for this problem ,   despite it being a visual problem and thus impossi-   ble to observe directly in pre - training . This raises   questions about the objectivity of RA VEN and pos-   sibly the RPM task . Further , when decomposi-   tion abstractions are applied , models achieve higher   accuracies than when not , suggesting that decom-   position encodes some of this prior knowledge for   the task . In Table 2 , we take a closer look at GPT-3   175B ’s performance within sub - tasks . Surprisingly ,   we find the highest accuracies on the grid - based   sub - tasks , despite them being the most difficult   tasks for humans .   This motivates future work to compare human   and PLM performance on ablated analogy - making   tasks like these to further evaluate their objective-   ness and identify commonalities . Future work in AI   and analogy may also consider building diagnostic   datasets to tease apart attribute and relation types   to better understand how they contribute to model   performance and identify areas for improvement .   In - context learning of attributes and relations.1960   We may wonder whether specific relations or at-   tributes are easier to understand than others with   less context . For example , the Progression or   Constant relations may be possible to recognize   only from the first two items of the third row in an   RPM , as we can easily observe patterns in attribute   values here , e.g. , that entity size is increasing   orcolor remains constant . In Figures 11 and   12 , we surprisingly observe only marginal differ-   ences here , except for the number attribute , which   seems significantly better captured than other at-   tributes in this no - context setting.6 Conclusion   In this work , we explored the ability of large PLMs   to perform zero - shot analogical reasoning in visual   Raven ’s Progressive Matrices ( RPM ) . Upon the   simplest mapping to language , they can achieve   striking results , while applying higher - level nam-   ing and decomposition abstractions over the task   features further raises performance to the level of   humans and supervised approaches in some cases .   We find that while ordinal naming abstractions are   a powerful way to enable analogical reasoning in   larger PLMs , decomposition abstractions that break   the task down into atomic parts conserve their work-   ing memory such that even smaller PLMs under 1B   parameters can achieve competitive performance   on this challenging problem .   Our detailed analysis revealed insights about   which features of the task PLMs best capture , their   robustness to distracting features , and the role of   in - context learning and prior knowledge in picking   up this complex task . Surprisingly , we find that   even without two complete rows of prior context   from the matrix , GPT-3 175B and smaller mod-   els can achieve above - chance performance on the   task , raising questions about the objectivity and   true role of prior knowledge in RPM tasks , which   are assumed to require minimal prior knowledge .   These results also raise some questions about   the role PLMs may play in future AI systems capa-   ble of analogy . While previously thought to be a   difficult problem for AI systems , PLMs can solve   the reasoning step of analogy easily given strong   abstractions over visual perception . Many of these   abstractions are intuitive and commonly researched   in computer vision , including the detection of ob-   ject types , sizes , colors , counts , and global arrange-   ments . As such , future work may dive deeper into   the challenging problem of generalized perception   across domains , where we must robustly tease apart   the key features of tasks and experiences that may   facilitate analogy - making , e.g. , in recognizing the   commonalities between a physical bridge and the   bridge of a song ( Mitchell , 2021 ) . Recent efforts to-   ward understanding how humans describe abstract   visual features in language by mapping them to nat-   ural conceptsare a promising direction toward   this goal ( Lachmy et al . , 2022 ; Ji et al . , 2022).1961Acknowledgements   This work was supported in part by DARPA PTG   program HR00112220003 . We would like to thank   the anonymous reviewers for their valuable com-   ments and suggestions .   Limitations   Perception and reasoning in text - based RA VEN .   In this work , one limitation is that we do not at-   tempt to solve the perception problem of analogy-   making in RPM , rather we apply perfect perception   in solving the reasoning part , and assume the per-   ception problem is simple . By doing so , we find   that PLMs may be a strong solution to the reasoning   problem here , which may better direct future efforts   toward AI and analogy . Obviously , the perception   problem for idealized domains is a lot different than   more natural domains , and identifying key features   across many domains that can facilitate a mapping   is still a challenging unsolved problem . We hope   that our work sparks more interest in this problem .   Meanwhile , one may argue that our decomposi-   tion abstractions are too strong , and actually con-   tribute to the reasoning problem in RPM , as they   make an independence assumption about which   features of the task can be teased apart . Making   such an assumption requires an understanding of   the problem that can not be inferred by only see-   ing one instance . However , we decomposed the   task based on very intuitive and common attributes ,   e.g. , shapes , colors , sizes , and counts of items .   We believe that the strength of such an abstrac-   tion , which could be applied in many problems ,   should not be understated . Nonetheless , we include   decomposition - free forms of results as much as   possible throughout the paper to help compare the   contributions of decomposition versus naming ab-   stractions , which is more clearly only providing per-   ceptual information . In fact , we find that without   any decomposition , PLMs still achieve very strong   performance in many cases , and performance gains   from decomposition are not always large .   Human performance . Lastly , we note some lim-   itations in the human performance measurements   used as reference points . In Zhang et al . ( 2019a ) ,   human performance on RA VEN was measured by   giving subjects some task - specific training , then   evaluating them on the original visual form of   the task . This differs from our results in two   ways . First , PLMs had no task - specific trainingfor RA VEN , given that experiments were zero - shot   and the text data we generate is new and thus im-   possible to appear directly in PLM pre - training .   This may give humans an advantage . Second , the   task is presented to PLMs in text form , not visually .   While the essential information from the task is   preserved by our conversion , it is possible that this   conversion would affect the difficulty of the task   for humans ( making it easier or harder ) . As such ,   it becomes unclear how to contextualize our results   with these past human results . Future work may   carry out systematic human studies to compare the   analogical reasoning capabilities of humans and   PLMs in different settings .   Ethical Considerations   This work does not use any human subjects or   human - generated data . Our work deals with ab-   stract visual features that are described with nu-   merical symbols , thus not strongly targeting any   language . A possible ethical concern for this work   is the amount of computational resources used in   evaluating PLMs . To reduce unnecessary computa-   tion in our study , we chose to apply PLMs to only a   subset of 500 testing examples from each sub - task   of the RA VEN dataset , while the full testing set is   four times as large .   References196219631964A Expanded Results   In Table 3 , we present additional results with a   wider range of OPT model sizes ( Zhang et al . ,   2022 ) . We observe similar mostly monotonic in-   creases of accuracy with model size .   B Results and Analysis with I - RA VEN   As the generation strategy for the negative choices   in RA VEN can introduce distributional bias that is   problematic for supervised learning and leads to   artificially high performance ( Hu et al . , 2021 ) , this   could be a possible reason behind PLMs ’ strong   performance on the task even without any com-   plete rows of context . As such , in Table 4 and   Figure 13 , we include some supplementary anal-   ysis on the Impartial - RA VEN ( I - RA VEN ) dataset   from Hu et al . , which introduces more variation in   negative choices . However , we observe similar per-   formance trends in I - RA VEN . Performance mostly   monotonically increases with model sizes and more   abstraction . Further , PLMs achieve above - chance   performance again without any rows of context   provided , even with no decomposition abstractions .   This provides further evidence that RPM , at least   formulated in this way , is in part addressed by   PLMs ’ prior knowledge , despite the assumptions   of minimal background knowledge that the task   makes .   C Example Prompts   In Figure 14 , we include example prompts for   2x2Grid , 3x3Grid , L - R andI - OG subtasks   under different abstractions . Note that U - D andI - OC are isomorphic to L - R , and therefore share   the same prompt format.196519661967ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations discussed after Section 6 .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Dataset introduced in Section 3 .   /squareB1 . Did you cite the creators of artifacts you used ?   We cited the authors of the RAVEN dataset when introducing it in Section 3 ( and other sections ) . We   also cited the authors of the I - RAVEN dataset in appendices involving it .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   We were unable to ﬁnd license information for the RAVEN dataset we used , although it is publicly   available . We will not be re - distributing the dataset .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Our method of adapting the vision - based RAVEN dataset to language is described in Section 4 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   We describe the dataset in detail in Section 3 ; it is idealized abstract data which does n’t pertain to   speciﬁc languages or demographic groups .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Discussed at beginning of Section 5.1968C / squareDid you run computational experiments ?   Section 5 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In Section 5 , we reported all model complexities . When it comes to compute budget , this is difﬁcult   to report as experiments were run on several different platforms ( OpenAI cloud API , institutional   computing cluster , and more ) . However , we provided the number of examples experiments were run   on , allowing a fair estimate of this .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Not applicable . Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   All evaluations occur in a greedy setting where PLMs choose the most probable answer . Since this   makes modal predictions consistent , we can not report such summary statistics . In analyses in Section   5.5 , we report some mean performance measurements , and make it clear how such calculations are   done .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Not applicable . Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.1969