  Xiao AoDanae Sánchez VillegasDaniel Preo¸ tiuc - PietroNikolaos AletrasComputer Science Department , University of Sheffield , UKBloomberg   Abstract   Parody is a figurative device used for mimick-   ing entities for comedic or critical purposes .   Parody is intentionally humorous and often   involves sarcasm . This paper explores jointly   modelling these figurative tropes with the goal   of improving performance of political parody   detection in tweets . To this end , we present a   multi - encoder model that combines three paral-   lel encoders to enrich parody - specific represen-   tations with humor and sarcasm information .   Experiments on a publicly available data set   of political parody tweets demonstrate that our   approach outperforms previous state - of - the - art   methods .   1 Introduction   Parody is a figurative device which imitates enti-   ties such as politicians and celebrities by copying   their particular style or a situation where the en-   tity was involved ( Rose , 1993 ) . It is an intrinsic   part of social media as a relatively new comedic   form ( Vis , 2013 ) . A very popular type of parody   is political parody , which is used to express politi-   cal opposition and civic engagement ( Davis et al . ,   2018 ) .   One of the hallmarks of parody expression is the   deployment of other figurative devices , such as hu-   mor and sarcasm , as emphasized on studies of par-   ody in linguistics ( Haiman et al . , 1998 ; Highfield ,   2016 ) . For example , in Table 1 the text expresses   sarcasm about Myspacebeing a ‘ winning tech-   nology ’ , while mocking the fact that three more   popular social media sites were unavailable . This   example also highlights the similarities between   parody and real tweets , which may pose issues to   misinformation classification systems ( Mu and Ale-   tras , 2020 ) .   Table 1 : Example of a parody tweetby the Twitter han-   dle @Queen_UK . Humor and sarcasm are expressed   simultaneously .   These figurative devices have so far been stud-   ied in isolation to parody . Previous work on mod-   eling humor in computational linguistics has fo-   cused on identifying jokes , i.e. , short comedic   passages that end with a hilarious line ( Hetzron ,   1991 ) , based on linguistic features ( Taylor and Ma-   zlack , 2004 ; Purandare and Litman , 2006 ; Kid-   don and Brun , 2011 ) and deep learning techniques   ( Chen and Soo , 2018 ; Weller and Seppi , 2019 ; An-   namoradnejad and Zoghi , 2020 ) . Similarly , compu-   tational approaches for modeling sarcasm ( i.e. , a   form of verbal irony used to mock or convey con-   tent ) in texts have been explored ( Davidov et al . ,   2010 ; González - Ibáñez et al . , 2011 ; Liebrecht et al . ,   2013 ; Rajadesingan et al . , 2015 ; Ghosh et al . , 2020 ,   2021 ) , including multi - modal utterances , i.e. texts ,   images , and videos ( Cai et al . , 2019 ; Castro et al . ,   2019 ; Oprea and Magdy , 2020 ) . Recently , parody   has been studied with natural language processing   ( NLP ) methods by Maronikolakis et al . ( 2020 ) who   introduced a data set of political parody accounts .   Their method for automatic recognition of posts   shared by political parody accounts on Twitter is   solely based on vanilla transformer models .   In this paper , we hypothesize that humor and   sarcasm information could guide parody specific   text encoders towards detecting nuances of figu-1800   rative language . For this purpose , we propose a   multi - encoder model ( § 2 ) consisting of three paral-   lel encoders that are subsequently fused for parody   classification . The first encoder learns parody spe-   cific information subsequently enhanced using the   representations learned by a humor and sarcasm   encoder respectively .   Our contributions are : ( 1 ) new state - of - the - art   results on political parody detection in Twitter , con-   sistently improving predictive performance over   previous work by Maronikolakis et al . ( 2020 ) ; and   ( 2 ) insights on the limitations of neural models   in capturing various linguistic characteristics of   parody from extensive qualitative and quantitative   analyses .   2 Multi - Encoder Model for Political   Parody Prediction   Maronikolakis et al . ( 2020 ) define political parody   prediction as a binary classification task where a   social media post T , consisting of a sequence of to-   kensT={t , ... , t } , is classified as real or parody .   Real posts have been authored by actual politicians   ( e.g. , realDonaldTrump ) while parody posts   come from their corresponding parody accounts   ( e.g. , realDonaldTrFan ) .   Parody tends to express complex tangled seman-   tics of both humor and sarcasm simultaneously   ( Haiman et al . , 1998 ; Highfield , 2016 ) . To better   exploit this characteristic of parody , we propose a   multi - encoder model that consists of three paral-   lel encoders , a feature - fusion layer and a parody   classification layer depicted in Fig.1 ..   2.1 Text Encoders   Parody As a task - specific parody encoder , we   use the vanilla pretrained BERTweet ( Nguyen et al . ,   2020 ) , a BERT ( Devlin et al . , 2019 ) based model   pre - trained on a corpus of English Tweets and fine-   tuned on the parody data set ( § 3.1 ) .   Humor To capture humor specific characteris-   tics in social media text , we use the data set in-   troduced by Annamoradnejad and Zoghi ( 2020 )   which contains humorous and non - humorous short   texts collected from Reddit and Huffington Post .   First , we adapt BERTweet using domain - adaptive   pre - training ( Sun et al . , 2020a ; Gururangan et al . ,   2020 ) on 10,000 randomly selected humor - only   short texts with masked language modeling . Sub-   sequently , we use a continual learning strategy ( Li   and Hoiem , 2018 ; Sun et al . , 2020b ) to gradu-   ally learn humor - specific properties by further fine-   tuning BERTweet on a humor classification task   ( i.e. , predicting whether a text is humorous or not )   by using 40,000 randomly selected humorous and   non - humorous short texts from the humor corpus   described above ( see Figure 2 ) .   Sarcasm Similar to humor , we extract sarcasm-   related semantic information from a post Tby us-   ing sarcasm annotated data sets from Oprea and   Magdy ( 2020 ) and Rajadesingan et al . ( 2015 ) . The   first data set consists of 777 and 3,707 sarcasm and   non - sarcasm posts from Twitter and the second data   set consists of 9,104 sarcasm and more than 90,000   non - sarcasm posts from Twitter . We first perform   domain - adaptive pre - training of BERTweet on all   sarcastic posts with masked language modeling .   Then , we fine - tune the model on a sarcasm clas-   sification task , similar to the humor encoder ( see   Figure 3 ) . For the fine - tuning step , we use the 9,881   sarcastic tweets and 10,000 randomly sampled non-1801sarcasm tweets from the two data sets ( i.e. , 3,707   from the first and 6,293 from the second ) .   We compute parody f , humor f , and sarcasm   frepresentations by extracting the ‘ classification ’   [ CLS ] token from each encoder respectively , where   f∈R.   2.2 Combining Encoders   We explore three approaches to combine f , f ,   andfrepresentations .   Concatenation First , the three text representa-   tions are simply concatenated to form a combined   representation f∈R.   Self - Attention We also use a 4 - head self-   attentionmechanism ( Vaswani et al . , 2017 ) on   f , f , f. The goal is to find correlations between   representations and learn the contribution of each   encoder in the final representation .   Max - Pooling Finally , we perform a max - pooling   operation on each dimension of f , f , fto obtain   a representation f∈R. The aim is to use the   most dominant features learned by each encoder .   2.3 Classification   Finally , we pass the combined representation f   to a classification layer with a sigmoid activation   function for predicting whether a post is a parody or   not . Three encoders are fine - tuned simultaneously   on the parody data set ( § 3.1 ) .   3 Experimental Setup   3.1 Data   We use the data set introduced by Maronikolakis   et al . ( 2020 ) which contains 131,666 tweets writ-   ten in English , with 65,956 tweets from political   parody accounts and 65,710 tweets posted by real   politician accounts . The data set is publicly avail-   ableand allows us to compare our results to state-   of - the - art parody detection methods .   We use the three data splits provided : ( i ) Person   Split , each split ( train , dev , test ) contains tweets   from different real – parody account pairs ; ( ii ) Gen-   der Split , two different splits based on the genderof the politicians ( i.e. , female accounts in train / dev   and male in test , and male accounts in train / dev   and female in test ) ; Location Split , data is split ac-   cording to the location of the politicians in three   groups ( US , UK , Rest of the World or RoW ) . Each   group is assigned to the test set and the other two   groups to the train and dev sets .   3.2 Baselines   We compare our multi - encoder models with trans-   formers for parody detection ( Maronikolakis et al . ,   2020 ): BERT ( Devlin et al . , 2019 ) and RoBERTa   ( Liu et al . , 2019 ) . Also , we compare our models to   BERTweet ( Nguyen et al . , 2020 ) .   3.3 Implementation details   Humor Encoder For adaptive pre - training , the   batch - size is set to 16and the number of training   epochs is set to 3with a learning rate of 2e . For   humor classification , we use batch size of 128and   the number of epochs is set to 2with a learning   rate of 3e .   Sarcasm Encoder We pretrain using a batch - size   of16over 5epochs with a learning rate of 2e .   For fine - tuning on a sarcasm classification task ,   we use the 9,881sarcasm tweets and 10,000ran-   domly sampled non - sarcasm tweets from the two   data sets ( i.e. , 3,707from the first and 6,293from   the second ) using the same hyperparameters to the   humor - specific encoder .   Multi - encoder For the complete multi - encoder   model , we use a batch size of 128and the learning   rate is set to 2e . The entire model is fine - tuned   for2epochs .   3.4 Evaluation   We evaluate the performance of all models using   F1 score as Maronikolakis et al . ( 2020 ) . Results are   obtained over 3runs using different random seeds   reporting average and standard deviation .   4 Results   4.1 Predictive Performance   Table 2 shows the results for parody detection on   thePerson Split . We observe that BERTweet has the   best performance ( F1 : 90.72 ) among transformer-   based models ( BERT , RoBERTa , BERTweet ) , out-   performing previous state - of - the - art by Maroniko-   lakis et al . ( 2020 ) . This is due to the fact that   BERTweet has been specifically pre - trained on1802   Twitter text . Similar behavior is observed on the   Gender andLocation splits ( see Table 3 and 4 re-   spectively ) .   Our proposed multi - encoder achieves the best   performance when using Self - Attention to com-   bine the three parallel encoders ( F1 : 91.19;89.97 ,   88.56;88.37,87.91,87.16 ; for Person , Gender ,   andLocation splits respectively ) . Moreover , it out-   performs the best single - encoder model BERTweet   in the majority of cases which corroborates that   parody detection benefits from combining general   contextual representations with humor and sarcasm   specific information , as humor and sarcasm are im-   portant characteristics of parody ( Haiman et al . ,   1998 ; Highfield , 2016 ) . On the other hand , sim-   ply concatenating the three parallel encoders de-   grades the performance across different splits ( Per-   son:88.99;Gender : 86.84,84.21Location : 85.41 ,   84.74,83.62 ) . This happens because the concatena-   tion operation treats the three encoders as equally   important . While humor and sarcasm are related   to parody , they may not necessarily have the same   relevance as indicators of parody .   Our best performing model ( Self - Attention ) out-   performs the vanilla BERTweet by3F1 points   when trained on female accounts and by almost   2F1 points when trained on male accounts . We   speculate that the additional linguistic information   from the two encoders ( i.e. , sarcasm and humor )   is more beneficial in low data settings . The num-   ber of female politicians is considerably smaller   than males in the data set ( see Maronikolakis et al .   ( 2020 ) for more details ) .   4.2 Ablation Study   We also examine the effect of combining parody-   specific representations with humor and sarcasm   information by running an ablation study . We com-   pare performance of four models : using parody rep-   resentations only ( P ) , and combining parody rep-   resentations with humor ( P+H ) , or sarcasm ( P+S )   information , as well as with both ( P+S+H ) . The re-   sults of this analysis are depicted in Tables 5 , 6 and   7 . We observe that both sarcasm and humor con-   tribute to the performance gain , but using both is   more beneficial . Modelling sarcasm leads to more   gains than humor and this could be attributed to the   characteristics of the parody corpus , namely that it   focuses primarily on the political domain , which   have a high sarcastic component ( Anderson and   Huntington , 2017 ) .   5 Error Analysis   Finally , we perform an error analysis to examine   the behavior and limitations of our best - performing   model ( multi - encoder with Self - Attention ) .   The next two examples correspond to real tweets   that were misclassified as parody :   ( 1)Congratulations , < mention > ! < url > .   ( 2)It ’s a shame that Boris is n’t here answering   questions from the public this evening .   We speculate that the model misclassified these   tweets as parody because they contain terms that1803   are related to sarcastic short texts such as user men-   tions , punctuation marks ( ! ) , and negation ( is n’t )   ( González - Ibáñez et al . , 2011 ; Highfield , 2016 ) .   The following two examples correspond to par-   ody tweets that were misclassified as real :   ( 3)Hey America , it ’s time to use your safe word .   ( 4)I fully support the Digital Singles Market .   Example ( 3 ) is a call - to - action message , while   Example ( 4 ) is a statement expressing support for a   particular subject . These statements are written in a   style that is similar to political slogans or campaign   speeches ( Fowler et al . , 2021 ) that the model fails   to recognise . As a result , in addition to humor and   sarcasm semantics , the model might be improved   by integrating knowledge from the political domain   such as from political speeches .   6 Conclusion   In this paper , we studied the impact of jointly mod-   elling figurative devices to improve predictive per-   formance of political parody detection in tweets .   Our motivation was based on studies in linguis-   tics which emphasize the humorous and sarcastic   components of parody ( Haiman et al . , 1998 ; High-   field , 2016 ) . We presented a method that combines   parallel encoders to capture parody , humor , and sar-   casm specific representations from input sequences ,   which outperforms previous state - of - the - art pro-   posed by Maronikolakis et al . ( 2020 ) .   In the future , we plan to combine information   from other modalities ( e.g. , images ) for improving   parody detection ( Sánchez Villegas and Aletras ,   2021 ; Sánchez Villegas et al . , 2021 ) .   Acknowledgements   We would like to thank all reviewers for their   valuable feedback . DSV is supported by the Cen-   tre for Doctoral Training in Speech and Lan-   guage Technologies ( SLT ) and their Applications   funded by the UK Research and Innovation grant   EP / S023062/1 .   References180418051806A Multitask - Learning   We also tested applying multi - task learning ap-   proaches ( Caruana , 1993 ) to use either sarcasm   prediction ( P+S ) , humor prediction ( P+H ) or both   ( P+S+H ) as auxiliary tasks for parody detection .   We utilize BERTweet as the share encoder and inde-   pendent classification layers for parody and humor   or sarcasm . Three sets of weights are applied to   losses from each independent classification layer   and the three layers are stacked . The best results   are chosen and depicted in Table 8 , Table 9 and   Table 10.1807