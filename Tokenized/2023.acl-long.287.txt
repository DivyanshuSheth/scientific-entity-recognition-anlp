  Yiwei Wei , Shaozu Yuan , Ruosong Yang , Lei Shen   Longbiao Wang , Zhangmeizhi Li , Meng ChenTianjin Key Laboratory of Cognitive Computing and Application ,   College of Intelligence and Computing , Tianjin University , Tianjin , ChinaJD AI Research , Beijing , ChinaHuiyan Technology ( Tianjin ) Co. , Ltd , Tianjin , ChinaChina University of Petroleum(Beijing ) at Karamay , Karamay , ChinaThe Hong Kong Polytechnic University , Hongkong , China   Abstract   With the popularity of social media , detecting   sentiment from multimodal posts ( e.g. image-   text pairs ) has attracted substantial attention   recently . Existing works mainly focus on fus-   ing different features but ignore the challenge   ofmodality heterogeneity . Specifically , dif-   ferent modalities with inherent disparities may   bring three problems : 1 ) introducing redundant   visual features during feature fusion ; 2 ) caus-   ing feature shift in the representation space ; 3 )   leading to inconsistent annotations for differ-   ent modal data . All these issues will increase   the difficulty in understanding the sentiment of   the multimodal content . In this paper , we pro-   pose a novel Multi - View Calibration Network   ( MVCN ) to alleviate the above issues system-   atically . We first propose a text - guided fusion   module with novel Sparse - Attention to reduce   the negative impacts of redundant visual ele-   ments . We then devise a sentiment - based con-   gruity constraint task to calibrate the feature   shift in the representation space . Finally , we   introduce an adaptive loss calibration strategy   to tackle inconsistent annotated labels . Exten-   sive experiments demonstrate the competitive-   ness of MVCN against previous approaches   and achieve state - of - the - art results on two pub-   lic benchmark datasets .   1 Introduction   Multimodal sentiment detection(Xu , 2017 ; Xu and   Mao , 2017 ) aims to explore the sentiment embed-   ded in the multimodal contents , such as text , im-   ages , and videos . With the growth of social media ,   it shows broad applications for the understanding   of one ’s position , attitude , or opinion towards an   entity , person , or topic , which has attracted sub-   stantial attention from both academic and industrialFigure 1 : Examples of multimodal sentiment detection .   communities . In this paper , we focus on detecting   the sentiment of multimodal posts in social media .   As shown in Figure 1 , the model is required to in-   fer the human sentiments given the image and text   pairs .   Previous works mainly focus on how to integrate   modalities and have achieved astonishing progress .   Earlier works apply concatenation to fuse different   features , such as Xu ( 2017 ) ; Xu and Mao ( 2017 ) .   To promote the fusion , Yang et al . ( 2020 ) ; Yu and   Jiang ( 2019 ) ; Kumar and Vepa ( 2020 ) ; Xu et al .   ( 2018 ) construct different modules to realize deeper   multimodal interaction . However , these methods   of viewing different modalities in the same light   neglect the modality heterogeneity , hindering the   performance of the model . Although CLMLF ( Li   et al . , 2022 ) tries to apply contrastive learning to   alleviate this problem , it is in coarse granularity and   only considers feature level , which is insufficient   to address the issue of feature shift .   Modality heterogeneity is mainly caused by the   modality gap , which has been discussed in some   related multimodal research ( Hazarika et al . , 2020 ;   Lin and Hu , 2022 ; Liang et al . , 2022 ; Radford et al . ,   2021 ) . In multimodal sentiment detection , it is   because image modality often has less informa-   tion and more redundant features compared to text   modality . As shown in Figure 1 , only a few visual5240elements reveal sentiment information(e.g . , people   smile , ill dog ) , causing redundant visual elements   and low information density for the sentiment . In   contrast , text modality is more indicative of sen-   timent owing to higher information density ( Tsai   et al . , 2019 ; Sun et al . , 2021 ) . Consequently , this   heterogeneity across different modalities will bring   difficulty in understanding the sentiment of multi-   modal content .   Concretely , the ignorance of modality hetero-   geneity will directly cause three problems : a ) It   takes sparse and noisy visual features into fusion   and causes confusion in understanding sentimental   information . b ) Integrating modalities of differ-   ent properties will further lead to the multimodal   feature shift and makes the model capture spuri-   ous correlations between multimodal features and   sentiments . c ) As mentioned by Niu et al . ( 2016 ) ;   Yang et al . ( 2021 ) , it will affect the data annota-   tors to vote inconsistent labels for unimodal , which   leads to uncertain annotated labels and weaken the   label confidence . Taking MVSA - Single ( Niu et al . ,   2016 ) for example , approximately 47 % of the sam-   ples suffer from inconsistent annotated labels .   To tackle the above problems systematically ,   we propose a Multi - View Calibration Network   ( MVCN ) from three different views :   ( 1 ) To avoid sparse and redundant visual features   of direct integrating modalities ( Li et al . , 2022 ) , we   propose a Text - Guided Fusion ( TGF ) module to   leverage text data to dominate the fusion process .   Specially , we propose Sparse - Attention mechanism   using sparsemax ( Martins and Astudillo , 2016 ) to   automatically eliminate redundant visual features   and capture the essential parts of the image with   respect to the sentiment .   ( 2 ) To further calibrate the feature shift , we   propose a Sentiment - based Congruity Constraint   ( SCC ) task to restrain representation space . In the   SCC task , we propose relative distance to gather   multimodal features around the corresponding sen-   timental centroids estimated with samples ’ labels .   In addition , to overcome the limitation of the mini-   batch , we also introduce Accumulating Calibra-   tion ( AC ) strategy to accumulate sampling infor-   mation , thus computing the sentiment centroids   from a global perspective . Compared to contrastive   learning ( Li et al . , 2022 ) , SCC has more strength to   calibrate the feature shift for it brings sentimental   semantic labels from a global perspective .   ( 3 ) To alleviate uncertain annotated labels mis - leading the model during the training stage , we   also introduce an adaptive loss calibration ( ALC )   strategy to calibrate the training loss in the senti-   ment detection task , where the detection model is   forced to be less confident for uncertain annotated   labels . The experiments conducted on different   benchmark datasets ( Niu et al . , 2016 ; Cai et al . ,   2019 ) show that MVCN has significantly improved   the performance on all metrics compared with pre-   vious state - of - the - art models . In summary , the key   contributions of this paper are as follows :   •We introduce a novel Multi - View Calibration   Network ( MVCN ) including Text - Guided Fu-   sion module , Sentiment - based Congruity Con-   straint , and Adaptive Loss Calibration strategy   for multimodal sentiment detection to system-   atically solve the problems of modality het-   erogeneity from different views .   •The thorough experiments show that MVCN   improves the performance over all metrics and   achieves state - of - the - art on two benchmark   datasets ( Niu et al . , 2016 ; Cai et al . , 2019 ) .   2 Methodology   The architecture of the proposed multi - view cal-   ibration network ( MVCN ) is shown in Figure 2 .   Generally , MVCN consists of text - guided fusion   module and two paralleled sub - tasks . The two par-   alleled sub - tasks are sentiment classification and   sentiment - based congruity constraint respectively .   2.1 Text - Guided Fusion Module   As shown in Figure 2 , Text - Guided Fusion Module   contains three components : Unimodal Encoders ,   Text - Guided Unit , and Reduction Unit .   Unimodal Encoders . To obtain visual and tex-   tual features for multimodal fusion , we apply two   different unimodal encoders to extract their repre-   sentations . For text modality , we use the pretrained   BERT ( Jacob Devlin , 2019 ) model as the text en-   coder to obtain the text representation . Given a   sequence of text T={t , t , ... , t } , where nis   the number of text length , the output of the BERT   model can be defined as :   X={E , E , E , ... , E}=BERT ( T;θ )   ( 1 )   where E∈Ris the embedding of the CLS   token and θdenotes the parameters of the BERT   model . For image modality , we use the pretrained5241   ViT model ( Dosovitskiy et al . , 2020 ) as the image   encoder to obtain the image representation . Given   an image I , the output of the ViT model can be   defined as :   X={I , I , I , ... , I}=V iT(I;θ)(2 )   where nis the length of the image representation   andθdenotes the parameters of the ViT model .   Text - Guided Unit ( TGU ) . Taking the extracted   image features Xand the text features Xas in-   puts , we perform multimodal fusion by passing   the inputs through Nlayers stacking Text - Guided   Unit ( TGU ) . For each layer of TGU , Xis first fed   into a self - attention ( α ) as query , key , and value   to generate text - aware feature X. We then in-   putXandXto a Sparse - Attention ( γ ) layer   to obtain text - guided visual sparse features X ,   and finally apply a feed - forward network ( FFN )   forXto produce the TGU output . Here , we   design Sparse - Attention in two places : ( 1 ) We   use the textual features to attend to visual fea-   tures and get text - guided features . ( 2 ) We nor-   malize attention weight with sparsemax ( Martins   and Astudillo , 2016 ) , thus obtaining sparse pos-   terior attention weights , where the weight for re-   dundant visual features are set to 0 . Moreover , in-   spired by previous works ( Rahman et al . , 2019 ;   Yuan et al . , 2022 ) that leverage pretrained lan-   guage model to enhance the ability of capturing   multimodal context , we initialize TGU with the   weight of the pretrained BERT . In the formula ,   the output of Nlayer TGU can be defined as :   TGU ( X , X)= [ X = α(X , X , X ) , X=   γ(X , X , X ) , FFN ( X)].Reduction Unit ( RU ) . To get the multimodal   representation Q∈Rfor sentiment classifica-   tion , we follow Li et al . ( 2022 ) and use stacked at-   tention layer and fully - connected layer with GELU   activate function ( Hendrycks and Gimpel , 2016 ) as   Reduction Unit to perform dimensionality reduc-   tion for the features .   2.2 Sentiment Congruity Constraint ( SCC )   Intuitively , samples ’ features Qwith the same senti-   mental labels should be closer in the representation   space . However , feature shift caused by modality   heterogeneity hinders this trend and makes it dif-   ficult to capture correlations between multimodal   features and sentiments . To address this problem ,   we take advantage of label information to estimate   sentimental centroids , then restrict the distance be-   tween the samples and their centroids , thus calibrat-   ing the feature shift .   Here , we first employ a fully - connected layer to   map multimodal representation Qto normalized   representations ´ Qfor the SCC task . Then , we com-   pute the positive centroid ( C ) , the neutral centroid   ( C)and the negative centroid ( C)of the repre-   sentation ´ Qduring training stage . Let 0,1,2 denote   the positive label , neutral label , and negative label ,   the formula for computing centroids is defined as :   C=/summationtextI(Y(j ) = ( 0|1|2))·´Q / summationtextI(Y(j ) = ( 0|1|2))(3 )   where Bis the number of all the candidate comput-   ing samples , I(.)is an indicator function , and ´ Q   andY(j)are the representation and Ground - Truth   label respectively of the j - th sample.5242To restrict the distance between the samples and   their corresponding centroids , an intuitive idea is to   minimize absolute distance , such as L2 loss . How-   ever , we find it causes all the samples to get too   close to their centroids and completely eliminates   data distribution , which makes the training hard to   converge . To tackle this issue , we propose a relative   L2 loss to measure the distance . Here , we first cal-   culate the absolute L2 distance D={d , ... , d }   between the centroid Cand the multimodal repre-   sentation ´ Qfor a batch :   D={∥´Q−C∥}√ι(4 )   where Y(i)represents the ground - truth label of the   isample , ιis the feature dimension , and√ιserve   as a scale factor . We then normalize the absolute   distance to a relative distance and optimize the SCC   task with the following formula :   L=−/summationdisplaylog(exp(−d)/summationtextexp(−d ) ) ( 5 )   Accumulating calibration strategy . Neverthe-   less , there still exist two problems to optimize the   SCC task because of the limitation of the mini-   batch . On the one hand , computing centroid for   iterating mini - batch results in a frequently updated   centroid . On the other hand , the samples in a mini-   batch are insufficient to estimate an accurate cen-   troid . To solve this problem , we propose an accu-   mulating calibration strategy to enlarge the comput-   ing space and narrow the change for the samples .   We first employ an auxiliary accumulating TGF   module ( denoted as TGF ’ ) to produce sufficient rep-   resentations Qas candidate computing samples   for centroids in advance . To accumulate comput-   ing information , we then build a queue to restore   all the representations . The queue is dynamically   updated by replacing the premier mini - batch with   the current mini - batch during training for each it-   eration . Thus , we can estimate stable and slow-   updated centroids with Equation 3 from a more   global perspective . Here , to guarantee training sta-   bility , we leverage the momentum optimization ( He   et al . , 2020 ; Li et al . , 2021 ) to slowly update TGF ’ ,   which can be defined as :   θ←βθ+ ( 1−β)θ ( 6 )   where θdenotes the parameters of the accumu-   lating TGF ’ , θdenotes the parameters of the TGF   module and β∈[0,1)is a balance parameter.2.3 Sentiment Classification ( SC )   For the sentiment classification task , we feed mul-   timodal representation Q∈Rinto the fully con-   nected layer with softmax function to predict the   logits . However , directly optimizing this task with   Cross Entropy ( CE ) as previous work still suffers   from the issue of uncertain annotated labels during   the training stage .   Adaptive loss calibration ( ALC ) strategy . To   overcome the above issue , we design a simple   but effective strategy , adaptive loss calibration . It   forces the detection model to decrease confidence   for the training examples of inconsistent annotated   labels , thus calibrating the loss . To better elaborate   the strategy , we first define cross entropy loss uti-   lized in most previous works . Suppose that p∈   { p , p , ... , p}denotes the predicted logit and   y∈ { y , y , ... , y}represents the ground - truth ,   CE loss can be defined as L=−/summationtextylog(p ) ,   where y∈ { 0,1},p∈[0,1]andKdenotes   the number of categories . Different from label   smoothing ( Szegedy et al . , 2016 ) , we first leverage   unimodal labels provided by the datasets to adap-   tively adjust the confidence factor α∈ { 0,0.1 } ,   then normalize the ground - truth probability . Intu-   itively , it makes the model becomes confident about   its ground - truth by setting αas 0 . In the formula ,   we normalize the ground - truth probability ˆyfor   each label as :   ˆy= ( 1−α)·I(y= 1 ) + ( α   K−1)·I(y= 0 )   ( 7 )   where I(.)is a indicator function . Moreover , the   loss function Lcan be reformatted as :   L=/summationdisplay   ( ( 1−α)∗L·I(y= 1 )   + α·L·I(y= 0))(8 )   2.4 Training Loss   We optimize the above two tasks with total loss L :   L = λL+λL ( 9 )   where λandλare hyper - parameters to balance   the different training losses .   3 Experiments   In this section , we first introduce the experimental   setup and report the experimental results , then con-   duct the ablation study and visualization analysis.5243   3.1 Datasets   All our experiments were conducted on three public   datasets : MVSA - Single ( Niu et al . , 2016 ) , MVSA-   Multiple ( Niu et al . , 2016 ) and HFM ( Cai et al . ,   2019 ) . Here , we give a brief introduction to these   datasets and dataset statistics are shown in Table 2 .   MVSA - Single , MVSA - Multiple . MVSA-   Single and MVSA - Multiple are popular text - image   sentiment datasets crawled from Twitter , where   MVSA - Multiple is an upgraded version of MVSA-   Single and contains more text - image pairs . Both   of them have three categories : positive , neutral ,   and negative . For fair comparison , the two MVSA   datasets are processed in the same way as Xu and   Mao ( 2017 ) .   HFM . HFM has two sentimental categories : pos-   itive and negative . We follow Cai et al . ( 2019 ) and   adopt the same preprocessing method for exper-   iments , which has been widely used in previous   works .   3.2 Implementation Details   For feature extraction , we use 12 - layer visual trans-   former ViT - B/16 ( Dosovitskiy et al . , 2020 ) as vi-   sual encoder and BERT - base ( Jacob Devlin , 2019 )   as textual encoder . The text - guided fusion encoder   is composed of 6 stacked text - guided units . In   ALC , the confidence factor is set as 0 or 0.1 accord-   ing to unimodal labels . For Sentiment Congruity   Constraint , the queue sizes of accumulating calibra-   tion are set as 3611 , 13624 , and 19816 for MVSA-   Single , MVSA - Multiple , and HFM respectively .   During the training stage , the learning rate is   set to 2e-5 . We train the model for 10 epochswith batch size of 16 , 32 , and 32 for MVSA-   Single , MVSA - Multiple , and HFM . In addition ,   we adopt AdamW optimizer with ϵof1e−8and   βof(0.9,0.999 ) . For loss function , both hyper-   parameters λandλare set to 1.0 . Following   previous settings , we adopt ACC and Weighted   F1 as the evaluation metrics for MVSA datasets   and Macro - F1 and ACC for HFM to evaluate the   performance of the model .   3.3 Baselines   To fully validate the performance of MVCN , we   select both unimodal and multimodal baselines .   Unimodal Baselines . For text modality , we   choose CNN ( Kim , 2014 ) , Bi - LSTM ( Zhou et al . ,   2016 ) and BERT ( Jacob Devlin , 2019 ) as baselines   since they are popular models for text classification .   For image modality , ResNet ( He et al . , 2016 ) and   ViT(Dosovitskiy et al . , 2020 ) are selected for their   superior capability for image classification .   Multimodal Baselines . For MVSA- ∗datasets ,   the compared baselines include : MultiSentiNet   ( Xu and Mao , 2017 ) that designs an attention-   based semantic network for multimodal senti-   ment analysis ; HSAN ( Xu , 2017 ) applying a hi-   erarchical semantic attentional network for mul-   timodal sentiment analysis ; Co - MN - Hop6 ( Xu   et al . , 2018 ) , employing a co - memory network to   iteratively model the interactions between multi-   ple modalities ; MGNNS ( Yang et al . , 2021 ) uti-   lizing a multi - channel graph neural networks with   sentiment - awareness for image - text sentiment de-   tection . CLMLF ( Li et al . , 2022 ) is the previous   SOTA model that aligns and fuses the text and im-   age modalities through contrastive learning . For   HFM datasets , we compare two variants of Concat   ( Schifanella et al . , 2016 ): Concat(2 ) concatenates   text and image , while Concat(3 ) brings extra image   attribute features ; MMSD ( Cai et al . , 2019 ) fusing   text , image , and image attributes with a multimodal   hierarchical framework ; and D&RNet(Xu et al . ,   2020 ) that fuses text , image , and visual attributes   by the decomposition and relation network .   3.4 Main Results   The comparison between MVCN and the baselines   is demonstrated in Table 1 . Obviously , the text-   only model is more competitive with image - only   methods . It indicates that the image modality is   less helpful and contains more redundant features   compared to text modality , which supports our in-   tuition of eliminating redundant visual features . In5244Modality ModelMVSA - Single MVSA - MultipleModelHFM   Acc F1 Acc F1 Acc F1   TextCNN 0.6819 0.5590 0.6564 0.5766 CNN 0.8003 0.7532   BiLSTM 0.7012 0.6506 0.6790 0.6790 BiLSTM 0.8190 0.7753   BERT 0.7111 0.6970 0.6759 0.6624 BERT 0.8389 0.8326   ImageResNet-50 0.6467 0.6155 0.6188 0.6098 ResNet-50 0.7277 0.7138   ViT 0.6378 0.6226 0.6194 0.6119 ViT 0.7309 0.7152   MultimodalMultiSentiNet 0.6984 0.6984 0.6886 0.6811 Concat(2 ) 0.8103 0.7799   HSAN 0.6988 0.6690 0.6796 0.6776 Concat(3 ) 0.8174 0.7874   Co - MN - Hop6 0.7051 0.7001 0.6892 0.6883 MMSD 0.8344 0.8018   MGNNS 0.7377 0.7270 0.7249 0.6934 D&R Net 0.8402 0.8060   CLMLF 0.7533 0.7346 0.7200 0.6983 CLMLF 0.8543 0.8487   CLMLF0.7378 0.7291 0.7112 0.6863 CLMLF0.8489 0.8446   MVCN 0.7606 0.7455 0.7207 0.7001 MVCN 0.8568 0.8523   Dataset Label Train Val Test   MSV A - SPositive 2147 268 268   Neutral 376 47 47   Negative 1088 135 135   MSV A - MPositive 9056 1131 1131   Neutral 3528 440 440   Negative 1040 129 129   HFMPositive 8642 959 959   Negative 11174 1451 1450   addition , the multimodal models surpass the uni-   modal models because of fusing more information .   Overall , MVCN achieves state - of - the - art with a   considerable performance gain over other methods ,   which indicates the necessity of tackling modality   heterogeneity from different views . Specially , we   find that MVCN achieves better results on MSV A-   Single compared to the other two datasets . We   conjecture that small dataset suffers more of the   modality heterogeneity problem due to lack of data   diversity .   3.5 Ablation Study   To investigate the effectiveness of each module , we   conduct an ablation study in Table 3 . Firstly , com-   pared to the MFS model that equally fuses the im-   age and text features , it is straightforward that TGF   module can aid sentiment detection since it elim-   inates redundant visual features . In addition , the   model equipped with Sentiment - based Congruity   Constraint ( SCC ) brings a significant improvement , implying the importance of calibrating feature shift   with congruity constraint . And accumulating cali-   bration ( AC ) strategy by additionally augmenting   SCC with more accurate and stable centroids , con-   sistently improves performance . Furthermore , it   can be observed that ALC strategy can further boost   performance , demonstrating ALC is an effective   way to reduce the impact of the uncertain annotated   labels . Finally , MVCN equipped with all novel   modules achieves the best performance , illustrating   the effectiveness of all the above modules .   4 Analysis   4.1 Discussion of Components   Variants of Text - Guided Unit . To verify the de-   sign for Text - Guided Unit in TGF , we evaluate the   performance of TGU variants in Table 4 . Here ,   “ w/ softmax ” denotes Self - Attention , while “ w/   spm ” represents we normalize attention weight   with sparsemax . “ w/ pretrain ” and “ w/o pretrain ”   indicate whether the guided unit initialized with   the pretrained BERT weight or not . From the table ,   we observe “ w/ spm ” shows superiority compared   to “ w/ softmax ” , indicating Sparsemax - Attention   boosts the performance of the model by eliminat-   ing noisy visual features . Additionally , “ w/ pre-   train ” can further promote the performance , which   is consistent with previous works that pretrained   language models can enhance the capacity for cap-   turing multimodal context .   Effectiveness of SCC . In Figure 3 , we plot the   curves during training to explore why we apply   relative L2 distance to optimize SCC . From Figure   3 ( a ) , “ w/ SCC(aL2 ) ” that applies absolute L2 dis-5245ModelMVSA - Single MVSA - Multiple HFM   Acc F1 Acc F1 Acc F1   BERT 0.7111 0.6970 0.6759 0.6624 0.8389 0.8326   ViT 0.6378 0.6226 0.6194 0.6119 0.7309 0.7152   MFS 0.7217 0.7205 0.7063 0.6851 0.8434 0.8375   TGF 0.7396 0.7355 0.7095 0.6887 0.8493 0.8451   TGF , SCC 0.7515 0.7403 0.7158 0.6929 0.8522 0.8499   TGF , SCC+AC 0.7563 0.7422 0.7188 0.6971 0.8568 0.8523   TGF , SCC+AC , ALC 0.7606 0.7455 0.7207 0.7001 - -   TGFMVSA - Single HFM   Acc F1 Acc F1   w/ softmax 0.7365 0.7339 0.8476 0.8427   w/ spm 0.7396 0.7355 0.8493 0.8451   w/o pretrain 0.7232 0.7148 0.8306 0.8217   w/ pretrain 0.7396 0.7355 0.8493 0.8451   tance dramatically narrows the distance between   the samples and the centroids to an extremely low   level . However , it hinders the training and eventu-   ally leads to the model being hard to optimize in   Figure 3 ( b ) . Because it will completely eliminate   data distribution and make the data samples lose   correlation with each other by directly minimizing   L2 distance . Conversely , “ w/ SCC(aL2 ) ” that uti-   lizes relative L2 distance can reduce the semantic   distance to a reasonable extent , and more impor-   tantly , maintain the data distribution . Therefore , it   ensures the SCC task can be optimized and the ac-   curacy of “ w/ SCC(rL2 ) ” in Figure 3 ( b ) gradually   increases during training .   Different Loss Calibration Strategies . To explore   the effectiveness of ALC , we compare different   loss calibration strategies in Table 5 . It shows label   smoothing ( LS ) only brings slight improvement   since it avoids overfitting and strengthens the ro-   bustness of the model . However , LS can not handle   the label problem caused by modality heterogeneity .   Compared to the LS , our proposed ALC exhibits   superiority and outperforms LS in performance ,   verifying it is necessary to mitigate the adverse   effects of inconsistent labels . Dataset Strategy ACC F1   MSVD - Single- 0.7563 0.7422   LS 0.7579 0.7436   ALC 0.7606 0.7455   4.2 Visualization   Sparse - Attention Visualization . To verify the ad-   vantage of Sparse - Attention in the TGF module ,   we visualize the attention weight in Figure 4 . Com-   pared to Self - Attention , the sampling cases show   Sparse - Attention captures the essential parts of the   image with respect to the sentiment and meanwhile   attenuates the negative effect of redundant visual   features . As the example in Figure 4(a ) , the model   paid more attention on the “ ill dog ” in the image   for it reflects negative sentiment , certifying that the   model can focus the sentimental regions in the im-   age and avoid the interference of irrelevant objects .   This also confirms it is necessary to eliminate re-   dundant visual features , reinforcing the importance   of Sparse - Attention .   Feature Distribution Visualization . In order to   visually demonstrate the superiority of SCC task   with AC strategy , we visualize the feature distribu-   tion on the Multiple - Single dataset with contrastive   learning ( Li et al . , 2022 ) and SCC . Here , we apply   T - SNEalgorithm to perform dimensionality re-   duction for the feature , obtaining a 2 - dimensional   feature vector distribution visualized in Figure 5 .   From Figure 5(b ) , we observe that the SCC task   forces samples belonging to the same category to5246   gather around their corresponding centroids . Con-   versely , Figure 5(a ) shows that when we remove   SCC task , the degree of data aggregation is less   obvious . The reason for this phenomenon is that ,   compared to contrastive learning , the SCC taking   sentimental labels into consideration to constrain   the distribution from a more global perspective , is   more capable of calibrating the feature shift . Hence ,   this strength facilitates the MVCN to learn the dis-   tinguished features in representation space and im-   prove the performance of the model .   4.3 Case Study   In Figure 6 , we conduct a case study for MVCN   and the previous SOTA model CLMLF . We find   that for these complicated and confusing cases , it is   difficult for CLMLF to capture the user ’s sentiment   of samples because of its limitations in handling   modality heterogeneity . For example , in the first   case , although the phrases ( e.g. , vibrant colors ) in   texts represent specific sentiments , CLMLF was   still misled by sparse visual sentimental informa-   tion . For the following two cases in Figure 6 , we   find the text may express negative sentiments , and   images contain elements ( e.g. , serious man , smil-   ing man ) that reflect sentiments . However , CLMLF   fails to predict the real sentiment due to modality   heterogeneity . In contrast , our model can distin-   guish these cases for the corresponding sentiment ,   which also proves the advantage of tackling modal-   ity heterogeneity from different views .   5 Related Work   In this section , we introduce the background of   multimodal sentiment detection and modality het-   erogeneity .   5.1 Multimodal Sentiment Detection   Multimodal sentiment detection has become a sig-   nificant research topic and previous works mainly   focus on fusing multimodal features with differ-   ent strategies . Some earlier works fuse the multi-   modal features by concatenation , such as HSAN   ( Xu , 2017 ) and MultiSentiNet ( Xu and Mao , 2017 ) .   Later relevant works mainly focus on how to in-   tegrate modalities and promote better inter - modal   interaction . For example , CoMN ( Xu et al . , 2018 )   designs a co - memory network to iteratively model   the interactions between image and text features ,   TomBERT ( Yu and Jiang , 2019 ) fuses the multi-   modal representations with a bilinear interaction   layer , Kumar and Vepa ( 2020 ) proposes to use gat-   ing mechanism and attention to perform deep mul-   timodal interaction , and MV AN ( Yang et al . , 2020 )   adapts memory network to fuse the multimodal fea-   tures via perceptron and stacking - pooling module .   Recent work ( Li et al . , 2022 ) applies contrastive   learning and data augmentation to handle this prob-   lem . Different from these works , we aim to im-5247prove this task by tackling modality heterogeneity   from multiple views .   5.2 Modality Heterogeneity   Modality heterogeneity is reflected in that multiple   modalities show different properties . Typically , het-   erogeneity usually exists in human - generated sig-   nals ( language ) with high information density and   other natural signals modalities ( e.g. , image , video ,   audio ) with heavy redundancy . In the past few   years , some works have been done in multimodal   representation learning to alleviate the impact of   modality heterogeneity . Hazarika et al . ( 2020 )   project different modalities to modality - invariant   representation space and learn their commonalities .   Furthermore , Wu et al . ( 2021 ) ; Zhao et al . ( 2021 )   employs the modality translation method to con-   vert the source modality to the target one in order   to learn the commonalities between the different   modalities . However , due to the huge modality gap ,   it is insufficient to handle multimodal heterogene-   ity by projecting different modal features into the   same representation space . To overcome the modal-   ity gap and encourage learning useful features , Li   et al . ( 2022 ) ; Lin and Hu ( 2022 ) design various   contrastive learning tasks to predict cross - modal   representation in an implicit way . However , they   are feature - level alignment and in coarse granular-   ity , which is not sufficient to address the problem   of feature shift .   6 Conclusion   In this paper , we present a Multi - View Calibra-   tion Network ( MVCN ) to address the modality   heterogeneity problem for text - image sentiment   detection from different views . Specially , we re-   spectively introduce the text - guided fusion mod-   ule to calibrate multimodal fusion and reduce the   negative impacts of redundant visual elements , a   sentiment - based congruity constraint task to further   calibrate the feature shift in representation space   and an adaptive loss calibration strategy to cali-   brate the training loss in terms of uncertain anno-   tated labels . The thorough experiments show the   MVCN achieves state - of - the - art performance on   two benchmark datasets .   Limitations   Considering Modality Heterogeneity can promote   many related multimodal applications , it is worth   continually exploring . In this paper , we proposeText - Guided Fusion ( TGF ) module equipped with   Sparse - Attention to integrate different modalities   in representation aspects , which is an implicit way   to build the relations of fine - grained features , such   as visual objects , and textual words . Previous work   ( Khademi , 2020 ; Wang et al . , 2020 ) has proven that   Graph Convolutional Network ( GCN ) ( Scarselli   et al . , 2008 ) shows advantages in modeling the rela-   tions among visual and textual elements . Inspired   by these works , we argue that explicitly introducing   the relationship of fine - grained features via GCN   can better guide the model to eliminate redundant   features . Thus it can further narrow the modalities   gap and facilitate fusion for multimodal content   understanding . In the future , we will bring GCN to   learn multimodal relationships and boost the per-   formance of the model .   References524852495250ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 6   /squareA2 . Did you discuss any potential risks of your work ?   Section 3   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Scetion 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 3   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The data that was collected / used does n’t contain any information that names or uniquely identiﬁes   individual people or offensive content .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3.1   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.1   C / squareDid you run computational experiments ?   Section 3   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   No response.5251 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3.2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   we do n’t use human annotators .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   we do n’t have any paid participants .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   we do n’t collect data via crowdsourcing .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No ethics review board was involved   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Our data does n’t include any protected information.5252