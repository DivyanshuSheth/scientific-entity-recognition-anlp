  Saneem A. ChemmengathAmar Prakash AzadRonny LussAmit DhurandharMicrosoftIBM Research   Abstract   Contrastive explanations for understanding the   behavior of black box models has gained a   lot of attention recently as they provide po-   tential for recourse . In this paper , we propose   a method Contrastive Attributed explanations   forText ( CAT ) which provides contrastive ex-   planations for natural language text data with   a novel twist as we build and exploit attribute   classiﬁers leading to more semantically mean-   ingful explanations . To ensure that our con-   trastive generated text has the fewest possible   edits with respect to the original text , while   also being ﬂuent and close to a human gener-   ated contrastive , we resort to a minimal pertur-   bation approach regularized using a BERT lan-   guage model and attribute classiﬁers trained   on available attributes . We show through qual-   itative examples and a user study that our   method not only conveys more insight because   of these attributes , but also leads to better qual-   ity ( contrastive ) text . Quantitatively , we show   that our method outperforms other state - of-   the - art methods across four data sets on four   benchmark metrics .   1 Introduction   Explainable AI ( XAI ) has seen an explosion of in-   terest over the last ﬁve years , not just in research   ( Molnar , 2019 ; Arya et al . , 2019 ) , but also in the   real world where governments ( Yannella and Ka-   gan , 2018 ; Gunning , 2017 ) and industry have made   sizeable investments . The primary driver for this   level of interest has been the inculcation of deep   learning technologies ( Goodfellow et al . , 2016 ) ,   which are inherently black box , into decision mak-   ing systems that affect billions of people . Trust   thus has become a central theme in relying on these   black box systems , and one way to achieve it is   seemingly through obtaining explanations .   Although many feature - based ( Ribeiro et al . ,   2016 ; Lundberg and Lee , 2017 ; Simonyan et al . ,2013 ) and exemplar - based methods ( Gurumoor-   thy et al . , 2019 ; Koh and Liang , 2017 ; Kim et al . ,   2016 ) have been proposed to explain local instance   level decisions of black box models , contrastive/-   counterfactual explanations have seen a surge of   interest recently ( Wachter et al . , 2017 ; Dhurand-   har et al . , 2018 ; Madaan et al . , 2021 ; Luss et al . ,   2021 ; Ross et al . , 2021 ) . One reason for this is that   contrastive explanations are viewed as one of the   main tools to achieve recourse ( Karimi et al . , 2021 ) .   For example , companies commonly use chatbots to   communicate with customers , which starts by pass-   ing customer text through a classiﬁer that decides   which support department should handle the cus-   tomer . A common problem is locating bias in these   classiﬁer models since “ the chatbot will continue   to show the behavior ” due to biased knowledge   bases(Brown , 2021 ) ; recourse in terms of remov-   ing bias could be achieved by identifying examples   that drive the bias .   Given this surge of interest and its importance in   recourse , in this paper , we propose a novel method   Contrastive Attributed explanations for Text ( CAT )   which provides contrastive explanations for natural   language data , a modality that has received com-   paratively less attention when it comes to these   type of explanations . We show that our method pro-   duces ﬂuent contrasts and possesses an additional   novel twist not seen in prior works . As such , our   method also outputs a minimal set of semantically   meaningful attributes that it thinks led to the ﬁnal   contrast . These attributes could be subtopics in a   dataset , different from the class labels , that charac-   terize a piece of text or the attributes could even be   obtained from a different dataset . Our approach is   to leverage these attributes by building models ( viz .   classiﬁers ) for them and then using these classiﬁers   to guide the search for contrasts in the original task .   Regarding the motivating biased chatbot example   above , learning attributes that lead to contrasts can   be particularly useful . Gender bias , for instance , is7190   hard to locate because contrasts can modify gen-   der on individual chats in different ways ( removing   pronouns , replacing gendered words with neutral ,   etc . ) . Such bias can be more easily identiﬁed if the   different modiﬁcations all affect a ( latent ) attribute   that is gendered such as motherhood , which our   CAT explanations should highlight .   To better understand what CAT offers , consider   the examples provided in Table 1 . Here we show   two example sentences from the AG News dataset   ( Zhang et al . , 2015 ) which were classiﬁed as Sci-   Tech by our neural network black box ( BB ) model   ( details in Section 4 ) . Each explanation method   generates a contrast in the class Business where the   other choices were World and Sports . As can be   seen , our method CAT produces closer contrasts   than two recent methods : Generate Your Counter-   factuals ( GYC ) ( Madaan et al . , 2021 ) and Minimal   Contrastive Editing ( MICE ) ( Ross et al . , 2021 ) .   A key novelty is that CAT provides additional   information in terms of characteristic attributes it   thinks the contrast sentence belongs to ( indicated   by+sign ) , while also indicating characteristics   it thinks are no longer applicable ( indicated by −   sign ) . Our method picks a few relevant attributes   that guide generation of the contrast ; the attributes   themselves provide additional insight into the func-   tioning of the black box model . This is conﬁrmed   through two separate user studies conducted with   a total of 75 participants for which the results are   reported in section 4.4 . Users found it easier to   predict the class of the input sentence given our   explanation over GYC , MICE , MICE with no ﬁne   tuning ( MICE - nft ) , and an ablation of our method ,   called CAT with no attributes ( CAT - na ) , and more-   over , users qualitatively preferred our method in   terms of understandability , sufﬁciency , satisﬁabilityand completeness on a ﬁve point Likert scale .   It is important to note that there are various ways   to generate text using different language models   ( GPT-2 , BERT , T5 , etc . ) and even different tech-   niques on how inﬁlling might be performed ( for-   ward or bi - directional ) . The key idea of guiding the   generation of contrasts through attributes can be   considered for other recent methods , whether the   contrast is learned through differentiable optimiza-   tion ( Madaan et al . , 2021 ) or through combinatorial   search ( Ross et al . , 2021 ) .   We note that author provided implementations   of GYC and MICE require models that use speciﬁc   text embeddings and are not easily adaptable to   other embeddings . Our method CAT , however , is   easily adaptable , and we thus compare against the   methods GYC and MICE using the embeddings   they are respectively implemented for in order to   get stringent and fair comparisons for CAT with   previous state - of - the - art methods . This means we   must compare against GYC and MICE on different   text classiﬁcation models , and hence require two   separate user studies , one comparing CAT with   GYC and the other comparing CAT with MICE   ( and we suspect this is why no comparisons are   found in the literature ) .   As such , our contributions are as follows : 1 ) CAT   introduces the idea of using attributes to drive the   generation of text contrastives . This contribution   is both conceptual as it brings new insight to the   user as well as methodological as it leads to user-   preferred contrasts as seen in the user study . 2 ) The   CAT implementation is easily adaptable to classi-   ﬁers with different embeddings . 3 ) We qualitatively   evaluate CAT through examples on four different   datasets from different domain tasks . 4 ) We quanti-   tatively evaluate CAT over other methods in terms7191   of ﬂip rate , content preservation , ﬂuency , Leven-   stein distance and efﬁciency . 5 ) We demonstrate   the utility of CAT through two user studies that   ask users to determine a model ’s prediction on an   instance given an explanation ; CAT is compared   with GYC , MICE , MICE with no ﬁne tuning , and   an ablation of our method CAT - na .   2 Related Literature   Regarding the explanations of machine learn-   ing predictions on text data , a recent survey   ( Danilevsky et al . , 2020 ) considered 50 recent ex-   plainability works for natural language processing ,   and moreover only methods that “ justify predic-   tions " rather than understanding “ a model ’s behav-   ior in general " . Our intention is also to explain   individual predictions . Little work has been done   for global explainability with respect to text classiﬁ-   cation ; ( Ribeiro et al . , 2016 ) suggests using various   “ representative " local predictions to get a global un-   derstanding of model behavior . The vast majority   of explainability techniques found by ( Danilevsky   et al . , 2020 ) fall under local explainability .   Local methods can be divided among post - hoc   methods that explain a ﬁxed model ’s prediction and   self - explainable methods where the model is itself   understandable ; our focus is on the former . One   large group of explainability methods are feature   based where the explanation outputs some form of   feature importance ( i.e. , ranking , positive / negative   relevance , contributions , etc . ) of the words in text   ( Wallace et al . , 2018 ; Papernot and Patrick , 2018 ;   Feng et al . , 2018 ; Harbecke et al . , 2018 ; Ribeiro   et al . , 2016 ; Alvarez - Melis and Jaakkola , 2017).Other types of local post - hoc explanations include   exemplar based ( Gurumoorthy et al . , 2019 ; Koh   and Liang , 2017 ; Kim et al . , 2016 ) that output sim-   ilar instances to the input .   Amongst local methods our focus is on con-   trastive / counterfactual methods ( Dhurandhar et al . ,   2018 ; Madaan et al . , 2021 ; Luss et al . , 2021 ) that   modify the input such that the class changes and   explains the prediction as “ If the sample were mod-   iﬁed byX , the prediction would have been Yin-   stead , " where Xis a change to the input and Yis a   new class . Such explanations are complementary   to the other methods discussed above . They offer   different types of intuition and thus should be used   in conjunction with rather than instead of counter-   factuals . ( Luss et al . , 2021 ) learn contrasts using   latent attributes , but speciﬁcally for color images .   Their approach does not readily translate to text as   humans do not perceive minor unrealistic parts of a   generated image , whereas any nuance in text is eas-   ily noticed . Thus , generating ( ﬂuent ) contrasts for   text models is inherently much more challenging .   The most relevant comparisons to our work   are GYC ( Madaan et al . , 2021 ) and MICE ( Ross   et al . , 2021 ) . GYC builds on the text generation of   ( Dathathri et al . , 2020 ) by learning perturbations to   the history matrix used for language modeling that   are also trained to reconstruct input from the gen-   erated counterfactuals . A diversity regularization   term ensures that this does not result in counterfac-   tuals that are identical to the input . A more recent   work MICE masks and replaces important words   selected by ( Simonyan et al . , 2013 ) in order to ﬂip   the prediction . MICE requires ﬁne - tuning their lan-   guage model to each dataset , which is a signiﬁcant   overhead especially given the fact that we are sim-   ply generating local explanations versus our CAT   framework . Not to mention in many real applica-   tions ( sufﬁcient ) data may not be available to ﬁne   tune explanations ( Dhurandhar et al . , 2019 ) . Both   GYC and MICE works are targeted , i.e. , the user   must decide what class the counterfactual should be   in as opposed to CAT which automatically decides   the contrast class .   Other recent methods are POLYJUICE ( Wu   et al . , 2021 ) and a contrastive latent space method   ( Jacovi et al . , 2021 ) . The former is a human - in - the-   loop method requiring supervision about the type   of modiﬁcation to be performed to the text such   as negation , word replacement , insertion , deletion ,   and is not catered towards explaining a speciﬁc7192classiﬁer by automatically ﬁnding the appropriate   edits . The latter does not generate contrastive text   but rather highlights ( multiple ) words in the input   text that are most likely to alter the prediction if   changed , where again the target class has to be pro-   vided . Further , ( Jacovi et al . , 2021 ) assume access   to encodings from the second - to - last layer of the   model being explained , and is thus not a black box   method like CAT . Our focus being automated con-   trastive explanation generation , we compare with   GYC and MICE , i.e. , methods designed towards   explaining a classiﬁer , where a valid contrast is   also generated . The value of CAT versus GYC and   MICE comes from the output of ( hidden ) subtopics   that are added / removed from the original text to   create the contrast , giving important intuition that   is missing from these other methods . This is con-   ﬁrmed through two user studies we conduct and   qualitative examples we provide . The attribute clas-   siﬁers built to predict these subtopics also aid in   creating better contrasts . Moreover , as will be evi-   dent , such attribute classiﬁers can be used across   datasets , thus precluding the need for each dataset   to contain such attributes . Furthermore , topic mod-   els or autoencoders could be used to divulge such   attributes .   3 Proposed Approach   We now describe our Contrastive Attributed expla-   nations for Text ( CAT ) method . Contrastive expla-   nations , convey why the model classiﬁed a certain   input instance to a class p , and not another class q.   This is achieved by creating contrastive examples   ( also called contrasts ) from input instance which   get predicted as q. Contrastive examples are cre-   ated by minimally perturbing the input such that   the model prediction changes . In the case of text   data , perturbations can be of three types : ( 1 ) insert-   ing a new word , ( 2 ) replacing a word with another ,   and ( 3 ) deleting a word . In addition to keeping the   number of such perturbations small , constrastive   explainers also try to maintain grammatical correct-   ness and ﬂuency of the contrasts ( Madaan et al . ,   2021 ; Ross et al . , 2021 ) .   As an example , take the case of a black box   model trained on the AG News dataset that predicts   which category a certain news headline falls under .   Given a headline , “ Many technologies may be a   waste of time and money , researcher says " which is   predicted as Sci - Tech , a contrastive explainer will   try to explain why this headline was n’t predictedas , say , Business by generating a contrastive exam-   ple , “ Many technologies jobs may be a waste of time   and money , researcher says " which is predicted as   Business . Observe that a single word replacement   achieves a prediction change . Such contrastive ex-   planations can help users test the robustness of   black box classiﬁcation models .   We observed that even with constraints for min-   imal perturbation and ﬂuency on a given black   box model and an instance , there are multiple con-   trastive examples to choose from and , very often ,   many are less informative than others . For example ,   another possible contrast is , “ Many technologies   may be a waste of time investment and money ,   researcher says " which also gets predicted as Busi-   ness . However , this particular explanation is not as   intuitive as the previous one as “ money " is a form   of “ investment " and the nature of the sentence has   not changed in an obvious sense with the word   “ technologies " still present in the sentence .   To alleviate this problem , we propose to con-   struct and use a set of attribute classiﬁers , where   the attributes could be tags / subtopics relevant to   the classiﬁcation task obtained from the same or a   related dataset used to build the original classiﬁer .   Attribute classiﬁers indicate the presence / absence   of a certain subtopic in the text and conﬁdence   scores from these classiﬁers could be used as a   regularization to create a contrast . We thus prefer   contrasts which change attribute scores measur-   ably as opposed to those contrasts which do not .   However , at the same time , we want a minimal   number of attribute scores to change so as to have   crisp explanations . Hence , our regularization not   only creates more intuitive contrasts , but also pro-   vides additional information to the user in terms   of changed subtopics which , as conﬁrmed through   our user study in Section 4.4 , provide better un-   derstanding of the model behavior . The important   steps in our method are depicted in Figure 1 .   Formally , given an input text x∈ X , and a   text classiﬁcation model f(·)which predicts y=   f(x)∈Y , we aim to create a perturbed instance   xsuch that the predictions f(x)/negationslash = f(x)andx   is “ minimally " different from x. We use a set of m   attribute classiﬁers ζ : X→R,∀i∈{1, ... ,m } ,   which produce scores indicative of presence ( higher   scores ) or absence ( lower scores ) of corresponding   attributes in the text . We say that attribute iis added   to the perturbed sentence if ζ(x)−ζ(x)>τand   removed when ζ(x)−ζ(x)<−τ , for a ﬁxed7193τ > 0 . Word - level Levenshtein distance between   original and perturbed instance d(x , x ) , which   is the minimum number of deletions , substitutions ,   or insertions required to transform xtoxis used   to keep the perturbed instance close to the original .   The naturalness ( ﬂuency ) of a generated sentence   xis quantiﬁed by the likelihood of sentence xas   measured by the language model used for genera-   tion ; we denote this likelihood by p(x ) . For a   predicateφ , we denote 1the indicator of φ , which   takes the value 1 if φis true and 0otherwise . Given   this setup , we propose to ﬁnd contrastive examples   by solving the following optimization problem :   max||ζ(x)−ζ(x)||−β / summationdisplay1   + λ·max{[f(x)]−[f(x ) ] }   + η·p(x)−ν·d(x , x ) , ( 1 )   whereζ(x)is a vector such that [ ζ(x)]=ζ(x ) ,   β , λ , η , ν > 0are hyperparameters that trade - off   different aspects , and ||·||is thelnorm . The   ﬁrst term in the objective function encourages to   pick anxwhere at least one attribute is either   added / removed from x. The second term minimizes   the number of such attributes for ease of interpreta-   tion . The third term is the contrastive score , which   encourages the perturbed instance to be predicted   different than the original instance . Fourth and ﬁfth   terms ensure that the contrast is ﬂuent and close to   the original instance , respectively .   The above objective function deﬁnes a controlled   natural language generation problem . Earlier meth-   ods for controlled generation that shift the latent   representation of a language model ( such as GPT-2 )   ( Madaan et al . , 2021 ; Dathathri et al . , 2020 ) have   resulted in generated sentences being very differ-   ent from the original sentence . We thus adopt a   different strategy where we ﬁrst take the original   sentence and identify locations where substitution-   s / insertions need to be made using available feature   attribution methods such as Integrated Gradients   ( Sundararajan et al . , 2017 ) . These words are or-   dered by their attribution and greedily replaced   with a [ MASK ] token . An MLM pre - trained BERT   model ( Vaswani et al . , 2017 ; Devlin et al . , 2018 )   is then used to ﬁll these masks . We take the top k   such replacements ranked by BERT likelihood . For   insertions , a mask token is inserted to the right and   left of important words in order to generate a set   of perturbations similar to the input example . Theattribute classiﬁers are applied to each generated   candidate contrast , and the best xis selected as   evaluated by Eq . 1 . For mtoken perturbations , the   above process is repeated mtimes , where at each   round , the top kperturbed texts are ranked and   selected according to Eq . 1 , and the above pertur-   bation process is applied to all selected perturbed   texts from the previous round . Note that we per-   form the hyperparameter tuning for Eq . 1 only once   per dataset . Details on hyperparameter tuning and   optimizing Eq . 1 are in Appendix A.   Regarding generalizability of our approach , as   already noted , the attribute classiﬁers can be de-   rived from other sources of data and are not nec-   essarily dependent on the data and model being   explained . Furthermore , other methods could be   used to obtain attributes ; unsupervised methods   such as LDA , V AEs , GANs could be leveraged to   ascertain semantically meaningful attributes . The   attribute classiﬁers that appear in the loss function   of Eq . 1 could be replaced by disentangled repre-   sentations learned by V AEs ( Kumar et al . , 2018 )   or by topic models . Hence , CAT is generalizable   beyond annotated datasets .   4 Experimental Study   4.1 Setup Details   We use an MLM pre - trained BERTmodel from   Huggingface ( Wolf et al . , 2019 ) to generate text per-   turbations . For attributes , classes from the Huffpost   News - Category ( Misra , 2018 ) and 20 Newsgroups   ( Newsgroup , 2008 ) datasets were used . The Huff-   post dataset has 200 K news headlines split into 41   classes . We merged similar classes and removed   those which were n’t a standard topic ; 22 classes   remained . The 20 Newsgroups dataset has 18000   newsgroup posts with 20 topic classes . Together ,   we obtained 42 attributes . For 22 classes from Huff-   post , we trained 22 1 - vs - all binary classiﬁers with   a distilbert ( Sanh et al . , 2019 ) base , so that the   same sentence can have multiple classes . For 20   Newsgroups , we trained multiclass classiﬁers on   the other 20 classes . More details on attribute clas-   siﬁers are provided in Appendix B. Note that at-   tribute classiﬁers are transferable as they need not   depend on the dataset and model being explained .   We evaluate our explanation method on models   trained on AgNews ( Zhang et al . , 2015 ) , DBPedia   ( Lehmann et al . , 2015 ) , Yelp ( Shen et al . , 2017),7194and NLI ( Bowman et al . , 2015 ) . For an apples-   to - apples comparison of our methods with GYC   on AgNews , DBPedia and Yelp , we trained models   with the same architecture as the ones in their work :   an Embedding Bag layer followed by a linear layer .   For MICE the Roberta based model was used for   all datasets as that is what the publicly provided   implementation naturally applies to . MICE uses a   two - step framework to generate conterfactual ex-   planations , with the generator being T5 ( Raffel   et al . , 2019 ) ﬁne tuned on the task - speciﬁc dataset .   More details on model training are provided in Ap-   pendix C and on datasets in Appendix D.   4.2 Qualitative Evaluations   We now provide qualitative examples from two   datasets , AgNews and NLI , with additional exam-   ples for Yelp , and DBpedia in the Appendix G.   AgNews . The dataset is from a real - world news   domain which contains short news headlines and   bodies from four news categories - world , business ,   sports , and sci - tech . Our experiments focus on ex-   planations for predicting the class from headlines .   Table 2 ( top ) shows results of applying CAT to   ﬁve headlines in the AgNews dataset . The ﬁrst row   explains that the headline is predicted as sci - tech   because if the headline was more related to topics   such as things for sale , baseball , hockey , and less   about computer - related topics , it would have been   predicted sports , which is achieved in the contrast   by replacing “ File " with “ Salary " . It is important to   consider the interaction of words ; here , the black   box model considers Kazaa a sports team because   of the change . The second and third rows offer intu-   itive examples as to how the attribute changes relate   to the contrasts . The insight is that we learn what   topics the black box model ﬁnds most relevant to   the prediction , as opposed to only knowing the sin-   gle word and needing to ﬁgure out why that caused   the change . In the fourth row , adding politics and   removing money leads to changing the input from   business to sci - tech as “ factory growth " has more   relationship to money while “ population growth " is   related to politics . The ﬁfth row shows the opposite   as adding money and removing politics changes   the input from world to business . This last example   illustrates that , for longer text , single perturbations   are often insufﬁcient to ﬂip the label and multiple   changes are needed . These last two examples offer   the insight that the classiﬁer associates business   with politics , which is not obvious a priori . NLI . The Natural Language Inference ( Bowman   et al . , 2015 ) dataset contains samples of two short   ordered texts , and the labels are either contradic-   tion if the second text contradicts the ﬁrst , neutral   if the two texts do not contradict or imply one an-   other , or entailment if the second text is a logical   consequence of the ﬁrst text .   Table 2 ( bottom ) illustrates CAT applied to ﬁve   example texts from the NLI dataset . The ﬁrst row   shows an entailment that was modiﬁed to a contra-   diction by replacing the word “ break " with “ lawn " .   While this would be the explanation offered by a   typical counterfactual method , CAT additionally   shows that the topic of electronics ( often associ-   ated with the word “ break " ) was removed . Such   insight can offer more clarity as to why the contrast   was predicted a contradiction rather than neutral ,   which seems more likely until we learn that the   electronics topic that has nothing to do with the   hypothesis was removed from the text . In row two ,   the change of “ chrome " to “ rust " is attributed to   adding the space topic ( as rust is an important issue   in space exploration ) and removed the graphics and   electronics topics associated with chrome ( books   or web browsers ) . In row three , the difference of   children being a part of a tournament versus simply   watching the tournament is attributed to a reduc-   tion of the entertainment topic ( and similarly in row   four for playing a guitar versus stealing a guitar ) .   In row ﬁve , the change of “ naps " to “ photos " is at-   tributed to adding the graphics topic , which makes   sense and helps build trust in the model .   4.3 Quantitative Evaluations   We evaluate the explanation methods on 500 ran-   domly selected test instances from each dataset .   We do not report results on NLI for GYC as it   did not produce valid contrasts possibly because   of the longer length of the texts . For each dataset ,   we measure the following properties : i ) Flip rate   ( Flip ) , ii ) Edit distance ( Dist ) , iii ) Content Preser-   vation ( Cont ) , and iv ) Fluency . Flip rate is a mea-   sure of the model ’s efﬁcacy to generate contrastive   sentences and is deﬁned as the fraction of inputs   for which an edit successfully ﬂips the prediction .   Edit Distance is the number of edits as measured   by the word - level Levenstein distance between in-   put and contrastive sentences , i.e. , the minimum   number of deletions , insertions , or substitutions re-   quired to transform one into the other . We report a   normalized version given by the Levenshtein dis-7195   tance divided by the number of words in the input ;   this metric ranges from 0 to 1 . Content preserva-   tion measures how much input content is preserved   while generating a contrastive sentence in a latent   embedding space . For this , we compute the cosine   similarity between input and contrastive sentence   embeddings obtained from a pre - trained sentence   BERT ( Reimers and Gurevych , 2019 ) model . Flu-   ency measures the alignment of the contrastive and   input sentence distributions . We evaluate ﬂuency by   calculating masked language modeling loss on both   original and edited sentences using a pre - trained   GPT-2 model and compute ﬂuency as the ratio of   the loss of the contrast to the loss of the input sen-   tence . A value of 1.0 indicates the contrast is as ﬂu-   ent as the original sentence . Table 3 reports means   of each metric across all instances obtained from   generated contrastive sentences .   Observations : In Table 3 we compare the perfor-   mance of CAT with two state - of - the - art contrastive   methods GYC and MICE . MICE - nft denotes MICE   without ﬁne tuning . As can be seen CAT produces   contrasts with perfect ﬂip rate , retains highest con-   tent relative to the original sentence , that too with   fewest changes , and maintains best language ﬂu-   ency in all cases , but one . This can be accredited to   the core details of the CAT approach which is based   on minimal but relevant perturbations through acontrolled local greedy search procedure guided by   attribute classiﬁers . The single case where CAT is   not best performing is possibly because ﬁne tuning   helps create more natural contrasts ; also conﬁrmed   by the similar performance of MICE - nft and CAT .   We also estimated the efﬁciency of CAT by com-   puting the time it takes to obtain a contrastive expla-   nation normalized by the input length . CAT , MICE ,   and GYC were evaluated on a NVIDIA V100 GPU   for 10 contrastive explanations . The mean times   taken by CAT , MICE and GYC were 2.37,1.17   and10.69seconds respectively . The efﬁciency of   CAT over GYC can be credited to the controlled   local greedy search approach , although guiding the   search with attribute classiﬁers makes it slightly   more expensive than MICE , where for the latter we   are ignoring the time for ﬁne tuning .   4.4 Human Evaluation   We now describe two user studies we conducted to   ascertain the value of our attributed explanations .   User studies have become ubiquitous in explainable   AI literature ( Ribeiro et al . , 2016 ; Singh et al . , 2019 ;   Ramamurthy et al . , 2020 ; Luss et al . , 2021 ) because   they illustrate the beneﬁt of these tools to the end   user . We follow in the direction of most user studies   by ascertaining beneﬁt by requiring participants to   perform a task given our explanations.7196   Methods : We consider ﬁve different explanation   methods : 1 ) CAT , 2 ) CAT - na ( i.e. CAT without at-   tributes ) , 3 ) GYC , 4 ) MICE and 5 ) MICE - nft ( i.e.   MICE without ﬁne - tuning ) . Comparison of CAT   with CAT - na provides an ablation , while its com-   parison with GYC , MICE , MICE - nft showcases its   value relative to other state - of - the - art contrastive   text explanations . Again , here we ran two separate   user studies , one comparing with GYC ( User Study   1 ) and the other with MICE ( User Study 2 ) , since   each implementation was more suited to a speciﬁctype of model and embedding . CAT , however , was   amenable to either setting .   Setup : We chose the AgNews dataset for the study   since this was the only dataset where we under-   performed on one of the benchmark metrics ( ﬂu-   ency ) w.r.t . the competitors . We thus wanted to   see if this had any effect in terms of the quality   of insight provided by our explanations to typi-   cal users of such explanations . For each study , we   built a four class neural network black box model   ( described earlier ) to predict articles as either be-   longing to Business , Sci - Tech , World or Sports   categories . The task given to users was to deter-   mine the classiﬁcation of the article based on a   contrastive explanation from one of the respective   methods . Five options were provided to the user   which included the four classes along with a “ Ca n’t   Tell " option . For each method , seven ( User Study   1 ) or ﬁve ( User Study 2 ) randomly chosen sentence-   explanation pairs were provided to the user where   the users were blinded to the exact method produc-   ing the explanation . For each ( anonymized ) expla-   nation method , we additionally asked the users for   their qualitative assessment along four dimensions ;   completeness , sufﬁciency , satisfaction and under-   standability based on a 5 point Likert scale . A total   of 24 questions were answered by users in both   user studies . Users were also allowed to leave op-   tional comments which we provide in the appendix ,   along with screen shots from the user study .   We hosted our survey on Google Forms . A total   of 75 participants with backgrounds in data science ,   engineering and business analytics voluntarily took   part in user studies ( 37 in study 1 and 38 in study   2 ) . We chose this demographic as recent studies   show that typical users of such explanations have   these backgrounds ( Bhatt et al . , 2020 ) .   Observations : Figure 2 depicts the results from   our user studies . Figure 2a demonstrates that both7197our methods CAT and its ablation CAT - na ( which   does not use attributes ) signiﬁcantly outperform   GYC in terms of usability towards the task . Figure   2b shows similar performance for CAT and CAT-   na , although MICE is a much stronger competitor   than GYC . It seems that the embeddings used by   MICE lead to better contrasts than GYC , not to   mention the mechanistic difference between them   where GYC biases towards changing the end of   sentences which may not be preferable in many   cases . Additionally , the ﬁne tuning done in MICE   seems to further help elevate the quality of the con-   trasts . Nonetheless , CAT still outperforms MICE   without the ( expensive ) need to ﬁne tune .   Our method performs much better than GYC ,   MICE and CAT - na from a qualitative perspective ,   as well , as seen in Figures 2c and 2d , where CAT   seems to score highest in understandability . These   qualitative preferences of CAT are further con-   ﬁrmed through the ( optional ) comments written   by some participants , e.g. , “ ... explainer B was   very good , and explainer C was reasonably good ” ,   where “ explainer B ” refers to CAT and “ explainer   C ” to CAT - na in user study 1 ; or “ the additional   info in explanation C was useful ” in user study 2 ,   where “ explanation C ” here refers to CAT .   5 Conclusion   In this paper , we proposed a contrastive explana-   tion method , CAT , with a novel twist where we con-   struct attribute classiﬁers from relevant subtopics   available in the same or different dataset used to   create the black box model , and leverage them to   produce high quality contrasts . The attributes them-   selves provide additional semantically meaning-   ful information that can help gain further insight   into the behavior of the black box model . We have   provided evidence for this through diverse quali-   tative examples on multiple datasets , a carefully   conducted user study and through showcasing su-   perior performance on four benchmark quantitative   metrics .   In the future , it would be interesting to test   CAT on other applications with the appropriate   attribute classiﬁers . Another useful direction is gen-   erating counterfactually - augmented data ( CAD ) us-   ing CAT . If used to generate multiple contrasts ,   CAT could offer attributional / topic diversity . Since   diversity in types is key to producing robust models ,   CAT could potentially to a large degree alleviate   “ the lack of perturbation diversity that limits CAD’seffectiveness " ( N. Joshi , 2022 ) .   Limitations   Although our work has the potential to have a pos-   itive impact on discovering models that have un-   known discrimination , a nefarious agent could po-   tentially provide accurate but purposely incorrectly   labeled attribute classiﬁers in order to drive cer-   tain misleading or simply incorrect insights . An-   other unethical act by a developer could be to hide   sensitive attributes so that biases could not be dis-   covered . In order to use the explanation system in   a beneﬁcial manner we do assume the developer   can be trusted . It is also possible that we may not   uncover the globally minimal contrast since the op-   timization is non - convex given the complexity of   classiﬁers we are trying to explain . Thus , smaller   edits may be possible that change the output of   a classiﬁer that go unnoticed . This however , is a   concern for even other contrastive / counterfactual   explainability methods .   References719871997200   A Hyperparemeter tuning for CAT   Since no ground truth exists for our contrastive   explanations , hyperparameters for CAT objective   ( given in Eq.1 ) were tuned qualitatively by observa-   tion . It is important to consider that , in practice   a user will only need to tune the hyperparame-   ters once at the beginning and will then be able   to generate explanations for an arbitrary number of   instances from the dataset . Our tuning led to the   following values which we used across datasets :   We kept highest weights for λ= 5.0to make sure   that perturbed sentences have a label different from   that of original sentence . We used mean of BERT   logits of words inserted / replaced as a surrogate for   p(x)with regularization parameter ηset to be   1.0and regularization parameter νfor Levenshtein   distance set to 2.0.βwas set to 3.0 . The threshold   for predicting addition or removal of attributes τ ,   we used two values . For binary attribute classiﬁers   from the Huffpost News dataset , we set τ= 0.3 ,   and for multiclass attribute classiﬁers trained on   the 20 Newsgroup dataset , we set τ= 0.05 .   We took 50 randomly selected examples from   the training set , generated explanations and eval-   uated them manually to choose hyperparameters   which is similar to prior works ( Luss et al . , 2021 ;   Madaan et al . , 2021 ) . The boundary values for hy-   perparameter search were λ∈[4,10],β∈[1,5 ] ,   η∈[0.5,2],ν∈[1,4 ] , andτ∈[0.01,0.5 ] . With   more examples for tuning we would expect better   explanations , although we found this number to be   sufﬁcient .   B Attribute classiﬁers   In our experiments we created 42 attributes from   Huffpost News - Category dataset(Misra , 2018 ) and   20 Newsgroups ( Newsgroup , 2008 ) .   News - Category datasethas 41 classes of which   we merged similar classes and removed those   which were n’t standard topic . For instance , classes   “ food & drink " and “ taste " was merged and labelled   as a new class “ food " . At the end we obtained   the following 22 classes : education , money , world ,   home & living , comedy , food , black voices , parent-   ing , travel , sports , women , religion , Latino voices ,   weddings , entertainment , crime , queer voices , arts ,   politics , science , ﬁfty , and environment . We created22 1 - vs - all binary classiﬁers , so that the same sen-   tence can have multiple classes . To alleviate the   class imbalance issue in training these binary clas-   siﬁers we sampled fewer instances uniformly at   random from the negative classes making sure that   the number of negative instances are no more than   80 % of the training data .   The 20 Newsgroup datasethas the following 20   classes : atheism , graphics , ms-windows.misc , com-   puter , mac.hardware , ms - windows.x , forsale , autos ,   motorcycles , baseball , hockey , cryptography , elec-   tronics , medicine , space , christian , guns , mideast ,   politics , and religion . A distilbert multi - class clas-   siﬁer was trained on this data .   All attribute classiﬁers were trained with DistliB-   ERT basefor10epochs using the Adam optimizer   with a learning rate of 5×10 , weight decay of   0.01 , and a batch size of 16 . Each of the models   were trained using NVIDIA V100 GPUs in under   12 hours .   C Text classiﬁers   We conducted experiments on two sets of text clas-   siﬁers trained on four datasets : One set to compare   CAT with GYC and another to compared CAT with   MICE . All experiments were run on NVIDIA V100   GPUs .   For experiments comparing CAT with   GYC(Madaan et al . , 2021 ) , we trained model with   the same architecture as the one used in their paper .   This model is composed of an EmbeddingBag   layer followed by a linear layer , a ReLU and   another linear layer to obtain the logit . The logit   is provided to a classiﬁer we trained with cross   entropy loss . A GPT2 tokenizerwas used to   convert text into bag of words . The models were   trained using the Adam optimizer with a learning   rate of 1×10 , weight decay of 0.01 , and a batch   size of 32 .   For experiments comparing CAT with   MICE(Ross et al . , 2021 ) , we used author provided   implementationto train models with AllenNLP   ( Gardner et al . , 2018 ) . This architecture is com-   posed of a RoBERTa - base model with a linear7201   Embedding Bag based Classiﬁer   StatAgNews Yelp   Dist Cont Fluency Dist Cont Fluency   p - value < 1e<1e<1e<1e<1e<1e   std . dev . ( GYC ) 0.164 0.110 0.286 0.140 0.105 0.467   std . dev . ( CAT ) 0.098 0.084 0.105 0.100 0.076 0.134   StatDbpedia NLI   Dist Cont Fluency Dist Cont Fluency   p - value < 1e<1e<1e<1e<1e<1e   std . dev . ( GYC ) 0.132 0.115 0.344 NR   std . dev . ( CAT ) 0.059 0.069 0.064 0.323 0.010 0.033   Roberta based Classiﬁer   StatAgNews Yelp   Dist Cont Fluency Dist Cont Fluency   p - value < 1e<1e<1e<1e<1e<1e   std . dev . ( MICE ) 0.182 0.121 0.196 0.174 0.109 0.208   std . dev . ( MICE - nft ) 0.307 0.209 0.224 0.240 0.154 0.212   std . dev . ( CAT ) 0.307 0.209 0.144 0.084 0.044 0.098   StatDbpedia NLI   Dist Cont Fluency Dist Cont Fluency   p - value < 1e<1e<1e<1e<1e<1e   std . dev.(MICE ) 0.151 0.077 0.168 0.132 0.109 0.131   std . dev.(MICE - nft ) 0.200 0.160 0.160 0.164 0.088 0.143   std . dev.(CAT ) 0.058 0.039 0.071 0.037 0.013 0.0547202layer . For all four datasets the models were trained   for5epochs with batch size of 8using Adam   optimizer with a learning rate of 4e−05 , weight   decay of 0.1 , and slanted triangular learning rate   scheduler with cut frac 0.06 .   D Datasets   We performed experiments on 4 datasets : Ag-   News(Zhang et al . , 2015 ) , NLI(Bowman et al . ,   2015 ) , DBpedia ( Lehmann et al . , 2015 ) , and   Yelp(Shen et al . , 2017 ) . AgNews dataset was taken   from Kaggle websiteand rest three datasets from   huggingface datasets ( Lhoest et al . , 2021 ) .   AgNews . The dataset is from a real - world news   domain which contains short news headlines and   bodies from four new categories - world , business ,   sports , and sci - tech . It contains 30 K training and   1.9 K test examples per class , comprising of 128 K   samples . Our experiments focus on explanations   for predicting the class from the headlines .   NLI . The Natural Language Inference ( Bowman   et al . , 2015 ) datasetcontains samples of two short   ordered texts , and the labels are either contradic-   tionif the second text contradicts the ﬁrst , neutral if   the two texts do not contradict or imply one another ,   orentailment if the second text is a logical conse-   quence of the ﬁrst text . The dataset contains 550 K   training , 10 K test and 10 K validation examples .   DBpedia . This dataset is a subset of original   DBpedia data which is a crowd - sourced commu-   nity effort to extract structured information from   Wikipedia . This datasetis constructed by picking   14 non - overlapping classes from DBpedia 2014   with 40 K training and 5 K test examples per class .   Task here is to predict the class a DBpedia entry be-   long to . In our experiments we only use content and   drop the titleﬁeld provided with with the dataset .   Yelp . This is a binary sentiment classiﬁcation   datasetcontaining 560 K highly polar reviews for   training and 38 K for testing . The dataset consists   of reviews from Yelp which is extracted from the   Yelp Dataset Challenge 2015 .   E Quantitative Evaluation Statistics   Table 3 ( in the main paper ) shows the performance   evaluation of our proposed approach , CAT , on ﬁvedifferent datasets and 2classiﬁer models , namely   Embedding Bag based classiﬁer and Roberta based   classiﬁer models . It was noted that CAT outper-   formed GYC and MICE over AgNews , Yelp and   Dbpedia by statistically signiﬁcant margins on re-   spective model . Recall that GYC usage Embed-   ding Bag based classiﬁer model and MICE us-   age Roberta based classiﬁer . Therefore , we imple-   mented both the classiﬁer models with CAT to com-   pare both GYC and MICE . We verify this statement   in Table 4 where we report pairwise t - tests compar-   ing the means for CAT with GYC and CAT with   MICE and standard deviation of CAT , GYC , MICE   for each metric and dataset . The improvement of   CAT over GYC and MICE is observed to be sta-   tistically signiﬁcant across all metrics . We do not   report additional statistics for the ﬂip rate metric as   CAT always produces a contrastive sentence with   a ﬂipped class label unlike GYC or MICE which   sometimes fails to ﬂip .   F Additional Information for User Study   Figure 3 shows screenshots of user study 1 , includ-   ing the instructions and example questions for the   three different methods discussed in the study . The   same instructions and format was used for user   study 2 , except that we asked ﬁve task oriented   questions ( rather than seven ) per explainer as there   were four explainers ( as opposed to 3 ) keeping the   total number of questions to be 24 , and thus keep-   ing the overall effort of both user studies roughly   the same for participants . For the users the methods   were named as Explainer A , B and C , where they   correspond to GYC , CAT , and CAT - na respectively   for user study 1 . For user study 2 , Explainer A , B , C   and D corresponded to MICE , MICE - nft , CAT and   CAT - na . Some users also left optional comments at   the end of the user study which we list here :   •“I found the questions confusing ... What is   complete , sufﬁcient and understandable expla-   nation in a word change ? Also , in each exam-   ple , what was I supposed to guess , the cate-   gory of the article or the possible prediction   of the method ? Are those things different ? ”   ( Study 1 )   •“Explainer A was pretty bad , explainer B was   very good , and explainer C was reasonably   good . ” ( Study 1 )   •“Explainer b was the best . The q&a at the end   of each page allows multiple choices per row,7203which is bad . Each question should remove   the modiﬁed classiﬁcation as an option ( if the   modiﬁed sentence is World , I should n’t be al-   lowed to choose World ) . Early on the survey   should state clearly that there are only 4 cate-   gories ( business / sci - tech / sports / world ) which   I did n’t know right away , I expected each ques-   tion have 4 different options , and knowing this   might have helped me understand what we ’re   doing here better . The opening text was very   confusing . ” ( Study 1 )   • “ I found B to be the best one . ” ( Study 1 )   • “ nice survey ” ( Study 1 )   •“the additional info in explanation C was use-   ful ” ( Study 2 )   G Additional Qualitative Examples   Table 5 offers at least ﬁve more examples of con-   trastive explanations from each dataset . Additional   insight can be gained from the attributes ; in the ﬁrst   row , adding travel - related text can ﬂip the class   from business to world , i.e. , the article was pre-   dicted business because if it was more about travel   it would have been predicted as world . This type   of explanation adds extra intuition as opposed to   only being given the replacement of words , “ oil " to   “ winds " in this case . As can also been seen , insight   is also a function of using attributes that have a   relationship to the dataset and task . For example ,   attributes derived from news sources are often not   helpful for explaining sentiment classiﬁers ( e.g. ,   Yelp ) for which sentiment is often ﬂipped by the   negation of the text . This is to be expected ; good   explanations require good attributes.720472057206