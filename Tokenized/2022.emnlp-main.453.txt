  Baijun Ji , Tong Zhang , Yicheng Zou , Bojie Hu , Si ShenTencent Minority - Mandarin Translation , Beijing , ChinaSchool of Computer Science , Fudan UniversityResearch Base on Interdisciplinary Terminology and Translation , Nanjing University{baijunji , zatozhang , bojiehu}@tencent.comyczou18@fudan.edu.cn   Abstract   Multimodal machine translation ( MMT ) aims   to improve translation quality by equipping   the source sentence with its corresponding im-   age . Despite the promising performance , MMT   models still suffer the problem of input degra-   dation : models focus more on textual informa-   tion while visual information is generally over-   looked . In this paper , we endeavor to improve   MMT performance by increasing visual aware-   ness from an information theoretic perspective .   In detail , we decompose the informative visual   signals into two parts : source - specific informa-   tion and target - specific information . We use   mutual information to quantify them and pro-   pose two methods for objective optimization   to better leverage visual signals . Experiments   on two datasets demonstrate that our approach   can effectively enhance the visual awareness   of MMT model and achieve superior results   against strong baselines .   1 Introduction   Multimodal machine translation ( MMT ) typically   improves text translation quality by introducing an   extra visual modality input . This task hypothesizes   that the corresponding visual context is helpful for   disambiguation and omission completion when the   source sentence is ambiguous or even incorrect .   Compared to the purely text - based machine trans-   lation , the critical point of MMT is to find out an   effective approach to integrating images into a MT   model that makes the most of visual information .   Most of the existing studies have dedicated their   efforts to the way of extracting multi - granularity vi-   sual features for integration ( Calixto and Liu , 2017 ;   Delbrouck and Dupont , 2017 ; I ve et al . , 2019 ; Zhao   et al . , 2022 ; Li et al . , 2022 ; Fang and Feng , 2022 ) or   designing model architectures for better message   passing across various modalities ( Calixto et al . ,   2017 , 2019 ; Yao and Wan , 2020 ; Yin et al . , 2020;Lin et al . , 2020 ; Liu et al . , 2021 ) , which achieve   promising performances in most of the multimodal   scenarios .   Despite the success of these methods , some re-   searchers reveal that visual modality is underuti-   lized by the modern MMT models ( Grönroos et al . ,   2018 ; Elliott , 2018 ) . Elliott ( 2018 ) leverage an ad-   versarial evaluation method to appraise the aware-   ness of visual contexts of MMT models . An in-   teresting observation is that the disturbed image   input generally shows no significant effects on the   performance of public MMT models , indicating   the insensitivity to the visual contexts . Recently ,   Wu et al . ( 2021 ) revisit the need for visual context   in MMT and find that their strong MMT model   tends to ignore the multimodal information . They   suggest that the gains from the multimodal signals   over text - only counterparts are , in fact , due to the   regularization effect .   In practice , the existing approaches mainly fo-   cus on a better interaction method between textual-   and visual- information . In training , they generally   employ maximum likelihood estimation ( MLE ) to   optimize the entire model . In most cases , the source   sentence carries sufficient context for translation ,   leading to the neglect of visual modality under such   training paradigm . However , the visual informa-   tion is still crucial when the source sentence faces   the problem of incorrection , ambiguity and gender-   neutrality . Accordingly , we argue that the visual   modality remains an excellent potential to facilitate   translation , which inspire us to explore taking full   advantage of the visual modality in MMT .   To this end , we propose increasing visual aware-   ness from an information theoretic perspective .   Specifically , we quantify the amount of visual in-   formation with an information - theoretic metric ,   namely Mutual Information ( MI ) . The quantifica-   tion is then divided into two parts : 1 ) the source-   specific part , which means the information between   source texts and images ; 2 ) the target - specific part,6755which means the mutual information between tar-   get texts and images given source texts , namely con-   ditional mutual information ( CMI ) . For the source-   specific part , we maximize the mutual informa-   tion by resorting to a lower bound of MI . For the   target - specific part , we propose a novel optimiza-   tion method for maximizing the conditional mutual   information ( See details in 3.2 ) . We evaluate our   approach on two public datasets . The experimen-   tal results and in - depth analysis indicate that our   method can effectively enhance the MMT model ’s   sensitivity to visual context , which manifests that   MMT has great potential when the utilization of   visual context is adequate .   To conclude , the main contributions of our paper   are three - fold :   1)We propose a novel MI - based learning frame-   work to improve multimodal machine trans-   lation by increasing visual awareness . This   framework is interpretable and can quantify   how much visual information is utilized .   2)To optimize the mutual information objective ,   we divide the corresponding visual informa-   tion into source / target - specific part . Further-   more , we propose a novel method to maxi-   mize the conditional mutual information in   the target - specific part .   3)Comprehensive experiments demonstrate that   our method can effectively encourage MMT   models to keep sensitive to visual context and   significantly outperform strong baselines on   two public MMT datasets .   2 Background   2.1 Multimodal Machine Translation   We start with the formulation of a regular MMT   task . Given a triplet dataset of { ( x , y , z ) } ,   the problem naturally turns into the likelihood max-   imization :   L = −1   N / summationdisplaylogp(y|x , z),(1 )   where zdenotes the image itself and x , ydenote the   description of the image in two different languages .   Recently , MMT models are usually consisted of   a Transformer - based encoder - decoder model and   an additional image encoder . There exist several   lines of works to integrate convolutional features   into the NMT model . We adopt the global feature ,   which is in the form of a single vector of an image .   This integration method has been proven beneficial   to some extent in prior works .   2.2 Mutual Information ( MI )   Mutual Information ( MI ) relates two random vari-   ables XandYand measures the amount of infor-   mation obtained from Xby observing the random   variable Y , which is defined as :   I(X ; Y ) = H(X)−H(X|Y ) , ( 2 )   where H(·)denotes information entropy . Mutual   information is symmetric and non - negative . High   mutual information shows a large reduction of the   random variable X ’s uncertainty given another vari-   ableY.   3 Methodology   In this section , we mathematically describe the   basic notion of our proposed method from an in-   formation theoretic perspective . Let Xdenotes a   random variable over source sentences , Ydenotes   a random variable over target sentences , and Zis   a random variable over images . They are jointly   distributed according to some true p.m.f . p(x , y , z ) .   We formulate the Visual Awareness of a MMT   model as a joint mutual information I(X , Y ; Z ) ,   which indicates how much the visual modality con-   tributes to the overall system . A high joint mu-   tual information suggests a strong sensitivity of the   MMT model to visual information . However , it   is intractable to directly maximize this joint mu-   tual information in training . Thus , in this work we   decompose I(X , Y ; Z ) into two parts by the chain   rule of mutual information and optimize these two6756   parts separately :   I(X , Y ; Z ) = I(X ; Z)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright+I(Y ; Z|X)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright .   ( 3 )   As shown in Figure 1 , the green part I(X ; Z )   is the source - specific mutual information ( SMI ) ,   which indicates the mutual information between   source texts and images ; the yellow part I(Y ; Z|X )   is the target - specific conditional mutual informa-   tion ( TMI ) , which indicates the mutual information   between target texts and images given source texts .   Figure 2 illustrates the overview of our proposed   method . In the following , we describe how to opti-   mize the source - specific and target - specific mutual   information in detail .   3.1 Source - Specific Mutual Information   Source - specific mutual information I(X ; Z ) deter-   mines how much information is shared between   the source texts and images . Inspired by van den   Oord et al . ( 2018 ) , we minimize the InfoNCE ( Lo-   geswaran and Lee , 2018 ) loss L to maximize   a lower bound on I(X ; Z ) .   I(X ; Z ) ≥ −E / bracketleftbigg   f(F(x),R(z))−   Elog / bracketleftbigg / summationdisplayexp f(F(x),R(z))/bracketrightbigg / bracketrightbigg   = L , ( 4)where f(·)is the cosine similarity function between   F(x)andR(z).F(x)denotes the pooled textual   representation of a source sentence xandR(z )   denotes the pooled visual representation from a   pre - trained vision model ( e.g. , ResNet or Vision   Transformer ) . xis a negative source sentence   drawn from the proposal distribution q(x ) . This   contrastive learning objective aligns the textual   and visual representations into a unified semantic   space , and encourages the text encoder to generate   grounded representations , which has shown effec-   tiveness in various scenarios ( Elliott and Kádár ,   2017 ; Kádár et al . , 2017 ) .   3.2 Target - Specific Conditional Mutual   Information   In MMT , I(Y ; Z|X)represents how much informa-   tion the image Zprovides corresponding to the tar-   getYgiven the source X. Our goal is to maximize   the conditional mutual information as follows :   I(Y ; Z|X ) = H(Y|X)−H(Y|X , Z).(5 )   Note that the term H(Y|X)refers to the   conditional probability P(Y|X ) , which is non-   accessible in a MMT model . To remedy this issue ,   we instead introduce a pseudo conditional mutual   information I(Y;/tildewideZ|X ) , which is formulated as :   I(Y;/tildewideZ|X ) = H(Y|X)−H(Y|X,/tildewideZ),(6 )   where / tildewideZrepresents a set of deteriorated images   ( /tildewidez,/tildewidez,···,/tildewidez ) . The visual information of these   deteriorated images are artificially interfered by   some means . For a well - trained visual - aware MMT ,   we suppose a deteriorated image would not signif-   icantly reduce the uncertainty of target sentence   prediction compared to its text - only counterpart .   Thus our goal is to minimize the pseudo condi-   tional mutual information I(Y;/tildewideZ|X ) . Accordingly ,   we have the training objective as follows :   I(Y ; Z|X)−I(Y;/tildewideZ|X )   = −H(Y|X , Z ) + H(Y|X,/tildewideZ )   = /summationdisplayp(x , y , z ) logp(y|x , z )   −/summationdisplayp(x , y,/tildewidez ) logp(y|x,/tildewidez).(7 )   Minimizing I(Y;/tildewideZ|X)means that the MMT can   correctly discriminate the useless visual informa-   tion . Moreover , maximizing I(Y ; Z|X)means that6757the MMT model is able to make the most of the   grounding information from images .   However , the above training objective can not   be directly calculated since the probability mass   function p(x , y , z ) is unobtainable . Therefore , we   follow Bugliarello et al . ( 2020 ) ; Fernandes et al .   ( 2021 ) to use a Monte Carlo estimator to approx-   imate it . Under sufficient held - out training data   of{(x , y , z ) } , we can estimate the true cross-   entropy as follows :   H(Y|X , Z)≈ −1   N / summationdisplaylogp(y|x , z).(8 )   This estimation will get more accurate when the   training data set is large enough . We note that /tildewidezis   an deteriorated version of the image zand is also   drawn from the same true probability distribution .   We can employ the same estimation method to ap-   proximate the cross - entropy . Finally , we have an   estimation for the optimization objective :   L = −1   N / summationdisplaylogp(y|x , z )   p(y|x,/tildewidez).(9 )   Moreover , the training process can be unstable   when the term p(y|x,/tildewidez)becomes very small ,   ignoring the fact that xhas already contained the   majority of information for translation . Following   Huang et al . ( 2018 ) , we perform back propagation   only when logis lower than a margin   value m. The above objective can be converted   into :   L = 1   N / summationdisplaymax{0 , m−logp(y|x , z )   p(y|x,/tildewidez ) } .   ( 10 )   3.3 Deteriorated Image Generation   In this section we describe how to generate a dete-   riorated view of the image z. One straightforward   approach is masking relevant objects in the visual   modality by using an object detector . This method   can reduce the amount of information contained   in a picture . However , using an external object de-   tector is time - consuming , making the MMT task   more complex . In this work , we adopt a more   efficient and feasible dropout - based method to gen-   erate the deteriorated images at the representation   level . Specifically , we randomly select the units   in the visual vector and mask them with zeros byfollowing a Bernoulli distribution of probability   ρ . In this way , the majority of information in the   image representations will be corrupted .   3.4 Model Architecture   We use a simple gated mechanism for multimodal   fusion , which has achieved competitive results ver-   sus other complicated models . Following the re-   cent work ( Wu et al . , 2021 ) , the source sentences   and the images are separately encoded by using a   transformer - based encoder and a pre - trained Resnet   model . Formally , the textual and image represen-   tations are combined by a learnable gate vector λ   as :   λ = Sigmoid ( W·Concat [ F(x);R(z ) ] ) ,   H = F(x ) + λ⊙ R(z ) . ( 11 )   Wis a trainable matrix and ⊙denotes element-   wise product . The decoder takes the modality input   H in the recurrent fashion as the text - only   NMT does . The deteriorated image input is fused   in the same way by replacing R(z)withR(/tildewidez ) .   3.5 Training Objective   Finally , the two MI - based objectives are jointly   optimized with MMT loss from scratch :   L = L + α|s|L+β|s|L,(12 )   where αandβ∈[0,1]are the hyperparameters for   balancing MI maximizing objective and MT loss ,   and|s|denotes the average length of the sequence   since the MI - based loss is sentence - level ( Pan et al . ,   2021 ) .   4 Experiments   4.1 Data   We evaluate our methods on two standard MMT   datasets , including AmbigCaps ( Li et al . , 2021 )   andMulti30 K ( Elliott et al . , 2016 ) .   AmbigCaps contains 81 K Turkish - English paral-   lel sentence pairs with visual annotations . We use   1,000 sentences provided by Li et al . ( 2021 ) for val-   idation and testing , respectively . This dataset is pro-   cessed into a gender - ambiguous one , in which sen-   tences containing the gender of the entity and pro-   fessions with gender implications removed . More-   over , AmbigCaps is designed to translate a gender-   neutral language ( Turkish ) into a gender - specified   language ( English ) , which is suitable for the MMT6758Row MethodValidation Test   BLEU METEOR BLEU METEOR   Only Text   1 Transformer(Li et al . , 2021 ) - - 35.71 -   2 Transformer37.46 69.31 37.60 68.64   Existing MMT Systems   3 Imagination(Elliott and Kádár , 2017 ) 37.83 68.92 38.11 69.25   4 Gated Fusion(Li et al . , 2021 ) - - 36.68 -   5 Selective Attn(Li et al . , 2022 ) 38.47 68.91 38.30 69.31   Our MMT System   6 Gated Fusion38.29 69.62 38.06 69.13   7 Our Method 39.24 70.30 39.40 70.22   task since the source textual information is not nat-   urally sufficient .   Multi30 K is a widely used dataset for MMT ,   which contains 29 K images with one English de-   scription and a German manual translation . We   follow a standard split for experiments and use   1,014 sentences for validation and 1,000 sentences   for testing ( Test2016 ) . In addition , we also evaluate   the WMT17 test set ( Test2017 ) and the ambiguous   MSCOCO test set , which include 1,000 and 461   triplets , respectively .   4.2 System Setting   We use the Transformer - Tiny ( Vaswani et al . , 2017 )   configuration to conduct all of our experiments ,   which can even obtain better performance than   those large models ( Wu et al . , 2021 ) . The tiny   transformer model consists of 4 encoder and de-   coder layers , with 1024 embedding / hidden units ,   4096 feed - forward filter size and 4 heads per layer .   We employ the Adam optimizer with lr= 0.005 ,   t = 2000 anddropout = 0.3for optimiza-   tion . At the training time , each training batch in-   cludes 4096 source tokens and target tokens . For   a fair comparison with previous works , we use the   early stopping strategy if the performance on vali-   dation does not gain improvements for ten epochs   on Multi30K. Empirically , 1eforαand1efor   βcan get the best results . The model for Ambig-   Caps is trained on a single Tesla P40 GPU and the   one for Multi30k is trained on two GPUs for a fair   comparison with previous work .   In particular , the number of patience on Am-   bigCaps is set to twenty epochs since the converge   speed on this dataset is slow . We average the resultsof the last ten checkpoints during inference . At de-   coding time , we generate with beam _ size = 5and   length penalty is equal to 1.0 .   For visual features , we use a ResNet-50 ( He   et al . , 2016 ) model pre - trained on ImageNet as the   image encoder . The dimension of the global feature   is 2048 and we use a projection matrix to convert   the shape of image features into that of text features .   For data processing , we generate subwords follow-   ing Sennrich et al . ( 2016 ) with 10,000 merging   operations jointly to segment words into subwords ,   which generates a vocabulary of 6,765 and 9,712   tokens for AmbigCaps and Multi30 K , respectively .   Finally , we report BLEU(Papineni et al . , 2002 )   and METEOR(Denkowski and Lavie , 2014 )   scores to evaluate the quality of different transla-   tion system . Our implementation are implemented   on Fairseq ( Ott et al . , 2019 ) .   4.3 Baseline Methods   We compare our method with baselines as follows :   •Vanilla Transformer ( Vaswani et al . , 2017 ) .   We report the Transformer - tiny results .   •Imagination ( Elliott and Kádár , 2017 ; Helcl   and Libovický , 2018 ) . This method adopts   a margin loss as a regularizer to the text en-   coder in order to learn grounded representa-   tions . We reimplement this method in our6759MethodTest2016 Test2017 MSCOCO   B M B M B M   Transformer ( Vaswani et al . , 2017 ) 41.02 68.22 33.36 62.05 29.88 56.64   Imagination ( Elliott and Kádár , 2017 ) 41.31 68.06 32.89 61.29 29.9 56.57   UVR ( Zhang et al . , 2020 ) 40.79 - 32.16 - 29.02 -   DCCN ( Lin et al . , 2020 ) 39.7 - 31.0 - 26.7 -   Multimodal Graph(Yin et al . , 2020 ) 39.8 - 32.2 - 28.7 -   Gated Fusion ( Wu et al . , 2021 ) 41.55 68.34 33.59 61.94 29.04 56.15   Selective Attn ( Li et al . , 2022 ) 41.93 68.55 33.62 61.61 29.72 56.94   Our Method 41.77 68.60 34.58 62.40 30.61 56.70   Transformer - based models instead of RNNs   for a fair comparison .   •Gated Fusion ( Wu et al . , 2021 ) . It uses a   gate vector to combine textual representations   and image representations and then feds them   into the MMT decoder . This is a simple yet   effective method and can outperform most of   MMT systems .   •Selective Attn ( Li et al . , 2022 ) . This method   has the same model architecture with Gated   Fusion and supports more fine - grained fea-   tures extracted from images .   Since AmbigCaps is a new MMT dataset , we   reproduce the above baseline methods and report   the corresponding results .   4.4 Results on Tr ⇒En Translation Task   The evaluation results of different MMT systems   on the Tr ⇒En translation task are presented in   Table 1 . By observing the reported results , we draw   the following interesting conclusions :   First , we observe that our implementation for   the text - only transformer and Gated Fusion out-   perform the results reported in the original paper .   This is due to the larger number of patience . It also   indicates our method is more competitive and still   achieves improvement against the strong baselines .   Second , all of the MMT models beat the text-   only transformer , which indicates that AmbigCaps   is a suitable dataset and verifies the benefit of the   visual modality . Gated Fusion gains 0.83 BLEU   points and 0.31 METEOR points compared to the   text - only model . Selective Attn shows further im-   provement due to the more - grained features . Ourmodel significantly outperforms the text - only trans-   former by 1.8 on BLEU and 1.58 on METEOR .   Third , our model also beats Imagination , Se-   lective Attn and Gated Fusion . The gap between   different baseline results is not significant . Note   that the Imagination model does not use the image   during the inference time . Our model uses the same   model architecture with Selective Attn and Gated   Fusion and consistently improves the BLEU score ,   1.30 and 1.34 points more than Selective Attn and   Gated Fusion . It indicates that our method can sig-   nificantly utilize the image context and improve   translation by using the same model configuration .   4.5 Results on En ⇒De Translation Task   Table 2 shows the main results on the En ⇒De   translation task . We find that the text - only trans-   former can act as a solid baseline with the aid of   the elaborate model configuration and training set-   tings , which even beats DCCN and Multimodal   Graph models . The BLEU score of our method on   Test2016 is quite marginal compared to the base   model Gated Fusion , which is the point that we at-   tempt to improve . We argue that the Test2016 test   set may contain less textual ambiguity . Besides ,   we observe significant BLEU / METEOR gains on   Test2017 and MSCOCO , outperforming the text-   only baseline by 1.22/0.73 points and 0.99/1.57   points . Moreover , our method beats all robust base-   line systems on the two test sets . These results   further indicate the necessity of visual context for   multimodal translation.6760System B M ID↑ GA↑   L 38.06 69.13 3.32 79.73 %   + L 38.72 69.50 3.35 80.23 %   + L 38.74 69.93 4.37 79.93 %   Full Model 39.40 70.22 8.31 80.65 %   5 Model Analysis   5.1 Ablation Study   To make a better understanding of the influence of   the training objective , we perform ablation studies   to validate the impact of each part on translation   quality . Column 2 to column 3 in Table 3 shows   bothL andL results in a close gain in   BLEU score , 0.66 and 0.68 points more than the   vanilla MMT model . We find L can perform   better than L in METEOR score . Combining   the two training objectives can further boost the   translation performance , which means L and   L can be beneficial to each other . It indicates   that our method improves the image utilization at   both the source and target sides .   Besides , we also investigate how the visual sig-   nals help to translate and whether the gains on trans-   lation come from the increasing visual awareness .   To achieve this goal , we introduce two new metrics   to evaluate the visual awareness of the MMT model ,   which are Incongruent Decoding andGender Ac-   curacy . We will first explain how these metrics are   calculated .   Incongruent Decoding ( ID ) We follow El-   liott et al . ( 2018 ) to use an adversarial evaluation   method to test if our method is more sensitive to   the visual context . We randomly replace the im-   age with an incongruent image or set the congruent   image ’s feature to all zeros . Then we observe the   value △ BLEU by calculating the difference be-   tween the congruent data and the incongruent one .   A larger value means the model is more sensitive   to the image context .   Gender Accuracy ( GA ) Following Li et al .   ( 2021 ) , we divide the translation sentences into   three categories : male , female andundetermined .   Male means that the target sentences contains at   least one of the male pronouns { ‘ he ’ , ‘ him ’ , ‘ his ’ ,   ‘ himself ’ } . The definition of Female is similar to   Male . The sentence is undetermined when it con - tains both male and female pronouns or neither , and   will not be considered for the calculation of gender   accuracy . It is designed to determine whether the   ambiguous words are indeed translated correctly .   Column 4 to column 5 in Table 3 show the re-   sults of visual awareness evaluation . When given   the incongruent image , L drops more than 3   points , indicating that this simple method can also   utilize the visual context to some extent . When   L orL is used , the dropped value will be-   come larger . Combining the two objective can lead   to a great decline . It means the visual part plays a   vital role in our system . There is also a significant   increase in gender accuracy with our method . It   also proves that the translation improvement partly   benefits from the correct translation of ambiguous   words .   5.2 The Influence of Weight αandβ   To evaluate the effects of the coefficients of αand   β , we observe the translation performance over var-   ious values by setting the corresponding coefficient   to zero . Figures 3 shows the performance fluctu-   ates greatly with a low rate of αandβ . With the   coefficients increasing slightly , the BLEU score   improves , which probes the effectiveness of in-   creasing visual awareness by applying our method .   The translation performance drops slightly as the   coefficients increase since the MI - based loss will   dominate the optimization and ignore the MLE6761training objective . In particular , the best perfor-   mance is achieved when α= 1eorβ= 1e   respectively .   5.3 The Influence of Deteriorated Image   Generation .   First , we compare the effects of different methods   to generate the deteriorated image in Figure 4 . We   find that using a object detector for masking rele-   vant objects can generate the best translation results .   It is not surprising since the picture in AmbigCaps   is not complex and the detector model is competent   to detect the most objects correctly . We should   note that our method is a unsupervised manner but   can still generate the competitive results . We also   explore the different influence of L with the   various dropout rate ρ . When the value of ρis very   small , the BLEU score is lower than the baseline .   This is due to that a small value of ρmeans that   most image information is preserved and the term   I(Y;/tildewideZ|X)should be as large as possible instead of   being minimized . Meanwhile , ρ= 1 means that   the image is totally unused , and the global vector   is all zeros . This conspicuous feature induces the   model to take a shortcut to discriminate the useless   image easily . Only the appropriate value of ρ≥0.5   can get stable and good results . It indicates a well-   designed method to generate deteriorated image is   critical for better translation .   6 Related Work   6.1 Mutual Information Maximization   Mutual information has been widely explored in re-   cent NLP work . Bugliarello et al . ( 2020 ) proposedcross - mutual information to measure the neural   translation difficulty . Fernandes et al . ( 2021 ) used   conditional mutual information as the metric to es-   timate the context usage in document NMT . Our   work differs in that we employ MI as a training   objective to enhance image usage instead of a pure   metric . Wang et al . ( 2022 ) maximized MI to solve   the OOV problem in NER . Kong et al . ( 2020 ) im-   proved language model pre - training from a mutual   information maximization perspective . To our best   knowledge , we are the first to employ mutual in-   formation to increase image awareness in MMT   generation task .   6.2 Efficient Visual Context Modeling   To equip the MMT model with the visual context   modeling ability , Caglayan et al . ( 2021 ) and Song   et al . ( 2021 ) used cross - modal pre - training to learn   visually - grounded representations . This point is   similar to our source - related MI method , which   aligns the textual and visual representations in a   shared semantic space . Wang and Xiong ( 2021 )   forced the model to reward masking irrelevant ob-   jects and penalize masking relevant objects . It also   makes the model sensitive to the changes in the vi-   sual modality and enhances visual awareness . But   detecting the relevant objects and correlating the   regions with the vision - consistent target words are   rather complex .   6.3 Contrastive Learning   Our method is highly related to contrastive learn-   ing . The source - specific loss is optimized by using   theInfoNCE bound , and the target - specific loss   can also be viewed as using a contrastive image   form and lowering the probability of generating the   target . Contrastive learning has been recently used   in translation tasks . Pan et al . ( 2021 ) closed the   gap among representations of different languages   to improve the multilingual translation . Zhang et   al . ( 2021 ) meliorated word representation space to   solve the low - frequency word prediction . Hwang   et al . ( 2021 ) used coreference information for gen-   erating the contrastive samples to train the model to   be sensitive to coreference inconsistency . Different   from this work , our method does not need external   knowledge .   7 Conclusion   In this paper , we summarize that only using the   MLE objective is unable to ensure the image in-6762formation is fully utilized , and there still exists   tremendous potential to explore . We propose a   method based on mutual information to increase   visual awareness in multimodal machine transla-   tion . The information from images is divided into   two parts(source - specific and target - specific ) . We   quantify this contribution from the image and put   forward the corresponding optimization solutions .   Experiments on Multi30 K and AmbigCaps show   the superiority of our methods . Detailed analysis   experiments conducted probes that our methods are   quite interpretable . In the feature , we would like to   apply our MI - based method to visual language pre-   training and other unsupervised tasks to improve   the downstream tasks .   8 Limitations   Since we conduct all experiments on the machine   translation task , it is unclear whether our approach   can benefit other multimodal tasks , e.g. Visual QA   or multimodal Dialog . Another thing worth noting   is that the image - source - target triplet data is not   easily available in reality . Thus how to effectively   use the unpaired image in the unsupervised way is   a more promising research .   References67636764