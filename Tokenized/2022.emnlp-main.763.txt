  Yuan Yao , Qianyu Chen , Ao Zhang , Wei Ji   Zhiyuan Liu , Tat - Seng Chua , Maosong SunDept . of Comp . Sci . & Tech . , Institute for AI , Tsinghua University , Beijing , China   Beijing National Research Center for Information Science and Technology   Innovation Center of Tsinghua University , Shanghai , ChinaSea - NExT Joint Lab , Singapore   School of Computing , National University of Singapore , Singapore   yaoyuanthu@163.com socqyc@gmail.com   Abstract   Vision - language pre - training ( VLP ) has shown   impressive performance on a wide range of   cross - modal tasks , where VLP models with-   out reliance on object detectors are becoming   the mainstream due to their superior compu-   tation efficiency and competitive performance .   However , the removal of object detectors also   deprives the capability of VLP models in ex-   plicit object modeling , which is essential to   various position - sensitive vision - language ( VL )   tasks , such as referring expression comprehen-   sion and visual commonsense reasoning . To   address the challenge , we introduce PEVL that   enhances the pre - training and prompt tuning   of VLP models with explicit object position   modeling . Specifically , PEVL reformulates   discretized object positions and language in   a unified language modeling framework , which   facilitates explicit VL alignment during pre-   training , and also enables flexible prompt tun-   ing for various downstream tasks . We show that   PEVL enables state - of - the - art performance of   detector - free VLP models on position - sensitive   tasks such as referring expression comprehen-   sion and phrase grounding , and also improves   the performance on position - insensitive tasks   with grounded inputs . We make the data   and code for this paper publicly available at   https://github.com/thunlp/PEVL .   1 Introduction   Recent progress on self - supervised learning has   led to powerful vision - language pre - training ( VLP )   models that achieve state - of - the - art performance on   a wide range of cross - modal tasks ( Lu et al . , 2019;Li et al . , 2020c ; Radford et al . , 2021 ; Zhang et al . ,   2021 ; Kamath et al . , 2021 ) . Typically , VLP models   are first pre - trained on large - scale image - text data   to learn universal cross - modal representations , and   then fine - tuned to adapt to downstream tasks ( Bom-   masani et al . , 2021 ) . While most traditional VLP   models heavily rely on external object detectors to   obtain the visual inputs ( Lu et al . , 2019 ; Su et al . ,   2020 ; Li et al . , 2020c ; Zhang et al . , 2021 ) , recently   there is a growing interest in VLP models that re-   move the reliance on object detectors due to their   superior computation efficiency and competitive   performance ( Li et al . , 2021 ; Kim et al . , 2021 ; Rad-   ford et al . , 2021 ; Kamath et al . , 2021 ) .   However , the removal of object detectors also   deprives the capability of VLP models in explicit   object modeling . The drawback hinders successful   handling of vision - language ( VL ) tasks which are   inherently object - centric , where deep understand-   ing of objects and their interactions plays an essen-   tial role ( Antol et al . , 2015 ; Plummer et al . , 2015 ;   Krishna et al . , 2017 ; Hudson and Manning , 2019 ) .   Therefore , it is typically difficult for detector - free   VLP models to handle various position - sensitive   tasks ( i.e. , tasks that demand explicit object posi-   tions as input or output ) , such as visual common-   sense reasoning ( Zellers et al . , 2019 ) , visual re-   lation detection ( Krishna et al . , 2017 ) , referring   expression comprehension ( Yu et al . , 2016 ) and   phrase grounding ( Plummer et al . , 2015 ) , which   greatly undermines their generality and practicality   as foundation models ( Bommasani et al . , 2021 ) .   For tasks that do not require explicit object mod-   eling , such as visual question answering ( Antol11104   et al . , 2015 ) , previous works have shown that in-   troducing explicit grounding can also lead to better   performance and robustness ( Anderson et al . , 2018 ;   Huang et al . , 2019 ) , which can hardly be achieved   in current detector - free VLP models .   In a preliminary exploration , MDETR ( Kamath   et al . , 2021 ) proposes to enhance detector - free VLP   models by regressing object positions with Trans-   former decoders , serving position - output tasks such   as referring expression comprehension . However , it   is still unknown how to deal with various position-   input tasks , such as visual commonsense reasoning   and visual relation detection . Moreover , during   fine - tuning , task - specific classification heads are   typically introduced , resulting in a significant gap   between pre - training and fine - tuning , which hin-   ders taking full advantage of pre - trained model   capabilities in downstream tasks .   In this work , we propose PEVL that enhances   the pre - training and prompt tuning of VLP models   with explicit object position modeling . Inspired by   the recent Pix2Seq ( Chen et al . , 2022 ) that casts ob-   ject detection as a language modeling task , PEVL   reformulates object positions as discrete tokens ,   and learns the joint distribution of object positions   and language in a unified language modeling frame-   work , as shown in Figure 1 . Specifically , PEVL   exploits explicit region - text alignments in existing   VL datasets . Discretized position tokens are placedafter object text tokens to indicate the object loca-   tions in both pre - training and prompt tuning :   ( 1 ) During pre - training , PEVL learns explicit   VL alignment based on a generalized masked lan-   guage modeling ( GMLM ) task , where the model   recovers masked text tokens and position tokens   from cross - modal context . We note that although   the discretization of positions enables their uni-   fied modeling with language , it also eliminates   the ordering of positions as compared with tradi-   tional continuous regression methods ( i.e. , predict-   ing nearby and faraway positions to ground - truth   are equally punished ) . The problem is exacerbated   by the inevitable small disturbances in the human   annotation of bounding boxes . To address the chal-   lenge , we present a novel ordering - aware objec-   tive for masked position reconstruction , which as-   signs larger probabilistic soft labels for nearby po-   sition tokens , and therefore retains the ordering .   ( 2 ) During prompt tuning , PEVL can support var-   ious downstream VL tasks in a flexible prompt   tuning framework , where VL tasks are addressed   by the reused GMLM head in a fill - in - the - blank   paradigm . In this way , PEVL maximally mitigates   the gap between pre - training and tuning , and better   stimulates the pre - trained model capabilities .   We conduct comprehensive experiments on five   VL tasks , including position- output , input and   insensitive tasks . Experimental results show that11105through position enhancement , PEVL enables state-   of - the - art performance of detector - free VLP mod-   els on position - sensitive tasks such as referring   expression comprehension and phrase grounding ,   and also improves the performance on position-   insensitive tasks with grounded inputs .   Our contributions are threefold : ( 1 ) We unify   the modeling of positions and language in a lan-   guage modeling framework , which enhances both   pre - training and prompt tuning of VLP models . ( 2 )   We present a novel ordering - aware objective that re-   tains the ordering of position tokens and avoids the   influence of position annotation noise . ( 3 ) We con-   duct comprehensive experiments on five position-   sensitive and insensitive VL tasks , which demon-   strates the effectiveness of the proposed model .   2 Preliminary   In principle , the PEVL framework is orthogonal   to VLP architectures and can be built on any VLP   models to achieve position enhancement . In this   work , without loss of generality , we adopt AL-   BEF ( Li et al . , 2021 ) as the model backbone , which   is a representative detector - free VLP model that   achieves state - of - the - art performance on many VL   tasks . We briefly introduce the pre - training and   fine - tuning procedure of ALBEF , and refer readers   to the original paper for additional details .   Pre - training . The ALBEF architecture is com-   posed of two unimodal encoders followed by a   cross - modal encoder . Images and text are first   encoded using a vision Transformer ( Dosovitskiy   et al . , 2021 ) and a text Transformer ( Vaswani et al . ,   2017 ) respectively , and then fused with a cross-   modal Transformer . The model is pre - trained with   three tasks , including masked language model-   ing , image - text contrastive learning and image-   text matching . ( 1 ) Masked language modeling   aims to recover masked text tokens from the cross-   modal context . ( 2 ) Image - text contrastive learning   aligns the intermediate unimodal representations   of image - text pairs by a contrastive loss . ( 3 ) Image-   text matching classifies whether an image - text pair   is aligned based on the [ CLS ] token of the cross-   modal Transformer . To alleviate the noise in the   pre - training text , a momentum model is maintained   based on the moving - average of model parameters   to provide pseudo - targets as additional supervision .   Fine - tuning . During fine - tuning , ALBEF intro-   duces new classification heads or decoders to han-   dle VL tasks , which leads to significant gap frompre - training . The gap hinders taking full advantage   of pre - trained capabilities for downstream tasks .   Moreover , since object positions can not be explic-   itly modeled , detector - free VLP models typically   struggle on position - sensitive tasks , which greatly   undermines their generality and practicality .   3 Methodology   We introduce the PEVL framework , including the   position reformulation for VL models , and position-   enhanced VL pre - training and prompt tuning .   3.1 Reformulating Positions for VL Models   Cross - modal position modeling that explicitly con-   nects image regions and text units underpins a   broad range of VL tasks . To enable strong cross-   modal position modeling capability of VLP mod-   els , a primary challenge is to find a good position   formulation that can be ( 1 ) easily integrated and   unified into mainstream VLP models , and can be   ( 2 ) flexibly prompt - tuned in various downstream   tasks with minimal gap from pre - training as well .   To this end , previous works attempt to indi-   cate image regions by introducing region embed-   dings ( Cho et al . , 2021 ) or colors ( Yao et al . , 2021b )   that correspond to pre - defined text tokens , which   require pre - detected image regions from costly ex-   ternal object detectors . In contrast , we note that for   the mainstream VLP models with vision Transform-   ers ( Dosovitskiy et al . , 2021 ) as visual encoders ,   image patch positions are already well indicated by   positional embeddings ( Vaswani et al . , 2017 ) , and   therefore no special treatments are in fact needed   for visual position coordination .   To explicitly express visual positions in text , in-   spired by Pix2Seq ( Chen et al . , 2022 ) that casts ob-   ject detection as a language modeling task , PEVL   reformulates object bounding box coordinates as   discrete position tokens . The position tokens can be   easily unified with text tokens in a language model-   ing framework , where the vocabulary includes both   text and position tokens , and can also be easily pre-   trained with existing VLP techniques . In addition   to the convenience in pre - training , another impor-   tant advantage is that VLP models can be easily   prompt - tuned to handle various position- sensitive   and insensitive VL tasks with minimal gap from   pre - training , as shown in Figure 1 .   Specifically , given an image - text pair for pre-   training ( I , T ) , we exploit the composing object   texts and their bounding boxes O={(c , b)},11106where cis the object text ( e.g. , person ) in text T ,   andb= ( x , y , x , y)is the coordinates   of the corresponding bounding box . The bounding   box coordinates are discretized into position tokens   as⌊Mx / w ⌋and⌊My / h⌋ , where wandhare the   width and height of the image , and Mis the total   number of the position tokens . Intuitively , a larger   number of position tokens will lead to a coordinate   system with higher resolution , but will be more   compute- and data- expensive to learn . Finally the   position tokens are placed after the corresponding   object text cinTto explicitly indicate the object   position . Note that two special tokens “ < ” and “ > ”   are introduced to indicate the start and end of posi-   tion tokens , which are useful in prompting models   to produce position tokens in position - output tasks .   3.2 Position - enhanced VL Pre - training   After unifying positions and text in a language mod-   eling framework , PEVL can be easily integrated   into existing VLP models . To effectively learn   the position and text interactions , in addition to   the image - text contrastive and image - text matching   tasks ( see Section 2 ) , we present a novel gener-   alized masked language modeling ( GMLM ) pre-   training task , which recovers both masked text and   position tokens based on a generalized vocabulary   Vthat includes both types of tokens . We introduce   two main components of the GMLM task , includ-   ing masking strategy and reconstruction objective .   Masking Strategy . While the text tokens are   usually masked with low ratios ( e.g. , 15 % ) in tra-   ditional MLM tasks ( Devlin et al . , 2019 ; Li et al . ,   2021 ) , we find that the same masking strategy can-   not well serve position modeling . The reason is that   object positions are relatively low - level signals , and   therefore models can easily reconstruct the masked   position tokens when the masking ratio is low . For   example , reconstructing a single masked position   token ( e.g. , x ) given the other three unmasked   ones will be largely equivalent to enclosing an ob-   ject by moving a corner of the bounding box in a   straight line , which does not require deep under-   standing of the VL semantics . Similar problems   are also discussed in self - supervised learning on   images ( He et al . , 2022 ) .   To address the issue , we adopt high masking   ratios for position tokens , and encourage mask-   ing a more complete subset of object positions .   Specifically , for each object , we randomly mask   nof its four position tokens with 0.25probabil - ity , where n= 1,2,3,4 . For example , for 25 %   of the time , the four object positions ( i.e. , n= 4 )   are completely masked for reconstruction . For text   tokens , we follow the 15 % masking strategy in pre-   vious works ( Li et al . , 2021).In this way , models   are forced to learn high - level semantic interactions   among image regions , text and position tokens .   Reconstruction Objective . Traditional MLM   tasks typically adopt a one - hot target for token re-   construction . However , we note that the one - hot   target essentially eliminates the ordering of the po-   sitions : If the position prediction is not exactly   correct , predicting nearby and faraway positions   to ground - truth are equally punished . The prob-   lem is exacerbated by the inevitable small distur-   bances in the human annotation process of bound-   ing boxes , which confuses models in discrete posi-   tion learning . To address the problem , we present a   novel ordering - aware objective for position recon-   struction that assigns larger probabilistic soft labels   for nearby position tokens . Specifically , given a   masked position token , the unnormalized proba-   bilistic label yfor each position token pdecreases   exponentially with its distance to the ground - truth :   where |p−p|is the distance between pand the   ground - truth p , andαis a hyperparameter control-   ling the decay rate . The normalized probabilistic   label ˜yis then used to compute the ordering - aware   objective for position tokens :   The probability of position tokens is given by   the GMLM head as :   where h is the hidden representation of the   [ MASK ] token , and pis the representation of po-   sition token pin the GMLM head . In this way , the   objective retains the ordering of position tokens and   avoids the influence of position annotation noise .   For the text token reconstruction loss L , we follow   the traditional implementation in ALBEF ( Li et al . ,   2021 ) . The final GMLM loss is the weighted sum   of the loss for position token reconstruction and11107text token reconstruction : L = λL+L ,   where λis a weighting hyperparameter .   Pre - training Corpora . PEVL exploits explicit   object position annotation in VL datasets for po-   sition learning . The pre - training corpora consist   of referring expressions ( Yu et al . , 2016 ; Mao   et al . , 2016 ) , Flickr30k ( Plummer et al . , 2015 ) ,   GQA ( Hudson and Manning , 2019 ) , VCR ( Zellers   et al . , 2019 ) and Visual Genome ( Krishna et al . ,   2017 ) , with 4.7 M image - text pairs in total . Follow-   ing Chen et al . ( 2020 ) , we remove the images in   the downstream test and validation sets from the   pre - training corpora .   3.3 Position - enhanced VL Prompt Tuning   To adapt VLP models to downstream tasks , pre-   vious works typically introduce new classification   heads or even Transformer decoders ( Kamath et al . ,   2021 ; Li et al . , 2021 ; Chen et al . , 2020 ) , lead-   ing to significant gap from pre - training . Recent   works in pre - trained language models have shown   that a consistent tuning approach with pre - training   ( i.e. , prompt tuning ) can better stimulate the pre-   trained capability in downstream tasks ( Schick and   Schütze , 2021 ; Gao et al . , 2021 ; Liu et al . , 2021 ) .   However , it is still unknown whether and how   VLP models can be prompt tuned to support both   position- sensitive and insensitive VL tasks .   In this context , a crucial advantage of unifying   positions with language is that , VLP models can   be easily prompt - tuned to handle various VL tasks   based on the reused GMLM head with minimal gap   from pre - training . We divide VL tasks according   to the role of positions , including position - output   tasks , position - input tasks , and position - insensitive   tasks . Here we introduce the main prompt tuning   procedure for position - sensitive tasks . In our exper-   iments , we show that position - insensitive tasks can   also benefit from well - grounded inputs in PEVL   framework ( see Section 4.1 ) .   Position - output Tasks demand positions as task   outputs ( e.g. , predicting the positions of objects   described by text ) , such as referring expression   comprehension and phrase grounding . To handle   position - output tasks , we simply place four con-   secutive [ MASK ] tokens wrapped by “ < ” and “ > ”   after object texts to be grounded for position pre-   diction . ( 1 ) Referring Expression Comprehension .   Since the task requires locating the head noun , we   place the mask tokens after the first object text for   position prediction . ( 2 ) Phrase Grounding . Sincethe task requires locating all objects , mask tokens   are placed after each object text . After placing   mask tokens , the model is prompt - tuned to produce   position tokens with reused GMLM head based on   the ordering - aware objective as in Equation 2 .   Position - input Tasks require a mixture of po-   sition and text ( i.e. , grounded text ) as task inputs ,   such as visual commonsense reasoning and visual   relation detection . To handle position - input tasks ,   PEVL first explicitly indicates the object positions   in input text ( see Section 3.1 ) , and then produces   answers in a fill - in - the - blank paradigm based on   the reused GMLM head .   Visual Commonsense Reasoning . Given a ques-   tion , models are asked to choose the answer sen-   tence ( and rationale ) from multiple candidates .   For answer selection , the question qand answer   candidate aare put in a prompt template as :   “ q aanswer : [ MASK ] ” . Then the model can be   prompted to decide which token t∈ { yes , no}is   more proper to reconstruct the [ MASK ] token . An-   other plausible alternative is to reuse the image - text   matching head to discriminate whether the image is   aligned with the concatenated question and answer .   The intuition is that a question concatenated with   the correct answer can better match the image con-   tent than concatenated with a wrong answer . In our   experiments , we find that the latter approach yields   better performance on VCR . Despite the essential   equivalence of the two prompting approaches ( i.e. ,   classifying special tokens in the last layer into bi-   nary labels with reused pre - trained heads ) , image-   text matching task focuses more on the holistic   matching between cross - modal signals during pre-   training , which better fits the VCR task containing   typically long text answers .   Visual Relation Detection . Given an object pair   ( s , o)(e.g . , woman , horse ) in the image , models are   required to classify their semantic relation r(e.g . ,   watching , riding ) . We design the prompt template   as : “ Thesis[MASK ] theo ” . Then the model is   prompted to produce the relational tokens from the   relation set with reused GMLM head . To deal with   relations that consist of different numbers of tokens ,   we pad relational tokens to a maximum length l ,   and place lconsecutive masks in the template for   relation prediction . We also include a special re-   lation no relation with in the relation set , which   indicates no relation between the object pair . Dur-11108   ing inference , the score of relation ris given by   the average log probability of non - padding tokens :   s=/summationtextlogP([MASK]=r ) , where   [ MASK]is the i - th mask token , and ris the   i - th token of r. An important advantage of prompt   tuning for the task is that , the large number of long-   tail relations can be better learned thanks to the rich   knowledge in VLP models .   4 Experiments   We evaluate PEVL on five popular VL tasks . The   models are in base size unless otherwise specified .   4.1 Main Results   Referring Expression Comprehension . We adopt   three popular datasets for the task , including Re-   fCOCO , RefCOCO+ ( Yu et al . , 2016 ) and Ref-   COCOg ( Mao et al . , 2016 ) . We use accuracy@0.5   as the evaluation metric ( Kamath et al . , 2021 ) . For   baselines , we compare with state - of - the - art models   for the task , and VLP models with large - size back-   bones . We report the weakly supervised results   from ALBEF ( Li et al . , 2021 ) , which uses GRAD-   CAM ( Selvaraju et al . , 2017 ) heat map to rank the   object candidates from external detectors .   From the experimental results in Table 1 , we   have the following observations : ( 1 ) PEVL outper-   forms all baseline models , achieving a new state-   of - the - art on all three datasets for the task . Specifi-   cally , the base - size PEVL outperforms the state - of-   the - art regression - based MDETR by 2.9 absolute   points on the RefCOCO+ testA set , and large - size   VLP models that use external detector feature in-   puts , such as ERNIE - ViL and VILLA . ( 2 ) PEVL   significantly improves the ALBEF backbone by   explicit object position modeling , effectively ad-   dressing the shortcoming in position - output tasks.11109   Phrase Grounding . We perform experiments   on the Flickr30k entities dataset ( Plummer et al . ,   2015 ) . Following MDETR , we adopt merged - box   accuracy@0.5 as the evaluation metric , and com-   pare our model with the state - of - the - art baselines   for the task ( Kamath et al . , 2021 ; Yang et al . , 2022 ) .   From Table 1 we observe that PEVL achieves a   new state - of - the - art on the phrase grounding task   in grounding multiple objects in text . The results   show that PEVL can effectively integrate positions   with language to achieve competitive performance   for various position - output tasks .   Visual Commonsense Reasoning . We adopt   the popular VCR benchmark ( Zellers et al . , 2019 ) ,   which provides human - annotated positions for ob-   jects . We report the accuracy of predicting the an-   swer ( Q →A ) , rationale ( QA →R ) and both ( Q →   AR ) . We compare with task - specific baselines and   strong VLP models . For fair comparisons , we fur-   ther pre - train ALBEF baseline on the same corpora   as PEVL in all experiments . From the results in Ta-   ble 2 , we observe that PEVL significantly improves   the ALBEF backbone ( e.g. , by 3.9 absolute points   in Q→AR ) , achieving comparable performance   to strong UNITER equipped with external object   detectors . While the results are not state - of - the - art   on the VCR benchmark , they are quite reasonable   considering the current literature . The results show   that PEVL can effectively provide clues for com-   plex reasoning through grounded inputs . Visual Relation Detection . We evaluate PEVL   on the widely used Visual Genome dataset ( Krishna   et al . , 2017 ) , which contains 50 visual relation   types . Given the human - annotated positions and   labels of an object pair , models are required to pre-   dict the relations . Following previous works ( Tang   et al . , 2020 ; Lin et al . , 2020 ) , we report the re-   call@K ( R@K ) and mean recall@K ( mR@K ) as   evaluation metrics . We compare PEVL with state-   of - the - art baselines with detector feature inputs .   From Table 3 , we observe that : ( 1 ) Without task-   specific designs or heuristics , PEVL achieves com-   petitive performance in both R@K and mR@K.   The results show that PEVL can effectively stimu-   late the knowledge in VLP models for both frequent   and long - tail relations through prompt tuning . ( 2 )   ALBEF struggles on the visual relation detection   task , since the positions of the target object pair can-   not be informed . In contrast , PEVL can effectively   integrate the object position information through   simple position tokens for relation prediction .   Visual Question Answering . For position-   insensitive tasks such as visual question answering ,   object positions are not required to be explicitly   modeled . However , we argue that explicit object   position modeling can provide fine - grained clues   for complex question reasoning . Specifically , we   are interested in the question : Can VLP models   benefit from grounded text for answering complex   questions in PEVL framework?11110   In principle , to achieve explicit position augmen-   tation , VQA can be decomposed into two position-   sensitive stages , including an object grounding   ( position - output ) stage and a question answering   ( position - input ) stage . However , in our experi-   ments , we find that the unneglectable errors in   current visual grounding models constitute a bot-   tleneck for such a two - stage model . Therefore ,   we turn to an ideal experiment , where the ground-   truth object positions are available . Specifically ,   we adopt the object position annotation provided   by the GQA dataset ( Hudson and Manning , 2019 ) .   We explicitly indicate the position of each object   in a similar approach as in position - input tasks ( see   Section 3.3 ) . Then the position - enhanced ques-   tion is put into the prompt template : “ qanswer :   [ MASK ] ” . Finally models are asked to generate   answer tokens from answer candidate set . We use   the same approach as in visual relation detection to   cope with multi - token answers .   From Table 4 we observe that with grounded in-   puts , PEVL significantly improves the performance   of ALBEF backbone in compositional question an-   swering . The results show that object grounding is   still one of the key obstacles in VQA , and PEVL   can effectively utilize grounded questions for VQA   in a simple prompt tuning framework . To investi-   gate which type of questions benefit from grounded   inputs , we divide GQA validation set according to   the question types from MDETR . From Table 5 ,   we can see that high - quality grounding signals im-   prove the performance on all question types . In-   terestingly , relation and attribute - based questions   benefit more from grounded text than object - based   questions , indicating the fundamental role of object   modeling in reasoning over complex questions .   4.2 Experimental Analysis   Ablation Study . We ablate key components of   PEVL , including prompt tuning , ordering - aware   objective , and position tokens to investigate their   contribution . From the results in Table 6 , we can   see that all components contribute to the final per-   formance . Specifically , position enhancement and   prompt tuning are essential for PEVL to perform   position - output tasks . Prompt tuning can also be   helpful in learning long - tail relations for visual   relation detection , which is consistent with the   results from previous works ( Yao et al . , 2021b ) .   The ordering - aware objective contributes more to   position - output tasks than position - input tasks .   Influence of Masking Strategies . We investi-   gate the influence of different masking strategies   for position tokens during pre - training . Specifi-   cally , for baselines , the position tokens are inde-   pendently chosen with a certain probability during   pre - training , where the ratios of masked , replaced   and unchanged tokens for the chosen token are kept   identical to BERT ( Devlin et al . , 2019 ) . We report   the performance and the number of epochs required   in the intermediate pre - training on the RefCOCO   dataset . From the results in Table 7 , we observe   that our masking strategy achieves both better per-   formance and faster convergence . The results show   that a high masking ratio and a more complete sub-   set of masked positions are both important for good   position learning results .   Case Study . We visualize the position predic-   tions on the validation sets of RefCOCO+ , Ref-   COCOg and Flicker30k . Previous visual local-   ization models are either based on continuous re-11111gression ( Kamath et al . , 2021 ) , or limited to non-   language tasks ( Chen et al . , 2022 ) . From Fig-   ure 2 , we can see that discretized positions can be   closely integrated with language in Transformers   to achieve strong visual reasoning and localization   results . Similar to regression - based models , the   localization of small objects ( e.g. , wheels in the   right figure ) can also be challenging .   5 Related Work   Position Enhancement for VLP . Object position   modeling underpins a wide range of VL tasks . To   deal with position - output tasks , some works ( Ka-   math et al . , 2021 ; Gupta et al . , 2022 ; Yang et al . ,   2022 ) propose to perform object position predic-   tion using Transformer decoders , but are unable to   handle various position - input tasks . To explicitly   indicate position inputs for VLP models , previous   works explored learning region embeddings ( Cho   et al . , 2021 ) , or color - based cross - modal corefer-   ential markers ( Yao et al . , 2021b ) , but rely on ex-   ternal object detectors . MERLOT ( Zellers et al . ,   2021 ) also proposes to highlight objects in images   with colors for position - input tasks . X - VLM ( Zeng   et al . , 2022 ) aligns multi - grained concepts in text   and image for VLP models . In comparison , PEVL   supports both position- input and output VL tasks   in a unified prompt tuning framework .   Prompt Tuning . Prompt tuning for pre - trained   language models is in rapid growth in natural lan-   guage processing ( Petroni et al . , 2019 ; Raffel et al . ,   2020 ; Brown et al . , 2020 ; Schick and Schütze ,   2021 ; Gao et al . , 2021 ; Qin and Eisner , 2021 ; Liu   et al . , 2021 ; Yao et al . , 2022 ) . Recently there is   also growing interest in prompt tuning VLP models .   Most existing works prompt tune contrastively pre-   trained image - text matching models ( Radford et al . ,   2021 ; Jia et al . , 2021 ) for recognition tasks ( Zhou   et al . , 2022 ; Rao et al . , 2022 ; Wang et al . , 2021 ; Gu   et al . , 2022 ; Xie and Zheng , 2021 ; Ju et al . , 2022 ) .   CPT ( Yao et al . , 2021b ) prompt tunes VLP models   with color - based prompts , and achieves promising   results for zero- and few - shot tasks . Some works   perform pre - training and VL tasks using identical   Transformer decoders in an auto - regressive fash-   ion ( Wang et al . , 2022b ; Cho et al . , 2021 ; Yang   et al . , 2022 ; Tsimpoukelli et al . , 2021 ) , which   avoids the gap between pre - training and tuning ,   but are typically limited in performance due to the   unidirectional architecture.6 Conclusion and Future Work   In this work , we present PEVL that enhances   the pre - training and prompt - tuning of detector-   free VLP models with unified position and lan-   guage modeling . Comprehensive experimental re-   sults demonstrate the effectiveness of the proposed   model . Future works include exploring weakly su-   pervised signals for position and language learning   without human annotation .   7 Acknowledgement   This work is supported by the National Key R&D   Program of China ( No . 2020AAA0106502 ) , In-   stitute Guo Qiang at Tsinghua University and   NExT++ project from the National Research Foun-   dation , Prime Minister ’s Office , Singapore under   its IRC@Singapore Funding Initiative .   Yuan Yao designed the framework and exper-   iments , and wrote the paper . Qianyu Chen con-   ducted the experiments . Ao Zhang and Wei Ji par-   ticipated in the discussion . Zhiyuan Liu , Tat - Seng   Chua and Maosong Sun advised the project .   8 Limitations   We identify several key limitations of PEVL that   are promising for future explorations .   Computation Cost . To explicitly model object   positions and text in a unified framework , we intro-   duce four additional position tokens for each target   object , which requires more computation cost . Im-   proving the computation efficiency of additional   position tokens is an important direction for future   improvements .   Object Annotation . Similar to other explicit ob-   ject position modeling VLP models , PEVL requires   manual object annotation in multi - modal datasets .   It will be promising to explore pre - training tasks   that can learn position tokens with less supervi-   sion . For example , VLP models can be boot-   strapped from small - scale human annotations and   large - scale predicted annotations .   References111121111311114   A Pre - training Details   We provide pre - training details and statistics of the   pre - training corpora .   Implementation details . Our backbone consists   of a 6 - layer text Transformer encoder , a ViT - B/16   visual encoder , and a 6 - layer cross - modal Trans-   former encoder ( commonly referred to as base size   in the literature ) , with 209.5 M parameters in to-   tal . The backbone is open - sourced for research   usage . In pre - training , we initialize PEVL with pre-   trained parameters from ALBEF for computation   efficiency . PEVL is pre - trained with learning rate   8e-5 , batchsize 512 on 32 NVIDIA V100 GPUs for   5 epochs . The number of position tokens is 512 ,   with decay rate α= 0.25 , and weighting hyperpa-   rameter λ= 2 in ordering - aware reconstruction .   The hyperparameters are selected by grid search on   the validation sets . For data augmentation , follow-   ing MDETR ( Kamath et al . , 2021 ) , we augment   images with random size crop . We also follow   Pix2Seq ( Chen et al . , 2022 ) to adopt horizontal flip   to augment images , where “ left ” and “ right ” in text   are swapped after flip to ensure the semantic cor-   rectness . Previous works suggest that an interme-   diate in - domain pre - training can better adapt VLP   models to downstream tasks ( Chen et al . , 2020 ) .   We therefore conduct an intermediate pre - training   before tuning on each downstream task .   Pre - training Corpora . The pre - training corpora   consist of referring expressions ( Yu et al . , 2016 ;   Mao et al . , 2016 ) , Flickr30k ( Plummer et al . , 2015 ) ,   GQA ( Hudson and Manning , 2019 ) , VCR ( Zellers   et al . , 2019 ) and Visual Genome dense captions ( Kr-   ishna et al . , 2017 ) , with 4.7 M image - text pairs and   210 K images in total . We provide the detailed   statistics of the datasets in Table 8.11115   B Downstream Tasks   We provide details of dataset , prompt tuning and   baseline models for each downstream task .   B.1 Referring Expression Comprehension   Datasets . RefCOCO ( Yu et al . , 2016 ) is collected   from a referential game between two players . The   dataset is split into train , validation , testA and   testB sets , containing 120,624 , 10,834 , 5,657 and   5,095 expression - object pairs respectively . Ref-   COCO+ ( Yu et al . , 2016 ) is also constructed in an   interactive fashion , and contains 120,191 , 10,758 ,   5,726 and 4,889 expression - object pairs in train ,   validation , testA and testB sets respectively . Re-   fCOCOg ( Mao et al . , 2016 ) is built in a non-   interactive way , and contains 80,512 , 4,896 and   9,602 expression - object pairs in train , validation   and test sets respectively .   Prompt Tuning . We tune the model with learn-   ing rate 1e-5 , weight decay 0.02 , and batchsize 32   for 10 epochs . Following previous works ( Dosovit-   skiy et al . , 2021 ; Li et al . , 2021 ) , we use a higher   image resolution of 512 in downstream tuning . The   hyperparameters are selected by grid search on the   validation set for all experiments . During infer-   ence , we select the position token with the largest   reconstruction score for each of the four masked   tokens .   Baselines . We compare with state - of - the - art base-   lines , including MAttNet ( Yu et al . , 2018a ) ,   DDPN ( Yu et al . , 2018b ) , VL - T5 ( Cho et al . , 2021 ) ,   ViLBERT ( Lu et al . , 2019 ) , UNITER ( Chen et al . ,   2020 ) , VL - BERT ( Su et al . , 2020 ) , VinVL ( Zhang   et al . , 2021 ) , VILLA ( Gan et al . , 2020 ) , ERNIE-   ViL ( Yu et al . , 2021 ) , MDETR ( Kamath et al . ,   2021 ) , and ALBEF ( Li et al . , 2021 ) . We also   compare with two concurrent works that achieve   competitive performance on visual grounding   tasks , including UniTAB ( Yang et al . , 2022 ) and   OFA ( Wang et al . , 2022a ) . We adopt accuracy@0.5   as the evaluation metrics , where an expression is   considered correctly grounded if the intersection   over union between the top prediction and ground   truth is greater than 0.5B.2 Phrase Grounding   Datasets . Flickr30k entities dataset ( Plummer   et al . , 2015 ) is collected through annotating 276 K   entities in the 158 K captions from Flickr30k with   object bounding boxes . The dataset is split into   train , validation and test sets , with 148,915 , 14,433 ,   14,481 noun phrases respectively .   Prompt Tuning . We tune the model with learn-   ing rate 1e-5 , weight decay 0.02 , and batchsize 128   for 10 epochs . Following previous works ( Kamath   et al . , 2021 ; Yang et al . , 2022 ) , we evaluate our   model under the merged - boxes protocol , where the   boxes of a phrase ( e.g. , crowd ) referring to multiple   objects are merged by their union . We use resolu-   tion 512 during downstream tuning . During tuning   and inference , our model predicts the bounding box   of each object separately .   Baselines . We compare with state - of - the - art   baselines , including DDPN ( Yu et al . , 2018b ) ,   UniTAB ( Yang et al . , 2022 ) and MDETR ( Ka-   math et al . , 2021 ) . UniTAB ( Yang et al . , 2022 )   performs multi - task fine - tuning with several down-   stream task datasets . We adopt accuracy@0.5 as   the evaluation metrics .   B.3 Visual Relation Detection   Datasets . We use Visual Genome ( Krishna et al . ,   2017 ) dataset for the evaluation of this task . The   dataset is split into train , validation and test sets ,   with 65,651 , 5,000 , 32,422 images respectively .   The of object categories and relation categories in   the dataset are 150 and 50 respectively .   Prompt Tuning . We tune the model with learn-   ing rate 2e-5 , weight decay 0.02 , and batchsize 256   for 5 epochs . The resolution of images is 512 . The   ratio of negative samples ( i.e. , no relations between   the object pair ) and positive samples is 3:1 .   Baselines . We compare with strong baselines ,   including MotifNet ( Zellers et al . , 2018 ) , Unbi-   ased ( Tang et al . , 2020 ) , GPS - Net ( Lin et al . ,   2020 ) , MSDN ( Xu et al . , 2017 ) , VCTree ( Tang   et al . , 2019 ) , DT2 - ACBS ( Desai et al . , 2021 ) , Vi-   sualDS ( Yao et al . , 2021a ) , and IETrans ( Zhang11116et al . , 2022 ) . For evaluation metrics , we adopt   Recall@K(R@K ) , which is the ratio of correct re-   lationship in the top K confident relationship pre-   dictions , and mean Recall@K(mR@K ) , which is   the average recall upon all predicate classes .   B.4 Visual Commonsense Reasoning   Datasets . VCR dataset ( Zellers et al . , 2019 ) is   collected through creating questions requiring com-   monsense reasoning by workers for given images   from 110 K movie scenes . The dataset is split into   train , validation , test sets with 212,923 , 26,534 ,   25,263 questions respectively .   Prompt Tuning . We tune the model with learn-   ing rate 1e-5 , weight decay 0.02 , and batchsize   4,096 for 5 epochs . We use resolution 512 in down-   stream tuning . PEVL predicts binary labels indicat-   ing the whether candidate is correct , given the text   of question concatenated with answer , or question ,   answer concatenated with rationale . In Q →AR ,   we first predict an answer from four answer candi-   dates , and then pick a rationale from four rationale   candidates based on the predicted answer .   Baselines . We compare with strong baselines , in-   cluding R2C ( Zellers et al . , 2019 ) , TAB - VCR ( Lin   et al . , 2019 ) and strong VLP models including   VisualBERT ( Li et al . , 2020b ) , ViLBERT ( Lu   et al . , 2019 ) , Unicoder - VL ( Li et al . , 2020a ) , VL-   BERT ( Su et al . , 2020 ) , B2T2 ( Alberti et al . , 2019 )   and UNITER ( Chen et al . , 2020 ) . We report the ac-   curacy of predicting the answer ( Q →A ) , rationale   ( QA→R ) and both ( Q →AR ) .   B.5 Visual Question Answering   Datasets . GQA dataset ( Hudson and Manning ,   2019 ) is collected through automatically generating   questions and answers with functional programs   based on the scene graphs in Visual Genome . The   dataset is split into train , validation , test - dev , and   test sets , with 14,305,356 , 2,011,853 , 172,174 and   1,340,048 questions respectively .   Prompt Tuning . We tune the model with learn-   ing rate 1e-5 , weight decay 0.02 , batchsize 256   for 5 epochs . Following MDETR ( Kamath et al . ,   2021 ) , the intermediate pre - training is conducted   on the unbalanced train set , and prompt tuning on   the balanced train set . The resolution of image   is 384 . We infer the answers based on the 1,853   candidates from Kamath et al . ( 2021).Baselines . We compare with existing methods re-   ported on the GQA balanced validation dataset ,   including LXMERT ( Tan and Bansal , 2019 ) ,   BAN ( Kim et al . , 2018 ) , CTI ( Do et al . , 2019 )   and CFR ( Nguyen et al . , 2022 ) .   C Ethical Considerations   Potential risks of this work lie in ( 1 ) privacy is-   sues of the pre - training images and text from the   Web , ( 2 ) misuse of the model ( e.g. , visual relation   detection for monitoring human activity ) , and ( 3 )   toxic model outputs . The initial version of this pa-   per is released at https://arxiv.org/abs/   2205.11169 . The picture in Figure 1 is obtained   from the RefCOCO dataset.11117