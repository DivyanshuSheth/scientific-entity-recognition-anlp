  Songlin Yang , Wei Liu , Kewei Tu   School of Information Science and Technology , ShanghaiTech University   Shanghai Engineering Research Center of Intelligent Vision and Imaging   { yangsl,liuwei4,tukw}@shanghaitech.edu.cn   Abstract   Hidden Markov Models ( HMMs ) and Proba-   bilistic Context - Free Grammars ( PCFGs ) are   widely used structured models , both of which   can be represented as factor graph grammars   ( FGGs ) , a powerful formalism capable of de-   scribing a wide range of models . Recent re-   search found it beneﬁcial to use large state   spaces for HMMs and PCFGs . However , in-   ference with large state spaces is computation-   ally demanding , especially for PCFGs . To   tackle this challenge , we leverage tensor rank   decomposition ( aka . CPD ) to decrease infer-   ence computational complexities for a sub-   set of FGGs subsuming HMMs and PCFGs .   We apply CPD on the factors of an FGG   and then construct a new FGG deﬁned in the   rank space . Inference with the new FGG   produces the same result but has a lower   time complexity when the rank size is smaller   than the state size . We conduct experiments   on HMM language modeling and unsuper-   vised PCFG parsing , showing better perfor-   mance than previous work . Our code is pub-   licly available at https://github.com/   VPeterV / RankSpace - Models .   1 Introduction   Hidden Markov Models ( HMMs ) and Probabilistic   Context - Free Grammars ( PCFGs ) are widely used   structured models in natural language processing .   They can both be represented as factor graph gram-   mars ( FGGs ) ( Chiang and Riley , 2020 ) , which are   a powerful tool to describe a wide range of mod-   els , allowing exact and tractable inference in most   situations of interest .   Over - parameterization has been shown beneﬁ-   cial in facilitating optimization of deep networks   ( Arora et al . , 2018 ; Xu et al . , 2018 ; Du et al . ,   2019 ) . Buhai et al . ( 2020 ) found that over-   parameterization is also helpful in learning latentvariable models by increasing the number of hidden   states . Buhai et al . ( 2020 ) argued that it is impor-   tant to study over - parameterization in structured   settings because structured latent variable models   are more suitable to model real - word phenomena   which exhibit complex dependencies . HMMs and   PCFGs are typical structured latent variable mod-   els , and recently researchers have found it beneﬁ-   cial to use large state spaces for HMMs and PCFGs   ( Dedieu et al . , 2019 ; Chiu and Rush , 2020 ; Yang   et al . , 2021b ; Chiu et al . , 2021 ) . However , struc-   tured inference with large state spaces is computa-   tionally demanding , especially for PCFGs , push-   ing researchers to develop methods to decrease the   computational complexities . Chiu and Rush ( 2020 )   propose a neural VL - HMM with 2states for lan-   guage modeling , narrowing down the performance   gap between HMMs and LSTMs . They follow   Dedieu et al . ( 2019 ) to impose a strong sparsity   constraint ( i.e. , each hidden state can only generate   a small subset of terminal symbols ) to decrease   the time complexity of the forward algorithm , thus   requiring pre - clustering of terminal symbols . Yang   et al . ( 2021b ) use a large state space for neural   PCFG induction and achieve superior unsupervised   constituency parsing performance . They follow   Cohen et al . ( 2013 ) to use tensor rank decomposi-   tion ( aka . canonical - polyadic decomposition ( CPD )   ( Rabanser et al . , 2017 ) ) to decrease the computa-   tional complexity of the inside algorithm , but only   scale the state size from tens to hundreds because   the resulting complexity is still high . Chiu et al .   ( 2021 ) use tensor matricization and low - rank ma-   trix decomposition to accelerate structured infer-   ence on chain and tree structure models . However ,   their method has an even higher complexity than   Yang et al . ( 2021b ) on PCFGs . Recently , Fu and   Lapata ( 2021 ) propose a family of randomized dy-   namic programming algorithms to scale structured   models to tens of thousands of states , which is   orthogonal to the aforementioned low - rank - based4797approaches as the former performs approximate in-   ference whereas the latter perform exact inference .   In this work , we propose a new low - rank - based   approach to scale structured inference , which can   be described by FGG notations intuitively . We ﬁrst   provide an intuitive and unifying perspective to-   ward the work of Yang et al . ( 2021b ) and Chiu et al .   ( 2021 ) , showing that their low - rank decomposition-   based models can be viewed as decomposing large   factors in an FGG — e.g. , the binary rule probabil-   ity tensor in PCFGs — into several smaller factors   connected by new “ rank ” nodes . Then we target at   a subset of FGGs — which we refer to as B - FGGs —   subsuming all models considered by Chiu et al .   ( 2021 ) , whereby the inference algorithms can be   formulated via B - graphs ( Gallo et al . , 1993 ; Klein   and Manning , 2001 ) . We propose a novel frame-   work to support a family of inference algorithms in   the rank space for B - FGGs . Within the framework ,   we apply CPD on the factors of a B - FGG and then   construct a new B - FGG deﬁned in the rank space   by marginalizing all the state nodes . Inference with   the new B - FGG has the same result and a lower   time complexity if the rank size is smaller than the   state size .   We conduct experiments in unsupervised PCFG   parsing and HMM language modeling . For PCFG   induction , we manage to use 20 times more hidden   states than Yang et al . ( 2021b ) , obtaining much bet-   ter unsupervised parsing performance . For HMM   language modeling , we achieve lower perplexity   and lower inference complexity than Chiu et al .   ( 2021 ) .   2 Background   2.1 Factor graph grammar   Factor graphs are ﬁxed - sized and thus incapable   of modeling substructures that repeat a variable   number of times . Chiang and Riley ( 2020 ) pro-   pose factor graph grammars ( FGGs ) to overcome   this limitation , which are expressive enough to sub-   sume HMMs and PCFGs . The main purpose of   introducing FGGs in this work is to facilitate more   intuitive presentation of our method , and to enable   generalization beyond HMMs and PCFGs .   2.1.1 Basics   We display necessary notations and concepts of   FGGs ( Chiang and Riley , 2020 , Def . 1,2,5,6,8 ) .   Deﬁnition 1 . A hypergraph is a tuple / parenleftbig   V , E , att , lab , lab / parenrightbig   where•VandEare ﬁnite set of nodes and hyper-   edges .   •att : E→Vmaps each hyperedge to zero   or more ( not necessarily distinct ) endpoint   nodes .   •lab : V→Lassigns labels to nodes .   •lab : E→Lassigns labels to edges .   Deﬁnition 2 . A factor graph is a hypergraph with   mappings ΩandFwhere   •Ωmaps node labels to sets of possible values .   Ω(v)/definesΩ(lab(v ) ) .   •Fmaps edge labels to functions . F(e)/defines   F(lab(e))is of type Ω(v)×···× Ω(v )   whereatt(e ) = v···v .   In the terminology of factor graphs , a node vwith   its domain Ω(v)is avariable , and an hyperedge e   withF(e)is afactor . We typically use T , N , Oto   denote hidden state , nonterminal state and observa-   tion variables for HMMs and PCFGs .   Deﬁnition 3 . A hypergraph fragment is a tuple   ( V , E , att , lab , lab , ext)where   •(V , E , att , lab , lab)is a hypergraph .   •ext∈Vis a set of zero or more external   nodes and each of which can be seen as a   connecting point of this hypergraph fragment   with another fragment .   Deﬁnition 4 . A hyperedge replacement graph   grammar ( HRG ) ( Drewes et al . , 1997 ) is a tuple   ( N , T , P , S ) where   •N , T⊂Lis ﬁnite set of nonterminal and   terminal symbols . N∩T=∅.   •Pis a ﬁnite set of rules ( X→R ) where   X∈NandRis a hypergraph fragment with   edge labels in N∪T.   •S∈Nis the start symbol .   Deﬁnition 5 . A HRG with mapping Ω,F(Def .   2 ) is referred to as an FGG . In particular , Fis   deﬁned on terminal edge labels Tonly .   Notations .   •N : variable node . N : external node .   • X : hyperedge ewith labelX∈N.   indicates zero or more endpoint nodes.4798   • : factorF(e ) .   Fig . 1 illustrates HGG representations of HMM   and PCFG .   Generative story . An FGG starts with S , re-   peatedly selects Xand uses rule X→R   fromPto replaceewithR , until no Xex-   ists .   2.1.2 Conjunction   The conjunction operation ( Chiang and Riley , 2020 ,   Sec . 4 ) allows modularizing an FGG into two parts ,   one deﬁning the model and the other deﬁning a   query . In this paper , we only consider querying the   observed sentence w,···,w , which is exem-   pliﬁed by the red part of Fig . 1 . We sometimes   omit the red part without further elaboration .   2.1.3 Inference   Denoteξas an assignment of all variables , Ξas   the set of all assignments of factor graph D , and   D(G)as the set of all derivations of an FGG G ,   i.e. , all factor graphs generated by G. an FGGG   assigns a score w(D , ξ)to eachD∈D(G)along   with eachξ∈Ξ. A factor graph D∈D(G )   assigns a score w(ξ)to eachξ∈Ξ :   w(ξ ) = /productdisplayF(e)(ξ(v), ... ,ξ ( v ) ) ( 1 )   withatt(e ) = v···v . Notably , w(ξ)/defines   w(D , ξ ) . The inference problem is to compute   the sum - product of G :   Z=/summationdisplay / summationdisplayw(D , ξ ) ( 2)To obtainZ , the key difﬁculty is in the marginal-   ization over all derivations , since / summationtextw(ξ )   can be obtained by running standard variable elim-   ination ( VE ) on factor graph D. To tackle this ,   Chiang and Riley ( 2020 , Thm . 15 ) propose an ex-   tended VE . For each X∈N , ξ∈Ξ , deﬁneP   as all rules in Pwith left - hand side X , and then   deﬁne :   ψ(ξ ) = /summationdisplayτ(ξ ) . ( 3 )   for each rhs R = ( V , E∪   E , att , lab , lab , ext ) , whereE , Econsist   of nonterminal / terminal - labeled edges only , and   τ(ξ)is given by :   τ(ξ ) = /summationdisplay / productdisplayF(e)/parenleftbig   ξ(att(e))/parenrightbig   /productdisplayψ / parenleftbig   ξ(att(e))/parenrightbig(4 )   This deﬁnes a recursive formula for computing   ψ , i.e. ,Z. Next , we will show how Eq . 3 - 4   recover the well - known inside algorithm .   Example : the inside algorithm . Considerπin   Fig.1(b ) . All possible fragments R(rhs ofπ ) dif-   fers in the value of k , i.e. , the splitting point , so we   useRto distinguish them . Then Eq . 3 becomes :   ψ(ξ ) = /summationdisplayτ(ξ ) ( 5)4799Putting values into Eq . 4 :   τ(ξ ) = /summationdisplayp(ξ , n , n)ψ(n)ψ(n )   ( 6 )   wherepdenotes FGG rule probability p(N→   NN ) . It is easy to see that ψis exactly the   inside score of span [ i , k ) , and Eq . 5 - 6 recovers the   recursive formula of the inside algorithm .   Remark . Eq . 4 can be viewed as unidirectional   ( frome∈Eto external nodes ) belief propaga-   tion ( BP ) in the factor graph fragment R , where the   incoming message is ψ(e)fore∈E , and the   outcome of Eq . 4 can be viewed as the message   passed to the external nodes . The time complexity   of message updates grows exponentially with the   number of variables in the factors . Therefore , to de-   crease inference complexity , one may decompose   large factors into smaller factors connected by new   nodes , as shown in the next subsection .   2.2 Tensor rank decomposition on factors   Consider a factor F(e)(Def . 2 ) , it can be repre-   sented as an order- ktensor in Rwhere   N / defines|Ω(v)| . We can use tensor rank decom-   position ( aka . CPD ) to decompose F(e)into a   weighted sum of outer products of vectors :   F(e ) = /summationdisplayλw⊗w⊗···⊗ w   whereris the rank size ; w∈R;⊗is outer   product;λis weight , which can be absorbed into   { w}and we omit it throughout the paper .   Dupty and Lee ( 2020 , Sec . 4.1 ) show that BP   can be written in the following matrix form when   applying CPD on factors :   m = W / parenleftbig   ⊙Wn / parenrightbig   ( 7 )   n=⊙m ( 8)   where m∈Ris factor - to - node message ;   n∈Ris node - to - factor message ; N(·)indi-   cates neighborhood ; W= [ w,···,w]∈   R;⊙is element - wise product . We remark that   this amounts to replacing the large factor F(e)with   smaller factors{F(e)}connected by a new node   Rthat represents rank , where each F(e)can be   represented as W. Fig . 2 illustrates this intuition .   We refer to Ras rank nodes and others as state   nodes thereafter .   3 Low - rank structured inference   In this section , we recover the accelerated inside   algorithms of TD - PCFG ( Cohen et al . , 2013 ; Yang   et al . , 2021b ) and LPCFG ( Chiu et al . , 2021 ) in   an intuitive and unifying manner using the FGG   notations . The accelerated forward algorithm of   LHMM ( Chiu et al . , 2021 ) can be derived similarly .   Denote T∈Ras the tensor represen-   tation ofp(N→NN ) , andα∈R   as the inside score of span [ i , j ) . Cohen et al .   ( 2013 ) and Yang et al . ( 2021b ) use CPD to decom-   poseT , i.e. , let T=/summationtextu⊗v⊗wwhere   u , v , w∈R. Denote U , V , W∈Ras   the resulting matrices of stacking all u , v , w ,   Cohen et al . ( 2013 ) derived the recursive form :   α=/summationdisplayU((Vα)⊙(Wα ) ) ( 9 )   = U / summationdisplay((Vα)⊙(Wα))(10 )   Eq . 9 can be derived automatically by combining   Eq . 7 ( or Fig . 3 ( a ) ) and Eq . 5 - 6 . Cohen et al .   ( 2013 ) note that Ucan be extracted to the front   of the summation ( Eq . 10 ) , and Vα , Wαcan   be cached and reused , leading to further complex-   ity reduction . The resulting inside algorithm time   complexity is O(nr+nmr).4800Recently , Chiu et al . ( 2021 ) use low - rank matrix   decomposition to accelerate PCFG inference . They   ﬁrst perform tensor matricization to ﬂatten Tto   T∈R , and then let T = UVwhere   U∈R , V∈R. By un-ﬂattening Vto   V∈R , their accelerated inside algorithm   has the following recursive form :   α=/summationdisplayU / parenleftbig   V·α·α / parenrightbig   ( 11 )   = U / summationdisplay / parenleftbig   V·α·α / parenrightbig   ( 12 )   Eq . 11 can be derived by combining Fig . 3 ( b ) and   Eq . 5 - 6 . The resulting inside time complexity is   O(nmr+nmr ) , which is higher than that of   TD - PCFG .   When learning a PCFG and a HMM , there is no   need to ﬁrst learn Tand then perform decomposi-   tion on T. Instead , one can learn the decomposed   matrices ( e.g. , U , V ) to learn Timplicitly . During   inference , one can follow Eq . 10 or 12 without the   need to reconstruct T.   Validity of probability . The remaining problem   is to ensure that Tis a valid probability tensor ( i.e. ,   being nonnegative and properly normalized ) when   learning it implicitly . Yang et al . ( 2021b ) essen-   tially transform Fig . 3(a ) into a Bayesian network ,   adding directed arrows N→R , R→N , R→   N. This is equivalent to requiring that V , Ware   nonnegative and column - wise normalized and U   is nonnegative and row - wise normalized , as de-   scribed in Yang et al . ( 2021b , Thm . 1 ) . One can   apply the Softmax re - parameterization to enforce   such requirement , which is more convenient in end-   to - end learning . Chiu et al . ( 2021 ) replace the local   normalization of Yang et al . ( 2021b ) with global   normalization , and we refer readers to their paper   for more details . We adopt the strategy of Yang   et al . ( 2021b ) in this work .   4 Rank - space modeling and inference   4.1 Rank - space inference with B - FGGs   Interestingly , when applying CPD on factors and if   the rank size is smaller than the state size , we can   even obtain better inference time complexities for   a subset of FGGs which we refer to as B - FGGs .   We call a hyperedge a B - edge if its head contains   exactly one node . B - graphs ( Gallo et al . , 1993 ) are   a subset of directed hypergraphs whose hyperedgesare all B - edges . Many dynamic programming algo-   rithms can be formulated through B - graphs ( Klein   and Manning , 2001 ; Huang , 2008 ; Azuma et al . ,   2017 ; Chiu et al . , 2021 ; Fu and Lapata , 2021 ) , in-   cluding the inference algorithms of many struc-   tured models , e.g. , HMMs , Hidden Semi - Markov   Models ( HSMMs ) , and PCFGs . We follow the   concept of B - graphs to deﬁne B - FGGs .   Deﬁnition 6 . A hypergraph fragment is a B-   hypergraph fragment iff . there is exactly one ex-   ternal node and there is no nonterminal - labeled   hyperedge connecting to it . An FGG is a B - FGG   iff . all rhs of its rules are B - hypergraph fragments .   It is easy to see that the aforementioned models   are subsumed by B - FGGs . We can design a fam-   ily of accelerated inference algorithms for B - FGGs   based on the following strategy . ( 1 ) If there are mul-   tiple factors within a hypergraph fragment , merge   them into a single factor . Then apply CPD on the   single factor , thereby introducing rank nodes . ( 2 )   Find repeated substructures that take rank nodes   as external nodes . Marginalize all state nodes to   derive new rules . ( 3 ) Design new inference algo-   rithms that can be carried out in the rank space   based on the general - purpose FGG inference al-   gorithm and the derived new rules . We give two   examples , the rank - space inside algorithm and the   rank - space forward algorithm , in the following two   subsections to help readers understand this strategy .   4.2 The rank - space inside algorithm   Consider an B - FGG Gshown in Fig.1(b ) and re-   place the rhs of πwith Fig . 3(a ) , i.e. , we use   CPD to decompose binary rule probability tensor .   Besides U , V , W∈Rdeﬁned in Sec . 3 , we   deﬁne the start rule probability vector as s∈R ,   and the unary rule probability matrix as E∈R   whereois the vocabulary size .   Fig . 4(a ) is an example ( partial ) factor graph D   generated by G. We highlight substructures of inter-   est with dashed rectangles . Each substructure con-   sists of a node Nand two factors connecting to it .   Nis an external node connecting two hypergraph   fragments which contain the two factors respec-   tively . For each substructure , we can marginalize   the state node Nout , merging the two factors into a   single one . After marginalizing all state nodes , we   obtain a ( partial ) factor graph Dshown in the right   of Fig . 4(a ) where H = VU , I = WU , J=   VE , K = WE , L= ( Us ) . We denote this   transformation asM(D ) = D. It is worth men-4801   tioning that H , I , J , K , Lare computed only once   and then reused multiple times during inference ,   which is the key to reduce the time complexity .   Then we deﬁne a new B - FGG Gwith rules   shown in Fig 4(b ) . It is easy to verify that for   eachD∈D(G ) , we haveM(D)∈D(G ) , and   vice versa . Moreover , we have :   /summationdisplayw(D , ξ ) = /summationdisplayw(M(D),ξ )   because marginalizing hidden variables does not af-   fect the result of sum - product inference . Therefore ,   Z = Z(Eq . 2 ) .   We can easily derive the inference ( inside ) algo-   rithm ofGby following Eq . 3 - 4 and Fig . 4(b ) .   Letα∈Rdenote the rank - space inside score   for span [ i , j ) . Whenj > i + 2 :   α=/bracehtipdownleft / bracehtipupright / bracehtipupleft / bracehtipdownright / summationdisplay(Hα⊙Iα )   + J⊙Iα / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright+Hα⊙K / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright   and whenj = i+ 2,α = J⊙K(from   π).wis the index of the j - th word of the input   sentence in the vocabulary ; Aindicates the j - th   column of A.   We note that , similar to Cohen et al . ( 2013 ) , we   can cache Hα , Iαand reuse them to furtheraccelerate inference . Denoteα , α∈Ras   the inside scores of span [ i , j)serving as a left / right   child of a larger span . Then we have :   α = Kα = J   α = Hαα = Iα   α=/summationdisplay(α⊙α )   and ﬁnally , Z = Lα . We minimize−logZ   using mini - batch gradient descent for unsupervised   learning . The resulting inference complexity is   O(nr+nr ) , which is lower than O(nr+   nmr)of TD - PCFG when r < m , enabling the   use of a large state space for PCFGs in the low - rank   setting .   The key difference between the rank - space in-   ference and the original state - space inference is   that they follow different variable elimination or-   ders . The former marginalizes all state nodes be-   fore performing inference and marginalizes rank   nodes from bottom up during inference ; whereas   the later marginalizes both state and rank nodes   alternately from bottom up during inference .   Parsing . Low - rank inference does not support   the Viterbi semiring , inhibiting the use of CYK4802   decoding . Therefore , we follow Yang et al . ( 2021b )   to use Minimum Bayes - Risk decoding ( Good-   man , 1996 ) . Speciﬁcally , we estimate the span   marginals using auto - differentiation ( Eisner , 2016 ;   Rush , 2020 ) , which has the same complexity as the   inside algorithm . Then we use the CYK algorithm   to ﬁnd the ﬁnal parse with the maximum number   of expected spans in O(n)time , similar to Smith   and Eisner ( 2006 ) .   Implementation . The implementation of the in-   side algorithm greatly inﬂuences the actual run-   ning speed . First , O(n)out ofO(n)can be   computed in parallel using parallel parsing tech-   niques ( Yi et al . , 2011 ; Canny et al . , 2013 ; Zhang   et al . , 2020 ; Rush , 2020 ) . In this work , we adapt   the efﬁcient implementation of Zhang et al . ( 2020 )   for fast inside computation . Second , we adopt   thelog - einsum - exp trick ( Peharz et al . , 2020 )   to avoid expensive log - sum - exp operations on   high - dimensional vectors , which reduces both GPU   memory usage and total running time .   4.3 The rank - space forward algorithm   Consider an B - FGG Gshown in Fig . 1 ( a ) . We   replace the rhs of πby the hypergraph fragment   in the right of Fig . 5(a ) , i.e. , we merge the factor   p(T|T)andp(O|T)into a single factor ,   which can be represented as T∈Rand   can be decomposed into three matrices U , V∈   R , W∈Rvia CPD , where m / o / r is the   state / vocabulary / rank size . Fig . 5(b ) gives an exam-   ple factor graph of HMMs with sentences of length   3 . Similar to previous subsection , we marginal-   ize state nodes Tto construct a new B - FGG G.   The rule set of Gcan be obtained by replacing all   variable nodes TwithRand modifying all fac-   tors accordingly , as one can easily infer from Fig .   5(c ) . Inference with Gsimply coincides with the   forward algorithm , which has a O(nr)time com-   plexity and is lower than O(nmr)of LHMM ( Chiu   et al . , 2021 ) when r < m .4.4 Neural parameterization   We use neural networks to produce probabilities   for all factors , which has been shown to beneﬁt   learning and unsupervised induction of syntactic   structures ( Jiang et al . , 2016 ; He et al . , 2018 ; Kim   et al . , 2019 ; Han et al . , 2019 ; Jin et al . , 2019 ; Zhu   et al . , 2020 ; Yang et al . , 2020 , 2021b ; Zhao and   Titov , 2020 ; Zhang et al . , 2021 ; Chiu and Rush ,   2020 ; Chiu et al . , 2021 ; Kim , 2021 ) . We use the   neural parameterization of Yang et al . ( 2021b ) with   slight modiﬁcations . We show the details in Appd .   A and Appd . B.   5 Experiments   5.1 Unsupervised parsing with PCFGs   Setting . We evaluate our model on Penn Tree-   bank ( PTB ) ( Marcus et al . , 1994 ) . Our implemen-   tation is based on the open - sourced code of Yang   et al . ( 2021b)and we use the same setting as theirs .   For all experiments , we set the ratio of nonterminal   number to the preterminal number to 1:2which is   the common practise . We set the rank size to 1000 .   We show other details in Appd . C and D.   Main result . Table 1 shows the result on PTB .   Among previous unsupervised PCFG models , TN-   PCFG ( Yang et al . , 2021b ) uses the largest number   of states ( 500 perterminals and 250 nonterminals ) .   Our model is able to use much more states thanks to   our new inside algorithm with lower time complex-   ity , surpassing all previous PCFG - based models by   a large margin and achieving a new state - of - the - art   in unsupervised constituency parsing in terms of   sentence - level F1 score on PTB.4803   Ablation study . Fig . 6 shows the change of the   sentence - level F1 scores and perplexity with the   change of the number of preterminals . As we can   see , when increasing the state , the perplexity tends   to decrease while the F1 score tends to increase , val-   idating the effectiveness of using large state spaces   for neural PCFG induction .   5.2 HMM language modeling   Setting . We conduct the language modeling ex-   periment also on PTB . Our implementation is based   on the open - sourced code of Chiu et al . ( 2021 ) .   We set the rank size to 4096 . See Appd . C and D   for more details .   Main result . Table 2 shows the perplexity on the   PTB validation and test sets . As discussed ear-   lier , VL - HMM ( Chiu and Rush , 2020 ) imposes   strong sparsity constraint to decrease the time com-   plexity of the forward algorithm and requires pre-   clustering of terminal symbols . Speciﬁcally , VL-   HMM uses Brown clustering ( Brown et al . , 1992 ) ,   introducing external information to improve perfor-   mance . Replacing Brown clustering with uniform   clustering leads to a 10 point increase in perplexity   on the PTB validation set . LHMM ( Chiu et al . ,   2021 ) and our model only impose low - rank con-   straint without using any external information and   are thus more comparable . Our method outper-   forms LHMM by 4.8point when using the same   state number ( i.e. , 2 ) , and it can use more states   thanks to our lower inference time complexity .   Ablation study . As we can see in Table 3 , the   perplexity tends to decrease when increasing the   state number , validating the effectiveness of using   more states for neural HMM language modeling.4804Discussion . It is interesting to note that our   HMM model is roughly equivalent to another   HMM with interchanged rank and state sizes as   can be seen in Fig.5(c ) . To verify this equivalence ,   we run LHMM in the original state space with   2048 states and rank 2 . The resulting perplexity   is 133.49 on average on the PTB test set , which is   worse than that of ours ( 126.4 ) . We leave further   experimentation and analyses of this discrepancy   for future work .   6 Related work   Tensor and matrix decomposition have been used to   decrease time and space complexities of probabilis-   tic inference algorithms . Siddiqi et al . ( 2010 ) pro-   pose a reduced - rank HMM whereby the forward al-   gorithm can be carried out in the rank space , which   is similar to our model , but our method is more gen-   eral . Cohen and Collins ( 2012 ) ; Cohen et al . ( 2013 )   use CPD for fast ( latent - variable ) PCFG parsing ,   but they do not leverage CPD for fast learning and   they need to actually perform CPD on existing   probability tensors . Rabusseau et al . ( 2016 ) use   low - rank approximation method to learn weighted   tree automata , which subsumes PCFGs and latent-   variable PCFGs . Our method can subsume more   models . Yang et al . ( 2021b , a ) propose CPD - based   neural parameterizations for ( lexicalized ) PCFGs .   Yang et al . ( 2021b ) aim at scaling PCFG inference .   We achieve better time complexity than theirs and   hence can use much more hidden states . Yang   et al . ( 2021a ) aims to decrease the complexity of   lexicalized PCFG parsing , which can also be de-   scribed within our framework . Chiu et al . ( 2021 )   use low - rank matrix decomposition , which can be   viewed as CPD on order-2 tensors , to accelerate   inference on chain and tree structure models includ-   ing HMMs and PCFGs . However , their method is   only efﬁcient when the parameter tensors are of   order 2 , e.g. , in HMMs and HSMMs . Our method   leverages full CPD , thus enabling efﬁcient infer-   ence with higher - order factors , e.g. , in PCFGs . Our   method can be applied to all models considered   by Chiu et al . ( 2021 ) , performing inference in the   rank - space with lower complexities .   Besides HMMs and PCFGs , Wrigley et al .   ( 2017 ) propose an efﬁcient sampling - based   junction - tree algorithm using CPD to decompose   high - order factors . Dupty and Lee ( 2020 ) also use   CPD to decompose high - order factors for fast be-   lief propagation . Yang and Tu ( 2022 ) use CPD todecompose second - order factors in semantic depen-   dency parsing to accelerate second - order parsing   with mean-ﬁeld inference . Besides CPD , Ducamp   et al . ( 2020 ) use tensor train decomposition for fast   and scalable message passing in Bayesian networks .   Bonnevie and Schmidt ( 2021 ) leverage matrix prod-   uct states ( i.e. , tensor trains ) for scalable discrete   probabilistic inference . Miller et al . ( 2021 ) lever-   age tensor networks for fast sequential probabilistic   inference .   7 Conclusion and future work   In this work , we leveraged tensor rank decompo-   sition ( CPD ) for low - rank scaling of structured in-   ference . We showed that CPD amounts to decom-   posing a large factor into several smaller factors   connected by a new rank node , and gave a unifying   perspective towards previous low - rank structured   models ( Yang et al . , 2021b ; Chiu et al . , 2021 ) . We   also presented a novel framework to design a fam-   ily of rank - space inference algorithms for B - FGGs ,   a subset of FGGs which subsume most structured   models of interest to the NLP community . We have   shown the application of our method in scaling   PCFG and HMM inference , and experiments on   unsupervised parsing and language modeling val-   idate the effectiveness of using large state spaces   facilitated by our method .   We believe our framework can be applied to   many other models which have high inference time   complexity and are subsumed by B - FGGs , includ-   ing lexicalized PCFGs , quasi - synchronous context-   free grammars ( QCFGs ) , etc . A direct application   of our method is to decrease the inference complex-   ity of the neural QCFG ( Kim , 2021 ) .   Acknowledgments   This work was supported by the National Natural   Science Foundation of China ( 61976139 ) .   References480548064807   A Neural parameterization of PCFGs   In this section , we give the full parameterization   of PCFGs . We follow Yang et al . ( 2021b ) with   slight modiﬁcations for generations of U , V , W∈   Rin 4.2 . We use the same MLPs with two   residual layers as Yang et al . ( 2021b ):   s = exp(uh(w)/summationtextexp(uh(w ) )   E = exp(uh(w)/summationtextexp(uh(w ) )   U = exp(uf(w)/summationtextexp(uf(w ) )   V = exp(uf(w)/summationtextexp(uf(w ) )   W = exp(uf(w)/summationtextexp(uf(w ) )   h(x ) = g(g(˜Wx ) )   g(y ) = ReLU ( ˜VReLU ( ˜Uy ) ) + y4808where Σis the vocabulary set , His the   set of rank , Nis a ﬁnite set of nonterminals ,   W= [ W;W],w , w , w∈W , W , W.   The main differences of neural parameterization   between ours and previous work are that we   make the projection parameter ushared among   U , V , andU.   B Neural parameterization of HMMs   In this section , we give the full parameterization of   HMMs , which is similar to PCFGs ’ parameteriza-   tion . Deﬁne sas start probability for HMMs . And   the deﬁnitions of U , V , Ware same as deﬁnitions   in4.3 :   s = exp(uh(w))/summationtextexp(uh(w ) )   U = exp(uw)/summationtextexp(uw )   V = exp(uw)/summationtextexp(uw )   W = exp(uh(w)/summationtextexp(uh(w ) )   h(x ) = g(g(˜Wx ) )   g(y ) = ReLU ( ˜VReLU ( ˜Uy ) ) + y   whereSis a ﬁnite set of states , His the set of   rank , Σis vocabulary set .   C Data details   Penn Treebank ( PTB ) ( Marcus et al . , 1994)con-   sists of 929k training words , 73k validation words ,   and 82k test words , with a vocabulary of size 10k .   For PCFGs , we follow Yang et al . ( 2021b ) and   use their code to preprocess dataset . This process-   ing discards punctuation and lowercases all tokens   with 10k most frequent words as the vocabulary .   The splits of the dataset are : 2 - 21 for training , 22   for validation and 23 for test .   For HMMs , we follow Chiu et al . ( 2021 ) and   use their code to preprocess dataset . We lowercase   all words and substitutes OOV words with UNKs .   EOS tokens have been inserted after each sentence . D Experimental details   For PCFGs , we use Xavier normal initialization   to initialize the weights in handf . We opti-   mize our model using Adam optimizer with β=   0.75,β= 0.999 , and the learning rate 0.002 , set-   ting the dimension of all embeddings to 256 .   For HMMs , we initialize all parameters by   Xavier normal initialization except for wandw .   We use AdamW optimizer with β= 0.99,β=   0.999 , and the learning rate 0.001 , and a max grad   norm of 5 . We use dropout rate of 0.1to dropout   wandU , Vin HMMs . We train for 30 epochs   with a max batch size of 256 tokens , and reduce   the learning by multiplyingif the validation per-   plexity fails to improve after 2 evaluations . Evalua-   tions are performed one time per epoch . We follow   Chiu et al . ( 2021 ) to shufﬂe sentences and lever-   age bucket iterator , where batch of sentences are   drawn from buckets containing sentences of similar   lengths to minizing padding .   We run all experiments on NVIDIA TITAN RTX   and NVIDIA RTX 2080ti and all experimental re-   sults are averaged from four runs.4809