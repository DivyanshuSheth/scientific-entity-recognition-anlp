  Elisa KreissFei FangNoah D. GoodmanChristopher PottsDepartment of LinguisticsDepartment of Computer ScienceDepartment of Psychology   Stanford University   Stanford , CA 94305 USA   { ekreiss , feifang , ngoodman , cgpotts}@stanford.edu   Abstract   Current deep learning models often achieve   excellent results on benchmark image - to - text   datasets but fail to generate texts that are useful   in practice . We argue that to close this gap , it   is vital to distinguish descriptions from cap-   tions based on their distinct communicative   roles . Descriptions focus on visual features   and are meant to replace an image ( often to   increase accessibility ) , whereas captions ap-   pear alongside an image to supply additional   information . To motivate this distinction and   help people put it into practice , we introduce   the publicly available Wikipedia - based dataset   Concadia consisting of 96,918 images with   corresponding English - language descriptions ,   captions , and surrounding context . Using in-   sights from Concadia , models trained on it ,   and a preregistered human - subjects experiment   with human- and model - generated texts , we   characterize the commonalities and differences   between descriptions and captions . In addition ,   we show that , for generating both descriptions   and captions , it is useful to augment image - to-   text models with representations of the textual   context in which the image appeared .   1 Introduction   Image - based natural language generation ( NLG )   has great potential for productive applications in   accessibility , image search , creative writing , and   navigational instruction tasks , among other areas .   However , current systems fall short of realizing this   potential even though they can generally produce   fluent , truthful texts in a wide range of scenarios   ( e.g. , Dognin et al . , 2022 ; Guinness et al . , 2018 ;   Gurari et al . , 2020 ) . In this paper , we identify and   address one of the fundamental limitations holding   these systems back : insufficient attention to the   communicative purpose of the text being generated .   We focus on the high - level distinction between   descriptions andcaptions . Descriptions are created   for the purpose of replacing an image in context , Figure 1 : An example from our Wikipedia - based corpus   Concadia of an image from the article on bananas with   two associated texts : the description as provided in the   image ’s alt text , and the caption as displayed below the   image in the article . For captions , the image content is   presupposed whereas descriptions aim to stand in for the   image . While grounded in the same image , description   and caption convey vastly different information .   most notably used to make an image accessible to   users who ca n’t see them . Captions , in contrast ,   presuppose access to the image and seek to provide   additional information . For example , in Figure 1 ,   the description summarizes the scene and describes   the features that are clearly visible in the image ,   whereas the caption provides almost no visual de-   tails and instead highlights the relevance of this   image to the article . Both texts are grounded in the   same image and have clear purposes , but neither   can play the role of the other .   To enable in - depth exploration of the descrip-   tion / caption distinction , we introduce Concadia ,   a Wikipedia - based corpus of 96,918 images with   associated English - language captions , alt text de-   scriptions , and accompanying context from the re-   spective Wikipedia article . Concadia provides the   opportunity to directly compare descriptions and   captions for shared images , and it can be used to4667develop image - based NLG systems that serve these   distinct communicative purposes as well .   To further substantiate the description / caption   distinction , we report on a linguistic analysis of   the commonalities and differences between these   two kinds of text in Wikipedia ( Section 3 ) . We   then develop and assess image - based NLG systems   using Transformer- and LSTM - based architectures .   Two clear lessons emerge from these modeling ef-   forts . First , description generation is easier than   caption generation , as one might expect given the   tighter relationship between descriptions and im-   ages . Second , it is highly beneficial to include   representations of the textual context in which an   image appeared . This finding aligns with much re-   cent work showing that blind and low - vision ( BLV )   users value image descriptions that tie the image to   its surrounding context ( Stangl et al . , 2021 , 2020 ;   Muehlbradt and Kane , 2022 ) . Similarly , Biten et al .   ( 2019 ) identify benefits of using context in the cap-   tion domain , for instance , for inferring names of   people visible in an image . In the example in Fig-   ure 1 , both the caption and description reflect the   context ( commercial aspects of bananas ) , and the   caption additionally connects to the type of banana   discussed in the article . Most datasets for image-   based NLG provide only image – text pairs , with   no context , which may fundamentally limit their   usefulness . A strength of Concadia is that all the   image – text pairs are given within their Wikipedia   contexts .   Finally , we provide evidence from a preregis-   tered human - subjects experiment ( Section 5 ) . Our   results further reinforce the observation that de-   scriptions and captions serve distinct communica-   tive purposes . In addition , the results hold not   only for the descriptions and captions from Con-   cadia but extend to our model - generated texts as   well . This serves to further validate that systems   trained to generate captions are inadequate for the   task of generating descriptions , and vice versa , and   that model evaluations need to be sensitive to com-   municative purpose . Together with the modeling   results , this speaks to the importance of both com-   municative purpose and context when it comes to   developing effective image - based NLG systems .   2Captions vs. Descriptions in Prior Work   In AI , the task of generating prose text from images   is often referred to as “ image captioning ” , which   glosses over distinctions of communicative purpose . The current section offers a synopsis of research ,   within and outside of AI , that is relevant to our pro-   posed description / caption distinction and supports   a more nuanced view of image - based NLG tasks .   The conceptual description / caption distinction   we propose poses a terminological challenge . We   use “ caption ” to mean “ image - supporting informa-   tion expressed in text ” , which we contrast with   “ ( alt ) description ” to mean “ text intended to replace   an image ” . This is aligned with general popular us-   age and with the terminology in ( photo-)journalism   and graph accessibility , fields that conceptually al-   ready make a description / caption distinction . How-   ever , the term “ caption ” is now a loaded term in AI ,   and future work should be careful in considering   the conceptual distinctions despite the challenges   in terminology .   2.1 Image - to - Text Relations   Most previous work has focused on characterizing   the relation between images and the texts that occur   alongside them . Marsh and Domas White ( 2003 )   summarize cross - disciplinary efforts , and propose a   unifying taxonomy based on the different functions   an image takes on in a text ( e.g. , from decorative   through explanatory to interpretative ) .   Work in communication sciences situates image –   text relations based on their role in discourse ( e.g. ,   Martinec and Salway , 2005 ) , and work in library   sciences investigates the role that these relations   play in efficient image indexing ( e.g. , Jaimes and   Chang , 1999 ; Shatford , 1986 ) .   More recently , the relations of images and vi-   sually co - occurring texts have been explored in   AI , specifically for news ( Otto et al . , 2020 ; Oost-   dijk et al . , 2020 ) , advertising ( Otto et al . , 2020 ;   Zhang et al . , 2018 ) , and social media ( Kruk et al . ,   2019 ; Hodosh et al . , 2013 ) . Alikhani et al . ( 2020 )   introduce an annotation protocol to investigate co-   herence relations between images and associated   texts . They analyze to what extent texts from a va-   riety of resources address what ’s visible , subjective ,   representing an action , telling a story , or contain-   ingmeta information . Our description vs. caption   distinction is focused on communicative purpose   and can cross - cut these categories . For instance ,   evidence from BLV users suggests that a descrip-   tion intended to replace an image of politicians   in a news article should include visible features   that describe the scene , but also meta information   that names the politicians and the actions they’re4668engaging in ( Stangl et al . , 2021 ) .   A majority of the previously proposed catego-   rizations are based on analyses of the text content ,   and most generally assume that images and texts   visually co - occur . The description vs. caption dis-   tinction is a higher - level categorization that is based   on whether the image content is presumed to be   visible in the first place . This is motivated by com-   municative principles : when the image is visible ,   texts that simply describe the image would be re-   dundant , and instead those texts need to serve dis-   tinct communicative purposes ( Kruk et al . , 2019 ) .   Hodosh et al . ( 2013 ) follow similar reasoning to   explain why crowd - sourced image – text datasets at   the time contained high rates of information that   ca n’t be directly extracted from the image . This   distinction is particularly pronounced in the area of   image accessibility . For instance , recent studies on   the accessibility of computational research papers   explicitly distinguish figure captions and figure de-   scriptions when assessing a paper ’s accessibility   ( Williams et al . , 2022 ; Chintalapati et al . , 2022 ) .   2.2 Image - Based Text Generation Datasets   We can loosely categorize existing datasets accord-   ing to communicative purpose . Caption datasets   contain texts that are useful when appearing along-   side images , mainly to contextualize them . De-   scription datasets contain texts that are intended   to replace images . Some datasets are difficult to   decisively place due to uncertainty about the nature   of the texts or major postprocessing of the data .   2.2.1 Description Datasets   The first group of datasets were created by present-   ing sighted participants with images and the task   to describe them . Presenting the images out of con-   text restricts the text to information that can be ex-   tracted from the image itself . We therefore consider   these to be description datasets . Examples include   MS - COCO ( Lin et al . , 2014 ; Chen et al . , 2015 ) ,   Flickr8k/30k ( Young et al . , 2014 ; Hodosh et al . ,   2013 ) , and more recently VizWizCaptions ( Gurari   et al . , 2020 ) and TextCaps ( Sidorov et al . , 2020 ) .   Localized Narratives ( Pont - Tuset et al . , 2020 ) and   DIDEC ( van Miltenburg et al . , 2018 ) are similarly   constructed , except that the descriptions are spo-   ken and the datasets further contain real - time data   aligning regions of the image with the description . Conceptual Captions ( Sharma et al . , 2018 ) and   its extension Conceptual 12 M ( Changpinyo et al . ,   2021 ) represent a second group of description   datasets . These datasets contain crawled images   and their associated alt texts from the Web . A   primary purpose of alt texts is to replace the im-   ages when they ’re absent , and they are what screen   readers use for providing non - visual image access .   These use - cases align with the assumption that the   text stands in for the image , which makes them   description datasets .   None of these datasets contain information on   the context an image is situated in . In the case of   crowd - sourced descriptions , no context was pro-   vided , which means that the description content   solely relies on cognitive priors and heuristics to   determine what is relevant . In the crawled datasets ,   we can generally assume that the people writing alt   texts were aware of the context the images appear   in , suggesting that the descriptions are likely to be   context - sensitive . However , the context itself is   often not preserved in the datasets , meaning that   the effect of context ca n’t be further investigated or   learned by a model .   2.2.2 Caption Datasets   Some recent datasets focus on texts expressing in-   formation that ca n’t be extracted from the image   alone . The news domain is a prominent example   ( Biten et al . , 2019 ; Ramisa et al . , 2017 ) , since the   texts associated with images in newspapers gener-   ally presuppose that the user can see the image .   Thus , we consider these to be caption datasets .   These datasets generally do contain contextual in-   formation in the form of the articles the images   appear in .   2.2.3 Other Datasets   Im2Text is a dataset crawled from Flickr , contain-   ing 1 million images and the user - generated text   appearing alongside it . In this case , the purpose   of the text is difficult to define since this same text   is used for image search as well as providing in-   formation that might be read alongside the image .   Recognizing this challenge , the dataset was post-   processed to make the text appear more descrip-   tive . However , subsequent work has suggested that   the dataset still contains a large number of non-   descriptive texts ( Hodosh et al . , 2013 ) , making it   difficult to decisively place .   While often not summarized in a single static   dataset , Twitter data is also used for image - based4669   text generation ( e.g. , Hessel et al . , 2021 ) . The Twit-   ter API offers access to the image as well as the   tweet and , if present , a user - written image descrip-   tion to make it non - visually accessible . The tweet   assumes that a user has access to the posted image   which aligns it with the captioning purpose . The ac-   cessibility text seeks to substitute it , aligning with   the description purpose .   The WIT dataset contains crawled images with   their corresponding captions ( which are called ref-   erence descriptions in WIT ) , alt texts , and the sur-   rounding paragraph from Wikipedia ( Srinivasan   et al . , 2021).WIT provides a rich multilingual   resource for images and associated texts in context .   While it contains captions as well as alt descrip-   tions for each image within a context , the authors   note that the descriptions are rare and often con-   tain only the filename . Perhaps for that reason ,   the subsequent analyses and experiments were not   performed on the alt description data . Concadia   is specifically created for capturing the alt descrip-   tions , and is controlled and filtered for their quality .   3 The Concadia Dataset   We introduce Concadia , Contextualized captions   andaltdescriptions from Wikiped ia , a corpus ex-   tracted from Wikipedia consisting of images with   their naturally occurring alt descriptions , captions ,   and surrounding context . Concadia allows for a   direct comparison of descriptions and captions for   a specific image in a specific context . In this sec-   tion , we characterize the distribution of captions   and descriptions on all of Wikipedia , describe the   Concadia dataset generation process , and provide   a qualitative comparison on the similarities and dif - ferences of the captions and descriptions in Conca-   dia . Appendix A provides implementation details   and further illustrations .   3.1 Descriptions and Captions on Wikipedia   Descriptions and captions fulfill distinct commu-   nicative purposes that are already apparent in their   distribution . Where present , captions are accessible   for all readers , often printed below their respective   image . Image descriptions are harder to access ,   since they ’re usually defined in the image ’s alt tag .   Although hidden to most users , alt descriptions are   essential for making images accessible to users who   ca n’t see them . The fact that alt tags are hidden by   default likely decreases awareness and use of the   field ( Gleason et al . , 2019 ) , resulting in major ac-   cessibility challenges specifically for blind and low   vision users . The scarcity of alt descriptions online   that could make these images nonvisually accessi-   ble is widely established , but existing studies have   focused on social media ( Morris et al . , 2016 ) and   most frequently visited websites overall ( Bigham   et al . , 2006 ; Guinness et al . , 2018 ) . We expect to   see a similar pattern on Wikipedia .   In 2020 , English Wikipedia consisted of over 6   million articles with more than 4 million images .   Based on 10,000 randomly sampled articles , our ap-   proximation suggests that while around 91 % of all   images on Wikipedia are associated with a caption ,   only 6 % contain alt description tags . Moreover , this   small subset includes alt descriptions that simply   sayalt text orImage . The number of informative   alt descriptions is likely much lower.4670   3.2 Dataset Curation   For Concadia , we extracted all images from   Wikipedia that have captions as well as alt de-   scriptions . Images were excluded where the pic-   ture was n’t publicly available at Wikimedia Com-   mons , descriptions contained reference to the cap-   tion ( e.g. , refer to caption ) , consisted of fillers ( e.g. ,   alt text , text ) , only consisted of the file name , or   where any associated text was shorter than 3 char-   acters . The extracted context paragraph is the para-   graph following the image in the HTML , and is   minimally 20 words long .   Table 1 provides basic corpus statistics . The final   corpus consists of 96,918 images with descriptions ,   captions , and surrounding text from 41,143 articles .   Additionally , we include a train / dev / test split for   the data . Images that occur multiple times ( but   have potentially different associated texts ) , and im-   ages where caption and description are identical   are sorted into the training set to ensure the highest   quality in the validation and test sets . All other   assignments are random , while ensuring that data   from the same article are assigned to the same split .   The dataset is publicly available .   3.3 Caption / Description Similarities   Concadia provides the opportunity to directly com-   pare descriptions and captions naturally written by   Wikipedia users for a single image . If captions   and descriptions show perfect similarity , there is   no reason to assume a difference in communicative   purpose , and captions could for instance simply re-   place the rarely present descriptions when they are   absent . A complete dissimilarity would indicate a   major effect of communicative purpose , and wouldsuggest few commonalities that could be induced   by the shared image and context . Crucially , if the   content of captions and descriptions is partially   related , this would be compatible with the texts   fulfilling distinct purposes while still being shaped   by their shared image and context .   To investigate the semantic similarity between   captions and descriptions , we computed the co-   sine similarity of the SBert embeddings ( Reimers   and Gurevych , 2019 ) for all matching descrip-   tion / caption pairs ( see Figure 2 in red ) . Descrip-   tions and captions are significantly more similar   than would be assumed under a baseline where   descriptions and captions are randomly paired ( in   blue ; two - sample Kolmogorov - Smirnov test : p <   0.0001 ) , suggesting that they share item - specific   content . ( We replicate this using Jaccard distance   of the raw strings , as presented in Appendix A.3 . )   Having established that there is a semantic simi-   larity between captions and descriptions , we turn to   how they come apart . A quantitative analysis of the   Parts - of - Speech ( POS ) shows distinct patterns for   descriptions and captions . Adjectives and nouns   are significantly more frequent in descriptions but   captions contain significantly more proper nouns .   This aligns with previous work on news captions   ( Biten et al . , 2019 ) , which found that proper nouns   contribute 20 % of captions from New York Times   articles while they ’re completely absent in datasets   such as MS - COCO ( a description dataset ) . This   similarly replicates for the adjective and noun dis-   tributions ( Biten et al . , 2019 ; Ramisa et al . , 2017 ) .   Intuitively , these patterns are already reflected in   the most frequent bigrams on Concadia ( see Fig-   ure 3 ) . Descriptions are dominated by descriptive   attributes such as people ’s looks ( e.g. , white shirt ,   baseball cap , dark hair ) , and meta information   about the image ( e.g. , colour photograph ) . The   most frequent bigrams in captions are dominated   by proper noun compounds such as San Francisco   orTour de France , as well as common places and   times ( e.g. , national park , 19th century ) .   4 Model Experiments   Our investigation of the similarity between cap-   tions and descriptions in Section 3.3 indicates that   they differ in principled ways but that they are not   unrelated . This suggests that , while captions and   descriptions ca n’t stand in for each other , they can   potentially inform each other ’s content when we   automatically generate them . We further motivated4671   that there is evidence that both description and cap-   tion generation would benefit from integrating the   larger context where the image occurred . We now   pursue these hypotheses in the context of large-   scale caption and description generation .   4.1 Model Architectures   Previous work in image - based NLG has shown   promising results from using both recurrent and   Transformer - based models to generate text . To   investigate the importance of a description vs. cap-   tion distinction and the relevance of context , we   compare three model variants . Depending on the   condition , the models are trained on predicting de-   scription or caption labels and receive as context   captions , descriptions , the broader context , or noth-   ing . We provide further details on the models and   training in Appendix B.   ResNet - LSTM generates a label token - by-   token , using an LSTM decoder ( Hochreiter and   Schmidhuber , 1997 ) . The input to the LSTM is a   concatenated representation of the image using a   pretrained ResNet ( He et al . , 2016 ) , and the context   using BERT embeddings ( Devlin et al . , 2019 ) . We   took inspiration from the importance of attention   mechanisms ( Vaswani et al . , 2017 ) on the input im-   age as used in state - of - the - art “ captioning ” models   ( Xu et al . , 2015 ; Lu et al . , 2017 ; Anderson et al . ,   2018 ) and extended them to the context input . To   ensure the same number of trainable parameters   between the with - context vs. no - context models ,   the no - context models received all - ones vectors in   place of context embeddings .   DenseNet - LSTM follows the same structure   as the ResNet - LSTM but leverages pretrained   DenseNet features instead ( Deng et al . , 2020 ; Hos-   sain et al . , 2021 ) .   OSCAR(VinVL ) is a pretrained Transformer-   based vision - language model that achieves state - of-   the - art results on many tasks via finetuning ( Zhang   et al . , 2021 ) . For each image , the VinVL pretrainedvisual feature extractor provides visual features ,   i.e. , vector embeddings of object - enclosing regions ,   and object tags in text . To obtain a unified represen-   tation of the image – text pair , the OSCAR model   concatenates the BERT embeddings ( Devlin et al . ,   2019 ) of the text and object tags with the visual   features ( Li et al . , 2020 ) . We supply context by   appending it to the object tags before the BERT   encoding . Since BERT can handle variable - length   inputs , the trainable parameters remain constant   regardless of whether context is incorporated .   4.2 Evaluation Metrics   Since Concadia is extracted from naturally occur-   ring data , each image is only associated with a sin-   gle ground - truth caption and description . Currently   established evaluations of text generated from im-   ages rely on multiple references to reliably esti-   mate performance . As suggested in previous work   ( Biten et al . , 2019 ) , we consider CIDEr ( Vedantam   et al . , 2015 ) to be most appropriate for our setting .   First , CIDEr has been shown to outperform BLEU   ( Papineni et al . , 2002 ) and ROUGE ( Lin , 2004 )   specifically when reference sentences are sparse   ( Vedantam et al . , 2015 ; Anderson et al . , 2016 ) .   Second , while METEOR ( Denkowski and Lavie ,   2014 ) and SPICE ( Anderson et al . , 2016 ) achieve   slightly higher alignment with human judgments on   some datasets with few references , their use of soft-   similarity and lemmatization is not well - defined for   proper names ( Biten et al . , 2019 ) , which make up   more than 26 % of our caption data and more than   15 % of our description data ( Section 3.3 ) .   4.3 Experiment 1   In our first experiment , we probe how the distinc-   tion between the tasks of description and caption   generation affects model performance .   Predictions If descriptions are meant to replace   an image while captions are meant to complement   it , we expect models to perform better at generating   descriptions from image input alone , as compared   to captions . In Experiment 1 , we test whether this   prediction is borne out in the three models . A high   performance gap between the two conditions would   further highlight the necessity of differentiating   those tasks to address their distinct challenges .   Results Table 2 shows the performance on the   test set for description and caption generation . Test   set evaluation was performed using beam search   ( n= 5 ) for all models . As predicted , all models   achieve higher performance on the description data4672   than the caption data . This suggests that the in-   formation from the image alone is more helpful to   description generation , suggesting a closer connec-   tion between the two . Experiments using shuffled   versions of the dataset further show that the perfor-   mance gap ca n’t be explained simply by the smaller   vocabulary in descriptions , the higher proportion   of proper nouns in captions , or the higher n - gram   overlap across descriptions ( see Section 4.5 ) .   4.4 Experiment 2   We have seen that descriptions and captions play   different roles and pose different modeling chal-   lenges . Can these two kinds of text mutually re-   inforce each other in the context of automatic text   generation ? We investigate this question by treat-   ing one as the context that can inform generation   for the other . For a second notion of context , we   also test whether the closely occurring paragraph   can help the models ’ performances . A positive ef-   fect of context would point to the value of treating   image - based NLG as a contextual problem .   Predictions We predict that , while distinct in   purpose , descriptions and captions from the same   image and context share enough information that   they can benefit from one another . Part of this con-   textual relevance should also be captured when us-   ing the closely appearing paragraph . To ensure that   not just language in general accounts for potential   performance gains , we provide control conditions   where the model receives paragraphs from other   images . Improvements over these controls can then   only be due to semantic relatedness between the   context and the generated description / caption .   Results Table 2 shows that , where models are   provided contextual representations , performanceimproves for both description and caption genera-   tion compared to the image - only input baselines .   As predicted , this improvement is most pronounced   when providing the respective caption or descrip-   tion . Even the closely appearing paragraph leads to   increased CIDEr scores compared to the no - context   baselines . Furthermore , this improvement goes be-   yond what models achieved when training on ran-   domly assigned paragraphs , which suggests that   they make use of item - specific information and   points to the more general benefit of integrating the   broader context an image appears in .   4.5 Discussion   Across models , we find that they achieve higher   CIDEr ratings on the description than the caption   generation task ( see Table 2 ) . In part this might   be due to the smaller set of tokens present in de-   scriptions ( Table 1 ) , the higher proportion of proper   nouns in captions ( Figure 7 ) , and the higher n - gram   overlap across descriptions ( Figure 3 ) . An addi-   tional potential explanation for the gap is that de-   scriptions are more tightly connected to their image   than captions , and are therefore easier to learn .   The first set of factors are image - independent ,   i.e. , they are simply properties of the language la-   bels . If these components solely drive the higher   performance on the description data , this gap   should be reflected in a version of the dataset where   the image - label mappings are shuffled . We con-   ducted this experiment using the ResNet - LSTM   model instead of the OSCAR(VinVL ) model since   the OSCAR(VinVL ) model already carries biases   from its pretraining . Although the shuffled descrip-   tion model achieves higher CIDEr scores than the   shuffled caption model ( 0.017 vs. 0.016 , respec-   tively ) , it does n’t compare to the performance gain   on the non - shuffled data ( 0.09 ) . The tighter rela-   tionship of descriptions and their images predicts   this gap and falls in line with the general descrip-   tion / caption purpose distinction .   Further support for the purpose of the descrip-   tion / caption directly affecting their predictability   comes from the experiment where we provided the   closely appearing paragraph as the context for the   images ( Table 2 ) . In the best - performing model ,   OSCAR(VinVL ) , caption generation performance   surpasses description performance when the para-   graph is provided . Furthermore , while caption per-   formance does n’t surpass description generation   performance in the LSTM models , providing the4673paragraph as context to the caption models results   in a higher performance increase than for the de-   scription models . This is intuitive given the pre-   dicted connection of the descriptions / captions to   the images and context . If the primary purpose of   the caption is to connect the image to its context ,   the paragraph should be crucial for generating it .   For the description , on the other hand , the image   takes on a primary role , and adding the context   should result in lesser gains . Taken together , the   quantitative model results are in line with the de-   scription / caption purpose distinction , suggesting a   closer alignment of the description with the image   and the caption with the context .   In sum , we find that providing linguistic context   has the potential to improve model - generated de-   scriptions as well as captions . These results also   have implications for applications . Our results sug-   gest that models trained to generate descriptions ,   which are generally sparse , might benefit from re-   ceiving the more available caption information as   additional input , providing a new potential path for   addressing image accessibility challenges .   5 Human Evaluation   In Section 3.3 and Section 4 , we provided initial   evidence that descriptions and captions fulfill sep-   arate communicative purposes . We now present   a human - subject experiment in which we directly   test whether captions are more suitable than de-   scriptions for providing information that ca n’t be   obtained from the image alone , and whether de-   scriptions are more useful than captions for replac-   ing an image . Furthermore , we investigate whether   models might capture these differences . Our ex-   periment focuses on the Resnet - LSTM model , the   weakest of our models , which should then provide   a conservative assessment of how closely automatic   generations align with human texts .   Our quantitative and qualitative hypotheses ,   along with our exclusion criteria and the analy-   ses , were preregistered . Details on the recruitment   and data exclusions are provided in Appendix C.   Materials We sampled 300 images from the   validation set in Concadia . In the experiment ,   these could be paired with their original descrip-   tions from Concadia , their original captions , or the   model - generated descriptions or captions , resulting   in four text conditions for each image . Design In each trial , an image was displayed   next to one of the descriptions / captions sampled   from the four text conditions , followed by two ques-   tions ( Figure 4 ) . Participants assessed how useful   the text would be to help someone imagine the pic-   ture ( rated from not useful tovery useful ) , and how   much they learned from the text that they could n’t   have learned from the image ( rated from nothing   toa lot ) . Each question was associated with a con-   tinuous scale initialized at zero and ranging to one .   Participants could further opt out of responding to   the questions by selecting a checkbox to indicate   that image and text seemed to be unrelated .   Procedure Each participant saw 32 unique im-   ages , and the corresponding texts were uniformly   sampled from the four text conditions . Image selec-   tion and trial order were randomized and question   order was randomized between participants .   Participants We recruited 421 participants over   Amazon ’s Mechanical Turk , who were paid $ 2.60   for participation with an average completion time   of 12 minutes ( $ 13 / hr ) . After preregistered exclu-   sions , the analysis is based on 277 participants .   5.1 Predictions   Overall , we predicted that descriptions and captions   fulfill different communicative purposes . Con-   cretely , we made the following predictions for   the original human - generated descriptions and cap-   tions in Concadia :   H1The original descriptions are judged more use-   ful for imagining the picture than the original   captions .   H2A reader receives more extra - image informa-   tion from the original captions than from the   original descriptions .   We further predicted that a model should be ca-   pable of reflecting these differences when trained   on descriptions and captions separately . Predic-   tions ( H1 ) and ( H2 ) should therefore also hold for   model - generated descriptions and captions .   Finally , we qualitatively predicted that models   would be strong enough to show the following : gen-   erated descriptions should be more similar to orig-   inal descriptions than the original captions would   be , and vice versa for generated captions .   5.2 Results   To test our quantitative predictions , we performed a   Bayesian mixed effects regression analysis , where4674   we predicted continuous slider ratings from the   centered categorical text condition variable with   random intercepts and slopes for each participant   and image . We find strong evidence for all of our   quantitative predictions and we also see the pre-   dicted qualitative patterns , as shown in Figure 4 .   The results lead us to two main conclusions . Firstly ,   there are multiple dimensions in which an image-   text pair can be successful . Our results show that   we can reverse which texts are rated as “ better ”   depending on what goal we prioritize : the abil-   ity of to reconstruct the image from the text in   its absence , or the amount of information learned   from the text in its presence . Secondly , models   trained on caption and description data separately   capture core aspects of these differences . Even   with its limitations , the generated descriptions and   captions provide more useful alternatives for their   original counterparts than original captions and de-   scriptions , respectively .   Unsurprisingly , the generated text was more of-   ten rejected as being unrelated to the image than   the original descriptions and captions were , which   were only rejected in less than 5 % of all trials .   These results underline the quality of Concadia and   shortcomings of the ResNet - LSTM model .   Overall , our results highlight the importance of   specifying the purpose a text is intended to ful-   fill in the image - text generation domain – both   for dataset creation and model evaluation . The   questions posed for evaluation have to be assessed   against the purpose that they ’re intended to fulfill .   6 Conclusion   We argue for a distinction between descriptions   ( text intended to replace an image ) and captions   ( text intended to contextualize an image ) , and   we provide evidence for their similarities and dif - ferences through linguistic analyses and human-   subject evaluations . We introduce a corpus , Con-   cadia , consisting of images and their correspond-   ing alt descriptions , captions , and context , and we   show that Concadia is a valuable resource for train-   ing NLG systems for both description and caption   generation . The structure of Concadia addition-   ally allows us to show that both generation tasks   benefit from contextual information . Beyond the   conceptual relevance for advancing image - based   NLG systems , this has practical implications specif-   ically for image accessibility . Captions which often   co - occur with images that lack an accessibility de-   scription could be a valuable resource for building   more powerful description generation models .   Limitations and Ethics   The proposed dataset Concadia is crawled from   Wikimedia Commons . Our understanding is that   the images and associated texts are compliant with   the policies of Wikimedia and Wikipedia .   Concadia is a subset of Wikipedia , filtered for   images that had associated alt texts . Therefore ,   data might disproportionately be extracted from   articles historically containing many images , such   as the Tour de France or Award Shows , frequently   visited articles where lacking image accessibility   is more likely to be noticed , and from authors that   are aware of the alt texts ’ uses , for instance , from   accessibility - related Wikipedia articles . Addition-   ally , Concadia inherits biases generally present   on Wikipedia ( extraction date : December 2020 ) .   For instance as of February 2020 , only 19 % of   biographical articles were about women ( Tripodi ,   2021 ) , and it ’s likely that gender as well as ethnic-   ity biases are perpetuated in Concadia . Concadia   is further constrained to English - language entries   since this is the language we chose for our case4675study . We provide a corresponding datasheet in our   Github repository ( Gebru et al . , 2021 ) .   The image - based natural language generation   models were trained to allow for comparisons be-   tween description and caption generation , and to   investigate the role of context in both cases . The   models are not intended for deployment of any kind   since they were not tested for crucial dimensions   such as their robustness , truthfulness , or output of   potentially discriminatory language . The model   outputs further need to be tested to determine the   extent to which they actually capture the intended   use cases . For instance , a description might be in-   tended to serve an image accessibility purpose , but   rigorous testing is required to determine the extent   to which the data and associated models fulfill this   goal for blind and low vision users .   The human subject experiment was conducted   under an IRB protocol . Participants were paid   $ 2.60 with an average completion time of 12 min-   utes ( $ 13 / hr ) .   Acknowledgements   This work is supported in part by a grant from   Google through the Stanford Institute for Human-   Centered AI . We thank our experiment participants   for their invaluable input . We are further grateful   to Desmond Elliott , and Dan Jurafsky for their thor-   ough feedback on previous drafts . We thank Rachit   Dubey , all members of Stanford ’s CoCoLab , and   Jessica Mankewitz for their insightful comments   on this work , and to Mike Wu for sharing useful   materials .   References467646774678   Appendix   A Additional Details on Concadia   A.1 Datasheet   A datasheet ( Gebru et al . , 2021 ) for Concadia is   included in our Github repository .   A.2 Occurrence Frequency Estimates   To estimate the sparsity of alt descriptions and cap-   tions for images on Wikipedia , we randomly sam-   pled 10,000 articles , extracted the number of im-   ages , and counted how many of those contained   captions and how many contained descriptions . We   then took the average proportion of captions and de-   scriptions per image for each article , which yields   the data distribution displayed in grey in Figure 5 .   A.3 Similarity of Captions and Descriptions   In Section 3.3 , we demonstrate that captions and   descriptions for the same image in Concadia are   semantically similar to each other using SBert em-   beddings . These results replicate when calculat-   ing similarity using Jaccard distance of the raw   strings , a more form - based and superficial per-   spective on the caption / description comparison .   Figure 6 summarizes this analysis . The random   pairings are significantly more distant from each   other than their ordered counterparts ( two - sample   Kolmogorov - Smirnov test : p < 0.0001 ) . The re-   sults still support the observation of distinctness   between captions and descriptions since the Jac-   card distance in the ordered set is still high ( 0.78 ) .   In Section 3.3 , we further present differences in   occurrence frequencies of the syntactic categories4679   proper nouns , nouns , and adjectives for descrip-   tions and captions , as illustrated in Figure 7 . How-   ever , the Part - of - Speech ( POS ) tag analysis using   the Python library NLTK ( Bird and Loper , 2004 )   allows for much more finegrained distinctions . Fig-   ure 8 contains all analyzed POS tags and shows   the proportion at which they occur in captions and   descriptions of the Concadia corpus , revealing in-   formative sub - patterns . For example , while there   is a clear over - representation of adjectives in de-   scriptions , this pattern qualitatively flips for su-   perlatives . We speculate that superlatives are po-   tentially more evaluative and therefore considered   less appropriate in descriptions . There is also a   clear over - representation of determiners in descrip-   tions , which we attribute to the higher frequency   of common nouns over proper names , since in En-   glish Jesse does n’t need a determiner to make an   utterance grammatical whereas person most often   does . Proportions are computed by dividing the   number of occurrences of a POS tag by the number   of all tokens in that specific text form . This allows   for a direct comparison between descriptions and   captions , even though captions contain more tokens   overall . Table 3 contains an overview of all POS   tags and their respective definition , as provided by   NLTK ( Bird and Loper , 2004 ) .   Section 3.3 also addresses qualitative semantic   differences between captions and descriptions . Due   to space constraints , we only use the most frequent   bigrams ( without stopwords ) to demonstrate these   differences in the main paper . Figure 9 illustrates   the most frequent trigram frequencies when in-   cluding stopwords . Similarly to the bigrams in   Figure 3 , the most frequent trigrams in captions   have instances of event descriptions and proper   names ( e.g. , the battle of , Tour de France ) . For   descriptions , there are again primarily descriptive   attributes for people ( e.g. , man wearing a , is wear-   ing a ) , and meta information about the image ( e.g. ,   ( black ) and white photograph , in the foreground ) .   A similar pattern arises when directly compar-   ing which words occur more frequently in captions   and descriptions , as shown in Figure 10 . Words   that lie on the dashed line occur equally often in   both text forms . Words below that line occur more   frequently in descriptions and words above it oc-   cur more often in captions . Words that occur more4680frequently in descriptions focus on descriptive ad-   jectives and nouns such as hat , bearded , orblouse .   Words that occur more frequently in captions rather   refer to an event , e.g. , award orwinner , and use   more evaluative language , e.g. , achieved .   Finally , a simple binary classifier based on the   SBert embeddings of the descriptions and captions   achieves an accuracy of 79.3 % ( train set : 79.5 % ,   dev set : 79.5 % ) providing further quantitative ev-   idence suggesting that descriptions and captions   carry distinguishable features .   These additional perspectives are in further sup-   port of the quantitative and qualitative analyses of   the principled ways in which the content of cap-   tions and descriptions differs ( see Section 3.3 ) .   A.4 Illustrations   In Figure 11 , we provide examples from Conca-   dia , illustrating a range of topics and image types :   photographs , drawings and paintings .   B Models   B.1 ResNet - LSTM   B.1.1 Architecture   Encoders Two pretrained encoders create im-   age and context representations which are input   to an LSTM with attention . As an image encoder ,   we use ResNet-101 ( He et al . , 2016 ) , pretrained   on ImageNet . For the context representation , we   use pretrained BERT ( Devlin et al . , 2019).When   predicting labels in absence of context , we induce   an uninformative context representation by setting   all values to 1 . This way , we ensure that potential   model performance improvements are not due to an   increase of model complexity when adding context .   This is also one of the reasons why neither image   nor context encoders were finetuned . To establish   the importance of input components , e.g. , context ,   we needed to fix the number of trainable param-   eters to avoid conflations from changes in model   complexity .   Decoder with Attention The initial input to the   LSTM hidden states of the decoder is a vector con-   catenating an image and a context representation .   The LSTM decoder generates each token in a pre-   dicted label based on a previous token , the previousPOS tags Explanation   CC Coordinating Conjunction   CD Cardinal Digit   DT / PDT Determiner / Predeterminer   EX Existential There   FW Foreign Word   IN Preposition / Subordin . Conjunct .   JJ / JJR / JJS Adj./Comparative / Superlative   LS List Marker 1   MD Modal   NN / NNS Noun Singular / Plural   NNP / NNPS Proper Noun Singular / Plural   POS Possessive Ending   PRP / PRP$ Personal / Possessive Pronoun   RB / RBR / RBS Adverb / Comparative / Superlative   RP Particle   TO to   UH Interjection   VB Verb , Base Form   VBD / VBN V , Past Tense / Past Participle   VBG V , Gerund or Present Participle   VBP / VPZ V , Sg . Present , non-3rd/3rd P.   WDT / WRB wh - determiner / wh - abverb   WP / WP$ wh - pronoun / possessive   hidden state , and an attention vector . The atten-   tion vector is obtained by using soft attention ( Xu   et al . , 2015 ; Bahdanau et al . , 2015 ) separately pa-   rameterized on the image and the context and then   concatenating the resulting representations .   B.1.2 Implementation   The ResNet - LSTM models were implemented in   PyTorch ( Paszke et al . , 2019 ) using a codebase that   has successfully replicated results from Xu et al .   ( 2015).Additional details on model training and   optimization will be available in our code release .   B.1.3 Hyperparameters and Training Details   We used the hyperparameters suggested in this   codebase . The only exception is that we used a   reduced batch size ( 32 instead of 80 ) since our   models that include context have more parameters .   Overview of hyperparameters :   •dimension of image encoder output : 2048   ( predetermined by the pretrained ResNet-101   He et al . ( 2016 ) output size)4681   •dimension of context encoder output : 768   ( predetermined by the pretrained Bert Dognin   et al . ( 2019 ) output size )   • dimension of word embeddings : 512   • dimension of attention linear layers : 512   • dimension of decoder RNN : 512   • dropout : 0.5   • loss function : cross - entropy loss   • optimizer : Adam ( Kingma and Ba , 2015 )   • batch size : 32   • learning rate for decoder : 4e-4   • clip gradients at an absolute value of 5   •regularization parameter for “ doubly stochas-   tic attention ” : 1The model contains 396 M parameters , of which   43 M are in the image encoder , 110 M are in the   context encoder , and the rest are in the decoder .   The model has 244 M trainable parameters , all of   which are in the decoder , as the encoders are frozen .   B.1.4 Compute   All models were trained and evaluated on AWS   EC2 . Description - generation models were trained   either on an g4dn.xlarge instance or an g5.xlarge in-   stance . All caption - generation models were trained   on an g5.xlarge instance . When training on an   g4dn.xlarge instance , each model takes about 1.5   days . When training on an g5.xlarge instance , each   model takes about 1 day . This includes evaluation   on the validation set after each epoch , where the   model generates a sequence with a beam size of 1   for each example .   For each model , evaluation on the test set takes   about 2 hours on a p2.xlarge instance . Evaluation   on the test set uses a beam size of 5 .   B.2 DenseNet - LSTM   The DenseNet - LSTM follows the same structure as   the ResNet - LSTM except that the ResNet-101 im-   age encoder is replaced by a DenseNet-161 . Con-   sequently , the models differ in the number of pa-   rameters . The DenseNet - LSTM contains 382 M   parameters , of which 26 M are in the image en-   coder , 110 M are in the context encoder , and the   rest are in the decoder . The model has 245 M train-   able parameters , all of which are in the decoder ,   as the encoders are frozen . For all other aspects,4682   consult the ResNet - LSTM specifications .   B.3 OSCAR(VinVL )   B.3.1 Architecture   OSCAR is a Transformer - based visual - language   model that is pretrained on a large - scale corpus and   can be finetuned for a variety of downstream vision-   language tasks ( Li et al . , 2020 ) , including image-   to - text generation . See Figure 13 for a diagram of   the architecture , and refer to Section 4 of Li et al .   2020 for the image - to - text finetuning procedure .   There are potentially many ways in which one   could integrate context into the model . We took   the approach of appending the context to the object   tags that OSCAR receives .   B.3.2 Implementation   We applied the VinVL feature extractor to im-   ages in Concadia using the VinVL codebase   available at https://github.com/microsoft/   scene_graph_benchmark . We adopted the OS-   CAR model implementation and training script   from the OSCAR codebase available at https :   //github.com / microsoft / Oscar .   B.3.3 Hyperparameters and Training Details   We used the pretrained BERT - based OS-   CAR(VinVL ) checkpoint as our starting point and   finetuned it on Concadia . We used the same hyper-   parameters as the original OSCAR implementation ,   with two exceptions : due to compute constraints ,   we fine - tuned OSCAR for 20 epochs ( compared to   40 ) and used a batch size of 32 ( instead of 256 ) .   Aside from optimizing for cross - entropy loss ,   the original implementation further finetuned the   model with CIDEr optimization as the objective .   Since it was n’t central to our investigation , we omit-   ted the CIDEr optimization step . By the end of   training , the differences between conditions had al-   ready persisted over several epochs and validation   set performance had converged .   The model contains ≈110 M parameters , 88 M of   which are trainable .   B.3.4 Compute   All models were trained and evaluated on AWS   EC2 g4dn.xlarge instances . Each model takes   about 2 days to train . This includes evaluation   on the validation set after each epoch , where the   model generates a sequence with a beam size of 1   for each example .   For each model , evaluation on the test set takes   about 8 hours on a g4dn.xlarge instance . Evaluation   on the test set uses a beam size of 5 .   B.4 Validation Set Performance   The test set performances in Table 2 are similarly re-   flected on Concadia ’s validation set . Table 4 shows   the validation set results . Note that evaluation on   the validation set uses a beam size of 1 ( different   from 5 on the test set ) .   C Human Subject Experiment   C.1 Participants   We recruited 421 participants over Amazon ’s Me-   chanical Turk , restricted to participation within the   US and a previous average approval rate above   98 % . Participants were paid $ 2.60 for participa-   tion with an average completion time of 12 minutes   ( $ 13 / hr ) . Participants were only allowed to com-   plete the experiment once.4683   C.2 Details on Materials & Procedure   Throughout the experiment , participants never saw   the same picture twice . Each participant saw at   least seven samples of each text condition , result-   ing in 28 trials . The last two trial conditions were   randomly sampled . An example trial as it appeared   to participants is provided in Figure 14 . Image se-   lection and trial order was completely randomized   and question order was randomized between par-   ticipants . The experiments themselves are released   as part of the Github repository .   C.3 Data Exclusions   Participants were excluded based on their self-   reported native language , their self - assessment of   task performance , two attention check questions ,   and their overall response . Firstly , we only in-   cluded participants who indicated that English is   among their native languages ( 12 exclusions ) , and   who reported that they thought they did the exper-   iment correctly ( 74 exclusions ) . We constructed   attention check questions where the text clearly   described something different than what was dis-   played in the image and excluded participants who   did n’t select the checkbox “ Ca n’t say because im-   age and text seem to be unrelated ” ( 85 exclusions ) .   Furthermore , we excluded participants who se-   lected the checkbox in more than 80 % of all trials   ( 6 exclusions ) . Overall , we excluded 177 partici - pants ( 42 % ) . Furthermore , we excluded trials from   the quantitative analysis where participants indi-   cated that text and image did n’t relate . If more   than 80 % of participants agreed that image and   text did n’t relate , we excluded that data sample   from our quantitative analysis as well ( 13,942 out   of 15,616 datapoints ( 89.3 % ) remain).4684