  Qinyuan YeMadian KhabsaMike Lewis   Sinong WangXiang RenAaron JaechUniversity of Southern CaliforniaMeta AI   { qinyuany,xiangren}@usc.edu   { mkhabsa,mikelewis,sinongwang,ajaech}@fb.com   Abstract   Distilling state - of - the - art transformer models   into lightweight student models is an effec-   tive way to reduce computation cost at in-   ference time . The student models are typi-   cally compact transformers with fewer param-   eters , while expensive operations such as self-   attention persist . Therefore , the improved in-   ference speed may still be unsatisfactory for   real - time or high - volume use cases . In this   paper , we aim to further push the limit of in-   ference speed by distilling teacher models into   bigger , sparser student models – bigger in that   they scale up to billions of parameters ; sparser   in that most of the model parameters are n-   gram embeddings . Our experiments on six   single - sentence text classiﬁcation tasks show   that these student models retain 97 % of the   RoBERTa - Large teacher performance on aver-   age , and meanwhile achieve up to 600x speed-   up on both GPUs and CPUs at inference time .   Further investigation reveals that our pipeline   is also helpful for sentence - pair classiﬁcation   tasks , and in domain generalization settings .   1 Introduction   Large pre - trained Transformers ( Devlin et al . , 2019 ;   Liu et al . , 2019 ) are highly successful , but their   large inference costs mean that people who host   low - latency applications , or who are simply con-   cerned with their cloud computing costs have   looked for ways to reduce the costs . Prior work   mainly achieves this by leveraging knowledge dis-   tillation ( Hinton et al . , 2015 ) , which allows for   the capabilities of a large well - performing model   known as the teacher to be transferred to a smaller   student model . For example , DistillBERT ( Sanh   et al . , 2019 ) is a smaller transformer model distilled   from BERT ( Devlin et al . , 2019 ) , which reduces   BERT ’s size by 40 % and becomes 60 % faster dur-   ing inference . However , such speed - up may be stillFigure 1 : Performance vs. Inference Speed . With   Deep Averaging Network ( DAN ; Iyyer et al . 2015 ) and   knowledge distillation , we obtain a student model with   competitive performance on IMDB dataset , while be-   ing 607x faster than RoBERTa - Large , and 20x faster   than bi - directional LSTMs at inference time .   insufﬁcient for high - volume or low - latency infer-   ence tasks . In this paper , we aim to further push   the limit of inference speed , by introducing Sparse   Distillation , a framework that distills the power   of state - of - the - art transformer models into a shal-   low , sparsely - activated , and richly - parameterized   student model .   Counter to the convention of using “ smaller ,   faster , [ and ] cheaper ” ( Sanh et al . , 2019 ) student   models , our work explores a new area of the design   space , where our fast and cheap student model is   actually several times larger than the teacher . The   student model we use is modiﬁed from Deep Aver-   aging Network ( DAN ) in Iyyer et al . ( 2015 ) . DANs   take a simple architecture by mapping the n - grams   in the input sentence into embeddings , aggregating   the embeddings with average pooling , and then us-   ing multiple linear layers to perform classiﬁcation   ( see Fig . 2 ) . This architecture is reminiscent of   the high expressive power of billion - parameter n-   gram models ( Buck et al . , 2014 ; Brants et al . , 2007 )   from before the existence of pre - trained language2361models . By selecting the n - gram vocabulary and   the embedding dimension , DANs also scale up to   billions of parameters . Meanwhile , the inference   costs are kept low as DANs are sparsely - activated .   One weakness of DANs is that they are restricted   in modeling high - level meanings in long - range con-   texts , as compared to the self - attention operator in   Transformers . However , recent studies have shown   that large pre - trained Transformers are rather in-   sensitive to word order ( Sinha et al . , 2021 ) and   that they still work well when the learned self-   attention is replaced with hard - coded localized at-   tention ( You et al . , 2020 ) or convolution blocks   ( Tay et al . , 2021 ) . Taken together , these studies   suggest that on some tasks it may be possible to get   competitive results without computationally expen-   sive operations such as self - attention .   To verify our hypothesis , we use six single-   sentence text classiﬁcation tasksand apply knowl-   edge distillation to DANs . We observe that the re-   sulting student models retain 97 % of the RoBERTa-   Large teacher performance on average . We also   show that our method falls outside of the Pareto   frontier of existing methods ; compared to a base-   line of distilling to a LSTM student , our method   gives comparable accuracy at less than 1/20 the   inference cost ( see Fig . 1 ) . Based on our empirical   results , we conclude that faster and larger student   models provide a valuable beneﬁt over existing   methods . We further examine our method ( 1 ) with   QQP , a sentence - pair task , ( 2 ) in privacy - preserving   settings ( i.e. , no access to task - speciﬁc data during   distillation ) , and ( 3 ) in domain generalization and   adaptation settings ( i.e. , student models are applied   and adapted to new data domains ) , where we ﬁnd   our method continues to bring improvements over   non - distillation baselines .   2 Sparse Distillation with DANs   2.1 Problem Deﬁnition   Our goal is to train an efﬁcient text classiﬁcation   model Mfor a given task T. In a n - way classiﬁca-   tion problem , the model Mtakes input text x , and   produces ˆy∈R , where ˆyindicates the likelihood   that the input xbelongs to category i. The task T   has a train set D and a validation / development   setD. Additionally , we assume access to a   large unlabeled corpus Cwhich is supposedly in   a domain relevant to task T. We comprehensively   evaluate the efﬁciency of the model Mby report-   ing : ( 1 ) accuracy on D , ( 2 ) inference speed , and   ( 3 ) the number of parameters in the model .   2.2 Method Overview   To train a text classiﬁer that is both efﬁcient and   powerful , we employ knowledge distillation ( Hin-   ton et al . , 2015 ) , by having a powerful teacher   model provide the supervision signal to an efﬁ-   cient student model . In particular , we are interested   in using sparse n - gram based models as our student   model . We explain the teacher and student model   we use in § 2.3 , the training pipeline in § 2.4 , and   implementation details in § 2.5   2.3 Models   Teacher Model . Fine - tuning a pre - trained trans-   former model is the predominant recipe for obtain-   ing state - of - the - art results on various text classiﬁca-   tion tasks . Our teacher model is a RoBERTa - Large   model ( Liu et al . , 2019 ) ﬁne - tuned on the training   setD of task T.   Student Model . Our student model is based on   the Deep Averaging Network ( DAN , Iyyer et al .   2015 ) with the modiﬁcation that we operate on n-   grams instead of just words . See Fig . 2 for an illus-   tration of the model architecture . Speciﬁcally , for   an input sentence x , a list of n - grams g , g , ... , g   are extracted from the sentence . These n - gram in-   dices are converted into their embeddings ( with   dimension d ) using an embedding layer Emb ( . ) .   The sentence representation hwill be computed as   the average of all n - gram embeddings , i.e. ,h=   Mean ( Emb(g),Emb(g ) , ... , Emb(g))∈R.   The sentence representation then goes through two2362   fully connected layers , ( W , b)and(W , b ) ,   to produces the ﬁnal logits ˆz , i.e. , ˆz = M(x ) =   W(ReLu ( Wh+b ) ) + b∈R. The logits   are transformed into probabilities with the Softmax   function , i.e. , ˆy = Softmax ( ˆz)∈R.   Remarks on Computation Complexity . Multi-   headed self - attention is considered the most expen-   sive operation in the teacher transformers , where   the computation complexity is O(m)for a se-   quence with msub - word tokens . The student   model , Deep Averaging Network ( DAN ) , can be   considered as pre - computing and storing phrase   representations in a large embedding table . By do-   ing so , the computation complexity is reduced to   O(m ) . However , unlike the teacher , the context is   limited to a small range , and no long - range infor-   mation ( beyond n - gram ) is taken into account by   the student model .   2.4 Training Pipeline   Our training pipeline is illustrated in Fig . 3 . It has   three stages : ( 1)We ﬁrst ﬁne - tune a RoBERTa-   Large model on the train set D of task T , and   use the resulting model as the teacher model . ( 2 )   We train the student model by aligning the predic-   tions of the teacher ( ˜y ) and the predictions of the   student ( ˆy ) on the union of unlabeled corpus Cand   the train set D. We align the predictions by   minimizing the KL divergence between the two dis-   tributions , i.e. ,L=/summationtext˜ylog . The resulting   student model is denoted as “ DAN ( KD ) ” . ( 3)We   further ﬁne - tune the student model from step ( 2 )   with the task train set D , and get a new student   model . This model is denoted as “ DAN ( KD+FT ) ” .   This third stage is optional .   2.5 Implementation Details   Determine N - gram Vocabulary . Our student   model takes in n - grams as input . We determinethe n - gram vocabulary by selecting the top |V|   frequent n - grams in D andC. For each down-   stream dataset , we compute the vocabulary sep-   arately . We use CountVectorizer with default   whitespace tokenization in sklearn ( Pedregosa   et al . , 2011 ) to perform this task . We set n-   gram range to be ( 1,4)and set|V|=1,000,000 ,   d= 1,000 , unless speciﬁed otherwise .   Optimization . The architecture of DAN is   sparsely - activated , and thus can be sparsely-   optimized to reduce memory footprint . To facilitate   this , we design a hybrid Adam optimizer , where we   use SparseAdamfor the sparse parameters ( i.e. ,   the embedding layer ) , and regular Adam for dense   parameters . This implementation helps to improve   speed and reduce memory usage greatly – we can   train a 1 - billion parameter DAN with the batch size   of 2048 at the speed of 8 batches / second , on one   single GPU with 32 GB memory .   Additional Details . Due to space limit , we de-   fer details such as hyper - parameters settings and   hardware conﬁgurations in Appendix A.   3 Experiment Settings   3.1 Data   Downstream Datasets . Following Tay et al .   ( 2021 ) , we mainly use six single - sentence classiﬁ-   cation datasets as the testbed for our experiments   and analysis . These datasets cover a wide range   of NLP applications . We use IMDB ( Maas et al . ,   2011 ) and SST-2 ( Socher et al . , 2013 ) for senti-   ment analysis , TREC ( Li and Roth , 2002 ) for ques-   tion classiﬁcation , AGNews ( Zhang et al . , 2015 )   for news classiﬁcation . We use Civil Comments2363   ( Borkan et al . , 2019 ) and Wiki Toxic ( Wulczyn   et al . , 2017 ) dataset for toxicity detection .   Knowledge Distillation Corpora . We manually   select a relevant unlabeled corpus Cbased on the   task characteristics and text domain . For exam-   ple , the IMDB and SST-2 models , which are tasked   with classifying the sentiment of movie reviews , are   paired with a corpus of unlabeled Amazon prod-   uct reviews ( Ni et al . , 2019 ) . TREC , a question   classiﬁcation task , is paired with PAQ ( Lewis et al . ,   2021 ) , a collection of 65 million questions . AG-   News , a news classiﬁcation task , is paired with CC-   News corpus ( Nagel , 2016 ) . For Civil Comments ,   a dataset for detecting toxic news comments , we   select the News subreddit corpus from ConvoKit   ( Chang et al . , 2020 ) , which is built from a previ-   ously existing dataset extracted and obtained by a   third party and hosted by pushshift.io . Details   of all datasets and corpora are listed in Table 1 .   3.2 Compared Methods   To comprehensively evaluate and analyze the   n - gram student models , we additionally experi-   ment with ( 1 ) training a randomly - initialized DAN   model with D , without knowledge distillation   ( “ from scratch ” ) ; ( 2 ) directly ﬁne - tuning general-   purpose compact transformers , e.g. , DistilBERT   ( Sanh et al . , 2019 ) , MobileBERT ( Sun et al . , 2020 ) ;   ( 3 ) using other lightweight architectures for the stu-   dent model , such as DistilRoBERTa ( Sanh et al . ,   2019 ) , Bi - LSTM ( Tang et al . , 2019 ) and Convolu-   tion Neural Networks ( Chia et al . , 2019 ) , in task-   speciﬁc distillation setting . We also quote perfor-   mance from ( Tay et al . , 2021 ) when applicable.4 Results and Analysis   4.1 Main Results   How well can DANs emulate the performance   of the teacher ? In Table 2 , we present the re-   sults on 6 single - sentence classiﬁcation datasets .   Firstly , we ﬁnd that in 5 out of the 6 datasets , the   gap between the teacher and the student model is   within 3 % . This suggests the power of simple n-   gram models may be underestimated previously ,   as they are typically trained from scratch , with-   out modern techniques such as pre - training and   knowledge distillation . This also echoes with a   series of recent work that questions the necessity   of word order information ( Sinha et al . , 2021 ) and   self - attention ( You et al . , 2020 ) , in prevalent trans-   former architectures . Secondly , we observe that   knowledge distillation help close more than half   the gap between the teacher model and the student   model trained from scratch . The effect is more   signiﬁcant with TREC dataset ( 13 % improvement ) ,   a 46 - way classiﬁcation problem , whose train set   has a small size of 5,452 . It is hard to estimate pa-   rameters of a large sparse model with merely 5,452   examples ; however , supervising it with large - scale   corpus and distillation target effectively densiﬁed   the supervision signals and help address the spar-   sity issues during model training .   How fast are DANs ? We have previously hy-   pothesized that DANs will have superior inference   speed due to its simple and sparse architecture . In   this section we quantify this advantage by compar-   ing the student model with the RoBERTa - Large   teacher model . We also include the baselines listed   in § 3.2 for a comprehensive comparison . For sim-   plicity , we use BPE tokenizer and re - use the embed-   ding table from RoBERTa - Large for our student   Bi - LSTM and CNN model . We use 2 - layer Bi-   LSTM with hidden dimension of 4 , 64 , 256 and   512 . For the CNN model , we use one 1D con-   volution layer with hidden dimension of 128 and   context window of 7 .   We provide speed comparison across all datasets   in Table 3 . We provide more ﬁne - grained com-   parison on IMDB dataset in Table 4 and Fig . 1 .   DAN achieves competitive performance and the   fastest inference efﬁciency among all different stu-   dent model architectures . The speed - up differs   across datasets , ranges from 4x to 1091x . It is most   signiﬁcant on Civil Comments ( 1091x ) , Wiki Toxic   ( 668x ) and IMDB dataset ( 607x ) , as they have2364   longer input sequences , and the complexity grows   quadratically with sequence length in transformer   models . Moreover , as shown in Table 4 , DAN has   an acceptable CPU inference speed , which greatly   reduce the hardware cost for inference . We believe   all these characteristics makes student DAN model   as an ideal option for production or real - time use   on single - sentence classiﬁcation tasks .   Simplest is the best : Exploring different design   choices for DAN . We try several modiﬁcations   to our current experiment pipeline , including ( 1 )   replace average pooling with max pooling , atten-   tive pooling , or taking sum in the DAN model ; ( 2 )   pre - compute a n - gram representation by feeding   the raw n - gram text to a RoBERTa - Large model ,   and using the representations to initialize the em-   bedding table of the student model ; ( 3 ) attach more   dense layers in the DAN ; ( 4 ) use even larger stu-   dent models by leveraging parallel training across   multiple GPUs . More details about these variations   are in Appendix B.1 . We experiment with IMDB   dataset and list the performance in Table 5 . In gen-   eral , we do not observe signiﬁcant performance   improvements brought by these variations . Thus ,   we keep the simplest design of DAN for all other   experiments.2365   4.2 Controlling the Parameter Budget   Given a ﬁxed parameter budget , how to allocate it   wisely to achieve optimal performance ? We discuss   this question in two scenarios : the users wish to   control the parameter budget ( 1 ) during knowledge   distillation ( KD ) , or ( 2 ) during inference .   During KD : Trade - off between vocabulary size   and embedding dimension . We explore how the   conﬁguration of vocabulary size and embedding di-   mension inﬂuence the student model performance .   We train student models on the IMDB dataset with   19 conﬁgurations , and show the results graphically   in Figure 4 . Detailed results are deferred in Table 8   in Appendix C. All else being equal , having more   parameters in the student model is beneﬁcial to the   performance . For a ﬁxed parameter budget , higher   accuracy was achieved by increasing the embed-   ding dimension and making a corresponding reduc-   tion in the vocabulary size . Our best performing   model has|V|= 1,000,000andd= 1,000 . We   keep this conﬁguration for the main experiments in   previous sections .   During inference : Reduce the model size with   n - gram pruning . The model size of DANs is   ﬂexible even after training , by excluding the least   frequent n - grams in the vocabulary . We test this   idea on IMDB and AGNews dataset and plot the   performance in Fig . 5 . We try two ways to estimate   n - gram frequency : ( 1 ) using distillation corpus C   and the training set D ; ( 2 ) using D only .   We observe that : ( 1 ) n - gram frequencies estimated   onD are more reliable , as Dhas a n - gram   distribution more similar to D compared to   C+D ; ( 2 ) DANs maintain decent accuracy   ( > 90 % ) even when the model size is cut to 3 % of   its original size . In this case , users of DANs can   customize the model ﬂexibly based on their needs   and available computational resources .   4.3 Privacy - preserving Settings   NLP datasets sometimes involve user generated   text or sensitive information ; therefore , data pri-   vacy can be a concern when training and deploying   models with certain NLP datasets . In this section ,   we modify our experiment setting to a practical   and privacy - preserving one . We assume the user   has access to a public teacher model that is trained   on private train dataset ( D ) , but does not has   access to D itself . This is realistic nowadays   with the growth of public model hubs such as Ten-   sorFlow Huband Hugging Face Models . After   downloading the model , the user may wish to de-   ploy a faster version of this model , or adapt the   model to the user ’s own application domain .   Knowledge Distillation without D.To   simulate the privacy - preserving setting , we remove   D from the knowledge distillation stage in our   experiment pipeline and only use the unlabeled cor-   pusC. We use “ DAN ( KD ) ” to denote this model   in Table 2 . By comparing “ DAN ( KD ) ” and “ DAN   ( KD ) ” , we found that the performance difference   brought by task speciﬁc data D is small for   all single - sentence tasks , with the largest gap be-   ing1.2%on IMDB dataset . This suggests that the   proposed pipeline is still useful in the absence of   task - speciﬁc data.2366   Domain Generalization and Adaptation . We   select the two sentiment analysis tasks : IMDB and   SST-2 , and further explore the domain generaliza-   tion / adaptation setting . Speciﬁcally , during stage   1 of our three - stage pipeline ( § 2.4 ) , we ﬁne - tune   the RoBERTa - Large model on a source dataset ;   during stage 2 , we apply knowledge distillation   with unlabeled corpus Conly and get the student   model ; during stage 3 , we further ﬁne - tune the stu-   dent model on the target dataset . The last step is   optional and serves to simulate the situation where   the user collects additional data for domain adapta-   tion . We list the results in Table 6 . With weakened   assumptions about the teacher model and distilla-   tion supervision , we still have observations similar   to those in our main experiments ( § 4.1 ): Perfor-   mance of the ﬁnal student model is signiﬁcantly   improved compared to DANs trained from scratch .   4.4 Limitations and Discussions   Extension to sentence - pair tasks . So far we   have limited the scope to single - sentence classiﬁ-   cation tasks . We consider extending our sparse dis-   tillation framework to a sentence - pair task , Quora   Question Pair ( QQP ) , which aims to identify du-   plicated questions . We create pseudo sentence - pair   data for knowledge distillation by randomly sam-   pling 10 million question pairs from PAQ . To bet-   ter model the relation between a pair of sentence ,   we modify DANs by introducing a concatenate-   compare operator ( Wang and Jiang , 2017 ) , fol-   lowing the practice in ( Tang et al . , 2019 ) . More   speciﬁcally , the two input sentences , xandx , go   through the embedding layer and average pooling   independently , resulting in two sentence represen-   tations , handh . We then apply the concatenate-   compare operator , i.e. , f(h , h ) = [ h , h , h⊙   h,|h−h| ] , where⊙represents element - wise   multiplication . Finally , f(h , h)go through two   fully connected layers for classiﬁcation , the same   as DANs for single - sentence tasks .   The results on QQP dataset is listed in the right-   most column in Table 2 . Firstly , knowledge distil-   lation still helps close the gap between RoBERTa-   Large and DANs trained from scratch ( 2 % improve-   ment ) and leads to a decent accruacy of 84.2 % ;   however the beneﬁt brought by KD is not as strong   as with single - sentence tasks . Secondly , the perfor-   mance of DAN(KD)(i.e . , without access to D   during KD ) is much worse than the performance of   DAN(KD ) . We hypothesize that this is due to the   quality and distribution of knowledge distillation   corpus . We randomly sample questions pairs as   the knowledge distillation examples , which may   not carry sufﬁcient supervision signals – more than   99 % of them are negative ( “ not duplicated ” ) ex-   amples . Creating more suitable distillation corpus   for sentence - pair tasks is beyond the scope of our   work , and we leave this as future work .   Impact of N - gram Coverage . One potential   drawback of n - grams ( based on white - space to-   kenization ) is that they can not directly handle out-   of - vocabulary words , while WordPiece / BPE tok-   enization together with contextualization can better   handle this issue . In Fig . 6 , we quantify the inﬂu-   ence of n - gram coverage on IMDB dev set . Here ,   n - gram coverage for an input sentence is deﬁned as   |G∩V|/|V| , where Grepresents the set of n - grams2367   in the sentence and Vis the n - gram vocabulary   ( § 2.5 ) . We ﬁrst group the instances into buckets of   n - gram coverage ( e.g. ,[40%,50%),[50%,60 % ) )   and then compute the statistics of cross - entropy   loss in each bucket . We observe that performance   is worse on sentences with more out - of - vocabulary   words . Future work may build upon this observa-   tion and improve DANs performance by addressing   out - of - vocabulary words . For example , BPE - based   n - grams may be used for creating the vocabulary .   Case study : What are DANs still not capable   of ? We take a closer look at the predictions made   by our DAN model ( student ) and the RoBERTa-   Large model ( teacher ) on the IMDB dataset . We list   several representative cases in Table 7 . These cases   typically require understating of complex language   phenomena , such as irony , conditional clauses , and   slang . In addition , these phenomena typically oc-   cur in contexts longer than 4 words , which DANs   are not capable of modeling by design . For exam-   ple , “ bad actors ” can mean “ good actors ” based on   the later context “ much funnier to watch ” . We con-   clude that sparse distillation is not suitable to cases   where modeling complex language phenomena has   a higher priority than improving inference speed .   Understanding the performance gaps . Tay   et al . ( 2021 ) advocate that architectural advances   should not be conﬂated with pre - training . Our ex-   periments further support this claim , if we con-   sider knowledge distillation as a “ substitute ” for   pre - training that provides the student model with   stronger inductive biases , and interpret the remain-   ing teacher - student performance gap as the differ-   ence brought by architectural advances . On the   other hand , we believe the power of DANs are   previously undermined due to the challenges in   optimizing large sparse models with limited su-   pervision . Our experiments show that knowledge   distillation effectively densify the supervision and   greatly improve the performance of DANs .   Additional Analysis and Speciﬁcations . Due   to space limit , we leave some additional analy-   sis and speciﬁcations in Appendix C. We discusstokenization speed ( Table 9 ) and impact of nin   n - grams ( Table 10 ) . We provide more detailed   speed comparison in Table 12 , model storage and   memory usage information in Table 11 . We pro-   vide ﬁne - grained n - gram coverage information in   Table 13 .   5 Related Work   Efﬁcient Transformers . Recent work attempts   to improve computation or memory efﬁciency of   transformer models mainly from the following per-   spectives : ( 1 ) Proposing efﬁcient architectures or   self - attention variants , e.g. , Linformer ( Wang et al . ,   2020a ) , Longformer ( Beltagy et al . , 2020 ) . Tay   et al . ( 2020 ) provide a detailed survey along this   line of work . ( 2 ) Model compression using knowl-   edge distillation , e.g. , DistillBERT ( Sanh et al . ,   2019 ) , MobileBERT ( Sun et al . , 2020 ) , MiniLM   ( Wang et al . , 2020b ) . These compressed models are   typically task - agnostic and general - purpose , while   in this work we focus on task - speciﬁc knowledge   distillation . ( 3 ) Weight quantization and pruning ,   e.g. , Gordon et al . ( 2020 ) ; Li et al . ( 2020 ) ; Kundu   and Sundaresan ( 2021 ) .   Task - speciﬁc Knowledge Distillation in NLP .   Researchers explored distilling a ﬁne - tuned trans-   former into the following lightweight architectures ,   including smaller transformers ( Turc et al . , 2019 ;   Jiao et al . , 2020 ) , LSTMs ( Tang et al . , 2019 ; Ad-   hikari et al . , 2020 ) and CNNs ( Chia et al . , 2019 ) .   Wasserblat et al . ( 2020 ) distill BERT into an archi-   tecture similar to DAN , however they restrict the   model to only take unigrams ( thus having small   student models ) , and adopt a non - standard low-   resource setting . To summarize , existing work typ-   ically focuses on reducing both number of param-   eter and the amount of computation , while in the   paper we study an under - explored area in the de-   sign space , where the amount of computation is   reduced by training a larger student model .   Reducing Contextualized Representations to   Static Embeddings . Related to our work , Etha-   yarajh ( 2019 ) and Bommasani et al . ( 2020 ) show2368how static word embeddings can be computed from   BERT - style transformer models . Ethayarajh ( 2019 )   suggest that less than 5 % of the variance in a word ’s   contextualized representation can be explained by a   static embedding , justifying the necessity of contex-   tualized representation . Bommasani et al . ( 2020 )   found that static embeddings obtained from BERT   outperforms Word2Vec and GloVe in intrinsic eval-   uation . These two papers mainly focus on post - hoc   interpretation of pre - trained transformer models us-   ing static embeddings . In our work we opt to use   knowledge distillation to learn n - gram embeddings .   Meanwhile we acknowledge that the technique in   Ethayarajh ( 2019 ) and Bommasani et al . ( 2020 )   could be used as an alternative method to convert   transformer models to fast text classiﬁers .   Sparse Architectures . In our work we aggres-   sively cut off computation cost by compensating   it with more parameters in the student model . Al-   ternatively , one could ﬁx the computational cost   at the same level as a transformer while greatly   expanding the parameter count , as explored in the   Switch Transformer ( Fedus et al . , 2021 ) . Both their   work and ours agree in the conclusion that scaling   up parameter count allows the model to memorize   additional useful information .   6 Conclusions & Future Work   We investigated a new way of using knowledge   distillation to produce a faster student model by re-   versing the standard practice of having the student   be smaller than the teacher and instead allowed the   student to have a large table of sparsely - activated   embeddings . This enabled the student model to   essentially memorize task - related information that   if an alternate architecture were used would have   had to be computed . We tested this method on   six single - sentence classiﬁcation tasks with mod-   els that were up to 1 billion parameters in size ,   approximately 3x as big as the RoBERTa - Large   teacher model , and found that the student model   was blazing fast and performed favorably .   We hope that our work can lead to further explo-   ration of sparse architectures in knowledge distilla-   tion . There are multiple directions for future work ,   including extending the DAN architecture to better   support tasks with long range dependencies like   natural language inference or multiple inputs like   text similarity . Additionally , more work is needed   to test the idea on non - English languages where   n - gram statistics can be different from English . Acknowledgments   We would like to thank Robin Jia , Christina Sauper ,   and USC INK Lab members for the insightful   discussions . We also thank anonymous review-   ers for their valuable feedback . Qinyuan Ye and   Xiang Ren are supported in part by the Ofﬁce of   the Director of National Intelligence ( ODNI ) , In-   telligence Advanced Research Projects Activity   ( IARPA ) , via Contract No . 2019 - 19051600007 ,   the DARPA MCS program under Contract No .   N660011924033 , the Defense Advanced Research   Projects Agency with award W911NF-19 - 20271 ,   NSF IIS 2048211 , NSF SMA 1829268 .   References236923702371   A Reproducibility   A.1 Datasets   Datasets and corpora used , and their speciﬁcations   are previously listed in Table 1 . Here we provide   links to download these data .   •IMDB : https://ai.stanford.edu/~ama   as / data / sentiment/   •SST-2 : https://huggingface.co/dataset   s / glue   •AGNews : https://huggingface.co/dat   asets / ag_news   •TREC : https://huggingface.co/dataset   s / trec   •CivilComments : https://huggingface.co   /datasets / civil_comments•WikiToxic : https://www.tensorflow.org   /datasets / catalog / wikipedia_toxicit   y_subtypes andhttps://meta.m.wikimed   ia.org/wiki/Research:Detox/Data_Rel   ease   •QQP : https://huggingface.co/dataset   s / glue   •Amazon Reviews : https://nijianmo.git   hub.io/amazon/index.html   •PAQ : https://github.com/facebookres   earch / PAQ   •Reddit News : https://zissou.infosci.c   ornell.edu/convokit/datasets/subredd   it - corpus / corpus - zipped / newreddits   _ nsfw~-~news / news.corpus.zip   QQP dataset has 363,846 training instances and   40,430 development instances . The average input   length is 13 tokens . We thank huggingface dataset   team ( Lhoest et al . , 2021 ) for providing easy access   to these datasets .   Licensing . For WikiToxic , the dataset is licensed   under CC0 , with the underlying comment text be-   ing governed by Wikipedia ’s CC - SA-3.0 . The PAQ   QA - pairs and metadata is licensed under CC - BY-   SA . The licensing information of other datasets are   unknown to us .   A.2 Implementation Details   N - gram pre - processing are implemented with   scikit - learn ( Pedregosa et al . , 2011 ) . Dis-   tilBERT ( Sanh et al . , 2019 ) and MobileBERT   baselines are implemented in huggingface   transformers ( Wolf et al . , 2020 ) . RoBERTa-   Large , BiLSTM , CNN , and DAN experiments are   implemented with fairseq ( Ott et al . , 2019 ) .   A.3 Hyperparameters   For ﬁne - tuning in stage 1 , we select batch size   from { 16 , 32 } and learning rate from { 1e-5 , 2e-   5 , 5e-5}following the recommendations in ( Liu   et al . , 2019 ) . We train the model for 10 epochs on   D. For knowledge distillation in stage 2 , we   set the batch size to be 2048 , learning rate to be   5e-4 , and total number of updates to be 1,000,000 ,   as they work well in our preliminary experiments .   The embedding table is randomly initialized and   the embedding dimension dis set to 1,000 , un-   less speciﬁed otherwise . For further ﬁne - tuning in2372stage 3 , we set the batch size to be 32 and select the   learning rate from{3e-4 , 1e-4 , 3e-5 } . We train the   model for 10 epochs on D. For all training pro-   cedures , we validate the model at the end of each   epoch in the case of ﬁne - tuning , or every 100,000   steps in the case of knowledge distillation . We save   the best checkpoint based on dev accuracy . Due   to the analysis nature of this work and the scale of   experiments , performance are computed using dev   set and based on one single run .   A.4 Hardware   Model Training . Except for the parallel training   attempt in Table 5 , all experiments are done on   one single GPU . We train DAN models on either   A100 40 GB PCIe or Quadro RTX 8000 depending   on availability . Knowledge distillation ( Stage 2 )   with 1,000,000 updates typically ﬁnishes within 36   hours .   Inference Speed Tests . All inference speed tests   are done with the batch size of 32 . GPU inference is   performed with one Quadro RTX 8000 GPU , and   CPU inference is performed with 56 Intel Xeon   CPU E5 - 2690 v4 CPUs .   B Additional Details   B.1 DAN Variations   Due to space limits we have omitted the details   for the DAN variations we studied in § 4.1 . We   introduce these variations in the following .   Attentive Pooling . We consider adding attentive   pooling to the DAN model to capture more com-   plicated relations in the input . Our attention layer   is modiﬁed from the one in ( Zhang et al . , 2017 ) .   we use the representation hafter mean pooling as   query , and each n - gram embedding e = Emb(g )   as key . More speciﬁcally , for each n - gram gwe   calculate an attention weight aas :   u = vtanh ( We+Wh ) ( 1 )   a = exp(u)/summationtextexp(u)(2 )   HereW , W∈Randv∈Rare learn-   able parameters . dis the dimension of the embed-   ding table , and dis the size of the attention layer .   To maintain an acceptable training speed , for at-   tentive pooling , we use a batch size of 512 during   knowledge distillation . Parallel Training We try further scaling up the   student model by splitting the gigantic embedding   table to different GPUs and enable parallel training ,   as implemented in Megatron - LM ( Shoeybi et al . ,   2019 ) . We train a 2 - billion parameter model in   parallel on two GPUs . The embedding dimension   is set to be 2,000 in total , and each GPU handles an   embedding table of hidden dimension 1,000 . The   vocabulary size is 1 million .   B.2 Comments on SparseAdam   SparseAdam is a modiﬁed version of the regular   Adam optimizer . For Adam , the ﬁrst and second   moment for each parameter is updated at every   step . This can be costly , especially for DAN , as   most parameters in the embedding layer are not   used during the forward pass . SparseAdam com-   putes gradients and updates the moments only for   parameters used in the forward pass .   C Additional Results   Speed Comparison . Table 12 is an extended ver-   sion of Table 4 which contains inference speed   comparison on IMDB and SST-2 dataset , in three   different settings ( GPU - FP32 , GPU - FP16 , CPU-   FP32 ) . Our major conclusion remains the same :   DANs achieve excellent inference speed in various   settings .   Vocabulary Size vs. Embedding Dimension   Trade - off . Table 8 contains original results that   were visualized in Fig . 4 .   N - gram Coverage Statistics . In our work , we   opt to determine the n - gram vocabulary with the   training set D and the corpus C , by selecting   the top 1 million n - grams according to frequency.2373N - gram range is set to be within 1 to 4 . For refer-   ence , we list statistics about the n - gram vocabulary   in Table 13 . It is possible that adjustments to this   pre - processing step ( e.g. , up - weighting n - grams in   D and down - weighting n - grams in C ) will fur-   ther improve performance , however we stop further   investigation .   Tokenization Speed . The speed comparison in   our work does not take pre - processing process   into account . When the inference speed is at mil-   lisecond level ( e.g. , with our DAN model ) , pre-   processing time can become non - negligible . For   reference , in Table 9 we report the tokenization   time on the 25,000 training instances in the IMDB   dataset with ( 1 ) n - gram tokenization ( used by   DAN , implemented with scikit - learn ) ; ( 2 ) BPE   tokenization ( used by RoBERTa / DistilRoBERTa ,   implemented with fairseq ) ; ( 3 ) WordPiece tok-   enization ( used by DistilBERT , implemented with   huggingface transformers ) .   First of all , by setting the number of workers   to be equal to the batch size ( 32 ) we use in the   speed test , the tokenization speed will be 48632   instances / sec (= 25000/16.45 * 32 ) , which is roughly   3x faster than the inference speed . Tokenization   speed is non - negligible in this case . Still , the main   conclusion from the speed comparison remains the   same : DANs are typically 10x-100x faster than the   compared models .   Secondly , DAN models still have better tok-   enization speed than transformer models that use   BPE / WordPiece tokenization . This is because our   DAN model computes n - grams based on whites-   pace tokenization , which can be done in linear time   when the n - gram to i d mapping is implemented   with a hashmap , i.e. ,O(n)where nis the input   length . BPE / WordPiece tokenization has higher   complexity according to Song et al . ( 2021 ) .   We would also like to emphasize that this part   is also highly dependent on the design choice and   implementation . For example , the user could im - plement a DAN model with BPE tokenzation . The   choice and optimization of tokenization is beyond   the scope of this work .   Impact of nin n - grams . Similar to the post - hoc   pruning experiments in § 4.2 , we gradually disable   the usage of four - grams , trigrams and bigrams at   inference time , and report the performance in Ta-   ble 10 .   Model Storage . In Table 11 we provide more de-   tails about the disk space and memory required for   using DAN models and the baseline models . Note   that the GPU memory listed below is the mem-   ory used to load the static model . During training ,   more memory will be dynamically allocated during   forward and backward passes . DAN uses smaller   memory during training because only a small por-   tion of the parameters are activated and trained   ( see the last row in Table 13 ) . In this way we are   able to use batch sizes as large as 2048 to train   DANs on one single GPU , which is not possible   for transformer based models .   D Potential Risks   It is risky to deploy DAN models to high - stakes   applications ( e.g. , medical decisions ) as the model   lacks the ability of understanding long context ( see   case study in § 4.4 ) . DANs may raise fairness con-   cerns : it lacks ability to understand the meaning   of words in context , so it may learn spurious corre-   lations such as overemphasis on group identiﬁers.2374   We believe a thorough analysis is needed and bias   mitigation methods such as ( Bolukbasi et al . , 2016 ;   Kennedy et al . , 2020 ) are necessary for combating   these issues.2375