  Nelson F. LiuTony LeeRobin JiaPercy LiangComputer Science Department , Stanford University , Stanford , CADepartment of Computer Science , University of Southern California , Los Angeles , CA   { nﬂiu , tonyhlee , pliang } @cs.stanford.edu   robinjia@usc.edu   Abstract   Do question answering ( QA ) modeling im-   provements ( e.g. , choice of architecture and   training procedure ) hold consistently across   the diverse landscape of QA benchmarks ? To   study this question , we introduce the notion of   concurrence — two benchmarks have high con-   currence on a set of modeling approaches if   they rank the modeling approaches similarly .   We measure the concurrence between 32 QA   benchmarks on a set of 20 diverse modeling   approaches and ﬁnd that human - constructed   benchmarks have high concurrence amongst   themselves , even if their passage and ques-   tion distributions are very different . Surpris-   ingly , even downsampled human - constructed   benchmarks ( i.e. , collecting less data ) and   programmatically - generated benchmarks ( e.g. ,   cloze - formatted examples ) have high concur-   rence with human - constructed benchmarks .   These results indicate that , despite years of in-   tense community focus on a small number of   benchmarks , the modeling improvements stud-   ied hold broadly .   1 Introduction   The NLP community has created a diverse land-   scape of extractive question answering ( QA )   benchmarks — their context passages may come   from different sources , their questions may focus   on different phenomena or be written by different   populations , or other aspects of the data collection   process may differ . Driven to improve benchmark   performance , researchers have proposed a variety   of QA modeling approaches . However , not all   benchmarks receive equal attention from the com-   munity ( Koch et al . , 2021 ) ; many QA modeling   approaches are developed on a small handful of   benchmarks , especially those with popular leader-   boards ( e.g. , SQuAD ; Rajpurkar et al . , 2016 ) . As   a result , it is conceivable that some modeling im-   provements may not hold because they are ( perhaps   inadvertently ) benchmark - speciﬁc , while othersFigure 1 : Two benchmarks have high concurrence   if they rank a set of modeling approaches similarly .   Surprisingly , we ﬁnd that human - constructed bench-   marks ( e.g. , SQuAD , NaturalQuestions ) have high con-   currence with other human - constructed benchmarks ,   downsampled human - constructed benchmarks , and   even programmatically - generated cloze benchmarks   ( e.g. , the Children ’s Book Test ; CBT ) . In addition , we   are able to construct synthetic benchmarks that have   high concurrence with human - constructed benchmarks   despite lacking natural language passages or questions .   ( e.g. , pre - training on more data ) hold more broadly .   In this work , we evaluate whether improvements   from modeling approaches hold ( e.g. , choices in   model architecture or training procedure)—if a par-   ticular modeling approach improves performance   when trained and evaluated on one benchmark ,   does it also improve performance on others ? Al-   though much existing work studies whether sys-   tems generalize ( i.e. , a model with a particular set   of parameters ; Jia and Liang , 2017 ; Talmor and   Berant , 2019 ; Miller et al . , 2020 ) , research value   often comes not from the systems themselves ( e.g. ,   model weights ) , but from the underlying ideas ,   techniques , and approaches . We study the com-   paratively under - investigated question of whether13186such modeling approaches generalize .   To study whether modeling improvements hold   across benchmarks , we introduce the notion of con-   currence . We say that two benchmarks have high   concurrence on a set of modeling approaches if   they rank the modeling approaches similarly . To as-   sess whether modeling improvements hold across   the space of QA benchmarks , we measure the con-   currence between 32 diverse QA benchmarks on a   testbed of 20 representative modeling approaches   introduced between 2016 and 2020 .   Overall , we ﬁnd that benchmarks that differ   substantially still often have high concurrence .   Human - constructed benchmarks ( e.g. , SQuAD and   MRQA NaturalQuestions ) have high concurrence   with each other , despite differences in crowdsourc-   ing setups , passage and question distributions , and   even linguistic phenomena of focus ( § 3 ) .   How different can a benchmark be , while   still maintaining high concurrence with human-   constructed benchmarks ? In § 4.1 , we investigate   the role of training dataset size by measuring con-   currence with downsampled training datasets ( e.g. ,   using 20 K SQuAD training examples rather than   the full 88 K ) . We ﬁnd that downsampled training   datasets are sufﬁcient for high concurrence with   other human - constructed benchmarks . In § 4.2 , we   measure concurrence between human - constructed   and programmatically - generated benchmarks ( e.g. ,   cloze - formatted or synthetic ) to better understand   the importance of human - written questions and pas-   sages . We ﬁnd that cloze - formatted benchmarks   have high concurrence with human - constructed   benchmarks , so human - written questions and pas-   sages are not strictly necessary for concurrence .   However , programmatically - generated synthetic   benchmarks ( e.g. , the bAbI task suite ) have low   concurrence . Having found this breaking point   of low concurrence , we construct two minimal   synthetic benchmarks that achieve high concur-   rence with human - constructed benchmarks , de-   spite lacking linguistic structure . Intuitively , the   benchmarks that concur with human - constructed   benchmarks are those that require model capabili-   ties that are also useful for better performance on   human - constructed benchmarks ( e.g. , identifying   paraphrase and lexical overlap ; § 4.3 - 4.5 ) .   Our results have several implications for the fu-   ture development of benchmarks and modeling ap-   proaches . To summarize :   1.Human - constructed benchmarks have highconcurrence with each other on our testbed   of 20 modeling approaches . The model-   ing approaches studied are not particularly   benchmark - speciﬁc and that their modeling   improvements largely hold across different   benchmarks , despite intense community focus   on a small number of benchmarks . This is   especially true of recent modeling improve-   ments driven by better pre - training , which is   largely downstream benchmark - agnostic .   2.Many benchmarks require reasoning over   predicate - argument structure ( e.g. , SQuAD ,   NewsQA , NaturalQuestions ) , and improve-   ments on these benchmarks also transfer to   more specialized benchmarks ( e.g. , HotpotQA   or MRQA DROP ) because ( 1 ) almost all   benchmarks involve reasoning over predicate-   argument structure and/or ( 2 ) better reason-   ing over predicate - argument structure is corre-   lated with improvements on other phenomena .   3.Human - constructed benchmarks are not   strictly necessary for improving performance   on other human - constructed benchmarks .   Synthetic benchmarks may be useful tools for   isolating , understanding , and improving on   particular model capabilities .   4.Downsampling benchmarks to as few as 10 K   training examples does not signiﬁcantly af-   fect concurrence , especially since recent pre-   trained modeling approaches have greater   sample efﬁciency . We recommend the com-   munity build benchmarks that are smaller but   more challenging ( e.g. , harder / more expensive   to label per - example ) .   5.Since human - constructed benchmarks have   high concurrence amongst themselves , we en-   courage researchers to seek diversity and build   benchmarks that explore qualitatively differ-   ent modeling capabilities that push research   in new directions .   2 Measuring Concurrence   Informally , we say that two benchmarks have high   concurrence on a set of modeling approaches if the   two benchmarks rank the modeling approaches sim-   ilarly . We compare the performance of a modeling   approach when trained and tested on one bench-   mark with its performance when trained and tested   on another benchmark — we use each benchmark ’s   original i.i.d . train - test split , so all evaluation is in-   domain . Repeating this process for many modeling13187approaches , we can assess whether performance   gains between modeling approaches are generally   preserved when moving between benchmarks .   Formally , deﬁne a benchmark Bas a pair of   datasets ( D , D ) , whereD⊆X×Y and   D⊆X×Y for an input spaceXand an output   spaceY. Asystem is a function s : X→Y ( i.e. , a   trained model with a particular set of parameters ) .   In contrast , a modeling approach ( i.e. , a neural   architecture coupled with a training procedure ) is   a functionathat takes in a training dataset D   and outputs a system . Let denote an evalua-   tion function , where ( a , B ) returns the perfor-   mance ( under a given evaluation function , e.g. , ex-   act match ) of a modeling approach awhen trained   on the train split of Band tested on the test split of   B. Finally , C ( B , B;A , ) is the con-   currence between the benchmarks BandBwith   respect to a set of modeling approaches Aand the   evaluation function . Leta∼uniform ( A ) ,   where uniform ( A)denotes the uniform distribu-   tion over the set of modeling approaches A. Deﬁn-   ing the random variables P= ( a , B)and   P= ( a , B ) , we ﬁnally deﬁne   C ( B , B;A , ) = ( P , P ) ,   where is some correlation function .   We use the SQuAD exact match ( EM ) metric as   our evaluation function , and we consider the   Pearson correlation coefﬁcient ( r ) and the Kendall   rank correlation coefﬁcient ( τ ) as our correlation   functions . The former measures whether the   relationship between model performance on the   two benchmarks is approximately linear , whereas   the latter measures whether pairwise rank com-   parisons between models are preserved between   benchmarks . As a rough guideline , we consider   τ > 0.8to be high concurrence , though interpret-   ing concurrence often requires more than compar-   ing overall correlation .   Extractive QA modeling approaches . To as-   sess concurrence in this work , we use a represen-   tative set of 20 diverse modeling approaches intro-   duced between 2016 to 2020 ( A ) . These model-   ing approaches include RaSoR ( Lee et al . , 2016 ) ,   BiDAF ( Seo et al . , 2017 ) , DocumentReader ( Chen   et al . , 2017 ) , QANet ( Yu et al . , 2018 ) , BiDAF++   ( Clark and Gardner , 2018 ) , MnemonicReader ( Hu   et al . , 2017 ) , FusionNet ( Huang et al . , 2018 ) , BERT   ( Devlin et al . , 2019 ) , ALBERT ( Lan et al . , 2020 ) ,   RoBERTa ( Liu et al . , 2019 ) , ELECTRA ( Clarket al . , 2020 ) , and SpanBERT ( Joshi et al . , 2020 ) .   10 of our 20 modeling approaches are non-   pretrained . These approaches generally propose   ( 1 ) better sequence encoders for passages and ques-   tions ( e.g. , Lee et al . , 2016 ; Yang et al . , 2017 ; Yu   et al . , 2018 ) and/or ( 2 ) improved attention mecha-   nisms for question - passage interactions ( e.g. , Seo   et al . , 2017 ; Wang et al . , 2017 ; Huang et al . , 2018 ) .   In contrast , the other 10 of our 20 model-   ing approaches are pre - trained ; these modeling   approaches all use the Transformer architecture   ( Vaswani et al . , 2017 ) , but improve performance by   proposing better pre - training procedures and objec-   tives . These pre - trained modeling approaches are   generally evaluated on a suite of downstream tasks ,   in contrast to non - pretrained modeling approaches ,   which generally evaluate on a single benchmark .   All of these modeling approaches were origi-   nally evaluated on SQuAD , though several ( e.g. ,   SpanBERT ) were also evaluated on other QA   benchmarks . We evaluate each modeling approach   on each benchmark with the same training hyperpa-   rameters used for SQuAD , as well as 5 additional   randomly sampled hyperparameter settings .   Extractive QA benchmarks . In this work , we   study concurrence between three broad classes of   extractive QA benchmarks : ( i ) human - constructed ,   ( ii ) cloze , and ( iii ) synthetic . Human - constructed   benchmarks contain human - written natural lan-   guage questions and passages ; examples include   SQuAD , NewsQA ( Trischler et al . , 2017 ) , and   NaturalQuestions ( Kwiatkowski et al . , 2019 ) . On   the other hand , cloze benchmarks ( e.g. , Children ’s   Book Test or CNN ; Hill et al . , 2016 ; Hermann et al . ,   2015 ) contain cloze questions , which are “ ﬁll - in-   the - blank ” statements with masked answers . These   questions are usually automatically - generated from   human - written natural language passages . Finally ,   synthetic benchmarks contain programmatically-   generated questions and passages ( e.g. , the bAbI   task suite ; Weston et al . , 2016 ) .   3 Do Modeling Improvements Hold   Across Human - Constructed   Benchmarks ?   Many extractive question answering benchmarks   are human - constructed — they contain human-   written natural language questions and passages.13188   However , differences in the data collection pro-   cedure may yield benchmarks with dramatically   different passage and question distributions . Do   modeling improvements hold across benchmarks   despite these differences ?   Setup . We study the concurrence between   six human - constructed benchmarks : SQuAD ,   NewsQA , NaturalQuestions , DROP ( Dua et al . ,   2019 ) , HotpotQA ( Yang et al . , 2018 ) , and QAMR   ( Michael et al . , 2018 ) . We use the MRQA versions   of NewsQA , NaturalQuestions , DROP , and Hot-   potQA ( Fisch et al . , 2019 ) . Table 2 summarizes   their high - level differences . See Appendix C.1 for   examples from human - constructed benchmarks .   3.1 Results   Human - constructed benchmarks have high   concurrence amongst themselves . Despite dif-   ferences in benchmark crowdsourcing setups , pas-   sage and questions distributions , and even linguis-   tic phenomena of interest , modeling improvements   generally hold across human - constructed bench-   marks ( Table 1 ) . Furthermore , concurrence is high   over both non - pretrained and pre - trained modelingapproaches ( Figure 2 ) .   For example , SQuAD , NewsQA , and Natu-   ralQuestions differ in their passage - question joint   relationship . In SQuAD , crowdworkers are em-   ployed to write questions given Wikipedia passages ,   but this results in questions with high lexical over-   lap with salient passage sentences . To minimize   such overlap in NewsQA , crowdworkers write ques-   tions given only bullet - point summaries of the pas-   sages , rather than the passages themselves . Finally ,   questions in NaturalQuestions are written indepen-   dently of their provided passage . These different   crowdsourcing protocols drastically affect the ease   and cost of benchmark construction , but SQuAD ,   NewsQA , and NaturalQuestions have high concur-   rence despite these differences .   Concurrence is high even when benchmarks   focus on different phenomena . We also see   that MRQA DROP and MRQA HotpotQA have   surprisingly high concurrence with other human-   constructed benchmarks ( e.g. , SQuAD and Natu-   ralQuestions ) , despite their relatively specialized   focus on particular linguistic phenomena ( numeri-   cal and multi - hop reasoning , respectively).This   suggests that modeling improvements on bench-   marks that target general reasoning over predicate-   argument structure also improve performance on   benchmarks that focus on different phenomena . We   hypothesize this occurs because benchmarks are   more similar than we ’d otherwise expect ( e.g. , due   to reasoning shortcuts ; Min et al . , 2019 ) , and better   reasoning over predicate - argument structure may   be generally useful for other phenomena of interest .   4 Exploring the Limits of Concurrence   Our results in § 3 indicate that human - constructed   benchmarks have high concurrence with each other,13189   despite differences in their phenomena of inter-   est and passage and question distributions . Just   how different can a benchmark be , while main-   taining high concurrence with human - constructed   benchmarks ? In § 4.1 we investigate the role of   training dataset size on concurrence — while larger   training datasets often yield better systems with   higher end - task accuracy , are they necessary for   comparing modeling approaches ? In § 4.2 , we mea-   sure concurrence between human - constructed and   cloze benchmarks to better understand the role of   human - written questions and passages in concur-   rence . Cloze benchmarks have high concurrence   with human - constructed benchmarks , indicating   that human - written questions and passages are not   necessary for concurrence with human - constructed   benchmarks . To take this to an extreme , § 4.3   evaluates concurrence between programmatically-   generated synthetic benchmarks ( the bAbI task   suite ) with human - constructed benchmarks . Our   results show that the bAbI tasks have low concur-   rence with human - constructed benchmarks . Hav-   ing found this breaking point , we work backwards   to build a minimal benchmark with high concur-   rence , which will enable us to better understand   sufﬁcient conditions for concurrence . In § 4.4 , we   construct a benchmark that has no linguistic struc-   ture or complex reasoning but still has high con-   currence with human - constructed benchmarks over   non - pretrained models . Finally , § 4.5 shows that a   synthetic benchmark that requires richer reasoning   between question and passage tokens can achieve   high concurrence with human - constructed bench-   marks on both pre - trained and non - pretrained mod-   eling approaches .   4.1 Downsampling Benchmarks   Many existing human - constructed extractive QA   benchmarks contain a large number of examples ,   increasing their cost of construction . For example ,   SQuAD has 87,599 question - answer pairs in its   training split . Are large training datasets necessary   for comparing modeling approaches ?   Setup . We study the extent to which subsamples   of SQuAD concur with the full SQuAD benchmark   ( 88 K examples ) and ﬁve other human - constructed   benchmarks . We experiment with randomly gen-   erated subsets of the SQuAD training set with 1 K ,   10 K , 20 K , 40 K , and 60 K training examples . We   use the original SQuAD development set ( ∼10 K   examples ) for evaluation .   Results . Downsampling the SQuAD training set   from 88 K to 20 K examples does not substantially   affect concurrence with the full SQuAD benchmark   and other human - constructed benchmarks ( Table 3 ) .   Concurrence is high on both non - pretrained and   pre - trained modeling approaches ( Figure 3 ) . Down-   sampling to 10 K examples slightly reduces concur-   rence with non - pretrained modeling approaches .   Concurrence with pre - trained models only begins   to degrades when using 1 K training examples , indi-   cating that few - shot settings are likely categorically   different and worth studying separately .   4.2 Cloze Benchmarks   To better understand the importance of human-   written questions and passages , we measure con-   currence between human - constructed benchmarks   and cloze benchmarks . Cloze extractive ques-   tion answering benchmarks contain cloze ques-   tions , which are “ ﬁll - in - the - blank ” statements13190   with masked answers . Large cloze benchmarks   are cheap to construct because examples can be   automatically generated by eliding spans from   naturally - occurring text . Although the passages   in cloze benchmarks are natural language , their ﬁll-   in - the - blank require more guessing from context ,   rather than the answer deduction typically found in   human - constructed benchmarks .   Setup . We study the Children ’s Book Test ( CBT ;   Hill et al . , 2016 ) , LAMBADA ( Paperno et al . ,   2016 ) , CNN ( Hermann et al . , 2015 ) , and ReCoRD   ( Zhang et al . , 2018 ) cloze benchmarks and measure   their concurrence with human - constructed bench-   marks on our testbed of modeling approaches . We   follow prior work ( Dhingra et al . , 2017 ) and eval-   uate on subsets of CBT where the answer token   is either a common noun ( CBT - CN ) or a named   entity ( CBT - NE ) . In addition , we use a subsampled   version of the CNN benchmark with 100 K training   examples to save compute . See Appendix C.2 for   examples from the cloze benchmarks we study .   Results . Despite using programmatically-   generated cloze questions , cloze benchmarks ( e.g. ,   CBT and LAMBADA ) can have high concurrence   with human - constructed benchmarks ( Table 4 ) .   On the other hand , CNN and ReCoRD have   lower concurrence with human - constructed bench-   marks , especially on non - pretrained modeling   approaches — the performance improvements   between pre - trained modeling approaches are still   largely preserved ( Figure 4 ) .   Concurrence on CNN is lower due to a pair of   outlier modeling approaches — DocumentReader ,   with and without external linguistic features . We   hypothesize that these models do poorly on CNN   because some aspects of their preprocessing are   SQuAD - speciﬁc ; this may have also inﬂuenced   architecture design . ReCoRD ’s low overall con-   currence comes from the poor performance of non-   pretrained modeling approaches . This may be due   to ReCoRD ’s construction procedure , since a ﬁlter-   ing step removed all examples that were correctly13191answered by a strong non - pretrained modeling ap-   proach ( SAN , with SQuAD dev . EM of 76.24 ; Liu   et al . , 2018 ) . ReCoRD has low concurrence with   SQuAD on modeling approaches that are weaker   than SAN , and high concurrence on modeling ap-   proaches that outperform SAN .   4.3 High Concurrence Is Not Universal :   Improvements Do Not Hold On bAbI   Having established that human - written passages   are not necessary for high concurrence with human-   constructed benchmarks ( § 4.2 ) , we take this to an   extreme by evaluating concurrence between human-   constructed benchmarks and synthetic extractive   question answering benchmarks , which contain   questions and passages that are programmatically   generated ( and possibly not even natural language ) .   The bAbI task suite contains 20 synthetic question-   answering benchmarks , each of which focuses on   a particular skill required by a competent dialogue   system ( e.g. , fact retrieval , subject - object relations ,   counting ) . The textual data is generated from a   simulated toy environment .   Setup . We consider the 11 tasks that can be loss-   lessly converted to an extractive format ( Tasks 1 , 2 ,   3 , 4 , 5 , 11 , 12 , 13 , 14 , 15 , 16 ) . For each task , we   use the two ofﬁcially - released data settings : one   setting has 900 training examples and 100 devel-   opment examples , and the other has 9,000 training   examples and 1,000 development examples . In this   section , we focus on the setting with 900 training   examples , since all modeling approaches do nearly   perfectly on almost all tasks with 9,000 examples   ( Appendix D.3 ) . See Appendix C.3 for examples   from the existing synthetic benchmarks we study .   Results and Discussion . The bAbI tasks   have low concurrence with human - constructed   benchmarks — high concurrence is not universal .   Modeling approaches often have either near - perfect   or near - random performance ( Figure 5 ) .   4.4 What is Sufﬁcient for Concurrence on   Non - Pretrained Modeling Approaches ?   To better understand the sufﬁcient conditions   for concurrence with human - constructed bench-   marks , we are interested in constructing a mini-   mal synthetic benchmark with high concurrence .   Given that human - written passages and ques-   tions are not necessary for high concurrence with   human - constructed benchmarks ( § 4.2 ) , but the   programmatically - generated bAbI synthetic bench - marks have low concurrence ( § 4.3 ) , we design a   minimal synthetic benchmark with high concur-   rence with human - constructed benchmarks over   non - pretrained modeling approaches .   Setup . Questions in extractive QA benchmarks   can often be answered by exploiting lexical overlap   between question and passage tokens ( Weissenborn   et al . , 2017 ; Krishna et al . , 2020 ) . To better under-   stand the limits of concurrence , we build a minimal   synthetic cloze benchmark ( FuzzySyntheticQA )   that explicitly targets this fuzzy pattern - matching   and ﬁnd that it has high concurrence with SQuAD   on non - pretrained modeling approaches . Figure 6   shows a sample passage and question - answering   pairs . We use 10,000 questions for training and   10,000 questions for evaluation . See Appendix E   for further details about FuzzySyntheticQA ’s con-   struction .   Passage Generation . We generate the passage   by randomly sampling 150 tokens from the uniform   distribution over a token vocabulary . The token   vocabulary is taken from the WikiText-2 training   set ( Merity et al . , 2017 ) and has 68,429 types .   Answer Generation . The answer token is ran-   domly selected from the generated passage .   Cloze Question Generation . To generate the   cloze question , we ﬁrst extract the answer token ’s   local context ( up to 10 tokens ) and mask out the   answer token . Then , we corrupt the cloze question   by ( 1 ) randomly replacing its tokens with related to-   kens ( 100 approximate nearest neighbor tokens in   the vocabulary , measured by vector distance in the   pre - trained English FastText embeddings ) , ( 2 ) lo-   cally permuting its tokens ( within 3 positions ) , and   ( 3 ) applying word dropout ( with rate 0.2 ) .   Results and Discussion . FuzzySyntheticQA has   high concurrence with human - constructed bench-   marks , but only on non - pretrained modeling   approaches — concurrence on pre - trained modeling   approaches is much lower ( Figure 7 ) . Even bench-   marks that lack much linguistic structure can have   high concurrence with human - constructed bench-   marks , as long as they require similar phenomena   ( in this case , fuzzy lexical matching between the   question and passage ) .   Why do improvements in pre - training not hold   on FuzzySyntheticQA ? One potential reason is that   passages in FuzzySyntheticQA lack of linguistic   structure . To evaluate this hypothesis , we gen-   erate FuzzySyntheticQA questions from English   Wikipedia passages , rather than sampling from the13192   uniform distribution over tokens , but this still re-   sults in low concurrence with human - constructed   benchmarks on pre - trained modeling approaches   ( r=−0.49,τ=−0.19 ) , indicating that the low   concurrence comes from more than just a lack of   natural language passages ( Appendix F ) .   4.5 What is Sufﬁcient for Concurrence on   Pre - Trained and Non - Pretrained   Modeling Approaches ?   Having found a minimal synthetic benchmark   that achieves high concurrence with human-   constructed benchmarks on non - pretrained mod-   eling approaches ( § 4.4 ) , we show that a synthetic   benchmark that requires richer reasoning between   question and passage tokens is sufﬁcient for high   concurrence on both non - pretrained and pre - trained   modeling approaches .   Setup . We construct WikidataSyntheticQA , a   benchmark derived from Wikidata triples ; Figure 8   shows a sample passage and question - answering   pairs . Knowledge graphs like Wikidata are rich   sources of complex relations between entities ,   which enables us to increase the complexity of   question - passage token relations beyond the sim-   ple noising and corruptions of FuzzySyntheticQA .   We use 10,000 questions for training and 9,835   question - answer pairs for evaluation . See Ap-   pendix G for further details about WikidataSyn-   theticQA ’s construction .   Wikidata Background . Wikidata is a knowl-   edge graph connecting entities via relations . Wiki-   data entities and relations include a label , the most   common name that an entity is known by , and   aliases , alternative names for entities . For example ,   the entity Mae_C._Jemison has the label “ Mae C.   Jemison ” , with aliases “ Mae Jemison ” and“Mae   Carol Jemison ” . We treat labels and aliases as po-   tential surface realizations of entities and relations.13193Generation Preliminaries . Generating a pas-   sage requires a set of Wikidata triples . To select   these triples , we ﬁrst randomly choose a seed entity   from the 10,000 Wikidata entities with the highest   PageRank score ( Page et al . , 1999 ) . We then ex-   tract the triples from the seed entity and all entities   connected to the seed entity . Finally , we randomly   sample 50 triples for use in generation .   Passage Generation . Given the set of 50 Wiki-   data triples , we realize triples into textual surface   forms by selecting a random Wikidata label or alias   for each triple element . The ﬁnal passage is formed   by concatenating the realizations of all triples and   adding a delimiter token between them to mimic   sentential structure .   Answer Generation . We generate an answer   span by selecting a random triple used in the pas-   sage generation process , and then choosing a ran-   dom element of that triple . The passage realization   of this random element is the answer span .   Cloze Question Generation . To generate the   cloze question , we take the triple used for answer   generation and mask out the particular element   marked as the answer . We realize the non - answer   triple elements into textual forms by selecting a ran-   dom Wikidata label or alias for each triple element .   Then , we optionally and randomly replace the pred-   icate with its inverse ( if one exists ) , reversing the   subject and the object to maintain consistency . We   also optionally and randomly replace the remaining   unmasked entity ( i.e. , the triple subject or object   that was not masked ) with one of its hypernyms ,   challenging models ’ knowledge of such relations .   Results and Discussion . As Figure 7 shows ,   WikidataSyntheticQA has high concurrence with   human - constructed benchmarks , despite its lack of   natural language passages or questions .   We hypothesize that WikidataSyntheticQA has   higher concurrence with human - constructed bench-   marks than FuzzySyntheticQA because correctly   answering its examples often requires reasoning   about hypernymy relations between entities and   inverse relations between predicates — it is con-   ceivable that pre - trained modeling approaches   are better - equipped to handle and use these lex-   ical relations . In addition , the Wikidata aliases   provide sufﬁcient lexical variation such that the   benchmark is not trivially solvable through string   pattern - matching ( removing aliases from the gen-   eration procedure results in near - perfect perfor-   mance from all modeling approaches ) . In contrast , high performance on FuzzySyntheticQA simply re-   quires matching similar tokens in the passage and   question — models can achieve high performance   by simply learning the similarity relationships in   the FastText vector space .   5 Related Work   A recent line of work examines whether systems   have overﬁt to particular test sets by taking existing   systems and evaluating them on newly - constructed   test sets ( Recht et al . , 2019 ; Yadav and Bottou ,   2019 ; Miller et al . , 2020 ) . Recent work has also   studied whether higher - performing systems are   more robust by studying the correlation between   in - domain and out - of - domain improvements ( Taori   et al . , 2020 ; Djolonga et al . , 2020 ) .   In contrast , this work examines whether im-   provements from modeling approaches hold across   benchmarks . We train and test modeling ap-   proaches on a variety of existing and newly-   constructed benchmarks . In this regard , our work   is similar to the study of Kornblith et al . ( 2019 ) ,   who ﬁnd that performance improvements on Im-   ageNet are well - correlated with performance im-   provements on other benchmarks .   6 Conclusion   This work studies whether QA modeling improve-   ments hold across the diverse landscape of QA   benchmarks . We develop the notion of concur-   rence , which quantiﬁes the similarity between   benchmarks ’ rankings of modeling approaches .   Experiments with 32 QA benchmarks and 20 di-   verse modeling approaches indicate that human-   constructed benchmarks largely have high concur-   rence amongst themselves , even when their passage   and question distributions or linguistic phenomena   of focus are very different . To better understand   how different benchmark attributes affect concur-   rence , we explore downsampled benchmarks and   various programmatically - generated benchmarks ,   the latter having high concurrence only when they   target phenomena that are also useful for better per-   formance on human - constructed benchmarks ( e.g. ,   identifying paraphrase and lexical overlap ) . Our   results indicate that the modeling improvements   studied hold broadly , despite years of intense com-   munity focus on a small number of benchmarks.13194Acknowledgements   We thank the anonymous reviewers for their feed-   back and comments that helped improve this work .   NL was supported by an NSF Graduate Research   Fellowship under grant number DGE-1656518 .   Other funding was provided by a PECASE Award .   Limitations   While we conducted an extensive set of experi-   ments to gain a broad picture of whether modeling   improvements hold between benchmarks , it is al-   ways possible to investigate more settings . While   our study covers a representative set of 20 non-   pretrained and pre - trained modeling approaches ,   it is conceivable that evaluating more modeling   approaches ( or a different set of modeling ap-   proaches ) on additional benchmarks ( or a differ-   ent set of benchmarks ) would have led to different   results .   Furthermore , although we evaluate each mod-   eling approach on each benchmark with the same   training hyperparameters used for SQuAD , as well   as 5 additional randomly sampled hyperparame-   ter settings ( 20 ×32×6 = 3840 experiments in   total ) , it is possible that the SQuAD hyperparam-   eters for some modeling approaches happen to be   more general than other modeling approaches . Ide-   ally , each modeling approach would be individually   tuned to maximize performance on every bench-   mark , but doing so requires prohibitive amounts of   compute and researcher effort — we believe that our   experiments have enough coverage with respect to   hyperparameter optimization .   References131951319613197   A Implementation Details of Modeling Approaches Evaluated   We evaluated a representative subset of 20 extractive question answering modeling approaches , published   between 2016 to 2020 ( Table 5 ) . Below , we describe implementation details for all the modeling   approaches evaluated .   RaSoR We reimplement the RaSoR model of ( Lee et al . , 2016 ) with PyTorch in the AllenNLP ( Gardner   et al . , 2018 ) framework , following the original paper as closely as possible . While the authors released   an implementation of their method ( github.com/shimisalant/rasor ) , the codebase is in Theano and   inexplicably fails on passages that are signiﬁcantly longer than those found in SQuAD ( e.g. , those found   in the CNN benchmark ) .   BiDAF We use the reimplementation of BiDAF ( Seo et al . , 2017 ) found in AllenNLP ( Gardner et al . ,   2018 ) .   DocumentReader ( with and without external features ) We use an reimplementation of   DocumentReader ( Chen et al . , 2017 ) released at github.com/felixgwu/FastFusionNet . The original   DocumentReader approach uses external features from a part - of - speech tagger and named entity recogni-   tion system . To fairly compare to systems that do not use such external resources , we also run the models   without these features . We keep the hand - crafted term - frequency and token exact match features deﬁned   in the DocumentReader paper .   We also make some changes to the DocumentReader preprocessing code . In particular , the original   implementation ( github.com/facebookresearch/DrQA ) of these two modeling approaches ( intended for   training and evaluation on SQuAD ) replaces all tokens without a pre - trained GloVe embedding ( trained   on 840B tokens from the Common Crawl ) with a special unknown token — the reimplementation we use   adopts the same practice . This preprocessing assumption works well for SQuAD , since the vast majority   of SQuAD tokens also appear in the GloVe vocabulary . However , this preprocessing assumption does13198not apply to CNN — many of the special @entity Nand@placeholder markers , which anonymize entities   to prevent models from deriving answers from world knowledge , are not in the GloVe vocabulary . As a   result , the original DocumentReader implementation maps them all to a single unknown token , effectively   preventing the model from telling valid answer choices apart and yielding a model that performs no better   than the majority baseline . Keeping these special tokens in the model ’s vocabulary enables differentiating   between different entities in a passage , which naturally improves performance ( and are the reported   numbers)—however , the modeling approaches ’ improvements on SQuAD still do not transfer to CNN .   BiDAF++ We modify an AllenNLP ( Gardner et al . , 2018 ) reimplementation of the BiDAF++ Clark   and Gardner ( 2018 ) model originally used in pair2vec ( Joshi et al . , 2019 ) for evaluation on SQuAD 2.0   ( Rajpurkar et al . , 2018 ) .   MnemonicReader We use an reimplementation of MnemonicReader ( Hu et al . , 2017 ; note the spe-   ciﬁc arXiv revision ) released at github.com/HKUST-KnowComp/MnemonicReader . In particular , the   reimplementation is of the vanilla MnemonicReader without reinforcement learning .   QANet We use the reimplementation of QANet ( Yu et al . , 2018 ) found in AllenNLP ( Gardner et al . ,   2018 ) . This reimplementation was used as a baseline method for DROP ( Dua et al . , 2019 ) .   FusionNet We use an reimplementation of FusionNet ( Chen et al . , 2017 ) released at   github.com/felixgwu/FastFusionNet . This reimplementation was used as a baseline in Wu et al . ( 2019 ) .   Drawing inspiration from DocumentReader , the FusionNet approach also uses external features from a   part - of - speech tagger and named entity recognition system . As a result , we also run the models with-   out these features to fairly compare to systems that do not use such external resources . We keep the   hand - crafted term - frequency and token exact match features originally used in the FusionNet paper .   BERT ( base , large , and wwm ) We use the HuggingFace Transformers ( Wolf et al . , 2020 ) library to   ﬁne - tune BERT ( Devlin et al . , 2019 ) on extractive question answering benchmarks . In particular , we use   the base , uncased , BERT pre - trained model , the large , uncased , BERT pre - trained model , and the large ,   uncased , BERT model pre - trained with whole - word masking .   ALBERT ( base and xxlarge ) We use the HuggingFace Transformers ( Wolf et al . , 2020 ) library to   ﬁne - tune ALBERT ( Lan et al . , 2020 ) on extractive question answering benchmarks . In particular , we use   the base and xxlarge V1 ALBERT pre - trained models .   RoBERTa ( base and large ) We use the HuggingFace Transformers ( Wolf et al . , 2020 ) library to ﬁne-   tune RoBERTa ( Liu et al . , 2019 ) on extractive question answering benchmarks . In particular , we use the   base and large RoBERTa pre - trained models .   ELECTRA ( base ) We use the HuggingFace Transformers ( Wolf et al . , 2020 ) library to ﬁne - tune the   ELECTRA base discriminator ( Clark et al . , 2020 ) on extractive question answering benchmarks .   SpanBERT ( base and large ) We use the author - released codebase   ( github.com/facebookresearch/SpanBERT ) to ﬁne - tune SpanBERT ( Joshi et al . , 2020 ) on ex-   tractive question answering benchmarks . In particular , we use the base and large SpanBERT pre - trained   models .   B Preprocessing Existing Benchmarks   B.1 Existing Human - Constructed Benchmarks   We use the MRQA NewsQA , MRQA DROP , and MRQA HotpotQA benchmarks exactly as released   by the MRQA 2019 shared task ( Fisch et al . , 2019 ) . The passages in MRQA NaturalQuestions contain   HTML entities ( e.g. , < P > and</P > ) . The tokenizers used in non - pretrained models frequently split   these entities into separate tokens . For example , < P > may become < , P , and > . This is problematic   because the entities are quite common in passages , and expanding them during tokenization drastically   increases the passage lengths , which some non - pretrained modeling approaches can not handle due to GPU   memory limits . HTML entities are tokenized like this because they contain non - alphanumeric characters .   As a result , we normalize HTML entities by replacing the non - alphanumeric characters . For example ,   < P > becomesBPB , and</P > becomesEEPE . These tokens are correctly kept intact . It ’s possible that13199modeling approaches that use subword information will perform worse with these normalized HTML   entities , but we empirically observe that this normalization does not have a measurable impact on model   performance .   QAMR questions were originally collected at the sentence level , but we concatenate these sentences to   reconstruct the original passages they were sourced from . We then pair these reconstructed passages with   the original QAMR questions . It ’s possible for questions to become unanswerable at the passage - level .   One case of his happens when two sentences have the same question — we ﬁlter out such questions that   are asked for multiple sentences in a reconstructed passage . Questions can also become unanswerable   if relations between entities change between sentences . For example , given the passage “ Bill lived in   California in 1920 . Bill lived in Washington in 1921 . ” , the question “ Where did Bill live ” is answerable   within the context of a particular sentence , but not in the context of the entire passage . Manual examination   of generated QAMR passages and questions suggests that this case is rather uncommon , but it may still   introduce a small amount of noise into the benchmark .   B.2 Existing Cloze Benchmarks   To convert the CBT and CNN benchmarks to extractive format , we take the passages and question   as - is . The answer span is designated as the ﬁrst occurrence of the answer token in the passage . To   convert LAMBADA into extractive format , we follow the setup of Cheng and Erk ( 2020 ) . The ReCoRD   benchmark is used as - is , since it includes span - level annotations of answer tokens in passages .   B.3 Existing Synthetic Benchmarks   We consider tasks 1 , 2 , 3 , 4 , 5 , 11 , 12 , 13 , 14 , 15 , 16 . The other tasks can not be converted to extractive   format ( e.g. , they require “ yes”/“no ” answers that do not appear in passages ) . To convert the tasks in the   bAbI benchmark to extractive format , we take the passages and question as - is . While the bAbI benchmark   does not provide character - level span annotations for answers , questions come with “ supporting facts ” —   sentences in the passage that contain the answer . Thus , choose the ﬁrst occurrence of the answer token in   the supporting fact sentence as our answer span .   Some of the bAbI tasks , while usable in an extractive format in theory , can not be trivially converted   to the extractive format via the procedure above because the released benchmark ’s annotations do not   appear in the passage . For instance , consider Figure 9 , which shows an example drawn from the training   set of Task 15 . The answer provided in the benchmark is “ cat ” , although this token never appears in   the passage — instead , “ cats ” does . In cases where the originally - labeled answer can not be found in the   supporting fact , but its pluralization is present , we use the pluralized answer as our answer span.13200C Examples From Existing Benchmarks   C.1 Examples From Existing Human - Constructed Benchmarks   Table 6 shows examples from the existing human - constructed benchmarks we study.13201C.2 Examples From Existing Cloze Benchmarks   Table 7 shows examples from the existing cloze benchmarks we study.13202C.3 Examples From Existing Synthetic Benchmarks   Table 8 shows examples from the existing synthetic benchmarks we study . The contents of this table are   reproduced from Weston et al . ( 2016).13203D Full Results on Existing Benchmarks   D.1 Full Results on Existing Human - Constructed Benchmarks   Table 9 and Table 10 show the performance of each modeling approach on each existing human - constructed   benchmark.13204D.2 Full Results on Existing Cloze Benchmarks   Table 11 and Table 12 show the performance of each modeling approach on each existing cloze benchmark.13205D.3 Full Results on Existing Synthetic Benchmarks   Table 13 and Table 14 and Table 15 show the performance of each modeling approach on each existing of   the bAbI tasks ( 900 training examples ) .   Figure 10 shows how well the bAbI tasks ( 9000 ) training examples concur with SQuAD .   Table 16 and Table 17 and Table 18 show the performance of each modeling approach on each existing   of the bAbI tasks ( 9000 training examples).13206132071320813209E FuzzySyntheticQA Construction Details   Figure 11 provides an overview of the construction of FuzzySyntheticQA .   To efﬁciently replace tokens with related tokens , we consider each token ’s 100 approximate nearest   neighbors as replacement candidates . In particular , we use Annoy ( Bernhardsson and the Annoy develop-   ment team , 2020 ) to perform the approximate nearest neighboor look - ups . Similarities are derived from   the Euclidean distance of normalized vectors between two tokens.13210F Full Results on FuzzySyntheticQA   Figure 12 shows that changing the passage generation method in FuzzySyntheticQA has a minimal effect   on concurrence . We experiment with generating passages from a 3 - gram language model , a probabilistic   context - free grammar , a large neural language model ( GPT-2 1.5B ; Radford et al . , 2019 ) , and by taking   real Wikipedia paragraphs .   The 3 - gram language model is trained with maximum likelihood estimation on WikiText-103 ( Merity   et al . , 2017 ) . The PCFG is trained with maximum likelihood estimation on the Penn Treebank ( Marcus   et al . , 1993 ) . Lastly , we take GPT-2 1.5B generations from the ofﬁcially - released output samples   ( github.com/openai/gpt-2-output-dataset ; generated with top - k truncated sampling with k = 40 ) .   Table 19 and Table 20 show the performance of each modeling approach on each of our constructed   synthetic fuzzy pattern - matching benchmarks.1321113212 G WikidataSyntheticQA Construction Details   Figure 13 summarizes the data generation procedure for WikidataSyntheticQA .   Inverses of Properties . Some of our generated questions use the inverse relationships between two   properties . To obtain the inverse relationship for a given property , we ﬁrst retrieve its list of property   constraints by using Wikidata property P2302 ( property constraint ) . IfQ21510855 ( inverse constraint )   is present , we then retrieve the corresponding property of this inverse relationship . If the inverse constraint   is not present , we check the corresponding property of P7087 ( inverse label item ) , which outputs the   item with a label of the inverse relationship of the property .   Entity Hyponyms . Some of our generated questions replace entities with their hyponyms . To obtain the   hyponyms for a given entity , we retrieve any object entities of the P31 ( instance of ) andP279 ( subclass   of)properties.1321313214H Full Results on WikidataSyntheticQA   Table 21 shows the performance of each modeling approach on WikidataSyntheticQA.13215I Full Results on Subsampled SQuAD   Table 22 and Table 23 show the performance of each modeling approach on subsamples of the SQuAD   benchmark.13216ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Yes , at the very end of the paper in an unmarked section .   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Yes , we used code published by prior researchers for training and evaluating QA models they had   proposed . We also used existing datasets . See section 2 and 3 .   /squareB1 . Did you cite the creators of artifacts you used ?   Yes , see section 2 and 3 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Not applicable . Left blank .   C / squareDid you run computational experiments ?   Yes , sections 3 and 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Yes , Appendix A.13217 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Yes , Appendix A.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Yes , Appendix A.   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Yes , Appendix A.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.13218