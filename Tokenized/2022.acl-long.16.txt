  Shaoyi Huang , Dongkuan Xu , Ian En - Hsu Yen ,   Yijue Wang , Sung - En Chang , Bingbing Li , Shiyang Chen ,   Mimi Xie , Sanguthevar Rajasekaran , Hang Liu , Caiwen DingUniversity of Connecticut , Penn State University , Moffett AI , Northeastern University , Stevens Institute of Technology , University of Texas at San Antonio   Abstract   Conventional wisdom in pruning Transformer-   based language models is that pruning reduces   the model expressiveness and thus is more   likely to underfit rather than overfit . How-   ever , under the trending pretrain - and - finetune   paradigm , we postulate a counter - traditional hy-   pothesis , that is : pruning increases the risk of   overfitting when performed at the fine - tuning   phase . In this paper , we aim to address the   overfitting problem and improve pruning per-   formance via progressive knowledge distilla-   tion with error - bound properties . We show for   the first time that reducing the risk of overfit-   ting can help the effectiveness of pruning under   the pretrain - and - finetune paradigm . Ablation   studies and experiments on the GLUE bench-   mark show that our method outperforms the   leading competitors across different tasks .   1 Introduction   Recently , the emergence of Transformer - based   language models ( using pretrain - and - finetune   paradigm ) such as BERT ( Devlin et al . , 2019 ) and   GPT-3 ( Brown et al . , 2020 ) have revolutionized   and established state - of - the - art ( SOTA ) records ( be-   yond human - level ) on various natural language   ( NLP ) processing tasks . These models are first   pre - trained in a self - supervised fashion on a large   corpus and fine - tuned for specific downstream   tasks ( Wang et al . , 2018 ) . While effective and   prevalent , they suffer from redundant computation   due to the heavy model size , which hinders their   popularity on resource - constrained devices , e.g. ,   mobile phones , smart cameras , and autonomous   driving ( Chen et al . , 2021 ; Qi et al . , 2021 ; Yin et al . ,   2021a , b ; Li et al . , 2021 ; Choi and Baek , 2020 ) .   Various weight pruning approaches ( zeroing out   certain weights and then optimizing the rest ) have   been proposed to reduce the footprint requirements   of Transformers ( Zhu and Gupta , 2018 ; Blalock   Figure 1 : Pruning under non - pretrain - and - finetune vs.   pruning under pretrain - and - finetune . In the subfigures ,   the cylinders on the left describe the pruning process ,   and the circles on the right represent the knowledge   analysis of the sparse model .   et al . , 2020 ; Gordon et al . , 2020 ; Xu et al . , 2021 ;   Huang et al . , 2021 ; Peng et al . , 2021 ) . Conventional   wisdom in pruning states that pruning reduces the   overfitting risk since the compressed model struc-   tures are less complex , have fewer parameters and   are believed to be less prone to overfit ( Ying , 2019 ;   Wang et al . , 2021 ; Tian et al . , 2020 ; Gerum et al . ,   2020 ) . However , under the pretrain - and - finetune   paradigm , most pruning methods understate the   overfitting problem .   In this paper , we postulate a counter - traditional   hypothesis , that is : model pruning increases the   risk of overfitting if pruning is performed at the   fine - tuning phase . As shown in Figure 1b , the   pretrain - and - finetune paradigm contains two types   of knowledge , the general - purpose language knowl-   edge learned during pre - training ( L ) and the task-   specific knowledge from the downstream task data   ( D ) . Compared to conventional pruning that only   discards task - specific knowledge ( Figure 1a ) , prun-   ing under pretrain - and - finetune ( Figure 1b ) dis-   cards extra knowledge ( red area ) learned in pre-   training phase . Thus , to recover both the ex-   tra discarded general - purpose knowledge and the   discarded task - specific knowledge , pruning under190   pretrain - and - finetune increases the amount of infor-   mation a model needs , which results in relative data   deficiency , leading to a higher risk of overfitting .   To empirically verify the overfitting problem , we   visualize the training and evaluation performance   on a real - world task data of MRPC ( Devlin et al . ,   2019 ) in Figure 2 . From Figure 2 ( b ) , it is ob-   served that the evaluation accuracy on the training   dataset remains improved while it keeps the same   for the validation set through the training process .   From Figure 2 ( c ) , the difference in performance   becomes more significant when the pruning rate   becomes higher and the performance on the vali-   dation set even becomes worse after 2,000 training   steps . All these observations verify our hypothesis .   The main question this paper attempts to an-   swer is : how to reduce the risk of overfitting of   pre - trained language models caused by pruning ?   However , answering this question is challenging .   First , under the pretrain - and - finetune paradigm ,   both the general - purpose language knowledge and   the task - specific knowledge are learned . It is non-   trivial to keep the model parameters related to both   knowledge when pruning . Second , the amount of   data for downstream tasks can be small , such as   the data with privacy . Thus , the overfitting prob-   lem can easily arise , especially in the face of high   pruning rate requirements . A little recent progress   has been made on addressing overfitting associated   with model compression . However , their results   are not remarkable and most of them focus on the   vision domain ( Bai et al . , 2020 ; Shen et al . , 2021 ) .   To address these challenges , we propose SPD , a   sparse progressive distillation method , for pruning   pre - trained language models . We prune and opti-   mize the weight duplicates of the backbone of the   teacher model ( a.k.a . , student modules ) . Each stu-   dent module shares the same architecture ( e.g. , the   number of weights , the dimension of each weight)as the duplicate . We replace the corresponding   layer(s ) of the duplicated teacher model with the   pruned sparse student module(s ) in a progressive   way and name the new model as a grafted model .   We validate our proposed method through the ab-   lation studies and the GLUE benchmark . Experi-   mental results show that our method outperforms   the existing approaches .   We summarize our contributions as follows :   •We postulate , analyze , and empirically verify   a counter - traditional hypothesis : pruning in-   creases the risk of overfitting under the pretrain-   and - finetune paradigm .   •We propose a sparse progressive pruning method   and show for the first time that reducing the   risk of overfitting can help the effectiveness of   pruning .   •Moreover , we theoretically analyze that our prun-   ing method can obtain a sub - network from the   student model that has similar accuracy as the   teacher .   •Last but not least , we study and minimize the   interference between different hyperparameter   strategies , including pruning rate , learning rate ,   and grafting probability , to further improve per-   formance .   2 Related Work   To summarize , our contribution is determining the   overfitting problem of pruning under the pretrain-   and - finetune paradigm and proposing the sparse   progressive distillation method to address it . We   demonstrate the benefits of the proposed frame-   work through the ablation studies . We validate our   method on eight datasets from the GLUE bench-   mark . To test if our method is applicable across191   tasks , we include the tasks of both single sentence   and sentence - pair classification . Experimental re-   sults show that our method outperforms the leading   competitors by a large margin .   Network Pruning . Common wisdom has shown   that weight parameters of deep learning models   can be reduced without sacrificing accuracy loss ,   such as magnitude - based pruning and lottery ticket   hypothesis ( Frankle and Carbin , 2019 ) . ( Zhu   and Gupta , 2018 ) compared small - dense models   and large - sparse models with the same parame-   ters and showed that the latter outperforms the for-   mer , showing the large - sparse models have better   expressive power than their small - dense counter-   parts . However , under the pretrain - and - finetune   paradigm , pruning leads to overfitting as discussed .   Knowledge Distillation ( KD ) . As a common   method in reducing the number of parameters , the   main idea of KD is that the small student model   mimics the behaviour of the large teacher model   and achieves a comparable performance ( Hinton   et al . , 2015 ; Mirzadeh et al . , 2020 ) . ( Sanh et al . ,   2019 ; Jiao et al . , 2020 ; Sun et al . , 2020 ) utilized KD   to learn universal language representations from   large corpus . However , current SOTA knowledge   distillation methods are not able to achieve a high   model compression rate ( less than 10 % remaining   weights ) while achieving an insignificant perfor-   mance decrease .   Progressive Learning . The key idea of progres-   sive learning is that student learns to update module   by module with the teacher . ( Shen et al . , 2021 )   utilized a dual - stage distillation scheme where stu-   dent modules are progressively grafted onto the   teacher network , it targets the few - shot scenario   and uses only a few unlabeled samples to achievecomparable results on CIFAR-10 and CIFAR-100 .   ( Xu et al . , 2020 ) gradually increased the probability   of replacing each teacher module with their corre-   sponding student module and trained the student   to reproduce the behavior of the teacher . However ,   the performance on Transformer - based models of   the aforementioned first method is unknown while   the second method has an obvious performance   drop with a low sparsity ( 50 % ) .   3 Methodology   3.1 Problem Formulation   The teacher model and the grafted model ( shown in   Figure 3 ) are denoted as fandf , respectively .   Both models have N+ 1layers ( i.e. , the first N   layers are encoder layers , and the ( N+ 1 ) -th layer   is the output layer ) . Denote f(·),f(·)as the   behaviour function induced from the i - th encoder   of the teacher model , and the grafted model , re-   spectively . As shown in Figure 4 , we utilize layer-   wise knowledge distillation ( KD ) , where we aim to   bridge the gap between f(·)andf ( · ) .   The grafted model is trained to mimic the be-   havior of the teacher model . During training , we   minimize the summation loss L :   L = XXλL(f(x)f(x ) ) , ( 1 )   where Xdenotes the training dataset , λis coef-   ficient of i - th layer loss , Lis the distillation loss   of the layer pair , xis the input of the i - th layer .   During KD , each student module mimics the   behavior of the corresponding teacher layer . Sim-   ilar to ( Jiao et al . , 2020 ) , we take the advantage192   of abundant knowledge in self - attention distribu-   tion , hidden states of each Transformer layer , and   the final output layer ’s soft logits of teacher model   to help train the student model . Specifically , we   design the KD loss as follows   L= (   L+L 1≤i≤N   L i = N+ 1(2 )   where L= MSE ( H , H ) ( 1≤i≤N ) in-   dicates the difference between hidden states , L   = MSE ( A , A ) indicates the difference between   attention matrices . MSE ( · ) is the mean square error   loss function and iis the index of Transformer layer .   L = -softmax ( z)·log_softmax ( z / temp )   indicates the difference of soft cross - entropy loss ,   where zandzare the soft logits of teacher and   student model , respectively . Tis the temperature   hyper - parameter .   We further reduce the number of non - zero pa-   rameters in the weight matrix while maintaining   accuracy . We denote { W}as the collection   of weights in the first ilayers , θas the sparsity   of the j - th layer . Then , the loss function of sparse   knowledge distillation becomes   After training , we find the sparse weight matrix   Wusing   W= Π(W)forj= 1 , ... , N , ( 4 )   where Π(·)denotes the Euclidean projection onto   the set S={W|sparsity ( W)≤θ}.3.2 Our Methods   3.2.1 Error - bound Analysis   Our pruning method is similar to finding match-   ing subnetworks using the lottery ticket hypothe-   sis ( Frankle and Carbin , 2019 ; Pensia et al . , 2020 )   methodology . We analyze the self - attention ( ex-   cluding activation ) . Some non - linear activation   functions has been analyzed in ( Pensia et al . , 2020 ) .   Feed - forward layer . Consider a feed - forward net-   work f(x ) = w·x , and g(x ) = ( Pw)x .   Lueker et al . ( Lueker , 1998 ) and Pensia et al . ( Pen-   sia et al . , 2020 ) show that existing a subset of w ,   such that the corresponding value of g(x)is very   close to f(x ) .   Corollary : When w , ... , wbelongs to i.i.d . uni-   form distribution over [ -1,1 ] , where n≥Clog ,   δ≤min{1 , ϵ } . Then , with probability at least 1- δ ,   we have   ∃G⊂ { 1,2 , ... , n},∀w∈[−0.5,0.5 ] ,   s.t  w−Xw  ≤ϵ(5 )   Analysis on self - attention . The self - attention can   be presented as :   Z = attention ( Q , K , V)=softmax ( Q·K   √d)·V.   ( 6 )   Consider a model f(x)with only one self-   attention , when the token size of input xis 1 ,   softmax ( ) = 1 , we have Z = V , where   V = wx .   Consider f(x ) = Pw   xand a pruning   sparsity θ , base on Corollary , when d≥Clog 4/ϵ ,   there exists a pattern of w , such that , with proba-   bility 1−ϵ ,   ∀w∈[−1,1],∃θ∈ { 0,1 } ,   s.t .  w−(XwI(θ ) )  < ϵ(7 )   where I(θ)is the indicator to determine whether   wwill be remained .   In general , let the token x ’s size be n. sox=   ( x , x , ... , x ) . Consider a teacher model f(x)193with a self - attention , then   f(x ) = softmax ( Q·K   p   ( d))·V   = ( Pe   PP(e))·V   = ( Pe   PP(e))wx   = wx(8 )   where cis the ( i , j)element of the matrix√.   Base on Corollary , when d≥Clog 4/ϵ , there   exists a pattern of w , such that , with probability   1−ϵ ,   ∀w∈[−1,1],∃θ∈ { 0,1 } ,   s.t .  w−(XwI(θ ) )  < ϵ(9 )   In summary :   ∀i∈ { 1,2 , ... , n } ,  f(x)−f(x )  < ϵ ( 10 )   3.2.2 Progressive Module Grafting   To avoid overfitting in the training process for the   sparse Transformer model , we further graft stu-   dent modules ( scion ) onto the teacher model dupli-   cates ( rootstock ) . For the i - th student module , we   use an independent Bernoulli random variable I(θ )   to indicate whether it will be grafted on the root-   stock . To be more specific , I(θ)has a probability   ofp(grafting probability ) to be set as 1 ( i.e. , stu-   dent module substitutes the corresponding teacher   layer ) . Otherwise , the latter will keep weight ma-   trices unchanged . Once the target pruning rate is   achieved , we apply linear increasing probability   to graft student modules which enable the student   modules to orchestrate with each other .   Different from the model compression methods   that update all model parameters at once , such as   TinyBERT ( Jiao et al . , 2020 ) and DistilBERT ( Sanh   et al . , 2019 ) , SPD only updates the student modules   on the grafted model . It reduces the complexity of   network optimization , which mitigates the overfit-   ting problem and enables the student modules to   learn deeper knowledge from the teacher model .   The overview is described in Algorithm 1 . We will   further demonstrate the effectiveness of progressive   student module grafting in 4.2.Algorithm 1 Sparse Progressive Distillation   4 Experiments   4.1 Experimental Setup   Datasets . We evaluate SPD on the General Lan-   guage Understanding Evaluation ( GLUE ) bench-   mark ( Wang et al . , 2018 ) and report the metrics ,   i.e. , accuracy scores for SST-2 , QNLI , RTE , and   WNLI , Matthews Correlation Coefficient ( MCC )   for CoLA , F1 scores for QQP and MRPC , Spear-   man correlations for STS - B.   Baselines . We first use 50 % sparsity ( a widely   adopted sparsity ratio among SOTA ) , and com-   pare SPD against two types of baselines – non-   progressive and progressive . For the former ,   we select BERT - PKD ( Sun et al . , 2019 ) , Distil-   BERT ( Sanh et al . , 2019 ) , MiniLM ( Wang et al . ,   2020 ) , TinyBERT ( Jiao et al . , 2020 ) , Sparse-   BERT ( Xu et al . , 2021 ) and E.T. ( Chen et al . , 2021 ) ,   while for the latter , we choose Theseus ( Xu et al . ,   2020 ) . We further compare SPD against other   existing works under higher sparsity , e.g. , Tiny-   BERT ( Jiao et al . , 2020 ) , SparseBERT ( Xu et al . ,   2021 ) and RPP ( Guo et al . , 2019 ) .   SPD Settings . We use official BERT , uncased   model as the pre - train model and the fine - tuned   pre - train model as our teacher . Both BERT   and teacher model have the same architecture   ( i.e. , 12 encoder layers ( L = 12 ; embedding di-   mension d = 768 ; self - attention heads H =   12 ) ) . We finetune BERT using best perfor-   mance from { 2e,3e,4e,5e } as the learn-   ing rate . For SPD model training , the number of   pruning epochs , linear increasing module grafting   epochs , finetuning epochs vary from [ 10 , 30 ] , [ 5,19420 ] , [ 5 , 10 ] , respectively . For pruning , we use   AdamW ( Loshchilov and Hutter , 2018 ) as the opti-   mizer and run the experiments with an initial graft-   ing probability from { 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 ,   0.7 , 0.8 , 0.9 } . The probability with the best perfor-   mance will be adopted . After pruning , we adjust   the slope of the grafting probability curve so that   the grafting probability equals 1 at the end of mod-   ule grafting . For module grafting and finetuning ,   an AdamW optimizer is used with learning rate   chosen from { 3e,1e,3.2e,5e,6.4e } .   The model training and evaluation are performed   with CUDA 11.1 on Quadro RTX6000 GPU and   Intel(R ) Xeon(R ) Gold 6244 @ 3.60GHz CPU .   4.2 Experimental Results   Accuracy vs. Sparsity . We do experiments   on eight GLUE benchmark tasks ( Table 1 ) . For   non - progressive baselines , SPD exceeds all of   them on QNLI , SST-2 , CoLA , STS - B , and MRPC .   For RTE , TinyBERThas a 1.6 % higher accu-   racy than SPD . However , TinyBERTused aug-   mented data while SPD does not use data augmen-   tation to generate the results in Table 1 . On av-   erage , SPD has 6.3 % , 5.6 % , 1.2 % , 1.7 % , 3.7 %   improvement in performance than BERT - PKD ,   DistilBERT , TinyBERT , SparseBERT , E.T. respec-   tively . Furthermore , on CoLA , SPA achieves up   to 25.9 % higher performance compared to all non-   progressive baselines . For the progressive baseline ,   we compare SPD with BERT - of - Theseus . Exper-   imental results show that SPD exceeds the latter   on all tasks . SPD has a 3.9 % increase on aver-   age . Among all the tasks , CoLA and RTE have   20.2 % and 5.9 % gain respectively . For the compar-   ison with sparse and non - progressive baseline , SPD   has an improvement of 16.8 % , 5.5 % , 3.2 % , 2.7 % ,   2.0 % , 1.9 % , 1.6 % , 1.6 % on CoLA , RTE , MNLI ,   QNLI , QQP , MRPC , STS - B , SST-2 , respectively .   On all listed tasks , SPD even outperforms the   teacher model except for RTE . On RTE , SPD re-   tains exactly the full accuracy of the teacher model .   On average , the proposed SPD achieves a 1.1 %   higher accuracy / score than the teacher model . We   conclude the reason for the outstanding perfor-   mance from three respects : 1 ) There is redundancy   in the original dense BERT model . Thus , prun-   ing the model with a low pruning rate ( e.g. , 50 % )   will not lead to a significant performance drop . 2 )   SPD decreases the overfitting risk which helps the   student model learn better . 3 ) The interferencebetween different hyperparameter strategies is miti-   gated , which enables SPD to obtain a better student   model .   We also compare SPD with other baselines ( i.e. ,   4 - layer TinyBERT ( Jiao et al . , 2020 ) , RPP ( Guo   et al . , 2019 ) , and SparseBERT ( Xu et al . , 2021 ) )   under higher pruning rates . Results are summa-   rized in Table 2 . For the fairness of comparison ,   we remove data augmentation from the above meth-   ods . We mainly compare the aforementioned base-   lines with very high sparsity ( e.g. , 90 % , 95 % ) SPD .   For the comparison with TinyBERT , both SPD   ( 90 % sparsity ) and SPD ( 95 % sparsity ) win . SPD   ( 90 % sparsity ) has 63.4 % and 9%higher evalua-   tion score than TinyBERTon CoLA and MRPC ,   respectively . For the setting of 95 % sparsity , SPD   outperforms TinyBERTwith 41.3%and7.6 %   higher performance , respectively . Compared to   RPP , both SPD ( 90 % sparsity ) and SPD ( 95 % spar-   sity ) show higher performance on MRPC , with   9.8%and8.3%higher F1 score , respectively . For   SparseBERT , SPD exceeds it on all tasks in Table 2 .   Especially on CoLA , SPD ( 90 % sparsity ) and SPD   ( 95 % sparsity ) have 2.69 ×and 2.33 ×higher Mcc   score on CoLA , respectively . SparseBERT has   competitive performance with SOTA when using   data augmentation . The reason for the performance   drop for SparseBERT may because its deficiency   of ability in mitigating overfitting problems .   Overfitting Mitigation . We explore the effective-   ness of SPD to mitigate the overfitting problem .   Depending on whether progressive , grafting , or   KD is used , we compare 4 strategies : ( a ) no pro-   gressive , no KD ; ( b ) progressive , no KD ; ( c ) no   progressive , KD ; ( d ) progressive , KD ( ours ) . We   evaluate these strategies on both training and valida-   tion sets of MRPC . The results are summarized in   Figure 5 . From ( a ) to ( d ) , the gap between the eval-   uation results of the training set and the dev set is   reduced , which strongly suggests that the strategy   adopted by SPD , i.e. , progressive + KD , outper-   forms other strategies in mitigating the overfitting   problem . Figure 5 ( a ) , ( b ) , and ( c ) indicate that   compared to progressive only , KD has a bigger im-   pact on mitigating overfitting , as the performance   gap between the training set and the dev set de-   creases more from ( a ) to ( c ) than from ( a ) to ( b ) .   From Figure 5 ( a ) , ( b ) and ( c ) , we also observe that   compared to no progressive , no KD , either using   progressive ( Figure 5 ( b ) ) or KD ( Figure 5 ( c ) ) is   very obvious to help mitigate the overfitting prob-195   lem . Figures 5 ( b ) , ( c ) and ( d ) indicate that the   combination of progressive and KD brings more   benefits than only using progressive or KD as Fig-   ure 5 ( d ) has the smallest performance gap between   the training set and the dev set . Combined with   Table 1 and Table 2 , Figure 5 shows that SPD miti-   gates overfitting and leads to higher performance.4.3 Ablation Studies   In this section , we justify the three schedulers used   in our method ( i.e. , grafting probability , pruning   rate , and learning rate ) , and study the sensitivity of   our method with respect to each of them .   Study on Components of SPD . The proposed SPD   consists of three components ( i.e. , sparse , knowl-   edge distillation , and progressive module grafting ) .   We conduct experiments to study the importance of   each component on GLUE benchmark tasks with   the sparsity of 50 % and results are shown in Ta-   ble 3 . Compared to both sparse + KD and sparse   + progressive , SPD achieves gains on performance   among all tasks .   Effects of Grafting Probability Strategy . In our   method , we set the grafting probability greater than   0 during pruning , to allow student modules to learn   deeper knowledge from the teacher model . To ver-   ify the benefit of this design , we change the graft-   ing probability to zero and compare it with our196   method . The result on RTE is shown in Figure 6 .   Pruning with grafting ( the red curve ) shows better   performance than pruning without grafting , which   justifies the existence of grafting during pruning . In   addition , we study the sensitivity of our method to   grafting probability ( Figure 7 ) . It is observed that   p= 0.6 achieves the best performance , and the pro-   gressive design is better than the non - progressive .   Effects of Pruning Rate Strategy . For the pruningrate scheduler , we compare the strategies with dif-   ferent pruning ending steps . The results are shown   in Figure 8 . It is observed that the pruning during   when grafting probability p = phas a higher F1   score than other strategies on MRPC .   Effects of Optimizer Strategy . We also compare   our strategy with the strategy that only has one   learning rate scheduler . The results ( Figure 9 ) indi-   cate that our strategy ( i.e. , two independent optimiz-   ers ) is better . We also evaluate different learning   rates with the pruning rate of 0.9 and the grafting   probability of 0.8 .   5 Conclusion   In this paper , we postulate a counter - traditional   hypothesis that pruning increases the risk of over-   fitting under the pretrain - and - finetune paradigm .   We analyze and empirically verify this hypothesis ,   and propose a sparse progressive pruning method197to address the overfitting problem . We theoretically   analyze that our pruning method can obtain a sub-   network from the student model that has a similar   accuracy as the teacher . We study and minimize   the interference between different hyperparameter   strategies , including pruning rate , learning rate , and   grafting probability . A number of ablation studies   and experimental results on eight tasks from the   GLUE benchmark demonstrate the superiority of   our method over the leading competitors .   Acknowledgement   This research was supported in part by National Sci-   ence Foundation ( NSF ) CRII Award No . 2000722   and NSF CAREER Award No . 2046102 . Sanguthe-   var Rajasekaran has been supported in part by the   NSF RAISE Award No . 1743418 and NSF EA-   GER Award No . 1843025 . In addition , it used   the Extreme Science and Engineering Discovery   Environment ( XSEDE ) through allocations TG-   CCR200004 .   References198199Appendix   We provide the sensitivity analysis of learning rate   on RTE and STS - B ( dev set ) and the evaluation   curves of four tasks ( CoLA , STS - B , MRPC , and   RTE ) with the target pruning rate of 0.95 .   Sensitivity Analysis of Learning Rate . The   analysis results on RTE and STS - B are shown in   Figure 10 and Figure 11 , respectively . Results vary   with different learning rate settings . Among the   eight learning rates listed in the legend of Figure 10 ,   3.2×eachieves the best performance . For STS-   B,4.0×egives the best performance among the   learning rate choices in Figures 11 .   Evaluation Curves of Four Tasks at Tar-   get Pruning rate of 0.95 . We plot the evalu-   ation curves of CoLA ( Figure 12 ) , STS - B ( Fig-   ure 13 ) , MRPC ( Figure 14 ) , RTE ( Figure 15 ) to   further demonstrate the advantages of our proposed   method SPD . In each figure , the x - axis is the train-   ing steps while the y - axis represents evaluation   metrics . To obtain the curves , we use the same   settings as Table 2 .   Moreover , we describe the hyper - parameters set-   tings in detail . For CoLA , we set the max sequence   length as 128 , the learning rate as 5.0e , the graft-   ing probability during pruning as 0.8 , the number   of training epochs as 60 , and the number of pruning   epochs as 30 . For STS - B , we use the same setting   as CoLA . For MRPC , we set the max sequence   length as 128 , the learning rate as 6.4×e , the   grafting probability during pruning as 0.8 , the num-   ber of training epochs as 60 , and the number of   pruning epochs as 30 . For RTE , we set the max se-   quence length as 128 , the learning rate as 3.0×e ,   the grafting probability during pruning as 0.6 , the   number of training epochs as 60 , and the number   of pruning epochs as 30.200