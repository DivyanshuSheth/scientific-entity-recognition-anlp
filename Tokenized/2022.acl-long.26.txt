  Zhengxiao DuYujie QianXiao LiuMing DingJiezhong Qiu   Zhilin YangJie TangTsinghua UniversityBeijing Academy of Artiﬁcial Intelligence ( BAAI)MIT CSAILShanghai Qi Zhi Institute   zx-du20@mails.tsinghua.edu.cn yujieq@csail.mit.edu   { zhiliny,jietang}@tsinghua.edu.cn   Abstract   There have been various types of pretrain-   ing architectures including autoencoding mod-   els ( e.g. , BERT ) , autoregressive models ( e.g. ,   GPT ) , and encoder - decoder models ( e.g. , T5 ) .   However , none of the pretraining frameworks   performs the best for all tasks of three main cat-   egories including natural language understand-   ing ( NLU ) , unconditional generation , and con-   ditional generation . We propose a General   Language Model ( GLM ) based on autoregres-   sive blank inﬁlling to address this challenge .   GLM improves blank ﬁlling pretraining by   adding 2D positional encodings and allowing   an arbitrary order to predict spans , which re-   sults in performance gains over BERT and T5   on NLU tasks . Meanwhile , GLM can be pre-   trained for different types of tasks by varying   the number and lengths of blanks . On a wide   range of tasks across NLU , conditional and   unconditional generation , GLM outperforms   BERT , T5 , and GPT given the same model   sizes and data , and achieves the best perfor-   mance from a single pretrained model with   1.25parameters of BERT , demonstrat-   ing its generalizability to different downstream   tasks .   1 Introduction   Language models pretrained on unlabeled texts   have substantially advanced the state of the art in   various NLP tasks , ranging from natural language   understanding ( NLU ) to text generation ( Radford   et al . , 2018a ; Devlin et al . , 2019 ; Yang et al . , 2019 ;   Radford et al . , 2018b ; Raffel et al . , 2020 ; Lewis   et al . , 2019 ; Brown et al . , 2020 ) . Downstream task   performance as well as the scale of the parame-   ters have also constantly increased in the past few   years . Figure 1 : Illustration of GLM . We blank out text spans   ( green part ) and generate them autoregressively . ( Some   attention edges are omitted ; cf . Figure 2 . )   In general , existing pretraining frameworks can   be categorized into three families : autoregressive ,   autoencoding , and encoder - decoder models . Au-   toregressive models , such as GPT ( Radford et al . ,   2018a ) , learn left - to - right language models . While   they succeed in long - text generation and show few-   shot learning ability when scaled to billions of   parameters ( Radford et al . , 2018b ; Brown et al . ,   2020 ) , the inherent disadvantage is the unidirec-   tional attention mechanism , which can not fully cap-   ture the dependencies between the context words   in NLU tasks . Autoencoding models , such as   BERT ( Devlin et al . , 2019 ) , learn bidirectional con-   text encoders via denoising objectives , e.g. Masked   Language Model ( MLM ) . The encoders produce   contextualized representations that suit natural lan-   guage understanding tasks , but could not be directly   applied for text generation . Encoder - decoder mod-   els adopt bidirectional attention for the encoder ,   unidirectional attention for the decoder , and cross   attention between them ( Song et al . , 2019 ; Bi et al . ,   2020 ; Lewis et al . , 2019 ) . They are typically de-   ployed in conditional generation tasks , such as   text summarization and response generation ..   T5 ( Raffel et al . , 2020 ) uniﬁes NLU and condi-   tional generation via encoder - decoder models but   requires more parameters to match the performance320of BRET - based models such as RoBERTa ( Liu   et al . , 2019 ) and DeBERTa ( He et al . , 2021 ) .   None of these pretraining frameworks is ﬂexible   enough to perform competitively across all NLP   tasks . Previous works have tried to unify differ-   ent frameworks by combining their objectives via   multi - task learning ( Dong et al . , 2019 ; Bao et al . ,   2020 ) . However , since the autoencoding and au-   toregressive objectives differ by nature , a simple   uniﬁcation can not fully inherit the advantages of   both frameworks .   In this paper , we propose a pretraining frame-   work named GLM ( General Language Model ) ,   based on autoregressive blank inﬁlling . We ran-   domly blank out continuous spans of tokens from   the input text , following the idea of autoencoding ,   and train the model to sequentially reconstruct the   spans , following the idea of autoregressive pretrain-   ing ( see Figure 1 ) . While blanking ﬁlling has been   used in T5 ( Raffel et al . , 2020 ) for text - to - text pre-   training , we propose two improvements , namely   span shufﬂing and 2D positional encoding . Empiri-   cally , we show that with the same amount of param-   eters and computational cost , GLM signiﬁcantly   outperforms BERT on the SuperGLUE benchmark   by a large margin of 4.6 % – 5.0 % and outperforms   RoBERTa and BART when pretrained on a corpus   of similar size ( 158 GB ) . GLM also signiﬁcantly   outperforms T5 on NLU and generation tasks with   fewer parameters and data .   Inspired by Pattern - Exploiting Training ( PET )   ( Schick and Schütze , 2020a ) , we reformulate NLU   tasks as manually - crafted cloze questions that   mimic human language . Different from the BERT-   based models used by PET , GLM can naturally   handle multi - token answers to the cloze question   via autoregressive blank ﬁlling .   Furthermore , we show that by varying the num-   ber and lengths of missing spans , the autoregressive   blank ﬁlling objective can pretrain language mod-   els for conditional and unconditional generation .   Through multi - task learning of different pretraining   objectives , a single GLM can excel in both NLU   and ( conditional and unconditional ) text genera-   tion . Empirically , compared with standalone base-   lines , GLM with multi - task pretraining achieves   improvements in NLU , conditional text generation ,   and language modeling tasks altogether by sharing   the parameters.2 GLM Pretraining Framework   We propose a general pretraining framework GLM   based on a novel autoregressive blank inﬁlling ob-   jective . GLM formulates NLU tasks as cloze ques-   tions that contain task descriptions , which can be   answered by autoregressive generation .   2.1 Pretraining Objective   2.1.1 Autoregressive Blank Inﬁlling   GLM is trained by optimizing an autoregressive   blank inﬁlling objective . Given an input text x=   [ x;;x ] , multiple text spans fs;;sgare   sampled , where each span scorresponds to a   series of consecutive tokens [ s;;s]inx .   Each span is replaced with a single [ MASK ] to-   ken , forming a corrupted text x . The model   predicts the missing tokens in the spans from the   corrupted text in an autoregressive manner , which   means when predicting the missing tokens in a   span , the model has access to the corrupted text   andthe previously predicted spans . To fully cap-   ture the interdependencies between different spans ,   we randomly permute the order of the spans , simi-   lar to the permutation language model ( Yang et al . ,   2019 ) . Formally , let Zbe the set of all possi-   ble permutations of the length- mindex sequence   [ 1;2;;m ] , andsbe[s;;s ] , we de-   ﬁne the pretraining objective as   maxE"Xlogp(sjx;s ) #   ( 1 )   We always generate the tokens in each blank fol-   lowing a left - to - right order , i.e. the probability of   generating the span sis factorized as :   p(sjx;s )   = Yp(sjx;s;s)(2 )   We implement the autoregressive blank inﬁlling   objective with the following techniques . The input   xis divided into two parts : Part A is the corrupted   textx , and Part B consists of the masked   spans . Part A tokens can attend to each other , but   can not attend to any tokens in B. Part B tokens can   attend to Part A and antecedents in B , but can not   attend to any subsequent tokens in B. To enable au-   toregressive generation , each span is padded with   special tokens [ START ] and[END ] , for input and321   output respectively . In this way , our model auto-   matically learns a bidirectional encoder ( for Part   A ) and a unidirectional decoder ( for Part B ) in a   uniﬁed model . The implementation of GLM is   illustrated in Figure 2 .   We randomly sample spans of length drawn from   a Poisson distribution with = 3 . We repeatedly   sample new spans until at least 15 % of the original   tokens are masked . Empirically , we have found   that the 15 % ratio is critical for good performance   on downstream NLU tasks .   2.1.2 Multi - Task Pretraining   In the previous section , GLM masks short spans   and is suited for NLU tasks . However , we are   interested in pretraining a single model that can   handle both NLU and text generation . We then   study a multi - task pretraining setup , in which a   second objective of generating longer text is jointly   optimized with the blank inﬁlling objective . We   consider the following two objectives :   •Document - level . We sample a single span   whose length is sampled from a uniform distri-   bution over 50%–100 % of the original length .   The objective aims for long text generation .   •Sentence - level . We restrict that the masked   spans must be full sentences . Multiple spans   ( sentences ) are sampled to cover 15 % of   the original tokens . This objective aims for   seq2seq tasks whose predictions are often   complete sentences or paragraphs .   Both new objectives are deﬁned in the same wayas the original objective , i.e. Eq . 1 . The only differ-   ence is the number of spans and the span lengths .   2.2 Model Architecture   GLM uses a single Transformer with several mod-   iﬁcations to the architecture : ( 1 ) we rearrange   the order of layer normalization and the resid-   ual connection , which has been shown critical for   large - scale language models to avoid numerical   errors ( Shoeybi et al . , 2019 ) ; ( 2 ) we use a sin-   gle linear layer for the output token prediction ;   ( 3 ) we replace ReLU activation functions with   GeLUs ( Hendrycks and Gimpel , 2016 ) .   2.2.1 2D Positional Encoding   One of the challenges of the autoregressive blank   inﬁlling task is how to encode the positional infor-   mation . Transformers rely on positional encodings   to inject the absolute and relative positions of the   tokens . We propose 2D positional encodings to   address the challenge . Speciﬁcally , each token is   encoded with two positional ids . The ﬁrst posi-   tional i d represents the position in the corrupted   textx . For the masked spans , it is the position   of the corresponding [ MASK ] token . The second   positional i d represents the intra - span position . For   tokens in Part A , their second positional ids are   0 . For tokens in Part B , they range from 1 to the   length of the span . The two positional ids are pro-   jected into two vectors via learnable embedding   tables , which are both added to the input token   embeddings .   Our encoding method ensures that the model is   not aware of the length of the masked span when322   reconstructing them . It is an important difference   as compared to other models . For example , XL-   Net ( Yang et al . , 2019 ) encodes the original posi-   tion so that it can perceive the number of missing   tokens , and SpanBERT ( Joshi et al . , 2020 ) replaces   the span with multiple [ MASK ] tokens and keeps   the length unchanged . Our design ﬁts downstream   tasks as usually the length of the generated text is   unknown beforehand .   2.3 Finetuning GLM   Typically , for downstream NLU tasks , a linear clas-   siﬁer takes the representations of sequences or to-   kens produced by pretrained models as input and   predicts the correct labels . The practices are differ-   ent from the generative pretraining task , leading to   inconsistency between pretraining and ﬁnetuning .   Instead , we reformulate NLU classiﬁcation tasks   as generation tasks of blank inﬁlling , following   PET ( Schick and Schütze , 2020a ) . Speciﬁcally ,   given a labeled example ( x;y ) , we convert the in-   put text xto a cloze question c(x)via a pattern   containing a single mask token . The pattern is writ-   ten in natural language to represent the semantics   of the task . For example , a sentiment classiﬁcation   task can be formulated as “ { SENTENCE } . It ’s   really [ MASK ] ” . The candidate labels y2Y are   also mapped to answers to the cloze , called ver-   balizerv(y ) . In sentiment classiﬁcation , the labels   “ positive ” and “ negative ” are mapped to the words   “ good ” and “ bad ” . The conditional probability of   predictingygiven xis   p(yjx ) = p(v(y)jc(x))Pp(v(y)jc(x))(3 )   whereYis the label set . Therefore the probability   of the sentence being positive or negative is propor-   tional to predicting “ good ” or “ bad ” in the blank .   Then we ﬁnetune GLM with a cross - entropy loss   ( see Figure 3).For text generation tasks , the given context con-   stitutes the Part A of the input , with a mask token   appended at the end . The model generates the text   of Part B autoregressively . We can directly apply   the pretrained GLM for unconditional generation ,   or ﬁnetune it on downstream conditional generation   tasks .   2.4 Discussion and Analysis   In this section , we discuss the differences between   GLM and other pretraining models . We are mainly   concerned with how they can be adapted to down-   stream blank inﬁlling tasks .   Comparison with BERT ( Devlin et al . , 2019 ) .   As pointed out by ( Yang et al . , 2019 ) , BERT fails   to capture the interdependencies of masked tokens   due to the independence assumption of MLM . An-   other disadvantage of BERT is that it can not ﬁll in   the blanks of multiple tokens properly . To infer the   probability of an answer of length l , BERT needs   to performlconsecutive predictions . If the length l   is unknown , we may need to enumerate all possible   lengths , since BERT needs to change the number   of[MASK ] tokens according to the length .   Comparison with XLNet ( Yang et al . , 2019 ) .   Both GLM and XLNet are pretrained with autore-   gressive objectives , but there are two differences   between them . First , XLNet uses the original posi-   tion encodings before corruption . During inference ,   we need to either know or enumerate the length of   the answer , the same problem as BERT . Second ,   XLNet uses a two - stream self - attention mechanism ,   instead of the right - shift , to avoid the information   leak within Transformer . It doubles the time cost   of pretraining .   Comparison with T5 ( Raffel et al . , 2020 ) . T5   proposes a similar blank inﬁlling objective to pre-   train an encoder - decoder Transformer . T5 uses   independent positional encodings for the encoder   and decoder , and relies on multiple sentinel tokens   to differentiate the masked spans . In downstream   tasks , only one of the sentinel tokens is used , lead-   ing to a waste of model capacity and inconsistency   between pretraining and ﬁnetuning . Moreover , T5   always predicts spans in a ﬁxed left - to - right order .   As a result , GLM can signiﬁcantly outperform T5   on NLU and seq2seq tasks with fewer parameters   and data , as stated in Sections 3.2 and 3.3 .   Comparison with UniLM ( Dong et al . , 2019 ) .   UniLM combines different pretraining objectives   under the autoencoding framework by changing the323attention mask among bidirectional , unidirectional ,   and cross attention . However , UniLM always re-   places masked spans with [ MASK ] tokens , which   limits its ability to model the dependencies between   the masked spans and their context . GLM feeds in   the previous token and autoregressively generates   the next token . Finetuning UniLM on downstream   generation tasks also relies on masked language   modeling , which is less efﬁcient . UniLMv2 ( Bao   et al . , 2020 ) adopts partially autoregressive model-   ing for generation tasks , along with the autoencod-   ing objective for NLU tasks . Instead , GLM uniﬁes   NLU and generation tasks with autoregressive pre-   training .   3 Experiments   We now describe our pretraining setup and the eval-   uation of downstream tasks .   3.1 Pretraining Setup   For a fair comparison with BERT ( Devlin et al . ,   2019 ) , we use BooksCorpus ( Zhu et al . , 2015 ) and   English Wikipedia as our pretraining data . We use   the uncased wordpiece tokenizer of BERT with 30k   vocabulary . We train GLMand GLM with   the same architectures as BERTand BERT ,   containing 110 M and 340 M parameters respec-   tively .   For multi - task pretraining , we train two Large-   sized models with a mixture of the blank inﬁll-   ing objective and the document - level or sentence-   level objective , denoted as GLMand GLM .   Additionally , we train two larger GLM models of   410 M ( 30 layers , hidden size 1024 , and 16 atten-   tion heads ) and 515 M ( 30 layers , hidden size 1152 ,   and 18 attention heads ) parameters with document-   level multi - task pretraining , denoted as GLM   and GLM .   To compare with SOTA models , we also train   a Large - sized model with the same data , tokeniza-   tion , and hyperparameters as RoBERTa ( Liu et al . ,   2019 ) , denoted as GLM . Due to resource   limitations , we only pretrain the model for 250,000   steps , which are half of RoBERTa and BART ’s   training steps and close to T5 in the number of   trained tokens . More experiment details can be   found in Appendix A.   3.2 SuperGLUE   To evaluate our pretrained GLM models , we   conduct experiments on the SuperGLUE bench - mark ( Wang et al . , 2019 ) and report the standard   metrics . SuperGLUE consists of 8 challenging   NLU tasks . We reformulate the classiﬁcation tasks   as blank inﬁlling with human - crafted cloze ques-   tions , following PET ( Schick and Schütze , 2020b ) .   Then we ﬁnetune the pretrained GLM models on   each task as described in Section 2.3 . The cloze   questions and other details can be found in Ap-   pendix B.1 .   For a fair comparison with GLM and   GLM , we choose BERTand BERT   as our baselines , which are pretrained on the same   corpus and for a similar amount of time . We report   the performance of standard ﬁnetuning ( i.e. classiﬁ-   cation on the [ CLS ] token representation ) . The per-   formance of BERT with cloze questions is reported   in Section 3.4 . To compare with GLM , we   choose T5 , BART , and RoBERTa as our   baselines . T5 has no direct match in the number   of parameters for BERT , so we present the re-   sults of both T5(220 M parameters ) and T5   ( 770 M parameters ) . All the other baselines are of   similar size to BERT .   Table 1 shows the results . With the same amount   of training data , GLM consistently outperforms   BERT on most tasks with either base or large archi-   tecture . The only exception is WiC ( word sense dis-   ambiguation ) . On average , GLMscores 4.6 %   higher than BERT , and GLM scores 5.0 %   higher than BERT . It clearly demonstrates   the advantage of our method in NLU tasks . In   the setting of RoBERTa , GLM can still   achieve improvements over the baselines , but with   a smaller margin . Speciﬁcally , GLM outper-   forms T5 but is only half its size . We also ﬁnd   that BART does not perform well on the challeng-   ing SuperGLUE benchmark . We conjecture this   can be attributed to the low parameter efﬁciency of   the encoder - decoder architecture and the denoising   sequence - to - sequence objective .   3.3 Multi - Task Pretraining   Then we evaluate the GLM ’s performance in a   multi - task setting ( Section 2.1 ) . Within one train-   ing batch , we sample short spans and longer   spans ( document - level or sentence - level ) with   equal chances . We evaluate the multi - task model   for NLU , seq2seq , blank inﬁlling , and zero - shot   language modeling .   SuperGLUE . For NLU tasks , we evaluate mod-   els on the SuperGLUE benchmark . The results324   are also shown in Table 1 . We observe that with   multi - task pretraining , GLMand GLMper-   form slightly worse than GLM , but still outper-   form BERT and UniLM . Among multi-   task models , GLMoutperforms GLMby   1.1 % on average . Increasing GLM ’s param-   eters to 410 M ( 1.25 BERT ) leads to better   performance than GLM . GLM with 515 M pa-   rameters ( 1.5BERT ) can perform even better .   Sequence - to - Sequence . Considering the   available baseline results , we use the Gigaword   dataset ( Rush et al . , 2015 ) for abstractive summa-   rization and the SQuAD 1.1 dataset ( Rajpurkar   et al . , 2016 ) for question generation ( Du et al . ,   2017 ) as the benchmarks for models pretrained   on BookCorpus and Wikipedia . Additionally , we   use the CNN / DailyMail ( See et al . , 2017 ) and   XSum ( Narayan et al . , 2018 ) datasets for abstrac-   tive summarization as the benchmarks for modelspretrained on larger corpora .   The results for models trained on BookCorpus   and Wikipedia are shown in Tables 3 and 4 . We   observe that GLM can achieve performance   matching the other pretraining models on the two   generation tasks . GLMcan perform better than   GLM , while GLMperforms slightly worse   than GLM . This indicates that the document-   level objective , which teaches the model to extend   the given contexts , is less helpful to conditional   generation , which aims to extract useful informa-   tion from the context . Increasing GLM ’s pa-   rameters to 410 M leads to the best performance on   both tasks . The results for models trained on larger   corpora are shown in Table 2 . GLM can   achieve performance matching the seq2seq BART   model , and outperform T5 and UniLMv2 .   Text Inﬁlling . Text inﬁlling is the task of pre-   dicting missing spans of text which are consistent325   with the surrounding context ( Zhu et al . , 2019 ;   Donahue et al . , 2020 ; Shen et al . , 2020 ) . GLM   is trained with an autoregressive blank inﬁlling   objective , thus can straightforwardly solve this   task . We evaluate GLM on the Yahoo Answers   dataset ( Yang et al . , 2017 ) and compare it with   Blank Language Model ( BLM ) ( Shen et al . , 2020 ) ,   which is a speciﬁcally designed model for text in-   ﬁlling . From the results in Table 5 , GLM outper-   forms previous methods by large margins ( 1.3 to   3.9 BLEU ) and achieves the state - of - the - art result   on this dataset . We notice that GLMslightly   underperforms GLM , which is consistent with   our observations in the seq2seq experiments .   Language Modeling . Most language model-   ing datasets such as WikiText103 are constructed   from Wikipedia documents , which our pretraining   dataset already contains . Therefore , we evaluate   the language modeling perplexity on a held - out   test set of our pretraining dataset , which contains   about 20 M tokens , denoted as BookWiki . We also   evaluate GLM on the LAMBADA dataset ( Paperno   et al . , 2016 ) , which tests the ability of systems to   model long - range dependencies in text . The task   is to predict the ﬁnal word of a passage . As the   baseline , we train a GPT model ( Radford et al . ,   2018b ; Brown et al . , 2020 ) with the same data and   tokenization as GLM .   The results are shown in Figure 4 . All the models   are evaluated in the zero - shot setting . Since GLM   learns the bidirectional attention , we also evalu-   ate GLM under the setting in which the contexts   are encoded with bidirectional attention . Without   generative objective during pretraining , GLM   can not complete the language modeling tasks ,   with perplexity larger than 100 . With the same   amount of parameters , GLMperforms worse   than GPT . This is expected since GLM   also optimizes the blank inﬁlling objective . In-   creasing the model ’s parameters to 410 M ( 1.25 of   GPT ) leads to a performance close to GPT .   GLM ( 1.5of GPT ) can further outper-   form GPT . With the same amount of param-   eters , encoding the context with bidirectional at-   tention can improve the performance of language   modeling . Under this setting , GLM outper-   forms GPT . This is the advantage of GLM   over unidirectional GPT . We also study the con-   tribution of 2D positional encoding to long text   generation . We ﬁnd that removing the 2D posi-   tional encoding leads to lower accuracy and higher   perplexity in language modeling.326   Summary . Above all , we conclude that GLM   effectively shares model parameters across natu-   ral language understanding and generation tasks ,   achieving better performance than a standalone   BERT , encoder - decoder , or GPT model .   3.4 Ablation Study   Table 6 shows our ablation analysis for GLM .   First , to provide an apple - to - apple comparison with   BERT , we train a BERT model with our im-   plementation , data , and hyperparameters ( row 2 ) .   The performance is slightly worse than the ofﬁcial   BERT and signiﬁcantly worse than GLM .   It conﬁrms the superiority of GLM over Masked   LM pretraining on NLU tasks . Second , we show   the SuperGLUE performance of GLM ﬁnetuned as   sequence classiﬁers ( row 5 ) and BERT with cloze-   style ﬁnetuning ( row 3 ) . Compared to BERT with   cloze - style ﬁnetuning , GLM beneﬁts from the au-   toregressive pretraining . Especially on ReCoRD   and WSC , where the verbalizer consists of multi-   ple tokens , GLM consistently outperforms BERT .   This demonstrates GLM ’s advantage in handling   variable - length blank . Another observation is that   the cloze formulation is critical for GLM ’s perfor-   mance on NLU tasks . For the large model , cloze-   style ﬁnetuning can improve the performance by   7 points . Finally , we compare GLM variants with   different pretraining designs to understand their   importance . Row 6 shows that removing the span   shufﬂing ( always predicting the masked spans from   left to right ) leads to a severe performance drop on   SuperGLUE . Row 7 uses different sentinel tokens   instead of a single [ MASK ] token to represent dif-   ferent masked spans . The model performs worse   than the standard GLM . We hypothesize that it   wastes some modeling capacity to learn the differ-   ent sentinel tokens which are not used in down-   stream tasks with only one blank . In Figure 4 , we   show that removing the second dimension of 2D   positional encoding hurts the performance of longtext generation .   We note that T5 is pretrained with a similar blank   inﬁlling objective . GLM differs in three aspects :   ( 1 ) GLM consists of a single encoder , ( 2 ) GLM   shufﬂes the masked spans , and ( 3 ) GLM uses a   single [ MASK ] instead of multiple sentinel tokens .   While we can not directly compare GLM with T5   due to the differences in training data and the num-   ber of parameters , the results in Tables 1 and 6 have   demonstrated the advantage of GLM .   4 Related Work   Pretrained Language Models . Pretraining large-   scale language models signiﬁcantly improves the   performance of downstream tasks . There are three   types of pretrained models . First , autoencoding   models learn a bidirectional contextualized encoder   for natural language understanding via denoising   objectives ( Devlin et al . , 2019 ; Joshi et al . , 2020 ;   Yang et al . , 2019 ; Liu et al . , 2019 ; Lan et al . , 2020 ;   Clark et al . , 2020 ) . Second , autoregressive mod-   els are trained with a left - to - right language mod-   eling objective ( Radford et al . , 2018a , b ; Brown   et al . , 2020 ) . Third , encoder - decoder models are   pretrained for sequence - to - sequence tasks ( Song   et al . , 2019 ; Lewis et al . , 2019 ; Bi et al . , 2020 ;   Zhang et al . , 2020 ) .   Among encoder - decoder models , BART ( Lewis   et al . , 2019 ) conducts NLU tasks by feeding the   same input into the encoder and decoder , and tak-   ing the ﬁnal hidden states of the decoder . Instead ,   T5 ( Raffel et al . , 2020 ) formulates most language   tasks in the text - to - text framework . However , both   models require more parameters to outperform au-   toencoding models such as RoBERTa ( Liu et al . ,   2019 ) . UniLM ( Dong et al . , 2019 ; Bao et al . , 2020 )   uniﬁes three pretraining models under the masked   language modeling objective with different atten-   tion masks .   NLU as Generation . Previously , pretrained   language models complete classiﬁcation tasks for327NLU with linear classiﬁers on the learned rep-   resentations . GPT-2 ( Radford et al . , 2018b ) and   GPT-3 ( Brown et al . , 2020 ) show that generative   language models can complete NLU tasks such   as question answering by directly predicting the   correct answers without ﬁnetuning , given task in-   structions or a few labeled examples . However ,   generative models require much more parameters   to work due to the limit of unidirectional atten-   tion . Recently , PET ( Schick and Schütze , 2020a , b )   proposes to reformulate input examples as cloze   questions with patterns similar to the pretraining   corpus in the few - shot setting . It has been shown   that combined with gradient - based ﬁnetuning , PET   can achieve better performance in the few - shot set-   ting than GPT-3 while requiring only 0.1 % of its   parameters . Similarly , Athiwaratkun et al . ( 2020 )   and Paolini et al . ( 2020 ) convert structured predic-   tion tasks , such as sequence tagging and relation   extraction , to sequence generation tasks .   Blank Language Modeling . Donahue et al .   ( 2020 ) and Shen et al . ( 2020 ) also study blank-   ing inﬁlling models . Different from their work ,   we pre - train language models with blank inﬁlling   objectives and evaluate their performance in down-   stream NLU and generation tasks .   5 Conclusions   GLM is a general pretraining framework for nat-   ural language understanding and generation . We   show that the NLU tasks can be formulated as con-   ditional generation tasks , and therefore solvable by   autoregressive models . GLM uniﬁes the pretrain-   ing objectives for different tasks as autoregressive   blank inﬁlling , with mixed attention masks and   the novel 2D position encodings . Empirically we   show that GLM outperforms previous methods for   NLU tasks and can effectively share parameters for   different tasks .   Acknowledgements   The work is supported by the NSFC for Distin-   guished Young Scholar(61825602 ) , and Beijing   Academy of Artiﬁcial Intelligence ( BAAI ) .   References328329   A Pretraining Setting   A.1 Datasets   To train GLMand GLM , we use Book-   Corpus ( Zhu et al . , 2015 ) and Wikipedia used by   BERT ( Devlin et al . , 2019 ) .   To train GLM , we follow the pretraining   datasets of RoBERTa ( Liu et al . , 2019 ) , which con-   sist of BookCorups ( Zhu et al . , 2015),Wikipedia   ( 16 GB ) , CC - News ( the English portion of the Com-   monCrawl News dataset76 GB ) , OpenWebText   ( web content extracted from URLs shared on Red-   dit with at least three upvotes(Gokaslan and Co-   hen , 2019 ) , 38 GB ) and Stories ( subset of Common-   Crawl data ﬁltered to match the story - like style of   Winograd schemas ( Trinh and Le , 2019 ) , 31 GB ) .   The Stories dataset is no longer publicly available .   Therefore , we remove the Stories dataset and re-   place OpenWebText with OpenWebText2(66 GB ) .   The CC - News dataset is not publicly available and   we use the CC - News - en published by ( Mackenzie   et al . , 2020 ) . All the datasets used total 158 GB of   uncompressed texts , close in size to RoBERTa ’s   160 GB datasets .   A.2 Hyperparameters   The hyperparameters for GLMand GLM   are similar to those used by BERT . For trade - off   of training speed and fair comparison with BERT   ( batch size 256 and 1,000,000 training steps ) , we   use batch size of 1024 and 200,000 training steps   for GLM . Since GLMis smaller , we re-   duce the number of training steps to 120,000 to   speed up pre - training . The hyperparameters for   GLMand GLMare the same as those of   GLM . The hyperparameters except Trans-   former architecture for GLM and GLM   are the same as those of GLM . The models   are trained on 64 V100 GPUs for 200 K steps with   batch size of 1024 and maximum sequence length   of 512 , which takes about 2.5 days for GLM .   To train GLM , we follow most of the hy-   perparameters of RoBERTa . The main difference330   Hyperparameters GLM GLM GLM   Number of Layers 12 24 24   Hidden size 768 1024 1024   FFN inner hidden size 3072 4096 4096   Attention heads 12 16 16   Attention head size 64 64 64   Dropout 0.1 0.1 0.1   Attention Dropout 0.1 0.1 0.1   Warmup Steps 6k 8k 30 K   Peak Learning Rate 4e-4 2e-4 4e-4   Batch Size 1024 1024 8192   Weight Decay 0.1 0.1 0.01   Max Steps 120k 200k 250k   Learning Rate Decay Cosine Cosine Cosine   Adam 1e-6 1e-6 1e-6   Adam   0.9 0.9 0.9   Adam   0.98 0.98 0.98   Gradient Clipping 1.0 1.0 1.0   includes : ( 1 ) Due to resource limit , we only pre-   train GLM for 250,000 steps , which are   half of RoBERTa and BART ’s training steps , and   close to T5 in number of trained tokens . ( 2 ) We use   cosine decay instead of linear decay for learning   rate scheduling ( 3 ) We additionally apply gradient   clipping with value 1.0 .   The hyperparameters for all the pre - training set-   tings are summarized in Table 7 .   A.3 Implementation   Our pretraining implementation is based on   Megatron - LM ( Shoeybi et al . , 2019 ) and Deep-   Speed ( Rasley et al . , 2020 ) . We include our code in   the supplementary material . Due to the size limit of   supplementary material , we can not include the pre-   trained models , but will make them public available   in the future .   B Downstream Tasks   B.1 SuperGLUE   The SuperGLUE benchmark consists of 8 NLU   tasks . We formulate them as blank inﬁlling tasks ,   following ( Schick and Schütze , 2020b ) . Table 8   shows the cloze questions and verbalizers we used   in our experiments . For 3 tasks ( ReCoRD , COPA ,   and WSC ) , the answer may consist of multiple   tokens , and for the other 5 tasks , the answer is   always a single token . When ﬁnetuning GLM on the SuperGLUE tasks ,   we construct the input using the cloze questions   in Table 8 and replace the blank with a [ MASK ]   token . Then we compute the score of generating   each answer candidate . For the 5 single - token tasks ,   the score is deﬁned to be the logit of the verbal-   izer token . For the 3 multi - token tasks , we use   the sum of the log - probabilities of the verbalizer   tokens . Thanks to the autoregressive blank inﬁll-   ing mechanism we proposed , we can obtain all the   log - probabilities in one pass . Then we compute the   cross entropy loss using the groundtruth label and   update the model parameters .   For the baseline classiﬁers , we follow the stan-   dard practice to concatenate the input parts of each   task ( such as the premise and hypothesis for textual   entailment , or the passage , question and answer   for ReCORD and MultiRC ) and add a classiﬁca-   tion layer on top of the [ CLS ] token representa-   tion . We also implemented cloze - style ﬁnetuning   for the other pre - trained models , but the perfor-   mance was usually similar to the standard classiﬁer ,   as we shown in the ablation study . Models with   blank - inﬁlling objectives , such as T5 and our GLM ,   beneﬁts more from converting the NLU tasks into   cloze questions . Thus for T5 and GLM , we report   the performance after such conversion in our main   results.331   B.2 Sequence - to - Sequence   Fot the text summarization task , we use the dataset   Gigaword ( Rush et al . , 2015 ) for model ﬁne - tuning   and evaluation . We ﬁnetune GLM on the   training set for 4 epochs with AdamW optimizer .   The learning rate has a peak value of 3e-5 , warm-   up over the 6 % training steps and a linear decay .   We also use label smoothing with rate 0.1 ( Pereyra   et al . , 2017 ) . The maximum document length is 192   and the maximum summary length is 32 . During   decoding , we use beam search with beam size of 5   and remove repeated trigrams . We tweak the value   of length penalty on the development set . The   evaluation metrics are the F1 scores of Rouge-1 ,   Rouge-2 , and Rouge - L ( Lin , 2004 ) on the test set .   For the question generation task , we use the   SQuAD 1.1 dataset ( Rajpurkar et al . , 2016 ) and   follow the dataset split of ( Du et al . , 2017 ) . The   optimizer hyperparameters are the same as those of   abstractive summarization . The maximum passage   length is 464 and the maximum question length   is 48 . During decoding , we use beam search with   beam size 5 and tweak the value of length penalty   on the development set . The evaluation metrics are   the scores of BLEU-1 , BLEU-2 , BLEU-3 , BLEU-   4 ( Papineni et al . , 2002 ) , METEOR ( Denkowski   and Lavie , 2014 ) and Rouge - L ( Lin , 2004 ) .   Results of T5 on XSum are obtained by run-   ning the summarization script provided by Hug-   gingface transformers . All the other results ofbaselines on seq2seq tasks are obtained from the   corresponding papers .   B.3 Text Inﬁlling   We follow ( Shen et al . , 2020 ) and evaluate text in-   ﬁlling performance on the Yahoo Answers dataset   ( Yang et al . , 2017 ) , which contains 100K/10K/10 K   documents for train / valid / test respectively . The av-   erage document length is 78 words . To construct   the text inﬁlling task , we randomly mask a given ra-   tior2f10%50%gof each document ’s tokens   and the contiguous masked tokens are collapsed   into a single blank . We ﬁnetune GLM on the   training set for 5 epochs with dynamic masking , i.e.   the blanks are randomly generated at training time .   Similar to the sequence - to - sequence experiments ,   we use an AdamW optimizer with a peak learning   rate 1e-5 and 6 % warm - up linear scheduler .   For comparison with previous work , we use the   same test set constructed by ( Shen et al . , 2020 ) .   The evaluation metric is the BLEU score of the in-   ﬁlled text against the original document . We com-   pare with two baselines : ( 1 ) BERT , which learns a   left - to - right language model to generate the masked   tokens on top of the blank representation , and ( 2 )   BLM proposed by ( Shen et al . , 2020 ) , which can   ﬁll in the blank with arbitrary trajectories .   B.4 Language Modeling   We evaluate the model ’s ability of language model-   ing with perplexity on BookWiki and accuracy on   the LAMBDA dataset ( Paperno et al . , 2016 ) .   Perplexity is an evaluation criterion that has been332well studied for language modeling . Perplexity is   the exponentiation of the average cross entropy of   a corpus .   PPL= exp( 1   TXp(xjx ) ) ( 4 )   where x= [ x;;x ] . Since transformers   can only operate on a window of ﬁxed input size   w , we can not fully calculate p(xjx)and can   only calculate p(xjx ) . Even calculating   this value for each token is prohibitively expensive ,   since we need to conduct Tevaluations of w - size   contexts . To improve evaluation efﬁciency , we   adopt overlapping evaluation , where we advance   the sliding windows by some overlap oeach time   and only compute the cross entropy loss for the last   otokens of the window . In our experiments we set   o= 256 for all the models .   LAMBDA is a cloze - style dataset to test the abil-   ity of long - range dependency modeling . Each ex-   ample is a passage consisting of 4 - 5 sentences with   the last word missing and the model is required to   predict the last word of the passage . Since we use   WordPiece tokenization , a word can be split into   several subword units . We use teacher forcing and   consider the prediction correct only when all the   predicted tokens are correct .   C Results on Other NLU Benchmarks   GLUE ( Wang et al . , 2018 ) is another widely - used   NLU benchmark , including single sentence tasks   ( e.g. sentiment analysis ( Socher et al . , 2013 ) ) and   sentence pair tasks ( e.g. text similarity ( Cer et al . ,   2017 ) and natural language inference ( Williams   et al . , 2018 ; Dagan et al . , 2005 ) ) . The benchmark is   usually considered as less challenging than Super-   GLUE . SQuAD ( Rajpurkar et al . , 2016 , 2018 ) is an   extractive question answering benchmark . We fur-   ther compare GLM with BERT on the two bench-   marks .   The results on GLUE and SQuAD are shown in   Tables 9 and 10 . On the two benchmarks , GLM   can still outperform BERT with the same amount   of parameters , but with a smaller margin .   D Text Generation Samples   We show texts generated by GLMgiven unseen   contexts randomly sampled from the test set . We   use top - krandom sampling with k= 40 for gen-   eration and set maximum sequence length to 512 .   Some of the texts are cut short . Example D.1 . The Wyoming State Legislature is   the legislative branch of the U.S. State of Wyoming .   GLM : It consists of a state senate and a house   of representatives . As of 2019 , state senators are   Steve Kish , Joe L. Williams , and Linda S. Bollens .   Members of the Wyoming State Legislature are   elected from single - member districts representing   the majority of the state . The current state senate   members are : In recent years , there have been four   changes to the senate . The most recent is the cre-   ation of a six - seat district that includes all or part of   the following : In the 2009 elections , the state sen-   ate members were elected to six - year terms . The   current state house members are : The Wyoming   Constitution assigns certain powers to the governor .   Most notably , the governor is president of the sen-   ate and governor . However , if the governor desires   to appoint a member to the Wyoming state senate , a   law authorizes the governor to do so . The governor   of Wyoming holds no legislative power but has the   power to veto lawmakers , which is not limited to   the veto of laws . Under the wyoming state con-   stitution , the governor can veto the actions of the   other members of the wyoming house of represen-   tatives . The governor can also appoint members of   the wyoming senate . In addition , the governor can   appoint members of the Wyoming house of repre-   sentatives . Wyoming ’s constitution provides that   the governor can appoint a member of the wyoming   state senate to the wyoming supreme court , and the   chairman of the wyoming senate .   Example D.2 . Jonathan Terry is a television and   ﬁlm actor .   GLM : He was a voice actor for the “ X - Men "   cartoon series . He was also a voice actor for “ the   Simpsons " as well as “ the marvelous misadven-   tures of superman . He also has voiced characters   in “ the legend of korra " He has appeared on several   television series , including “ the simpsons " , “ the   x-ﬁles " , “ heroes " and “ the simpsons movie " as the   character captain billy higgledy - pig , and the web   series “ krusty mysteries " as the character Colonel   Trungus . He wrote for “ the Guardian " newspaper .   Jonathan Terry was born in London . Terry grew   up in Surrey , England and attended the University   of Sussex in the United Kingdom , graduating with   a degree in english literature . He was a guest lec-   turer at King ’s College London , and then took two   years of acting courses at the brit school of acting   to prepare for his future career in the entertainment   industry . Terry ﬁrst appeared in the TV series “ the333   Model MNLI QNLI QQP RTE SST-2 MRPC CoLA STS - B Avg   BERT 86.6 92.3 91.3 73.6 93.2 88.0 60.6 90.0 84.4   GLM 86.7 92.8 91.5 74.0 93.5 90.0 61.4 90.7 85.1   ModelSQuAD v1.1   EM / F1SQuAD v2.0   EM / F1   BERT 80.8 / 88.5 73.7 / 76.3   GLM 81.5 /88.6 74.7 /77.8   BERT 84.1 / 90.9 79.0 / 81.8   GLM 85.4 /91.6 80.3 /83.3   Simpsons " as the character captain Billy Higgledy-   pig , but his character was only a one - time recurring   character in the series ’ ﬁrst six seasons . He later ap-   peared as a regular for the show ’s ﬁnal six seasons ,   and has been a frequent guest in the show since .   He appeared in the ﬁrst few episodes of “ " as the   character major Jack Ryan . He has also appeared   as part of the supporting cast of several episodes of   “ the secret life of pets " . He has also worked on “ the   simpsons " TV show since “ the simpsons movie " ,   most notably playing the roles of Captain Skeletor   and the ghost of the same name . He plays char-   acters in several ﬁlms , including “ " , “ " , “ " and “ " .   He has appeared in music videos for the killers in   1993 , the pretenders in 1995 , and in the TV shows   “ the royal " and “ the bill " .   Example D.3 . Corona was a station along the port   Washington branch of the long island rail road in   the Corona section of queens , New York City . It   was one of two stations built by the ﬂushing rail-   road in Corona , this one having been at Grand Av-   enue ( later called National Avenue , now National   Street ) and 45th Avenue .   GLM : It had a brick entrance building next to   the tracks , and one platform with 2 side platforms .   The station burned down and was rebuilt in 1908 .   The station closed on september 15 , 1927 , with   the train service transferred from Grand Avenue to   45th Avenue . There were plans to build a subway   extension to Corona , but it was never built . In the   1980s , two stations were constructed on the line ,   Corona Road and Corona Park . The new Corona   station opened in 1988 , and the original Corona   station was demolished . Example D.4 . Robert Lee Smith ( born July 5 ,   1938 ) is a former American football cornerback   in the national football league . He played for the   Los Angeles Rams ( 1962–1965 ) and the Detroit   Lions ( 1965–1966 ) . He was also a kick and punt   returner .   GLM : In his four - year NFL career , he played   in 33 games and started 14 , registering 62 career   interceptions . He completed his NFL career with   ﬁve interceptions . Smith was born in La Canada   Flintridge , Michigan , in 1938 . He attended Bishop   O ’ Dowd high school in Flintridge . Smith ’ s fa-   ther was a pharmacist . He graduated from Michi-   gan State University in 1958 with a degree in busi-   ness administration . Smith played for the Michi-   gan Wolverines football team from 1959 to 1963 .   In his two years as a reserve cornerback , he led   the conference in interceptions with ﬁve . In 1962 ,   he set the Wolverines ’ all - time interception record   with 13 , and was second overall in the 1962 sea-   son ’s Heisman Trophy voting . Smith also won   the Dick Butkus award as the nation ’s outstand-   ing linebacker . In 1961 , the “ Los Angeles Times "   wrote that Smith “ is an outstanding pass rusher ,   with an average of almost 100 yards per punt re-   turn . " Smith was inducted into the university of   Michigan athletic hall of honor in 1989 and the   national football foundation hall of fame in 1991 .   He was elected to the Michigan sports hall of fame   in 1995 . Smith earned the honor because of his ac-   complishments prior to his NFL career . He was one   of four Michigan players honored as ﬁrst - overall   selections in the 1964 NFL draft . The others were   Joe Namath , Bill Nelsen , and Jerry Kramer . In   1966 , the NFL gave players $ 300,000 a season to   play football . After his rookie season , he was not   selected to play in the 1966 pro bowl . On January   13 , 1966 , the Rams traded smith to the Detroit Li-   ons for Paul Hornung , and later that year he was   traded to the Lions for Ray “ the Lion " Jones in   exchange for Linebacker Jim “ the Hawk " Johnson .   On September 10 , 1968 , he was traded back to Los   Angeles for a second round pick in the 1970 draft .   He was also traded to the St. Louis Cardinals for334a second round pick in the 1970 draft . On June 2 ,   1970 he was cut by the Cardinals . On November   15 , 1970 , the Los Angeles Rams acquired Smith   from the Lions in exchange for Linebacker Tony   Harris . The Rams waived Smith during the Septem-   ber 1 , 1972 offseason . Smith ’s number at Michigan   State was # 7 in 1969.335