  Junyi Li , Tianyi Tang , Zheng Gong , Lixin Yang , Zhuohao Yu , Zhipeng Chen ,   Jingyuan Wang , Wayne Xin ZhaoandJi - Rong WenGaoling School of Artificial Intelligence , Renmin University of ChinaRenmin University of ChinaPeng Cheng LaboratorySchool of Computer Science and Engineering , Beihang UniversityBeijing Key Laboratory of Big Data Management and Analysis Methods   { lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com   Abstract   Nowadays , pretrained language models ( PLMs )   have dominated the majority of NLP tasks .   While , little research has been conducted on   systematically evaluating the language abilities   of PLMs . In this paper , we present a large - scale   empirical study on gen Erallanguage ab ility   evaluation of PLM s ( ElitePLM ) . In our study ,   we design four evaluation dimensions , i.e. ,   memory , comprehension , reasoning , and com-   position , to measure ten widely - used PLMs   within five categories . Our empirical results   demonstrate that : ( 1 ) PLMs with varying train-   ing objectives and strategies are good at differ-   ent ability tests ; ( 2 ) fine - tuning PLMs in down-   stream tasks is usually sensitive to the data size   and distribution ; ( 3 ) PLMs have excellent trans-   ferability between similar tasks . Moreover , the   prediction results of PLMs in our experiments   are released as an open resource for more deep   and detailed analysis on the language abilities   of PLMs . This paper can guide the future   work to select , apply , and design PLMs for   specific tasks . We have made all the details   of experiments publicly available at .   1 Introduction   Recent years have featured a trend towards Trans-   former ( Vaswani et al . , 2017 ) based pretrained lan-   guage models ( PLMs ) in natural language process-   ing ( NLP ) systems . By being pretrained on massive   unlabeled text , PLMs can be directly fine - tuned on   downstream tasks , entirely removing the need for   task - specific architectures ( Radford et al . , 2018 ) .   This paradigm has led to significant progress on   many challenging NLP tasks such as reading com-   prehension ( Devlin et al . , 2019 ) and text genera-   tion ( Brown et al . , 2020 ) .   With rising new state - of - the - art results that ap-   proach or surpass human performance on several   tasks , it is a non - trivial research topic about howto systematically evaluate the language abilities of   PLMs from a wide range of perspectives . Given a   wide range of publicly released PLMs , it is partic-   ularly useful to derive principles or guidelines for   selecting suitable PLMs for specific downstream   tasks . However , existing works either target some   single ability ( Talmor et al . , 2020 ; Zhou et al . ,   2020 ) , or consider a simple mixture of multiple   ( small - scale ) tasks that lack a comprehensive de-   sign and test ( Wang et al . , 2019b ; Liang Xu , 2020 ) .   There has been no detailed and systematic analysis   of PLM ’s abilities in large - scale NLP tasks . To   fill the gap of PLMs evaluation , we introduce the   genErallanguage ab ilityevaluation ( ElitePLM )   for empirically and systematically assessing the   general language abilities of PLMs .   The ideal goal behind PLMs is to create a human-   like machine learner where it can understand the   language and then perform any specific task re-   lated to language . In cognitive science , Wechsler   Adult Intelligence Scale ( WAIS ) ( Kaufman and   Lichtenberger , 2005 ) is the most commonly used   intelligence quotient ( IQ ) test for measuring the   intelligence and cognitive ability of humans . This   test would assess the level of individuals on ver-   bal comprehension , perceptual reasoning , working   memory , and processing speed . Thus , by imitat-   ing the intelligence test on humans , we design four   evaluation dimensions in ElitePLM for measuring   the abilities of PLMs , including memory , compre-   hension , reasoning , and composition . Following   previous works ( Zhou et al . , 2020 ; Wang et al . ,   2019b ) , for each ability in ElitePLM , we elabo-   rate and select multiple representative tasks ( e.g. ,   question answering for the comprehension ability )   and commonly - used benchmarks ( e.g. , GLUE and   SQuAD ) to quantitatively evaluate the performance   of PLMs . These results can serve as numerical ex-   planations of PLMs at a specific ability .   In human intelligence tests , the background of   participants ( e.g. , gender , race , and occupation)3519should be as much as diverse . Thus , in ElitePLM ,   we select a diversity of PLMs to conduct general-   ized and meaningful comparisons . According to   training objectives , PLMs can be divided into three   types : bidirectional LMs ( e.g. , BERT ( Devlin et al . ,   2019 ) ) for natural language understanding ( NLU ) ,   unidirectional LMs ( e.g. , GPT ( Radford et al . ,   2019 ) ) for natural language generation ( NLG ) , and   hybrid LMs ( e.g. , UniLM ( Dong et al . , 2019 ) )   for combining these two paradigms . Furthermore ,   knowledge - enhanced LMs ( e.g. , ERNIE ( Zhang   et al . , 2019 ) ) and text - to - text LMs ( e.g. , T5 ( Raffel   et al . , 2020 ) ) also emerge as important branches of   PLMs . Considering the variety , we finally select   ten widely - used PLMs within the above five cate-   gories and evaluate their abilities on four dimen-   sions . We show the comparisons of these PLMs in   Table 7 of Appendix A.   From the ability test results , we have three salient   findings . First , PLMs with varying pretraining ob-   jectives and strategies are good at different kinds   of downstream tasks . Specifically , we observe that   bidirectional LMs like BERT and pretraining strate-   gies like larger training batches in RoBERTa are   helpful for memorizing pretraining corpora ; permu-   tation language modeling in XLNet is beneficial   for modeling the bidirectional context in language   comprehension ; inter - sentence coherence objective   in ALBERT is suitable for sentence - level reasoning   tasks ; text - to - text LMs using denoising objective   like BART perform better in short text generation .   Second , when fine - tuning PLMs in downstream   tasks , their performance is typically sensitive to the   data distribution in fine - tuning stage , which can be   addressed by incorporating intermediate datasets or   tasks to alleviate such a discrepancy . Third , PLMs   have excellent transferability between similar tasks ,   especially reasoning tasks . This finding will inspire   future researchers to leverage data - rich tasks for   improving data - scarce tasks . For more clarity , we   illustrate the impact level of each factor for PLMs ’   abilities in Table 8 of Appendix A.   Besides ElitePLM being an evaluation bench-   mark of PLMs ’ language ability , more importantly ,   the predicted results of ElitePLM can be used as   an open resource for more depth and granularity in   analyzing PLMs performance on each ability . For   example , we further analyze the comprehension   test results of PLMs across answer types in QA   tasks . The analysis shows that PLMs are good at   simple single - token answers such as dates but morechallenged on intricate phrase answers . Moreover ,   by analyzing human test and Turing test results   on composition , we observe that summaries with   high accuracy are more likely to pass the Turing   test while rich information is more important for   story generation . Overall , ElitePLM can act as an   analysis tool to gain more insight into PLMs . We   show the details of our used datasets and predicted   outputs of PLMs in Appendix B.   This paper is intended to help establish sound   principles for choosing , applying , interpreting and   designing PLMs for NLP tasks in practical settings .   We have released the code and predicted results   of each ability experiment , providing the research   and industry community with off - the - shelf tools to   evaluate and analyze their PLMs .   2 ElitePLM   In this section , we will detail these four kinds of   language abilities , i.e. ,memory , comprehension ,   reasoning , and composition , in ElitePLM .   Memory Ability . Memory is the most basic abil-   ity of humanity , involved in how much informa-   tion we recall throughout our lives ( Miyake and   Shah , 1999 ) . By analogy , ElitePLM will measure   how much knowledge and language patterns PLMs   have memorized in pretraining , as assessed by tests   of recalling words based on contexts . Based on   the memorized information , PLMs can effectively   adapt to downstream tasks for understanding and   reasoning about the similar context in a specific   text . On the other hand , efficiency is also a critical   aspect of memory ability for PLMs learning from   new data distribution in the fine - tuning stage . Thus ,   besides recalling words , we also compare the mem-   ory efficiency of PLMs in terms of memorizing the   given new information .   Comprehension Ability . Comprehension is an in-   tricate and multifaceted ability . It typically consists   of understanding a text ’s vocabulary , background   knowledge of a specific topic , and comprehension   of its linguistic structures like grammar ( Cain and   Oakhill , 2008 ) . In particular , background ( prior )   knowledge is used to comprehend a special situa-   tion , lesson , or text . For example , readers should   be aware of the background knowledge of dog be-   havior when reading a text about dog training . In   ElitePLM , we will assess PLMs ’ comprehension   ability from three aspects , i.e. ,vocabulary , back-   ground knowledge , and linguistic structures.3520Reasoning Ability . Based on the comprehension   of a text , reasoning ability refers to the power of the   processes and strategies used in drawing inferences ,   reaching conclusions , arriving at solutions , and   making decisions ( Kyllonen and Christal , 1990 ) .   In ElitePLM , we mainly focus on three types of rea-   soning abilities . In detail , commonsense reasoning   requires PLMs to draw inferences using common-   sense knowledge about the world , like the fact that   “ matches ” plus “ logs ” usually equals “ fire ” ( Sap   et al . , 2020 ) ; Note that subtle differences exist be-   tween commonsense knowledge and background   knowledge in comprehension ability . Common-   sense knowledge is broadly defined as the total   accumulation of facts and information that a per-   son has gained from previous experiences . Deduc-   tive reasoning involves PLMs drawing conclusions   from a set of given premises in the form of cate-   gorical syllogisms ( e.g. , allxarey ) or symbolic   logic ( e.g. , ifpthenq ) ( Johnson - Laird , 1999 ) ; Ab-   ductive reasoning involves reaching the most likely   explanation for a set of facts , such as a scientific   theory to explain a set of empirical findings ( Wal-   ton , 2014 ) .   Composition Ability . In the literature ( Connors ,   1997 ) , composition is a highly intelligent and syn-   thetic process where a writer assembles words and   sentences to create a coherent and meaningful work   ( e.g. , poem , music , and novel ) from scratch , which   closely resembles to the text generation task in   NLP ( Berninger , 1999 ) . Therefore , in ElitePLM ,   we introduce several text generation tasks to eval-   uate the composition ability of PLMs , including   story generation , text summarization , and question   generation . Note that , story generation is a repre-   sentative composition task which needs PLMs to   not only comprehend the given story background ,   but also reason about and create reasonable and   coherent story endings ( Fan et al . , 2018 ) . During   the composition process , PLMs should include a   good vocabulary , grammar , spelling , and punctua-   tion knowledge , and deliberate the text structure .   3 Experiments   In this section , we first set up baselines , and then   report the results and analysis on four ability tests .   3.1 Models   As mentioned before , we compare the performance   of ten publicly released PLMs from five categories :   ( 1)Bidirectional LMs : BERT ( Devlin et al . , 2019),RoBERTa ( Liu et al . , 2019b ) , and ALBERT ( Lan   et al . , 2020 ) ; ( 2 ) Unidirectional LMs : GPT-2 ( Rad-   ford et al . , 2019 ) ; ( 3 ) Hybrid LMs : XLNet ( Yang   et al . , 2019 ) and UniLM ( Dong et al . , 2019 ) ; ( 4 )   Knowledge - enhanced LMs : ERNIE ( Zhang et al . ,   2019 ) ; ( 5 ) Text - to - Text LMs : BART ( Lewis et al . ,   2020 ) , T5 ( Raffel et al . , 2020 ) , and ProphetNet ( Qi   et al . , 2020 ) . We implement these models and abil-   ity tests mostly on huggingface ( Wolf et al . , 2020 ) ,   fairseq ( Ott et al . , 2019 ) , and jiant ( Phang et al . ,   2020 ) . To reflect the true level of language abilities ,   we adopt the best hyper - parameter values reported   in their original papers for each PLM .   3.2 Memory Tests   Datasets and Metrics . The goal of memory tests   is to assess how much knowledge and language   patterns PLMs have memorized during pretraining .   For this purpose , we adopt two datasets for evalua-   tion , i.e. ,LAMA ( F. Petroni and Riedel , 2019 ) and   English Wikipedia ( 2,500 M words ) . Specifically ,   LAMA is a knowledge probe corpus containing   a set of knowledge facts , where facts are either   subject - relation - object triples or question - answer   pairs . Each fact is converted into a cloze state-   ment where the subject or object entity is masked .   Wikipedia is one of the widely - used pretraining   corpora for our selected PLMs ( except GPT-2 and   T5 ) . Therefore , to conduct a fair comparison , we   continuously train GPT-2 and T5 on Wikipedia us-   ing their pretraining objectives . Similar to LAMA ,   we randomly sample 100,000 texts from Wikipedia   and then mask a proportion of 15 % tokens follow-   ing BERT . By querying PLMs with the missing   tokens on Wikipedia and LAMA , we can test the   language patterns and factual knowledge in PLMs ’   memory . For metrics , we use Mean Precision at   One ( P@1)of predicting missing tokens . For ef-   ficiency , we measure it as the performance w.r.t .   the number of training epochs : the more efficient a   model is , the fewer epochs to achieve a reference   performance .   Results and Analysis . To evaluate how much text   PLMs have recalled in pretraining , we directly test   PLMs using Wikipedia and LAMA without fine-   tuning , similar to zero - shot learning . The results   onP@1metric are shown in Table 1 . Compared   with bidirectional and hybrid LMs ( e.g. , BERT and   XLNet ) , GPT-2 uses auto - regressive self - attention   where every token can only attend to the context to3521   its left . This unidirectional training objective natu-   rally limits the performance of GPT-2 in terms of   memorizing information . It has been previously re-   ported that PLMs can remember more information   by scaling up the model size ( Brown et al . , 2020 ) .   However , in our tests , BART - large ( 400 M ) achieves   worse results than RoBERTa - base ( 125 M ) with the   same training corpus and similar vocabulary sizes   ( 50,295 vs 50,265 ) . During pretraining , RoBERTa   adopts bidirectional objectives and novel strategies   like larger training batches . It can be concluded   that , as opposed to model size , training objectives   and strategies reflect the way that PLMs memo-   rize information , making significant impacts on   PLMs ’ memory ability . Besides , we can clearly   observe that all PLMs achieve their best resultsin T - REx ( created from Wikipedia triples ) among   LAMA , and perform relatively well on Wikipedia .   This implies that PLMs indeed remember a large   proportion of knowledge and language patterns   from pretraining corpora .   To test the memory efficiency , we fine - tune five   models , BERT , ALBERT , GPT-2 , BART , and XL-   Net , for several epochs . As shown in Figure 1 , to   achieve a reference performance , the bidirectional   training objective like BERT needs fewer epochs   than other kinds of objectives . This further im-   plies that the bidirectional training objective is also   helpful to facilitate the memory efficiency since   bidirectional language modeling can make PLMs   more quickly capture the language patterns .   Based on the memory test results , we further   analyze how to effectively elicit the information   from PLMs ’ memory . LAMA hand - crafts tem-   plates to test PLMs by filling the [ MASK ] token .   Therefore , we conduct a pilot study on designing   different templates for two relations in Google - RE .   Table 2 shows that different templates can result in   substantial differences in eliciting PLMs ’ memory .   The bidirectional LMs , e.g. , BERT , show relatively   adaptability to varying templates , further verifying   their strength in memory ability . Therefore , with   large - scale knowledge stored in PLMs , how to de-   rive an effective and appropriate method to provoke   them is a key challenge.3522   3.3 Comprehension Tests   Datasets and Metrics . In comprehension tests , we   take into account three aspects of comprehension   ability , including vocabulary , background knowl-   edge , and linguistic structures . Therefore , we em-   ploy five datasets for comprehension tests , i.e. ,   GLUE ( Wang et al . , 2019b ) , SuperGLUE ( Wang   et al . , 2019a ) , SQuAD v1.1 ( Rajpurkar et al . ,   2016 ) , SQuAD v2.0 ( Rajpurkar et al . , 2018 ) , and   RACE ( Lai et al . , 2017 ) . Among these datasets ,   GLUE and SuperGLUE are two widely - used com-   prehension benchmarks . Several tasks , like word   sense disambiguation and coreference resolution ,   can assess PLMs ’ understanding of vocabulary   meaning and grammatical structure of a text . By   contrast , SQuAD v1.1&v2.0 , and RACE are three   popular question answering datasets . To answer the   natural language questions , PLMs should be aware   of the background knowledge about some partic-   ular topic . For example , to answer the question   “ what can be used as rewards for dog training ? ” ,   the background knowledge “ dogs like bones ” will   be helpful for PLMs to answer “ bones ” . For evalu-   ation , we report the corresponding metrics results   for each task , such as the Matthews corr . metric for   CoLA .   Results and Analysis . Table 3 presents the results   of comprehension test in GLUE dataset ( results in   other four datasets can be found in Appendix D ) .   The last column in this table indicates the average   overall performance across all tasks . Interestingly ,   the models behaving well in memory tests ( e.g. ,   RoBERTa and XLNet ) also present good results   in many comprehension tasks . The results indi-   cate that the improvement on memory ability is   beneficial for the performance of comprehen-   sion ability , which is in line with our intuition .   Compared with bidirectional language modeling in3523   BERT , permutation language modeling ( relying on   all permutations of the factorization order ) used in   XLNet enables PLMs to learn more context for en-   hancing PLMs ’ understanding of text , which seems   to be effective for good comprehension ability .   Among these tasks , we observe a significant per-   formance drop in the linguistic acceptability task   ( CoLA ) since it has different data distribution from   the pretraining corpora ( Wang et al . , 2021 ) . This   kind of sensitiveness to unfamiliar tasks is also re-   flected in Figure 2 , where the model performance   on CoLA shows a more volatile fluctuation ( rang-   ing from 10 to 35 ) than QNLI ( ranging from 15 to   20 ) . It indicates that the performance of PLMs is   closely related to the similarity of data distribu-   tions in pretraining and fine - tuning . To solve this   challenge , it will be better to adopt intermediate   fine - tuning , which involves first fine - tuning PLMs   on an intermediate dataset similar to the final target   dataset and then transferring tuned PLMs to the   final dataset .   To gain more insights into PLMs ’ comprehen-   sion ability , we choose four representative PLMs   ( i.e. ,BERT , RoBERTa , ALBERT , and BART ) and   humans to analyze their performance across the   answer types of SQuAD v1.1&v2.0 . The results   in Figure 3 show that PLMs perform well on sim-   ple answers such as dates and persons . For these   categories of answers , there are usually only a few   plausible candidates and most answers are single   tokens . The models are more challenged on other   intricate answer types ( e.g. , noun and verb phrases )   because there are many more plausible candidates   and multiple tokens . Thus , improving PLMs ’ un-   derstanding of intricate named entities during the   pretraining phase will possibly benefit PLMs ’ com-   prehension ability later .   3.4 Reasoning Tests   Datasets and Metrics . In reasoning tests , we   mainly consider three forms of reasoning , i.e. ,com-   monsense reasoning , deductive reasoning , and ab-   ductive reasoning , focusing on commonsense uti-   lization , conclusion induction , and reason deriva-   tion , respectively . For evaluation , we choose six   reasoning datasets , namely CommonsenseQA ( Tal-   mor et al . , 2019 ) , ROCStories ( Mostafazadeh   et al . , 2016 ) , SWAG ( Zellers et al . , 2018 ) , Hel-   laSwag ( Zellers et al . , 2019 ) , Sense Making ( Wang   et al . , 2019c ) , and ARCT ( Habernal et al . , 2018 ) .   Specifically , CommonsenseQA requires PLMs to   reason about commonsense knowledge in human   experience of everyday life ( Liu and Singh , 2004 ) .   ROCStories , SWAG , HellaSwag , and Sense Mak-   ing Task A are concerned with deriving the con-   clusions of stories and events , while Sense Making   Task B and ARCT focus on identifying the reason   behind a statement . For evaluation , we report the   Accuracy results for each dataset .   Results and Analysis . Table 4 shows the model   performances in reasoning ability . It can be clearly   observed that performing well in comprehension   tests , ALBERT and RoBERTa also achieve stronger   performance in almost all reasoning tasks . In pre-3524   training , ALBERT introduces an inter - sentence co-   herence objective to capture the relationship among   sentences , which is helpful for the sentence - level   reasoning ability of PLMs . It has been found   that the next sentence prediction ( NSP ) loss in   BERT might hurt the performance of PLMs in   sentence - level tasks of downstream datasets ( Liu   et al . , 2019b ) . Interestingly , despite being the best   in comprehension tests , XLNet does not perform   as well as we expected in reasoning tests . We spec-   ulate that the permutation operation in XLNet dis-   turbs the semantic relationship between sentences ,   thus leading to poor reasoning ability . To improve   PLMs ’ reasoning ability , it would be useful to   design sentence - level reasoning objectives like   inter - sentence coherence loss in ALBERT . More-   over , despite incorporating knowledge , ERNIE still   shows mediocre performance in knowledge - related   datasets such as CQA . A possible reason might be   that ERNIE only uses trained KB embeddings to   enhance semantic representations but ignores the   reasoning structure of KBs . This inspires us that   designing appropriate and effective fusion methods   to integrate knowledge is more important .   To further analyze the transferability of PLMs ’   reasoning ability , we conduct a two - stage study on   three task datasets , i.e. ,ROCStories , SM - A , and   ARCT . We first train PLMs on source tasks with   full data and then fine - tune PLMs on target tasks   with ten instances . In Figure 4 , it can be observed   thatPLMs have better reasoning transferability   between similar tasks such as deductive reason-   ing tasks ( ROCStories and SM - A ) . This shows that   model performance on data - scarce reasoning tasks   can be improved by incorporating additional train-   ing on data - rich similar tasks ( Wang et al . , 2021 ) .   3.5 Composition Tests   Datasets and Metrics . Composition is similar to   the text generation task , aiming at generating new   content from scratch . Therefore , we use four text   generation benchmarks for composition tests , i.e. ,   WritingPrompts ( Fan et al . , 2018 ) on story genera-   tion , CNN / Daily Mail ( Hermann et al . , 2015 ) and   GigaWord ( Rush et al . , 2015 ) on text summariza-   tion , and SQuAD v1.1 ( Rajpurkar et al . , 2016 ) on   question generation . According to the length of   the target text , text summarization and question   generation is short text generation , while story gen-   eration is long text generation . For evaluation , we   adopt three automatic metrics , i.e. ,BLEU ( Pap-   ineni et al . , 2002 ) , ROUGE ( Lin , 2004 ) , and ME-   TEOR ( Banerjee and Lavie , 2005 ) . Besides , follow-   ing ( Zou et al . , 2021 ) , we conduct human test from   five aspects , i.e. , Fluency , Informativeness , Accu-   racy , Relevance andOverall . The overall score is   rated from 1 to 10 , while the others are rated from   1 to 5 . Inspired by Turing ( 2009 ) , we further de-3525   sign a Turing test to assess the generation ability of   PLMs , where a human interrogator is requested to   distinguish whether the given text is generated by   a human . From the generated texts of each model   and gold texts , we randomly select 500 texts scored   by judges . More details of human test and Turing   test are shown in Appendix F.   Results and Analysis . Table 5 and Table 6 present   the automatic evaluation and human evaluation re-   sults on composition ability , respectively . We can   observe that ProphetNet and BART achieve great   performance on short text generation , while GPT-2   and T5 show better results on long text generation .   Specifically , BART employs denoising objectives   for reconstructing the corrupted original text , and   ProphetNet adopts future n - gram prediction , which   is flexible for modeling the semantic relations be-   tween tokens and phrases in short texts . However ,   in long texts , a small ratio of masked tokens ( i.e. ,   15 % ) might be not effective in capturing the com-   plex long - range dependency . By comparison , the   left - to - right prediction objective in GPT-2 can be   more suitable to model the long - range semantic   continuity in long texts , and T5 has the largest   model size to achieve a strong composition abil-   ity . For composition ability , we conclude that the   denoising objective is helpful for short text com-   position , while the left - to - right objective is more   powerful for long text composition . Besides , the   model size is also an important factor in improving   PLMs ’ composition ability .   To further investigate what factors affect the pass   rate of the Turing test , we deeply analyze the in-   termediate scoring results in the human test and   Turing test . As shown in Figure 5 , we calculate   the pass rate of the Turing test for each human   test metric across 1 to 5 scale . Moreover , we com-   pute the Pearson correlation coefficient between   the pass rate and each metric . In story genera - tion ( WritingPrompts ) , the coefficients for Fluency ,   Informativeness , and Relevance are 96.63 , 97.93 ,   96.44 , respectively . While , in text summarization   ( GigaWord ) , the coefficients for Fluency , Informa-   tiveness , and Accuracy are 96.08 , 97.67 , 98.38 ,   respectively . From these analysis results , we can   conclude that Informativeness is more important   for story generation , while Accuracy is more influ-   ential in text summarization . Besides , we compute   the text similarity between the generated texts from   different PLMs , which is shown in Appendix F.   4 Discussion   Based on the above four ability tests , we intend to   provide a guideline for helping researchers choose ,   apply , interpret and design PLMs for NLP tasks .   In section 3.3 , we observe that the improvement   in memory ability is likely to be helpful for the   performance of comprehension ability . Hence , de-   signing PLMs with special objectives like bidirec-   tional language modeling in BERT and strategies   like larger training batches in RoBERTa for larger   memory capacity will further benefit PLMs in the   downstream comprehension tasks . Besides , when   applying PLMs to downstream tasks , the similarity   of data distribution between pretraining and fine-   tuning has a great impact on PLMs performance .   Possible solutions such as introducing intermedi-   ate tasks or datasets can alleviate such a discrep-   ancy . Moreover , we further find some limitations   in PLMs ’ comprehension ability , where PLMs are   good at simple single - token answer types in QA   such as dates but perform worse in complex phrase   answers .   Compared to comprehension , reasoning in sec-   tion 3.4 is much more intricate and usually in-   volves inferring the semantic relationships among   multiple sentences . Therefore , PLMs such as AL-   BERT trained with sentence - level objectives can   be more suitable for conducting reasoning tasks .   Intuitively , incorporating sentence - level objectives   during pretraining will help PLMs learn the corre-   lation among different sentences . Note that PLMs   have better reasoning transferability between sim-   ilar tasks , thus data - scarce reasoning tasks can be   improved by first training on data - rich tasks .   For composition ability , PLMs with denoising   training objectives perform much better on short   text composition , while PLMs with left - to - right   objectives or larger model size are more suitable   for long text composition . This might be because3526PLMs with different training objectives can finally   capture different ranges of semantic dependency   between tokens and phrases . Moreover , to obtain a   higher pass rate of Turing test , different text gener-   ation tasks will be concerned with varying factors ,   such as informativeness is much more critical for   story generation .   5 Related Work   Pretrained Language Models . Owing to the   great achievements Transformer ( Vaswani et al . ,   2017 ) has made , the paradigm of pretrained lan-   guage models ( PLMs ) is thriving ( Radford et al . ,   2019 ; Devlin et al . , 2019 ; Liu et al . , 2019b ; Lewis   et al . , 2020 ; Raffel et al . , 2020 ) . It is widely rec-   ognized that PLMs can learn massive knowledge   from corpora ( Li et al . , 2021c ) , leading to signifi-   ca nt progress in various language tasks ( Li et al . ,   2021a , b ) . With such encouraging results in exten-   sive NLP tasks , it is a non - trivial topic to system-   atically evaluate the abilities of PLMs , which can   further deepen our understanding of PLMs and fa-   cilitate their application to more fields .   Language Model Evaluation . Many efforts have   studied the evaluation of language model perfor-   mance . Liu et al . ( 2019a ) evaluate BERT ( De-   vlin et al . , 2019 ) , GPT ( Radford et al . , 2018 ) , and   ELMo ( Peters et al . , 2018 ) on a variety of linguis-   tics tasks . Their findings indicate that the features   generated by PLMs are sufficient for good perfor-   mance on a board set of tasks but fall short on tasks   requiring fine - grained linguistic knowledge . Ten-   ney et al . ( 2019 ) evaluate similar models on a range   of sub - sentence linguistic analysis tasks , showing   that PLMs encode both syntax and semantics into   parameters . Zhou et al . ( 2020 ) also report that   PLMs can learn rich knowledge but focus on eval-   uating the commonsense . However , these studies   only look at one dimension of PLMs ability evalua-   tion . Other work such GLUE ( Wang et al . , 2019b )   and CLUE ( Liang Xu , 2020 ) just consider a simple   mixture of multiple tasks lacking comprehensive   evaluation . To the best of our knowledge , this is   the first work to systematically evaluate PLMs by   defining various kinds of language abilities and   performing extensive comparison .   6 Conclusion   This paper investigates the general language abil-   ity evaluation of pretrained language models . Wedesign four kinds of language abilities of PLMs ,   including memory , comprehension , reasoning , and   composition , and measure ten widely - used PLMs   within five categories . For each language ability ,   we select multiple representative tasks to quanti-   tatively evaluate the performance of PLMs . Our   experimental results demonstrate that PLMs with   varying objectives and strategies are good at dif-   ferent ability tests . Note that our final predicted   outputs of PLMs can also be reused as an open re-   source for more depth and granularity in analyzing   PLMs ’ language abilities . As a result , it is believed   that this study will benefit future work about choos-   ing or designing suitable PLMs for the target NLP   tasks based on their properties .   Acknowledgement   This work was partially supported by Beijing Natu-   ral Science Foundation under Grant No . 4222027 ,   National Natural Science Foundation of China un-   der Grant No . 61872369 and 82161148011 , Bei-   jing Outstanding Young Scientist Program under   Grant No . BJJWZYJH012019100020098 , and the   Outstanding Innovative Talents Cultivation Funded   Programs 2021 of Renmin University of China . We   are grateful to Amazon Web Services for providing   efficient GPU computing resource support and tech-   nical support for this NLP research project . Xin   Zhao is the corresponding author .   References3527352835293530   We give some experiment - related information   as supplementary materials . The appendix is orga-   nized into six sections :   •Configurations and pretraining setting com-   parisons for selected models are presented in   Appendix A ;   •Data statistics of each test are presented in   Appendix B ;   •Full results for memory tests are presented in   Appendix C ;   •Full results for comprehension tests are pre-   sented in Appendix D ;   •Full results for reasoning tests are presented   in Appendix E ; and   •Full results for composition tests are presented   in Appendix F.   AConfigurations of Pretrained Language   Models   The selected ten PLMs within five categories and   the comparisons of these PLMs in configuration   and pretraining setting have been shown in Table 7 .   The effect extent of each factor for PLMs abilities   in Table 8 .   B Data Statistics   Memory Tests . The data statistics of LAMA and   Wikipedia of each model are presented in Table 9 .   Due to the differences of each PLM , we drop the   data that are not in the vocabulary .   Comprehension Tests . The data statistics of   GLUE , SuperGLUE , SQuAD and RACE are pre-   sented in Table 10 .   Reasoning Tests . The data statistics for common-   sense reasoning , deductive reasoning and abductive   reasoning are presented in Table 11 .   Composition Tests . The data statistics for text   summarization , question generation and story gen-   eration are presented in Table 12 . For the first three   datasets , we truncate the source text considering   the input length of PLMs during training . And   for WritingPrompts , we reconstruct the originaldataset and discard examples where text contains   more than 512 tokens .   C Memory Tests   Full results on LAMA and Wikipedia datasets are   presented in Table 13 .   D Comprehension Tests   Full results on SuperGLUE , SQuAD and RACE   are presented in Table 14 and Table 15 .   E Reasoning Tests   Full results on CommonsenseQA , ROCStories ,   SWAG , HellaSwag , Sense Making , and ARCT are   presented in Table 16 .   F Composition Tests   For automatic metrics , BLEU- nand ROUGE- n   compute the ratios of overlapping n - grams between   generated and real text , while METEOR measures   word - to - word matches based on WordNet between   generated and real text . For the human test , Flu-   ency evaluates whether the text is well - formed and   logical to read ; Informativeness measures whether   the text contains useful information ; Accuracy tests   whether the text describes the given content ac-   curately ; Relevance measures whether the text is   relevant to the given context ; Overall evaluates the   overall quality of the text .   In the human test , we ramdomly select 500 gen-   erated texts for each PLM and 500 gold text . There-   fore , there are 3000 texts totally . The judges are   all PhD students which do not know about where   each text comes from . Each text will be scored by   two judges from the above five aspects , and the   final score is the average of the two scores . In the   Turing test , each text will also be distinguished by   two judges . Only when two judges make the same   decisions that the text is generated by human , we   will consider the text is true .   Full results on CNN / Daily - Mail , GigaWord ,   SQuAD , and WritingPrompts are presented in Ta-   ble 17 . Turing test results are presented in Table 6 .   We also show some summaries and stories gener-   ated by different PLMs in Table 19 , Table 20 , and   Table 21.3531   G - RE T - REx ConceptNet SQuAD Wikipedia   # Origin 6,106 34,014 14,878 305 100,000   # Relation 3 41 16 - -   BERT / UniLM 5,527 34,014 11,658 305 85,836   RoBERTa 4,618 29,500 12,505 286 85,862   ALBERT 5,469 33,636 12,389 291 86,533   ERNIE 1,900 9,071 11,649 173 -   BART 4,618 29,500 12,505 286 85,862   T5 4,256 25,850 10,905 230 78,069   GPT-2 4,618 29,500 7,477 196 1,184   XLNet 5,202 32,293 12,080 279 85,228   ProphetNet 5,527 34,014 12,506 305 87,516   The Predicted Outputs The predicted token of “ [ MASK ] ” in each template.353235333534Model WSC CB RTE COPA Wic BoolQ MultiRC Avg   Acc . F1 / Acc . Acc . Acc . Acc . Acc . F1 / EM   BERT 60.6 78.7/80.4 66.4 65.0 69.9 74.6 68.1/16.9 65.5   BERT 63.5 89.0/92.9 70.1 73.0 72.7 75.6 69.4/22.6 70.3   RoBERTa 71.1 89.1/91.1 75.1 78.0 67.2 81.1 72.6/31.9 73.6   RoBERTa 75.0 95.0/96.4 88.2 84.0 72.7 85.4 81.7/47.2 80.8   ALBERT 63.5 81.1/85.7 62.5 75.0 66.5 62.2 63.6/12.4 64.4   ALBERT 64.4 87.6/92.9 70.4 91.0 74.3 62.2 85.1/54.0 74.6   GPT-2 54.8 64.0/76.8 62.1 62.0 64.1 68.2 67.3/19.5 60.7   GPT-2 61.5 84.4/82.1 63.6 63.0 67.2 73.9 71.5/29.2 66.1   XLNet 64.4 91.0/91.1 59.9 65.0 67.9 76.9 72.5/29.6 68.0   XLNet 65.3 87.6/92.9 88.5 82.0 69.7 84.7 79.0/41.6 77.3   UniLM 63.5 74.7/82.1 60.3 67.0 68.5 73.3 67.9/20.5 65.0   UniLM 65.4 86.5/87.5 70.9 76.0 72.3 82.3 75.7/36.3 72.8   ERNIE 65.4 81.6/82.1 68.8 64.0 70.8 74.4 68.7/21.3 67.2   T5 79.8 86.2/94.0 80.1 71.2 68.3 81.4 79.7/43.1 76.0   T5 84.6 91.6/94.8 87.2 83.4 69.3 85.4 83.3/50.7 81.4   BART 64.4 86.6/85.7 69.5 70.0 65.7 75.7 74.2/31.7 69.2   BART 65.4 97.4/96.4 83.5 86.0 70.4 85.1 82.9/50.6 79.2   ProphetNet 63.5 94.7 /92.9 51.3 61.0 60.7 67.4 64.7/17.2 62.7   ModelsSQuAD v1.1 SQuAD v2.0 RACE   EM F1 EM F1 RACE RACE - M RACE - H   BERT 80.8 88.5 72.8 76.0 65.0 71.7 62.3   BERT 84.1 90.9 78.7 81.9 72.0 76.6 70.1   RoBERTa 86.1 92.3 80.3 83.4 72.8 72.6 26.6   RoBERTa 88.9 94.6 86.5 89.4 83.2 86.5 81.3   ALBERT 86.1 92.5 83.1 86.1 78.1 76.7 79.8   ALBERT 88.3 94.1 85.1 88.1 87.4 85.9 87.1   GPT-2 63.6 75.1 57.1 61.5 61.2 62.9 58.2   GPT-2 70.3 80.8 61.5 66.0 62.2 65.0 61.4   XLNet 12.8 14.7 78.5 81.3 71.3 72.8 67.5   XLNet 89.7 95.1 87.9 90.6 85.4 88.6 84.0   UniLM 82.8 89.9 74.9 78.0 59.0 64.1 50.3   UniLM 86.5 92.7 80.5 83.4 70.3 70.0 66.4   ERNIE - - - - - 67.8 -   T5 85.4 92.1 77.6 81.3 70.6 74.4 68.4   T5 86.7 93.8 - - 80.4 82.6 77.8   BART 84.6 91.0 76.0 79.2 70.1 72.4 63.2   BART 88.8 94.6 86.1 89.2 82.2 82.5 79.6   ProphetNet - - - - - 74.1 -3535Model CQA ROCStories SWAG HellaSwag SM - A SM - B ARCT   BERT 53.0 88.1 81.6 40.5 87.3 80.1 65.1   BERT 55.9 90.2 86.3 47.3 89.4 85.8 71.2   RoBERTa 72.1 93.3 82.6 61.0 89.3 87.5 46.1   RoBERTa 72.2 97.4 89.9 85.2 93.0 92.3 57.9   ALBERT 66.2 90.4 84.6 75.9 87.9 89.4 56.1   ALBERT 80.0 97.1 90.7 90.1 92.5 92.3 79.5   GPT-2 47.8 58.8 48.1 39.9 84.2 74.7 66.0   GPT-2 60.8 59.9 79.7 60.4 88.7 73.4 66.7   XLNet 53.8 92.0 80.4 55.1 81.6 85.4 80.2   XLNet 62.9 93.8 86.8 79.7 83.7 88.7 83.1   UniLM 47.6 80.6 77.0 36.3 86.2 83.6 48.4   UniLM 62.3 86.9 83.1 46.7 89.3 86.4 72.3   ERNIE 54.1 84.7 - - 88.7 - 73.7   T5 61.9 88.2 65.8 55.2 89.2 82.9 63.3   T5 69.8 91.4 73.7 79.1 92.7 88.2 69.4   BART 61.0 88.9 81.2 53.4 72.0 67.9 71.8   BART 75.8 91.7 87.9 76.6 82.9 67.9 84.2   ProphetNet 21.3 82.2 70.1 26.4 85.5 78.0 65.5   Models TT ( % ) Fluency Informativeness Accuracy Coherence Overall   GPT-2 45.7 3.42 3.17 3.20 3.23 5.87   UniLM 1.2 1.32 1.88 2.03 1.71 2.74   T5 34.4 3.01 2.80 3.09 2.87 5.18   BART 45.2 3.37 3.16 3.39 3.22 5.96   ProphetNet 29.6 2.95 2.91 3.10 2.89 5.18   Gold 71.3 3.79 4.07 3.87 3.80 7.373536353735383539