  Xinglin Lyu , Junhui Li , Shimin Tao , Hao Yang , Ying Qin , Min ZhangSchool of Computer Science and Technology , Soochow University , Suzhou , ChinaHuawei Translation Services Center , Beijing , China   xllv2020@stu.suda.edu.cn   { lijunhui,minzhang}@suda.edu.cn   { taoshimin,yanghao30,qinying}@huawei.com   Abstract   In this paper we aim to relieve the issue of   lexical translation inconsistency for document-   level neural machine translation ( NMT ) by   modeling consistency preference for lexical   chains which consist of repeated words in a   source - side document and provide a represen-   tation of the lexical consistency structure of   the document . Specifically , we first propose   lexical - consistency attention to capture consis-   tency context among words in the same lex-   ical chains . Then for each lexical chain we   define and learn a consistency - tailored latent   variable , which will guide the translation of   corresponding sentences to enhance lexical   translation consistency . Experimental results   on Chinese →English and French →English   document - level translation tasks show that our   approach not only significantly improves trans-   lation performance in BLEU , but also substan-   tially alleviates the problem of the lexical trans-   lation inconsistency .   1 Introduction   Generally , the translations of source - side words   repeated within a document tend to be consistent   ( Merkel , 1996 ; Carpuat , 2009 ; Türe et al . , 2012 ;   Guillou , 2013 ; Al Khotaba and Al Tarawneh , 2015 ;   Lyu et al . , 2021 ; Kang et al . , 2021 ) while sentence-   level neural machine translation ( NMT ) suffers   from the serious problem of lexical translation in-   consistency due to the lack of inter - sentence con-   text . Although most recent studies in document-   level NMT propose various context - aware models   to better capture inter - sentence context , they do   not handle specific discourse phenomena , e.g. , lex-   ical consistency . In this paper , we therefore study   lexical consistency for document - level NMT by   modeling consistency preference for lexical chains   which represent the lexical consistency structure of   a text . Lexical translation consistency is a common   discourse phenomenon . Many studies in statis-   tical machine translation ( SMT ) ( Merkel , 1996 ;   Carpuat , 2009 ; Türe et al . , 2012 ; Guillou , 2013 ;   Al Khotaba and Al Tarawneh , 2015 ) discuss and   apply the one translation per discourse hypothe-   sis . More recently , Lyu et al . ( 2021 ) give a de-   tailed analysis about the lexical translation con-   sistency on Chinese →English translation task . In   their analysis , the proportion of words related with   this phenomenon reaches about 20 percent against   all of words in the whole corpus . Furthermore , they   find out that the translations of the words repeated   within a document indeed tend to be consistent .   Intuitively , enhancing lexical translation consis-   tency should consider inter - sentence context . How-   ever , existing researches in document - level NMT   mainly focus on capturing inter - sentence context   in general and do not explicitly handle specific   discourse phenomena . As a result , these models   have limited effect on enhancing lexical transla-   tion consistency . Different from them , both Lyu   et al . ( 2021 ) and Kang et al . ( 2021 ) recently in-   troduce auxiliary consistency losses to encourage   the translations of the repeated source - side words   being same .   Alternatively , in this paper we propose a softer   approach to model consistency preference for lexi-   cal chains in document - level NMT , where the lex-   ical chains consist of repeated words in a source-   side document . Specifically , we enhance the trans-   lation consistency of lexical chains from two as-   pects : 1 ) we propose a lexical - consistency attention   to capture consistency context among words in the   same lexical chains while we also use a general-   context attention to capture general inter - sentence   context ; and 2 ) we propose a consistency - tailored   latent variable for each lexical chain to model its6312consistency preference , which will guide the trans-   lation of the lexical chain in decoding . These latent   variables are properly learned via a conditional vari-   ational autoencoder ( CV AE ) module , which does   not explicitly constrain the translations of repeated   source words to be same , and thus could ease   over - correction . Experimental results on Chinese-   to - English and French - to - English document - level   translation tasks show that our approach not only   significantly improves the translation performance   in BLEU , but also greatly alleviates the problem of   lexical translation inconsistency .   2 Related Work   There have been substantial studies in document-   level NMT that focus on effective utilization of   general inter - sentence context . These studies can   be roughly categorized into three groups , includ-   ing those who only consider the source - side inter-   sentence context ( Jean et al . , 2017 ; Wang et al . ,   2017 ; V oita et al . , 2018 ; Zhang et al . , 2018 ; Tan   et al . , 2019 ; Yang et al . , 2019 ; Mace and Servan ,   2019 ; Kang et al . , 2020 ; Xu et al . , 2021 ; Fernan-   des et al . , 2021 ) , those who only consider the   target - side inter - sentence context ( or translation   history ) ( Kuang et al . , 2018 ; Tu et al . , 2018 ; Xiong   et al . , 2019 ) , and those who consider both the   source and target inter - sentence context ( Bawden   et al . , 2018 ; Maruf and Haffari , 2018 ; Maruf et al . ,   2019 ; Zheng et al . , 2020 ; Bao et al . , 2021 ; Sun   et al . , 2022 ) . Different from ours , these studies   leverage the inter - sentence context in a indistin-   guishable way and do not handle specific discourse   phenomena . Therefore , although they achieve im-   pressive improvement of translation accuracy with   the expectation of alleviating the discourse issues   in general , they usually have limited effect on en-   hancing lexical translation consistency .   There also exist many researches in MT that aim   to enforce or encourage lexical translation consis-   tency . In SMT , Carpuat ( 2009 ) , Xiao et al . ( 2011 )   and Garcia et al . ( 2014 , 2017 ) propose post - editing   approaches to enforce lexical translation consis-   tency by re - translating those repeated source words   which have been translated differently . Tiedemann   ( 2010a , b ) and Gong et al . ( 2011 ) propose cache-   based approaches to encourage translation consis-   tency . Ma et al . ( 2011 ) and He et al . ( 2011 ) propose   discriminative approaches to improve the consis-   tency of translations . Türe et al . ( 2012 ) add three   super - sentential “ consistency features ” to the trans - lation model . Beside , Pu et al . ( 2017 ) propose   to improve the translation consistency of repeated   nouns in post - editing or / and reranking . In NMT ,   Lyu et al . ( 2021 ) and Kang et al . ( 2021 ) propose   consistency losses which encourage the transla-   tions of repeated words being same . Different from   theirs , our approach automatically models the trans-   lation consistency preference of repeated words via   a CV AE module in a softer way without explicitly   constraining their translations to be same .   3 Proposed Approach   As our goal is to model consistency preference for   lexical chains , we first construct lexical chains of   repeated words in the source - side document . Each   lexical chain represents a repeated word that ap-   pear two or more times in the document . Then we   encode source - side documents with prepared lexi-   cal chains by a consistency - aware encoder ( Sec-   tion 3.1 ) . Meanwhile , we propose to learn a   consistency - tailored latent variable for each lex-   ical chain ( Section 3.2 ) by a CV AE - based module .   These learned latent variables are dynamically inte-   grated into each decoding step to further enhance   translation consistency ( Section3.3 ) . Finally , we   define a joint training objective to optimize the   model with the CV AE - based module ( Section 3.4 ) .   We define some notations before describing   our approach . Given a parallel document pairwithsentence pairs , we   assume that each source sentenceconsists of   nwordswhile its target sentence   consists of mwords . From source   document , we extract a setwith   Mlexical chains . Specifically , each lexical chainrecords all positions of a word   repeatedtimes in document , whereand   indicate the sentence index and word index of a   position , respectively . As shown in the bottom of   Figure 1 , lexical chain indicates that , , , , andare a   repeated word . See Appendix A for more about the   construction and statistics of the lexical chains . We   useas the model size of embeddings and hidden   states , andas the size of consistency - tailored   latent variable .   3.1 Consistency - aware Encoding   We propose a consistency - aware encoder to en-   code source documents . As shown in Figure 1 ,   different from standard Transformer encoder , the6313   consistency - aware encoder equips with two addi-   tional attention sublayers , Lexical - Consistency at-   tention and General - Context attention between the   self - attention sublayer and the feed - forward sub-   layer . The two sublayers aim to capture consis-   tency context and general inter - sentence context   from source document , respectively .   Modeling General Inter - Sentence Context . As   related studies show that modeling general inter-   sentence context is helpful to improve translation   performance , we propose General - Context Atten-   tionsublayer to properly modeling the general inter-   sentence context .   Specifically , we follow BERT ( Devlin et al . ,   2019 ) and add a special token [ cls ] at the begin-   ning of each sentence , as shown in the bottom of   Figure 1 . In the l - th encoder layer , we encode   sentenceswithin documentwith a multi-   head attention function ( Self - Attention in Figure 1 )   synchronously :   whereis the layer normalization function ( Ba   et al . , 2016),is the input sequenceof the encoder layer , andis the out-   put sequence of the self - attention sublayer . Then   we feedto the General - Context attention sub-   layer to exchange information among sentences   with documentvia those [ cls ] tokens :   whereis indexed fromfor [ cls ]   tokens.is further broadcast into shapeand added to :   whereis the output of the General-   Context attention sublayer .   Modeling Consistency Context via Lexical   Chains . Givenlexical chainsin , we   first index their states intofrom .   Inspired by Lyu et al . ( 2021 ) , then we employ an-   other multi - head attention ( Lexical - Consistency At-   tention in Figure 1 ) to exchange information among   tokens within a lexical chain :   whereis further used to replace   their corresponding states in(i.e . , ) . We   refer it asafter the replacement .   is fed into the feed - forward sublayer to get the final   output of l - th encoder layer :   The output of the final encoder layer , i.e,(here-   afterfor simplicity ) , will be used as the encoder   output of the document . Specifically , we useto   indicates the hidden state for , i.e. , the j - th word   in the i - th sentence .   3.2 Modeling Consistency Preference via   Latent Variational Module   As shown in Figure 2 , we propose a latent varia-   tional module to learn the distribution of consis-   tency preference for every lexical chain . Given a   lexical chain , next we describe how   to learn its consistency - tailored latent vairable in   training and inference , respectively .   Learning Consistency - Tailored Latent Variable   in Training . For lexical chain ,   we first encode the chain and extract its poten-   tial translation . Then we produce a consistency-   tailored latent variable.6314   The hidden states of the chain , can   be extracted from the encoder outputas :   g=/parenleftig   g , · · · , g / parenrightig   . ( 6 )   To obtain the potential translation of the chain   from the target - side document , we first use the   decoder to synchronously get the hidden states   of all target - side sentences :   whereis the output of the last decoder   layer . Specifically , we denote the target - side hid-   den states of the i - th sentenceas . As there   is no explicit word - level alignment between the   source - side and target - side documents , we could   not directly obtain the chain ’s translation . Alterna-   tively , we then employ an attention mechanism to   implicitly learn its translation . For the k - th word in   the chainwith its source - side hidden states ,   we obtain its target - side counterpartas   weighted sum of the target - side hidden states :   whereis a trainable parameter matrix .   Consequently , we obtain   , as the   target - side hidden states of the chain .   Once we obtain both the hidden statesof the   chainand its target - side counterpart , we followWang and Wan ( 2019 ) and use eisotropic Gaussian   distribution as the posterior distribution to sample   the latent variable :   wheredenotes the identify matrix , andare   learned via neural networks :   whereandare multi - layer per-   ceptron and approximation of ReLU function , re-   spectively.is MaxPooling function that   converts chain - level hidden states(or ) into a - sized vector.is concatenation operation .   Learning Consistency - Tailored Latent Variable   in Inference . In inference , the target - side hidden   states of the chain , i.e. , , is unavailable due to   the unobservability of the ground - truth translation .   Therefore , we sample the consistency - tailored la-   tent variablefor the chainfrom a prior dis-   tribution which is conditioned only on the source-   side hidden states of the chain , i.e. , :   Similarly , we employ another neural network to   learn the prior distribution :   The prior distribution is properly trained by ap-   proaching the posterior distribution via a KL diver-   gence loss during the training stage :   which enables the prior network learn the reliable   consistency - tailored distribution even though theis unobservable .   3.3 Consistency - aware Decoding   After obtaining a consistency variables seteither from the posterior distribution ( at   training stage ) or the prior distribution ( at inference   stage ) for the lexical chains set , we use the these6315   consistency variables to enhance the translation   consistency at each decoding step .   Dynamic Query over Consistency Variables . Fig-   ure 3 illustrates how to use these consistency-   tailored latent variables in decoding for the i - th   sentence . On the one hand , we note that the j - th   variable zis relevant to the i - th sentence only if   thej - th lexical chain contains words that are from   sentence . So we define functionwhich   returns 1 if the j - th chain contains at least one word   from , otherwise 0 . On the other hand , even z   is relevant , it concerns to very few decoding steps .   So we define a similarity function to compare the   decoder hidden states against the latent variable .   Overall , given the decoder output hat the t - th de-   coding step of sentence , we perform dynamic   query over latent variablesand obtain con-   sistency feature ¯z :   whereis a trainable parameter matrix ,   and sim ( · , · ) is the cosine similarity function .   Fusion of Consistency Feature . We fuse the con-   sistency feature ¯zof the t - th decoding step with   the decoder output hin the output layer as o :   whereandare trainable   parameters . Finally , ois fed to a linear transfor - mation and a Softmax layer to get the probability   distribution of y , i.e. , :   where we use target - side word embedding parame-   ters as , andis vocabulary size .   3.4 Joint Training Objective   We have introduced a KL - divergence loss to learn   the consistency variable in Section 3.2 . The joint   objective function of our modelover docu-   ment pair ( ) is defined as :   wheredetermines the contributions of KL-   divergence loss , andis the cross   entropy loss function .   4 Experimentation   As inspired by the conclusion in Guillou ( 2013 ) and   Lyu et al . ( 2021 ) that lexical translation consistency   is encouraged in Chinese ( ZH ) →English ( EN )   and French ( FR ) →English ( EN ) human transla-   tion , we evaluate our approach on { ZH , FR } →EN   document - level translation tasks .   4.1 Experimental Settings   Datasets . For ZH →EN ( News ) , we follow Zhang   et al . ( 2018 ) and use document parallel corpora   from LDC as the training set , NIST2006 dataset as   the development set and combination of NIST2002 ,   2003 , 2004 , 2005 and 2008 as the test set . For   ZH→EN ( TED ) , the training set is from the IWSLT   2014 and 2015 evaluation ( Cettolo et al . , 2012 ,   2015 ) . We use dev2010 as the development and   combine tst2010 - 2013 as the test set . For FR →EN   ( TED ) , the training set is from the 2015 evaluation   ( Cettolo et al . , 2015 ) . We use dev2010 as the devel-   opment and combine tst2010 - 2013 as the test set .   More statistics and preprocessing of the experimen-   tal datasets are in Appendix A.   Model details . We use OpenNMT ( Klein et al . ,   2017 ) as the sentence - level Transformer and ex-   tend it . For the dimension of latent variables , we   setd= 96 . See Appendix B for more details of   implementation of model.6316   Training and inferring strategy . To train the mod-   els more effectively , we follow Lyu et al . ( 2021 )   and adapt a two - stage training strategy . In the first   training stage , we use the sentence pairs to pretrain   the sentence - level modules with the training ob-   jectivewhile in the second training stage   we train all modules with the joint training objec-   tive . To alleviate the degeneration problem of   the variational framework , we follow Liang et al .   ( 2021 ) and apply KL annealing . Other training set-   tings are in Appendix B. In inferring , we set the   beam size to 5 and the length penalty to 0.6 .   Evaluation metrics . We report both sentence - level   metric ( s - BLEU ) and document - level metric ( d-   BLEU ) to evaluate the quality of the translation .   For all translation tasks , we report case - insensitive   BLEU score calculated by multi-bleu.perl script .   To evaluate the ability of enhancing the lexical   translation consistency , we follow Lyu et al . ( 2021 )   and report the LTCR ( Lexical Translation Consis-   tency Ratio ) score . We also report the Herfindahl   Hirschman Index ( HHI ) score in Appendix D.4.2 Experimental Results   Besides sentence - to - sentence ( Sent2Sent ) Trans-   former baseline , we compare our performance   to three representative context - aware models :   HAN ( Miculicich et al . , 2018 ) which models   document - level context for better translating   source sentences , G - Transformer ( Bao et al . ,   2021 ) which directly views source documents as   long sequences and perform seq2seq translation   with a long sequence - tailored Transformer , and   W - Link ( Lyu et al . , 2021 ) which encourages lexi-   cal translation consistency via word links . Among   them , HANandW - Link are document - to - sentence   ( Doc2Sent ) models while G - Transformer is a   document - to - document ( Doc2Doc ) model . For fair   comparison , we run their source code or our re-   implementation with our experimental settings .   Results on ZH →EN ( News ) . Table 1 shows   the performance on the test set of ZH →EN   ( News ) . From it , we first observe that using lexical-   consistency attention alone ( + LC - Attn ) to capture   consistency - context via lexical chain significantly   improves translation performance in both BLEU   ( from 40.55 to 42.39 in s - BLEU ) and LTCR ( from631756.87 to 64.56 ) . It is also not surprised to observe   that although incorporating the latent variational   module ( + LC - Attn + ConVar ) slightly improves   BLEU score ( e.g. , from 42.39 to 42.54 in s - BLEU ) ,   it significantly boosts the LTCR score from 61.31 to   64.09 . This suggests that the proposed consistency-   tailored latent variable is effective in modeling con-   sistency preference for document translation . In   contrast , furthering modeling general document-   level context via the General - Context Attention   ( + LC - Attn + ConVar + GC - Attn ) has limited effect   in LTCR performance while it further improves   translation performance from 42.59 to 43.27 in s-   BLEU . This is reasonable since modeling general   inter - sentence context does not aim to resolve a   particular discourse phenomenon . Similar perfor-   mance trend is also observed when using more sen-   tence pairs ( e.g. , 2 M ) for pretraining . Comparing   the performance when using 0.8 M and 2 M sentence   pairs for pretraining , we observe that although there   exists a big performance gap in BLEU , the gap of   LTCR is quite small ( e.g. , 64.56 v.s. 64.72 ) .   Compared to the three previous context - aware   NMT models , our approach achieves the best per-   formance in both BLEU and LTCR scores . Specif-   ically , we note that G - Transformer consistently   achieves higher LTCR scores than HANand baseline   as it views a source document as a long sequence   and obtains its translation sentence by sentence .   Therefore , to some extent it implicitly encourages   lexical translation consistence among target - side   sentences . However , different from Doc2Sent mod-   els , Doc2Doc models could not translate sentences   within a document in a synchronous way .   Results on other translation tasks . Table 2   shows translation performance on the test sets of   the ZH / FR →EN ( TED ) translation tasks . From   it we have similar conclusions as on ZH →EN   ( News ) that lexical - consistency attention con-   tributes improvement on both BLEU and LTCR   while consistency - tailored latent variables and   general - context attention mainly contribute on im-   provement on LTCR and BLEU , respectively .   5 Discussion   Next , we take ZH →EN ( News ) translation as a rep-   resentative to discuss how our proposed approach   improves translation performance . We also provide   more discussion in Appendix D ∼G.   5.1 Discourse Coherence   Lapata and Barzilay ( 2005 ) propose to measure   discourse coherence as the degree of similarity be-   tween sentences in a document . They view the   representation of a sentence as the mean ( centroid )   of the distributed vectors of its words , and the simi-   larity between two sentencesandas the cosine   of their means . Similar to Liang et al . ( 2021 ) , we   use Word2Vecto learn the 100 - sized word vectors   on the English Gigaword . Given target - side docu-   ments , we use the averaged cosine similarity   between the current sentenceand its adjacentas the discourse coherence :   Table 3 ( left ) shows the discourse coherence   scores on ZH →EN ( News ) test set . It reveals that   all context - aware models , including HAN , G - Trans ,   W - Link , and ours have better coherence in docu-   ment translation than the baseline system while our   model achieves the best coherence .   5.2 Discourse Cohesion   Next Sentence Prediction ( NSP ) is proposed as a   pre - training task in BERT ( Devlin et al . , 2019 ) .   Given two sentences ( ) , NSP will compute the   probability ofbeing the next sentence of . We   propose to measure the discourse cohesion by the   mean of NSP probabilities of all adjacent sentence   pairs in the test set .   Specifically , the discourse cohesion on the trans-   lationof test set is calculated as following :   We use the bert - base - uncased model from Hug-   gingface ( Wolf et al . , 2020 ) to compute the proba-   bility of NSP.Table 3 ( right ) shows the discourse6318   cohesion scores of various systems on the test set .   It suggests that the utilization of the inter - sentence   context improves discourse cohesion while our   model achieves the best cohesion .   5.3 Attention in Target - side Information   In Section 3.2 , we employ an attention mechanism   ( Equation 8) to extract the target - side counterpart   for each source token in lexical chains . To verify   whether the attention module could effectively map   the source token in lexical chain to its most - related   translation , we visualize the attention score of a   lexical chain in development set .   As shown in Figure 4 , source token 兼   职 / jian_zhi appears in the 1st , 4th , 5th and 6th   sentences of the source document , and we observe   the attention module consistently assigns higher   attention scores to moon _ andligh _ tokens in their   corresponding target - side translation . The averaged   and summed attention weights of moon _ andligh _   is 0.63 . This confirms our conjecture that we can   rely on the attention module to implicitly learn the   translation of a source token .   5.4 Dimension of Consistency Variables   As shown in Section 5 , we model the consistency   preference by introducing a consistency - tailored   latent variable z∈R. Figure 5 presents the   translation performance of + LC - Attn + ConVar +   GC - Attn system when dranges from 48 to 144 . As   shown , it improves performance when increasing   dfrom 48 to 96 while it no longer improves or   even hurts performance on BLEU score and LTCR   score when dincreases from 96 to higher .   5.5 Interpretability of Consistency Variables   In Section 3.2 , we adapt a dynamic way to obtain   the consistency feature ¯zat each decoding step   ( Equation 17 ) over all consistency variables . To   examine what learned by latent variational mod-   ule ( i.e. , interpretability of consistency variables )   and whether the module could effectively extract   the most - related consistency feature at decoding   steps , we use an example from the development to   visualize the similarity scores against its relevant   consistency variables at each decoding step .   As shown in Figure 6 , the sentence has 6 rele-   vant consistency variables . It shows that at certain   decoding steps , the query module assigns higher   similarity score to their corresponding most - related   consistency variable . For example , at decoding   steps for predicting heilongjiang andecological , it   assigns 0.62 and 0.69 similarity scores to z(黑龙   江 ) and z(生态 ) , respectively , while it assigns very   low scores to other consistency variables . This   suggests that the information inside consistency   variables is closely tied to their corresponding most-   related words , which confirms our conjecture that   we can rely on the query module to implicitly ex-   tract the most relevant consistency feature at each   decoding step.6319   5.6 Human Evaluation   Similar to Lyu et al . ( 2021 ) , we randomly select   500 sentences from the test set and conduct a hu-   man evaluation on them . For each selected source-   side sentence , we assign its translation background   and corresponding two generated translations from   the sentence - level Transformer and our approach   to two human annotators without order . The trans-   lation background consists of its two preceding and   two future source - side sentences and their target-   side references . Following V oita et al . ( 2019 ) and   Lyu et al . ( 2021 ) , the annotators are asked to pick   one of the tree options : ( 1 ) the first translation is   better , ( 2 ) the second translation is better , and ( 3 )   the two translations are equal quality . Both annota-   tors are postgraduate students and not involved in   other parts of the paper .   Table 4 shows the results of human evaluation .   It shows that on average 41 % cases have equal   quality . Among the other cases , the annotators   have an obvious preference for our approach since   it outperforms Transformer in 66 % cases . We also   provide a case study in Appendix G.   6 Conclusion   In this paper , we have proposed a lexical - chain   based approach to alleviate the issue of translation   inconsistency for document - level NMT . We first   use lexical - consistency attention to capture con-   sistency context among words in the same lexical   chains . Then we learn a consistency - tailored latent   variable for each lexical chain to model consistency   preference in translation . Experimental results onChinese →English and French →English document-   level translation tasks show that our approach could   both improve translation performance in BLEU and   enhance lexical translation consistency .   Acknowledgments   The authors would like to thank the anonymous   reviewers for their constructive feedback . This   work was supported by the National Natural Sci-   ence Foundation of China ( Grant No . 62036004 ) .   Limitations   The lexical chains in this work consist of repeated   words in the source - side document . However , some   source words with similar semantics but different   morphology also potentially have same transla-   tions . For example , both 名誉 / ming_yu and声   望 / sheng_wang can be translated into reputation .   Therefore , these lexical chains in this work are lim-   ited in diversity . Introducing more diversity into   lexical chains , e.g. , synonym - based lexical chains   proposed in Xiong et al . ( 2013 ) , will be explored   in our future work . We also note that the compu-   tation of both the LTCR and HHI scores is based   on whether translations of a repeated word are con-   sistent or same . However , it does not take the ref-   erence into account and ignores the correctness of   these translations . Therefore , introducing a more   appropriate metric to evaluate the translations of   repeated words from both consistency and correct-   ness aspects will also be explored in future work.6320References63216322   A Experimental Datasets and   Preprocessing   For ZH →EN(News ) , the training set consists   of 41,341 documents with 0.8 M sentence pairs .   In addition , we use a larger sentence - level   training set with 2 M sentence pairs ( including   the 0.8 M from the above document parallel   training set ) for pre - training to build a strong   baseline . The sentence - level training set consists   of LDC2002E18 , LDC2003E07 , LDC2003E14 ,   news part of LDC2004T08 and the document - level   training set from LDC2002T01 , LDC2004T07 ,   LDC2005T06 , LDC2005T10 , LDC2009T02 ,   LDC2009T15 , LDC2010T03 . The development   ( /test ) set contains 79 ( /509 ) documents with 1,649   ( /5,146 ) sentence pairs .   For ZH →EN(TED ) , the training set consists of   3124 documents and 0.3 M sentence pairs . The   development ( test ) set contains 8 ( /56 ) documents   with 887 ( /5,232 ) sentence pairs .   For FR→EN(TED ) , the training set consists of   3124 documents and 0.2 M sentence pairs . The   development ( test ) set contains 8 ( /46 ) documents   with 887 ( /4,632 ) sentence pairs .   For all translation tasks , the English and French   sentences are tokenized and lowercased by Mose   toolkit ( Koehn et al . , 2007)while the Chinese   sentences are segmented by Jieba . For ZH →EN   ( News ) and ZH / FR →EN ( TED ) , we segment the   source and target sentences into sub - words by a   BPE model with 32 K and 21 K merged operations   ( Sennrich et al . , 2016 ) , respectively .   We split long documents in training datasets into   sub - documents with at most 20 sentences for effi-   cient training . When constructing lexical chains , we use the most - frequency 1,000 source words in   corresponding training set as the stop - word list and   only consider words that are not in the stop - word   list and appear two or more times in a document .   Besides , the construction of the lexical chains is   done before applying BPE operation . When apply-   ing BPE operation , a lexical chain could be split   into multiple chains if the corresponding word is   split into multiple sub - words . For example , the lex-   ical chains of 科学家 and科学家们will be split   into five different lexical chains , i.e. , the chains of   科学_,家 , 科学_,家 _ and们 , since the source   words科学家 and科学家们are segmented into   [ 科学_,家 ] and [ 科学_,家_,们 ] , respectively .   That is to say , the two lexical chains of 科学 _ are   notmerged since they are from different words 科   学家 and科学家们 . Table 5 presents statistics   about the lexical chains . It shows that the propor-   tion of source words , the average length and the   average number of the lexical chains vary across   different translation tasks .   B Model Setting and Training   For all translation models , the hidden size and the   filter size are set to 512 and 2048 , respectively . The   number of heads in multi - head attention is set to   8 . For models on ZH →EN ( News and TED ) , the   numbers of layers in the encoder and the decoder   are set 6 , while for FR →EN ( TED ) , we change the   numbers to 4 . For models on ZH →EN ( News )   under + 2 M Pretrain setting , we set the dropout to   0.1 . For other models , we set the dropout to 0.3 .   In the first training stage , we train the sentence-   level modules for 200 K steps , warm - up steps as   8 K , learning rate as 1.0 while in the second train-   ing stage , we continue to train all modules for 50 K   steps , learning rate 0.5 . The weight of lexcial-   consistency loss αgradually increases from 0 to   1.0 over the first 20 K steps . We train all models   on eight V100 GPUs with batch - size 4096 and use   Adam with β1 = 0.9 , β2 = 0.98 for optimization   ( Kingma and Ba , 2015 ) .   C Model Parameters and Training Speed   As shown in Section 3 , we use General - Context At-   tention , Lexical - Consistency Attention andLatent   Variational Module to model consistency prefer-   ence . Next we analyze the number of parameters   and training speed of these various models .   Table 7 shows the number of parameters and   training speed of these models . In total our ap-6323   proach ( + LC - Attn + ConVar + GC - Attn ) introduces   22.5 % additional parameters and encumbers the   training speed down 50.1 % compared to sentence-   level Transformer . It also shows that though the   latent variational module ( + ConVar ) slightly in-   creases the number of parameters , it significantly   lowers the training speed . This is reasonable   since the computation of post distribution is time-   consuming in training ( as shown in Figure 2 ) .   D Performance on HHI score   Beside LTCR , we also evaluate the lexical transla-   tion consistency by Herfindahl - Hirschman Index   ( HHI ) score ( Melamed , 1997 ; Itagaki et al . , 2007 ;   Guillou , 2013 ) which is commonly accepted as   measurement of market concentration . Specifically ,   for a lexical chainwith size , we assume that the   chain has nvarious translations . Then we compute   HHI score of the chain as following :   whereis the ratio of the i - th translation against   the total number of translations ( i.e. , ) . Finally ,   given a corpus with Nlexical chains , we compute   corpus - level HHI score as :   wherereturns the size of chain .   Table 6 lists HHI scores on test sets of differ-   ent translation tasks . We observe the similar trend   as that of LTCR score while our systems ( + LC-   Attn + ConVar or + LC - Attn + ConVar + GC - Attn )   achieve the best performance in HHI on all transla-   tion tasks .   E Ablation Study   Table 8 shows the performance of ablation study   on ZH→EN ( News ) translation task . This further   confirms our conclusions in Section 4.2 . First , the   general - context attention is effective to improve   translation performance in BLEU while it has lim-   ited effect in LTCR . In contrast , incorporating the6324   latent variables contributes more to the improve-   ment of LTCR while it has negligible effect in   BLEU . Finally , lexical - consistency attention plays   important role in improving both BLEU and LTCR .   F KL divergence   Following Liang et al . ( 2021 ) , we analyze whether   our CV AE module works well for modeling con-   sistency preference . Figure 7 shows that the KL   divergence of consistency latent variables main-   tains around 0.5 during the second training stage ,   which indicates that the degeneration problem of   variational framework does not appear in our model .   The consistency - tailored latent variable learned by   our CV AE module plays its corresponding role .   G Case Study   We use two examples to illustrate how our pro-   posed approach helps translation . As shown in   Figure 8 , we observe that in the first example , the   sentence - level model may confuse readers by trans-   lating source word 书画 / shu_hua into three dif-   ferent translations , i.e. , painting and calligraphy ,   painting , and writing and painting . In contrast , our   approach consistently and correctly translate it into   painting and calligraphy . In the second example ,   the source word 佩里斯 / pei_li_si is consistently   translated into belize by our model while it is trans-   lated into two different translations , i.e. , petris and   belize , by the sentence - level model .   We note that over - corrected cases would be   caused by our model . As shown in Figure 9 , the   source word 信任投票 / xin_ren_tou_piao is repeat-   edly translated into the trust voting by our model   while it is translate into the trust vote andthe vote ofconfidence in both the reference and the sentence-   level translation , respectively.63256326