  Wenchao Gu , Yanlin Wang , Lun Du , Hongyu Zhang ,   Shi Han , Dongmei Zhang , and Michael R. LyuDepartment of Computer Science and Engineering ,   The Chinese University of Hong Kong , China . Microsoft Research Asia , Beijing , ChinaThe University of Newcastle , Australia   Abstract   Code search is to search reusable code snippets   from source code corpus based on natural lan-   guages queries . Deep learning - based methods   on code search have shown promising results .   However , previous methods focus on retrieval   accuracy , but lacked attention to the efÔ¨Åciency   of the retrieval process . We propose a novel   method CoSHC to accelerate code search with   deep hashing and code classiÔ¨Åcation , aiming   to perform efÔ¨Åcient code search without sacri-   Ô¨Åcing too much accuracy . To evaluate the ef-   fectiveness of CoSHC , we apply our method   on Ô¨Åve code search models . Extensive experi-   mental results indicate that compared with pre-   vious code search baselines , CoSHC can save   more than 90 % of retrieval time meanwhile   preserving at least 99 % of retrieval accuracy .   1 Introduction   Code reuse is a common practice during software   development process . It improves programming   productivity as developers ‚Äô time and energy can be   saved by reusing existing code . According to pre-   vious studies ( Brandt et al . , 2009 ; Lv et al . , 2015 ) ,   many developers tend to use natural language to   describe the functionality of desired code snippets   and search the Internet / code corpus for code reuse .   Many code search approaches ( Brandt et al . ,   2009 ; McMillan et al . , 2011 ; Lv et al . , 2015 ; Du   et al . , 2021 ) have been proposed over the years .   With the rapid growth of open source code bases   and the development of deep learning technology ,   recently deep learning based approaches have be-   come popular for tackling the code search prob-   lem ( Gu et al . , 2018 ; Husain et al . , 2019 ; Gu et al . ,   2021 ) . Some of these approaches adopt neural   network models to encode source code and query   descriptions into representation vectors in the sameembedding space . The distance between the repre-   sentation vectors whose original code or descrip-   tion are semantically similar should be small . Other   approaches ( Feng et al . , 2020 ; Guo et al . , 2021 ; Du   et al . , 2021 ) regard the code search task as a binary   classiÔ¨Åcation task , and calculate the probability of   code matching the query .   In the past , deep learning - based methods focused   on retrieval accuracy , but lacked attention to the   efÔ¨Åciency of retrieval on large - scale code corpus .   However , both types of these deep learning - based   approaches directly rank all the source code snip-   pets in the corpus during searching , which will   incur a large amount of computational cost . For   the approaches that separately encode code and de-   scription representation vectors , the similarity of   the target query vector with all code representation   vectors in the corpus needs to be calculated for ev-   ery single retrieval . In order to pursue high retrieval   accuracy , a high dimension is often set for the repre-   sentation vectors . For example , in CodeBERT , the   dimension of the Ô¨Ånal representation vector is 768 .   The similarity calculation between a pair of code   and query vectors will take 768 multiplications and   768 additions between two variables with double   data type . The total calculation of single linear   scan for the whole code corpus containing around   1 million code snippets is extremely large - around   1 billion times of multiplications and additions . As   for the approaches adopting binary classiÔ¨Åcation ,   there is no representation vectors stored in advance   and the inference of the target token sequence with   all the description token sequences needs to be   done in real time for every single retrieval . Due   to the large number of parameters in the current   deep learning models , the computation cost will be   signiÔ¨Åcant .   Hashing is a promising approach to improve the   retrieval efÔ¨Åciency and widely adopted in other   retrieval tasks such as image - text search and image-   image search . Hashing techniques can convert high2534dimensional vectors into low dimensional binary   hash code , which greatly reduce the cost of stor-   age and calculation ( Luo et al . , 2020 ) . Hamming   distance between two binary hash code can also   be calculated in a very efÔ¨Åcient way by running   XOR instruction on the modern computer archi-   tectures ( Wang et al . , 2016 ) . However , the perfor-   mance degradation is still not avoidable during the   conversion from representation vectors to binary   hash codes even the state - of - the - art hashing models   are adopted . The tolerance of performance degra-   dation from most users is quite low and many of   them are willing to sweep the performance with   efÔ¨Åciency . In order to preserve the performance   of the original code search models that adopt bi-   encoders for the code - query encoding as much as   possible , we integrate deep hashing techniques with   code classiÔ¨Åcation , which could mitigate the perfor-   mance degradation of hashing model in the recall   stage by Ô¨Åltering out the irrelevant data .   SpeciÔ¨Åcally , in this paper , we propose a novel   approach CoSHC ( Accelerating Semantic Code   Search with Deep Hashing and Code ClassiÔ¨Åcation )   for accelerating the retrieval efÔ¨Åciency of deep   learning - based code search approaches . CoSHC   Ô¨Årstly clusters the representation vectors into differ-   ent categories . It then generates binary hash codes   for both source code and queries according to the   representation vectors from the original models .   Finally , CoSHC gives the normalized prediction   probability of each category for the given query ,   and then CoSHC will decide the number of code   candidates for the given query in each category   according to the probability . Comprehensive exper-   iments have been conducted to validate the perfor-   mance of the proposed approach . The evaluation   results show that CoSHC can preserve more than   99 % performance of most baseline models . We   summarize the main contributions of this paper as   follows :   ‚Ä¢We propose a novel approach , CoSHC , to im-   prove the retrieval efÔ¨Åciency of previous deep   learning based approaches . CoSHC is the Ô¨Årst   approach that adopts the recall and re - rank mech-   anism with the integration of code clustering and   deep hashing to improve the retrieval efÔ¨Åciency   of deep learning based code search models .   ‚Ä¢We conduct comprehensive experimental evalua-   tion on public benchmarks . The results demon-   strate that CoSHC can greatly improve the re-   trieval efÔ¨Åciency meanwhile preserve almost thesame performance as the baseline models .   2 Background   2.1 Code Search   In this subsection , we brieÔ¨Çy review some deep   learning based code search approaches . Sachdev   et al . ( 2018 ) Ô¨Årstly propose the neural network   based model NCS to retrieve the source code from   a large source code corpus according to the given   natural language descriptions . Cambronero et al .   ( 2019 ) propose a neural network model UNIF   based on bag - of - words , which embeds code snip-   pets and natural language descriptions into a shared   embedding space . Gu et al . ( 2018 ) propose to   encode source code representation with API se-   quences , method name tokens and code tokens .   Yao et al . ( 2019 ) treat code annotation and code   search as dual tasks and utilize the generated code   annotations to improve code search performance .   Husain et al . ( 2019 ) explore different neural ar-   chitectures for source code representation and dis-   cover that the self - attention model achieves the   best performance . Gu et al . ( 2021 ) extract the pro-   gram dependency graph from the source code and   adopt long short term memory ( LSTM ) networks   to model this relationship . Feng et al . ( 2020 ) pro-   pose a pre - trained model for source code represen-   tation and demonstrate its effectiveness on the code   search task .   2.2 Deep Hashing   In this subsection , we brieÔ¨Çy introduce some repre-   sentative unsupervised cross - modal hashing meth-   ods . In order to learn a uniÔ¨Åed hash code , Ding   et al . ( 2014 ) propose to adopt collective matrix   factorization with latent factor model from differ-   ent modalities to merge multiple view information   sources . Zhou et al . ( 2014 ) Ô¨Årstly utilize sparse   coding and matrix factorization to extract the latent   features for images and texts , respectively . Then   the learned latent semantic features are mapped to   a shared space and quantized to the binary hash   codes . Wang et al . ( 2014 ) suggest using stacked   auto - encoders to capture the intra- and inter - modal   semantic relationships of data from heterogeneous   sources . He et al . ( 2017 ) and Zhang et al . ( 2018 )   adopt adversarial learning for cross - modal hash   codes generation . Wu et al . ( 2018 ) propose an ap-   proach named UDCMH that integrates deep learn-   ing and matrix factorization with binary latent fac-   tor models to generate binary hash codes for multi-2535modal data retrieval . By incorporating Laplacian   constraints into the objective function , UDCMH   preserve not only the nearest neighbors but also the   farthest neighbors of data . Unlike using Laplacian   constraints in the loss function , Su et al . ( 2019 )   construct a joint - semantic afÔ¨Ånity matrix that inte-   grates the original neighborhood information from   different modalities to guide the learning of uniÔ¨Åed   binary hash codes .   3 Method   We propose a general framework to accelerate ex-   isting Deep Code Search ( DCS ) models by decou-   pling the search procedure into a recall stage and a   re - rank stage . Our main technical contribution lies   in the recall stage . Figure 1 illustrates the overall   framework of the proposed approach . CoSHC con-   sists of two components , i.e. , OfÔ¨Çine and Online .   In OfÔ¨Çine , we take the code and description embed-   dings learned in the given DCS model as input , and   learn the corresponding hash codes by preserving   the relations between the code and description em-   beddings . In Online , we recall a candidate set of   code snippets according to the Hamming distance   between the query and code , and then we use the   original DCS model to re - rank the candidates .   3.1 OfÔ¨Çine Stage   Multiple Code Hashing Design with Code Clas-   siÔ¨Åcation Module Since the capacity of binary   hashing space is very limited compared to Eu-   clidean space , the Hamming distance between simi-   lar code snippets will be too small to be distinguish-   able if we adopt a single Hashing model . To be   speciÔ¨Åc , we cluster the codebase using K - Means   algorithm with the code embeddings learned from   the given DCS model . The source code whose rep-   resentation vectors are close to each other will be   classiÔ¨Åed into the same category after the cluster-   ing .   Deep Hashing Module The deep hashing module   aims at generating the corresponding binary hash   codes for the embeddings of code and description   from the original DCS model . Figure 2 illustrates   the framework of the deep hashing module . To   be speciÔ¨Åc , three fully - connected ( FC ) layers with   tanh()activation function are adopted to replace   the output layer in the original DCS model to con-   vert the original representation vectors into a soft   binary hash code .   The objective of the deep hashing module is   to force the Hamming distance between hashingrepresentations of code pairs and description pairs   approaching the Euclidean distance between the   corresponding embeddings . Thus , we need to cal-   culate the ground truth similarity matrix between   code pairs and description pairs Ô¨Årstly . For perfor-   mance consideration , we calculate the similarity   matrix within a mini - batch .   To construct such a matrix , we Ô¨Årst deÔ¨Åne   the code representation vectors and the descrip-   tion representation vectors in the original code   search model as V = fv;:::;vgandV=   fv;:::;vg , respectively . VandVrepre-   sent the representation vectors matrix for the en-   tire batch , while vandvrepresent the rep-   resentation vector for the single code snippet or   query . After normalizing V , Vto^V,^Vwith   l - norm , we can calculate the code similarity matri-   cesS=^V^Vand summary similarity matrices   S=^V^Vto describe the similarity among code   representation vectors and summary representation   vectors , respectively . In order to integrate the simi-   larity information in both SandS , we combine   them with a weighted sum :   ~S=  S+ ( 1   ) S ;  2[0;1 ] ( 1 )   where  is the weight parameter . Since the pairwise   similarity among the code representation vectors   and description representation vectors still can not   comprehensively present the distribution condition   of them in the whole embedding space , we involve   a matrix ~S ~ Sto describe a high order neighbor-   hood similarity that two vectors with high similar-   ity should also have the close similarity to other   vectors . Finally , we utilize a weighted equation to   combine both of these two matrices as follows :   S= ( 1 )~S+~S ~ S   m ; ( 2 )   whereis a hyper - parameter and mis the batch   size which is utilized to normalize the second term   in the equation . Since we hope the binary hash   codes of the source code and its corresponding   description to be the same , we replace the diagonal   elements in the similarity matrix with one . The   Ô¨Ånal high order similarity matrix is :   S= (   1 ; i = j   S;otherwise(3 )   Binary Hash Code Training We propose to re-   place the output layer of the original code search2536   model with three FC layers with tanh()activate   function . We deÔ¨Åne the trained binary hash code   for code and description as B = fb;:::;bg   andB = fb;:::;bg , respectively . To ensure   that the relative distribution of binary hash codes   is similar to the distribution of representation vec-   tors in the original embedding space , the following   equation is utilized as the loss function of the deep   hashing module :   L( ) = minkmin(S;1) BB   dk   + kmin(S;1) BB   dk   + kmin(S;1) BB   dk ;   s : t : B;B2f  1;+1g;(4 )   whereare model parameters , is the weighted   parameters to adjust the similarity score between   different pairs of code and description , ,are   the trade - off parameters to weight different terms   in the loss function , and dis the dimension of the   binary hash code generated by this deep hashing   module . These three terms in the loss function are   adopted to restrict the similarity among binary hash   codes of the source codes , the similarity amongbinary hash codes of the descriptions , and the simi-   larity between the binary hash codes of source code   and description , respectively .   Note that we adopt BB = dto replace   cos(B;B)because cos(B;B)only mea-   sures the angle between two vectors but ne-   glects the length of the vectors , which makes   cos(B;B)can still be a very large value even   the value of every hash bits is close to zero . Unlike   cos(B;B),BB = dcan only achieve a high   value when every bit of the binary hash code is 1   or -1 since the value of BB = dwill be close to   zero if the value of every hash bits is close to zero .   Since it is impractical to impose on the output   of neural network to be discrete values like 1 and   -1 , we adopt the following equation to convert the   output of deep hashing module to be strict binary   hash code :   B= sgn(H)2f  1;+1 g ; ( 5 )   whereHis the output of the last hidden layer with-   out the activation function in the deep hashing mod-   ule and sgn()is the sign function and the output   of this function is 1 if the input is positive and the   output is -1 otherwise .   However , the gradient of the sign function will   be zero in backward propagation which will induce2537   the vanishing gradients problem and affect model   convergence . To address this problem , we follow   the previous research ( Cao et al . , 2017 ; Hu et al . ,   2019 ) and adopt a scaling function :   B= tanh (  H)2f  1;+1 g ; ( 6 )   where  is the parameter which is increased dur-   ing the training . The function of tanh (  H)is an   approximate equation of sgn(H)when  is large   enough . Therefore , the output of Eq . 6 will Ô¨Ånally   be converged to 1 or -1 with the increasing of   during the training and the above problem is ad-   dressed .   3.2 Online Stage   Recall and Re - rank Mechanism The incoming   query from users will be fed into the description   category prediction module to calculate the nor-   malized probability distribution of categories at   Ô¨Årst . Then the number of code candidates Rfor   each category iwill be determined according to   this probability distribution . The Hamming dis-   tance between the hash code of the given query and   all the code inside the database will be calculated .   Then code candidates will be sorted by Hamming   distance in ascending order and the top Rcode   candidates in each category iwill be recalled . In   the re - rank step , the original representation vectors   of these recalled code candidates will be retrieved   and utilized for the cosine similarity calculation .   Finally , code snippets will be returned to users in   descending order of cosine similarity .   Description Category Prediction Module The   description category prediction module aims to pre - dict the category of source code that meets user ‚Äôs   requirement according to the given natural lan-   guage description . The model adopted for category   prediction is the same as the original code search   model , except that the output layer is replaced with   a one - hot category prediction layer and the cross-   entropy function is adopted as the loss function of   the model .   Since the accuracy of the description category   prediction module is not perfect , we use the prob-   ability distribution of each category instead of the   category with the highest predicted probability as   the recall strategy for code search . We deÔ¨Åne the   total recall number of source code as N , the normal-   ized predicted probability for each code category   asP = fp;:::;pg , wherekis the number of cat-   egories . The recall number of source code in each   category is :   R= min(bp(N k)c;1 ) ; i21;:::;k ; ( 7 )   whereRis the recall number of source code in   categoryi . To ensure that the proposed approach   can recall at least one source code from each cat-   egory , we set the minimum recall number for a   single category to 1 .   4 Experiments   4.1 Dataset   We use two datasets ( Python and Java ) provided   by CodeBERT ( Feng et al . , 2020 ) to evaluate the   performance of CoSHC . CodeBERT selects the   data from the CodeSearchNet ( Husain et al . , 2019)2538dataset and creates both positive and negative ex-   amples of < description , code > pairs . Since all the   baselines in our experiments are bi - encoder models ,   we do not need to predict the relevance score for   the mismatched pairs so we remove all the negative   examples from the dataset . Finally we get 412,178   < description , code > pairs as the training set , 23,107   < description , code > pairs as the validation set , and   22,176 < description , code > pairs as the test set in   the Python dataset . We get 454,451 < description ,   code > pairs as the training set , 15,328 < descrip-   tion , code > pairs as the validation set , and 26,909   < description , code > pairs as the test set in the Java   dataset .   4.2 Experimental Setup   In the code classiÔ¨Åcation module , we set the num-   ber of cluster to 10 . In the deep hashing module , we   add three fully connected ( FC ) layer in all the base-   lines , the hidden size of each FC layer is the same   as the dimension of the original representation vec-   tors . SpeciÔ¨Åcally , the hidden size of FC layer for   CodeBERTa , CodeBERT , GraphCodeBERT is 768 .   The hidden size of FC layer for UNIF is 512 and   for RNN is 2048 . The size of the output binary   hash code for all the baselines is 128 . The hyper   parameters  ; ;;;are 0.6 , 0.4 , 1.5 , 0.1 , 0.1 ,   respectively . The parameter  is the epoch num-   ber and will be linear increased during the training .   In the query category prediction module , a cross-   entropy function is adopted as the loss function and   the total recall number is 100 .   The learning rate for CodeBERTa , CodeBERT   and GraphCodeBERT is 1e-5 and the learning rate   for UNIF , RNN is 1.34e-4 . All the models are   trained via the AdamW algorithm ( Kingma and Ba ,   2015 ) .   We train our models on a server with four 4x   Tesla V100 w / NVLink and 32 GB memory . Each   module based on CodeBERT , GraphCodeBERT   and CodeBERTa are trained with 10 epochs and   Each module based on RNN and UNIF are trained   with 50 epochs . The early stopping strategy is   adopted to avoid overÔ¨Åtting for all the baselines .   The time efÔ¨Åciency experiment is conducted on the   server with Intel Xeon E5 - 2698v4 2.2Ghz 20 - core .   The programming for evaluation is written in C++   and the program is allowed to use single thread of   CPU.4.3 Baselines   We apply CoSHC on several state - of - the - art and   representative baseline models . UNIF ( Cam-   bronero et al . , 2019 ) regards the code as the se-   quence of tokens and embeds the sequence of   code tokens and description tokens into representa-   tion vectors via full connected layer with attention   mechanism , respectively . RNN baseline adopts a   two - layer bi - directional LSTM ( Cho et al . , 2014 )   to encode the input sequences . CodeBERTais a   6 - layer , Transformer - based model trained on the   CodeSearchNet dataset . CodeBERT ( Feng et al . ,   2020 ) is a pre - trained model based on Transformer   with 12 layers . Similar to CodeBERT , Graph-   CodeBERT ( Guo et al . , 2021 ) is a pre - trained   Transformer - based model pre - trained with not only   tokens information but also dataÔ¨Çow of the code   snippets . As we introduced , the inference efÔ¨Å-   ciency of cross - encoder based models like Code-   BERT is quite low and the purpose of our approach   is to improve the calculation efÔ¨Åciency between the   representation vectors of code and queries . Here   we slightly change the model structure of Code-   BERTa , CodeBERT , and GraphCodeBERT . Rather   than concatenating code and query together and in-   putting them into a single encoder to predict the rel-   evance score of the pair , we adopt the bi - encoder ar-   chitecture for the baselines , which utilize the inde-   pendent encoder to encoding the code and queries   into representation vectors , respectively . Also , co-   sine similarity between the given representation   vector pairs is adopted as the training loss function   to replace the cross entropy function of the output   relevance score .   4.4 Evaluation Metric   SuccessRate @kis widely used by many previ-   ous studies ( Haldar et al . , 2020 ; Shuai et al . , 2020 ;   Fang et al . , 2021 ; Heyman and Cutsem , 2020 ) . The   metric is calculated as follows :   whereQdenotes the query set and FRankis   the rank of the correct answer for query q. If the   correct result is within the top kreturning results ,   (FRankk)returns 1 , otherwise it returns 0 .   A higherR@kindicates better performance.2539   4.5 Experimental Results   In this section , we present the experimental results   and evaluate the performance of CoSHC from the   aspects of retrieval efÔ¨Åciency , overall retrieval per-   formance , and the effectiveness of the internal clas-   siÔ¨Åcation module .   4.5.1 RQ1 : How much faster is CoSHC than   the original code search models ?   Table 1 illustrates the results of efÔ¨Åciency compari-   son between the original code search models and   CoSHC . Once the representation vectors of code   and description are stored in the memory , the re-   trieval efÔ¨Åciency mainly depends on the dimension   of representation vectors rather than the complexity   of the original retrieval model . Therefore , we select   CodeBERT as the baseline model to illustrate efÔ¨Å-   ciency comparison . Since code search process in   both approaches contains vector similarity calcula-   tion and array sorting , we split the retrieval process   into these two steps to calculate the time cost .   In the vector similarity calculation step , CoSHC   reduces 97.29 % and 96.90 % of time cost in the   dataset of Python and Java respectively , which   demonstrates that the utilization of binary hash   code can effectively reduce vector similarity calcu-   lation cost in the code retrieval process .   In the array sorting step , CoSHC reduces 53.61 %   and 37.74 % of time cost in the dataset of Python   and Java , respectively . The classiÔ¨Åcation module   makes the main contribution on the improvement   of sorting efÔ¨Åciency . The sorting algorithm applied   in both original code search model and CoSHC is   quick sort , whose time complexity is O(nlogn ) .   ClassiÔ¨Åcation module divides a large code dataset   into several small code datasets , reducing the aver-   age time complexity of sorting to O(nlog ) . The   reason why the improvement of sorting in the Java   dataset is not so signiÔ¨Åcant as in the Python datasetis that the size of Java dataset is much smaller than   the size of Python dataset . However , the combi-   nation of the algorithm of divide and conquer and   max - heap , rather than quick sort , is widely applied   in the big data sorting , which can greatly shrink   the retrieval efÔ¨Åciency gap between these two ap-   proaches . Therefore , the improvement of efÔ¨Åciency   in the sorting process will not be as large as what   shown in Table 1 .   In the overall code retrieval process , the cost time   is reduced by 94.09 % and 93.51 % in the dataset   of Python and Java , respectively . Since the vector   similarity calculation takes most of cost time in   the code retrieval process , CoSHC still can reduce   at least 90 % of cost time , which demonstrates the   effectiveness on the efÔ¨Åciency improvement in the   code search task .   4.5.2 RQ2 : How does CoSHC affect the   accuracy of the original models ?   Table 2 illustrates the retrieval performance com-   parison between the original code search models   and CoSHC . We have noticed that the performance   of the conventional approaches like BM25 ( Robert-   son and Zaragoza , 2009 ) is not good enough . For   example , we set the token length for both code   and queries as 50 , which is the same as the setting   in CodeBERT , and apply BM25 to recall top 100   code candidates for the re - rank step on the Python   dataset . BM25 can only retain 99.3 % , 95.6 % and   92.4 % retrieval accuracy of CodeBERT in terms   ofR@1,R@5andR@10 on the Python dataset .   Here we only compare the performance of our ap-   proach with the original code search models since   the purpose of our approach is to preserve the per-   formance of the original code search models . As   can be observed , CoSHC can retain at least 99.5 % ,   99.0 % and 98.4 % retrieval accuracy of most origi-   nal code search models in terms of R@1,R@5and   R@10 on the Python dataset . CoSHC can also re-   tain 99.2 % , 98.2 % and 97.7 % of the retrieval accu-   racy as all original code search baselines in terms of   R@1,R@5andR@10 on the Java dataset , respec-   tively . We can Ô¨Ånd that CoSHC can retain more   than 97.7 % of performance in all metrics . R@1is   the most important and useful metric among these   metrics since most users hope that the Ô¨Årst returned   answer is the correct answer during the search .   CoSHC can retain at least 99.2 % of performance   onR@1 in both datasets , which demonstrates that   CoSHC can retain almost the same performance as   the original code search model.2540   It is interesting that CoSHC presents a rela-   tively better performance when the performance   of the original code retrieval models is worse .   CoSHC even outperforms the original   baseline model in Java dataset . CoSHC and   CoSHC outperform the original model in both   Python and Java datasets . The integration of deep   learning and code classiÔ¨Åcation with recall make   the contribution on this result . The worse perfor-   mance indicates more misalignment between the   code representation vectors and description repre-   sentation vectors . Since the code classiÔ¨Åcation and   deep hashing will Ô¨Ålter out most of irrelevant codes   in the recall stage , some irrelevant code represen-   tation vectors but has high cosine similarity with   the target description representation vectors are Ô¨Ål-   tered , which leads the improvement on the Ô¨Ånal   retrieval performance .   4.5.3 RQ3 : Can the classiÔ¨Åcation module   help improve performance ?   Table 2 illustrates the performance comparison be-   tween the CoSHC variants which adopt different   recall strategies with query category prediction re-   sults . CoSHC represents CoSHC   without code classiÔ¨Åcation and description pre-   diction module . CoSHC represents   the CoSHC variant that recalls N k+ 1candi-   dates in the code category with highest prediction   probability and one in each of the rest categories .   CoSHC is an ideal classiÔ¨Åcation   situation we set . Assuming the correct descrip-   tion category is known , N k+ 1candidates are   recalled in the correct category and one candidate   is recalled in each of the rest categories . Note that   the display of CoSHC is only to ex-   plore the upper threshold of performance improve-   ment of the category prediction module and will   not be counted as a variant of CoSHC we compare .   By comparing the performance be-2541tween CoSHC and   CoSHC , we can Ô¨Ånd that cor-   rect classiÔ¨Åcation can signiÔ¨Åcantly improve   the retrieval performance . With the ideal cat-   egory labels , CoSHC can even outperform all   baseline models . As mentioned in Sec . 4.5.2 ,   code classiÔ¨Åcation can mitigate the problem of   vector pairs misalignment via Ô¨Åltering out wrong   candidates whose representation vectors has high   cosine similarity with the target representation   vectors in the recall stage . The more serious the   misalignment problem , the more effective the   code classiÔ¨Åcation . That is the reason why the   improvement of CoSHC with ground - truth labels   on UNIF , RNN , and CodeBERTa is more signiÔ¨Å-   ca nt than the improvement of it on CodeBERT and   GraphCodeBERT since the retrieval accuracy of   former models is much lower than the latter ones .   Similar conclusions can also be drawn at the aspect   of binary hash code distribution via the comparison   between CoSHC and CoSHC   since CoSHC utilizes the distribution of the   original representation vectors as the guidance   for model training . Therefore , the distribution of   binary hash codes will be similar to the distribution   of original representation vectors .   Since we have explored the theoretical upper   limit of the effectiveness of code classiÔ¨Åcation   for code retrieval , the effectiveness of code clas-   siÔ¨Åcation for code retrieval in the real applica-   tion will be validated . By comparing the experi-   mental results between CoSHC and   CoSHC , we can Ô¨Ånd that the per-   formance of CoSHC with predicted labels is even   worse than the performance of CoSHC without   code classiÔ¨Åcation module . The reason is that the   accuracy of description category prediction is far   from the satisfactory . Table 3 illustrates the accu-   racy of description category prediction module in   all baseline models . We regard the category with   the highest probability as the predicted category   from the description category prediction module   and check whether the module could give a correct   prediction . It can be seen that the classiÔ¨Åcation ac-   curacy is not very high ( less than 75 % ) . By observ-   ing the experimental results of CoSHC in Graph-   CodeBERT on the Java dataset , we can also Ô¨Ånd   that low accuracy greatly affect the performance of   CoSHC , which makes 7.8 % , 11.6 % ,   and 13.9 % performance drop in terms of R@1 ,   R@5 , andR@10 , respectively . Fortunately , although the description category   prediction module can not accurately tell the ex-   act category which this description belongs to ,   the module still can give a relative high predicted   probability on the correct category . By compar-   ing the experimental results among all the vari-   ants of CoSHC , we can Ô¨Ånd the performance is   increased signiÔ¨Åcantly once the recall strategy is   replaced to that the number of code candidates   for each category is determined by the normalized   predication probability . CoSHC with new recall   strategy almost achieve the best performance in all   metrics on all baseline models . Even on RNN in   the Python dataset , CoSHC still achieve the same   performance as CoSHC without classiÔ¨Åcation un-   derR@1and achieve similar performance in other   metrics . Above experimental results have demon-   strated the effectiveness of the adoption of code   classiÔ¨Åcation in code search .   5 Conclusion   To accelerate code search , we present CoSHC , a   general method that incorporates deep hashing tech-   niques and code classiÔ¨Åcation . We leverage the two-   staged recall and re - rank paradigm in information   retrieval Ô¨Åeld and apply deep hashing techniques   for fast recall . Furthermore , we propose to utilize a   code classiÔ¨Åcation module to retrieve better quality   code snippets . Experiments on Ô¨Åve code search   models show that compared with the original code   search models , CoSHC can greatly improve the   retrieval efÔ¨Åciency meanwhile preserve almost the   same performance .   6 Acknowledgement   Wenchao Gu ‚Äôs and Michael R. Lyu ‚Äôs work de-   scribed in this paper was in part supported by the   Research Grants Council of the Hong Kong Special   Administrative Region , China ( CUHK 14210920   of the General Research Fund ) .   References254225432544