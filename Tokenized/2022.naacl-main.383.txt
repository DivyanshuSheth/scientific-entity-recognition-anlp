  Zhewei Sun , Richard Zemel , Yang XuDepartment of Computer Science , University of Toronto , Toronto , CanadaDepartment of Computer Science , Columbia University , New York , USACognitive Science Program , University of Toronto , Toronto , CanadaVector Institute for Artiﬁcial Intelligence , Toronto , Canada   { zheweisun , zemel , yangxu}@cs.toronto.edu   Abstract   Slang is a predominant form of informal lan-   guage making ﬂexible and extended use of   words that is notoriously hard for natural lan-   guage processing systems to interpret . Ex-   isting approaches to slang interpretation tend   to rely on context but ignore semantic exten-   sions common in slang word usage . We pro-   pose a semantically informed slang interpreta-   tion ( SSI ) framework that considers jointly the   contextual and semantic appropriateness of a   candidate interpretation for a query slang . We   perform rigorous evaluation on two large - scale   online slang dictionaries and show that our ap-   proach not only achieves state - of - the - art accu-   racy for slang interpretation in English , but   also does so in zero - shot and few - shot sce-   narios where training data is sparse . Further-   more , we show how the same framework can   be applied to enhancing machine translation of   slang from English to other languages . Our   work creates opportunities for the automated   interpretation and translation of informal lan-   guage .   1 Introduction   Slang is one of the most common forms of infor-   mal language , but interpreting slang can be difﬁcult   for both humans and machines . Empirical studies   have shown that , although it is done instinctively ,   interpretation and translation of unfamiliar or novel   slang expressions can be quite hard for humans   ( Braun and Kitzinger , 2001 ; Mattiello , 2009 ) . Sim-   ilarly , slang interpretation is also notoriously difﬁ-   cult for state - of - the - art natural language processing   ( NLP ) systems , which presents a critical challenge   to downstream applications such as natural lan-   guage understanding and machine translation .   Consider the sentence “ I got really steamed   when my car broke down ” . As illustrated in Fig-   ure 1 , directly applying a translation system such   as Google Translate on this raw English sentence   would result in a nonsensical translation of theFigure 1 : Illustrations of slang interpretation in English   ( top panel ) and slang translation ( bottom panel ) from   English to French on the original sentence ( nonsensi-   cal ) , or on the interpreted version of the sentence ( sen-   sical ) .   slang term steamed in French . This error is due   partly to the underlying language model that fails   to recognize the ﬂexible extended use of the slang   term from its conventional meaning ( e.g. , “ vapor ” )   to the slang meaning of “ angry ” . However , if   knowledge about such semantic extensions can be   incorporated into interpreting the slang prior to   translation , as Figure 1 shows the system would be   quite effective in translating the intended meaning .   Here we consider the problem of slang inter-   pretation illustrated in the top panel of Figure 1 .   Given a target slang term like steamed in a novel   query sentence , we want to automatically infer its   intended meaning in the form of a deﬁnition ( e.g. ,   “ angry ” ) . Tackling this problem has implications in   both machine interpretation and understanding of   informal language within individual languages and   translation between languages .   One natural solution to this problem is to use   contextual information to infer the meaning of a   slang term . Figure 2 illustrates this idea by show-   ing the top inﬁlled words predicted under a GPT-25213   ( Radford et al . , 2019 ) based language inﬁll model   ( Donahue et al . , 2020 ) . Each of these words can   be considered a candidate paraphrase for the tar-   get slang steamed conditioned on its surrounding   words . Although the groundtruth meaning “ angry ”   is among the list of top candidates , this model infers   “ sick ” as the most probable interpretation . A simi-   lar context - based approach has been explored in a   previous study led by Ni and Wang ( 2017 ) showing   that a sequence - to - sequence model trained directly   on a large number of pairs of slang - contained sen-   tences along with their corresponding deﬁnitions   from Urban Dictionary can be a useful starting   point toward the automated interpretation of slang .   We present an alternative approach to slang in-   terpretation that builds on but goes beyond the   context - based models . Inspired by recent work on   generative models of slang ( Sun et al . , 2019 , 2021 ) ,   we consider slang interpretation to be the inverse   process of slang generation and propose a semanti-   cally informed framework that takes into account   both contextual information and knowledge about   slang meaning extensions ( e.g. , “ vapor ” →“angry ” )   in inferring candidate interpretations . Our frame-   work incorporates a semantic model of slang that   uses contrastive learning to capture semantic ex-   tensions that link conventional and slang meanings   of words ( Sun et al . , 2021 ) . Under this frame-   work , meanings that are otherwise far apart can   be brought close , resulting in a semantic space   that is sensitive to the ﬂexible extended usages   of slang . Rather than using this learned semantic   space to generate novel slang usages , we apply it   to the inverse problem of slang interpretation by   checking whether a candidate interpretation may   be suitably expressed as a slang using the to - be-   interpreted slang expression . For example , “ sick ”   and “ angry ” can both replace the slang steamed   in a given context , but “ angry ” may be a more ap-   propriate meaning to be expressed using steamed   in the slang context . As such , we build a com-   putational framework that takes into account the   semantic knowledge of words as well as the context   of slang in the interpretation process . Figure 2 illustrates the workﬂow of our approach .   We begin with a set of candidate interpretations   informed by a context - based model ( e.g. , a lan-   guage inﬁll model ) , where the set would contain   a list of possible meanings that ﬁt reasonably in   the given context . We then rerank this set of candi-   date interpretations by selecting the meaning that   is most likely to be extended as slang from the   to - be - interpreted slang expression .   For the scope of this work , we focus on inter-   preting slang expressions with existing word forms   because extensive studies in slang have suggested   that a high proportion of slang usages relies on   the extended reuse of existing word forms ( Warren ,   1992 ; Green , 2010 ; Eble , 2012 ) . We show that our   framework can enhance state - of - the - art language   models in slang interpretation in English and slang   translation from English to other languages .   2 Related Work   2.1 Natural Language Processing for Slang   Existing approaches in the natural language pro-   cessing for slang focus on efﬁcient construction ,   extension , and retrieval from dictionary - based re-   sources for detection ( Pal and Saha , 2013 ; Dhu-   liawala et al . , 2016 ) , interpretation ( Gupta et al . ,   2019 ) , and sentiment analysis of slang ( Dhuliawala   et al . , 2016 ; Wu et al . , 2018 ) . These studies of-   ten rely on heuristic measures to determine or re-   trieve the meaning of slang and can not generalize   beyond what was available in the training data . Re-   cent work such as Kulkarni and Wang ( 2018 ) and   Pei et al . ( 2019 ) proposed deep learning based ap-   proaches to generalize toward unseen slang .   Closely related to our study is Ni and Wang   ( 2017 ) that formulated English slang interpretation   as a translation task ( although they did not tackle   slang machine translation per se ) . In this work ,   each slang query sentence in English is paired with   the groundtruth slang deﬁnition ( also in English ) ,   and such pairs are fed into a translation model . In   addition , the spellings of slang word forms are also   considered as input . In their model , both the con-   text and the slang form are encoded using separate   LSTM encoders . The two encoded representations   are then linearly combined to form the encoded in-   put for a sequence - to - sequence network ( Sutskever   et al . , 2014 ) . During training , the combined state   is passed onto an LSTM decoder to train against5214the corresponding deﬁnition sentence . During test   time , beam search ( Graves , 2012 ) is applied to de-   code a set of candidate deﬁnition sentences .   One key problem with this approach is that the   Dual Encoder tends to rely on the contextual fea-   tures surrounding the target slang but does not   model ﬂexible meaning extensions of the slang   word itself . Similar issues are present in a language-   model based approach , whereby one can use an   inﬁll model to infer the meaning of a target slang   based solely on its surrounding words . Our work   extends these context - based approaches by jointly   considering the contextual and semantic appropri-   ateness of a slang expression in a sentence , using   generative semantic models of slang .   2.2 Generative Semantic Models of Slang   Recent work by Sun et al . ( 2019 , 2021 ) proposed a   neural - probabilistic generative framework for mod-   eling slang word choice in novel context . Given a   query sentence with the target slang blanked out   and the intended meaning of that slang , their frame-   work predicts which word(s ) would be appropriate   slang choices that ﬁll in the blank . Relevant to their   framework is a semantic model of slang that uses   contrastive learning from Siamese networks ( Baldi   and Chauvin , 1993 ; Bromley et al . , 1994 ) to relate   conventional and slang meanings of words . This   model yields a semantic embedding space that is   sensitive to ﬂexible slang meaning extensions . For   example , it may learn that meanings associated   with “ vapor ” can extend to meanings about “ angry ”   ( as in the steamed example in Figure 1 ) .   Differing from slang generation , our work con-   cerns the inverse problem of slang interpretation   that has more direct applications in natural lan-   guage processing particularly machine translation   ( e.g. , of informal language ) . Building on work of   slang generation , we incorporate the generative se-   mantic model of slang in a semantically informed   interpretation framework that integrates context to   infer the intended meaning of a target slang .   3 Computational Framework   Our computational framework is comprised of   three key components following the workﬂow il-   lustrated in Figure 2 : 1 ) A context - based baseline   interpreter that generates an n - best list of candi-   date interpretations for a target slang in a query   sentence ; 2 ) A semantic model of slang that checks   the appropriateness of a candidate interpretation tothe slang context ; 3 ) A reranker informed by the se-   mantic model in 2 ) that re - prioritizes the candidate   interpretations from the context - based interpreter   in 1 ) . We use this framework for both interpret-   ing slang within English and translating slang from   English to other languages .   3.1 Context - based Interpretation   We deﬁne slang interpretation formally as follows .   Given a target slang term Sin context Cof a   query sentence , interpret the meaning of Sby a   deﬁnition M. The context is an important part of   the problem formulation since a slang term Smay   be polysemous and context can be used to constrain   the interpretation of its meaning . We deﬁne a slang   interpreter Iprobabilistically as :   I(S , C ) = arg maxP(M|S , C ) ( 1 )   Given this formulation , we retrieve an n - best list of   candidate interpretations K(i.e . ,|K|=n ) based   on an interpretation model of choice P(M|S , C ) .   Here , we consider two alternative models for   P(M|S , C ): 1 ) a language - model ( LM ) based ap-   proach that treats slang interpretation as a cloze   task , and 2 ) a sequence - to - sequence based ap-   proach similar to work by Ni and Wang ( 2017 ) .   LM - based interpreter . The ﬁrst model we con-   sider is a language inﬁll model in a cloze task , in   which the model itself is based on large pre - trained   language models such as GPT-2 ( Radford et al . ,   2019 ) . Although slang expressions may make spo-   radic appearances during training , this model is   not trained speciﬁcally on a slang related task and   thus serves as a baseline that reﬂects the state - of-   the - art language - model based NLP systems ( e.g. ,   Donahue et al . , 2020 ) .   Given context Ccontaining target slang S , we   blank out Sin the context and ask the language   inﬁll model to infer the most likely words to ﬁll in   the blank . This results in a probability distribution   P(w|C\S)over candidate words w. The inﬁlled   words can then be viewed as candidate interpreta-   tions of the slang S :   I(S , C ) = D[arg maxLM(w|C\S )   + 1[T(C\S ) ] ] ( 2 )   Here , Dis a dictionary lookup function that maps   a candidate word wto a deﬁnition sentence . In   this case , we constrain the space of meanings con-   sidered to the set of all meanings corresponding5215to words in the lexicon . Additionally , we apply a   Part - of - Speech ( POS ) tagger Tto check whether   the candidate word wshares the same POS tag as   the blanked - out word in the usage context . Words   that share the same POS tags are preferred in the   list of n - best retrievals .   This baseline approach by itself does not take   into account any ( semantic ) information from the   target slang S. In the case where two distinctive   slang terms may be placed in the same context ,   the model would generate the exact same output .   However , this LM based approach does not require   task - speciﬁc data to train . We show later that by   reranking language model outputs , it is possible to   achieve state - of - the - art performance using much   less on - task data than existing approaches .   Dual encoder . Ni and Wang ( 2017 ) partly ad-   dressed the context - only limitation by encoding the   slang term using a character - level recurrent neu-   ral network in an end - to - end model inspired by   the sequence - to - sequence architecture for neural   machine translation ( Sutskever et al . , 2014 ) . We   implement their dual encoder architecture as an   alternative context - based interpreter to LM . In this   model , separate LSTM encoders are applied on   the context Cand the character encoding of the   to - be - interpreted slang Srespectively . The two en-   coders are then linearly combined using learned   parameters . The combined state is passed onto an   LSTM decoder to train against the corresponding   deﬁnition sentence in Urban Dictionary ( as in the   original work of Ni and Wang 2017 ) . For inference ,   beam search ( Graves , 2012 ) is applied to decode   an n - best list of candidate deﬁnition sentences .   While this approach is trained directly on slang   data and considers the slang word forms , it requires   a large on - task dataset to be trained effectively .   This model also does not take into account the ap-   propriateness of meaning extension in slang usage .   We next describe how a semantic model of slang   can be incorporated to enhance the context - based   interpreters .   3.2 Semantic Model of Slang   Given an n - best list of candidate interpretations K   for the target slang Sin context C , we wish to   model the semantic plausibility of each candidate   interpretation k∈K . Speciﬁcally , we ask how   likely one would relate the ( conventional meaning   of ) target slang expression Sto a candidate inter-   pretation k. Sun et al . ( 2019 , 2021 ) modeled therelationship between a to - be - expressed meaning   and a word form using the prototype model ( Rosch ,   1975 ; Snell et al . , 2017 ) . We adapt this model in   the context of slang interpretation :   f(k , S ) = sim(E , E )   = exp(−d(E , E )   h ) ( 3 )   Eis an embedding for a candidate interpretation   kandEis the prototypical conventional meaning   ofScomputed by averaging the embeddings of its   conventional meanings in dictionary ( E ):   E=1   |E|/summationdisplayE ( 4 )   The similarity function fcan then be computed by   taking the negative exponential of the Euclidean   distance between the two resulting semantic em-   beddings . his a kernel width hyperparameter .   Following Sun et al . ( 2021 ) , we learn seman-   tic embeddings EandEunder a max - margin   triplet loss scheme , where embeddings of slang   sense deﬁnitions ( E ) are brought close in Eu-   clidean space to those of their conventional sense   deﬁnitions ( E ) yet kept apart from irrelevant word   senses ( E ) by a pre - speciﬁed margin m :   Loss = /bracketleftBig   d(E , E)−d(E , E ) + m / bracketrightBig   ( 5 )   The resulting contrasive sense encodings are   shown to be sensitive to slang semantic extensions   that have been observed during training . We lever-   age this knowledge to check whether pairing a can-   didate interpretation kwith the slang expression   Sis likely given the common semantic extensions   observed in slang usages .   3.3 Semantically Informed Reranking   We deﬁne a semantic scorer gover the set of can-   didate interpretations Kand the to - be - interpreted   slang S. The candidates are reranked based on the   resulting scores to obtain semantically informed   slang interpretations ( SSI ):   SSI(K ) = arg max g(k , S ) ( 6 )   We deﬁne g(K , S)as a score distribution over the   set of candidatesKgiven slang S , where each score   is computed by checking the semantic appropriate-   ness of a candidate meaning k∈K with respect to5216target slang Sby querying the semantic model f   from Equation 3 :   g(k , S ) = P(k|S)∝f(k , S ) ( 7 )   In addition , we apply collaborative ﬁltering   ( Goldberg et al . , 1992 ) to account for a small neigh-   borhood of words L(S)akin to the slang expres-   sionSin conventional meaning :   g(k , S)∝/summationdisplaysim(S , S)g(k , S ) ( 8)   sim(S , S ) = exp(−d(S , S )   h ) ( 9 )   Here , d(S , S)is the cosine distance between the   two slang ’s word vectors and his a hyperparam-   eter controlling the kernel width . The collaborative   ﬁltering step encodes intuition from studies in his-   toric semantic change that similar words tend to   extend to express similar meanings ( Lehrer , 1985 ;   Xu and Kemp , 2015 ) , which was found to extend   well in the case of slang ( Sun et al . , 2019 , 2021 ) .   4 Datasets   We use two online English slang dictionary re-   sources to train and evaluate our proposed slang in-   terpretation framework : 1 ) the Online Slang Dictio-   nary ( OSD)dataset from Sun et al . ( 2021 ) and 2 ) a   collection of Urban Dictionary ( UD)entries from   1999 to 2014 collected by Ni and Wang ( 2017 ) .   Each dataset contains slang gloss entries includ-   ing a slang ’s word form , its deﬁnition , and at least   one corresponding example sentence containing   the slang term . We use the same training and test-   ing split provided by the original authors and only   use entries where a corresponding non - informal   entry can be found in the online version of the Ox-   ford Dictionary ( OD ) for English , which allows   the retrieval of conventional senses for all slang   expressions considered . We also ﬁlter out entries   where the example usage sentence contains none or   more than one exact references of the correspond-   ing slang expression . When a deﬁnition entry has   multiple example usage sentences , we treat each ex-   ample sentence as a separate data entry , but all data   entries corresponding to the same deﬁnition entry   will only appear in the same data split . Table 1   shows the size of the datasets after pre - processing . While OSD contains higher quality entries , UD   offers a much larger dataset . We thus use OSD   to evaluate model performance in a low resource   scenario and UD for evaluation of larger neural   network based approaches .   5 Evaluation and Results   5.1 Evaluation on Slang Interpretation   We ﬁrst evaluate the semantically informed and   baseline interpretation models in a multiple choice   task . In this task , each query is paired with a set of   deﬁnitions that construe the meaning of the target   slang in the query . One of these deﬁnitions is the   groundtruth meaning of the target slang , while the   other deﬁnitions are incorrect or negative entries   sampled from the training set ( i.e. , all taken from   the slang dictionary resources described ) . To score   a model , each deﬁnition sentence is ﬁrst compared   with the model - predicted deﬁnition by computing   the Euclidean distance between their respective   Sentence - BERT ( Reimers and Gurevych , 2019 ) em-   beddings . The ideal model should produce a deﬁni-   tion that is semantically closer to the groundtruth   deﬁnition , more so than the other competing neg-   atives . For each dataset , we sample two sets of   negatives . The ﬁrst set of negative candidates con-   tains only deﬁnition sentences from the training   set that are distinct from the groundtruth deﬁnition .   We consider two deﬁnition sentences to be distinct   if the overlap in the number of content words is   less than 50 % . The other set of negative deﬁnitions   is sampled randomly . We measure the performance   of the models by computing the standard mean   reciprocal rank ( MRR ) of the groundtruth deﬁni-   tion ’s rank when checked against 4 other sampled   negative deﬁnitions .   We train the semantic reranker on all deﬁnition   entries in the respective training sets from the two   data resources . When training the Dual Encoder ,   we use 400,431 out - of - vocabulary slang entries   ( i.e. , entries with a slang expression that does not   contain a corresponding lexical entry in the stan-   dard dictionary ) from UD in addition to the in-   vocabulary entries used to train the reranker . This   is necessary since the baseline Dual Encoder per-   forms poorly without a large number of training   entries . Similarly , training the Dual Encoder di-   rectly on the OSD training set does not result in an   adequate model for comparison . We instead train   the Dual Encoder on all UD entries and experiment   with the resulting interpreter on OSD . Any UD5217Dataset # of unique   slang word forms # of slang   deﬁnition entries # of context   sentences # of deﬁnitions   in the test set # of context sentences   in the test set   OSD 1,635 2,979 3,718 299 405   UD 9,474 65,478 65,478 1,242 1,242   ModelDistinctively   sampled   candidatesRandomly   sampled   candidates   Dataset 1 : Online Slang Dictionary ( OSD ) ( Sun et al . , 2021 )   Language Inﬁll Model ( LM Inﬁll ) ( Donahue et al . , 2020 ) , n= 50 0.532 0.502   + Semantically Informed Slang Interpretation ( SSI ) 0.557 0.563   Dual Encoder * ( Ni and Wang , 2017 ) , n= 5 0.584 0.583   + SSI 0.592 0.588   Dual Encoder * , n= 50 0.568 0.602   + SSI 0.616 0.607   Dataset 2 : Urban Dictionary ( UD ) ( Ni and Wang , 2017 )   LM Inﬁll , n= 50 0.517 0.521   + SSI 0.569 0.579   Dual Encoder , n= 5 0.556 0.555   + SSI 0.573 0.572   Dual Encoder , n= 50 0.547 0.550   + SSI 0.582 0.584   entries corresponding to words found in the OSD   testset are ﬁltered out in this particular experiment .   Detailed training procedures for all models can be   found in Appendix A.   Table 2 summarizes the multiple - choice evalu-   ation results on both slang datasets . In all cases ,   applying the semantically informed slang interpre-   tation framework improves the MRR of the respec-   tive baselines under both types of negative candi-   date sampling . On the UD evaluation , even though   the language inﬁll model ( LM Inﬁll ) is not trained   on this speciﬁc task , LM inﬁll based SSI is able to   select better and more appropriate interpretations   than the dual encoder baseline , which is trained   speciﬁcally on slang interpretation with more than   7 times the number of deﬁnition entries for training .   We also ﬁnd that while increasing the beam size   ( speciﬁed by n ) in the sequence - to - sequence based   Dual Encoder model impairs its performance , SSI   can take advantage of the additional variation inthe generated candidates and outperform its coun-   terpart with a smaller beam size .   Table 3 provides example interpretations pre-   dicted by the models . The litexample shows a   case where the semantically informed models were   able to correctly pinpoint the intended deﬁnition ,   among alternative deﬁnitions that describe individ-   uals . The lush example suggests that the SSI model   is not perfect and points to common errors made   by the model including predicting deﬁnitions that   are more general and applying incorrect semantic   extensions . In this case , the model predicts the   slang lush to mean “ something that is not cool ” be-   cause polarity shift is a common pattern in slang   usage ( Eble , 2012 ) , even though the groundtruth   deﬁnition does not make such a polarity shift in   this speciﬁc example .   Note that the improvement brought by SSI is   less prominent in the OSD experiment where the   Dual Encoder trained on UD was used . This is5218Query ( target slang in bold italic ): That chick is lit !   Groundtruth deﬁnition of target slang : Attractive .   LM Inﬁll baseline prediction : Cute , beautiful , adorable .   LM Inﬁll + SSI prediction : Hot , cool , fat .   Dual Encoder baseline prediction : Another word for bitch .   Dual Encoder + SSI prediction : Word used to describe someone who is very attractive .   Query : That Louis Vuitton purse is lush !   Groundtruth deﬁnition of target slang : High quality , luxurious . ( British slang . )   LM Inﬁll baseline prediction : Amazing , beautiful , unique .   LM Inﬁll + SSI prediction : Lovely , stunning , expensive .   Dual Encoder baseline prediction : Something that is cool or awesome .   Dual Encoder + SSI prediction : An adjective used to describe something that is not cool .   expected because the Dual Encoder is trained to   generate deﬁnition sentences in the style of UD en-   tries , whereas the SSI is trained on OSD deﬁnition   sentences instead . The mismatch in style between   the two datasets might have caused the difference   in performance gain .   5.2 Zero - shot and Few - shot Interpretation   Recent studies in deep learning have shown that   large neural network based models such as GPT-3   excel at learning new tasks in a few - shot learn-   ing setting ( Brown et al . , 2020 ) . We examine to   what extent the superior performance of our SSI   framework may be affected by ﬁne - tuning the LM   baseline model in zero - shot and few - shot scenarios .   We ﬁnetune the language inﬁll model ( LM Inﬁll )   on the ﬁrst example usage sentence that correspond   to each deﬁnition entry in the OSD dataset , result-   ing in 2,979 sentences . Given an example sentence ,   we mask out the slang expression and train the   language inﬁll model to predict the corresponding   slang term . We randomly shufﬂe all examples and   ﬁnetune LM Inﬁll for one epoch . We then compare   the resulting model with the off - the - shelf LM using   examples in the test set that were not used in ﬁne-   tuning ( i.e. , entries with usage sentences that do   not correspond to the ﬁrst example usage sentence   of a deﬁnition entry ) . This results in 106 novel   examples for evaluation .   Table 4 shows the result of this experiment .   While ﬁnetuning does improve test performance ( a   6 point gain in MRR ) , it remains beneﬁcial to con-   sider semantic information in slang context . In both   the zero - shot and the few - shot cases , SSI bringsModelDistinct   negativesRandom   negatives   LM Zero - shot , n= 50 0.444 0.443   + SSI 0.571 0.565   LM Few - shot , n= 50 0.504 0.513   + SSI 0.567 0.564   signiﬁcant performance gain even though SSI itself   is only trained on entries from the training set .   5.3 Evaluation on Slang Translation   We next apply the slang interpretation framework   to neural machine translation . Existing machine   translation systems have difﬁculty in translating   source sentences containing slang usage partly be-   cause they lack the ability to properly decode the   intended slang meaning . We make a ﬁrst attempt   in addressing this problem by exploring whether   machine interpretation of slang can lead to bet-   ter translation of slang . Given a source English   sentence containing a slang expression S , we ap-   ply the LM based slang interpreters to generate a   paraphrased word to replace S. The paraphrased   sentence would then contain the intended mean-   ing of the slang in its literal form . Here , we take   advantage of the LM - based approaches ’ ability to   directly generate a paraphrase instead of a deﬁni-   tion sentence ( i.e. , without dictionary lookup Din   Equation 2 ) , which allows direct insertion of the   resulting interpretation into the original sentence.5219   We perform our experiment on the OSD test   set because it contains higher quality example sen-   tences than UD . To mitigate potential biases , we   consider only entries that correspond to single word   slang expressions , and that the slang has not been   seen during training ( where the slang attaches to   a different slang meaning than the one in the test   set ) . For the remaining 102 test entries , we obtain   gold - standard translations by ﬁrst manually replac-   ing the slang word in the example sentence with its   intended deﬁnition , condensed to a word or short   phrase to ﬁt into the context sentence . We then   translate the sentences to French and German using   machine translation .   We make all machine translations using pre-   trained 6 - layer transformer networks ( Vaswani   et al . , 2017 ) from MarianMT ( Tiedemann and Thot-   tingal , 2020 ) , which are trained on a collection of   web - based texts in the OPUS dataset ( Tiedemann ,   2012 ) . Here , we select models pre - trained on web-   based texts to maximize the baseline model ’s abilityto correctly process slang . We evaluate the trans-   lated sentences using three metrics : 1 ) Sentence-   level BLEU scores ( Papineni et al . , 2002 ) com-   puted using sentence_bleu implementation from   NLTK ( Bird et al . , 2009 ) with smoothing ( method4   in NLTK , Chen and Cherry , 2014 ) to account for   sparse n - gram overlaps ; 2 ) BLEURT scores ( Sel-   lam et al . , 2020 ) computed using the pre - trained   BLEURT-20 checkpoint ; 3 ) COMET scores ( Rei   et al . , 2020 ) computed using the pre - trained wmt20-   comet - da checkpoint . For COMET scores , we re-   place slang expressions in the source sentences   with their literal equivalents to reduce confusion   that the COMET model might have on slang .   Figure 3 summarizes the results . Overall , the   semantically informed approach tends to outper-   form the baseline approaches for the range of top   retrievals ( from 1 to 20 ) under all three metrics   considered , with the exception of BLEURT evalu-   ated on German where the semantically informed   approach gives very similar performance as the5220Query ( target slang in bold italic ): I want to go get coffee but it ’s bitter outside .   Deﬁnition of target slang : Abbreviated form of bitterly cold .   Groundtruth interpreted sentence : I want to go get coffee but it ’s bitterly cold outside .   Original query sentence translation : Je veux aller prendre un café mais c’est amer dehors .   Gold - standard translation : Je veux aller prendre un café , mais il fait très froid   dehors .   LM Inﬁll interpretation & translation :   ( 1 ) I want to go get coffee but it ’s raining Je veux aller prendre un café mais il pleut dehors .   outside .   ( 2 ) I want to go get coffee but it ’s closed Je veux aller prendre un café mais il est fermé dehors .   outside .   LM Inﬁll + SSI interpretation & translation :   ( 1 ) I want to go get coffee but it ’s cold Je veux aller prendre un café , mais il fait froid dehors .   outside .   ( 2 ) I want to go get coffee but it ’s warm Je veux aller prendre un café mais il fait chaud dehors .   outside .   language model baseline . While not all predicted   interpretations correspond to the groundtruth deﬁni-   tions , the set of interpreted sentences often contain   plausible interpretations that result in improved   translation of slang . Table 5 provides some exam-   ple translations . We observe that quality transla-   tions can be found reliably with a small number   of interpretation retrievals ( i.e. , around 5 ) and the   quality generally improves as we retrieve more can-   didate interpretations . Our approach may be ulti-   mately integrated with a slang detector ( e.g. , Pei   et al . 2019 ) to produce fully automated translations   in natural context that involves slang .   6 Conclusion   The ﬂexible nature of slang is a hallmark of in-   formal language , and to our knowledge we have   presented the ﬁrst principled framework for auto-   mated slang interpretation that takes into account   both contextual information and knowledge about   semantic extensions of slang usage . We showed   that our framework is more effective in interpreting   and translating the meanings of English slang terms   in natural sentences in comparison to existing ap-   proaches that rely more heavily on context to infer   slang meaning .   Future work in this area may beneﬁt from prin-   cipled approaches that model the coinage of slang   expressions with novel word forms and multi - word   expressions with complex formation strategies , aswell as how slang terms emerge in speciﬁc individ-   uals and groups . Our current study shows promise   for advancing methodologies in informal language   processing toward these avenues of future research .   Ethical Considerations   We analyze entries of slang usage in our work and   acknowledge that such usages may contain offen-   sive information . We retain such entries in our   datasets to preserve the scientiﬁc validity of our re-   sults , as a signiﬁcant portion of slang usage aligns   to possibly offensive usage context . In the presen-   tation our of results , however , we strive to select   examples or illustrations that minimize the extent   to which offensive content is represented . We also   acknowledge that models trained on datasets such   as the Urban Dictionary have a greater tendency   to generate offensive language . All model outputs   shown are results of model learning and do not re-   ﬂect opinions of the authors and their afﬁliated or-   ganizations . We hope that our work will contribute   to the greater good by enhancing AI system ’s abil-   ity to comprehend such offensive language use ,   allowing better ﬁltering of online content that may   be potentially harmful .   Acknowledgements   We thank the ARR reviewers for their constructive   comments and suggestions , and Walter Rader for5221permission to use The Online Slang Dictionary .   This work was supported by a NSERC Discovery   Grant RGPIN-2018 - 05872 , a SSHRC Insight Grant   # 435190272 , and an Ontario ERA Award to YX .   References52225223A Training Procedures   A.1 Baseline Models   We train two context - based slang interpreters de-   scribed in Section 3.1 as our baseline models . For   the LM - based interpreter , we use a pre - trained   language inﬁll model from Donahue et al . ( 2020 )   based on the GPT-2 ( Radford et al . , 2019 ) architec-   ture . Here , we obtain the n - best list of interpreta-   tions by retrieving the list of inﬁlled words with the   highest inﬁll probability . Words containing non-   alphanumeric characters are ﬁltered out . For the   dictionary lookup function Din Equation 2 , if a   matching dictionary entry can be found in Oxford   Dictionary ( OD ) , the top deﬁnition sentence is re-   trieved as the deﬁnition sentence for the input word .   Otherwise , the word itself is used as the deﬁnition .   In addition to the word‘s original form , we apply   lemmatization or stemming to the original form   using NLTK ( Bird et al . , 2009 ) to ﬁnd matching   dictionary entries . To check for Part - of - Speech   ( POS ) tags , we apply the Flair tagger ( Akbik et al . ,   2018 ) on the context sentence with the slang ex-   pression replaced by a mask token and use counts   from Histwords ( Hamilton et al . , 2016 ) to deter-   mine POS tags for individual words .   To train the Dual Encoder , we use LSTM en-   coders with 256 and 1024 hidden units to encode   a slang expression ’s spelling and its usage context   respectively , with 100 and 300 dimensional input   embeddings for the characters and words respec-   tively . Following Ni and Wang ( 2017 ) , we use   random initialization for the input embeddings and   use stochastic gradient descent ( SGD ) with an adap-   tive learning rate . We train the model for 20 epochs   beginning with a learning rate of 0.1 and add an   exponential decay of 0.9 every epoch . We reserve   5 % of the training examples as a development set   for hyperparameter tuning . We train the model for   20 epochs on a Nvidia Titan V GPU and took 12   hours to complete . During inference , we obtain   the n - best list of interpretations by running a beam   search of corresponding beam width on the LSTM   decoder .   A.2 Semantic Reranker   We obtain the contrastive sense encodings ( CSE )   described in Section 3.2 by using 768 - dimensional   Sentence - BERT ( Reimers and Gurevych , 2019 )   embeddings as our baseline embedding . Follow-   ing Sun et al . ( 2021 ) , we train the contrastive net-   work with a 1.0 margin ( min Equation 5 ) usingAdam ( Kingma and Ba , 2015 ) with a learning rate   of2 , resulting in 768 - dimensional deﬁnition   sense presentations . We reserve 5 % of the training   examples as a development set for hyperparameter   tuning . The contrastive models are trained on a   Nvidia Titan V GPU for 4 epochs . The OSD model   took 85 minutes to train and the UD model took 8   hours . We follow the training procedure from Sun   et al . ( 2021 ) to estimate the kernel width parame-   ters ( hin Equation 3 and hin Equation 9 ) via   generative training when it is computationally fea-   sible to do so and otherwise use 0.1 as our default   value .   We check the similarity between two expressions   in Equation 9 by comparing their fastText ( Bo-   janowski et al . , 2017 ) embeddings . For collabo-   rative ﬁltering , the neighborhood of words L(S )   in Equation 8 is deﬁned as the 5 closest words   ( including the query word itself ) in the dataset ’s   slang expression vocabulary to the query word ,   measured in terms of cosine similarity between   their respective fastText embeddings . We use the   list of stopwords from NLTK ( Bird et al . , 2009 ) to   check whether a word is a content word . We apply   thesimple_preprocess routine from Gensim ( Re-   hurek and Sojka , 2011 ) before checking for the   degree of content word overlap between two sen-   tences .   B Additional Results   B.1 Additional Interpretation Examples   Table 7 show additional example interpretations   made by the models evaluated in Section 5.1 .   The ﬁrst three examples illustrate cases where the   semantically informed models were not able to   predict the exact deﬁnitions , but came up with   deﬁnitions that are more closely related to the   groundtruth compared to the baseline . The latter   two examples show cases where the semantically   informed models fail to make an improvement .   B.2 Effect of Context Length   In the model evaluation described in Section 5.1 ,   we control for the content - word length of the usage   context sentence to examine its effect with respect   to interpretation performance for both the baseline   and the semantically informed models . Figure 4   shows the results partitioned by the number of con-   tent words in the example usage sentence excluding   the slang expression , evaluated against four distinc-   tively sampled candidates . To our surprise , we do5224ModelDistinct   negativesRandom   negatives   Dual Encoder , n= 5 0.604 0.598   + SSI 0.612 0.599   Dual Encoder , n= 50 0.583 0.570   + SSI 0.627 0.633   not observe any consistent trends when controlling   for context length . Interpretation performance for   both the context - based baseline models and their   semantically informed variants is fairly consistent   under different context length .   B.3 Finetuning Dual Encoder   We consider the case of ﬁnetuning the Dual En-   coder by training it on all available UD data entries   and test on the full OSD test set . Under this sce-   nario , the Dual Encoder model would have seen   examples of slang in the OSD test set , though the   difference between the deﬁnition sentences and us-   age examples would not allow it to memorize the   exact answer . While examining how much knowl-   edge can be transfered from one dataset to another ,   we also apply the SSI reranker trained on OSD   training data on the ﬁnetuned results to simulate   a stronger baseline model . Table 6 shows the re-   sults . When compared to the zero - shot results in   Table 2 , ﬁnetuning on entries corresponding to the   same slang , albeit coming from two very different   resources , does noticeably improve interpretation   accuracy . Moreover , applying SSI to the improved   interpretation candidates from the ﬁnetuned Dual   Encoder further increases interpretation accuracy .   This ﬁnding suggests that the improvement brought   by SSI can indeed generalize in cases where the   baseline context - based interpretation model out-   puts better interpretation candidates .   B.4 Machine Translation Examples   Table 8 to Table 11 show full example translations   ( English to French ) made for the experiment de-   scribed in Section 5.3 , translating sentences con-   taining slang before and after applying slang inter-   pretation . C Data Permissions   At the time when the research is performed , Online   Slang Dictionary ( OSD ) explicitly forbids auto-   mated downloading of data from its website ser-   vice . We therefore have obtained written permis-   sion from its owner to download and use the dataset   for personal research use . We download data from   the online version of the Oxford Dictionary ( OD )   under personal use . We can not publically share the   two datasets used above as a result . Readers inter-   ested in obtaining the exact datasets used in this   work must ﬁrst obtain relevant permission from   the respective data owner before the authors of this   work can share the data . The Urban Dictionary   ( UD ) dataset is obtained from the authors of Ni and   Wang ( 2017 ) under a research only license . We re-   lease entries relevant to our study with the original   data license attached.5225[Example 1 ]   Query ( target slang in bold italic ): That girl has a donkey .   Groundtruth deﬁnition of target slang : Used to describe a girl ’s butt in a good way .   LM Inﬁll baseline prediction : Name , crush , boyfriend .   LM Inﬁll + SSI prediction : Horse , dog , puppy .   Dual Encoder baseline prediction : Penis .   Dual Encoder + SSI prediction : Girl with big ass and big boobs .   [ Example 2 ]   Query : I am an onion .   Groundtruth deﬁnition of target slang : A native of Bermuda .   LM Inﬁll baseline prediction : Adult , man , athlete .   LM Inﬁll + SSI prediction : Ren , adult , guard .   Dual Encoder baseline prediction : An idiot .   Dual Encoder + SSI prediction : An asian person .   [ Example 3 ]   Query : In Blastem version 4 , they really nerf the EnemyToaster .   Groundtruth deﬁnition of target slang : In an update or sequel to a video game , to make a   weapon weak or weaker , such that it ’s like a Nerf gun .   LM Inﬁll baseline prediction : Were , called , attack .   LM Inﬁll + SSI prediction : Made , hacked , came .   Dual Encoder baseline prediction : To do something .   Dual Encoder + SSI prediction : To beat someone in the face with your penis .   [ Example 4 ]   Query : I heard Steve was sent to the cooler for breaking and entering .   Groundtruth deﬁnition of target slang : Reform school .   LM Inﬁll baseline prediction : School , house , class .   LM Inﬁll + SSI prediction : Bathroom , kitchen , grounds .   Dual Encoder baseline prediction : Slang term for the police .   Dual Encoder + SSI prediction : One of the most dangerous things in the world the best .   [ Example 5 ]   Query : Do you have any safety   Groundtruth deﬁnition of target slang : Marijuana .   LM Inﬁll baseline prediction : Money , friends , cash .   LM Inﬁll + SSI prediction : Self , shoes , money .   Dual Encoder baseline prediction : Marijuana .   Dual Encoder + SSI prediction : Word that is used to describe something that is very good.52265227[Example 1 ]   Query ( target slang in bold italic ): Let ’s smoke a bowl of marijuana .   Deﬁnition of target slang : a marijuana smoking pipe . Most frequently bowls   are made out of blown glass , but can be made of   metal , wood , etc .   Groundtruth interpreted sentence : Let ’s smoke a pipe of marijuana .   Original query sentence translation : Faisons fumer un bol de marijuana .   Gold - standard translation : Faisons fumer une pipe de marijuana .   LM Inﬁll interpretation & translation :   ( 1 ) Let ’s smoke a forof marijuana . Fumons un pour de la marijuana .   ( 2 ) Let ’s smoke a inof marijuana . On fume un peu(little ) de marijuana .   ( 3 ) Let ’s smoke a myself of marijuana . Nous allons fumer moi - même de la marijuana .   ( 4 ) Let ’s smoke a orof marijuana . Fumons un oude marijuana .   ( 5 ) Let ’s smoke a vapor of marijuana . Fumons une vapeur de marijuana .   LM Inﬁll + SSI interpretation & translation :   ( 1 ) Let ’s smoke a potof marijuana . Faisons fumer un potde marijuana .   ( 2 ) Let ’s smoke a pipe of marijuana . Faisons fumer une pipe de marijuana .   ( 3 ) Let ’s smoke a pack of marijuana . Faisons fumer un paquet de marijuana .   ( 4 ) Let ’s smoke a leaf of marijuana . Faisons fumer une feuille de marijuana .   ( 5 ) Let ’s smoke a cigarette of marijuana . Faisons fumer une cigarette de marijuana.5228[Example 2 ]   Query : That band was so totally vast .   Deﬁnition of target slang : Cool or anything good .   Groundtruth interpreted sentence : That band was so totally cool .   Original query sentence translation : Ce groupe était si vaste .   Gold - standard translation : Ce groupe était tellement cool .   LM Inﬁll interpretation & translation :   ( 1 ) That band was so totally popular . Ce groupe était tellement populaire .   ( 2 ) That band was so totally good . Ce groupe était si bon .   ( 3 ) That band was so totally different . Ce groupe était complètement différent .   ( 4 ) That band was so totally famous . Ce groupe était si célèbre .   ( 5 ) That band was so totally new . Ce groupe était totalement nouveau .   LM Inﬁll + SSI interpretation & translation :   ( 1 ) That band was so totally huge . Ce groupe était tellement énorme .   ( 2 ) That band was so totally big . Ce groupe était tellement grand .   ( 3 ) That band was so totally important . Ce groupe était si important .   ( 4 ) That band was so totally cool . Ce groupe était tellement cool .   ( 5 ) That band was so totally bad . Ce groupe était si mauvais .5229[Example 3 ]   Query ( target slang in bold italic ): Man , I ai n’t been to that place in a fortnight !   Deﬁnition of target slang : An unspeciﬁc , but long - ish length of time .   Groundtruth interpreted sentence : Man , I ai n’t been to that place in a long time !   Original query sentence translation : Je ne suis pas allé à cet endroit en une quinzaine !   Gold - standard translation : Je n’y suis pas allé depuis longtemps !   LM Inﬁll interpretation & translation :   ( 1 ) Man , I ai n’t been to that place in a while ! Je ne suis pas allé à cet endroit depuis un moment !   ( 2 ) Man , I ai n’t been to that place in a million ! Je ne suis pas allé à cet endroit dans un million !   ( 3 ) Man , I ai n’t been to that place in a both ! Je ne suis pas allé à cet endroit dans les deux !   ( 4 ) Man , I ai n’t been to that place in a vanilla ! Mec , je n’ai pas été à cet endroit dans une vanille !   ( 5 ) Man , I ai n’t been to that place in a ignment ! Mec , je n’ai pas été à cet endroit dans un ignement !   LM Inﬁll + SSI interpretation & translation :   ( 1 ) Man , I ai n’t been to that place in a week ! Je ne suis pas allé à cet endroit en une semaine !   ( 2 ) Man , I ai n’t been to that place in a minute ! Je ne suis pas allé à cet endroit en une minute !   ( 3 ) Man , I ai n’t been to that place in a hour ! Je ne suis pas allé à cet endroit en une heure !   ( 4 ) Man , I ai n’t been to that place in a decade ! Je n’y suis pas allé depuis une décennie   ( 5 ) Man , I ai n’t been to that place in a day ! Je ne suis pas allé à cet endroit en une journée ! 5230[Example 4 ]   Query : I want to go get coffee but it ’s bitter outside .   Deﬁnition of target slang : Abbreviated form of bitterly cold .   Groundtruth interpreted sentence : I want to go get coffee but it ’s bitterly cold outside .   Original query sentence translation : Je veux aller prendre un café mais c’est amer dehors .   Gold - standard translation : Je veux aller prendre un café , mais il fait très froid   dehors .   LM Inﬁll interpretation & translation :   ( 1 ) I want to go get coffee but it ’s raining Je veux aller prendre un café mais il pleut dehors .   outside .   ( 2 ) I want to go get coffee but it ’s closed Je veux aller prendre un café mais il est fermé dehors .   outside .   ( 3 ) I want to go get coffee but it ’s pouring Je veux aller chercher du café , mais ça coule dehors .   outside .   ( 4 ) I want to go get coffee but it ’s been Je veux aller prendre un café , mais ça a étédehors .   outside .   ( 5 ) I want to go get coffee but it ’s starting Je veux aller prendre un café , mais ça commence   dehors .   outside .   LM Inﬁll + SSI interpretation & translation :   ( 1 ) I want to go get coffee but it ’s cold Je veux aller prendre un café , mais il fait froid dehors .   outside .   ( 2 ) I want to go get coffee but it ’s warm Je veux aller prendre un café mais il fait chaud dehors .   outside .   ( 3 ) I want to go get coffee but it ’s driving Je veux aller prendre un café mais il conduit dehors .   outside .   ( 4 ) I want to go get coffee but it ’s closing Je veux aller prendre un café mais il se ferme dehors .   outside .   ( 5 ) I want to go get coffee but it ’s dark Je veux aller prendre un café , mais il fait noir dehors .   outside.5231