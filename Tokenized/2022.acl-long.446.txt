  Sheena PanthaplackelAdrian BentonMark DredzeComputer Science , The University of Texas at Austin , TX , USABloomberg , New York , NY USAComputer Science , Johns Hopkins University , Baltimore , MD USA   spantha@cs.utexas.edu , adbenton@google.com , mdredze@cs.jhu.edu   Abstract   We propose the task of updated headline gen-   eration , in which a system generates a head-   line for an updated article , considering both   the previous article and headline . The system   must identify the novel information in the ar-   ticle update , and modify the existing headline   accordingly . We create data for this task us-   ing the NewsEdits corpus ( Spangher and May ,   2021 ) by automatically identifying contiguous   article versions that are likely to require a sub-   stantive headline update . We find that models   conditioned on the prior headline and body re-   visions produce headlines judged by humans to   be as factual as gold headlines while making   fewer unnecessary edits compared to a standard   headline generation model . Our experiments   establish benchmarks for this new contextual   summarization task .   1 Introduction   Automatic text summarization condenses the most   important and salient information from a large   quantity of text . The task takes many different   forms depending on the type of information be-   ing summarized , the modality of the information ,   the type of summary desired and the needs of the   end user . Examples include news headline gener-   ation ( Banko et al . , 2000 ; Zajic et al . , 2002 ; Dorr   et al . , 2003 ; Takase et al . , 2016 ; Matsumaru et al . ,   2020 ) , summarization of social media ( Liu et al . ,   2012 ; Ding and Jiang , 2015 ; Kim et al . , 2019 ) ,   and medical documents ( Schulze and Neves , 2016 ;   Liang et al . , 2019 ; Adams et al . , 2021 ) .   In many settings , users encounter information   progressively instead of all at once . For instance , news stories are revised as events unfold ( Tannier   and Moriceau , 2013 ) , social media streams evolve   as people post content ( Tarnpradab et al . , 2021 ) ,   and biomedical texts are revised as clinical trial   results emerge ( uptodate , 2021 ) . In such dynamic   settings , existing summaries should be updated as   new information becomes available . To address   this , we could in principle leverage static summa-   rization systems for generating a summary of the   underlying content at any given point in time . How-   ever , a more natural approach would be to produce   a new summary based on what the reader already   knows and what content changed .   Consider the case of a news article being updated   as events unfold ( Figure 1 ) . The article first reports   that a man is charged with stealing an ice cream   van , and the article is later updated when the man   admits to the crime . By the time the article is   updated , the reader already knows what was stolen ,   who was charged , and where it happened . At this   point , the reader is most interested in what changed ,   namely the admission of guilt . In the case of news   articles , the new headline must both convey critical   new information and provide a holistic overview   for readers unfamiliar with the story . Updating a   summary instead of wholesale replacement falls   outside the scope of static summarization systems .   To address these shortcomings , we envision a   summarization system that combines an existing   summary with information updates . More con-   cretely , following prior work of using headlines as   article summaries ( Graff et al . , 2003 ) , we consider   the task of news headline generation . We instead   propose updated headline generation , which en-   tails updating headlines based on changes to the   content of the article . In this work , we make the   following contributions:6438   •Introduce updated headline generation as a   model for contextual , dynamic summariza-   tion , and support the task with the release   of the Headline Revision for Evolving News   dataset ( HREN ) , a subset of the NewsEdits   corpus ( Spangher and May , 2021 ) consisting   of contiguous article versions .   •Evaluate the contribution of different types of   information – previous headline , edits to the   article body – to a model that makes updates   to an existing news headline .   •Conduct a human evaluation demonstrating   that leveraging this additional context leads   to headlines which are as factual as standard   headline generation models , while applying   fewer unnecessary edits .   •Perform an error analysis to determine which   types of headline updates are addressed by our   model , and what challenges remain .   2 Updated Headline Generation   A news article consists of a body ( B ) and a head-   line ( H).Headline generation ( Banko et al . , 2000 ;   Zajic et al . , 2002 ; Dorr et al . , 2003 ; Takase et al . ,   2016 ; Matsumaru et al . , 2020 ) asks a system to   consider Band produce H. We propose updated   headline generation as a modification of this task .   A system receives an existing article ( B , H ) and   an updated version of the article body ( B ) . The   goal is to update Hto produce a new headline   ( H ) that reflects important new information in B.   This task introduces several challenges . First ,   a system must identify the most critical new in-   formation in B. Changes to the article can be   small or very significant , and it must determinewhich of these changes , if any , should be reflected   in the headline . Second , it needs to consider how   to modify H. Oftentimes a revision to an article   will preserve most of the structure of H , even if   a completely rewritten headline might convey the   same information . New information should be re-   flected in an updated headline with minimal edits ,   for the sake of continuity and minimizing cogni-   tive load on a reader who is following an evolving   story . Third , there are different types of updated   stories that each require a different style of headline   update . Stories can be updated as the underlying   event progresses ( e.g. , criminal investigations , natu-   ral disasters , voting on legislation or appointments ,   live events ) , new or corrected information becomes   available ( e.g. , number of people injured following   an accident ) , or public figures react to the event   ( e.g. , political figure commenting on a situation ) .   See Table 1 for examples .   3 Dataset   The NewsEdits ( Spangher and May , 2021 ) cor-   pus contains articles with revision histories derived   from 22 wires : 5 from News Snifferand the re-   mainder from Twitter accounts powered by Diff-   Engine .It consists of over one million articles   with 4.6 million revisions . In this work , we fo-   cus on the 5 English language wires from News   Sniffer ( Washington Post , NY Times , Independent ,   BBC , Guardian ) , as we found them to have cleaner   revision histories .   From the revision history of a given article , we   extract body - headline pairs by examining consec-   utive versions , ( B , H ) , ( B , H ) , resulting   in examples of the form { ( B , H ) , ( B , H ) } . We6439   exclude cases without a change in the body , and   group examples into two different classes : posi-   tive – examples where the headline is updated ( i.e. ,   H̸=H ) – and negative – the headline remains   unchanged ( i.e. , H = H ) . We observed that the   headline change associated with a particular body   change sometimes occurred in the subsequent revi-   sion ( not contemporaneous ) . So , we also include   positive examples which have the following prop-   erty across three consecutive revisions : only the   body is changed between the first and second ver-   sions and only the headline is changed between the   second and third versions , i.e. , ( B , H)→(B ,   H)→(B , H ) . We do not include ( B , H)→   ( B , H ) as a negative example for such cases .   To avoid spurious positive examples , we tried   removing versions that were incorrectly paired to-   gether , or where the headline change was trivial .   This process produced a dataset of 144,218 positive   and 794,372 negative examples . Even after filtering   by heuristic , we found that many of the remaining   headline changes still do not reflect a substantive   update to the article . These include purely stylistic   changes , embellishments , and rephrasings .   To filter such cases , we develop a classifier   which is trained to determine whether Hneeds   to be updated based on the changes between B   andB. The classifier achieves 51.9 F1 , indicat-   ing that this is a challenging problem ; the training   and evaluation data are silver - labeled , and noisy .   We filter the remaining positive examples with this   classifier . Empirically , we find that training on this   filtered subset leads to improved performance . We   provide a complete description of the classifier and   attendant experiments in Appendix A.64403.1 The HREN Dataset   After data cleaning and filtering , we obtain the   Headline Revision for Evolving News dataset   ( HREN ) , which contains 69,243 examples with   meaningful headline edits . Descriptive statistics   for each fold are listed in Table 2 . Average num-   ber of tokens per document are broken by source   text type , with B andB ( change only ) de-   scribed in Section 4.2 .   We partition the data into 80/10/10 training , val-   idation , and test splits . While constructing the data ,   we took care to ensure that the underlying articles   from which examples are drawn are disjoint for par-   titions , and that the timestamps corresponding to   examples in the training set strictly precede those   in the validation set , which in turn precede those in   the test set . This ensures that we train on strictly   historical data . Our main experiments use HREN ,   though we include negative examples and filtered   positive examples in some later experiments .   4 Sources of Information   We study the importance of several types of infor-   mation – in the form of baselines and inputs to   models – for updated headline generation .   4.1 Rule - based Baselines   CH : Updated headlines usually copy parts   of the original headline and the overall structure .   For instance , in Figure 1 , 8 of the 9 tokens in the   updated headline come from the original one . So ,   we consider copying Has the prediction .   L-1 : Newsroom style guides dictate that the   most significant information should appear first   ( Siegal and Connolly , 1999 ) . Consequently , the   lead sentence typically includes information that   is mentioned in the headline , as shown in Figure 1 .   This baseline uses the lead sentence of Bas the   prediction for H.   S : Many headlines can be cor-   rectly updated by a simple token replacement , re-   flecting an analogous replacement in the body . Ta-   ble 1 : His “ At least 19Hurt in Tractor - Trailer   and Bus crash on I-64 in Virginia " and a sentence   inB“At least 19people ... " is updated to “ At least   24people " in B , prompting a similar change in   the headline H. So , if a single token ( t=19 ) ap-   pearing in both HandBis replaced with a newtoken ( t=24 ) inB , we form Hby substituting   twithtinH. We only consider single - token   replacements and copy Hif a substitution can not   be made . Note that this is a high precision baseline ,   with 10.8 % of headlines able to be updated by this   heuristic .   4.2 Context Representations   We study various configurations for representing   the input context for training the models .   H : Many headline updates follow a natu-   ral progression of events ( e.g. , “ Lori Loughlin   Expected to Plead Guilty via Zoom in College Ad-   missions Case " →“Lori Loughlin Pleads Guilty   via Zoom in College Admissions Case " ) . In these   cases , knowing the old headline may be sufficient   to predict the subsequent headline . Therefore , we   consider providing only Hto a statistically trained   model .   B : This is the standard headline generation set-   ting in which a model must predict Hgiven B.   H+B : We provide both HandB. Faithful-   ness to the article body is paramount for automatic   headline generation ( Matsumaru et al . , 2020 ) , and   leveraging the original headline removes some of   the burden of generating a headline from scratch .   H+B+B : We provide all available con-   text to the model , so that the model can compare   story versions and consider the old headline during   decoding .   H+B : Asking the model to compare two   full articles may be unrealistic . Instead , we provide   the sequence of edits between BandB :   This sequence consists of edit actions : insert ,   delete , replace , and keep , and are represented in the   format proposed by Panthaplackel et al . ( 2020 ) . We   study whether providing explicit body edits helps a   model learn to apply analogous headline edits .   H+B ( change only ): Rather than feeding   in the full edit sequence , we discard keep spans .   While this removes information about where the   edits are made , it significantly reduces the amount   of context a model must reason about ( Table 2).64415 Models   We evaluate two encoder - decoder models that uti-   lize each of the representations described in Sec-   tion 4.2 . Note that we first preprocess all repre-   sentations using the Penn Treebank tokenizerto   tokenize and split text into sentences and words ,   prior to model - specific preprocessing .   Pointer Networks consist of separate LSTM en-   coders for body and headline text , and these are   concatenated to form the initial states for an LSTM   decoder , equipped with attention ( Vinyals et al . ,   2015 ; Wang et al . , 2016 ) . The hidden states are   concatenated for both attention and copy mecha-   nisms . We posit that this model might be effec-   tive at headline updating , as this task benefits from   copying tokens from the input context ( especially   H ) . We initialize embeddings for the model with   GloVe ( Pennington et al . , 2014 ) .   BART ( Lewis et al . , 2020 ) is a pretrained trans-   former network considered state - of - the - art for sum-   marization . Because we focus on the news domain ,   we consider a version of BART already fine - tuned   for summarization on news articles from CNN-   Daily Mail ( Hermann et al . , 2015).We further   fine - tune on our data , by concatenating inputs into   a single sequence , separated by special tokens ( e.g. ,   < OLD_HEADLINE > , < NEW_BODY > ) .   We evaluate all context representations with both   of these architectures , with the exception of H+   B+Bfor BART , due to limitations in fitting   the entire input context within BART ’s 1024 token   limit . We use beam search with a beam size of 20 to   decode for all models along with bigram blocking   ( Paulus et al . , 2018).These decoding hyperpa-   rameters were found to work well across models   during preliminary experimentation based on an   unweighted average across automated metrics .   6 Experiments   We evaluate with common text - generation met-   rics : METEOR ( Banerjee and Lavie , 2005 ) ,   ROUGE - L ( Lin and Och , 2004 ) , and BLEU-4 ( Pa-   pineni et al . , 2002 ) . Given the editing nature of   our task , we also use two edit - specific metrics :   GLEU ( Napoles et al . , 2015 ) and SARI ( Xu et al . ,2016 ) . SARI measures the average n - gram F1   scores corresponding to edit operations ( add , delete ,   and keep ) . GLEU closely follows BLEU except   that it places more importance on n - grams which   have been correctly changed . We compute sta-   tistical significance at the p < 0.05level using   bootstrap tests ( Berg - Kirkpatrick et al . , 2012 ) .   Rule - Based Baselines : Our results ( Table 3 )   show that rule - based baselines achieve relatively   high performance , even beating the headline gen-   eration setting ( B ) for the pointer network and   BART in some cases . Due to the high lexical over-   lap between HandH , the CHbaseline can   perform well on automated metrics , specifically the   three text - generation metrics . The S   baseline performs slightly better than simply copy-   ingHby making simple substitutions in 10.8 % of   examples , demonstrating improvements in the two   edit - based metrics . The L-1baseline performs   lower than the other baselines on most metrics due   to the discrepancy between the structure and style   of the lead sentence and headlines , with the average   lead sentence length being 36.7 tokens . However ,   the SARI score is substantially higher .   UsingH : For both the pointer network and   BART , providing only Hresults in lower perfor-   mance than CHfor most metrics , except for   SARI , which is designed to evaluate edits . Higher   SARI suggests that these models are able to make   the necessary edits in some cases by guessing the   natural progression of events , without the news   body , such as forecasting the order of events follow-   ing a police investigation ( e.g. , suspect is arrested ,   charged , and then appeared in court ) . However ,   the SARI score is still much lower than if only   Bis provided , as in standard headline generation .   This highlights the importance of the latest version   of the article body in updated headline generation .   Nonetheless , by comparing performance of Band   H+Bacross both architectures , we see the ex-   tent to which Hcan guide headline generation   models in selecting important content and deter-   mining structure for the output . This demonstrates   the inadequacy of framing this as a static headline   generation task . The improvements on edit metrics   are more limited because a model which has access6442   toHwill learn to copy many parts of this input ,   and consequently will make fewer edits .   Using Body Edits : To investigate whether pro-   viding body edits can further improve performance   by helping a model learn to correlate them with H   and apply analogous updates , we consider different   ways of incorporating B. First , in the pointer net-   work , we evaluate performance when just feeding   it in as another input ( H+B+B ) . We observe   no improvement in performance , suggesting that   the model fails to implicitly learn the edits . Next ,   we consider collapsing BandBinto a sequence   of edits ( H+B ) , with which we see a slight   improvement in performance over H+Bfor the   pointer network but a reduction in performance for   BART . We believe this is because BART struggles   to model longer input sequences . When we reduce   the context length and provide only the changes   in the edit sequence ( H+B ( change only ) ) ,   we see an improvement in BART . Note that the   performance of H+B ( change only ) is lower   for the pointer network across most metrics . This   may be due to a lack of pretraining , whereas BART   is already equipped with a strong language model .   Pointer Network vs. BART : While both model   classes perform well , BART models tend to per-   form better overall , demonstrating the value of   BART ’s larger transformer - based architecture and   pretraining . Nonetheless , the benefits of using H   and body edits generalize across both architectures .   We expect that the performance of more recent   summarization models such as PEGASUS ( Zhang   et al . , 2020 ) or SimCLS ( Liu and Liu , 2021 ) will   exhibit a similar trend as BART , but we welcome   evaluation of other large pretrained summarization   models on HREN .   6.1 Human Evaluation   Design We conduct a human evaluation of the   ( more performant ) BART models with the follow-   ing configurations : B , H+B , H+B   ( change only ) . As points of reference , we also eval-   uate the gold headline ( H ) and the output of the   CHbaseline .   Annotators were presented with a visual diffbe-   tween BandBalong with H , and were asked   to judge a candidate updated headline according to   five dimensions on a Likert scale – whether the up-   dated headline was factual , grammatical , appears   to be written in headlinese , focus es on important   changes / information in the updated body ( similar   to the relevance criterion commonly used to evalu-   ate natural language generation models ( Sai et al . ,   2020 ) ) , and makes only minimal edits to the orig-   inal headline . We introduce the last dimension   since we frame our task as an editing task . The un-   derlying idea behind editing is that change should   only be made to be parts that warrant it ; all other6443parts that do not need to be changed should be pre-   served , which is consistent with how humans edit   text ( Panthaplackel et al . , 2020 ) . Additionally , this   is consistent with the task motivation , in which we   expect a reader to interpret the important changes   in a minimally edited headline with less cognitive   effort . We sampled 200 test examples , 143 from   HREN and 57 from the unfiltered sample , result-   ing in 806 unique annotation tasks . Each task   was independently annotated by three paid annota-   tors who were trained on this task – native English   speakers , two of whom were journalism majors .   See Appendices C and D for more details on the   annotation procedure .   Results We present average annotator ratings for   each dimension in Table 4 . Following the human   evaluation analyses in Reiter and Belz ( 2009 ) and   Wiseman et al . ( 2021 ) , we compute statistical sig-   nificance using multi - way ANOV A tests , followed   by Tukey ’s post hoc HSD tests for pairwise statisti-   cal significance ( at the p < 0.05level ) .   Forheadlinese andgrammatical , we find no   significant difference between the approaches ; all   achieve relatively high scores . With respect to fac-   tual andfocus , all approaches perform similarly   except for CHwhich significantly underper-   forms the others , by inaccurately reflecting the state   of matters after the story is updated and failing to   highlight important changes in B. On the other   hand , CHachieves the best performance on   minimal edits by definition ( i.e. , Hhas minimal   edits with itself ) . As expected , without access to   H , the headline generation model ( B ) achieves   the lowest performance on this dimension . Overall ,   we find that the two BART models which also in-   clude Has context performed better , even beating   gold headlines on this dimension . This is unsur-   prising as gold headlines often undergo stylistic   rewrites , in addition to reflecting changes to the   facts of evolving news stories . For example , By-   ron Burger Menu ‘ Reassured ’ Allergy Death Owen   Carey is rewritten as Byron Burger Death : Owen   Carey ’s Family Demand Law Change – the form of   the headline changes in addition to the release of   new information . Although H+B ( change   only ) performs slightly better than H+Bon   automated metrics in Table 3 , we find that they   perform similarly on the five dimensions .   In summary , incorporating Hleads to predic-   tions which make fewer unnecessary edits to the   original headline , while simultaneously maintain-   ing performance with respect to factuality , focus ,   headlinese , and grammaticality of headline genera-   tion models ( on par with gold headlines ) .   7 Discussion   Case Study Table 5 presents BART predictions   for the example in Figure 1 under different con-   text representations . Given only H , the model   predicts an updated headline by speculating about   what might follow a person being charged with a   crime . Using only B , the model generates a head-   line which reflects that a person has admitted to the   crime , but it deviates from the form of the original   headline by inserting the name of the person and   altering terminology . These aspects of the story   have not changed , and should not be changed in the   headline . With H+B , the prediction captures   the major change in the article and better retains   the form of H , but it still makes an unnecessary   change by inserting the person ’s age into the head-   line . Given H+B , the model learns to only   edit the part which is relevant to the body changes ,   but the terminology used to perform the edit ( i.e. ,   pleads guilty ) varies from the article ( admits to the   crime . ) In contrast , H+B ( change only ) is   able to simultaneously perform minimal edits and   correlate edits between the article and headline .   Performance by Edit Level Headlines require   more extensive edits when there are more substan-   tial changes to the article . We perform a fine-   grained analysis to better understand how vari-   ous context representations fare for these different   types of examples . We group examples based on6444   the Jaccard similarity between Hand the gold   H ; low similarity means significant edits , high   similarity means minimal edits . For the BART-   based model we calculate the GLEU score for each   bucket because we find that it is better suited for si-   multaneously evaluating whether appropriate edits   were made along with generation quality .   Figure 2 shows the change in performance at-   tributed to each of the context representations rela-   tive to headline generation ( B ) . For low similarity   values , none of the specialized context represen-   tations outperform standard headline generation .   This suggests that when more substantial edits are   needed , starting from scratch may be best . As   the similarity increases , models which utilize H   perform substantially better . For moderate simi-   larity , having Binstead of body edits performs   marginally better , but this changes as the similarity   score increases , with B ( change only ) leading   to drastic improvements .   Analyzing Attention To better understand how   models explicitly make use of the old headline ,   we analyze how the H+B ( change only )   BART model ’s decoder attends to H. For this ,   we follow the methodology of Vig and Belinkov   ( 2019 ) . Namely , we label each context token by   which span it belongs to : H , one of the edit spans ,   or a span delimiter token ( Other ) . We compute   the average attention paid to each context token   class by the BART decoder across all examples and   layers . We find that even though only 1.5 % of the   context tokens are from H , they attract over 17 %   of the BART decoder ’s attention .   Interestingly , the attention paid to added content   and headline tokens increases in later layers at the   expense of Other tokens ( Figure 3 ) . We posit that   this is because the initial layers need to attend to   special tag tokens in order to understand which type   of span each enclosed token belongs to . This may   also arise from the fact that initially the decoderattends to all tokens relatively uniformly ( 49.9 %   of tokens are Other on average ) . However , even   in the initial layers , the tokens in Hare attended   to more than would be expected by a uniform at-   tention distribution , likely because Htext always   appears near the start of the context . Because of   this , locating the Htokens is less dependent on   identifying enclosing tags – absolute position also   helps .   We also find that the decoder attends to tokens   inHmore often than would be expected under a   uniform attention model , until it needs to refer to   a new piece of information that was added to the   article body . Figure 4 displays the relative attention   paid to each token type for a decoded headline ex-   emplifying this phenomenon . See Appendix E for   additional detail on the decoder attention analysis .   Error Cases Finally , we inspected cases where   annotators assigned very low or high scores . We ob-   serve with Balone , the headline generation model   makes factual errors by mixing up important details   when two similar types of entities are discussed in   the article ( e.g. , mixing up the victim and suspect   of a crime , mixing up locations and dates ) .   Additionally , it makes factual errors by omitting   something important , which drastically changes   the meaning ( e.g. , missing a letter in the acronym   for an organization ) . On the other hand , because   Hoften includes important background that can   be directly copied , we find fewer such factual er-   rors caused by omission for the H+BandH   + B ( change only ) models . Having Halso   helps in maintaining important details ( e.g. , event   location ) and specifying the level of detail that is   needed .   In general , His most useful when there is high   lexical overlap with the lead sentences of B. If the   content is significantly different ( e.g. , the focus of   the article changes ) , it becomes less useful and can   even hurt performance in some cases , since His   likely very different from H. Body edits are most   useful when there are few edits and these edits   can be easily grounded in H. For H+B   ( change only ) , we also noticed errors where the   model incorrectly correlates body edits with H ,   resulting in it erroneously inserting body tokens   that are edited into the headline .   8 Related Work   Summarization : Summarization is a widely   studied topic in the NLP community , with multiple6445   subtopics relevant to our task . For instance , multi-   document summarization ( Barzilay and McKeown ,   2005 ) pertains to generating a unified summary by   synthesizing non - redundant content from multiple   related documents . In our setting , we consider mul-   tiple documents ( i.e. , the old and new versions of   an article ) as well , but we also have an existing sum-   mary , and our task requires reasoning about how   the non - redundant content from the newer version   of the article affects this existing summary . With   update summarization ( Dang et al . , 2008 ) , there is   an older set of documents as well as a newer set of   documents , and the goal is to generate a summary   which captures only added and changed informa-   tion . In contrast , our task aims to incorporate these   changes into an already existing holistic summary .   Natural language edits : Our work focuses on   learning from edits in news articles to apply up-   date and existing headline . Prior work stud-   ies the nature of edits in various texts including   news ( Faigley and Witte , 1981 ; Tamori et al . , 2017)and Wikipedia ( Yang et al . , 2017 ; Faruqui et al . ,   2018 ) . There has also been extensive work on gen-   erating edits for tasks such as grammatical error   correction ( Bryant et al . , 2019 ) , sentence simplifi-   cation ( Zhu et al . , 2010 ) , style transfer ( Fu et al . ,   2018 ) , fact - based sentence editing ( Shah et al . ,   2020 ; Iso et al . , 2020 ) , text improvement ( Tanaka   et al . , 2009 ) , and comment updating based on   source code changes ( Panthaplackel et al . , 2020 ) .   9 Conclusion   In this work , we show that headline generation   models can benefit from access to the past state   of the article . Our proposed model , H+B   ( change only ) , can generate headline predictions   that are statistically tied with gold headlines in   terms of factuality , while making fewer unnec-   essary edits . By releasing the HREN dataset ,   we hope to encourage the community to produce   higher quality tools for aiding journalists , as well   as encourage research in NLP over dynamic texts .   Acknowledgements   Sheena Panthaplackel receives support from   Bloomberg ’s Data Science Ph.D. Fellowship Pro-   gram . We would like to thank Alex Spangher for   sharing an early version of the NewsEdits corpus .   We would also like to thank the Bloomberg AI   group and Lina V ourgidou for early feedback on   this project , and illuminating conversations on the   application of machine learning and natural lan-   guage processing to the newsroom . Finally , we   thank the reviewers for their constructive feedback.6446References644764486449A Classifier Details   Even after filtering by heuristic , we found that   many of the remaining headline changes still do   not reflect a substantive update to the article . To   identify such cases and filter them out , we de-   velop a binary classifier which is trained to de-   termine whether Hneeds to be updated based on   the changes between BandB. The class labels   correspond to the positive and negative classes that   we previously defined . We fit a logistic regression   model trained on a set of 1,041 features . Since   articles pertaining to certain topics are more likely   to evolve ( e.g. , severe weather ) , we learn topic vec-   tors using a 200 - component non - negative matrix   factorization ( NMF ) of article text . NMF top-   ics are derived from TF - IDF bag of word feature   vectors constructed with a vocabulary size 67,950   ( corresponding to a minimum document frequency   of 10 ) . We partitioned tokens into separate docu-   ments based on where they occurred : H , B , B ,   removed tokens , and added tokens ; thus we are   able to learn separate NMF topics for each of these   modalities . We additionally incorporate 41 fea-   tures , many of which are derived from prior work   in classifying edits in Wikipedia articles ( Daxen-   berger and Gurevych , 2013 ; Yang et al . , 2017 ) . De-   scriptions of these features are given in Table A.1 .   We trained this filter by weighting example loss by   the inverse class weight , and tuning an L1 regular-   ization penalty on the validation set ( tuned for F1   score ) .   We use a random sample of 10 % of the examples   belonging to each class for training and evaluation .   Our best classifier achieves 51.9 F1 after tuning the   regularization penalty . This is a difficult problem ,   as both the training and evaluation data are silver-   labeled , and noisy ( i.e. , not all revised headlines   required revision ) . Note that we are only learning   this model in order to additionally clean the data ,   and operate under the assumption that this classifier   will learn that more extreme body edits warrant a   headline update .   During inference , we predict the positive label   if the probability is above a threshold of 0.7 and   the negative label otherwise – tuned to maximize   F1 on the validation set . We compare this model tomajority and random classifier baselines ( averaged   across three runs ) . Additionally , since there is a   strong correlation between the content mentioned   in the headline and the lead sentences , we include   baselines which predict the positive label if there is   a change to the lead-1 , lead-3 , or lead-5 sentences   ( Table A.3 ) . In spite of outperforming all baselines   on F1 and accuracy , the relatively low F1 score   underscores the difficulty of this problem , as there   are many reasons why a headline may need to be   updated : based on editorial whim , stylistic concern ,   or other cosmetic changes that are not grounded in   a change to the underlying facts of the article .   A.1 Feature Weights   Only 87 out of 1,041 had non - zero weight , a   byproduct of training with an L1 regularization   penalty . The following features had high posi-   tive weight : Has change in lead-1 , Has change   in lead-5 , lexical overlap between Hand edited   body tokens , lexical overlap between Hand re-   moved tokens in lead-1 , lexical overlap between   Hand removed body tokens , wire = NY Times , and   ratio of unique added tokens in the lead-1 . On the   other hand , the features which had the most nega-   tive weight were : wire = BBC , wire = Guardian , and   CS(H , B ) .   The majority of these follow our intuition . For   example , a change in the lead sentence(s ) means a   headline update is more likely ; if HandBarenot   similar , then Hlikely needs to be updated to better   reflect B. With respect to the sources , New York   Times headlines tend to be edited more often than   other sources , and BBC and The Guardian tend to   have more examples with body - only updates . We   included the source as a feature to account for the   effect of different newsrooms as well as the process   used to collect article updates for those particular   sources .   We also explored which topics had high pos-   itive / negative weights . We find the topic with   the highest weight corresponded to { ’ arrested ’ ,   ’ charged ’ , ’ old ’ , ’ murder ’ , ’ suspicion ’ , ’ custody ’ ,   ’ magistrates ’ , ’ bail ’ , ’ appear ’ , ’ aged ’ } ( shown here   by the ten tokens in this topic vector with highest   weight ) . This aligns with our observation that head-   lines for articles tracking criminal investigations   are updated frequently in our corpus . The following   topic had a large negative weight : { ’ send ’ , ’ com-   ments ’ , ’ conditions ’ , ’ pictures ’ , ’ 100 ’ , ’ yourpics-   bbccouk ’ , ’ terms ’ , ’ text ’ , ’ upload ’ , ’ file ’ } . This6450   topic represents metadata that is often added or re-   moved in articles which usually have no impact on   the headline , since they are unrelated to the arti-   cle ’s content . Below , we list the all topics with a   positive weight , with the specific document type   for each indicated in parentheses .   •arrested , charged , old , murder , suspicion , cus-   tody , magistrates , bail , appear , aged ( added ,   removed , B , B )   •officers , officer , ipcc , policing , force , consta-   ble , chief , armed , pc , taser ( added , B )   •maduro , venezuela , chavez , opposition , presi-   dent , venezuelan , caracas , assembly , capriles ,   hugo ( B )   •incident , woman , scene , bst , area , old , anyone ,   house , injuries , street ( added , removed , B ,   H )   •israel , israeli , netanyahu , jewish , jerusalem ,   minister , jews , prime , palestinians , israelis   ( B )   •korea , north , korean , south , pyongyang , mis - sile , jong , seoul , test , sanctions ( B )   •report , committee , review , found , recommen-   dations , commission , findings , published , con-   cluded , evidence ( removed )   •gas , fracking , shale , drilling , cuadrilla , explo-   sion , site , energy , coal , natural ( removed )   •ship , coastguard , rescue , boat , search , vessel ,   crew , helicopter , coast , missing ( B )   •prices , price , market , house , average , fuel ,   cost , costs , nationwide , petrol ( removed )   •russia , russian , putin , moscow , kremlin ,   vladimir , russians , sanctions , crimea , soviet   ( B )   •india , indian , modi , delhi , singh , mumbai ,   kashmir , hindu , gandhi , bjp ( B )   •burma , suu , kyi , aung , myanmar , san ,   burmese , nld , military , democracy ( B )   •assange , embassy , sweden , wikileaks ,   ecuador , swedish , extradition , julian , asylum ,   arrest ( B )   •company , business , firm , executive , compa-   nies , chief , shareholders , shares , profit , profits   ( removed )   •french , france , paris , hollande , sarkozy , mali ,   calais , president , francois , nicolas ( H )   •state , islamic , governor , group , officials , de-   partment , airstrikes , fighters , official , isil ( B )   •madeleine , mccann , portuguese , missing ,   search , portugal , murat , disappearance , luz ,   praia ( B )   The topics with a negative weight :   •send , comments , conditions , pictures , 100,6451   yourpicsbbccouk , terms , text , upload , file   ( added , removed )   •press , associated , copyright , redistributed ,   rewritten , material , reserved , broadcast , 2016 ,   published ( B )   •music , festival , band , album , singer , song ,   show , songs , concert , fans ( B )   •editors , picks , commentary , inbox , delivered ,   top , morning , day , news , subscribe ( removed ,   added , B )   •gas , fracking , shale , drilling , cuadrilla , explo-   sion , site , energy , coal , natural ( H )   •storm , hurricane , winds , mph , coast , typhoon ,   hit , tropical , power , cyclone ( B )   •dr , research , study , researchers , brain , univer-   sity , scientists , cells , science , disease ( B )   •prices , price , market , house , average , fuel ,   cost , costs , nationwide , petrol ( B )   •film , films , actor , movie , best , star , award ,   director , actress , hollywood ( B )   •cent , pound , poll , sterling , billion , million ,   survey , since , around , according ( B )   •immigration , home , immigrants , office , mi-   gration , illegal , deportation , asylum , visa , net   ( B )   •park , choi , south , site , festival , parks , parking ,   national , lee , seoul ( B )   •refugees , refugee , asylum , seekers , camp , syr-   ians , camps , countries , aid , syrian ( B )   •bank , rbs , barclays , lloyds , banking , hsbc , cus-   tomers , england , co , carney ( B , H )   •news , hacking , murdoch , phone , coulson , edi-   tor , brooks , world , newspaper , paper ( B )   •company , business , firm , executive , compa-   nies , chief , shareholders , shares , profit , profits   ( B)•rates , rate , interest , fed , economy , unemploy-   ment , reserve , federal , percent , bank ( B )   •flooding , flood , rain , river , floods , heavy ,   flooded , homes , environment , weather ( B )   A.2 Value of Filtering the Train Set   To study the impact of using the classifier to filter   the training data , we also compare with training and   evaluating on unfiltered data , particularly unfiltered   positive examples ( H̸=Hbut could include cos-   metic updates unrelated to body changes ) and un-   filtered positive + negative examples ( additionally   including examples where H = H ) . Note that the   training , validation , and test sets for these use the   same date cutoffs listed in Appendix B. We provide   the sizes of these specialized datasets in Table A.4 .   In Table A.6 , we evaluate the effect of training on   these three differently filtered datasets .   First , by comparing performance between train-   ing on unfiltered positive+negative and unfiltered   positive , especially with respect to edit metrics   ( GLEU and SARI ) , we show that a generation   model can not easily differentiate between posi-   tiveandnegative examples , to identify when to   make edits . Now , we compare training on unfil-   tered positive and the filtered positive ( i.e. , HREN )   datasets . We find that training on HREN achieves   the best headline generation performance overall ,   even for the unfiltered positive test set , highlighting   the value of training on this cleaner subset .   We also find that the examples scored positively   by the classifier are less likely to correspond to   purely stylistic headline rewrites than unfiltered   examples . See Table A.7 for headlines of test ex-   amples that were passed or rejected by the classifier ,   versus examples that had some textual change to   the headline but were not filtered by the classifier.6452   The examples in Table A.7 correspond to   examples that would have been considered posi-   tive examples in HREN , if we had not filtered by   classifier . Unlike the filteredexamples , which   predominantly correspond to substantive changes   in the article , there are several instances of headline   changes in that are stylistic in nature .   A.3 Filter & Generate Pipeline Evaluation   Although we primarily use this classifier for fil-   tering , we believe it can be useful in a headline   udpating pipeline for determining when a headline   update is necessary , as not all body changes warrant   corresponding headline changes . Here we conduct   a preliminary analysis of this . Namely , for the   unfiltered positive+negative test set , we compare   how training on the various configurations from   Table A.5 compares to a pipelined approach .   In the pipeline , examples are first passed through   the classifier , and if the probability of the positive   label is below 0.7 , His simply copied as the pre-   diction for H. Otherwise , we use the BART H   + B ( change only ) model trained on HREN ’s   training set and take its output as the prediction for   H.   Pipeline performance is displayed in Table A.6 .   In addition to the generation metrics described in   Section 6 , we also evaluate performance with re-   spect to classifying whether Hneeds to be up-   dated , where we treat a predicted headline that is   not identical to Has implicitly predicting the pos-   itive label . We find that pipelining outperforms   all generation models alone , regardless of the data   they were trained on . While training on unfiltered   positive+negative examples achieves comparable   performance on generation metrics , it performs   very poorly on classification , as it results in the   model not learning to make edits for almost all   positive examples .   Human Evaluation on Unfiltered Examples Ta-   ble A.8 contains the results from the human eval-   uation on both the Pos only dataset in addition   to pipelined systems on the unfiltered test set us - ing difference headline generation models . On the   unfiltered test examples , the only significant in-   teraction is for minimal edits , and Tukey ’s HSD   identifies both H+B ( change only ) and C   Has being rated higher than B(p= 3.3e−4   andp= 1.1e−2 , respectively ) . Note that 93 % of   these examples are negative ( i.e. , H = H ) , and   so it is not surprising that there is little difference   in how competing model predictions are scored ; on   this subset , most models copy H.   B Time - Based Partitioning   The time frames used for partitioning the classifier   data are given below :   •Train : 08/29 - 2006 11:30 - 09/01/2017 19:00   •Valid : 09/01/2017 20:00 - 01/24/2019 11:15   •Test : 01/24/2019 12:15 - 01/14/2021 23:38   The same date cutoffs are used for partitioning   generation task as well , except that the minimum   date for the test set is set to March 1 , 2019 . This is   to avoid potential contamination between the data   used to pretrain BART ( Lewis et al . , 2020 ) , the   pretrained transformer we finetune in many of our   experiments . BART pretraining data includes CC-   Newsarticles crawled between September 2016   and February 2019 . The specific version of BART   we use in our work was originally fine - tuned for   summarization on CNN - Daily Mail , consisting of   news articles before April 2015 . Therefore , we do   not expect there to be any overlap with our test set ,   in terms of stories tracking the same events .   C Human Evaluation Design   Annotators were presented with a diff between   BandBalong with H , and they were asked   to judge a candidate updated headline on five di-   mensions using a Likert scale . Following estab-   lished guidelines in evaluating generated text ( Van   Der Lee et al . , 2019 ) , the first two dimensions corre-   spond to whether the candidate headline is factual6453Score Old Headline New HeadlineThree hurt as car strikes buffalo Man rescued as car hits buffalo   Syria conflict : Peace talks due to begin in   Astana , KazakhstanSyria conflict : Peace talks begin in Astana ,   Kazakhstan   Israeli woman killed as Palestinian stabbings   add to escalating violenceIsraeli woman and soldier killed in Pales-   tinian stabbings   Security alert after cash raid Cash box found after Lisburn raid   Santander profits hit by higher PPI compen-   sationSantander confirms profits hit by PPI com-   pensationUN Security Council ’ failing Syrian people ’ Syria crisis : UN Security Council ’ failing   victims ’   Ikea relaunches furniture recall after child   diesIkea US relaunches furniture recall after   child dies   Mali ’s Festival au Désert cancelled amid   fears of extremist violenceMali cancels return of famous music festival   after al - Qaida attack   European governments refuse to follow   Trump on status of JerusalemEurope tells Netanyahu it rejects Trump ’s   Jerusalem move   Graves exhumed in hunt for missing mother   Natalie PuttGraves dug up in hunt for missing mother   Natalie PuttChile tycoon ’ wins ’ first round Chilean tycoon wins first round   Helen Bailey murder detective charged with   stealing £ 9,000Helen Bailey murder detective charged with   stealing £ 9,000 from a safe   HSBC shares down as full year profit falls   62%HSBC shares down as annual profit falls 62 %   Paddy Power ’s Oscar Pistorius ad to be with-   drawn with immediate effectPaddy Power ’s Oscar Pistorius ad to be   pulled after record 5,200 complaints   Bank of England keeps interest rates on hold Pound jumps as Bank of England hints at   rate rise   andgrammatical . Based on the intuition that as   few changes as possible should be made to the orig-   inal text for such editing tasks ( Dahlmeier et al . ,   2013 ) , our third dimension corresponds to minimal   edits . Next , given that our task pertains to updat-   ing headlines for evolving news stories , we want   to ensure that the candidate headline focuses on   the important changes / information in the updated   version of the article , which we refer to as focus . Fi-   nally , since the structure and phrasing of headlines   often deviate from other forms of text ( Straumann ,   1935 ; Mårdh , 1980 ) , we aim to evaluate whether   the candidate headline is brief and uses language   the way that a typical headline would . We call this   last dimension headlinese .   We select 200 examples for this study , with 143   being randomly sampled from HREN . The remain-   ing 57 are randomly sampled from 83,545 unfil - tered examples , consisting of 14.4 % positive and   85.6 % negative examples which fall within the   same date ranges of HREN ’s test set ( and could pos-   sibly have overlap with HREN as well ) . Because   we trained generation models only on the filtered   training set , during inference we pipelined these   generation models with our classifier , copying the   Hif the example did not meet the threshold of 0.7 ,   otherwise the headline predicted by the generation   model was used . We report results separately for   the two test sets .   Each candidate headline was completed by 3   unique annotators . All annotators were native En-   glish speakers familiar with news headlines from   major US and UK papers , and two received degrees   in journalism . Annotators were financially compen-   sated with an hourly rate above the minimum wage   for their location . When more than one model6454   made identical predictions , we attributed each an-   notator ’s judgments to all models that would have   generated that prediction .   To avoid using untrained small batch annotations   for human evaluation of NLG models ( Clark et al . ,   2021 ) , we worked closely with annotators and en-   gaged in a round of remediation on 75 examples   prior to running this study , to make sure the instruc-   tions were clear . Note that our task is grounded in   an actual news article , making it easier to discrim-   inate between clearly misleading / false headlines ,   rather than tasks where a model can freely draft   text ( e.g. , draft a work of fiction ) .   Inter - Annotator Agreement Table C.9 displays   the % agreement between annotators for each item .   There was a strong bias toward scoring most items   with 5 ( Strongly agree ) , which partially drives the   strong agreement rates . This is underscored by   lower correlation coefficients between annotators   ( Table C.10 ) . Grammatical andheadlinese have   low correlation coefficients as there is near unani-   mous agreement for this item , with only a few ex-   amples available to provide signal for ranking . For   example , only 16 of 3000 unique annotations were   scored lower than 5 for grammatical . Conversely ,   annotators achieved relatively low inter - annotator   agreement on minimal edits , but achieve high rank   correlation .   Statistical Significance In order to test for statis-   tical significance , we ran multi - way ANOV As for   each Likert item with headline prediction model   ( gold , Copy H , B , H+B , H+B ( change   only ) ) , example ID , and annotator ID as indepen - dent variables . Separate tests were run for the un-   filtered and the positive only ( filtered ) test sets . If a   statistically significant effect was found for model   at the p < 0.05level , we ran Tukey ’s post - hoc   HSD test to identify which models tended to be   rated differently from each other .   D Human Evaluation Guidelines   Figure D.1 displays an example of the task interface   for the human evaluation . Below are the exact   guidelines provided to annotators , as recommended   by Schoch et al . ( 2020 ) .   Overview   News articles are often updated after they are pub-   lished online . When facts are corrected , or new   facts added to the news article , the headline may   also need to be updated to reflect those changes .   In this task , you will be shown an original English   news article , the original headline , and all revisions   made to the article body . Given a headline for the   revised article , your task is to mark how strongly   you agree or disagree with whether the updated   headline :   • Is factually correct   • Is free of typos and is grammatical   •Focuses on important changes / information in   the updated version   •Makes as few changes as possible to the origi-   nal headline   •Is brief and uses language the way that a typi-   cal headline would ; looks like " headlinese"6455Raw Binned   1 & 2 1 & 3 2 & 3 1 & 2 1 & 3 2 & 3   factual 86.5 90.1 83.6 93.9 92.8 93.2   focus 70.0 56.7 56.1 89.3 84.0 88.0   minimal edits 65.1 53.3 56.0 89.2 83.1 82.3   grammatical 98.5 95.8 96.7 99.0 98.6 98.9   headlinese 96.5 95.0 93.5 98.3 97.8 98.0   1 & 2 1 & 3 2 & 3   factual 0.266 ( 1.2e-14 ) 0.135 ( 9.3e-5 ) 0.180 ( 1.4e-7 )   focus 0.174 ( 2.2e-7 ) 0.027 ( 0.44 ) 0.156 ( 8.0e-6 )   minimal edits 0.751 ( 9.5e-131 ) 0.712 ( 5.3e-119 ) 0.651 ( 1.1e-106 )   grammatical -0.006 ( 0.87 ) 0.047 ( 0.18 ) -0.009 ( 0.80 )   headlinese 0.057 ( 0.10 ) 0.096 ( 6.3e-3 ) 0.047 ( 0.18 )   Steps   1.Read through the original news article and   headline on the lefthand side , and the revised   article body on the righthand side . Pay partic-   ular attention to what revisions were made to   the article , as well as the content in the orig-   inal article . Text that was removed from the   original article will be highlighted in red , text   that was added to the revised article will be   highlighted in green , and substitutions will be   highlighted in yellow .   2.After reading the original and revised news   articles , consider the candidate headline for   the revised article and rate how strongly you   agree with the statements :   ( a)Is factually correct . A headline should   never state facts that are not supported   by the body of the news article , either ex-   trapolations or clearly contradicting the   news body .   ( b)Is free of typos and is grammatical .   A good headline should not contain ty-   pographical errors or clear grammatical   mistakes .   ( c)Focuses on important   changes / information in the updated   version . If there are any critical changes   to the new version of the story , the head-   line should highlight these changes . If   only minor changes were made to the   article , then the new headline should fo-   cus on the important information in thearticle overall .   ( d)Makes as few changes as possible to   the original headline . It is also impor-   tant that the new headline preserves the   structure of the original headline as much   as possible . In other words , a good re-   vised headline should make as few edits   to the original headline as possible . This   is most important if there were only mi-   nor changes to the article .   ( e)Is brief and uses language the way   that a typical headline would ; looks   like " headlinese " . English headlines   are written in a unique form of language   called " headlinese " . Some hallmarks of   headlinese are omission of articles like   " a " or " the " , constructions like " Parlia-   ment to pass bill " for an event that is   expected to occur in the future , and gen-   erally keeping headline as short as possi-   ble . A good headline should look like it   is written in headlinese .   3.You should judge how strongly you agree with   each of the above statements on a scale of   Strongly disagree , if the statement is certainly   wrong , to Strongly agree , if you are sure it is   correct .   4.Write any additional comments or questions   about the example in the comment box . You   should use the comment box if you think this   example is malformed or there were prob-   lems in processing , if there is a problem with   the headline that is n’t captured by your judg-6456   ments , if you are uncertain about your judg-   ments , or for any other reason .   5.Click the Submit button in order to record   your choices and move on to the next task .   Tips   •You should not try to find these articles using   a search engine . Make your decisions based   only on the information presented in the task .   This is especially important since these partic-   ular news articles you will annotate evolved   over time , and different versions may have   different headlines .   •For most examples , you do not need to read   both versions of the article in detail . Reading   the first few paragraphs and looking at what   changed between versions is usually sufficient   to judge the revised headline . Annotating ex-   amples in this way is perfectly acceptable .   •The fact that the candidate headline is lower-   cased and tokenized should not influence your   judgment . All headlines are lowercased , split   up into words / punctuation , and then separated   by spaces as part of our preprocessing .   •Only a small portion of the news article will   be visible , so that you do not need to scroll   very far to view the questions . You can scrollwithin the story box to view the rest of the   article .   •You may see the same example with a similar   candidate headline you annotated before . This   is not an error , but rather , the prediction of   a model that happened to be similar to one   before .   •Use the Comments box for any additional   comments .   •You should not use Internet Explorer for com-   pleting these tasks .   E BART Attention   Which tokens are most important for headline   rewriting ? Understanding which words are at-   tended to by a neural network with multiple layers   of multi - headed attention is , needless to say , diffi-   cult . Here we follow the method proposed in Vig   and Belinkov ( 2019 ) and aggregate attention across   classes of context token types to reduce the num-   ber of attention distributions . We investigate the   strongest headline rewriting model in all analyses ,   theH+B ( change only ) BART model , and   only consider the attention heads for the decoder   network . We focus solely on the decoder network   as we would like to determine which sections of   the context were attended to most by the network6457as different tokens are ( greedily ) decoded .   We label all tokens in the context by the span   they occur in . A token can either belong to the   Headline , or a Delete ( removed without replace-   ment from B),Replace - Old ( replaced token from   B),Replace - New ( substitute token added in B ) ,   orInsert ( Btoken without analogue in B ) edit   span . Because this model operated only on body   text that differed between article versions , we have   no tokens which were present in both the old and   new article bodies . Special tokens indicating the   start or end of different spans were assigned the   Other type to ensure a valid probability distribu-   tion across span types .   Corpus - level Analysis We first investigate   whether particular layers / heads are biased towards   particular span types . We compute the average at-   tention placed by a head , α , across the entire corpus   for a particular span type by :   P(span ) = XXXα 1[c = span ]   XXXXα 1[c = S ]   ( 1 )   where Xis the set of examples , cis the list of   span types for each context token , his the list of   tokens in H , Sis the list of span types , and α   is the attention placed on context token iwhile   decoding token jforH.   Figure E.2 shows the mean attention paid to   different span types . Headlines and added con-   tent – Replace - New and Insert – are heavily at-   tended to , whereas removed tokens are less im-   portant to the BART decoder . This trend is even   more pronounced if we plot the difference between   mean attention and what one would expect from   an attention head that paid attention to every to-   ken equally ( Figure E.3 ) . Across the validation set ,   only 1.5 % of tokens are Headline , 5.8 % Delete ,   10.6 % Replace - Old , 19.8 % Replace - New , 12.3 %   Insert , and 49.9 % Other .   In addition , the amount of attention paid to added   content and headline tokens increase in later layers   at the expense of Other tokens ( Figure E.4 ) . We   posit that this is because the initial layers need to   attend to special tag tokens in order to understand   the span type of enclosed tokens . It may also just   arise from the fact that initially the decoder attends   to all tokens relatively uniformly ( 49.9 % of tokens   are Other on average ) . Even in the initial layers   though , Htokens are attended to more than would   be expected by a uniform attention distribution ,   likely because that text always appears near the   start of the context , and thus identifying enclosing   tags is less critical .   Attention Anecdotes We also plot the average   attention paid by the decoder to different span types   while decoding individual examples . In this case ,   we aggregate attention over a fixed context , c , for   decoded token jby :   P(span ) = XXα 1[c = span ] XXXα 1[c = S](2 )   where Ais the set of 192 attention heads in the   BART decoder , αcorresponds to a single decoder   attention head , and αis the attention paid to   token iwhile decoding token jinH. Figure E.5   displays a handful of examples where the decoder   is attending to tokens that were either copied from   H , or new replacement tokens sourced from B.   F Sample Output   We provide sample output from BART generation   models in Tables F.11 - F.13.6458   G Reproducibility Checklist   We supplement details provided in the main pa-   per regarding aspects of the reproducibility check-   list . We provide automated metrics on the valida-   tion set in Table F.14 . For pointer networks , we   select hyperparameters based on random search .   We explored values for dropout between 0.0and   0.8 , learning rate between 10and10 , number   of encoder layers { 2 , 3 } , number of decoder lay-   ers { 1,2,3 } , and hidden dimension { 32 , 64 , 128 ,   256 } . After 8 such configurations , we select the   best ones based on performance on the validation   data : dropout rate = 0.333 , learning rate = 0.00099 ,   encoder layers = 2 , decoder layers = 3 , hidden size   = 64 . We use a batch size of 16 . We specifically   useglove.6B.300d.txt GloVe vectors to initialize   embeddings . For BART models , we use a batch   size of 8 , 50 warm - up steps , a weight decay of 0.01,and we fine - tune up to 5 epochs ( with fine - tuning   taking approximately 6 hours on a single NVIDIA   V100 GPU ) . During inference , we use beam search   with beam width = 20 for both models , after con-   sidering all values between 1 and 20 . We use 20 as   it achieved the best performance on the validation   set while completing in reasonable time.645964606461