  Indira SenMattia SamoryClaudia WagnerIsabelle AugensteinGESIS – Leibniz Institute for the Social SciencesRWTH Aachen UniversityUniversity of Copenhagen ;   Abstract   Counterfactually Augmented Data ( CAD )   aims to improve out - of - domain generalizabil-   ity , an indicator of model robustness . The   improvement is credited to promoting core   features of the construct over spurious arti-   facts that happen to correlate with it . Yet ,   over - relying on core features may lead to un-   intended model bias . Especially , construct-   driven CAD — perturbations of core features —   may induce models to ignore the context in   which core features are used . Here , we test   models for sexism and hate speech detection   on challenging data : non - hateful and non-   sexist usage of identity and gendered terms .   On these hard cases , models trained on CAD ,   especially construct - driven CAD , show higher   false positive rates than models trained on the   original , unperturbed data . Using a diverse   set of CAD — construct - driven and construct-   agnostic — reduces such unintended bias .   1 Introduction   As fully or semi - automated models are increasingly   used for platform governance ( Gorwa , 2019 ; Gille-   spie , 2018 ; Nakov et al . , 2021 ) there are several   questions about their performance and the implica-   tions of model errors ( Gorwa et al . , 2020 ; Gillespie ,   2020 ; Roberts , 2019 ) . Language technologies un-   derpinning these content moderation strategies , es-   pecially models for detecting problematic content   like hate speech and sexism , need to be designed   to ensure several complex desiderata , including   robustness across domains of application as well   as low misclassiﬁcation rates . Indeed , misclassi-   ﬁcations can have a range of repercussions from   allowing problematic content to proliferate to sanc-   tioning users who did nothing wrong , often minori-   ties and activists ( Gray and Stein , 2021 ; Haimson   et al . , 2021 ) . Such misclassiﬁcations are a threat to   model robustness and non - robust models can cause   a great deal of collateral damage . To facilitate model robustness , several solu-   tions encompass improving training data for these   models ( Dinan et al . , 2019 ; Vidgen et al . , 2021 ) ,   such as by training them on counterfactually aug-   mented data ( CAD ) . CAD , also called contrast   sets ( Gardner et al . , 2020 ; Atanasova et al . , 2022 ) ,   are obtained by making minimal changes to ex-   isting datapoints to ﬂip their label for a particu-   lar NLP task ( Kaushik et al . , 2019 ; Samory et al . ,   2021 ) . Previous research has established that train-   ing on CAD increases out - of - domain generalizabil-   ity ( Kaushik et al . , 2019 ; Samory et al . , 2021 ; Sen   et al . , 2021 ) . Sen et al . ( 2021 ) explores charac-   teristics of effective counterfactuals , ﬁnding that   models trained on construct - driven CAD , or CAD   obtained by directly perturbing manifestations of   the construct , e.g. , gendered words for sexism , lead   to higher out - of - domain generalizability . Previous   research also notes that gains from training on CAD   can be attributed to learning more core features ,   rather than dataset artifacts ( Kaushik et al . , 2019 ;   Samory et al . , 2021 ) . However , it is unclear how   learning such core features can affect model mis-   classiﬁcations , especially for cases where the effect   of the core feature is modulated by context — e.g. ,   how models trained on CAD classify non - sexist   examples containing gendered words . Investigat-   ing this type of misclassiﬁcation can help uncover   unintended false positive bias .   Unintended false positive bias can lead to wrong-   ful moderation of those not engaging in hate speech ,   or even worse , those reporting or protesting it . Such   type of bias is especially concerning in the use of   social computing models for platform governance .   Recent work has shown that AI - driven abusive lan-   guage or toxicity detection models disproportion-   ately ﬂag and penalize content that contains mark-   ers of identity terms even though they are not toxic   or abusive ( Gray and Stein , 2021 ; Haimson et al . ,   2021 ) . Over - moderation of this type , facilitated by   unintended false positive bias , can end up hurting4716marginalized communities even more .   This work . We assess the interplay between   CAD as training data and unintended bias in sex-   ism and hate speech models . Grounding the mea-   sure of unintended bias as the prevalence of falsely   attributing hate speech or sexism to posts which   use identity words without being hateful , we as-   sess if training on CAD leads to higher false pos-   itive rates ( FPR ) . In line with past research , Lo-   gistic Regression and BERT models trained on   CAD show higher accuracy on out - of - domain data   ( higher model robustness ) butalso have higher FPR   on non - hateful usage of identity terms . This effect   is most prominent in models trained on construct-   driven CAD . Our results uncover potential negative   consequence of using CAD , and its different types ,   for augmenting training data . We release our code   to facilitate future research here : .   2 Background   For a given text with an associated label , say a sex-   ist tweet , a counterfactual example is obtained by   making minimal changes to the text to ﬂip its label ,   i.e. , into a non - sexist tweet . Counterfactual exam-   ples in text have the interesting property that , since   they were generated with minimal changes , they   allow one to focus on the manifestation of the con-   struct ; in our example , that would be what makes   a text sexist . Previous research has exploited this   property to nudge NLP models to look at the points   of departure and thereby learn core features of   the construct rather than dataset artifacts ( Kaushik   et al . , 2019 ; Samory et al . , 2021 ; Sen et al . , 2021 ) .   Types of CAD . Sen et al . ( 2021 ) used a   causal inference - inspired typology to categorize   different types of CAD and found that models   trained on certain types of CAD are more ro-   bust . We follow the same typology and distinguish   between — Construct - driven CAD obtained by   making changes to an existing item by acting on   the construct , e.g , on the gendered terms for sexism ,   andConstruct - agnostic CAD obtained by making   changes to general characteristics of an item , such   as inserting negation . Following Sen et al . ( 2021 ) ,   we use lexica to automatically characterize CAD .   3 Datasets and Methods   We use the same experimental setup and notation   as Sen et al . ( 2021 ) , but instead only focus on sex-   ism and hate speech as these are the NLP taskswidely used in text - based content moderation . Ta-   ble 1 summarizes the datasets we use , training on   an in - domain dataset and using two datasets for   testing — Identity Subgroup ( ISG ) which is a sub-   set of the out - of - domain dataset used by Sen et al .   ( 2021 ) and Hatecheck ( HC ) ( Röttger et al . , 2021 ) .   The test sets are described in more detail in Sec-   tion 4.1 . All the in - domain datasets come with   CAD , gathered by crowdworkers ( Samory et al . ,   2021 ) or expert annotators ( Vidgen et al . , 2021 ) in   previous research .   Since previous work has shown that models   trained on CAD tend to perform well on counterfac-   tual examples ( Kaushik et al . , 2019 ; Samory et al . ,   2021 ) , we do not include CAD in any of the test   sets . All datasets contain only English examples .   We use two different families of models : logistic   regression ( LR ) with a TF - IDF bag - of - words repre-   sentation , and ﬁnetuned - BERT ( Devlin et al . , 2019 ) .   We train two types of binary text classiﬁcation mod-   els of each model family on the in - domain data   only — nCF models trained on original data , and   CF models trained on both original data and CAD .   The nCF models are trained on 100 % original data ,   namely , the “ Original ” in the “ Train ” column in   Table 1 . The CF models for hate speech are trained   on∼50 % original data and ∼50 % CAD , sampled   from the “ Train ” and “ Counterfactual ” columns in   Table 1 , respectively . Since only non - sexist CAD   are provided for sexism classiﬁcation , the sexism   models are trained on 50 % original sexist data , 25 %   original non - sexist data , and 25 % counterfactual   non - sexist data ( Samory et al . , 2021 ) .   Based on Sen et al . ( 2021 ) , to unpack the ef-   fect of different types of CAD on model perfor-   mance , we further disaggregate the CAD training   sets , and train models on different types of CAD :   only construct - driven counterfactuals ( CF_const ) ,   only construct - agnostic counterfactuals ( CF_agn ) ,   and equal proportions of both ( CF_mix).Due to   the lack of data and unequal distributions of differ-   ent types of CAD , instead of training on 50 % CAD ,   we train on 20 % for these three types of models .   Training details including model hyperparameters   are described in the Appendix ( Section 9).4717   4 Unintended Bias   Previous research has shown that training on CAD   can improve model robustness , i.e , generalization   to data beyond the training domain ( Kaushik et al . ,   2019 ; Samory et al . , 2021 ; Sen et al . , 2021 ) . Here ,   we take a closer look at one aspect of model ro-   bustness , i.e false positives , and conduct a focused   error analysis inspired by real - world applications   of social NLP systems – particularly the case of   misclassiﬁcation of content with identity terms , an   example of unintended bias . Previous research has   shown that CF models tend to promote core fea-   tures , namely , gender words for sexism and identity   terms for hate speech ( Sen et al . , 2021 ) . One po-   tential consequence of this promotion of identity   features for detecting problematic content could   be an increase in false positives , particularly in in-   nocuous posts that contain identity terms . This   can be especially harmful if the misclassiﬁed posts   happen to be reports or disclosures of facing hate   speech . Our work builds on recent literature that   investigates the performance and misclassiﬁcation   rate of toxicity or hate speech detection models   on test sets containing identity terms ( Dixon et al . ,   2018 ; Borkan et al . , 2019 ; Kennedy et al . , 2020 ;   Nozza et al . , 2019 ; Calabrese et al . , 2021 ) . How-   ever , unlike previous work , we speciﬁcally focus   on the behaviour of models trained on CAD on test   cases with identity terms . We do so since previ-   ous work established that models trained on CAD   tend to learn “ core ” features ( Kaushik et al . , 2019 ;   Samory et al . , 2021 ; Sen et al . , 2021 ) which are   often identity terms for sexism and hate speech   detection — therefore , it is important to uncover   how this increased focus on core features modu-   lates misclassiﬁcations of instances where these   terms are used in a non - hateful context .   4.1 Test Sets for Measuring Unintended Bias   To understand if CF models facilitate this type of   unintended bias , we leverage two tests sets . First ,   we include a subset of the out - of - domain datasets   used in Sen et al . ( 2021 ) which contains both sexist   ( hateful ) and non - sexist ( non - hateful ) posts with   gendered ( identity ) words , called the Identity Sub-4718   group . Second , we include the HateCheck test   suite ( Röttger et al . , 2021 ) , designed to test hate   speech models .   Identity Subgroup in Out - of - Domain Data   ( ISG ) . While Sen et al . ( 2021 ) used the entire out-   of - domain dataset for testing , we distill a subset   of tweets that contain gender and identity terms ,   for sexism and hate speech respectively ( based on   lexica for gendered wordsand identity terms ) ,   calling this dataset the identity subgroup . ISG is   an effective dataset for investigating unintended   bias since it speciﬁcally contains tweets with men-   tions of identities in both hateful and non - hateful   contexts .   HateCheck ( HC ) . HateCheck ( Röttger et al . ,   2021 ) is a functional test suite for hate speech de-   tection models that is inspired by software test-   ing ( Ribeiro et al . , 2020 ) . HateCheck provides   3728 test cases covering 29 different functionalities   such as “ Non - hateful homonyms of slurs ” , making   it a challenging test set for models where many   of the instances include identity terms used in a   non - hateful context . To evaluate the hate speech   models , we use all instances , while for the sexism   models , we use a subset that targets women . To   ﬁnd the subset of instances targeting women , we   reuse the gender word lexica to exclude instances   that do not have gendered words .   4.2 Results   We use false positive rate ( FPR ) to measure unin-   tended bias as our concept of unintended bias in   harm detection systems entails misclassiﬁcations   of non - harmful content containing identity - related   terminology . We contrast measures of FPR with   Macro F1 ( as an overall performance metric ) and   fale negative rate ( FNR ) . We assess the overall per-   formance of models trained on CAD as well as   models trained on speciﬁc types of CAD .   Performance of CAD vs. Original Data . Ta-   ble 2 shows the result of CF and nCF models on   ISG while Table 3 shows the results for HateCheck .   Our results indicate that CF models do , indeed ,   have higher FPR compared to their nCF coun-   terparts , for both sexism and hate speech , while   having a lower FNR . On all examples , CF models   have higher F1 ( e.g. , 88 % vs. nCF model ’s 67 % for   hate speech in Table 3 ) , but higher FPR . The CF   BERT model for hate speech is the exception , hav-   ing a lower FPR compared to its nCF counterpart .   To unpack this discrepancy , we qualitatively as-   sessed cases where nCF models had misclassiﬁca-   tions but CF models did not . These included cases   of positive sentiment towards minorities , e.g. “ I ab-   solutely adore Muslims " . As Sen et al . ( 2021 ) note ,   the majority of CAD generated for hate speech   changes affect words ( 55 % ) , possibly explaining   how the CF BERT model became proﬁcient at cor-   rectly recognizing such instances . This adds to   the evidence that a mixture of different types of   CAD is ideal for aiding models in understanding   the concept of hate speech in a holistic manner .   Performance of CAD Types . We repeat this   analysis with models trained on different types of   CAD , namely , construct - driven CAD ( CF_const ) ,   construct - agnostic ( CF_agn ) , and equal propor-   tions of both ( CF_mix ) for ISG ( Figure 1 ) and HC4719(Figure 2 ) . Figures 1 and 2 show the results for   the BERT models while the results for the logistic   regression models are included in the Appendix   ( Section 7 ) . Overall , complementing the compar-   ison between CF and nCF models , we ﬁnd that   models trained on either type of CAD have higher   F1 than nCF models . However , the ranking be-   tween models trained on different types of CAD   is not clear , especially for ISG . In ISG , For hate   speech , we see that construct - driven ( CF_const )   models demonstrate high FPR . For HC , the high   FPR of models trained on construct - driven CAD   is more pronounced — for both sexism and hate   speech , CF_const models incur a high FPR . Sur-   prisingly , for hate speech , a higher FPR does not   translate to higher F1 , indicating that a combina-   tion of different types of CAD ( CF_mix ) reduces   unintended bias without sacriﬁcing F1 score .   Overall , we ﬁnd that CF models have higher F1 ,   especially models trained on construct - driven CAD   ( for e.g. , the sexism CF_const model for HC ) . A   potential reason for this is that construct - driven   CAD is obtained by editing identity words ; while   identity words indeed co - occur with hate speech   or sexism , they can also have a confounding im-   pact , i.e. , sexism manifests via attacks on gender   identity , however mentioning gender is not always   associated with sexism . Indeed , many minorities   may disclose their experience and identity using   such terms without being sexist or hateful . The   confounding nature of identity terms makes auto-   mated methods all the more vulnerable to unin-   tended false positive bias . Our analysis reveals   that while construct - driven CAD has its uses and is   often easier to generate ( based on the distribution   of different types of CAD ( Sen et al . , 2021 ) ) , we   should use them judiciously . Speciﬁcally , we need   more research on various characteristics of indi-   vidual CAD such as minimality , semantic distance   from the original instance , as well as corpus - level   attributes like training data composition and diver-   sity .   5 Conclusion   Text counterfactuals , drawing from and inform-   ing current developments for causal inference in   NLP ( Keith et al . , 2021 ; Feder et al . , 2021 ; Jin et al . ,   2021 ) can be used for training , testing , and explain-   ing models ( Wu et al . , 2021 ) . We build on research   that explores the former — using counterfactually   augmented data ( CAD ) as training examples , to in - vestigate conditions of model robustness and unin-   tended false positive bias . Performing experiments   on challenging datasets for hate speech and sexism   detection , we ﬁnd that models trained on CAD have   higher false positive rates compared to those that   are not . In addition , models trained on construct-   driven counterfactuals tend to have the highest false   positive rate . Our analysis and results indicate that   while training on CAD can lead to gains in model   robustness by promoting core features , not taking   into account the context surrounding these core   features can lead to false positives , possibly due   to the confounding relationship between identity   terms and hate speech . Future work includes un-   packing the strengths and weaknesses of different   types of CAD by studying their various characteris-   tics , including but not limited to the exact changes   made to derive a counterfactual and their impact   on unintended bias .   6 Ethical Considerations   Constructs like sexism and hate speech detection   are often depicted as neutral or objective , but   they are deeply contextual , subjective , and am-   biguous ( Vidgen et al . , 2019 ; Jurgens et al . , 2019 ;   Nakov et al . , 2021 ) . Promoting features like iden-   tity terms can increase the risk of misclassifying   non - hateful content with such terms , such as dis-   closures or reports of facing hate speech , leading to   unintended bias ( Dixon et al . , 2018 ) that can cause   harm ( Blackwell et al . , 2017 ) . Following up on   this subjectivity and based on recommendations by   Blodgett et al . ( 2020 ) , we motivate our analyses   of unintended bias on normative grounds , situated   in the context of the harms wrought by misclassiﬁ-   cation of content containing identity terms despite   being non - sexist or non - hateful . We acknowledge   that we only study one type of unintended bias and   there are other aspects that require further investi-   gation ( Blackwell et al . , 2017 ) .   Acknowledgements   We thank the members of the Computational So-   cial Science department at GESIS , the CopeNLU   group , and the anonymous reviewers for their con-   structive feedback . Isabelle Augenstein ’s research   is partially funded by a DFF Sapere Aude research   leader grant with grant number 0171 - 00034B.4720References472147224723Appendix   This is the appendix for the paper , “ Counterfactu-   ally Augmented Data and Unintended Bias : The   Case of Sexism and Hate Speech Detection ” . The   appendix contains results of the logistic regression   models trained on different types of CAD ( 7 ) , per-   formance on in - domain data ( 8) , and details for   facilitating reproducibility ( 9 ) .   7 Performance of LogReg Models   trained on different types of CAD   In Figures 3 and 4 , we present th results for logis-   tic regression models trained on different types of   CAD . We note that , similar to results for the CF   and nCF models , logistic regression models have   lower performance than BERT models . Further-   more , we also note that , like the BERT models ,   the logistic regression models trained on construct-   driven CAD ( CF_const ) have high false positive   rates compared to models trained on other types of   CAD .   8 In - domain Performance   In Table 4 , we report the performance of the CF and   nCF models on the in - domain datasets . To ensure   fair comparison with the results in 2 , instead of   computing results on the entire test set , we subset   it in a manner similar to ISG ; i.e. , we retain only   those instances which have identity words for hate   speech and gender word for sexism . The results   are in line with what Sen et al . reported — nCF   models perform better in the in - domain datasets .   We note that even though CF models have lower   F1 score , they have a higher FPR even in - domain .   We report the results of models trained on differ-   ent types of CAD in Table 5 . Notably , the CF_const   models have the highest FPR similar to results on   ISG and HC , but also have the lowest F1 score in   the in - domain subset .   9 Reproducibility   9.1 Compute Infrastructure   For the logistic regression models we used the   scikit learn package ( Pedregosa et al . , 2011 ) and for   ﬁnetuning BERT , we used the Transformers library   from HuggingFace ( Wolf et al . , 2020 ) . All mod-   els were trained or ﬁnetuned on a 40 core Intel(R )   Xeon(R ) CPU E5 - 2690 ( without GPU ) .   9.2 Model Training Details :   Hyperparameters and Time Taken   We preprocess all the data by removing social me-   dia features such as hashtags and mentions . The   hyperparameter bounds for LR models are :   1.stopwords : English , none , English without nega-   tion words   2 . norm : ( ’ l1 ’ , ’ l2 ’ )   3 . C : ( 0.01 , 0.1 , 1 )   4 . penalty : ( ’ l2 ’ , ’ l1 ’ )   while for BERT we use :   1 . epochs:[4 , 5 ]   2 . learning rate : 2e-5 , 3e-5 , 5e-5   For LR , we have 36 combinations over 5 fold   cross - validation , leading to 180 ﬁts , while for   BERT , we have 6 combinations also over 5 fold   CV , leading to 30 ﬁts .   We use gridsearch for determining hyperparam-   eter , where the metric for selection was macro F1 .   Run times and hyperparameter conﬁguartions for   the best performance for all CF ( with randomly   sampled 50 % data ) and nCF models ( RQ1 ) are in-   cluded in Table 6 . The hyperparameters and run   times for the CF models trained on different types   of CAD ( RQ2 ) are in Table 7.4724   9.3 Metrics   The evaluation metrics used in this paper are   macro average F1 , False Positive Rate ( FPR )   and False Negative Rate ( FNR ) . We used the   sklearn implementation of the macro F1 score : . The code for   computing FPR and FNR is included in our code   ( uploaded with the submission )   9.4 Model Parameters   Model parameters are included in Table 8.47254726