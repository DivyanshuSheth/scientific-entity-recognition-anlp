  Jiong Cai , Shen Huang , Yong Jiang , Zeqi Tan , Pengjun Xie , Kewei TuSchool of Information Science and Technology , ShanghaiTech University   Shanghai Engineering Research Center of Intelligent Vision and Imaging   Shanghai Institute of Microsystem and Information Technology , Chinese Academy of Sciences   University of Chinese Academy of SciencesCollege of Computer Science and Technology , Zhejiang UniversityDAMO Academy , Alibaba Group   Abstract   Data augmentation is an effective solution to   improve model performance and robustness for   low - resource named entity recognition ( NER ) .   However , synthetic data often suffer from poor   diversity , which leads to performance limita-   tions . In this paper , we propose a novel Graph   Propagated Data Augmentation ( GPDA ) frame-   work for Named Entity Recognition ( NER ) ,   leveraging graph propagation to build relation-   ships between labeled data and unlabeled nat-   ural texts . By projecting the annotations from   the labeled text to the unlabeled text , the un-   labeled texts are partially labeled , which has   more diversity rather than synthetic annotated   data . To strengthen the propagation precision ,   a simple search engine built on Wikipedia is   utilized to fetch related texts of labeled data   and to propagate the entity labels to them in the   light of the anchor links . Besides , we construct   and perform experiments on a real - world low-   resource dataset of the E - commerce domain ,   which will be publicly available to facilitate   the low - resource NER research . Experimental   results show that GPDA presents substantial   improvements over previous data augmenta-   tion methods on multiple low - resource NER   datasets .   1 Introduction   Data augmentation is an effective solution to im-   prove model performance and robustness , and is   especially useful when the labeled data is scarce . In   computer vision and speech , simple hand - crafted   manipulations ( Zhong et al . , 2020 ; Zhang et al . ,   2018 ) are widely used to generate synthetic data   that preserve the original information . However , when applied to natural language processing ( NLP ) ,   it is challenging to edit a sentence without changing   its syntax or semantics .   There are two successful attempts of applying   data augmentation on sentence - level NLP tasks .   One is manipulating a few words in the origi-   nal sentence , which can be based on synonym re-   placement ( Zhang et al . , 2015 ; Kobayashi , 2018 ;   Wu et al . , 2019 ; Wei and Zou , 2019 ) , random   insertion or deletion ( Wei and Zou , 2019 ) , ran-   dom swap ( ¸ Sahin and Steedman , 2018 ; Wei and   Zou , 2019 ; Min et al . , 2020 ) . The other is gen-   erating the whole sentence with the help of back-   translation ( Yu et al . , 2018 ; Dong et al . , 2017 ; Iyyer   et al . , 2018 ) , sequence to sequence models ( Ku-   rata et al . , 2016 ; Hou et al . , 2018 ) or pre - trained   language models ( Kumar et al . , 2020 ) . However ,   when applied to token - level tasks such as NER ,   these methods suffer heavily from token - label mis-   alignment or erroneous label propagation .   To overcome the issue of token - label misalign-   ment , Dai and Adel ( 2020 ) extend the replacement   from token - level to entity - level with entities of   the same class , which proves to be a simple but   strong augmentation method for NER . Li et al .   ( 2020 ) adopt a seq2seq model to conditionally   generate contexts while leaving entities / aspect   terms unchanged . Ding et al . ( 2020 ) exploit an   auto - regressive language model to annotate entities   while treating NER as a text tagging task . Zhou   et al . ( 2022 ) utilize labeled sequence linearization   to enable masked entity language model to explic-   itly condition on label information when predicting   masked entity tokens . Still , these methods generate   synthetic data , which inevitably introduces incoher-   ence , semantic errors and lacking in diversity .   In this work , we investigate data augmentation   with natural texts instead of synthetic ones . We   are inspired by the fact that professional annota-   tors usually understand the semantics of an entity   through its rich context . However , in low - resource110NER , the semantic information of a specific entity   is relatively limited due to fewer annotations . To   this end , we propose to improve the NER models   by mining richer contexts for the existing labeled   entities . More particularly , we propose a Graph   Propagation based Data Augmentation ( GPDA )   framework for NER , leveraging graph propaga-   tion to build relationships between labeled data   and unlabeled natural texts . The unlabeled texts   are accurately and partially labeled according to   their connected labeled data , which has more diver-   sity rather than synthetic hand - crafted annotations .   Furthermore , not restricted to the existing anno-   tated entities in the training data , we explore exter-   nal entities from the unlabeled text by leveraging   consistency - restricted self - training .   The contributions of GPDA can be concluded :   •We propose a data augmentation framework   that utilizes graph propagation with natural   texts for augmentation , which is rarely inves-   tigated in previous work ( Section 2 ) ;   •We utilize a simple Wikipedia - based search   engine to build the graph with two retrieval   methods ( Section 2.2 ) ;   •With consistency - restricted self - training , we   further make the most efficient utilization of   externally explored unlabeled text ( Section   2.3 ) ;   •By conducting experiments on both public   datasets and a real - world multilingual low-   resource dataset , GPDA achieves substantial   improvements over previous data augmenta-   tion methods ( Section 3 ) .   2 Method   Fig . 1 presents the workflow of our proposed data   augmentation framework . First , we build a graph   between labeled data nodes and unlabeled text   nodes according to their textual similarity . Then ,   the entity annotations are propagated to obtain aug-   mented data . Finally , the marginalized likelihood   for conditional random field ( CRF ) ( Tsuboi et al . ,   2008 ) is applied during the training phase as the   augmented data are partially labeled . Moreover ,   we adopt the consistency - restricted self - training   strategy to further improve the model performance .   2.1 NER with Pure Labeled Data   We take NER as a sequence labeling prob-   lem , which predicts a label sequence y=   { y , · · · , y|y∈Y}at each position for the in-   put tokens x={x , · · · , x } , where Ydenotes   the label set . The sequence labeling model feeds   the input xinto a transformer - based encoder ( such   as BERT ( Devlin et al . , 2019 ) ) which creates con-   textualized embeddings rfor each token . Then a   linear - chain CRF layer that captures dependencies   between neighboring labels is applied to predict   the probability distribution :   ψ(y , y , r ) = exp ( Wr+b )   P(y|x ) = /producttextψ(y , y , r )   /summationtext / producttextψ(y , y , r )   Unified Training Objective Instead of directly   minimizing the negative log - likelihood , we unify   the training objectives in Section 2.1 , 2.2 and 2.3 .   Specifically , we compute the marginal probability   of each token P(y|x)with the forward - backward   algorithm .   α(y ) = /summationdisplay / productdisplayψ(y , y , r )   β(y ) = /summationdisplay / productdisplayψ(y , y , r )   P(y|x)∝α(y)×β(y )   The marginal distributions can be computed effi-   ciently . Given a partially annotated label sequence   y={∗ , . . . , y , . . . , ∗}that∗denotes the label   that is not observed , we can obtain the probability .   Q(y|x ) = /productdisplayQ(y|x)111Method AI Literature Music Politics Science Average   State - of - the - art Approaches   Zheng et al . ( 2022 ) 63.28 70.76 76.83 73.25 70.07 70.84   Hu et al . ( 2022 ) 65.79 71.11 78.78 74.06 71.83 72.31   Tang et al . ( 2022 ) 66.03 68.59 73.1 71.69 75.52 70.99   Baseline w/o Data Augmentation   BERT - CRF 65.06 71.39 78.18 74.46 73.95 72.61   Data Augmentation Approaches   DAGA ( Ding et al . , 2020 ) 66.77 71.15 78.48 73.30 73.07 72.55   NERDA ( Dai and Adel , 2020 ) 70.20 71.28 79.56 75.30 74.37 74.14   GPDA ( sparse retrieval w/o EEA ) 67.14 72.20 79.55 74.96 74.69 73.71   GPDA ( dense retrieval w/o EEA ) 67.76 72.11 77.54 74.86 73.07 73.07   GPDA ( sparse retrieval w/ EEA ) 70.05 72.3480.1675.9575.5574.81   where Q(y|x)is defined as P(y|x)ifyis   observed , otherwise Q(y|x ) = 1 .   The final model parameters can be optimized by   minimizing the following objective :   L(θ ) = −logQ(y|x )   For the pure labeled data D={(x , y ) } ,   we direct set y = yand obtain the loss function .   L(θ ) = −/summationdisplaylogQ(y = y|x )   2.2 NER with Propagated Unlabeled Data   Building Propagating Graph Compared to la-   beled data , large - scale unlabeled natural texts can   be acquired much more easily . We attempt to uti-   lize these natural texts for augmentation by build-   ing a graph between the labeled data nodes and   the unlabeled text nodes according to their tex-   tual similarity . Given a labeled sample ( x , y ) ,   we retrieve its corresponding augmented sentences   { x}via a search engine . For common NER   datasets , the search engine is built on the Wikipedia   corpus with one of the two methods we explore :   sparse retrieval based on BM25or dense retrieval   based on L2 similarity . The top related sentences   will be treated connected to the original labeled   sentence in the graph . Label Propagation While building the graph ,   label propagation is conducted from labeled data   ( x , y)to unlabeled data { x}to gen-   erate partially annotated { ( x , y ) } . To   strengthen the precision , propagation will not hap-   pen unless the anchor text in Wikipedia matches the   labeled entity . By graph propagation , we obtain the   augmented data D={(x , y)}sharing   the same entities but with more diverse contexts .   Along with the original labeled data D , we train   the NER model following the same objective in   Section 2.1 :   L(θ ) = −/summationdisplaylogQ(y = y|x )   2.3 NER with Explored Entity Annotations   To make the most efficient utilization of the ex-   plored annotations in D , we adopt consistency-   restricted self - training . A well - trained model from   Section 2.2 will be utilized to re - annotate the par-   tially labeled augmented data under consistency   restriction . Particularly , an augmented sample   ( x , y)will be re - annotated to ( x,ˆy ) .   Now we have ˆD={(x,ˆy ) } . Along with   the original labeled data D , we train a better NER   model following the objective in Section 2.1 :   L(θ ) = −/summationdisplaylogQ(y = y|x)1123 Experiments   3.1 Dataset   We conduct experiments on the CrossNER ( Liu   et al . , 2020 ) dataset of 5 genres ( AI , Litera-   ture , Music , Politics , Science ) and an anonymous   multi - lingual E - commerce query NER dataset   ( Ecom ) consisting of 3 languages ( English , Span-   ish , French ) Detailed statistics about these two   datasets is provided in the Table 2 .   For CrossNER , the search engine is manually   built on the Wikipedia corpus . While for Ecom , an   off - the - shelf E - commerce search engine is utilized   to build the augmentation graph .   3.2 Results and Analysis   Low - resource NER Tasks As illustrated in Ta-   ble 1 , the proposed GPDA consistently achieves the   best F1 scores across the five genres of CrossNER   and gains an average improvement of 2.2 % over   the baseline BERT - CRF model . It also outperforms   other data augmentation methods , demonstrating   its effectiveness on multi - domain low - resource   NER .   Furthermore , GPDA with Explored Entity An-   notation ( EEA ) strategy achieves 1.1 % higher F1   than GPDA without EEA , suggesting that it is also   crucial to extend unique entities rather then only   diversifying entity contexts in data augmentation .   It can be noticed that GPDA with dense retrieval   performs worse than with sparse retrieval , which   is not intuitive . This may be attributed to dense   retrieval requires careful supervised training in the   target domain , but our pre - trained matching model   is not finetuned . We will leave this part for future   work .   Real - world Low - resource NER Scenarios Ta-   ble 3 shows the F1 results on three languages   from the real - world Ecom dataset . The augmented   data generated by GPDA improves model perfor-   mances for multilingual NER . For specific domain   datasets where high - quality knowledge or texts can   be fetched easily , GPDA are indeed helpful .   Size of Gold Samples We study the impact of   GDPA on different size of gold samples in Fig . 2 .   On the low - resource settings where 10%-25 % gold   samples are available , the improvement is striking   which outperforms the baseline model by at most   37 % .   Case Study Taking a closer look at the aug-   mented cases in Fig . 3 , we notice that GPDA   generates different contexts concerning the entity   " Adobe Creative Suite " . The augmented data gen-   erated by GPDA introduces more diversity to help   reduce overfitting . Different from synthetic data ,   these generated data are all from natural texts so   that there is no need to worry about the coherence   in syntax or semantics .   4 Discussion   Retrieving relevant texts from databases has been   widely used in NLP tasks . RaNER ( Wang et al . ,   2021 ) retrieves context using a search system to   enhance the token representation for NER tasks .   To help entity disambiguation in domain - specific   NER , Zhang et al . ( 2022 ) retrieves the domain-   specific database to find the correlated sample . In   order to leverage the extensive information about   entities in Wikipedia and Wikidata , Wang et al .   ( 2022 ) and Tan et al . ( 2023 ) construct databases   and retrieve context to enhance model performance .   In this study , we propose the utilization of retrieval   techniques for data augmentation in low - resource   settings . Furthermore , while they perform retrieval   on both the training and testing datasets , we only   use the small seed training dataset for retrieval . It ’s   noteworthy that our approach can also be combined   with theirs to further enhance the performance of   NER in low - resource settings .   5 Conclusion   We present GPDA as a data augmentation frame-   work for low - resource NER , which utilizes graph   propagation with natural texts for augmentation .   To make the most efficient utilization of the ex-   plored partially labeled text , we adopt consistency-   restricted self - training . Experiment results show113   Method en es fr Avg   Baseline 76.54 85.50 72.78 78.27   DAGA 77.11 86.51 81.32 81.65   NERDA 77.10 87.05 81.64 81.93   GPDA 77.83 87.23 82.48 82.51   that our proposed GPDA achieves substantial im-   provements over previous data augmentation meth-   ods on multiple low - resource NER datasets .   Acknowledgements   This work was supported by the National Natu-   ral Science Foundation of China ( 61976139 ) and   by Alibaba Group through Alibaba Innovative Re-   search Program .   6 Limitations   There are some limitations in the use of GPDA .   •The label propagation procedure requires an-   chor matching in the light of annotation preci-   sion , which limits the unlabeled data source .   However , Wikipedia is a open - domain easy-   to - fetch corpus with anchor links , which can   somehow mitigate the issue.•Augmented Data generated by GPDA provide   more diversity . But for some datasets , simple   modifications ( NERDA ) on the original words   performs better . We are investigating a hybrid   approach to apply GPDA and NERDA in the   same framework .   References114115116ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 5   /squareA2 . Did you discuss any potential risks of your work ?   Section 5   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Section 3   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Due to the limitation of page , we did n’t report these.117 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Due to the limitation of page , we did n’t report these .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.118