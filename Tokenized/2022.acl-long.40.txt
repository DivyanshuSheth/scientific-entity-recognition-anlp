  Jingfeng YangAditya GuptaShyam Upadhyay   Luheng HeRahul GoelShachi PaulGeorgia Institute of TechnologyGoogle Assistant   jingfengyangpku@gmail.com   tableformer@google.com   Abstract   Understanding tables is an important aspect of   natural language understanding . Existing mod-   els for table understanding require lineariza-   tion of the table structure , where row or col-   umn order is encoded as an unwanted bias .   Such spurious biases make the model vulner-   able to row and column order perturbations .   Additionally , prior work has not thoroughly   modeled the table structures or table - text align-   ments , hindering the table - text understanding   ability . In this work , we propose a robust and   structurally aware table - text encoding architec-   ture T F , where tabular structural   biases are incorporated completely through   learnable attention biases . T F is   ( 1 ) strictly invariant to row and column or-   ders , and , ( 2 ) could understand tables better   due to its tabular inductive biases . Our eval-   uations showed that T F outper-   forms strong baselines in all settings on SQA ,   WTQ and TF table reasoning datasets ,   and achieves state - of - the - art performance on   SQA , especially when facing answer - invariant   row and column order perturbations ( 6 % im-   provement over the best baseline ) , because pre-   vious SOTA models ’ performance drops by   4 % - 6 % when facing such perturbations while   T F is not affected .   1 Introduction   Recently , semi - structured data ( e.g. variable length   tables without a ﬁxed data schema ) has attracted   more attention because of its ubiquitous presence   on the web . On a wide range of various table rea-   soning tasks , Transformer based architecture along   with pretraining has shown to perform well ( Eisen-   schlos et al . , 2021 ; Liu et al . , 2021 ) .   In a nutshell , prior work used the Transformer   architecture in a BERT like fashion by serializing   tables or rows into word sequences ( Yu et al . , 2020 ;   Figure 1 : Examples showing the limitations of exist-   ing models ( a ) vulnerable to perturbations , and ( b ) lack-   ing structural biases . In contrast , our proposed T -   F predicts correct answers for both questions .   Liu et al . , 2021 ) , where original position ids are   used as positional information . Due to the usage   of row / column ids and global position ids , prior   strategies to linearize table structures introduced   spurious row and column order biases ( Herzig et al . ,   2020 ; Eisenschlos et al . , 2020 , 2021 ; Zhang et al . ,   2020 ; Yin et al . , 2020 ) . Therefore , those models are   vulnerable to row or column order perturbations .   But , ideally , the model should make consistent pre-   dictions regardless of the row or column ordering   for all practical purposes . For instance , in Figure 1 ,   the predicted answer of T model ( Herzig et al . ,   2020 ) for Question ( a ) “ Of all song lengths , which   one is the longest ? ” based on the original table is   “ 5:00 ” , which is incorrect . However , if the ﬁrst row   is adjusted to the end of the table during inference ,   the model gives the correct length “ 5:02 ” as an-528swer . This probing example shows that the model   being aware of row order information is inclined   to select length values to the end of the table due   to spurious training data bias . In our experiments   on the SQA dataset , T models exhibit a 4 % -   6 % ( Section 5.2 ) absolute performance drop when   facing such answer - invariant perturbations .   Besides , most prior work ( Chen et al . , 2020 ; Yin   et al . , 2020 ) did not incorporate enough structural   biases to models to address the limitation of sequen-   tial Transformer architecture , while others induc-   tive biases which are either too strict ( Zhang et al . ,   2020 ; Eisenschlos et al . , 2021 ) or computationally   expensive ( Yin et al . , 2020 ) .   To this end , we propose T F , a   Transformer architecture that is robust to row and   column order perturbations , by incorporating struc-   tural biases more naturally . T F re-   lies on 13 types of task - independent table $ text   attention biases that respect the table structure and   table - text relations . For Question ( a ) in Figure 1 ,   T F could predict the correct answer   regardless of perturbation , because the model could   identify the same row information with our “ same   row ” bias , avoiding spurious biases introduced by   row and global positional embeddings . For Ques-   tion ( b ) , T predicted only partially correct   answer , while T F could correctly pre-   dict“Spain , Ukraine ” as answers . That ’s because   our“cell to sentence ” bias could help table cells   ground to the paired sentence . Detailed attention   bias types are discussed in Section 5.2 .   Experiments on 3 table reasoning datasets show   thatT F consistently outperforms orig-   inal T in all pretraining and intermediate   pretraining settings with fewer parameters . Also ,   T F ’s invariance to row and column   perturbations , leads to even larger improvement   over those strong baselines when tested on pertur-   bations . Our contributions are as follows :   •We identiﬁed the limitation of current table-   text encoding models when facing row or col-   umn perturbation .   •We propose T F , which is guaran-   teed to be invariant to row and column order   perturbations , unlike current models .   •T F encodes table - text structures   better , leading to SoTA performance on SQA   dataset , and ablation studies show the effec-   tiveness of the introduced inductive biases.2 Preliminaries : T for Table   Encoding   In this section , we discuss TAPAS which serves   as the backbone of the recent state - of - the - art table-   text encoding architectures . T ( Herzig et al . ,   2020 ) uses Transformer architecture in a BERT   like fashion to pretrain and ﬁnetune on tabular   data for table - text understanding tasks . This is   achieved by using linearized table and texts for   masked language model pre - training . In the ﬁne-   tuning stage , texts in the linearized table and text   pairs are queries or statements in table QA or table-   text entailment tasks , respectively .   Speciﬁcally , T uses the tokenized and ﬂat-   tened text and table as input , separated by [ SEP ]   token , and preﬁxed by [ CLS ] . Besides token , seg-   ment , and global positional embedding introduced   in BERT ( Devlin et al . , 2019 ) , it also uses rank em-   bedding for better numerical understanding . More-   over , it uses column and row embedding to encode   table structures .   Concretely , for any table - text linearized se-   quenceS = fv;v;;vg , wherenis the   length of table - text sequence , the input to T   is summation of embedding of the following :   token ids ( W ) = fw;w;;wg   positional ids ( B ) = fb;b;;bg   segment ids ( G ) = fg;g;;gg   column ids ( C ) = fc;c;;cg   row ids ( R ) = fr;r;;rg   rank ids ( Z ) = fz;z;;zg   whereseg ; col ; row ; rankcorrespond to   the segment , column , row , and rank i d for the ith   token , respectively .   As for the model , T uses BERT ’s self-   attention architecture ( Vaswani et al . , 2017 ) off-   the - shelf . Each Transformer layer includes a multi-   head self - attention sub - layer , where each token   attends to all the tokens . Let the layer input   H= [ h;h;;h]2Rcorresponding to   S , wheredis the hidden dimension , and h2R   is the hidden representation at position i. For   a single - head self - attention sub - layer , the input   His projected by three matrices W2R ,   W2R , andW2Rto the corre-   sponding representations Q , K , andV :   Q = HW ; V = HW ; K = HW(1)529   Then , the output of this single - head self-   attention sub - layer is calculated as :   Attn(H ) = softmax ( QK   pd)V ( 2 )   3 T F : Robust Structural   Table Encoding   As shown in Figure 2 , T F encodes the   general table structure along with the associated   text by introducing task - independent relative atten-   tion biases for table - text encoding to facilitate the   following : ( a ) structural inductive bias for better   table understanding and table - text alignment , ( b )   robustness to table row / column perturbation .   Input of T F .T F   uses the same token embeddings W , segment   embeddings G , and rank embeddings ZasT .   However , we make 2 major modiﬁcations :   1 ) No row or column ids . We do not use row em-   beddingsRor column embeddings Cto avoid any   potential spurious row and column order biases .   2 ) Per cell positional ids . To further remove any   inter - cell order information , global positional em-   beddingsBare replaced by per cell positional em-   beddingsP = fp;p;;pg , wherewe follow Eisenschlos et al . ( 2021 ) to reset the   index of positional embeddings at the beginning   of each cell , and poscorrespond to the per cell   positional i d for the ith token .   Positional Encoding in T F .Note   that the Transformer model either needs to spec-   ify different positions in the input ( i.e. absolute   positional encoding of Vaswani et al . ( 2017 ) ) or   encode the positional dependency in the layers ( i.e.   relative positional encoding of Shaw et al . ( 2018 ) ) .   T F does not consume any sort of   column and row order information in the input . The   main intuition is that , for cells in the table , the only   useful positional information is whether two cells   are in the same row or column and the column   header of each cell , instead of the absolute order   of the row and column containing them . Thus , in-   spired by relative positional encoding ( Shaw et al . ,   2018 ) and graph encoding ( Ying et al . , 2021 ) , we   capture this with a same column / row relation as   one kind of relative position between two linearized   tokens . Similarly , we uses 12 such table - text struc-   ture relevant relations ( including same cell , cell   to header and so on ) and one extra type represent-   ing all other relations not explicitly deﬁned . All   of them are introduced in the form of learnable530attention bias scalars .   Formally , we consider a function  ( v;v ) : V   V!N , which measures the relation between v   andvin the sequence ( v;v2S ) . The function   can be deﬁned by any relations between the tokens   in the table - text pair .   Attention Biases in T F .In our   work ,  ( v;v)is chosen from 13 bias types , cor-   responding to 13 table - text structural biases . The   attention biases are applicable to any table - text pair   and can be used for any downstream task :   •“same row ” identiﬁes the same row infor-   mation without ordered row i d embedding or   global positional embedding , which help the   model to be invariant to row perturbations ,   •“same column ” , “ header to column cell ” , and   “ cell to column header ” incorporates the same   column information without ordered column   i d embedding ,   •“cell to column header ” makes each cell   aware of its column header without repeated   column header as features ,   •“header to sentence ” and“cell to sentence ”   help column grounding and cell grounding of   the paired text ,   •“sentence to header ” , “ sentence to cell ” , and   “ sentence to sentence ” helps to understand the   sentence with the table as context ,   •“header to same header ” and “ header to   other header ” for better understanding of ta-   ble schema , and “ same cell bias ” for cell con-   tent understanding .   Note that , each cell can still attend to other cells   in the different columns or rows through “ others ”   instead of masking them out strictly .   We assign each bias type a learnable scalar ,   which will serve as a bias term in the self - attention   module . Speciﬁcally , each self - attention head   in each layer have a set of learnable scalars   fb;b;;bgcorresponding to all types of in-   troduced biases . For one head in one self - attention   sub - layer of T F , Equation 2 in the   Transformer is replaced by :   A = QK   pd ; A = A+^A ( 3)Attn(H ) = softmax ( A)V ( 4 )   where Ais a matrix capturing the similarity be-   tween queries and keys , ^Ais the Attention Bias   Matrix , and ^A = b.   Relation between T F and ETC .   ETC ( Ainslie et al . , 2020 ) uses vectors to repre-   sent relative position labels , although not directly   applied to table - text pairs due to its large computa-   tional overhead ( Eisenschlos et al . , 2021 ) . T -   F differs from ETC in the following as-   pects ( 1 ) ETC uses relative positional embeddings   while T F uses attention bias scalars .   In practice , we observed that using relative posi-   tional embeddings increases training time by more   than 7x , ( 2 ) ETC uses global memory and local at-   tention , while T F uses pairwise atten-   tion without any global memory overhead , ( 3 ) ETC   uses local sparse attention with masking , limiting   its ability to attend to all tokens , ( 4 ) ETC did not   explore table - text attention bias types exhaustively .   Another table encoding model MATE ( Eisensch-   los et al . , 2021 ) is vulnerable to row and column   perturbations , and shares limitation ( 3 ) and ( 4 ) .   4 Experimental Setup   4.1 Datasets and Evaluation   We use the following datasets in our experiments .   Table Question Answering . For the table QA   task , we conducted experiments on WikiTableQues-   tions ( WTQ ) ( Pasupat and Liang , 2015 ) and Se-   quential QA ( SQA ) ( Iyyer et al . , 2017 ) datasets .   WTQ was crowd - sourced based on complex ques-   tions on Wikipedia tables . SQA is composed of   6;066 question sequences ( 2.9 question per se-   quence on average ) , constructed by decomposing a   subset of highly compositional WTQ questions .   Table - Text Entailment . For the table - text en-   tailment task , we used TF dataset ( Chen   et al . , 2020 ) , where the tables were extracted from   Wikipedia and the sentences were written by crowd   workers . Among total 118 , 000 sentences , each   one is a positive ( entailed ) or negative sentence .   Perturbation Evaluation Set . For SQA and   TF , we also created new test sets to measure   models ’ robustness to answer - invariant row and col-   umn perturbations during inference . Speciﬁcally,531row and column orders are randomly perturbed for   all tables in the standard test sets .   Pre - training All the models are ﬁrst tuned on   theWikipidia text - table pretraining dataset ( Herzig   et al . , 2020 ) , optionally tuned on synthetic dataset   at an intermediate stage ( “ inter ” ) ( Eisenschlos et al . ,   2020 ) , and ﬁnally ﬁne - tuned on the target dataset .   To get better performance on WTQ , we follow   Herzig et al . ( 2020 ) to further pretrain on SQA   dataset after the intermediate pretraining stage in   the “ inter - sqa ” setting .   Evaluation For SQA , we report the cell selection   accuracy for all questions ( ALL ) using the ofﬁcial   evaluation script , cell selection accuracy for all se-   quences ( SEQ ) , and the denotation accuracy for all   questions ( ALL ) . To evaluate the models ’ robust-   ness in the instance level after perturbations , we   also report a lower bound of example prediction   variation percentage :   VP=(t2f+f2 t )   ( t2t+t2f+f2t+f2f)(5 )   where t2 t , t2f , f2 t , and f2f represents how many ex-   ample predictions turning from correct to correct ,   from correct to incorrect , from incorrect to correct   and from incorrect to incorrect , respectively , after   perturbation . We report denotation accuracy on   WTQ and binary classiﬁcation accuracy on T-   F respectively .   4.2 Baselines   We use T and T as base-   lines , where Transformer architectures are exactly   same as BERT and BERT ( Devlin   et al . , 2019 ) , and parameters are initialized from   BERT and BERT respectively . Cor-   respondingly , we have our T F   andT F , where attention bias   scalars are initialized to zero , and all other pa-   rameters are initialized from BERT and   BERT .   4.3 Perturbing Tables as Augmented Data   Could we alleviate the spurious ordering biases   by data augmentation alone , without making any   modeling changes ? To answer this , we train an-   other set of models by augmenting the training data   for TAPAS through random row and column order   perturbations .   For each table in the training set , we randomly   shufﬂe all rows and columns ( including corre-   sponding column headers ) , creating a new table   with the same content but different orders of rows   and columns . Multiple perturbed versions of the   same table were created by repeating this process   f1;2;4;8;16gtimes with different random seeds .   For table QA tasks , selected cell positions are also   adjusted as ﬁnal answers according to the perturbed   table . The perturbed table - text pairs are then used   to augment the data used to train the model . During   training , the model takes data created by one spe-   ciﬁc random seed in one epoch in a cyclic manner .   5 Experiments and Results   Besides standard testing results to compare T -   F and baselines , we also answer the follow-   ing questions through experiments :   •How robust are existing ( near ) state - of - the-   art table - text encoding models to semantic   preserving perturbations in the input ?   •How does T F compare with ex-   isting table - text encoding models when tested   on similar perturbations , both in terms of per-   formance and robustness?532   •Can we use perturbation based data augmen-   tation to achieve robustness at test time ?   •Which attention biases in T F   contribute the most to performance ?   5.1 Main Results   Table 1 , 2 , and 3 shows T F perfor-   mance on SQA , TF , and WTQ , respec-   tively . As can be seen , T F outper-   forms corresponding T baseline models in all   settings on SQA and WTQ datasets , which shows   the general effectiveness of T F ’s   structural biases in Table QA datasets . Speciﬁ-   cally , T F combined with inter-   mediate pretraining achieves new state - of - the - art   performance on SQA dataset .   Similarly , Table 2 shows that T F   also outperforms T baseline models in all set-   tings , which shows the effectiveness of T -   F in the table entailment task . Note that ,   Liu et al . ( 2021 ) is not comparable to our results , be - cause they used different pretraining data , different   pretraining objectives , and BART NLG model in-   stead of BERT NLU model . But T F   attention bias is compatible with BART model .   5.2 Perturbation Results   One of our major contributions is to systematically   evaluate models ’ performance when facing row and   column order perturbation in the testing stage .   Ideally , model predictions should be consistent   on table QA and entailment tasks when facing such   perturbation , because the table semantics remains   the same after perturbation .   However , in Table 1 and 2 , we can see that in our   perturbed test set , performance of all T mod-   els drops signiﬁcantly in both tasks . T models   drops by at least 3.7 % and up to 6.5 % in all settings   on SQA dataset in terms of ALL accuracy , while   ourT F being strictly invariant to such   row and column order perturbation leads to no drop   in performance . Thus , in the perturbation setting ,   T F outperforms all T baselines   even more signiﬁcantly , with at least 6.2 % and   2.4 % improvements on SQA and TF dataset ,   respectively . In the instance level , we can see that ,   with T , there are many example predictions   changed due to high VP , while there is nearly no   example predictions changed with T F   ( around zero VP).533   5.3 Model Size Comparison   We compare the model sizes of T F   andT in Table 4 . We added only a few atten-   tion bias scalar parameters ( 13 parameters per head   per layer ) in T F , which is negligible   compared with the BERT model size . Meanwhile ,   we delete two large embedding metrics ( 512 row   ids and 512 column ids ) . Thus , T F   outperforms T with fewer parameters .   5.4 Analysis of T F Submodules   In this section , we experiment with several variants   ofT F to understand the effectiveness   of its submodules . The performance of all variants   ofT andT F that we tried on the   SQA development set is shown in Table 5 .   Learnable Attention Biases v / s Masking . In-   stead of adding learnable bias scalars , we mask out   some attention scores to restrict attention to those   tokens in the same columns and rows , as well as   the paired sentence , similar to Zhang et al . ( 2020 )   ( SAT ) . We can see that T performs   worse than T , which means that restrict-   ing attention to only same columns and rows by   masking reduce the modeling capacity . This led to   choosing soft bias addition over hard masking .   Attention Bias Scaling . Unlike T -   F , we also tried to add attention biases   before the scaling operation in the self - attention   module ( SO ) . Speciﬁcally , we compute pair - wise   attention score by :   A=(hW)(hW)+^Apd(6 )   instead of using :   A=(hW)(hW )   pd+^A ; ( 7 )   which is the element - wise version of Equa-   tion 1 and 3 . However , Table 5 shows   thatT F performs worse than   T F , showing the necessity of   adding attention biases after the scaling operation .   We think the reason is that the attention bias term   does not require scaling , because attention bias   scalar magnitude is independent of d , while the   dot products grow large in magnitude for large val-   ues ofd . Thus , such bias term could play an   more important role without scaling , which helps   each attention head know clearly what to pay more   attention to according to stronger inductive biases .   Row , Column , & Global Positional IDs .   With T , T F , and   T F , we ﬁrst tried the full - version   where row ids , column ids , and global positional   ids exist as input ( rc - gp ) . Then , we deleted row   ids ( c - gp ) , and column ids ( gp ) sequentially . Fi-   nally , we changed global positional ids in gpto   per - cell positional ids ( pcp ) . Table 5 shows that   T performs signiﬁcantly worse from rc-   gp!c - gp!gp!pcp , because table structure in-   formation are deleted sequentially during such pro-   cess . However , with T F , there is   no obvious performance drop during the same pro-   cess . That shows the structural inductive biases in   T F can provide complete table struc-   ture information . Thus , row ids , column ids and   global positional ids are not necessary in T -   F . We pick T F pcpsetting as   our ﬁnal version to conduct all other experiments in   this paper . In this way , T F is strictly   invariant to row and column order perturbation by   avoiding spurious biases in those original ids.534   5.5 Comparison of T F and   Perturbed Data Augmentation   As stated in Section 4.3 , perturbing row and col-   umn orders as augmented data during training can   serve as another possible solution to alleviate the   spurious row / column ids bias . Table 6 shows the   performance of T model trained with   additional { 1 , 2 , 4 , 8 , 16 } perturbed versions of   each table as augmented data .   We can see that the performance of T   on SQA dataset improves with such augmentation .   Also , as the number of perturbed versions of each   table increases , model performance ﬁrst increases   and then decreases , reaching the best results with   8 perturbed versions . We suspect that too many   versions of the same table confuse the model about   different row and column ids for the same table ,   leading to decreased performance from 8p to 16p .   Despite its usefulness , such data perturbation is   still worse than T F , because it could   not incorporate other relevant text - table structural   inductive biases like T F .   Although , such data augmentation makes the   model more robust to row and column order per-   turbation with smaller VPcompared to standard   T , there is still a signiﬁcant prediction   drift after perturbation . As shown in Table 6 , VP   decreases from 1p to 16p , however , the best VP   ( 7.0 % ) is still much higher than ( nearly ) no varia-   tion ( 0.1 % ) of T F .   To sum up , T F is superior to row   and column order perturbation augmentation , be-   cause of its additional structural biases and strictly   consistent predictions after perturbation .   5.6 Attention Bias Ablation Study   We conduct ablation study to demonstrate the util-   ity of all 12 types of deﬁned attention biases . For   each ablation , we set the corresponding attention   bias type i d to “ others ” bias i d. Table 7 shows   T ’s performance SQA dev set . Over-   all , all types of attention biases help the T -   F performance to some extent , due to cer-   tain performance drop after deleting each bias type .   Amongst all the attention biases , deleting “ same   row ” bias leads to most signiﬁcant performance   drop , showing its crucial role for encoding table   row structures . There is little performance drop   after deleting “ same column ” bias , that ’s because   T F could still infer the same column   information through “ cell to its column header ”   and “ header to its column cell ” biases . After   deleting all same column information ( “ same col-   umn ” , “ cell to column header ” and“header to col-   umn cell ” biases ) , T F performs signif-   icantly worse without encoding column structures .   Similarly , there is little performance drop after   deleting “ same cell ” bias , because T F   can still infer same cell information through “ same   row ” and“same column ” biases .   5.7 Limitations of T F   T F increases the training time by   around 20 % , which might not be ideal for very   long tables and would require a scoped approach .   Secondly , with the strict row and column order in-   variant property , T F can not deal with   questions based on absolute orders of rows in ta-   bles . This however is not a practical requirement   based on the current dataset . Doing a manual study   of 1800 questions in SQA dataset , we found that535there are 4 questions(0.2 % percentage ) whose   answers depend on orders of rows . Three of them   asked “ which one is at the top of the table ” , an-   other asks “ which one is listed ﬁrst ” . However ,   these questions could be potentially answered by   adding back row and column order information   based on T F .   6 Other Related Work   Transformers for Tabular Data . Yin et al .   ( 2020 ) prepended corresponding column headers   to cells contents , and Chen et al . ( 2020 ) used cor-   responding column headers as features for cells .   However , such methods encode each table header   multiple times , leading to duplicated computing   overhead . Also , tabular structures ( e.g. same row   information ) are not fully incorporated to such mod-   els . Meanwhile , Yin et al . ( 2020 ) leveraged row   encoder and column encoder sequentially , which   introduced much computational overhead , thus re-   quiring retrieving some rows as a preprocessing   step . Finally , SAT ( Zhang et al . , 2020 ) , Deng   et al . ( 2021 ) and Wang et al . ( 2021 ) restricted atten-   tion to same row or columns with attention mask ,   where such inductive bias is too strict that cells   could not directly attend to those cells in different   row and columns , hindering the modeling ability   according to Table 5 . Liu et al . ( 2021 ) used the   seq2seq BART generation model with a standard   Transformer encoder - decoder architecture . In all   models mentioned above , spurious inter - cell or-   der biases still exist due to global positional ids   of Transformer , leading to the vulnerability to row   or column order perturbations , while our T -   F could avoid such problem . Mueller et al .   ( 2019 ) and Wang et al . ( 2020 ) also used relative   positional encoding to encode table structures , but   they modeled the relations as learnable relation vec-   tors , whose large overhead prevented pretraining   and led to poor performance without pretraining ,   similarly to ETC ( Ainslie et al . , 2020 ) explained in   Section 3 .   Structural and Relative Attention . Modiﬁed   attention scores has been used to model relative   positions ( Shaw et al . , 2018 ) , long documents ( Dai   et al . , 2019 ; Beltagy et al . , 2020 ; Ainslie et al . ,   2020 ) , and graphs ( Ying et al . , 2021 ) . But addinglearnable attention biases to model tabular struc-   tures has been under - explored .   7 Conclusion   In this paper , we identiﬁed the vulnerability of   prior table encoding models along two axes : ( a )   capturing the structural bias , and ( b ) robustness   to row and column perturbations . To tackle   this , we propose T F , where learnable   task - independent learnable structural attention bi-   ases are introduced , while making it invariant to   row / column order at the same time . Experimental   results showed that T F outperforms   strong baselines in 3 table reasoning tasks , achiev-   ing state - of - the - art performance on SQA dataset ,   especially when facing row and column order per-   turbations , because of its invariance to row and   column orders .   Acknowledgments   We thank Julian Eisenschlos , Ankur Parikh , and   the anonymous reviewers for their feedbacks in   improving this paper .   Ethical Considerations   The authors foresee no ethical concerns with the   research presented in this paper .   References536537