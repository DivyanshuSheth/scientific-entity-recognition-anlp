  Yahan Yu , Bojie Huand Yu LiTencent Minority - Mandarin Translation , Beijing , ChinaBeijing Key Lab of Traffic Data Analysis and Mining , Beijing Jiaotong University , China   jasmineyuyh@gmail.com , bojiehu@tencent.com , 20112043@bjtu.com   Abstract   Text - video retrieval focuses on two aspects :   cross - modality interaction and video - language   encoding . Currently , the mainstream approach   is to train a joint embedding space for multi-   modal interactions . However , there are struc-   tural and semantic differences between text and   video , making this approach challenging for   fine - grained understanding . In order to solve   this , we propose an end - to - end graph - based hi-   erarchical aggregation network for text - video   retrieval according to the hierarchy possessed   by text and video . We design a token - level   weighted network to refine intra - modality rep-   resentations and construct a graph - based mes-   sage passing attention network for global - local   alignment across modality . We conduct exper-   iments on the public datasets MSR - VTT-9 K ,   MSR - VTT-7 K and MSVD , and achieve Re-   call@1 of 73.0 % , 65.6 % , and 64.0 % , which   is 25.7 % , 16.5 % , and 14.2 % better than the   current state - of - the - art model .   1 Introduction   Text - Video Retrieval ( TVR ) is a fundamental re-   search task in multimodal video and language un-   derstanding , which aims to retrieve the most rel-   evant video for a given text query , or to retrieve   relevant text for a given video query . The main-   stream modeling approaches for TVR are to jointly   learn cross - modal interactive information between   video and text in the same representation space ( Lei   et al . , 2021 ; Dzabraev et al . , 2021 ; Luo et al . , 2020 ;   Liu et al . , 2021 ; Cheng et al . , 2021 ) .   However , the mismatched problem of informa-   tion capacity and information density between   video and text is not fully studied in these ap-   proaches . As shown in Figure 1 , there are structural   and semantic differences between the modalities ,   making this approach challenging for TVR . The   video itself expresses a much wider range of globalFigure 1 : Examples of differences between text and   video . ( a ) The content mentioned in the text appears in   a small number of frames in the video . ( b ) Text contains   a different semantic range than video , making it difficult   to achieve semantic alignment .   representations than the text , so the textual seman-   tic usually can not be fully mapped to every detailed   information of the video . If the content mentioned   by the text appears in a small number of frames in   video , the text may be misled by unrelated seman-   tics when interacting across modalities . In addition ,   the local representations in text are usually more   specific than that in video . Due to different annota-   tors , text may have different description habits and   contain different semantic ranges , such as using   person / man / actor to describe a character appearing   in the video , which makes it difficult with semantic   understanding and alignment across modalities .   If text and video are divided into multiple parts   and then aligned , the problem in Figure 1 will be re-   duced . Therefore , we can divide video into frames   and clips , and text into words and phrases accord-   ing to the structural hierarchy of them . From a   semantic point of view , it also realizes the semantic   hierarchy segmentation from abstract to specifics .   Moreover , different parts have different extents of   importance for retrieval . In Figure 2 , we call the   parts related to query information " effective seman-   tics " . TVR aims to refine and align the effective5547   semantics of paired text - video through structural   hierarchy . Through the step - by - step aggregation of   structural and semantic hierarchy , intra - modality   feature refining and cross - modality interaction are   realized , so that the effective semantics of text and   video can be aligned .   To this end , we design a Graph - based   Hierarchical Aggregation Network ( GHAN ) to re-   fine effective semantics within modality and align   effective semantics cross modality . To achieve   the former , a token - level weighted network is con-   structed to aggregate words and frames . As for   the latter , we aggregate clips and phrases through   graph - based message passing attention network for   global - local alignment . Our contributions are sum-   marized below :   •We propose a GHAN method to solve the mis-   matched problem of information density and ca-   pacity between video and text from the perspec-   tive of effective information refining and align-   ing .   •According to the proposed concept of " effective   semantics " , we design a token - level weighted   network to refine intra - modality features , and   construct a graph - based message passing atten-   tion network for global - local alignment across   modality .   •We conducted experiments on the public datasets   MSR - VTT-9 K , MSR - VTT-7 K and MSVD ,   achieving Recall@1 of 73.0 % , 65.6 % , and   64.0 % , which remarkably boosts the retrieval   performance of the current state - of - the - art modelCAMoE ( Cheng et al . , 2021 ) by 25.7 % , 16.5 % ,   and 14.2 % .   2 Related Work   Vision - language research . Visual - language re-   search is currently a popular research field , includ-   ing image - text research ( Xing et al . , 2021 ; Frank   et al . , 2021 ) and video - text research ( Yu et al . ,   2020 ) . In the early days , visual - language models   ( Chen et al . , 2022 ) were usually designed indepen-   dently . Images were usually encoded using hand-   crafted descriptors ( Socher et al . , 2013 ; Elhoseiny   et al . , 2013 ) . Videos were mostly encoded using   2D/3D spatial - temporal convolution ( Tran et al . ,   2015 ; Feichtenhofer et al . , 2019 ) . Texts were en-   coded using pre - trained word vectors ( Frome et al . ,   2013 ) or TF - IDF features ( Lei Ba et al . , 2015 ) .   Recently , language pretraining models ( Devlin   et al . , 2018 ) have achieved great success on NLP   tasks . Vision - language research has been similarly   inspired ( I m et al . , 2021 ; Tang et al . , 2021 ; Tan   and Bansal , 2020 ) . For image - text pre - training ,   Lu et al . ( 2019 ) , Tan and Bansal ( 2019 ) used two   independent Transformers ( Vaswani et al . , 2017 )   to encode image and text respectively . As for Li   et al . ( 2019 , 2020 ) ; Su et al . ( 2019 ) , a shared Bert   model was then used . CLIP ( Radford et al . , 2021 )   learns images directly from text , which leverages   a wider range of source of supervision . Lei et al .   ( 2021 ) proposed an end - to - end method through   sparse sampling , which extracted visual and lin-   guistic features with higher efficiency and lower   memory usage . Luo et al . ( 2021 ) directly used   CLIP and trained the model in an end - to - end man-   ner .   Text - Video Retrieval . Although text - image re-   search ( Khademi , 2020 ; Zhang et al . , 2020a ) has   been extensively studied , text - video retrieval is   still quite challenging . The earlier works , such as   Liu et al . ( 2019 ) and Gabeur et al . ( 2020 ) , solved   this task by Mixture - of - Experts ( MoE , Ma et al .   ( 2018 ) ) , which taked advantage of modalities to   integrate generalizable features . Liu et al . ( 2019 )   used 7 kinds of feature encoding for video , and then   aggregated through Mean Pooling ( Lee et al . , 2016 )   or NetVLAD ( Arandjelovic et al . , 2016 ) , and fused   multimodal features together . Gabeur et al . ( 2020 )   encoded features through Transformer ( Vaswani   et al . , 2017 ) , calculating the similarity with the text   and weighting different modalities according to the   text . Based on the idea of NetVLAD ( Arandjelovic5548et al . , 2016 ) , Wang et al . ( 2021 ) adaptively ag-   gregated sound , action , scene , speech , OCR , face ,   etc . in video and a series of shared semantics in   text . With the rise of pre - training models , Dzabraev   et al . ( 2021 ) used the image - text pre - training model   CLIP ( Radford et al . , 2021 ) to encode the original   video , which improved the retrieval accuracy . Luo   et al . ( 2021 ) transfered CLIP ( Radford et al . , 2021 )   to the video domain to solve text - video retrieval   task . Cheng et al . ( 2021 ) used MoE and CLIP to   extract multi - view video representations , includ-   ing actions , entities , scenes , etc . , and then aligned   them with corresponding text parts . Our model   GHAN also benefits from existing image - text pre-   training models , but we are more concerned about   the design of interaction between modalities , and   extensive ablation studies are conducted to demon-   strate the effectiveness of our method .   3 Method   We propose an end - to - end graph - based hierarchi-   cal aggregation network , aiming to obtain features   with the simplest structure and the most abundant   semantics for TVR . Figure 3 describes the archi-   tecture of our model named GHAN . Details are   described in the rest of this section .   3.1 Encoding Layer   Relevant text and video are fed into our model   GHAN in pairs . The pretrained text - image model   CLIP ( Radford et al . , 2021 ) is effective for the   text - video retrieval in this paper . As a result , we   utilize CLIP as the backbone to encode the input   text and video , which enables us to learn cross-   modality interaction with less frames and is more   computationally efficient . In our work , we mainly   focus on the aggregation and interaction of features   rather than the pre - training itself .   For text , Let T= [ w , w , ... , w]be an input   text , where w(1≤i≤N)is the iword in T.   Then we take Bert ( Devlin et al . , 2018 ) pretrained   by CLIP to encode them as semantic representa-   tions :   e= [ e , e , ... , e ] = BERT ( T),(1 )   where e∈Ris the hidden state sequence   output by the last layer of Bert , and Dmeans the   dimension of each word representation . Nrep-   resents the output sequence length , implying that   we encode the original input text into Nword   representations . Similarly , for video , we define a video as a time   sequence of Nsampled image frames . Let V=   [ f , f , ... , f]be a raw video , where f(1≤j≤   N)is the jframe . We use ViT ( Dosovitskiy   et al . , 2020 ) pretrained by CLIP as the backbone :   e= [ e , e , ... , e ] = ViT ( V ) , ( 2 )   where e∈Ris the sequence of hidden   states output by the last layer of ViT. In particular ,   considering the temporal features between video   frames , we add positional encoding to the frame   sequence e = e+eto enforce this . Nde-   notes the output sequence length , implying that we   encode the frame sequence into Ntemporal frame   representations .   3.2 Intra - Modality Refining Layer   According to our proposed structural hierarchy , the   representations of the lowest hierarchy word and   frame have been generated by the Encoding Layer .   This level contains the richest original input seman-   tics , so in the process of generating representations   at other hierarchies within the modality , we tend   to preserve the local effective semantics as much   as possible . " Local " can be understood from two   perspectives : ( i ) it contains the original semantic   information from the Encoding Layer . ( ii ) The over-   all effective semantics consists of multiple discrete   local semantics , and the local effective semantics   can be regarded as different views of the overall   semantics . For example , the reference to the event   subject and the reference to the time and place in   the text usually belong to different phrase parts .   Therefore , in this layer , we aggregate eande   separately to generate Nphrase representations   andNclip representations . In order to achieve   the refining of effective semantic information to   the next hierarchy , considering that all words and   frames do not have the same contribution , we pro-   pose a token - level weighted network to perform   weighted aggregation of words and frames :   e= Att ( e)m(e ) , ( 3 )   e= Att ( e)m(e ) , ( 4 )   where Att(·)is the token - level weighted function ,   m(·)is the stack of linear layers , e∈R   is the set of phrase representations , e∈R   is the set of clip representations . Although self-   attention has been very popular in research work in   recent years , in order to keep our model structure5549   from being too complicated , we choose to rela-   tively simplify self - attention . All word and frame   weights are generated using only stacking of linear   layers , and then aggregated into phrases and clip   representations .   Att(e ) = [ σ(We+b ) ] , ( 5 )   where σmeans softmax activation , Wis the   trainable parameter matrix , and bis the bi-   ase . From the aspect of feature dimension ,   Att(·)∈R.Through the visual analysis of   the weights after training , it is proved that our net-   work is effective . The aggregation at the structural   hierarchy realizes the aggregation at the semantic   hierarchy and the refinement of effective semantics .   3.3 Cross - Modality Interaction Layer   So far , we have completed the representation learn-   ing of phrases and clips , and the effective informa-   tion of the original input is weighted and assigned   to them . Considering that in this layer , our goal is   to achieve cross - modal feature interaction , we first   take the average for each phrase and clip to initial-   ize the top - hierarchy text and video representations :   e=/summationtexteande=/summationtexte . Where   e∈Ris the text initial global representation   ande∈Ris the video initial global represen - tation . In terms of information interaction , humans   can reason from spatial or semantic dimensions ,   and graphs can well describe spatial and semantic   information(Chen et al . , 2018 ) , so that computers   can learn to use this information like humans to   make inferences . Therefore , we built two global   graphs for cross - modal interaction . The construc-   tion details are as follows :   ( a ) T - C graph Gcontains one text node nand   Nclip nodes n(1≤i≤N ) . Specially ,   e∈Rdenotes the representation of n ,   e∈R(1≤i≤N)denotes represen-   tation of the iclip node n. We add edges   between any two nodes in the graph . T - C   graph aligns text global semantics using clip   local semantics .   ( b ) V - P graph Gcontains one video node n   andNphrase nodes n(1≤j≤N ) . Spe-   cially , e∈Rdenotes the representation   ofn , e∈R(1≤j≤N)denotes   representation of the jphrase node n. We   add edges between any two nodes in the graph .   V - P graph aligns video global semantics using   phrase semantics .   By constructing two independent graph , the   global information of one modality is aggregated5550with the local information of the other modality .   At the same time of interaction at the structural hi-   erarchy , the effective semantic alignment between   modalities is realized . In the aggregation of graph   nodes we treat all node and edge types as the same ,   i.e. we build homogeneous graphs . We also ex-   plore other ways of constructing graph , which are   described in Section 4 .   The Message Passing Neural Network ( MPNN ,   Zhang et al . ( 2020b ) ) is a framework based on the   core idea of recursive neighborhood aggregation ,   where nodes can pass messages to each other , and   the representation of each node is updated accord-   ing to the messages received from its neighbors .   MPNN consists of two phases : aggregation phase   and the readout phase . The aggregation phase in-   cluding Aggregation Function and Combination   Function runs for ttime steps in total . The Ag-   gregation Function aggregates features of neighbor   nodes , ready to be passed to the central node . The   Combination Function updates the node represen-   tation , combining the representation of the node   with the message obtained from the Aggregation   Function . The readout phase obtains graph - level   representations through the Readout Function for   subsequent classification or regression tasks . Con-   sidering the graph attention mechanism proposed   by Graph Attention Networks ( GAT , Veli ˇckovi ´ c   et al . ( 2017 ) ) , we design a Message Passing Atten-   tion network for global - local Alignment ( MPAA )   as shown in Figure 4 . We apply the same MPAA   toGandG. For a graph G , we design the   following Aggregation Function :   m=/summationdisplayαhW , ( 6 )   where hmeans the hidden representation of j   node at the t−1time step , Wis the trainable pa-   rameter matrix in aggregation function , αmeans   thekattention coefficient between inode and   jnode , kmeans a total of kattention mecha-   nisms need to be considered , Nmeans the set of   neighbor nodes of the inode,| N|is the num-   ber of nodes in N , and mis the message vector .   More specifically , the kattention coefficient α   expands as follows :   α = exp / parenleftig   δ / parenleftig   a / bracketleftig   hW∥hW / bracketrightig / parenrightig / parenrightig   /summationtextexp / parenleftbig   δ / parenleftbig   a / bracketleftbig   hW∥hW / bracketrightbig / parenrightbig / parenrightbig ,   ( 7 )   where his the jneighbor node ’s hidden rep-   resentation of the inode , ais the trainable pa-   rameter matrix , and δis LeakyReLu activation . We   design the Combination Function as follows which   takes mas input :   h=∥[ϕ(m+hW ) ] , ( 8)   where ϕis ELU activation , Wis the trainable pa-   rameter matrix in combination function , ∥means   concatenating the outputs of the kattention mecha-   nisms , and hmeans the updated representation of   inode at the ttime step . The purpose of our set-   tingWis to make the updated representations not   deviate too much from the original representations   and enhance the robustness of the MPAA . Finally ,   for the Readout Function , we design as follows :   r = σ[ϕ(/summationdisplayαhW+hW ) ] , ( 9 )   where h∈Rmeans the hidden representa-   tion of the text node nor the video node n ,   Nmeans the set of neighbor nodes of norn ,   | N|is the number of nodes in N , his the rep-   resentation of nodes in N , αmeans the attention   coefficient with single head , ϕmeans ELU acti-   vation , σmeans softmax activation , and Wand   Ware the trainable parameter matrixs in readout   function . Finally , we set e = rin graph G , and   e = rin graph G.   3.4 Matching Layer   In this layer , we define two types of retrieval tasks :   retrieving video with text ( T2V ) and retrieving text5551   with video ( V2 T ) . The goal of the retrieval task is   to interact with representations of text and video so   that the larger the pairwise similarity , the smaller   the others . So we apply cross - entropy loss ( Zhai   and Wu , 2018 ) to this task . There are Ntext - video   pairs in a batch Bthat treat text - video pairs as   positive samples and others as negative samples ,   and define the overall loss as the average of the two   retrieval tasks :   Loss=−1   B / summationdisplaylogexp(d(e , e))/summationtextexp(d(e , e ) ) ,   ( 10 )   Loss=−1   B / summationdisplaylogexp(d(e , e))/summationtextexp(d(e , e ) ) ,   ( 11 )   Loss= ( Loss+Loss)/2 , ( 12 )   where d(·)is the cosine similarity function used for   text and video distance measurement . The cross-   entropy loss enables our model to learn matching   the most relevant text and video . The loss function   uses a symmetric cross - entropy loss over similarity   scores . Every text and video are calculated similar-   ity to all videos or texts , which should be maximum   in ground truth pairs . When the cosine similarity   between embeddings output by the model is the   largest , the loss is the smallest . This can meet the   needs of model training .   4 Experiment   4.1 Datasets   MSR - VTT-9 K ( Gabeur et al . , 2020 ) . MSR - VTT   ( Xu et al . , 2016 ) consists of 10k videos ranging   in length from 10 to 30 seconds , each paired with   approximately 20 texts . In MSR - VTT-9 K we use   the training split in Gabeur et al . ( 2020 ) which   consists of about 9k videos and 180k texts and the   1K - A split test set ( Yu et al . , 2018 ) which contains   1k selected text - video pairs . MSR - VTT-7 K ( Miech et al . , 2019 ) . We use a   training split of Miech et al . ( 2019 ) which contains   approximately 7k video sets and 140k texts and   split the entire dataset into 7k for training and 3k   for testing .   MSVD ( Chen and Dolan , 2011 ) contains 1,970   videos ranging in length from 1 second to 60 sec-   onds and about 120k texts . Each text describes a   video . The training , validation , and test datasets   consist of 1,200 , 100 , and 670 videos .   4.2 Experimental Settings   In the Encoding Layer , we process raw videos and   texts using the same ViT ( ViT - B/32 ) and Bert in   CLIP , and initialize all encoder parameters from   CLIP ’s pretrained weights . We sample 1frame   per second and resize each frame to 224×224 .   Both frame and word embeddings have dimension   D= 512 and use the same logit scaling parameter   as CLIP .   In the Intra - Modality Refining Layer , Att ( · )   uses a linear layer with the input dimension 512   and output dimension Nfor words and output di-   mension Nfor frames . m(·)uses linear layers   with input dimension of 512 , hidden size of 1024   and output dimension of 512 .   In the Cross - Modality Interaction Layer , there   are(N+ 1 ) nodes nand(N+ 1)edges in   T - C graph and ( N+ 1 ) nodes nand(N+ 1 )   edges in V - P graph . The two graphs use two same   MPAAs . Their input and output dimensions are   512 , hidden size is 256 , dropout is 0.2 , attention   heads are 8and Xavier initialization method is   used .   In the Matching Layer , we follow the evaluation   metric ( Luo et al . , 2021 ) and report recall with rank   K ( R@K),and median rank ( Med R ) . Higher R@K   and lower Med R indicate better performance . We   set batch size to 128for all experiments , N=   N= 32 , N = N= 6 , learning rate to 1e−7   for the CLIP initialization weights and 1e−4for5552   others . We optimized our model for 5epochs using   the Adam optimizer . We use 4A100 GPUs for   training , and the training duration is about 4h .   4.3 Main Results   Table 1 , 2 and 3 list the results of the comparative   models and our model GHAN on MSR - VTT-9 K ,   MSR - VTT-7 K and MSVD , respectively . We have   the following observations :   ( 1)The models of hierarchical aggregation ( Cheng   et al . , 2021 ; Dzabraev et al . , 2021 ) gener-   ally perform better . The Mixtures of Experts   method that integrates features from various   modalities is also a hierarchical aggregation   architecture , which makes the semantic in-   teraction across modalities more sufficient   through hierarchical aggregation of features .   ( 2)By comparing our model with Dzabraev et al .   ( 2021 ) ; Cheng et al . ( 2021 ) , we build a graph   structure in hierarchical aggregation networks   for cross - modal information aggregation , and   experiments show that the effect is remark-   able .   ( 3)The image - text pre - trained model is helpful   for improving the results . Compared with Dz-   abraev et al . ( 2021 ) and Gabeur et al . ( 2020 ) ;   Liu et al . ( 2019 ) , fusing pre - trained model has   advantages.(4)GHAN achieves the best results among all mod-   els . We attribute this to the design of the hier-   archical structure , the cross - modal interaction   of the graph structure , the intra - modal feature   refining , and the transfer of the image - text   pre - trained model . Further studies are shown   below .   4.4 Effects of each layer of GHAN   The baseline we use simply average the obtained   frame and word features from the output of the   pre - trained model , and then calculate the cosine   similarity for retrieval . As can be seen from the   Table 4 , our subsequent measures of intra - modal   aggregation(token - level weighted network ) and   cross - modal interaction ( Message Passing Atten-   tion Network for Global - Local Alignment , MPAA )   are effective . w/o inter refers to only using the en-   coding layer and token - level weighted network for   training , and w/o intra refers to using the encod-   ing layer and MPAA for training . Neither single   intra - modal aggregation nor single cross - modal ag-   gregation can be used to achieve good results . It   demonstrates that simple linear layer stacking fails   to adequately interact with semantics across modal-   ities . Meanwhile , direct cross - modal aggregation   is easy to lose fine - grained features , and effective   semantics are disturbed by a large number of irrel-   evant semantics . Thus , the effects of each layer   design of our model is verified.5553   4.5 Effects of Different Graph Structures   In the Table 5 , we explore four graph struc-   tures for updating text and video representations ,   namely Ho-1Graph , Ho-2Graph , He-1Graph and   He-2Graph . Ho-1Graph refers to the construct of   one homogeneous T - V - C - P graph for all four kinds   of nodes , and Ho-2Graph refers to the method fi-   nally selected in this paper , which constructs two   graphs T - C graph and V - P graph respectively . He-   1Graph & He-2Graph retain the corresponding   node connections , and treat nodes and edges at   different hierarchies and modalities as a difference .   Firstly , the influence of homogeneous graph and   heterogeneous graph is analyzed . The homoge-   neous graph still uses MPAA to achieve message-   passing . As there are various types of nodes and   edges in the heterogeneous graph , we choose R-   GCN ( Schlichtkrull et al . , 2018 ) for training . From   the Table 5 , it can be concluded that the method of   heterogeneous graph is more effective , but training   time in 5 epochs of heterogeneous graph is 9.2h ,   while the homogeneous graph only needs 4h , so we   finally ignored the slight improvement brought by   heterogeneous graph and chose to build the same   composition .   After choosing the way of homogeneous graphs ,   we analyze whether to build features into one graph   or two graphs . It is better to aggregate text and   video separately according to the Table 5 . As the   global feature already contains the local features of   the same modality , the structure of two graphs can   make the effective semantics of the global feature   directly align with the local effective semantics   of another modality , bringing about a significant   improvement in the results .   4.6 Analysis   In this section , we conduct further analyses to   demonstrate the inner workings of GHAN . We se-   lect a text - video pair to visualize the weights of the   aggregation process , as shown in Figure 5 .   Intra - Modality Refining We first explore the   capability of the proposed method to refine seman-   tics in the structural hierarchy . Frames contains   3 scenes : opening subtitles , person speaking and   crowd . Frame - clip weight shows that these scenes   are well refined and aggregated into different clips   respectively . Word - phrase weight shows the in-   formation in the 6 phrases : a person giving his   crowded /his crowded world /his opinion world /   crowded world /opinion on how world /a person   giving his crowded , effectively dividing the seman-   tics .   Cross - Modality Interaction We provide a fur-   ther investigation on whether the designed MPAA   has aligned clips and phrases containing different   semantics across modalities . We visualize one head   of the multi - head attention in MPAA . As shown   in Figure 5 , Text - clip weight shows that the fourth   clip is filtered during the alignment process , whose   main semantics is the scene of the opening subtitles   and is irrelevant to the text . Video - phrase weight   shows that video pays more attention to phrases   which express his opinion world /crowded world /   opinion on how world . This demonstrates that   MPAA can satisfactorily achieve cross - Modality   alignment and interaction , leading to the inherent   superiority in text - video retrieval.55545 Conclusion   In this paper , we propose a graph - based hierarchi-   cal aggregation network named GHAN for text-   video retrieval . We noticed that text and video   have structural and semantic hierarchies . In this   regard , we propose the concept of effective seman-   tics , which maps retrieval tasks to refine and align   effective semantics between modalities . We de-   sign a token - level weighted network to refine intra-   modality features , and build a message passing   attention network for global - local alignment across   modality . Our model significantly improves on   multiple datasets . More experiments indicate that   our model can well achieve semantic refining and   alignment .   6 Limitations   Our model simply performs text - video retrieval and   does not process speech information . Speech infor-   mation in the video , such as character dialogue or   narration , can enrich the semantics of the video . If   the speech information is added , it is envisaged that   the retrieval accuracy can be improved , and more   practical tasks can be designed , such as mutual re-   trieval of text , voice , video , and images . On the   other hand , our approach lacks alignment between   texts and semantics within frames . Some objects   may occupy a small proportion of a frame , result-   ing in retrieval failure . In future work , we plan   to take advantage of more modal informationbuild   a unified model that simultaneously implements   retrieval tasks and generation tasks .   References555555565557