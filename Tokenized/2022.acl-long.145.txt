  Chenhua Chen , Zhiyang Teng , Zhongqing Wangand Yue ZhangSchool of Engineering , Westlake University , ChinaInstitute of Advanced Technology , Westlake Institute for Advanced Study , Soochow University ,   { chenchenhua , tengzhiyang}@westlake.edu.cn ,   wangzq@suda.edu.cn , yue.zhang@wias.org.cn   Abstract   Dependency trees have been intensively used   with graph neural networks for aspect - based   sentiment classification . Though being effec-   tive , such methods rely on external depen-   dency parsers , which can be unavailable for   low - resource languages or perform worse in   low - resource domains . In addition , dependency   trees are also not optimized for aspect - based   sentiment classification . In this paper , we pro-   pose an aspect - specific and language - agnostic   discrete latent opinion tree model as an alter-   native structure to explicit dependency trees .   To ease the learning of complicated structured   latent variables , we build a connection between   aspect - to - context attention scores and syntac-   tic distances , inducing trees from the attention   scores . Results on six English benchmarks , one   Chinese dataset and one Korean dataset show   that our model can achieve competitive perfor-   mance and interpretability .   1 Introduction   Aspect - based sentiment classification ( ABSA ) is   the task of recognizing the sentiment polarities of   specific aspect categories or aspect terms in a given   sentence ( Jiang et al . , 2011 ; Dong et al . , 2014 ;   Wang et al . , 2016 ; Tang et al . , 2016 ; Li et al . , 2018 ;   Du et al . , 2019 ; Sun et al . , 2019a ; Seoh et al . , 2021 ;   Xiao et al . , 2021 ) . Different from document - level   sentiment analysis , different aspect terms in the   same document can bear different sentiment polari-   ties . For example , given a restaurant review “ decor   is nice though service can be spotty " , the corre-   sponding sentiment labels of “ decor ” and “ service ”   are positive and negative , respectively .   How to locate the corresponding opinion con-   texts for each aspect term is a key challenge for   ABSA . To this end , recent efforts leverage depen-   dency trees ( Zhang et al . , 2019 ; Sun et al . , 2019a ;   Wang et al . , 2020 ) . Syntactic dependencies have   been shown to better capture the interaction be-   tween the aspect and the opinion contexts ( Huang   Figure 1 : A dependency tree of the input sentence   “ decor is nice though service can be spotty ” and two   induced trees of two aspects in this sentence .   et al . , 2020 ; Tang et al . , 2020 ) . For example , in   Figure1(a ) , using syntactic relations , we can find   that the corresponding opinion words for “ decor ”   and “ service ” are “ nice ” and “ spotty “ , respectively .   Despite its effectiveness , dependency syntax has   the following limitations . First , dependency parsers   can be unavailable for low - resource languages or   perform worse in low - resource domains ( Duong   et al . , 2015 ; Rotman and Reichart , 2019 ; Vania   et al . , 2019 ; Kurniawan et al . , 2021 ) . Second , de-   pendency trees are also not optimized for aspect-   based sentiment classification . Previous stud-   ies transform dependency trees to aspect - specific   forms by hand - crafted rules ( Dong et al . , 2014 ;   Nguyen and Shirai , 2015 ; Wang et al . , 2020 ) to   improve the aspect sentiment classification perfor-   mance . However , the tree structure is adjusted   mainly by the node hierarchy , without optimizing   dependency relations for ABSA .   In this paper , we explore a simple method to in-   duce a discrete opinion tree structure automatically   for each aspect . Two examples are shown in Fig-   ure 1 . In particular , given a target and a sentence,2051our algorithm induces a tree structure recursively   according to a set of attention scores , calculated   using a neural layer on top of BERT representa-   tion of the sentence ( Devlin et al . , 2019 ) . Starting   with the root node , the algorithm builds a tree by   selecting one child node on each side of a current   node and recursively continue the partition process   to obtain a binarized and lexicalized tree structure .   The resulting tree serves as the input structure and   is fed into graph convolutional networks ( Kipf and   Welling , 2017 ) for learning the sentiment classi-   fier . We study policy - based reinforcement learning   ( Williams , 1992 ) to train the tree inducer . One   challenge is that the generated policy can be easily   remembered by the BERT encoder , which leads   to insufficient explorations ( Shi et al . , 2019 ) . To   alleviate this issue , we propose a set of regularizers   to help BERT - based policy generations .   Although our method is conceptually simple and   straightforward for the inference stage , we show   that it has a deep theoretic grounding . In par-   ticular , the attention based tree induction parsers   trained using the policy network can be viewed as   a simplified version to a standard latent tree struc-   tured V AE model ( Kingma and Welling , 2014 ; Yin   et al . , 2018 ) , where the KL divergence between   the prior and the posterior tree probabilities is ap-   proximated by attention - based syntactic distance   measures ( Shen et al . , 2018a ) .   Experiments on six English benchmarks , a Chi-   nese hotel review dataset and a Korean automotive   review dataset show the effectiveness of our pro-   posed models . The discrete structure also makes   it easy to interpret the classification results . In ad-   dition , our algorithm is faster , smaller and more   accurate than a full variational latent tree variable   model . To our knowledge , we are the first to learn   aspect - specific discrete opinion tree structures with   BERT . We make our code publicly available at .   2 Model   Figure 2 shows the architecture of our proposed   model . Given an input sentence xand a specific as-   pect term a , we induce an opinion tree taccording   to a recognition network Q(t|x , a ) , where ϕis the   set of network parameters . We apply multi - layered   graph convolutional networks ( GCNs ) over the   BERT output vectors to model the structural rela-   tions in the opinion tree and extract aspect - specific   features . Finally , we use an attention - based clas-   sifier to learn the sentiment classifier P(y|x , a , t ) ,   where θis the set of parameters .   To train the model , RL is used for Q(t|x , a )   ( Section 2.3 ) and standard backpropagation is used   for training P(y|x , a , t ) ( Section 2.2 ) .   2.1 Opinion Tree Based Classifier   Opinion Tree Denote the input sentence as x=   ww . . . wand the aspect as a = ww . . . w.   [ b , e]is a continuous span of [ 1 , n].wis the i-   th word . As shown in Figure 1 , the opinion tree   forais a binarized tree . Each node contains a   word span and at most two children . ais placed   at the root node . Except for the root node , each   node contains only one word . An in - order traversal   overtcan recover the original sentence . Ideally ,   the nodes near the root node should contain the   corresponding opinion words , such as “ nice ” for   “ decor ” and “ spotty ” for “ service ” .   Algorithm 1 shows the process of building an   opinion tree tforathat conforms to the above   conditions using a node score function v , where   vindicates the informative score of the i - th word   contributing to the sentiment polarity yofa.vis   the corresponding scores of words in the span [ i , j ] .   We first make the aspect span [ b , e]as the root node   and then build its left and right children from the   spans [ 1 , b−1]and[e+1 , n ] , respectively . To build   the left or right subtree , we first select the element   with the largest score in the span as the root node of   the subtrees and then recursively use the build_tree   call for the corresponding span partitions .   Calculating vFollowing Song et al . ( 2019 ) , we   feed the inputs “ [ CLS ] ww . . .w[SEP ] w   w. . .w ” to BERTto obtain the aspect - specific   sentence representation H , and then calculate a set2052   Algorithm 1 : Aspect - specific construction al-   gorithm given a scoring function v.   of attention scores of the aspect words ,   where u , WandWare model parame-   ters , σis the ReLU activation function , his   the aspect representation by sum pooling from   HH . . .H.ϕinQ(t|x , a)contains the   model parameters of BERT , u , WandW.   Graph Representation Given tandH , we use   GCNs to learn the representation vectors for each   word . We convert tto an undirected graph G.   Specifically , we take each word as a node in G   and design the adjacency matrix A∈RofG   by considering four types of edges . First , we in-   clude self loops for each word . Second , we fully   connect each word within the aspect term . Third ,   for the child node wof the root node , we link w   to each word in a. Last , we consider edges in t   between single word nodes except the root node .   Formally , Ais given by   Ais ensured to be symmetric by Eq 2 .   We then use GCNs to capture the structured re-   lations between word pairs , given the adjacency   matrix Abetween nodes and the representation   matrix of the ( l−1)-th layer H∈R , the   l - th layer representation Hgiven by a GCN is ,   where fis an activation function ( i.e. , ReLU ) ,   W∈Randb∈Rare the model param-   eters for the l - th layer . The input to the first GCN   layerHisHgiven by the sentence encoder . Target Aspect Representation We consider both   the representation vector of the “ [ CLS ] ” token   ( H ) and the aspect vectors given by the last GCN   layer ( H , H. . . , H ) as the aspect - specific   representation vector to query the input sentence   representation H. The final aspect - specific feature   representation cover the input sentence representa-   tion is given by an attention layer ,   where αis the attention scores of atow , αis the   normalized scores and cis the final feature .   Output layers usecfor computing the senti-   ment polarity scores . The final sentiment distribu-   tion is given by a softmax classifier ,   where Wandbare model parameters and pis   the predicted distribution .   2.2 Training the Sentiment Classifier   Cross Entropy Loss The classifier is trained by   maximizing the log - likelihood of the training sam-   ples . Formally , the objective is to minimize   where |D|is the size of training data , yis the   sentiment label of ain the i - th example xand   pis the classification probability for a , which   is given by Eq 5 . The set of model parameters   θinP(y|x , a , t ) includes GCN blocks and the   classifier parameters in Eq 5 .   Tree Distance Regularized Loss Following   Pouran Ben Veyseh et al . ( 2020 ) , we introduce a   syntax constraint to regularize the attention weights .   Ideally , the words near to the root node should re-   ceive high attention weights . Given an opinion tree   t , we compute the tree distance dfor each word   iusing the length of the shortest path to the root .   Given the distances and the attention scores α , we   use the KL divergence to encourage the aspect term   to attend the contexts with shorter distances .   where tdis the normalized tree distance and KL   is the Kullback - Leibler ( KL ) divergence .   Backpropagation During training , we replace the   argmax operator in Algorithm 1 with stochastic2053sampling to explore more discrete structures . Since   the tree sampling process is a discrete decision   making procedure , it is non - differentiable . The   gradient can be propagated from Lin Eq 6 to t   andθ , but can not be further propagated from tto   ϕ. Therefore , we use the policy gradient given by   REINFORCE ( Williams , 1992 ) to optimize ϕof   the policy network ( Section 2.3 ) .   2.3 Training the Tree Inducer   Suppose that the reward function for a latent tree   tisR , the goal of reinforcement learning is to   minimize the negative expected reward function ,   For each t , we use the sentiment log - likelihood   logP(y|x , t , a ) asR. Using REINFORCE , the   gradient of Lwith respect to ϕis ,   logQ(t|x , a)is the log - likelihood of the gen-   erated sample t , which can be decomposed to   a sum of log - likelihood at each tree - building   step . According to Algorithm 1 , each call of   build_tree ( v , i , j)involves selecting an action k   from the span [ i , j]given the scores v. The action   space contains j−i+1actions . The log - likelihood   of this action is given by ,   In particular , we use vin Eq 1 as the score func-   tionv . Enumerating all possible trees to calculate   the expectation term in Eq 9 is intractable , and we   use a Monte Carlo method ( Rubinstein and Kroese ,   2016 ) , approximating the training objective by tak-   ingMsamples ,   Attention Consistency Loss Instead of solely re-   lying on the reinforced gradient to train the policy   network , we also apply an attention consistency   loss to directly supervise the policy network . Note   that there are two attention scores in our model .   The first is the attention score sdefined in Eq 1 ,   which is trained by the reinforcement learning algo-   rithm . The second is the attention score αdefinedin Eq 4 for extracting useful context features for   the aspect - specific classifier . αis trained via end-   to - end back propagation . Intuitively , words that   receive the largest attention scores should be effec-   tive opinion words of the target aspect . Therefore ,   it should be put closer to the root node by the pol-   icy network . To this end , we enforce a consistent   regularization between the two attention scores so   that polarity oriented attention αcan be directly   used to supervise the scoring policy s. Formally ,   Lis given by ,   where detach is a stop gradient operator .   Overall Loss Finally , the overall loss is given by   where Lis the supervised loss , Lis the rein-   forcement learning loss , Lis a novel attention   consistency loss and Lis a loss to guide the at-   tention score distributions by tree constraints . λ ,   λandλare hyper - parameters .   3 A Variational Inference Perspective   Interestingly , L , LandLcan be unified in   a theoretic framework using variational inference   ( Kingma and Welling , 2014 ) . We show in this   section , that our method can be viewed as a stronger   extension to a latent tree V AE model .   3.1 Variational Latent Tree Model   To model P(y|x , a ) , we introduce a latent discrete   structured variable t. Formally , the training ob-   jective is to minimize the negative log - likelihood ,   Eq 14 calculates log - of - sum over all possible   treest , which is exponential . Eq 14 can be approx-   imated by the evidence lower bound ( ELBO ) us-   ing variational parameters ϕ(Kingma and Welling ,   2014 ; Yin et al . , 2018 ) ,   where p(t|x , a)is the prior distribution for gen-   erating latent trees , q(t|x , y , a ) is the corre-   sponding posterior distribution , logP(y|x , a , t )   is the log - likelihood function by assuming2054that the latent tree tis already known , and   E[logP(y|x , a , t ) ] is the expected log-   likelihood function over q(t|x , y , a ) by consid-   ering all the potential trees . The KL term acts   as a regularizer to force the matching of the prior   and the posterior distributions . During training ,   q(t|x , y , a ) is used to induce the tree . For infer-   ence , p(t|x , a)is used since yis still unknown .   In practice , a scale hyper - parameter βcan be   used to control the behaviour of the KL term ( Bow-   man et al . , 2016b ) ,   The first term is an expectation term and the   second term is a KL term . Eq 16 is a standard V AE   model for the ABSA task , which , however , has not   been discussed in the research literature . It can be   trained using the tree entropy ( Kim et al . , 2019b )   and neural mutual information estimation ( Fang   et al . , 2019 ) . However , both are slow because they   both need to consider a large batch of tree samples .   To model q(t|x , y , a ) , we instead calculate a score   function sfor the posterior by a MLP layer similar   to Eq 1 ,   where u , WandWare parameters , Hand   hare the posterior sentence and aspect represen-   tations respectively given y. To ensure that ycan   guide the encoder , we feed the input sequence to-   gether with yto BERT by using “ [ CLS ] ww   . . .w[SEP ] ww . . .wy ” to obtain H.   3.2 Correlation with Our Model   Our method can be regarded as a novel simplifica-   tion to the above model , which can be shown by cor-   relating the expectation term and the KL term de-   fined in Eq 16 with the attention scores in Eq 1 and   Eq 4 , respectively . In particular , we consider con-   verting tinto a special type of tree distance , namely   the aspect - to - context attention scores . Then we del-   egate the probability distribution over structured   tree samples to a set of attention scores . Intuitively ,   if the attention scores are similar , the generated   trees should be highly similar .   Approximate Expectation Term Considering   the gradient of the first expectation term with re - spect to ϕis ,   Assuming that the posterior q(t|x , y , a ) is approx-   imate to Q(t|x , a)given by the recognition net-   work , Eq 18 is equivalent to Lin Eq 11 .   Approximate KL Term The KL term resem-   blesLin Eq 12 for β = λ , namely   KL   q(t|x , y , a ) ||p(t|x , a)   ≈KL(α , s ) . First ,   we delegate the probability distribution over tree   samples to a set of attention scores . In particular ,   we use sandsas the proxies for p(t|x , a)and   q(t|x , y , a ) , respectively . This is equivalent to say   that the posterior scores sand the prior score s   are fed to Algorithm 1 to derive the corresponding   trees during training . Second , since both sand the   attention score αin Eq 4 are directly supervised   by the output label y , we can safely assume that   s≈α . Then the KL term KL(s , s)in Eq 16   becomes KL(α , s ) , which is the attention - based   regularization loss defined in Eq 12 .   4 Experiments   We perform experiments on eight aspect - based   sentiment analysis benchmarks , including six En-   glish datasets , one Chinese dataset , and one Korean   datase . The data statistics is shown in Appendix   A.3 . We use Stanza ( Qi et al . , 2020 ) as the external   parser to produce dependency parses for compar-   ing with dependency tree based models , reporting   accuracy ( Acc . ) and macro - f1 ( F1 ) scores for each   model . More details are presented in Appendix   A.1 .   MAMS Jiang et al . ( 2019 ) provide a recent chal-   lenge dataset with 4,297 sentences and 11,186 as-   pects . We take it as the main dataset because it is a   large - scale multi - aspect dataset with more aspects   in each sentence compared to the other datasets .   MAMS - small is a small version of MAMS .   Chinese hotel reviews dataset Liu et al . ( 2020 )   provide manually annotated 6,339 targets and 2,071   items for multi - target sentiment analysis .   Korean automotive comments dataset Hyun et al .   ( 2020 ) provide a dataset with 30,032 comment-   aspect pairs in Korean .   SemEval datasets We use five SemEval datasets ,   including twitter posts ( Twitter ) from Dong   et al . ( 2014 ) , laptop comments ( Laptop ) provided2055   by Pontiki et al . ( 2014 ) , restaurant reviews of Se-   mEval 2014 task 4 ( Rest14 ; Pontiki et al . 2014 ) ,   SemEval 2015 task 12 ( Rest15 ; Pontiki et al .   2015 ) and SemEval 2016 task 5 ( Rest16 ; Pontiki   et al . 2016 ) . These datasets are pre - processed fol-   lowing Tang et al . ( 2016 ) and Zhang et al . ( 2019 ) .   4.1 Baselines   We denote our model as dotGCN ( discrete opinion   tree GCN ) , making comparisons with BERT - based   models , including models without using trees and   dependency tree based models . In addition , the   variational inference baseline ( Section 3.1 ) is de-   noted as viGCN . Baselines are ( 1 ) BERT - SPC is a   simple baseline by fine - tuning the vector of “ [ CLS ] ”   of BERT from Jiang et al . ( 2019 ) ; ( 2 ) AEN . Song   et al . ( 2019 ) use an attentional encoder with BERT ;   ( 3)CapsNet . Jiang et al . ( 2019 ) combine capsule   network with BERT ; ( 4 ) Hard - Span . Hu et al .   ( 2019 ) use RL to determine aspect - specific opinion   spans ; ( 5 ) depGCN . Zhang et al . ( 2019 ) applies   aspect - specific GCNs over dependency trees ; ( 6 )   RGAT . Wang et al . ( 2020 ) use relational graph at-   tention networks over aspect - centered dependency   trees to incorporate the dependency edge type infor-   mation ; ( 7 ) SAGAT . Huang et al . ( 2020 ) use graph   attention network and BERT , exploring both syn-   tax and semantic information in the sequence ; ( 8)   DGEDT . Tang et al . ( 2020 ) jointly consider BERT   outputs and dependency tree based representations   by a bidirectional GCN . ( 9 ) kumaGCN . Chen et al .   ( 2020 ) combine the dependency trees and latent   graphs induced by self - attention neural networks ;   4.2 Development Results   We perform development experiments using   MAMS since this is the largest dataset and the   examples are more challenging compared to the   other datasets . We implement three baselines , in-   cluding BERT - SPC , depGCN and kumaGCN . For   fair comparison , we also combine depGCN and   kumaGCN with the syntax regularization loss in   Eq 7 by calculating syntactic distances on the input   dependency trees with respect to the aspect terms .   Table 1 shows the results on MAMS validation-   set . BERT - SPC achieves 84.08 accuracy and 83.52   F1 . Surprisingly , the dependency tree based mod-   els can not outperform BERT - SPC , which veri-   fies the limitation of using cross - domain depen-   dency parsers for this task . kumaGCN outperforms   depGCN due to its ability to include an implicit   latent graph . Adding the syntax regularization   loss generally improves the model performance   of syntax - based models . In particular , kumaGCN +   Lis on par with BERT - SPC .   viGCN outperforms kumaGCN + Land   depGCN + L , which shows the potential of struc-   tured latent tree models . Our dotGCN model   achieves 84.53 accuracy and 83.97 F1 , outperform-   ing all the baselines by a large margin , which em-   pirically shows the induced discrete opinion tree is   promising to this task . Compared to viGCN , our   model gives better scores . In addition , our model   converges nearly 1.8 times faster ( 0.66h / epoch v.s.   1.25h / epoch ) than viGCN . dotGCN does not have   to calculate the true posterior distribution over   structured tree samples and thus largely reduce   computation overhead .   Ablation Study Table 1 shows ablation studies on   MAMS validation set by removing three proposed   loss items during training , namely L , Land   L. We can observe that the model performance   degrades after removing either one of them . Re-   moving the syntax regularization loss Lslightly   hurts the performance . Without using the atten-   tion consistency loss L , the model falls behind   BERT - SPC , which suggests the importance of our   proposed attention consistency regularizations . Ex-   cluding the reinforcement learning loss leads to the2056   biggest performance drop ( Acc : 84.53→83.48 )   among the three settings . This shows that the rein-   forcement learning component plays a central role   in the full model .   4.3 Main Results   MAMS Table 2 shows the results of dotGCN   and the baselines from Jiang et al . ( 2019 ) on   the MAMS test set . We implement BERT - SPC ,   denoted as BERT - SPC , which outperforms the   BERT - SPC model of Jiang et al . ( 2019 ) . Compared   to baselines ( BERT - SPC , CapsNet , CapsNet - DR   and BERT - SPC ) without using dependency trees ,   dotGCN gives significantly better results ( p <   0.01 ) . For fair comparison with dependency tree   based models , we also implement depGCN+ L   and kumaGCN+ L. depGCN+ Lachieves 84.36   accuracy and 83.88 F1 on the MAMS test set .   kumaGCN+ Lgives similar results with 84.37 ac-   curacy and 83.83 F1 . Our dotGCN outperforms all   the baselines , giving 84.95 accuracy and 84.44 F1 .   In terms of the averaged accuracy of F1 scores on   MAMS and MAMS - small , dotGCN is significantly   better than depGCN and kumaGCN ( p < 0.05 ) .   The results demonstrate that the induced aspect-   specific discrete opinion trees are promising to han-   dle multi - aspect sentiment tasks .   Multilingual The resultson the Chinese hotel   review dataset are shown in Table 2 . dotGCN out-   performs the baseline BERT - SPCby 0.72 accu-   racy points and 0.61 F1 , respectively . The result   shows that our model can be generalized across   languages without relying on language - specific   parsers . On the Korean dataset , we obtain 5.20   accuracy and 11.61 F1 improvements compared to   the LCF - BERT ( Zeng et al . , 2019 ) , which is the   best BERT - based model . These results show that   our model can be well generalized to multiple lan-   guages and may potentially benefits low - resource   languages for this task .   SemEval Table 3 shows the results of our model   on the SemEval datasets . First , tree based graph   neural network models are generally better than   BERT - SPC . On the five datasets , which are rela-   tively small , our model still achieves competitive   performances in terms of the averaged F1 and ac-   curacy scores as shown in Table 3 . In particular ,   our model in general outperforms depGCN and   depGCN + Lon four out of five datasets , which   verifies that the reinforced discrete opinion trees   can be promising structured representations com-   pared to auto - parsed dependency trees .   We also compare our models with span - based re-   inforcement learning models ( Hard - Span ; Hu et al .   ( 2019 ) ) on the dataset of laptops and restaurants   preprocessed by Tay et al . ( 2018 ) . As shown in   Table 4 , our model outperforms Hard - Span by 2.55   accuracy points on laptops . On restaurants , our   model achieves a comparable result to Hard - Span .   It shows that the opinion tree is a better representa-   tion compared to an opinion span .   4.4 Case Study   Figure 3a and Figure 3b show the induced tree and   dependency parse for the aspect term “ scallops ” , re-   spectively . The opinion words “ unique ” and “ tasty”2057   are far away from the aspect ( more than 10 words )   in the dependency tree . In the induced tree by dot-   GCN , the opinion word “ tasty ” and “ unique ” are   2 and 3 depths from the aspect “ scallops ” respec-   tively , which shows that dotGCN can potentially   handle complex interactions among aspects and   opinion contexts . In addition , the tree induced by   dotGCN is binarized , and the root node can contain   multiple words as shown in Figure 4a .   Figure 4a and Figure 4b show the induced trees   for two aspect terms with different sentiment po-   larities . For “ creme brulee ” , the policy network   assigns high weights to both “ delicious ” and “ sa-   vory ” . Interestingly , it assigns a higher weight to   “ delicious ” than “ savory ” , though “ savory ” is closer   to its aspect term than “ delicious ” . For “ appetizer ” ,   the word “ interesting ” receives higher attention   scores than the other two sentiment words . These   results show that dotGCN is able to distinguish dif-   ferent sentiment contexts for different aspect terms   in the same sentence .   4.5 Analysis   Distances between Aspect Terms and Opinion   Words Figure 5 shows the distances between as-   pect terms and opinion words . We use the anno-   tated opinion words of Rest16 provided by Fan et al .   ( 2019 ) to compare our induced trees and depen-   dency trees . The distances calculated over the orig-   inal sequences are also included . We can observe   that the distance distribution over the sequences is   relatively flat compared to that over tree structures .   For the two tree structures , nearly 90 % of opinion   words are within 3 depths from the aspect terms .   The distance distribution of our induced trees is   similar to that of the dependency trees , which em-   pirically demonstrates that induced discrete trees   are able to capture the interactions between aspect   terms and opinions . By treating dependency trees   as gold standard , our tree inducer obtains 35.4 %   unlabeled attachment scores ( UAS ) , which shows   the induced trees are significantly different from   the dependency trees although both can connect   opinion words with aspect terms .   Low frequent aspects Table 5 shows the classifi-2058   cation accuracy of the MAMS test set with respect   to the aspect frequency . For aspect terms which   appear in the training corpus , both methods give   similar results . However , for unseen aspects , dot-   GCN gives better results than depGCN . This is   potentially due to the severe parsing errors for the   low - frequent aspects . dotGCN does not depend on   external parsers and thus can circumvent this prob-   lem . It empirically suggests that the induced tree   structures have strong robustness for capturing the   aspect - opinion interactions compared to depGCN .   5 Related Work   Tree Induction for ABSA There has been much   work on unsupervised discrete induction ( Bow-   man et al . , 2016a ; Shen et al . , 2018b ; Kim et al . ,   2019b , a ; Jin et al . , 2019 ; Cao et al . , 2020 ; Yang   et al . , 2021 ; Dai et al . , 2021 ) , which aims to obtain   general constituent trees without explicit syntax   annotations and task - dependent supervised signals .   We focus on learning task - specific tree - structures   for ABSA , where the tree is fully binarized and lex-   icalized . Choi et al . ( 2018 ) propose Gumbel Tree-   LSTM for learning task - specific tree for seman-   tic compositions . Similarly , Maillard et al . ( 2019 )   propose an unsupervised chart parser for jointly   learning sentence embeddings and syntax . How-   ever , they focus on sentence - level tasks and do not   consider aspect information .   Aspect - level Sentiment Classification Much re-   cent work has explored neural attention mecha-   nism to this task ( Tang et al . , 2016 ; Ma et al . ,   2017 ; Li et al . , 2018 ; Liang et al . , 2019 ) . Among   tree - based methods , Zhang et al . ( 2019 ) and Sun   et al . ( 2019b ) encode dependency tree using GCN   for aspect - level sentiment analysis ; Zhao et al .   ( 2019 ) use GCN to model fully connected graphs   between aspect terms ; Wang et al . ( 2020 ) use re-   lational graph attention networks to incorporate   the dependency edge type information , and con-   struct aspect - specific graph structures ; Barnes et al .   ( 2021 ) attempt to directly predict dependency-   based sentiment graphs . Tang et al . ( 2020 ) use   duel - transformer structure to enhance the depen - dency graph for this task . Our work is similar   in that we also consider the structure dependen-   cies , but different in that we rely on automatically   induced tree structures instead of external parses .   Chen et al . ( 2020 ) propose to induce aspect - specific   latent graph by sampling from self - attention - based   Hard Kumaraswamy distributions ( Bastings et al . ) .   However , to achieve competitive performance , their   method still requires a combination of external de-   pendency parse trees and the induced latent graphs .   Sun et al . ( 2019a ) and Xu et al . ( 2019 ) con-   structed aspect related auxiliary sentences as inputs   to BERT ( Devlin et al . , 2019 ) for strong contextual   encoders . Xu et al . ( 2019 ) proposed BERT - based   post training for enhancing domain - specific contex-   tual representations for aspect sentiment analysis .   Our work shares a similar feature extraction ap-   proach , but differently we focus on inducing latent   trees for ABSA .   6 Conclusion   We proposed a method to induce aspect - specific dis-   crete opinion trees for aspect - based sentiment anal-   ysis , obtaining trees by viewing aspect - to - context   attention scores as syntactic distances . The atten-   tion scores are trained using both RL and a novel   attention - based regularization . Our model empiri-   cally achieves competitive performance compared   with dependency tree based models , while being   independent of parsers . We also provide a theoretic   view of our method using variational inference .   Acknowledgements   Zhiyang Teng and Yue Zhang are the correspond-   ing authors . Our thanks to anonymous reviewers   for their insightful comments and suggestions . We   appreciate Prof. Pengyuan Liu sharing the Chinese   Hotel dataset , Prof. Jingjing Wang sharing the re-   inforcement learning code of Wang et al . ( 2019 ) ,   Mr. Chuang Fan helping obtain the MAMS - Small   dataset , Prof. Hwanjo Yu and Mr. Dongmin Hyun   sharing the Korean automotive datasets , Prof. De-   jiang Dou and Mr. Amir Veyseh responding to   our questions when reproducing their results on   MAMS , Mr. Zhen Wu for releasing their codes of   Wu et al . ( 2020 ) upon our request . We thank Dr.   Xuebin Wang for providing us with 2 V100 GPU   cards for use . This publication is conducted with   the financial support of “ Pioneer ” and “ Leading   Goose ” R&D Program of Zhejiang under Grant   Number 2022SDXHDX0003.2059References2060206120622063   A Appendix   A.1 Settings   Our codes are implemented based on the Py-   Torch Transformers library ( Wolf et al . , 2020 ) .   We use bert - based - uncasedfor English , bert-   base - chinesefor Chinese , bert - base - multilingual-   uncasedfor Korean . We tune the hyper-   parameters on the MAMS dataset . We select the   best model according to the accuracy scores on   the development set . For each model , we train   it 10 epochs with the Adam optimizer ( Kingma   and Ba , 2014 ) . The initial learning rate for fine-   tuning BERT parameters is 2eand the weight   decay is 1e . The number of GCN layers is 2 by   following Zhang et al . ( 2019 ) . The hidden size   of the MLP layer in Eq 1 is 256 . For the pol-   icy network training , we generate M= 3 trees .   λ = λ = λ= 0.1 . For the variational in-   ference model , β= 0.05 . We try five options for   these hyper - parameters ( λ , λ , λ ) including 0 ,   0.01 , 0.05 , 0.1 and 0.2 . We report accuracy ( Acc . )   and macro - f1 ( F1 ) scores for each model . For the   other settings about neural network architectures   and reinforcement learning , we follow Zhang et al .   ( 2019 ) and Shi et al . ( 2019 ) , respectively .   We run our models using a single GPU Card   ( TitanXP 1080ti or Titan XP 2080 or V100 ) . Each   training epoch for MAMS taskes about 40 mins .   A.2 Statistics of Tay et al . ( 2018 ) ’s dataset   We compare our discrete opinion tree RL model   with span - based RL model on a dataset prepro-   cessed by Tay et al . ( 2018 ) . Table 6 shows the   statistics .   A.3 Data   Table 7 shows the data statistics . The   MAMS dataset can be obtain from . The   five SemEval datasets can be downloaded from , the Chinese dataset can be   obtained from   and the Korean dataset can be obtained from .2064