  Chantal AmrheinFlorian SchottmannRico SennrichSamuel LäubliUniversity of Zurich , Textshuttle , ETH Zurich , University of Edinburgh   { amrhein,sennrich}@cl.uzh.ch , { schottmann,laeubli}@textshuttle.ai   Abstract   Natural language generation models reproduce   and often amplify the biases present in their   training data . Previous research explored us-   ing sequence - to - sequence rewriting models to   transform biased model outputs ( or original   texts ) into more gender - fair language by cre-   ating pseudo training data through linguistic   rules . However , this approach is not practi-   cal for languages with more complex morphol-   ogy than English . We hypothesise that creat-   ing training data in the reverse direction , i.e.   starting from gender - fair text , is easier for mor-   phologically complex languages and show that   it matches the performance of state - of - the - art   rewriting models for English . To eliminate the   rule - based nature of data creation , we instead   propose using machine translation models to   create gender - biased text from real gender - fair   text via round - trip translation . Our approach   allows us to train a rewriting model for German   without the need for elaborate handcrafted rules .   The outputs of this model increased gender-   fairness as shown in a human evaluation study .   1 Introduction   From facial recognition to job matching and medi-   cal diagnosis systems , numerous real - world appli-   cations suffer from machine learning models that   are discriminative towards minority groups based   on characteristics such as race , gender , or sexual   orientation ( Mehrabi et al . , 2021 ) . In natural lan-   guage processing ( NLP ) , gender bias is a particu-   larly significant issue ( Sheng et al . , 2021b ) . While   research in psychology , linguistics and education   studies demonstrates that inclusive language can in-   crease the visibility of women ( Horvath et al . , 2016 ;   Tibblin et al . , 2022 ) and encourage young individu-   als of all genders to pursue stereotypically gendered   occupations ( Vervecken et al . , 2013 , 2015 ) without   Figure 1 : De - biasing rewriters can be implemented as   neural sequence - to - sequence models trained on source   ( S ) to target ( T ) text examples . Previous work creates   artificial Tfrom real Sthrough complex augmentation   ( a ) . We propose to use real Tand generate artificial Sto   accommodate morphologically complex languages and   avoid target - side noise ( b ) . Furthermore , we show that   by leveraging biased off - the - shelf machine translation   ( MT ) models , complex rules can be avoided altogether   to generate training data for de - biasing rewriters ( c ) .   sacrificing comprehensibility ( Friedrich and Heise ,   2019 ) , state - of - the - art text generation models still   overproduce masculine forms and perpetuate gen-   der stereotypes ( Stanovsky et al . , 2019 ; Nadeem   et al . , 2021 ; Renduchintala and Williams , 2022 ) .   There is a variety of work on correcting gen-   der bias in generative models . Some approaches4486include curating balanced training data ( Saunders   et al . , 2020 ) , de - biasing models with modifications   to the training algorithms ( Choubey et al . , 2021 ) ,   and developing better inference procedures ( Saun-   ders et al . , 2022 ) . The focus of our work lies on   so - called rewriting models , yet another line of de-   biasing research that revolves around models that   map any input text ( e.g. , the output of a biased   generative model ) to a gender - fair version of the   same text . The main challenge here is training data :   due to the lack of large amounts of parallel biased-   unbiased text segments , previous work ( Section 2 )   produces the latter through handcrafted rules ( For-   ward Augmentation , Figure 1a ) .   We identify two key problems with the For-   ward Augmentation paradigm . First , the rule - based   de - biasing of real - world text comes at a risk of   introducing target - side noise , which tends to de-   grade output quality more than source - side noise   ( Khayrallah and Koehn , 2018 ; Bogoychev and Sen-   nrich , 2019 ) . Second , while already intricate for   English , it is likely even harder to define de - biasing   rules for more morphologically complex languages   with grammatical gender ( Figure 2 ) . An approach   proposed by Diesner - Mayer and Seidel ( 2022 ) ,   for example , requires morphological , dependency   and co - reference analysis , as well as named entity   recognition and a word inflexion database .   In this paper , we propose two modifications to   the prevalent data augmentation paradigm and we   find that biased models can be used to train de-   biasing rewriters in a simple yet effective way . Our   three main contributions are :   •We reverse the data augmentation direction   ( Backward Augmentation , Figure 1b ) . By us-   ing human - written unbiased segments filtered   from large monolingual corpora as target - side   data , we train a neural rewriting model that   matches or outperforms the word error rate   ( WER ) of two strong Forward Augmentation   baselines in English ( Section 3.3 ) .   •We dispose of handcrafted de - biasing rules   ( Round - trip Augmentation , Figure 1c ) . By   leveraging biased off - the - shelf NLP models ,   we train a neural rewriting model that out-   performs the WER of a heavily engineered   rule - based system in German ( Section 4.3 ) .   •We test our best model with potential stake-   holders . In our human evaluation campaign ,   participants rated the outputs of our German   rewriter model as more gender - fair than the   original ( biased ) input texts ( Section 4.4 ) .   2 Background   Gender - fairrewriting is a conditional text gen-   eration problem . It can be approached with con-   ventional sequence - to - sequence models trained on   large amounts of parallel data , i.e. , biased segments   Sand their gender - fair counterparts T. Since such   corpora do not exist in practice , Sun et al . ( 2021 )   and Vanmassenhove et al . ( 2021 ) create artificial   gender - fair target segments T from existing   source segments Swith a rule - based de - biasing   pipeline ( Forward Augmentation , Figure 1a ) . The   sequence - to - sequence model trained on the outputs   of this pipeline removes the need for computation-   ally expensive toolchains ( such as dependency pars-   ing ) at runtime and increases robustness towards   noisy inputs ( Vanmassenhove et al . , 2021 ) .   2.1 Rule - based De - biasing for English   Converting StoT is relatively straightfor-   ward for languages like English , which only ex-   presses social gender in pronouns and a small set   of occupation nouns . A simple dictionary lookup   is often sufficient to produce a gender - fair variant   of the biased text , except for two issues:4487   These issues are tractable for English because they   only happen in a limited number of cases that can   be covered with a limited number of rules . Sun   et al . ( 2021 ) solve Issue 1 based on part - of - speech ,   morphological and dependency information , and Is-   sue 2 by scoring different variants with a language   model . Vanmassenhove et al . ( 2021 ) solve Issue   1using a grammatical error correction tool and   Issue 2 using part - of - speech , morphological and   dependency information . Both Sun et al . ( 2021 )   and Vanmassenhove et al . ( 2021 ) train Transformer   models ( Vaswani et al . , 2017 ) using the original   texts as the source and the de - biased augmenta-   tions as target data ( Forward Augmentation ) , and   achieve WER below 1 % on several test sets .   2.2 Rule - based De - biasing for Other   Languages   In languages with more complex morphology , Is-   sue 1 is much more prevalent than in English but   Issue 2 is even more challenging because it requires   animacy prediction : A direct application of Van-   massenhove et al . ’s ( 2021 ) “ first rule - based , then   neural ” approach to Spanish ( Jain et al . , 2021 ) re-   sults in a model that does not distinguish between   human referents and objects . Similarly , Alhafni   et al . ( 2022)train an end - to - end rewriting system   for Arabic but their pipeline for creating training   data requires labelled data to train an additional   classification model to identify gendered words .   Both of these works only focus on a binary inter-   pretation of gender . Non - sequence - to - sequence ap-   proaches have also been explored ( Zmigrod et al . ,   2019 ; Diesner - Mayer and Seidel , 2022 ) but re-   quired extensive linguistic tools such as morpholog-   ical , dependency and co - reference analysis , named   entity recognition and word inflexion databases .   2.3 Round - trip Translation   Previous work employed round - trip translations   to create pseudo data for automatic post - editing   ( Junczys - Dowmunt and Grundkiewicz , 2016 ; Fre-   itag et al . , 2019 ; V oita et al . , 2019 ) , grammatical   error correction ( Madnani et al . , 2012 ; Lichtarge   et al . , 2019 ) or paraphrasing ( Mallinson et al . , 2017 ;   Iyyer et al . , 2018 ; Fabbri et al . , 2021 ; Cideron et al . ,2022 ) . While such uses of round - trip translations   exploit the fact that machine translations can be   diverse and can contain accuracy and fluency er-   rors , we are the first to exploit them for their social   biases .   3 Backward Augmentation   We hypothesise that the data augmentation direc-   tion for gender - fair rewriters can be reversed ( Fig-   ure 1b ) without a negative impact on quality . Our   motivation is rooted in work on data augmenta-   tion for MT ( Sennrich et al . , 2016b ) , where back-   translation of monolingual target text tends to re-   sult in better quality than forward - translation of   original source text ( Khayrallah and Koehn , 2018 ;   Bogoychev and Sennrich , 2019 ) . We use Back-   ward Augmentation to train a gender - fair rewriter   model for English and compare its performance to   the Forward Augmentation approach proposed by   Vanmassenhove et al . ( 2021 ) and Sun et al . ( 2021 ) .   3.1 Method   We propose to filter large monolingual corpora for   gender - fair text Tand use a rule - based pipeline to   derive artificially biased source text S from   T. Based on this data , we can train a sequence - to-   sequence model which maximises p(T|S , θ ) ,   rather than p(T|S , θ)as in previous work   ( Section 2 ) .   3.2 Experimental Setup   Data We extract English training data from OS-   CAR ( Abadji et al . , 2022 ) , a large multilingual   web corpus . For Forward Augmentation , we select   segments that contain at least one biased word as   S , following Vanmassenhove et al . ’s ( 2021 ) and   Sun et al . ’s ( 2021 ) lookup tables ( Appendix E ) .   For Backward Augmentation , we filter for seg-   ments that contain at least one of the correspond-   ing gender - fair words in the lookup tables as T.   We filter out duplicates and noisy segments with   OpusFilter ( Aulamo et al . , 2020 ) and then ran-   domly subselect 5 M segments each . For both mod-   els and as in previous work , we extend the training   data by creating complementary source versions   with only masculine forms , only feminine forms   and copies of gender - fair targets and by adding ad-   ditional non - gendered segments where no rewriting   is necessary ( amounting to 30 % of the total data ) .   A full overview of the training data can be found   in Appendix A.4488   Rule - based Processing To be able to compare   directly to previous work , we first reproduce the   rule - based Forward Augmentation approach pro-   posed by Sun et al . ( 2021 ) and Vanmassenhove   et al . ( 2021 ) to create T fromS. We combine   their lookup tables ( Appendix E ) and re - implement   their rules based on part - of - speech , morphological   and dependency information via spaCy(Honnibal   et al . , 2020 ) , both for resolving ambiguities and   producing the correct number for verbs . We decide   to follow Sun et al . ( 2021 ) and use “ themself ” and   not “ themselves ” as a gender - fair form of “ herself ”   and “ himself ” . Taking this implementation as a ba-   sis , we derive a Backward Augmentation pipeline   by reversing the lookup tables and rules to map   fromTtoS .   Model Architecture Following Sun et al . ( 2021 )   and Vanmassenhove et al . ( 2021 ) , we train 6 - layer   encoder , 6 - layer decoder Transformers ( Vaswani   et al . , 2017 ) with 4 attention heads , an embed-   ding and hidden state dimension of 512 and a   feed - forward dimension of 1024 . For optimiza-   tion , we use Adam ( Kingma and Ba , 2015 ) with   standard hyperparameters and a learning rate of   5e−4 . We follow the Transformer learning sched-   ule in Vaswani et al . ( 2017 ) with a linear warmup   over 4,000 steps . The only differences to Sun et al .   ( 2021 ) and Vanmassenhove et al . ( 2021 ) are that   we train our models with sockeye 3 ( Hieber et al . ,   2022 ) and use a smaller joint byte - pair vocabulary   ( Sennrich et al . , 2016c ) of size 8k computed with   SentencePiece ( Kudo and Richardson , 2018 ) .   3.3 Automatic Evaluation   Test Sets We benchmark our models with the test   sets published in conjunction with our baselines :   •Sun et al . ( 2021 ) : Two test sets   ( gendered / non - gendered ) with 500 sen - tence pairs each , from five different domains :   Twitter , Reddit , news articles , movie quotes   and jokes . For the gendered version , there are   balanced numbers of sentences with feminine   and masculine pronouns for each domain .   The non - gendered source texts do not contain   any forms that need to be rewritten and should   not be changed .   •Vanmassenhove et al . ( 2021 ) : Three test sets   from three different domains : OpenSubtitles   ( Lison and Tiedemann , 2016 , 500 sentence   pairs ) , Reddit ( Baumgartner et al . , 2020 , 500   sentence pairs ) , and WinoBias+ ( Zhao et al . ,   2018 , 3,167 sentence pairs ) . Each test set   has a balanced amount of gender - fair pronoun   types .   We manually double - check the target side of all test   sets from previous work and if necessary correct   sporadic human annotation errors . The test sets   used by Vanmassenhove et al . ( 2021 ) also cover   grammatical error corrections outside the scope   of gender - fair rewriting . To restrict evaluation to   the phenomenon of interest , we produce a target   side version that only covers gender - fair rewriting .   Note that this means that the model outputs by   Vanmassenhove et al . ( 2021 ) will perform slightly   worse on this version of the test set than reported   in their paper because this model also makes such   additional grammatical corrections . We revert tok-   enization and change “ themselves ” to “ themself ” in   the model outputs of Vanmassenhove et al . ( 2021 )   to be able to compare them against our models ’   outputs and our references .   Method We evaluate our English model out-   puts and compare them to previous work with   tokenisedWER based on the Python package4489jiwer . We compute statistical significance p <   0.05with paired bootstrap resampling ( Koehn ,   2004 ) , sampling 1,000 times with replacement .   Results Results are shown in Table 1 . Backward   Augmentation matches the low WER of the orig-   inal as well as our combined reproduction of the   Forward Augmentation models by Sun et al . ( 2021 )   and Vanmassenhove et al . ( 2021 ) , and performs   slightly better than previous work on OpenSubti-   tles and Reddit and WinoBias+ .   4 Round - trip Augmentation   Artificially biasing gender - fair target segments   rather than de - biasing gender - biased source seg-   ments is especially useful for languages with gram-   matical gender and more complex morphology than   English . Taking German as a running example ,   we would need some form of animacy prediction   in Forward Augmentation to transform ambiguous   nouns such as “ Leiter ” only if they refer to a person   ( “ leader ” ) and not to an object ( “ ladder ” ) . In Back-   ward Augmentation , we do not need an animacy   prediction model since this information is implic-   itly encoded in the gender - fair forms , as seen in   Figure 2 . Nevertheless , defining rules for mapping   gender - fair segments to gender - biased segments or   vice versa requires expert knowledge and will likely   never completely cover morphologically complex   languages .   As an alternative to handcrafting rules , we pro-   pose to exploit the fact that current MT models   generate inherently biased text : we create pseudo   source segments via round - trip translation through   a pivot language that ( mostly ) does not mark gen-   der ( Figure 1c ) . We use this method to train a   gender - fair rewriter model for German , and bench-   mark it against a highly engineered fully rule - based   baseline ( Diesner - Mayer and Seidel , 2022 ) .   4.1 Method   We propose to filter large monolingual corpora for   gender - fair text Tand use off - the - shelve MT to   first translate Tinto a pivot language as P ,   and then translate P back into the original   language as S . As in Backward Augmenta-   tion , we use the resulting data to train a sequence-   to - sequence model that maximises p(T|S , θ ) .   We enrich this framework with several extensions   as detailed in the next section and evaluated sepa-   rately in Section 4.3.4.2 Experimental Setup   Data We filter OSCAR ( Abadji et al . , 2022 )   for German sentences that contain at least one   gender - fair form with simple regular expressions   that match gender - fair patterns ( Appendix F ) . After   creating pseudo sources with round - trip translation   ( see next paragraph ) and removing duplicates and   noisy segments with OpusFilter ( Aulamo et al . ,   2020 ) , we obtain 8.8 M parallel sentences . As in our   Backward Augmentation experiment ( Section 3.2 ) ,   we complement the augmented training data with   copies of the gender - fair segments on the source   side and non - gendered segments where no rewrit-   ing is necessary ( amounting to 30 % of total data ) .   Roundtrip Translation English is a natural   choice for the pivot language since it does not ex-   press gender in most animate words , meaning that   this information is often lost when translating from   a language with grammatical gender to English .   Indeed , we find that gender - fair forms are trans-   lated to generic masculine forms in about 90 % of   the cases when we translate them to English and   back to German . We make use of this bias to cre-   ate pseudo source segments , without the need for   any hand - crafted rules , by leveraging Facebook ’s   WMT 2019 models ( Ng et al . , 2019 ) for German-   to - Englishand English - to - German . To avoid   training on other translation differences aside from   gender - fair forms , we identify the counterparts of   gender - fair words in the round - trip translation and   merge those into the original gender - fair segment   to form the pseudo source . We explain our merging   algorithm in detail in Appendix C.   LM Prompting One potential issue we discov-   ered with our training data is that gender - fair plural   noun forms are much more frequent than gender-   fair singular noun forms . To boost singular forms ,   we generate additional gender - fair training data by   prompting GerPT2 - large(Minixhofer , 2020 ) –   a large German language model – using a seed   list of gender - fair animate nouns . Since we do   not want to bias the model towards segments that   start with a prompt , we sentence split the language   model outputs and only keep singular - form seg-   ments that either do not start with a prompt or that4490contain at least one other gender - fair form .   Gender Control As the majority of the nouns in   the German round - trip outputs are masculine forms ,   we create additional training data by finetuning the   English - to - German MT model on data marked with   sentence - level gender tags ( Appendix G ) , similar   to a previous approach for controlling politeness   in MT outputs ( Sennrich et al . , 2016a ) . We lever-   age the original training datafor the WMT 2019   shared task and finetune the original wmt19 - en - de   checkpoint for 50,000 steps with a batch size of 30   on a single GPU , following the official Hugging   Face ( Wolf et al . , 2020 ) translation finetuning script .   The resulting model does not always translate ac-   cording to the given tag but produces much more   balanced translations overall : with the feminine   tag , only 36 % of the produced forms are masculine   as compared to 90 % with the original checkpoint   and 94 % with the masculine tag .   Model Architecture We train a Transformer   model using the same hyperparameters and train-   ing procedure as in our Backward Augmentation   experiment described in Section 3.2 .   4.3 Automatic Evaluation   We compare the performance of our model to the   rule - based rewriter by Diesner - Mayer and Seidel   ( 2022 ) ; we are not aware of any neural rewriter for   German .   Test Set Diesner - Mayer and Seidel ( 2022 ) eval-   uate their system on a subset of the TIGER Tree-   bank ( Brants et al . , 2004 ) with German news ar-   ticles from the 1990s . Since masculine forms are   prevalent in these articles , we create a random set   of 1,200 TIGER sentences with gendered forms   which we balance for masculine , feminine , sin-   gular and plural forms , and a random set of 300   non - gendered sentences . For singular forms , we   decide to create our test set with a balanced mix   of forms referring to unspecific people as well as   real persons . There are two reasons for this design   choice : First , we want to closely mirror the setup   in English , where e.g. any occurrence of “ she ” or   “ he ” is rewritten to “ they ” , irrespective of whether   it refers to a specific person or not . Second , there   are several cases where we can not assume that an   input text referring to a specific person uses their   desired pronouns . One example is machine trans-   lation output from a language that does not mark   gender on pronouns . We believe that a rewriter that   produces gender - fair pronouns ( i.e. mentioning all   genders ) is less biased than one that assumes all   gender mentions in the input text are correct ( while   those could be actual instances of misgendering ) .   Method We compute tokenised WER and statis-   tical significance as in Section 3.3 .   Results Results are shown in Table 2 . Our sim-   plest model ( Round - trip Augmentation ) , which is   only trained on filtered data round - tripped with the   original Facebook model , already reduces the WER   compared to the biased inputs from the test set ( no   rewriting ) . Avoiding differences in roundtrip trans-   lations aside from gender - fair forms ( + merged )   further reduces the WER , as do additional training   examples obtained through a language model ( +   LM prompting ) and an MT system finetuned for   gender control ( + gender control ) .   Combining all of these extensions into a sin-   gle model ( + all ) results in the best WER . It also   performs surprisingly well compared to Diesner-   Mayer and Seidel ( 2022 ) , a rule - based system that   uses an abundance of language - specific NLP tools   for gender - fair rewriting ( Section 2.2 ) .   4.4 Human Evaluation   While making biased text more gender - fair ac-   cording to our automatic evaluation , our best-   performing model still produces numerous errors :   13.18 % of the words in the model ’s output differ   from the gender - fair reference texts in our test set   ( Table 2 ) . We conduct a human quality rating ex-   periment to assess whether the imperfect outputs   of this model are perceived as more gender - fair and4491whether erroneous gender - fair forms in general are   preferred over unaltered gender - biased text .   Participants To refrain from convenience sam-   pling and to focus on potential beneficiaries of our   model ( i.e. , people who may be offended by gender-   biased text ) , we recruit 294 volunteers ( 141 female ,   82 non - binary , 55 male , 16 other ) through unpaid   posts in newsletters and social media channels of   special interest groups against gender discrimina-   tion ( Appendix I ) . Participation is anonymous and   voluntary .   Materials We select one paragraph each from six   unrelated German texts ( P1–6 , Appendix H ) . Each   paragraph contains at least one gender - biased word   and is about an unspecific person whose gender   is unknown , a specific non - binary person , and/or   a group of people whose gender is unknown . We   use three transformations of each paragraph in the   experiment : unaltered ( Original ) , automatically de-   biased by our best model ( Rewriter ) , and manually   de - biased by one of the authors ( Gold ) .   Task and Procedure We use a 6 ( paragraphs )   x 3 ( transformations ) mixed factorial design , im-   plemented as a questionnaire in three versions ( A –   C ) to which participants are assigned at random .   Participants see all factor levels but not all com-   binations : to avoid repetition priming , each ques-   tionnaire includes all six paragraphs in the same   order but in different transformations . For example ,   P1 is presented as Original in questionnaire A and   as Rewriter in questionnaire B , etc . ; participants   are not informed if and how the paragraphs were   transformed for gender - fairness .   After completing a pre - experiment survey with   demographic questions , participants are shown a   single paragraph at a time and asked if that para-   graph is gender - fair , which they answer using a 5-   point Likert scale ( 1 : strongly disagree , 5 : strongly   agree ) . A short post - experiment survey on general   opinions regarding gender - fair writing concludes   the experiment .   Results The distribution of Likert - scale rat-   ings by transformation is shown in Figure 3 .   Albeit not as good as the human references   ( Gold , mean=3.98 ) , our model outputs ( Rewriter ,   mean=2.93 ) are rated better than the unaltered   gender - biased paragraphs ( Original , mean=1.79 )   overall . This finding equally holds for all indi-   vidual paragraphs ( Table 3 ): Rewriter consistently   outperforms Original in terms of perceived gender-   fairness and , in some cases , comes close to Gold   ( P5 , P6 ) .   While these findings confirm – as in our auto-   matic evaluation in Section 3.3 – that our model out-   puts are more gender - fair than original input texts ,   it leaves the most relevant question for use in prac-   tice unanswered : if given the choice , would users   choose potentially erroneous Rewriter outputs over   error - free but gender - biased original texts ? We in-   clude this question in the post - experiment survey   independently of any specific text , and compare   participants ’ responses with their average rating for   Original and Rewriter outputs in the main part of   the experiment . Out of the 294 participants , 201   ( 68.37 % ) disagreed or strongly disagreed that “ If   a text has errors in gender - fair wording , I prefer   an error - free non - gender - fair version ( e.g. , generic   masculine ) instead . ” , and 198 ( 98.51 % ) of these   participants rated Rewriter more gender - fair than   Original in the experiment . In comparison , 35   ( 68.63 % ) of the 51 participants who agreed or   strongly agreed with that statement still gave higher   gender - fairness scores to Rewriter in the experi-   ment ( where , recall from above , the type of trans-   formation was not revealed).44925 Discussion   Our experimental findings yield insights for fu-   ture research and design implications for real - world   NLP applications .   Biased models are useful for de - biasing . At   least in the subfield of gender - fair rewriting , de-   biasing research has focussed extensively on hu-   man annotation ( Qian et al . , 2022 ) and rule - based   processing and training data creation ( e.g. , Sun   et al . , 2021 ; Vanmassenhove et al . , 2021 ; Jain et al . ,   2021 ; Alhafni et al . , 2022 ; Diesner - Mayer and Sei-   del , 2022 ) . Conversely , our work demonstrates that   robust de - biasing rewriters can be implemented   by leveraging inherently biased NLP models . Our   Round - trip Augmentation experiment covers a sin-   gle language , but we note that both the training   data ( Abadji et al . , 2022 ) and MT models ( Ng et al . ,   2019 ) we leverage are readily available for many   other languages . We also assume that ( biased )   MT models – the only requirement for gender - fair   rewriters based on Round - trip Augmentation apart   from simple data filters – are available for more   languages or are easier to create than the different   NLP tools used in typical rule - based augmentation   pipelines , such as robust models for lemmatisa-   tion , morphological analysis , dependency parsing ,   named entity recognition , and co - reference resolu-   tion .   Rule - based de - biasing lacks robustness . Hand-   written rules are limited by design . For example , a   breakdown of the results shown in Table 2 ( see Ap-   pendix B ) reveals that Diesner - Mayer and Seidel ’s   ( 2022 ) rewriter handles masculine forms better than   our model ( with a WER as low as 7.81 ) . However ,   their rewriter performs poorly with feminine forms   ( with a WER as high as 22.22 , which is worse than   the WER of the biased input texts ) likely because   these forms are not covered in its rule set . Addition-   ally , we find that while Diesner - Mayer and Seidel ’s   ( 2022 ) approach features a solution for compounds ,   e.g. it can correctly de - bias “ Strassenbauarbeiter ”   ( road construction worker ) , this only applies to   compounds where the gendered word appears at the   end , e.g. it does not de - bias “ Arbeitergesetz ” ( em-   ployee legislation ) . Furthermore , their approach   uses a word database to identify gendered nouns   which does not generalise to unknown gendered   words . Finally , there is always a risk of error prop-   agation with the NLP tools used in rule - based ap-   proaches . We conclude that language - specific rulesets will likely never cover all relevant phenomena   for gender - fair rewriting . As shown by previous   work , seq2seq models provide a model - based alter-   native that boosts generalisation ( Vanmassenhove   et al . , 2021 ) which is why they should be used in   as many languages as possible . We believe that   Round - trip Augmentation provides an easy way   to create parallel data to train gender - fair rewrit-   ing models for new languages without the need for   in - depth linguistic knowledge of the language .   Users prefer errors over bias . Potential bene-   ficiaries of gender - fair rewriters – the 294 partic-   ipants of our human evaluation campaign – rated   the outputs of our German rewriter as more gender-   fair than the biased original texts and explicitly ( in   the post - experiment survey ) stated they prefer ( po-   tentially erroneous ) gender - fair forms over error-   free non - gender - fair forms . This is an important   finding because our model is far from perfect , as   evidenced by a high error rate compared to En-   glish and manual inspection of the outputs used in   the evaluation campaign ( Appendix H ) . Previous   work has found that non - binary people consider   NLP as harmful , particularly due to the risk of be-   ing misgendered by MT outputs ( Dev et al . , 2021 ) .   Vanmassenhove et al . ( 2021 ) caution that gender-   fair rewriters may not be applicable to languages   other than English because “ few languages have   a crystallized approach when it comes to gender-   neutral pronouns and gender - neutral word endings . ”   While there is an active debate ( and no established   standard ) about the form that gender - fair German   should take ( e.g. , Burtscher et al . , 2022 ) , our evalu-   ation campaign makes a strong case for using NLP   technology – even if not flawless – to offer one   form of de - biased text in real - world applications .   Since rewriters are relatively lightweight models   that operate independently from any input provider ,   be it a human author or a biased MT model , they   would seem suitable for integration into a wide   range of systems with reasonable effort .   6 Conclusions and Future Work   Despite an impressive performance in a wide range   of tasks and applications , state - of - the - art NLP mod-   els contain numerous biases ( Stanovsky et al . , 2019 ;   Nadeem et al . , 2021 ; Renduchintala and Williams ,   2022 ) . Our work shows that knowledge of a bias   can be used to correct that bias with the biased mod-   els themselves . In the case of gender - fair rewriting ,   we demonstrated that reversing the data augmen-4493tation direction and using round - trip translations   from biased MT models can substitute the prevalent   rewriting paradigm that relies on handcrafted and   often complex rules on top of morphological anal-   ysers , dependency parsers , and many other NLP   tools . In our case study for German , our model   surpasses the performance of a strong baseline in   terms of WER and produces outputs that were per-   ceived as more gender - fair than unaltered biased   text in a human evaluation campaign with 294 po-   tential beneficiaries .   While our approach enables the application of   gender - fair rewriting to any language for which   translation models exist , we believe there are sev-   eral other uses cases where biased models can be   leveraged for de - biasing , including dialect rewrit-   ing ( Sun et al . , 2022 ) , subjective bias neutralisation   ( Pryzant et al . , 2020 ) , and avoiding discrimination   in dialogue systems ( Sheng et al . , 2021a ) .   Limitations   While we consider our approach more easily appli-   cable to new languages than rule - based Forward   Augmentation , it relies on the existence of suffi-   cient original gender - fair text in the language of   interest and it is currently unclear what the mini-   mum amount of parallel data is to learn a gender-   fair rewriting model . Additionally , our survey only   targets affinity groups which limits the generalis-   ability of our results to all German speakers . Since   people who choose to not use gender - fair language   can simply not use a rewriting system , we do not   think that this lack of generalisability is a problem   in this case . Another limitation is that we use a   specific form of gender - fair German in our survey .   We made participants aware of this in a disclaimer   at the beginning of the survey . It should be stated   that there are many different acceptable gender - fair   forms in German ( see Section F ) . While using a   different gender - fair form could affect the individ-   ual ratings in our survey , we do not expect that it   would change our finding that Rewriter outputs are   rated more gender - fair than the Original texts .   Ethics Statement   Participation in our study was voluntary and fully   anonymous . We did not collect any personal data   that would allow us to identify people and did not   exclude any participants unless they specifically   requested their participation be ignored in the last   open commentary field of our survey or they statedtechnical difficulties . Concerning our rewriting   models , we did not filter the publicly available data   to exclude harmful content . However , since our   models mainly learn to copy text , we do not believe   they will hallucinate such text of their own accord .   Acknowledgements   We thank Marcos Cramer for their valuable inputs   and support with our human evaluation and all con-   tacts who shared our survey on their mailing lists   and social media accounts ( Appendix I ) as well   as all survey participants . Many thanks also go to   the OSCAR project who shared their data with us .   Further , we are grateful to Alex Flückiger , Anne   Göhring , Nikita Moghe and the anonymous review-   ers for their helpful feedback . Chantal Amrhein   and Rico Sennrich received funding from the Swiss   National Science Foundation ( project MUTAMUR ;   no . 176727 ) .   References4494449544964497A Additional Data Details   When filtering our data with OpusFilter ( Aulamo   et al . , 2020 ) , we define noisy segments as segments   that do not pass the following filters :   • LengthFilter : unit = word , min=1 , max=150   • LongWordFilter : threshold=40   • AlphabetRatioFilter : threshold=0.5   • LanguageIDFilter fasttext : threshold=0.0   The individual dataset sizes after deduplication   and filtering can be seen in Table 4 . Note that for   English , we restricted the total of gendered data to   15 million parallel segments to be comparable to   Sun et al . ( 2021 ) by only considering a subset of   the English portion of the OSCAR corpus ( Abadji   et al . , 2022 ) .   For German , we use Round - trip Augmentation   to produce the gendered pseudo sources . The   finetuned MT model we use to produce feminine   pseudo sources sometimes produces round - trip   translations that are identical to the gender - fair   segments or the round - trip translations with the   original checkpoint . Consequently , the Table has   fewer parallel segments for this category because   fewer unseen segments are added to the training   data overall for + gender control models in Table 2 .   Our English models in Table 1 are trained on the   following dataset combinations :   •Forward Augmentation Reimplementation   ( a+b ) : OSCAR SRC masculine + feminine +   gender - fair•Backward Augmentation : OSCAR TRG   masculine + feminine + gender - fair   Our German models in Table 2 are trained on   the following dataset combinations :   •Round - trip Augmentation ( + merged ) : OS-   CAR TRG masculine + gender - fair   •+ LM prompting : OSCAR TRG masculine +   gender - fair and LM TRG masculine + gender-   fair   •+ gender control : OSCAR TRG masculine +   feminine + gender - fair   •+ all : OSCAR TRG masculine + feminine +   gender - fair and LM TRG masculine + femi-   nine + gender - fair   Note that for every combination of data sets ,   we also add non - gendered data such that the non-   gendered data from OSCAR makes up 30 % of the   total parallel data .   B Detailed Results for German   We provide a more detailed evaluation of our re-   sults for German in Table 5 . The first two columns   show the results isolated for generic feminine and   generic masculine forms in the input that should be   rewritten as gender - fair forms . The third and fourth   columns show the results grouped by plural and   singular gender - fair forms , respectively . This eval-   uation highlights two points . First , we can see our   strategies introduced in Section 4.1 are effective for   the cases they were designed for : Using a gender-   aware machine translation model for round - trip   translation ( + gender control ) is particularly help-   ful on the feminine test set and adding language   model generated singular - form training data ( + LM   prompting ) reduces the WER on the singular test   set significantly . Second , as discussed in Section   5 , the results show that the rule - based approach by   Diesner - Mayer and Seidel ( 2022 ) is limited by de-   sign . While it performs well on generic masculine   forms , it does not cover generic feminine forms at   all which results in a higher WER than the source   where no rewriting is performed .   C Merging Algorithm   One issue with round - trip translations is that   they are likely to contain edits unrelated to the4498   gender - fair words in the target :   This round - trip translation not only contains the   desired generic masculine form for student ( marked   in orange ) but also several other deviations from   the original sentence ( marked in bold ) which even   changes the meaning slightly .   Ideally , we would want to identify the generic   masculine form and allow no other changes in the   pseudo source :   To this end , we develop a merging algorithm that   aims to insert the generic forms in the round - trip   translation into the context of the original gender-   fair segment . For all gender - fair words in the target   segment , we check if there are any close matches   in the round - trip translation using the difflib   Python librarywith a cutoff of 0.6 . If yes , we   replace the gender - fair word with its closest match .   This process can be seen in Algorithm 1 . If not   all gender - fair words can be matched in the round-   trip translation , we keep the round - trip translation   as our pseudo source and do not merge with the   gender - fair target . Note that this merging algorithm   can potentially introduce case and other grammati - Algorithm 1 Merging Round - Trip Translations   Input : list of tokens in gender - fair target t , list of   tokens in RT translation r   Output : merged pseudo source sfortoken wat index iintdo ifwis gender - fair form then m= get_close_matches ( w , r , 0.6 ) iflen(m)>0thent[i]=m[0 ] end if end ifend fors= detokenise ( t )   cal errors in the pseudo source . This is , however ,   not a serious problem as these potentially non-   grammatical forms will only occur on the source   side , meaning our model does not learn to pro-   duce such forms on the target side . The merging   algorithm could be improved in the future to also   consider grammatical acceptability , e.g. by scoring   the merged pseudo source against the gender - fair   target with a language model .   D Additional LM Prompting Details   Our prompts consist of a gender - fair determiner   and noun from a seed list , following the pattern   “ Ein*eNOUN * in ” . An example can be seen here   with the prompt in bold and gender - fair forms   generated by the language model in orange :   German:4499   Prompting large language models for gender - fair   text also works with other language models   and for languages other than German . Here ,   we show example gender - fair prompts ( marked   in bold ) and the text generated by a multilin-   gual language model . Many forms in the   generated text are also gender - fair ( marked   in orange ) even over long distances . Not all   generated forms include non - binary people ( e.g.   French “ il / elle ” ) but these can easily be normalised .   French :   Spanish :   E English Lookup Tables   For completeness , we list the lookup tables used in   our reproduction of Vanmassenhove et al . ( 2021 )   and Sun et al . ( 2021 ) and in the other direction for   our Backward Augmentation . E.1 Pronouns   E.2 Nouns   F German Gender - fair Patterns   For German , we can not use a lookup approach   to identify gender - fair noun forms but rather work   with several gender - fair patterns . Here , we describe   the different gender - fair forms we consider and , for   each , show a plural form example and its corre-   sponding pattern :   Pair forms are forms that explicitly state the   feminine and masculine form connected with a   coordinating conjunction like “ and ” or “ or ” . The4500order of the feminine and masculine forms can be   variable . This form assumes binary gender and   does not include non - binary people .   Example : Studentinnen und Studenten   Pattern :   Binnen - I forms are forms that take the feminine   form but with a capitalised “ I ” at the beginning of   the feminine suffix “ innen ” ( plural ) or “ in ” ( singu-   lar ) . This form also assumes binary gender .   Example : StudentInnen   Pattern : \w+Innen   Gender slash forms are forms that take the fem-   inine form but with a slash ( “ / ” ) separating the fem-   inine suffix “ innen ” ( plural ) or “ in ” ( singular ) from   the stem . This form also assumes binary gender .   Example : Student / innen   Pattern : \w+\ ? /\ ? innen   Gender gap forms are forms that take the femi-   nine form but with an underscore ( “ _ ” ) separating   the feminine suffix “ innen ” ( plural ) or “ in ” ( singu-   lar ) from the stem . This form includes non - binary   people .   Example : Student_innen   Pattern : \w+_innen   Gender colon forms are forms that take the   feminine form but with a colon ( “ : ” ) separating the   feminine suffix “ innen ” ( plural ) or “ in ” ( singular )   from the stem . This form also includes non - binary   people .   Example : Student : innen   Pattern : \w+:innen   Gender star forms are forms that take the femi-   nine form but with an asterisk ( “ * ” ) separating the   feminine suffix “ innen ” ( plural ) or “ in ” ( singular )   from the stem . This form also includes non - binary   people .   Example : Student*innen   Pattern : \w+\*innenAlternatively , and out of the scope of this work ,   gender - fair text can also use present participles   as gender - neutral nouns ( only gender - neutral in   plural forms , e.g. Studierende - “ those who are   studying ” ) , synonymous gender - neutral nouns or it   can completely avoid gendered words and express   content with structures where no gender - fair forms   are needed ( e.g. “ bei den Dorfbewohner*innen ” -   “ among the villagers ” could also be expressed as   “ i m Dorf ” - “ in the village ” ) . We believe that our   proposed approach can be extended to those cases   in the future .   G Gender - Tagged Data With Pair Forms   To make the English - to - German machine transla-   tion model that we use for round - trip translations   gender - aware , we finetune on artificial data with   sentence - level gender labels . Previous work pre-   sented several approaches how parallel data can   be filtered or created to specifically contain mascu-   line or feminine forms ( Costa - jussà and de Jorge ,   2020 ; Saunders and Byrne , 2020 ; Choubey et al . ,   2021 ; Corral and Saralegi , 2022 ) . In our work , we   create such data by making use of pair forms ( see   Appendix F ) in existing parallel data that consist of   the feminine and the masculine form of the same   noun :   Using a simple replace operation , we can use this   data to create two contrasting targets in German and   tag them with a corresponding tag that indicates   whether the translation should contain feminine or   masculine noun forms :   This is possible for plural nouns because all Ger-   man plural nouns share inflexion across genders   which means that no rewriting is necessary for   pronouns , adjectives or determiners that refer to4501them . For singular forms , we can not easily con-   struct contrasting versions because sometimes ad-   ditional modifications to pronouns , adjectives or   determiners are necessary to preserve the gram-   matical agreement . Instead , we create feminine   examples for singular pair forms if the first form   in the pair form is feminine and we create mascu-   line examples if the first form in the pair form is   masculine .   Pair forms are not only specific to German but   are also common in many other languages . For   example , the United Nations advise the use of pair   forms in all their official languages with grammati-   cal gender - Arabic , French , Russian and Spanish -   as well as in English and Chinese for added empha-   sis when gender is relevant for communication . Our   approach to generating finetuning data for gender-   aware machine translation models based on pair   forms is not limited to German but is applicable to   other languages as well .   H Overview of Survey Texts   The six text excerpts used in our survey and the   corresponding three versions can be seen in Table   6 .   P1 is from a website informing about study reg-   ulations for becoming a forester and uses generic   masculine forms and gender slash forms ( that do   not include non - binary people ) in plural and singu-   lar . P2 is from an online privacy policy of an insur-   ance company and uses generic masculine forms   in plural and singular . P3 is from game instruc-   tions and uses generic masculine in singular . P4   is from a news article about Ezra Miller who uses   “ they / them ” pronouns in English and the text uses   generic masculine forms in singular . P5 is from a   company blog and uses generic masculine forms   in plural and singular . P6 is from a package insert   of a birth control pill and uses generic masculine   forms in singular and generic feminine forms in   plural . ( Websites last accessed on 27.12.2022 )   I List of Survey Contacts   Our survey was kindly shared on mailing lists or   other platforms by the networks below .   Austria :   • Venib - Verein Nicht - Binär   Germany:•MinaS - Verein für Menschen i m nichtbinären   und agender Spektrum   • Verein für geschlechtsneutrales Deutsch e. V .   Switzerland :   • Gender Campus   • nonbinary.ch   • Queerstudents Bern   • Queers usem Kaff   • romanescos mailing list   • Transgender Network Switzerland   • Verein Geschlechtergerechter   • Zurich Pride   Each contact was provided with a link to the   survey and the following accompanying text   ( English translation below ):   German original :   English translation :   J Intended Use   •The models presented in this paper are in-   tended to rewrite biased text with possible   gender - fair forms ; they are notintended to   identify a person ’s gender nor to prescribe a   particular gender - fair form.45024503•The models are primarily trained for research   purposes , showing that gender - fair rewrit-   ing models can be trained without language-   specific handwritten rules .   •While our survey with potential beneficiaries   highlights that gender - fair rewriting models –   even though not error - free – may also be ben-   eficial in real - world applications , we caution   that they should be thoroughly tested by po-   tential users before being deployed outside of   research contexts.4504ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 4.4 , Section 5 , Limitations   /squareA2 . Did you discuss any potential risks of your work ?   Section 5 , Limitations , Ethics Section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 , Section 4 , Appendix A , Appendix B , Appendix C , Appendix D , Appendix F   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3.2 , Section 3.3 , Section 4.2 , Section 4.3 , Appendix A , Appendix B , Appendix C , Appendix D ,   Appendix G   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Supplementary materials   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 5 , Appendix I   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The data collected in our survey is completely anonymous ( Section 4.4 ) and we did not ask for   information that would allow identifying individual participants . See the discussion in the Ethics   Section regarding ﬁltering offensive content in the publicly available data that was used to train the   rewriting models .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3.2 , Section 3.3 , Section 4.2 , Section 4.3 , Section 4.4 , Limitations   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.2 , Section 4.2 , Appendix A4505C / squareDid you run computational experiments ?   Section 3 , Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 3.2 , Section 4.2   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.2 , Section 4.2 , we did not run hyperparamter search   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Table 1 , Table 2 , Table 3 , Figure 3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3.2 , Section 3.3 , Section 4.2 , Section 4.3 , Appendix A , Appendix B   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 4.4   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 4.4 , Limitations , Ethics Statement , Appendix G , Appendix H   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Section 4.4 , Limitations , Appendix H   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Section 4.4 , Limitations , Appendix H   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Self - assessment with the University   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Section 4.44506