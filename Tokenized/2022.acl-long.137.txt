  Georgios Katsimpras and Georgios Paliouras   NCSR Demokritos , Athens , Greece   { gkatsibras,paliourg}@iit.demokritos.gr   Abstract   Clinical trials offer a fundamental opportunity   to discover new treatments and advance the   medical knowledge . However , the uncertainty   of the outcome of a trial can lead to unfore-   seen costs and setbacks . In this study , we   propose a new method to predict the effec-   tiveness of an intervention in a clinical trial .   Our method relies on generating an informa-   tive summary from multiple documents avail-   able in the literature about the intervention un-   der study . Speciﬁcally , our method ﬁrst gath-   ers all the abstracts of PubMed articles related   to the intervention . Then , an evidence sen-   tence , which conveys information about the   effectiveness of the intervention , is extracted   automatically from each abstract . Based on   the set of evidence sentences extracted from   the abstracts , a short summary about the inter-   vention is constructed . Finally , the produced   summaries are used to train a BERT - based   classiﬁer , in order to infer the effectiveness   of an intervention . To evaluate our proposed   method , we introduce a new dataset which   is a collection of clinical trials together with   their associated PubMed articles . Our exper-   iments demonstrate the effectiveness of pro-   ducing short informative summaries and using   them to predict the effectiveness of an interven-   tion .   1 Introduction   Clinical Trials ( CT ) present the basic evidence-   based clinical research tool for assessing the ef-   fectiveness of health interventions . Nevertheless ,   only a small number of interventions make it suc-   cessfully through the process of clinical testing .   Approximately , 39%-64 % of interventions actually   advance to the next step of each phase of clinical   trials ( DiMasi et al . , 2010 ) . The uncertainty of a CT   outcome could lead to increased costs , prolonged   drug development and ineffective treatment for the   participants . At the same time , the volume of pub-   lished scientiﬁc literature is rapidly growing andoffers the opportunity to explore a valuable knowl-   edge . Therefore , there is a need to develop new   tools which can i ) integrate such information and   ii ) enhance the process of intervention approval in   CT .   Predicting the approval of an intervention , a task   that describes the ability of a system to predict   whether an intervention will reach the ﬁnal stage   of clinical testing , is a topic that has been studied   before ( Gayvert et al . , 2016 ; Lo et al . , 2018 ) . The   majority of these studies use various traditional   machine learning methods and rely on structured   data from various sources , including biomedical ,   chemical or drug databases ( Munos et al . , 2020 ;   Heinemann et al . , 2016 ) . However , only a few stud-   ies take into account the textual information that   is available online , and mostly in a supplementary   manner ( Follett et al . , 2019 ; Geletta et al . , 2019 ) . In   fact , employing natural language processing ( NLP )   techniques to address the outcome prediction task   has been hardly explored .   Recognising this lack of related studies , the work   presented here addresses the task of predicting in-   tervention approval with the use of NLP . Particu-   larly , we relied on generating concise and infor-   mative summaries from multiple texts that are rel-   evant to the intervention under evaluation . In a   sense , we built an intervention - speciﬁc narrative   which combines key information from multiple   inter - connected documents . The beneﬁt of using   multiple articles to generate summaries is that they   can cover the inherently multi - faceted nature of an   intervention ’s clinical background .   More precisely , given an intervention , our sys-   tem retrieves all PubMed abstracts that are relevant   to the intervention and refer to a clinical study . It   then extracts the evidence sentences from each ab-   stract using a BERT - based evidence sentence clas-   siﬁer , in a similar fashion to ( DeYoung et al . , 2020 ) .   This set of evidence sentences , which captures the   consolidated narrative about the intervention , can1947grow gradually , as new articles become available .   Thus , further analysis is necessary in order to select   the most important information . Using the set of   evidence sentences for each intervention , we gen-   erate short summaries by leveraging the power of   language models ( BERT or BART ) . The resulted   summaries are then fed to a BERT - based binary   sequence classiﬁer which makes a prediction about   the likely approval or not of the intervention .   Overall , the main contributions of the paper are   the following :   •We propose a new approach for predicting the   approval of an intervention which is based on   a three - step NLP pipeline .   •We provide a new dataset for the task of in-   tervention approval prediction that consists of   704 interventions and 15,800 PubMed articles   in total .   •We conﬁrm through experimentation the ef-   fectiveness of the proposed approach .   2 Related Work   Intervention Success Prediction The predic-   tion of intervention approval belongs to a broader   category of medical prediction tasks . Relevant   work includes clinical trial outcome prediction   ( Munos et al . , 2020 ; Tong et al . , 2019 ; Hong et al . ,   2020 ) , drug approval ( Gayvert et al . , 2016 ; Lo   et al . , 2018 ; Siah et al . , 2021 ; Heinemann et al . ,   2016 ) , clinical trial termination ( Follett et al . , 2019 ;   Geletta et al . , 2019 ; Elkin and Zhu , 2021 ) , pre-   dicting phase transition ( Hegge et al . , 2020 ; Qi   and Tang , 2019 ) . All these studies rely either on   speciﬁc types of structured data or on combining   structured data with limited unstructured data .   Differently from this line of work , the authors of   ( Lehman et al . , 2019 ) proposed an approach that   employs NLP to infer the relation between an in-   tervention and the outcome of a speciﬁc clinical   trial . Their method is based on extracting evidence   sentences from unstructured text . An extension   of this work suggests the use of BERT - based lan-   guage models for the same task ( DeYoung et al . ,   2020 ) . Another closely related study ( Jin et al . ,   2020 ) , performs a large - scale pre - training on un-   structured text data to infer the outcome of a clin-   ical trial . Our approach builds upon this related   work , aiming to incorporate information from mul-   tiple articles . This extension is motivated by theassumption that the inter - connected clinical knowl-   edge , coming from multiple sources can provide a   more holistic picture of the intervention , facilitat-   ing more precise analysis and accurate prediction .   Although all these prior efforts tackle , more or   less , the problem of intervention approval , none of   them attempted to predict the effectiveness of an   intervention using summarization methods .   Summarization The goal of summarization is   to produce a concise and informative summary of   a given text . There are two main categories of ap-   proaches : i ) extractive , which tackles summariza-   tion by selecting the most salient sentences from   the text without changing them , and ii ) abstrac-   tive , which attempts to generate out - of - text words   or phrases instead of extracting existing sentences .   Early systems were primarily extractive and relied   on sentence scoring , selection and ranking ( Allah-   yari et al . , 2017 ) . However , both extractive and   abstractive approaches have advanced signiﬁcantly   due to the novel neural network architectures , such   as Transformers ( Vaswani et al . , 2017 ) . The Trans-   formers architecture is utilized by the BERT ( De-   vlin et al . , 2018 ) and BART ( Lewis et al . , 2019 )   language models which are used by the state - of - the   art solutions for multiple NLP tasks , including sum-   marization . Although most of the summarization   literature focuses on single - document approaches ,   there is also a line of work that applies summariza-   tion on a set of documents , i.e. multi - document   summarization ( Ma et al . , 2020 ) . Such approaches   are of particular relevance to our work , as we aim   to summarize a set of sentences about a particular   intervention .   Summarization in the Medical Domain Sum-   marization has been used to address various prob-   lems in the ﬁeld of medicine . These include elec-   tronic health record summarization ( Liang et al . ,   2019 ) , medical report generation ( Zhang et al . ,   2019 ; Liu et al . , 2021 ) , medical facts generation   ( Wallace et al . , 2021 ; Wadden et al . , 2020 ) and med-   ical question answering ( Demner - Fushman and Lin ,   2006 ; Nentidis et al . , 2021 ) .   Our work is inspired by recent work on multi-   document summarization of medical studies ( DeY-   oung et al . , 2021 ) . Apart from introducing a new   summarization dataset of medical articles , that   work also proposed a method to generate abstrac-   tive summaries from multiple documents . Their   model is based on the BART language model , ap-   propriately modiﬁed to handle multiple texts . Our1948model differs in the way it handles the input texts .   Instead of concatenating all texts into a single repre-   sentative document , we order them chronologically   and split them into equal - size chunks . Doing so ,   we expect the clinical studies that were conducted   during a similar time period , to reside in the same   chunk .   3 Task Overview   According to the U.S. Food and Drug Administra-   tion ( FDA ) , a CT addresses one of ﬁve phases of   clinical assessment : Early Phase 1 ( former Phase   0 ) , Phase 1 , Phase 2 , Phase 3 and Phase 4 . Each   phase is deﬁned by the study ’s objective , the inter-   ventions under evaluation , the number of partici-   pants , and other characteristics . Notably , Phase 4   clinical trials take place after FDA has approved a   drug for marketing . Therefore , we can assume that   a CT in Phase 4 assesses effective intervention . On   this basis , our task is to predict whether an inter-   vention will advance to the ﬁnal stage of clinical   testing ( Phase 4 ) , as shown in Figure 1 .   We model the task of predicting the success or   failure of an intervention as a binary classiﬁcation   task . All data relevant to Phase 4 are omitted from   the training stage .   4 Data   In this work , we introduce a new datasetfor   the task of predicting intervention approval . The   dataset is a collection of structured and unstruc-   tured data in English derived from clinicaltrials.gov   and PubMed during May - June 2021 .   As a ﬁrst step in the construction of the dataset ,   we retrieve all available CT studies from clinical-   trials.gov that satisfy some criteria . Then , we as-   sociate each CT with PubMed articles based on   the CT study identiﬁer . Following some cleaning   process ( i.e. deduplication and entity resolution )   we generate the ﬁnal dataset .   Clinical Trials Studies At the time of writing ,   more than 350,000 studies were available onlineatclinicaltrials.gov . We focused on cancer related   clinical testing and we retrieved approximately   85,000 studies related to this topic using a list of   associated keywords .   From this set , we were interested in interven-   tional clinical trials and speciﬁcally in two cate-   gories that indicate the status of the trial : i ) “ Com-   pleted ” , meaning that the trial has ended normally ,   and ii ) “ Terminated ” , meaning that the trial has   stopped early and will not start again . The result-   ing set of studies contains 34,517 completed and   6,872 terminated trials .   Interventions Dataset Using the selected CTs ,   we associated each intervention with its correspond-   ing trials . Therefore , a clinical trial record was   formed for each intervention . Then , we selected   all interventions that are assessed in at least one   Phase 4 CT to form our positive target class ( i.e.   approval ) . Likewise , we built our negative target   class ( i.e. termination ) using interventions that led   to a trial termination . In total , our dataset contains   404 approved and 300 terminated interventions .   For each intervention , we collect all articles from   PubMed that are explicitly related to one of the CTs   of the intervention . To achieve this , we combine   two approaches . First , we search for eligible arti-   cles ( or links to articles ) in the corresponding struc-   tured results of clinicaltrials.gov . Secondly , we   use the CT unique identiﬁers to query the PubMed   database . Then , the selected PubMed articles are   associated with the intervention . This way an in-   tervention is linked with multiple studies that are   inter - connected , and thus an intervention - speciﬁc   narrative is developed . In our dataset , an interven-   tion is associated on average with 22.4 pubmed   articles , though for terminated interventions this   number is just 1.4 . This is because terminated in-   terventions are usually not assessed in many CTs .   Overall , our dataset contains 15,800 pubmed ar-   ticles . The details of the dataset are presented in   Table 1 .   In addition , we attempted to evaluateour ap-   proach on a previously used dataset ( Gayvert et al . ,   2016 ) , which consists of 884 ( 784 approved , 100   terminated ) drugs along with a set of 43 features ,   including molecular properties , target - based prop-   erties and drug - likeness scores.1949   Type |I| |A| avg   Approved 404 15,379 38.1   Terminated 300 421 1.4   Total 704 15,800 22.4   5 Methodology   In Figure 2 , we illustrate the proposed approach ,   which consists of three main steps . Initially , we   use the abstracts of the intervention ’s clinical trial   record to extract evidence sentences . These sen-   tences are then used to generate a short summary   that contains information about the efﬁcacy of the   intervention . The summary is then processed by a   BERT - based sequence classiﬁer to make the ﬁnal   decision about the intervention . Each of the three   steps is detailed in the following subsections .   5.1 Evidence Sentences   Identifying evidence bearing sentences in an arti-   cle for a given intervention is an essential step in   our approach . Differently from other sentences in   an article , evidence sentences contain information   about the effectiveness of the intervention ( Figure   3 ) . Therefore , it is crucial that our model has the   ability to discriminate between evidence and non-   evidence sentences .   First , all abstracts related to the given interven-   tion are broken into sentences . The sentences of   each abstract are then processed one - by - one by a   BERT - based classiﬁer that estimates the probabil-   ity of each sentences containing evidence about theeffectiveness of the intervention . For the classiﬁer ,   we selected a version of the PubMedBERT ( Gu   et al . , 2020 ) model , which is pre - trained only on   abstracts from PubMed . We tested several mod-   els , including BioBERT ( Lee et al . , 2020 ) , clinical-   BERT ( Alsentzer et al . , 2019 ) and RoBERTa ( Liu   et al . , 2019 ) , but PubMedBERT performed the best   in our task . On top of PubMedBERT , we trained   a linear classiﬁcation layer , followed by a Soft-   max , using the dataset from ( DeYoung et al . , 2020 ) .   This dataset is a corpus especially curated for the   task of evidence extraction and consists of more   than 10,000 annotations . The classiﬁer is trained   with annotated evidence sentences ( i.e. positive   samples ) and a random sample of non - evidence   sentences ( i.e. negative samples ) . Regarding the ra-   tio of positive to negative samples , cross - validation   on the training set showed 1:4 to be a reasonable   choice . The evaluation of the different BERT - based   models was done based on the same data splits   ( train , test and validation ) as in ( DeYoung et al . ,   2020 ) .   Once scored by the classiﬁer , the highest scoring1950sentence is selected from each abstract . Therefore ,   for each intervention we extract as many sentences   as the number of abstracts in its clinical record .   5.2 Short Summaries   To generate short and informative summaries we   explore both extractive and abstractive approaches .   Extractive Summaries were based on the ev-   idence sentences extracted in the previous step .   Speciﬁcally , we re - rank them and choose the top k   ( k= 5 ) to compose our ﬁnal summary . The model   we use here is the same BERT - based model as in   Section 5.1 .   Abstractive Considering that an intervention is   linked to multiple abstracts and thus to multiple   evidence sentences , we ﬁrst order all evidence sen-   tences chronologically and combine them into a   single text . Then , we split them to equal chunks   and each chunk then is fed to a BART - based model   to produce the ﬁnal summary .   BART has been shown to lead to state - of-   the - art performance on multiple datasets ( Fabbri   et al . , 2021 ) . Speciﬁcally , we used the pre - trained   distilBART - cnn-12 - 6 model which is trained on   the CNN summarization corpus ( Lins et al . , 2019 ) .   Since abstractive summarization produces out - of-   text phrases , it needs to be ﬁne - tuned with domain   knowledge . In our case , we ﬁne - tuned the BART   model with the MS2 dataset ( DeYoung et al . , 2021 ) ,   which contains more than 470 K articles and 20 K   summaries of medical studies .   We limited the length of the output summary to   140 words . For the extractive setting , in case the   topksentences exceeded this limit , we removed   the extra words . For the abstractive setting we iter-   atively summarized and concatenated the chunks   for each intervention until the expected number of   140 words was accomplished .   5.3 Inferring Efﬁcacy   We model the task of inferring the approval of an   intervention as a binary classiﬁcation task . In our   approach , each intervention is represented by a   short summary . For the classiﬁcation of the sum-   maries , we used again a PubMedBERT model . On   top of it , we trained a linear classiﬁcation layer ,   followed by a sigmoid , using the summaries gen-   erated in the previous step : Our positive training   instances were the summaries of interventions thathave been approved , and correspondingly , the neg-   ative ones were the summaries of interventions that   have been terminated . Hence , the model decides   on the approval of the interventions .   5.4 Technical set - up   All models were pre - trained and ﬁne - tuned for the   corresponding task . The maximum sequence size   was 512 and 1024 for BERT - based and BART-   based models respectively . The Adam optimizer   ( Kingma and Ba , 2015 ) was used to minimize the   cross - entropy losses with learning rate 2e-5 and   epsilon value 1e-8 for all models . We trained all   models for 5 epochs , with batch sizes of 32 , except   the abstractive summarizer for which the batch size   was decreased to 4 due to RAM memory limita-   tions of our system . The implementation was done   using the HuggingFace library ( Wolf et al . , 2020 )   and Pytorch(Paszke et al . , 2019 ) .   6 Results and Analysis   We followed different training approaches for the   different trainable components of our pipeline . For   the evidence sentence selection and the abstractive   summarization models we split the data into devel-   opment and test and then split the development set   further into training ( 90 % ) and validation ( 10 % ) .   We kept the model that performed best on the vali-   dation set and evaluated it on the held - out test set   of each task respectively , averaged over three ran-   dom data splits . Considering the small size of the   interventions dataset , we applied a 10 - fold cross   validation for the ﬁnal classiﬁcation task . For this   task , we report macro averages of the evaluation   metrics over the ten folds .   6.1 Ablation Study   Our experimentation started with a comparison of   different variants and choices that were available   for the various modules of our approach .   Evidence Classiﬁer Coming early in the   pipeline , the performance of the evidence classiﬁer   can play a signiﬁcant role in downstream tasks . The   chosen approach relied on domain - speciﬁc BERT   models . As domain - speciﬁc training that can affect   the performance of BERT - based models , we con-   ducted a comparison between different variants of   BERT . The results in Table 2 demonstrate that the   performance of the models is comparable , with all   models obtaining scores over 90 % in terms of F1   and AUC . PubMedBERT model achieved the best1951scores and was used in the rest of the experiments .   Model P R F1 AUC   BioBERT .928 .938 .933 .957   ClinicalBERT .913 .925 .919 .945   RoBERTa .905 .919 .912 .931   PubMedBERT .931 .956 .943 .969   Summarization Adequacy We assess the per-   formance of the summarization methods on the   MS2 dataset which is a collection of summaries   extracted from medical studies . The task of the   summarizers is to produce texts that approximate   the target summaries . We measure the performance   of the summarization methods using ROUGE and   the results are presented in Table 3 . As expected ,   the abstractive method achieves higher scores , as it   has more ﬂexibility in forming summaries . We   also observed that domain - speciﬁc training im-   proves performance . The abstractivemodel   is a generic BART model without ﬁne - tuning in   the domain . Comparing its performance to the   abstractive model , which was ﬁne - tuned on a   small sample of the MS2 dataset that was excluded   from the evaluation process , we notice a statisti-   cally signiﬁcant improvement .   Model R-1 R-2 R - L   abstractive 24.85 4.34 15.48   abstractive 39.38 11.98 20.13   extractive 19.24 3.22 13.19   Abstractive methods seem to provide better sum-   maries , however , whether these are more useful   than the extractive summaries for our donwstream   task is still to be determined .   6.2 Predicting Intervention Efﬁciency   Having made the choices for the individual mod-   ules , we now turn to the ultimate task , which is the   prediction of the efﬁciency of the intervention . We   evaluate two variations of our proposed method ; i )   with abstractive summarization denoted as PIAS   and ii ) with extractive summarization denoted asPIAS . We compare their performance against   two baselines :   •BS : This is a PubMedBERT model that is   trained with a single evidence sentence per   intervention ( instead of a summary ) . The   sentence is extracted from the most recent   PubMed article relevant to the intervention .   •BN : This is similar to BSbut instead of using   a single sentence for each intervention it is   trained with nevidence sentences extracted   from ndifferent articles ( n= 3 ) . The arti-   cles are selected randomly among the ones   referring to the intervention .   The performance of all models is shown in Table   4 . The proposed method outperforms the baselines   independent of the summarization methods that is   used . Interestingly , even selecting randomly se-   lected evidence sentences seem to help , as BN   achieved a higher performance than BS . Still , the   use of summarization provides a signiﬁcant boost   over both baseline methods , validating the value of   using short summaries to evaluate the efﬁciency of   an intervention . Models that do not take advantage   of the inter - connected documents suffer a signiﬁ-   ca nt drop in performance . Thus , this result justiﬁes   the design of the proposed method .   We can also observe that the best performance   of the proposed method is achieved when using   the extractive summarization method . Extractive   summaries have demonstrated low ROUGE scores   in Section 6.1 . Still , they can properly capture the   properties involved in the data for the classiﬁca-   tion task . On the other hand , although the abstrac-   tive summarizer achieved better ROUGE scores , it   seems that the generated summaries can not discrim-   inate the target classes ( approved or terminated ) as   well as the extractive ones . This indicates that the   quality of the summary , in terms of the ROUGE   score , is not decisive in the classiﬁcation of the   intervention .   Model P R F1   BS .717 .706 .702   BN .732 .731 .731   PIAS .781 .774 .773   PIAS .796 .793 .7921952Analyzing further the performance of our best   model , PIAS , we report macro average scores   for each target class in Table 5 . We notice that the   class P R F1   positive ( approved ) .808 .819 .815   negative ( terminated ) .778 .765 .772   model is slightly better at predicting the approval   of an intervention rather than its termination . This   can be explained by the fact that the approved inter-   ventions are associated with a considerably larger   number of articles than the terminated ones . This   leads to richer summaries for the approved inter-   ventions and thus to a more informed decision .   6.3 Predicting Phase Transition   Early prediction of approval To build our mod-   els , we considered all the available data from Phase   1 , Phase 2 and Phase 3 . However , predicting the   success of an intervention at the earliest phase pos-   sible is compelling . Therefore , we examine the   ability of our model in making early predictions .   More precisely , we evaluate PIASmodel on the   following three transitions : Phase 1 to Approval ,   Phase 2 to Approval and Phase 3 to Approval .   To perform this experiment , we select the inter-   ventions that have CTs in various stages and there   is least one article for each phase . In total , this   subset contains 249 interventions ( 193 approved   and 56 terminated ) . Then , we use 80 % for train-   ing and 20 % for testing . For each transition , we   train our model only with training instances from   the corresponding phase . In Table 6 , we report the   macro average scores over ten random splits of the   data .   transition P R F1   phase1  ! approval .39 .50 .44   phase2  ! approval .78 .70 .72   phase3  ! approval .81 .84 .82   The results indicate that prediction of approval ,   while at Phase 1 is very hard , but the transitionfrom Phase 2 and Phase 3 to approval can be pre-   dicted with considerable success . The large gap in   performance between Phase 1 and Phase 2 , 3 tran-   sitions is explained by the lack of clinical evidence   in early phases .   Phase to Phase Another interesting and chal-   lenging task is to predict the transition of an inter-   vention to the next phase of the clinical trial process .   In this experiment , we want to predict Phase 1 to   Phase 2 and Phase 2 to Phase 3 transitions . For   each transition , we use data only from the former   phase for training ( e.g. for Phase 2 to Phase 3 tran-   sition we use data from Phase 2 ) for both target   classes . Again , we use 80 % for training and 20 %   for testing and present the average scores over ten   random splits .   transition P R F1   phase2  ! phase3 .84 .82 .83   phase1  ! phase2 .77 .76 .77   Table 7 shows the results for the two transitions ,   which are comparable to the overall predictive per-   formance of the model . Considering the small size   of the datasets used in both phase transition tasks ,   these results can serve only as an indication of how   our model behaves . Further analysis and experi-   ments should be conducted for a more thorough   evaluation .   6.4 Explainability of Predictions   It is clinically very valuable to identify the factors   that contribute most to a particular decision of the   classiﬁer . Interestingly , the summaries generated   from our models can also serve that purpose very   well .   Table 8 illustrates some examples of interven-   tions along with their abstractive and extractive   summaries as produced by our pipeline . For   the ﬁrst intervention , pertuzumab , it is notable   that both summaries report a improved median   progression - free survival which somewhat explains   the prediction . For the second intervention , tax-   ane , the summaries mention the greater incidence   of serious adverse events and lower median over-   all survival , which counts against the approval of   the intervention . We also notice that many numer-   ical entities are randomly placed or changed in1953Intervention PIAS PIAS   pertuzumab 3   taxane 7   the abstractive summary . This contributes to the   tendency of the abstractive methods to generate   " hallucinated " evidence , as observed in the litera-   ture ( Cao et al . , 2018 ) . However , the abstractive   summaries look more readable . A more exhaus-   tive analysis , including also a human evaluation ,   is needed to assess the ultimate explainability of   these summaries .   7 Conclusion   Predicting intervention approval in clinical trials   is a major challenge with signiﬁcant impact in   healthcare . In this paper , we have proposed a new   pipeline to address this problem , based on state-   of - the - art NLP techniques . The proposed method   consists of three steps . First , it identiﬁes evidence   sentences from multiple abstracts related to an inter-   vention . Then , these sentences are used to produce   short summaries . Finally , a classiﬁer is trained on   the generated summaries in order to predict the   approval or not of an intervention .   Moreover , we introduced a new dataset for this   task which contains 704 interventions associatedwith 15,800 abstracts . This data was used to evalu-   ate our pipeline against other baseline models . The   experimental results veriﬁed the effectiveness of   our approach in predicting the approval of an in-   tervention and the contribution of each step of the   proposed pipeline to the ﬁnal result . Further evalu-   ation on predicting phase transitions , showed that   our model can assist in all stages of a clinical trial .   Besides , the generated multi - document summaries   can be naturally used to explain the predictions of   the model .   There are multiple ways to extend this work .   In terms of multi - document summarization , there   is room to explore more advanced summarization   models , quality and performance metrics as well as   better explainability assessment . In the bigger pic-   ture , we shall also consider to expand the dataset by   extending its size and incorporating different types   of resources ( e.g. drug interaction networks ) . Fi-   nally , we are interested in enhancing the proposed   method to incorporate temporal information associ-   ated with the CTs to maintain the history of clinical   changes.1954Acknowledgements   We would like to thank the anonymous reviewers   for their valuable and constructive comments on   this research . This works was partially supported   by the ERA PerMed project P4 - LUCAT ( Personal-   ized Medicine for Lung Cancer Treatment : Using   Big Data - Driven Approaches For Decision Sup-   port ) ERAPERMED2019 - 163 .   References19551956A Results on Proctor Dataset   To further evaluate our method , we attempted a   comparison with the method presented in ( Gayvert   et al . , 2016 ) using their data . The data contains   a list of approved and terminated drugs together   with various features . Using this dataset , we ex-   perienced two issues that made the comparison   incomparable : i ) For many drugs we could not ﬁnd   relevant articles in PubMed . The original dataset   contains 828 drugs whereas we managed to collect   information only for 537 . Thus , the scores of our   method are not directly comparable to the ones re-   ported in ( Gayvert et al . , 2016 ) ii ) Four important   features that were used in ( Gayvert et al . , 2016 ) are   missing in the dataset . Therefore , the reproduction   of the exact model is not possible .   Despite these facts , we performed a comparison   of the methods for the subset that we collected :   •RF : This model reports the scores from   ( Gayvert et al . , 2016 ) .   •RF : This is a Random Forest model similar   to the original one , but it is trained only with   the available features .   The overall performances of all models are de-   picted in Table 9 .   Model AUC   RF .826   RF .484   PIAS .5861957