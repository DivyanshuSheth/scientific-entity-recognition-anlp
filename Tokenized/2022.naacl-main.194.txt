2   Michael Glass , Gaetano Rossiello , Md Faisal Mahbub Chowdhury ,   Ankita Rajaram Naik , Pengshan Cai , Alfio GliozzoIBM Research AI , Yorktown Heights , NY , USAUniversity of Massachusetts Amherst , MA , USA   Abstract   As demonstrated by GPT-3 and T5 , transform-   ers grow in capability as parameter spaces be-   come larger and larger . However , for tasks   that require a large amount of knowledge , non-   parametric memory allows models to grow dra-   matically with a sub - linear increase in compu-   tational cost and GPU memory requirements .   Recent models such as RAG and REALM   have introduced retrieval into conditional gen-   eration . These models incorporate neural ini-   tial retrieval from a corpus of passages . We   build on this line of research , proposing ReG ,   which combines both neural initial retrieval   and reranking into a BART - based sequence-   to - sequence generation . Our reranking ap-   proach also permits merging retrieval results   from sources with incomparable scores , en-   abling an ensemble of BM25 and neural initial   retrieval . To train our system end - to - end , we   introduce a novel variation of knowledge dis-   tillation to train the initial retrieval , reranker   and generation using only ground truth on the   target sequence output . We find large gains in   four diverse tasks : zero - shot slot filling , ques-   tion answering , fact checking and dialog , with   relative gains of 9 % to 34 % over the previous   state - of - the - art on the KILT leaderboard . We   make our code available as open source .   1 Introduction   GPT-3 [ Brown et al . , 2020 ] and T5 [ Raffel et al . ,   2020 ] are arguably the most powerful members   in a family of deep learning NLP models called   transformers . Such models store surprising amount   of world knowledge . They have been shown to   produce good performance on a range of demand-   ing tasks , especially in generating human like texts .   However , such large transformers ’ capability is   tied to the increasingly larger parameter spaces on   which they are trained . Recently , there has been work towards trans-   formers that make use of non - parametric knowl-   edge . REALM ( Retrieval Augmented Language   Model ) [ Guu et al . , 2020 ] and RAG ( Retrieval Aug-   mented Generation ) [ Lewis et al . , 2020b ] both use   an indexed corpus of passages to support condi-   tional generation . By using the corpus as a source   of knowledge these models can extend the informa-   tion available to the model by tens or even hundreds   of gigabytes with a sub - linear scaling in computa-   tion cost .   These recent advancements , in turn , have   been inspired by BART ( Bidirectional and Auto-   Regressive Transformer ) [ Lewis et al . , 2020a ] that   combines a Bidirectional Encoder ( e.g. BERT [ De-   vlin et al . , 2019 ] ) with an Autoregressive decoder   ( e.g. GPT [ Brown et al . , 2020 ] ) into one sequence-   to - sequence model .   We build on this line of research , pioneered   by REALM and RAG , and propose a new ap-   proach that we call ReG(Retrieve , Rerank ,   Generate ) , which combines both neural initial re-   trieval and reranking into a BART - based sequence-   to - sequence generation .   There are two particular aspects on which our ap-   proach is different from the previous works . Firstly ,   our reranking approach permits merging retrieval   results from sources with incomparable scores , e.g.   enabling an ensemble of BM25 and neural initial   retrieval . Secondly , to train our system end - to - end ,   we introduce a novel variation of knowledge dis-   tillation to train the initial retrieval , reranker and   generation using only ground truth on the target   sequence output .   The KILT benchmark [ Petroni et al . , 2021 ] has   been recently introduced to evaluate the capabili-   ties of pre - trained language models to address NLP   tasks that require access to external knowledge . We   evaluate on four diverse tasks from KILT : slot fill-   ing , question answering , fact checking and dialog .   Figure 1 shows examples of these tasks . ReG2701makes significant gains on all four tasks , reaching   the top of the KILT leaderboards and establishing   a new state - of - the - art .   The contributions of this work are as follows :   •We introduce ReG , demonstrating the effec-   tiveness of reranking for generative language   models that incorporate retrieval .   •We further extend ReGby ensembling ini-   tial retrieval methods , combining neural and   traditional keyword - based approaches .   • ReGimproves the current state - of - the - art of   9 % , 31 % , 34 % , 22 % and 10 % relative gains   on the headline KILT metrics for T - REx ( slot   filling ) , Natural Questions ( question answer-   ing ) , TriviaQA ( question answering ) , FEVER   ( fact checking ) , and Wizard of Wikipedia ( di-   alog ) , respectively .   •We publicly release our code as open source   to support continued development .   2 Related Work   The KILT benchmark and public leaderboardcom-   bines eleven datasets across five tasks . The main ad-   vantage of the KILT distribution of these datasets is   that the provenance information from each dataset   is realigned to reference the same snapshot of   Wikipedia . A unified evaluation script and set   of metrics is also provided . In this work , we   focus on four tasks , such as Slot Filling [ Levy   et al . , 2017 , Elsahar et al . , 2018 ] , Question Answer-   ing [ Kwiatkowski et al . , 2019 , Joshi et al . , 2017 ] ,   Fact Checking [ Thorne et al . , 2018a , c ] , and Dia-   log [ Dinan et al . , 2019 ] ( see Figure 1 ) .   A set of baseline methods have been proposed   for KILT . GENRE [ Cao et al . , 2021 ] is trained   on BLINK [ Wu et al . , 2020 ] and all KILT tasks   jointly using a sequence - to - sequence language   model to generate the title of the Wikipedia page   where the answer can be found . This method is   a strong baseline to evaluate the retrieval perfor-   mance , but it does not address the downstream   tasks . On the other hand , generative models , such   as BART [ Lewis et al . , 2020a ] and T5 [ Raffel et al . ,   2020 ] , show interesting performance when fine-   tuned on the downstream tasks relying only on the   implicit knowledge stored in the weights of theneural networks , without the use of any explicit   retrieval component .   RAG [ Lewis et al . , 2020b ] , an end - to - end   retrieval - based generative model , is the best per-   forming baseline in KILT and it incorporates   DPR [ Karpukhin et al . , 2020 ] to first retrieve rel-   evant passages for the query , then it uses a model   initialized from BART [ Lewis et al . , 2020a ] to per-   form a sequence - to - sequence generation from each   evidence passage concatenated with the query in   order to generate the answer . Figure 2 shows the   architecture of RAG .   Multi - task DPR [ Maillard et al . , 2021 ] ex-   ploits multi - task learning by training both DPR   passage and query encoder on all KILT tasks .   DensePhrases [ Lee et al . , 2021 ] addresses the   knowledge intensive tasks with a short answer , such   as slot filling . It indexes the phrases in the cor-   pus that can be potential answers . The extracted   phrases are represented by their start and end to-   ken vectors from the final layer of a transformer   initialized from SpanBERT [ Joshi et al . , 2020 ] .   Knowledge Graph Induction ( KGI ) [ Glass et al . ,   2021 ] combines DPR and RAG models , both   trained with task and dataset specific training . KGI   employs a two phase training procedure : first train-   ing the DPR model , i.e. both the query and context   encoder , using the KILT provenance ground truth .   Then , KGI trains the sequence - to - sequence genera-   tion and further trains the query encoder using only   the target output as the objective . This results in   large improvements in retrieval performance and ,   as a consequence , in the downstream tasks .   KILT - WEB 2 [ Piktus et al . , 2021 ] addresses the   KILT tasks by broadening the knowledge source   used . Rather than rely only on KILT ’s Wikipedia   snapshot , KILT - WEB 2 creates S as a knowl-   edge source . S is built from CCNet [ Wenzek   et al . , 2020 ] and over twenty times the size of the   Wikipedia corpus . It can use either BM25 or DPR   retrieval ( though not both combined ) followed by   a ‘ reader ’ component , but not trained end - to - end .   The reader component is the Fusion - in - Decoder   [ Izacard and Grave , 2021 ] model , where retrieved   documents are encoded independently , then their   encoded representations are concatenated for the   decoder .   SEAL [ Bevilacqua et al . , 2022 ] introduces a   novel generative approach to retrieval . Rather   than generating the unique document identifier like   GENRE , SEAL can generate any ngrams present2702   in the corpus , which are then mapped to passages .   The neural retrieval generator is based on BART   and constrained to generate ngrams that appear   in the corpus with an FM - Index [ Ferragina and   Manzini , 2000 ] . Like KILT - WEB 2 , SEAL uses   Fusion - in - Decoder as the component responsible   for generating the output conditioned on the re-   trieved passages .   Multi - stage or cascade approaches to retrieval   have received ample attention in Information Re-   trieval ( IR ) research . The multi - stage approach   begins with the initial retrieval phase , where an ini-   tial set of documents or passages form the pool of   candidates to be considered for ranking . Then one   or more phases of increasingly computationally de-   manding rerankers are applied . Early approaches in   learning to rank [ Liu , 2009 ] used features and linear   classifiers . Pre - trained language models , especially   BERT [ Devlin et al . , 2019 ] , have shown state - of - the - art performance when applied to the task of   relevance ranking . Transformers may be applied as   classifiers to each query and passage pair indepen-   dently [ Nogueira and Cho , 2019 ] or as generators   to produce labels for passages in a sequence - to-   sequence model [ Nogueira et al . , 2020 ] .   3 Methodology   The approach of RAG , Multi - DPR , and KGI is   to train a neural IR ( Information Retrieval ) com-   ponent and further train it end - to - end through its   impact in generating the correct output . Figure 2   illustrates the end - to - end RAG system .   It has been previously established that results   from initial retrieval can be greatly improved   through the use of a reranker [ Liu , 2009 , Wang   et al . , 2011 ] . Therefore we hypothesized that natu-   ral language generation systems incorporating re-   trieval can benefit from reranking.2703   In addition to improving the ranking of passages   returned from DPR , a reranker can be used after   merging the results of multiple retrieval methods   with incomparable scores . For example , the scores   returned by BM25 [ Robertson and Zaragoza , 2009 ]   are not comparable to the inner products from DPR .   Using the scores from a reranker , we can find the   top - k documents from the union of DPR and BM25   results . Figure 3 illustrates our extension of RAG   with a reranker . We call our system ReG(Retrieve ,   Rerank , Generate ) .   3.1 Reranker   The reranker we use is based on the sequence - pair   classification of Nogueira and Cho [ 2019 ] . This   model is shown in Figure 4 . The query and passage   are input together to a BERT [ Devlin et al . , 2019 ]   transformer . Cross attention is applied over the   tokens of both sequences jointly . This is called an   interaction model .   This model contrasts with the representation   model used for initial retrieval . Figure 5 shows   the bi - encoder representation model for DPR . The   representation vectors for the query and passage   are produced independently . This allows for ef-   ficient retrieval by pre - computing vectors for all   passages in the corpus and indexing them with an   ANN ( Approximate Nearest Neighbors ) index . By   using an interaction model to rerank the top - N pas-   sages from the representation model , we can get   the advantages of both model types : accuracy and   scalability .   We initialize the reranker from the BERT model   trained on MS MARCO [ Nguyen et al . , 2016 ] by   NBoost [ Thienes and Pertschuk , 2019 ] and avail - able through Hugging Face .   3.2 Training   As Figure 1 illustrates , KILT tasks are provided   with two types of ground truth : the target output se-   quence and the provenance information indicating   the passage or passages in the corpus that support   the output .   Our training is carried out in four phases : DPR   training , generation training , reranking training ,   and full end - to - end training . The initial DPR   and reranking phases make use of the provenance   ground truth . The generation and full end - to - end   training make use of only the target output .   Formally :   •The original KILT instances are a tuple :   ⟨q , t , Prov⟩where qis the input or prompt ,   tis the target output , and Prov is the set of   provenance passages that support the target   output .   •DPR training is a tuple : ⟨q , p , p⟩where   p∈Prov andpwhere p∈BM25 ( q)∧   p/∈Prov   •Reranking training begins with the applica-   tion of DPR and BM25 , producing tuples :   ⟨q , P , Prov⟩where P = BM25 ( q)∪DPR ( q )   •Generation and end - to - end training instances   are pairs of query and target : ⟨q , t⟩   The first two phases , DPR and generation , are   identical to KGI , specifically KGI . We use the   codes from Glass et al . [ 2021 ] .   DPR Stage 1 training is the same training used   by Karpukhin et al . [ 2020 ] . The triplets of query ,   positive passage and “ hard negative ” passages from   BM25 are put into batches of 128 instances . The   positives and hard negatives from other instances   form the “ batch negatives ” for each instance . The   DPR bi - encoder model gives each query a proba-   bility distribution over the positive , hard negative ,   and batch negatives . The loss is the negative log-   likelihood for the positive . After DPR Stage 1 train-   ing the passages from the corpus are indexed with   a Hierarchical Navigable Small World ( HNSW )   [ Malkov and Yashunin , 2018 ] using FAISS [ John-   son et al . , 2017].2704Generation training extends the training of   the query encoder and trains the BART   sequence - to - sequence model on the target sequence   output . This training is the same as that described   by Lewis et al . [ 2020b ] .   3.3 Reranking Training   The next phase , training the reranking in isolation ,   begins with gathering the initial retrieval results   from DPR and BM25 on the training set . These   results are merged and used as training data for the   reranker .   In some datasets there are multiple positive   passages . Therefore , we use the negative of the   summed log - likelihood for the positive passages as   the loss function . The logits given by the reranker   arezand the indices for the correct passages ( from   the ground truth provenance ) are Prov .   loss = −/summationdisplaylog(softmax ( z ) )   3.4 End - to - End Training   Training end - to - end poses a special challenge . In   RAG , the gradient propagates to the query encoder   because the inner product between the query vec-   tor and the passage vector is used to weight the   influence of each sequence , a process RAG calls   marginalization . The inputs to the BART model   are sequences ( s = p q ) that comprise a   query qplus retrieved passage p. The probability   for each sequence is determined from the softmax   over the retrieval ( or reranker ) scores for the pas-   sage . The probability for each target token tgiven   the sequence sis a softmax over BART ’s token   prediction logits . The loss therefore is a negative   log - likelihood summed over all target tokens and   sequences , weighted by each sequence ’s probabil-   ity .   Consider that in ReGthe score from the   reranker , not the initial retrieval , is used to weight   the impact of each sequence in generation . This al-   lows the reranker to be trained through the ground   truth on target output , but it means the gradient for   the query encoder will be zero since the marginal-   ization no longer depends on the inner product from   the query and passage representation vectors . P(s ) = softmax ( z )   P(t|s ) = softmax ( BART ( s ) )   loss = −/summationdisplaylog(P(t|s)·P(s ) )   We consider three possible resolutions to this   issue .   • Combine the DPR and reranker scores   • Freeze the query encoder   • Online Knowledge Distillation   The first candidate solution is tempting but fa-   tally flawed . By adding the log softmax from DPR   and the reranker we can ensure that both systems   are trained through impact in generation . However ,   if the DPR score is added to the reranker score , then   the DPR score is being trained to provide a com-   plementary signal to the reranker . Therefore , when   DPR is used to gather the candidate passages , it   does not give the highest scores to the passages that   are most likely to be relevant , but instead gives the   highest scores to the passages the reranker is most   likely to underrate . We find that this theoretical   concern is also a practical concern , as DPR perfor-   mance ( and overall system performance ) declines   greatly when trained in this way .   The simplest solution is to freeze the parameters   of the query encoder , training only the reranker   and generation components . We find this is indeed   the best solution for one of our datasets , Wizard of   Wikipedia . Note that DPR has already been trained   in two phases , first from the provenance ground   truth and then again in generation training in the   RAG model .   The third solution is our novel application of   knowledge distillation [ Hinton et al . , 2015 ] . We use   the reranker as a teacher model to provide labels to   the DPR student model . We distill the knowledge   across architectures : from an interaction model   to a representation model . Further , this knowl-   edge distillation occurs online , while the reranker   is being trained . The loss for the initial retrieval is   therefore the KL - divergence between the probabil-   ity distribution it gives over the retrieved passages   and the reranker ’s probability distribution over the   same passages . A temperature hyperparameter T   smooths these distributions to prevent excessive   loss and stabilize training.2705   loss =D / parenleftig / parenleftigz   T / parenrightig / vextenddouble / vextenddouble / vextenddouble / parenleftigz   T / parenrightig / parenrightig   · T   The knowledge distillation has the usual advan-   tage of providing signal not only of positive and   negative instances , but degrees of negativeness . In   addition , since we retrieve n= 12 passages from   DPR but only use the top- k(k= 5 ) for generation ,   the knowledge distillation loss is providing a ( soft )   label for more passages.3.5 Inference   At inference time the query is encoded using the   DPR query encoder and the top-12 passages from   the HNSW index are returned . The query is also   passed to BM25 search , specifically Anserini ,   gathering the top-12 BM25 results . Both sets of   passages are passed to the reranker and scored . The   top-5 passages are then joined with the query and   passed to BART to generate the output . The   five output sequences are weighted according to   the softmax over the reranker scores to produce the2706final output .   4 Experiments   We test our model on five datasets , over four dis-   tinct tasks in the KILT benchmark : slot filling ,   question answering , fact checking and dialog . Fig-   ure 1 shows an example of these four tasks .   The slot filling dataset , T - REx [ Elsahar et al . ,   2018 ] , provides as input a head entity and relation ,   and expects as output the entity or term that fills the   slot , also called the tail entity . The T - REx dataset   contains 2.3 M instances . We use only 370k training   instances by downsampling the relations that occur   more than 5000 times . This reduces the training   time required while keeping state - of - the - art perfor-   mance . The development and test sets each have   5k instances .   The question answering datasets are “ open ” ver-   sions of Natural Questions [ Kwiatkowski et al . ,   2019 ] and TriviaQA [ Joshi et al . , 2017 ] . Unlike   the original versions , the relevant Wikipedia page   must be found by a retrieval step . The training sets   for Natural Questions and TriviaQA contain 87k   and 62k questions , with another 3k and 5k for the   development and 1.4k and 6.5k for test .   The fact checking dataset in KILT is FEVER   ( Fact Extraction and VERification ) . It is a com-   bination of the two FEVER versions [ Thorne   et al . , 2018b , 2019 ] omitting the NE -   I class . There are approximately 10k instances   in the development and test sets , and 100k for train-   ing . FEVER is a classification task , but we cast it as   a generation task by training the model to generate   either the token “ SUPPORTS ” or “ REFUTES ” .   Wizard of Wikipedia [ Dinan et al . , 2018 ] is the   dialog dataset . The input is a short dialog history   ending with the information seeker ’s turn . The ex-   pected output is a fact presented conversationally   or just an utterance or question mentioning content   from a relevant Wikipedia page . It is the smallest   dataset with approximately 3k instances in devel-   opment and test and 64k in train .   For all tasks , systems are expected to produce the   target output as well as justify it with provenance   information from the KILT knowledge source . The   metrics of R - Precision and Recall@5 measure the   correctness of the provenance . R - Precision mea-   sures what fraction of the Rdocuments in the   ground truth provenance ( |Prov|=R ) are present   in the top- Rdocuments returned by the system .   Accuracy and ( token - level ) F1 measure the cor - rectness of the generated output . For Wizard of   Wikipedia , Rouge - L [ Lin , 2004 ] is used instead of   accuracy , since systems are very unlikely to gen-   erate the exact target output . The metrics of KILT-   Accuracy , KILT - F1 and , for Wizard of Wikipedia ,   KILT - Rouge - L are the underlying metric ( e.g. Ac-   curacy ) for instances where R - Precision is one , oth-   erwise zero . These metrics indicate output correct-   ness when provenance is also correctly supplied .   Table 1 shows the performance of ReGon the   KILT leaderboard . We achieved 9 % , 31 % , 34 % ,   22 % and 10 % relative gains over the previous state-   of - the - art on the headline KILT metrics for T - REx ,   Natural Questions , TriviaQA , FEVER , and Wizard   of Wikipedia , respectively . Furthermore , ReGhas   held the lead in the headline KILT metrics in all   datasets except for Wizard of Wikipedia where it is   now second best .   Since our submission to the KILT leaderboard   for the Wizard of Wikipedia , a new system called   Hindsight [ Paranjape et al . , 2021 ] achieved even   better results on the generation metrics on that par-   ticular task . The new system of SEAL has also   achieved top results for some metrics on the Natu-   ral Questions and TriviaQA benchmarks .   4.1 Retrieval   Table 2 examines how the retrieval improves   through each step of training . In the first half of the   table we consider the initial retrieval alone . DPR   Stage 1 is the DPR training described earlier - train-   ing only from the provenance ground truth with   batch negatives and hard negatives from BM25 .   KGIfurther trains the query encoder of DPR Stage   1 through its impact in generating the target output .   Finally ReGextends the training of DPR with on-   line knowledge distillation from the reranker . This   step is beneficial in two of the three datasets , while   the previous steps improve performance across all   datasets .   In the second half of the table we examine the   improvement in reranking . The baseline of KGI   DPR+BM25 merges the results of KGI ’s DPR and   BM25 by scoring each passage by the sum of the in-   verse rank from each method . For both T - REx and   FEVER , even this simple approach to ensembling   DPR and BM25 improves Recall@5 , although not   R - Precision . Following reranker training using the   provenance ground truth ( Reranker Stage 1 ) , we   find improvement over DPR across all five datasets   on both retrieval metrics . The reranker ’s improve-2707   ment following end - to - end training is mixed . In   FEVER and Wizard of Wikipedia there is substan-   tial gain in R - Precision , approximately 2 % . T - REx   and Natural Questions are flat . However , there is a   sharp decline in the performance of TriviaQA , in   retrieval metrics . This is true despite the fact that   retrieving these passages greatly improves answer   accuracy and F1 . This suggests some incomplete-   ness in the provenance ground truth for TriviaQA .   4.2 Ablations   Table 3 explores ablations of the ReGsystem . The   point estimates and 95 % confidence intervals are   reported . ReG - KD excludes the online knowl-   edge distillation , instead freezing the query encoder   when training the reranker and generator during   end - to - end training . ReG - BM25 excludes BM25   results , fetching 24 passages from DPR rather than   12 from DPR and 12 from BM25 . The passages are   still reranked . KGIis the baseline system , without   a reranker and therefore also without BM25 results   or online knowledge distillation during training .   Both online knowledge distillation and ensem-   bling with BM25 improve performance in four   out of five datasets . Online knowledge distillation   failed to improve for Wizard of Wikipedia and en-   sembling with BM25 failed to improve for Natural   Questions .   5 Analysis   Since the ReGmodel differs from the KGI model   only in the retrieval phase , we hypothesized that   its gains in output quality are driven by its better   retrieval quality . To test this hypothesis we con-   sidered all cases where the ReGmodel produces   better output than the KGImodel and calculated   the fraction of such cases where ReG ’s rank for   the first correct passage is lower than KGI ’s .   We find that for T - REx , NQ , and FEVER the   fractions of output gains that could be attributed toimproved retrieval and ranking are 67.73 % , 61.08 %   and 66.86 % respectively . While for TriviaQA and   Wizard of Wikipedia only 36.86 % and 27.74 % of   output improvements were accompanied by im-   proved ranking for the correct passage . It is impor-   tant to note that in Wizard of Wikipedia , many of   these improved outputs have only a small gain in   token - level F1 .   While much of the gain in output quality is at-   tributable to improved recall , at least a third is   not . This reinforces an observation of Glass et al .   [ 2021 ] , that models trained with better retrieval   can produce better output even when the retrieved   passages are equivalent at test time .   5.1 Slot filling error analysis   To understand the types of errors ReGmakes we   sampled 50 instances of the development set of the   T - REx dataset where the Accuracy and token - level   F1 score was zero .   Interestingly , the most common class of er-   ror ( 33/50 ) was due to the incompleteness of the   ground truth . Often the head entity is ambiguous   ( 19/50 ) , or the relation has multiple fillers ( 16/50 ) .   As an example , consider the following where there   are two Joe O’Donnell notable for sports in the   passages retrieved , and each played for at least two   different teams .   When ReGproduces genuine errors it is usually2708   because it has selected some entity as a filler related   in a different way ( 6/17 ) or it has failed to retrieve   the necessary passage ( 9/17 ) .   6 Conclusions   ReGconsiderably advanced the state - of - the - art   across five KILT datasets , and still holds the top po-   sition in four of the five . Relative to previous work ,   such as RAG or KGI , ReGsubstantially improves   both in retrieval and end - to - end performance on   slot filling , question answering , fact checking , and   dialog . The reranker alone improves performance   and enables the inclusion of multiple sources of   initial retrieval . This architecture permits us to   integrate results from BM25 , further improving ac-   curacy . Our online knowledge distillation is able   to improve the performance of DPR in four of thefive datasets , despite the loss in end - to - end training   not depending on the DPR scores . Similarly , the   ensembling of DPR and BM25 , which is enabled   by our incorporation of a reranker , benefits four   of the five datasets tested . We have directed our   efforts towards improving the retrieval of relevant   knowledge . This also enables improvement in end-   to - end performance by supplying better passages to   the generation component . Further experiments on   domain adaptation of ReGon tasks like question   answering or dialog might provide useful insight   on the application of this technology to real world   use cases . We are releasing our source code as   open source ( Apache 2.0 license ) to enable further   research.2709References27102711   Appendix   A Hyperparameters   We have not done hyperparameter tuning for DPR   Stage 1 , Generation , or Reranking training . Instead   we used hyperparameters similar to the originalworks on training DPR , BERT reranking and RAG .   Table 4 shows the hyperparameters used in our   experiments .   For knowledge distillation we used the same   hyperparameter settings as Generation . For the   additional hyperparameters in online knowledge   distillation : temperature and KD learn rate scaling ,   we experimented with temperatures of 10 and 40   and KD learn rate scaling of 1.0 and 0.1 . For our   reported results we used a temperature of 10.0and   a learn rate scaling of 1.0 .   When training using online knowledge distilla-   tion , there is a separate optimizer for the query   encoder while training generation . This optimizer   uses the same hyperparameter settings .   Table 6 shows the settings for retrieval and gen-   eration used for all datasets .   All results are from a single run . The random   seed for python , numpy and pytorch was 42 .   B Software Details   We used the following software versions :   • Ubuntu 18   • Pytorch 1.7   • Transformers 4.3.2   • Anserini 0.4.1   C Model Details   Number of parameters ReGuses three   BERT transformers : query encoder , passage   encoder and reranker . Each has 110Mparame-   ters . The generation component is a BART   model with 400Mparameters . There are 730 M   parameters in total .   Computing infrastructure Using a single   NVIDIA V100 GPU DPR training of two epochs   takes approximately 24 hours for T - REx and less   than 12 hours for FEVER and WoW.   Using a two NVIDIA P100 GPUs generation   training for 370k T - REx instances takes two days ,   while FEVER and WoW training completes in half   a day .   The FAISS index on the KILT knowledge source   requires a machine with large memory , we use ma-   chines with 128 GB of memory.2712Hyperparameter DPR Reranker Generation   learn rate 5e-5 3e-5 3e-5   batch size 128 32 128   epochs 2 1 1 *   warmup instances 0 10 % 10 %   learning schedule linear triangular triangular   max grad norm 1 1 1   weight decay 0 0 0   Adam epsilon 1e-8 1e-8 1e-8   D Generation Analysis   We examined 20 instances coupled with 3 output   texts : the baseline KGI , ReG , and the target   text in the ground - truth . The three output texts   were presented unlabeled and in random order to   avoid bias . For each instance , we read the conver-   sation history and then mark each text either , or generation . To our surprise ,   5/20 ground - truth target texts are   which indicates the WoW benchmark might have   limitations in annotation quality . Both the sys-   tems have similar results .   Second , we checked a set of 20 WoW instances   where ReG ’s F1 score was in the bottom quin-   tile . The conversation history was presented along   with ReGgenerated text and the passages re-   Hyperparameter Value   type IndexHNSWSQ   m 128   ef search 128   ef construction 200   index batch size 100000   scalar quantizer 8   Hyperparameter Value   DPR passages 12   BM25 passages 12   BART sequences 5   BART beam size 6   BART length penalty 1.0   BART minimum length 2   BART maximum length 64trieved . Manual examination showed 8/20 as and in 4/8 cases supporting ground-   truth passages were not retrieved . Below is one   of the 12/20 cases where ReGgenerated text was   found with respect to the conversation   history , although it has low F1 and Rouge - L scores .   D.1 Generation Quality   Table 7 shows couple of examples that were part of   the set of randomly selected instances from WoW   dataset and used for manual inspection . We choose   these two particular instances to show when we   thought the ground truth ( i.e. target ) is not coher-   ent with respect to the corresponding conversation   history .   In the first example , the system generated out-   puts were judged as coherent . We found that both   ReGandKGIretrieved the following passage   which might have helped generation of the above   output -   Horseshoe Falls / Horseshoe Falls   Horseshoe Falls , also known as Cana-   dian Falls , is the largest of the three wa-   terfalls that collectively form Niagara   Falls on the Niagara River along the   Canada – United States border . Approx-   imately 90 % of the Niagara River , af-   ter diversions for hydropower generation,2713flows over Horseshoe Falls . The remain-   ing 10 % flows over American Falls and   Bridal Veil Falls . It is located between   Terrapin Point on Goat Island in the US   state of New York , and Table Rock in the   Canadian province of Ontario . Section :   International border .   As for the ground truth , we marked it ( factu-   ally ) inconsistent based on the following retrieved   passage -   Niagara Falls / Located on the Niagara   River , which drains Lake Erie into Lake   Ontario , the combined falls have the   highest flow rate of any waterfall in   North America that has a vertical drop of   more than . During peak daytime tourist   hours , more than 168,000 m ( six million   cubic feet ) of water goes over the crest of   the falls every minute . Horseshoe Falls   is the most powerful waterfall in North   America , as measured by flow rate .   In the second example , all three texts were   marked inconsistent . Interestingly , all the items in   the conversation contains subjective opinion . Con-   sequently , all the three candidate texts also contains   subjective opinion . The problem is both the sys-   tems generated texts that are almost repetition of   earlier conversation . In case of the ground truth ,   we find that the text is semantically incoherent .   We have also submitted files that contain all in-   stances that were used to generate the different   analysis reported in Section 4.2 of the paper . These   files also contains our annotations / remarks where   applicable.2714Conversation history :   •Have you ever been to Niagra Falls ? I want to go there , but I know it ’s very far . Those beautiful   3 waterfalls are right between New York and Ontario , thousands of miles from me !   • I never have but I would love to . It looks beautiful .   •Yes , they do ! They are different sized waterfalls , named Horseshoe , American and Bridal Veil   Falls .   • I had no idea . I ’ve never heard that before . Very cool .   • Yes , and Horseshoe Falls in actually the most powerful waterfall in all of the US !   •I never new any details about it . I have just seen it in pictures and on TV and it looks very neat .   ReG : Horseshoe falls , also known as Canadian Falls , is the largest of the three waterfalls that   collectively form Niagara Falls on the Niagara River along with American Falls and Bridal Veil   Falls .   [ GOOD ]   KGI : Horseshoe falls is also known as Canadian Falls , which straddles the international border   between Canada and the U.S.   [ GOOD ]   Ground truth : It ’s really quite interesting and beautiful . There ’s over six million cubic feet of   water that crosses the fall ’s crest every 60 seconds . Can you imagine ?   [ INCONSISTENT - 6 M cubic feet of water crosses for the 3   waterfalls combined , not just Horseshoe falls . ]   Conversation history :   • I love the taste of pasta mixed with tomato sauce and shredded cheese all over it   • I just love mines with chunky tomatoes mixed with meatballs and mushrooms .   •Olive Garden is the place that makes the best pastas . I try to visit the place as much I can with   my friends   • I love Olive Garden especially the original one in Orlando they opened in 1982 .   • I think Italians have special genes that makes them so adept at preparing pastas   •I believe so as well I would love to visit Argentina that is where Italian - born people reside great   pasta I bet .   •I do n’t know how they do it , but no matter how hard I try to prepare one at home , mine ends up   with some peculiar taste   ReG : I do n’t know how they do it but I love the taste of pasta mixed with tomato sauce and   shredded cheese all over it   [ INCONSISTENT - Dialogue repetition ]   KGI : I just love the taste of pasta mixed with tomato sauce and shredded cheese all over it   [ INCONSISTENT - Dialogue repetition ]   Ground truth : The taste is so full of an abundance of taste it just ca n’t be copied lol .   [ INCONSISTENT - Semantically incoherent text “ taste is full of ..   taste”.]2715