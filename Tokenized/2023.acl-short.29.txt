  Daniel Varab   Novo Nordisk   IT University of Copenhagen   djam@itu.dkYumo Xu   School of Informatics   University of Edinburgh   yumo.xu@ed.ac.uk   Abstract   In this paper , we explore the efficacy of model-   ing extractive summarization with an abstrac-   tive summarization system . We propose three   novel inference algorithms for sequence - to-   sequence models , evaluate them on established   summarization benchmarks , and show that re-   cent advancements in abstractive designs have   enabled them to compete directly with extrac-   tive systems with custom extractive architec-   tures . We show for the first time that a single   model can simultaneously produce both state-   of - the - art abstractive and extractive summaries ,   introducing a unified paradigm for summariza-   tion systems . Our results question fundamental   concepts of extractive systems and pave the   way for a new paradigm - generative modeling   for extractive summarization .   1 Introduction   Extractive summarization selects a set of salient   sentences from the original document(s ) and com-   poses them into a summary . Compared to abstrac-   tive summaries , made up of words or phrases that   do not appear in the input document , extractive   summaries are less flexible but avoid inconsisten-   cies and hallucinations . The pipeline for build-   ing an extractive summarizer typically consists of   two separate stages : sentence labeling andextrac-   tive modeling . Since few summarization datasets   come with gold labels indicating which document   sentences are summary - worthy , the first step is   to create oracle sentence labels ( Nallapati et al . ,   2017 ) . The task is commonly modeled with a   sequence labeling architecture ( Cheng and Lap-   ata , 2016 ) where a salience score is estimated for   each document sentence , and top - ranked sentences   are selected for summary inclusion . Recent work   has expanded extractive modeling to higher - order   sentence selection to account for complex labelFigure 1 : Proposed inference methods for GenX. We   show ( a ) a two - stage approach with generative rank-   ing / reranking , which creates a set of candidate sum-   maries Cfrom document sentences D , and ( b ) a single-   stage inference method , generative search , which ex-   tracts summary sentences yautoregressively .   dependencies , via extracting sentences stepwise   ( Narayan et al . , 2020 ) , or reranking a small set of   summary candidates ( Zhong et al . , 2020 ; An et al . ,   2022 ) .   In this work , we revisit these fundamental con-   cepts in extractive summarization . Specifically , we   highlight that heuristically - derived sentence labels   can be highly suboptimal ( Narayan et al . , 2018b ;   Xu and Lapata , 2022b ) , and that customized neu-   ral architectures for extractive modeling prevent   taking advantage of independent improvements .   We recognize that generative modeling with a neu-   ral encoder - decoder architecture ( Bahdanau et al . ,   2015 ; Sutskever et al . , 2014 ) , the de facto choice for   abstractive summarization ( Nallapati et al . , 2017 ;   Zhang et al . , 2020 ; Lewis et al . , 2020 ) , constitutes   a promising direction for extractive summariza-   tion . In particular , such models learn directly from   abstractive references and do , therefore , not re-   quire sentence labeling , while also embodying the   extractive capabilities previously enabled by spe-   cialized neural architectures . Existing literature330has established varied and many connections be-   tween abstractive and extractive modeling such as   copy mechanism ( See et al . , 2017 ) , content selec-   tion ( Kedzie et al . , 2018 ; Gehrmann et al . , 2018 ) ,   and generation guidance ( Dou et al . , 2021 ) . These   connections , however , are mostly abstract - centric   which are identified or constructed to improve ab-   stractive summarization . In contrast , there are few   studies from an extract - centric point of view .   In this work , we propose a new summarization   paradigm that unifies extractive and abstractive   summarization with generative modeling , without   compromising abstractive performance . To this   end , we treat extractive summarization as an in-   ference -time task and explore methods for adapt-   ing a pre - trained abstractive system for extrac-   tive summarization without further optimization .   We hypothesize that an abstractive system can   be used as a summary evaluator for not only ab-   stracts but extracts as well . A model optimized   on abstractve references should be able to pro-   vide an accurate quality estimation for an extrac-   tive candidate summary when conditioned on the   input document . A straightforward approach to   validate this assumption is to search for the best   document extract with an abstractive model for   candidate evaluation . However , performing an   exhaustive search over a combinatorial space of   all eligible summary candidates is computation-   ally intractable . To tackle this challenge , we pro-   pose GenX , Generative e Xtractive summarization ,   which introduces a set of inference algorithms   ( shown in Figure 1 ) to reduce the search complex-   ity via various approximations of the entire search   space , at either sentence- or summary - level .   Experiments show that GenX achieves competi-   tive or superior performance compared to custom   systems developed for extractive summarization   on the CNN / DM benchmark without compromis-   ing its ability to generate abstracts . Particularly ,   for one - stage summarization the proposed method   shows superior results to custom extractive state-   of - the - art systems . GenX also exhibits high robust-   ness in zero - shot transfer : on XSum , its zero - shot   performance surprisingly surpasses its fully super-   vised counterpart . We further conduct an extensive   analysis of GenX ’s properties , providing potential   directions for future research on generative model-   ing for extractive summarization.2 Generative Modeling for Extracts   Given a generative model θtrained on summariza-   tion data comprising documents and abstractive   references , at inference time , for an input docu-   ment Dand a summary sequence Y , we estimate   the length - normalized log probability of Y , follow-   ing the standard practice in neural text generation   ( Cho et al . , 2014 ):   p(Y|D ) = 1   |Y|/summationdisplaylogp(Y|D , Y)(1 )   Asθis optimized at the token level , we eval-   uate both complete andpartial summaries with   p(Y|D ) .   The candidate summary space for a document   D={s}ofnsentences is combinatorial , con-   sisting of |C(D)|=C / parenleftbig / parenrightbig   candidate summaries of   length m. To sidestep the computational intractabil-   ity , we introduce three inference algorithms that   reduce the search complexity via approximations .   The first two ( ranking and reranking ) construct a   candidate summary set , using either a discrimina-   tive or generative model ( see Figure 1(a ) ) , while   the last approach searches directly over the partial   summary candidate space ( see Figure 1(b ) ) .   Generative Ranking We employ a pre - trained   generative model at both sentence- and summary-   level for hierarchical ranking . Specifically , we in-   put each document sentence sinto a generator and   evaluate its summary - worthiness independently via   its likelihood . We then rank all document sen-   tences , and any subset of size mof the top- ksen-   tences is considered as a candidate summary c. The   sequence - to - sequence generator then evaluates and   ranks all candidate summaries , and the highest-   ranked one is selected as the extractive hypothesis :   y= argmaxp(⊕(c)|D ) ( 2 )   where ⊕concatenates the selected document sen-   tences in c , ordered by their rank .   Generative Reranking Instead of using the same   generative model for both sentence and summary   evaluation , we assume access to an existing dis-   criminative model p(s|D)for sentence evaluation   and ranking . Following Zhong et al . ( 2020 ) , we   adopt BERTSE(Liu and Lapata , 2019 ) to   score each document sentence and then build candi-   date summaries as the combinations of top - scoring331   sentences . In this case , the role of generative mod-   eling is a summary - level reranker p(⊕(c)|D ) .   Generative Search Instead of ranking , we con-   sider constructing a summary by searching directly   over the sentence space , i.e. , without first compos-   ing candidate summaries from the input document .   We propose a novel search algorithm that autore-   gressively selects a sentence until a stopping crite-   rion is satisfied . Specifically , at each search step t ,   we evaluate and select a sentence as :   y= argmaxp(y⊕s|D ) ( 3 )   where ⊕concatenates the selected sentences y   and a candidate sentence s. The selected sen-   tence yis then concatenated with yto form   the selection history for the next step , as shown   in Figure 1(c ) . We follow common practice in   non - autoregressive extractive summarization ( Liu   and Lapata , 2019 ; Zhong et al . , 2020 ) and as-   sume a fixed number of sentences in the summary   hypothesis , leading to a fixed number of search   steps . Narayan et al . ( 2020 ) introduced a step-   wise model which employs a special stop - token   where the search stops when the token is generated .   To explore this we additionally experiment with   a dynamic stopping criterion where search over   sentences continues until the end of the sequence   token , EOS , provides a higher summary likelihood   than adding an additional sentence :   s.t.maxp(y⊕s|D ) > p(y⊕EOS|D).(4 )   3 Experimental Setup   We perform supervised experiments on CNN / DM   ( Hermann et al . , 2015 ) and zero - shot experiments   on XSum ( Narayan et al . , 2018a ) . We evaluate   summaries with ROUGE ( Lin and Hovy , 2003 ) .   Details for our experimental settings and datasets   can be found in Appendix A.   As there is no established baseline for extractive   summarization with generative modeling , we con-   struct Posthoc Rank , a posthoc method for direct   comparison with GenX. The baseline first generates   an abstract using the abstractive model . Then , the   generated abstract is used to query document sen-   tences and msentences are retrieved with BM25   as the summary while applying tri - gram blocking .   4 Results   Supervised Summarization Table 1 shows the   results of various systems trained and evaluated on   CNN / DM . The first block presents the performance   of the Lead-3 baseline which considers the first 3   sentences in a document as the summary and an   Oracle baseline which serves as an upper bound .   The second block reports the performance of   one - stage summarization systems . Stepwise ECT-   Sum ( Narayan et al . , 2020 ) is a state - of - the - art   autoregressive system that learns to score partial   summaries by selecting which sentence is a sum-   mary sentence iteratively . Different from GenX , it   is a highly - customized extractive architecture opti-   mized with extractive oracle summaries . As can be   seen , GenX performs on par with Stepwise ETC-   Sum , and outperforms BERTSumExt ( Liu and La-   pata , 2019 ) and RoBERTaSumExt ( Narayan et al . ,   2020 ) . Two popular extractive systems based on   sequence labeling.332   The third block presents the results of two - stage   systems . TRB denotes an additional stage for sen-   tence selection with Trigram Blocking , an effec-   tive method for reducing redundancy . MatchSum   ( Zhong et al . , 2020 ) is a state - of - the - art extrac-   tive system that takes top - ranked sentences from   BERTSumExt and then re - ranks the summary can-   didates composed by them with a model based on a   Siamese - BERT architecture . As can be seen , GenX   models improve over the one - stage BERTSumExt   and RoBERTaSumExt , i.e. , with or without BERT-   SumExt as a sentence - level ranker . Its reranking   variant also outperforms BERTSumExt+TRB and   RoBERTaSumExt+TRB , showing that generative   summary - level evaluation is more effective than   heuristically - derived selection criteria . Note , the   performance of GenX still falls short of state - of-   the - art MatchSum . This is all achieved while the   design allows the base generative model to retain   its ability to produce abstractive summaries . This   is not applicable to any existing extractive systems   except Posthoc Rank , which shows significantly   inferior performance .   Zero - Shot Summarization We also examine the   generalization capability of extractive systems in   azero - shot setting . As shown in Table 2 , GenX   generalizes to a different dataset robustly , outper-   forming strong one- and two - stage systems . It is   generally perceived that a model ’s zero - shot per-   formance is inferior to the supervised performance .   Surprisingly , GenX performs substantially better   in the zero - shot setting than its supervised counter-   part . One potential reason for this is that despite   the discrepancy between training and inference ,   CNN / DM is a more extractive dataset than XSum   ( Liu and Lapata , 2019 ) , and therefore contains   more extract - specific knowledge . Compared to   existing systems , GenX is more capable of transfer-   ring the extractive ability learned from CNN / DM   to XSum . This shows that treating extractive sum-   marization as an inference task can significantlyreduce the risk of overfitting to one specific dataset ,   shedding light on a new direction for knowledge   transferring in zero - shot summarization .   5 Ablation Study   We further assessed GenX with an ablation study .   Replacing BRIO ( trained with MLE and Con-   trastive Loss ) with Bart ( trained with MLE ) leads to   the largest performance drop . With the augmenta-   tion of contrastive learning , the abstractive system   is competent in the dual role of both a generation   and evaluation model , emphasizing the importance   of calibrating a generative model on its summary-   level probability , even for its extractive inference .   The dynamic stopping mechanism introduced in   Equation ( 4 ) performs on par with fixed - step search ,   showing that learning directly from abstracts is   a promising way to teach models when to stop   for summary extraction . GenX is also shown to   be able to search for extractive summaries of less   redundancy as its performance can notbe further   improved by incorporating Trigram Blocking .   6 Efficiency   We have shown that abstractive systems are capa-   ble extractive summarizers , however , it is impor-   tant to highlight that the proposed method exhibits   different computational requirements than that of   contemporary extractive designs . Unlike extrac-   tive designs that compute a single score for a can-   didate sentence or summary ( via a classification   token ) , abstractive systems produce scores for all   individual tokens in a candidate summary . Com-   puting these extra tokens causes approaches such   asranking andreranking with GenX more compu-   tationally demanding . However , when combined   with search GenX stands as an efficient solution   to searching through an otherwise intractable can-   didate summary space . This is enabled by an ab-   stractive system ’s ability to sequentially score text   ( see Equation 1 ) and boils down to the complexity   of beam search . This is a clear improvement in   computational efficiency over systems like Match-   Sum which only supports scoring complete sum-   maries and must exhaustively recompute different   permutations in the candidate summary spaces . To   make this strategy computationally tractable these   models resort to heavy pruning which limits the333expressiveness that high - order modeling otherwise   enables .   7 Related Work   There is a plethora of work on controlling differ-   ent aspects of summarization , from content ( Xu   and Lapata , 2022a ; Ahuja et al . , 2022 ) to formats   ( Zhong et al . , 2022 ) . In this work , we offer effi-   cient and effective control over the summary type   ( extract versus abstract ) during inference . Recent   work also investigates how to treat discriminative   tasks such as information extraction and retrieval   with generative modeling and its effectiveness for   entities ( De Cao et al . , 2020 ) and string identifiers   ( Bevilacqua et al . , 2022 ) . Others have suggested   delegating extractive inference to the encoder of   a generative model ( An et al . , 2022 ) . Despite the   resemblances , extractive summarization with gener-   ative modeling remains under - explored and stands   as a promising research direction with the surge of   innovations in large language models .   8 Conclusion   In this paper , we explored the possibility of mod-   eling extractive summarization with an abstractive   system . We proposed three novel inference algo-   rithms which allow an abstractive model to perform   the extractive task . Our results showed that not only   is extractive summarization feasible , but recent sys-   tems are directly competitive with contemporary   extractive systems . This work shows that extractive   and abstractive paradigms can be unified through a   sequence - to - sequence design , removing the need   for oracle summary labels and custom extractive   model architectures .   9 Limitations   One potential way to improve the extractive per-   formance of a generative system is to explicitly   model the likelihood of extracts during training .   Driven by this intuition , we investigate creating   a mixture of extractive and abstractive candidates   for contrastive learning in BRIO . Specifically , we   obtain extractive candidates with beam labeling   proposed in Xu and Lapata ( 2022b ) , while the ab-   stractive ones are from the original BRIO training   data . Nevertheless , as we can see , this mixing   method hurts both BRIO ’s extractive and abstrac-   tive performance . However , it is noteworthy that   extractive summary is important in a wider con-   text , as shown in Section 4 : reference summariesin CNN / DM are highly extractive and optimizing   a model on these summaries therefore may have   provided it with the task instruction needed for ex-   tractive summarization , albeit implicitly . We leave   the study of a more effective extract - aware learning   strategy for future study .   Furthermore , we emphasize that the conclusions   drawn in this paper are based on results produced   on English datasets from the news domain . Even   though these datasets are established benchmark   datasets for summarization it is imaginable that   other domains and languages may have produced   different evidence . Despite this , the results remain   insightful as the results show that extractive summa-   rization is in fact feasible with modern abstractive   systems . In future research , we look forward to   shedding light on the possibilities and limitations   of the proposed methods in a broader context .   References334335   A Implementation Details   We show detailed data statistics in Table 4 . For   our GenX experiments , we use the BRIO sys-   tem ( Liu et al . , 2022 ) as our underlying abstrac-   tive model . To replicate the BRIO system we   run the published code repository associated with   the paper . Specifically , we initialize a BART   model with the Huggingface Models Hub check-   point facebook / bart - large - cnn and fine - tune it   with the provided configuration using the train-   ing scheme presented in the paper on both the   CNN / Dailymail , and XSum dataset using the data   distributed in said repository . We train the model   with full precision on a single machine with four   Tesla V100 GPUs for 30 hours and choose the   checkpoint with the lowest cross - entropy ( genera-   tive ) loss term on a held - out validation set . Inter-   estingly choosing the checkpoint with the lowest   contrastive term produces poor results . Also , using   mixed precision training does n’t appear to work .   To run the inference algorithms we initialize a   BART system with different weights , either ob-   tained through the above training procedure ( BRIO )   or the baseline facebook / bart - large - cnn check-   point . The hyperparameter mis identical to the   desired length of the generated summary . mwas   tuned on the validation set and set to 3 for the   CNN / DM dataset , and 2 for the XSum . kwas set   to 5 , following MatchSum system . We studied the   effects of various length penalties in Equation 1 and   did not find our approach sensitive to its choice and   omitted it from the equation . For this computation   we run the model under fp16 mixed precision to   save memory , however , casting the model entirely   to half - precision for inference does not appear to   work . Datasets CNN / DM XSum   Language En En   Domain Newswire Newswire   # Train 287,084 203,02   # Validation 13,367 11,273   # Test 11,489 11,332   # Sentences in Extract 3 2   We used standard parameter settings for all ex-   periments : ROUGE-1.5.5.pl -c 95 -m -r 1000 -n 2   -a .   B License Information   The datasets used in this work , CNN / DM ( Her-   mann et al . , 2015 ) and XSum ( Narayan et al . ,   2018a ) , are both released under the MIT License .   C System Output336Document : We spend a third of our lives asleep , but most of us do n’t pay attention to what our mind   and body actually need during these resting hours in order to feel refreshed every day . The Sleep   Health Foundation have released a study reporting that 30 percent of Australians complain about their   lack of sleep on a daily basis . According to Chair Professor David Hillman , those misplaced hours of   sleep must be paid back in order to be functional for the entire week . A study has outlined that 30   percent of Australians complain about their lack of sleep on a daily basis . The average adult needs   around eight hours of sleep per night with a range of seven to nine . The average amount of sleep for an   adult is around eight hours , with a range of seven to nine , the ABC have reported . Any less than six   hours or any more than 10 hours is unusual for the standard person . Professor Hillman added that our   sleep pattern is influenced by how much we are willing to compromise from the work week . ’ A lot   of us pay back a bit of that debt on the weekend but I think it ’s possible to exist in a sort of tolerable ,   sleep - restricted state , ’ he said . ’ In other words you ’re not optimal , but you ’re still functional . ’ Pushing   these sleep - debt boundaries can lead to micro sleeps in certain people . Therefore , the hours must be   paid back to avoid an error rate in alertness tasks . Any less than six or any more than ten hours is   unusual for the standard person . If power napping , it is important to get no more than 20 minutes or   inertia will set in . In relation to a sleep schedule , Professor Hillman said the eight hours per night does   not necessarily need to be consecutive . ’ Interestingly enough , your slow wave sleep , is in the first four   hours , ’ he said . ’ Most adults , the most convenient way our particular society is organised is to have   your eight hours in a continuous block overnight but that ’s not a necessary thing . ’ If choosing to break   up your eight hours of sleep , napping throughout the day is the answer . Professor Hillman advises 20   minute power naps to avoid falling into deep sleep and suffering from inertia which makes you feel   temporarily worse off . ’ The longer naps , you get the sleep inertia but ultimately once you ’ve got up ,   they sustain you better , ’ he said . Professor Hillman has also advised that if you are waking up tired   and fatigued it could be due to sleep apnoea which is often associated with snoring .   Reference Summary : The Sleep Foundation study has shown that adults need 8 hours of sleep .   According to the study , 30 percent of Australians say they lack sleep daily . Professor David Hillman   said it ’s important to pay back our sleep debts . He also says sleep can be broken up as long as you get   the first 4 hours . Power naps should not be longer than 20 minutes or inertia will set in .   BertSumExt : The Sleep Health Foundation have released a study reporting that 30 percent of   Australians complain about their lack of sleep on a daily basis . The average adult needs around eight   hours of sleep per night with a range of seven to nine . Any less than six hours or any more than 10   hours is unusual for the standard person .   MatchSum : The Sleep Health Foundation have released a study reporting that 30 percent of Australians   complain about their lack of sleep on a daily basis . The average adult needs around eight hours of   sleep per night with a range of seven to nine .   GenX ( Search ) : A study has outlined that 30 percent of Australians complain about their lack of sleep   on a daily basis . The average adult needs around eight hours of sleep per night with a range of seven to   nine . According to Chair Professor David Hillman , those misplaced hours of sleep must be paid back   in order to be functional for the entire week.337ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 9 .   /squareA2 . Did you discuss any potential risks of your work ?   Section 9 .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3 and 4 .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3 - 4 and Appendix A - B.   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix B.   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix A.   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We used existing benchmarks as they are for fair comparisons .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Appendix A.   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Appendix A.   C / squareDid you run computational experiments ?   Section 4 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A.338 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Appendix A.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.339