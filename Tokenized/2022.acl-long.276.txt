  Sayan Ghosh Shashank Srivastava   UNC Chapel Hill   { sayghosh,ssrivastava}@cs.unc.edu   Abstract   While large language models have shown ex-   citing progress on several NLP benchmarks ,   evaluating their ability for complex analogical   reasoning remains under - explored . Here , we   introduce a high - quality crowdsourced dataset   of narratives for employing proverbs in con-   text as a benchmark for abstract language un-   derstanding . The dataset provides ﬁne - grained   annotation of aligned spans between proverbs   and narratives , and contains minimal lexical   overlaps between narratives and proverbs , en-   suring that models need to go beyond surface-   level reasoning to succeed . We explore three   tasks : ( 1 ) proverb recommendation and align-   ment prediction , ( 2 ) narrative generation for   a given proverb and topic , and ( 3 ) identify-   ing narratives with similar motifs . Our experi-   ments show that neural language models strug-   gle on these tasks compared to humans , and   these tasks pose multiple learning challenges .   1 Introduction   Large language models ( LLMs ) ( Devlin et al . ,   2019 ; Liu et al . , 2019a ; Raffel et al . , 2020 ; Lewis   et al . , 2020 ; Reimers and Gurevych , 2019 ; Sanh   et al . , 2019 ; Lan et al . , 2020 ) have led to a paradigm   shift in NLP , and have shown exciting progress on   benchmarks such as GLUE ( Wang et al . , 2019b )   and SuperGLUE ( Wang et al . , 2019a ) . In particu-   lar , these include tasks such as reading comprehen-   sion , natural language inference , and coreference   resolution . Many of these tasks rely on semantic   and syntactic reasoning , which has been mastered   by these LLMs . For example , apart from improv-   ing on distributional semantics through contextual-   ized embeddings ( Ethayarajh , 2019 ) , recent work   has shown evidence that these models implicitly   learn emergent concepts such as subject - verb agree-   ment ( Jawahar et al . , 2019 ) , semantic roles ( Tenney   et al . , 2019 ) and dependency structures ( Hewitt and   Manning , 2019).Figure 1 : We introduce ePiC , a crowdsourced dataset   of narratives for employing proverbs in context . Our   dataset contains narratives ( N1 and N2 ) paired against   proverbs ( P ) along with a ﬁne - grained annotation of   aligned spans between the narratives and proverbs .   Aligned spans are shown with matching colors and in-   dicate correspondences in roles between proverbs and   narratives . We explore three tasks : ( 1 ) proverb recom-   mendation and alignment prediction ( predict P given   N1 ) , ( 2 ) narrative generation for a given proverb and   topic ( generate N1 given P and K1 ) , and ( 3 ) identify-   ing narratives with similar motifs ( e.g. identify N2 in a   set of narratives given N1 ) .   However , humans show an ability for deeper   linguistic reasoning . We can identify people ’s in-   tentions and goals ( Douglas and Sutton , 2006 ) , per-   form relational reasoning ( Alexander et al . , 2016 ) ,   and ﬁnd analogies in situations with little surface   overlap ( Holyoak , 2013 ) . In particular , making ver-   bal analogies in the form of proverbs is noted as   an indicator of literary ability ( Penﬁeld and Duru ,   1988 ; Nippold et al . , 2001 ) . Proverbs are also repos-   itories of information on culture , societal norms ,   values , and folk wisdom ( Raymond , 1956 ; White ,   1987 ) . In this work , we investigate proverbs in nar-   rative contexts as a testbed for evaluating abstract   reasoning and analogical abilities of LLMs .   We introduce ePiC ( employing Proverbs in3989Context ) , a high - quality crowdsourced dataset of   narratives paired with proverbs . The dataset pro-   vides ﬁne - grained annotation of aligned spans be-   tween proverbs and narratives , and is designed to   minimize lexical overlap between narratives and   proverbs . Figure 1 shows two examples of narra-   tives for a proverb from our dataset , along with   corresponding alignment annotations . We diverge   from related extant resources ( Wang et al . , 2020 ;   Tan et al . , 2015 , 2016 ) on using proverbs in terms of   quality of narratives , direct supervision , and having   ﬁne - grained alignment annotations . We explore   three tasks : ( 1 ) proverb and alignment prediction   ( § 5.1 ) , ( 2 ) narrative generation for a given proverb   and a set of keywords specifying a topic ( § 5.2 ) ,   and ( 3 ) discovering narratives with similar motifs   ( § 5.3 ) . By benchmarking several LLMs , we ﬁnd   that existing models struggle with these tasks , sug-   gesting much scope of improvement in abstract   reasoning . In particular , humans show much higher   performance in many cases .   In § 3 , we describe the crowdsourced creation of   theePiC dataset . In § 4 , we analyze lexical overlap ,   biases , and narrative quality in ePiC . § 5 describes   the three tasks and details of experimental evalua-   tion of LLMs for each task . We conclude with a   discussion , and a statement of ethics and broader   impact relevant to our work . Our contributions are :   •We introduce ePiC , a high - quality dataset for em-   ploying proverbs in context . It contains multiple   narratives for English proverbs and ﬁne - grained   annotation of aligned spans between them .   •We design three challenging tasks that require   models to go beyond surface - level reasoning and   provoke research towards making more socially   grounded NLP systems .   •We benchmark the performance of several state-   of - the - art large language models in our proposed   tasks using our dataset .   Our dataset and code are publicly available at :   https://epic-benchmark.github.io   2 Related Work   Prior works in ﬁgurative language understanding   have explored a diverse set of topics , such as simile   detection and generation ( Niculae and Danescu-   Niculescu - Mizil , 2014 ; Mpouli , 2017 ; Zeng et al . ,   2020 ; Chakrabarty et al . , 2020 ) , metaphor detectionand generation ( Dagan et al . , 2005 ; Gao et al . , 2018 ;   Stowe et al . , 2019 , 2021 ; Chakrabarty et al . , 2021b ) ,   pun identiﬁcation ( Poliak et al . , 2018 ; Miller and   Turkovi ´ c , 2016 ) , and quote / proverb recommenda-   tion ( Tan et al . , 2015 , 2016 ; Wang et al . , 2020 ) .   Recent work ( Chakrabarty et al . , 2021a ) has also   focused on interpreting idioms and similes in nar-   ratives . Liu et al . ( 2019b ) has explored recom-   mending Chinese idioms through context - based   recommendation and Zheng et al . ( 2019 ) formu-   lated idiom recommendation as cloze - style reading   comprehension task . Learning to quote has been ex-   plored based on ﬁction ( Tan et al . , 2015 , 2016 ) and   noisy social media conversations from Twitter , Red-   dit or Weibo ( Lee et al . , 2016 ; Wang et al . , 2020 ) .   In the most related prior work , authors explore a   quote retrieval task borrowing inspiration from con-   text based recommendation systems ( Huang et al . ,   2012 ; He et al . , 2010 ) . Wang et al . ( 2020 ) formu-   lated learning to quote as a generation task by us-   ing topic modeling ( Miao et al . , 2017 ; Wang et al . ,   2019c ) in a sequence - to - sequence network . While   previous work has considered idioms , proverbs and   common phrases as quotes , we speciﬁcally work   with proverbs . Compared to earlier datasets , our   dataset is manually created and labeled . Further ,   ePiC includes ﬁne - grained annotations aligning   parts of proverb to parts of the narrative , which has   signiﬁcant possibilities for model training , evalua-   tion and interpretability .   3 Dataset Creation   In this section , we describe the steps involved in   creating the dataset in detail .   Proverb collection : We obtained a candidate set   of English proverbs by scraping websites of ‘ The   Phrase Finder’and WikiQuotes . Next , this set   was manually pruned to remove lexical variations   of the same proverb . This manual curation led to   a set of 250 proverbs , which we consider in the   current version of our dataset .   Narrative collection : In the second step , we use   Amazon Mechanical Turk to collect a diverse set   of narratives corresponding to each proverb . We   collect 10 narratives contributed by distinct turkers   for each proverb , leading to a total of 2500 proverb-   narrative pairs . We also ensure that no turker con-   tributes a large number of narratives to alleviate3990annotator bias ( Geva et al . , 2019 ) ( where models   can overﬁt to annotator characteristics ) while en-   couraging diversity in writing style and content .   The turkers were asked to write short realistic sto-   ries , preferably within 100 words . Additionally , to   avoid surface - form biases , turkers were encouraged   to minimize lexical overlap and to not mention the   proverb or parts of it in the narrative . This was   done so that doing well on the tasks requires a de-   tailed understanding of the narratives rather than   relying on surface - level cues . Turkers were paid 50   cents for each narrative for this task .   Span alignment annotation : Next , we solicit ﬁne-   grained annotations between the narratives and   the proverb in form of aligned spans . For this ,   we present proverb - narrative pairs to turkers ask-   ing them to ﬁnd contiguous spans in the narra-   tive which align well with contiguous spans in the   proverb . Turkers could submit up to 5 pairs of   aligned spans per proverb - narrative pair . These   aligned spans highlight the grounding of a proverb   in the narrative ( see Figure 1 ) . These annotations   can help to verify the reasoning capabilities of var-   ious neural models by checking if these models   are able to identify these correspondences , and add   interpretability to our tasks . Turkers were paid 25   cents for each proverb - narrative pair annotation for   this task .   Statistics : Table 1 shows the statistics of narrative   collection for the proverbs . The narrative writing   task was perceived as challenging yet interesting   by most turkers due to ( a ) not having outlines about   topics for the narrative beforehand ( b ) requirement   of low lexical overlap with the proverb . Thus , the   narrative writing task had a learning curve and   some of the narratives submitted initially were not   included in the dataset .   4 Dataset Analysis   Table 2 shows some statistics of the dataset col-   lected through the process described in § 3 . In this   section , we analyze the characteristics and biases   of the ePiC dataset in detail .   4.1 Lexical overlap analysis   Using n - grams : We evaluate the extent of lexi-   cal overlap between proverbs and narratives by   computing common n - grams between them . Ta-   ble 3 reports the average Jaccard similarity score   between n - gram sets of proverbs and narratives ,   and the average number of common n - grams . On   average , there are 1.27 unigrams common between   narratives and proverbs ( including stopwords ) . In   comparison , randomly permuting assignments of   proverbs for narratives yields an average unigram   Jaccard similarity of 0.0211 and 1.06 common uni-   grams . Thus , the overlap metrics in the dataset are   comparable to those between unrelated texts .   To evaluate diversity among narratives corre-   sponding to a proverb , we compute average Jaccard   similarity between sets of unigrams for the narra-   tives . This score is 0.107 , which is comparable to   a value of 0.098 for unigram overlap between pairs   of narratives from different proverbs . This suggests   a high lexical diversity between narratives .   Using distributional embeddings : We explore if   we can retrieve the correct proverb corresponding   to a narrative only by using similarity in their distri-   butional representations . The similarity between a   proverb and a narrative is deﬁned as the cosine simi-   larity between the representation of the proverb and   the narrative obtained using word2vec embeddings   ( Mikolov et al . , 2013 ) or contextual embeddings   from LLMs . Details of implementation are pro-   vided in Appendix § F.1 .   For this retrieval task , we report the accuracy and   Mean Reciprocal Rank of the correct proverb in3991   Table 4 . We note that while all models perform bet-   ter than random ( with Sentence - BERT performing   the best ) , the performance is very low when using   out - of - the - box representations . In § 5 , we explore   learning - based methods for the same setup .   4.2 Data characteristics   Diversity of narrative events : Fig 2 shows the   distribution of events in our dataset . Following   Mostafazadeh et al . ( 2016 ) we ﬁnd events as the   hyponyms of the word ‘ event ’ or ‘ process ’ using   WordNet ( Fellbaum , 2010 ) . We see that the top   events comprise less than 3 % of all events in our   dataset , and the long tail of less frequent events   shows the diversity of the dataset .   Sentiment analysis : To evaluate the presence of   sentiment association bias between proverbs and   corresponding narratives ( e.g. , if negative senti-   ment proverbs only correspond to negative senti-   ments in narratives ) , we perform sentiment analysis   of the narratives using V ADER ( Hutto and Gilbert ,   2014 ) . Figure 3 shows the average sentiment scores   of the narratives corresponding to a proverb plotted   against the sentiment score of the proverb . We ﬁnd   that the narratives are diverse in terms of their senti-   ment polarities showing a weak positive correlation   ( Pearson correlation score 0.35 ) with the sentiment   score of the proverbs . Figure 4 shows the variance   in terms of the number of positive and negative   sentiment narratives ( out of 10 ) for each proverb ,   showing a diverse spread of narrative sentiment   polarities across proverbs . For additional details ,   please refer to Appendix § A.   We perform a few additional analyses on our   dataset and found that ( 1 ) around 61 % of mentions   in the narratives were male , ( 2 ) diverse spread of   reading complexity values in narratives measured   using Fleisch reading ease , and ( 3 ) absence of any   hate speech in the narratives of our dataset . The   detailed experiments for these analyses are given   in Appendix § A.   4.3 Human Evaluation of Dataset Quality   We perform a human evaluation of the narratives in   our dataset on various criteria to judge the quality   of our dataset . We perform this evaluation using the   AMT platform . We randomly sample 250 proverb-   narrative pairs and ask the turkers to evaluate the   narratives on the following criteria :   •Relatedness : how closely the narrative reﬂects3992   the meaning of the proverb ( 1 : totally unrelated ,   5 : perfectly related )   •Interesting / Creative : how much is the narrative   like a short creative or interesting story ( 1 : very   uninteresting / boring , 5 : very creative / story - like )   •Fluency : grammatical correctness of the narra-   tive ( 1 : poor English with grammatical mistakes ,   5 : perfect English with no errors in writing )   •Overall rating   All the ratings are done on Likert scales from 1 to   5 , where 1 is the lowest value for each criterion and   5 is the highest . Also , the rating value ‘ 3 ’ was cali-   brated to be slightly leaning to the higher end of the   scale ( instead of neutral ) so that the turkers take a   clear stand on the polarity of each criterion . Table 5   shows the qualitative evaluation of our dataset . The   average overall rating was 3.67 and the average   pair - wise inter - annotator agreement for labeling a   narrative as overall good vs overall poor ( overall   score > = 3 vs < 3 ) is 0.84 . We also rate the quality   of the aligned spans in our dataset similarly on a   scale of 1 to 5 . The average rating of the alignment   between spans was 3.91 and the average pair - wise   inter - annotator agreement for alignment as good vs   poor ( rating > = 3 vs < 3 ) is 0.86 .   Table 6 highlights the key differences between   ePiC and prior work that dealt with related ﬁgu-   rative language tasks involving quotes . Notably ,   ePiC exclusively deals with proverbs unlike prior   work ( which includes common phrases and idioms   such as “ trust your gut " ) and also provides granu-   lar annotations in form of annotated spans . Also , ePiC contains narratives crowdsourced by specif-   ically keeping proverbs in focus , rather than ob-   taining them using heuristic supervision . To quan-   tify dataset quality , we ran human evaluation sim-   ilar to ePiC over ( 1 ) 200 randomly drawn sam-   ples from the “ Reddit " dataset of quotations in   context from the Wang et al . ( 2020 ) , and ( 2 ) 200   randomly drawn samples from the corpus of Tan   et al . ( 2015 ) . Based on average Likert scores in Ta-   ble 5 we ﬁnd that ePiC is ( 1 ) signiﬁcantly superior   ( using t - test ; p < 0:05 ) on all criteria than Wang   et al . ( 2020 ) , and ( 2 ) better in overall ratings than   Tan et al . ( 2015 ) .   5 Tasks & Evaluation   In this section , we introduce three tasks associated   with ePiC and describe their experimental setup   and benchmark results : ( 1 ) Proverb and Alignment   Prediction , ( 2 ) Narrative Generation , and ( 3 ) Iden-   tifying narratives with similar motifs .   5.1 Proverb and alignment prediction   5.1.1 Task details   In this task , the objective is to predict the correct   proverb for a given narrative from the set of 250   proverbs in the dataset . The motivation of this task   is to test whether language models can abstract the   underlying meaning of the narratives and make an   analogy with the correct proverb from a large set   of proverbs . In terms of applications , this task is   related to proverb recommendation , which can be   useful in creative writing assistants . The task is   challenging as there might be multiple proverbs   loosely related to the narrative context , but not   be completely consonant with subliminal themes   in the narrative . An underlying assumption here   is that a narrative would match well with exactly   one proverb . We found this reasonable for most   examples in the dataset .   5.1.2 Experiment Setup and Results   We consider two settings , predicting ( 1 ) Seen and   ( 2 ) Unseen proverbs .   •Seen proverbs : The set of proverbs in the train   and test set are the same . We divide narratives   corresponding to each proverb into train and test   in 6:4 ratio . So , the train and test sets have 1500   and 1000 proverb - narrative pairs respectively .   •Unseen proverbs : Here , we consider 150   proverbs in the train set and the remaining 100   proverbs in the test set ( 6:4 split on the set of   proverbs ) . The sets of proverbs in the train and3993   test split are disjoint . So , the train and test sets   have 1500 and 1000 proverb - narrative pairs re-   spectively ( since each proverb has 10 narratives ) .   Proverb prediction : Here we focus on only pre-   dicting the corresponding proverb for a narrative ,   without employing the span alignments in training   or evaluation . For this , we ﬁne - tune the retrieval   models based on different LLMs previously de-   scribed in § 4 ( details of models in Appendix § F.2 ) .   To evaluate performance we consider accuracy and   Mean Reciprocal Rank as metrics . Table 7 shows   best proverb prediction performance on test split   for ‘ seen ’ and ‘ unseen ’ proverbs . RoBERTa per-   forms the best for both the ‘ seen ’ and ‘ unseen ’   settings , and the performance for all models is con-   sistently lower for unseen proverbs ( as would be   expected , since this task involves much greater gen-   eralization ) . Further , while the performance of all   models is much better than chance , even the high-   est performance is only 28.2%.Alignment prediction : Here we focus only on pre-   dicting an aligned span from the narrative given the   narrative , proverb , and a span from the proverb   as inputs . We ﬁne - tune two large language mod-   els ( BERT and RoBERTa ) for this by adopting a   learning framework similar to answer span pre-   diction for SQUAD ( Rajpurkar et al . , 2016 ) . The   language model outputs two probability distribu-   tions corresponding to the start and end positions   of a span , over the narrative tokens . We iterate over   all the combinations of the start and end tokens and   choose the span with maximum likelihood . For   span prediction , we report token - level precision , re-   call , and F1 . Table 8 shows the results of alignment   prediction on the ‘ seen ’ proverbs using BERT and   RoBERTa models . We ﬁnd that the performance   is low for both models indicating major scope for   improvements .   Predicting proverbs and alignment jointly : We   formulate this as multi - task learning . We extend the   models from the proverb prediction task by adding   a component to predict span from narrative given   a span from the proverb and the narrative . The   language model is thus shared across the proverb   prediction and span prediction tasks . The span pre-   diction branch predicts the start and end position   of the corresponding narrative span . We jointly   train the model with multi - task learning of the two   tasks , i.e. , proverb and alignment prediction , on the   ‘ seen ’ proverbs data split . We report the accuracy   for proverb prediction and precision , recall , and F1   for span prediction . Apart from this joint model ,   we also consider a pipelined baseline model which   ﬁrst does proverb prediction , followed by span pre-3994diction if the correct proverb was predicted . Table 9   shows results for the joint model and the pipelined-   baseline model . The low performance of the mod-   els indicates major scope for improvements in the   individual tasks . While in principle the two tasks   should beneﬁt from joint training , we ﬁnd that joint   training performs worse than pipelined - baseline   for both proverb and alignment prediction . Future   work can explore designing better models for joint   training to leverage the interdependence between   proverb prediction and alignment prediction .   5.1.3 Qualitative analysis of proverb   prediction models   Figure 5 shows a heatmap to study the differences   in prediction accuracies of BERT and RoBERTa   models . We see that RoBERTa generally outper-   forms BERT for many cases ( in Figure 5 , values   in the bottom - right triangle are typically greater   than the top - left ) . Looking into the narratives for   proverbs in the test set with high accuracy ( > = 0.75 ) ,   we think a reason for the high performance could   be the presence of certain words / phrases which are   synonymous to some words / phrases in the proverb   ( for example , presence of word ‘ group ’ for the   proverb ‘ birds of a feather ﬂock together ’ ) . On the   other hand , there are cases when the model is con-   fused because of multiple topics being discussed   in the narrative resulting in an incorrect prediction .   For example , some narratives in the test set for the   proverb ‘ life ’s not all beer and skittles ’ describe   earning money the hard way , which confused the   RoBERTa model into predicting ‘ time is money ’   for such narratives .   5.1.4 MCQ task for human performance   comparison   To formulate a feasible task for humans , we frame   proverb prediction as a multiple choice question   ( MCQ ) task where for each narrative , 5 proverbs   are provided as choices . The set of choices in-   cludes the correct proverb and 4 other distractor   proverbs , chosen by using the ﬁne - tuned RoBERTa   model . Examples of the MCQ task and details of   choosing distractors are provided in Appendix § B.   Table 10 shows the accuracy of the human evalu-   ation for this MCQ task on a random sample of   100 narratives from the test split of " seen " proverbs   conducted using AMT . Compared to RoBERTa ,   we ﬁnd humans are much better at this adversari-   ally created MCQ task . Note that the performance   for RoBERTa in Table 10 and Table 7 is different ,   as Table 10 reports accuracy only on the random   sample of narratives chosen for human evaluation .   The estimate for human performance is likely an   under - estimate since in many cases human subjects   were unfamiliar with the meanings of some of the   proverbs provided in the options and as a result ,   focused more on surface - level cues ( details of this   analysis are provided in Appendix § B ) . The aver-   age pair - wise inter - annotator agreement between   human subjects for this task was 0.73 .   This evaluation does not take into account semantic   similarity between proverbs ( two proverbs might be   equally suitable for the same context ) . To explore   this , we analyze the human errors on the MCQ task   and ﬁnd that in only around 11 % of the errors , the   proverb chosen by humans is semantically similar   to the annotated proverb and can also be a suit-   able answer to the MCQ task . Details about this   analysis are given in Appendix § C. Future work   can consider handling semantic similarity between   proverbs explicitly and devise suitable evaluation   metrics.3995   5.2 Narrative Generation   5.2.1 Task details   One of the important use - cases for NLP models in   the creative writing domain is to use these mod-   els to generate content . We explore the task of   generating narratives corresponding to a proverb   and a given topic ( speciﬁed as a set of keywords ) .   We benchmark the performance of two recently   proposed state - of - the - art models in text generation ,   T5 ( Raffel et al . , 2020 ) and BART ( Lewis et al . ,   2020 ) , by ﬁne - tuning them on ePiC .   5.2.2 Experiments and Results   We divide our dataset into train and test split under   ‘ seen ’ and ‘ unseen ’ proverbs settings similar to the   proverb prediction task . We consider the set of   verbs and named - entities as the keywords for a   narrative . We train our narrative generation model   conditioned on the proverb and the keywords .   Table 11 shows results for automatic evaluation   of the generated narratives using BLEU ( Papineni   et al . , 2002 ) , ROUGE - L ( Lin , 2004 ) , and recall of   the keywords mentioned in the generated narrative   as metrics . Examples of generated narratives are   given in Appendix § D. We ﬁnd that BART per-   forms better than T5 on the automatic evaluation   metrics . Further , we perform human evaluation   to evaluate the quality of the generated narratives   in AMT by considering the same criteria ( and rat-   ing semantics ) employed in Section 4.3 . Table 12   shows the human evaluation of generated narra-   tives using BART and T5 when tested over ‘ seen ’   proverbs . Low scores for BLEU and ROUGE - L   in automatic metrics and low Likert ratings of the   generated narratives indicate much scope for future   improvement on this task .   5.3 Identifying narratives with similar motifs   5.3.1 Task details   An important aspect of language understanding is   the ability to make linguistic ( and narrative ) analo-   gies , i.e. , identifying ‘ similarity ’ between narra-   tives ( e.g. , identifying two narratives that are vari-   ations on the ‘ Cinderella story ’ theme ) . Here , we   explore the task of identifying narrative analogy   by modeling ‘ similarity ’ between narratives based   on proverbs illustrated by them . For this task , two   narratives are taken to be similar if they are related   to the same proverb .   5.3.2 Experiments and Results   For this task , we use the train and test split of ‘ seen ’   proverbs setup in the proverb prediction task . The   aim is to ﬁnd similar narratives for each narrative   in the test split amongst all narratives in the test   split . So for each narrative , there are 3 other similar   narratives ( corresponding to the same proverb ) in   the test split ( containing 1000 narratives ) .   Modeling similarity between narratives We   use the learned models in the proverb prediction   task to obtain a probability distribution over the   proverbs for each narrative . To model similarity ,   we compute the distance between the ( vectors rep-   resenting ) two probability distributions using one   of the following : ( 1 ) cosine distance ; ( 2 ) Jenson-   Shannon divergence ; ( 3 ) L2 ( Euclidean ) distance ;   and ( 4 ) L1 ( Manhattan ) distance . We predict the   narrative closest ( in terms of distance metrics ) to   the input narrative as the most similar . Table 13   shows the accuracy of getting a similar narrative   using different distance metrics and different ﬁne-   tuned LLMs . Using cosine or Jenson - Shannon   divergence as the distance metric on the proba-   bility distribution over proverbs predicted by the   RoBERTa model performs best on this task . How-   ever , the overall performance of models are still low   and can be beneﬁted by devising suitable training   methods for this task.3996We perform an additional experiment on ﬁnd-   ing similar narratives without performing proverb   prediction as an intermediate step . We use a pre-   trained Sentence - BERT model to obtain represen-   tations of each narrative . For a given input narra-   tive , we calculate the cosine distance between the   Sentence - BERT representation of the input narra-   tive and all other narratives in the test set . We pre-   dict the narrative having minimum cosine distance   to the input narrative as the most similar . Using   this approach we ﬁnd the accuracy of identifying   similar narratives as 6.6 % , which is lower than   most values reported in Table 13 . This low value   highlights the diversity between narratives and the   challenge in ﬁnding analogies between narratives .   6 Conclusion and Future Work   We introduce ePiC , a high - quality crowdsourced   dataset of narratives paired with proverbs , and   a suite of challenging tasks associated with this   dataset . We show that these provide a challeng-   ing testbed for evaluating abstract reasoning and   analogical abilities of LLMs . Future work can ex-   plore more sophisticated mechanisms to use align-   ment annotations in improving the performance   for proverb prediction and model interpretabil-   ity . Additionally , researchers can explore condi-   tional narrative generation through more informa-   tive prompts than using keywords . ePiC can also   be extended in the future by incorporating more   proverbs and adding more layers of complexity like   sarcasm or adversarially creating harder narratives .   Most of all , the development of similarly challeng-   ing resources and tasks can enable the possibility   of socially grounded NLP systems .   Ethics and Broader Impact   In § 4 , we note that our dataset shows considerable   differences in the distribution of gender of entities   ( 61 % male vs 39 % female ) , whereas in the realworld we expect the ratios to be about equally bal-   anced . Systems that do n’t account for this bias   might end up performing better for narratives with   male entities than with females . However , we note   that narratives with male and female entities show   no differences in overall length or the average num-   ber of mentions to those entities .   The proverbs used in our dataset were collected   from free public resources without violating in-   tellectual property rights . We do not collect any   personal information from the turkers who partic-   ipated in our crowdsourced tasks . We release our   dataset publicly without mentioning any personal   details of turkers available automatically in AMT   ( such as turker IDs ) . The turkers were compensated   fairly and the payment per task is equivalent to an   hourly compensation that is greater than minimum   wage ( based on the median time taken by turkers ) .   For all the crowdsourcing tasks in this work , we   limited the locale of eligible turkers to the USA ,   Canada , and the UK . Further , to ensure good - faith   turkers , we required that the approval rate of the   turkers be above 97 % .   Our screening process has selection biases that   likely over - samples narrative - writers from demo-   graphics that are over - represented on AMT ( eth-   nically white , college - educated , lower - to - medium   income , and young ) ( Hitlin , 2016 ) , and this is likely   to have affected the topics and type of language us-   age in the collected narratives .   Finally , our investigation here has focused on tra-   ditional English proverbs , even while proverbs are   universal in human languages and cultures ( Pen-   ﬁeld and Duru , 1988 ) . This poses a real risk of   the development of AI models that better under-   stand and employ speciﬁc types of ﬁgurative lan-   guage than others . Such systems are likely to be   less user - friendly to users that do n’t belong to spe-   ciﬁc social - cultural backgrounds . To mitigate these   risks , but also since proverbs are universal reposi-   tories of culture - speciﬁc knowledge , future work   should extend our effort to more equitably repre-   sent the variety and diversity of human thought   and cultural experiences . Our investigation here ,   unfortunately , does not adequately do this . As the   proverb goes , the road to hell is paved with good   intentions .   References3997399839994000Appendix   A Additional dataset analysis   Additional details on sentiment analysis : An ex-   ample of proverb for which the narratives were   close in sentiment scores to the proverb is ‘ a thing   of beauty is a joy forever ’ while for ‘ there ’s no fool   like an old fool ’ the sentiment polarity of narra-   tives was on average opposite to that of the proverb .   We note that there are indeed a small number of   proverbs for which all or most narratives leaning   towards a particular sentiment polarity . Quantita-   tively , for 23 proverbs , either 9 or all 10 of the   narratives have positive V ADER sentiment score .   These include : ‘ Nothing succeeds like success ’ ,   ‘ Christmas comes but once a year ’ and ‘ Genius is   one percent inspiration , ninety - nine percent perspi-   ration ’ . There are 6 proverbs for which either 9   or all 10 narratives have a negative V ADER senti-   ment score . These include : ‘ The wages of sin is   death ’ , ‘ Fish always stink from the head down ’ and   ‘ Do n’t wash your dirty linen in public ’ . However ,   as seen in Figure 4 , the vast majority of proverbs in   the dataset are represented by narratives with both   positive and negative sentiment polarities .   Gender distribution of entities : Using an off - the-   shelf neural coreference pipeline , we ﬁnd that 61 %   of the mentions in the narratives are male , while   39 % are female . Around 48 % of the narratives   have predominantly male mentions , 26 % of the nar-   ratives have predominantly female mentions and   the rest have equal number of male and female   mentions . The average number of words in pre-   dominantly male and female mention containing   narratives was comparable ( 65 words ) .   Language complexity : We use the Fleisch read-   ing easeto calculate language complexity of nar-   ratives in our dataset . The reading scores vary from   112.1 ( equivalent to 3rd grade reading levels ) to   -41.5 ( signiﬁcantly above college graduate reading   levels ) with an average score for the narratives in   our dataset as 66.5 ( equivalent to 8th/9th grade   reading levels ) , showing a considerable spread in   the complexity of language in our dataset .   Hate speech : Using an off - the - shelf hate speech   classiﬁer ( Davidson et al . , 2017 ) , we found no in-   stances of hate or toxic speech in the dataset . B Human evaluation on MCQ task   We formulated a MCQ task for proverb prediction   to gauge human performance . The MCQ task has 5   options – correct proverb and 4 distractor proverbs .   The distractor proverbs were chosen using the ﬁne-   tuned RoBERTa model on the proverb prediction   task . We choose the distractor proverbs from a   mix of proverbs with the highest prediction prob-   abilities , and proverbs that are assigned the most   similar probabilities to the correct answer from the   RoBERTa model . We performed this study using   the Amazon Mechanical Turk platform . We ob-   served that this task is not that simple even for hu-   mans and requires a certain level of proﬁciency in   English language or in proverbs speciﬁcally . The   task is more challenging since the options other   than the correct choice in the MCQ task were cho-   sen by picking the most confusing options deemed   by the RoBERTa ( Liu et al . , 2019a ) model . How-   ever , we ﬁnd that these wrong choices are confus-   ing for humans too . This is because superﬁcially   these wrong choices also seem quite related to the   narrative and it requires good reasoning skills to   identify the correct narrative . The other situation   where the turkers failed was when the options con-   tained multiple proverbs which are quite close in   meaning . For example , when the options contained   both ‘ there ’s no accounting for tastes ’ and ‘ Beauty   is in the eye of the beholder ’ the turkers often chose   the former when the annotated proverb was the lat-   ter . Table 14 shows examples of narratives along   with the choices of proverbs where turkers failed   to identify the correct proverb .   C Semantically similar proverbs   Our chosen set of 250 proverbs in ePiC includes in-   stances of proverbs that are semantically very simi-   lar , or even paraphrases ( e.g. , ‘ never judge a book   by its cover ’ and ‘ appearances can be deceptive ’ ) .   This can be problematic since the presence of se-   mantically similar proverbs as different options in   MCQ ( and as different classes in proverb classiﬁca-   tion task ) can confuse both humans and automated   models . To estimate the extent of this phenomenon ,   we perform an analysis of human errors on the   aforementioned MCQ task . Out of 64 errors we   ﬁnd that for 20 cases , the chosen proverb was com-   pletely unrelated to the actual answer . For 29 out of   the remaining 44 cases , the chosen proverb seems   related to the narrative at ﬁrst glance , but is not   aligned and thus not the best ﬁt . For the remaining4001Narrative 1 :   She had been so happy when he had asked her to marry him but three years on ,   it seemed that he had so many excuses for not setting a date that she thought that   it was never going to happen . Her happiness eventually turned to despair and   she considered breaking the engagement .   Choice A : You win some , you lose some   Choice B : Jam tomorrow and jam yesterday , but never jam today ( Correct )   Choice C : Cowards may die many times before their death   Choice D : The course of true love never did run smooth ( Marked )   Choice E : Nothing is certain but death and taxes   Narrative 2 :   She did n’t want to embarrass her friend when she asked her , " It ’s beautiful ,   is n’t it ? " She looked at her friend ’s new car and nodded her head in agreement .   It was purple , the worst car colour she had ever seen , but she faked a smile   and congratulated her .   Choice A : Imitation is the sincerest form of ﬂattery   Choice B : From the sublime to the ridiculous is only one step   Choice C : There ’s no accounting for tastes ( Marked )   Choice D : Beauty is in the eye of the beholder ( Correct )   Choice E : All publicity is good publicity   15 cases ( 23 % of human errors ) , the chosen proverb   would have been equally appropriate for the nar-   rative . Further , in 7 out these 15 cases ( 11 % of   human errors ) , the chosen proverb is also semanti-   cally similar to the annotated proverb . Future work   can consider handling semantic similarity between   proverbs explicitly and devise suitable evaluation   metrics .   D Generated Narratives   We show some examples of the narratives gener-   ated by the BART ( Lewis et al . , 2020 ) and T5 ( Raf-   fel et al . , 2020 ) models for the narrative generation   task in Table 15 . We see that even though the mod-   els try to mention all the keywords but they are not   able to generate a coherent narrative .   E Evaluation of alignment prediction for   jointly trained models   In § 5.1 , we present models to predict proverb and   alignment jointly . During evaluation , we ﬁrst per-   form proverb prediction and then perform align-   ment prediction if the correct proverb was pre - dicted . If an incorrect proverb is predicted , the   span precision , span recall , and span F1 are consid-   ered as zero .   If we isolate the layers responsible for align-   ment prediction from the joint model and evaluate   it solely on the task of alignment prediction , the   span F1 scores for BERT ( base ) and RoBERTa   ( base ) are 0.078 and 0.074 respectively .   F Training details   In this section we discuss about the model param-   eters , hyper - parameter settings and hardware and   software speciﬁcations of training .   F.1 Retrieval models ’ implementation details   As discussed in § 4.1 , we formulate a retrieval task   to explore if we can retrieve the correct proverb cor-   responding to a narrative only by using similarity   in their distributional representations . We deﬁne   similarity between a proverb and a narrative by us-   ing cosine similarity between the embeddings of   the proverb and the narrative . We use ( 1 ) word2vec   embeddings ( Mikolov et al . , 2013 ) ( 2 ) contextual   embeddings from LLMs to represent the proverb4002Proverb 1 :   Absence makes the heart grow fonder   Gold narrative :   Although they never got along together much , she somehow always looked forward   to the holidays each year .   Keywords :   got , looked , the holidays   Generated narrative by BART :   Polly got back into the swing of things , she looked forward to the holidays each year .   Generated narrative by T5 :   Although he got through the holidays each year , he looked over everything   again to his friends , family and friends .   Proverb 2 :   Those who do not learn from history are doomed to repeat it   Gold narrative :   Bob cheated on his girlfriend in college . He was heartbroken when she broke up with   him but moved on . A few years later he married and cheated on his wife . She left him ,   just as his girlfriend did in college .   Keywords :   broke , did , a few years later , cheated , moved , left , married , heartbroken , Bob   Generated narrative by BART :   Bob cheated on his girlfriend with her best friend . He was heartbroken when she broke   up with him and moved out of his apartment . A few years later , he got married to the   girl he cheated on with , and left his apartment with her . He never did again .   Generated narrative by T5 :   Bob was heartbroken . a few years later , he broke into town . He moved out in his life and   moved out .. a few years later , he married a girl . A few years later , she cheated on him but ,   unfortunately , she left him because she did not do well .   Proverb 3 :   A trouble shared is a trouble halved   Gold narrative :   Mike knows that this new problem is going to need some serious help . He tells his friend   Josh about the new problem , and Josh agrees to help him solve it .   Keywords :   help , solve , knows , going , Mike , Josh , need , tells , agrees   Generated narrative by BART :   Mike knows a thing or two about what is going on in his life that he need help with   anything . he tells his friend josh about it and Josh agrees to help solve the problem .   Generated narrative by T5 :   Mike , Josh , knows that he is going to need help to solve the problem . He tells me that   he agrees but he will not help me solve the problem .   and narrative . We obtain the embeddings for a con-   textc(where ccan be a proverb or a narrative )   as :   •Word2vec : average of word embeddings for to-   kens in c.   •BERT ( Devlin et al . , 2019)/RoBERTa ( Liu et al . ,2019a ) : [ CLS]token embedding on passing c   through BERT / RoBERTa .   •DistilBERT ( Sanh et al . , 2019)/AlBERT ( Lan   et al . , 2020 ) : [ CLS]token embedding on pass-   ingcthrough DistilBERT / AlBERT   •SentenceBERT ( Reimers and Gurevych , 2019 ) : 4003normalized SentenceBERT embeddings obtained   by using ‘ all - mpnet - base - v2’model on c.   •T5 ( Raffel et al . , 2020)/GPT-2 ( Radford et al . ,   2019 ) Encoder : sum of embeddings of tokens in   cafter passing through the encoder   F.2 Proverb prediction models ’   implementation details   We use the same LLM models ( and implementa-   tions ) used for the retrieval setup discussed in § 4.1   and § F.1 .   F.3 Obtaining keywords for narrative   generation   We consider the named entities and verbs present   in a narrative ( extracted using spacy ( Honnibal   et al . ) ) as keywords for generating that narrative .   F.4 Model parameters   Our proverb prediction models do not introduce   any additional parameters over the existing param-   eters in the large language models . For joint predic-   tion of proverb and span , we introduce new fully   connected layers over the language models , thus   introducing 0.6 M additional parameters .   F.5 Hyper - parameter settings   For all the transformer based models we use the im-   plementation of HuggingFace library ( Wolf et al . ,   2019 ) . All the model based hyper - parameters are   thus kept default to the settings in the Hugging-   Face library . We use the publicly available check-   points to initialise the pre - trained models ( for exam-   ple “ bert - base - uncased " checkpoint for initialising   BERT(Devlin et al . , 2019 ) ) . For the proverb predic-   tion models we did not truncate any tokens from   the proverb and considered the maximum length   of the narrative sequence to be 256 tokens . For   the alignment prediction and joint training mod-   els , we considered the maximum length of the   narrative sequence as 230 tokens . We used the   AdamW ( Loshchilov and Hutter , 2018 ) optimizer   commonly used to train these models except for T5   ( Raffel et al . , 2020 ) . We used AdaFactor(Shazeer   and Stern , 2018 ) to train our T5 based proverb   prediction model . We kept the learning rate as   0.00002 for training . Batch sizes was kept as 16   except for T5 , for which we reduced the batch size   to 4 . The random seed for all experiments was 42 .   The proverb prediction models were trained for 25epochs . The BART narrative generation model was   trained for 15 epochs and loss converged after that .   T5 took longer and was trained for 25 epochs .   F.6 Software and hardware speciﬁcations   All the models are coded using Pytorch 1.4.0   ( Paszke et al . , 2019 ) and related libraries like   numpy ( Oliphant , 2006 ) , scipy ( Virtanen et al . ,   2020 ) etc . We run all experiments on GeForce   RTX 2080 GPU of size 12 GB . The system has 256   GB RAM and 40 CPU cores . The proverb predic-   tion models typically take 2 - 5 mins for one epoch .   For the joint proverb and span prediction models it   took roughly 10 mins for one epoch . For narrative   generation models it takes 10 mins for BART and   around 18 mins for T5 to complete one epoch of   training.4004