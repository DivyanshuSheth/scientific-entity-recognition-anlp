  Niccolò Campolungo   Sapienza University of RomeFederico Martelli   Sapienza University of Rome   Francesco Saina   SSML Carlo Bo , RomeRoberto Navigli   Sapienza University of Rome   Abstract   Lexical ambiguity poses one of the greatest   challenges in the ﬁeld of Machine Translation .   Over the last few decades , multiple efforts   have been undertaken to investigate incorrect   translations caused by the polysemous nature   of words . Within this body of research , some   studies have posited that models pick up se-   mantic biases existing in the training data , thus   producing translation errors . In this paper , we   present DBMT , the ﬁrst entirely manually-   curated evaluation benchmark which enables   an extensive study of semantic biases in   Machine Translation of nominal and verbal   words in ﬁve different language combinations ,   namely , English and one or other of the fol-   lowing languages : Chinese , German , Italian ,   Russian and Spanish . Furthermore , we test   state - of - the - art Machine Translation systems ,   both commercial and non - commercial ones ,   against our new test bed and provide a thor-   ough statistical and linguistic analysis of the   results . We release DBMT athttps://   nlp.uniroma1.it/dibimt as a closed   benchmark with a public leaderboard .   1 Introduction   The polysemous nature of words poses a long-   standing challenge in a wide range of Natural Lan-   guage Processing ( NLP ) tasks such as Word Sense   Disambiguation ( Navigli , 2009 ; Bevilacqua et al . ,   2021 ) ( WSD ) , Information Retrieval ( Krovetz and   Croft , 1992 ) ( IR ) and Machine Translation ( Emelin   et al . , 2020 ) ( MT ) .   In MT , some research works have addressed   the ability of systems to disambiguate polysemous   words . For instance , given the sentence He poured   ashot of whiskey , the polysemous target word shot   unequivocally means a small quantity and therefore   a possible translation into Italian could be : Versò   ungoccio di whiskey . However , some MT systems   propose the following translation : Versò uno sparodi whiskey in which the noun sparo means gun-   shot . This is one of many examples that seem to   encourage a deeper performance analysis in sce-   narios in which MT systems are required to deal   with polysemous words and , speciﬁcally , with in-   frequent meanings of polysemous words . Although   state - of - the - art MT systems , both commercial and   non - commercial ones , achieve impressive BLEU   scores on standard benchmarks , in our work we   demonstrate that they still present signiﬁcant limi-   tations when dealing with infrequent word senses ,   which standard metrics fail to recognize .   In the last few decades , attempts have been made   to investigate the aforementioned phenomena . In   fact , recent studies have observed a direct correla-   tion between semantic biases in the training data   and semantic errors in translation . However , their   ﬁndings are limited by the following shortcomings :   i ) they are not based on entirely manually - curated   benchmarks ; ii ) they rely heavily on automatically-   generated resources to determine the correctness   of a translation ; and iii ) they do not cover multiple   language combinations .   In this work , we address the aforementioned   drawbacks and present DBMT , to the best of our   knowledge the ﬁrst fully manually - curated evalua-   tion benchmark aimed at investigating the impact   of semantic biases in MT in ﬁve language com-   binations , covering both nouns and verbs . This   benchmark allows the community not only to bet-   ter explore the described phenomena , but also to   devise innovative MT systems which better deal   with lexical ambiguity . Speciﬁcally , the contribu-   tions of the present work are threefold :   •We present DBMT , a novel gold - quality test   bed for semantic biases in MT that goes be-   yond a simple accuracy score , covering ﬁve   language combinations , namely English and   one or other of the following languages : Chi-   nese , German , Italian , Russian and Spanish ;   •We deﬁne four novel metrics that better clarify   the semantic biases within MT models ;   • We provide a thorough statistical and linguis-   tic analysis in which we compare 7 state - of-   the - art MT systems , including both commer-   cial and non - commercial ones , against our   new benchmark . Furthermore , we extensively   discuss the results .   To enable further research , we release DBMT   as a closed benchmark with a public leaderboard at   https://nlp.uniroma1.it/dibimt .   2 Related Work   Over the course of the last few decades , several   approaches to the evaluation of the lexical choice in   MT have been proposed . To this end , cross - lingual   benchmarks were created in which systems were   required to provide the translation or a substitute for   a given target word in context in a target language   ( Vickrey et al . , 2005 ; Mihalcea et al . , 2010 ; Lefever   and Hoste , 2013 ) .   More recently , Gonzales et al . ( 2017 ) put for-   ward ContraWSD , a dataset which includes 7,200   instances of lexical ambiguity for German →En-   glish , and 6,700 for German →French . This   dataset pairs every reference translation with a   set of contrastive examples which contain incor-   rect translations of a polysemous target word . For   each instance , the answer provided by systems is   considered correct if the reference translation is   scored higher . Based on a denoised version of   the ContraWSD dataset and focusing on the lan-   guage combination German →English , Gonzales   et al . ( 2018 ) present the Word Sense Disambigua-   tion Test Suite which , unlike ContraWSD , eval-   uates MT output directly rather than by scoring   translations . The suite consists of a collection of   3,249 sentence pairs in which the German sourcesentences contain one ambiguous target word . As   target words , the authors considered only words in   German whose translation into English does not   cover multiple senses , thus making the evaluation   more straightforward . Despite their effectiveness ,   such benchmarks do not allow systems to be tested   in multiple language combinations , and only cover   a very limited number of words and senses . To   address these limitations , Raganato et al . ( 2019 )   proposed MuCoW , an automatically - created test   suite covering 16 language pairs , with more than   200,000 sentence pairs derived from word - aligned   parallel corpora .   Other research studies investigated the disam-   biguation capabilities of MT systems by exploring   their internal representations ( Marvin and Koehn ,   2018 ; Michel et al . , 2019 ) , or improving them via   context - aware word embeddings ( Liu et al . , 2018 ) .   More recently , Emelin et al . ( 2020 ) introduced a   statistical method for the identiﬁcation of disam-   biguation errors in neural MT ( NMT ) and demon-   strated that models capture data biases within the   training corpora , which leads these models to pro-   duce incorrect translations . Although the authors   expected their approach to be transferable to other   language combinations , they only focused on Ger-   man→English .   Based on the ﬁndings and open research ques-   tions raised in the aforementioned works , the   present paper aims at investigating not only the   presence , but also , most importantly , the nature   and properties of semantic biases in MT in mul-   tiple language combinations , via a novel entirely   manually - curated benchmark called DBMT and   a thorough performance analysis .   3 Building DBMT   The DBMT benchmark focuses on detecting   Word Sense Disambiguation biases in NMT , i.e. ,   biases of certain words towards some of their more   frequent meanings . The creation of such a dataset   requires i ) a set of unambiguous and grammatically-   correct sentences containing a polysemous target   word ; ii ) a set of correct and incorrect translations   of each target word into the languages to be cov-   ered . Figure 1depicts an example of a dataset item .   3.1 Preliminaries   BabelNet Similarly to previous studies , we rely   on BabelNet(Navigli et al . , 2021 ) , a large multilin - gual encyclopedic dictionary whose nodes are con-   cepts represented by synsets , i.e. , sets of synonyms ,   containing lexicalizations in multiple languages   and coming from various heterogeneous resources ,   including , inter alia , WordNet ( Miller et al . , 1990 )   and Wiktionary . Let us deﬁne Bas an abstraction   used to query the subset of synsets in BabelNet that   contain at least one sensefrom WordNet and one   or more senses in languages other than English ,   while only considering senses coming from high-   quality sources , i.e. , language - speciﬁc wordnets .   Formal Notation Given an arbitrary synset σ ,   we deﬁne Λ(σ)as the set of lexicalizations of   σin language Lcontained within B. As an ex-   ample , let us consider the synset ˜σcorresponding   to the drink meaning of the word shot.˜σcon-   tains lexicalizations in different languages , includ-   ing : Shot , shot , nip , chupito , trago ,   bicchierinoandgoccio . Hence , Λ(˜σ ) =   { shot , nip } , whileΛ(˜σ ) = { chupito , trago } .   Furthermore , let λrepresent a ( lemma , part   of speech ) pair , where Pis the part of speech .   We denote Ω(λ ) = { σ, ... ,σ}as the set of   synsets which contain λas a lexicalization in lan-   guageLaccording to B. Additionally , we deﬁne   δ(λ ) = |Ω(λ)|as the polysemy degree , i.e. ,   the number of senses , of λin language L. For ex-   ample , given λ = shot , Ω(λ)would   be the set of synsets associated with the nominal   term shot ( e.g. , the act of ﬁring , a photograph and   a drink , among others ) .   3.2 Sentence Selection Process   In this section , we detail the creation process of our   dataset , i.e. , the selection of our sentences as well   as the construction and ﬁltering of our items .   Item Structure and Notation Before we pro-   ceed , let us formally state how each item in the   dataset is structured : given a source sentence   s= [ w, ... ,w]as a sequence of words , and   given a target wordwinstagged with some   synsetσ , we consider X= ( s , w , σ)as an initial   item of the dataset , i.e. , an instance composed ofan English sentence s , a target word wand its as-   sociated synset σ ; this instance can be annotated   for candidate translations of win some language   L. We also denote λas the ( lemma , POS ) pair of   w.   3.2.1 Starting Sentence Pool   We collect our initial items from two main sources :   WordNet and Wiktionary . Speciﬁcally , we use the   examples from WordNet Tagged Glosses ( Langone   et al . , 2004 ) , where each sentence ’s target word   was manually associated with its synset , thereby   readily providing the ﬁrst batch of initial items .   As for Wiktionary , instead , we start by obtaining   every usage example sand its associated deﬁni-   tiond(ﬁltering out archaic usages and slang ) , then ,   we automatically extract the target words from the   corresponding example . Now , the only step that   remains in order to construct an initial item is to   associate a synset σwith the word wused in the   example s. We perform this association in two   phases : ﬁrst , we try to map the deﬁnition drelated   to the example sto a BabelNet synset by relying   on the automatic mappings available in BabelNet 5   between WordNet and Wiktionary , discarding ex-   amples for which this association can not be found ;   second , we manually validate and correct these suc-   cessful associations to ensure that our initial items   are of high quality .   3.2.2 Sentence Filtering   We apply a ﬁltering step to the original sentences in   order to select examples that are likely to be more   challenging for the models to translate : i ) we dis-   card every initial item Xfor which δ(λ)<3 ,   i.e. , we retain only sentences whose associated   ( lemma , POS ) pair has a polysemy degree of at   least 3 in B ; ii ) we retain at most only one sen-   tence per sense per source ; iii ) differently from   previous works , which impose a strict requirement   on synsets that are monosemous in the target lan-   guage , we retain sentences satisfying the following   requirement . Let us consider the nominal senses   of the word bank : among them , one represents a   speciﬁc aviation maneuver . In Italian , this synsetincludes one lexicalization , avvitamento ; although   this is not monosemous in Italian ( e.g. , avvitamento   might also refer to a screw thread ) , neither of the   other possible senses of avvitamento hasbank as an   English lexicalization , which , for Italian , satisﬁes   our third condition . If the same holds true for all   languages , the synset passes the test and thus the   sentence is retained .   3.3 Annotating the Dataset   Once the set of initial items is ready , we can pro-   ceed with the annotation phase , which will produce   ourannotated items .   Speciﬁcally , given a language Land an initial   itemX= ( s , w , σ ) , we associate a set of good   ( G ) and bad ( B ) translation candidates with X ,   which represent words that , respectively , we do ,   and do not , expect to see in a translation of sentence   sin language L. Finally , we refer to Xas an   annotated item , i.e. , the tuple ( s , w , σ , G , B ) .   3.3.1 Pre - annotation Item Creation   Before moving forward with the annotation phase ,   we pre - populate the sets of good ( G ) and bad ( B )   lexicalizations for a given initial item Xin lan-   guageLextracting them from B. Formally , we   assignG= Λ(σ ) , i.e. , the set of lemmas in lan-   guageLof the BabelNet synset associated with σ ;   furthermore , we set B=/uniontextΛ(ˆσ ) ,   i.e. , the set of all lemmas in language Lof BabelNet   synsets associated with any ˆσexcluding σ . With   this step , we produce an automatically populated   version of our annotated items .   3.3.2 Annotation Guidelines   We instruct annotators to update the set of good   ( G ) and bad ( B ) lexicalizations of w∈ssuch   that each lexicalization contained in the respective   set can be considered a good or a bad translation   equivalent for the target word in the provided sen-   tential context .   We also instruct annotators to discard sentences   in which i ) the target word wis an idiomatic ex-   pression or a proper noun , and ii ) the semantic   context is not sufﬁcient to properly disambiguate   w.   Given the expertise required to carry out this   task , we rely on three highly qualiﬁed translators :   one for Italian , German and Russian ; one for Span-   ish and one for Chinese . Our annotators satisfy theAll Nouns Verbs   # items 597 314 283   # lemmas 305 186 147   # synsets 471 254 217   % OG % RG % SL   DE 50.9 25.0 59.7   ES 49.6 19.5 47.7   IT 49.1 38.2 67.1   RU 67.4 57.3 54.4   ZH 55.2 69.0 46.3   Mean 54.4 41.8 55.0   following requirements : they are native speakers   or hold C2 - level certiﬁcations and work as pro-   fessional translators in the given language com-   binations . The full instructions provided to the   annotators can be found in Appendix C.   3.3.3 Resulting Dataset   Our annotators analyzed around 800 sentences ,   discarding 200 of them , ﬁnally obtaining approxi-   mately 600 annotated items in 5 languages . Due to   a coverage issue of the Russian language in Babel-   Net , we retain only sentences tagged with nominal   or verbal synsets . Dataset statistics are reported in   Table 1 .   As expected , we note that the lexicalizations   found inBhave been substantially reﬁned by our   annotators in all languages , as reported in Table 2 .   Indeed , across languages , on average , 54 % of the   good lexicalizations have been added by our an-   notators , while 42 % of the pre - existing lexicaliza-   tions have been removed . More importantly , given   a language and two sentences containing words   referring to the same synset , on average only in   55 % of cases do they also share those words ’ goodlexicalizations , conﬁrming that the assumption that   all synonyms of a word are valid replacements can   lead to incorrect results .   These statistics lead us to a straightforward , but   important , conclusion : only in a limited number of   cases is a lexicalization belonging to a given synset   to be considered as a suitable translation equivalent   for the provided target word and its context . Exam-   ined jointly , these metrics suggest that relying on   synset lexicalizations from BabelNet alone is prone   to producing errors , either due to BabelNet ’s intrin-   sic noise , or due to the lack of different granularity   of synsets and contextualized words .   Sentences ’ Properties Description As we   stated in Section 3.2.1 , the sentences we annotate   are all usage examples of speciﬁc concepts   obtained from WordNet or Wiktionary . Such   examples are typically short main clauses with   no subordinates , featuring on average 9 words   ( around 50 characters per sentence ) . All selected   sentences include a semantic context which allows   the meaning of the target word to be properly   identiﬁed .   3.4 Analysis Procedure   DBMT ’s analysis procedure is fairly simple :   given an annotated item X= ( s , w , σ , G , B )   and a translation model M , we compute t=   M(s ) , i.e. , the translation of sin language Lac-   cording to M. Then , we use Stanza ( Qi et al . ,   2020 ) to perform tokenization , part - of - speech tag-   ging and lemmatization of tand , ﬁnally , we check   if there is any matchbetween the lemmas of the   translated sentence and those contained in Gor   B. In case there is no match , we mark the transla-   tion as aMISS ; otherwise , we mark it as GOOD or   BAD depending on which set matched the lemma .   This produces an analyzed item , which for sim-   plicity we denote as X= ( X , t , R , ω ) ,   whereRis one ofGOOD , BAD orMISS andω   represents the matched lemma in case there was a   match ( GOOD orBAD),ǫotherwise .   4 Results and Discussion   We now : i ) use DBMT to carry out an evalua-   tion of 7 different machine translation systems ; ii )   report the obtained results , including a thorough   statistical and linguistic evaluation ; iii ) extensively   discuss our ﬁndings , providing multiple measuresof semantic bias ; and iv ) offer some insights into   the causes of such biases . In Appendix Dwe in-   clude a model - speciﬁc breakdown of the various   scores and metrics reported throughout this section .   4.1 Comparison Systems   We test a wide range of models , both commercial   and non - commercial ones , and report their perfor-   mances on DBMT ’s evaluation metrics :   •DeepL Translator , a state - of - the - art com-   mercial NMT system .   •Google Translate , arguably the most popu-   lar commercial NMT system .   •OPUS ( Tiedemann and Thottingal , 2020 ) , the   smallest state - of - the - art NMT model available   to date , a base Transformer ( each model has   approximately 74 M parameters ) trained on a   single language pair on large amounts of data .   •MBart50 ( Tang et al . , 2021 ) , multilingual   BART ﬁne - tuned on the translation task for   50 languages ( 610 M parameters ) . We refer to   MBart50 as the English - to - many model , and   toMBart50 as the many - to - many model .   •M2M100 ( Fan et al . , 2021 ) , a multilin-   gual model able to translate from / to 100   languages . We test both versions of the   model , the 418 M parameter one ( which we   dubM2M100 ) and the 1.2B parameter one   ( dubbed M2M100 ) .   4.2 Discussion of MISS   Figure 2reports general results of the analysis per   ( model , language ) pair . Given the high percentage   ofanalyzed items classiﬁed as MISS , we asked our   annotators to perform an inspection on a random   sample of 70 items per language in order to unearth   the reasons , with varying results . We identiﬁed   multiple causes , namely : i ) word omission in the   translation ( around 19 % of items , mostly in Chi-   nese and Italian ) ; ii ) issues with Stanza ’s tokeniza-   tion ( around 11 % , mostly Chinese and Russian )   and lemmatization ( around 12 % , mostly Italian   and German ) ; iii ) words translated as themselves   ( approximately 5 % , often in multilingual neural   models ) ; iv ) translations which have nothing to   do with the source text(around 23 % ) ; and v )   missing terms from either B(around 18 % ) or G   ( around 11 % ) . We intend to thoroughly investigate   and tackle these issues and translation phenomena   as future work .   4.3 General Results   Table 3reports accuracy for non- MISS analyzed   items ( i.e. , ) . With the sole exception   of DeepL , which greatly outperforms every other   competitor , models achieve extremely low scores ,   in the range of 20%-33 % . Surprisingly , Google   Translate performs worst across languages .   4.4 Analyzing the Semantic Biases   In addition to accuracy , DBMT analyzes the se-   mantic biases of a translation model via four novel   metrics , which we deﬁne in detail in what follows .   Sense Frequency Index Inﬂuence ( SFII ) We   study the sensitivity of models to disambiguat-   ing senses with respect to their frequency . To do   this , we deﬁne µ(σ)as the index of synset σin   Ω(λ)ordered according to WordNet ’s sense   frequency , as computed from SemCor . That is , in - dexkmeans that synset σis thek - th most frequent   meaning for λ .   In Figure 3(a ) , we plot the number and percent-   age of errors made on average by the models , group-   ing items by µ(σ ) , whereXis a non - MISS   analyzed item . As expected , the less frequent a   meaning for a given word is , the harder it is for the   model to correctly disambiguate it .   Finally , given a ( model , language ) pair , we de-   ﬁne the Sense Frequency Index Inﬂuence ( SFII ) as   the average percentage of errors , for each group ,   that we detected . Values are reported in Table 4 .   Interestingly , DeepL proves once again to be the   best , obtaining a score of 51 % , far below the aver-   age 80 % achieved by the other models , with most   non - commercial models performing ≤80 % .   Sense Polysemy Degree Importance ( SPDI )   Similarly to SFII , we also study the extent to which   the polysemy degree , i.e. , how many senses a given   word can have , impacts the models ’ disambigua-   tion capabilities . This experiment mirrors SFII , but   groups items by their lemma ’s polysemy degree   δ(λ)instead of µ. Figure 3(b ) reports the re-   sults on all items . Unsurprisingly , similarly to the   frequency index , we observe that higher polysemy   leads to more errors , conﬁrming that models still   struggle with very polysemous words . Similarly to   SFII , SPDI is deﬁned as the average percentage of   errors at varying polysemy degrees , and its values   are reported in Table 4 : once again , DeepL outper-   forms all other systems by a large margin , conﬁrm-   ing that it is the least biased across the board .   Most and More Frequent Senses To further cor-   roborate our ﬁndings about semantic biases , we   study how often models predict senses that are   more frequent than the target one . Given a BAD   analyzed item X , we denote ˆσas the synset as-   sociated with the wrongly translated lemma ω .   Then , we check the frequency of σandˆσwith   respect to λ : ifµ(ˆσ ) < µ(σ ) , then the sys-   tem ’s disambiguation steered towards a sense that   is more frequent than the target one , which weALL NOUN VERB   Accuracy 32.11 34.15 30.02   % MISS 38.03 29.36 47.57   MFS 57.86 60.13 52.60   MFS+ 88.68 87.57 88.74   SFII 76.98 69.16 76.90   SPDI 70.80 66.86 72.87   dub More Frequent Sense ( MFS+ ) ; additionally , if   µ(ˆσ ) = 1 , then the model disambiguated the   source word wto the Most Frequent Sense ( MFS )   of the associated lemma λ . The results of both   these analyses are reported in Table 5 .   We can observe a few interesting results : ﬁrst , on   average , almost 60 % of the time a mistake reﬂects   the Most Frequent Sense of the target word ( second-   last column ) ; second , almost 90 % of the errors con-   cern translations towards more frequent senses of   the target word ( last column ) . Importantly , these   results are consistent across systems , whether com-   mercial or not . Although it might seem straightfor-   ward , NMT models are still strongly biased towards   senses that are more likely to be encountered during   training ; while this could be related to the pattern-   matching nature of neural networks , it also depends   heavily on the training data the model was trained   upon , and this needs to be further investigated in   future research .   4.5 Are verbs harder than nouns ?   The existing literature in WSD points to the fact   that verbs are generally harder than nouns , mostly   due to their highly polysemous nature ( Barba et al . ,   2021b ) . We try to analyze whether MT models   are affected by the same phenomenon : in Table 6 ,   we report the average results obtained by running   DBMT on all its sentences ( column ALL ) and   the subset of sentences whose target word was ei-   ther a NOUN or a VERB . In general , we observe   an average drop of accuracy of 4 points , as well as   an astounding difference of 18 percentage points   inMISS handling , which we will investigate more   thoroughly in future work . Interestingly , MT mod-   els are much more inclined to translate nouns into   their most frequent sense ; we attribute this differ-   ence to the generally higher polysemy of verbs   compared to nouns , which increases the size of   the space of possible translations for a given verb ,   thus decreasing the chance that it gets translated   into the MFS . Aside from this , we draw the same   conclusion as that drawn by previous works in the   ﬁeld of WSD , with nouns being generally easier to   translate than verbs .   4.6 Is the encoder disambiguating ?   We try to assess to what extent , in a multilingual   encoder - decoder architecture , the encoder is deter-   mining the implicit disambiguation of the source   sentence before generating the translation . For in-   stance , we ask ourselves this question : given an   ambiguous word win the source sentence s , how   often does the model translate it into a lexicaliza-   tion representing the same sense , if prompted to   translatesinto different languages ? Intuitively , if   the encoder was the sole contributor to the implicit   disambiguation performed by the model , we would   expect to see the meaning to always be the same ,   regardless of the target language .   To measure this , we perform the following ex-   periment : given a model M , two languages L1   andL2and an initial item X , we take M’sana-   lyzed items XandXand check if transla-   tions inL1andL2have a synset in common , i.e. ,   |Ω(ω)∩Ω(ω)|>0 . The results of this   experiment are reported in Figure 4 .   We observe that , on average , this phenomenon   occurs around 70 % of the time . Hence , it is safe   to assume that , while the encoder certainly plays   an important role in the disambiguation of the in-   put sentence , the decoder is also contributing sig-   niﬁcantly . Another interesting observation is that   the alphabet of the target language does not seem   to have any inﬂuence , as language pairs involving   Russian display scores that are very similar to those   of the other three European languages . We attribute   lower scores in Chinese to coverage issues in Ba-   belNet , which would hinder a correct fulﬁllment of   the condition deﬁned for this experiment .   4.7 How challenging is DBMT ?   Given the low performances achieved by MT mod-   els , we test a WSD system on the English sentences   within DBMT , both to assess the toughness of   our system and to establish an additional baseline .   We use ESCHER(Barba et al . , 2021a ) , a state-   of - the - art model on English WSD . Interestingly ,   ESCHER achieves an overall accuracy score of   66.33 , almost 15 points lower than the results on   the standard WSD benchmark ( 80.7 on ALL , Ra-   ganato et al . , 2017 ) , therefore conﬁrming the chal-   lenging nature of DBMT . Furthermore , in order   to estimate the difference in disambiguation capa-   bility between NMT models and a dedicated WSD   system , we compute ESCHER ’s performances on   the set of English sentences of non- MISS analyzed   items for each ( model , language ) pair . We report   these results in Table 7 , whose accuracy scores can   be directly compared to those in Table 3 .   As expected , the average MT accuracy is sig-   niﬁcantly lower than ESCHER ’s , with the sole   exception of DeepL , which manages to surpass   it on German and Russian . These results clearly   demonstrate that current NMT models are still not   on par with dedicated WSD systems , and thus that   they might beneﬁt from the inclusion of such WSD   systems within the NMT ecosystem .   4.8 Is this a decoding issue ?   As a ﬁnal experiment , we assess whether the se-   mantic biases are caused by search errors ( i.e. , fail-   ures of the decoding algorithm ) , or model errors   ( i.e. , the models deemed their translations the best   possible ) . For each ( model M , language L ) pair ,   we sample a BAD translation ( t ) , pair it with   aGOOD translation ( t ) produced by another   model ( prioritizing DeepL ) , and ask annotators to   check their correctness and apply corrections where   needed , then compute the perplexities according   toMwith the corresponding English sentence , and   call them p andprespectively . We repeat   this sampling 50 times per ( M , L ) pair and check   how often p < p. Table 8shows that , on   average , this happens in 93 % of cases , thus con-   ﬁrming that most semantic biases are embedded   within models and are not caused by the decoding   strategy .   5 Conclusions   In this work , we presented DBMT , a novel bench-   mark for measuring and understanding semantic   biases in NMT , which goes beyond simple accu-   racy and provides novel metrics that summarize   how biased NMT models are . We tested DBMT   on 7 widely adopted NMT systems , extensively   discussing their performances and providing novel   insights into the possible causes and relations of   semantic biases within NMT models .   Furthermore , statistics of our annotations sug-   gest that , when dealing with translations , synsets ’   lexicalizations can not be used interchangeably , as   their choice depends heavily on the context .   In the future , we plan to improve DBMT by   introducing better heuristics to recognize and han-   dleMISS cases , especially covering the linguistic   phenomena we described ( see Section 4.2 ) ; we also   aim at widening language coverage and increasing   the number of sentences in the benchmark , conse-   quently improving word and sense coverage . To   enable further research , we release DBMT as a   closed benchmark with a public leaderboard at :   https://nlp.uniroma1.it/dibimt .Acknowledgements   The authors gratefully acknowl-   edge the support of the ERC   Consolidator Grant MOUSSE No .   726487 and the ELEXIS project   No . 731015 under the European   Union ’s Horizon 2020 research and   innovation programme , and the   PerLIR project ( Personal Linguis-   tic resources in Information Re-   trieval ) funded by the MIUR Pro-   getti di ricerca di Rilevante Inter-   esse Nazionale programme ( PRIN   2017 ) .   This work was also partially supported by the   MIUR under the grant “ Dipartimenti di eccellenza   2018 - 2022 " of the Department of Computer Sci-   ence of Sapienza University .   ReferencesA Analysis Procedure Details   Our analysis procedure , which we described in   Section 3.4 , involves steps that go beyond simple   lemma matching . For instance , in case of multi-   word expressions , we allowed annotators to specify   a wildcard , i.e. , any number of tokens ( including   zero ) were allowed to expand and still trigger a   match . Additionally , since Stanza has multi - word   expansion tokenization for some of the languages   in our list , when available , we try to perform match-   ing on both the list of words ( alongside the list of   tokens ) in the translated sentence . Finally , in case   no match is produced by the aforementioned steps ,   we apply a surface - level string matching heuris-   tic which , especially in Chinese , helps us increase   coverage .   B Neural Models Implementation   We use HuggingFace ’s Transformers library ( Wolf   et al . , 2020 ) for all neural models . As per stan-   dard practice , we generate translations using beam   search as decoding algorithm with beam size 5 .   C Instructions for Dataset Annotation   In this work , we investigate semantic biases in Ma-   chine Translation across languages . You are pro-   vided with a spreadsheet containing 300 instances ,   each including the following information : a lemma ,   its part of speech , a deﬁnition and some good and   bad translation candidates derived from BabelNet .   Your task is to manually verify the correctness of   the good candidates and add new good candidates   if deemed necessary . Furthermore , you are asked   to verify that all bad candidates are wrong .   From a translation perspective , a good candidate   is a word which correctly translates the English   target word in the given context . Instead , a bad can-   didate is a wrong translation of the English target   word in the given context .   Please adopt the following guidelines while an-   notating :   • Do not annotate idioms .   •Do not annotate instances in which the seman-   tic context does not allow us to unequivocally   determine the meaning of the target word .   •Do not annotate proper names , e.g. , “ Run ” in   the sentence The military campaign near that   creek was known as “ The battle of Bull Run”.•You are allowed to include cross - PoS   candidates ( that is , candidates whose   PoS is different from that of the target   word ) , in this case please include the   candidate in square brackets like this : ,   wherexrepresents the part - of - speech tag of   the translated word . Do this for multi - word   expressions as well .   Mark with the tag “ DISCUSS ” difﬁcult instances   which you would like to discuss .   D Model - speciﬁc Analyses   We include model - speciﬁc analyses with per-   language breakdown of the scores achieved on our   benchmark . The column named ESCHER provides   the scores of the WSD system on the subset of sen-   tences of the speciﬁed model and language , and   should be treated as an additional baseline to com-   pare with the accuracy achieved by the system . De-   tails can be found in Section 4 .   •DeepL   •Google   •OPUS   •M2M100   •M2M100   •MBart50   •MBart50DeepL   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 36.07 74.60 53.68 84.21 28.30 34.78 66.86   ES 25.26 57.87 59.89 87.91 46.14 56.04 67.89   IT 20.21 53.49 68.08 86.38 49.01 57.71 66.67   RU 35.04 71.58 50.00 83.33 33.64 41.97 66.76   ZH 31.86 46.00 49.07 88.89 59.58 64.97 68.42   Mean 29.69 60.71 56.14 86.15 43.33 51.10 67.32Google   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 35.87 21.90 56.76 86.82 79.54 86.61 71.04   ES 23.29 22.54 61.96 89.05 78.41 83.84 72.76   IT 23.12 18.04 61.96 87.23 80.62 85.47 72.58   RU 35.37 22.89 48.12 83.28 83.49 84.01 69.55   ZH 32.49 15.04 56.05 88.20 87.98 91.97 71.89   Mean 30.03 20.08 56.97 86.92 82.01 86.38 71.56OPUS   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 37.84 27.99 56.98 87.92 76.25 79.85 66.95   ES 25.69 36.66 64.47 91.21 69.12 74.85 66.83   IT 29.11 29.95 64.48 89.66 72.02 80.59 65.82   RU 45.84 41.07 48.40 84.04 69.27 68.49 69.21   ZH 38.31 27.75 51.71 87.45 75.66 79.96 69.88   Mean 35.36 32.68 57.21 88.06 72.46 76.75 67.74M2M100   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 49.41 22.19 61.28 87.23 76.15 82.00 65.85   ES 41.91 25.51 61.81 89.37 77.95 83.08 66.77   IT 42.44 21.83 60.75 86.79 76.58 80.22 66.35   RU 51.77 26.22 47.87 83.41 78.34 79.85 66.42   ZH 48.66 16.99 59.06 91.34 87.18 91.81 69.26   Mean 46.84 22.55 58.15 87.63 79.24 83.39 66.93M2M100   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 42.02 26.96 59.13 87.30 74.71 78.90 67.18   ES 36.75 30.00 61.78 88.03 73.84 79.87 66.86   IT 37.07 25.14 62.82 88.81 76.10 78.69 68.50   RU 42.50 35.19 45.25 84.16 69.69 74.72 67.69   ZH 39.73 22.35 59.35 92.45 82.17 88.79 69.82   Mean 39.61 27.93 57.66 88.15 75.30 80.19 68.01MBart50   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 40.24 28.73 58.89 89.72 73.86 84.10 66.87   ES 39.29 33.89 60.17 91.10 71.06 77.13 65.37   IT 40.10 29.34 62.90 87.50 71.51 78.67 64.33   RU 44.16 36.06 47.39 87.20 70.11 73.86 66.35   ZH 44.35 31.21 50.66 89.87 73.14 80.39 68.93   Mean 41.63 31.85 56.00 89.08 71.94 78.83 66.37MBart50   Back to Model - speciﬁc Analyses .   % MISS Accuracy MFS MFS+ SPDI SFII ESCHER   DE 41.25 28.65 55.82 89.56 74.24 84.95 67.77   ES 41.06 32.66 63.09 91.85 71.57 79.06 67.18   IT 43.29 30.54 68.97 91.81 69.48 79.41 65.81   RU 45.18 33.33 44.91 87.96 72.87 78.58 64.29   ZH 44.59 34.15 54.17 90.28 71.50 76.59 69.58   Mean 43.07 31.87 57.39 90.29 71.93 79.72 66.93bn:00057755nHepoureda shotofwhiskey .   Asmalldrinkofliquor .   German ✓ ✗   Schl¨uckchen Schlag   Schuss Injektion   Spanish ✓ ✗   trago pistolero   chupito tiro   Italian ✓ ✗   goccio iniezione   bicchierino sparo   Russian ✓ ✗   шот стрелок   рюмкa выстрел   Chinese ✓ ✗   杯 枪 手   小杯 本 垒打   bn:00036083nTheytrackedhimbacktowardthe headofthestream .   Thesourceofwaterfromwhichastreamarises .   German ✓ ✗   Ursprung Kopf   Quelle Kommando   Spanish ✓ ✗   fuente cabeza   manantial jefe   Italian ✓ ✗   fonte testa   sorgente capo   Russian ✓ ✗   исток проход   вопрос   Chinese ✓ ✗   源头 头   族长bn:00094769vIfyoutake oﬀforThanksgivingyou   mustworkChristmasandviceversa .   Toabsentoneselffromworkorotherre-   sponsibility , especiallywithpermission .   German ✓ ✗   sicheineAuszeitnehmen losgehen   sichfreinehmen starten   Spanish ✓ ✗   pedirunpermiso salir   coger llevar   Italian ✓ ✗   prendersideigiorni togliersi   prendersiunpermesso decollare   Russian ✓ ✗   братьвыходной вычесть   отдыхать убить   Chinese ✓ ✗   请假 离开   休假 减