  Shulin Cao , Jiaxin Shi , Liangming Pan , Lunyiu Nie , Yutong Xiang ,   Lei Hou , Juanzi Li , Bin He , Hanwang ZhangDepartment of Computer Science and Technology , BNRistKIRC , Institute for Artificial Intelligence , Tsinghua University , Beijing 100084 , ChinaCloud BU , Huawei Technologies , National University of Singapore , ETH ZürichNoah ’s Ark Lab , Huawei Technologies , Nanyang Technological University   { caosl19@mails . , houlei@,lijuanzi@}tsinghua.edu.cn   shijx12@gmail.com , liangmingpan@u.nus.edu   Abstract   Complex question answering over knowledge   base ( Complex KBQA ) is challenging because   it requires various compositional reasoning ca-   pabilities , such as multi - hop inference , attribute   comparison , set operation . Existing bench-   marks have some shortcomings that limit the   development of Complex KBQA : 1 ) they only   provide QA pairs without explicit reasoning   processes ; 2 ) questions are poor in diversity   or scale . To this end , we introduce KQA   Pro , a dataset for Complex KBQA including   ~120 K diverse natural language questions . We   introduce a compositional and interpretable   programming language KoPL to represent the   reasoning process of complex questions . For   each question , we provide the corresponding   KoPL program and SPARQL query , so that   KQA Pro serves for both KBQA and semantic   parsing tasks . Experimental results show that   SOTA KBQA methods can not achieve promis-   ing results on KQA Pro as on current datasets ,   which suggests that KQA Pro is challenging   and Complex KBQA requires further research   efforts . We also treat KQA Pro as a diagnos-   tic dataset for testing multiple reasoning skills ,   conduct a thorough evaluation of existing mod-   els and discuss further directions for Complex   KBQA . Our codes and datasets can be obtained   from https://github.com/shijx12/   KQAPro_Baselines .   1 Introduction   Thanks to the recent advances in deep models , es-   pecially large - scale unsupervised representation   learning ( Devlin et al . , 2019 ) , question answering   of simple questions over knowledge base ( Simple   KBQA ) , i.e. , single - relation factoid questions ( Bor-   des et al . , 2015 ) , begins to saturate ( Petrochuk   and Zettlemoyer , 2018 ; Wu et al . , 2019 ; Huanget al . , 2019 ) . However , tackling complex ques-   tions ( Complex KBQA ) is still an ongoing chal-   lenge , due to the unsatisfied capability of composi-   tional reasoning . As shown in Table 1 , to promote   the community development , several benchmarks   are proposed for Complex KBQA , including LC-   QuAD2.0 ( Dubey et al . , 2019 ) , ComplexWebQues-   tions ( Talmor and Berant , 2018 ) , MetaQA ( Zhang   et al . , 2018 ) , CSQA ( Saha et al . , 2018 ) , CFQ ( Key-   sers et al . , 2020 ) , and so on . However , they suffer   from the following problems :   1 ) Most of them only provide QA pairs without   explicit reasoning processes , making it challenging   for models to learn compositional reasoning . Some   researchers try to learn the reasoning processes   with reinforcement learning ( Liang et al . , 2017 ;   Saha et al . , 2019 ; Ansari et al . , 2019 ) and search-   ing ( Guo et al . , 2018 ) . However , the prohibitively   huge search space hinders both the performance   and speed , especially when the question complexity   increases . For example , Saha et al . ( 2019 ) achieved   a 96.52 % F1 score on simple questions in CSQA ,   whereas only 0.33 % on complex questions that   require comparative count . We think that interme-   diate supervision is needed for learning the compo-   sitional reasoning , mimicking the learning process   of human beings ( Holt , 2017 ) .   2 ) Questions are not satisfactory in diversity and   scale . For example , MetaQA ( Zhang et al . , 2018 )   questions are generated using just 36 templates ,   and they only consider relations between entities ,   ignoring literal attributes ; LC - QuAD2.0 ( Dubey   et al . , 2019 ) and ComplexWebQuestions ( Talmor   and Berant , 2018 ) have fluent and diverse human-   written questions , but their scale is less than 40K.   To address these problems , we create KQA Pro ,   a large - scale benchmark for Complex KBQA . In   KQA Pro , we define a Knowledge - oriented Pro-   gramming Language ( KoPL ) to explicitly describe6101   the reasoning process for solving complex ques-   tions ( see Fig . 1 ) . A program is composed of sym-   bolic functions , which define the basic , atomic   operations on KBs . The composition of functions   well captures the language compositionality ( Ba-   roni , 2019 ) . Besides KoPL , following previous   works ( Yih et al . , 2016 ; Su et al . , 2016 ) , we also   provide the corresponding SPARQL for each ques-   tion , which solves a complex question by parsing   it into a query graph . Compared with SPARQL ,   KoPL 1 ) provides a more explicit reasoning pro-   cess . It divides the question into multiple steps ,   making human understanding easier and the inter-   mediate results more transparent ; 2 ) allows humans   to control the model behavior better , potentially   supporting human - in - the - loop . When the system   gives a wrong answer , users can quickly locate the   error by checking the outputs of intermediate func-   tions . We believe the compositionality of KoPL   and the graph structure of SPARQL are two com-   plementary directions for Complex KBQA .   To ensure the diversity and scale of KQA Pro , we   follow the synthesizing and paraphrasing pipeline   in the literature ( Wang et al . , 2015a ; Cao et al . ,   2020 ) , first synthesize large - scale ( canonical ques-   tion , KoPL , SPARQL ) triples , and then paraphrasethe canonical questions to natural language ques-   tions ( NLQs ) via crowdsourcing . We combine the   following two factors to achieve diversity in ques-   tions : ( 1 ) To increase structural variety , we leverage   a varied set of templates to cover all the possible   queries through random sampling and recursive   composing ; ( 2 ) To increase linguistic variety , we   filter the paraphases based on their edit distance   with the canonical utterance . Finally , KQA Pro   consists of 117,970 diverse questions that involve   varied reasoning skills ( e.g. , multi - hop reasoning ,   value comparisons , set operations , etc . ) . Besides   a QA dataset , it also serves as a semantic parsing   dataset . To the best of our knowledge , KQA Pro is   currently the largest corpus for NLQ - to - SPARQL   and NLQ - to - Program tasks .   We reproduce the state - of - the - art KBQA models   and thoroughly evaluate them on KQA Pro . From   the experimental results , we observe significant   performance drops of these models compared with   on existing KBQA benchmarks . It indicates that   Complex KBQA is still challenging , and KQA Pro   could support further explorations . We also treat   KQA Pro as a diagnostic dataset for analyzing a   model ’s capability of multiple reasoning skills , and   discover weaknesses that are not widely known ,   e.g. , current models struggle on comparisonal rea-   soning for lacking of literal knowledge ( i.e. ,(Le-   Bron James , height , 206 centimetre ) ) , or perform   poorly on questions whose answers are not obver-   seved in the training set . We hope all contents of   KQA Pro could encourage the community to make   further breakthroughs .   2 Related Work   Complex KBQA aims at answering complex ques-   tions over KBs , which requires multiple reasoning   capabilities such as multi - hop inference , quanti-   tative comparison , and set operation ( Lan et al . ,   2021 ) . Current methods for Complex KBQA can   be grouped into two categories : 1 ) semantic pars-   ing based methods ( Liang et al . , 2017 ; Guo et al . ,   2018 ; Saha et al . , 2019 ; Ansari et al . , 2019 ) , which   parses a question to a symbolic logic form ( e.g. ,λ-   calculus ( Artzi et al . , 2013 ) , λ - DCS ( Liang , 2013 ;   Pasupat and Liang , 2015 ; Wang et al . , 2015b ; Pa-   supat and Liang , 2016 ) , SQL ( Zhong et al . , 2017 ) ,   AMR ( Banarescu et al . , 2013 ) , SPARQL ( Sun et al . ,   2020 ) , and etc . ) and then executes it against the   KB and obtains the final answers ; 2 ) information   retrieval based methods ( Miller et al . , 2016 ; Saxena6102   et al . , 2020 ; Schlichtkrull et al . , 2018 ; Zhang et al . ,   2018 ; Zhou et al . , 2018 ; Qiu et al . , 2020 ; Shi et al . ,   2021 ) , which constructs a question - specific graph   extracted from the KB and ranks all the entities in   the extracted graph based on their relevance to the   question .   Compared with information retrieval based meth-   ods , semantic parsing based methods provides bet-   ter interpretability by generating expressive logic   forms , which represents the intermediate reason-   ing process . However , manually annotating logic   forms is expensive and labor - intensive , and it is   challenging to train a semantic parsing model with   weak supervision signals ( i.e. , question - answer   pairs ) . Lacking logic form annotations turns out to   be one of the main bottlenecks of semantic parsing .   Table 1 lists the widely - used datasets in Complex   KBQA community and their features . MetaQA   and CSQA have a large number of questions , but   they ignore literal knowledge , lack logic form an-   notations , and their questions are written by tem-   plates . Query graphs ( e.g. , SPARQLs ) are provided   in some datasets to help solve complex questions .   However , SPARQL is weak in describing the inter-   mediate procedure of the solution , and the scale of   existing question - to - SPARQL datasets is small .   In this paper , we introduce a novel logic form   KoPL , which models the procedure of Complex   KBQA as a multi - step program , and provides a   more explicit reasoning process compared with   query graphs . Furthermore , we propose KQA Pro ,   a large - scale semantic parsing dataset for Complex   KBQA , which contains ~120k diverse natural lan-   guage questions with both KoPL and SPARQL an-   notations . It is the largest NLQ - to - SPARQL dataset   as far as we know . Compared with these existing   datasets , KQA Pro serves as a more well - rounded   benchmark.3 Background   3.1 KB Definition   Typically , a KB(e.g . , Wikidata ( Vrande ˇci´c and   Krötzsch , 2014 ) ) consists of :   Entity , the most basic item in KB .   Concept , the abstraction of a set of entities , e.g. ,   basketball player .   Relation , the link between entities or concepts . En-   tities are linked to concepts via the relation instance   of . Concepts are organized into a tree structure via   relation subclass of .   Attribute , the literal information of an entity . An   attribute has a key and a value , which is one of four   types : string , number , date , and year . The number   value has an extra unit , e.g. , 206 centimetre .   Relational knowledge , the triple with form ( entity ,   relation , entity ) , e.g. ,(LeBron James Jr. , father ,   LeBron James ) .   Literal knowledge , the triple with form ( entity ,   attribute key , attribute value ) , e.g. ,(LeBron James ,   height , 206 centimetre ) .   Qualifier knowledge , the triple whose head is a   relational or literal triple , e.g. ,((LeBron James ,   drafted by , Cleveland Cavaliers ) , point in time ,   2003 ) . A qualifier also has a key and a value .   3.2 KoPL Design   We design KoPL , a compositional and interpretable   programming language to represent the reasoning   process of complex questions . It models the com-   plex procedure of question answering with a pro-   gram of intermediate steps . Each step involves a   function with a fixed number of arguments . Every   program can be denoted as a binary tree . As shown   in Fig . 1 , a directed edge between two nodes rep-   resents the dependency relationship between two6103functions . That is , the destination function takes the   output of the source function as its argument . The   tree - structured program can also be serialized by   post - order traversal , and formalized as a sequence   withnfunctions . The general form is shown below .   Each function ftakes in a list of textual arguments   a , which need to be inferred according to the ques-   tion , and a list of functional arguments b , which   come from the output of previous functions .   f(a , b)f(a , b) ... f(a , b ) ( 1 )   Take function Relate as an example , it has two tex-   tual inputs : relation and direction ( i.e. , forward or   backward , meaning the output is object or subject ) .   It has one functional input : a unique entity . Its out-   put is a set of entities that hold the specific relation   with the input entity . For example , in Question 2 of   Fig . 1 , the function Relate ( [ father , forward ] , [ Le-   Bron James Jr. ] ) returns LeBron James , the father   of LeBron James Jr. ( the direction is omitted in the   figure for simplicity ) .   We analyze the generic , basic operations for   Complex KBQA , and design 27 functionsin   KoPL . They support KB item manipulation ( e.g. ,   Find , Relate , FilterConcept , QueryRelationQuali-   fier , etc . ) , various reasoning skills ( e.g. ,And , Or ,   etc . ) , and multiple question types ( e.g. ,QueryName ,   SelectBetween , etc . ) . By composing the finite func-   tions into a KoPL program , we can model the   reasoning process of infinite complex questions .   Note that qualifiers play an essential role in   disambiguating or restricting the validity of a   fact ( Galkin et al . , 2020 ; Liu et al . , 2021 ) . How-   ever , they have not been adequately modeled in   current KBQA models or datasets . As far as we   know , we are the first to explicitly model qualifiers   in Complex KBQA .   4 KQA Pro Construction   To build KQA Pro dataset , first , we extract a   knowledge base with multiple kinds of knowledge   ( Section 4.1 ) . Then , we generate canonical ques-   tions , corresponding KoPL programs and SPARQL   queries with novel compositional strategies ( Sec-   tion 4.2 ) . In this stage , we aim to cover all the   possible queries through random sampling and re-   cursive composing . Finally , we rewrite canonical   questions into natural language via crowdsourcing   ( Section 4.3 ) . To further increase linguistic variety , we reject the paraphrases whose edit distance with   the canonical question is small .   4.1 Knowledge Base Extraction   We took the entities of FB15k-237 ( Toutanova et al . ,   2015 ) as seeds , and aligned them with Wikidata via   Freebase IDs . The reasons are as follows : 1 ) The   vast amount of knowledge in the full knowledge   base ( e.g. , full Freebase ( Bollacker et al . , 2008 ) or   Wikidata contains millions of entities ) may cause   both time and space issues , while most of the enti-   ties may never be used in questions . 2 ) FB15k-237   is a high - quality , dense subset of Freebase , whose   alignment to Wikidata produces a knowledge base   with rich literal and qualifier knowledge . We added   3,000 other entities with the same name as one of   FB15k-237 entities to increase the disambiguation   difficulty . The statistics of our final knowledge   base are listed in Table 2 .   4.2 Question Generation Strategies   To generate diverse complex questions in a scalable   manner , we propose to divide the generation into   two stages : locating andasking . In locating stage   we describe a single entity or an entity set with   various restrictions , while in asking stage we query   specific information about the target entity or entity   set . We define several strategies for each stage . By   sampling from them and composing the two stages ,   we can generate large - scale and diverse questions   with a small number of templates . Fig . 2 gives an   example of our generation process .   For locating stage , we propose 7 strategies and   show part of them in the top section of Table 3 . We   can fill the placeholders of templates by sampling   from KB to describe a target entity . We support   quantitative comparisons of 4 operations : equal ,   not equal , less than , and greater than , indicated by   “ < OP > ” of the template . We support optional qual-   ifier restrictions , indicated by “ ( < QK > is < QV > ) ” , 6104   which can narrow the located entity set . In Recur-   sive Multi - Hop , we replace the entity of a relational   condition with a more detailed description , so that   we can easily increase the hop of questions . For   asking stage , we propose 9 strategies and show   some of them in the bottom section of Table 3 . Our   SelectAmong is similar to argmax andargmin oper-   ations in λ - DCS . The complete generation strate-   gies are shown in Appendix D due to space limit .   Our generated instance consists of five elements :   question , SPARQL query , KoPL program , 10 an-   swer choices , and a golden answer . Choices are se-   lected by executing an abridged SPARQL , which   randomly drops one clause from the complete   SPARQL . With these choices , KQA Pro supports   both multiple - choice setting and open - ended set-   ting . We randomly generate lots of questions , and   only preserve those with a unique answer . For ex-   ample , since Akron has different populations in   different years , we will drop questions like What is   the population of Akron , unless the time constraint   ( e.g. ,in 2010 ) is specified .   4.3 Question Paraphrasing and Evaluation   After large - scale generation , we release the gener-   ated questions on Amazon Mechanical Turk ( AMT)and ask the workers to paraphrase them without   changing the original meaning . For the conve-   nience of paraphrasing , we visualize the KoPL   flowcharts like Fig . 1 to help workers understand   complex questions . We allow workers to mark a   question as confusing if they can not understand   it or find logical errors . These instances will be   removed from our dataset .   After paraphrasing , we evaluate the quality by   5 other workers . They are asked to check whether   the paraphrase keeps the original meaning and give   a fluency rating from 1 to 5 . We reject those para-   phrases which fall into one of the following cases :   ( 1 ) marked as different from the original canonical   question by more than 2 workers ; ( 2 ) whose aver-   age fluency rating is lower than 3 ; ( 3 ) having a very   small edit distance with the canonical question .   4.4 Dataset Analysis   Our KQA Pro dataset consists of 117,970 instances   with 24,724 unique answers . Fig . 3(a ) shows the   question type distribution of KQA Pro . Within the   9 types , SelectAmong accounts for the least frac-   tion ( 4.6 % ) , while others account for more or less   than 10 % . Fig . 3(b ) shows that multi - hop questions   cover 73.7 % of KQA Pro , and 4.7 % questions even   require at least 5 hops . We compare the question   length distribution of different Complex KBQA6105   datasets in Fig . 3(c ) . We observe that our KQA   Pro has longer questions than others on average . In   KQA Pro , the average length of questions / program-   s / SPARQLs is 14.95/4.79/35.52 respectively . More   analysis is included in Appendix G.   5 Experiments   The primary goal of our experiments is to show the   challenges of KQA Pro and promising Complex   KBQA directions . First , we compare the perfor-   mance of state - of - the - art KBQA models on current   datasets and KQA Pro , to show whether KQA Pro   is challenging . Then , we treat KQA Pro as a diag-   nostic dataset to investigate fine - grained reasoning   abilities of models , discuss current weakness and   promising directions . We further conduct an ex-   periment to explore the generation ability of our   proposed model . Last , we provide a case study to   show the interpretablity of KoPL .   5.1 Experimental Settings   Benchmark Settings . We randomly split KQA   Pro to train / valid / test set by 8/1/1 , resulting in three   sets with 94,376/11,797/11,797 instances . About   30 % answers of the test set are not seen in training .   Representative Models . KBQA models typically   follow a retrieve - and - rank paradigm , by construct-   ing a question - specific graph extracted from the KB   and ranks all the entities in the graph based on their   relevance to the question ( Miller et al . , 2016 ; Sax-   ena et al . , 2020 ; Schlichtkrull et al . , 2018 ; Zhang   et al . , 2018 ; Zhou et al . , 2018 ; Qiu et al . , 2020 ) ; or   follow a parse - then - execute paradigm , by parsing a   question to a query graph ( Berant et al . , 2013 ; Yih   et al . , 2015 ) or program ( Liang et al . , 2017 ; Guo   et al . , 2018 ; Saha et al . , 2019 ; Ansari et al . , 2019 )   through learning from question - answer pairs .   Experimenting with all methods is logistically   challenging , so we reproduce a representative sub-   set of mothods : KVMemNet ( Miller et al . , 2016 ) ,   a well - known model which organizes the knowl-   edge into a memory of key - value pairs , and iter-   atively reads memory to update its query vector .   EmbedKGQA ( Saxena et al . , 2020 ) , a state - of - the   art model on MetaQA , which incorporates knowl-   edge embeddings to improve the reasoning per-   formance . SRN ( Qiu et al . , 2020 ) , a typical path   search model to start from a topic entity and pre-   dict a sequential relation path to find the target   entity . RGCN ( Schlichtkrull et al . , 2018 ) , a variant6106of graph convolutional networks , tackling Com-   plex KBQA through the natural graph structure of   knowledge base .   Our models . Since KQA Pro provides the annota-   tions of SPARQL and KoPL , we directly learn our   parsers using supervised learning by regarding the   semantic parsing as a sequence - to - sequence task .   We explore the widely - used sequence - to - sequence   model — RNN with attention mechanism ( Dong   and Lapata , 2016 ) , and the pretrained generative   language model — BART ( Lewis et al . , 2020 ) , as   our SPARQL and KoPL parsers .   For KoPL learning , we design a serializer to   translate the tree - structured KoPL to a sequence .   For example , the KoPL program in Fig . 2 is serial-   ized as : Find⟨arg⟩LeBron James ⟨func⟩Relate   ⟨arg⟩drafted by ⟨arg⟩backward ⟨func⟩Filter-   Concept ⟨arg⟩team⟨func⟩QueryName . Here ,   ⟨arg⟩and⟨func⟩are special tokens we designed   to indicate the structure of KoPL .   To compare machine with Human , we sample   200 instances from the test set , and ask experts to   answer them by searching our knowledge base .   Implementation Details . For our BART model ,   we used the bart - base model of HuggingFace . We   used the optimizer Adam ( Kingma and Ba , 2015 )   for all models . We searched the learning rate for   BART paramters in { 1e-4 , 3e-5 , 1e-5 } , the learning   rate for other parameters in { 1e-3 , 1e-4 , 1e-5 } , and   the weight decay in { 1e-4 , 1e-5 , 1e-6 } . According   to the performance on validation set , we finally   used learning rate 3e-5 for BART parameters , 1e-3   for other parameters , and weight decay 1e-5 .   5.2 Difficulty of KQA Pro   We compare the performance of KBQA models on   KQA Pro with MetaQA and WebQSP ( short for   WebQuestionSP ) , two commonly used benchmarks   in Complex KBQA . The experimental results are   in Table 4 , from which we observe that :   Although the models perform well on MetaQA   and WebQSP , their performances are significantly   lower and not satisfying on KQA Pro . It indicates   that our KQA Pro is challenging and the Com-   plex KBQA still needs more research efforts . Ac-   tually , 1 ) Both MetaQA and WebQSP mainly fo-   cus on relational knowledge , i.e. , multi - hop ques-   tions . Therefore , previous models on these datasets   are designed to handle only entities and relations .   In comparison , KQA Pro includes three types ofknowledge , i.e. , relations , attributes , and qualifiers ,   thus is much more challenging . 2 ) Compared with   MetaQA which contains template questions , KQA   Pro contains diverse natural language questions and   can evaluate models ’ language understanding abil-   ities . 3 ) Compared with WebQSP which contains   4,737 fluent and natural questions , KQA Pro covers   more question types ( e.g. , verification , counting )   and reasoning operations ( e.g. , intersect , union ) .   5.3 Analyses on Reasoning Skills   KQA Pro can serve as a diagnostic dataset for in-   depth analyses of reasoning abilities ( e.g. , counting ,   comparision , logical reasoning , etc . ) for Complex   KBQA , since KoPL programs underlying the ques-   tions provide tight control over the dataset .   We categorize the test questions to measure fine-   grained ability of models . Specifically , Multi - hop   means multi - hop questions , Qualifier means ques-   tions containing qualifier knowledge , Compari-   sonmeans quantitative or temporal comparison   between two or more entities , Logical means logi-   cal union or intersection , Count means questions   that ask the number of target entities , Verify means   questions that take “ yes ” or “ no ” as the answer ,   Zero - shot means questions whose answer is not   seen in the training set . The results are shown in   Table 5 , from which we have the following obser-   vations :   ( 1 ) Benefits of intermediate reasoning supervi-   sion . Our RNN and BART models outperform   current models significantly on all reasoning skills .   This is because KoPL program and SPARQL query   provide intermediate supervision which benefits   the learning process a lot . As ( Dua et al . , 2020 )   suggests , future dataset collection efforts should   set aside a fraction of budget for intermediate an-   notations , particularly as the reasoning required6107   becomes more complex . We hope our dataset   KQA Pro with KoPL and SPARQL annotations will   help guide further research in Complex KBQA . ( 2 )   More attention to literal and qualifier knowledge .   Existing models perform poorly in situations re-   quiring comparison capability . This is because they   only focus the relational knowledge , while ignor-   ing the literal and qualifier knowledge . We hope   our dataset will encourage the community to pay   more attention to multiple kinds of knowledge in   Complex KBQA . ( 3 ) Generalization to novel ques-   tions and answers . For zero - shot questions , current   models all have a close to zero performance . This   indicates the models solve the questions by simply   memorizing their training data , and perform poorly   on generalizing to novel questions and answers .   5.4 Compositional Generalization   We further use KQA Pro to test the ability of KBQA   models to generalize to questions that contain novel   combinations of the elements observed during train-   ing . Following previous works , we conduct the   “ productivity ” experiment ( Lake and Baroni , 2018 ;   Shaw et al . , 2021 ) , which focuses on generaliza-   tion to longer sequences or to greater composi-   tional depths than have been seen in training ( for   example , from a length 4 program to a length 5   program ) . Specifically , we take the instances with   short programs as training examples , and those   with long programs as test and valid examples , re-   sulting in three sets including 106,182/5,899/5,899   examples . The performance of BART KoPL drops   from 90.55 % to 77.86 % , which indicates learning   to generalize compositionally for pretrained lan-   guage models requires more research efforts . Our   KQA Pro provides an environment for further ex-   perimentation on compositional generalization .   5.5 Case Study   To further understand the quality of logical forms   predicted by the BART parser , we show a case in   Fig . 4 , for which the SPARQL and KoPL parsers   both give wrong predictions . The SPARQL parser   fails to understand prior to David Lloyd George   and gives a totally wrong prediction for this part .   The KoPL parser gives a function prediction which   is semantically correct but very different from our   generated golden one . It is a surprising result , re-   vealing that the KoPL parser can understand the se-   mantics and learn multiple solutions for each ques-   tion , similar to the learning process of humans . We   manually correct the errors of predicted SPARQL6108and KoPL and mark them in red . Compared to   SPARQLs , KoPL programs are easier to be under-   stood and more friendly to be modified .   6 Conclusion and Future Work   In this work , we introduce a large - scale dataset   with explicit compositional programs for Complex   KBQA . For each question , we provide the corre-   sponding KoPL program and SPARQL query so   that KQA Pro can serve for both KBQA and seman-   tic parsing tasks . We conduct a thorough evaluation   of various models , discover weaknesses of current   models and discuss future directions . Among these   models , the KoPL parser shows great interpretabil-   ity . As shown in Fig . 4 , when the model predicts   the answer , it will also give a reasoning process and   a confidence score ( which is ommited in the figure   for simplicity ) . When the parser makes mistakes ,   humans can easily locate the error through read-   ing the human - like reasoning process or checking   the outputs of intermediate functions . In addition ,   using human correction data , the parser can be   incrementally trained to improve the performance   continuously . We will leave this as our future work .   Acknowledgments   This work is founded by the National Key   Research and Development Program of China   ( 2020AAA0106501 ) , the Institute for Guo Qiang ,   Tsinghua University ( 2019GQB0003 ) , Huawei   Noah ’s Ark Lab and Beijing Academy of Artifi-   cial Intelligence .   References6109611061116112A Function Library of KoPL   Table 6 shows our 27 functions and their explana-   tions . Note that we define specific functions for   different attribute types ( i.e. , string , number , date ,   and year ) , because the comparison of these types   are quite different . Following we explain some   necessary items in our functions .   Entities / Entity : Entities denotes an entity set , which   can be the output or functional input of a function .   When the set has a unique element , we get an En-   tity .   Name : A string that denotes the name of an entity   or a concept .   Key / Value : The key and value of an attribute .   Op : The comparative operation . It is one of   { = , ̸= , < , > } when comparing two values , one   of{greater , less}inSelectBetween , and one of   { largest , smallest } inSelectAmong .   Pred / Dir : The relation and direction of a relation .   Fact : A literal fact , e.g. , ( LeBron James , height ,   206 centimetre ) , or a relational fact , e.g. , ( LeBron   James , drafted by , Cleveland Cavaliers ) .   QKey / QValue : The key and value of a qualifier .   B Grammar Rules of KoPL   As shown in Table 7 , the supported program space   of KoPL can be defined by a synchrounous context-   free grammar ( SCFG ) , which is widely used to gen-   erate logical forms paired with canonical questions   ( Wang et al . , 2015a ; Jia and Liang , 2016 ; Wu et al . ,   2021 ) . The programs are meant to cover the desired   set of compositional functions , and the canonical   questions are meant to capture the meaning of the   programs .   C Knowledge Base Extraction   Specifically , we took the entities of FB15k-   237 ( Toutanova et al . , 2015 ) , a popular subset of   Freebase , as seeds , and then aligned them with   Wikidata via Freebase IDs , so that we could ex-   tract their rich literal and qualifier knowledge from   Wikidata . Besides , we added 3,000 other entities   with the same name as one of FB15k-237 entities ,   to further increase the difficulty of disambiguation .   For the relational knowledge , we manually merged   the relations of FB15k-237 ( e.g. ,/people / person-   /spouse_s./people / marriage / spouse ) and Wikidata(e.g . ,spouse ) , obtaining 363 relations totally . Fi-   nally , we manually filtered out useless attributes   ( e.g. , about images and Wikidata pages ) and enti-   ties ( i.e. , never used in triples ) .   D Generation Strategies   Table 8 list the complete generation strategies , in-   cluding 7 locating and 9 asking strategies . In locat-   ing stage we describe a single entity or an entity set   with various restrictions , while in asking stage we   query specific information about the target entity   or entity set .   E SPARQL Implementation Details   We build a SPARQL engine with Virtuosoto exe-   cute generated SPARQLs . To denote qualifiers , we   create a virtual node for those literal and relational   triples . For example , to denote the point in time of   ( LeBron James , drafted by , Cleveland Cavaliers ) ,   we create a node _ BN which connects to the sub-   ject , the relation , and the object with three special   edges , and then add ( _ BN , point in time , 2003 ) into   the graph . Similarly , we use virtual node to repre-   sent the attribute value of number type , which has   an extra unit . For example , to represent the height   ofLeBron James , we need ( LeBron James , height ,   _ BN ) , ( _ BN , value , 206 ) , ( _ BN , unit , centimetre ) .   F Generation Examples   Consider the example of Fig . 2 in Section 4.2 in   the main text , following is a detailed explanation .   At the first , the asking stage samples the strat-   egyQueryName and samples Cleveland Cavaliers   from the whole entity set as the target entity . The   corresponding textual description , SPARQL , and   KoPL of this stage is “ Who is < E > ” , “ SELECT   ? e WHERE { } ” , and “ QueryName ” , respectively .   Then we switch to the locating stage to describe   the target entity Cleveland Cavaliers . We sample   the strategy , Concept + Relational , to locate it . For   the concept part , we sample team from all concepts   ofCleveland Cavaliers . The corresponding textual   description , SPARQL , and KoPL is “ team ” , “ ? e   < pred : instance_of > ? c . ? c < pred : name > “ team ”   . ” , and “ FilterConcept(team ) ” , respectively . For the   relation part , we sample ( LeBron James , drafted   by)from all triples of Cleveland Cavaliers . The   corresponding textual description , SPARQL , and   KoPL is “ drafted LeBron James ” , “ ? e_1 < drafted6113   by>?e . ? e_1 < pred : name > ’ LeBron James ’ . ” ,   “ Find(LeBron James ) →Relate(drafted by , back-   ward ) ” , respectively . The locating stage combines   the concept and the relation , obtaining the entity   description “ the team that drafted LeBron James ”   and the corresponding SPARQL and KoPL . Finally ,   we combine the results of the two stages and output   the complete question . Figure 8 and 9 show more   examples of KQA Pro .   G Data Analysis   There are 24,724 unique answers in KQA Pro . We   show the top 20 most frequent answers and their   fractions in Fig . 5 . “ yes ” and “ no ” are the most   frequent answers , because they cover all questions   of type Verify . “ 0 ” , “ 1 ” , “ 2 ” , “ 3 ” , and other quantity   answers are for questions of type Count , which   accounts for 11.5 % .   Fig . 6 shows the Program length distribution .   Most of our problems ( 28.42 % ) can be solved by 4   functional steps . Some extreme complicated ones   ( 1.24 % ) need more than 10 steps .   Fig . 7 shows sunburst for first 4 words in ques-   tions . We can see that questions usually start with   “ what ” , “ which ” , “ how many ” , “ when ” , “ is ” and   “ does ” . Frequent topics include “ person ” , “ movie ” ,   “ country ” , “ university ” , and etc .   H Baseline Implementation Details   KVMemNet . For literal and relational knowledge ,   we concatenated the subject and the attribute / re-   lation as the memory key , e.g. , “ LeBron James   drafted by ” , leaving the object as the memory value.6114   For high - level knowledge , we concatenated the fact   and the qualifier key as the memory key , e.g. , “ Le-   Bron James drafted by Cleveland Cavaliers point in   time ” . For each question , we pre - selected a small   subset of the KB as its relavant memory . Following   ( Miller et al . , 2016 ) , we retrieved 1,000 key - value   pairs where the key shares at least one word with   the question with frequency < 1000 ( to ignore stop   words ) . KVMemNet iteratively updates a query   vector by reading the memory attentively . In our   experiment we set the update steps to 3 .   SRN . SRN can only handle relational knowledge .   It must start from a topic entity and terminate with   a predicted entity . So we filtered out questions that   contain literal knowledge or qualifier knowledge , retaining 5,004 and 649 questions as its training set   and test set . Specifically , we retained the questions   with Find as the first function and QueryName as   the last function . The textual input of the first Find   was regarded as the topic entity and was fed into   the model during both training and testing phase .   EmbedKGQA . EmbedKGQA utilizes knowledge   graph embedding to improve multi - hop reasoning .   To adapt to existing knowledge embedding tech-   niques , we added virtual nodes to represent the   qualifier knowledge of KQA Pro . Different from   SRN , we applied EmbedKGQA on the entire KQA   Pro dataset , because its classification layer is more   flexible than SRN and can predict answers outside   the entity set . The topic entity of each question was6115   extracted from the golden program and then fed   into the model during both training and testing .   RGCN . To build the graph , we took entities as   nodes , connections between them as edges , and re-   lations as edge labels . We concatenated the literal   attributes of an entity into a sequence as the node   description . For simplicity , we ignored the qualifier   knowledge . Given a question , we first initialized   node vectors by fusing the information of node de-   scriptions and the question , then conducted RGCN   to update the node features , and finally aggregated   features of nodes and the question to predict the   answer via a classification layer . Our RGCN imple - mentation is based on DGL , a high performance   Python package for deep learning on graphs . Due   to the memory limit , we set the graph layer to 1   and set the hidden dimension of nodes and edges   to 32 .   RNN - based KoPL and SPARQL Parsers . For   KoPL prediction , we first parsed the question to the   sequence of functions , and then predicted textual   inputs for each function . We used Gated Recur-   rent Unit ( GRU ) ( Cho et al . , 2014 ; Chung et al . ,   2014 ) , a well - known variant of RNNs , as our en-   coder of questions and decoder of functions . At-6116   tention mechanism ( Dong and Lapata , 2016 ) was   applied by focusing on the most relavant question   words when predicting each function and each tex-   tual input . The SPARQL parser used the same   encoder - decoder structure to produce SPARQL to-   ken sequences . We tokenized the SPARQL query   by delimiting spaces and some special punctuation   symbols .   I BART KoPL Accuracy of different   # hops   In Table 9 , we presents the BART KoPL accuracy   of different # hops . Note that KQA Pro not only   consider multi - hop relations , but also consider at-   tributes and qualifiers . We count all of them into   the hop number . So in KQA Pro , given a question   with “ 4 - hops ” , it does not mean 4 relations , but may   be 1 relations + 2 attributes + 1 comparison . E.g. ,   “ Who is taller , LeBron James Jr. or his father?”.611761186119