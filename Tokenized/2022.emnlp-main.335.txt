  Hao Peng , Xiaozhi Wang , Shengding Hu , Hailong Jin , Lei Hou ,   Juanzi Li , Zhiyuan Liu , Qun LiuDepartment of Computer Science and Technology , BNRist;KIRC , Institute for Artificial Intelligence ,   Tsinghua University , Beijing , 100084 , ChinaHuawei Noah ’s Ark Lab   { peng - h21 , wangxz20}@mails.tsinghua.edu.cn   Abstract   Conceptual knowledge is fundamental to hu-   man cognition and knowledge bases . However ,   existing knowledge probing works only focus   on evaluating factual knowledge of pre - trained   language models ( PLMs ) and ignore concep-   tual knowledge . Since conceptual knowledge   often appears as implicit commonsense behind   texts , designing probes for conceptual knowl-   edge is hard . Inspired by knowledge represen-   tation schemata , we comprehensively evaluate   conceptual knowledge of PLMs by designing   three tasks to probe whether PLMs organize   entities by conceptual similarities , learn con-   ceptual properties , and conceptualize entities   in contexts , respectively . For the tasks , we col-   lect and annotate 24k data instances covering   393 concepts , which is COPEN , a COncep-   tual knowledge Probing bENchmark . Exten-   sive experiments on different sizes and types   of PLMs show that existing PLMs systemat-   ically lack conceptual knowledge and suffer   from various spurious correlations . We be-   lieve this is a critical bottleneck for realiz-   ing human - like cognition in PLMs . COPEN   and our codes are publicly released at https :   //github.com / THU - KEG / COPEN .   1 Introduction   Pre - trained language models ( PLMs ) have achieved   superior performance on most NLP tasks requir-   ing substantial world knowledge ( Qiu et al . , 2020 ;   Han et al . , 2021 ) . It is interesting and meaning-   ful to probe the extent and scope of world knowl-   edge within PLMs . Existing knowledge probing   works have evaluated PLMs ’ knowledge about en-   tities ( Broscheit , 2019 ; Tenney et al . , 2019a ) and   their relations ( Petroni et al . , 2019 ; Jiang et al . ,   2020 ; Roberts et al . , 2020 ) , i.e. , factual knowledge ,   but ignore conceptual knowledge . Figure 1 : An example knowledge graph . Entities are or-   ganized by concepts through the Instance of relation   and concepts are organized into a taxonomy through the   Subclass of relation . Each concept has certain prop-   erties . Existing work only probes factual knowledge   in entity graphs , ignoring conceptual knowledge in the   concept taxonomy and Instance of relation .   Conceptual knowledge , especially the abstrac-   tion ability , is fundamental to all kinds of hu-   man cognition ( Carey , 1991 ; Collins and Olson ,   2014 ) including language processing ( Waxman and   Markow , 1995 ; Wellsby and Pexman , 2014 ) . Just   as the quote of psychologist Gregory Murphy , con-   cepts are the glue that holds our mental world   together ( Murphy , 2004 ) . Moreover , knowledge   bases ( Suchanek et al . , 2007 ; Auer et al . , 2007 ;   Vrande ˇci´c , 2012 ) organize massive entities via con-   cept taxonomies as illustrated in Figure 1 , which   enable broad applications ( Lv et al . , 2018 ; Zhou   et al . , 2021 ) . Therefore , probing whether PLMs   have human - like conceptual knowledge is neces-   sary in knowledge probing .   Inspired by the conceptual schema in knowledge   representations ( Sowa , 1976 ; Decker et al . , 2000 ;   McGuinness et al . , 2004 ; Antoniou and Van Harme-   len , 2004 ) , we comprehensively evaluate the con-   ceptual knowledge of PLMs by asking three ques-   tions : Do PLMs organize entities by conceptual5015similarities ? Do PLMs know the properties of con-   cepts ? Can PLMs correctly conceptualize entities   in contexts ? In this paper , we design three probing   tasks for these questions : ( 1 ) The conceptual simi-   larity judgment ( CSJ ) task studies whether PLMs   organize entities by conceptual similarities , which   is the basis of understanding concepts . Given a   query entity , CSJ requires PLMs to choose the most   conceptually similar entity among candidate enti-   ties . For example , in Figure 1 , given Dolly as the   query entity , although UKhas a direct relation and   more co - occurrences with it , PLMs should choose   Grumpy Cat . ( 2 ) The conceptual property judg-   ment ( CPJ ) task probes whether PLMs have the   knowledge of conceptual properties , which are the   generic abstractions of factual knowledge . Given a   statement about a specific property , such as “ have   feathers ” , CPJ requires PLMs to judge whether it   is true for a specific concept and also a concept   chain , which evaluates whether PLMs understand   the property transitivity among a chain of hierar-   chical concepts . ( 3 ) The conceptualization in con-   texts ( CiC ) task evaluates the abilities of PLMs   to correctly conceptualize entities within contexts .   Given an entity mentioned in a specific context ,   PLMs are required to choose the most appropri-   ate concept in a concept taxonomy according to   its context . CiC requires not only disambiguating   entity mentions , but also distinguishing superordi-   nate and subordinate concepts . For instance , given   the context “ Dolly is running on the grassland ” ,   PLMs should conceptualize Dolly as an Animal   since there is no enough evidence for Mammal .   Based on the above considerations , we con-   struct a conceptual knowledge probing benchmark ,   COPEN , which contains a concept taxonomy with   446 concepts and high - quality data of 24 K in-   stances for the three probing tasks . The concept   taxonomy is curated by experts based on DBpe-   dia ( Auer et al . , 2007 ) and Wikidata ( Vrande ˇci´c   and Krötzsch , 2014 ) to form a well - defined hierar-   chy and cover broad entities . The data instances   for three tasks are collected by aligning entities   in Wikidata and sentences in GenericsKB ( Bhak-   thavatsalam et al . , 2020 ) , Wikipedia , and Simple   Wikipediainto the concept taxonomy and then   manually annotated by crowd - sourcing annotators .   We conduct extensive experiments on COPEN   to evaluate various widely - used language mod - els ( LMs ) , which include three types : masked   LMs ( Devlin et al . , 2019 ; Liu et al . , 2019b ) , autore-   gressive LMs ( Radford et al . , 2019 ; Black et al . ,   2021 ) , and sequence - to - sequence LMs ( Lewis   et al . , 2020 ; Raffel et al . , 2020 ) . We conduct the   experiments in three settings : ( 1 ) zero - shot prob-   ing , which reformulates the probing tasks into pre-   training objectives and lets PLMs score answers   without any training ( Petroni et al . , 2019 ) ; ( 2 ) linear   probing , which only tunes additional linear classifi-   cation heads and uses them to handle probing tasks   with the frozen representations produced by PLMs ;   ( 3 ) fine - tuning , which tunes all the PLM parame-   ters . Experiments show that existing PLMs achieve   non - trivial performance but still significantly un-   derperform ordinary persons on all three probing   tasks . Further analyses show that PLMs suffer from   spurious correlations like word co - occurrences and   out - of - context predictions , and increasing model   scale brings marginal improvements .   To summarize , our contributions are three - fold :   ( 1 ) We propose to probe PLMs for conceptual   knowledge , which has long been ignored , and de-   sign three probing tasks inspired by the knowledge   representation works . ( 2 ) We construct COPEN ,   a probing benchmark containing high - quality con-   cept taxonomy and probes . ( 3 ) We empirically   show that existing PLMs systematically lack con-   ceptual knowledge and analyze the reasons . We   hope our benchmark and findings could facili-   tate further research on concept - aware PLMs and   human - like language understandings .   2 COPEN Benchmark   In this session , we introduce our COPEN bench-   mark , including the construction of the concept   taxonomy ( § 2.1 ) and the datasets for three prob-   ing tasks ( § § 2.2 to 2.4 ) . More construction and   annotation details are shown in appendix D.   2.1 COPEN Concept Taxonomy   Designing the three probing tasks takes inspira-   tion from concept schemata in knowledge rep-   resentations ( Decker et al . , 2000 ; McGuinness   et al . , 2004 ) , which are widely used in knowledge   graphs ( Suchanek et al . , 2007 ; Auer et al . , 2007 ;   Vrande ˇci´c , 2012 ) . In general , it uses the instance   ofrelation to link the entities ( specific instances )   into abstract concepts , and uses the subclass of   relation to organize the concepts into a taxonomy .   Each concept has certain properties describing it as5016   the example shown in Figure 1 .   To support probing dataset construction , we man-   ually curate a concept taxonomy based on DBpe-   dia ( Auer et al . , 2007 ) and Wikidata ( Vrande ˇci´c   and Krötzsch , 2014 ) in 3steps : ( 1 ) Obtain a basic   taxonomy from DBpedia . We extract the frequent   concepts of DBpedia , which are the concepts with   at least 5instances , and keep the subclass of   relations between them . ( 2 ) Align DBpedia and   Wikidata . For each DBpedia concept , we man-   ually find its equivalent Wikidata item and then   use the subclass of ( P279 ) relations in Wiki-   data to expand the concept taxonomy and use the   instance of ( P31 ) relations to link massive Wiki-   data entities into the concepts . ( 3 ) Simplify the   taxonomy . We further remove some unusual con-   cepts to simplify the taxonomy by the guidance   from Schema.org ( Guha et al . , 2016 ) . For example ,   Person is a sub - concept of Animal , Eukaryote ,   andSpecies in DBpedia , which is reasonable but   inconvenient for real - world applications . Follow-   ing Schema.org , we set Person as a top - level con-   cept in the taxonomy . Finally , we achieve a tree-   structure concise concept taxonomy , which con-   tains 446concepts covering 45million Wikidata   entities . There are 23top - level concepts , and we   use11of them and their sub - concepts for construct-   ing training and development datasets as well as   the other concepts for the testing datasets .   2.2 Conceptual Similarity Judgment   The conceptual similarity judgment ( CSJ ) task is   a multiple - choice classification task , which probes   whether PLMs organize entities by conceptual sim-   ilarities , i.e. , whether PLMs learn the instance   ofrelation . Given a query entity , CSJ requires   PLMs to choose the most conceptually similar en-   tity ( instance of the same superordinate concept )   among some candidates . As in Figure 2 ( a ) , PLMs   should choose Pohang Steelers forInter Milan   since they are both football clubs , although Milan   andInter Milan co - occur more frequently . The   conceptual similarity here is similar to the cohy-   ponym relation in lexical semantics ( Cruse , 1986 ) ,   which has been shown to be distinct from but eas-   ily influenced by spurious co - occurrence associa-   tions ( Hill et al . , 2015 ) . Thus we need to control the   influence of co - occurrences to get faithful results .   Data Collection The data for CSJ is collected   in two steps : ( 1 ) Automatic collection . We first   sample 174concepts that are not subordinates to   each other . Then we retrieve 50Wikidata entities   most frequently showing up in the Wikipedia cor-   pus for each concept , and then build data instances   by combining them . Each instance consists of a   query entity , an answer entity of the same concept ,   and20distractor entities , among which 5are hard   distractors of concepts sharing superordinates with   the concept of query entity . To check the data qual-   ity , we sample 200instances and find little noise .   ( 2 ) Co - occurrence - based filtering . To reduce the   influence of co - occurrences , we need to filter out   the instances that can be easily solved with co-5017occurrences . Lastra - Díaz et al . ( 2019 ) show that   Glove word embedding ( Pennington et al . , 2014 )   contains rich word co - occurrence information but   limited cohyponym knowledge . Hence we use it   to filter out instances with higher word similarity   between the query and answer entity than distrac-   tor entities . We finally get 9,487instances , each   including a query entity and 21candidate entities .   The statistics of data subsets are shown in Table 1 .   2.3 Conceptual Property Judgment   The conceptual property judgment ( CPJ ) task is a   binary sentence classification task , which probes   whether PLMs know the properties of concepts .   Given a statement describing a certain conceptual   property , PLMs are required to judge whether it is   true . For example in Figure 2 ( b ) , PLMs should   predict “ true ” for the statement instance Mammals   raise their young on milk .   Besides evaluating CPJ at instance level , which   reflects the PLMs ’ knowledge about properties for   different individual concepts , we also set a chain-   level evaluation , in which a PLM correctly judges   a property if and only if it correctly judges the   property for every concept in a concept chain . As   the example in Figure 2 ( b ) , a concept chain is a   chain of concepts connected with the subclass   ofrelation in order . The chain - level evaluation   evaluates whether PLMs understand the transitivity   of conceptual properties . It means that a property   holds for a concept also holds for its subordinate   concepts , but may not hold for its superordinate   concepts like the case in Figure 2 ( b ) .   Data Collection The data for CPJ is collected   in two steps : ( 1 ) Automatic collection . For each   concept in our taxonomy , we align it with the   statements of GenericsKB ( Bhakthavatsalam et al . ,   2020 ) , a high - quality knowledge base for naturally   occurring generic statements , by lexical matching   so as to get positive instances . Then we replace the   concept mention with other concept names to ob-   tain negative instances . ( 2 ) Human annotation . To   ensure data quality , we invite annotators to check   whether the instances are correctly labeled , gram-   matically correct , and describing concept proper-   ties . All annotators are well - trained and pass a qual-   ification before annotation . We finally get 8,855   instances for CPJ and the statistics of data subsets   are shown in Table 1 . Additionally , the final test   data includes 102concept chains and correspond-   ing properties used for chain - level evaluation.2.4 Conceptualization in Contexts   The conceptualization in contexts ( CiC ) task is a   multiple - choice classification task , which probes   whether PLMs can correctly conceptualize entities   within contexts . Given an entity mentioned in a   specific sentence , PLMs are required to choose the   most appropriate concept among a concept chain ,   which is a chain of concepts connected with the   subclass of relation in order . This requires PLMs   to understand the subclass of relation and cap-   ture the subtle differences of different - level con-   cepts in a hierarchy . For example in Figure 2   ( c ) , given the sentence Dolly is running on the   grassland . and a concept chain Horse – > Mammal   – > Animal , PLMs shall choose Animal forDolly   since the context do not support more fine - grained   concepts . Sometimes the entity is of multiple con-   cept chains , for example , Jimmy Carter is both   aWriter and a Politician , which additionally   requires PLMs to disambiguate .   Data Collection The data for CiC is collected   in two steps : ( 1 ) Sentence collection . For each   concept , we first retrieve 10Wikidata entities most   frequently showing up in the Wikipedia corpus .   Among the retrieved entities , we only keep the enti-   ties linked with the concept chains containing more   than one concepts and collect 5sentences for each   of them from Wikipedia and SimpleWiki , which   provides various contexts for conceptualization . A   sentence , together with an entity mentioned in the   sentence and concept chains of the entity , consti-   tutes an instance . ( 2 ) Human annotation . We then   organize crowd - sourcing annotation to obtain the   labels . All annotators are well - trained and quali-   fied . We finally get 5,978instances for CiC and the   statistics of data subsets are shown in Table 1 .   3 Evaluation Setup   We introduce the various widely - used PLMs inves-   tigated in our experiments ( § 3.1 ) and the three   adopted probing methods ( § 3.2 ) .   3.1 Investigated PLMs   We investigate three mainstream types of PLMs : ( 1 )   Masked LM , including BERT ( Devlin et al . , 2019 ) ,   which is pre - trained with the bidirectional masked   language modeling and next sentence prediction   objectives , and RoBERTa ( Liu et al . , 2019b ) , which   is a robustly optimized version of BERT . ( 2 ) Au-   toregressive LM , including GPT-2 ( Radford et al . ,5018   2019 ) , which is pre - trained with the unidirectional   left - to - right language modeling objective , and GPT-   Neo ( Black et al . , 2021 ) , which adopts the same ob-   jective but improves some implementation details .   ( 3)Sequence - to - sequence LM , which adopts the   encoder - decoder architecture . This type includes   BART ( Lewis et al . , 2020 ) , which is pre - trained   with the text infilling and sentence permutation   objectives , and T5 ( Raffel et al . , 2020 ) , which is   pre - trained with the span - corruption objective and   multiple downstream tasks .   In § 4 , we report the results of the frequently-   used BASE versions of these PLMs , and results   for the other versions are shown in appendix C.   3.2 Probing Method   Zero - Shot Probing reformulates probing tasks to   the format of pre - training language modeling objec-   tives ( Liu et al . , 2021a ) so that PLMs can do these   tasks without any training . It is widely adopted   by knowledge probing work ( Petroni et al . , 2019 ;   Tenney et al . , 2019a ) since it prevents PLMs from   learning new knowledge from training data so that   the achieved performance reflects PLMs ’ intrin-   sic knowledge . Hence the performance of zero-   shot probing is commonly interpreted as the lower   bound of PLMs ’ knowledge ( Jiang et al . , 2020 ) .   As illustrated in Figure 2 , for each data instance   of the three probing tasks , we cast its choices into   natural language prompts by filling them into man-   ually designed templates , and then let PLMs score   the prompts by the likelihood of language model-   ing . The choice with the highest score is regarded   as the predicted answer of PLMs . Some implemen-   tation details like taking which parts of the prompts   into scoring calculation may influence the PLMs ’   performance . We search these details with prelimi-   nary trials and only report the performance of thebest configuration in experiments .   Linear Probing adds an additional shallow lin-   ear classifier on top of the output contextualized   representations of PLMs , and only trains the addi-   tional classifier while keeping the PLMs ’ parame-   ters fixed . Since the model capacity of the shallow   linear classifier is too limited to fit the tasks , the   achieved performance shall mainly come from the   knowledge in the PLMs ’ representations ( Alain   and Bengio , 2017 ) . Hence linear probing is widely   used in knowledge probing ( Tenney et al . , 2019b ;   Hewitt and Manning , 2019 ) .   Fine - Tuning is the standard method to adapt   PLMs to downstream tasks , which trains all the   PLMs ’ parameters on the training data with task-   specific objectives . Considering the strong model   capacity of the PLMs , PLMs will inevitably fit   the probing tasks through the information in train-   ing data rather than only resort to their intrinsic   knowledge . Hence the fine - tuning performance   shall serve as an upper bound of the PLMs ’ con-   ceptual knowledge in our experiments .   For CSJ and CiC , we take the filled prompts of   identical templates in zero - shot probing as inputs   and train PLMs with the cross - entropy loss . For   CPJ , we take the property statements as inputs and   use the binary cross entropy loss .   More detailed implementations about three prob-   ing methods are shown in appendix A.   4 Experiment and Analysis   We first introduce the overall results in § 4.1 and   conduct detailed analyses on the three probing tasks   ( § § 4.2 to 4.4 ) , respectively . We then analyze the   performance at different model scales ( § 4.5 ) . More   observations and discussions on experimental re-   sults are placed in appendix B.5019   4.1 Overall Results   The overall experimental results are shown in Ta-   ble 2 , from which we can observe that : ( 1 ) All the   PLMs can achieve non - trivial ( better than random   guess ) performance on all the probing tasks with   zero - shot probing or linear probing , which indi-   cates that existing PLMs capture a certain concep-   tual knowledge with pre - training on massive texts .   ( 2 ) However , even with fine - tuning , all PLMs ’ ac-   curacies are still well below human performance ,   which urges further efforts on concept - aware pre-   training . ( 3 ) The accuracies of PLMs using differ-   ent types of pre - training objectives are generally   on the same level . It suggests that any existing   pre - training objective has no special advantages in   understanding concepts and further improvements   may come from targeted pre - training design . We   provide some analyses in the following sections to   help targeted concept - aware PLMs development .   4.2 Conceptual Similarity Judgment   We analyze the predictions and performance of   various PLMs on CSJ , and find that :   PLMs better distinguish coarse - grained con-   cepts . As mentioned in § 2.2 , among 20distrac-   tor entities , 5of them are hard distractors of con-   cepts sharing superordinates with the concept of   the query entity , and the others are easy distrac-   tors . For example , if the query entity is of Mammal   concept , the entities of Bird concept are hard dis-   tractors and the entities of Country concept are   easy distractors . Table 3 shows the mean reciprocal   ranks of these two kinds of distractors . We can   see that the hard distractors are significantly ranked   higher than easy distractors , which indicates that   PLMs generally better distinguish coarse - grained   concepts , such as telling the differences between   Animal andCountry , but fail in distinguishing fine-   grained concepts . It suggests that future methods   should focus more on how to capture the subtle   differences between fine - grained concepts .   4.3 Conceptual Property Judgment   We analyze the error cases on CPJ and find that :   Conceptual transitivity challenges PLMs . Ta-   ble 2 shows that PLMs can achieve high instance-   level accuracies , but all perform poorly in the chain-   level evaluation . It suggests that PLMs can rela-   tively well recall the properties for individual con-   cepts like recalling the facts about entities in factual   knowledge probing , but hardly understand the hier-   archical relations of concepts and the property tran-   sitivity . It suggests that further PLM works should   not only focus on better memorizing knowledge but   also consider how to better organize knowledge .   PLMs have conceptual hallucination .It has   been observed that PLMs frequently generate non-   sensical and unfaithful outputs , which are factu-   ally incorrect , and previous work ( Rohrbach et al . ,   2018 ; Reiter , 2018 ; Ji et al . , 2022 ) dubs this phe-   nomenon as hallucination . In our experiments , we   observe that many PLMs ’ failure cases on CPJ task   can be described as conceptual hallucination , i.e. ,   PLMs hallucinate that concepts have certain proper-   ties while they actually do not . As shown in Table 4 ,   the errors of most PLMs are generally mainly from   making false positive predictions , i.e. , taking false   conceptual property statements as true . It suggests   that PLMs tend to hallucinate the false conceptual   properties as true rather than can not recall the true   conceptual properties , which is interesting and we   further explore whether there are certain spurious   correlations causing this .   Word co - occurrence causes conceptual hal-   lucination . We hypothesize that the word co-   occurrence in the pre - training corpora causes   PLMs ’ conceptual hallucination . For example , if   a PLM has seen the text “ The temple ’s Jufu Hall   was included in the 1998 World Monuments Watch   by the World Monuments Fund ( WMF ) ... preser-   vation of the painted decoration ” , it may be more5020   likely to predict the statement “ Monuments are   used for decoration ” as true . We empirically   find pieces of evidence supporting this hypothe-   sis . For each CPJ instance , to assess the word   co - occurrence in pre - training corpora , we retrieve   the most similar document of it from Wikipedia ,   which is a widely - used corpus in pre - training , with   the BM25 ( Robertson et al . , 1995 ) algorithm im-   plemented in Whoosh ( Mchaput , 2016 ) , and use   the BM25 score of the top one of retrieved docu-   ments as the indicator of this CPJ instance ’s word   co - occurrence rate in pre - training corpus . We di-   vide the negative instances of CPJ dataset into dif-   ferent subsets by their BM25 scores and observe   the false positive rate of BERT ’s fine - tuning pre-   dictions on them . The results are plotted in Fig-   ure 3 , from which we can see that the false positive   prediction rates , indicating conceptual hallucina-   tion , have strong positive correlations to the BM25   scores , indicating word co - occurrence . This sug-   gests that the conceptual hallucination of PLMs   comes from capturing the spurious correlations of   word co - occurrence in pre - training , and further pre-   training work shall explore to fix it .   4.4 Conceptualization in Contexts   We analyze the error cases on CiC and find that :   PLMs conceptualize entities over - relying on   memories . In CiC , we find that if we remove   the contexts , PLMs can still predict a possibly   correct concept , which is similar to previous   works ( Petroni et al . , 2019 ; Roberts et al . , 2020 ;   Cao et al . , 2021 ) showing that PLMs memorize a   certain knowledge about entities ’ types . We dub   these predictions out - of - context predictions , which   can be regarded as the PLMs ’ memories obtained   in pre - training . What we evaluate in CiC is the   in - context conceptualization abilities rather than   the memorized knowledge about the concepts of   entities , which is evaluated by CSJ . Hence rely-   ing on the memories and making out - of - context   predictions are wrong for handling CiC. However ,   as shown in Table 5 , in most of the error cases ,   PLMs wrongly conceptualize the entities within   contexts as the default out - of - context predictions .   It demonstrates that PLMs conceptualize entities   by over - relying on memories rather than under-   standing the contexts , which reflects the lack of   genuine conceptualization abilities . We encourage   future works to study whether the memories inhibit   learning to conceptualize during pre - training .   Understanding hierarchy is more difficult than   disambiguation . In Table 6 , we analyze the two   error types on CiC task . Disambiguation indicates   the PLM selects a wrong concept chain for the   given entity and Wrong Level indicates the PLM   selects a wrong - level concept in the correct chain .   In the analysis , we only consider entities with more   than one concept chain . The Wrong Level errors   take up the majority , which shows that understand-   ing concept hierarchy is more difficult than disam-   biguation for PLMs and how to teach the PLMs to   understand it is essential .   4.5 Analysis on Model Scale   Inspired by recent advances showing the superior   advantages of large - scale models ( Kaplan et al . ,   2020 ; Lester et al . , 2021 ) , we explore how the   model scale influences PLMs ’ conceptual knowl-   edge . We investigate the family of three repre-   sentative PLMs : BERT , GPT-2 and T5 . Since   fine - tuning extremely - large PLMs is too compu-   tationally expensive , for models with more than   2.5billion parameters , we instead adopt BitFit ( Za-   ken et al . , 2022 ) , which can achieve similar perfor-   mance to fine - tuning ( He et al . , 2021 ) but requires   much less computation . The results are shown in   Figure 4 , and we have following observations : ( 1 )   Larger - scale PLMs generally achieve better perfor-   mance on all the probing tasks , which suggests that   increasing model scale can store more conceptual5021   knowledge . However , the improvements brought   by increasing model scale are generally marginal ,   especially on CiC task , and the improvements in   zero - shot probing and linear probing results are   not so obvious like in fine - tuning , which poses a   question that whether the fine - tuning improvements   come from the intrinsic knowledge of PLMs . ( 2 )   The fine - tuning accuracies of T5with 11bil-   lion parameters , are still well below ordinary peo-   ple , which demonstrates that acquiring conceptual   knowledge is quite challenging for existing pre-   training methods , which encourages further efforts   on building concept - aware PLMs .   5 Related Work   Knowledge Probing To understand the success   of PLMs , extensive works explore to know what   PLMs know , and find PLMs have strong linguis-   tic knowledge ( Liu et al . , 2019a ; Hewitt and Man-   ning , 2019 ; Tenney et al . , 2019b ; Vuli ´ c et al . , 2020 ) .   Moreover , it has been shown that PLMs have a cer-   tain world knowledge , which is typically stored   in world knowledge bases , such as the knowl-   edge about entities ( Broscheit , 2019 ; Tenney et al . ,   2019a ) and their relationships ( Petroni et al . , 2019 ;   Roberts et al . , 2020 ; Jiang et al . , 2020 ; Bouraoui   et al . , 2020 ; Zhong et al . , 2021 ) . However , these ex - plorations are limited in the scope of factual knowl-   edge , ignoring the conceptual knowledge , which   is essential for both knowledge bases ( Wu et al . ,   2012 ; Ji et al . , 2019 ) and intelligence ( Carey , 1991 ;   Collins and Olson , 2014 ) . Hence we explore the   conceptual knowledge probing in this paper .   Conceptual Knowledge in PLMs Previous   works also explore the concept in PLMs ( Michael   et al . , 2020 ; Talmor et al . , 2020 ; Aspillaga et al . ,   2021 ; Dalvi et al . , 2021 ) , which study principally   similar topics with us . However , the concept they   refer to is essentially word sense . They focus on   whether PLMs discover the word senses and rec-   ognize their hierarchical relations . While in this   work , we study the concepts defined in knowledge   bases to abstract real - world entities , which support   broader applications ( Lv et al . , 2018 ; Zhou et al . ,   2021 ; Zeng et al . , 2021 ) , and probe knowledge   about conceptual similarity and properties of con-   cepts as well as PLMs ’ conceptualization ability .   6 Conclusion and Future Work   In this paper , we systematically analyze the concep-   tual knowledge in existing PLMs by constructing a   high - quality conceptual knowledge probing bench-   mark ( COPEN ) . Extensive experiments show that5022existing PLMs have a certain conceptual knowl-   edge , but are significantly worse than humans ,   even with billions of parameters . We further find   that PLMs fail in distinguishing fine - grained con-   cepts and understanding concept hierarchy , and   suffer from conceptual hallucination caused by   word occurrence and out - of - context bias . In the fu-   ture , inspired by works infusing factual knowledge ,   we will try to develop conceptual knowledgeable   PLMs by exploring concept - aware pre - training ob-   jectives and knowledge - enhanced architectures .   Limitations   In the section , we discuss the limitations of this   work : ( 1 ) COPEN benchmark . COPEN only in-   volves English corpora , which limits the use of the   benchmark to PLMs pre - trained on other languages .   In the future , we will consider more languages and   construct multilingual COPEN . ( 2 ) Large PLMs .   We do not experiment on very large PLMs , such as   GPT-3 ( Brown et al . , 2020 ) and PaLM ( Chowdhery   et al . , 2022 ) , due to our limited access to them . We   conduct experiments on T5with 11 billion pa-   rameters instead . Experimental results demonstrate   that acquiring conceptual knowledge is quite chal-   lenging for existing pre - training methods , which   urges concept - aware pre - training objectives and   model architectures . ( 3 ) Environmental impact .   In this paper , we conduct a lot of experiments with   various PLMs , some of which even contain several   billions of parameters . It consumes large amounts   of energy and causes large amounts of carbon diox-   ide emissions , which incurs negative influence to   our environment ( Strubell et al . , 2019 ) . But the   experiments are necessary for drawing faithful and   comprehensive conclusions . We hope our findings   could facilitate further research on more powerful   PLMs with fewer parameters .   Ethical Considerations   We discuss the ethical considerations and broader   impact of this work in this section : ( 1 ) Intellec-   tual property . The Wikipedia , Simple Wikipedia   corpora , and Wikidata are obtained from the Wiki-   media dump , which is shared under the CC BY-   SA 3.0 license . The DBpediais shared under   the CC BY - SA 3.0 license and GNU Free Docu - mentation License . The GenericsKB corpusis   shared under the CC BY 4.0 license . These are   all public and established resources , which are in-   tended to support broad artificial intelligence and   NLP research . We believe these resources are well   desensitized and anonymized . ( 2 ) Data annota-   tion . We invite 19annotators without background   of expertise to annotate our datasets and produce   human performance . They are all employed by   commercial data production companies . The in-   vited annotators are fairly paid according to agreed   working hours and prices . The annotators are all   informed about how the data will be processed ,   used , and released , and this is confirmed in the data   production contract . ( 3 ) Intended use . COPEN   is a high - quality benchmark used for evaluating   conceptual knowledge in PLMs and developing   concept - knowledgeable PLMs . Researchers can   use COPEN to assess new concept - aware objec-   tives and conceptual - knowledge - enhanced archi-   tectures . ( 4 ) Misuse risks . Considering COPEN is   built on top of a limited scope of natural texts and   the probing methods are inevitably influenced by   some spurious correlations , a good enough perfor-   mance on COPEN can not fully guarantee that the   developed methods really understand concepts and   shall not be used to support relevant commercial   and political claims . ( 5 ) Potential risks control .   The texts in COPEN are from public data and do   not involve private information , sensitive topics   and social issues . The three tasks in COPEN also   do not involve sensitive topics or social issues . We   manually check some randomly sampled instances   in COPEN and find no sensitive information or   other risky issues . Hence we believe that COPEN   does not create additional risks .   Acknowledgements   This work is supported by the Key - Area Research   and Development Program of Guangdong Province   ( 2019B010153002 ) , the Institute for Guo Qiang ,   Tsinghua University ( 2019GQB0003 ) , and Huawei   Noah ’s Ark Lab . The authors thank all the anony-   mous reviewers for their detailed and valuable com-   ments and suggestions . The authors also thank all   the annotators for their substantial efforts in the   annotation process.5023References502450255026Appendices   A Implementation Details   We use the implementation code and pre - trained pa-   rameters of PLMs released in HuggingFace Trans-   formers library ( Wolf et al . , 2020 ) to run our experi-   ments . The model_name s we used in Transformers   for different PLMs are shown in Table 7 . We run   experiments for large models ( T5 , and T5 ) on   NVIDIA V100 GPUs , which approximately con-   sumes 160 GPU hours , and the other PLMs on   Nvidia GEFORCE RTX 3090 GPUs , which con-   sumes about 300 GPU hours . We will introduce   the implementation details for zero - shot probing   ( appendix A.1 ) , linear probing ( appendix A.2 ) , and   fine - tuning ( appendix A.3 ) .   A.1 Zero - Shot Probing   As mentioned in § 3.2 , we take different text parts   of the prompts into scoring calculation . Table 8   shows the text parts used by various PLMs to score   prompts on the three datasets .   A.2 Linear Probing   We use the final outputs of specific tokens as the   features extracted by PLMs : [ CLS ] for BERT ; < s >   for RoBERTa ; the last token for GPT-2 , GPT - Neo ,   and BART ; the first token for T5 . We then tune a   lightweight linear classifier on the fixed features   for BERT , RoBERTa , GPT-2 , GPT - Neo , BART and   tune the final vocabulary classification head for T5 .   Moreover , we reformulate the original instances   into the text - to - text format for T5 , and the input   and output formats are shown in Table 9 .   Hyperparameters We set the learning rate as   1×10and apply early stopping ( Prechelt , 1996 )   on the accuracy on the development dataset with a   patience of 20 epochs . We keep the other hyperpa-   rameters the same as in Table 10 .   A.3 Fine - Tuning   We follow the fine - tuning methods in original   papers to fine - tune BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019b ) , GPT-2 ( Radford   et al . , 2019 ) , GPT - Neo ( Black et al . , 2021 ) , and   BART ( Lewis et al . , 2020 ) . As in appendix A.2 ,   we reformulate the original instances into the text-   to - text format for T5 ( Raffel et al . , 2020 ) , and the   input and output formats are shown in Table 9 .   Hyperparameters We follow the hyperparame-   ters mostly used in previous literature . The hyper-   parameters are shown in Table 10 . And we apply   early stopping ( Prechelt , 1996 ) on the accuracy on   the development dataset .   Parameter - efficient Tuning for Big Models   Due to the limits of computation , we consider the   parameter - efficient tuning for models with more   than2.5billion parameters ( T5and T5 ) . Pre-   vious works ( He et al . , 2021 ) have proven that   parameter - efficient tuning methods can save GPU   memory , accelerate training for PLMs , and achieve   comparable performance to fine - tuning all parame-   ters , especially at large scales . Therefore , we adopt   BitFit ( Zaken et al . , 2022 ) implemented by Open-   Deltato fine - tune big models .   B More Discussions on Experimental   Results   In the section , we discuss some detailed and inter-   esting observations.502750285029Comparison of Pre - training Method In Fig-   ure 2 , we can observe that : ( 1 ) For PLMs using   the same architecture , T5 generally outperforms   BART , and BERT generally outperforms RoBERTa .   The differences may come from the different pre-   training corpora . ( 2 ) Autoregressive LMs ( GPT-2 ,   GPT - Neo ) perform worse on CSJ , which is con-   sistent with the observations on factual knowledge   probing ( Liu et al . , 2021b ) . As we are the first to   study conceptual knowledge in PLMs , we focus   on the general question “ to what extent do current   PLMs understand conceptual knowledge ? ” and   provide more general conclusions in the paper . We   leave the detailed and in - depth analysis of a spe-   cific PLM , e.g. , layer - wise analysis ( Dalvi et al . ,   2021 ) , in future works .   Comparison of Probing Method Intuitively ,   zero - shot probing reflects the lower bound of   PLMs ’ knowledge ( Jiang et al . , 2020 ) , while linear   probing learns a task - specific linear classifier and   performs better than zero - shot probing , and fine-   tuning reflects the upper bound of PLMs ’ knowl-   edge . However , as shown in Figure 2 , linear prob-   ing sometimes underperforms zero - shot probing ,   especially in CSJ and chain - level CPJ . The reason   may be that the concepts used for training and test-   ing are disjoint , and linear probing involves train-   able parameters , which may learn spurious or shal-   low correlations on training sets and hence strug-   gles on generalization . Meanwhile , fine - tuning still   performs poorly , which demonstrates that existing   PLMs systematically lack conceptual knowledge .   Comparison of Instance - Level and Chain - Level   CPJ For chain - level , BERT performs the best ,   but for instance - level performs worse than T5 . The   reason may be that BERT better understands con-   cept transitivity ( i.e. , making more consistent pre-   dictions ) but stores fewer conceptual properties   overall . A thorough and comprehensive analysis   is needed on this phenomenon and we leave it in   future works .   C Additional Experimental Results   Table 11 shows overall zero - shot probing results on   COPEN . The experimental results of linear prob-   ing and fine - tuning are obtained at 3random trials   using seeds 42,43,44 . Table 12 shows overall   linear probing and fine - tuning results on COPEN .   And we provide additional results for the analytical   experiments : analysis of conceptual hallucination   on the CPJ dataset ( appendix C.1 ) , error analysis   on the CiC dataset ( appendix C.2 ) , and analysis on   avoiding dataset artifacts ( appendix C.3 ) .   C.1 Conceptual Hallucination on CPJ   Figure 5 shows the false negative rates on subsets   with different BM25 scores for various PLMs . We   can observe that the false positive rates , which indi-   cates conceptual hallucination , have strong positive   correlations to the BM25 scores , which indicates   word co - occurrence .   C.2 Error Analysis on CiC   Table 13 shows the proportions of different error   types . We can observe that in most wrong predic-   tions , PLMs select concepts of wrong levels . It   indicates that PLMs lack a comprehensive under-   standing of concept hierarchy and fail to conceptu-   alize entities according to contexts .   C.3 Analysis on Avoiding Dataset Artifacts   Dataset artifacts leak shallow information and   cause the PLMs to learn spurious correlations   rather than exhibit inner knowledge . When con-   struct COPEN , we avoid two kinds of artifacts :   Lexical Overlap means that the query and the an-   swer have word overlap , which may enable PLMs   to make correct predictions using spurious corre-   lations without the correct knowledge . For ex-   ample , in CSJ , if the query entity is Stanford   University and the answer entity is University   of California ; in CiC , if the context is She grad-   uated from Stanford University and the answer   concept is University ; they have lexical overlap .   We conduct experiments on the data with lex-   ical overlap . As shown in Table 14 , on the data   with lexical overlap , PLMs perform much better .   But this should be interpreted as they learn shallow   clues leaked by artifacts since they can not achieve   similar performance on data without lexical over-5030   lap . Hence , we filter out all instances with lexical   overlap in COPEN to avoid this kind of artifact .   Concept Overlap is that the same concepts show   up in both training and test datasets , which may   leak conceptual knowledge , i.e. , the PLMs may   learn some knowledge from training data . In   COPEN , as mentioned in § 2.1 , we split different   top - level concepts and their subconcepts into differ-   ent sub - datasets , so as to avoid concept overlap . To   empirically show the influence of concept overlap ,   we randomly re - split the datasets into same - size   training , development , and test sets and see the   fine - tuning performance on the new split .   The results of fine - tuning BERT are shown in   Figure 6 , and the results of fine - tuning and linear   probing for all PLMs are shown in Table 15 . Fine-   tuning on datasets with concept overlap achieves   much higher accuracies , especially on CSJ . It indi-   cates that if we do not avoid concept overlap , PLMs   can easily learn conceptual knowledge from train-   ing data and lead to false optimistic conclusions .   D COPEN   We provide a detailed introduction to COPEN .   D.1 COPEN Taxonomy   Disjoint Concepts We divide all the concepts   into two disjoint sets : one set containing 11top-   level concepts together with all their sub - concepts   for constructing training and development datasets ,   and the other set containing the other concepts for5031   testing datasets . As shown in Table 16 , there are   248concepts including 11top - level concepts for   training and development datasets and 198con-   cepts including 12top - level concepts for testing .   Concept Hierarchy We present the concepts   for training and development datasets in Figure 7   and the concepts for testing datasets in Figure 8 .   Object is a virtual concept for visualization and is   not included in the overall 446concepts .   D.2 Concept Similarity Judgment   Human Performance We sample 1,000 in-   stances from the testing dataset and invite anno-   tators with no linguistic background to perform the   CSJ task . All the annotators are trained with a few   instances before the evaluation .   Co - occurrence - based Filtering We filter out in-   stances of which query entities and answer entities   have a high association , which are estimated by   cosine similarity of their Glove word embeddings .   Specifically , for a query entity , we sample 5answer   entities and select the entity with the lowest asso-   ciation with the query entity as the answer entity .   Then we choose distractor entities iteratively fol-   lowing the rules : ( 1 ) Sample a distractor entity , ifthe entity has a higher association with the query   entity than the answer entity , then select the distrac-   tor entity as a candidate entity . ( 2 ) If not , select the   distractor entity as a candidate entity with a 20 %   probability , otherwise start the next iteration until   the number of distractor entities reaches 20 .   D.3 Conceptual Property Judgment   Human Annotation We invite annotators with   no linguistics background to check whether the   instances are correctly labeled , grammatically cor-   rect , and describing concept properties . All an-   notators are well - trained and required to pass a   qualification before the annotation . The instances   originally labeled as false are annotated 4times ,   and the other instances are annotated once . During   the annotation , an author of the paper and another   experienced annotator separately sample 10 % of   the instances to check the quality of annotation .   The acceptance criterion of the annotation is that   the percentage of obvious annotation errors in the   sampled instances ( e.g. , label the statement The sun   has two eyes as true ) does not exceed 3 % , and the   inter - annotator agreement rates exceed 85 % for the   instances annotated 4times . Major voted results of   the instances annotated 4times together with the5032instances annotated once constitute the CPJ dataset .   Human Performance We use the 2,159 in-   stances that are annotated 4times in the testing   dataset to evaluate human performance . We con-   duct a 4 - round evaluation : take the major voted   results of 3annotators as labels and the other one   as human predictions to calculate the accuracy of   the round . The mean accuracy of 4rounds is re-   ported as the human accuracy on the CPJ dataset .   D.4 Conceptualization in Contexts   Human Annotation We invite annotators with   no linguistics background to annotate the dataset .   To ensure quality , all annotators are well - trained   and required to pass a qualification before the an-   notation . All instances are annotated four times .   Moreover , during the annotation , an author of the   paper and another experienced annotator separately   sample 10 % of the examples to check the qual-   ity of annotation . The acceptance criterion of the   annotation is that the percentage of obvious annota-   tion errors ( e.g. , Select Horse forDolly according   to the context Dolly is running on the grassland . )   does not exceed 3 % , and the inter - annotator agree-   ment rates exceed 80 % . Major voted results of the   4annotated results constitute the final CiC dataset .   Human Performance We use all instances in   the testing dataset , which are annotated 4times ,   to evaluate human performance . We conduct a 4-   round evaluation : take the major voted results of 3   annotators as labels and the other one as human pre-   dictions to calculate the accuracy of the round . The   mean accuracy of 4rounds is the human accuracy.503350345035