  Chenkai Sun , Jinning Li , Hou Pong Chan , ChengXiang Zhai , and Heng JiUniversity of Illinois Urbana - ChampaignFaculty of Science and Technology , University of Macau{chenkai5 , jinning4 , czhai , hengji}@illinois.eduhpchan@um.edu.mo   Abstract   Predicting how a user responds to news events   enables important applications such as allow-   ing intelligent agents or content producers to   estimate the effect on different communities   and revise unreleased messages to prevent un-   expected bad outcomes such as social conflict   and moral injury . We present a new task , Re-   sponse Forecasting on Personas for News Me-   dia , to estimate the response a persona ( charac-   terizing an individual or a group ) might have   upon seeing a news message . Compared to   the previous efforts which only predict generic   comments to news , the proposed task not only   introduces personalization in the modeling but   also predicts the sentiment polarity and inten-   sity of each response . This enables more accu-   rate and comprehensive inference on the mental   state of the persona . Meanwhile , the generated   sentiment dimensions make the evaluation and   application more reliable . We create the first   benchmark dataset , which consists of 13,357   responses to 3,847 news headlines from Twit-   ter . We further evaluate the SOTA neural lan-   guage models with our dataset . The empirical   results suggest that the included persona at-   tributes are helpful for the performance of all   response dimensions . Our analysis shows that   the best - performing models are capable of pre-   dicting responses that are consistent with the   personas , and as a byproduct , the task formula-   tion also enables many interesting applications   in the analysis of social network groups and   their opinions , such as the discovery of extreme   opinion groups .   1 Introduction   To prevent the flooding of misinformation and hate   speech on the internet , a great amount of progress   has been made toward identifying and filtering such   content on social media using machine learningFigure 1 : An example illustrating the task . The input   consists of persona attributes ( e.g. , historical activities   and profile ) and a news message . The model is asked to   predict response in multiple dimensions .   models ( Fung et al . , 2021 ; Su et al . , 2022 ; ElSh-   erief et al . , 2021 ; Sap et al . , 2019 ) . While directly   creating message - level labels is a natural way to   address the issue , it is equally important to measure   the influence of the message on different viewers   as a way to decide how to manage the publication   of the messages .   Existing efforts ( Lin and Chen , 2008 ; Giachanou   et al . , 2018 ; Yang et al . , 2019 ; Artzi et al . , 2012 )   have made steps toward predicting population - level   news response ( e.g. , predicting the most likely re-   sponse to a news message ) , but neglected the impor-   tance of personas in measuring influence . Accord-   ing to Individual Differences Theory ( Riley , 1959 ) ,   which proposes that individuals respond differently   to the mass media according to their psychological   needs , the same message can impact different pop-   ulation groups / personas in different ways . For ex-   ample , a message claiming the honor of sacrificing   others ’ lives for a religious goal might agitate peo-   ple who are prone to agreeing with such messages .   It is therefore essential to consider personalization   when inferring viewers ’ responses .   On the other hand , the previous approaches that554   predict text - level responses ( Yang et al . , 2019 ; Wu   et al . , 2021 ; Lu et al . , 2022 ) have only used genera-   tion metrics for automatic evaluation , yet the same   sentiment can be expressed in a multitude of ways ,   and text alignment metrics like BLEU ( Papineni   et al . , 2002 ) and ROUGE ( Lin , 2004 ) do not credit   cases where the sentiments match but semantics do   not align well . As a result , it is crucial to evaluate   the sentiment dimensions of user responses .   We propose Response Forecasting on Personas   for News Media , a task for measuring the influence   of news media messages on viewers by predicting   viewers ’ responses . In particular , the input con-   sists of the news message and persona information   ( e.g. , user profile and history in our dataset ) , and   we define response in terms of sentiment polarity ,   sentiment intensity , and textual response . While we   include three categories in this work , many other   interesting aspects can also be defined ( e.g. , change   of attitude toward real - world entities ) and we leave   them to future work . Studying the problem of   forecasting individual viewers ’ responses allows   the creation of tools to assist analysts and online   content producers to estimate the potential impact   of messages on different communities , and sheds   light on new applications such as automatically   re - writing a message / email to achieve a communi-   cation goal ( e.g. , to obtain a positive response from   the receiver ) . Furthermore , this new task also helps   to understand associations between user attributes   and emotional responses .   To construct a test bed for this task , we collect a   dataset from Twitter consisting of 13,357 labeled   responses to 3,847 news headlines from Twitter .   Using the corpus , we examine how state - of - the - art   neural models work in our task . We find that the   models can predict responses with reasonable ac-   curacy yet still have a large room for improvement .   We also find that the best - performing models are   capable of predicting responses that are consistent   with the personas , indicating that the models may   be used for many exciting applications such as the   discovery of groups with different opinions.2 Dataset Collection   In this section , we describe how we construct data   from Twitter . Specifically , we used Twitter API   to crawl news headlines and comments below each   headline from CNN Breaking News , which is one   of the most popular news accounts on Twitter .   Preprocess . We collected news headlines and cor-   responding comments from CNN Breaking News   between January 2017 and January 2019 and re-   moved the comments that are over 50 tokens to   avoid spamming . We stripped away HTML syntax   tokens and normalized user reference with special   tokens “ @user ” .   2.1 Persona Data   We categorize the users who post comments as   responders . To describe responders , we gathered   various persona attributes from Twitter , including   ( 1 ) User Profile , which is a short paragraph describ-   ing the user , and ( 2 ) User History , which are tweets   written directly by the user . We consider persona   as a representation of an individual or a commu-   nity that characterizes interests and beliefs . User   profiles and history serve as effective indicators   of persona , as they reveal such information well .   Since users ’ behavior is generally influenced by   their personas , we can potentially infer personas   by analyzing data that reflects their behavior . Ad-   ditionally , studying historical tweets helps us un-   derstand users ’ communication styles . To ensure   that future posting activities are not included when   predicting the comment , we collect the historical   posts prior to the earliest data sample in our dataset   for each individual user .   2.2 Annotation   We obtained 14k headline and comment pairs from   preprocessing . In the annotation stage , we collect   labels for sentiment intensity and polarity of com-   ments based on the context of the headline . For   the 10k training instances , we produce automatic la-   bels using deep - learning models trained on existing   message - level datasets . More specifically , we train   a Deberta - based model ( He et al . , 2020 ) using data   from SemEval-2018 Task 1(Mohammad et al . ,   2018 ) , reaching over 85 % Pearson correlation . We   then proceed to use crowd - sourcing to annotate the   remaining 2k samples as our evaluation set.555   Task Setup . The annotation for the evaluation set   is performed using the Amazon Mechanical Turk   ( MTurk ) crowd - sourcing platform . The workers   were each asked to annotate a headline and com-   ment pair with three workers assigned to each data   sample . During the annotation , the annotator is   asked to select the sentiment polarity label and   the intensity of the sentiment based on their under-   standing of the input . The workers select positive ,   negative , or neutral for the sentiment polarity label   and select on the integer scale of 0 to 3 for intensity .   415 workers participated in this task in total and all   annotators are paid a fair wage above the federal   minimum .   Quality Control . To ensure the quality of anno-   tation , we allowed only the workers who have at   least 95 % approval rate and have had at least 5,000   hits approved to access our tasks . We further re-   moved workers who have a < 70 % accuracy in the   first 30 annotations and discarded the assignments   that have completion time deviated from the ex-   pected average largely . We used majority voting   to determine the final labels : if at least two anno-   tators agreed on a label , we chose it as the final   label . The resulting annotated samples achieve an   inter - annotator agreement accuracy of 81.3 % . We   show the statistics of the dataset in Table 1 .   3 Response Forecasting on Personas for   News Media   3.1 Task Formulation   In this task , we aim to predict sentiment polarity ,   sentiment intensity , and textual response from an   individual when the individual sees a message on   news media . Formally , given persona P(repre-   sented by profile , or historical posts ) , and a source   message M , the task is to predict the persona ’s sen-   timent polarity ϕ(i.e . , Positive , Negative , Neutral )   and sentiment intensity ϕ(i.e . , in the scale of 0 toModel Persona Label Context   GPT2 3.18 3.84 2.84   T5 3.68 4.23 3.57   BART 4.35 4.42 3.99   3 ) , and textual expression t. Our goal is to encode   Pand produce ϕ,ϕ , andtat decoding time . We   formulate the task as a conditional generation prob-   lem and use the following maximum - likelihood   objective to train a generative model:/summationdisplaylogp(O|O , P )   where Ois the output string concatenating ϕ,ϕ ,   andtwith special separator tokens .   3.2 Experimental Setup   For deep learning - based text generators , we fine-   tune decoder - only text generator GPT2 ( Radford   et al . , 2019 ) as well as two Encoder - Decoder mod-   els T5 ( Raffel et al . , 2019 ) and BART ( Lewis et al . ,   2019 ) . Greedy decoding is used for all the models   during training . We further perform ablation on the   best - performing model by removing different user   attributes . We further include two naive baselines ,   Random andMajority , for sentiment dimensions ,   where each prediction follows either the majority   label or a random label . Our neural models are   implemented using Pytorch ( Paszke et al . , 2019 )   and Huggingface Transformers ( Wolf et al . , 2020 ) .   The reproducibility and hyperparameter details can   be found in Appendix Table 4 .   3.2.1 Evaluation Metrics   Automatic . We use BARTScore ( Yuan et al . , 2021 ) ,   BLEU ( Papineni et al . , 2002 ) , METEOR ( Baner-556jee and Lavie , 2005 ) , and ROUGE ( Lin , 2004 ) to   evaluate textual response generation performance .   Note that BARTScore computes the log - likelihood   of producing the reference text given the gener-   ated text using a BART model pretrained on Para-   Bank2 . Furthermore , we use Pearson and Spear-   man correlation to evaluate sentiment intensity , and   F1 to evaluate sentiment polarity .   Manual . We conduct human evaluation to mea-   sure the consistency of the generated outputs from   those models . We define three types of consistency   metrics : ( 1 ) persona consistency : whether the out-   put reflects the persona ’s characteristics , ( 2 ) label   consistency : whether the response text and senti-   ment are consistent with each other , ( 3 ) and context   consistency : whether the output is responding to   the input news headline . We randomly select 10   personas with distinct characteristics ( i.e. , the writ-   ing style / interest / profession do not clearly overlap )   and 10 news headlines from distinct topics , and   consequently generate 100 responses using each   model . The samples are distributed to 5 raters who   score each output based on our metrics . The raters   are master students who passed a small quiz of   20 samples with at least 80 % accuracy . We addi-   tionally make sure that each rater is familiar with   the persona information ( e.g. , profile and history )   before starting to work on the task .   3.3 Results   Automatic Evaluation . Across the metrics in Ta-   ble 2 , we can see that BART provides us with the   highest quality response predictions on both senti-   ment and text levels . As expected , the performance   of simple baselines is relatively low compared to   other models , showing that the dataset does not   have a class imbalance issue . While the automatic   generation scores are generally low ( i.e. , words   do not align well ) , the sentiment prediction scores   are much higher in scale , demonstrating the impor-   tance of sentiment scoring to make a fair judgment   of the result ; the model needs to be credited for   correctly predicting the latent sentiment even if it   does not utter the exact sentence . Finally , we ablate   user attribute features one by one . As shown in the   table , not only both features included are effective   for the task , but they are also complementary of   each other .   Human Evaluation . The results from human judg-   ments ( Table 3 ) in general support the automaticevaluation findings . Among all three models , our   approach with BART reaches the highest on all met-   rics , showing it can generate responses of better   quality than others . The difference between models   on Label Consistency is noticeably lower than other   metrics , and the number suggests that pretrained   language models are capable of producing senti-   ment labels consistent with the textual expression .   On the other hand , we find that BART can produce   responses more consistent with the controllable   variables than GPT2 , which might be attributed to   its denoising pretraining ( e.g. , it adapts better to   different modeling formats ) . In fact , the outputs   show that GPT2 hallucinates more often than other   models .   3.4 Application   We hypothesize that the formulation of the task   enables the application of discovering groups with   different opinions on issues . We verify the hypoth-   esis by collecting personas with contrasting stances   on an issue and generating responses based on this   issue . We find that the output from the model stays   consistent with the persona ( examples are shown in   the Appendix Table 5 ) . The result demonstrates the   potential for application on social network analysis .   Since the model is able to generalize to different   personas or news , an analyst can therefore replace   the news headline with others to segment the popu-   lation based on different issues , or manually con-   struct a persona to visualize how a person from   a particular community would respond to certain   issues .   4 Conclusions and Future Work   We propose Response Forecasting on Personas for   News Media , a new task that tests the model ’s ca-   pability of estimating the responses from different   personas . The task enables important applications   such as estimating the effect of unreleased mes-   sages on different communities as an additional   layer of defense against unsafe information ( e.g. ,   information that might cause conflict or moral in-   jury ) . We also create the first dataset for evaluating   this new task and present an evaluation of the state-   of - the - art neural models . The empirical results   show that the best - performing models are able to   predict responses with reasonable accuracy and pro-   duce outputs that are consistent with the personas .   The analysis shows that the models are also able to   generate contrasting opinions when conditioned on557contrasting personas , demonstrating the feasibility   of applying the models to discovering social groups   with different opinions on issues for future work .   In addition to this , an intriguing avenue for fur-   ther research lies in utilizing response forecasting   techniques to predict the popularity of discussion   threads , as explored in previous studies ( He et al . ,   2016 ; Chan and King , 2018 ) .   Limitations   While the training method makes use of user pro-   file description and history , one additional factor   that is important is the structure between users and   news articles . Knowing a user ’s social circles can   often give hints about the user ’s interests and be-   liefs , which can potentially help the model to infer   how a particular persona would respond to an is-   sue . A possible direction is to design a method that   explores the social context features ( e.g. , social   network ) via graph - based algorithms .   Ethics   During annotation , each worker was paid $ 15 per   hour ( converted to per assignment cost on MTurk ) .   If workers emailed us with any concerns , we re-   sponded to them within 1 hour . The research study   has also been approved by the Institutional Review   Board ( IRB ) and Ethics Review Board at the re-   searchers ’ institution . Regarding privacy concerns   our dataset may bring about , we follow the Twitter   API ’s Terms of Useand only redistribute content   for non - commercial academic research only . We   will release pointers to the tweets and user profiles   in the dataset .   Acknowledgement   This research is based upon work supported   in part by U.S. DARPA INCAS Program No .   HR001121C0165 . The views and conclusions   contained herein are those of the authors and   should not be interpreted as necessarily repre-   senting the official policies , either expressed or   implied , of DARPA , or the U.S. Government .   The U.S. Government is authorized to repro-   duce and distribute reprints for governmental pur-   poses notwithstanding any copyright annotation   therein . Hou Pong Chan was supported in part by   the Science and Technology Development Fund ,   Macau SAR ( Grant Nos . FDCT/060/2022 / AFJ , FDCT/0070/2022 / AMJ ) and the Multi - year Re-   search Grant from the University of Macau ( Grant   No . MYRG2020 - 00054 - FST ) .   References558   A Appendix   A.1 Implementation Details   We implement the models using the 4.8.2 version   of Huggingface Transformer library(Wolf et al . ,   2020 ) . We use Oct 1 , 2021 commit version of the   BART - base model ( 139 M parameters ) from Hug-   gingface . We use Huggingface datasetsfor auto-   matic evaluation metrics . The BART Score comes   from the author ’s repositoryand we used the one   trained on ParaBank2 . The hyperparameters for   the experiment are shown in Table 4 ( applied to all   models ) and the ones not listed in the table are set   to be default values from the transformer library . In   order to make the distribution of training and devel-   opment sets align , we used automatically - generated   labelsduring training . We use RAdam ( Liu et al . ,559Name Value   seed 42   learning rate 5e-5   batch size 16   weight decay 5e-4   RAdam epsilon 1e-8   RAdam betas ( 0.9 , 0.999 )   scheduler linear   warmup ratio ( for scheduler ) 0.06   number of epochs 20   metric for early stop SacreBLEU   patience ( for early stop ) 15   length penalty 1.2   beam search size during eval 5   2019 ) as the optimizer . We perform hyperparam-   eter search on the batch size from { 16 , 32 } , pre-   trained language model learning rate from { 3e-5 ,   4e-5 , 5e-5 } . We perform our experiments on 32 GB   V100 . The experiments can take up to 15 hours . Headline : Millions are under a blizzard   warning as a powerful storm is expected to   bring heavy snow , wind and rain to a   large swath of the country   Purity & Love Degradation   We ’re in the northern   part of the country .   Hope everyone is safeMother Nature sure   is pissed off at us   Headline : Judge says Trump may have been   urging supporters to ’ do something more ’   than protest on Jan. 6   Pro - President Trump Anti - President Trump   The liberal media &   Dems are always   negative when it   comes to anything .   They do n’t care   about anything   except themselvesHahahahahahaha !   They figured that   Trump would be   impeachedby now !   But the traitorous   Republicans are   slowing down the   process .   Headline : Russia and Ukraine are at war   Pro - Russia Pro - Ukraine   Support Russia Support Ukraine560ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitation   /squareA2 . Did you discuss any potential risks of your work ?   Ethics   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   2   /squareB1 . Did you cite the creators of artifacts you used ?   Not applicable . Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Ethics   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Ethics   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Ethics   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   2   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   3   C / squareDid you run computational experiments ?   3   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   appendix561 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   3   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   appendix   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   2   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   2   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   2 , Ethics   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Ethics   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.562