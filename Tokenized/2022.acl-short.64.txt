  Miryam de Lhoneux , Sheng Zhang , Anders S√∏gaardUniversity of Copenhagen , DenmarkUppsala University , SwedenKU Leuven , BelgiumNational University of Defense Technology , China   { ml,soegaard}@di.ku.dk , zhangsheng@nudt.edu.cn   Abstract   Large multilingual pretrained language mod-   els such as mBERT and XLM - RoBERTa have   been found to be surprisingly effective for   cross - lingual transfer of syntactic parsing mod-   els ( Wu and Dredze , 2019 ) , but only between   related languages . However , source and train-   ing languages are rarely related , when pars-   ing truly low - resource languages . To close   this gap , we adopt a method from multi - task   learning , which relies on automated curricu-   lum learning , to dynamically optimize for pars-   ing performance on outlier languages . We   show that this approach is signiÔ¨Åcantly better   than uniform and size - proportional sampling   in the zero - shot setting .   1 Introduction   The Ô¨Åeld of multilingual NLP is booming ( Agirre ,   2020 ) . This is due in no small part to large multilin-   gual pretrained language models ( PLMs ) such as   mBERT ( Devlin et al . , 2019 ) and XLM - RoBERTa   ( Conneau et al . , 2020 ) , which have been found to   have surprising cross - lingual transfer capabilities   in spite of receiving no cross - lingual supervision .   Wu and Dredze ( 2019 ) , for example , found mBERT   to perform well in a zero - shot setting when Ô¨Åne-   tuned for Ô¨Åve different NLP tasks in different lan-   guages . There is , however , a sharp divide between   languages that beneÔ¨Åt from this transfer and lan-   guages that do not , and there is ample evidence that   transfer works best between typologically similar   languages ( Pires et al . , 2019 ; Lauscher et al . , 2020,among others ) . This means that the majority of   world languages that are truly low - resource are still   left behind and inequalities in access to language   technology are increasing .   Large multilingual PLMs are typically Ô¨Åne - tuned   using training data from a sample of languages that   is supposed to be representative of the languages   that the models are later applied to . However , this   is difÔ¨Åcult to achieve in practice , as multilingual   datasets are not well balanced for typological di-   versity and contain a skewed distribution of typo-   logical features ( Ponti et al . , 2021 ) . This problem   can be mitigated by using methods that sample   from skewed distributions in a way that is robust to   outliers .   Zhang et al . ( 2020 ) recently developed such   a method . It uses curriculum learning with a   worst - case - aware loss for multi - task learning . They   trained their model on a subset of the GLUE bench-   mark ( Wang et al . , 2018 ) and tested on outlier tasks .   This led to improved zero - shot performance on   these outlier tasks . This method can be applied   to multilingual NLP where different languages are   considered different tasks . This is what we do in   this work , for the case of multilingual dependency   parsing . Multilingual dependency parsing is an   ideal test case for this method , as the Universal   Dependency treebanks ( Nivre et al . , 2020 ) are cur-   rently the manually annotated dataset that covers   the most typological diversity ( Ponti et al . , 2021 ) .   Our research question can be formulated as such :   Can worst - case aware automated curriculum learn-   ing improve zero - shot cross - lingual dependency   parsing?5782 Worst - Case - Aware Curriculum   Learning   In multi - task learning , the total loss is generally the   average of losses of different tasks :   min`( ) = min1   nX`( ) ( 1 )   wherelis the loss of task i. The architecture   we use in this paper is adapted from Zhang et al .   ( 2020 ) , which is an automated curriculum learning   ( Graves et al . , 2017 ) framework to learn a worst-   case - aware loss in a multi - task learning scenario .   The architecture consists of a sampler , a buffer ,   a trainer and a multilingual dependency parsing   model . The two main components are the sampler ,   which adopts a curriculum sampling strategy to   dynamically sample data batches , and the trainer   which uses worst - case - aware strategy to train the   model . The framework repeats the following steps :   ( 1 ) the sampler samples data batches of different   languages to the buffer ; ( 2 ) the trainer uses a worst-   case strategy to train the model ; ( 3 ) the automated   curriculum learning strategy of the sampler is up-   dated .   Sampling data batches We view multilingual   dependency parsing as multi - task learning where   parsing in each individual language is considered   a task . This means that the target of the sampler   at each step is to choose a data batch from one   language . This is a typical multi - arm bandit prob-   lem ( Even - Dar et al . , 2002 ) . The sampler should   choose bandits that have higher rewards , and in   our scenario , data batches that have a higher loss   on the model are more likely to be selected by the   sampler and therefore , in a later stage , used by the   trainer . Automated curriculum learning is adopted   to push a batch with its loss into the buffer at each   time step . The buffer consists of nÔ¨Årst - in-Ô¨Årst - out   queues , and each queue corresponds to a task ( in   our case , a language ) . The procedure repeats k   times and , at each round , kdata batches are pushed   into the buffer .   Worst - case - aware risk minimization In multi-   lingual and multi - task learning scenarios , in which   we jointly minimize our risk across nlanguages   or tasks , we are confronted with the question of   how to summarize nlosses . In other words , the   question is how to compare two loss vectors  and   containing losses for all tasks l;:::l :  = [ ` ; : : : ; ` ]   and   = [ ` ; : : : ; ` ]   The most obvious thing to do is to minimize the   mean of the nlosses , asking whetherP ` < P ` . We could also , motivated by robust-   ness ( S√∏gaard , 2013 ) and fairness ( Williamson   and Menon , 2019 ) , minimize the maximum ( supre-   mum ) of the nlosses , asking whether max ` <   max ` . Mehta et al . ( 2012 ) observed that these   two loss summarizations are extremes that can be   generalized by a family of multi - task loss functions   that summarize the loss of ntasks as theLnorm   of then - dimensional loss vector . Minimizing the   average loss then corresponds to computing the L   norm , i.e. , asking whether j  j < j  j , and mini-   mizing the worst - case loss corresponds to comput-   ing theL(supremum ) norm , i.e. , asking whether   j  j < j  j.   Zhang et al . ( 2020 ) present a stochastic general-   ization of the Lloss summarization and a prac-   tical approach to minimizing this family of losses   through automated curriculum learning ( Graves   et al . , 2017 ): The core idea behind their general-   ization is to optimize the worst - case loss with a   certain probability , otherwise optimize the average   ( loss - proportional ) loss with the remaining prob-   ability . The hyperparameter  is introduced by   the worst - case - aware risk minimization to trade off   the balance between the worst - case and the loss-   proportional losses . The loss family is formally   deÔ¨Åned as :   wherep2[0;1]is a random generated rational   number , and P = is the normalized prob-   ability distribution of task losses . If p <   the   model chooses the maximum loss among all tasks ,   otherwise , it randomly chooses one loss according   to the loss distribution . If the hyperparameter   equals 1 , the trainer updates the model with respect   to the worst - case loss . On the contrary , if  = 0 ,   the trainer loss - proportionally samples one loss .   Sampling strategy updates The model updates   its parameters with respect to the loss chosen by the   trainer . After that , the sampler updates its policy   according to the behavior of the trainer . At each579   round , the policy of the task that is selected by   the trainer receives positive rewards and the policy   of all other tasks that have been selected by the   sampler receive negative rewards .   The multilingual dependency parsing model   We use a standard biafÔ¨Åne graph - based dependency   parser ( Dozat and Manning , 2017 ) . The model   takes token representations of words from a con-   textualized language model ( mBERT or XLM - R )   as input and classiÔ¨Åes head and dependency rela-   tions between words in the sentence . The Chu - Liu-   Edmonds algorithm ( Chu and Liu , 1965 ; Edmonds ,   1967 ) is then used to decode the score matrix into   a tree . All languages share the same encoder and   decoder in order to learn features from different lan-   guages , and more importantly to perform zero - shot   transfer to unseen languages .   3 Experiments   We base our experimental design on √úst√ºn et al .   ( 2020 ) , a recent paper doing zero - shot dependency   parsing with good performance on a large number   of languages . They Ô¨Åne - tune mBERT for depen-   dency parsing using training data from a sample   of 13 typologically diverse languages from Univer-   sal Dependencies ( UD ; Nivre et al . , 2020 ) , listed   in Table 1 . For testing , they use 30 test sets from   treebanks whose language has not been seen at Ô¨Åne-   tuning time . We use the same training and test sets   and experiment both with mBERT and XLM - R as   PLMs . It is important to note that not all of the test   languages have been seen by the PLMs .   We test worst - case aware learning with differ-   ent values of  and compare this to three main   baselines : size - proportional samples batches pro - portionally to the data sizes of the training tree-   banks , uniform samples from different treebanks   with equal probability , thereby effectively reducing   the size of the training data , and smooth - sampling   uses the smooth sampling method developed in   van der Goot et al . ( 2021 ) which samples from mul-   tiple languages using a multinomial distribution .   These baselines are competitive with the state - of-   the - art when using mBERT , they are within 0.2 to   0.4 LAS points from the baseline of √úst√ºn et al .   ( 2020 ) on the same test sets . When using XLM - R ,   they are largely above the state - of - the - art .   We implement all models using MaChAmp   ( van der Goot et al . , 2021 ) , a library for multi - task   learning based on AllenNLP ( Gardner et al . , 2018 ) .   The library uses transformers from HuggingFace   ( Wolf et al . , 2020 ) . Our code is publicly available .   Our main results are in Table 2 where we report   average scores across test sets , for space reasons .   Results broken down by test treebank can be found   in Table 4 in Appendix A. We can see that worst-   case - aware training outperforms all of our baselines   in the zero - shot setting , highlighting the effective-   ness of this method . This answers positively our   research question Can worst - case aware automated   curriculum learning improve zero - shot dependency   parsing ?   Our results using mBERT are more than 1 LAS   point above the corresponding baselines . Our best   model is signiÔ¨Åcantly better than the best baseline   withp < : 01according to a bootstrap test across   test treebanks . Our best model with mBERT comes   close to Udapter ( 36.5 LAS on the same test sets )   while being a lot simpler and not using external re-   sources such as typological features , which are not   always available for truly low - resource languages .   The results with XLM - R are much higher in   generalbut the trends are similar : all our models   outperform all of our baselines albeit with smaller   differences . There is only a 0.4 LAS difference   between our best model and the best baseline , but   it is still signiÔ¨Åcant with p < : 05according to a   bootstrap test across test treebanks . This highlights   the robustness of the XLM - R model itself . Our   results with XLM - R outperform Udapter by close   to 7 LAS points.580mBERT XLM - R  = 0 36.4 42.1   = 0.5 36.1 42.3   = 1 36.1 42.3size - proportional 35.0 41.9   smooth - sampling 35.2 41.7   uniform 35.2 41.4   4 Varying the homogeneity of training   samples   We investigate the interaction between the effec-   tiveness of worst - case learning and the represen-   tativeness of the sample of training languages . It   is notoriously difÔ¨Åcult to construct a sample of   treebanks that is representative of the languages in   UD ( de Lhoneux et al . , 2017 ; Schluter and Agi ¬¥ c ,   2017 ; de Lhoneux , 2019 ) . We can , however , easily   construct samples that are notrepresentative , for   example , by taking a sample of related languages .   We expect worst - case aware learning to lead to   larger improvements in cases where some language   types are underrepresented in the sample . We can   construct an extreme case of underrepresentation   by selecting a sample of training languages that   has one or more clear outliers . For example we   can construct a sample of related languages , add   a single unrelated language in the mix , and then   evaluate on other unrelated languages . We also   expect that with a typologically diverse set of train-   ing languages , worst - case aware learning should   lead to larger relative improvements than with a   homogeneous sample , but perhaps slightly smaller   improvements than with a very skewed sample .   We test these hypotheses by constructing seven   samples of training languages in addition to the   one used so far ( 13 ) . We construct three dif-   ferent homogeneous samples using treebanks from   three different genera : , and . We construct four skewed samples using   the sample of romance languages and a language   from a different language family , an outlier lan-   guage : Basque ( eu ) , Arabic ( ar ) , Turkish ( tr ) and   Chinese ( zh ) . Since we keep the sample of test   sets constant , we do not include training data from   languages that are in the test sets . The details of   which treebanks are used for each of these samplessample  RER   13 35.2 36.4 1.2 1.9 30.7 31.4 0.7 1.0 30.4 31.7 1.3 1.9 31.3 32.5 1.2 1.7 + 33.3 34.8 1.5 2.2 + 32.0 32.2 0.2 0.3 + 32.2 33.0 0.8 1.2 + 33.4 34.1 0.7 1.1   can be found in Table 5 in Appendix B.   Results are in Table 3 where we report the aver-   age LAS scores of our best model ( out of the ones   trained with the three different  values ) to the best   of the three baselines . We can see Ô¨Årst that , as ex-   pected , our typologically diverse sample performs   best overall . This indicates that it is a good sam-   ple . We can also see that , as expected , the method   works best with a skewed sample : the largest gains   from using worst - case learning , both in terms of ab-   solute LAS difference and relative error reduction ,   are seen for a skewed sample ( + ) . However ,   contrary to expectations , the lowest gains are ob-   tained for another skewed sample ( + ) . The   gains are also low for + , + and for . Additionally , there are slightly more   gains from using worst - case aware learning with   the sample than for our typologically di-   verse sample . These results could be due to the   different scripts of the languages involved both in   training and testing .   Looking at results of the different models on indi-   vidual test languages ( see Figure 1 in Appendix C ) ,   we Ô¨Ånd no clear pattern of the settings in which this   method works best . We do note that the method   always hurts Belarusian , which is perhaps unsur-   prising given that it is the test treebank for which   the baseline is highest . Worst - case aware learning   hurts Belarusian the least when using the   sample , indicating that , when using the other sam-   ples , the languages related to Belarusian are likely   downsampled in favour of languages unrelated to it .   Worst - case learning consistently helps Breton and   Swiss German , indicating that the method might   work best for languages that are underrepresented   within their language family but not necessarily   outside of it . For Swiss German , worst - case learn-581ing helps least when using the sample   where it is less of an outlier .   5 Conclusion   In this work , we have adopted a method from multi-   task learning which relies on automated curriculum   learning to the case of multilingual dependency   parsing . This method allows to dynamically opti-   mize for parsing performance on outlier languages .   We found this method to improve dependency pars-   ing on a sample of 30 test languages in the zero-   shot setting , compared to sampling data uniformly   across treebanks from different languages , or pro-   portionally to the size of the treebanks . We investi-   gated the impact of varying the homogeneity of the   sample of training treebanks on the usefulness of   the method and found conÔ¨Çicting evidence with dif-   ferent samples . This leaves open questions about   the relationship between the languages used for   training and the ones used for testing .   6 Acknowledgements   We thank Daniel Hershcovich and Ruixiang Cui   for comments on a draft of the paper , as well as   the members of CoAStaL for discussions about   the content of the paper . Miryam de Lhoneux was   funded by the Swedish Research Council ( grant   2020 - 00437 ) . Anders S√∏gaard was funded by the   Innovation Fund Denmark and a Google Focused   Research Award . We acknowledge the compu-   tational resources provided by CSC in Finland   through NeIC - NLPL and the EOSC - Nordic NLPL   use case ( www.nlpl.eu ) .   References582583   A Results by treebank   Results by language of the test treebanks are in   Table 4 .   B Training samples   The training samples are summarized in Table 5 .   C Results by treebank with the different   samples   Relative error reduction between our best worst-   case aware result and the best baseline for each   training sample used , with mBERT , in Figure 1.584mBERT XLM - R   iso  = 0  = 0.5  = 1 S - P S - S U  = 0  = 0.5  = 1 S - P S - S U   aii * # 8 11.3 10.8 1.6 6.4 6.0 2 3.3 3.1 2.9 3.5 3.1   akk * # 1.5 1.4 1.6 2.5 3.0 1.9 2.5 2.5 2.8 1.9 2.2 2.3   am * 16.5 10.9 13.2 6.6 10.8 10.6 68.0 68.6 68.3 68.4 68.8 68.1   be 78.5 79.4 79.6 82.0 80.9 80.5 85.6 85.5 85.6 86.4 86.8 86.8   bho * # 38.1 37.8 37.9 37.0 36.7 36.7 37.3 37.4 37.1 37.4 37.6 37.2   bm * # 9.0 8.7 8.7 6.9 6.7 6.9 6.0 6.4 6.2 6.5 6.3 6.4   br 62.9 62.6 62.0 60.3 60.3 59.6 59.5 59.6 60.5 59.9 59.5 58.9   bxr * # 25.9 26.0 25.6 24.6 25.5 25.4 27.7 28.2 28.0 27.2 27.2 26.2   cy 55.5 55.0 55.2 55.1 54.4 54.2 59.8 60.1 59.9 60.2 60.6 59.6   fo * # 67.4 67.8 68.0 66.3 67.2 66.4 73.5 72.8 73.5 72.6 72.4 73.0   gsw * # 48.3 48.8 48.2 44.9 42.2 42.3 46.0 46.5 46.5 43.6 42.2 44.3   gun * # 8.2 8.5 8.7 7.3 8.0 8.3 6.8 6.8 7.6 6.5 5.8 5.6   hsb * # 50.8 51.3 51.4 49.4 49.2 49.1 62.6 61.9 62.0 61.4 61.6 60.0   kk 60.1 58.9 58.4 58.5 59.0 58.2 63.0 62.7 62.5 63.7 62.3 61.5   kmr * 9.3 9.2 8.9 8.6 9.6 9.5 53.5 53.1 53.2 51.8 51.7 52.0   koi * # 19.3 18.8 19.8 15.8 15.8 16.0 17.0 20.1 19.1 17.8 17.8 16.0   kpv * # 16.8 17.0 17.2 15.6 16.2 15.8 18.3 19.1 19.5 17.0 17.8 16.3   krl * # 46.6 46.4 46.3 46.5 47.1 46.4 61.0 61.2 60.7 62.0 62.1 61.8   mdf * # 26.1 24.3 24.3 22.5 24.5 25.4 20.4 20.7 19.6 18.4 18.4 16.8   mr 60.6 61.2 60.1 56.9 57.7 57.7 69.2 69.7 70.0 67.8 70.0 69.7   myv * # 20.2 19.9 19.8 18.5 19.3 19.9 16.8 17.2 16.9 16.0 16.3 15.5   olo * # 40.7 41.7 41.0 41.0 40.9 40.5 56.5 56.7 56.1 55.8 54.3 54.4   pcm * # 33.9 32.8 33.0 32.5 34.3 35.4 39.2 39.2 38.9 38.0 37.6 37.8   sa * 22.5 21.9 22.3 21.1 21.0 20.6 50.2 49.7 50.9 50.9 50.1 50.0   ta 52.3 54.7 54.3 53.2 52.0 51.6 54.9 55.0 54.8 53.8 53.8 54.0   te 69.9 69.8 70.0 69.4 70.6 68.7 76.0 76.0 76.7 76.3 77.1 76.3   tl # 65.4 57.5 56.5 65.8 59.3 65.4 77.1 75.7 75.7 78.1 76.7 76.4   wbp * # 5.9 8.8 9.2 7.5 7.5 7.2 7.8 9.5 7.5 8.5 5.2 8.8   yo # 37.8 37.9 38.5 39.7 38.0 37.5 3.3 3.6 3.2 2.3 2.7 1.8   yue * # 33.0 32.5 32.5 32.4 32.4 32.4 41.9 41.7 42.0 42.9 42.4 42.8   average 36.4 36.1 36.1 35.0 35.2 35.2 42.1 42.3 42.3 41.9 41.7 41.4585   Afrikaans - AfriBooms X   Danish - DDT X   Dutch - Alpino X   English - EWT X X   German - HDT X   Gothic - PROIEL X   Icelandic - IcePaHC X   Norwegian - Bokmaal X   Swedish - Talbanken X X   Czech - PDT X   Old_Church_Slavonic - PROIEL X   Old_Russian - TOROT X   Polish - LFG X   Russian - SynTagRus X X   Serbian - SET X   Slovak - SNK X   Ukrainian - IU X   French - GSD X X X X X   Italian - ISDT X X X X X X   Portuguese - GSD X X X X X   Romanian - RRT X X X X X   Spanish - AnCora X X X X X   Basque - BDT X X   Arabic - PADT X X   Chinese - GSD X X   Turkish - IMST X X   Finnish - TDT X   Hebrew - HTB X   Hindi - HDTB X   Japanese - GSD X   Korean - GSD X586587