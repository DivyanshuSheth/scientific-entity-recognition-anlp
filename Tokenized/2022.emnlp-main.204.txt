2   Hao Wang , Yixin Cao , Yangguang Li , Zhen Huang   Kun Wang , Jing ShaoNational University of Defense TechnologySingapore Management UniversitySenseTime   Abstract   Document - level natural language inference   ( DNLI ) is a new challenging task in natural   language processing , aiming at judging the en-   tailment relationship between a pair of hypoth-   esis and premise documents . Current datasets   and baselines largely follow sentence - level set-   tings , but fail to address the issues raised by   longer documents . In this paper , we estab-   lish a general solution , named Retrieval , Read-   ing and Fusion ( RF ) framework , and a new   setting , by analyzing the main challenges of   DNLI : interpretability , long - range depen-   dency , and cross - sentence inference . The basic   idea of the framework is to simplify document-   level task into a set of sentence - level tasks , and   improve both performance and interpretabil-   ity with the power of evidence . For each hy-   pothesis sentence , the framework retrieves ev-   idence sentences from the premise , and reads   to estimate its credibility . Then the sentence-   level results are fused to judge the relationship   between the documents . For the setting , we   contribute complementary evidence and entail-   ment label annotation on hypothesis sentences ,   for interpretability study . Our experimental   results show that RF framework can obtain   state - of - the - art performance and is robust for   diverse evidence retrieval methods . Moreover ,   it can give more interpretable prediction re-   sults . Our model and code are released at   https://github.com/phoenixsecularbird/R2F.   1 Introduction   Natural Language Inference ( NLI ) is the task of   determining whether a hypothesis is entailed or not   in apremise . While earlier works ( Bowman et al . ,   2015 ; Williams et al . , 2018 ; Wang et al . , 2019 ;   Nie et al . , 2020 ) assume that both hypothesis and   premise are single sentences , recent research pays   more attention on document - level task , namely   Document - level NLI ( DNLI ) ( Yin et al . , 2021 ) .   The task can enlarge the task scope to judge thevariability of semantic expression for many Natural   Language Processing ( NLP ) tasks , e.g. , exposure   bias ( Bengio et al . , 2015 ; Arora et al . , 2022 ) alle-   viation for text summarization ( Sandhaus , 2008 ;   Narayan et al . , 2018 ) , and human - manipulated   news articles recognition for automatic fake news   detection ( Jawahar et al . , 2022 ; Huang et al . , 2022 ) .   Compared with sentence - level NLI , DNLI   poses many new challenges , while there are only   a few datasets and models . In terms of datasets ,   Yin et al . ( 2021 ) reformat some mainstream NLP   tasks , e.g. , text summarization and question answer-   ing , and build the ﬁrstlarge scale dataset DNLI   with over 1 million document pairs . However , the   dataset does not provide evidence annotation about   how the labels are inferred , i.e. , which hypothesis   sentences lead to semantic inconsistency , or which   premise sentences help to decide the entailment   relationship . As shown in Figure 1 , although the   sample is annotated as not entailment , most of the   hypothesis sentences are actually entailed . By con-   trast , the detailed disinformation of “ in 1989 ” in   the third hypothesis sentence eventually decides   the entailment relationship between the documents .   For each hypothesis sentence , only several premise   sentences are enough to serve as the exact evidence   to judge its own sentence - level entailment label .   In this paper , we argue that evidence discovery   is important and challenging for DNLI . Our pi-   lot experiments in Section 4.3 and 4.5 show that   randomly selected evidences can still contribute to   comparable performance . Thus , only the black - box   models may be not so convincing . However , to   annotate evidence for evaluation is non - trivial . For   each hypothesis sentence , on one hand , it may refer   to multiple premise sentences . On the other hand ,   there may be several evidence groups , where each   group can independently serve the label prediction .   We highlight this as interpretability challenge.3122   In term of models , current baselines ( Yin et al . ,   2021 ; Zhong et al . , 2020 ) still largely follow   sentence - level NLI . They either concatenate two   documents for mutual information interaction for   classiﬁcation , or encode them separately for se-   mantic match with document - level representations .   However , except for the interpretability issue , they   will leave the following challenges unexplored :   •Long - range Dependency Modeling The task   requests to handle a pair of long documents at the   same time , where we observe that 29.81 % samples   of DNLI dataset ( Yin et al . , 2021 ) contain more   than 500 words , while 10.47 % samples contain   more than 1000 words . This will not only exceed   the input limit of Pre - trained Language Models   ( PLMs ) , but also make it far more difﬁcult to cap-   ture long - range dependency . Necessary informa-   tion interaction between the hypothesis and some   key premise sentences may not be guaranteed . Be-   sides , most contexts are uninformative for entail-   ment inference and will only serve as noise .   •Cross - sentence Inference To judge the rela-   tionship between the documents , it is supposed to   consider all hypothesis sentences , where the de-   tailed disinformation issue still remains unsolved .   Besides , the veriﬁcation of one hypothesis sen-   tence may request to combine multiple and distant   premise sentences , different from the sentence pair   mode in sentence - level NLI . As shown in Figure 1,to process the ﬁrst one , it needs to take both the   ﬁrst and sixth premise sentences ( all in red fonts ) .   In this paper , we establish a general solution ,   named Retrieval , Reading and Fusion ( RF ) frame-   work , and a new setting for the task . The basic   idea of the framework is to simplify document-   level classiﬁcation task into a set of sentence - level   tasks , and then improve both performance and inter-   pretability with the power of evidence . Speciﬁcally ,   the framework splits the hypothesis document into   sentences . Then for each hypothesis sentence , it   retrieves evidence sentences from the premise , and   reads to estimate its credibility score upon the evi-   dences . Finally , it fuses the sentence - level results   and judge the entailment relationship between the   two documents . For the setting , we contribute   complementary ﬁne - grained annotations for inter-   pretability study . For each hypothesis sentence ,   we manually annotate entailment label and several   evidence groups , where each group is enough to   independently infer the label . In summary , our   contributions are as follows :   •We propose a Retrieval , Read and Fusion   framework as a general solution for DNLI task .   •We contribute complementary evidence and   entailment label annotation for each hypothesis   sentence on a subset of DNLI dataset for inter-   pretability study .   •Our experimental results on DNLI dataset   indicate that the framework obtains state - of - the-3123   art performance . Besides , it is robust for diverse   retrieval methods . Moreover , the framework can   give more interpretable prediction results .   2 RF Framework   OurRFframework aims at a general solution for   DNLI task with interpretability , i.e. , to obtain   corresponding evidence and predict entailment la-   bel for each hypothesis sentence . As shown in   Figure 2 , the framework consists of 3 components ,   namely evidence retrieval , reading for credibility   estimation , and credibility fusion . For efﬁciency ,   the retrieval component is an independent unit to   provide evidence input for the other two compo-   nents , which are optimized jointly .   2.1 Task Formulation   Similar to previous sentence - level NLI tasks , for   each sample in DNLI task , given a hypothe-   sis document Hand a premise document P , it is   requested to judge the entailment relationship R   between these two documents . Here , R∈{“entail-   ment ” , “ not_entailment ” } for DNLI dataset , but   may not be restricted to binary classiﬁcation .   2.2 Evidence Retrieval   Given each sample , we split the hypothesis into   sentences and retrieve evidence sentences from the   premise . Formally , we split the hypothesis Hand   the premise Pinto single sentences { H , H , · · · ,   H } and { P , P,···,P } , through NLTK tool .   Here , mandnare the sentence numbers .   For each hypothesis sentence H , we respec-   tively utilize the following retrieval methods to   calculate the relevance score with all premise sen-   tences . Then according to the scores , we remain topKsentences as the corresponding evidence . The   value of Kis a trade - off between evidence recall   and precision . A lower value pursues higher evi-   dence precision , but may lead to evidence missing ,   while a higher value guarantees higher evidence   recall , but may introduce too many uninformative   sentences as noise . Moreover , to keep and utilize   contextual information , for each hypothesis sen-   tence , we reorder the evidence sentences according   to their original order in the premise .   To calculate the relevance score , we take several   sparse and dense retrieval methods into considera-   tion :   •ROUGE-1 Inspired by Mao et al . ( 2022 )   and Zhang et al . ( 2022 ) , we adopt ROUGE-1 re-   trieval . For a pair of sentences , this sparse retrieval   method focuses on n - gram match of the pair to cal-   culate ROUGE-1 score as the relevance metric . We   take it as the main retrieval method .   •BM25BM25 is one of the most advanced   sparse retrieval methods . We take all premise   sentences as the corpus . For a pair of sentences ,   BM25 involves not only the pair itself but also the   whole corpus , to count term frequency andinverse-   document frequency to obtain the relevance score .   •SimCSEInspired by Gao et al . ( 2021 ) , we uti-   lize SimCSE ( Gao et al . , 2021 ) , a strong sentence   embedding model , as dense retrieval method for   semantic match . For a pair of sentences , we take   the cosine similarity of the sentence embeddings   as the relevance score .   Except for above retrieval methods , we also   adopt another simple but effective strategy . If a   hypothesis sentence is a substring of the premise,3124then it is naturally entailed in the premise through   string match and does not need further study .   2.3 Reading & Credibility Estimation   For each hypothesis sentence , we concentrate in-   formative premise sentences and ﬁlter out most   noisy ones through evidence retrieval . Then this   component aims to estimate its credibility , which   involves the hypothesis sentence itself and several   evidence sentences . This is different from con-   ventional sentence - level NLI , which studies the   relationship between a pair of sentences . Hence   we adopt reading models . Our general RFframe-   work is compatible to any arbitrary reading models ,   which may be enhanced by several advanced learn-   ing technologies , e.g. , graph neural network ( Kipf   and Welling , 2017 ; Velickovic et al . , 2018 ) , com-   monsense knowledge injection ( Zhang et al . , 2019 ;   Wang et al . , 2021 ) , and syntactic structure analy-   sis ( Kitaev et al . , 2022 ) . Herein , we adopt a simple   and straightforward one without loss of generality .   Speciﬁcally , for a hypothesis sentence H , and   its corresponding evidence { Evi , Evi , · · · ,   Evi } , we concatenate them all as the input :   [ CLS ] H[SEP ] EviEvi···Evi[SEP ]   ( 1 )   Then we leverage transformer - based pre - trained   language model , i.e. , RoBERTa ( Liu et al . , 2019 ) ,   to encode the input sequence . Through the multi-   head self - attention mechanism ( Vaswani et al . ,   2017 ) , token - level information interaction among   the hypothesis sentence and all its evidences is con-   ducted . Besides , since evidence are concentrated ,   it is much easier to handle the multiple evidence   combination issue for cross - sentence inference .   Then the credibility score is calculated through   a Multi Layer Perceptron ( MLP ) with sigmoid acti-   vation function :   ˆy= Sigmoid(MLP ( h ) ) ( 2 )   wherehis the hidden state of the special [ CLS ]   token , and is taken as the inference vector of H.   Besides , ˆy∈[0.0 , 1.0 ] is the credibility score of   H , and a higher score means that the sentence is   more likely to be entailed by the premise .   2.4 Credibility Fusion   After reading , the inference vector hand credi-   bility score ˆyof each hypothesis sentence His   obtained . Nevertheless , the reading model can not   be trained directly since the detailed entailmentlabel ofHis not available . To this end , we fuse   the sentence - level results to judge the entailment   relationship between the documents , and indirectly   train the model through document - level entailment   label . Besides , the fusion process is also expected   to solve the detailed disinformation issue for cross-   sentence inference , and expand the interpretability   of the framework . Herein , we design three fusion   methods for comparison .   •Credibility Score Minimum Pooling The   logic basis for this method is that if the premise   entails the hypothesis , then it will entail all the hy-   pothesis sentences , even the one with the lowest   credibility score . By contrast , if the premise does   not entail the hypothesis , then it will conﬂict to at   least one hypothesis sentence . This one is expected   to be assigned with the lowest credibility score .   Formally , for a pair of documents HandP , the   credibility scores of the hypothesis sentences are   { ˆy,ˆy,···,ˆy } . Then the credibility score of the   sample is calculated as :   ˆy = min({ˆy,ˆy,···,ˆy } ) ( 3 )   For this fusion method , the sample prediction re-   sult comes from that of the least credible hypothesis   sentence , i.e. , with the lowest credibility score . The   framework is requested to conduct internal contrast   among the hypothesis sentences to decide the least   credible one . Thus , it is forced to understand the   entailment relationship between each hypothesis   sentence and its corresponding evidences although   without direct entailment label . During prediction ,   the credibility score ˆyis utilized to predict the en-   tailment label of hypothesis sentence H. We take   this as the main fusion method .   •Inference Vector Minimum Pooling For this   method , we conduct minimum pooling on the in-   ference vector hrather than the credibility score   ˆy . For the inference vector of the sample h , the   j - th dimension is :   h = min({h , h,···,h } ) ( 4 )   Then the credibility score of the sample is :   ˆy= Sigmoid(MLP ( h ) ) ( 5 )   Many conventional neural models tend to adopt   this fusion method for better performance , but suf-   fer from low interpretability , since the practical   meaning of each dimension of the inference vector   can hardly be probed.3125•Gaussian Kernel Pooling To further explore   the inﬂuence of fusion methods , we adopt Gaus-   sian Kernel Pooling ( Xiong et al . , 2017 ; Liu et al . ,   2020 ; Sheng et al . , 2022 ) . Speciﬁcally , we utilize C   Gaussian kernels{K } . For a credibility score   ˆy , the output of the j - th kernel is :   V= exp(−(ˆy−µ )   2σ ) ( 6 )   whereµandσare respectively the mean and   width of the j - th kernel . In this way , the score ˆyis   projected to a kernel vector V∈R. The kernel   vector of the sample is :   V=1   m / summationdisplayV∈R(7 )   And the credibility score of the sample is :   ˆy= Sigmoid(MLP ( V ) ) ( 8)   This is a mean pooling method , which conducts   mean pooling on the corresponding kernel vectors ,   rather than the credibility scores .   During training , the loss function of the sample   is set as binary cross entropy loss :   L = ylog(ˆy)+(1−y)log(1−ˆy)(9 )   whereyis the sample entailment label , 1 for   entailment samples while 0 for not entailment ones .   During prediction , we set a threshold on the sample   credibility score ˆyto obtain the result .   3 Experiment   3.1 Dataset   We conduct our experiments on DNLI   dataset ( Yin et al . , 2021 ) , which is a newly   proposed and the only large scale dataset in the   ﬁeld . The detailed statistic information is shown   in Table 1 . The training set comes with balanced   label distribution , while the development and test   sets come with pretty unbalanced ones .   Set Entailment Not_Entailment Total   train 466,653 475,661 942,314   dev 28,890 205,368 234,258   test 33,128 233,958 267,0863.2 Complementary Annotation   For interpretability study , we contribute comple-   mentary annotation . The hypothesis sentences   may involve cross - sentence inference and request   multiple evidences . Besides , they may also cor-   respond to several evidence groups , where each   group itself is enough to independently infer the en-   tailment label . Thus the annotation process needs   heavy workload and comes with great complexity .   Herein , we adopt a proposal and correction anno-   tation strategy . Speciﬁcally , for each hypothesis   sentence , we ﬁrstly retrieve candidate evidences   through the diverse methods in Section 2.2 . Then   we manually check , remove repeated or unrelated   ones and add missing ones , and combine several   evidence groups . Finally , we decide the entailment   label according to the evidences .   Due to the heavy workload and great complexity ,   we manually annotate 100 longer samples ( all over   800 words ) randomly selected from the testset ,   which contain more than 350 hypothesis sentences   in total . An annotation example is shown in Fig-   ure 3 . The hypothesis sentence is annotated as en-   tailment with two evidence groups . More detailed   statistic information is summarized in Appendix A.   3.3 Evaluation Metric   For DNLI task , we adopt micro and macro F1   scores , and attach more importance to the latter .   Due to the unbalanced label distribution , even ma-   jority guess model will obtain high micro F1 score ,   but pretty low macro F1 score . For sentence - level3126ModelEntailment Not_Entailment Total   P R F1 P R F1 Micro F1 Macro F1   Concatenation - Longformer ♣ - - 44.42 - - - - -   Concatenation - Longformer ♠ 29.73 80.32 43.39 96.33 73.11 83.13 74.01 63.26   Concatenation - RoBERTa ♠ 39.73 91.57 55.41 98.54 80.33 88.51 81.72 71.96   Semantic Match - RoBERTa / triangle31.60 85.49 46.15 97.29 73.80 83.93 75.25 65.04   oursRF - RoBERTa 43.52 87.04 58.02 97.86 84.01 90.41 84.38 74.22   Concatenation - RoBERTa ♣ 45.15 94.99 61.21 99.16 83.66 90.75 85.06 75.98   Concatenation - RoBERTa ♠ 44.70 94.74 60.74 99.11 83.40 90.58 84.81 75.66   oursRF - RoBERTa 50.61 88.07 64.28 98.11 87.82 92.68 87.86 78.48   evaluation , we adopt evidence precision , recall and   F1 for retrieval , while micro and macro F1 scores   for label prediction . Moreover , inspired by Thorne   et al . ( 2018 ) , we propose a more strict metric full   accuracy . Herein , evidence recall requests to ﬁnd   at least one complete evidence group , and full ac-   curacy further requests correct label prediction .   3.4 Experiment Setup   OurRFframework is implemented through Py-   torch 1.8.0 . We adopt AdamW optimizer , keep a   random number seed of 42 , set max input length as   256 , and set mini batch size as 8 with gradient ac-   cumulation step as 4 . For base encoder , we choose   initial learning rate of 1e-5 , while for large encoder ,   we choose 5e-6 . For evidence retrieval , we set Kas   5 . During prediction , we adopt a threshold of 0.5 .   More setup is shown in Appendix B.   3.5 Baseline   Since DNLI is still a new task , we adopt the   concatenation model from Yin et al . ( 2021 ) , and   modify the semantic match model from Zhong et al .   ( 2020 ) for comparison . Please refer to the original   papers and Appendix C for detailed information .   4 Results   4.1 Main Results   Main results of our framework on the test set are   displayed in Table 2 . The framework obtains the   best performance with the highest micro and macro   F1 scores , with both base and large encoders , indi-   cating its strength . The situation is similar on the   development set in Appendix D.   All models show far better performance ( higher   F1 score ) on not entailment samples than entail-   ment samples , while entailment samples still comewith even higher recall . This may be due to the   label distribution difference among different sets   ( in Table 1 ) . Among the baselines , semantic match   model conducts coarse - grained information inter-   action between the hypothesis and premise through   document - level vector representations . Thus it   shows pretty low performance although it can avoid   possible key information missing caused by the   truncation of overlong samples . This indicates that   ﬁne - grained information interaction is essential for   this task . Besides , concatenation model with Long-   former encoder , although it is able to handle much   longer inputs , shows much lower performance than   that with RoBERTa encoder , which truncates the   inputs . The simpliﬁed multi - head self - attention   mechanism in Longformer encoder ( Beltagy et al . ,   2020 ) seems not competent for ﬁne - grained infor-   mation interaction in the task .   4.2 Performance vs Sample Length   To examine the ability of our framework on pro-   cessing long inputs , the performance with vary-   ing sample length on the test set is shown in Fig-   ure 4 . Here , the length of a sample is counted in   the number of words . Both models obtain far better   performance on shorter samples than longer ones   ( more than 10 % absolute difference on both mi-   cro F1 and macro F1 scores ) . Hence it is still a   relatively difﬁcult problem to process longer sam-   ples . Moreover , our RF framework consistently   and greatly outperforms the strongest concatena-   tion baseline on samples with varying length . Espe-   cially , it shows much higher performance on longer   samples . These indicate the framework is able to   handle longer ones efﬁciently through breaking the   document - level task into sentence - level task with   the retrieval , reading and fusion process . On the de-3127velopment set , the tendency is similar , and detailed   results are displayed in Appendix D.   4.3 Inﬂuence of Evidence Retrieval   To investigate the inﬂuence of evidence retrieval ,   we focus on different retrieval methods , and take   those mentioned in Section 2.2 for comparison .   Besides , we also adopt a random baseline , which   adopts Krandomly selected premise sentences as   the evidences .   Retrieval   MethodDev Set Test Set   Mi F1 Ma F1 Mi F1 Ma F1   Random 83.27 71.77 82.45 70.98   ROUGE-1 85.58 75.44 84.38 74.22   BM25 85.55 75.36 83.98 73.71   SimCSE86.13 75.57 85.34 74.74   SimCSE84.87 74.33 83.65 73.28   As shown in Table 3 , the random baseline can   still obtain relatively high performance although   it can not outperform the concatenation baseline .   This may be due to the evidence dependency bias   discussed in Section 4.5 . Besides , for shorter sam-   ples with only a few premise sentences , evidence   retrieval will show less importance . All other re-   trieval methods can contribute to higher perfor-   mance than the concatenation baseline , indicating   the strength of the framework on the task and itsrobustness for diverse retrieval methods . However ,   supervised SimCSE , although trained on human-   annotated NLI benchmarks , shows the lowest per-   formance , which may suffer transfer issue caused   by the domain difference between its own training   data and DNLI dataset .   4.4 Inﬂuence of Fusion Method   Performance of different fusion methods are com-   pared in Figure 5 . Gaussian kernel pooling tends to   incorrectly predicts all samples as not entailment   and can hardly recognize entailment samples . Thus   it comes with much higher micro F1 score but far   lower macro F1 score under the pretty unbalanced   label distribution ( in Table 1 ) . This mean pool-   ing fusion method seems not feasible for the task .   On both sets , credibility score minimum pooling   outperforms all other fusion methods . This fusion   method also comes with clear logic basis to expand   the interpretability of the framework .   4.5 Interpretability Study   To investigate the interpretability of our framework ,   we conduct sentence - level evaluation on the subset   annotated by ourselves . As shown in Table 4 , we   focus on evidence retrieval and entailment label   prediction for hypothesis sentences .   •Evidence Retrieval For these longer samples   ( all over 800 words ) , the random baseline can   hardly obtain the evidence , with extremely low evi-   dence recall . All other retrieval methods can ﬁnd at   least one complete evidence group for most of the   hypothesis sentences , with relatively high evidence   recall around 85 % . However , with pretty low ev-   idence precision and F1 score , all these methods   will introduce plenty of uninformative sentences as   noise . Thus , the evidence retrieval component may   need further improvement.3128   Retrieval   MethodEvi . Retri . Label Pre . FAP R F1 Mi F1 Ma F1   Random 6.83 17.21 9.78 66.94 63.32 13.11   ROUGE-1 31.53 87.43 46.35 76.23 73.92 68.85   BM25 31.53 87.16 46.31 73.22 70.46 66.12   SimCSE30.38 84.97 44.76 76.50 74.69 67.49   SimCSE31.31 84.97 45.76 73.50 70.48 65.03   •Entailment Label Prediction Regardless of   poor retrieval performance , the random baseline   still shows relatively high performance on entail-   ment label prediction . This is due to evidence de-   pendency bias .Entailment samples strictly request   at least one complete evidence group . However ,   not entailment samples are insensitive to evidence   retrieval . With complete evidence group obtained ,   they are taken as conﬂict to the premise , while with   evidence missing , they will be taken as not men-   tioned . Both situations are considered as not entail-   ment . This kind of bias will also contribute to the   high document - level performance of the random   baseline in Table 3 . All other retrieval methods   obtain much higher performance with the power of   evidence . However , it seems that high evidence re-   call is not promising for more accurate entailment   label prediction for hypothesis sentences . This may   be also due to evidence dependency bias . Besides ,   with evidence precision at only around 30 % , evi-   dence noise is also an important issue , the inﬂuence   of which is difﬁcult to estimate . Moreover , the high   full accuracy score means for more than 65 % hy-   pothesis sentences , our framework can ﬁnd at leastone complete evidence group and correctly pre-   dict their entailment label . Therefore , taking that   sentence - level annotation is not available during   training , our RF framework is able to give more   interpretable prediction results and help to locate   the semantic inconsistency .   4.6 Case Study   To further display the interpretability of our frame-   work , we conduct case study on the sample in Fig-   ure 1 . As shown in Figure 6 , for this long sample   ( about 667 words in total ) , the random baseline   is pretty weak and can not ﬁnd complete evidence   group for any hypothesis sentence , although it can   obtain correct prediction for the sample . This also   demonstrates the importance of evidence discov-   ery for interpretability study . By contrast , all other   retrieval methods can obtain complete evidence   groups , and contribute to correctly predict sentence-   level entailment label for most of the hypothesis   sentences . Thus the framework can give more in-   terpretable prediction results , and accurately locate   the semantic inconsistency in the third hypothe-   sis sentence . Furthermore , with the two sparse   retrieval methods , our framework can even success-   fully handle all the hypothesis sentences . However ,   the two dense methods fail to obtain complete evi-   dence group for the ﬁrst hypothesis sentence . As   shown in Figure 1 , this one requests two evidences ,   whose most words are related to the ﬁrst evidence   while only a few words are related to the second   one . Moreover , it rephrases the second one with   totally different expressions . Therefore , it is difﬁ-   cult to ﬁnd the second one . This may be the exact   situation that leads to the lower evidence recall of   dense retrieval methods than sparse ones in Table 4.31295 Related Work   Natural language inference is a fundamental yet   important task in natural language processing . For   sentence - level NLI , several benchmarks ( Bowman   et al . , 2015 ; Williams et al . , 2018 ; Wang et al . ,   2019 ; Nie et al . , 2020 ) have been proposed and   they have been attracting research attention . More-   over , Koreeda and Manning ( 2021 ) propose Con-   tract NLI targeting the legal and business domain ,   which is a small scale benchmark . The premises   are long contract documents while the hypotheses   are actually single sentences . Besides , Tian et al .   ( 2022 ) suggest to debias natural language inference   and understanding models through causal interven-   tion and counterfactual reasoning . Lin et al . ( 2022 )   adopt commonsense inference to enhance future   event generation . For document - level NLI , Yin   et al . ( 2021 ) propose the ﬁrst large scale bench-   mark DNLI based on a set of early benchmarks .   Current models for the task still largely follow   sentence - level NLI . Differently , in this paper , we   emphasize the importance of evidence discovery   and aim at a general solution for the task .   6 Conclusion   In this paper , we propose RF framework as a gen-   eral solution for DNLI task and contribute com-   plementary annotation on DNLI dataset . Our   experimental results show that our framework can   obtain state - of - the - art performance . Besides , the   framework is robust for diverse retrieval methods ,   and consistently obtains higher performance on   samples with varying length , especially longer sam-   ples . Moreover , the framework can give more in-   terpretable prediction results and help to locate the   semantic inconsistency . In the future , we will ex-   plore an end - to - end framework for the task .   Limitations   The main limitation of our RF framework is that   the framework is a pipeline one rather than an   end - to - end one . For efﬁciency , the evidence re-   trieval component is an independent unit , which   provides evidence input for the jointly trained read-   ing and fusion components . However , the evidence   retrieval component in our pipeline framework will   not bring additional heavy computation . First , it   can be conducted ofﬂine efﬁciently . The sparse re-   trieval methods only involve item frequency count-   ing , and the dense ones are based on pretrainedsentence embeddings without further ﬁne - tuning .   Second , there are many acceleration techniques for   retrieval in industrial ﬁeld .   Furthermore , we will try to improve the evidence   retrieval component in the future . On one hand ,   we will try to utilize document - level entailment   label to improve sentence - level evidence retrieval .   On the other hand , we will also explore an efﬁ-   cient end - to - end model , which may beneﬁt from   reinforcement learning ( Williams , 1992 ; Lei et al . ,   2016 ) , reparameterization trick ( Maddison et al . ,   2017 ; Jang et al . , 2017 ) , or Expectation - Maximum   algorithm ( Dempster et al . , 1977 ) . However , the   evidence dependency bias issue , discussed in Sec-   tion 4.5 , will pose a great challenge . That is , entail-   ment samples strictly request complete evidence   groups , while not entailment samples are insensi-   tive to evidence retrieval .   Acknowledgments   This research is supported by the Singapore Min-   istry of Education ( MOE ) Academic Research   Fund ( AcRF ) Tier 1 grant , under the RIE2020 In-   dustry Alignment Fund – Industry Collaboration   Projects ( IAF - ICP ) Funding Initiative , as well as   cash and in - kind contribution from the industry   partner(s ) .   References31303131   A Statistic Information of Complemen-   tary Annotation   Due to heavy workload and great complexity , we   manually annotate 100 longer samples ( all over   800 words ) randomly selected from the testset ,   which contain more than 350 hypothesis sentences   in total . Among the samples , 84 % are annotated as3132ModelEntailment Not_Entailment Total   P R F1 P R F1 Micro F1 Macro F1   Concatenation - Longformer ♣ - - 46.18 - - - - -   Concatenation - Longformer ♠ 31.09 81.33 44.99 96.60 74.64 84.22 75.47 64.60   Concatenation - RoBERTa ♠ 42.27 90.82 57.69 98.45 82.55 89.81 83.57 73.75   Semantic Match - RoBERTa / triangle31.01 84.32 45.34 97.09 73.61 83.74 74.93 64.54   oursRF - RoBERTa 45.54 86.43 59.66 97.82 85.46 91.22 85.58 75.44   Concatenation - RoBERTa ♣ 47.15 95.09 63.04 99.19 85.01 91.55 86.25 77.30   Concatenation - RoBERTa ♠ 47.04 94.82 62.89 99.15 84.99 91.52 86.20 77.21   oursRF - RoBERTa 53.70 87.10 66.43 98.01 89.43 93.53 89.15 79.98   not entailment , while 16 % are annotated as entail-   ment . However , among the hypothesis sentences ,   about 43 % are annotated as not entailment , while   about 57 % are annotated as entailment . The great   label distribution difference is natural . For entail-   ment samples , all hypothesis sentences are entailed   by the premise . However , not entailment sam-   ples can still contain entailed hypothesis sentences .   Besides , about 47 % hypothesis sentences involve   cross - sentence inference and request multiple evi-   dence sentences . Moreover , about 17 % hypothesis   sentences are annotated with more than one evi-   dence groups . These also indicate the great com-   plexity of ﬁne - grained sentence - level annotation .   B Detailed Experiment Setup   OurRFframework is implemented through Py-   torch 1.8.0 and hugging face transformers . All   experiments are conducted on a computation node   with Nvidia 40 G A100 GPUs . For evidence   retrieval , we set Kas 5 to remain top 5 sen-   tences as evidences during retrieval . We adopt   RoBERTa ( Liu et al . , 2019 ) as encoder , including   base and large version . For all experiments , we   adopt AdamW optimizer , keep a random number   seed of 42 , set max input length as 256 , and set mini   batch size as 8 with gradient accumulation step as   4 . For base encoder , we choose initial learning rate   as 1e-5 , while for large encoder , we choose initial   learning rate as 5e-6 . We train 5 epochs , evaluate   each 3750 steps , and choose the model parameters   with the highest performance on the development   set . For Gaussian Kernel Pooling , we keep 11 ker-   nels with the same width of 0.01 . However , the   mean values of the kernels come from a uniformdistribution within the interval of [ 0.0 , 1.0 ] . During   prediction , we adopt a threshold of 0.5 .   C Detailed Baseline   Since DNLI is still a new task , we adopt the   concatenation model from Yin et al . ( 2021 ) , and   modify the semantic match model from Zhong et al .   ( 2020 ) for comparison .   •Concatenation We concatenate the hypothe-   sis and the premise documents into a sequence as   input . Overlong samples will be truncated to max   input length . Then we adopt the hidden state of the   special [ CLS ] token as the sample representation   for binary classiﬁcation .   •Semantic Match We respectively encode the   hypothesis and premise documents to obtain their   own document - level vector representation . For doc-   uments exceeding the max input length , we split   them into chunks with sliding window . Then in-   spired by Chen et al . ( 2017 ) , we enhance the vector   representations of these two for classiﬁcation .   D Performance on Development Set   The detailed model performance on the develop-   ment set are shown in Table 5 and Figure 7 . The   situations are similar to those on the text set . Our   framework obtains the highest performance on the   development set . Furthermore , except for the slight   performance decrease on samples no longer than   150 words , it greatly and consistently outperforms   the strongest concatenation baseline on samples   with varying length , especially on longer samples .   These also show the strength of our RF framework   on the task.3133   E Inﬂuence of KValue   To investigate the inﬂuence of evidence retrieval ,   we also pay attention to the value of K , which is   the hyperparameter about how many sentences to   remain during retrieval . As shown in Table 6 , the   framework is pretty sensitive to the value . Specif-   ically , the highest performance is obtained with   Kas 5 . Besides , with Kas 3 , 6 or 7 , the frame-   work can also obtain promoted performance than   the concatenation baseline . However , with Kas 4 ,   it shows similar performance with the baseline . As   discussed in Section 2.2 , Kis to balance evidence   precision and recall . Lower values pursue higher   evidence precision , but may lead to evidence miss-   ing , while higher values guarantee higher evidence   recall , but may introduce too much noise . Besides ,   the choice of the value is also closely related to   data distribution . The high sensitivity to the value   indicates that the evidence retrieval process will   need further improvement , where a possible way   is to utilize the document - level entailment label to   improve it .   F Related Work on Long Document Pro-   cessing   For long document processing , two common meth-   ods are to truncate the input sequence to the max-   imum length , or cut into several independent seg-   ments with sliding window . Dai et al . ( 2019 ) pro-   pose segment - level recurrence mechanism for in-   formation interaction among segments . Beltagy   et al . ( 2020 ) simplify the self - attention mecha - KDev Test   Mi F1 Ma F1 Mi F1 Ma F1   3 84.79 74.50 83.05 72.65   4 83.68 73.14 81.68 71.30   585.58 75.44 84.38 74.22   6 84.64 74.50 83.50 73.39   7 84.34 73.95 82.50 72.31   nism ( Vaswani et al . , 2017 ) to reduce memory   overhead to handle longer input sequence . More-   over , Ding et al . ( 2020 ) suggest to introduce two in-   dependent iteratively trained models , respectively   for neural evidence retrieval and semantic infer-   ence . Wu et al . ( 2021 ) propose hierarchical in-   teractive transformer structure with stacked trans-   formers respectively for sentence and document   encoding . However , a recent study ( Park et al . ,   2022 ) on text classiﬁcation task indicates that none   of these models will consistently outperform other   models across datasets . The situation is similar   to the famous No Free Lunch principle ( Wolpert   and Macready , 1997 ) . Therefore , long document   processing may need further exploration.3134