  Jaehoon Oh   Graduate School of DS , KAIST   jhoon.oh@kaist.ac.krJongwoo Ko , Se - Young Yun   Graduate School of AI , KAIST   { jongwoo.ko , yunseyoung}@kaist.ac.kr   Abstract   Translation has played a crucial role in improv-   ing the performance on multilingual tasks : ( 1 )   to generate the target language data from the   source language data for training and ( 2 ) to   generate the source language data from the   target language data for inference . However ,   prior works have not considered the use of   both translations simultaneously . This paper   shows that combining them can synergize the   results on various multilingual sentence classi-   fication tasks . We empirically find that trans-   lation artifacts stylized by translators are the   main factor of the performance gain . Based on   this analysis , we adopt two training methods ,   SupCon and MixUp , considering translation   artifacts . Furthermore , we propose a cross-   lingual fine - tuning algorithm called MUSC ,   which uses SupCon and MixUp jointly and im-   proves the performance . Our code is available   athttps://github.com/jongwooko/MUSC .   1 Introduction   Large - scale pre - trained multilingual language mod-   els ( Devlin et al . , 2019 ; Conneau and Lample , 2019 ;   Huang et al . , 2019 ; Conneau et al . , 2020 ; Luo et al . ,   2021 ) have shown promising transferability in zero-   shot cross - lingual transfer ( ZSXLT ) , where pre-   trained language models ( PLMs ) are fine - tuned   using a labeled task - specific dataset from a rich-   resource source language ( e.g. , English or Span-   ish ) and then evaluated on zero - resource target lan-   guages . Multilingual PLMs yield a universal repre-   sentation space across different languages , thereby   improving multilingual task performance ( Pires   et al . , 2019 ; Chen et al . , 2019 ) . Recent work has en-   hanced cross - lingual transferability by reducing the   discrepancies between languages based on trans-   lation approaches during fine - tuning ( Fang et al . ,   2021 ; Zheng et al . , 2021 ; Yang et al . , 2022 ) . Our   paper focuses on when translated datasets are avail-   able for cross - lingual transfer ( XLT).Conneau et al . ( 2018 ) provided two translation-   based XLT baselines : translate - train and   translate - test . The former fine - tunes a mul-   tilingual PLM ( e.g. , multilingual BERT ) using the   original source language and machine - translated   target languages simultaneously and then evaluates   it on the target languages . Meanwhile , the latter   fine - tunes a source language - based PLM ( e.g. , En-   glish BERT ) using the original source language   and then evaluates it on the machine - translated   source language . Both baselines improve the per-   formance compared to ZSXLT ; however , they are   sensitive to the translator , including translation arti-   facts , which are characteristics stylized by the trans-   lator ( Conneau et al . , 2018 ; Artetxe et al . , 2020 ) .   Artetxe et al . ( 2020 ) showed that matching   the types of text ( i.e. , origin or translationese )   between training and inference is essential due   to the presence of translation artifacts under   translate - test . Recently , Yu et al . ( 2022 ) pro-   posed a training method that projects the original   and translated texts into the same representation   space under translate - train . However , prior   works have not considered the two baselines simul-   taneously .   In this paper , we combine translate - train   andtranslate - test using a pre - trained multilin-   gual BERT , to improve the performance . Next , we   identify that fine - tuning using the translated tar-   getdataset is required to improve the performance   on the translated source dataset due to translation   artifacts even if the languages for training and infer-   ence are different . Finally , to consider translation   artifacts during fine - tuning , we adopt two training   methods , supervised contrastive learning ( SupCon ;   Khosla et al . 2020 ) and MixUp ( Zhang et al . , 2018 )   and propose MUSC , which combines them and im-   proves the performance for multilingual sentence   classification tasks.6747   Notation Description   S given source dataset for training   T given target dataset for training   Tmachine - translated target dataset   fromSfor training   Tback - translated target dataset   fromTfor training   T given target dataset for inference   Smachine - translated source dataset   fromTfor inference   2 Scope of the Study   In this study , four datasets are used : MARC and   MLDoc for single sentence classification , and   PAWSX and XNLI from XTREME ( Hu et al . ,   2020 ) for sentence pair classification . The de-   tails of datasets are provided in Appendix A. Each   dataset consists of the source dataset for training   Sand the target dataset for inference T , where   Sis original and Tis original ( for MARC   and MLDoc ) or human - translated ( for PAWSX and   XNLI ) . For MARC and MLDoc , the original target   dataset for training Tis additionally given .   We use the given translated datasets Tfor   PAWSX and XNLI . However , for MARC and ML-   Doc , the translated datasets are not given . There-   fore , we use an m2m_100_418 M translator ( Fan   et al . , 2021 ) from the open - source library EasyNMT   to create the translated datasets . Tis translated   fromS(i.e . ,S→ T ) , and Tis back-   translated from T(i.e . ,T→ S→ T ;   Sennrich et al . 2016 ) . Similarly , for inference , S   is translated from T. The notations used in this   paper are listed in Table 1 .   We use the pre - trained cased multilingual   BERT ( Devlin et al . , 2019 ) from HuggingFace   Transformers ( Wolf et al . , 2020 ) and use accuracy   as a metric . Detailed information for fine - tuning is   provided in Appendix B.   3 Original and Translationese Ensemble   In this section , we demonstrate that the two base-   lines , translate - train and translate - test ,   are easily combined to improve performance ,   which we call it translate - all . Table 2 describes   the differences between algorithms .   Table 3 presents the results according to the infer-   ence dataset when the models are fine - tuned using   SandT. Inference on Tis a general way   to evaluate the models , i.e. , translate - train .   In addition , we evaluate the models on Slike   translate - test . Furthermore , we ensemble the   two results from different test datasets by averag-   ing the predicted predictions , i.e. , translate - all ,   because averaging the predictions over models or   data points is widely used to improve predictive   performance and uncertainty estimation of mod-   els ( Gontijo - Lopes et al . , 2022 ; Kim et al . , 2020a ) .   From Table 3 , it is shown that even if the mul-   tilingual PLMs are fine - tuned with SandT ,   the performance on the translated source data S   is competitive with that on the target data T. Fur-   thermore , ensemble inference increases the perfor-   mance on all datasets . This can be interpreted as the   effectiveness of the test time augmentation ( Kim   et al . , 2020a ; Ashukha et al . , 2021 ) , because the   results on the two testdatasets , TandS(aug-   mented from T ) , are combined .   To explain the changes in inferences via test time   augmentation , we describe the predicted probabil-   ity values on the correct label when the models are   evaluated on TandS , as depicted in Figure 1 .   The green and orange dots represent the benefits6748   and losses via the ensemble , respectively . The im-   proved performance through the ensemble means   that the number of green samples is greater than   the number of orange samples in Figure 1 .   To analyze where the performance gain comes   from , we focus on the green samples . The green   samples are concentrated around the right down   corner , which implies that wrong predictions on   Tcan be right predictions with high confidence   onS. In fact , this phenomenon is the opposite   of what we expected ; the samples are expected to   be concentrated around the y = xline , because the   semantic meaning between TandSis similar   even though the languages are different . This im-   plies that semantic meaning is not the main factor   explaining the performance gain of the ensemble .   4 Translation Artifacts for Training   To find the main factor of performance gain , we   hypothesize that matching the types of text ( i.e. ,   original or translated ) between training and infer-   ence is important even if the languages used fortraining and inference are different , by expand-   ing on Artetxe et al . ( 2020 ) . For the analysis , we   use MARC and MLDoc because they provide T ,   which has no artifacts .   Table 4 describes the results according to the   matching between texts for training and inference .   Well - matched texts are better than badly matched   ones . In particular , the results that T – Sis bet-   ter than T – Ssupport our hypothesis . This im-   plies that biasing training and inference datasets us-   ing the same translator can lead to performance im-   provement , and that translation artifacts can change   wrong predictions on Tinto right predictions on   Swhen the models are trained using T , as   shown in Section 3 .   4.1 Proposed Method : MUSC   We propose an XLT method called MUSC ,   by applying SupCon ( Khosla et al . , 2020 ) and   MixUp ( Zhang et al . , 2018 ) jointly . Namely , our   method is contrastive learning with mixture sen-   tences in supervised settings . Several works have   attempted to employ the idea of mixtures on unsu-   pervised contrastive learning ( Kim et al . , 2020b ;   Shen et al . , 2022 ) ; however , ours is the first to lever-   age the label information in a mixture . In this sec-   tion , the loss functions are formulated at batch level   with a batch size of N , and↑and↓indicate the nor-   mal and reverse order , respectively , in a batch . All   methods are designed upon the translate - all .   SupCon . We adopt SupCon , which makes the sam-   ples in the same class closer ( Gunel et al . , 2021 ) ,   to reduce the discrepancies between original and   translated texts . Namely , SupCon helps models to   learn both originality of Sand artifacts of T   comprehensively . The loss function of SupCon   ( L ) with I≡[1 , . . . , 2N]is as follows :   where Z= [ Z;Z]∈Ris the projections   of [ CLS ] token representations through an encoder   fand a projector g , i.e. , g(f(E ) ) , andzin-   dicates the i - th row of Z.Zis concatenated along   with the batch dimension and dis the dimension   of projections . The positive set of the sample i ,   P(i ) , is defined as { j|y = y , j∈I\ { i } } , where   [ y , . . . , y ] = [ y , . . . , y ] = y.   MixUp . We adopt MixUp to densify original and   translated texts , respectively . MixUp is performed   on the word embeddings by following Chen et al.6749   ( 2020 ) , because it is infeasible to directly apply   MixUp to discrete word tokens . MixUp with α∈   [ 0,1]is as follows :   ˜E = Mix(E , E ) = αE+ ( 1−α)E ,   where E = XW∈Ris the output of the   embedding layer for a given batch X∈R   with weight matrix W∈R.L,|V| , anddin-   dicate maximum sequence length , vocab size , and   dimension of word embeddings , respectively . E   is reversed along with the batch dimension . We ap-   ply MixUp between the same language to densify   each type of text . For convenience in implemen-   tation , we mix a normal batch ( ↑)and a reversed   batch ( ↓ ) , following Shen et al . ( 2022 ) . The mix-   ing process is conducted elementwisely . The loss   function of MixUp ( L ) with cross - entropy ( L )   is as follows :   L(˜Q , y ) = αL(˜Q , y ) + ( 1 −α)L(˜Q , y ) ,   where ˜Q = h(f(˜E))is the logits of [ CLS ]   token for the mixed embeddings , with an encoder   fand a classifier h.yis a set of labels in the same   batch .   MUSC . We replace the original projected represen-   tations in Lwith mixture ones , i.e. , Z→˜Zor   Z→˜Z , to use MixUp and SupCon jointly . The   loss functions of MUSC ( L ) are as follows :   We calculate L by decomposing it in two op-   posite orders , similar to L. Finally , the total loss   function ( L ) , descried in Figure 2 , is as follows:6750Table 5 describes the ablation study according   to the applied loss functions . −denotes base-   line which only applies L. Other methods in-   cludeLand additionally apply the correspond-   ing loss , respectively . It is shown that SupCon   ( L ) and MixUp ( L ) improve performance on   most datasets even when they are used separately .   The effectiveness of these losses is powerful when   dataset size is small . Moreover , our total loss   ( L ) , which includes learning a model using Sup-   Con and MixUp jointly ( L ) , outperforms both   SupCon and MixUp on all datasets . In addition ,   our total loss ( L ) brings more performance gains   than the simple conjunction of SupCon and MixUp   ( L+L ) for all datasets except for MARC   dataset . These results demonstrate that our pro-   posed MUSC effectively collaborates the SupCon   and MixUp . The optimized hyperparameters are   reported in Appendix B.   5 Conclusion   In this paper , we showed that translate - train   andtranslate - test are easily synergized from   the test time augmentation perspective and found   that the improved performance is based on transla-   tion artifacts . Based on our analysis , we propose   MUSC , which is supervised contrastive learning   with mixture sentences , to enhance the general-   izability on translation artifacts . Our work high-   lighted the role of translation artifacts for XLT .   Limitations   Our work addressed the role of translation artifacts   for cross - lingual transfer . Limitation of our work   is that we experimented for sentence classification   tasks using multilingual BERT , because it is almost   impossible to get token - level ground truths using   translator .   Ethics Statement   Our work does not violate the ethical issues .   Furthermore , we showed that a new baseline ,   translate - all , could achieve higher perfor-   mance , and proposed MUSC designed upon the   translate - all approach . We believe that var-   ious algorithms can be developed based on the   translate - all for multilingual tasks .   Acknowledgements   This work was supported by Institute of Informa-   tion & communications Technology Planning & Evaluation ( IITP ) grant funded by the Korea gov-   ernment(MSIT ) ( No.2019 - 0 - 00075 , Artificial Intel-   ligence Graduate School Program(KAIST ) , 10 % )   and Institute of Information & communications   Technology Planning & Evaluation(IITP ) grant   funded by the Korea government(MSIT ) ( No.2022-   0 - 00641 , XV oice : Multi - Modal V oice Meta Learn-   ing , 90 % )   References675167526753A Dataset Description   MARC ( Keung et al . , 2020 ) is Amazon review   classification dataset . MLDoc ( Schwenk and   Li , 2018 ) is news article classification dataset .   PAWSX ( Yang et al . , 2019 ) is paraphrase identi-   fication dataset . XNLI ( Conneau et al . , 2018 ) is   natural language inference dataset .   B Implementation Detail   Learning rate and λare searched by grid from [ 1e-   5 , 3e-5 , 5e-5 ] and from [ 0.1 , 0.5 , 0.9 ] , respectively .   Fine - tuning epochs are 4 , 10 , 4 , and 2 on MARC ,   MLDoc , PAWSX , and XNLI , respectively . The   batch size is 32 for all datasets . The evaluation   is executed every 300 batches on all languages .   Table 7 describes the optimized hyperparameters .   C XNLI resultsD Additional Related Works   Cross - lingual Transfer . As the recent advances in   NLP demonstrate the effectiveness of pre - trained   language models ( PLMs ) like BERT ( Devlin et al . ,   2019 ) and RoBERTa ( Liu et al . , 2019 ) , the per-   formances of XLT rapidly improve by extend-   ing the monolingual PLMs to the multilingual   settings ( Conneau and Lample , 2019 ; Conneau   et al . , 2020 ) . While these multilingual PLMs   show state - of - the - art performances in ZSXLT ,   one promising approach for improving the cross-   lingual transferability is instance - based transfer   by translation such as translate - train and   translate - test ( Conneau et al . , 2018 ) . Due to   the effectiveness and acceptability of translation ,   most recent works ( Fang et al . , 2021 ; Zheng et al . ,   2021 ; Yang et al . , 2022 ) focus on better utilization   of translation .   Test - time augmentation . Data augmentation ,   which expands a dataset by adding transformed   copies of each example , is a common practice   in supervised learning . While the data augmen-   tation is also widely used in XLT ( Zheng et al . ,   2021 ) during training models , it can also be used at   the test time to obtain greater robustness ( Prakash   et al . , 2018 ) , improved accuracy ( Matsunaga et al . ,   2017 ) , and estimates of uncertainty ( Smith and Gal ,   2018 ) . Test time augmentation ( TTA ) combines   predictions from a multi - viewed version of a sin-   gle input to get a “ smoothed ” prediction . We also   point out that using translation with XLT can be   viewed as TTA , which can get performance gain   from a different view of original and translation sen-   tences . In this direction of the necessity of study   for TTA ( Kim et al . , 2020a ) , we propose better uti-   lization of translation artifacts in XLT .   Translation artifacts . “ Translationese ” can be   referred to as characteristics in a translated text   that differentiate it from the original text in the   same language . While the effect of translationese   has been widely studied in translation tasks ( Gra-   ham et al . , 2020 ; Freitag et al . , 2020 ) , the effi-   cacy of translationese in XLT is under - explored .   Artetxe et al . ( 2020 ) and Kaneko and Bollegala   ( 2021 ) investigate the effect of translationese in   translate - test and ZSXLT settings , however ,   these are apart from general training approach of   XLT . Recently , Yu et al . ( 2022 ) firstly attempt to   study translate - train , which focuses on single   QA task.6754