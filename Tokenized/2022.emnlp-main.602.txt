  Anubhav JangraPreksha NemaAravindan Raghuveer   Google Research , India   { anubhavjangra , preksh , araghuveer } @google.com   Abstract   Unavailability of parallel corpora for training   text style transfer ( TST ) models is a very chal-   lenging yet common scenario . Also , TST   models implicitly need to preserve the content   while transforming a source sentence into the   target style . To tackle these problems , an in-   termediate representation is often constructed   that is devoid of style while still preserving the   meaning of the source sentence . In this work ,   we study the usefulness of Abstract Meaning   Representation ( AMR ) graph as the interme-   diate style agnostic representation . We posit   that semantic notations like AMR are a natu-   ral choice for an intermediate representation .   Hence , we propose T - STAR : a model com-   prising of two components , text - to - AMR en-   coder and a AMR - to - text decoder . We pro-   pose several modeling improvements to en-   hance the style agnosticity of the generated   AMR . To the best of our knowledge , T - STAR   is the ﬁrst work that uses AMR as an intermedi-   ate representation for TST . With thorough ex-   perimental evaluation we show T - STAR signif-   icantly outperforms state of the art techniques   by achieving on an average 15.2 % higher con-   tent preservation with negligible loss ( ∼3 % ) in   style accuracy . Through detailed human evalu-   ation with 90,000ratings , we also show that T-   STAR has upto 50 % lesser hallucinations com-   pared to state of the art TST models .   1 Introduction   A well accepted deﬁnition of style refers to the   manner ( via linguistic elements like word choices ,   syntactic structures , metaphors ) in which semantics   of a sentence are expressed ( McDonald and Puste-   jovsky , 1985 ; Jin et al . , 2020 ) . Text Style Transfer   ( TST ) is the task of rephrasing an input sentence to   contain speciﬁc stylistic properties without altering   the meaning of the sentence ( Prabhumoye et al . ,   2018 ) . We refer the reader to Jin et al . ( 2020 ) for adetailed survey of approaches towards TST prob-   lem formulation , metrics and models . In the practi-   cal scenario , that we consider in this paper , where   a large corpus of parallel data is not available ( Niu   and Bansal , 2018 ; Ma et al . , 2020 ; Wu et al . , 2020 ) ,   two family of approaches have been proposed in the   literature ( Jin et al . , 2020 ) . 1 . Disentanglement :   Content and Style are disentangled in a latent space   and only the style information is varied to trans-   form the sentence . 2 . Prototype Editing : Style   bearing words in the source sentence are replaced   with those corresponding to the target style . The   sentence may be further re - arranged for ﬂuency and   naturalness . Both the above approaches have draw-   backs in the way the style agnostic intermediate   representation is constructed , described as follows .   First , in the disentangling approaches , it is not easy   to verify the efﬁcacy of separation between style   and content . Recent approaches ( Subramanian   et al . , 2018 ; Samanta et al . , 2021 ) even state that as   content and style are so subtly entangled in text , it   is difﬁcult to disentangle both of them in a latent   space . Consequently , this affects the model ’s inter-   pretability in that it is hard to attribute an effect we   see in the output to the latent intermediate vector   or the style vector . Second , with prototype edit-   ing approaches for lingustic styles ( such as author ,   formality ) , where the content and style are tightly   coupled it is not feasible to segregate style and con-   tent carrying words ( word examples : cometh , thou ) .   Furthermore , style - marker detection is a non - trivial   NLP task and needs to be addressed for every new   style that is added to the system causing scalabilty   concerns ( Jin et al . , 2020 ) . In this paper , we pro-   pose T - STAR ( Truthful StyleTransfer using AMR   Graph as Intermediate Representation ) that uses a   symbolic semantic graph notation called Abstract   Meaning Representation ( AMR ) as the style ag-   nostic intermediate stage . AMR ( Banarescu et al . ,   2013 ) is designed to capture semantics of a given   sentence in all entirety while abstracting away the8805   syntactic variations , inﬂections and function words .   In other words , two sentences with the same mean-   ing but written in very different styles will have   a very similar AMR if not exactly the same ( See   Figure 1 ) .   This addresses the shortcomings with Disentan-   glement andPrototype Editing approaches . First ,   AMR being a representation with well - deﬁned se-   mantics , we can inspect , interpret and measure   quality of the intermediate representation and the   provenance of knowledge transfer between source   and target sentence . Second , AMR being a well   known standard has high quality , robust reference   implementations , especially for head languages   ( e.g. , English ) . Our contributions are as follows :   1.We propose a novel TST approach with AMRs ,   an interpretable , symbolic intermediate representa-   tion , to achieve better content preservation . To this   end , we enhance AMR parsing techniques to better   suit the TST task . To the best of our knowledge ,   we are the ﬁrst work to use AMR representations   for the style transfer task ( Sections 3 , 4 ) .   2.Through novel experimentation , we show that   an AMR , as a style agnostic intermediate represen-   tation , has better content preservation and less style   information of the given source sentence compared   to competitive baselines.(Sections 5 , 6 )   3.On multiple datasets we show T - STAR is able to   beat competitive baselines by producing sentences   with signiﬁcantly higher content preservation with   similar style transfer scores . ( Section 7 )   4.With thorough human evaluations spanning   90,000 ratings we show T - STAR has ∼70 % better   content preservation compared to state of art base-   line with 50 % lesser hallucinations . ( Section 8)   2 Related Work   2.1 Unsupervised Text Style Transfer   TST systems broadly use two family of approaches :   disentanglement ( Shen et al . , 2017 ) and prototype - editing ( Jin et al . , 2020 ) . Prior works ( Hu et al . ,   2017 ; John et al . , 2019 ; Fu et al . , 2017 ; Singh and   Palod , 2018 ; Logeswaran et al . , 2018 ) disentangle   the content and style information in latent space   using style - based classiﬁer or adversarial learning .   Prototype - editing based approaches ( Li et al . , 2018 ;   Madaan et al . , 2020 ; Sudhakar et al . , 2019 ) are used   to gain better controllability and interpretability .   Recently , some works propose to jointly opti-   mize for content and style information , to over-   come the limitations of explicitly disentangling   the style and content information . Yamshchikov   et al . ( 2019b ) illustrates that architectures with   higher quality of information decomposition per-   form better on style transfer tasks . Subramanian   et al . ( 2018 ) argues that it is often easy to fool style   discriminators without explicitly disentangling con-   tent and style information , which may lead to low   content preservation ( Xu et al . , 2018 ) . Instead , Sub-   ramanian et al . ( 2018 ) ; Logeswaran et al . ( 2018 )   use back - translation to optimize for content preser-   vation . Luo et al . ( 2019 ) ; Liu et al . ( 2020 ) use   reinforcement learning framework with explicit re-   wards designed for content preservation . Wang   et al . ( 2019 ) pushes the entangled latent representa-   tion in the targeted - style direction using style dis-   criminators . Samanta et al . ( 2019 ) uses normaliz-   ing ﬂow to infuse the content and style information   back before passing it to the decoder .   Cheng et al . ( 2020 ) proposes a context aware   text style transfer framework using two separate en-   coders for input sentence and the context . Similar   to our work , Krishna et al . ( 2020 ) also generates   an interpretable intermediate representation . The   authors ﬁrst paraphrase the given source to ﬁrst con-   vert it to a destylized version before passing it to   the targeted style - speciﬁc decoder . Complementary   to our work , ( Shi et al . , 2021 ) uses syntactic graphs   ( dependency tree ) as an intermediate representation   for attribute transfer to retain the linguistic struc-   ture . On the other hand , we focus on retaining the   semantics using AMR graphs as intermediate rep-   resentation and modifying the linguistic structure   ( authorship style ) .   2.2 Text ⇐ ⇒ AMR   In order to improve the parsing performance for   AMRs , neural models are receving increasing atten-   tion . Neural AMR parsers can be divided into fol-   lowing categories : i ) sequence - to - sequence based   AMR - parsers ( Xu et al . , 2020a ) , ii ) sequence - to-8806   graph based AMR parsers ( Zhang et al . , 2019 ) ,   where the graph is incrementally built by spanning   one node at a time . A more detailed survey on   related works can be found in Appendix A.   3 AMR as an Intermediate   Representation   Abstract Meaning Representations ( AMRs ) is a se-   mantic formalism construct that abstracts away the   syntactic information and only preserves the seman-   tic meaning , in a rooted , directed and acyclic graph .   In Figure 1 , we present certain syntactic variations   ( changing the voice , and tense ) for a sentence with-   out altering meaning . All variations result in the   same AMR graph . The nodes in the AMR graphs   ( “ produce-01 ” , “ they ” , “ before ” , etc . ) are con-   cepts ( entities / predicates ) that are canonicalized   and mapped to semantic role annotations present in   Propbank framesets . The edges ( “ ARG0 ” , “ time ” ,   “ duration ” , etc . ) are then relations between these   concepts . In other words , AMRs aim at decoupling   “ what to say ” from “ how to say ” in an interpretrable   way . We posit that this could be beneﬁcial for text   style transfer , where the goal is to alter the “ how to   say ” aspect while preserving “ what to say ” .   Recently , semantic meaning representation prop-   erty of AMRs has been shown to be useful in   other generation tasks . In abstractive summariza-   tion , Liao et al . ( 2018 ) uses AMR as an intermedi-   ate representation to ﬁrst obtain a summary AMR   graph from document and then generate a summary   from it . Hardy and Vlachos ( 2018 ) use AMRs to   cover for the lack of explicit semantic modeling in   sequence - to - sequence models . In machine transla-   tion , Song et al . ( 2019 ) adopted AMRs to enforce   meaning preservation while translating from En-   glish to German . For Paraphrase Generation , Caiet al . ( 2021 ) found that using AMRs as intermedi-   ate representation reduces the semantic drift . More-   over , incorporating symbolic representations as an   intermediate representation provides a way to ef-   fectively understand the reasons behind a model ’s   shortcomings . We utilize this advantage to analyse   the weaknesses of T - STAR in Section 8.2 .   In order to demonstrate the semantic meaning   preservation property of AMRs , we design an ex-   periment using three publicly available paraphrase   benchmarks , i.e. , MRPC ( Dolan and Brockett ,   2005 ) , QQP , and STS . MRPC and QQP are sen-   tence pair datasets with each pair labeled yesif they   are paraphrases of each other nootherwise . STS   dataset assigns a scores from 0 ( not similar ) to 5   ( exactly similar ) to a sentence pair . We hypothesize   that if AMRs are indeed semantic meaning preserv-   ing , two sentences with similar meaning should   have highly similar AMRs . To measure the similar-   ity between two AMRs , we use the S score   ( Cai and Knight , 2013 ) that calculates the number   of overlapping triplets between two AMRs . We use   an off - the - shelf AMR parserto generate AMR   given a sentence . We plot the distribution of the   S scores for MRPC , QQP and STS datasets   in Figure 2 . For MRPC , we infer that the S   scores for paraphrases are signiﬁcantly higher than   theS scores for non - paraphrases . Simi-   larly , for QQP , quartile distribution of S   scores for paraphrases is higher in comparison to   non - paraphrases . For STS dataset , we observe a   gradual increase in quartile distribution of S   scores as we move towards more similar sentences .   The experiments above corroborate the claim   that AMRs can preserve meaning under lexical vari-   ations like paraphrasing , tense and voice changes.8807   Recent research discussed earlier , have success-   fully used this property to show task improvements .   Building on the above qualitative , quantitative and   prior research evidence , we further explore the ap-   plicability of AMR for the TST task .   4 Proposed Solution   Our proposed model T - STAR consists of two mod-   ules ( refer Figure 3 ) . First , T - STAR Encoder gen-   erates an AMR given source sentence in style i.   Second , T - STAR Decoder generates a sentence in   stylejwith similar meaning as preserved in the   generated intermediate AMR . We take T5 - Base   ( Raffel et al . , 2020 ) pre - trained model as our basic   seq2seq architecture for both the modules . In order   to use AMR as a sequence in T5 , we borrow the   choice of DFS Traversal from ( Bevilacqua et al . ,   2021 ) , that thoroughly study the effect of various   traversals on AMR parsing .   4.1 T - STAR Encoder   We train our simplest encoder , called the vanilla   T - STAR Encoder , by ﬁne - tuning T5 base with the   open source AMR 3.0 dataset ( Knight et al . , 2021 ) .   The AMR 3.0 dataset consists of roughly 59,000   generic English sentences ( denoted as sfor the   isentence ) and their corresponding AMRs ( de-   noted asA ) . In a qualitative analysis , we observe   that the vanilla T - STAR encoder under performs   in two signiﬁcant ways as illustrated in Table 1 .   First , style bearing words ( such as “ shew ” ) be-   come concepts in the AMRs ( Sentence-1 in Ta-   ble 1 ) as opposed to being canonicalized to their   respective propbank role annotations . Second , the   meaning of the stylized sentences get incorrectly   encoded in the AMRs , as shown in the second ex-   ample in Table 1 . To overcome this , we propose a   style - agnostic ﬁne - tuning strategy as follows .   Style - agnostic Fine Tuning : We hypothesize that   vanilla Text to AMR encoder is unable to effec-   tively transform stylized sentences to AMRs be-   cause it has only been trained on generic En-   glish sentences . Therefore , we propose a data-   augmentation strategy ( refer to Figure 4 ) , where   we use an off the shelf style transfer model , e.g. ,   STRAP ( Krishna et al . , 2020 ) , to stylize a generic   English sentence sin stylep(ˆs ) . While convert-   ingstoˆs , we alter the style of original sentence ,   keeping the meaning intact . For a high quality   synthetic dataset , we ﬁlter out samples with low se-   mantic similarity between sandˆs . We provide a   detailed empirical analysis on this ﬁltering strategy   in Appendix C. Since the meaning is preserved , we   can now map ˆsto the same AMR A. We then   ﬁne - tune our T - STAR Encoder on ¯S = S∪ˆS ,   where ˆS={ˆs , A}∀p∈PandPis the total   number of styles in the dataset .   4.2 T - STAR Decoders   Due to the unavailability of parallel style corpora ,   we are provided Pmono - style corpora R=   { r}written in style p , wherePrefers to the   total number of styles and rrefers to the isen-8808   Algorithm 1 : Iterative T - STAR   tence of thepstyle dataset which has Msentences .   Training a style speciﬁc decoder ( f ( . ) ) to generate   a sentence in style pfrom an AMR consists of two   steps . First , we use our ﬁne - tuned T - STAR En-   coder ( Section 4.1 ) to generate AMRs ˆAfor every   sentence rfor every style corpora . Second , we   ﬁne tune a T5 base model to recover the original   sentence rgiven the AMR ˆA , obtaining style-   speciﬁc decoders . In other words , we ﬁne tune   usingMpairs of ( r,ˆA ) constructed from the   ﬁrst step . Note that , we experimented with a data   augmentation technique for the decoders as well ,   however it did not lead to an improvement in the   style transfer performance ( refer to Appendix E ) .   Once style speciﬁc decoders have been trained   for every style in P , we can use the T - STAR En-   coder in tandem with the T - STAR Decoders to   convert between arbitrary style combinations as   in Krishna et al . ( 2020).4.3 Iterative T - STAR   The performance of our modules , T - STAR Encoder   and T - STAR Decoders depends on the quality of   the synthetic datasets ( ˆS,ˆR ) generated by their   complementary modules . We adopt the iterative   back - translation technique used in unsupervised   machine translation ( Hoang et al . , 2018 ) . Iteration   proceeds in rounds of training an encoder and de-   coder from mono - style data . In every round , we   aim to iteratively improve the quality of the encoder   and decoder modules by generating increasingly   better synthetic data from the previous round . We   brieﬂy describe this process in Algorithm 1 .   5 Experimental Setup   In this section we brieﬂy describe the various T-   STAR variations that are analyzed in the subse-   quent sections , baselines , and the implementation   details . The models are validated against the met-   rics summarized in Table 2 .   5.1 T - STAR variations   Vanilla T - STAR : The T - STAR Encoder used in   this version , is only trained on AMR 3.0 dataset ,   and not ﬁnetuned for stylized sentences .   T - STAR : We train the encoder and decoders using   Algorithm 1 for only one iteration .   Iterative T - STAR : We follow two iterations of Al-   gorithm 1 to obtain better quality synthetic dataset .   5.2 Baselines   UNMT : ( Subramanian et al . , 2018 ) models style   transfer as unsupervised machine translation task .   DLSM ( He et al . , 2020 ) is a deep generative model   that uniﬁes back - translation and adversarial loss.8809RLPrompt ( Deng et al . , 2022a ) uses a discrete   prompt optimization approach using reinforcement   learning . It is adopted in a zero - shot setting where   we use Distil - BERT ( Sanh et al . , 2019 ) and run the   optimization for 1k steps .   STRAP ( Krishna et al . , 2020 ) ﬁrst normalizes the   style information by paraphrasing the source text   into generic English sentence , which is then passed   through a style - speciﬁc GPT-2 based model to gen-   erate the styled output .   5.3 Datasets   We evaluate performance of T - STAR on two En-   glish datasets that capture Linguistic Styles ( Jin   et al . , 2020 ) . First , Shakespeare Author Imitation   Dataset ( Xu et al . , 2012 ) consists of 18 K pairs of   sentences written in two styles . Original Shake-   speare ’s plays have been written in Early Mod-   ern English , a signiﬁcantly different style . Sec-   ond , Corpus of Diverse Styles ( CDS ) ( Krishna   et al . , 2020 ) . This dataset consists of non - parallel   sentences written in 11 different styles . We will   present our results on a subset of four styles : Bible ,   Poetry , Shakespeare and Switchboard which con-   sists of 34.8K,29.8K,27.5K,148.8 K instances   respectively ( CDS uses MIT License ) .   5.4 Implementation Details   We use pre - trained T5 - Base model architecture for   both the encoder and decoder . Following iterative   back translation literature ( Kumari et al . , 2021 ) ,   we run Iterative T - STAR for two iterations . The   AMRs are preprocessed and postprocessed based   on the method mentioned in Appendix B. The best   modules are selected based on the performance   on validation set . Finer details about the model   architecture and hyperparameters can be found in   Appendix B.   6 Robustness of AMRs for Text Style   Transfer   An ideal style agnostic intermediate representation   for TST should a ) encode complete semantic infor-   mation and b ) minimal style information . We quan-   titatively measure AMR ’s efﬁcacy in these two di-   mensions . We compare T - STAR with STRAP ( Kr-   ishna et al . , 2020 ) for it uses a human readable   intermediate representation as well .   6.1 Semantic Containment of AMR   If semantics of input sentence is completely pre-   served in the intermediate representation , we   should be able to reconstruct the input sentence   from it . To evaluate the robustness of AMRs across   all styles for content preservation , we ﬁrst generate   intermediate AMR given the sentence in style pus-   ing our encoder and then reconstruct the same sen-   tence using our decoder f(.)in stylep . We study   how close the generated sentence ( from STRAP , T-   STAR ) is with respect to the original sentence . We   can infer from Table 3 that AMRs as an interme-   diate representation performs signiﬁcantly better   on content preservation as compared to STRAP   across all the styles . Speciﬁcally , it gives an av-   erage of 0.10and0.06absolute improvement on   SIM and WMD scores across the four styles re-   spectively . Also , we get comparable performance   on retaining the original style . We present an ab-   lation study on AMR parser in Appendix C , and   propose a new unsupervised Text - AMR evaluation   metric to measure the content preservation of AMR   parsing along with its results on the CDS dataset in   Appendix D.   6.2 Style Agnosticity of AMR   A style classiﬁer Cassigns a style class label to   an input sentence S. IfSdoes not encode style   information , it should result in poor classiﬁer per-   formance . We use this observation to design an ex-8810periment to evaluate the style - agnosticity of AMRs   as follows . We train three versions of 4 - way style   classiﬁer using original sentences , paraphrased sen-   tences ( as used in STRAP ) and AMRs as the input   sequences to the classiﬁer . Two models are used   to generate AMRs : Vanilla T - STAR Encoder and   T - STAR Encoder . For all versions of the style clas-   siﬁer , we mask content bearing words like entities ,   numbers , common nouns and AMR tags in the in-   put sequences .   The accuracy of the 4 classiﬁers is shown in   Table 4 . First , even Vanilla T - STAR Encoder has   lower classiﬁer accuracy for three out of four styles   in comparison to original and paraphrases sen-   tences . Second , with the T - STAR Encoder , we ob-   serve further reduction in classiﬁer ’s performance ,   and obtain an average absolute drop of 15.19 %   and 7.1 % as compared to paraphrase and Vanilla   T - STAR Encoder respectively .   We illustrate some examples in Table 1 , where   the T - STAR Encoder generates better AMRs as   compared to the Vanilla T - STAR Encoder . In   the ﬁrst example , the T - STAR Encoder , unlike the   vanilla T - STAR Encoder , is able to map style-   speciﬁc word “ shew ” to the valid concept “ show ”   and also associate “ existence ” to it . In the sec-   ond example , the T - STAR Encoder does not split   the AMR into two sentences while parsing . Addi-   tionally , it is able to make the association between   “ you ” and “ youth ” . Through the above quantitative   and qualitative analysis we demonstrate that the T-   STAR Encoder generates AMRs which are robust   in preserving meaning of the source sentence while   predominantly losing style information .   7 Performance on Style Transfer Tasks   In this section , we compare our model performance   T - STAR and Iterative - T - STAR to the baselines   across two datasets : Shakespeare Imitation Dataset   and CDS dataset .   7.1 Performance Analysis on Shakespeare   Imitation Dataset   In Table 5 , we present the results of T - STAR and   Iterative - T - STAR in comparison with the baselines   for the Shakespeare Author Imitation dataset for   both the directions : original to modern style and   vice versa . The Weighted Style Accuracy is the pri-   mary metric as it is shown to effectively combine   both style accuracy and content preservation ( Kr-   ishna et al . , 2020 ) .   We observe that our model , T - STAR slightly per-   forms better than STRAP model for original to   modern style , but has lower performance for mod-   ern to original style . However Iterative T - STAR ,   outperforms all the baselines for both the direc-   tions on Weighted Style Accuracy . We observe   that STRAP has very high style accuracy that it   achieves by compromising on Content Preserva-   tion . Through human evaluation ( Section 8) we see   STRAP employs signiﬁcantly higher hallucinations   to achieve style transfer . Vanilla T - STAR on the   other hand achieves high content preservation via   signiﬁcant copying from source as seen by the sub-   stantially high self - BLEU score . Iterative T - STAR   ﬁnds the middle ground of achieving style transfer   while not compromising on content preservation .   We also found that the length of generated sentence   is similar to the input sentence , with on average   one word difference . In the subsequent sections ,   we compare T - STAR to only the best performing   baseline , STRAP .   7.2 Performance Analysis on CDS Dataset   In Table 6 , we compare the performance of T - STAR   and Iterative T - STAR against STRAP and Vanilla   T - STAR , across all 12 directions for { Poetry , Shake-   speare , Switchboard , Bible } styles . We make the   following observations : First , both our models   T - STAR and Iterative T - STAR outperform the state-   of - the - art baseline , STRAP , on 11 out of 12 direc-   tions , with an average absolute improvement of   7.7 % and 9.7 % respectively . Second , Vanilla T-   STAR is observed to be a stronger baseline than   STRAP , as it beats STRAP on 8 out of 12 direc-8811   tions . Third , when we compare iterative T - STAR   against vanilla T - STAR , we beat it in 7 out of 12   directions , where the average absolute improve-   ment across these 7 directions is 8 % , as compare   to the average absolute loss in 5 out of 12 styles is   2 % . Fourth , Iterative T - STAR model outperforms   T - STAR model on 9 out of 12 directions , which   shows that the improvement in synthetic dataset   quality , is boosting the performance of the down-   stream task . Consistent with the ﬁndings of the   previous experiment , we see Iterative T - STAR is   able to ﬁnd the middle ground of transferring style   without compromising on content preservation.7.2.1 Qualitative Analysis   In Table 7 , we enumerate few examples with gener-   ated stylized sentences using STRAP and T - STAR   variations . We can infer that , although STRAP   performs well in transforming the sentence to the   given style , it alters the meaning ( row 1,3 , and 4 ) .   On the other hand , vanilla T - STAR does not always   transform the style ( row 1 and 2 ) . However , with   T - STAR and Iterative T - STAR are able to transform   the sentences while keeping the style intact .   We further quantify these observations through   an extensive set of human annotations as described   below .   8 Human Evaluations   Automatic metrics are insufﬁcient to thoroughly   understand subjective quality measures like content   preservation . Therefore , we conduct an extensive   case study with human evaluations . Our analysis   is two folds , ﬁrst we compare STRAP with our   models T - STAR and Iterative T - STAR on meaning   preservation . Second , we further understand the   various categories of meaning loss failures .   For both the human evaluation tasks below , the   criteria for choosing annotators were i ) proﬁcient in   English with a minimum education of Diploma . ii )   The annotators have to ﬁrst qualify on two simpler   questions , else they are not allowed to continue   on the task . Each instance is annotated by three   taskers , and the ﬁnal annotation is a majority vote .   8.1 Comparison on Meaning Preservation   In order to study the faithfulness of the T - STAR   models , we do a side - by - side human evaluation . In   this task , a source sentence is shown with 2 styl-   ized target sentence ( one from T - STAR and another   from STRAP ) . We present the annotators with three   options to judge content preservation with respect   to source sentence : option on left better than one   on right , right better than left and both equal .   We extensively compare the two models across   all 12 directions for four styles . For each direction   we randomly sample 500 instances . Each instance   is rated by 3 annotators leading to a total of 18,000   ratings . We summarize our ﬁndings in Table 8 .   Both T - STAR ad Iterative T - STAR signiﬁcantly out-   perform STRAP in terms of being better at content   preservation ( The > STRAP column in Figure 8) .   Further Iterative T - STAR has 7 % higher meaning   preservation compared to T - STAR . In addition to   the quantitative content preservation metrics dis-8812   cussed in Section 7 , this analysis gathers additional   qualitative evidence towards AMRs as an effective   intermediate representation for content preserving   style transfer . The complete statistics per direction   are available in Appendix F.   8.2 Error Analysis   In the next study , we further aim to study the nature   of meaning loss errors made by style tranfer models .   We categorize these errors into three categories i )   Hallucinations : new information not present in   the source sentence is added to target ii ) Semantic   Drift : the target sentence has a different meaning   to source sentence iii ) Incomplete : some important   content information is missed in the target . The   taskers also have the option to select “ No Error ” if   the meaning is preserved in the generated target .   As in the previous experiment , we collect 18,000   ratings and the results are summarized in Table 9 .   We observe that our models T - STAR and Itera-   tive T - STAR consistently beat STRAP in the “ No   Error ” category . Furthermore , the amount of hallu-   cinations signiﬁcantly drops to 24.46 % and further   22.6 % with Iterative T - STAR across all styles from39.3 % for STRAP . Reduction in hallucination can   be clearly seen as a beneﬁt of encoding critical   information in the source sentence using a seman-   tic parse representation like AMR . As a sign of   improving the AMR parsing quality , we see that   iterative T - STAR further reduce the Incomplete to   15.5 % . For further details refer to Appendix F.   8.2.1 Usefulness of an interpretable   intermediate representation   With intermediate AMRs being interpretable , it is   possible to broadly understand if such errors are   emerging from either encoder or decoder module .   To intuitvely understand the reason for high number   inIncomplete andSemantic Drift errors , we qualita-   tively analyzed some instances along with the gen-   erated intermediate AMRs . We have listed down   these examples in Appendix F. We observed that   forIncomplete errors , the generated AMRs were   not encoding complete information , and thus this   error percolated from the T - STAR encoder . For the   majority of the instances , either some entities were   missing , and if the clause was separated using “ : , ; ” ,   it was not parsed in the intermediate graph . Seman-   tic Drift errors indicate shortcomings in both the   modules , for some instances the encoder is not ab-   stracting out the meaning efﬁciently and for others   the decoder is not able to generate sentences with   the meaning encoded in the intermediate AMR .   9 Conclusion   We explored the use of AMR graphs as an inter-   mediate representation for the TST task . We see   that the performance of the proposed method T-   STAR surpasses state of the art techniques in con-   tent preservation with comparable style accuracy .   Through qualitative analysis we show that obtain-   ing very high style accuracy scores without altering   meaning is indeed a challenging problem.881310 Limitations   Some of the limitations for T - STAR based models   are the following . First , although our proposed   models are performing better in the joint objective   of content preservation and style transfer , but they   are not able to outperform vanilla T - STAR ( overall   best performing model for CP ) and STRAP ( overall   best performing model for ST ) . This is a promising   future direction , to keep boosting the performance   on both the directions without comprising on the   other dimension . Second , we are not incorporat-   ing graph structure in our models , and thus there   could be some information loss while interpreting   and generating the AMRs . Third , based on our er-   ror analysis , although our T - STAR encoder is able   to generate better AMRs for stylized sentences as   compared to vanilla T - STAR model , we are gener-   ating signiﬁcant incomplete AMRs that are missing   out on important entities and relations to preserve   meaning of source sentence . Fourth , similar to   prior research to generate synthetic dataset , initial   iteration of our model are dependant on an existing   off the shelf TST model , however the quality of   the generated AMRs improves signiﬁcantly using   the described data augmentation strategy . Fifth ,   our work is dependant on a robust AMR parsing   approach , which makes it challenging to adopt our   approach for other languages . However , with the   recent advancements in multilingual AMR parsing ,   it will be feasible in upcoming future works .   References8814881588168817A Extended Related Works   A.1 TST Metrics   Table 2 summarizes different metrics that have   been used to measure content preservation and   style transfer efﬁcacy . Yamshchikov et al . ( 2020 )   presents a comprehensive analysis and categoriza-   tion of several such metrics with respect to human   evaluations . Tikhonov et al . ( 2019 ) also points out   some ﬂaws in traditional evaluation techniques and   insists on using human written reformulations for   better evaluation .   A.2 Text to AMR   Recent works ( Bevilacqua et al . , 2021 ; Cai and   Lam , 2020 ; Zhou et al . , 2020 ) for text - to - AMR   task have pushed the SOTA , that makes it feasible   to automatically construct AMR given a sentence .   As a consequence , semantic - preserving NLG tasks ,   such as Neural Machine Translation ( Song et al . ,   2019 ; Xu et al . , 2020b ) , Abstractive Summariza-   tion ( Takase et al . , 2016 ) , and Question Decomposi-   tion for multi - hop question answering ( Deng et al . ,   2022b ) use AMRs as intermediate representations .   However , AMRs have not been explored for style   transfer tasks before our work .   The increase in AMRs being adopted for several   seq2seq tasks is due to the boost in the quality of   AMR parsers . Earlier works , relied on statistical   approaches ( Peng et al . , 2017 ; Flanigan et al . , 2014 ,   2016 ) to generate AMRs for a given piece of text .   With the emergence of deep learning , various AMR   parsers are being proposed , which can be divided   into following categories : i ) sequence - to - sequence   based AMR - parsers ( Xu et al . , 2020a ) , ii ) sequence-   to - graph based AMR parsers ( Zhang et al . , 2019 ) ,   where the graph is incrementally built by spanning   one node at a time . More recently , several works   have adopted pretrained models for AMR parsers ,   and have observed a boost in performance . Bai et al .   ( 2022 ) uses BART model and posit the AMR pars-   ing task as a seq2seq task , and generates a traversal   of the AMR parser as the output . Bai et al . ( 2022 )   incorporates a pretraining strategy to better encode   graph information in the BART architecture . Xu   et al . ( 2020a ) uses sentence encoding generated   from BERT model . In this work , we adopt the pre-   trained technique based AMR parser , to generate   high quality AMRs for the given stylized sentences . Although off - the - shelf AMR parsers work well   for some problems ( Fan and Gardent , 2020 ) , they   often need to be modiﬁed to be useful in the down-   stream tasks . For instance , Deng et al . ( 2022b )   proposed graph segmentation strategy to perform   question decomposition on a multi - hop query .   Xia et al . ( 2021 ) and Du and Flanigan ( 2021 )   illustrated that using silver data augmentation can   help improve in the task of AMR parsing . In this   work , we also illustrate the beneﬁt of using silver   data towards improving the style agnosticity of   AMR graphs as an intermediate representation .   A.3 AMR to Text   Similar to text - to - AMR models , AMR - to - text   frameworks can also be categorised into two types   - i ) sequence - to - sequence generation frameworks   ( ) , ii ) graph - encoder based frameworks ( Song et al . ,   2018 ; Wang et al . , 2021 , 2020 ) . Bai et al . ( 2020 )   propose a decoder that back - predicts projected   AMR graphs to better preserve the input meaning   than standard decoders . Bai et al . ( 2022 ) argues that   PLMs are pretrained on textual data , making is sub-   optimal for modeling structural knowledge , and   hence propose self - supervised graph - based training   objectives to improve the quality of AMR - to - text   generation .   B Implementation Details   Offensive language We used the “ List of Dirty ,   Naughty , Obscene or Otherwise Bad Words”to   validate that the source and the generated target   text do not contain any offensive text .   Model Architecture : We use a standard   t5 - base encoder - decoder model as described in   ( 2020 ) . The pre - trained HuggingFaceT5 trans-   former is used for both text - to - AMR and AMR - to-   text parts of the proposed architecture . The model   is pre - trained on the Colossal Clean Crawled Cor-   pus ( C4)comprising of∼750 GBs worth of text   articles . The model comprises of 220 billion pa-   rameters , and is pre - trained for 29steps before   ﬁne - tuning . For pre - training , AdaFactor optimizer   ( Shazeer and Stern , 2018 ) is used with “ inverse   square root ” learning rate schedule .   AMR Graph Construction : We use the   SPRING model ( Bevilacqua et al . , 2021 ) to gen-   erate AMR graphs from source style text . We use8818amrlibpackage to generate the AMR graphs .   This implementation uses T5 - base ( Raffel et al . ,   2020 ) as its underlying model , as opposed to   theBART model ( Lewis et al . , 2019 ) in the   SPRING architecture . It is trained on AMR 3.0   ( LDC2020T02 ) dataset ( Knight et al . , 2021 ) that   consists of 59 K manually created sentence - AMR   pairs . The model is trained for 8 epochs using a   learning rate of 10 . The source and target se-   quence lengths are restricted to 100 and 512 tokens   respectively . Note that t5 - based SPRING model   achieves an SMATCH score of 83.5 , which guaran-   tees the quality of obtained AMR representations   z.   AMR - Based Style Transfer : We use the   T5wtense ( T5 with tense ) architecture from the   amrlib package . The T5wtense architecture   encodes part - of - speech ( POS ) tags to the concepts   in the AMR graph , which helps the generation   model to predict the tense of the output sentence   since AMR graphs do not retain any tense infor-   mation from their corresponding sentence . This   model outperforms the standard T5 - based model by   10 BLEU points on the AMR 3.0 ( LDC2020T02 )   dataset ( Knight et al . , 2021 ) . To keep the training   steps comparable for the subsets of the CDS dataset   ( Krishna et al . , 2020 ) , we train this t5 - base   model for 20epochs for the Bible , Romantic Po-   etry , and Shakespeare datasets , and 5epochs for   the Switchboard dataset . The model was trained   for20epochs for the Shakespeare Author Imitation   Dataset ( Xu et al . , 2012 ) as well . We used a learn-   ing rate of 10for both datasets , and restricted   source and target sequence lengths to 512 and 90   throughout , respectively . Everything else was kept   same as the amrlib implementation to keep the   results consistent .   STRAP baseline : We train the model keeping   the same hyperparameter conﬁguration as reported   in Krishna et al . ( 2020 ) . We train each style-   speciﬁc decoder for 3 epochs with learning rate   of5×10with Adam optimizer ( Kingma and   Ba , 2014 ) . During inference we set the p - value   for nucleus sampling ( Holtzman et al . , 2019 ) to   0.7 to have an appropriate balance between content   preservation and style accuracy scores .   Style classiﬁers : Similar to Krishna et al .   ( 2020 ) , we ﬁne - tune a RoBERTa - large model   ( Liu et al . , 2019 ) using the ofﬁcial implementa - tion in the fairseq packageto train the style   classiﬁers mentioned in Table 3 . For all classiﬁer   variants , learning rate of 10and a mini - batch   size of 32was used . The models were trained for   10 epochs using Adam optimizer ( Kingma and Ba ,   2014 ) . We also masked out named entities , nouns   and numbers from the input before training the   classiﬁer . We used spacy packageto obtain   the named entities and POS tags . To obtain the   named - entities and POS for AMR graphs , informa-   tion extracted from original sentences was used .   Train - Validation - Test splits : The data splits   were kept the same as the baseline model , STRAP   ( Krishna et al . , 2020 ) and can be found in Table 10 .   Computational time and device setup : All ex-   periments were done on a 16 GB NVIDIA V100   GPU system with 120 GB n1 - standard-32 Intel   Broadwell CPU . It took ∼16 hrs to train the AMR-   to - Text models for Shakespeare Author Imitation   dataset ( Xu et al . , 2012 ) and ∼25 hrs for the CDS   dataset .   Evaluation Metrics : We used the gensim   package to compute the Word Mover Distance   ( WMD ) ( Kusner et al . , 2015 ) , nltkpackage to   compute the BLEU scores ( Papineni et al . , 2002 ) ,   smatch packageto compute the SMATCH   score ( Cai and Knight , 2013 ) and implementation   by Krishna et al . ( 2020)for the cosine similarity   using SIM embeddings ( Wieting et al . , 2019a ) .   License of the packages used : The follow-   ing packages use the MIT License - amrlib ,   spacy , fairseq , smatch , and STRAP . The   following packages use the Apache License 2.0   -nltk , andhuggingface ’s transformer .8819The following packages use the GNU LGPL license   3.0 - gensim .   C T - STAR- Encoder Ablation Study   The performance of our T - STAR - Encoder heavily   depends on the quality of synthetic dataset gener-   ated while stylizing the sentences present in AMR   3.0 dataset . Therefore , we conducted thorough em-   pirical analysis to identify a ﬁltering strategy to   boost the performance of the encoder .   To obtain the initial set of stylized sentences , we   use the state - of - the - art model available to transform   generic English sentences to relevant styles { Poetry ,   Shakespeare , Bible , Switchboard } , i.e. , seq2seq   inverse paraphrase module ( Krishna et al . , 2020 ) .   We then ﬁne - tune our T - STAR - Encoder on syn-   thetic datasets obtained from different ﬁltering   strategies , and compare the performan on test split   of AMR 3.0 , to ensure that the quality of the gener-   ated AMRs do not drop signiﬁcantly . We present   our ﬁndings in Table 11 . We observe that using the   whole set of generated samples , leads to a signiﬁ-   ca nt drop in the performance ( row-1 ) .   Therefore , we ﬁlter out the augmented stylized   sentences with SIM similarity score ( Wieting et al . ,   2019a ) below the threshold δ . This ﬁltering strategy   was able to signiﬁcantly improve over the T - STAR-   Encoder performance , giving competitive results to   the non - augmented Vanilla T - STAR - Encoder . We   select the best performing δbased on the perfor-   mance on the test - split of AMR 3.0 . Note that we   have used the best performing threshold , δ= 0.7   D Unsupervised Evaluation of AMR   parsing   Since we use AMR graphs as the intermediate rep-   resentation for the TST task , it is important to vali-   date the generation quality of generated AMRs in   terms of content preservation with respect to the   input sentence . However , there does not exist anunsupervised metric to evaluate content overlap be-   tween an AMR graph and a sentence . Hence we   propose to use a slight variation of the Word Mover   Distance ( Kusner et al . , 2015 ) for this purpose . We   choose WMD over other content preservation met-   rics for the following reasons -   •Yamshchikov et al . ( 2019a ) illustrate the efﬁ-   cacy of WMD to evaluate text style transfer   over other metrics based on correlation with   human evaluations . Which means that it is   more robust to the domain difference in the   input and the output sentence , making it an   ideal candidate for text - AMR similarity mea-   surement .   •Syntactic metrics like BLEU would not be   able to compute the content overlap between   an AMR graph and a sentence because word   representation in an AMR graph discards   noun forms and tense information , and some   verb tokens are mapped to a different Prop-   Bank verb . These modiﬁcations along with   the disparity in sequential - graphical represen-   tation makes syntactic metrics infeasible for   the task .   •Semantic representations like SIM are fragile   to the input sequence order , and affected by   non - content bearing words as well . However ,   WMD adopts on a bag - of - words paradigm ,   making it more suitable for the task .   We propose the following two variants of WMD   -   •WMD Overall - In this variant , we aimed   to keep the content bearing tokens from the   sentence and AMR graphs . For sentence , we   removed the stopwords ( after doing a detailed   corruption study on sentences , refer to Ta-   ble 13 ) , while for AMR Graphs we removed   AMR notation speciﬁc tokens ( like “ : op ? ” ,   “ ARG ? ” ) , punctuations ( like “ ( ” , ‘ ” ’ ) , as-   signed variables and propbank code for verbs   ( eg . changing “ s / say-01 ” to “ say ” ) .   •WMD Verb Overall - In this variant we   speciﬁcally want to compute the similarity   of verbs in the parsed AMR graphs and input   sentence . For this , use nltk POS tagging   tool to extract out verbs from the input sen-   tence , and directly extract out propbank based   verbs from the AMR graph.8820Refer to Table 12 for an example of the preprocess-   ing strategy adopted .   We also study the effect on text - AMR WMD   score juxtaposed to text - text WMD scores on vary-   ing degree of similarity between the compared se-   quences . For this we use the diverse paraphraser   trained by Krishna et al . ( 2020 ) on the test set of   AMR 3.0 dataset , and generate paraphrases with   varying nucleus sampling p - values ( Holtzman et al . ,   2019 ) from 0.0 to 1.0 with step size of 0.1 . We no-   tice that the WMD scores for text - text WMD ( blue   line in Fig . 5 ) and text - AMR WMD ( red line in   Fig . 5 ) are similar to each other throughout .   We present the WMD Overall scores and WMD   Verb Overall scores for the CDS dataset in Table   14 ; validating the content retention in parsed AMR   graphs for different strategies . We notice that It-   erative T - STAR Encoder outperforms the T - STAR   Encoder - Flt ( δ=0.7 ) and T - STAR Encoder ( unﬁl-   tered ) baselines in both WMD overall and Verb   WMD overall . Even though comparable , it is still   however lesser than the Vanilla T - STAR Encoder   numbers . However , we believe we can credit that to   more style information retention in Vanilla T - STAR   Encoder ( refer to Section 6.2 ) , leading to poorer   performance in downstream text style transfer task .   E Data Augmentation for T - STAR   Decoder   We hypothesize that the T - STAR decoder perfor-   mances will improve if the underlying model , is bet-   ter at generating text given an AMR graph . To this   end , we create synthetic dataset using sentences   from Wikipedia corpus . We sample 10 million8821   sentences from it and generate the corresponding   AMRs using our vanilla T - STAR model . We fur-   ther ﬁlter out the samples for which WMD Overall ,   mentioned in Appendix D and keep samples with   a WMD score below 0.15 , which results in 2.3 M   instances .   We ﬁrst ﬁne - tune the T5 - Base model for AMR to   Text task on this ﬁlter dataset . We obtain a BLEU   score of 49.13 on Gold AMR test set . Note that   this performance is very close to the state - of - the   art result 49.2 BLEU for this task ( Bai et al . , 2022 ) .   Table 15 lists down the different ﬁltering strategy   and dataset sizes we experimented to identify the   best strategy to improve the performance .   We then compare the performance of the best   performing model on the style transfer task again   STRAP . We observe that this model is not beat-   ing the STRAP performance across various style   directions . Therefore , we conclude that a vanilla   ﬁne - tuning of model for AMR to text task , does   not necessarily boost the performance in the down-   stream tasks .   F Error Analysis   F.1 Comparison on Meaning Preservation   We present the results across all the 12 directions   for content preservation comparitive analysis in Ta-   ble 16 and 17 respectively . We can observe that   for every direction our models are consistently bet-   ter in content preservation with respect to STRAP   model .   F.2 Error Analysis per direction   In this section , we present the error analysis for   each direction in Table 18 . We observe that T-   STAR and Iterative T - STAR models are consis-   tently better on No - Error and Hallucinations across   all the directions . Moreover , we observe that for 7   out of 12 direction , Iterative T - STAR model is bet-   ter than STRAP for Incompleteness error . Note that   our T - STAR model was under - performing , however   another iteration of model improvements increases   that number signiﬁcantly . Note that all the models   are giving high error in Semantic Drift , and im-   proving the model for this type of errors can be   explored in future works .   F.3 Qualitative Analysis for Incompleteness   and Semantic Drift   As we are using interpretable intermediate repre-   sentations , it is easily possible to understand the   intuition behind these errors , and broadly under-   stand which modules ( encoder or decoder ) needs   to be improved further . Therefore , we study few   instances and analyze the generated Intermediate   AMRs to understand the reason for high number   inSemantic Drift andIncomplete errors . We list   down some intstances in Table 20 and Table 19   Across the various instances that we analyzed ,   we observe that the generated AMRs were not en-   coding the complete information themselves . For   instance , either missing some entities ( example 1 ,   4 , 5 and 6 in Table 19 ) , if the clauses were sepa-   rated using ” : ” , ” ; ” , only one of those were parsed   in the intermediate graph ( example 2 and 3 in Ta-   ble 19 . For semantic drift , we observed that the   errors was arising due to the shortcomings in both   the modules , i.e. , it was leading to meaning change   if the Encoder did n’t generate an efﬁcient AMR8822   graph , or if the decoder was not able to interpret   AMR correctly . We have listed the modules that   could be the potential reason for the error in the   last column in Table 20 .   It is important to note that , the source of errors   is very easy to identify now because we are using   robust , interpretable and symbolic representation   as pivot to transfer from style A to style B. We   have also provided a case study of performance of   various baselines and proposed model in Table 7   on the CDS dataset.882388248825