  Chengwei Qinand Shafiq JotyNanyang Technological UniversitySalesforce Research   { chengwei003@e.ntu , srjoty@ntu}.edu.sg   Abstract   Existing continual relation learning ( )   methods rely on plenty of labeled training data   for learning a new task , which can be hard to   acquire in real scenario as getting large and   representative labeled data is often expensive   and time - consuming . It is therefore necessary   for the model to learn novel relational patterns   with very few labeled data while avoiding catas-   trophic forgetting of previous task knowledge .   In this paper , we formulate this challenging   yet practical problem as continual few - shot re-   lation learning ( ) . Based on the finding   that learning for new emerging few - shot tasks   often results in feature distributions that are   incompatible with previous tasks ’ learned dis-   tributions , we propose a novel method based on   embedding space regularization and data aug-   mentation . Our method generalizes to new few-   shot tasks and avoids catastrophic forgetting of   previous tasks by enforcing extra constraints on   the relational embeddings and by adding extra   relevant data in a self - supervised manner . With   extensive experiments we demonstrate that our   method can significantly outperform previous   state - of - the - art methods in task settings .   1 Introduction   Relation Extraction ( RE ) aims to detect the re-   lationship between two entities in a sentence , for   example , predicting the relation birthdate in the   sentence “ Kamala Harris was born in Oakland ,   California , on October 20 , 1964 . ” for the two enti-   tiesKamala Harris andOctober 20 , 1964 . It serves   as a fundamental step for downstream tasks such as   search and question answering ( Dong et al . , 2015 ;   Yu et al . , 2017 ) . Traditionally , RE methods were   built by considering a fixed static set of relations   ( Miwa and Bansal , 2016 ; Han et al . , 2018a ) . How-   ever , similar to entity recognition , RE is also an   open - vocabulary problem ( Sennrich et al . , 2016),Figure 1 : Difference between Continual Relation Learn-   ing ( ) and Continual Few - shot Relation Learning   ( ) . Except for the first task which has enough train-   ing data , the subsequent new tasks are all few - shot in . In contrast , assumes enough training data   for every task .   where the relation set keeps growing as new rela-   tion types emerge with new data .   A potential solution is to formalize RE as Contin-   ual Relation Learning or ( Wang et al . , 2019 ) .   In , the model learns relational knowledge   through a sequence of tasks , where the relation   set changes dynamically from the current task to   the next . The model is expected to perform well   on both the novel and previous tasks , which is chal-   lenging due to the existence of Catastrophic Forget-   tingphenomenon ( McCloskey and Cohen , 1989 ;   French , 1999 ) in continual learning . In this phe-   nomenon , the model forgets previous relational   knowledge after learning new relational patterns .   Existing methods to address catastrophic forget-   ting in can be divided into three categories :   ( i ) regularization - based methods , ( ii ) architecture-   based methods , and ( iii ) memory - based methods .   Recent work shows that memory - based methods   which save several key examples from previous   tasks to a memory and reuse them when learning   new tasks are more effective in NLP ( Wang et al . ,   2019 ; Sun et al . , 2020 ) . Successful memory - based methods include EAEMR ( Wang et al . , 2019 ) ,   MLLRE ( Obamuyide and Vlachos , 2019 ) , EMAR   ( Han et al . , 2020 ) , and CML ( Wu et al . , 2021 ) .   Despite their effectiveness , one major limitation2776of these methods is that they all assume plenty   of training data for learning new relations ( tasks ) ,   which is hard to satisfy in real scenario where con-   tinual learning is desirable , as acquiring large la-   beled datasets for every new relation is expensive   and sometimes impractical for quick deployment   ( e.g. , RE from news articles during the onset of an   emerging event like Covid-19 ) . In fact , one of the   main objectives of continual learning is to quickly   adapt to new environments or tasks by exploiting   previously acquired knowledge , a hallmark of hu-   man intelligence ( Lopez - Paz and Ranzato , 2017 ) .   If the new tasks are few - shot , the existing meth-   ods suffer from over - fitting as shown later in our   experiments ( § 4 ) . Considering that humans can ac-   quire new knowledge from a handful of examples ,   it is expected for the models to generalize well on   the new tasks with few data . We regard this prob-   lem as Continual Few - shot Relation Learning or ( Fig . 1 ) . Indeed , in relation to , Zhang   et al . ( 2021 ) , Zhu et al . ( 2021 ) and Chen and Lee   ( 2021 ) recently introduce methods for incremental   few - shot learning in Computer Vision .   Based on the observation that the learning of   emerging few - shot tasks may result in distorted   feature distributions of new data which are incom-   patible with previous embedding space ( Ren et al . ,   2020 ) , this work introduces a novel model based   on Embedding space Regularization and Data Aug-   mentation ( ERDA ) for . In particular , we   propose a multi - margin loss and a pairwise mar-   gin loss in addition to the cross - entropy loss to im-   pose further relational constraints in the embedding   space . We also introduce a novel contrastive loss   to learn more effectively from the memory data .   Our proposed data augmentation method selects   relevant samples from unlabeled text to provide   more relational knowledge for the few - shot tasks .   The empirical results show that our method can   significantly outperform previous state - of - the - art   methods . In summary , our main contributions are :   •To the best of our knowledge , we are the first one   to consider . We define the problem   and construct a benchmark for the problem .   •We propose ERDA , a novel method for   based on embedding space regularization and   data augmentation .   •With extensive experiments , we demonstrate the   effectiveness of our method compared to existing   ones and analyse our results thoroughly.2 Related Work   Conventional RE methods include supervised ( Ze-   lenko et al . , 2002 ; Liu et al . , 2013 ; Zeng et al . , 2014 ;   Miwa and Bansal , 2016 ) , semi - supervised ( Chen   et al . , 2006 ; Sun et al . , 2011 ; Hu et al . , 2020 ) and   distantly supervised methods ( Mintz et al . , 2009 ;   Yao et al . , 2011 ; Zeng et al . , 2015 ; Han et al . ,   2018a ) . These methods rely on a predefined rela-   tion set and have limitations in real scenario where   novel relations are emerging . There have been   some efforts which focus on relation learning with-   out predefined types , including open RE ( Shinyama   and Sekine , 2006 ; Etzioni et al . , 2008 ; Cui et al . ,   2018 ; Gao et al . , 2020 ) and continual relation learn-   ing ( Wang et al . , 2019 ; Obamuyide and Vlachos ,   2019 ; Han et al . , 2020 ; Wu et al . , 2021 ) .   Continual Learning ( CL ) aims to learn knowledge   from a sequence of tasks . The main problem CL   attempts to address is catastrophic forgetting ( Mc-   Closkey and Cohen , 1989 ) , i.e. ,the model forgets   previous knowledge after learning new tasks . Prior   methods to alleviate this problem can be mainly   divided into three categories . First , regularization-   based methods impose constraints on the update of   neural weights important to previous tasks to alle-   viate catastrophic forgetting ( Li and Hoiem , 2017 ;   Kirkpatrick et al . , 2017 ; Zenke et al . , 2017 ; Ritter   et al . , 2018 ) . Second , architecture - based meth-   ods dynamically change model architectures to ac-   quire new information while remembering previous   knowledge ( Chen et al . , 2016 ; Rusu et al . , 2016 ;   Fernando et al . , 2017 ; Mallya et al . , 2018 ) . Finally ,   memory - based methods maintain a memory to save   key samples of previous tasks to prevent forgetting   ( Rebuffi et al . , 2017 ; Lopez - Paz and Ranzato , 2017 ;   Shin et al . , 2017 ; Chaudhry et al . , 2019 ) . These   methods mainly focus on learning a single type of   tasks . More recently , researchers have considered   lifelong language learning ( Sun et al . , 2020 ; Qin   and Joty , 2022 ) , where the model is expected to   continually learn from different types of tasks .   Few - shot Learning ( FSL ) aims to solve tasks con-   taining only a few labeled samples , which faces   the issue of over - fitting . To address this , exist-   ing methods have explored three different direc-   tions : ( i)data - based methods use prior knowledge   to augment data to the few - shot set ( Santoro et al . ,   2016 ; Benaim and Wolf , 2018 ; Gao et al . , 2020 ) ;   ( ii)model - based methods reduce the hypothesis   space using prior knowledge ( Rezende et al . , 2016 ;   Triantafillou et al . , 2017 ; Hu et al . , 2018 ) ; and2777(iii)algorithm - based methods try to find a more   suitable strategy to search for the best hypothesis in   the whole hypothesis space ( Hoffman et al . , 2013 ;   Ravi and Larochelle , 2017 ; Finn et al . , 2017 ) .   Summary . Existing work in which involves   a sequence of tasks containing sufficient training   data , mainly focuses on alleviating the catastrophic   forgetting of previous relational knowledge when   the model is trained on new tasks . The work in few-   shot learning mostly leverages prior knowledge to   address the over - fitting of novel few - shot tasks . In   contrast to these lines of work , we aim to solve a   more challenging yet more practical problem   where the model needs to learn relational patterns   from a sequence of few - shot tasks continually .   3 Methodology   In this section , we first formally define the   problem . Then , we present our method for .   3.1 Problem Definition involves learning from a sequence of tasks   T= ( T , . . . , T ) , where every task Thas its   own training set D , validation set D , and   test set D. Each dataset Dcontains several   samples { ( x , y ) } , whose labels ybelong to   the relation set Rof task T. In contrast to the   previously addressed continual relation learning   ( ) , assumes that except for the first task   which has enough data for training , the subsequent   new tasks are all few - shot , meaning that they have   only few labeled instances ( see Fig . 1 ) . For exam-   ple , consider there are three relation learning tasks   T , TandTwith their corresponding relation   setsR , R , and R , each having 10relations . In , we assume the existing task Thas enough   training data ( e.g. ,100samples for every relation in   R ) , while the new tasks TandTare few - shot   with only few ( e.g. ,5 ) samples for every relation   inRandR. Assuming that the relation number   of each few - shot task is Nand the sample number   of every relation is K , we call this setup N - way   K - shot continual learning . The problem setup of is aligned with the real scenario , where we   generally have sufficient data for an existing task ,   but only few labeled data as new tasks emerge .   The model in is expected to first learn T   well , which has sufficient training data to obtain   good ability to extract the relation information in   the sentence . Then at time step k , the model will   be trained on the training set Dof few - shot task   T. After learning T , the model is expected to   perform well on both Tand the previous k−1   tasks , as the model will be evaluated on ˆD=   ∪Dconsisting of all known relations after   learning T , i.e. ,ˆR=∪R. This requires the   model to overcome the catastrophic forgetting of   previous knowledge and to learn new knowledge   well with very few labeled data .   To overcome the catastrophic forgetting problem ,   a memory M=   M , M , ... 	  , which stores   some key samples of previous tasks is maintained   during the learning . When the model is learning   T , it has access to the data saved in memory   M , ... , M. As there is no limit on the number   of tasks , the size of memory Mis constrained to   be small . Therefore , the model has to select only   key samples from the training set Dto save   them in M. In our setting , only one sample   per relation is allowed to be saved in the memory .   3.2 Overall Framework   Our framework for is shown in Fig . 2 and   Alg . 1 describes the overall training process ( see   Appendix A.1 for a block diagram ) . At time step   k , given the training data Dfor the task T ,   depending on whether the task is a few - shot or   not , the process has four or three working modules ,   respectively . The general learning process ( § 3.3 )   has three steps that apply to all tasks . If the task   is a few - shot task ( k > 1 ) , we apply an additional   step to create an augmented training set eD. For   the initial task ( k= 1 ) , we have eD = D.   For any task T , we use a siamese model to en-   code every new relation r∈Rintor∈I R   as well as the sentences , and train the model on   eDto acquire relation information of the new   data ( § 3.3.2 ) . To overcome forgetting , we select the   most informative sample for each relation r∈R   from Dand update the memoryˆM(§3.3.3 ) .   Finally , we combine eDandˆMas the train-   ing data for learning new relational patterns and2778Algorithm 1   remembering previous knowledge ( § 3.3.4 ) . We   also simultaneously update the representation of all   relations in ˆR , which involves making a forward   pass through the current model . The learning and   updating are done iteratively for convergence .   For data augmentation in few - shot tasks ( § 3.4 ) ,   we select reliable samples with high relational sim-   ilarity score from an unlabelled Wikipedia cor-   pus using a fine - tuned BERT ( Devlin et al . , 2019 ) ,   which serves as the relational similarity model S.   In the interests of coherence , we first present the   general learning method followed by the augmen-   tation process for few - shot learning .   3.3 General Learning Process   We first introduce the encoder network as it is the   basic component of the whole framework .   3.3.1 The Encoder Network   The siamese encoder ( f ) aims at extracting generic   and relation related features from the input . The   input can be a labeled sentence or the name of a   relation . We adopt two kinds of encoders :   •Bi - LSTM To have a fair comparison with pre-   vious work , we use the same architecture as Han   et al . ( 2020 ) . It takes GloVe embeddings ( Pen-   nington et al . , 2014 ) of the words in a given input   and produces a vector representation through a Bi-   LSTM ( Hochreiter and Schmidhuber , 1997).•BERT We adopt BERTwhich has 12 lay-   ers and 110 M parameters . As the new tasks are few-   shot , we only fine - tune the 12 - th encoding layer   and the extra linear layer . We include special to-   kens around the entities ( ‘ # ’ for the head entity and   ‘ @ ’ for the tail entity ) in a given labeled sentence   to improve the encoder ’s understanding of relation   information . We use the [ ] token features as   the representation of the input sequence .   3.3.2 Learning with New Data   At time step k , to have a good understanding of the   new relations , we fine - tune the model on the ex-   panded dataset eD. The model ffirst encodes   the name of each new relation r∈Rinto its   representation r∈I Rby making a forward pass .   Then , we optimize the parameters ( θ ) by minimiz-   ing a loss Lthat consists of a cross entropy loss ,   a multi - margin loss and a pairwise margin loss .   Thecross entropy lossLis used for relation   classification as follows .   ( 1 )   where ˆRis the set of all known relations at step   k , g(,)is a function used to measure similarity   between two vectors ( e.g. , cosine similarity or L2   distance ) , and δis the Kronecker delta function –   δ= 1ifaequals b , otherwise δ= 0 .   In inference , we choose the relation label that   has the highest similarity with the input sentence   ( Eq . 8) . To ensure that an example has the highest   similarity with the true relation , we additionally   design two margin - based losses , which increase   the score between an example and the true label   while decreasing the scores for the wrong labels .   The first one is a multi - margin loss defined as:(2 )   where tis the correct relation index in ˆRsatis-   fying r = yandmis a margin value . The   Lloss attempts to ensure intra - class compact-   ness while increasing inter - class distances . The   second one is a pairwise margin lossL:2779where mis the margin for Lands=   arg maxg(f(x),r)s.t.s̸=t , the closest   wrong label . The Lloss penalizes the cases   where the similarity score of the closest wrong   label is higher than the score of the correct label   ( Yang et al . , 2018 ) . Both LandLimprove   the discriminative ability of the model ( § 4.4 ) .   Thetotal loss for learning on Tis defined as :   L = λL+λL+λL ( 4 )   where λ , λandλare the relative weights of   the component losses , respectively .   3.3.3 Selecting Samples for Memory   After training the model fwith Eq . ( 4 ) , we use it   to select one sample per new relation . Specifically ,   for every new relation r∈R , we obtain the   centroid feature cby averaging the embeddings   of all samples labeled as rinDas follows .   c=1   |D|Xf(x ) ( 5 )   where D={(x , y)|(x , y)∈D , y = r } .   Then we select the instance closest to cfromD   as the most informative sample and save it in mem-   oryM. Note that the selection is done from D ,   not from the expanded set eD.   3.3.4 Alleviating Forgetting through Memory   As the learning of new relational patterns may   cause catastrophic forgetting of previous knowl-   edge ( see baselines in § 4 ) , our model needs to   learn from the memory data to alleviate forget-   ting . We combine the expanded set eDand the   whole memory dataˆM=∪MintoeHto   allow the model to learn new relational knowledge   and consolidate previous knowledge . However , the   memory data is limited containing only one sample   per relation . To learn effectively from such limited   data , we design a novel method to generate a hard   negative sample setPfor every sample inˆM.   The negative samples are generated on the fly .   After sampling a mini - batch BfromeH , we con-   sider all memory data in BasM. For every sam-   ple(ˆx,ˆy)inM , we replace its head entity eor   tail entity ewith the corresponding entity of a ran-   domly selected sample in the same batch Bto get   the hard negative sample set P={(ˆx,ˆy)}.Then ( ˆx,ˆy)andPare used to calculate a margin-   based contrastive loss Las follows.(6 )   where ˆtis the relation index satisfying r= ˆy   andmis the margin value for L. This loss   forces the model to distinguish the valid relations   from the hard negatives so that the model learns   more precise and fine - grained relational knowledge .   In addition , we also use the three losses Land   LandLdefined in § 3.3.2 to update θonB.   The total loss on the memory data is : ( 7 )   where λ , λ , λandλare the relative   weights of the corresponding losses .   Updating Relation Embeddings After training   the model on eHfor few steps , we use the mem-   oryˆMto update the relation embedding rof all   known relations . For a relation r∈ˆR , we aver-   age the embeddings ( obtained by making a forward   pass through f ) of the relation name and memory   data to obtain its updated representation r. The   training of θand updating of ris done iteratively   to grasp new relational patterns while alleviating   the catastrophic forgetting of previous knowledge .   3.3.5 Inference   For a given input xinˆD , we calculate the simi-   larity between xand all known relations , and pick   the one with the highest similarity score :   y= arg maxg(f(x),r ) ( 8)   3.4 Data Augmentation for Few - shot Tasks   For each few - shot task T , we aim to get more data   by selecting reliable samples from an unlabeled cor-   pusCwith tagged entities before the general learn-   ing process ( § 3.3 ) begins . We achieve this using a   relational similarity model Sand sentences from   Wikipedia as C. The model S(described later )   takes a sentence as input and produces a normal-   ized vector representation . The cosine similarity   between two vectors is used to measure the rela-   tional similarity between the two corresponding   sentences . A higher similarity means the two sen-   tences are more likely to have the same relation   label . We propose two novel selection methods ,   which are complementary to each other.2780(a ) Augmentation via Entity Matching For   each instance ( x , y)inD , we extract its entity   pair(e , e)withebeing the head entity and ebe-   ing the tail entity . As sentences with the same entity   pair are more likely to express the same relation , we   first collect a candidate set Q={ex}fromC ,   whereexshares the same entity pair ( e , e)with   x. IfQis a non - empty set , we pair all exinQ   withx , and denote each pair as ⟨ex , x⟩. Then we   useSto obtain a similarity score sfor⟨ex , x⟩.   After getting scores for all pairs , we pick the in-   stances exwith similarity score shigher than a   predefined threshold αas new samples and label   them with relation y. The selected instances are   then augmented to Das additional data .   ( b ) Augmentation via Similarity Search The   hard entity matching could be too restrictive at   times . For example , even though the sentences   “ Harry Potter is written by Joanne Rowling ” and   “ Charles Dickens is the author of A Tale of Two   Cities ” share the same relation author , hard match-   ing fails to find any relevance . Therefore , in cases   when entity matching returns an empty Q , we re-   sort to similarity search using Faiss ( Johnson et al . ,   2017 ) . Given a query vector q , it can efficiently   search for vectors { v}with the top- Khighest   similarity scores in a large vector set V. In our case ,   qis the representation of xandVcontains the   representations of the sentences in C. We use S   to obtain these representations ; the difference is   thatVis pre - computed while qis obtained dur-   ing training . We labeled the top- Kmost similar   instances with yand augment them to D.   Similarity Model To train S , inspired by Soares   et al . ( 2019 ) , we adopt a contrastive learning   method to fine - tune a BERT model on C ,   whose sentences are already tagged with entities .   Based on the observation that sentences with the   same entity pair are more likely to encode the same   relation , we use sentence pairs containing the same   entities in Cas positive samples . For negatives ,   instead of using all sentence pairs containing dif-   ferent entities , we select pairs sharing only one   entity as hard negatives ( i.e. ,pair(x , x)where   e = eande̸=eore = eande̸=e ) .   We randomly sample the same number of negative   samples as the positive ones to balance the training .   For an input pair ( x , x ) , we compute the simi-   larity score based on the following formula .   σ(x , x ) = 1   1 + exp ( −S(x)S(x))(9)whereS(x)is the normalized representation of x   obtained from the final layer of BERT . Then we   optimize the parameters πofSby minimizing a   binary cross entropy loss L as follows .   ( 10 )   where Cis a positive batch and Cis a negative   batch . This objective tries to ensure that sentence   pairs with the same entity pairs have higher cosine   similarity than those with different entities .   4 Experiment   We define the benchmark and evaluation metric for before presenting our experimental results .   4.1 Benchmark and Evaluation Metric   Benchmark As the benchmark for needs   to have sufficient relations as well as data and be   suitable for few - shot learning , we create the   benchmark based on FewRel ( Han et al . , 2018b ) .   FewRel is a large - scale dataset for few - shot RE ,   which contains 80relations with hundreds of sam-   ples per relation . We randomly split the 80relations   into8tasks , where each task contains 10relations   ( 10 - way ) . To have enough data for the first task   T , we sample 100samples per relation . All the   subsequent tasks T , ... , Tare few - shot ; for each   relation , we conduct 2 - shot , 5 - shot and10 - shot ex-   periments to verify the effectiveness of our method .   In addition , to demonstrate the generalizability   of our method , we also create a benchmark   based on the TACRED dataset ( Zhang et al . , 2017 )   which contains only 42relations . We filter out the   special relation “ n / a ” ( not available ) and split the   remaining 41relations into 8tasks . Except for the   first task that contains 6relations , all other tasks   have 5relations ( 5 - way ) . Similar to FewRel , we   randomly sample 100examples per relation in T   and conduct 5 - shot and10 - shot experiments .   Metric At time step k , we evaluate the model   performance through relation classification accu-   racy on the test sets ˆD=∪Dof all seen   tasks{T } . This metric reflects whether the   model can alleviate catastrophic forgetting while   acquiring novel knowledge well with very few data .   Since the model performance might be influenced   by task sequences and few - shot training samples ,   we run every experiment 6times each time with a   different random seed to ensure a random task or-   der and model initialization , and report the average2781   accuracy along with variance . We perform paired   t - test for statistical significance .   4.2 Model Settings & Baselines   The model settings are shown in Appendix A.2 . We   compare our approach with the following baselines :   •SeqRun fine - tunes the model only on the training   data of the new tasks without using any memory   data . It may face serious catastrophic forgetting   and serves as a lower bound .   •Joint Training stores all previous samples in the   memory and trains the model on all data for each   new task . It serves as an upper bound in .   •EMR ( Wang et al . , 2019 ) maintains a memory   for storing selected samples from previous tasks .   When training on a novel task , EMR combines   the new training data and memory data .   •EMAR ( Han et al . , 2020 ) is the state - of - the - art   on , which adopts memory activation and re-   consolidation to alleviate catastrophic forgetting .   •IDLVQ - C ( Chen and Lee , 2021 ) introduces   quantized reference vectors to represent previous   knowledge and mitigates catastrophic forgetting   by imposing constraints on the quantized vectors   and embedded space . It was originally proposed   for image classification with state - of - the - art re-   sults in incremental few - shot learning .   4.3 Main Results   We compare the performance of different methods   using the same setting as EMAR ( Han et al . , 2020 ) ,   which uses a Bi - LSTM encoder . We also report the   results with a BERT encoder .   FewRel Benchmark We report our results on   10 - way 5 - shot in Table 1 , while Fig . 3 shows the   results on the 10 - way 2 - shot and10 - way 10 - shot   settings . From the results , we can observe that :   •Our proposed ERDA outperforms previous base-   lines in all settings , which demonstrates the   superiority of our method . Simply fine - tuning the   model with new few - shot examples leads to rapid   drops in accuracy due to severe over - fitting and   catastrophic forgetting . Although EMR and EMAR   adopt a memory module to alleviate forgetting ,   their performance still decreases quickly as they   require plenty of training data for learning a new   task . Compared with EMR and EMAR , IDLVQ - C   is slightly better as it introduces quantized vectors   that can better represent the embedding space of   few - shot tasks . However , IDLVQ - C does not nec-   essarily push the samples from different relations   to be far apart in the embedding space and the up-2782   dating method for the reference vectors may not be   optimal . ERDA outperforms IDLVQ - C by a large   margin through embedding space regularization   and self - supervised data augmentation . To verify   this , we show the embedding space of IDLVQ-   C and ERDA using t - SNE ( Van der Maaten and   Hinton , 2008 ) . We randomly choose four classes   from the first task of FewRel and two classes from   the new task , and visualize the test data of these   classes in Fig . 4 . As can be seen , the embedding   space obtained by ERDA shows better intra - class   compactness and larger inter - class distances .   •Unlike , joint training does not always serve   as an upper bound in due to the extremely   imbalanced data distribution . Benefiting from the   ability to learn feature distribution with very few   data , both ERDA and IDLVQ - C perform better   than joint training in the 2 - shot setting . However ,   as the number of few - shot samples increases , the   performance of IDLVQ - C falls far behind joint   training , while ERDA still performs better . In the   5 - shot setting , ERDA could achieve better results   than joint training which verifies the effectiveness   of self - supervised data augmentation ( more on this   in § 4.4 ) . Although ERDA performs worse than   joint training in the 10 - shot setting , its results are   still much better than other baselines .   •After learning all few - shot tasks , ERDA outper-   forms IDLVQ - C by 9.69%,12.67 % and11.49 % in   the2 - shot , 5 - shot and10 - shot settings , respectively .   Moreover , the relative gain of ERDA keeps grow-   ing with the increasing number of new few - shot   tasks . This demonstrates the ability of our method   in handling a longer sequence of tasks .   TACRED Benchmark Fig . 5 shows the 5 - way   5 - shot and5 - way 10 - shot results on TACRED . We   can see that here also ERDA outperforms all other   methods by a large margin which verifies the strong   generalization ability of our proposed method .   Results with BERT We show the results with   BERT of different methods on FewRel in   Fig . 6 for 10 - way 2 - shot and10 - shot and Table 4   for10 - way 5 - shot ( in Appendix ) . The results of   on TACRED benchmark are shown in Fig . 7 for   5 - way 5 - shot and10 - shot . From the results , we can   observe that ERDA outperforms previous baselines   in all settings with a BERT encoder .   4.4 Ablation Study   We conduct several ablations to analyze the contri-   bution of different components of ERDA on the   FewRel 10 - way 5 - shot setting . In particular , we in-   vestigate seven other variants of ERDA by remov-   ing one component at a time : ( a ) the multi - margin   lossL , ( b ) the pairwise margin loss L , ( c ) the   margin - based contrastive loss L , ( d ) the whole   2 - stage data augmentation module , ( e ) the entity   matching method of augmentation , ( f ) the similarity   search method of augmentation , and ( g ) memory .   From the results in Table 3 , we can observe that   all components improve the performance of our   model . Specifically , Lyields about 1.51%per-   formance boost as it brings samples of the same2783   relation closer to each other while enforcing larger   distances among different relation distributions .   TheLimproves the accuracy by 3.18 % , which   demonstrates the effect of contrasting with the near-   est wrong label . The adoption of Lleads to   1.28%improvement , which shows that generating   hard negative samples for memory data can help to   better remember previous relational knowledge . To   better investigate the influence of L , we conduct   experiments with different λand show the re-   sults in Table 2 . We can see that the model achieves   the best accuracy of 53.38 with λ= 0.02while   the accuracy is only 52.13 with λ= 0.5 . In ad-   dition , the performance of the variant without L   is worse than the performance of all other variants ,   which demonstrates the effectiveness of L.   The data augmentation module improves the per-   formance by 1.72%as it can extract informative   samples from unlabeled text which provide more re-   lational knowledge for few - shot tasks . The results   of variants without entity matching or similarity   search verify that the two data augmentation meth-   ods are generally complementary to each other .   One could argue that the data augmentation mod-   ule increases the complexity of ERDA compared to   other models . However , astute readers can find that   even without data augmentation , ERDA outper-   forms IDLVQ - C significantly for all tasks ( compare   ‘ ERDA w.o . DA ’ with the baselines in Table 1 ) .   ERDA ’s Performance under Although   ERDA is designed for , we also evaluate the   embedding space regularization ( ‘ ERDA w.o . DA ’ )   in the setting . We sample 100 examples per   relation for every task in FewRel and compare our   method with the state - of - the - art method EMAR .   The results are shown in Fig . 8 . We can see that   ERDA outperforms EMAR in all tasks by 1.25 -   4.95%proving that the embedding regularization   can be a general method for .   5 Conclusion   We have introduced continual few - shot relation   learning ( ) , a challenging yet practical prob-   lem where the model needs to learn new relational   knowledge with very few labeled data continually .   We have proposed a novel method , named ERDA ,   to alleviate the over - fitting and catastrophic forget-   ting problems which are the core issues in .   ERDA imposes relational constraints in the em-   bedding space with innovative losses and adds ex-   tra informative data for few - shot tasks in a self-   supervised manner to better grasp novel relational   patterns and remember previous knowledge . Ex-   tensive experimental results and analysis show that   ERDA significantly outperforms previous methods   in all settings investigated in this work . In   the future , we would like to investigate ways to   combine meta - learning with .   References2784278527862787   A Appendix   A.1 Block Diagram of ERDA Training   A.2 Hyperparameter Search   We follow the settings in Han et al . ( 2020 ) for the   Bi - LSTM encoder to have a fair comparison . For   data augmentation , we set the threshold α= 0.65   and the number of samples selected by Faiss ( K )   as 1 . We adopt 0.2,0.2and0.01for the three mar-   gin values m , mandm , respectively . The loss   weights λ , λ , λandλare set to 1.0 , 1.0 ,   1.0 and 0.1 , respectively . In Alg . 1 , we set 1for   iterand2foriter . Hyperparameter search is   done on the validation sets . We follow EMAR   ( Han et al . , 2020 ) and use a grid search to select the   hyperparameters . Specifically , the search spaces   are :   •Search range for αis[0.3,0.8]with a step size   of0.05 .   •Search range for Kis[1,3]with a step size of 1 .   •Search range for mandmis[0.1,0.3]with a   step size of 0.1.•Search range for mis[0.01,0.03]with a step   size of 0.01 .   •Search range for iterin Alg . 1 is [ 1,3]with a   step size of 1 .   A.3 The Influence of Task Order   To evaluate the influence of the task order , we show   the results ( ERDA and IDLVQ - C ) of six different   runs with different task order on the FewRel bench-   mark for 10 - way 5 - shot setting in Table 5 . From   the results , we can see that the order of tasks will   influence the performance . For example , ERDA   achieves 55.59 accuracy after learning task8 on the   second run while the accuracy after learning task8   on the fifth run is only 51.35 . More importantly ,   ERDA outperforms IDLVQ - C by a large margin in   all six different runs .   A.4 The Contribution of Memory   We conduct the ablation without memory ( ‘ w.o .   M ’ ) to analyze the contribution of the memory   module on the FewRel 10 - way 5 - shot setting . From   the results in Table 7 , we can observe that ERDA   shows much better performance than ‘ w.o . M ’ ,   which verifies the importance of the memory mod-   ule . In addition , comparing the results of ‘ w.o . M ’   and ‘ SeqRun ’ in Table 1 , we can find that ‘ w.o . M ’   achieves much better accuracy . This demonstrates   the effectiveness of improving the representation   ability of the model through margin - based losses.27882789