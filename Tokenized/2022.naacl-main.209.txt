  Huda Hakami , Mona Hakami , Angrosh MandyaandDanushka BollegalaDepartment of Computer Science , Taif University , Saudi ArabiaKing Saud University , Saudi ArabiaUniversity of Liverpool , UKAmazon   hahakami@tu.edu.sa , mohakami@ksu.edu.sa   { angrosh , danushka } @liverpool.ac.uk   Abstract   Prior work on integrating text corpora with   knowledge graphs ( KGs ) to improve Knowl-   edge Graph Embedding ( KGE ) have obtained   good performance for entities that co - occur in   sentences in text corpora . Such sentences ( tex-   tual mentions of entity - pairs ) are represented   as Lexicalised Dependency Paths ( LDPs ) be-   tween two entities . However , it is not possible   to represent relations between entities that do   not co - occur in a single sentence using LDPs .   In this paper , we propose and evaluate sev-   eral methods to address this problem , where   weborrow LDPs from the entity pairs that co-   occur in sentences in the corpus ( i.e. with men-   tionentity pairs ) to represent entity pairs that   donotco - occur in any sentence in the cor-   pus ( i.e. without mention entity pairs ) . We   propose a supervised borrowing method , Su-   perBorrow , that learns to score the suitability   of an LDP to represent a without - mention en-   tity pair using pre - trained entity embeddings   and contextualised LDP representations . Ex-   perimental results show that SuperBorrow im-   proves the link prediction performance of mul-   tiple widely - used prior KGE methods such as   TransE , DistMult , ComplEx and RotatE.   1 Introduction   Knowledge Graphs ( KGs ) are a structured form   of information that underline the relationships   between real - world entities ( Ehrlinger and W ¨oß ,   2016 ; Kroetsch and Weikum , 2016 ; Paulheim ,   2017 ) . A KG is represented using a set of rela-   tional tuples of the form ( h , r , t ) , whererrepre-   sents the relation between the head entity hand   the tail entity t. For example , the relational tu-   ple ( Joe Biden , president - of , US ) indicates that the   president - of relation holds between Joe Biden and   US . There exists a large number of publicly avail-   able and widely used KGs , such as Freebase ( Bol-   lacker et al . , 2008 ) , DBpedia ( Auer et al . , 2007 ) ,   and YAGO ontology ( Suchanek et al . , 2007 ) . KGshave been effectively applied in various NLP tasks   such as , relation extraction ( Riedel et al . , 2013 ; We-   ston et al . , 2013 ) , question answering ( Das et al . ,   2017 ; Sydorova et al . , 2019 ) , and dialogue sys-   tems ( Xu et al . , 2020 ) . However , most KGs suffer   from data sparseness as many relations between   entities are not explicitly represented ( Min et al . ,   2013 ) .   To overcome the sparsity problem , Knowledge   Graph Embedding ( KGE ) models learn representa-   tions ( a.k.a . embeddings ) for entities and relations   in a given KG in a vector space , which can then be   used to infer missing links between entities ( Bordes   et al . , 2013 ; Nickel et al . , 2015 ; Wang et al . , 2017 ) .   Such models are trained to predict relations that   are likely to exist between entities ( known as link   prediction or KG completion ) according to some   scoring formula . Although previously proposed   KGE methods have shown good empirical perfor-   mances for KG completion ( Minervini et al . , 2015 ) ,   the KGEs are learnt from the KGs only , which   might not represent all the relations that exist be-   tween the entities included in the KG . To overcome   this limitation , prior work has used external text   corpora in addition to the KGs ( Toutanova et al . ,   2015 ; Xu et al . , 2016 ; Long et al . , 2016 ; An et al . ,   2018 ; Wang et al . , 2019b , a ; Lu et al . , 2020 ) . Com-   pared to structured KGs , unstructured text corpora   are abundantly available , up - to - date and have di-   verse linguistic expressions for extracting relational   information .   The co - occurrences of two entities within sen-   tences ( a.k.a textual mentions ) in a text corpus has   shown its success for text - enhanced KGEs ( Komni-   nos and Manandhar , 2017 ; An et al . , 2018 ) . For   example , the relational tuple in the Freebase KG   ( Joe Biden , president - of , US ) is mentioned in the   following sentence “ Joseph Robinette Biden Jr. is   an American politician who is the 46th and current   president of the United States . ” This sentence ex-   presses the president - of relation between the two2887entities Joe Biden andUS . As the entity - pair ( Joe   Biden , US ) appears in a single sentence , we call   it awith - mention entity - pair . However , even in a   large text corpus , not every related entity pair co-   occurs in a speciﬁed window , which are referred   to as without - mention entity - pairs in previous stud-   ies . For instance , if we consider the widely used   FB15K-237 KG ( Toutanova et al . , 2015 ) and the   ClueWeb12 ( Gabrilovich et al . , 2013 ) text corpus   with FB entity mention annotations,33 % of entity-   pairs in FB15k-237 do not have textual mentions   within the same sentences . This sparseness prob-   lem limits the generalisation capabilities of using   textual mentions for enhancing KGEs . Speciﬁ-   cally , Toutanova et al . ( 2015 ) ; Komninos and Man-   andhar ( 2017 ) have shown larger improvements in   link prediction for with - mention entity - pairs over   without - mention pairs .   In this paper , we propose a method to augment a   given KG with additional textual relations extracted   from a corpus and represented as LDPs . The aug-   mented KG can then be used to train anyKGE   learning method . This is attractive from both scala-   bility and compatibility point of views because our   proposal is agnostic to the KGE learning method   that is subsequently used for learning KGEs . Our   main contribution in this paper is to improve link   prediction for without - mention entity - pairs by bor-   rowing LDPs from with - mention entity - pairs to   overcome the sparseness in co - occurrences of the   without - mention entity - pairs . We show that learn-   ing a supervised borrowing method , SuperBorrow ,   that scores suitable LDPs to represent without-   mention entity - pairs based on pre - trained entity   embeddings and contextualised LDP embeddings   boosts the performance of link prediction using a   series of KGE methods , compared to what would   have been possible without textual relations .   2 Related Work   KGEs from a Multi - relational Graph : Typi-   cally , KG embedding models consist of two main   steps : ( a ) deﬁning a scoring function for a tu-   ple , and ( b ) learning entity and relation representa-   tions . Entities are usually represented as vectors ,   whereas relations can be represented by vectors   ( e.g. TransE ( Bordes et al . , 2013 ) , DistMult ( Yang   et al . , 2014 ) and ComplEx ( Trouillon et al . , 2016 ) )   matrices ( e.g. RESCAL ( Nickel et al . , 2011 ) ) , or   by 3D tensors ( e.g. Neural Tensor Network ( Socher   et al . , 2013 ) ) .   Using some form of a representation , scoring   functions are then deﬁned to evaluate the strength   of a relation rbetweenhandtentities in a   triple . TransE is one of the earliest and well - known   distance - based KGE method that performs a lin-   ear translation and its scoring function is given in   Table 1 . Alternatively , a bilinear function is used   in several KGE models , such as RESCAL , Dist-   Mult and ComplEx , for which scoring functions   are deﬁned in Table 1 . KGEs are learnt such that   the observed facts ( positive triples ) are assigned   higher scores compared to that of the negative triple   ( for example generated by perturbing a positive in-   stance by replacing its head or tail entities by an   entity randomly selected from the set of entities )   by minimising a loss function , such as the logistic   loss or the margin loss .   Conventional KGE models are trained using   the facts in the KGs , which are often incomplete .   Therefore , to overcome the sparsity of structured   KGs , we propose to integrate information from a   text corpus , thereby augmenting the KG . The aug-   mented KG is then used as the input to existing   KGE methods to learn accurate entity and relation   embeddings . In particular , we do not modify the   scoring functions nor optimisation objectives for   the respective KGE methods , which makes our pro-   posed approach applicable in many existing KGE   methods without any modiﬁcations .   Text - Enhanced KGEs : Recently , a new line of   research that combines textual information with re-   lational graphs has emerged ( Lu et al . , 2020 ) . Dif-   ferent combination methods have been proposed   for this purpose . Wang et al . ( 2014 ) proposed a   model to embed both entities and words ( using en-   tity names and Wikipedia anchors ) into the same   low - dimensional vector space to capture relational2888information from a KG and the co - occurrences   from the corpus . Rosso et al . ( 2019 ) control the   amount of information shared between the two data   sources in the joint embedding space using regu-   larisation . This joint model is further enhanced   by incorporating entity descriptions from an ex-   ternal corpus , which are jointly learnt with the   KG ( Zhong et al . , 2015 ; Xie et al . , 2016 ; Veira et al . ,   2019 ) . In a different scenario , the text - enhanced   knowledge embedding model by Wang et al . ( 2016 )   creates a co - occurrence network of words and enti-   ties from an entity - annotated corpus . The authors   deﬁne point - wise and pair - wise contexts using the   co - occurrence frequencies in the network . Then ,   entity and relation embeddings are enhanced using   textual point - wise and pair - wise embeddings , re-   spectively . Similarly , Rezayi et al . ( 2021 ) construct   an augmented KG that has nodes from external   text . The original and the augmented graphs are   then aligned to suppress the noise and distil relevant   information . In our work , we focus on adding extra   edges to the KG rather than nodes as in Rezayi et al .   ( 2021 ) and Wang et al . ( 2016 ) .   In addition to contextual information and textual   descriptions of individual words / entities , sentences   where two entities co - occur have been used as con-   textual evidence to learn KGEs ( Toutanova et al . ,   2015 ; Komninos and Manandhar , 2017 ; Tang et al . ,   2019 ) . For example , Toutanova et al . ( 2015 ) ex-   tracted LDPs by parsing co - occurring sentences   in a text corpus , which are then used as textual   relations in the KG . This model can be seen as   a special case of universal schema ( Riedel et al . ,   2013 ) , which combines textual and KG relations in   the same entity - pair co - occurrence matrix , subse-   quently decomposed to obtain entity embeddings .   Komninos and Manandhar ( 2017 ) proposed a novel   triple scoring function where textual mentions are   used as a source of supporting evidence for a triple .   Our problem setting differs from prior work on   text - enhanced KGEs in two important ways . First ,   we do not modify the underlying structure of the   KGE method , which is attractive from both scala-   bility and compatibility of our proposal . Second ,   rather than considering only entity - pairs that are oc-   curring within a speciﬁed context in the corpus ( i.e.   with - mention entity - pairs ) , we propose to borrow   LDPs from with - mention entity - pairs to overcome   the data sparseness in without - mention entity - pairs   that never co - occur within any sentence in the cor-   pus.3 Method   A relational KGDconsists of a set of entities Eand   a set of relationsR. InD , knowledge is represented   by relational tuples ( h , r , t ) ∈D , where the head   entityhis related to the tail entity tby the KG   relationr . In this work , we assume relations to be   asymmetric in general ( if ( h , r , t ) ∈D then it does   not necessarily follow that ( t , r , h ) ∈D ) . The goal   is to learn representations for entities and relations   such that missing tuples can be accurately inferred .   As KGsDare often sparse with many missing   edges between entities , the learnt KGEs are af-   fected , which in return impacts the performance of   KGEs on downstream tasks such as link prediction .   To address this sparseness problem , we consider the   availability of a text corpus Twhere relational facts   are expressed using contexts in which an entity - pair   co - occurs . The textual relations that are extracted   fromTcan be injected intoDbefore applying a   KGE method .   To alignDwithT , entity linking is applied to   resolve ambiguous entity mentions in the text with   unique entities in the KG ( Gabrilovich et al . , 2013 ;   Shen et al . , 2014 ) . Then , Sentences that mentions   two entities are considered as textual mentions of   relations between entities . Assuming that the cor-   pus is annotated using the entities in D , there are   multiple possibilities to obtain relational features   of sentences that mention the entities . Following   previous work ( Toutanova et al . , 2015 ) , we ﬁrst run   a dependency parser ( Chen and Manning , 2014 ) on   each sentence containing an entity - pair to obtain   LDPs . Then , ifEcontains the head and tail entities   of an LDPl(but the entity - pair might not be con-   nected by KG relations ) , we insert lintoRto form   a textual triple ( h , l , t ) ∈D. The augmented KG   is then used to learn embeddings for EandRus-   ing different KGE methods . During KGE processs ,   we treat both original relations in the KG and the   augmented LDPs equally . In principle , any exist-   ing KGE learning method can be applied on the   augmented KG as we later see in our experiments .   One obvious limitation of the above - described   method is that entity - pairs that never co - occur in   any contextual window ( e.g. a sentence ) will not   be connected by any LDP during the augmenta-   tion process . This is ﬁne if the two entities are   truly unrelated . However , this is problematic for   entities that are related in the KG but their rela-   tions were not sufﬁciently covered in the text cor-   pus because of the coverage issues and small size2889of the corpus . As we later see in our evaluations   ( § 5 ) , this is indeed the case for the majority of the   without - mention entity - pairs . To overcome this lim-   itation of our proposal , next we describe a method   toborrow LDPs from with - mention entity - pairs   to without - mention entity pairs . It is worth not-   ing that we do not connect any two entities by   LDPs , but only those that are related in the KG   and predicted to be associated with an LDP by the   proposed method .   3.1 Learn to Borrow LDPs   Given a without - mention entity pair ( h , t ) , we   propose a supervised borrowing method SuperBor-   row to rank LDPs that are extracted for the with   mention entity - pairs from a text corpus . Given pre-   trained entity representations handt , we learn an   entity - pair encoder , f , parametrised by θ , to create   an entity - pair representation , f(h , t;θ ) , for(h , t ) .   In this work , the encoder fis implemented as a   multilayer perceptron with a nonlinear activation ,   where the input entity - pair to the MLP is encoded   as follows :   x= [ h⊕t⊕(h−t)⊕(h ◦ t ) ] ( 1 )   Here,⊕denotes the concatenation of vectors and   ◦ is the element - wise multiplication between two   vectors . ( 1)considers the information in the head   and tail entity embeddings independently as well   as the interactions between their corresponding di-   mensions . These features for entity - pairs have been   used successfully for representing semantic rela-   tions in prior work ( Washio and Kato , 2018 ; Joshi   et al . , 2018 ; Hakami and Bollegala , 2019 ) . The   ﬁnal output vector f(h , t;θ)of the MLP is treated   as the representation of the entity - pair ( h , t ) .   As an alternative to representing the relation-   ship between two entities in an entity - pair ( h , t )   byf(h , t;θ)using the corresponding entity embed-   dings , we can useS , the set of LDPs connecting   handtentities ( Bollegala et al . , 2010 ) . Because   an LDP is a sequence of textual tokens , we can use   any sentence encoder to represent an LDP by a vec-   tor . Speciﬁcally , in our experiments later we use   the pretrained sentence encoder SBERT ( Reimers   and Gurevych , 2019 ) to represent an LDP , l , by a   vector , l.   We require LDPs that co - occur with an entity-   pair(h , t)to be similar to f(h , t;θ)than LDPs   that do not co - occur with ( h , t ) . Speciﬁcally , we   use the set of with - mention entity - pairs with their   associated LDPs as positive training instances S=   { ( h , l , t ) } . LDPs that are associated with either h   ortalone ( not both ) are used as negative training   instancesSas given by ( 2 ) .   S={(h , l , t)|∃t : ( h , l , t)∈D∧t / negationslash = t ,   ∃h : ( h , l , t)∈D∧h / negationslash = h } ( 2 )   We learn the parameters of f(h , t , θ ) by min-   imising the marginal loss over SandSas   shown in ( 3 ) .   ( 3 )   Here , γ(≥0)is the margin and is set to 1 in   our experiments . To determine which LDPs to   be borrowed for a particular without - mention en-   tity pair , ( h , t ) , we ﬁrst compute its represen-   tation , f(h , t;θ)using theθfound by minimis-   ing(3)above . We then score each LDP , l , using   the sentence encoder model , by the inner - product ,   f(h , t;θ)l . We then select the top- kLDPs with   the highest inner - products with f(h , t;θ)to aug-   ment the KG . The number of borrowed LDPs ( k ) is   a hyperparameter that is tuned using the validation   triples selected from the KG .   4 Experimental Setup   4.1 Dataset and Training Details   Datasets : We use FB15k237 as the KG and   ClueWeb12as the corpus for extracting LDPs for   the entity - pairs in the FB157k237 KG . Speciﬁcally ,   we use the textual triples consisting of LDPs that   are extracted and made availableby Toutanova   et al . ( 2015 ) . The number of extracted unique LDPs   and textual triples in this dataset are respectively2890   2,740,176and3,978,014 . To make the training   of KGE methods computationally efﬁcient , we ﬁl-   ter out LDPs that occur in less than 100distinct   entity - pairs in the corpus . The FB15k237 test set   is split into with - mention ( i.e. entity - pairs that   co - occur in some LDP ) and without - mention ( i.e   entity - pairs that do not co - occur in any LDP ) sets   as shown in Table 2 . According to Table 2 , there   are88.14 % without - mention entity - pairs in the test   set . Note that even if we consider the complete   set of LDPs from the ClueWeb12 , the portion of   without - mention test entity - pairs in FB15k237 is   only 73.23 % . This shows the signiﬁcance of the   problem of representing without - mention entity-   pairs , which is the focus of this paper .   Training SuperBorrow : We used the with-   mention entity - pairs in train split of FB15K237 to   train SuperBorrow . The number of entity - pairs in   the training set is 311,906 , and on average we have   1.32LDPs per entity - pair . On average , we generate   100 negative triples for each with - mention pair . We   hold - out 10 % of the training entity - pairs for valida-   tion purposes ( we obtain 280716 and31190 entity-   pairs for training and validation , respectively ) . To   represent each entity , we use the publicly avail-   able 100 - dimensional pre - trained RelWalk embed - dings , which are publicly available for the entities   and relations in FB15k237 .   According to ( 1 ) , the input layer of the trained   MLP has 400 features . The hyperparameters   including the number of hidden layers { 2,3 } ,   /lscript , regularisation coefﬁcient { 0,0.01,0.001 } ,   the learning rate{0.01,0.1}and the non - linear   activation{tanh , relu , sigmoid}are tuned using   the above - mentioned validation set . The MLP   consists of two 768 - dimensional layers , and   the last layer represents the entity - pair to be   mapped to the LDP embedding space that has   768 dimensions encoded using the SBERT   paraphrase - distilroberta - base   model , which has reported SoTA performance on   various knowledge - intensive tasks ( Warstadt et al . ,   2020 ) . SuperBorrow is trained for 50 iterations   using mini - batch Stochastic Gradient Descent with   momentum and a batch size of 128 . The source for   SuperBorrow is publicly available.2891Evaluation Protocol : After augmenting   FB15K237 with the borrowed kLDPs for   each without - mention entity - pair , we train a KGE   method to obtain embeddings for the entities   inE , relations inRand textual relations . The   hyperparameter kis tuned on the validation   set of FB15K237 for each KGE method from   { 1,3,10,15,20,25,30 } .   We use Link Prediction , which has been popu-   larly used as an evaluation task to compare the   KGEs we obtain from a KGE learning method   before and after augmenting the KG with the   LDPs borrowed using SuperBorrow and other base-   lines ( Wang et al . , 2021 ) . Link prediction is the   task of predicting the missing head ( i.e. ( ? , r , t ) ) or   tail ( i.e. ( h , r , ? ) ) entity in a given triple by ranking   the entities in the KG according to the scoring func-   tion of the KGE method . Following prior work ,   the performance is evaluated using Mean Recip-   rocal Rank ( MRR ) , Mean Rank ( MR ) and Hits at   Rank k ( H@k ) under the ﬁltered setting , which re-   moves all triples appeared in training , validating or   testing sets from candidate triples before obtaining   the rank for the ground truth triple . We consider   all entities that appear in the corresponding argu-   ment of the relation to be predicted to further ﬁlter   out incorrect candidates , which is known as type-   constraint setting ( Chang et al . , 2014 ; Toutanova   and Chen , 2015 ) .   We also evaluate the learnt KGEs using a relation   prediction task , which predicts the relation between   two given entities ( i.e. , ( h,?,t ) ) from the set of   relations in the KG . This task assumes that we   are given entity - pairs with candidate relations . The   performance is measured using the same evaluation   metrics as used in the link prediction task under   the ﬁltered setting . We use the publicly available   OpenKE tool to conduct experiments with different   KGE methods ( Han et al . , 2018 ) .   4.2 Baselines   We compare the proposed LDP borrowing method   against multiple baselines as follows .   LinkAll : In this baseline we connect the two en-   tities in each without - mention entity - pair with a   unique link , instead of reusing LDPs , and aug-   ment the KG with the without - mention entity - pairs .   This baseline enables us to simply incorporate all   without - mention entity - pairs into the KG without   requiring to borrow any LDPs . It will demonstratethe importance , if any , of sharing LDPs between   with- vs. without - mention entity - pairs as opposed   to simply connecting all without - mention entity-   pairs with distinct relations .   Co - occurrence : This baseline connects all   entity - pairs that co - occurs in any sentence in   the corpus ( T ) with a generic relation ( i.e. co-   occurrence relation ) in the augmented KG and   does not distinguish between different textual re-   lations . This baseline is designed to highlight   the importance of the context of entity - pair co-   occurrences in the corpus beyond simply treating   all co - occurrences equally during the augmenting   process .   NeighbBorrow : Given a without - mention entity-   pair(h , t ) , we can borrow the LDPs from the ﬁrst   nearest neighbouring ( 1NN ) with - mention entity-   pair(h , t ) . The similarity between entity - pairs can   be computed using ( 4)in an unsupervised manner   using pretrained entity embeddings such as Rel-   Walk embeddings ( Bollegala et al . , 2021 ) .   sim((h , t),(h , t ) ) = cos ( h , h ) cos ( t , t)(4 )   Here , cosis the cosine similarity between two vec-   tors converted to nonnegative values ( i.e. [ 0,1 ] )   using the linear transformation ( x+ 1)/2 . On aver-   age , when considering 1NN , we borrow 1.3LDPs   for each without - mention pair of entities . In con-   trast to the proposed SuperBorrow , NeighbBorrow   is unsupervised and decouples entities in each pair   when computing their similarity .   5 Results   Link Prediction : Table 3 shows the results of   link prediction for different settings on FB15K237   under different KGE methods . Two translational   distance - based KGE methods ( i.e. TransE and   RotatE ) and two semantic matching - based mod-   els ( i.e. DistMult andComplEx ) are used as the   KGE learning methods ( Rossi et al . , 2021 ; Wang   et al . , 2021 ) . We emphasize that our purpose here   isnotto compare the absolute performance among   those KGE methods , but to evaluate the effect of   using LDPs for augmenting the KG and represent-   ing the without - mention entity - pairs via different   borrowing methods . For SuperBorrow , the optimal   numbers of borrowed LDPs ( k ) determined using   the validation set for TransE , DistMult , ComplEx   and RotatE respectively are 30 , 20 , 15 and 25.2892   As shown in Table 3 , augmenting the KG   with the extracted LDPs ( i.e. , KG+ExtractedLDPs )   signiﬁcantly improves the performance for with-   mention entity - pairs for all KGE methods . How-   ever , the performance when predicting links for   without - mention entity - pairs decreases slightly for   all KGE methods , except for DistMult in the   KG+ExtractedLDPs setting . For the borrowing   models , even though the co - occurrence baseline   improves the prediction for without - mention set ,   borrowing relevant LDPs from the 1NN entity-   pairs ( NeighbBorrow ) or the proposed supervised   borrowing ( SuperBorrow ) reports superior results .   We can see that the best performance for the over-   allandwithout - mention sets are achieved with the   augmented KG using SuperBorrow , followed by   NeighbBorrow .   Relation Prediction : Table 4 shows the accura-   cies for the relation prediction task . Experimen-   tally , the best results for this task is obtained when   corruptingr , in addition to handtcorruptions , is   applied to generate negative triples to train the KGE   method . This negative sampling schedule follows   the evaluation procedure of relation prediction . As   shown in the table , SuperBorrow reports the best   MR and Hits@3 for DistMult KGEs , while Neibh-   Borrow baseline performs better than SuperBorrow   with ComplEx method . Further results for relation   prediction are in the Supplementary Appendix A.   Comparisons against Prior Work : We   compare our proposed method against   prior work , namely Feature Rich Network   ( FRN ) ( Komninos and Manandhar , 2017 ) and   Conv ( E+DistMult ) ( Toutanova et al . , 2015 ) .   In FRN , an MLP is trained to predict the   probability of a given triple being true using   different types of features such as the entity   types and features extracted from textual relation   mentions . Conv(E+DistMult ) represents LDPs   by vectors using a convolutional neural network ,   and combines DistMult scoring function with that   of the Entity model ( E ) proposed by Riedel et al .   ( 2013 ) . E model learns a vector for each entity   and two vectors for each relation corresponding   to the two arguments randrof a relation r.   The scoring function of a triple in E model is   deﬁned as hr+tr . The combined model   ( E+DistMult ) is trained on a linearly weighted   combination of KG triples and textual triples .   For a fair comparison , we consider the task of   predicting missing tail entities and we avoid the   type - constraint setting .   As shown in Table 5 , for the overall test set of   FB15K237 our models outperform both FRN and   Conv models according to MRR and H@10 . For   with - mention entity - pairs , our models report higher   scores compared to Conv(E+DistMult ) , while FRN   performs best . For with - mention entity - pairs FRN   can extract rich features from the contexts of co-   occurrences , which helps it to obtain superior per-   formances . However , both FRN and Conv models   perform poorly on without - mention entity - pairs ,   where such contextual features are unavailable . On   the other hand , by using the proposed SuperBorrow   to augment LDPs for KGs we can overcome this   limitation successfully .   6 Analysis   Borrowed LDPs : To provide examples of LDPs   injected into FB15K237 , Table 6 shows the bor-   rowed LDPs by NeighbBorrow and SuperBorrow   for some selected entity - pairs . We can see that   representative LDPs of various relation types are   ranked at the top by SuperBorrow . For example , for2893   theﬁlm - distributor relation , NeighbBorrow selects   LDPs containing speciﬁc tokens such as movie or   ﬁlm , whereas SuperBorrow retrieves LDPs that bet-   ter express the target relation such as 20th Century   Fox:/angbracketleft - dobj / angbracketright : released:/angbracketleftnsubj / angbracketright : Lincoln .   Relation Categories : To better analyse the ef-   fect of the proposed SuperBorrow for KGEs , we   evaluate the link prediction task on different re-   lation categories including 1to1 , 1toN , Nto1 and   NtoN as deﬁned in Bordes et al . ( 2013 ) .   Table 7 presents the results of predicting head   entities for all KGE methods considering KG only   and SuperBorrow . We can see that SuperBor-   row achieves higher performance over the original   graph on all relation categories . In particular , our   proposal signiﬁcantly boosts the performance of   predicting head entities for the Nto1 relation type   where all KGE methods report the lowest H@10 for   the KG only setting . Similar results are obtained   for predicting the tail entities as in Appendix B.   Overall , these results show that incorporating infor-   mation from text corpora into KGs enables us to   learn KGEs that encode diverse relation types .   Visualisation of Entity Embeddings : In Fig-   ure 1 , we visualise the entity embeddings of   KGonly and KG with LDPs using t - distributed   stochastic neighbour embeddings ( t - SNE ) ( Hinton   and Roweis , 2002 ) method . Relations in FB15k237   are labelled as domain / type / property where do-   main / type represents the type of a head entity in   the relation . Thus , for each entity in the KG , we   extract its types from all training triples where the   entity acts as the head . We label entities that belong   to the two most frequent entity types , which are   people / person ( 4,538 entities ) and ﬁlm/ﬁlm ( 1,923   entities ) . From Figure 1 , we see that the embed-   dings learnt from the augmented graph results in   distinct clusters of the same type , compared to the   clusters obtained from the KG alone . This empha-   sizes the importance of using textual mentions in   KGE learning.2894   7 Conclusion   We considered the problem of representing without-   mention entity - pairs in KGE learning . Speciﬁcally ,   we proposed a method ( SuperBorrow ) to determine   which LDPs to borrow from with - mention entity-   pairs to augment a KG using a corpus . Our pro-   posed method improves the performance of several   KGE learning methods in link and relation predic-   tion tasks .   References28952896   Appendix   A Relation Prediction   Relation prediction results for all the KGE meth-   ods are shown in Table 8 . As we see , unlike se-   mantic matching - based KGE models , incorporat-   ing LDPs into the KG do not improve relation pre-   diction for translational distance - based KGE meth-   ods ( TransE and RotatE ) . For KG+ExtractedLDPs   embeddings , the performance for with - mention set   decreases by 0.045and0.012on average for MRR   and H@{10,3,1 } , for TransE and RotatE respec-   tively . In - depth analysis for this observation can be   conducted in future research .   B Tail Prediction for Relation Categories   Table 9 presents Hits@10 for tail prediction con-   sidering 1to1 , 1toN , Nto1 and NtoN relation cate-   gories . As we see , SuperBorrow embeddings ob-   tain the best results for all KGE methods and all   the relation categories .   C Training KGE Methods   For reproducability , we list the hyperparameter   setting to train KGE methods in Table 10 . Ada-   Grad ( Duchi et al . , 2011 ) with 100 batches is used   to learn KGEs . Table 11 shows the training time ( in   hours ) to train KGE methods for KG only and Su-   perBorrow using OpenKE - Pytorch tool ( Han et al . ,   2018).28972898