  Terne Sasha Thorn Jakobsen , Anna RogersCopenhagen Center for Social Data Science , University of CopenhagenRIKEN Center for Computational Scienceterne.thorn@sodas.ku.dk , arogers@sodas.ku.dk   Abstract   Both scientific progress and individual re-   searcher careers depend on the quality of   peer review , which in turn depends on paper-   reviewer matching . Surprisingly , this problem   has been mostly approached as an automated   recommendation problem rather than as a mat-   ter where different stakeholders ( area chairs ,   reviewers , authors ) have accumulated experi-   ence worth taking into account . We present the   results of the first survey of the NLP commu-   nity , identifying common issues and perspec-   tives on what factors should be considered by   paper - reviewer matching systems . This study   contributes actionable recommendations for im-   proving future NLP conferences , and desider-   ata for interpretable peer review assignments .   1 Introduction   Peer review is increasingly coming under criticism   for its arbitrariness . Two NeurIPS experiments   ( Price , 2014 ; Cortes and Lawrence , 2021 ; Beygelz-   imer et al . , 2021 ) have shown that the reviewers are   good at identifying papers that are clearly bad , but   the agreement on the “ good ” papers appears to be   close to random . Among the likely reasons for that   are cognitive and social biases of NLP reviewers   ( see overview by Rogers and Augenstein , 2020 ) ,   fundamental disagreements in such an interdisci-   plinary field as NLP , and acceptance rates that are   kept lowirrespective of the ratio of high - quality   submissions .   Such arbitrariness leads to understandable frus-   tration on the part of the authors whose jobs and   graduation depend on publications , and it also   means lost time and opportunities ( Aczel et al . ,   2021 ; Gordon and Poulin , 2009 ) for science over-   all . Reviews written by someone who does not have   the requisite expertise , or does not even consider   the given type of research as a contribution , it is aFigure 1 : Overview of all respondents and overlap of   their roles for their last experience at NLP venues .   loss for all parties : the authors do not get the intel-   lectual exchange that could improve their projects   and ideas , and reviewers simply lose valuable time   without learning something they could use . It is   also a loss for the field overall : less popular topics   could be systematically disadvantaged , leading to   ossification of the field ( Chu and Evans , 2021 ) .   This paper contributes a snapshot of this prob-   lem in NLP venues , based on a survey of authors ,   reviewers and area chairs ( ACs ) . We collected 180   responses , which is is comparable to the volume   of feedback collected for implementing the ACL   Rolling Review ( ARR ) . The overall distribution of   respondents ’ roles is shown in fig . 1 . We present   the commonly reported issues and community pref-   erences for different paper assignment workflows   ( section 4 ) . We derive actionable recommendations   to how peer review in NLP could be improved ( sec-   tion 5 ) , discuss the limitations of survey methodol-   ogy ( section 6.2 ) , and conclude with desiderata for   interpretable peer review assignments ( section 6.3 ) .   2 Background : Peer Review in NLP   Paper - reviewer assignments are matches between   submissions to conferences or journals and their   available pool of reviewers , taking into account the   potential conflicts of interest ( COI ) and reviewer   assignment quotas.4810Among the systems used in recent NLP con-   ferences , the Softconf matching algorithm takes   into account bidding , quotas , and manual assign-   ments , and randomly assigns the remaining papers   as evenly as possible . NAACL and ACL 2021   used SoftConf , but also provided their ACs with   affinity scores produced by a “ paraphrastic simi-   larity ” system based on an LSTM encoder , which   is trained on Semantic Scholar abstracts ( Wieting   et al . , 2019 ; Neubig et al . , 2021 ) . Affinity scores   are scores indicating how well a given submission   matches a given reviewer . They are typically com-   puted as the similarity ( e.g. cosine similarity ) be-   tween the embeddings of certain information about   the submission and the reviewer ’s publication his-   tory ( e.g. abstracts and titles ) .   ARR switched to OpenReview and currently   usestheir SPECTER - MFR system ( OpenReview ,   2021 ) which is based on SPECTER ( Cohan et al . ,   2020 ) and MFR embeddings ( Chang and McCal-   lum , 2021 ) for computing affinity scores . The as-   signments are then made with the MinMax match-   ing algorithm .   The problem of paper - reviewer assignment is by   itself an active area of research ( see overview of key   issues for CS conferences by Shah ( 2022 ) ) . There   are many proposals for paper - reviewer assignment   systems ( Hartvigsen et al . , 1999 ; Wang et al . , 2010 ;   Li and Watanabe , 2013 , inter alia ) , some of which   also consider the problem of “ fair ” assignments   ( Long et al . , 2013 ; Stelmakh et al . , 2019 ) . Such   studies tend to be hypothesis - driven : they make an   assumption about what criteria should be taken into   account , design a system and evaluate it . To the   best of our knowledge , ours is the first study in the   field to address the opposite question : what criteria   should be taken into account , given the diversity of   perspectives in an interdisciplinary field ? We take   that question to the community .   3 Methodology : survey structure and   distribution   We developed three separate surveys for the main   groups of stakeholders in the peer review process :   authors , reviewers and ACs .   They follow the same basic structure : consent to   participation ( see Impact Statement ) , backgroundinformation , questions on most recent experiences   in the role which the survey pertains to , and how   the respondents believe paper - reviewer matching   should be performed . Most questions are asked   to respondents in all three roles , reformulated to   match their different perspectives .   The responses were collected late 2021 and all   respondents are required to confirm that their most   recent experience as an AC / reviewer / author is in   2019 - 2021 . The full surveys and response data are   publicly available .   Participant background . All surveys include   questions on career status and the number of times   the respondents have been ACs / reviewers / authors   at NLP venues . We ask what venues they have   experience with ( as broad categories ) and what   types of contributions they make in their work .   Participant experience with peer review . We   further ask the respondents a range of questions   about their experience as AC / reviewer / author : how   satisfied they are with the process , what issues they   have experienced , what was the assignment load   ( ACs and reviewers ) , how paper - reviewer match-   ing was done , how they would prefer it to be done ,   and which factors they believe to be important for   paper - review matching . Most of the questions are   multiple - choice , with addition of some open - ended   questions where appropriate , so that respondents   can elaborate their answers or add to the available   options . Whenever possible , the question formu-   lations were taken from the question bank of UK   Data Service ( Hyman et al . , 2006 ) . Attitude ques-   tions use a 5 - point Likert scale .   Limited memory is an important concern in sur-   veys ( Sudman and Bradburn , 1973 ; Öztas Ayhan   and Isiksal , 2005 ) , and we can not expect the re-   spondents to accurately recall all their experience   with peer review . To reduce memory recall errors ,   the survey focuses on the respondent ’s most recent   experience , but they also have a chance to reflect   on prior experience in open - ended questions , and   to report whether they experienced certain issues   at any time in their career .   Survey distribution . We distributed the surveys   via three channels : by handing out flyers at   EMNLP 2021 , through mailing lists ( ML - news ,   corpora list , Linguist list ) , and through Twitter with   the hashtag # NLProc . Participation was voluntary,4811   with no incentives beyond potential utility of this   study for improving NLP peer review .   Data validation . Given that links to surveys were   distributed openly and that we did not ask for any   identifiable information , the surveys needed to in-   clude other means of validation to ensure that the   responses included in the analysis were from atten-   tive , relevant individuals . Our approach for validat-   ing the data quality follows satisficing theory ( Liu   and Wronski , 2018 ) , with the main safeguards be-   ing 1 ) the checking of response consistency , includ-   ing a few “ traps " where inconsistency or illogical   responses can be exposed , and 2 ) the inclusion of   open - ended questions .   73 % ACs , 40 % reviewers , 33 % authors have   provided at least one response to our open - ended   questions , and we did not find any meaningless   or incoherent comments not addressing the ques-   tion . For consistency checks , all respondents were   asked :   •How many times they have been an   AC / reviewer / author . One of the options   was “ 0 ” , contradicting the earlier confirmation   of experience in a given role .   •When was the last time they were an   AC / reviewer / author . One of the options was “ ear-   lier than 2019 ” , contradicting the earlier confir-   mation of peer review experience in 2019 - 2021 .   •Whether they have performed the other roles .   New authors may have not reviewed or AC - ed ,   but reviewers should also have been authors , and   ACs should have experience with all roles .   4 Results   Overall we received 38 responses from ACs , 87   from reviewers and 81 from authors ( 206 in total).After removing 20 incomplete responses and 8 re-   sponses inconsistent with the “ trap ” questions , we   report the results for 30 responses from ACs , 77   from reviewers and 73 from authors ( 180 in total ) .   4.1 Who are the respondents ?   According to the past conference statistics , we   could expect that many submissions would be pri-   marily authored by the students , and reviewers are   generally expected to be relatively senior , which   should correspond to their going through peer re-   view more often . We can use this expected pattern   as an extra validation step for the survey responses .   Figure 2 shows that the responses are in line   with this expected pattern . We received the most   responses from academic researchers ( 62 ) , PhD   students ( 54 ) , and postdocs ( 32 ) . Most academic   researchers and postdocs , but not PhD students ,   have had their work reviewed more than 10 times .   At the same time 65 % of the PhD students who   served as reviewers went through peer review more   than 5 times , as opposed to 24.2 % of PhD students   in the author role . Fewer industry than academic   researchers responded to the survey . This could be   related to the fact that a large part of the “ academic ”   demographic are students – and in 2020 - 2021 the   ACL membership among students was equal to or   exceeding other demographics ( Rasmussen , 2021 ) .   4.2 Paper types   The next question is to see what kinds of research   papers the respondents to our surveys have au-   thored : engineering experiment , survey , position   paper etc . , according to the COLING taxonomy   by Bender and Derczynski ( 2018 ) . We expect that   more senior researchers will have more experience4812with different types of work . Indeed , on average   the authors have worked with 2.5 types of papers ,   vs. 3.0 for reviewers and 3.6 for ACs . The distribu-   tion is shown in fig . 3 . The most respondents have   authored engineering experiment papers ( with the   authors reporting the most such work ) .   Note that this only indicates whether the respon-   dents to our surveys have or have not authored   certain types of papers , rather than how many . In   terms of volume , the engineering papers are a lot   more prevalent : e.g. at ACL 2021 the “ Machine   learning ” track had 332 submissions , vs 168 in the   “ Resources and evaluation ” track ( Xia et al . , 2021 ) .   4.3 What kinds of problems do people report ?   As with any voluntary feedback , our surveys were   likely to receive more responses from people who   had a grievance with the current process . Indeed ,   we find that only 6.7 % of ACs , 20.5 % of authors ,   and 22.1 % of reviewers say that they have not had   any issues in their last encounter with NLP venues .   The overall distribution for the types of problems   reported by the authors , reviewers and ACs in their   last and overall experience is shown in fig . 5 . Given   that at the time of this survey the ARR was recently   deployed as the only ACL submission channel , we   highlight the responses from the people for whom   the most recent venue was ARR : 28 % reviewers ,   18 % authors , 50 % ACs .   The key takeaways are as follows :   •Two of the most frequent complaints of ACs   ( about 50 % of the respondents ) are insufficient in-   formation about reviewers and clunky interfaces ;   •Many paper - reviewer mismatches ( about 30 % , if   the report of the last experience is representative )   areavoidable : they should have been clear from   the reviewers ’ publication history ;   •Over a third of the author respondents in their last   submission ( about 50 % over all history ) received   reviews from reviewers lacking either expertise   or interest , and that is supported by the review-   ers ’ reports of being assigned papers that were   mismatched on one of these dimensions ;   •The authors report that many reviews ( over a   third in last submission , close to 50 % overtime )   are biased or shallow , which might be related to   the above mismatches in expertise or interest ;   •Two patterns are exclusive to ARR : insufficient   time for ACs , and zero authors with no issues .   4.4 Knowledge of the workflow   Our next question is what methods NLP venues   use to match submissions to reviewers , and to what   extent the stakeholders ( authors and reviewers ) are   aware of how it is done . We find that relatively   few authors ( 23.3 % ) and reviewers ( 23.4 % ) know   for sure what process was used , which begs for   more transparency in the conference process . The   ACs report that the most frequent case ( 37 % ) is a   combination of automated and manual assignments .   Interestingly , most reviewers believe that their as-   signments were automated ( 36 % ) , and only ( 28 % )   believe they were automated+manual . See App .   Figure 8 for full distribution .   5 The Ideal Process   5.1 Ideal workflow   When asked about what paper - assignment process   they would prefer ( given that fully manual match-   ing is impractical for large conferences ) , most ACs   and authors opted for automated+manual process ,   but for the reviewers this is the second preferred   process ( 26 % ) , with 30 % opting for bidding + man-   ual checks ( see fig . 4 ) . There was also relatively4813   large support for pure bidding ( 13 - 18 % of respon-   dents in all roles ) , and cumulatively pure bidding   and bidding with manual adjustments have as much   or more support from all respondent categories than   the automated matching + manual assignments .   The analysis of open - ended comments suggested   that the respondents were aware that bidding is   quite labor - intensive on the part of the reviewers .   5 ACs , 3 reviewers and 2 authors suggested using   affinity scores to filter the papers on which bidswould be requested , followed with manual check-   ing . Another suggestion was keywords or more   fine - grained areas / tracks , potentially as alternative   to affinity scores for filtering down the list of pa-   pers to bid on . One AC suggested “ an extensive ,   but still finite , set of tags ( e.g. an ACL - version of   ACM CCS concepts , or FAccT ’s submission tags ” .   One reviewer stressed that the keywords should be   provided by the authors , to match what they per-   ceive to be the salient aspects of the paper.48141 reviewer and 1 author suggested looking at   whether the paper cites the potential reviewer , as   this could be a good indicator for the reviewer ’s   interest . 1 reviewer and 2 authors voiced support   for some randomness in the assignments ( given a   track - level match ): “ Bidding + some random as-   signment to ensure diversity in the matching . We   do n’t want reviewers to review only papers they   * want * to review . However these random assign-   ments should be clearly indicated to all , and treated   accordingly . ”   5.2 Ideal assignment criteria   AC past experience . Figure 5 shows that one of   the most common problems for the ACs is that   they were not provided with enough information to   facilitate the paper - reviewer matching . The follow-   up question is what information they areprovided   with , and how useful they find it .   Figure 6 shows that the types of information with   the highest utility information are links to reviewer   profiles , bidding information , and affinity scores .   But affinity scores are also the most controversial :   it is the type of information that the most ACs find   “ not very useful ” or “ not useful at all ” ( 20 % ) .   Overall the results suggest that ACs are pre-   sented with little structured information about re-   viewers , and have to identify the information they   need from a glance at the reviewers ’ publication   record . Seniority , expertise , and reviewer history   notes from other ACs are all reported to be useful ,   but they were never provided directly to many ACs .   An avenue for future research is offered by three   types of information that the most ACs are not   sure about , presumably because they are rarely pro-   vided : structured information about the methods   that the reviewers were familiar with , the languages   they spoke , and affinity score explanations . We will   show below that there is much support for taking   such methods into account . For the languages , this   might be due to the “ default ” status of English   ( Bender , 2019 ) . We hypothesize that providing this   information would make it easier to provide bet-   ter matches for papers on other languages , which   would in turn encourage the authors to submit more   such work . Affinity will be discussed in section 6.3 .   Stakeholder preferences . We then asked the re-   spondents what factors they believe are importantfor paper - reviewer assignments . Their answers are   shown in fig . 7 . The overall mean importance rank-   ings ( on scale 0 - 5 ) are as follows :   The fact that affinity scores rank the least im-   portant for NLP researchers ( who would know the   most about them ) is interesting , and perhaps related   to the fact that evaluation of paper - reviewer match-   ing systems remains an open problem , with little   empirical evidence for how well our current sys-   tems really work . In the absence of such evidence ,   our results suggest that the respondents across all   groups are not very positive about their experience   with such systems . In the authors ’ personal expe-   rience , when the conference chairs provide auto-   mated affinity scores they caution the area chairs   against fully relying on them , and urge to adjust   the assignments manually .   Our data suggests that within groups of stake-   holders the individual variation in importance of   different factors is higher for some factors and   stakeholders than others : e.g. ACs vary within   1 point on the importance of knowing the data , but   only within 0.74 points on importance of knowing   the tasks . This has implications for approaches   who would rely on AC assignments as ground truth   for automated assignment systems : they could end   up modeling the annotator instead of the task ( Geva   et al . , 2019 ) . See App . table 2 for full data .   We then explored the question of whether the   experience of having authored research of a cer-   tain type correlates with any changes in the attitude   towards some of these paper - reviewer matching fac-   tors . For each pair of type of research and matching   factor , we ran two - sided Fisher ’s Exact tests for all   respondents who have authored ( or not ) the types   of research and the importance they attached to   different factors in paper - reviewer assignment ( bin-   ning on less than moderately important and more   than moderately important ) . For some pairs there   were statistically significant differences : e.g. the re-   spondents who have authored reproduction papers   were significantly more likely to believe it impor-   tant that the reviewer has worked with the same   kind of data ( p= 0.004 ) , and respondents who au-   thored position papers were significantly lesslikely4815   to believe a high automated affinity score is impor-   tant ( p= 0.003 ) . See table 3 in the appendix for   allp - values and more details on the tests . We note   that the relationships are not necessarily causal .   We conclude that our sample does provide evi-   dence ( the first , to our knowledge ) that researchers   in interdisciplinary fields who perform different   kinds of research may have differing preferences   for what information should be taken into account   for paper - reviewer assignments . If that effect is   robust , it should be considered in assignment sys-   tems for interdisciplinary fields . We hope that this   finding would be explored in a larger study , tak-   ing into account both the experience of authoring a   given type of paper and how central that type of re-   search is for a given researcher ( a factor that we didnot consider ) . Another direction for future work   is exploring this question from the perspective of   demographic characteristics and the type of insti-   tution the respondents work in . Should there be   significant differences , more targeted assignments   could be a powerful tool for diversifying the field .   5.3 Ideal workload   We asked our reviewer and AC respondents how   many assignments they received at their most re-   cent NLP venue , and what would be the optimal   number ( given a month to review , and a week for   AC assignments ) . For ACs , the mean optimal num-   ber of assignments is 8.5 ±4.2 vs. 9.1 ±5.1 they   received at the most recent venue , and for review-   ers it is 2.8 ±1.0 vs. 3.3 ±1.8 . Whether this is an4816issue depends on how much time a given venue   allows . The ARR reviewers have even less than a   month , and they indicated preference for fewer as-   signments than they received ( 2.4 ±1.0 vs 3.3 ±1.9 ) .   See App . fig . 10 for data on other venues .   The lack of reviewers is a well - known problem .   One of the possible causes is that many authors are   students not yet ready to be reviewers . To investi-   gate that , we asked the authors if they also reviewed   for the venues where they last submitted a paper ,   and the reviewers and ACs - if they also submitted .   If the core problem is that many authors are not   qualified , we would expect more non - student au-   thors to also be reviewers . Among all respondents   there are 24 % authors who submit to a venue but do   not review there or help in some other role ( fig . 1 ) ,   but if we consider only non - student respondents   that ratio is still 18 % ( see non - student role distri-   bution in App . fig . 9 ) . This suggests that many   qualified people do not review .   6 Discussion   6.1 Reviewer interests   Our results suggest the lack of interest is one of the   most common problems in paper - reviewer match-   ing , for both authors and reviewers . The authors   are aware of this problem and sometimes try to op-   timize for it by pursuing the “ safe ” , popular topics .   Unenthusiastic reviewers will likely produce shal-   low , heuristic - based reviews , essentially penalizing   non - mainstream research . Both tendencies con-   tribute to ossification of the field ( Chu and Evans ,   2021 ) , and generally need to be minimized .   It is in the AC ’s interest to find interested re-   viewers , since that minimizes late reviews , but they   need to know who finds what interesting . That is   not as simple as a match by topic / methodology ,   clear from the publication record . Interests change   not only gradually over time but also according   to what is popular or salient at the given moment   ( Tversky and Kahneman , 1974 ; Dai et al . , 2020 ) ,   or even in seemingly irrational ways ( e.g. by being   sensitive to the framing of the problem ) ( Tversky   and Kahneman , 1981 ) . But although experience   and knowledge may provide more stable descrip-   tions of a reviewer , looking into dated publication   records may be fundamentally counter - productive .   According to one of our respondents : “ I prefer the   conferences who offer bidding processes to select   the papers to review ... I am more enthusiastic to   review the papers compared to conferences thatassign papers based on what my interests were x   years ago . ”   Bidding however has its own set of problems ,   including the practical impossibility to elicit all   preferences over a big set of papers , the possibil-   ity of collusion rings ( Littman , 2021 ) , and , as one   of our respondents put it , “ biases towards / against   certain paper types when bidding is enabled ” . But   these problems potentially have solutions : there is   work on detecting collusion rings ( Boehmer et al . ,   2022 ) , and several respondents suggested that bid-   ding could be facilitated by subsampling with either   keyword- or affinity - score - based approaches .   We support some of our respondents ’ recom-   mendation for a combination of interest - based and   non - interest - based ( within a matching area ) assign-   ments , with the latter clearly marked as such for   ACs and reviewers , and separate playbooks for the   two cases . The reviewer training programs should   aim to develop the expectation that peer review is   something that combines utility and exploration .   6.2 Limitations   We readily acknowledge that , like with any surveys   with voluntary participation , our sample of respon-   dents may not be representative of the field overall ,   since the people who have had issues with peer   review system are more incentivized to respond .   However , precisely for that reason this method-   ology can be used to learn about the commonly   reported types of problems , which was our goal .   Our response rate turned out to be comparable to   the response rate of the official ACL survey solic-   iting feedback on its peer review reform proposal   ( Neubig , 2020 ) , which received 199 responses .   It is an open problem how future conferences   could systematically improve , if they can not rely   on surveys to at least reliably estimate at what scale   an issue occurs . Asking about satisfaction with re-   views does not seem to produce reliable results   ( Daumé III , 2015 ; Cardie et al . ) . Our survey in-   cluded a question about satisfaction with the paper-   reviewer matching , and whether the most recent   experience was better or worse than on average .   Both reviewers and authors were more satisfied   than dissatisfied , and considered the recent expe-   rience better than on average , despite reporting so   many issues ( see App . fig . 11 for the distribution).48176.3 Interpretable Paper - Reviewer Matching :   Problem Formulation   There already are many proposed solutions for   paper - reviewer matching ( see section 2 ) , but their   evaluation is the more difficult problem . The obvi-   ous approach would be to use bidding information   or real assignments made by ACs as ground truth ,   but this data is typically not shared to protect re-   viewer anonymity . It would also provide a very   noisy signal not just due to different assignment   strategies between ACs , but also different quality   of assignments depending on how much time they   have on a given day . Both ACs and bidding review-   ers are also likelyto favor top - listed candidates .   And , as our findings suggest , the optimal assign-   ment strategies in an interdisciplinary field might   genuinely vary between different types of papers   and tracks . A system unaware of that might sys-   tematically disadvantage whole research agendas .   Given that even the human experts can not tell   what the best possible assignments are , we pro-   pose to reformulate the problem as interpretable   paper - reviewer matching . That problem is notthe   same as the problem of faithfully explaining why   a given paper - reviewer matching system produced   a certain score , for which we have numerous in-   terpretability techniques ( Søgaard , 2021 ) . The AC   goal is fundamentally different : not to understand   the system , but to quickly find the information that   the ACconsiders relevant for making the best pos-   sible match . Therefore the task of interpretable   paper - reviewer matching is rather to help to iden-   tify the information that the stakeholders wish the   decisions to be based on , and to provide that infor-   mation as justification for the decisions .   7 Conclusion   We present the results of the first survey on paper-   reviewer assignment from the perspective of three   groups of stakeholders in the NLP community : au-   thors , reviewers , and ACs . The results point at a   host of issues , some immediately actionable ( e.g.   providing the ACs with better information ) , some   normative ( e.g. different kinds of research may   need different assignment strategies ) , and some   open ( e.g. how do we evaluate the effect of any   changes to peer review process ? ) A big issue forboth authors and reviewers is mismatches due to   lack of interest , which is in tension with explo-   rative aspects of peer review . We recommend to ad-   dress this issue with a combination of assignments   based on bidding and random matches within area ,   backed up by reviewer training .   Acknowledgments   Many thanks to Marzena Karpinska , Friedolin Mer-   hout , and the anonymous reviewers for their in-   sightful comments . We would also like to thank all   our survey respondents , without whom this study   would not have been possible .   Impact Statement   Broader impact . The study identifies types of   information that could be used to provide better   paper - reviewer matches . Used strategically by a   conference , it could be a powerful tool for diver-   sifying the field , by helping the non - mainstream   papers find the reviewers more open to them . By   the same token , if the entity organizing the review   process aimed for suppressing such research , de-   prioritising this information could harm such pa-   pers . Our proposal of interpretable paper - reviewer   assignments would mitigate this potential risk by   requiring the organizers to disclose their rationale   for any given match .   Personal data . The surveys are designed to not   solicit any personally identifiable information ( in-   cluding comments about individual peer review   cases in the past conferences ) , or demographic in-   formation about participants .   Potential risks . The respondents are participants   in anonymous peer review process , and as such   being tracked back to individual peer review cases   could expose them to retaliation . The survey   therefore did not solicit information about specific   venues ( only broader categories such as “ * ACL   conferences ” ) , and we manually verified that the   open - ended comments also do not contain refer-   ences to specific cases . We thus foresee no poten-   tial risks from deanonymization of the respondents .   Informed consent . The respondents are in-   formed about the organizers and the objective of   the study : to identity current practises of paper-   reviewer assignments in CL / NLP conferences and   ways in which this process can be improved . Re-   sponses are anonymous and respondents consent4818to the use and sharing of their responses for re-   search purposes . Respondents must give consent   to continue the survey .   Intended use . The survey data and forms will be   made publicly available for research purposes .   Institutional approval . The study was approved   by the Research Ethics Committee at the authors ’   institution .   References48194820A Appendix   In this appendix we introduce supplementary fig-   ures and tables.482148224823