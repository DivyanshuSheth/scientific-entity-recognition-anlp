  Bailin Wang   MIT   bailinw@mit.eduIvan Titov   University of Edinburgh   University of Amsterdam   ititov@inf.ed.ac.ukJacob Andreas   MIT   jda@mit.eduYoon Kim   MIT   yoonkim@mit.edu   Abstract   We describe a neural transducer that maintains   the flexibility of standard sequence - to - sequence   ( seq2seq ) models while incorporating hierarchi-   cal phrases as a source of inductive bias dur-   ing training and as explicit constraints during   inference . Our approach trains two models :   a discriminative parser based on a bracketing   transduction grammar whose derivation tree hi-   erarchically aligns source and target phrases ,   and a neural seq2seq model that learns to trans-   late the aligned phrases one - by - one . We use the   same seq2seq model to translate at all phrase   scales , which results in two inference modes :   one mode in which the parser is discarded and   only the seq2seq component is used at the   sequence - level , and another in which the parser   is combined with the seq2seq model . Decoding   in the latter mode is done with the cube - pruned   CKY algorithm , which is more involved but   can make use of new translation rules during   inference . We formalize our model as a source-   conditioned synchronous grammar and develop   an efficient variational inference algorithm for   training . When applied on top of both randomly   initialized and pretrained seq2seq models , we   find that both inference modes performs well   compared to baselines on small scale machine   translation benchmarks .   1 Introduction   Despite the improvements in performance and   data - efficiency enabled by recent advances in pre-   training , standard neural sequence - to - sequence   ( seq2seq ) models can still fail to model the hier-   archical structure of sentences and be brittle with   respect to to novel syntactic structures ( Lake and   Baroni , 2018 ; Kim and Linzen , 2020 ; Weißenhorn   et al . , 2022 ) . It has moreover not been clear how to   incorporate explicit constraints such as translation   rules ( e.g. , for translating idioms ) into these black-   box models without changing the underlying archi-   tecture . Classic grammar- and automaton - based   approaches are well - suited for capturing hierarchi-   cal structure and can readily incorporate new rules ,   but have trouble with examples that do not conform   exactly to the symbolic specifications . This paper   describes a neural transducer that maintains the   flexibility of neural seq2seq models but still mod-   els hierarchical phrase alignments between source   and target sentences , which have been shown to be   useful for transduction tasks ( Chiang , 2005 ; Wong   and Mooney , 2007 , inter alia ) . This transducer bot-   toms out in an ordinary neural seq2seq model , and   can thus take advantage of pre - training schemes   like BART ( Lewis et al . , 2020 ; Liu et al . , 2020 ) and   T5 ( Raffel et al . , 2020 ; Xue et al . , 2021 ) .   The transducer consists of two components   learned end - to - end : a discriminative parser based   on a bracketing transduction grammar ( BTG ; Wu ,   1997 ) whose derivation tree hierarchically aligns8211source and target phrases , and a neural seq2seq   model that learns to translate the aligned phrases   one - by - one . Importantly , a single seq2seq model is   trained to translate phrases at all scales , including   at the sentence - level , which results in two inference   modes . In the first mode , we discard the parser and   simply decode with the resulting seq2seq model at   the sequence level , which maintains fast inference   but still makes use of a seq2seq model that has   been exposed to ( and regularized by ) latent hier-   archical phrase alignments . This approach can be   seen a form of learned data augmentation ( Akyürek   et al . , 2021 ) , wherein a model is guided away from   wrong generalizations by additionally being trained   on crafted data that coarsely express the desired in-   ductive biases . In the second mode , we view the   combined parser and seq2seq model as a source-   conditioned neural synchronous grammar to derive   a set of synchronous grammar rules along with   their scores ( a “ neural phrase table ” ) , and decode   with a modified version of the classic cube - pruned   CKY algorithm ( Huang and Chiang , 2005 ) . While   more involved , this decoding scheme can incorpo-   rate explicit constraints and new translation rules   during inference .   We formulate our approach as a latent variable   model and use variational learning to efficiently   maximize a lower bound on the log marginal   likelihood . This results in an intuitive training   scheme wherein the seq2seq component is trained   on phrase tables derived from hierarchical align-   ments sampled from a variational posterior syn-   chronous parser , as shown in Figure 1 . When ap-   plied on top of both randomly initialized and pre-   trained seq2seq models across various small - scale   machine translation benchmarks , we find that both   modes improve upon baseline seq2seq models .   2 Approach   Notation . We use x , yto denote the source / target   strings , x , yto denote the source / target spans ,   andxto denote the token with index i.   2.1 A Seq2Seq - Based Synchronous Grammar   The inversion transduction grammar ( ITG ) , intro-   duced by Wu ( 1997 ) , defines a synchronous gram-   mar that hierarchically generates source and target   strings in tandem . The bracketing transduction   grammar ( BTG ) is a simplified version of ITG with   only one nonterminal , with the primary goal of   modeling alignments between source and target   words . We propose to use a variant of BTG which   defines a stochastic monolingual grammar over tar - get strings conditioned on source strings and can   use a neural seq2seq model to generate phrases .   As a motivating example , consider the following   English - Chinese example from Shao et al . ( 2018 ):   他们在桥上谈笑风生→   They were laughing it up on the bridge   A translator for this sentence is faced with several   problems : first , they must identify the appropri-   ate unit of translation ( number of phrases ) ; two ,   they must reorder phrases if necessary ; finally , they   must be able translate phrase - by - phrase , taking into   account phenomena such as idioms whose transla-   tions can not be obtained by stitching together trans-   lations of subparts ( e.g. , 谈笑风生→laughing   it up ) , while also making sure the final translation   is fluent as a whole . We model this process with a   source - conditioned synchronous grammar whose   target derivation probabilistically reifies the above   process . Informally , this grammar first samples the   number of segments to generate on the target ( n ) ,   then segments and reorders source phrases ( d ) ,   and finally translates phrase - by - phrase while still   conditioning on the full source sentence for fluent   translations ( d ) . This is shown in Figure 2 .   More formally , this monolingual grammar over   the target can be represented by a tuple   G[x ] = /parenleftig   Root,{T , S , I } , C , Σ , R[x]/parenrightig   ,   where Root is the distinguished start symbol ,   { T , S , I}are nonterminals where nde-   notes the number of segments associated with the   nonterminal ( to be explained below ) , |x|is the   source sentence length , Cis a special nontermi-   nal that emits phrases , and Σis the target alphabet .   The rule set R[x]is given by a fixed set of context-   free production rules . For the root node we have   ∀n∈ { 1,2 . . .|x| } ,   Root→T , T→S[x ] , T→I[x ] ,   where S , Irepresent the Straight and Inverted non-   terminals . Binary nonterminal rules are given by ,   ∀i , j , k∈ { 0,1 . . .|x| } s.t.i < j < k   S[x]→I[x]S[x ] ,   S[x]→I[x]I[x ] ,   I[x]→S[x]I[x ] ,   I[x]→S[x]S[x ] ,   where each nonterminal is indexed by ( i.e. , aligned8212   to ) a source span x , and the superscript nrefers   to the number of segments to generate . We force   l+r = nto ensure that the number of segments   generated by the left and right nonterminal is equal   ton . Finally , if SorIis generated ( i.e. , there is   only one segment to be generated ) , we move to the   leaf nonterminal Cand generate target phrases ,   S[x]→C[x ] , I[x]→C[x ] ,   C[x]→v , v ∈Σ.   We use a probabilistic version of the above gram-   mar where each rule is weighted via π : R →R.   Importantly , for the leaf nonterminal we use a ( po-   tentially pretrained ) neural seq2seq model , i.e. ,   π(C[x]→v ) = p ( v|x ) .   Foreshadowing , if we force Root→Tin the be-   ginning of derivation then this corresponds to the   usual seq2seq approach which generates the full tar-   get sequence conditioned on the source sequence .   2.2 Parameterization   By assigning scores to rules conditioned on xwith   function π , we can obtain a distribution p(d|x )   over all possible derivations dlicensed by G[x ] .   Note that dis fully characterized by : ( 1 ) n , the   total number of target segments , ( 2 ) d , the set   of derivation rules excluding the leaf rules ( i.e. ,   rules with C[x]on the left hand side ) , and ( 3 )   d , the set of leaf derivation rules . We hence   decompose the derivation into three distributions ,   p(d|x ) = p(n|x)p(d|x , n)p(d|x , d ) . Intu-   itively , dencodes the segmentations and reorder-   ings of x , while dencodes the phrase transla-   tions which implicitly segment y.p(n|x).For the distribution over the number of   target segments we use a geometric distribution ,   p(n|x ) = λ(1−λ ) ,   where 0 < λ < 1,1≤n < |x| . This sets the   probability of the upper bound n=|x|to be ( 1−   λ ) . This distribution is implemented by the   following configuration of the grammar :   π(Root→T ) = p(n|x ) .   The geometric distribution favors fewer target seg-   ments , which aligns with one of our goals which   is to be able to use the resulting model as a regular   seq2seq system ( i.e. , forcing Root→T ) .   p(d |x , n).For the distribution over source   segmentations and reorderings , we use a discrimi-   native CRF parser whose scores are given by neural   features over spans . The score for rule R∈ R , e.g. ,   S[x]→I[x]A[x ] , is given by ,   π(R ) = exp / parenleftig   ef(˜h,˜h)/parenrightig   ,   where A∈ { S , I},˜hand˜hare the span fea-   tures derived from the contextualized word repre-   sentations of tokens at i , j , k from a Transformer   encoder as in ( Kitaev and Klein , 2018 ) , fis an   MLP that takes into two span features , and eis   a one - hot vector that extracts the corresponding   scalar score . ( Hence we use the same score all n ) .   These scores are globally normalized , i.e. ,   p(d|x , n)∝/productdisplayπ(R ) ,   to obtain a distribution over d , where Rare the   rules in d. Algorithms 1 , 2 in appendix C give   the dynamic programs for normalization / sampling .   p(d|x , d).Finally , for the distribution   over the leaf translations , the phrase transduc-8213tion rule probabilities p(C[x]→v)are pa-   rameterized with a ( potentially pretrained ) neural   seq2seq model . For the seq2seq component , in-   stead of just conditioning on xto parameterize   p ( v|x ) , in practice we use the contextu-   alized representation of xfrom a Transformer   encoder that conditions on the full source sentence .   Hence , our approach can be seen learning contex-   tualized phrase - to - phrase translation rules . Specifi-   cally , for a length- mtarget phrase we have ,   p(d|x , d ) = /productdisplayπ(R )   π(C[x]→v ) = /productdisplayp ( v|v , h ) ,   h= Encoder ( x ) ,   where hrefers to the contextualized representa-   tion of xthat is obtained after passing it through   a Transformer encoder . The decoder attends over   handvto produce a distribution over v. The   two “ exit ” rules are assigned a score π(A[x]→   C[x ] ) = 1 where A∈ { S , I } .   Remark . This grammar is a mixture of both lo-   cally and globally normalized models . In particular ,   p(n|x)andp(d|x , d)are locally normalized   as the scores for these rules are already probabili-   ties . In contrast , p(d|x , n)is obtained by glob-   ally normalizing the score of dwith respect to   a normalization constant Z(x , n).In the ap-   pendix we show how to convert the unnormalized   scores πin the globally normalized model into nor-   malized probabilities ˜π(Algorithm 1 ) , which will   be used for CKY decoding in Section 2.4 .   We also share the parameters between compo-   nents whenever possible . For example , the encoder   of the seq2seq component is the same encoder that   is used to derive span features for d. The de-   coder of the seq2seq component is used as part of   the variational distribution when deriving span fea-   tures for the variational parser . As we will primar-   ily be working with small datasets , such parameter   sharing will provide implicit regularization to each   of the components of our grammar and militate   against overfitting . We provide the full parameteri-   zation of each component in appendix A.2.3 Learning   Our model defines a distribution over target trees   d= ( n , d , d)(and by marginalization , target   strings y ) conditioned on a source string x ,   p(y|x ) = /summationdisplayp(d|x )   = /summationdisplayp(n|x)p(d|x , n)p(d|x , d ) ,   where D(x , y)is the set of trees such that   yield ( D(x , y ) ) = ( x , y ) . While this marginaliza-   tion can theoretically be done in O(L)time where   L= min ( |x|,|y|),we found it impractical in prac-   tice . We instead to use variational approximations   to the posteriors of n , d , dand optimize a   lower bound on the log marginal likelihood whose   gradient estimator can be obtained in O(L)time .   Each of the three variational distributions qis anal-   ogous to the three distributions pintroduced previ-   ously , but additionally conditions on y.   q(n|x , y).Similar to p(n|x ) , we use a geomet-   ric distribution to model q(n|x , y ) . The only dif-   ference is that we have 1≤n≤min(|x|,|y| ) .   q(d |x , y , n ) .This is parameterized with neu-   ral span scores over xin the same way as in   p(d|x , n ) , except that the span features also con-   dition on the target y. That is , ˜h,˜hare now   the difference features of the decoder ’s contextu-   alized representations over xfrom a Transformer   encoder - decoder , where yis given to the encoder .   The dynamic programs for normalization / sampling   in this CRF is the same as in p(d|x , n ) .   q(d|x , y , d).Observe that conditioned on   ( x , y , d ) , the only source of uncertainty in   disC[x]→yfor spans y. That is ,   q(d|x , y , d)is effectively a hierarchical seg-   mentation model with a fixed topology given by   d. The segmentation model can be described by   a probabilistic parser over the following grammar ,   G[y , d ] = ( Root,{T},R[y , d ] ) ,   where rule set R[y , d]includes ( here m = l+r ) ,   Root→T[y ] , T[y]→T[y]T[y ]   where the second rule exists only when the rule like   A→BCis ind . We use span features over8214yfor parameterization similarly to p(d|x , n ) . Al-   gorithms 3 , 4 in appendix C give the normaliza-   tion / sampling dynamic programs for this CRF .   Optimization . With the variational distributions   in hand , we are now ready to give the following   evidence lower bound ( ignoring constant terms ) ,   logp(y|x)≥E / braceleftig   E / bracketleftig   E[logp(d|x , d ) ]   + H[q(d|x , y , d)]/bracketrightig   −KL[q(d|x , y , n ) ∥p(d|x , n)]/bracerightig   .   ( The KLbetween q(n|x , y)andp(n|x)is a con-   stant and hence omitted . ) See appendix B for the   derivation . While the above objective seems in-   volved , in practice it results in an intuitive training   scheme . Let θbe the grammar parameters and ϕ   be the variational parameters . Training proceeds   as follows :   1 . Sample number of target segments ,   n∼q(n|x , y ) .   2 . Sample trees from variational parsers ,   d∼q(d|x , y , n ) ,   d∼q(d|x , y , d ) ,   and obtain the set of hierarchical phrase   alignments ( i.e. , phrase table ) Aimplied by   ( d , d ) .   3.Train seq2seq model by backpropagating   the leaf likelihood logp(d|x , d)to the   seq2seq model,/summationdisplay∇logp ( y|x ) .   4.Train variational parsers by backpropagating   the score function and KL objectives with re-   spect to q(d|x , y , n ) , /parenleftbig   logp(d|x , d ) + H[q(d|x , y , d)]/parenrightbig   × ∇logq(d|x , y , n ) − ∇KL [ · ] ,   and also with respect to q(d|x , y , n ) ,   logp(d|x , d)× ∇logq(d|x , y , d )   + ∇H[q(d|x , y , d ) ] .   5.Train source parser by backpropagating   −∇KL[·]top(d|x , n).For the score function gradient estimators we also   employ a self - critical control - variate ( Rennie et al . ,   2017 ) with argmax trees . The KL and entropy met-   rics are calculated exactly with the inside algo-   rithm with the appropriate semirings ( Li and Eisner ,   2009 ) . This training scheme is shown in Figure 1 .   See Algorithms 5 , 6 in appendix C for the entropy-   /KL dynamic programs .   Complexity . The above training scheme reduces   time complexity from O(L)toO(L)since we   do not marginalize over all possible number of   segments and instead sample n , and further de-   compose the derivation tree into ( d , d)and   sample from CRF parsers , which takes O(L ) . The   KL / entropy calculations are also O(L ) .   2.4 Inference : Two Decoding Modes   During inference , we can use the seq2seq to ei-   ther directly decode at the sentence - level ( i.e. , set-   tingn= 1 and choosing Root→T ) , or gen-   erate phrase - level translations and compose them   together . The first decoding strategy maintains the   efficiency of seq2seq , and variational learning can   be seen as a structured training algorithm that regu-   larizes the overly flexible seq2seq model with latent   phrase alignments . The second decoding strategy   takes into account translations at all scales and can   further incorporate constraints , such as new trans-   lation rules during decoding .   Seq2Seq decoding . Setting n= 1 , decoding in   G[x]this case is reduced to the standard sentence-   level seq2seq decoding for the terminal rules   C[x]→vwith beam search . We call this vari-   ant of the model BTG- 1Seq2Seq .   CKY decoding . Here we aim to find   y with maximal marginal probability   argmax / summationtextp(d|x ) . This requires   exploring all possible derivations licensed by G[x ] .   We employ CKY decoding algorithm based on the   following recursion :   C[vv , S ] = /summationdisplay / braceleftbig   C[v , I]·C[v , S]·˜π()+   C[v , I]·C[v , I]·˜π()/bracerightbig   C[vv , I ] = /summationdisplay / braceleftbig   C[v , S]·C[v , I]·˜π()+   C[v , S]·C[v , S]·˜π()/bracerightbig8215where C[v , A]stores the score of generating tar-   get phrase vbased on source phrase xwith non-   terminal A. In the above we abbreviate ˜π ( )   to mean ˜π()since i , j , k are   implied by the Cterms that this quantity is multi-   plied with . Here v , v , vdenotes target phrases ,   andvvrefers to the concatenation of two phrases .   The chart is initialized by phrase pairs that are gen-   erated based on seq2seq ,   C[v , S ] = p ( v|x)·π ( ) ,   C[v , I ] = p ( v|x)·π ( )   The final prediction is based on the chart item for   the root node computed via :   C[v , T ] = C[v , S]·˜π()+   C[v , I]·˜π ( )   C[v , Root ] = /summationdisplayC[v , T]·π ( ) .   Cube pruning . Exact marginalization is still in-   tractable since there are theoretically infinitely   many phrases that can be generated by a seq2seq   model ( e.g. , there is C[v , S]for each possible   phrase v∈Σ ) . We follow the cube pruning   strategy from Huang and Chiang ( 2005 ) and only   store the top- Ktarget phrases within each chart   item , and discard the rest . During the bottom-   up construction of the chart , for A∈ { S , I } ,   C[v , A]only stores the Kphrases generated   by beam search with size K , andC[v , A]con-   structs at most 2·(n−1)·K·Ktarget phrase   translations and we only keep the top- Kphrases .   We call this decoding scheme BTG- NSeqSeq .   We generally found it to perform better than BTG- 1   Seq2Seq but incur additional cost in decoding .   External Translation Rules . An important fea-   ture of CKY decoding is that we can constrain   the prediction of BTG - Seq2Seq to incorporate new   translation rules during inference . When we have   access to new translation rules ⟨x→v⟩during   inference ( e.g. , for idioms or proper nouns ) , we   can enforce this by adding a rule C[x]→vand   setting the score π(C[x]→v ) = 1 ( or some   high value ) during CKY decoding.3 Experimental Setup   We apply BTG - Seq2Seq to a spectrum sequence - to-   sequence learning tasks , starting from diagnostic   toy datasets and then gradually moving towards   real machine translation tasks . Additional details   ( dataset statistics , model sizes , etc . ) are given in   appendix E.   Baselines . We use the standard Transformer-   based seq2seq models ( Vaswani et al . , 2017 ) as   the backbone module for all experiments . We con-   sider three baselines : ( 1 ) a seq2seq model trained   in a standard way , ( 2 ) a seq2seq model pretrained   with phrase pairs extracted with an off - the - shelf   tool , Fast - Align ( Dyer et al . , 2013),and ( 3 ) a   trivial baseline in which the model is pretrained   on phrase pairs generated by randomly splitting a   source - target sentence pair into phrase pairs . The   latter two approaches study the importance of using   learned alignments .   BTG- 1Seq2Seq and BTG- NSeq2Seq . We fo-   cus on evaluating the two decoding modes of   our model : CKY decoding where phrase - level   translations are structurally combined to form a   sentence - level prediction ( BTG- NSeq2Seq ) , and   standard “ flat ” Seq2Seq decoding where sentence-   level translations are generated directly by forcing   Root→T(BTG- 1Seq2Seq ) . In this way we test   the effectiveness of hierarchical alignments both as   a structured translation model and as a regularizer   on standard Seq2Seq learning . BTG- NSeq2Seq   is sometimes referred simply as BTG - Seq2Seq in   cases where CKY decoding is activated by default   ( e.g. , constrained inference ) .   Backbone Transformers . In each experiment ,   we use the same set of hyperparameters across   models for fair comparison . One crucial hyper-   parameter for the backbone Transformers of BTG-   Seq2Seq is to use relative positional encodings ( as   opposed to absolute ones ) which are better at han-   dling translation at phrase levels . Hence we use rela-   tive positional encodings whenever possible except   when utilizing pretrained Transformers that have   been pretrained with absolute positional embed-   dings .   4 Main Results   4.1 Toy SVO - SOV Translation   We start with a diagnostic translation task in which   a model has to transform a synthetic sentence from8216   SVO to SOV structure . The dataset is generated   with the following synchronous grammar ,   Root→ ⟨S V O , S O V⟩ ,   S→NP , O→NP , V→VP ,   NP→ ⟨np , np⟩,VP→ ⟨vp , vp⟩ ,   where the noun phrase pairs ⟨np , np⟩and the   verb phrase pairs ⟨vp , vp⟩are uniformly sampled   from two nonoverlapping sets of synthetic phrase   pairs with varying phrase lengths between 1 to 8 .   We create two generalization train - test splits .   Novel - Position . For this split , a subset of   np , npphrase pairs are only observed in one syn-   tactic position during training ( i.e. , only as a subject   or only as an object ) . At test time , they appear in a   new syntactic role . Each remaining np , nppairs   appears in both positions in training .   Few - Shot . In this setting , each subject ( or ob-   ject ) phrase is observed in three examples in three   different examples . During the evaluation , each   subject phrase appears with context words ( i.e. a   predicate and an object phrase ) which are found in   the training set but not with this subject phrase .   Results . In Table 1 , we observe that BTG- N   Seq2Seq perfectly solves these two tasks . We also   observed that BTG- NSeq2Seq correctly recov-   ers the latent trees when N= 3 . Interestingly ,   even without structured CKY decoding , BTG- 1   Seq2Seq implicitly captures the underlying trans-   duction rules , achieving near - perfect accuracy . In   contrast , standard seq2seq models fail in both set-   tings .   4.2 English - Chinese : Phrasal Few - shot MT   Next , we test our models on the English - Chinese   translation dataset from Li et al . ( 2021 ) , which was   designed to test for compositional generalization   in MT . We use a subset of the original dataset and   create three few - shot splits similar to the few - shotsetting in the previous task . For example in the   NP split noun phrases are paired with new contexts   during evaluation . Compared with the few - shot   setting for the SVO - SOV task , this setup is more   challenging due to a larger vocabulary and fewer   training examples ( approx . 800 data points ) .   Results . As shown in Table 1 , both BTG- 1   Seq2Seq and BTG- NSeq2Seq improve on the   standard Seq2Seq by a significant margin . BTG- N   Seq2Seq also outperforms the Fast - Align baseline   on VP and PP splits and slightly lags behind on   NP , highlighting the benefits of jointly learning   alignment along with the Seq2Seq model .   Analysis . In Figure 3 we visualize the in-   duced phrasal alignments for an example trans-   lation by extracting the most probable derivation   argmaxp(d|x)forn= 3 , which encodes the   phrase - level correspondence between xandy . In   this example “ at a restaurant ” is segmented and   reordered before “ busy night ” according to the Chi-   nese word order . We also give example translations   in Table 2 , which shows two failure modes of the   standard seq2seq approach . The first failure mode   is producing hallucinations . In the first example ,   the source phrase “ hang out with the empty car he   liked ” is translated to a Chinese phrase with a com-   pletely different meaning . This Chinese translation   exactly matches a Chinese training example , likely   triggered by the shared word 空车(“empty car ” ) .   The second failure mode is under - translation . In the   second example , standard seqsSeq completely ig-   nores the last phrase “ it was so dark ” in its Chinese   translation , likely because this phrase never co-   occurs with the current context . In contrast BTG-   Seq2Seq is able to cover all source phrases , and8217   in this case , predicts the corresponding Chinese   phrase for “ it was so dark ” .   4.3 German - English : Low - Data MT   Next , we move onto more a realistic dataset   and consider a low data German - English transla-   tion task , following Sennrich and Zhang ( 2019 ) .   The original dataset contains TED talks from the   IWSLT 2014 DE - EN translation task ( Cettolo et al . ,   2014 ) . Instead of training the seq2seq component   from scratch , we use pre - trained mBART ( Liu et al . ,   2020 ) to initialize the seq2seq component of our   grammar ( and the baseline seq2seq ) . This was   found to perform much better than random ini-   tialization , and highlights an important feature of   our approach which can work with arbitrary neural   seq2seq models . ( We explore this capability fur-   ther in section 4.5 . ) The use of pre - trained mBART   lets us consider extremely low - data settings , with   500 and 1000 sentence pairs , in addition to the   4978 setup from the original paper . We observe   in Table 1 that even the standard seq2seq achieves   surprisingly reasonable BLEU scores in this low-   data setting — much higher that scores of seq2seq   models trained from scratch ( Sennrich and Zhang ,   2019).BTG- NSeq2Seq and BTG- 1Seq2Seq   again outperform the baseline seq2seq model .   Pretraining on Fast - Align harms performance ,   potentially due to it ’s being unable to induce mean-   ingful phrase pairs in such low data regimes . In   contrast , all our aligners ( i.e. , parsers ) can also   leverage pretrained features from mBART ( we use   the same mBART model to give span scores for all   our parsers ) , which can potentially improve struc-   ture induction and have a regularizing effect .   Injecting New Translation Rules . We investi-   gate whether we can control the prediction of BTG-   Seq2Seq by incorporating new translation rules   during CKY inference . In particular , we give new   translation rules to mBART - based BTG - Seq2Seq   for DE - EN translation . Since the model has been   trained only on 4978 pairs , the translations are far   from perfect . But as shown in Table 3 we observe   that BTG - Seq2Seq can translate proper nouns , time   expression , and even idioms to an extent . The id-   iom case is particularly interesting as pure seq2seq   systems are known to be toocompositional when   translating idioms ( Dankers et al . , 2022 ) .   4.4 Cherokee - English : Low - Resource MT   Our next experiments consider a true low - resource   translation task with Cherokee - English , using the   dataset from Zhang et al . ( 2020 ) . This dataset con-   tains around 12k/1k/1k examples for train / dev / test ,   respectively . Unlike German - English , Cherokee   was not part of the mBART pretraining set , and   thus we train all our models from scratch . In Ta-   ble 1 we see that BTG- NSeq2Seq and BTG- 1   Seq2Seq again outperform the standard seq2seq .   4.5 Low Resource MT with Pretrained   Models   The current trend in low resource MT is to first pre-   train a single multilingual model on a large number8218   of language pairs ( Tang et al . , 2020 ; Fan et al . ,   2021 ; Costa - jussà et al . , 2022 ) and then optionally   finetune it on specific language pairs . In our final   experiment , we explore whether we can improve   upon models that have already pretrained on bitext   data . We focus mBART50 - large which has been   pretrained on massive monolingual andbilingual   corpora . We randomly pick four low - resource lan-   guages from the ML50 benchmark ( Tang et al . ,   2020 ) , each of which has fewer than 10 Kbitext   pairs . We also experiment with mT5 - base ( Xue   et al . , 2021 ) to see if we can plug - and - play other   pretrained seq2seq models .   From Table 4 , we see that our BTG models out-   perform the standard finetuning baseline in four   languages for mT5 , and achieve slight improve-   ment in two languages for mBART50 . We conjec-   ture that this is potentially due to ( 1 ) mBART50 ’s   being already trained on a large amount of bitext   data and ( 2 ) mT5 ’s use of relative positional en-   codings , which is more natural for translating at all   phrase scales . Finally , although we focus on pre-   trained seq2seq models in this work , an interesting   extension would be to consider prompt -based hier-   archical phrase - based MT where p ( v|x)is   replaced with a large language model that has been   appropriately prompted to translate phrases .   5 Related Work   Synchronous grammars Classic synchronous   grammars and transducers have been widely ex-   plored in NLP for many applications ( Shieber and   Schabes , 1990 ; Wu , 1997 ; Eisner , 2003 ; Ding and   Palmer , 2005 ; Nesson et al . , 2006 ; Huang et al . ,   2006 ; Wong and Mooney , 2007 ; Wang et al . , 2007 ;   Graehl et al . , 2008 ; Blunsom et al . , 2008 , 2009 ;   Cohn and Lapata , 2009 , inter alia ) . In this work ,   we focus on the formalism of bracketing transduc-   tion grammars , which have been used for model-   ing reorderings in SMT systems ( Nakagawa , 2015;Neubig et al . , 2012 ) . Recent work has explored   the coupling bracketing transduction grammars   with with neural parameterization as a way to in-   ject structural biases into neural sequence models   ( Wang et al . , 2021 ) . Our work is closely related to   the neural QCFG ( Kim , 2021 ) , a neural parameter-   ization of a quasi - synchronous grammars ( Smith   and Eisner , 2006 ) , which also defines a monolin-   gual grammar conditioned on the source . However   they do not experiment with embedding a neural   seq2seq model within their grammar . More gener-   ally our approach extends the recent line of work   on neural parameterizations of classic grammars   ( Jiang et al . , 2016 ; Han et al . , 2017 , 2019 ; Kim   et al . , 2019 ; Jin et al . , 2019 ; Zhu et al . , 2020 ; Yang   et al . , 2021b , a ; Zhao and Titov , 2020 , inter alia ) ,   although unlike in these works we focus on the   transduction setting .   Data Augmentation Our work is also related to   the line of work on utilizing grammatical or align-   ment structures to guide flexible neural seq2seq   models via data augmentation ( Jia and Liang , 2016 ;   Fadaee et al . , 2017 ; Andreas , 2020 ; Akyürek et al . ,   2021 ; Shi et al . , 2021 ; Yang et al . , 2022 ; Qiu et al . ,   2022 ) or auxiliary supervision ( Cohn et al . , 2016 ;   Mi et al . , 2016 ; Liu et al . , 2016 ; Yin et al . , 2021 ) .   In contrast to these works our data augmentation   module has stronger inductive biases for hierarchi-   cal structure due to explicit use of latent tree - based   alignments .   6 Conclusion   We presented a neural transducer that maintains   the flexibility and seq2seq models but still incor-   porates hierarchical phrases both as a source of   inductive bias during training and as explicit con-   straints during inference . We formalized our model   as a synchronous grammar and developed an effi-   cient variational inference algorithm for training .   Our model performs well compared to baselines   on small MT benchmarks both as a regularized   seq2seq model and as a synchronous grammar.82197 Limitations   Our work has several limitations . While varia-   tional inference enables more efficient training than   full marginalization ( O(L)vs . O(L ) ) , it is still   much more expensive compute- and memory - wise   than regular seq2seq training . This currently pre-   cludes our approach as a viable alternative on large-   scale machine translation benchmarks , though ad-   vances in GPU - optimized dynamic programming   algorithms could improve efficiency ( Rush , 2020 ) .   ( However , as we show in our experiments it is pos-   sible to finetune a pretrained seq2seq MT system   using our approach . ) Cube - pruned CKY decoding   is also much more expensive than regular seq2seq   decoding . In general , grammar- and automaton-   based models enable a greater degree of model   introspection and interpretability . However our use   of black - box seq2seq systems within the grammar   still makes our approach not as interpretable as   classic transducers .   Acknowledgements   This study was supported by MIT - IBM Watson AI   Lab .   References822082218222A Parameterization of BTG - Seq2Seq   The basic strategy for our parameterization is to   take as much advantage of the backbone Trans-   former seq2seq models as possible , which is the   main source of rich neural representations ( espe-   cially when working with pretrained models ) .   Seq2Seq p(d|x , d).The leaf nodes of   dare the production rules of C[x]→v , which   encode the correspondence between source seg-   ment xand target segment v. The probabil-   ity of such a rule is modeled by a conventional   Transformer - based seq2seq model ( Vaswani et al . ,   2017 ) . In BTG - Seq2Seq , the underlying seq2seq is   expected to handle segments at all levels , including   sentence- and phrase - level translations . We found   it beneficial to use two sets of special tokens to   indicate the beginning / end of sentences , one for   sentence - level translations and the other for phrase-   level translations . Concretely , in addition to the   typical ‘ BOS ’ and ‘ EOS ’ which are used to mark   the beginning and the end of a sentence , we addi-   tionally use ‘ Seg - BOS ’ and ‘ Seg - EOS ’ to mark the   beginning and the end of a segment .   Source parser p(d |x , n).To obtain a prob-   abilistic parser , we need to assign a score to each   rule involved in d. For the following rules ,   ∀n∈ { 1,2 . . .|x| } ,   T→S[x ] , T→I[x ] ,   we assign them a weight of 1 for simplicity , as   we find that using trainable weights does not help   during our initial experiments .   We use neural features to parameterize the score   of the following rules :   ∀i , j , k∈ { 0,1 . . .|x| } s.t.i < j < k   S[x]→I[x]S[x ]   S[x]→I[x]I[x ]   I[x]→S[x]I[x ]   I[x]→S[x]S[x ] .   For example the score   π ( ) = exp / parenleftig   ef(˜h,˜h)/parenrightig   relies on the difference feature ˜hand˜h ,   following Kitaev and Klein ( 2018 ) . In our pa-   rameterization , rules that have the same right-   hand spans will have the same score regard-   less of their nonterminals . That is , the score   ofI[x]→S[x]I[x]is identical to8223I[x]→S[x]S[x ] . These difference fea-   tures are calculated using   ˜h = Concat [ − →h−− →h;← −h−← −h ]   where− →hand← −hare the “ forward ” and “ back-   ward ” representations from Kitaev and Klein   ( 2018 ) .   The score of a derivation tree is the sum of the   scores of the production rules that form the tree ,   and we normalize the score to obtain the distribu-   tion over all possible derivations ,   p(d|x , n ) =    /productdisplayπ(R)   /Z(x , n ) ,   where Z(x , n ) = /summationtext / parenleftbig / producttextπ(R)/parenrightbig   is the   partition function . The dynamic program for com-   puting the partition function is given by Algo-   rithm 1 .   Variational parser q(d , d|x , y , n ) .We   factorize the variational synchronous parser as   q(d , d|x , y , n ) = q(d|x , y , n ) ×   q(d|x , y , d ) ,   which makes it possible to sample hierarchical   alignments in O(L)where L= min ( |x|,|y| ) .   The parameterization of q(d|x , y , n ) is very sim-   ilar to the parameterization of p(d|x , n ) , except   thath(which are used to compute the difference   features ) are the hidden states of a Transformer   decoder , with the backbone Transformer seq2seq   running from ytox . This choice makes it possible   to condition the contextualized representations of   xon the target y. Calculating the partition func-   tion uses the same dynamic program as in the prior   parser ( i.e. , Algorithms 1 ) . Algorithm 2 shows the   dynamic program for sampling d.   Given x , y andd , the only rules that re-   main unknown for the variational parser are the   target segments yinC[x]→y . Thus   q(d|x , y , d)is effectively a hierarchical seg-   mentation model . We use a grammar with the fol-   lowing rules ,   Root→T[y ] , T[y]→T[y]T[y ] ,   to model the segmentations . Since the segmen - tation is conditioned on d , we need to ensure   that the tree structures of danddis identical .   ( Note that by conditioning on d , we only utilize   the tree topology of dfor efficiency . )   We use the following function to score the rules :   π ( ) = exp / parenleftig   f(˜h,˜h)/parenrightig   ,   where ˜h,˜hare the difference features based   on the contextualized representations of y , and f   is an MLP that emits a scalar score . We can then   obtain a distribution over dvia ,   q(d|x , n , d)=   /productdisplayπ(R)   /Z(d , y )   where Z(d , y ) = /summationtext / parenleftig / producttextπ(R)/parenrightig   is   the partition function . Algorithm 3 shows the dy-   namic program for calculating the partition func-   tion above , and Algorithm 4 gives the dynamic pro-   gram for sampling from q(d|x , n , d ) . These   dynamic programs modify the standard inside algo-   rithm to only sum over chart entries that are valid   under d.   Since we share the encoder and decoder across   all components ( i.e. , prior / variational parsers and   the seq2seq model ) , the only additional parameters   for BTG - Seq2Seq are the parameters of the MLP   layers for the three CRF parsers .   Number of segments p(n|x),q(n|x , y).We   use a truncated geometric distribution of the fol-   lowing form ,   p(n ) = /braceleftigg   λ(1−λ ) , n∈ { 1,2 , . . . N −1 }   ( 1−λ ) , n = N   where Nis the maximum number of segments .   For the prior p(n|x ) , we have N=|x| . For   the variational posterior q(n|x , y ) , we have N=   min(|x|,|y| ) . In practice , we set an additional up-   per bound ( chosen from [ 3,4 . . .8 ] ) for efficient   training . λis chosen from { 0.5,0.6,0.7,0.8,0.9 } .   B Evidence Lower Bound   Figure 4 shows the lower bound derivation .   C Dynamic Programs for the Parsers   Sampling Algorithms   The algorithms for sampling from p(d|x , n)and   q(d|x , d)are provided in Algorithm 2 and 4 .   Sampling from q(d|x , y , n ) is omitted , as it is   the same algorithm as Algorithm 2.8224   logp(y|x ) = log / summationdisplayp(d|x ) = log / summationdisplayp(n|x)p(d|x , n)p(d|x , d )   ≥E / braceleftig   log / summationdisplayp(d|x , n)p(d|x , d)/bracerightig   −KL[q(n|x , y)∥p(n|x ) ]   ≥E / braceleftig   log / summationdisplayp(d|x , d)−KL[q(d|x , y , n ) ∥p(d|x , n)]/bracerightig   ≥E[logp(d|x , d ) ] + H[q(d|x , y , d ) ]   logp(y|x)≥E / braceleftig   E / bracketleftig   E[logp(d|x , d ) ] + H[q(d|x , y , d)]/bracketrightig   −KL[q(d|x , y , n ) ∥p(d|x , n)]/bracerightig   −KL[q(n|x , y)∥p(n|x ) ]   Computing the Entropy and KL   We provide the dynamic program to com-   puteH[q(d|x , y , d)]in Algorithm 5 , and   KL[q(d|x , y , n ) ||p(d|x , n)]in Algorithm 6 .   Note that KL[q(n|x , y)∥p(n|x)]is a constant , and   we can ignore it during optimization . In practice   the algorithms are implemented in log space for   numerical stability .   Gradient Estimation   We use policy gradients ( Williams , 1992 ) to opti-   mize the lower bound . Two techniques are addi-   tionally employed to reduce variance . First , we   use a self - critical control variable ( Rennie et al . ,   2017 ) where the scores from argmax trees are used   as a control variates . Second , we utilize a sum - and-   sample strategy ( Liu et al . , 2019 ) where we alwaystrain on n= 1 , i.e. , sentence - level translations , and   sample another n > 1to explore segment - level   translations .   D Recipe for Using BTG - Seq2Seq   Although it is expensive to train our BTG - Seq2Seq   from scratch due to O(L)time complexity , we   find that a pretrain - then - finetune strategy is a prac-   tical way to use BTG - Seq2Seq . During the pre-   training stage , we train a Transformer seq2seq in   the standard way until convergence , or simply use   public model checkpoints . Then during the fine-   tuning stage , we use the pretrained model as the   backbone seq2seq for our BTG - SeqSeq . Usually   we use a relatively smaller learning rate to finetune   the backbone seq2seq during finetuning .   E Dataset Statistics and   Hyperparameters   We show the statistics of the datasets and the hy-   perparameters of the model in Tables 5 and 6.8225Algorithm 1 Inside algorithm for p(d|x , n ) . Rules are highlighted in blue   Input : x : source sentence , n : number of segments   function I -T(x , n )   Initialize β[S ] = 0 , β[I ] = 0   form= 2tondo ▷number of segments   forw= 1to|x|do ▷width of spans   fori= 0to|x| −wdo ▷start point   k = i+w ▷ end point   forj = i+ 1tok−1do   forl= 1tom−1do ▷number of segments in the left part   r = m−l ▷ number of segments in the right part   β[S ] + = π()·β[I]·β[S ]   β[S ] + = π()·β[I]·β[I ]   β[I ] + = π()·β[S]·β[I ]   β[I ] + = π()·β[S]·β[S ]   ▷Compute locally - normalized scores ˜π   form= 2tondo ▷number of segments   forw= 1to|x|do ▷width of spans   fori= 0to|x| −wdo ▷start point   k = i+w ▷ end point   forj = i+ 1tok−1do   forl= 1tom−1do ▷number of segments in the left part   r = m−l ▷ number of segments in the right part   ▷DenoteasR   ˜π(R ) = /parenleftbig   π(R)·β[I]·β[S]/parenrightbig   /β[S ]   ▷DenoteasR   ˜π(R ) = /parenleftbig   π(R)·β[I]·β[I]/parenrightbig   /β[S ]   ▷DenoteasR   ˜π(R ) = /parenleftbig   π(R)·β[S]·β[I]/parenrightbig   /β[I ]   ▷DenoteasR   ˜π(R ) = /parenleftbig   π(R)·β[S]·β[S]/parenrightbig   /β[I ]   ▷ Trules are assigned a trivial score : π ( ) = 1 , π ( ) = 1   β[T ] = β[S ] + β[I],Z(x , n ) = β[T ] ▷partition function   ˜π ( ) = β[S]/β[T],˜π ( ) = β[I]/β[T ]   return Z(x , n ) , β,˜π8226Algorithm 2 Top - down sampling dfrom p(d|x , n ) . Rules are highlighted in blue   Input : ˜π : normalized scores obtained from I -T(·),n : number of segments   function S -T(˜π , n )   Initialize an empty tree d   Sample A∈ { S , I}wrt.˜π ( )   R -S -T(0,|x|,n , A,˜π , d )   return d   ▷Arguments : i : start point , k : end point , n : number of segments , A : nonterminal )   function R -S -T(i , k , n , A,˜π , d )   ifn= 1then   Add ruletod   return   ifA = Sthen   ▷1≤l < n , A , B ∈ { S , I } , i < j < k are the random variables , r = n−l   Sample ruleaccording to ˜π ( · )   Addtod   R -S -T(i , j , l , A,˜π , d )   R -S -T(j , k , r , B,˜π , d )   else ▷ A = I   ▷1≤l < n , A , B ∈ { S , I } , i < j < k are the random variables , r = n−l   Sample ruleaccording to ˜π ( · )   Addtod   R -S -T(j , k , l , A,˜π , d )   R -S -T(i , j , r , B,˜π , d )   Algorithm 3 Inside algorithm for q(d|x , y , d ) .   Input : y : target sentence , d : derivation tree   function I -L(y , d )   Infer number of segments nfrom d   Initialize β [ . . . ] = 0   form= 2toNdo ▷number of segments   forw= 1to|x|do ▷width of spans   fori= 0to|x| −wdo ▷start point   k = i+w ▷ end point   forj = i+ 1tok−1do   forl= 1tom−1do ▷number of segments in the left part   r = m−l ▷ number of segments in the right part   ifthere exist a rule like A→BCindthen   β[T ] + = π()·β[T]·β[T ]   return β8227Algorithm 4 Top - down sampling dfrom q(d|x , y , d ) .   Input : β : inside scores returned from I -L(·),d : derivation tree , y : target sentence   function S -L(β , d , y )   Infer the number of segments nfrom d   Initialize an empty tree d   R -S -L(0,|y|,n , β , d , d )   return d   ▷Arguments : i : start point , k : end point , n : number of segments   function R -S -L(i , k , n , β , d , d )   ifn= 1then   Add ruletod   return   Infer l , rbased on dandn   Compute Z=/summationtext(β[T]·β[T ] ) ▷normalization term   forj = i+ 1tok−1do   ˜π ( ) = ( β[T]·β[T])/Z   Sample rule , and add it to d ▷the rule only encodes split point j   R -S -L(i , j , l , β , d , d )   R -S -L(j , k , r , β , d , d )   Algorithm 5 Calculating the entropy of H[q(d|x , y , d ) ]   Input : β : inside scores returned by I -L(·),d : derivation tree , y : target sentence   function C -L - E ( β , d , y )   Find the number of segments nfrom d   return R -C -L - E ( 0,|y|,n , β , d )   ▷Arguments : i : start point , k : end point , n : number of segments   function R -C -L - E ( i , k , n , β , d )   ifn= 1then   return 0   Infer l , rbased on dandn   Compute Z=/summationtext(β[T]·β[T ] ) ▷normalization term   forj = i+ 1tok−1do   ˜π ( ) = ( β[T]·β[T])/Z   H= 0   forj = i+ 1tok−1do   H = R -C -L - E ( i , j , l , β , d )   H = R -C -L - E ( j , k , r , β , d )   ▷Denote ruleasR   H+= ( H+H−log[˜π(R)])·˜π(R )   return H ▷return the entropy8228Algorithm 6 Computing the KL - divergence of KL[q(d|x , y , n ) ∥p(d|x , n ) ]   Input : ˜π : normalized rule scores returned by I -T ( · ) forq(d|x , y , n ) , ˜π : rule scores   obtained similarly by calling I -T ( · ) forp(d|x , n),n : number of segments , x : source sentence   function C -T - KL ( ˜π,˜π , n , x )   Initialize KL [ · ] = 0   form= 2tondo ▷number of segments   forw= 1to|x|do ▷width of spans   fori= 0to|x| −wdo ▷start point   k = i+w ▷ end point   forj = i+ 1tok−1do   forl= 1tom−1do ▷number of segments in the left part   r = m−l ▷ number of segments in the right part   ▷DenoteasR   KL[S]+=/parenleftbig   KL[I ] + KL[S ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   ▷DenoteasR   KL[S]+=/parenleftbig   KL[I ] + KL[I ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   ▷DenoteasR   KL[I]+=/parenleftbig   KL[S ] + KL[I ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   ▷DenoteasR   KL[I]+=/parenleftbig   KL[S ] + KL[S ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   ▷Denote , asR , Rrespectively   KL[T ] + = /parenleftbig   KL[S ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   KL[T ] + = /parenleftbig   KL[I ] + log[˜ π(R)]−log[˜π(R)]/parenrightbig   · ˜π(R )   return KL[T]8229