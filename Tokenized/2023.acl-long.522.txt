  Jian Yang , Shuming Ma , Li Dong , Shaohan Huang , Haoyang Huang ,   Yuwei Yin , Dongdong Zhang , Liqun Yang , Furu Wei , Zhoujun LiState Key Lab of Software Development Environment , Beihang UniversityMicrosoft Research Asia;The University of Hong Kong   { jiaya , lqyang , lizj}@buaa.edu.cn ;   { shumma , lidong1 , shaohanh , haohua , dozhang , fuwei}@microsoft.com ;   yuweiyin@hku.hk   Abstract   Pre - trained models have achieved remarkable   success in natural language processing ( NLP ) .   However , existing pre - training methods under-   utilize the benefits of language understanding   for generation . Inspired by the idea of Genera-   tive Adversarial Networks ( GANs ) , we propose   a GAN - style model for encoder - decoder pre-   training by introducing an auxiliary discrimi-   nator , unifying the ability of language under-   standing and generation in a single model . Our   model , named as GLM , is trained with two   pre - training objectives : replaced token detec-   tion and replaced token denoising . Specifically ,   given masked source sentences , the generator   outputs the target distribution and the discrim-   inator predicts whether the target sampled to-   kens from distribution are incorrect . The target   sentence is replaced with misclassified tokens   to construct noisy previous context , which is   used to generate the gold sentence . In gen-   eral , both tasks improve the ability of lan-   guage understanding and generation by selec-   tively using the denoising data . Extensive ex-   periments in language generation benchmarks   show that GLMwith the powerful language   understanding capability outperforms various   strong pre - trained language models ( PLMs )   and achieves state - of - the - art performance .   1 Introduction   The pre - training - then - fine - tuning paradigm has   been proven successful in many natural language   processing tasks ( Devlin et al . , 2019 ; Liu et al . ,   2019 ; Schick and Schütze , 2021 ) . While there are   various pre - training approaches for the encoder-   only architectures ( Clark et al . , 2020 ; Conneau   et al . , 2020 ) , the encoder - decoder pre - training is un-   derexplored , which is essential for natural language   generation . To pre - train the entire encoder - decoderFigure 1 : A pre - training sample of our method , where   replaced token detection ( discriminator ) and replaced   token denoising ( generator ) are used for pre - training .   The discriminator classifies each generated token into   REPLACED orORIGINAL , where REPLACED denotes   the predicted token is different from the gold token . The   red fonts denote incorrect predictions .   model , BART ( Lewis et al . , 2020 ) proposes a de-   noising language model objective and T5 ( Raffel   et al . , 2020 ) pre - trains the models with a span cor-   ruption objective . Furthermore , mBART ( Liu et al . ,   2020 ) and mT5 ( Xue et al . , 2021 ) extend them to   be multilingual pre - trained language models .   Unlike most encoder - decoder pre - training meth-   ods that simply apply sequence - to - sequence tasks   on a single encoder - decoder architecture , we ex-   plore the approaches to pre - train the model in a   GAN - style manner with an auxiliary discrimina-   tor . GAN ( Goodfellow et al . , 2014 ) performs well   on both text and image generation tasks by com-   bining the generator and discriminator . It aims to   improve the ability of the generator to produce high-   quality samples , which is important for the encoder-   decoder pre - training when transferred to down-   stream generation tasks . Similarly , MaskGAN ( Fe-   dus et al . , 2018 ) shows the GAN - like training can   improve the quality of the autoregressive language   model . Therefore , it is intuitive to leverage GAN   to empower the encoder - decoder pre - training by   unifying language understanding and generation .   In this work , we propose a pre - training frame-   work GLM , using GAN - style learning to im-9394prove the transferability of pre - trained language   models for the natural language generation . Specifi-   cally , the encoder reads the masked source sentence   and the generator obtains target distribution . Then ,   the discriminator distinguishes whether each token   sampled from the target distribution matches the tar-   get gold sentence ( replaced token detection ) . The   misclassified tokens by discriminator are regarded   as hard tokens for the generator to predict accu-   rately . We replace original tokens in the target sen-   tence with misclassified sampled ones to construct   the noisy previous context for predicting the target   sentence ( replaced token denoising ) . In Figure 1 ,   the generator predicts the masked words “ guardian   watered ” , where the incorrect token “ guardian ” and   correct token “ watered ” are both misclassified into   REPLACED andORIGINAL by the discriminator .   Next , we resample a different token “ watering ”   from the generated distribution . Consequently , the   target tokens “ gardener watered ” are replaced with   the sampled tokens “ guardian watering ” to con-   struct the noisy sample . The generator predicts the   next word conditioned on previous noisy tokens   ( replaced token denoising ) . Through combing two   tasks , GLMstrengthen generation performance   with the enhanced language understanding capabil-   ity from the replaced token detection task .   Our method is effective for text generation and   can be extended to natural language understanding   tasks . We pre - train GLM model on large - scale   monolingual corpora and evaluate the performance   of our pre - trained English model GLMand mul-   tilingual model GLM - m on various downstream   tasks , including text summarization , machine trans-   lation , and data - to - text generation . Experimen-   tal results demonstrate that our method substan-   tially outperforms previous pre - trained encoder and   sequence - to - sequence models on generation tasks .   Our method is further tested on GLUE ( Wang et al . ,   2019 ) and XNLI ( Conneau et al . , 2018 ) to validate   the transferability of our pre - trained model . An-   alytic experiments emphasize the importance of   the discriminator in both the pre - training and fine-   tuning stage , leading to better performance .   2 GLM   2.1 Model Overview   Our GAN - style pre - trained model comprises a gen-   erator ( G ) and discriminator ( D ) , which are both   encoder - decoder frameworks and conditioned on   the same encoder ( Enc ) . In Figure 2 , the encoderreads the masked sentence and the generator de-   coder obtains the target distribution . Then the dis-   criminator decoder distinguishes whether each to-   ken in the sampled target sentence matches the gold   reference . Tokens in the target gold sentence are   randomly replaced with misclassified ones by the   discriminator to construct the noisy sample , which   is fed into the generator decoder to predict the tar-   get sentence ( replaced token denoising ) .   2.2 Masked Sequence Generator   Given a monolingual sentence x= ( x , . . . , x )   withnwords from the dataset Dof language L∈   L={L , . . . , L}(|L|=K ) , some random   spans of contiguous tokens in xare corrupted as   the source sentence , which is denoted as x=   ( x , . . . , x , . . . , x).xis a masked span of   x , where the fragment from position utovis   corrupted by [ MASK ] . Given x , the generator   predicts the original identities of the masked tokens   x= ( x , . . . , x , . . . , x)autoregressively :   where θandθdenote the encoder and decoder   parameters of the generator . Enc - Dec denotes an   encoder - decoder model . The generator predicts the   next position ttoken xbased on previous tokens .   The training objective of sequence - to - sequence   masked language modeling ( S2S - MLM ) on the   dataset Dof language Lis defined as :   where xandxare derived from x.   2.3 Replaced Token Detection   The generator outputs the distribution of each target   token and we create a sampled sentence ˆxby ran-   domly sampling tokens from the distribution . The   discriminator distinguishes whether each token in   ˆxis replaced compared to x. Given the target   distribution P(x|x ) ( x∈x)from the   generator , we construct ˆxfor the discriminator :   where ( · ) replaces target t - th position un-   masked token in xwith the sampled token x   from the generated distribution P(x|x ) .   Given the source sentence xand the en-   coder θ , the decoder of the discriminator θob-   tains a sequence of hidden representations H=9395   ( h , . . . , h)by feeding the sampled sentence ˆx   to the discriminator decoder :   where θandθdenote the encoder and decoder   parameters of the discriminator . The decoder of the   discriminator θadopts the bidirectional language   model to classify each input token by extracting   the past and future representations .   Given the representations H , the discriminator   classifies sampled tokens ˆxinto the REPLACED   orORIGINAL label with a sigmoid function σ :   where W∈Ris the matrix projects the token   representations to two categories ( REPLACED or   ORIGINAL ) and dis the model hidden size .   The training objective of the replaced token de-   tection task for the discriminator is :   where I(·)is the indicator function .   2.4 Replaced Token Denoising   Although our model structure is similar to GAN ,   the generator is trained with maximum likelihood   rather than the standard GAN objective due to the   difficulty of the GAN training in NLP . We replace   tokens in xwith misclassified tokens by dis-   criminator to construct the noisy previous context   x. If the sampled token ˆx = xis labeledwithORIGINAL , we will resample the token x   ( x̸=x ) from target distribution as the misclassi-   fied token to modify xinx . When ˆx = x   ( x̸=x ) is labeled with REPLACED , the mis-   calssified token xdirectly replaces xin the target   sentence . Given the target sentence xand gen-   erated probabilities P , we replace tokens in x   with sampled tokens as the previous noisy context :   where v={v , . . . , v}(|v|=p)denotes the   positions in xof the misclassified tokens .   The training objective of the replaced token de-   noising ( DG ) task based on the source sentence   xand target noisy context xis described as :   where xis predicted by the previous noisy to-   kensxinstead of previous gold context .   2.5 Multi - task Learning   Given multilingual corpora D={D , . . . , D }   ofKlanguages , the pre - trained model with pa-   rameters { θ , θ , θ}is jointly trained over Klan-   guages to optimize the combined self - supervised   objective as below :   where λ= 10 .0is the discriminator weight and   L={L , . . . , L } . To improve model effi-   ciency , a tiny discriminator decoder ( 4 layers ) is   adopted to help the generator decoder ( 12 layers).93963 Discriminator - enhanced Fine - tuning   To fully utilize the pre - trained parameters , we keep   the auxiliary discriminator in downstream genera-   tion tasks ( discriminator - enhanced fine - tuning ) to   enhance the generator , where both the pre - trained   generator and discriminator are recycled . Given   the annotated corpus DofKlanguages , the pre-   trained model { θ , θ , θ}is optimized by :   where xandyare the parallel pair from D. The ob-   jective in the fine - tuning stage use the original pair   xandywithout S2S - MLM . The generator { θ , θ }   are kept for inference by throwing out the discrimi-   nator decoder θ . Alternatively , the discriminator   ( D:{θ , θ})or generator ( G:{θ , θ})can also   be separately fine - tuned on the downstream task .   4 Experiment Setting   4.1 Pre - training Details   Model Configuration In the experiments , we   adopt a sequence - to - sequence base - setting Trans-   former architecture with 768 hidden size , 3072   FFN ( feed - forward network ) dimension , 12 atten-   tion heads , and 12 encoder / decoder layers . The   maximum sequence length of learned positions em-   beddings in the encoder / decoder is set as 1024 . All   token embedding matrices and output projection   matrix parameters are shared for model efficiency .   Dataset Following the previous work ( Liu et al . ,   2019 ) , our English pre - trained model GLMis   trained on 160 GB English monolingual data from   BookCorpus , CC - News , OpenWebText , and CC-   Stories . In addition , we pre - train GLM - m with   6 TB multilingual data as the pioneering work ( Ma   et al . , 2021 ) , which is a combination of CC100 , CC-   Net , and Wikipedia , covering 100 languages . All   texts are tokenized by SentencePiece ( Kudo and   Richardson , 2018 ) and encoded by the dictionary   from XLM - R ( Conneau et al . , 2020 ) .   Optimization For S2S - MLM , we randomly   mask 15 % of the words in each instance with an   average span length of 3 ( Raffel et al . , 2020 ) . For   the replaced token detection , we set the discrimi-   nator weight λ= 10.0 . We adopt Adam ( Kingma   and Ba , 2015 ) with a learning rate of 3e-4 and   10 K warm - up steps for pre - training . The model is   trained on 128 NVIDIA A100 GPUs ( 40 GB ) from   scratch and each batch contains 8 K samples . TheEnglish pre - trained model GLMand multilin-   gual model GLM - m are trained for 500 K steps .   Specifically , all methods in Table 1 are pre - trained   with 500 K steps for a fair comparison .   4.2 Downstream Tasks   Monolingual Summarization CNN / DailyMail   ( See et al . , 2017 ) is an abstractive summarization   dataset aiming at generating a concise summary   from an English news article in CNN and Dai-   lyMail . As a popular abstractive summarization   dataset , XSum ( Narayan et al . , 2018 ) compresses a   BBC news article to a short one - sentence summary .   Multilingual Summarization To test the capa-   bility of our multilingual pre - trained model , a   large - scale multilingual dataset named WikiLin-   gua ( Ladhak et al . , 2020 ) of 18 languages from   WikiHow is used to evaluate multilingual abstrac-   tive summarization systems .   Bilingual Translation For the bilingual task ,   we use the WMT-14 English - German , WMT-   14 English - French , and WMT-16 English-   Romanian dataset for evaluation . WMT-14 En - De   from WMT consists of 4.5 M sentence pairs and the   newstest2014 is used as the test set . WMT-14 En-   Fr is a large - scale dataset containing nearly 41 M   sentence pairs and newstest2014 is adopted for eval-   uation . WMT-16 En - Ro is comprised of original   parallel sentences and back - translation data .   Multilingual Translation IWSLT-17 of 5 lan-   guages and WMT-10 of 11 languages are utilized   for multilingual translation . For IWSLT-17 , En-   glish ( En ) , German ( De ) , Italian ( It ) , Dutch ( Nl ) ,   and Romanian ( Ro ) corpora are downloaded from   the IWSLT-2017 benchmark . We use dev2010 for   validation and tst2017 for test . For WMT-10 , we   use the parallel data of 11 languages from the WMT   benchmark for evaluation ( Wang et al . , 2020 ) .   Data - to - Text Generation Data - to - text genera-   tion accepts multiple triplets and produces a de-   scription . WebNLG ( Gardent et al . , 2017 ) contains   parallel DBpedia triple sets and short texts . The En-   En direction contains 17 K triple sets and 45 K short   texts and the En - Ru direction contains 7 K triple   sets and 19 K texts in Russian . The ROUGE scores   on the valid set are reported for a fair comparison   with the previous work ( Gehrmann et al . , 2021).9397   4.3 Fine - tuning Details   Abstractive Summarization During fine - tuning ,   we use the Adam ( Kingma and Ba , 2015 ) optimizer   with an initial learning rate of 1e-4 and the batch   size is set as 2048 tokens on 8 V100 GPUs . The   models are trained with the label smoothing cross-   entropy with a smoothing ratio of 0.1 .   Neural Machine Translation For the large - scale   multilingual dataset WMT-10 , our pre - trained   model is fine - tuned on 32 V100 GPUs with a learn-   ing rate of 3e-4 . For all bilingual translation tasks   and the IWSLT-2017 benchmark , we adopt Adam   with a learning rate of 1e-4 and set the batch size   as 2048 tokens on 8 V100 GPUs .   Data - to - text Generation We use Adam with a   learning rate of { 8e-5,1e-4 } and set the batch size   as 16 sentences on the WebNLG dataset.5 Comparing Pre - training Objectives   To verify the potential of our pre - training task   under a fair comparison , we re - implement pre-   vious pre - training tasks and pre - trains baselines   on the same corpora with 500 K steps , including   BERT / mBERT ( Devlin et al . , 2019 ) , ELECTRA   ( Clark et al . , 2020 ) , BART ( Lewis et al . , 2020)/   mBART ( Liu et al . , 2020 ) , and T5 ( Raffel et al . ,   2020)/mT5 ( Xue et al . , 2021 ) . Table 1 reports   the ROUGE and BLEU points on the summariza-   tion dataset XSum and multilingual translation   dataset IWSLT-17 . All models have 12 encoder   and 12 decoder layers with a hidden size of 768 .   We observe that the encoder - decoder pre - trained   model ( T5 / mT5 ) outperforms the pre - trained en-   coder ( ELECTRA , BERT / mBERT ) , which corrob-   orates the encoder - decoder pre - training is more   beneficial to the downstream generation task . Ex-   periments ⑥∼⑧show the importance of the dis-   criminator and replaced token denoising . Experi-   ment⑧demonstrates that only the replaced token   detection task can still bring improvement through   strengthening the encoder shared by both genera-   tor and discriminator . Besides , the replaced token   detection task is also helpful to downstream lan-   guage understanding tasks with a powerful encoder .   Lastly , the results verify that fine - tuning with the   help of the pre - trained auxiliary discriminator fur-   ther improves performance .   6 Results of GLM   The English pre - trained model GLM is eval-   uated on the abstractive text summarization task   with the ROUGE ( Lin , 2004 ) scores .   XSum As shown in Table 2 , the pre - training   methods achieve significant improvements over the   strong baseline PNwithout pre - training . The   sequence - to - sequence pre - trained model such as9398UniLMv2 + s2s - ft outperforms other pre - training   baselines , where the pseudo - masked technique is   applied to the fine - tuning stage . Our method beats   all pre - training baselines by a large margin with the   discriminator - enhanced fine - tuning strategy . It em-   phasizes the importance of the fine - tuning strategy   for the performance of downstream tasks .   CNN / DailyMail Our method is also evaluated   on the CNN / DailyMail dataset in Table 2 . The   comparisons further indicate that our method ob-   tains strong performance on generation by leverag-   ing the discriminator .   7 Results of GLM - m   To evaluate the multilingual pre - trained model   GLM - m , we report the BLEU ( Papineni et al . ,   2002 ) scores for machine translation and ROUGE   ( Lin , 2004 ) scores for text summarization and data-   to - text generation .   WikiLingua Table 3 reports the average ROUGE   scores of 18 WikiLingua languages . The large im-   provement over other pre - training method demon-   strate the summarization ability of our GLM - m.   WMT14 En - De The results on the bilingual   translation are presented at Table 4 . We observe   that the proposed GLMoutperforms all previ-   ous works in the high - resource machine translation   scenario ( > 4 M sentence pairs ) .   WMT14 En - Fr We further conduct experiments   on the WMT14 En - Fr bilingual translation task .   Table 4 GLM - m shows that GLM - m still   brings significant improvement to the downstream   task with large - scale machine translation fine-   tuning data ( > 40 M sentence pairs ) .   WMT16 En - Ro For the low - resource setting ( <   1 M sentence pairs ) , there is an average gain of +4   BLEU points compared to the Transformer base-   line in Table 5 . With the same back - translation   data , GLM - m further improves the model per-   formance and still beats other baselines .   WMT-10 For the multilingual translation , we   compare GLM - m with the strong multilingual   pre - trained models in Table 7 and Table 6 , such as   mBART ( Liu et al . , 2020 ) . It is notable our method   outperforms large pre - trained model mBART with   1024 hidden size by a large margin ( +1 ∼2 BLEU   points ) . Plus , there is a +1.5 BLEU gain over XLM-   R , whose encoder and decoder are initialized by the   cross - lingual pre - trained encoder ( Ma et al . , 2020 ) .   WebNLG Table 8 presents the performance on   the data - to - text generation task , showing that   GLM outperforms multilingual sequence - to-   sequence pre - training baselines mBART and mT5   by +2 ROUGE - L points on both languages .   8 Analysis   Ablation Study To analyze the effect of the pro-   posed pre - training and fine - tuning strategies , we   conduct an ablation study of each component of   our method in Table 9 . Experiment ④and⑥verify   the merits of the replaced token detection and re-   placed token denoising . Furthermore , experiment   ⑦shows that our model with the replaced token de-   noising task obtains the best performance by jointly   fine - tuning generator ( G ) and discriminator ( D ) .   Low - resource Setting To further analyze the per-   formance of GLM - m given different sizes of   downstream parallel data , we randomly extract K   percentage of the whole sentence pairs as the fine-   tuned parallel data from the full WMT-16 En →Ro   training data . We set K= { 10%,20 % , . . . , 100 % }   and compare our method with the Transformer   baseline model . Figure 3 shows the BLEU points of   our pre - trained multilingual model and the baseline .   When the parallel data size is small , the baseline   without pre - trained model produces unsatisfactory   results . Similarly , in Figure 3(a ) , GLM fine-9399   tuned on nearly half data ( purple line , 50 % ) defeats   the baseline trained on all pairs ( green line , 100 % ) ,   exemplifying the effectiveness of our method in   low - resource scenarios .   Discussion on Discriminator The weight value   λand layer number of the discriminator are key fac-   tors to our pre - training task . As shown in Figure 4 ,   we vary discriminator weight in Figure 4(a ) to find   a balance between the generator and discriminator   objective . To this end , we study the performance   ofGLMwith different λ , where λranges from   5.0to100.0 . When the weight of the discriminator   is 10.0 , multiple pre - training tasks are balanced .   Moreover , we find it more efficient to have a tiny   discriminator ( 3 ∼6 layers ) in Figure 4(b ) .   Multilingual Representations We randomly se-   lect 1000 parallel sentences of each language   in WMT-10 and visualize their representations   ( Maaten and Hinton , 2008 ) of the last two encoder   layers in Figure 5 using our multilingual model   fine - tuned on WMT-10 and the multilingual base-9400   line . The first hidden state of the encoder is adopted   as the sentence representation . Compared to Figure   5(a ) and 5(b ) of the baseline , different languages be-   come closer and likely to overlap with each other in   Figure 5(c ) and 5(d ) of our method , demonstrating   that our method effectively aligns representations   of different languages to the shared space .   Massively Multilingual Translation We com-   pare GLM - m with the state - of - the - art multilin-   gual NMT model M2M-124 ( Goyal et al . , 2021 ) .   M2M-124and DeltaLM + Zcode both have a   large hidden size of 1024 . Our pre - trained model   is fine - tuned on the same training data as DeltaLM   + Zcode ( Yang et al . , 2021 ) . Compared to M2M-   124,GLM - m with fewer training data and   only 430 M parameters depends more on the trans-   ferability of the cross - lingual pre - training model .   In Table 10 , our method outperforms the DeltaLM +   Zcode in zero - shot translation direction ( Avg )   by +1.5 BLEU points , benefiting from our pre-   trained model in cross - lingual zero - shot transfer .   Comparison of Pre - training Cost Our English   pre - trained model GLM is trained for nearly   2 weeks on 128 A100 GPUs ( 40 GB ) , with 500 K   training steps and a batch size of 8 K sequences .   Compared to the re - implemented T5 ( Raffel et al . ,   2020 ) , our method is only 0.5 times slower than   T5 with the same training steps but gets a signifi-   ca nt improvement on the machine translation , text   summarization , and data - to - text generation tasks .   Training of replaced token denoising To fully   understand the training procedure of the replaced   token denoising , we plot the training loss of   sequence - to - sequence masked language modeling   L , replaced token detection , and replaced token   denoising in Figure 6 . Furthermore , we investi-   gate how many tokens in the target sentence are   replaced with the misclassified tokens by discrimi-   nator in Figure 7 . We define pas the replaced rate   in the target gold sentence . Nearly 7.5 % tokens   of the target previous tokens are replaced with the   misclassified tokens to construct the noisy input   samples for the generator decoder .   Language Understanding Our method can be   easily extended to various downstream language   understanding tasks . We use the GLUE benchmark   ( Wang et al . , 2019 ) to estimate English pre - trained   model GLM and the XNLI dataset ( Conneau   et al . , 2018 ) to evaluate the capability of the mul-   tilingual language understanding . Our method is   tested on each language separately by fine - tuning   generator ( G ) or discriminator ( D ) on the XNLI   dataset . In Table 11 , Our English pre - trained model   performs better than RoBERTa . Additionally , our   pre - trained model outperforms the previous cross-   lingual pre - trained encoder XLM and pre - trained   encoder - decoder model mT5 in Table 12 .   9 Related Work   Pre - training for Generation Language model-   ing based on the self - supervised learning training   objective and large - scale data has been widely used   to acquire contextual representations . Pre - training   a large Transformer encoder ( Vaswani et al . , 2017;9401   Devlin et al . , 2019 ; Joshi et al . , 2019 ; Liu et al . ,   2019 ) with the masked language modeling ( MLM )   task brings significant improvement for various   downstream natural language understanding ( NLU )   tasks . Many enhanced versions of MLM tasks   ( Joshi et al . , 2019 ; Sun et al . , 2019 ; Liu et al . , 2019 ;   Clark et al . , 2020 ) are proposed to further enhance   the capability of the pre - trained model . Besides ,   pre - training a Transformer decoder ( Radford et al . ,   2018 , 2019 ; Schick and Schütze , 2021 ) is beneficial   for unconditional text generation . There have been   numerous attempts for pre - training a sequence - to-   sequence Transformer model by adding generative   training objectives , such as MASS ( Song et al . ,   2019 ) and BART ( Lewis et al . , 2020 ) . Further-   more , T5 ( Raffel et al . , 2020 ) explores different   pre - training tasks and proposes to corrupt consecu-   tive span of tokens for pre - training . Different from   previous works , our work focuses on leveraging the   auxiliary discriminator ameliorate encoder - decoder   pre - training on language generation tasks .   Multilingual Pre - training Inspired the success   of pre - training in a single language such as English ,   .   recent works ( Conneau and Lample , 2019 ; Con-   neau et al . , 2020 ; Yang et al . , 2022a , 2020 ; Chi   et al . , 2021b ; Yang et al . , 2022b , c , 2021 ) aim to   learn cross - lingual representations with different   training objectives in multiple languages . For the   sequence - to - sequence model , mBART ( Liu et al . ,   2020 ) pre - trains a Transformer model by denoising   training objective in multiple languages . mT5 ( Xue   et al . , 2021 ) extends the span corruption task for   multilingual training and mT6 ( Chi et al . , 2021a )   amplify generation task by introducing a partially   non - autoregressive objective . Along the line of re-   search , different multilingual pre - trained models   ( Ma et al . , 2020 ; Chi et al . , 2020 ) are proposed to   solve downstream cross - lingual generation tasks .   10 Conclusion   In this work , we introduce GLM , a state - of-   the - art pre - training encoder - decoder framework   for both language generation and understanding   tasks trained on large - scale corpora . Our GAN-   style models are pre - trained with replaced token   detection and replaced token denoising by intro-   ducing an auxiliary discriminator . Extensive ex-   periments prove the effectiveness of GLMon   various language generation and translation bench-   mark datasets . We further verify the capability of   the pre - trained model on multiple downstream un-   derstanding tasks .   Acknowledgments   This work was supported in part by the National   Natural Science Foundation of China ( Grant Nos .   62276017 , U1636211 , 61672081 ) , the 2022 Ten-   cent Big Travel Rhino - Bird Special Research Pro-   gram , and the Fund of the State Key Laboratory   of Software Development Environment ( Grant No .   SKLSDE-2021ZX-18).9402References940394049405A Statistics of Datasets   WMT-14 En - De WMT-14 En - De consists of   4.5 M sentence pairs . The validation set is de-   vtest2014 , and the test set is newstest2014 .   WMT-16 En - Fr WMT-14 En - Fr is a large - scale   dataset containing nearly 41 M sentence pairs ,   where newstest2014 is employed for evaluation .   WMT-16 En - Ro WMT-16 En - Ro is comprised   of original parallel sentences and back - translation   data . We use newsdev2016 for validation and new-   stest2016 for test . Following the previous work   ( Liu et al . , 2020 ) , we use the same back - translation   data for a fair comparison .   IWSLT-2017 We download English ( En ) , Ger-   man ( De ) , Italian ( It ) , Dutch ( Nl ) , and Romanian   ( Ro ) corpora from the IWSLT-2017 benchmark .   The dev2010 is used for validation and tst2017 for   test .   WMT-10 Table 13 lists the detailed statistics of   10 language pairs from WMT-10 , which is a col-   lection of parallel data in different languages from   the WMT datasets . The parallel data is paired with   English and other 10 languages , including French   ( Fr ) , Czech ( Cs ) , German ( De ) , Finnish ( Fi ) , Lat-   vian ( Lv ) , Estonian ( Et ) , Romanian ( Ro ) , Hindi   ( Hi ) , Turkish ( Tr ) and Gujarati ( Gu ) . The corpora   of the WMT benchmark , exclude WikiTiles , from   the latest available year of each language are cho-   sen . After removing the duplicated samples , we   limit the size of each parallel language pair data   up to 10 million by randomly sampling from the   whole corpus . We adopt the same valid and test sets   from the WMT benchmark as the previous work   ( Wang et al . , 2020 ) .   WikiLingua To test the capability of our multi-   lingual pre - trained model , a large - scale multilin-   gual dataset named WikiLingua ( Ladhak et al . ,   2020 ) of 18 languages from WikiHow is used to   evaluate multilingual abstractive summarization   systems .   B Pre - training and Fine - tuning Details   Pre - training Hyper - parameters Table 14 sum-   marizes the hyper - parameters for pre - training   GLMandGLM - m The task - specific hyper-   parameters for the downstream language genera-   tion and understanding tasks are in Table 15 .   Abstractive Summarization During fine - tuning ,   we use the Adam ( Kingma and Ba , 2015 ) optimizer   with an initial learning rate of 1e-4 and the batch   size is set as 2048 tokens on 8 V100 GPUs . The   models are trained with the label smoothing cross-   entropy with a smoothing ratio of 0.1 . The last 5   checkpoints are averaged for evaluation .   Neural Machine Translation We adopt Adam   with a learning rate of 1e-4 and set the batch size   as 2048 tokens on 8 V100 GPUs for all bilingual   translation tasks and the IWSLT-2017 benchmark .   For the large - scale multilingual dataset WMT-10 ,   our pre - trained model is fine - tuned on 32 V100   GPUs with a learning rate of 3e-4 . For a fair com-   parison , we adopt the same architecture and model   size as our pre - trained model .   Data - to - text Generation We use Adam with a   learning rate of { 8e-5,1e-4 } and set the batch size   as 16 sentences on the WebNLG dataset .   Multi - lingual Fine - tuning Following the previ-   ous work ( Wang et al . , 2020 ; Ma et al . , 2021 ) , we   adopt a dynamic temperate - based sampling strat-   egy to mitigate the unbalance of the multilingual   corpora , where we gradually sample more pairs in   low - resource languages with the number of epochs   increasing . The temperature of the i - th epoch is   calculated by :   τ= min ( τ , τ+i   N(τ−τ ) ) ( 11)9406   where τis the initial temperature , τis the peak   temperature , and Nis the number of warm - up   epochs . We set τ= 1.0,τ= 5.0 , and N= 5for   all multilingual experiments for a fair comparison .   Given the temperature τi - th epoch , we can cal-   culate the real sampling ratio of the language L ,   where L∈L={L , . . . , L } :   q(i ) = p   /summationtextp(12 )   where q(i)is the sampling ratio of the language   Lin the i - th epoch . pis the real data ratio of the   language Lin all languages . τis the temperature   of the i - th epoch , as described in Equation 11 .   C Results on Downstream Task   GLUE For each classification task of the GLUE   ( Wang et al . , 2019 ) , we conduct 5 experiments with   different seeds { 1,2,3,4,5}and report the average   accuracy of 5 experiments .   XNLI We also conduct 5 experiments with dif-   ferent seeds { 1,2,3,4,5}and report the average   accuracy of 5 experiments .   FLORES Since the corpora of X→Yare com-   monly scarce , the performance of low - resource   translation direction Avgmainly depends on   the zero - shot cross - lingual transferability of the   pre - trained model . Our model with the 12 encoderlayers and 12 decoder layers significantly outper-   forms the previous state - of - the - art model M2M-   124 with large model size . In Figure 8 , we re-   port the multilingual model initialized by our pre-   trained model in all translation directions , where   the languages are ordered alphabetically by the lan-   guage code . Following the previous work ( Yang   et al . , 2021 ) , we use the same training data , includ-   ing CCAligned ( El - Kishky et al . , 2020 ) , CCMatrix   ( Schwenk et al . , 2021 ) , OPUS-100 ( Zhang et al . ,   2020 ) , JW300 ( Agic and Vulic , 2019 ) , Tatoeba   ( Tiedemann , 2012 ) , WMT2021 news track , multi-   lingual track data .   D Weight Sharing   Our pre - trained model includes the discriminator   ( D:{θ , θ})and generator ( G:{θ , θ } ) . We   can use a 12 - layer generator decoder θand a 4-   layer tiny discriminator decoder θfor replaced   token denoising . We propose a weight sharing   strategy to improve the model efficiency of the pre-   training by sharing weights among the generator   and decoder ( θ = θ ) by setting the discrimina-   tor generator and generator decoder as the same   size ( both 12 layers ) . Table 18 lists the results of   different weight sharing strategies . It turns out the   sharing decoder setting performs worse than not   sharing . It is reasonable since the generator de-   coder is used for sequence generation whereas the   discriminator decoder is a classifier.94079408 ②94099410ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 11   /squareA2 . Did you discuss any potential risks of your work ?   Section 12   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 4   /squareB1 . Did you cite the creators of artifacts you used ?   Section 4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 4   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 4   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 4   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 4   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 4   C / squareDid you run computational experiments ?   Section 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 49411 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 4   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 4   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.9412