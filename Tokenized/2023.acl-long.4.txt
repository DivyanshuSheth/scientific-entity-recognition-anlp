  Hao Cheng , Shuo Wang , Wensheng Lu , Wei Zhang   Mingyang Zhou , Kezhong Lu , Hao LiaoCollege of Computer Science and Software Engineering , Shenzhen University , ChinaWeBank Institute of Financial Technology , Shenzhen University , ChinaPing An Bank Co. , Ltd.   { 2110276103 , 2110276109 , 2210273060 , 2210275010}@email.szu.edu.cn   { zmy , kzlu , haoliao}@szu.edu.cn   Abstract   Explainable recommendation is a technique   that combines prediction and generation tasks   to produce more persuasive results . Among   these tasks , textual generation demands large   amounts of data to achieve satisfactory ac-   curacy . However , historical user reviews of   items are often insufficient , making it chal-   lenging to ensure the precision of generated   explanation text . To address this issue , we   propose a novel model , ERRA ( Explainable   Recommendation by personalized Review re-   trieval and Aspect learning ) . With retrieval   enhancement , ERRA can obtain additional in-   formation from the training sets . With this addi-   tional information , we can generate more accu-   rate and informative explanations . Furthermore ,   to better capture users ’ preferences , we incorpo-   rate an aspect enhancement component into our   model . By selecting the top- naspects that users   are most concerned about for different items ,   we can model user representation with more   relevant details , making the explanation more   persuasive . To verify the effectiveness of our   model , extensive experiments on three datasets   show that our model outperforms state - of - the-   art baselines ( for example , 3.4 % improvement   in prediction and 15.8 % improvement in expla-   nation for TripAdvisor ) .   1 Introduction   Recent years have witnessed a growing interest in   the development of explainable recommendation   models ( Ai et al . , 2018 ; Chen et al . , 2021 ) . In gen-   eral , there are three different kinds of frameworks   for explainable recommendation models , which   are post - hoc ( Peake and Wang , 2018 ) , embedded   ( Chen et al . , 2018 ) and multi - task learning meth-   ods(Chen et al . , 2019b ) . Post - hoc methods gener-   ate explanations for a pre - trained model after the   fact , leading to limited diversity in explanations . Embedded methods , on the other hand , demon-   strate efficacy in acquiring general features from   samples and mapping data to a high - dimensional   vector space . However , since embedded methods   rely on historical interactions or features to learn   representations , they may struggle to provide ac-   curate recommendations for users or items with   insufficient data .   In addition to the two frameworks mentioned   above , there has been a utilization of multi - task   learning frameworks in explainable recommenda-   tion systems , where the latent representation shared   between user and item embeddings is employed   ( Chen et al . , 2019b ; Ai et al . , 2018 ) . These frame-   works often employ the Transformer ( Vaswani   et al . , 2017 ; Li et al . , 2021b ) , a powerful text en-   coder and decoder structure widely used for tex-   tual processing tasks . While efficient for predic-   tion tasks , they encounter challenges in generation   tasks due to limited review content , leading to a   significant decline in performance . Furthermore ,   these previous transformer - based frameworks do   not incorporate personalized information and treat   heterogeneous textual data indiscriminately . To   address these issues , we make adaptations to the   existing multi - task learning framework by incor-   porating two main components : retrieval enhance-   ment , which alleviates the problem of data scarcity ,   and aspect enhancement , which facilitates the gen-   eration of specific and relevant explanations .   Real - world datasets usually contain redundant   reviews generated by similar users , making the   selected reviews uninformative and meaningless ,   which is illustrated in Figure 1 . To address this   issue , a model - agnostic retrieval enhancement   method has been employed to identify and select   the most relevant reviews . Retrieval is typically   implemented using established techniques , such as   TF - IDF ( Term Frequency - Inverse Document Fre-   quency ) or BM25 ( Best Match 25 ) ( Lewis et al . ,   2020 ) , which efficiently match keywords with an51   inverted index and represent the question and con-   text using high - dimensional sparse vectors . This   approach facilitates the generation of sufficient spe-   cific text , thereby attaining enhanced textual quality   for the user . Generally , Wikipedia is utilized as a   retrieval corpus for the purpose of aiding statement   verification ( Karpukhin et al . , 2020 ; Yamada et al . ,   2021 ) . Here , we adopt a novel approach wherein   the training set of each dataset is utilized as the re-   trieval corpus . By integrating this component into   our framework , we are able to generate sentences   with more specific and relevant details . Conse-   quently , this enhancement facilitates the generation   of explanations that are more accurate , comprehen-   sive , and informative at a finer granularity .   Moreover , users rarely share a common prefer-   ence ( Papineni et al . , 2002 ) . Therefore , aspects   ( Zhang et al . , 2014 ) , extracted from corresponding   reviews , can be utilized to assist in the modeling   of user representation . The incorporation of aspect   enhancement has resulted in not only improved pre-   diction accuracy , but also more personalized and   user - specific text during the text generation process .   By incorporating retrieval enhancement and aspect   enhancement into our model , we adjust the trans-   former architecture to meet our needs , achieving   better performance in both prediction and genera-   tion tasks .   The main contributions of our framework are as   follows:•In response to the problem of insufficient histor-   ical reviews for users and items in explainable   recommendation systems , we propose a retrieval   enhancement technique to supplement the avail-   able information with knowledge bases obtained   from a corpus . To the best of our knowledge , this   study represents the first application of retrieval-   enhanced techniques to review - based explainable   recommendations .   •We propose a novel approach wherein different   aspects are selected for individual users when   interacting with different items , and are subse-   quently utilized to facilitate the modeling of user   representation , thereby leading to the generation   of more personalized explanations .   •Experimental results on real - world datasets   demonstrate the effectiveness of our proposed   approach , achieving superior performance com-   pared to state - of - the - art baselines .   2 Related Work   2.1 Explainable Recommendation with   Generation   Explainable recommendation systems ( Zhang et al . ,   2020 ) have been extensively studied using two   primary methodologies : machine learning and   human - computer interaction . The former ( Gedikli   et al . , 2014 ; Chen and Wang , 2017 ) investigates   how humans perceive different styles of explana-   tions , whereas the latter generates explanations   through the application of explainable recommen-   dation algorithms , which is more relevant to our   research . Numerous approaches exist for explain-   ing recommendations , including the use of def-   inition templates ( Li et al . , 2021a ) , image visu-   alization ( Chen et al . , 2019a ) , knowledge graphs   ( Xian et al . , 2019 ) , and rule justifications ( Shi et al . ,   2020 ) . Among these methods , natural language   explanations ( Chen et al . , 2019b ; Li et al . , 2021b )   are gaining popularity due to their user accessibil-   ity , advancements in natural language processing   techniques , and the availability of vast amounts of   text data on recommendation platforms . Several   studies have employed Recurrent Neural Network   ( RNN ) networks ( Li et al . , 2017 ) , coupled with   Long Short - Term Memory ( LSTM ) ( Graves and   Graves , 2012 ) , for generating explanatory texts ,   while others have utilized co - attention and Gated52Recurrent Unit ( GRU ) ( Cho et al . , 2014 ) in con-   junction with Convolutional Attentional Memory   Networks ( CAML ) ( Chen et al . , 2019b ) for text   generation . More recently , transformer - based net-   works have seen increased utilization for score pre-   diction and interpretation generation . ( Li et al . ,   2021b )   2.2 Pre - trained Models   The pre - trained model has gained significant trac-   tion in the field of NLP recently . These mod-   els , such as ( Devlin et al . , 2019 ; Reimers and   Gurevych , 2019 ) are trained on large - scale open-   domain datasets utilizing self - supervised learning   tasks , which enables them to encode common lan-   guage knowledge . The ability to fine - tune these   models with a small amount of labeled data has fur-   ther increased their utility for NLP tasks ( Qiu et al . ,   2020 ; Ren et al . , 2021 ) . For example , a pre - trained   model is Sentence - BERT ( Reimers and Gurevych ,   2019 ) , which utilizes a multi - layer bidirectional   transformer encoder and incorporates Masked Lan-   guage Model and Next Sentence Prediction to cap-   ture word and sentence - level representations . An-   other example is UniLM ( Dong et al . , 2019 ) , which   builds upon the architecture of BERT and has   achieved outstanding performance in a variety of   NLP tasks including unidirectional , bidirectional ,   and sequence - to - sequence prediction . Furthermore ,   research has demonstrated that pre - trained mod-   els possess the capability to capture hierarchy-   sensitive and syntactic dependencies ( Qiu et al . ,   2020 ) , which is highly beneficial for downstream   NLP tasks . The utilization of pre - trained models   has proven to be a powerful approach in NLP field ,   with the potential to further improve performance   on a wide range of tasks .   2.3 Retrieval Enhancement   Retrieval - enhanced text generation has recently re-   ceived increased attention due to its capacity to   enhance model performance in a variety of natural   language processing ( NLP ) tasks ( Ren et al . , 2021 ;   Qiu et al . , 2020 ) . For instance , in open - domain   question answering , retrieval - enhanced text gen-   eration models can generate the most up - to - date   answers by incorporating the latest information dur-   ing the generation process ( Li and Gaussier , 2021 ;   Li et al . , 2020a ) . This is not possible for tradi-   tional text generation models , which store knowl-   edge through large parameters , and the stored in-   formation is immutable . Retrieval - based methodsalso have an advantage in scalability , as they re-   quire fewer additional parameters compared to tra-   ditional text generation models ( Ren et al . , 2021 ) .   Moreover , by utilizing relevant information re-   trieved from external sources as the initial genera-   tion condition ( Ren et al . , 2021 ) , retrieval - enhanced   text generation can generate more diverse and ac-   curate text compared to text generation without any   external information .   3 Problem Statement   Our task is to develop a model that can accurately   predict ratings for a specific product and provide a   reasonable explanation for the corresponding pre-   diction . The model ’s input is composed of various   elements , namely the user ID , item ID , aspects ,   reviews , and retrieval sentences , whereas the re-   sulting output of the model encompasses both a   prediction and its explanation . We offer a detailed   description of our models ’ input and output data in   this section .   Input Data   •Heterogeneous information : The variables in-   cluded in the framework encompass user ID u ,   item ID v , aspects A , retrieval sentences Sand   review R. Aspects Aare captured in the form   of a vector representing user ’s attention , denoted   as(A , . . . , A ) , where Arepresents the   j - th aspect extracted from the reviews provided   by user u. As an illustration , the review The   screen of this phone is too small encompasses   the aspect ( screen , small ) . Regarding users , we   extract the most important sentence Sfrom   the set ( S , ... , S ) . Similar operations are   performed for items , where Sis employed . Ul-   timately , the user ’s review for the item Ris fed   into the training process to enhance the ability to   generate sentences .   Output Data   •Prediction and explaination : Given a user u   and an item v , we can obtain a rating predic-   tionˆr , representing user u ’s preference to-   wards item vand a generated explanatory text   L= ( l , l , . . . , l ) , providing a rationale for the   prediction outcome . In this context , ldenotes   thei - th word within the explanation text , while T   represents the maximum length of the generated   text.534 Methodology   4.1 Overview of Model   Here we present a brief overview of ERRA model .   As shown in Figure 2 , our model mainly consists   of three components , each corresponding to a sub-   process of the information processing model :   •Retrieval Enhancement aims to retrieve exter-   nal knowledge from the training sets .   •Aspect Enhancement aims to identify the most   important aspects that users are concerned about   in their reviews .   •Joint Enhancement Transformers is responsi-   ble for the integration of the retrieved sentences   and aspects with a transformer structure for si-   multaneously performing the prediction and ex-   planation tasks .   Next , we will provide an in - depth description of   each component and how they are integrated into a   unified framework .   4.2 Retrieval Enhancement   A major challenge in generating rich and accurate   explanations for users is the lack of sufficient re-   view data . However , this problem can be allevi-   ated via retrieval - enhanced technology , which in-   troduces external semantic information .   4.2.1 Retrieval Encode   The retrieval corpus is constructed using the train-   ing set . To obtain fine - grained information , lengthy   reviews are divided into individual sentences   with varied semantics . Using these sentences as   searching unit allows the model to generate more   fine - grained text . Sentence - BERT ( Reimers and   Gurevych , 2019 ) is utilized to encode each sen-   tence in the corpus , which introduces no additional   model parameters . We did not use other LLMs   ( Large Language Models ) for retrieval encoding   because it is optimized for dense retrieval and effi-   cient for extensive experiments . Sentence - BERT is   considerably faster than BERT - large or RoBERTa   when encoding large - scale sentences and possesses   an enhanced capacity for capturing semantic mean-   ing , making it particularly well - suited for the re-   trieval task . The encoded corpus is saved as an   embedding file , denoted as C. During the retrieval   process , the most relevant information is directly   searched from the saved vector C , which greatly   improves the efficiency of retrieval.4.2.2 Retrieval Method   We adopt a searching model commonly used in the   field of question answering ( QA ) and utilize cosine   similarity for querying as a simple and efficient   retrieval method . Here , we use the average of the   review embedding Uof each user as the query .   This representation is in the same semantic space   and also captures user preferential information to   a certain extent . The average embedding Uof   all the reviews for a user is used as a query to re-   trieve the most similar nsentences ( S , ... , S )   in the previous corpus C. Our approach incorpo-   rates the Approximate Nearest Neighbor ( ANN )   search technique , with an instantiation based on the   Faisslibrary to improve retrieval speed through   index creation . This optimization substantially de-   creases the total retrieval search duration . Then ,   in our implementation , we set nas3and stitch   these sentences together to form a final sentence .   Sentence - BERT is then used to encode this final   sentence to obtain a vector S , which represents   the user for the item retrieval . Similarly , Sis   used for items to retrieve users .   4.3 Aspect Enhancement   Users ’ preferences are often reflected in their re-   views . To better represent users , we need to select   the most important aspects of their reviews . Specif-   ically , we first extract aspects from each user and   item review using extraction tools . The extracted   aspects from user reviews represent the style of the   users in their reviews , while the extracted aspects   from item reviews represent the most important   features of the item . We aim to identify the most   important aspects that users are concerned about in   their reviews . It is worth noting that users ’ inter-   ests may vary in different situations . For example ,   when choosing a hotel , a user may care more about   the environment . Whereas , price is a key factor to   consider when buying a mobile phone . To address   this , we use the average vector A , v∈V ,   representing all aspects under the item reviews , as   the query . This vector is encoded using Sentence-   BERT . For each user , we construct a local corpus of   their aspects collection ( A , ... , A ) , u∈U   and use cosine similarity as the measurement indi-   cator . We search for the top - n aspects from the local   corpus by A. These retrieved aspects repre-   sent the top - n aspects that the user is concerned   about this item.54   4.4 Joint Enhancement Transformers   In our proposed model , we adopt the transformer   structure in the prediction and explanation tasks .   The transformer consists of multiple identical lay-   ers with each layer comprising two sub - layers : the   multi - head self - attention and the position - wise feed   feedback network . Previous research has made var-   ious modifications to the transformer architecture   ( Li et al . , 2021b ; Geng et al . , 2022 ) . Here we in-   tegrate the retrieved aspects with the encoding of   multiple sentences in various ways . The retrieved   sentences S , Sare encoded uniformly as the   input hidden vector s , sand are introduced   into the first layer of the transformer .   Below , we use one layer as an example to intro-   duce our calculation steps .   A= softmax / parenleftigg   QK√   d / parenrightigg   V ( 1 )   Q = SW , K = SW,(2 )   V = SW ( 3 )   where S∈Ris the i - th layer ’s output ,   W , W , W∈Rare projection matri-   ces , d denotes the dimension of embeddings and   is set to 384 . |S| denotes the length of the input   sequence . Subsequently , we incorporate aspect informa-   tion into the model . As aspects are closely re-   lated to both users and items , we modify the in-   ternal mask structure of the model and combine the   user ’s aspects and ID information through a self-   attention mechanism . Not only does this strategy   account for the uniqueness of the ID when mod-   eling users , but also increase the personalization   of the user ’s interactions with the item . Specifi-   cally , the same user may have different points of   attention when interacting with different items . As   illustrated in Figure 2 , we make the information   of the first four positions attend to each other , be-   cause the first two positions encode the unique   user identifier , while the third and fourth positions   encapsulate the personalized aspects of the user ’s   preferences . The underlying rationale for selecting   these positions is to facilitate the attention mech-   anism in capturing the interactions between users   and products , ultimately enhancing the model ’s ac-   curacy . At this point , our final input is as follows :   [ U , V , A , A , s , s , t , . . . , t ] . After   including the location [ P , P , P , . . . , P ] , where   |s|is the length of the input , the final input becomes   [ H , H , H , . . . , H ] .   For the two different information of ID and as-   pects , we use them jointly to represent the user   and item . We use the self - attention mechanism   to combine these two different semantic informa-   tion , however , we found that it causes the final ID   embedding matrix to be very close to the word   embedding matrix , resulting in the loss of unique   ID information and high duplication in generated   sentences . To address this problem , we adopt the   strategy from previous research ( Geng et al . , 2022 )   that only uses an ID to generate texts , and compares   the generated text with the real text to compute the   lossL. To a certain extent , this method preserves   the unique ID information in the process of com-   bining aspects , thereby reducing the problem of   repetitive sentences .   L=/summationdisplay1   |t|/summationdisplay−logH ( 4 )   where Tdenotes the training set . gdenotes   that only use the hidden vector of the position H   to generate the i - th word , i∈1,2 ... , t.   4.5 Rating Prediction   We utilized the two positions of the final layer ( de-   noted as H ) as the input . To combine the infor-55mation of the ID and the hidden vector H , we   employed a multi - layer perceptron ( MLP ) to map   the input into a scalar . The loss function used in   this model is the Root Mean Square Error ( RMSE )   function .   r= ReLU ( [ H , u , v]W)W ( 5 )   L=1   |T |/summationdisplay(r−ˆr)(6 )   where where W∈R , W∈Rare   weight parameters , ris the ground - truth rating .   4.6 Explanation Generation   We adopt an auto - regressive methodology for word   generation , whereby words are produced sequen-   tially to form a coherent interpretation text . Specif-   ically , we employ a greedy decoding strategy ,   wherein the model selects the word with the high-   est likelihood to sample at each time step . The   model predicts the subsequent hidden vector based   on the previously generated one , thereby ensuring   the preservation of context throughout the entire   generation process .   e= softmax ( WH+b ) ( 7 )   where W∈Randb∈Rare weight   parameters . The vector erepresents the probability   distribution over the vocabulary V .   4.6.1 Aspect Discriminator   To increase the probability that the selected aspects   appear in explanation generation . We use the pre-   vious method ( Chen et al . , 2019b ) and adapt it to   our task . We represent τas the aspects that interest   this user , τ∈R. If the generated word at time   tis an aspect , then τis 1 . Otherwise , it is 0 . The   loss function is as follows :   L=1   |T |/summationdisplay1   |t|/summationdisplay(−τloge)(8 )   4.6.2 Text Generation   We propose a mask mechanism that allows for the   efficient integration of ID , aspects , and retrieved   sentence information into the hidden vector of the   Beginning of Sentence ( BOS ) position . At each   time step , the word hidden vector is transformed   into a vocabulary probability through a matrix , and   the word with the highest probability is selected   via the Greedy algorithm . The generation process   terminates when the predicted word is the End of   Sentence ( EOS ) marker . To ensure that the gener-   ated text adheres to a specific length , we employ a   padding and truncation strategy . When the number   of generated words falls short of the set length , we   fill in the remaining positions with a padding token   ( PAD ) . Conversely , when the number of generated   words exceeds the set length , we truncate the later   words . Here we use the Negative log - likelihood   loss as a generated text L. This loss function en-   sures the similarity between the generated words   and the ground truth ones .   L=1   |T |/summationdisplay1   |t|/summationdisplay−loge(9 )   where Tdenotes the training set . gdenotes the   utilization of the 6+tposition hidden vector to gen-   erate the t - th word , t∈1,2 ... , t.6represents the   initial first six positions vector information before   the BOS , and trepresents the current moment .   4.7 Multi - Task Learning   We aggregate losses to form the final objective func-   tion of our multi - task learning framework . The   objective function is defined as :   L = plL+λL+glL+alL+λ∥Θ∥(10 )   where Lrepresents the loss function of text   generation and Lis the loss function for context   prediction , with plandglas their weights , respec-   tively . Lis the loss function for aspect discrimina-   tor and alis its weights . Θcontains all the neural   parameters .   5 Experiments   5.1 Datasets   We performed experiments on three datasets ,   namely Amazon ( cell phones ) , Yelp ( restaurants ) ,   and TripAdvisor ( hotels ) ( Li et al . , 2020b ) . We56   filtered out users with fewer than 5 comments and   re - divided the dataset into three sub - datasets in   the ratio of 8:1:1 . The details of the datasets are   shown in Table 1 . We use an aspects extraction tool   ( Zhang et al . , 2014 ) to extract the aspects in each   review and correspond it to the respective review .   5.2 Evaluation Metrics   For rating prediction , in order to evaluate the rec-   ommendation performance , we employ two com-   monly used indicators : Root Mean Square Error   ( RMSE ) and Mean Absolute Error ( MAE ) , which   measure the deviation between the predicted ratings   rand the ground truth ratings r. For generated   text , we adopt a variety of indicators that consider   the quality of the generated text from different lev-   els . BLEU ( Papineni et al . , 2002 ) , ROUGE ( Lin ,   2004 ) and BERTscore ( Reimers and Gurevych ,   2019 ) are commonly used metrics in natural lan-   guage generation tasks . BLEU - N ( N=1,4 ) mainly   counts on the N - grams . R2 - P , R2 - R , R2 - F , RL - P ,   RL - R and RL - F denote Precision , Recall and F1   of ROUGE-2 and ROUGE - L. BERT - S represents   similarity scores using contextual embeddings to   calculate . They are employed to objectively eval-   uate the similarity between the generated text and   the targeted content .   5.3 Baseline Methods   5.3.1 Prediction   The performance in terms of accuracy of rating   prediction is compared with two types of methods :   Machine Learning and Deep Learning :   •Deep learning models : NARRE ( Chen et al . ,   2018 ) is a popular type of neural network for text-   based tasks . PETER ( Li et al . , 2021b ) and NRT   ( Li et al . , 2017 ) are deep learning models that use   review text for prediction and explanation at the   same time .   •Factorization methods : PMF ( Salakhutdinov andMnih , 2007 ) is a matrix factorization method that   uses latent factors to represent users and SVD++   ( Koren , 2008 ) leverages a user ’s interacted items   to enhance the latent factors .   5.3.2 Explainability   To evaluate the performance of explainability ,   we compare against three explanation methods ,   namely CAML ( Chen et al . , 2019b ) and ReXPlug   ( Hada et al . , 2021 ) and NRT and PETER .   •ReXPlug uses GPT-2 to generate texts and is   capable of rating prediction .   •CAML uses users ’ historical reviews to repre-   sent users and uses co - attention mechanisms to   pick the most relevant reviews and concepts and   combine these concepts to generate text .   •NRT is an advanced deep learning method for   explanation tasks . As a generative method , NRT   mainly generates explanations based on predicted   ratings and the distribution of words in tips .   •PETER is a powerful model improved by a trans-   former . This model effectively integrates the ID   in the transformer and combines this ID informa-   tion as the starting vector to generate text .   5.4 Reproducibility   We conduct experiments by randomly splitting the   dataset into a training set ( 80 % ) , validation set   ( 10 % ) , and test set ( 10 % ) . The baselines are tuned   by following the corresponding papers to ensure   the best results . The embedded vector dimension   is 384 and the value yielded superior performance   after conducting a grid search within the range of   [ 128 , 256 , 384 , 512 , 768 , 1024 ] . The maximum   length of the generated sentence is set to 15 - 20 .   The weight of the rating prediction ( pl ) is set to   0.2 , and the weight of the λandalis set to either   0.8 or 0.05 . For the explanation task , the param-   eterglis adjusted to 1.0 and is initialized using   the Xavier method ( Glorot and Bengio , 2010 ) . The   models are optimized using the Adam optimizer   with a learning rate of 10and L2 regularization   of10 . When the model reaches the minimum   loss in a certain epoch , the learning rate will be   changed at that time and multiplied by 0.25 . When   the total loss of continuous three epochs has not de-   creased , the training process will terminate . More   implementation details can be found on github.57   5.5 Explainability Study   Explainability results : Table 3 shows that our   proposed ERRA method consistently outperforms   the baselines in terms of BLEU and ROUGE on   different datasets . For instance , take BLEU as an   example , our method demonstrates the largest im-   provement on the TripAdvisor dataset . It is likely   due to the smaller size of the dataset and the rela-   tively short length of the reviews , which allows for   additional information from the retrieved sentences   and aspects to supplement the generated sentences ,   leading to an enhancement in their richness and   accuracy . In contrast , the increase in BLEU on the   Yelp dataset is relatively small . It is due to the large   size of the Yelp dataset , which allows the model   to be trained on a vast amount of data . The GPT   ( Brown et al . , 2020 ) series also prove this case ,   large amounts of data can train the model well , re-   sulting in our retrieval not having as obvious an   improvement compared to other datasets .   Similarly , when compared with NRT and PE - TER , our model consistently outperforms them in   all metrics . Whether it is in terms of the fluency of   the sentence , the richness of the text , or the consis-   tency with the real label , our model has achieved   excellent results .   Case study : We take three cases generated from   three datasets by NRT , PETER , and ERRA method   as examples . Table 4 shows that ERRA model can   predict keywords , which are both closer to the orig-   inal text and match the consumers ’ opinions , gener-   ating better explanations compared to the baseline .   While the baseline model always generates state-   ments and explanations that are not specific and   detailed enough , our model can generate personal-   ized , targeted text , such as the battery does n’t last   long in Case 2 and excellent ! The food here is very   delicious ! in Case 3 . This either is the same as or   similar to the ground truth .   Human evaluation : We also evaluate the model ’s   usefulness in generated sentences via the fluency   evaluation experiment , which is done by human   judgment . We randomly selected 1000 samples   and invited 10 annotators to assign scores . Five58   points mean very satisfied , and 1 point means very   bad . Table 5 reports the human evaluation results .   Kappa ( Li et al . , 2019 ) is an indicator for measuring   classification accuracy . Results demonstrate that   our model outperforms the other three methods on   fluency and Kappa metrics .   5.6 Accuracy of Prediction   The evaluation result of prediction accuracy is   shown in Table 2 . As we can see , it shows that our   method consistently outperforms baseline methods   including PMF , NRT , and PETER in RMSE and   MSE for all datasets . We mainly compare the per-   formance of our model with the PETER model ,   which is a state - of - the - art method . Our model   demonstrates a significant improvement over the   baseline methods on the TripAdvisor dataset . We   attribute this improvement to the way we model   users . By taking aspects into consideration , our   model is capable of accurately modeling users .   And this in turn can generate more accurate pre-   dictions . As shown in Table 2 , ERRA ’s predictive   indicator is the best result on each dataset .   5.7 Ablation Analysis   In order to investigate the contribution of individ-   ual modules in our proposed model , we performed   ablation studies by removing the retrieval enhance-   ment and aspect module denoted as " ERRA - R "   and " ERRA - A " , From Figure 3(a ) , we can see that   the retrieval module plays a crucial role in enhanc-   ing the performance of the explanation generation   task . Specifically , for the Amazon and TripAdvisor   datasets , the difference between " ERRA - R " and   ERRA is the largest for explanation generation ,   while showing mediocrity in the prediction task .   Additionally , we also evaluated the impact of   the aspect enhancement module on performance .   Without this key module , discernible degradation   can be observed in both the prediction and expla-   nation tasks , which is shown in Figure 3(b ) . This   can be attributed to the diverse attention points of   individual users . The aspects can more accurately   represent the user ’s preference , thus making the   prediction more accurate and the generated text   more personalized .   6 Conclusion   In this paper , we propose a novel model , called   ERRA , that integrates personalized aspect selec-   tion and retrieval enhancement for prediction and   explanation tasks . To address the issue of incorrect   embedding induced by data sparsity , we incorpo-   rate personalized aspect information and rich re-   view knowledge corpus into our model . Experimen-   tal results demonstrate that our approach is highly   effective compared with state - of - the - art baselines   on both the accuracy of recommendations and the   quality of corresponding explanations .   7 Limitation   Despite the promising results obtained in our   model , there are still several areas for improve-   ment . Firstly , when dealing with a large corpus , the   online retrieval function becomes challenging as59it requires a significant amount of computational   resources and time . Additionally , creating a vec-   torized corpus dynamically every time becomes   difficult . Secondly , the process of collecting a large   number of reviews from users raises privacy con-   cerns . The collection of data , especially from pri-   vate and non - public sources , may pose difficulties .   8 Acknowledgments   The authors thank all the anonymous reviewers for   their valuable comments and constructive feedback .   The authors acknowledge financial support from   the National Natural Science Foundation of China   ( Grant Nos . 62276171 and 62072311 ) , Shenzhen   Fundamental Research - General Project ( Grant Nos .   JCYJ20190808162601658 , 20220811155803001 ,   20210324094402008 and 20200814105901001 ) ,   CCF - Baidu Open Fund ( Grant No . OF2022028 ) ,   and Swiftlet Fund Fintech funding . Hao Liao is the   corresponding author .   References606162ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Not applicable . Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Not applicable . Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Not applicable . Left blank .   C / squareDid you run computational experiments ?   5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   563 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   5   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   5   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   5.5   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.64