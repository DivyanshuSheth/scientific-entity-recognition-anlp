  Yuxin Heand Buzhou TangDepartment of Computer Science , Harbin Institute of Technology , Shenzhen , ChinaPeng Cheng Laboratory , Shenzhen , China   21S051047@stu.hit.edu.cn   tangbuzhou@gmail.com   Abstract   Recently , joint recognition of flat , nested and   discontinuous entities has received increasing   attention . Motivated by the observation that   the target output of NER is essentially a set of   sequences , we propose a novel entity set gener-   ation framework for general NER scenes in this   paper . Different from sequence - to - sequence   NER methods , our method does not force the   entities to be generated in a predefined order   and can get rid of the problem of error propa-   gation and inefficient decoding . Distinguished   from the set - prediction NER framework , our   method treats each entity as a sequence and is   capable of recognizing discontinuous mentions .   Given an input sentence , the model first en-   codes the sentence in word - level and detects po-   tential entity mentions based on the encoder ’s   output , then reconstructs entity mentions from   the detected entity heads in parallel . To let   the encoder of our model capture better right-   to - left semantic structure , we also propose an   auxiliary Inverse Generation Training task . Ex-   tensive experiments show that our model ( w/o .   Inverse Generation Training ) outperforms state-   of - the - art generative NER models by a large   margin on two discontinuous NER datasets ,   two nested NER datasets and one flat NER   dataset . Besides , the auxiliary Inverse Genera-   tion Training task is found to further improve   the model ’s performance on the five datasets .   1 Introduction   Named entity recognition ( NER ) is a fundamental   task in the field of information extraction and has   played an important role in the development of   natural language processing . There exist three well-   studied subtasks of NER , i.e. flat NER , nested NER   and discontinuous NER , as illustrated in Figure 1 .   Recently , researchers have grown more inter-   est in tackling the three subtasks jointly , which   we refer to as general NER ( Li et al . , 2021 ; DaiFigure 1 : Examples of the discontinuous / nested / flat   NER . It can be observed that entities may overlap with   each other in general NER scenes and these overlapped   entities are inherently unordered . Hence , instead of gen-   erating entities sequentially in a predefined order , it is   more suitable to generate the set of entities concurrently .   et al . , 2020 ; Yan et al . , 2021 ) . Existing frame-   works for general NER fall into three categories :   ( 1 ) span - based models ; ( 2 ) sequence - to - sequence   ( seq2seq ) models ; ( 3 ) models based on other tech-   niques like hyper - graphs and shift - reduce parsers .   Among them , seq2seq models ( Fei et al . , 2021 ; Yan   et al . , 2021 ) have demonstrated SOTA performance .   However , they organize target entities into a single   sequence according to a predetermined order . This   formulation violates the fact that the target enti-   ties are inherently a unordered set , and introduces   an incorrect bias ( entity - order confounder ) to the   model ( Tan et al . , 2021 ; Zhang et al . , 2022 ) . In   addition , the conduct of generating target entities   sequentially suffers from two negative side effects :   ( 1 ) Low inference speed ; ( 2 ) Error propagation , i.e.   previous errors will result in a misleading context   for current generation step .   In this paper , we abandon the linear design of tar-   get entity sequence adopted by previous generative   NER methods , and come up with a novel Entity Set   Generation framework , SetGNER . Given an input   sentence , the framework first encodes the sentence3074and detects potential entity mentions based on the   encoder ’s output , then utilizes detected entity heads   as a set of initial sequences to reconstruct the set   of target entities . Note that , this procedure is simi-   lar to the way human beings perform NER . While   reading , a human attends to potential mentions of   entities subconsciously . When encountering a plau-   sible start of entities , the mind will activate a cluster   of neurons for it ( Kemmerer , 2015 ) , which consis-   tently collects words related to it until all entities   starting with it are recollected . These clusters of   neurons just function as a distributed decoder that   generates a set of entities in parallel .   The distributed nature of our decoding scheme   also brings remarkable inference speed - up . With   a decoding time complexity of O(NL ) , where N   is the sentence length and Lis the average entity   length , our model is efficient enough for both of-   fline and online applications .   To correctly recognize the boundary of an entity ,   right - to - left semantic structure is as important as   left - to - right semantic structure , which is ignored   by previous generative NER research . Inspired by   this , we additionally propose an auxiliary learning   task — Inverse Generation Training . Guided by the   inverse generation loss , the encoder can get more   familiar with the right - to - left semantic structure of   entity mentions , making it easier for the decoder to   generate complete entity mentions .   Note that , our work is different from previous   work proposed to address the problems faced with   generative NER models . In Tan et al . ( 2021 ) , a   sequence - to - set network is proposed to predict the   set of target entities in a sentence . However , it as-   sumes that each entity is a span and can not handle   the recognition of discontinuous mentions . In con-   trast , our model treats each entity as a sequence and   can naturally tackle the discontinuous mentions .   Zhang et al . ( 2022 ) analyze two kinds of incorrect   bias in the seq2seq NER models , i.e. pre - context   confounder ( when generating a word of an entity ,   a model can be affected by pre - generated words   that has no causal relation with the word to be gen-   erated ) , entity - order confounder , and propose two   data augmentation methods to address them . How-   ever , their model is still trained to generate entities   sequentially after all .   To sum up , our main contributions include :   •We propose SetGNER , a novel Entity Set Gen-   eration framework for general NER scenes .   Distinct from seq2seq models , it effectivelygets rid of the entity - order confounder and   error propagation caused by linearization as-   sumption , and brings high inference speed - up .   •We also come up with a novel auxiliary learn-   ing task — Inverse Generation Training , to   help the encoder of our model capture better   right - to - left semantic structure .   •Experiment results show that our model out-   performs SOTA seq2seq NER models by   a large margin on two discontinuous NER   datasets , two nested NER datasets and one   flat NER dataset , while being 3 times faster   than SOTA seq2seq NER models .   2 Problem Formulation   We uniformly formulate the task of recognizing flat   / nested / discontinuous entities as a pointer - based   Entity Set Generation problem .   A pointer is used for copying a source word or a   class tag or a special tag into target sequence . There   are three special tags : ⟨∅⟩ , indicating no - entity-   found ; ⟨/Cþrcle⟩ , indicating fragment of entity is found ;   ⟨/s⟩ , indicating the end of the generated sequence .   Suppose the number of entity classes is C. We   first define the pointer to class tag T(0≤i < C )   asPtr(T ) = i , the pointers to special tags ⟨∅⟩ ,   ⟨/Cþrcle⟩,⟨/s⟩asPtr(⟨∅⟩ ) = C , Ptr(⟨/Cþrcle⟩ ) = C+ 1 ,   Ptr(⟨/s⟩ ) = C+ 2 .   Given a sentence with Nwords [ w , ... , w ] ,   the pointer to word wis then defined as :   Ptr(w ) = C+ 3 + j   To simplify annotation , we generalize the Ptr ( · )   operation to sequences of words / tags , i.e.   Ptr ( [ · ] ) = [ Ptr ( · ) ] .   The target output is the set of entity pointer se-   quences / braceleftbig   Ptr(e)/bracerightbig , where Mis the number of   entities in the input sentence , eis the i - th entity   consisting of lwords w , w , ... , w. And the   pointer sequence for entity eis defined as :   Ptr(e ) = Ptr([w , w , ... , w , T(e),⟨/s⟩ ] )   where T(e)is the class tag of e,⟨/s⟩indicates the   end of generated sequence .   Under this definition , each head word of entity is   associated with a target sequence or multiple target   sequences ( when there are multiple entities starting   with the head word ) . We additionally appoint a tar-   get sequence Ptr ( [ w,⟨∅⟩,⟨/s⟩])to word w , ifw3075does not belong to any entity ; or Ptr ( [ w,⟨/Cþrcle⟩,⟨/s⟩ ] )   ifwis not an head word but a fragment of entity .   3 Method   As shown in Figure 2 , the proposed model consists   of a word - level encoder , a mention detector and   a parallel generator based on copying mechanism .   We train it with a combination of generation loss ,   occurrence detection loss and entity part classifi-   cation loss , and leverage an adaptive beam - search   scheme during inference .   3.1 Word - level Encoder   The input sentence [ w , w , ... , w]is first to-   kenized into [ ⟨s⟩ , t , t , ... , t,⟨/s⟩]using BPE   ( Sennrich et al . , 2016 ) tokenizer , where Nis the   length of content tokens . We denote the tokenized   sentence as X.   We then calculate the contextual representation   of the tokenized sentence using pre - trained BART   ( Lewis et al . , 2020 ) encoder :   H = Encoder ( X ) ( 1 )   where H∈Randdis the dimension of our   model .   To obtain the word - level representation of each   word , we max - pool the contextual representations   of its first and last tokens :   r = MaxPool ( H[start],H[end ] ) ( 2 )   R= [ r ] ( 3 )   where ris the representation of w , startand   endare the indexes of the first and last tokens   ofwrespectively , R∈Ris the word - level   representation matrix .   3.2 Mention Detector   Occurrence Detection   Since there may be multiple entities sharing the   same head word , we have to predict how many   entities have occurred with the head word . This   is achieved by conducting classification over each   word , where the label of a word is the number of   entities starting with it .   Concretely , we first transforms the word - level   representation matrix into feature matrix V :   V = ReLU ( WR+b ) ( 4)Then utilize a softmax layer to predict the number   of occurrences :   P = softmax ( WV+b ) ( 5 )   The loss function for occurrence detection is   defined as follows .   L=−/summationdisplay / summationdisplay1(y = j)log(P )   + 1(y̸=j)log(1−P)(6 )   where O is the maximum number of entities   starting with the same head word in the dataset .   Note that , casting the occurrence prediction prob-   lem as a regression task may be a better choice and   we leave it for future research .   Entity Part Classification   Our model also detects potential parts of entities   by predicting whether a word is the head / tail of   any entity or not , and whether a word belongs to   any entity or not . This is essentially a multi - label   classification task , which can be handled by three   binary classifiers as follows .   p = sigmoid ( WV+b ) ( 7 )   p = sigmoid ( WV+b ) ( 8)   p = sigmoid ( WV+b ) ( 9 )   where h , tandbstand for “ entity head ” , “ entity   tail ” and “ belonging to entity ” respectively .   We utilize binary cross - entropy loss to optimize   this module :   L=−/summationdisplay / summationdisplayylog(p )   + ( 1−y)log(1−p)(10 )   3.3 Parallel Generation   The backbone of our decoder is the one proposed   by Yan et al . ( 2021 ) , which is based on BART   decoder and equipped with copying mechanism .   Boundary - guided Initialization   During inference , the decoder first initiates the set   of target sequences with pointers to detected entity   heads . Concretely , the target sequence correspond-   ing to entity head wis initialized as :   ˆy:= [ Ptr(w ) ] , i∈Ω ( 11 )   Ω={i|p≥0.5 } ( 12)3076   where Ω is the indexes of detected entity heads .   Such initialization provides explicit left bound-   aries for parallel generation , avoiding the negative   influence of generation errors that may occur ahead   of generating current entities .   Adaptive Position Embeddings   To guide our decoder with the position information   of the sentence ’s region that each target sequence   should focus on , we devise adaptive position em-   beddings for the decoder . Concretely , the position   embedding for the t - thpointer in ˆyis defined as   PosEmb ( ˆy ) = DecoderPosEmb [ i·τ+t](13 )   where τis a hyper - parameter controlling the posi-   tion interval , which intuitively represents the aver-   age number of tokens that each word holds .   Decoding   Since the target sequences are made up of pointers ,   they should be converted into words / tags before   decoding . We denote this operation as Retrieve ( · ) ,   which is defined as :   Retrieve ( ˆy ) =       T , ˆy < C   ⟨∅⟩/⟨/Cþrcle⟩/⟨/s⟩ , C≤ˆy≤C+ 2   w , ˆy≥C+ 3   The decoder then calculates the hidden state of   current time step for each target sequence via cross-   attention over encoder output and self - attention   over hidden states of previous time steps .   d = Decoder ( H;Retrieve ( ˆy ) ) ( 14 )   The probability distribution of ˆyis calculatevia copying mechanism as follows :   where Tis the learnable embeddings of class   tags,/tildewideRis the combination of word - level repre-   sentations Rand word - level embeddings E(trans-   formed from BART token embeddings ) .   We train the decoder in a teacher - forcing manner   using all ground - truth target sequences defined in   Section 2 . The generation loss is as follows :   L=−/summationdisplaylogP(y|X;θ ) ( 15 )   Note that , there can be multiple ground - truth se-   quences associated with w , in which case their   losses are all summed up .   Adaptive Beam - search   Since there may exist multiple entities starting with   the same head word , we devise an adaptive beam-   search mechanism to generate multiple entity se-   quences sharing the same start . Concretely , we first   utilize standard beam - search to generate a fixed   number of candidate sequences for each detected   head word wand then select the top- Kcandi-   date sequences as the entity sequences generated   fromw . Khere is the predicted number of entity   occurrences corresponding to w , i.e. ,   K= arg max P ( 16)3077   3.4 Training   Inverse Generation Training   Since the forward generation task is biased towards   the left - to - right semantic structure , we propose to   help our model learn better right - to - left semantic   structure via inverse generation training . To realize   this , we instantiate an auxiliary decoder and train it   to generate from entity tail to entity head . For the   instance “ Swollen , burning feet and ankles . ” , its   inverse target sequences are defined as in Figure 3 .   And the inverse generation loss is calculated over   all inverse target sequences ˜y :   L=−/summationdisplaylogP(˜y|X;θ ) ( 17 )   Note that , the auxiliary decoder is discarded after   training phase .   Joint Learning   In each optimization step , we alternately train our   model with the forward generation loss and inverse   generation loss , together with the occurrence detec-   tion loss and entity part classification loss :   L = L+L+L ( 18 )   where Lmeans alternating between forward   generation loss and inverse generation loss .   4 Experiments   4.1 Datasets and Evaluation Details   We experiment on two discontinuous NER datasets   CADEC and ShARe13 , two nested NER datasets   ACE04 and ACE05 , and one flat NER dataset   CoNLL03 . Please refer to Appendix A for details   and statistics of the five datasets . For CADEC and ShARe13 , we follow the same   data split used in Dai et al . ( 2020 ) ; Yan et al . ( 2021 ) .   For ACE04 and ACE05 , we use the same data split   as in Muis and Lu ( 2017 ) ; Yu et al . ( 2020 ) . For   CoNLL03 , we follow Yu et al . ( 2020 ) ; Yan et al .   ( 2021 ) to concatenate the train and development   sets . Strict evaluation metrics is applied , where an   entity is confirmed correct only if its boundary and   type label are both recognized correctly . Precision   ( P ) , Recall ( R ) and Micro F1 score ( F1 ) are reported   in the results . We report the average performance   on 3 random seeds .   4.2 Implementation Details   We initialize model parameters from pre - trained   BART - Large , which consists of 12 transformer   blocks for encoder and 12 transformer blocks for   decoder . The dimension size of the model is 1024 .   We use AdamW optimizer with different learning   rates for encoder and decoder . Linear learning rate   scheduling is employed . We fix the maximum num-   ber of words rather than the number of sentences   in each batch , since the memory occupied by our   model is determined by the number of words in   a batch . Details of hyper - parameter tuning and   settings are included in Appendix B.   4.3 Compared Methods   We compare our model principally with SOTA gen-   erative NER models and the set prediction NER   model . See Section 5 for an introduction of them .   Performances of SOTA discriminative NER mod-   els on the five datasets are also listed for reference .   See Appendix D for an introduction of them .   4.4 Main Results   Discontinuous NER Table 1 shows the overall   results of SetGNER ’s performance on discontin-   uous NER datasets . SetGNER outperforms the   SOTA generative model ( Fei et al . , 2021 ) by +0.75   F1 and +0.33 F1 on CADEC and ShARe13 respec-   tively , demonstrating the superiority of SetGNER   on the task of discontinuous NER . After introduc-   ing Inverse Generation Training into our frame-   work , the F1 scores of SetGNER further increase   by +0.39 and +0.28 .   Nested NER The results on nested NER tasks   are shown in Table 2 . SetGNER yields +0.26 and   +0.45 F1 gains over the SOTA generative baseline   ( Lu et al . , 2022 ) on ACE04 and ACE05 . Compared   with the SOTA set prediction model , SetGNER per-3078forms better on ACE04 ( + 0.21 F1 ) while performs   competitively on ACE05 . This demonstrates the ef-   fectiveness of SetGNER on tackling nested entities .   Besides , Inverse Generation Training also boosts   the model ’s performance on the two datasets .   Flat NER Table 3 shows the results of our   model ’s performance on the CoNLL03 dataset . We   can see that SetGNER is competitive with SOTA   models on CoNLL03 , verifying that our model can   identify flat entities with high precision and high   recall . When leveraging Inverse Generation Train-   ing , our model achieves SOTA performance on this   NER benchmark .   4.5 Ablation Study   We conduct ablation study on CADEC and ACE04   to verify the effectiveness of different components3079   of SetGNER . The results are shown in Table 4 . Af-   ter replacing the word - level representations with   the token - level representations and using token-   level pointers , the F1 scores on the two datasets   drop by 0.56 and 0.24 respectively , verifying the   advantage of word - level representations and word-   level pointers . After replacing the adaptive posi-   tion embeddings with vanilla position embeddings ,   our model can not function properly . This demon-   strates the necessity of guiding our decoder with   the position information of the sentence ’s region   that each target sequence should focus on . After   removing the entity part classification module   and generating from all words rather than detected   head words , the F1 - score of SetGNER drops by   0.40 and 0.21 on the two datasets , which means it   is helpful to detect different parts of entities and   generate selectively . Without the occurrence de-   tection module ( the adaptive beam - search degrades   to standard beam - search ) , SetGNER can only gen-   erate one entity from each detected head word , and   the recall of SetGNER drops significantly on the   two datasets .   4.6 Analysis   We conduct a series of experiments to analyze the   advantage of our Entity Set Generation model over   seq2seq NER models and the benefit of Inverse   Generation Training .   4.6.1 Recognizing Overlapped Entities   Since the sequential order assumed by seq2seq   NER models is most untenable when faced with   overlapped entities , we compare SetGNER with   a seq2seq baseline model in terms of the ability   to recognize overlapped entities . The experiment   is conducted on the test sets of two discontinuous   NER datasets and two nested NER datasets , where   the overlaps between entities are common . We   choose ( Yan et al . , 2021 ) as the baseline , since its   overall performance on the four datasets is the best   among SOTA seq2seq NER models . The results   are shown in Table 5 . We can see that SetGNER   greatly boost the Recall of overlapped entities and   the increase is several times higher than the over-   all increase of Recall on each dataset . This means   Entity Set Generation is superior to seq2seq on the   recognition of overlapped entities.30804.6.2 Removing Entity Order Confounder   and Error Propagation   We conduct case study on the CADEC dataset   to verify that SetGNER ( w/o Inverse Generation   Training ) can overcome the Entity Order Con-   founder and the Error Propagation problem that   seq2seq NER models suffer from . Figure 4 illus-   trates two cases from the dataset . In the first case ,   both SetGNER and the seq2seq baseline model   ( Yan et al . , 2021 ) can correctly generate all the enti-   ties “ bad pains in hands ” , “ bad pains in arms ” and   “ bad pains in shoulders ” . However , when we shuffle   the order of “ hands ” , “ arms ” and “ shoulders ” in the   sentence , the seq2seq NER model fails to generate   the entity “ bad pains in hands ” . This means the   seq2seq NER model is biased towards the original   entity order that occurs more frequently in the train-   ing data . And SetGNER can get rid of this incor-   rect bias . In the second case , the seq2seq baseline   model first generates the wrong mention “ Burning   sensations in neck shoulders ” , which consequently   disturbs the generation of “ Burning sensations in   shoulders ” . In contrast , such a phenomenon of   error propagation does not occur in our model .   4.6.3 Capturing Right - boundaries   We hypothesize that Inverse Generation Training   is effective in teaching our encoder the right - to-   left semantic structure . To verify this , we com-   pare the ability of SetGNER to caputure right-   boundaries before and after Inverse Generation   Training . Concretely , right - boundary accuracy ( #   correct boundaries / # correct left - boundaries ) and   right - boundary recall ( # correct right - boundaries /   # golden right - boundaries ) are measured on four   benchmark datasets . As shown in Table 6 , after   Inverse Generation Training , the right - boundary   accuracy of SetGNER increases by +2.12 , +1.46 ,   +0.72 , + 1.01 and the right - boundary recall of Set-   GNER increases by +0.41 , +0.55 , +1.08 + 0.40 on   the four datasets respectively . This demonstrates   the effectiveness of Inverse Generation Training .   4.7 Inference Efficiency   We compare the inference speed of SetGNER with   the SOTA seq2seq NER model ( Yan et al . , 2021 )   and the SOTA seq2set NER model ( Tan et al . , 2021 )   on three datasets . For a fair comparison , we fix the   maximum number of source tokens in each batch   as 800 and ensure that the sentence sampling order   in each run is the same . As shown in Table 7 ,   SetGNER is about 3 times faster than the seq2seq   NER model , thanks to the distributed nature of our   design . However , SetGNER is still slower than the   seq2set NER model based on non - autoregression .   5 Related Work   Seq2seq NER Models Straková et al . ( 2019 ) pro-   pose to linearize BILOU labels ( Ratinov and Roth ,   2009 ) of source tokens into a target sequence . Their   linearization of BILOU labels follows a heuristic   rule , which may introduce incorrect model bias .   Athiwaratkun et al . ( 2020 ) propose an aug-   mented natural language output format for flat   NER , where the type tags of words are placed   along with the words to form a sentence - alike tar-   get sequence . Lu et al . ( 2022 ) represent different   information structures with a structured extraction   language and solve general information extraction   tasks with a unified text - to - structure generation   framework . Zhang et al . ( 2022 ) point out two kinds   of incorrect bias ( pre - context confounder , entity-   order confounder ) in the seq2seq NER models and   propose two data augmentation methods to address   them . However , their model is still trained to gen-   erate entities sequentially after all .   There also exist pointer - based target sequences .   Fei et al . ( 2021 ) train a LSTM from scratch to   generate the target sequence and devise a novel   memory - augmented pointer mechanism to encour-   age interactions between the current pointer and   the prior recognized entity mentions . Instead ,   Yan et al . ( 2021 ) combine pre - trained BART with   a delicately - designed copying mechanism and   achieve promising performance on a wide range of3081NER benchmarks . Our work inherits the copying   mechanism proposed in this work .   The seq2set NER model Tan et al . ( 2021 ) ob-   serve that nested NER is essentially an unordered   recognition task and propose to predict the set of   entity spans in one pass via a non - autoregressive   model . In contrast , our model treats each entity as a   sequence rather than a span , and is able to handling   discontinuous entity mentions .   6 Conclusion   We observe that existing generative NER models   suffer from the entity order confounder and faces   the problems of error propagation and slow infer-   ence speed . To address this , a novel Entity Set Gen-   eration framework for general NER is proposed in   this paper . We also propose to train our model with   an auxiliary Inverse Generation task that helps the   encoder learn the right - to - left semantic structure .   Experiments on five datasets prove the effective-   ness of our methods .   Limitations   Since both the encoding module and decoding mod-   ule of SetGNER work in the word - level , SetGNER   requires the input sentence to be tokenized into   words beforehand . However , for many language ,   e.g. Chinese and Japanese , how to conduct word   segmentation and whether it is necessary to do so   are still open questions . This limits the usage of   SetGNER . To adopt SetGNER to these language ,   further research is required .   Another limitation of SetGNER is that when   generating the set of entity sequences , there is not   interaction between target sequences starting with   different head words . This may limit the model ’s   performance when explicit information about other   entities is helpful for the recognition of the target   entity .   Last but not least , although SetGNER is about   3 times faster than the SOTA seq2seq NER model   ( Yan et al . , 2021 ) , it consumes more memory .   The maximum memory occupation of SetGNER is   about 2 times larger than that of ( Yan et al . , 2021 ) ,   when the maximum number of source tokens in   a batch is set as 800 . This should be taken into   account when deploying the model on machines   with small graphical memory . Acknowledgments   We thank the reviewers for their insightful com-   ments and valuable suggestions . This study is par-   tially supported by National Key R&D Program   of China ( 2021ZD0113402 ) , National Natural Sci-   ence Foundations of China ( 62276082 , U1813215   and 61876052 ) , National Natural Science Founda-   tion of Guangdong , China ( 2019A1515011158 ) ,   Major Key Project of PCL ( PCL2021A06 ) , Strate-   gic Emerging Industry Development Special Fund   of Shenzhen ( 20200821174109001 ) and Pilot   Project in 5 G + Health Application of Ministry of   Industry and Information Technology & National   Health Commission ( 5 G + Luohu Hospital Group :   an Attempt to New Health Management Styles of   Residents ) .   References30823083   A Profile of Datasets   CADEC(Karimi et al . , 2015 ) is a discontinuous   NER dataset with a corpus of adverse drug event .   It originally contains 5 entity type , but only anno-   tations of " ADE " entities are considered . Because   only ADEs include discontinuous entities .   ShARe13(Suominen et al . , 2013 ) is a discon-   tinuous NER dataset with a corpus of clinical notes   and contains annotations of disorder mentions .   ACE04and ACE05(Doddington et al . , 2004 ;   Walker et al . , 2006 ) are two nested NER datasetwith corpuses of newswire , broadcast news and   telephone conversations . Both of them contains 7   entity categories : “ PER ” , “ ORG ” , “ LOC ” , “ GEP ” ,   “ VEH ” , “ WEA ” and “ FAC ” .   CoNLL03Tjong Kim Sang and De Meulder   ( 2003 ) is a flat NER dataset with a news corpus and   has annotated 4 types of entities as “ PER ” , “ LOC ” ,   “ ORG ” and “ MISC ” .   Statistics of the five datasets are listed in Tabel   8 - 10 .   B Hyper - parameter Settings   We manually tune the hyper - parameters for each   dataset . Specifically , we trial different values of   each hyper - parameter within a bound and the hyper-   parameter value that results in the best performance   ( measured in F1 - score ) on the development set are   chosen . The search bound of each hyper - parameter   and the final hyper - parameter configuration are   shown in Table 11 .   C Sensitivity Analysis   The position interval for adaptive position em-   bedding , τ , and the beam size of adaptive beam   search , β , are two important hyper - parameters of   SetGNER . To analyze their influence on the per-   formance of SetGNER , we experiment with differ-   ent values of them and record the corresponding   F1 scores on four datasets ( CADEC , ShARe13 ,   ACE04 and ACE05 ) . As shown in Figure 5 , when   τ= 0(the adaptive position embedding degrades   to vanilla position embedding ) , the model can-   not work properly , demonstrating the necessity of   adaptive position embedding for SetGNER . When   τ≥1 , the change of τslightly affects the per-   formance of SetGNER by a margin of around   0.2∼0.4F1 on the four datasets . Figure 6 shows   the F1 scores of SetGNER with different beam   sizes . We can see that the performance of Set-   GNER reaches the peak when the beam size is   around 4∼6 , and does not further improve when   the beam size grows bigger .   DCompared Discriminative NER Models   D.1 Models for Discontinuous NER   Tang et al . ( 2018 ) use LSTM - CRF to recognize con-   tinuous and discontinuous adverse drug reaction   mentions . ( Dai et al . , 2020 ) is a transition - based3084   method that utilizes shift - reduce parsers to iden-   tify discontinuous entities . Wang et al . ( 2021b )   solve discontinuous NER via the maximal clique   discovery algorithm based on graph theory .   D.2 Models for Nested NER   Yu et al . ( 2020 ) formulate NER as the dependency   parsing task and solve it with TreeCRF . ( Li et al . ,   2020 ) is a method based on machine reading com-   prehension . Xu et al . ( 2021 ) treat named entity   recognition as multi - class classification of spans   and solve it with a multi - head self - attention mech-   anism . ( Shen et al . , 2021 ) is a two - stage entity   identifier , which first generates candidate spans and   then labels the boundary - adjusted span proposals   with the corresponding categories .   D.3 Models for Flat NER   Akbik et al . ( 2019 ) dynamically aggregate contex-   tualized embeddings of each encountered string   and use a pooling operation to distill a global word   representation from all contextualized instances .   Wang et al . ( 2021a ) use the input sentence as a   query to retrieve external contexts with a search en-   gine and concatenate the sentence with its external   contexts.3085