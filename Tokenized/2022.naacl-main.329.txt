  Yulong Li , Martin Franz , Md Arafat Sultan ,   Bhavani Iyer , Young - Suk LeeandAvirup SilIBM Research , IBM Research AI   { yulongl , franzm , bsiyer , ysuklee , avi}@us.ibm.com   arafat.sultan@ibm.com   Abstract   We present D.D ( Dense Retrieval   with Distillation- Enhanced Cross - Lingual   Representation ) , a new cross - lingual infor-   mation retrieval ( ) system trained using   multi - stage knowledge distillation ( ) .   The teacher of D.D relies on a highly   effective but computationally expensive   two - stage inference process consisting of   query translation and monolingual , while   the student , D.D , executes a single step . We teach D.D powerful   multilingual representations as well as   by optimizing two corresponding ob-   jectives . Learning useful representations of   non - English text from an English - only re-   triever is accomplished through a cross - lingual   token alignment algorithm that relies on the   representation capabilities of the underlying   multilingual encoders . In both in - domain   and zero - shot out - of - domain evaluation ,   D.D demonstrates far superior accuracy   over direct ﬁne - tuning with labeled data .   It is also the best single - model retriever on   the XOR - TyDi benchmark at the time of this   writing .   1 Introduction   Multilingual models are critical for the democra-   tization of AI . Cross - lingual information retrieval   ( ) ( Braschler et al . , 1999 ; Shakery and Zhai ,   2013 ; Jiang et al . , 2020 ; Asai et al . , 2021a ) , for ex-   ample , can ﬁnd relevant text in a high - resource lan-   guage such as English even when the query is posed   in a different , possibly low - resource , language . In   this work , we develop useful models for this   constrained , yet important , setting where a retrieval   corpus is available only in a single high - resource   language ( English in our experiments ) .   A straightforward solution to this problem could   use machine translation ( ) to translate the query   into English , and then perform English(Asaiet al . , 2021a ) . While such a two - stage process can   produce reasonably accurate predictions , an alter-   native end - to - end approach that can tackle the prob-   lem purely cross - lingually , i.e. , without involvingfor inference , would clearly be more efﬁcient   and cost - effective . Pre - trained multilingual masked   language models ( s ) such as multilingual   ( Devlin et al . , 2019 ) or -o a ( - )   ( Conneau et al . , 2020 ) can provide the foundation   for such a one - step solution , as simply ﬁne - tuning   a with labeled data would yield a cross-   lingual retriever ( Asai et al . , 2021b ) .   Here we ﬁrst run an empirical evaluation of these   two approaches on a public benchmark ( Asai   et al . , 2021a ) , which involves both in - domain and   zero - shot out - of - domain tests . We use Col   ( Khattab and Zaharia , 2020 ; Khattab et al . , 2021 ) —   a state - of - the - art ( ) neuralmodel that has   been shown to outperform other recent methods   such as ( Karpukhin et al . , 2020)—as our   architecture and -as the underlying for   both methods ( § 2 ) . Results indicate that the-   based solution can be vastly more effective than   direct+ ﬁne - tuning , with observed differ-   ences of 22.2–28.6 Recall@5k - tokens ( § 3 ) . Cru-   cially , the modular design of the former allows it to   leverage additional English - only training data for   itscomponent , providing signiﬁcant boosts to   its performance .   The above ﬁndings lead naturally to the cen-   tral research question of this paper : Can a high-   performance model be trained that can op-   erate without having to rely on ? To answer   the question , instead of viewing the - based   approach as a competing one , we propose to   leverage its strength via knowledge distillation   ( ) into an end - to - end model , which we   callD.D ( Dense Retrieval with Distillation-   Enhanced Cross - lingual Representation).(Hin-   ton et al . , 2014 ) is a powerful supervision technique   typically used to distill the knowledge of a large4428teacher model about some task into a smaller stu-   dent model ( Mukherjee and Awadallah , 2020 ; Turc   et al . , 2020 ) . Here we propose to use it in a slightly   different context , where the teacher and the student   retriever are identical in size , but the former has su-   perior performance simply due to utilizingout-   put and consequently operating in a high - resource   and low - difﬁculty monolingual environment .   We run two independentoperations ( § 2.2 ) .   One directly optimizes anobjective by utiliz-   ing labeled data : parallel questions ( English   and non - English ) and corresponding relevant and   non - relevant English passages . The teacher and the   student , D.D , are shown the English and non-   English versions of the questions , respectively ; the   training objective is for D.D to match the soft   query - passage relevance predictions of the teacher .   The secondtask is representation learning from   parallel text , where D.D learns to encode a   non - English text in a way that matches the teacher ’s   encoding of the aligned English text , at the token   level . The cross - lingual token alignments needed to   create the training data for this task are generated   using a greedy alignment process , which exploits   the multilingual representation capabilities of the   underlying encoders .   In our evaluation on the XOR - TyDi benchmark   ( Asai et al . , 2021a ) , D.D outperforms the   ﬁne - tuned Col baseline by 25.4 ( in - domain )   and 14.9 ( zero - shot ) Recall@5k - tokens , recovering   much of the performance loss from the - based   solution . It is also the best single - modelsystem   on the XOR - TyDi leaderboardat the time of this   writing . Ablation studies show that each of our twoprocesses contribute signiﬁcantly towards the   ﬁnal performance of D.D.   Our contributions can be summarized as follows :   ( 1)We present an empirical study of the effective-   ness of a method ( Col ) on cross-   lingualwith and without,(2)We propose a   novel end - to - end cross - lingual solution that uses   knowledge distillation to learn both improved text   representation and retrieval , ( 3)We demonstrate   with a new cross - lingual alignment algorithm that   distillation using parallel text can strongly augment   cross - lingualtraining , and ( 4)We achieve new   single - model results on XOR - TyDi.2 Method   Here we ﬁrst describe our basearchitecture   ( Col ) and then the proposed - based cross-   lingual training algorithms .   2.1 The Col Model   Col ( Khattab and Zaharia , 2020 ) employs a   transformer - based encoder to separately encode   the input query and document , followed by a lin-   ear compression layer . Each training instance is   a < q , d , d > triple , where qis a query , dis a   positive ( relevant ) document and dis a negative   ( non - relevant ) document . A relevance score S   for the pair ( q , d)is ﬁrst computed using Eq . 1 ,   whered∈{d , d}andEandEare the out-   put embeddings of query token qand document   tokend , respectively . For a given training triple ,   a cross - entropy loss is minimized for the softmax   overSandS.   S,:=maxE·E(1 )   For inference , the embeddings of all documents   are calculated a priori , while the query embeddings   and the relevance score are computed at runtime .   2.2 Knowledge Distillation   Our teacher and D.D are both Col   models that ﬁne - tune the same underlying mul-   tilingual for . The teacher is ﬁrst trained   with all - English triples using the procedure of   § 2.1 . The goal of the subsequenttraining is   to teach D.D to reproduce the behavior of   this teacher when it sees non - English translations   of the teacher ’s English questions .   We applyat two different stages of the   Col workﬂow : ( a ) relevance score computa-   tion ( Sin Eq . 1 ) , and ( b ) encoding ( e.g. ,E ) .   Figure 1 depicts ( a ) in detail , where training mini-   mizes the KLdivergence between the D.D ’s   and the teacher ’s output softmax distributions ( with   temperature ) over SandS.   Labeled training data for are scarce ,   whereas , being a more established area of re-   search , has produced a large amount of parallel text   over the years . We seek to exploit existing paral-   lel corpora in our secondtraining , where we   teach D.D to compute representations of non-   English texts that closely match the teacher ’s rep-   resentations of aligned English texts . Importantly ,   since Col computes a single vector for each4429   Algorithm 1 : Cross - lingual alignment .   individual input token ( i.e. , a vocabulary item )   and not for the entire input text , our algorithm must   support distillation at the token level .   To achieve this , we design an unsupervised   cross - lingual token alignment algorithm . Assuming   ( ne, ... ,ne)to be the ordered tuple of tokens in a   non - English text and ( e, ... ,e)the corresponding   tuple from the parallel English text , each iteration   of this algorithm greedily picks the next ( ne , e )   pair with the highest cosine similarity of their out-   put embeddings . Algorithm 1 implements this idea   by repositioning the teacher ’s tokens so that they   are position - wise aligned with the corresponding   D.D tokens . Note that the design choice of   ﬁne - tuning a common multilingual for the   teacher and the D.D , even though the former   is tasked with only handling English content , is key   for this algorithm as it relies on thes ’ multilin-   gual representation capabilities . See Appendix A.1   for details on our parallel corpora used for training .   In addition to cross - lingual alignment , we also   perform a similarprocedure in which both the   teacher and the D.D are shown the same En-   glish text . This step is useful because Col uses   a shared encoder for the query and the document ,   necessitating a student that is able to effectively   encode text from both English documents and non-   English queries .   Using the alignment information , we train   D.D by minimizing the Euclidean distance   between its representation of a token ( English or   non - English ) and the teacher ’s representation of   the corresponding English token . Figure 2 shows   theprocess for representation learning.4430   3 Experiments   3.1 Setup   Our primary dataset is XOR - TyDi ( Asai et al . ,   2021a ) , which contains examples in seven typo-   logically diverse languages : Arabic ( Ar ) , Bengali   ( Bn ) , Finnish ( Fi ) , Japanese ( Ja ) , Korean ( Ko ) ,   Russian ( Ru ) and Telugu ( Te ) . For standard in-   domain experiments , we use a train - dev - test split   of this dataset . There are 2,113 questions in the   test set . For zero - shot experiments , we use the   MKQA ( Longpre et al . , 2020 ) dataset for train-   ing and validation , and the following shared lan-   guages in the XOR - TyDi test set for evaluation :   Ar , Fi , Ja , Ko and Ru . Both training sets contain   English questions and their human translations in   the other languages , their short answers and corre-   sponding relevant ( positive ) and non - relevant ( neg-   ative ) Wikipedia snippets . Additionally , we use   training examples from the Natural Questions ( NQ )   dataset ( Kwiatkowski et al . , 2019 ) for English pre-   training of the baseline model . Further details on   data pre - processing and the ﬁnal training sets are   provided in Appendix A.1 .   The baseline used in our experiments is   Col with an underlying - , which   we iteratively ﬁne - tune ﬁrst on English and then   on cross - lingualtriples for optimal performance .   Our D.D model is initialized with the pa-   rameter weights of this baseline , and is further   ﬁne - tuned using the twoobjectives . The   teacher is a Col model ﬁne - tuned with only   English triples , as stated before . During evaluation ,   it is given machine - translated questions that come   with the XOR - TyDi dataset . Appendices A.1 and   A.2 contain additional details on the supervision   of these models and the optimal hyperparameter   conﬁgurations .   We evaluate using Recall at ttokens fort∈   { 2000,5000},i.e . , R@2kt and R@5kt ( Asai et al . ,   2021a ) , which compute the fraction of questions for   which the ground truth short answer is contained   within the top ttokens of the retrieved passages .   3.2 Evaluation   Table 1 compares the performance of our different   models . First , looking at the R@5kt results , we   observe that pre - training the baseline model with   Englishtriples from the NQ train set ( rows 2 ,   6 ) substantially boosts its performance in both in-   domain and zero - shot settings . However , it still un-   derperforms the+Englishpipeline ( rows 3 ,   7 ) by 28.6 and 22.2 points , respectively . By distill-   ing ﬁrst with the parallel corpus ( for representation   learning ) and then with thetriples ( for ) ,   D.D ( row 4 ) yields an improvement of 25.4   points over the baseline model in in - domain evalua-   tion , which , quite impressively , is within 3.2 points4431   of the teacher ’s score . A sizable gain of 14.9 points   is also observed in zero - shot evaluation ( row 8) .   Finally , the R@2kt numbers show a very similar   pattern .   Table 2 shows the performance ( R@5kt ) of   D.D and the baseline on each individual   language : the former outperforms the latter both   with and without target domain supervision , yield-   ing large gains across all languages . These results   demonstrate the robustness of our approach , which   stems from combining the individual strengths of , Englishandin a single model .   3.3 Leaderboard Submission   The D.D model trained on the XOR - TyDi   training set , shown in Table 1 row 4 , is the best   single - model retriever on the XOR - TyDi leader-   boardat the time of this writing . Since our parallel   corpus extraction process relies on in - house source   code that is not publicly available , we submitted to   the “ Systems using External APIs ” category . Cru-   cially , all other submitted systems under the Ex-   ternal APIs category rely onat decoding time ,   avoiding which is one of the primary goals of our   work . We also created parallel corpora purely from   public available sources . Our model distilled with   these instances also achieved top position on the   white - box systems leaderboard of XOR - TyDi .   3.4 Ablation Study   We experiment with two more student models , one   distilled with only examples and the other   with only the parallel corpus . As the results in Ta-   ble 3 show , each has a substantial impact on system   performance . Interestingly , although the parallel   corpus does not provide anysignal , it contributes   more to the model ’s accuracy . These results alsoconﬁrm that our cross - lingual alignment algorithm   does indeed produce useful alignments .   4 Conclusion   We train highly effective end - to - end cross - lingualmodels by distilling the knowledge of an En-   glish retriever . We propose separate processes to   teachand multilingual text representations , and   present for the latter a cross - lingual alignment al-   gorithm that only relies on the underlying masked   language model ’s multilingual representation ca-   pabilities . Supervised and zero - shot evaluations   show that our model recovers much of the perfor-   mance lost due to operating in an efﬁcient cross-   lingual mode . Our - based method also yields   new single - model results on the XOR - TyDi   benchmark . Future work will exploreon unseen   languages and evaluation on additional datasets .   5 Ethics   5.1 Limitations   We show the effectiveness of multi - stage knowl-   edge distillation and cross - lingual token alignment   in training a cross - lingual information retrieval sys-   tem . We believe that it can be transferred to more   datasets and languages , but here we only show   proof of concept for the XOR - TyDi and MKQA   datasets and the seven languages mentioned in the   paper .   5.2 Risks   The intent of this work is to develop a new method   for high - performance cross - lingual information re-   trieval . It is possible that a malicious user could   try to attack the system by providing poor or offen-   sive training data . We do not support it being used   in such a manner . The risks of our system are the   same as other systems and we do not believe   we introduce any additional risk.4432Acknowledgements   We thank Graeme Blackwood and Christoph Till-   mann for providing the in - house parallel corpora .   We also thank Akari Asai for her help submitting   D.D to the XOR - TyDi leaderboard .   References4433A Appendix   A.1 Data Pre - processing   The ofﬁcial XOR - TyDi training set consists of   15,221 natural language queries , their short an-   swers , and examples of corresponding relevant   ( positive ) and non - relevant ( negative ) Wikipedia   snippets . For most queries , there are one positive   and three negative examples . We remove the 1,699   ( 11 % ) questions that have no answers in the dataset .   A random selection of 90 % of the remaining exam-   ples is used for training and the rest for validation .   Following the original XOR - TyDi process , we   also obtain additional training examples by running   BM25 - based retrieval against a Wikipedia corpus   and using answer string match as the relevance cri-   terion . These examples are added to the original set   to obtain three positive and 100 negative examples   per query . As the blind test set for ﬁnal evaluation ,   we use the 2,113 questions in the ofﬁcial XOR-   TyDi dev set .   Our monolingual ( English ) training data contain-   ing about 17.5triples are derived from the third   ﬁne - tuning round ( Col -QA3 ) of Col   relevance - guided supervision ( Khattab et al . , 2021 )   with NQ examples ( Kwiatkowski et al . , 2019 ) .   The parallel corpus used in ourexperiments   ( Table 1 ) for representation learning is constructed   from three different sources : ( 1 ) an in - house crawl   of Korean , ( 2 ) LDC releases ( Arabic ) , and ( 3 )   OPUS.The corpus has a total of 6.9 M passage   pairs which include .9 M pairs in Telugu and 1 M   pairs in each of the other six languages . The parallel   corpus used in our white - box system was created   purely from OPUS . The statistics and sources are   shown in the table below .   For zero - shot experiments , the training examples   are derived from MKQA ( Longpre et al . , 2020 ) ,   which consists of 10k queries selected from NQ ,   human translated into 25 additional languages , ﬁveof which overlap with XOR - TyDI : Ar , Fi , Ja , Ko   and Ru . We construct training data ( triples ) from   2,037 queries translated into these ﬁve languages   for which there are corresponding positive and neg-   ative passages in the NQ dataset . For each of the   ﬁve languages , there are 519k triples for a total of   2.6 M triples . We set aside 200 queries translated   into the 5 languages for a total of 1,000 queries as   a development set . We remove all MKQA queries   from the NQ training data for these experiments .   The baseline for our experiments is a   Col model with an - , which we   ﬁrst ﬁne - tune with 17.5 M NQ examples for one   epoch and then 2.9 M XOR - TyDi triples for ﬁve   epochs . Our D.D model is initialized with the   parameter weights of the baseline , and is further   ﬁne - tuned using the twoobjectives . The mono-   lingual teacher model — also a Col model run-   ning on top of the pre - trained -—is trained   with only the 17.5 M NQ triples for one epoch .   A.2 Model Selection   All the models were trained with single Nvidia   A100 GPU . The longest training time for a single   model was less than 200 hours . Following are the   ﬁnal hyperparameter conﬁgurations of our different   models . They were selected based on the respective   validation sets performance .   A.3 Qualitative Analysis   To ﬁnd out what exact weaknesses of the base-   line model the proposed method helps to address ,   we examine thirty random zero - shot test examples   where the baseline fails to retrieve the correct an-   swer in the top 5k tokens , but D.D succeeds   within the top 3 passages . We show four examples   in Table 6 with human translations of the original   non - English questions . The vast majority of our   observed cases are related to weak cross - lingual   encoding on the baseline model ’s part , where at   least one important non - English word / entity in the   question seems to be incorrectly matched with a   similar but different English entity in the passage   ( e.g. , the name of a different place ) . For the Korean ,   Russian and Arabic queries in the table , we observe   the presence of such topically similar entities ( e.g. ,   microwave↔gamma - ray , Germany↔places in   North America ) . Much more rarely , we see cases   similar to the Japanese query where the retrieved   passage is completely off - topic.443444354436