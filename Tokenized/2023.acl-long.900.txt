  Rosanne Liu , Dan Garrette , Chitwan Saharia , William Chan , Adam Roberts ,   Sharan Narang , Irina Blok , RJ Mical , Mohammad Norouzi , Noah Constant   Google Research   { rosanneliu,dhgarrette,nconstant}@google.com   Abstract   Current image generation models struggle to   reliably produce well - formed visual text . In   this paper , we investigate a key contribut-   ing factor : popular text - to - image models lack   character - level input features , making it much   harder to predict a word ’s visual makeup as   a series of glyphs . To quantify this effect ,   we conduct a series of experiments compar-   ing character - aware vs. character - blind text en-   coders . In the text - only domain , we ﬁnd that   character - aware models provide large gains on   a novel spelling task ( WikiSpell ) . Applying   our learnings to the visual domain , we train   a suite of image generation models , and show   that character - aware variants outperform their   character - blind counterparts across a range   of novel text rendering tasks ( our DrawText   benchmark ) . Our models set a much higher   state - of - the - art on visual spelling , with 30 +   point accuracy gains over competitors on rare   words , despite training on far fewer examples .   1 Introduction   Over the last year , image generation models have   made impressive quality gains ( Rombach et al . ,   2021 ; Ramesh et al . , 2022 ; Saharia et al . , 2022 ; Yu   et al . , 2022 ) . While many practical use cases are al-   ready within reach , rendering visual text in images   remains a challenge . Ramesh et al . ( 2022 ) observe   thatDALL·E-2“struggles at producing coherent   text , ” and the latest release of Stable Diffusion lists   “ can not render legible text ” as a known limitation .   In this paper , we seek to understand and improve   the ability of image generation models to render   high - quality visual text . To do so , we ﬁrst investi-   gate the spelling ability of text encoders in isolation .   We ﬁnd that despite their popularity , character-   blind text encoders — which receive no direct signal   as to the character - level makeup of their inputs —   have limited spelling ability . Building on Itzhakand Levy ( 2022 ) , we test the spelling ability of   text encoders across scales , architectures , input rep-   resentations , languages , and tuning methods . We   document for the ﬁrst time the miraculous ability   of character - blind models to induce robust spelling   knowledge ( > 99 % accuracy ) through web pretrain-   ing , but show that this does not generalize well   beyond English , and is only achieved at scales over   100B parameters , making it infeasible for most   applications . We ﬁnd that character - aware text   encoders , on the other hand , are able to achieve   robust spelling ability at far smaller scales .   Applying these ﬁndings to image generation , we   train a range of character - aware text - to - image mod-   els and demonstrate that they signiﬁcantly outper-   form character - blind models on text rendering . For   purely character - level models , this improved text   rendering comes at a cost — decreasing image - text   alignment for prompts that do n’t involve visual text .   To alleviate this , we propose combining character-   level and token - level input representations , and ﬁnd   that this delivers the best of both worlds .   Our main contributions are to : ( 1 ) Measure the   spelling ability of a range of text encoders , pulling   apart the effects of scale , character - awareness , and   multilinguality , using a new benchmark : WikiSpell .   ( 2 ) Present DrawText , the ﬁrst detailed benchmark   of visual text rendering for text - to - image models .   ( 3 ) Improve the state of the art in text rendering   ability of image generation models through the   use of character - aware text encoders . We release   code to reproduce our WikiSpell and DrawText   evaluations .   2 The spelling miracle   Language models can be categorized as to whether   they have direct access to the characters mak-   ing up their text input ( “ character - aware ” ) or do   not ( “ character - blind ” ) . Many early neural lan-16270Character-   Blind : Character-   Aware :   guage models operated directly on characters , with   no notion of multi - character “ tokens ” ( Sutskever   et al . , 2011 ; Graves , 2013 ) . Later models moved   to vocabulary - based tokenization , with some like   ELMo ( Peters et al . , 2018 ) retaining character-   awareness , and others like BERT ( Devlin et al . ,   2019 ) abandoning it in favor of more efﬁcient   pretraining . At present , most widely used lan-   guage models are character - blind , relying on data-   driven subword segmentation algorithms like Byte   Pair Encoding ( BPE ) ( Gage , 1994 ; Sennrich et al . ,   2016 ) to induce a vocabulary of subword pieces .   While these methods back off to character - level rep-   resentations for sufﬁciently rare sequences , they   compress common character sequences into un-   breakable units by design , as shown in Figure 2 .   Recent work on “ token - free ” modeling has   pointed to advantages of character - aware input rep-   resentations . Xue et al . ( 2022 ) show that ByT5 —   a character - aware multilingual language model   trained directly on UTF-8 bytes — outperforms   parameter - matched character - blind models on tasks   related to spelling and pronunciation . While op-   erating at the byte or character level comes at the   cost of training and inference speed , additional   work suggests that this can be overcome through   downsampling ( Clark et al . , 2022 ; Tay et al . , 2021 ) .   See Mielke et al . ( 2021 ) for a recent overview of   tokenization methods and character awareness .   Surprisingly , despite lacking direct access to a   token ’s spelling , character - blind models are , to   varying degree , able to infer the character - level   makeup of their tokens . Itzhak and Levy ( 2022 ) ob-   serve that , after ﬁne - tuning for spelling , RoBERTa   and GPT-2 can achieve 32 % and 33 % accuracy at   spelling held - out tokens . Kaushal and Mahowald   ( 2022 ) conﬁrm this ability and probe it further ;   however it remains unclear where in pretraining   this knowledge is coming from , and how to im-   prove it . For example , should we expect larger   character - blind models to reach 100 % spelling ac-   curacy across all tokens in their vocabulary ?   In section § 3 we ﬁnd that , with sufﬁcient scale ,   character - blind models can achieve near - perfect   spelling accuracy . We dub this phenomenon the   “ spelling miracle ” , to emphasize the difﬁculty of in-   ferring a token ’s spelling from its distribution alone .   At the same time , we observe that character - blind   text encoders of the sizes used in practice for image   generation are lacking core spelling knowledge .   With this in mind , it is unsurprising that to-16271day ’s image generation models struggle to translate   input tokens into glyph sequences . These mod-   els ’ text encoders are all character - blind , with Sta-   ble Diffusion , DALL·E , DALL·E-2 , Imagen , Parti   and eDiff - I all adopting BPE tokenizers ( Rombach   et al . , 2021 ; Ramesh et al . , 2021 , 2022 ; Saharia   et al . , 2022 ; Yu et al . , 2022 ; Balaji et al . , 2022 ) .   For image - text models , another key source of   knowledge is supervised image - caption data . Even   if its text encoder is character - blind , could a model   learn to spell by observing the makeup of words   within images ? While possible , we suspect this is   an inefﬁcient paradigm for learning , as each token   would need to be learned separately , and would   need to appear within an image - caption pair seen   in training . In section § 5 we ﬁnd that , indeed , this   “ late - stage ” learning of spelling is inferior to using   a pretrained character - aware text encoder .   3 Measuring text encoder spelling ability   Since text - to - image generation models rely on text   encoders to produce the representations for decod-   ing , we ﬁrst explore the ability of text encoders in   isolation , using a text - only spelling evaluation task .   3.1 The WikiSpell benchmark   We build the WikiSpell benchmark by sampling   words from Wiktionary . For each example in the   dataset , the input to the model is a single word , and   the expected output is its spelling , generated by   inserting spaces between each Unicode character :   elephant →e l e p h a n t   To examine the relationship between a word ’s   frequency and a model ’s ability to spell it , we group   words into buckets based on their frequency in the   mC4 corpus ( Xue et al . , 2021 ) . We create test and   development sets from each bucket by sampling   1,000words uniformly . The ﬁve buckets used are :   the top 1 % most common words , the 1–10 % most   common , 10–20%,20–30 % , and the bottom 50 %   ( which includes words never seen in the corpus ) .   Finally , we build a training set of 10,000words by   combining 5,000words sampled uniformly from   the bottom 50 % bucket with 5,000sampled pro-   portional to their frequencies in mC4 . We exclude   words in the dev and test sets from the training set ,   so evaluation is always on held out words .   Beyond English , we repeat this process for six   other languages : Arabic , Chinese , Finnish , Korean , Russian , and Thai . For language selection criteria   and further technical details , see Appendix D.   The WikiSpell benchmark is similar to Spelling-   Bee , introduced by Itzhak and Levy ( 2022 ) , but   differs in a few key ways . First , SpellingBee is   designed to probe a model ’s embedding matrix :   given an embedding vector , SpellingBee seeks to   output the character sequence of the corresponding   vocabulary element . As such , SpellingBee ’s inputs   are always a single token , and it does not mea-   sure spelling ability for words the model represents   as multiple tokens . Second , due to how subword   vocabularies are trained , model vocabularies only   contain high - frequency words , and thus Spelling-   Bee inputs are necessarily high - frequency . Finally ,   as inputs must be drawn from a model ’s vocabulary ,   SpellingBee training and evaluation data must be   tailored to a speciﬁc model , so a dataset can not   be reused across models . In contrast , WikiSpell   is model - agnostic , covers single- to many - token   words , and covers high- to low - frequency words .   3.2 Text generation experiments   We use the WikiSpell benchmark to evaluate pre-   trained text - only models across a variety of scales .   In particular , we experiment with : T5(Raffel et al . ,   2020 ) , a character - blind encoder - decoder model   pretrained on English data ; mT5 ( Xue et al . , 2021 ) ,   which is similar to T5 , but pretrained on > 100lan-   guages ; ByT5 ( Xue et al . , 2022 ) , a character - aware   version of mT5 that operates directly on UTF-8   byte sequences ; and PaLM ( Chowdhery et al . ,   2022 ) , a decoder - only model of much larger scale ,   pretrained predominantly on English . Experimen-   tal results from English - only evaluation are shown   in Table 1 , and multilingual evaluation in Table 2 .   Our ﬁrst ﬁnding is that character - blind models   T5 and mT5 perform much worse on the Top- 1 %   most frequent words . This result may seem counter-   intuitive since models typically perform better on   frequent items ; however due to how subword vo-   cabularies are trained , common words are typically   represented as atomic tokens ( e.g. , 87 % of words in   the English Top 1 % bucket are single - token for T5 ) ,   thereby obscuring their internal makeup . Scores are   a bit higher in the mid - frequency buckets , where   words are typically broken into a few common   tokens , and lower again in the lowest - frequency   bucket , where even the subword tokens may be less   frequent . The low spelling accuracies indicate that   T5 ’s encoder does not retain sufﬁcient information16272   about the spelling of subwords in its vocabulary .   Secondly , we ﬁnd that for character - blind mod-   els , scale is a key factor in spelling ability . Both T5   and mT5 improve with scale , but even at XXL size ,   they are not particularly strong ( e.g. , T5 - XXL ’s per-   formance on common English words is only 66 % ) .   Only when character - blind models reach PaLM ’s   scale do we start to see near - perfect spelling ability :   PaLM 540B achieves > 99 % accuracy across all fre-   quency buckets in English , despite the fact that it   sees only 20examples in its prompt ( as opposed to   the1,000ﬁne - tuning examples shown to T5 ) . How-   ever , performance is lower on other languages .   Our experiments on ByT5 show that character-   aware models have far greater spelling ability .   ByT5 ’s performance at Base and Large sizes is   only slightly behind XL and XXL ( though still in   at least the mid- 90 % range ) , and the frequency of   a word has little effect on ByT5 ’s ability to spell   it . These results far exceed those of ( m)T5 , and are   comparable to the English performance of PaLM ,   which has > 100×more parameters , and exceed   PaLM ’s performance on other languages . These   ﬁndings indicate that substantially more character-   level information is retained by the ByT5 encoder ,   and in such a way that it can be retrieved from thosefrozen parameters as needed for the decoding task .   We also test ﬁne - tuning the full model instead of   keeping the encoder frozen ( also in Table 1 ) . When   ByT5 ’s encoder is ﬁne - tuned , performance goes to   roughly 100 % across all scales and frequency buck-   ets . For T5 , the effect of full ﬁne - tuning is more   mixed : on rare words , it helps a lot ( 65%→90 %   for T5 - XXL on Bottom 50 % ) , while for common   words , it has little effect ( 66%→68 % on Top 1 % ) .   This tells us that for words that get broken into   smaller pieces ( which may be repeated across train-   ing examples ) , the model can memorize spelling   information provided during ﬁne - tuning , whereas   for single - token words , ﬁne - tuning provides no   spelling signal since , by deﬁnition , that single to-   ken will not appear in the ﬁne - tuning dataset .   4 The DrawText benchmark   Evaluating text - to - image models has been an on-   going topic of research , with the development of   standard benchmarks from COCO ( Lin et al . , 2014 )   to DrawBench ( Saharia et al . , 2022 ) , and metrics in-   cluding FID ( Heusel et al . , 2017 ) , CLIP score ( Hes-   sel et al . , 2021 ) , and human preferences ( Saharia   et al . , 2022 ) . However , there has been a lack of   work on text rendering and spelling evaluation . To   that end , we present a new benchmark , DrawText ,   designed to measure the text rendering quality of   text - to - image models . The benchmark consists   of two parts , assessing distinct model capabilities :   1 ) DrawText Spelling , which evaluates simple word   rendering over a large set of English terms ; and   2 ) DrawText Creative , which evaluates end - to - end   text rendering across diverse visual settings .   4.1 DrawText Spelling   To measure the spelling ability of image generation   models in a controlled and automatable fashion , we   construct 500 prompts by sampling 100 words from   each of the English WikiSpell frequency buckets16273(see § 3.1 ) , and plugging them into the template :   A sign with the word “ ” written on it . For   each prompt , we sample 4 images from the candi-   date model , and assess them using optical character   recognition ( OCR)-based metrics .   For OCR evaluation , we use the Google Cloud   Vision API , which returns all text found within an   image , along with bounding box locations . The   DrawText Spelling prompt tends to generate a   prominently positioned sign with text , which is   relatively simple for off - the - shelf OCR to identify ,   but if the system returns multiple bounding boxes ,   we only use the top - most one . Additionally , since   text may be rendered across multiple lines , we post-   process the OCR output by removing newline char-   acters that appear within a single bounding box .   Finally , since text on real signs is often written in   all capitals , and models often do the same regard-   less of how the word is written in the prompt , we   ignore case when computing the spelling accuracy .   4.2 DrawText Creative   Visual text is not limited to mundane examples   like street signs . Text can appear in many forms —   scribbled , painted , carved , sculpted , and so on . If   image generation models support ﬂexible and ac-   curate text rendering , this can help designers in   developing creative fonts , logos , layouts , and more .   To test the ability of image generation models   to support these use cases , we worked with a pro-   fessional graphic designer to construct 175diverse   prompts that require rendering text in a range of   creative styles and settings . The prompts vary in   how much text is speciﬁed , ranging from a single   letter to an entire sentence . We share these prompts   in Appendix G , with the expectation that they will   help the community work towards improving text   rendering . Many of the prompts are beyond the   abilities of current models , with state - of - the - art   models exhibiting misspelled , dropped , or repeated   words , as seen in Figure 3 .   5 Image generation experiments   In this section , we evaluate the spelling ability   of text - to - image generative models with the pro-   posed DrawText benchmark . State - of - the - art text-   to - image generative models consist of a text en-   coder plus a cascade of either diffusion models ( Sa - haria et al . , 2022 ) or autoregressive models ( Yu   et al . , 2022 ) that map the encoded text representa-   tions to realistic images . In section § 3 we saw that   character - aware text encoders greatly outperform   character - blind models on spelling in a text - only   setting ; in this section , we investigate whether mak-   ing the text encoder character - aware improves the   text rendering ability of text - to - image models .   5.1 Models   For an apples - to - apples comparison , we train two   character - blind and three character - aware image   generation models . Our training closely follows   the procedure of Saharia et al . ( 2022 ) , with the   following modiﬁcations . First , our models train   for500,000steps , which is 5.6×fewer steps than   Imagen . Second , we only train the initial 64×64   model , as text rendering ability can already be as-   sessed at this scale . This allows us to forgo the   training of super - resolution models .   Third , rather than a mixture of datasets , we train   exclusively on the publicly available Laion-400 M   ( Schuhmann et al . , 2021 ) . This improves repro-   ducibility and also increases the amount of visual   text seen during training . Inspecting a random sam-   ple of 100 images , we found that a relatively high   proportion ( around 71 % ) of Laion images contain   text , and many ( around 60 % ) exhibit correspon-   dence between caption text and visual text .   Fourth , to prevent models from clipping text , we   train on uncropped images with arbitrary aspect   ratios . In contrast with the widely used strategy of   cropping a square from the center of the image , we   maintain the image ’s true aspect ratio by padding   with black borders . The model receives an addi-   tional binary mask input indicating the padding .   To test the effects of text encoder size and   character - awareness , we vary the pretrained text   encoder as follows :   T5 - XL and T5 - XXL — Following Saharia et al .   ( 2022 ) , we use the ( character - blind ) pretrained T5   text encoders of Raffel et al . ( 2020 ) . The encoder   sizes are 1.2B ( XL ) and 4.6B ( XXL ) . Note , T5-   XXL is the same encoder used in both Imagen and   the recent eDiff - I ( Balaji et al . , 2022 ) .   ByT5 - XL and ByT5 - XXL — We use the pre-   trained ByT5 encoders of Xue et al . ( 2022 ) , with   encoders sizes 2.6B ( XL ) and 9.0B ( XXL ) . These   differ from T5 in several regards . First , ByT516274   models read and write UTF-8 bytes rather than to-   kens from a vocabulary , so they are fully character-   aware . Second , ByT5 is multilingual , trained on the   mC4 corpus of over 100languages . Third , ByT5   pretrains with sequence length 1024 , twice that of   T5 . When encoding text as input to the image gen-   eration module , we use a sequence length of 256   bytes , compared to 64tokens for the T5 models .   Concat(T5 - XXL , ByT5 - Small ) — We use as   the text encoding a concatenation of the encodings   from T5 - XXL and a small ByT5 model . ByT5-   Small ( 220 M ) represents a lightweight addition to   the Saharia et al . ( 2022 ) model in terms of overall   compute and model size ( a 4.8 % increase in en-   coder size ) , but makes the model character - aware .   Imagen Aspect - Ratio ( Imagen - AR ) — To test   the beneﬁt of training on uncropped images , we   ﬁne - tune the Imagen model of Saharia et al . ( 2022 )   for an additional 380,000steps , to 3.2 M steps to-   tal , training on uncropped images with preserved   original aspect ratio , as described above .   Beyond these custom models , we benchmark   Stable Diffusion version 1.5 ( Rombach et al . , 2021 ) ,   DALL·E-2(Ramesh et al . , 2022 ) , Parti ( Yu et al . ,   2022 ) and Imagen ( Saharia et al . , 2022 ) , all of   which use character - blind text encoders . Among   these , Imagen is most similar to our experimen-   tal models , using the same T5 - XXL encoder , but   trained much longer and with a larger scale of data.5.2 DrawText Spelling results   Char - aware models improve spelling . Figure 4   shows our DrawText Spelling results across 10   models , with 2,000 images sampled per model ,   evaluated with OCR . Accuracy is computed on   the full string ( i.e. , no credit is given for partial   matches ) . Across all word frequencies , character-   aware models ( ByT5 and Concat ) outperform the   rest , with 15 + point accuracy gains over Imagen-   AR on the most frequent words , and 30 + point   gains on the least frequent words . This is remark-   able given that Imagen - AR trained for 6.6×longer .   Our T5 models ( character - blind ) provide a more   controlled comparison against the character - aware   models , as they differ only in the choice of text   encoder — trained on the same dataset for the same   number of steps . Here , the gains are even larger :   25 + point gains on the most frequent words and 30 +   point gains on the least frequent . Notably , these   gains persist even for the smaller ByT5 - XL model ,   whose encoder is 43 % smaller than T5 - XXL .   To estimate the rate of OCR errors , we manually   validate a balanced set of 128samples from T5-   XXL and ByT5 - XXL ( see Appendix B for details ) .   We ﬁnd no false positives , but when OCR detects   an error , ByT5 - XXL is actually correct 34 % of the   time , while T5 - XXL is correct 9 % . This asymme-   try suggests the beneﬁt of character - aware model-   ing may be even greater than implied by Figure 4.16275   Char - aware models make fewer types of error .   To gain a better understanding of different models ’   failure modes , we manually inspect our T5 and   ByT5 model outputs . Table 3 illustrates common   error types . Several categories of error are only   observed in T5 models , suggesting that they stem   from the encoder ’s lack of core spelling knowledge .   Insemantic errors , the model makes a plausible   morpheme substitution , as in demonstrated→   demonstrafied . Inhomophone errors , the model   produces an incorrect spelling that could be pro-   nounced similarly to the target word . This suggests   that some of T5 ’s “ miraculous ” spelling ability mayderive from online pronunciation guides . In add   glyph errors , the model inserts a letter that was ab-   sent from the target , again reﬂecting the model ’s un-   certainty about a token ’s internal character makeup .   One notable sub - type of semantic error is   character - blind models “ regularizing ” irregular in-   ﬂection , as in fought→fighted . On a hand-   chosen set of 23common irregular past - tense verbs   ( began , chose , dug , etc . ) , we ﬁnd T5 - based mod-   els erroneously add -edin 11 % of samples ( see   Figure 6 ) , while our character - aware models never   exhibit this type of error . This is clear evidence   that character - blind models partly rely on a word ’s   meaning ( fought⇒ ) and fallible patterns of   morphology ( ⇒-ed ) to predict spelling .   Other error categories are found across all model   types ; these include dropped , repeated , merged , or   misshapen glyphs . Given that our ByT5 encoders   provide a robust spelling signal ( see § 3.2 ) , we un-   derstand these errors to be “ layout issues ” , where   the image generation module has trouble shaping   and positioning realistic glyphs within the image .   Char - aware models reduce consistent errors .   Another stark difference between our models lies   in whether they consistently misspell a given word   across multiple samples . As illustrated in Figure 5 ,   there are many words that our T5 models misspell   no matter how many samples are drawn . Again ,   we believe this indicates missing knowledge in the   text encoder . By contrast , our ByT5 models are   more likely to make sporadic errors . We quantify   this observation in Figure 7 by measuring the rates   at which the model is consistently right ( 4/4 ) or   wrong ( 0/4 ) across all four image samples . On   common words in particular ( Top 1 % ) , we see a   sharp contrast in that ByT5 models are never con-   sistently wrong , while T5 models are consistently16276T5 - XXLByT5 - XXL   wrong on 10 % or more of words .   5.3 DrawText Creative results   To test our models in a more realistic user - facing   setting , we sample 8images from each of our T5   and ByT5 models on our 175DrawText Creative   prompts in Appendix G. These prompts are more   diverse and challenging , with the majority targeting   three or more words of rendered text .   Focusing on text rendering ability , we ﬁnd once   again that character - aware models have a clear ad-   vantage . Figures 12 and 13 show non - cherrypicked   samples on two prompts where T5 - XXL consis-   tently misspells one or more words . On prompts   targeting longer text spans , all our models struggle ,   as seen in Figure 14 . Nevertheless , we observe   that character - aware text encoders provide a clear   lift on these prompts , reducing the misspellings of   words like refrain , arguing , andchimpanzees .   We conﬁrm the above observations quantita-   tively by comparing T5 - XXL vs. Concat using the   DrawBench methodology ( Saharia et al . , 2022 ) ,   evaluated over our 175creative prompts . Beyond   the standard DrawBench metrics of ﬁdelity and   alignment ( described in the following section ) , we   ask raters Which set of images more accurately   shows the text : “ < target text > ” ? Results in   Figure 8 show Concat is preferred on all metrics .   5.4 DrawBench results   We have shown that character - aware text encoders   excel at spelling , in both text ( § 3 ) and visual ( § 5 )   domains . But does this ability come at a cost ? Can   these models maintain competitive image quality   and text - image alignment , even on prompts that16277   do n’t require text rendering ? To shed light on this   question , we run several side - by - side comparisons   using the DrawBench evaluation of Saharia et al .   ( 2022 ) . This asks human raters to compare two   models ’ generations of 8images each across 200   prompts covering 11thematic categories . We fol-   low the procedure described in Saharia et al . ( 2022 )   closely , aggregating scores across 25raters .   Figure 9 shows DrawBench results of three side-   by - side comparisons of character - aware models   vs. T5 - XXL . While image quality ( “ ﬁdelity ” )   is similar across the board , we ﬁnd that purely   character - level models ( ByT5 ) score worse on   image - text alignment , with raters preferring T5-   XXL on 60 % of prompts . By contrast , our Concat   model closes this gap to within error bars . Thus ,   this “ hybrid ” character - aware model is able to   greatly improve text rendering ( Figure 4 ) , without   signiﬁcantly hurting performance elsewhere .   Appendix C provides a per - category breakdown   and further analysis ; while the character - aware   models excel on the DrawBench text category ,   the ByT5 models are dispreferred in most other   categories . Through manual inspection , we ﬁnd   the ByT5 models are more prone to ignore infor-   mation in the prompt , for example leaving out a   mentioned object , or choosing a canonical color   over a requested one . One possible explanation for   this behavior is that we did not tune the guidance   weight parameter used at inference time ( Saharia   et al . , 2022 ) , using a ﬁxed value of 30throughout .   Increasing this parameter is known to boost image-   text alignment , but at the cost of diversity . It may   be that character - level models beneﬁt from higher   guidance values than token - based models .   Another possibility is that the ByT5 models have   a shallower understanding of English language due   to their multilingual nature — as ByT5 was exposed   to roughly 70×less English than T5 during pre - training . Given this difference , we should also   expect to see corresponding gains on non - English   languages . We conﬁrm this expectation through   preliminary results in Appendix A.   6 Conclusion   In this paper , we set out to better understand what   is needed for image generation models to reliably   render well - formed visual text . Using our novel   WikiSpell and DrawText benchmarks , we were   able to precisely quantify the effects of character-   awareness and other design choices on spelling   ability in both the text and visual domains .   We found that character - aware text encoders con-   fer large gains on spelling , and when used within an   image generation model , these gains translate into   improved visual text rendering . However , using   exclusively character - level representations deterio-   rated overall text - image alignment — at least when   evaluating our multilingual ByT5 text encoder on   English prompts with untuned guidance weight . To   resolve this , we found that a hybrid model combin-   ing token - level and character - level signals offered   the best of both worlds : dramatically improving vi-   sual text without signiﬁcantly affecting alignment .   While we saw substantial improvements on   DrawText Spelling accuracy ( 75%→94 % on com-   mon words and 47%→83 % on rare words ) , some   failure modes remain unaddressed . Even our   strongest models were observed to occasionally   drop , repeat , or merge letters within a word , or   words within a phrase . Our results strongly suggest   that resolving these issues will require orthogonal   improvements outside the text encoder , speciﬁcally   changes to the image generation module .   As a secondary ﬁnding , we demonstrated for   the ﬁrst time that , with sufﬁcient scale , even   models lacking a direct character - level view of   their inputs can infer robust spelling information   through knowledge gained via web pretraining —   “ the spelling miracle ” . While remarkable , this ﬁnd-   ing is less immediately practical , as it requires mod-   els over 100B parameters , and even these did not   generalize well beyond English in our experiments .   Limitations   While we establish the “ miraculous ” ability of   character - blind models to induce robust spelling16278information through large - scale web pretraining ,   our work does not attempt to identify the mecha-   nisms or sources through which this information   is learned . Possible sources within web corpora   include : dictionaries containing phonetic pronunci-   ation guides , alphabetically ordered lists , typos and   other misspellings , and examples of spelling words   with dashes or spaces between every character .   Linguistic phenomena that may aide in inducing   spelling knowledge include words with predictable   morphemic makeup , and cases where meaning-   form relation is non - arbitrary , contra Saussure ’s   “ semiotic arbitrariness ” . We refer the reader to   Itzhak and Levy ( 2022 ) and Kaushal and Mahowald   ( 2022 ) for work in this direction .   Most of our image generation experiments are   limited to English . We present preliminary results   in Appendix A showing that our ByT5 - based mod-   els have stronger multilingual understanding than   T5 . However it would be valuable to test this fur-   ther , and to explore training image generation mod-   els on multilingual image - caption datasets , as op-   posed to merely using a pretrained multilingual text   encoder .   Ideally , it would be possible to conduct con-   trolled comparisons between pretrained text en-   coders that differ only in one regard , to isolate   all factors contributing to performance . However   as pretraining large language models is resource in-   tensive , we were only able to use off - the - shelf text   encoders , which often differ along multiple axes .   In our text - only experiments , we isolated the contri-   butions of character - awareness ( ByT5 vs. mT5 / T5 )   and multilinguality ( ByT5 / mT5 vs. T5 ) . However ,   in our image generation experiments , these factors   were conﬂated , as we had limited resources for   training new models . Still , the fact that ByT5-   based image generation models outperform T5   despite being multilingual ( which often degrades   performance on English - only tasks ) strongly sug-   gests that character - awareness is the key factor for   spelling ability .   Another limitation is that we focused on image   generation models that leverage frozen pretrained   text encoders . This enabled straightforward exper-   imentation by swapping encoders and retraining   the image generation module . However , it remains   to be seen whether our results extend to settings   where the text encoder is trained along with the rest   of the model , as in Yu et al . ( 2022).Ethics Statement   We note our image - caption training data comes   from Laion-400 M ( Schuhmann et al . , 2021 ) , which   is uncurated and known to contain harmful biases   and offensive content . We hope to utilize and con-   tribute to safer and better curated datasets in the   future , as well as to develop improved techniques   for debiasing and detoxifying existing models .   A potential risk for image generation models   is that they can be used for creating misleading   and harmful content . Our work on improving text   rendering could aide the creation of fake signs   and other misleading images containing visual text .   With the wide - spread availability of image genera-   tion models , we expect that improving education   around misinformation and adopting better digital   signature mechanisms will be important counter-   measures .   Acknowledgements   We thank Jason Baldridge , Jon Clark , Noah Fiedel ,   Linting Xue , and Jiahui Yu for helpful discussion   and comments on an earlier draft . We thank Sarah   Pratt for validating ﬁndings on Stable Diffusion   models .   References1627916280   A Multilingual results   As ByT5 is a multilingual model covering 100 +   languages , we are interested to see if image genera-   tion models built on ByT5 deliver improved perfor-   mance over T5 on non - English languages . While   the text encoder itself is multilingual , it is not obvi-   ous whether this is sufﬁcient to produce a multilin-   gual image generation model . The image caption   dataset used for training in all of our experiments is   Laion-400 M ( Schuhmann et al . , 2021 ) , which we   estimate through language ID detection to consist   of95 % English captions , with only minimal cov-   erage ( < 0.1 % ) of some widely spoken languages ,   such as Arabic and Hindi .   To test for multilingual understanding , we trans-   late two English prompts to 11languages using   Google Translate , and feed the outputs to our   models . As can be seen in the rows 1–2of Fig-   ure 10 , our T5 - XXL model demonstrates basic un-   derstanding of ﬁve high - resource European lan-   guages ( German , French , Spanish , Portuguese , Russian).However , in the remaining languages   ( Greek , Hindi , Arabic , Chinese , Japanese , Korean ) ,   T5 appears to ignore the caption completely .   By comparison , our ByT5 - XXL model exhibits   understanding across all 11languages . Given its   limited training on multilingual captions , we in-   terpret this ability as due to the pretrained ByT5   encoder ’s alignment of representations across lan-   guages . If the encoder already embeds similar   prompts into a shared space that factors out the   contribution of language , then the image genera-   tion model should be able to learn from just a hand-   ful of examples how to map any language seen in   pretraining into the space of images .   If this explanation is correct , it also suggests   that rendering text in different scripts will require   more than just a multilingual encoder . To learn the   glyph shapes , variants and fonts used for a given   script , we should expect to need to train models on   a large source of visual text in that script . Indeed ,   rows 3and6of Figure 10 show that neither of our   models can map prompt text onto visual text in   non - Latin scripts . While our ByT5 model captures   the intent to draw a sign across all languages , it   is unable to render the words for dogin Greek ,   Russian , Chinese and so on , presumably because it   has had little visual exposure to the glyphs making   up these words .   B OCR error estimation   To estimate the rate of false positives and false neg-   atives due to OCR errors , we sample 32examples   labeled correct and32labeled incorrect for each   of T5 - XXL and ByT5 - XXL , and perform a manual   validation . In our sample , we ﬁnd no false pos-   itives : when OCR detects the correct word , it is   always correct . However observe false negatives   for both models , including cases where OCR fails   to detect the text ( e.g. , due to it being too small ) ,   or misreads a character . For ByT5 - XXL , we ﬁnd   that34 % of examples labeled by OCR as incorrect16281T5 - XXLByT5 - XXLen de fr es pt ru el hi ar zh ja ko   are actually correct . For T5 - XXL , this error rate   is lower at 9 % . This asymmetry suggests that the   beneﬁt of character - aware modeling may be even   greater than implied by our results in Figure 4 .   C Per - category DrawBench analysis   To understand the alignment scores in more detail ,   we report per - category preference scores in Fig-   ure 11 . In line with our DrawText Spelling results ,   the character - aware models are always preferred in   thetext category — 21prompts testing the ability   to render 7short phrases in 3visual styles . The   ByT5 models are also preferred in the count cat-   egory , which tests prompts like Four dogs on the   street . However , they are dispreferred in nearly   all other cases , and perform particularly poorly on   thecolor category . Through manual inspection ,   we ﬁnd that in this category , the ByT5 models are   more prone to ignore information in the prompt , for   example leaving out a mentioned object , or choos-   ing a canonical color over a requested one ( e.g. a   yellow banana instead of a red one).D WikiSpell details   We select six languages to cover diverse propo-   erties that could affect the ability for models to   learn spellings : Arabic , written in the Arabic al-   phabet , has non - concatenative morphology ; Chi-   nese is written in Simpliﬁed and Traditional Chi-   nese scripts , which are logographic and do not use   whitespace to separate words ; Finnish , written in   the Latin alphabet , has rich inﬂectional and deriva-   tional sufﬁxes , and word stems often change when   sufﬁxes are attached ; Korean ’s writing system ,   Hangul , has a huge number of characters since al-   phabetic features are arranged into syllabic blocks ,   which Unicode represents as a single characters ;   Russian , written in the Cyrillic alphabet , has sub-   stantial fusional morphology , and uses inﬂection   for case - marking and agreement ; and Thai , written   in the alphabetic Thai script , is an analytic lan-   guage , but does not use whitespace between words .   Further implementation details are as follows :   •Example Python 3 code for transforming a   word into its spelling :   def to_spelling(word : str ) - > str :   return " " .join(word)16282•Since we want each entry to be a single word ,   we exclude entries that contain any ( Uni-   code ) whitespace , that are entirely punctua-   tion / symbols ( i.e. , all characters are from Uni-   code categories P and/or S ) , that are longer   than 30characters , or that have a “ part - of-   speech ” Proverb .   •For efﬁciency , word frequencies are computed   onsubsets of the full mC4 corpus . For lan-   guages other than English , this is a sample of   1 M documents from that language ’s section   of mC4 . For English , since it has such a long   tail of words in Wiktionary , we use the ﬁrst   140 M documents in mC4 ’s English section .   •For Arabic , English , Finnish , Korean , and   Russian , word - counting is performed by split-   ting document texts using the following delim-   iters:?!/:;,\"&()[]{}<>/uni02CB , plus any Uni-   code whitespace . For Chinese and Thai , since   they do not use whitespace to separate words ,   we instead count the number of documents in   which the word appeared as a substring .   E Additional DrawText creative samples   We show additional samples on DrawText Creative   prompts in Figures 12 , 13 and 14 .   F Representative DrawText Spelling   samples   We show generated image examples from all 5   models ( T5 - XL , T5 - XXL , ByT5 - XL , ByT5 - XXL ,   Concat ) in Figures 15 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 ,   24 .   Samples are selected based on model ’s perfor-   mance . Figures 15 - 20 are selected words that T5   models tend to get wrong ( regardless of ByT5 ’s   performance ) . Figures 21 - 24 are selected words   that ByT5 models tend to get wrong ( regardless of   T5 ’s performance ) .   G DrawText Creative prompts   We present below 175creative prompts targeting   rendered text of various lengths : one letter ( 10 ) ,   one word ( 50 ) , two words ( 25 ) , and three or more   words ( 90 ) .   Prompts used in Figure 316283T5 - XXLByT5 - XXL16284T5 - XXLByT5 - XXL16285T5 - XXLByT5 - XXL16286"0 - 6 - 2 " spelled by T5 - XL"0 - 6 - 2 " spelled by T5 - XXL"0 - 6 - 2 " spelled by ByT5 - XL"0 - 6 - 2 " spelled by ByT5 - XXL"0 - 6 - 2 " spelled by Concat   " barratrously " spelled by T5 - XL"barratrously " spelled by T5 - XXL"barratrously " spelled by ByT5 - XL"barratrously " spelled by ByT5 - XXL"barratrously " spelled by Concat16287"depositories " spelled by T5 - XL"depositories " spelled by T5 - XXL"depositories " spelled by ByT5 - XL"depositories " spelled by ByT5 - XXL"depositories " spelled by Concat   " enceinte " spelled by T5 - XL"enceinte " spelled by T5 - XXL"enceinte " spelled by ByT5 - XL"enceinte " spelled by ByT5 - XXL"enceinte " spelled by Concat16288"kilopascals " spelled by T5 - XL"kilopascals " spelled by T5 - XXL"kilopascals " spelled by ByT5 - XL"kilopascals " spelled by ByT5 - XXL"kilopascals " spelled by Concat   " rupiahs " spelled by T5 - XL"rupiahs " spelled by T5 - XXL"rupiahs " spelled by ByT5 - XL"rupiahs " spelled by ByT5 - XXL"rupiahs " spelled by Concat16289"Yongchuan " spelled by T5 - XL"Yongchuan " spelled by T5 - XXL"Yongchuan " spelled by ByT5 - XL"Yongchuan " spelled by ByT5 - XXL"Yongchuan " spelled by Concat   " isoaldehyde " spelled by T5 - XL"isoaldehyde " spelled by T5 - XXL"isoaldehyde " spelled by ByT5 - XL"isoaldehyde " spelled by ByT5 - XXL"isoaldehyde " spelled by Concat16290"constructivists " spelled by T5 - XL"constructivists " spelled by T5 - XXL"constructivists " spelled by ByT5 - XL"constructivists " spelled by ByT5 - XXL"constructivists " spelled by Concat   " ebike " spelled by T5 - XL"ebike " spelled by T5 - XXL"ebike " spelled by ByT5 - XL"ebike " spelled by ByT5 - XXL"ebike " spelled by Concat16291   DrawText Creative prompts : 1 letter   DrawText Creative prompts : 1 word16292   DrawText Creative prompts : 2 words   DrawText Creative prompts : 3 + words162931629416295ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   " Limitations ” section   /squareA2 . Did you discuss any potential risks of your work ?   " Ethics Statement ” section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Abstract , and Section 1 Introduction .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Created WikiSpell and DrawText Creative prompt datasets .   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3.1 , Section 3.2 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   If accepted we will release our prompts and evaluation datasets under a permissive license , most   likely CC - BY .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 1 Introduction discussed how image generation models are used to render visual text .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3.1 and Appendix D   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3.1 .   C / squareDid you run computational experiments ?   Section 5.1   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 3.2 , Section 5.1.16296 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 3.2 , Section 5.1 .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Sections 5.2 , 5.3 , 5.4 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 3.2   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 5.4 .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   We closely follow the procedure of Saharia et al.(2022 ) , which we cited .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   We closely follow the procedure of Saharia et al.(2022 ) , which we cited .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   We closely follow the procedure of Saharia et al.(2022 ) , which we cited .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Our institution does not have an IRB .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   We closely follow the procedure of Saharia et al.(2022 ) , which we cited.16297