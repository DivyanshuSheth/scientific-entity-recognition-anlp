  Jingwei Yi , Fangzhao Wu , Chuhan Wu , Xiaolong Huang ,   Binxing Jiao , Guangzhong Sun , Xing XieUniversity of Science and Technology of ChinaMicrosoft Research AsiaTsinghua UniversityMicrosoft STC Asia   yjw1029@mail.ustc.edu.cn { wufangzhao,wuchuhan15}@gmail.com   { xiaolhu,binxjia,xingx}@microsoft.com gzsun@ustc.edu.cn   Abstract   Query - aware webpage snippet extraction is   widely used in search engines to help users   better understand the content of the returned   webpages before clicking . Although important ,   it is very rarely studied . In this paper , we pro-   pose an effective query - aware webpage snippet   extraction method named DeepQSE , aiming to   select a few sentences which can best summa-   rize the webpage content in the context of input   query . DeepQSE ﬁrst learns query - aware sen-   tence representations for each sentence to cap-   ture the ﬁne - grained relevance between query   and sentence , and then learns document - aware   query - sentence relevance representations for   snippet extraction . Since the query and each   sentence are jointly modeled in DeepQSE , its   online inference may be slow . Thus , we fur-   ther propose an efﬁcient version of DeepQSE ,   named Efﬁcient - DeepQSE , which can signif-   icantly improve the inference speed of Deep-   QSE without affecting its performance . The   core idea of Efﬁcient - DeepQSE is to decom-   pose the query - aware snippet extraction task   into two stages , i.e. , a coarse - grained candidate   sentence selection stage where sentence repre-   sentations can be cached , and a ﬁne - grained   relevance modeling stage . Experiments on two   real - world datasets validate the effectiveness   and efﬁciency of our methods .   1 Introduction   Given an input search query , search engines such   as Googleand Bing , not only return the URLs   and the titles of the relevant webpages , but also   show the query - aware snippets of these webpages ,   aiming to help users better understand the webpage   content before clicking . These webpage snippets   are usually one or two sentences extracted from   the webpage , which can not only summarize the   key content of the webpage , but also be relevantFigure 1 : Examples of query - aware snippets in search   engines .   to the input query . Some examples are shown in   Figure 1 . For the query ‘ einstein ’ and the webpage   of ‘ Albert Einstein - Wikipedia ’ , a good snippet is   a brief introduction of Einstein ’s life . While for the   query ‘ einstein achievement ’ , a good snippet would   be sentences describing his inﬂuence on science .   In other words , the snippet is a summarization of   the webpage in the context of input query .   Although query - aware webpage snippet extrac-   tion is important and useful , it is very rarely studied .   Only a few works exist in this ﬁeld , and most of   them are based on simple word - level text match-   ing method ( Penin et al . , 2008 ; Zou et al . , 2021 ) .   For example , Turpin et al . ( 2007 ) proposed to   utilize the number of overlapping words between   queries and sentences in webpages to extract snip-   pets . Tsegay et al . ( 2009 ) proposed to select snip-   pets through the summation of Kullback - Leibler   divergence or TF - IDF weight of overlapping words   between queries and sentences in webpages . How-   ever , these methods rely on counting features of   overlapping words , and can not capture the deep   semantic relation between query and webpage .   In this paper , we propose an effective query-   aware webpage snippet extraction method for web   search , named DeepQSE . In DeepQSE , given an   input query and a webpage with multiple sentences,3035we ﬁrst learn query - aware sentence representations   for each sentence to capture the ﬁne - grained rele-   vance between query , sentence and webpage title   using a query - aware sentence encoder . Then we   model the query - sentence relevance in the context   of the whole webpage using a document - aware rel-   evance encoder . Since the query and each webpage   sentence are jointly modeled , the online inference   speed of DeepQSE can be slow , while the search   engines have extremely high requirements for low   latency . Thus , we further design an efﬁcient version   of DeepQSE named Efﬁcient - DeepQSE , aiming to   signiﬁcantly improve the inference speed of Deep-   QSE and keep its performance as much as possible .   The key idea of Efﬁcient - DeepQSE is to decom-   pose the query - aware webpage snippet extraction   task into two stages , i.e. , coarse - grained candidate   sentence selection and ﬁne - grained relevance mod-   eling . The coarse - grained candidate sentence se-   lection aims to select a moderate number of most   potential sentences for snippet extraction using a   bi - encoder where sentence representations can be   cached for fast online serving . The ﬁne - grained rel-   evance modeling stage aims to capture the deep se-   mantic relevance between the query and the candi-   date sentences selected by the previous stage using   query - aware cross - encoders . We conducted many   experiments on two real - world datasets , which ver-   ify the effectiveness and efﬁciency of our approach .   The contributions of this paper are as follows :   •We propose an effective query - aware web-   page snippet extraction method for web search   named DeepQSE , which can summarize the   webpage content in the context of input query .   •We further propose Efﬁcient - DeepQSE which   can improve the inference speed of Deep - QSE   with a minor performance drop .   •We conduct extensive experiments on two real-   world datasets to verify the effectiveness and   efﬁciency of our methods .   2 Related Work   2.1 Query - aware Snippet Extraction   Query - aware snippet extraction is a widely - used   technique to select snippets which can help users   better understand the webpage content before click-   ing ( Chen et al . , 2020 ; Bando et al . , 2010 ) . Al-   though important , only a few works have been pro-   posed for query - aware snippet extraction based on   word - level text matching method ( Zou et al . , 2021 ; Turpin et al . , 2007 ; Penin et al . , 2008 ; Tsegay et al . ,   2009 ) . For example , Turpin et al . ( 2007 ) propose   CTS , which selects snippets based on sentence po-   sitions and the number of overlapping words be-   tween queries and sentences . Zou et al . ( 2021 ) pro-   pose QUITE , which computes importance scores   for each word and sums the importance scores of   overlapping words to select snippets . These meth-   ods are mostly based on counting features of over-   lapping words and can not capture deep semantic   relations between query and webpage . Recently ,   Zhong et al . ( 2021 ) propose QMSUM for meeting   summarization , of which the locator can be used   for snippet extraction . The locator of QMSUM   applies a ﬁxed PLM and CNN to encode sentence   and query , and a Transformer to model interac-   tions between sentences . QMSUM is a bi - encoder   which fails to encoder the word - level interactions   between query and sentences . Zhao et al . ( 2021 )   propose QBSUM , which concatenates query and   body , and applies multiple predictors to compute   relevance scores . The simple body - query concate-   nation in QBSUM may ﬂuctuate the information of   query and lead to some sentences being cut off due   to the length limitation of PLM . Recently , some   works ( Ishigaki et al . , 2020 ; Chen et al . , 2020 )   use abstractive generation model to generate snip-   pets for ( query , document ) pairs . For example ,   Ishigaki et al . ( 2020 ) uses the RNN network with   copy mechanism to generate query - aware snippets .   However , abstractive methods need detailed pars-   ing and digesting , which usually takes a consider-   able amount of time ( Wang et al . , 2007 ) . Therefore ,   these methods are not compared in this paper .   2.2 Text Matching   Text matching has been widely applied in many   scenarios , such as information retrieval ( Pang et al . ,   2017 ) and clustering various articles for breaking   news detection ( Yang et al . , 2002 ) . Recently sev-   eral text matching methods have been proposed .   Following Humeau et al . ( 2020 ) , these methods   can be divided into two groups , i.e. , bi - encoders   and cross - encoders . Bi - encoders ( Palangi et al . ,   2014 ; Reimers and Gurevych , 2019 ; Hu et al . ,   2014 ) model the sentence - level interactions be-   tween queries and documents , in which the doc-   ument representations can be cached for fast on-   line serving . For example , Wan et al . ( 2016 ) pro-   pose C - DSSM , which computes query and docu-   ment representations with convolutional networks.3036   Cross - encoders ( Guo et al . , 2016 ; Li et al . , 2020 ;   Chen et al . , 2018 ) model the word - level interactions   between queries and documents in a ﬁne - grained   manner . For example , Yilmaz et al . ( 2019 ) pro-   pose Birch , where ( title , query ) pairs are input into   a pre - trained language model to compute match-   ing scores . Cross - encoders usually perform bet-   ter than bi - encoders ( Urbanek et al . , 2019 ) , but   have higher computation overhead since they can-   not cache the document representations . Since text   matching methods can retrieve the most relevant   sentence to query , we treat them as baseline meth-   ods and compare the performance with them in   Section 4 . However , the text matching methods   only consider the similarity between queries and   sentences , and ignore the contextual information   of webpages , which might be sub - optimal .   3 Methodology   In this section , we give the problem formulation of   query - aware snippet extraction . Then we introduce   our DeepQSE and Efﬁcient - DeepQSE in detail .   3.1 Problem Formulation   When a user submits a request with query q , the   search engine returns several webpages . Given one   of the webpage dwith title t , it contains several sen-   tences { s , s , ... s } , where Ris the max number   of sentences in a webpage . The snippet extraction   model aims to select several consecutive sentences   { s , ... s}as the snippet that can summarize   the webpage content in the context of input query .   Since the number of sentences nis given by the   pre - deﬁned snippet length , the snippet extraction   model needs to select the start sentence s.3.2 DeepQSE   DeepQSE aims to select snippets which can best   summarize the webpage content in the context of   the input query . The model structure of Deep-   QSE is shown in Figure 2 , which is composed   of a query encoder , a query - aware sentence en-   coder and a document - aware relevance encoder .   The query encoder learns query representations ,   which is initialized from a pre - trained language   model , such as BERT ( Devlin et al . , 2018 ) and   XML - RoBERTa ( Chi et al . , 2021 ) . Given the query   qand title t , the concatenation of them is input   into the query encoder . The ﬁnal hidden state of   the ﬁrst token is the query representation g. The   query - aware sentence encoder models the word-   level interactions between query , title and each sen-   tence to compute query - aware sentence representa-   tions . It is initialized from a pre - trained language   model , of which the input is the concatenation of   title , query and each sentence . The ﬁnal hidden   state of the ﬁrst token is the sentence representa-   tiong . The document - aware relevance encoder   aims to model the sentence - level interactions be-   tween the query and sentences in the context of   the whole webpage , which is composed of several   Transformer blocks ( Vaswani et al . , 2017 ) . We   concatenate the query representation and sentence   representations , add position embeddings and input   them into the document - aware relevance encoder .   The ﬁnal hidden states are used as document - aware   query - sentence relevance representations v , which   are then used to compute the selection score s.   3.3 Efﬁcient - DeepQSE   In DeepQSE , the query and each sentence are   jointly modeled , which may have slow compu-   tation speed for online serving . In order to re-   duce the computation overhead , we further design   an efﬁcient version of DeepQSE named Efﬁcient-   DeepQSE , which is shown in Figure 3 . We de-   compose the query - aware snippet extraction into   two stages , i.e. , coarse - grained candidate sentence   selection and ﬁne - grained relevance modeling .   3.3.1 Coarse - grained Sentence Selection   The coarse - grained candidate sentence selection   aims to select Kcandidate sentences and parse   them to the ﬁne - grained relevance model for ﬁ-   nal snippet extraction . It separates the modeling   of candidate sentences and queries , which enables   caching sentence representations for fast online   serving . The model structure of the coarse - grained3037   candidate selector is shown in Figure 3(a ) , which   contains three core modules , i.e. , a query encoder ,   a sentence encoder and a document - aware rele-   vance encoder . The query encoder and sentence   encoder aim to learn the query and sentence repre-   sentations respectively , which are initialized from   a pre - trained language model . We input the con-   catenation of query and title into the query encoder ,   and use the ﬁnal hidden states of the ﬁrst token as   the query representation h. The concatenation of   title and each sentence is input into the sentence   encoder , and the ﬁnal hidden states of the ﬁrst to-   ken are treated as the sentence representation h.   The document - aware sentence relevance encoder   aims to model query - sentence relevance in the con-   text of the whole webpage , which is composed of   several Transformer blocks . We concatenate the   query representation and sentence representations ,   add position embeddings and input them into the   document - aware sentence relevance encoder . The   ﬁnal hidden states are the document - aware query-   sentence relevance representations c , which are   further used to predict selection scores sthrough   an MLP network .   3.3.2 Fine - grained Relevance Modeling   The ﬁne - grained relevance modeling aims to cap-   ture the deep semantic relevance between query   and the candidate sentences parsed from the coarse-   grained sentence selection stage . It is composed of   a query encoder , a query - aware sentence encoder   and a document - aware relevance encoder , of which   the model structure is shown in Figure 3(b ) . A   naive implementation is directly using the samearchitecture of DeepQSE . However , in DeepQSE ,   the query and title are concatenated with differ-   entRsentences and their word representations are   repetitively computed for Rtimes . We assume the   word representations of query in the query - aware   sentence encoder have little help for query - aware   snippet selection , which is validated in Section 4.6 .   Therefore , we design the Cross Transformer , where   the word representations of the query and title   are only computed once in the query encoder and   parsed into the query - aware sentence encoder . The   architecture of a Cross Transformer block is shown   in Figure 4 .   The query encoder aims to learn query represen-   tations , which is initialized from a Transformer-   based pre - trained language model . We input the   concatenation of query and title into the query en-   coder and use the ﬁnal hidden state of the ﬁrst   token as the query representation l. Meanwhile ,   the query encoder outputs the key and value of ev-   ery multi - head self attention network to the query-   aware sentence encoder . Given the previous hidden   stateH , the key and value of the i - th multi - head   self attention network are computed as follows :   K = WH , V = WH , ( 1 )   where WandWare trainable parameters .   The query - aware sentence encoder aims to   model the ﬁne - grained interactions between query   and each sentence . It contains an embedding   layer and several Cross Transformer blocks , which   are initialized from a pre - trained language model .   Given a sentence s , we ﬁrst compute its initial hid-   den state Hthrough the embedding layer . The i - th3038   Cross Transformer block contains a multi - head at-   tention network and a feed - forward network . In or-   der to compute query - aware sentence hidden states ,   we modify the key ( or value ) of multi - head atten-   tion network as the concatenation of key ( or value )   from query encoder and the transformed hidden   state of the sentence . Given the hidden state H   of the previous Cross Transformer block , the query ,   key and value of the i - th multi - head attention net-   work are formulated as follows :   Q = WH ,   K = K||WH ,   V = V||WH,(2 )   where W , WandWare trainable parameters ,   ||is the concatenation operator . The query , key and   value are then input into the multi - head attention   network and feed - forward network to compute H.   We use the ﬁnal hidden state of the ﬁrst token as   the query - aware sentence representation l.   The document - aware relevance encoder aims to   model query - sentence relevance in the context of   the whole webpage , which contains several Trans-   former blocks . We concatenate the query represen-   tations and Kcandidate sentence representations ,   add position embeddings on them and input them   into several transformer blocks . The document-   aware query - sentence relevance representations f   are the ﬁnal hidden states , which are then fed into   an MLP to predict selection scores s.   3.3.3 Model Training and Serving   For model training , we use cross - entropy loss to   train the DeepQSE , which is computed as follows :   L=−/summationdisplayy×log(exp(s)/summationtextexp(s)),(3 )   where y∈ { 0,1}indicates whether the i - th sen-   tence is the start sentence of the snippet . For   Efﬁcient - DeepQSE , we also use the cross - entropy   loss to train the coarse - grained candidate sentence   selector and the ﬁne - grained relevance model re-   spectively , which is formulated as follows :   L=−/summationdisplayy×log(exp(s)/summationtextexp(s ) ) ,   L=−/summationdisplayy×log(exp(s )   /summationtextexp(s)).(4 )   For model serving , when a user submits a re-   quest with query q , the search engine returns sev-   eral webpages . For one of the webpages dwith title   t , DeepQSE directly computes selection scores for   all sentences { s , ... s } . The sentence with the   maximum selection score is selected . For Efﬁcient-   DeepQSE , the server needs to ofﬂine compute the   sentence representations of coarse - grained candi-   date sentence selector for every webpage . For   the webpage d , with its sentence representations   of the coarse - grained candidate sentence selector   [ h , ... h ] , we ﬁrst compute the query representa-   tion of the coarse - grained candidate sentence se-   lector hand the coarse - grained selection scores   { s , ... s } . Then we feed top - K candidate sen-   tences into the ﬁne - grained relevance model to   compute ﬁne - grained selection scores { s , ... s } .   The sentence with the maximum score is the start   sentence of the snippet s.3039   4 Experiments   4.1 Dataset and Experimental Settings   Since there is no off - the - shelf dataset for query-   aware snippet extraction , we ﬁrst manually labeled   two small English andMulti - lingual datasets , of   which the task is to select the more proper snippet   given a pair of candidate sentences . The Multi-   lingual dataset includes 10 languages , i.e. , Ger-   man , French , Spanish , Italian , Japanese , Korean ,   Portuguese , Russian and Chinese . Then we semi-   automatically build two large English andMulti-   lingual snippet extraction datasets with part of the   manually - labeled datasets , of which task is to select   the snippet from the sentences in the body . Due   to the space limitation , the detailed dataset con-   struction steps are described in Appendix 6 . 10 %   samples of the snippet extraction dataset are ran-   domly sampled for testing , and the rest for training .   We randomly sample 10 % samples of the training   dataset for validation . We also the rest manually   labeled dataset as another test dataset , which is   not used to construct the large snippet extraction   dataset . The detailed statistics of the datasets are   shown in Table 1 . We use precision@k ( k=1,3,5 )   as the evaluation metrics for performance on the   snippet extraction test dataset , accuracy ( ACC ) as   the evaluation metric for performance on the man-   ually labeled test dataset , ﬂoating - point operations   ( FLOPs ) and million seconds ( ms ) as the evaluation   metrics for efﬁciency .   4.2 Experimental Settings   In our experiments , we apply BERT - base ( De-   vlin et al . , 2018 ) for English dataset and a dis-   tilled XML - RoBERTa ( Chi et al . , 2021 ) for Multi - Lingual dataset to initialize the pre - trained lan-   guage model . We use Adam ( Kingma and Ba ,   2015 ) to optimize model training for both Deep-   QSE and Efﬁcient - DeepQSE . We set the learning   rate as 0.0001 and batch size as 64 . The maxi-   mum query length is 16 . The maximum title length   is 32 . The maximum sentence length is 64 . The   maximum number of sentences Rin a body is 160 .   The number of candidate sentences selected by   the coarse - grained sentence selector Kis 20 . All   hyper - parameters are selected according to the per-   formance on the validation dataset . We repeat each   experiment 5 times and report the average results   and the standard deviations .   4.3 Performance Comparison   We compare our method with multiple baselines ,   including conventional snippet extraction methods :   ( 1 ) CTS ( Turpin et al . , 2007 ) , extracting snippets   based on the number of overlapping words be-   tween queries and sentences ; ( 2 ) QUITE ( Zou et al . ,   2021 ) , selecting snippets with the summation of   importance scores of overlapping words between   queries and sentences ; PLM - empowered snippet   extraction methods : ( 3 ) QMSUM ( Zhong et al . ,   2021 ) , the locator of QMSUM for meeting sum-   marization which applies a ﬁxed PLM and CNN   to encode sentence and query , and a Transformer   to model interactions between sentences . ( 4 ) QB-   SUM ( Zhao et al . , 2021 ) , input the concatenation   of query and body into a PLM , and apply multi-   ple predictors to compute relevance scores ; some   text matching methods : ( 5 ) BM25 ( Robertson and   Zaragoza , 2009 ) , applying the BM25 algorithm   to compute similarity scores ; ( 6 ) DSSM ( Huang3040   et al . ,2013 ) , a deep structured semantic matching   method ; ( 7 ) C - DSSM ( Wan et al . , 2016 ) , a deep   semantic matching structure with convolution net-   work ; ( 8) MatchPyramid ( Pang et al . , 2016 ) , apply-   ing 2D convolution and max - pooling network on   the similarity matrix of query and sentence ; several   PLM - empowered text matching methods : ( 9 ) Poly-   Encoder ( Humeau et al . , 2020 ) , which adds a ﬁnal   attention mechanism to model the interactions be-   tween the cacheable multiple sentence representa-   tions and the query representation . ( 10 ) Birch ( Yil-   maz et al . , 2019 ) , inputting the concatenation of   queries and sentences into BERT for document re-   trieval ; ( 11 ) PARADE ( Li et al . , 2020 ) , using a   PLM to model similarity between sentences and   queries , and an aggregator to model interactions   between candidate sentences .   The performance of all methods on snippet ex-   traction test datasets is shown in Table 2 . The   performance of the methods on manually labeled   test datasets is shown in Table 3 . CTS , QUITE   and BM25 are deterministic methods , of which   standard deviations are zero . We have several ob-   servations from Table 2 . First , our DeepQSE and   Efﬁcient - DeepQSE outperform conventional snip-   pet extraction methods ( CTS and QUITE ) . This   is because these methods are based on the count-   ing features of overlapping words between queries   and sentences . Compared with our methods which   utilize PLMs , they can not capture the deep seman-   tic relation between query and sentences . Second ,   our methods outperform PLM - based snippet ex-   traction methods ( QMSUM and QBSUM ) . This   is because the simple body - query concatenation   in QBSUM may ﬂuctuate the information of the   short query . Due to the length limitation of PLM   some candidate sentences may be cut off . QM-   SUM is a bi - encoder which fails to encoder the   word - level interactions between the query and sen-   tences . Third , compared with several text matching   methods ( BM25 , DSSM , C - DSSM , MatchPyramid ,   Poly - Encoder , Birch , QBSUM ) , our methods have   better performance . This is because in our meth-   ods we apply webpage title and document - aware   relevance encoder to select snippets in the con-   text of the whole webpage , which can choose sen-   tences better summarizing the webpage in the con-   text of input query . Forth , PLM - based snippet ex-   traction methods outperform conventional snippet   extraction methods , and PLM - based text - matching   methods outperform shallow - model - based text-   matching methods . This is because the pre - trained   language model can help better understand the se-   mantic information in queries , titles and sentences .   Finally , cross - encoder - based text matching meth-   ods outperform bi - encoder - based text matching   methods . For example , MatchPyramid outperforms   CDSSM and DSSM , and Birch , PARADE and   DeepQSE outperform Poly - Encoder and QBSUM .   This is because bi - encoders only model sentence-   level similarity between queries and sentences , but   cross - encoders can model word - level similarity   between queries and sentences in a ﬁne - grained   manner . However , bi - encoders can cache sentence   representations , which have faster online serving   speed than cross - encoders .   4.4 Efﬁciency Comparison   In this subsection , we compare the efﬁciency of   DeepQSE and Efﬁcient - DeepQSE with baseline   methods . The results are summarized in Table 4.3041Since CTS , QUITE and BM25 are not based on   matrix multiplication and addition , we do not give   their FLOPs results . We have several observa-   tions from Table 4 . First , conventional snippet   extraction methods ( CTS and QUITE ) have rela-   tively low computation costs . This is because they   are based on simple hand - crafted features , which   can be calculated quickly . Second , cross - encoder-   based methods are more time - consuming than bi-   encoder - based methods . For example , DSSM and   CDSSM are more efﬁcient than MatchPyramid ,   and Poly - Encoder and QMSUM are more efﬁ-   cient than Birch , PARADE , QBSUM and Deep-   QSE . This is because the sentence representations   in bi - encoder - based methods can be cached for   quick inference . Third , PLM - based methods ( Poly-   Encoder , Birch , PARADE , QMSUM , QBSUM ,   DeepQSE and Efﬁcient - DeepQSE ) have higher   computation costs than other methods . This is be-   cause pre - trained language models have large size   of parameters , of which the computation cost is   high ( Sanh et al . , 2019 ; Beltagy et al . , 2020 ; Jiao   et al . ,2020 ; Sun et al . , 2020 ) . Finally , considering   both efﬁciency and the previous performance analy-   sis in Section 4.3 , our Efﬁcient - DeepQSE achieves   a great trade - off between performance and efﬁ-   ciency . This is because our Efﬁcient - DeepQSE ap-   plies two - stage model , in which the coarse - grained   selector can quickly select candidates for the ﬁne-   grained relevance encoder . In addition , we de-   sign the Cross Transformer which avoids repeti-   tively computing contextual word representations   of the same query for different candidate sentences .   Therefore , our Efﬁcient - DeepQSE has a lower com-   putation cost while keeping its performance .   4.5 Efﬁciency Analysis   In this subsection , we analyze how the Efﬁcient-   DeepQSE reduces the computation overhead of   DeepQSE with a minor performance drop . Com-   pared with DeepQSE , the core improvement of   Efﬁcient - DeepQSE is the Cross Transformer , the   coarse - grained candidate sentence selector and the   ﬁne - grained relevance model . We remove these   modules separately and show their performance   and efﬁciency in Figure 5 , Figure 6and Figure 7 .   We have several observations from the results .   First , Efﬁcient - DeepQSE has lower performance   and lower computation overhead without the ﬁne-   grained relevance model . This is because the ﬁne-   grained relevance model captures the deep seman-   tic relevance between queries , titles and sentences ,   which can improve the performance . And with-   out the ﬁne - grained relevance model , Efﬁcient-   DeepQSE does not need to perform the second   stage , which lowers the computation overhead . Sec-   ond , Efﬁcient - DeepQSE can achieve higher per-   formance but higher computation overhead with-   out the coarse - grained candidate sentence selec-   tor . This is because the coarse - grained candidate   sentence selector may select candidate sentences   incorrectly , which increases the error rate . How-   ever , it helps decrease the input size of the second   stage . Therefore , the computation overhead gets   higher without the coarse - grained candidate sen-   tence selector . Finally , the computation overhead   is higher without Cross Transformer . This is be-   cause in Cross Transformer we only compute the   query and title representations once , which avoids   the repetitive computation in DeepQSE . Combined   with these components , the Efﬁcient - DeepQSE re-   duces the computation overhead and achieves com-   parable performance with DeepQSE.3042   4.6 Ablation Study   In this section , we analyze the impact of adding   titles , queries and the document - aware relevance   encoder . Due to space limitation , we only show   the experimental result on Efﬁcient - DeepQSE . The   same phenomenon can be observed on DeepQSE   as well . The results are shown in Figure 8and Fig-   ure9 . From the results , we can observe that the   performance of Efﬁcient - DeepQSE gets lower with-   out titles . This is because the titles can be treated   as brief abstracts of webpages , which can help the   model select sentences better summarizing the web-   page content ( Wang et al . , 2007 ) . It is also observed   that the performance drops without queries . This is   because the selected snippets should not only sum-   marize the webpage content , but also be relevant to   queries . Finally , the document - aware relevance en-   coder ( DaRE ) beneﬁts snippet extraction . This may   be because the document - aware relevance encoder   can model the query - document relevance in the   context of the whole webpage , which helps select   snippets better summarizing the webpage content .   5 Conclusion   In this paper , we propose a query - aware snippet   extraction model for web search named DeepQSE .   DeepQSE ﬁrst learns a query - aware sentence rep-   resentation by modeling ﬁne - grained interactions   between queries , titles and sentences , then learns   document - aware sentence relevance representa-   tions for snippet extraction . To lower the computa-   tion overhead of DeepQSE , we further design the   Efﬁcient - DeepQSE , where the snippet extraction   is decomposed into two stages , i.e. coarse - grained   candidate sentence selection and ﬁne - grained rele-   vance modeling . The coarse - grained selector can   cache the sentence representations for fast online   serving and parse several candidate sentences to the   ﬁne - grained relevance model . In the ﬁne - grained   relevance model , we further design a Cross Trans-   former , to avoid the repetitive computation of query   and title representations for different sentences . Ex-   tensive experiments validate the effectiveness and   efﬁciency of our approach .   6 Limitations   Our DeepQSE is a cross - encoder - based snippet   extraction method . It has great performance but   heavy computation overhead , which is not ben-   eﬁcial for online inference . We further propose   Efﬁcient - DeepQSE , an efﬁcient version of Deep-   QSE , which divides the snippet extraction into two   stages . Although the Efﬁcient - DeepQSE keeps the   performance of DeepQSE and has much lower com-   putation overhead than other PLM - based methods ,   it still has larger computation overhead than the   conventional shallow - model - based methods . We   plan to further improve the efﬁciency of the snippet   extraction algorithm in the future.3043References30443045   Appendix   Detailed Data Construction Steps   The detailed data construction steps are as follows :   Collect manually labeled dataset : Given a pair   of candidate snippets extracted from a document ,   human evaluators are asked to select the more ap-   propriate one according to the corresponding query .   The manually labeled dataset can be formulated as   { ( q , s , s , d , l)|0≤i < M } , where Mis the   number of samples in the dataset , qis the query ,   sandsare candidate snippets , dis the docu-   ment , l∈ { 0,1}is the label . lequals 0when s   is more suitable than s , otherwise lequals 1 . At   least three annotators are assigned for a sample .   Build snippet extraction dataset : Since the   manually labeled dataset is for pair - wise selection   and is small for large PLM - based models , we train   an ensemble model with the dataset to extract snip-   pets for different documents according to corre-   sponding queries . The extracted samples with high   conﬁdence scores are then used as the snippet ex-   traction dataset .   Impact of Candidate Number   In this subsection , we study the impact of the num-   ber of candidate sentences selected by the coarse-   grained selector . The experimental results are   shown in Figure 12(a ) and Figure 12(b ) . We ob-   serve that with larger candidate sentence number ,   the performance of Efﬁcient - DeepQSE is higher .   This may be because the probability that the ground   truth sentence is selected in the candidate sen-   tences gets higher . However , the computation over-   head of Efﬁcient - DeepQSE linearly increases with   larger candidate sentence number . How to choose   a proper candidate sentence number to achieve a   great trade - off between performance and efﬁciency   is the key point of our method .   Case Study   In this subsection , we show some snippets extracted   by our DeepQSE in Figure 10and our Efﬁcient-   DeepQSE in Figure 11 . In all cases , the snippets   are relevant to the input query . This is because we   model the word - level interactions between query   and sentences in DeepQSE and Efﬁcient - DeepQSE .   Meanwhile , the selected snippets summarize the   webpage content in the context of the input query .   This is because we consider the context of webpage   in the document - aware relevance encoder , which   enables our method to capture the global webpage   information .   Experimental Environments   We conduct experiments with a Linux server with   8 V100 GPUs with 32 GB memory . The version of   CUDA is 11.1 . We implement both DeepQSE and   Efﬁcient - DeepQSE with pytorch 1.9.1.3046