  Jungo KasaiKeisuke SakaguchiRonan Le BrasHao Peng   Ximing LuDragomir RadevYejin ChoiNoah A. SmithPaul G. Allen School of Computer Science & Engineering , University of WashingtonAllen Institute for AITohoku UniversityDepartment of Computer Science , Yale University   { jkasai,lux32,yejin,nasmith}@cs.washington.edu keisuke.sakaguchi@tohoku.ac.jp   { ronanlb,haop}@allenai.org dragomir.radev@yale.edu   Abstract   Many language generation models are now   available for a wide range of generation tasks ,   including machine translation and summariza-   tion . Combining such diverse models may   lead to further progress , but ensembling gener-   ation models is challenging during inference :   conventional ensembling methods ( e.g. , shal-   low fusion ) require that the models share vo-   cabulary / tokenization schemes . We introduce   T decoding , a simple and general text   generation algorithm that benefits from diverse   models at inference time . Our method does   not assume the vocabulary , tokenization or   even generation order is shared . Our exten-   sive evaluations on machine translation and   scientific paper summarization demonstrate   thatT decoding substantially outperforms   each model decoded in isolation over vari-   ous scenarios , including cases where domain-   specific and general - purpose models are both   available . T decoding also consistently   outperforms the popular reranking heuristic   where output candidates from one model are   rescored by another . We hope that our work   will encourage researchers and practitioners to   examine generation models collectively , not   just independently , and to seek out models with   complementary strengths to the currently avail-   able models .   1 Introduction   Natural language generation is an important build-   ing block for many applications , such as machine   translation , summarization , and question answer-   ing ( Ng et al . , 2019 ; Lewis et al . , 2020 ; Raffel et al . ,   2020 ; Brown et al . , 2020 ; Asai et al . , 2021 , inter   alia ) . Researchers have recently explored and ad-   vanced models for generation in various aspects , in - Figure 1 : T decoding of two generation models ,   fandg , that does not assume a shared vocabulary , tok-   enization , or generation order . Beam search is first ap-   plied to fto generate y , followed by output mapping   to˜︁y(e.g . , f ’s detokenization and g ’s tokenization or   sequence reversal ) . gis then decoded with beam search   augmented with distances from the set of previously-   generated outputs ( here only one sequence yis shown ):   d(z,˜︁y ) . Subsequently , fis similarly decoded with   g ’s guidance . Here we show one iteration that already   achieves substantial improvements ( § 4 ) . @ indicates   the BPE separator .   cluding model architecture ( Bahdanau et al . , 2015 ;   Vaswani et al . , 2017 ) , domain adaptation ( Chu and   Wang , 2018 ; Bapna and Firat , 2019 ) , prompting   ( Brown et al . , 2020 ) , and even generation order   ( Gu et al . , 2018 ) . The resulting generation models   are diverse , trained on different data , with different4909assumptions , at different times . We hypothesize   that diverse generation models may achieve bet-   ter results through ensembling , if the various ap-   proaches have complementary strengths . Given the   high cost of unifying approaches during training   time ( Strubell et al . , 2019 ; Schwartz et al . , 2019 ) ,   inference - time combination of existing models is   an attractive alternative .   One well - established ensembling technique is   “ shallow fusion ” ( Sutskever et al . , 2014 ; Gulcehre   et al . , 2015 ; Firat et al . , 2016 , inter alia ) , which ag-   gregates models ’ scores during beam search . This   approach requires , however , that the models use the   same vocabulary / tokenization scheme and organize   the search in the same way ( e.g. , autoregressive ,   left - to - right factorization ) .   We introduce a new inference algorithm , T   decoding ( Fig . 1 ) , that enables more diverse gener-   ators to guide each other . T decoding can   combine generators with different vocabularies ,   ( de)tokenization , and even generation order with-   out any additional training or finetuning . Our   method decodes a model by standard beam search ,   but the scores at every step incorporate a simple   function that measures the distance from outputs   of the other model . We run this procedure on each   generation model in turn , so that both can benefit   from each other .   We present extensive experiments on machine   translation and scientific paper summarization   and show that T decoding can improve per-   formance over each model decoded in isolation   across several scenarios : combining 1 ) generic   and domain - specific models , 2 ) left - to - right and   right - to - left generation models , and 3 ) models that   generate using different conditioning inputs . Our   results show consistent performance gains from   combining generic and domain - specific translation   models over a wide range of domains , including   medical and legal translation . Applications in these   domains require particularly high accuracy , and   T decoding is a desirable alternative to stan-   dard beam search on a single model . Interestingly ,   we find that T decoding between generic and   domain models is effective even when parallel data   from the domain are scarce and the domain model   yields poor performance by itself , suggesting com-   plementary strengths of diverse generators ( § 3.4 ) .   T decoding can be seen as a generalization   of reranking heuristics that have proven effective   in syntactic parsing ( Shen and Joshi , 2003 ; Char-   niak and Johnson , 2005 ; Collins and Koo , 2005 ) ,   speech recognition ( Collins et al . , 2005 ) , and ma-   chine translation ( Shen et al . , 2004 ; Och et al . ,   2004 ): one model generates candidate sequences ,   followed by rescoring from another model . We   present extensive comparisons with reranking base-   lines and demonstrate that T decoding out-   performs reranking consistently . We also observe   that since the encoder computations on two models   can be parallelized , the inference time required for   T decoding is much shorter than the sum of   the two models , resulting only in a 50 % increase ,   relative to decoding of a single model in isolation   ( § 4 ) . T decoding is therefore a viable alterna-   tive to standard beam search on a single model and   the widespread reranking heuristic .   2 T Decoding   We propose T decoding , a general decoding   algorithm that generates text from diverse models   without assumptions of a shared vocabulary , tok-   enization , or generation order . At the core of the4910algorithm is a simple modification in standard beam   search ( highlighted in Fig . 2 ) ; we incorporate into   a scoring function the distance from outputs that   are previously generated by another model .   2.1 Initial Decoding   Let us assume that we have two generation mod-   els : fandg . Both fandgassign scores to out-   put sequences . For example , fcan be a domain-   specific translation model and ga generic one . f   andgperform their own pre / postprocessing ( e.g. ,   ( de)tokenization ) and factorization ( e.g. , left - to-   right or right - to - left factorization ) . Here we sup-   press for brevity the conditional dependence on   the input ( e.g. , machine translation input from the   source language ) . Standard beam search with a   beam size kis first applied to fto produce a set   ofkoutput sequences : Y. This approximately   solves topkf(y)by pruned breadth - first search ,   and often returns higher - quality outputs than the ex-   act search counterpart ( Stahlberg and Byrne , 2019 ;   Meister et al . , 2020a ) .   2.2 Mutually - Guided Decoding   OnceYis obtained , we proceed with decoding   generators with mutual guidance ( Fig . 2 ; t≥1 ) .   Output Sequence Mapping . The commonly-   used technique of ensembling ( Sutskever et al . ,   2014 ) or shallow fusion ( Gulcehre et al . , 2015 ;   Stahlberg et al . , 2018 ) adds scores from fandg   at every step and executes the same search algo-   rithm to approximately solve topkf(y ) + g(y ) .   This method thus necessitates a shared vocabulary ,   tokenization , and generation order ( Imamura and   Sumita , 2017 ) . We relax this assumption and first   map the candidates in Yto output sequences   forg:˜︁Y(Line 1 in Fig . 2 ) . This mapping   ( map _ output ) typically involves deterministic op-   erations of f ’s detokenization followed by g ’s tok-   enization . Sequence reversal is also performed if   fandggenerate in the opposite order . For exam-   ple , if guses byte - pair encoding ( Sennrich et al . ,   2016b ) , but fdoes not , we might have y = John   does n’t like Mary mapped to ˜︁y = Jo@ hn does n’t   like Mar@ y , where @ denotes subword separation . Decoding with Distance Terms . We then decode   gwith guidance from ˜︁Y. Specifically , we per-   form beam search with a simple modification in   scoring ( Line 7 ) . In this work , we use a simple   distance measure that adds binary distances at all   positions ( i.e. , the Hamming distance ):   d(z,˜︁y ) = ∑︂1{z̸=˜︁y }   We also explored using the distance be-   tween ( sub)word embeddings from the model:∑︁∥e(z)−e(˜︁y)∥ , but this did not bring   improvements ( § 4 ) . Note also that when iexceeds   the length of ˜︁y , we assume ˜︁y=. The overall   distance term is then   mind(z,˜︁y )   Here we minimize over the output sequences to   compute the distance to the closest candidate .   These candidates from ˜︁Ycan be equally good   outputs but differ only by one word ; in such cases ,   this minimization operation avoids overestimation   of the distances . The new score at step nin beam   search is now computed by :   g(z)−λmind(z,˜︁y ) ,   where λis a scalar coefficient for the distance   term that controls the importance of frelative to   g. We tune λ∈ { 0.1,0.3,1.0,3.0}during devel-   opment . After this beam search , we obtain a new   candidate set , Z. We then run the same beam   search ( Fig . 2 ) with the roles of f , Yandg , Z   swapped . Namely , we decode fwith distance   terms from Zat each step of beam search :   f(y)−λmind(y,˜︁z )   Finally , the highest - scoring sequence from Yis   output . This process of mutually - guided decod-   ing can be repeated multiple times . We observe ,   however , that one iteration ( t=1 ) suffices to bring   performance gains ( § 4 ) . We also present detailed   sensitivity analysis over varying λandλand   find that T decoding is particularly effective   when λ > λ(i.e . , initial exploration by gis en-   couraged with relatively little guidance from f ’s   original outputs ; see § 4 ) .   Reranking Heuristic as a Special Case . No-   tice that as λ→ ∞ , g ’s generation falls back   to a reranking heuristic : top ksequences from   the initial fdecoding are reranked according to4911 g. This reranking heuristic has proven successful   in a wide range of sequence generation tasks , in-   cluding machine translation ( Shen et al . , 2004 ) , syn-   tactic parsing ( Collins and Koo , 2005 ) , and speech   recognition ( Collins et al . , 2005 ) . Reranking is per-   formed in many strong machine translation systems   to use a right - to - left model to improve a left - to-   right model ; e.g. , top - performing systems in recent   WMT competitions ( Ng et al . , 2019 ; Kiyono et al . ,   2020 ; Wang et al . , 2021 ; Akhbardeh et al . , 2021 ) .   In our experiments , we extensively compare per-   formances of T decoding and reranking and   demonstrate that the former consistently outper-   forms the latter .   3 Experiments   We present experiments across three scenarios :   combining domain and generic models for machine   translation ( § 3.1 ) , left - to - right and right - to - left ma-   chine translation models ( § 3.2 ) , and scientific paper   summarization models that take as input different   parts of the paper ( § 3.3 ) . We empirically compare   T decoding with decoding in isolation and   the widely - adopted reranking baselines , illustrating   thatT decoding offers performance improve-   ments in various situations without any change to   the trained models .   3.1 Domain and Generic Models   Machine translation has now been used for many   domains , ranging from everyday conversations to   medical documents . Machine translation models   are often trained on large amounts of parallel data ,   such as the Europarl corpus ( Koehn , 2005 ) and the   OPUS data ( Tiedemann , 2012 ) . Applying these   models to out - of - domain data remains a challenge   ( Koehn and Knowles , 2017 ; Chu and Wang , 2018 ) ,   and users for some of these domains require high   accuracy in translation ( e.g. , medical and legal doc-   uments ) . We will demonstrate that T decod-   ing between general - purpose and domain - specific   models is a viable approach to tackle this problem .   Setups . We use machine translation datasets over   diverse domains from prior work ( Koehn and   Knowles , 2017 ; Hu et al . , 2019 ): German →English   over medical ( 1.1 M training sentence pairs ) , le-   gal ( 720 K pairs ) , Koran ( religious text , 480 K   pairs ) , and subtitles ( 14 M pairs ) domains . For   the domain - specific models , we train a base - sizedtransformer model ( Vaswani et al . , 2017 ) with a   6 - layer encoder and a 6 - layer decoder on the train-   ing data of each domain . The top - performing   German →English system from WMT19 ( Barrault   et al . , 2019 ; Ng et al . , 2019)is used as the   generic model . This generic model is a large-   sized transformer trained on a concatenation of   publicly available parallel data , including the Eu-   roparl ( Koehn , 2005 ) and UN ( Ziemski et al . , 2016 )   corpora with the backtranslation technique ( Sen-   nrich et al . , 2016a ) . We follow ( de)tokenization   ( Koehn et al . , 2007 ) and byte - pair encoding ( Sen-   nrich et al . , 2016b ) of previous work ( Koehn and   Knowles , 2017 ; Hu et al . , 2019 ) .   For every domain , we evaluate a total of six con-   figurations : decoding of the generic and domain   models each in isolation ; the reranking baseline and   T decoding with fbeing the generic model   andgbeing the domain model , as well as the ver-   sions where fandgare swapped to see the effect of   the two roles . In all cases , we use beam size 5 ( Fre-   itag and Al - Onaizan , 2017 ) and length penalty 1   ( Wu et al . , 2016 ) and conduct all experiments using   thefairseq library ( Ott et al . , 2019 ) . All perfor-   mance is measured with the COMET score ( Rei   et al . , 2020a , b ) and the S BLEU implementa-   tion ( Post , 2018 ) of the BLEU score ( Papineni et al . ,   2002 ) . Note that COMET is based on crosslingual   contextual representations ( Conneau et al . , 2020 ) ,   and recent work showed that it achieves signifi-   cantly higher correlation with expert human judg-   ment than BLEU and other n - gram - based metrics   ( Kasai et al . , 2022a , c ) . More experimental details   are described in Appendix § A.1 .   Results . Seen in Table 1 are the results from our   experiments over various domains . Firstly , given   two translation models fandg , T decoding   outperforms the reranking baseline in all configura-   tions ( indicated in blue ) with only one exception ( a   small drop in BLEU in the subtitles domain ) . Par-   ticularly noteworthy are the gains in the medical   domain : T decoding outperforms the rerank-   ing heuristic by 5.8 COMET and 1.4 BLEU points   when fis the domain model and gis the generic   model . T decoding is thus an effective gen-   eralization over the reranking heuristic commonly   used in the literature across domains .   Comparing the performance of decoding in iso-4912   lation and T decoding , we observe that the   best score from T decoding substantially out-   performs each individual model over all domains :   e.g. , 81.6 vs. 80.7 ( domain model ) and 81.6 vs.   44.5 ( generic model ) COMET points in the medi-   cal domain . In both medical and legal domains , the   generic model underperforms the domain model   by a large margin . Nonetheless , T decoding   between the two improves over the domain model ,   suggesting that T decoding makes use of their   complementary strengths . Finally , we see a consis-   tent pattern regarding fandg : both T decod-   ing and the reranking baseline perform better when   the higher - performing model is chosen as f. ( e.g. ,   the domain model performs better in medicine and   law , and vice versa in subtitles . ) This is expected   because fis used both for initial decoding and final   decoding with g ’s guidance ( Fig . 1 ) .   3.2 Left - to - Right and Right - to - Left Models   Language generation models usually factorize se-   quences autoregressively in a left - to - right order ,   but previous work showed that left - to - right ( L2R )   models can be improved by reranking their outputs   with a separate right - to - left ( R2L ) model ( Imamura   and Sumita , 2017 ; Ng et al . , 2019 ; Kiyono et al . ,   2020 , inter alia ) .T decoding can be readily   applied to such scenarios since it does not assume   shared generation order between models .   Setups . We experiment with two language   pairs from the WMT 2020 news translation   task ( Barrault et al . , 2020 ): Chinese →English   ( WMT20 ZH - EN , 48 M training sentence pairs )   and English →German ( WMT20 EN - DE , 48Mpairs ) . Submissions for these language pairs to the   shared task have human evaluations from profes-   sional translators ( Freitag et al . , 2021 ) , and the cor-   relation between automatic metrics and the human   ratings are studied in subsequent work ( Kasai et al . ,   2022a ) ; COMET ( Rei et al . , 2020b , a ) achieves the   highest correlation out of the 15 + metrics .   Similar to the previous experiments , we mea-   sure all performance using COMET and BLEU   scores . Note that we use two reference transla-   tions per instance for WMT20 ZH - EN and three   for WMT20 EN - DE , following Kasai et al . ( 2022a ) .   They both have reference translations from two   different services , and WMT20 EN - DE has an ad-   ditional translation created by linguists who are   asked to paraphrase the two translations as much as   possible . These paraphrased translations are shown   to increase correlation with human judgments by   mitigating the translationese effect ( Graham et al . ,   2020 ) and diversifying the reference ( Freitag et al . ,   2020 ) . On each dataset , we follow the preprocess-   ing and tokenization ( Koehn et al . , 2007 ; Sennrich   et al . , 2016b ) from Kasai et al . ( 2022a)and train   a large - sized transformer model for left - to - right   and right - to - left translation , in which the output   English / German sequences are reversed after tok-   enization . We implement all models and decoding   with fairseq and apply beam search with beam   size 5 and length penalty 1 . We again consider a   total of six settings : reranking and T decod-   ing with L2R as fand R2L as gor the reverse , as   well as the individual models . Further details can4913be found in Appendix § A.2 .   Results . Table 2 shows the results from L2R and   R2L translation models . T decoding again   outperforms the reranking counterpart by a con-   siderable margin in COMET and BLEU on both   language pairs ; e.g. , 43.1 vs. 41.2 COMET points   on WMT20 ZH - EN when fis R2L and gis L2R.   The best performance is achieved by T decod-   ing on both datasets and improves over the indi-   vidual models by more than 1 BLEU point . The   reranking baseline , on the other hand , does not out-   perform L2R in BLEU when fis R2L : 35.4 vs.   35.5 ( ZH - EN ) and 45.2 vs. 45.5 ( EN - DE ) . This   result illustrates that T decoding is a more ef-   fective approach to combine models with different   generation order than the popular reranking .   3.3 Summarization with Different Input   We also experiment with strong models on a highly   abstractive scientific paper summarization task : Sc-   iTLDR ( Cachola et al . , 2020 ) . Specifically , we use   two BART - based models from prior work ( Cachola   et al . , 2020 ) that differ in input type : one that only   takes as input the paper abstract ( Abst . ) and the   other a concatenation of the abstract , introduction ,   and conclusion ( AIC ) .   Setups . We use the train / dev./test split from Ca-   chola et al . ( 2020 ) . Again following Cachola et al .   ( 2020 ) , we use all human - written summaries ( writ-   ten either by authors or undergraduate computer   science students ) as the reference and evaluate   performance in terms of the ROUGE score ( Lin,2004).We average the instance - level scores from   the Python rouge - score implementation . Similar   to our previous experiments , we use beam size 5   and length penalty 1 . See more detail in Appendix   A.3 .   Results . Table 3 presents our results . T   decoding substantially outperforms the reranking   baseline when fis the AIC model ( e.g. , +0.5   ROUGE - L points ) , but they yield ( almost ) the same   performance when fis the Abst . model . Nonethe-   less , T decoding achieves the best perfor-   mance out of all configurations . Our small im-   provements might be attributed to the fact that the   input to the Abst . model is a strict subset of the   AIC model and there are only limited benefits from   combining them .   3.4 Low - Resource Scenarios   In our experiments over four diverse domains   ( § 3.1 ) , we assumed that plenty of parallel data is   available in every domain , and the domain model   generally outperformed the generic model . Con-   cretely , we used 1.1 M and 720 K training sentence   pairs for the medical and legal domains , based on   the data splits from previous work ( Koehn and   Knowles , 2017 ; Hu et al . , 2019 ) . In real - world   applications , however , these domain - specific trans-   lation data are often scarce since they need to be   annotated by bilingual speakers with expertise in4914those domains . The question arises : can a domain   model trained on small parallel data still help the   generic model by its complementary strengths ? To   simulate such low - resource scenarios , we randomly   sample { 10k , 20k , 40k , 80k } sentence pairs and   conduct the same evaluations with the generic and   domain models as fandg , respectively .   Fig . 3 plots COMET scores of various decoding   methods on the medical and legal domains . The   score from the generic model is constant because   we only change the domain training data . There   is a striking trend : even though the domain model   performs poorly by itself , it improves the generic   model through T decoding over varying sizes .   Reranking also helps the generic model as the data   size increases , but the improvement is less pro-   nounced than that of T decoding .   4 Analysis   Iterations . So far , we have only applied one iter-   ation of T decoding , but Fig . 4 plots perfor-   mance over multiple iterations . Iteration 0 signifies   f ’s initial decoding ( yin Fig . 1 ) , and every it-   eration involves g ’s decoding with f ’s guidance   ( z ) and its reverse ( y ) . We observe that the   first iteration brings most of the performance gains .   This makes T decoding practically appealing ,   as it improves performance without much increase   in the computation or inference time ( see below ) .   Inference Time . Table 4 reports the runtime of   each decoding method , relative to f ’s decoding in   isolation . We use batch size 1 on the same sin-   gle A100 - SXM GPU and measure the wall - clocktime from when all models are loaded until all   outputs are obtained . As expected , T decod-   ing results in a slowdown compared to decoding   in isolation , but the increase in time is only 50 % .   The inference time for T decoding is much   shorter than the sum of fandgin isolation ( 1.4 ×   vs. 2.1×on medical translation ) because 1 ) the en-   coder computation for fandgcan be parallelized   and 2 ) the encoder computation for fis done only   once while we need two runs of f ’s decoder . We   leave it to future work to further speed up T   decoding ; since the slowdown of T decoding   primarily comes from the decoder , it can be sped   up by best - first beam search ( Meister et al . , 2020b ) ,   a deep - encoder , shallow - decoder strategy ( Kasai   et al . , 2021a ) , or a fast , linear - complexity variant   of the transformer decoder ( Peng et al . , 2021 ; Ka-   sai et al . , 2021b ) that is shown to retain the perfor-   mance of the standard encoder - decoder transformer .   Another approach could be sequence - level knowl-   edge distillation ( Kim and Rush , 2016 ) , which has   proven successful in speeding up an ensemble trans-   lation model ( Freitag et al . , 2017).4915   Sensitivity Analysis on Distance Coefficients .   As discussed in § 2.2 , λandλweight the dis-   tance terms fromfandgrespectively . We tuned   λandλon the dev . set from the range of   { 0.1,0.3,1.0,3.0 } . Fig . 5 visualizes how they   affect the overall performance on the dev . sets .   λ > λgenerally yields good performance , sug-   gesting the effectiveness of the initial exploration   bygwith relatively weaker guidance from f.   Variants of Distance Functions . We experiment   with two variants of distance terms ( Table 5 ): 1 )   one candidate , which measures the distance from   the 1 - best candidate from the other model ( vs. mini-   mization over multiple candidates ; § 2.2 ) and 2 ) em-   bed . distance , which calculates the distance based   on the Euclidean distance between the embeddings .   Here the embeddings are taken from the output   layer of the decoder . Overall , both variants yield   similar performance to the original distance func-   tion , but the one candidate method has a substantial   performance drop on WMT20 ZH - EN . Note also   that the embed . distance method necessitates ad-   ditional distance computations between the token   embeddings . This result illustrates that our original   distance function is a simple yet effective design   choice .   Examples . Seen in Table 6 are example   German →English translations from the medical   domain . The left section presents a case where   the domain model translates the technical term , Spätdyskinesie , into the corresponding English   term : tardive dyskinesia . The generic model ,   on the other hand , generates a literal transla-   tion : late dyskinesia . In the right section , the   domain model fails to handle the coordinate   structure : 12.1 % and 3.2 % with aripiprazole vs.   12.1 % with aripiprazole and 3.2 % with placebo .   Further , the final output has wording closer   to the reference translation : trials vs.studies   and bipolar patients vs.bipolar disorder . These   examples illustrate that T decoding benefits   from the complementary strengths of the domain   and generic models .   5 Further Related Work   Decoding from Multiple Models . Much early   work proposed methods to generate text from mul-   tiple models especially for machine translation ( of-   ten called consensus - based decoding ; Bangalore   et al . , 2001 , 2002 ; Matusov et al . , 2006 ; Rosti   et al . , 2007 ; Sim et al . , 2007 ; Hildebrand and V ogel ,   2008 ) . Most of these methods limit their search   space to n - best candidates from individual transla-   tion models ( Li et al . , 2009 ) , contrasting with our   T decoding where one model can update its   translation outputs under the guidance of another   model . Collaborative decoding ( Li et al . , 2009 )   trains a separate feature - based scorer that measures   the consensus between phrase - based Chinese - to-   English translation models . Several recent works   proposed inference algorithms for decoding from   multiple generators for specific tasks , such as detox-   ification and abductive reasoning ( West et al . , 2021 ;   Liu et al . , 2021 ) .   Alternatives to Left - to - Right Decoding . We   showed that T decoding can be used to ben-   efit from models with diverging generation order .   Several prior works proposed approaches for gen-   erating text in a different fashion than the stan-   dard left - to - right order . For example , much recent   work explored non - autoregressive generation ( Gu   et al . , 2018 ; Lee et al . , 2018 ; Mansimov et al . , 2019 ;   Ghazvininejad et al . , 2019 ; Kasai et al . , 2020 , inter   alia ) primarily to parallelize and speed up infer-   ence . More specifically , several works introduced   training and/or inference algorithms that combine   left - to - right and right - to - left models for machine   translation ( Zhou et al . , 2019 ) and commonsense   inference ( Zaidi et al . , 2020 ) . Qin et al . ( 2020 ) in-   corporated right ( future ) context into a left - to - right   language model by iterative gradient - based updates4916   on the output representations . Those algorithms   are designed specifically for the combination of   left - to - right and right - to - left generation and can not   be easily extended to more general situations , such   as diverging tokenization and vocabularies where   T decoding has been shown effective .   6 Conclusion   We presented T decoding , a general inference   algorithm that generates text from diverse models   without the assumption of a shared vocabulary , tok-   enization , or generation order . Our method enables   diverse models to guide each other , thereby outper-   forming individual models over various scenarios ,   even when one of the models is much weaker be-   cause of limited data . We also demonstrated that   T decoding can be viewed as a generalization   and improvement of the commonly - adopted rerank-   ing heuristic . As it only requires a small change   in code , we hope that researchers and practitioners   will explore complementary strengths of diverse   generation models through T decoding .   Limitations   We evaluated our decoding method that combines   generation models both on machine translation and   scientific paper summarization over several sce-   narios : combining 1 ) generic and domain - specific   models , 2 ) left - to - right and right - to - left generation   models , and 3 ) models that generate using differ-   ent conditioning inputs . Our machine translationexperiments span diverse domains , including medi-   cal and legal text . We also presented results from   recent English - to - German and Chinese - to - English   WMT data . Nonetheless , our domain translation   experiments are limited to German - to - English , and   we only dealt with scientific papers written in En-   glish , mainly due to availability of data . There   are also many other language generation tasks for   which our method can be useful . Since we open-   source our codebase built on top of a popular li-   brary , we hope that practitioners will use it for   applications of their interest and further assess our   decoding algorithm in many application scenarios .   Evaluating language generation remains a chal-   lenging research problem . We carefully set up our   experiments to mitigate potential evaluation issues .   The WMT 2020 test data consist only of news text   written in the original language , in contrast to the   test data from WMT 2018 ( Bojar et al . , 2018 ) or   earlier . The WMT 2020 EN →DE and DE →EN   test data that we used thus come from completely   different documents . This avoids the translationese   effect that would overestimate the translation per-   formance due to the simplicity of translated text   ( Graham et al . , 2020 ) . Moreover , the WMT 2020   test data for English - to - German and Chinese - to-   English translation have multiple reference trans-   lations per instance , which increases the correla-   tion of reference - based , automatic evaluations with   human judgment ( Kasai et al . , 2022a ) . We pre-   sented results using automatic metrics from recent   work ( Rei et al . , 2020b ) as well as conventional,4917n - gram overlap metrics ( Papineni et al . , 2002 ; Lin ,   2004 ) . Recent automatic metrics have shown to   have higher correlation with human judgments , but   human judgments are sometimes inconsistent , es-   pecially when crowdsourced ( Clark et al . , 2021 ;   Kasai et al . , 2022c ) . Since our decoding method   is a simple modification of the widely - used beam   search algorithm , we hope that it will be tested and   used in real - world systems of language generation .   Acknowledgements   We thank Hila Gonen , Phillip Keung , the ARK   group at the UW , and the Mosaic team at the   Allen Institute for AI for their helpful feedback   on this work . This work was supported in part by   the DARPA MCS program through NIWC Pacific   ( N66001 - 19 - 2 - 4031 ) and Google Cloud Compute .   Hao Peng was supported by a Google Ph.D. Fel-   lowship .   References4918491949204921   A Hyperparameters and Settings   We provide training and implementation details for   easy replication of our work .   A.1 Domain Machine Translation   We generally follow the preprocessing and subword   tokenization from Koehn and Knowles ( 2017 ) ; Hu   et al . ( 2019 ) . Table 7 lists the hyperparameters   and setting on fairseq that we use for all domain-   specific translation models . All embeddings are   shared ( Press and Wolf , 2017 ; Inan et al . , 2017 ) .   We choose the checkpoint that achieved the best   loss on the validation data .   A.2 Left - to - Right and Right - to - Left   WMT20 ZH - EN Table 8 lists the hyperprame-   ters and setting on fairseq that we use for left-   to - right and right - to - left models on the WMT20   ZH - EN dataset . We generally follow the prepro-   cessing and tokenization from Kasai et al . ( 2022a ) .   We use newstest-2019 as the dev . set and the offi-   cial training data . We apply Moses tokenization   ( Koehn et al . , 2007 ) and BPE with 32 K operations   ( Sennrich et al . , 2016b ) to English text . We tok-   enize Chinese text with the Jieba package , follow - ing Hassan et al . ( 2018 ) . Separately from English ,   BPE with 32 K operations is then applied to Chi-   nese . The decoder input and output embeddings   are tied .   WMT20 EN - DE The same hyperparameters are   chosen as in WMT20 ZH - EN ( Table 8) . We again   follow Kasai et al . ( 2022a ) and preprocess both En-   glish and German text by the Moses tokenizer and   joint BPE with 32 K operations . All embeddings   are shared .   A.3 SciTLDR   We use two BART - based pretrained models from   Cachola et al . ( 2020 ): the abstract - only version   of BART and the AIC version of C .   These two models are both BART - based models ;   C is obtained by finetuning BART on   the XSUM dataset ( Narayan et al . , 2018 ) with mul-   titask scaffolding ( Cachola et al . , 2020 ) .   A.4 λTuning   We tune λandλfrom{0.1,0.3,1.0,3.0 } , based   on the dev . BLEU / ROUGE - L score on machine   translation and paper summarization , respectively .   Table 9 reports the selected λvalues in all scenar-   ios.4922   B Sensitivity Analysis on λ   Fig . 6 presents the sensitivity analysis in the   COMET score over many scenarios . Apart from a   few exceptions , λ > λtends to yield good per-   formance , suggesting the effectiveness of the initial   exploration by gwith relatively weaker guidance   fromf.4923