  Pengshan Cai , Hui Wan , Fei Liu , Mo Yu , Hong Yu , Sachindra JoshiManning College of Information & Computer Sciences , University of Massachusetts , AmherstIBM Research AIDepartment of Computer Science , University of Central FloridaCHORDS , University of Massachuetts , Lowell   { pengshancai , hongyu}@cs.umass.edu , { hwan , yum}@us.ibm.com ,   feiliu@cs.ucf.edu , jsachind@in.ibm.com   Abstract   We propose novel AI - empowered chat bots   for learning as conversation where a user   does not read a passage but gains informa-   tion and knowledge through conversation with   a teacher bot . Our information - acquisition-   oriented dialogue system employs a novel   adaptation of reinforced self - play so that the   system can be transferred to various domains   without in - domain dialogue data , and can carry   out conversations both informative and atten-   tive to users . Our extensive subjective and ob-   jective evaluations on three large public data   corpora demonstrate the effectiveness of our   system to deliver knowledge - intensive and at-   tentive conversations and help end users sub-   stantially gain knowledge without reading pas-   sages . Our code and datasets are publicly avail-   ablefor follow - up research .   1 Introduction   Communication is the central process of educa-   tion ( Dewey , 1923 ) . In learning as conversation   ( Sharples , 2005 ) , a student does not read a pas-   sage but gains information and knowledge through   conversation with a teacher who reads the passage .   Compared to the traditional learning by reading ,   learning as conversation has the advantages that   conversation helps students stay engaged and that   information is provided piece by piece , which helps   strengthen learning with a shorter attention span .   The advantages of learning as conversation have   been veriﬁed with educational evidence ( Mol et al . ,   2008 ; Lever and Sénéchal , 2011 ; Golinkoff et al . ,   2019 ) . For example , studies have shown that when   children read storybooks , parents ’ guided conver-   sation , e.g. , posing questions and providing respon-   sive feedback , substantially ampliﬁes the learning   beneﬁts . While high - quality conversations with   experts are not always available , it would be help - ful if AI - empowered chat bots could be applied to   facilitate users to gain information or knowledge .   In recent years , there has been signiﬁcant re-   search in content - grounded dialogue generation ,   where external passages are used to inspire knowl-   edge intensive dialogues . However , these systems   or datasets are either for chit chat ( Zhou et al . , 2018 ;   Dinan et al . , 2019 ) or for goal - oriented informa-   tion seeking ( Feng et al . , 2020 ; Chen et al . , 2021 ) ,   little work has explored applying chat bots for the   learning as conversation purpose .   In this work we propose a novel task for learning   as conversation : information - acquisition - oriented   dialogue generation . Given a passage , our chat   bot actively engages with an end - user to form a   coherent conversation , so that the user could gain   knowledge without reading the passage . Our task   has a broad range of potential application venues in   which people traditionally rely on reading to obtain   information , including :   •Education : Chat bot helps an user gain knowl-   edge from books or research papers ;   •News and Media : Through conversation , a4781user could be provided stories tailored for   his / her preference ;   •Tutorial : While reading an instruction book   could be tedious and time - consuming , a chat   bot could efﬁciently walk an user through the   process .   As shown in Figure 1 , for our task , a good con-   versation should have the following characteristics :   1.Coverage : The chat bot should try to convey   as much information in the passage as pos-   sible , instead of mumbling about irrelevant   information ;   2.Coherence : The chat bot ’s response should be   coherent to the user ’s utterance , making the   user feel that his / her questions are followed   and addressed .   In summary , we propose a novel framework   which consists of the following two chat bot mod-   ules : 1 ) Teacher bot , which attempts to transfer the   information in an input passage to a user through   conversation ; and 2 ) Student bot , which responds   to Teacher bot to form coherent conversations dur-   ing training . The two bots are trained in a two-   phase manner : In Phase 1 , we pre - tune the two   chat bots on Wizard of Wikipedia ( Dinan et al . ,   2019 ) dataset , enabling both bots with the basic   ability of conversing over a passage . In Phase 2 ,   we ﬁne - tune Teacher bot through self - play with Stu-   dent bot , guided by reinforcement rewards . In this   process , we enhance Teacher bot to be more infor-   mative while maintaining the ability to coherently   address human users . Speciﬁcally , the ﬁne - tuning   phase is unsupervised , i.e. Teacher bot could be   transferred to various domains or corpora without   additional annotated dialogue datasets .   Our contributions include : 1 ) A novel task of   information - acquisition - oriented dialogue system ;   2 ) A novel unsupervised learning framework which   enables a teacher bot to carry out informative and   coherent conversations with human users for in-   formation acquisition purpose ; 3 ) Extensive ex-   periments with human evaluation demonstrate the   effectiveness of our proposed approach .   2 Approach   In order to obtain an informative and attentive   teaching dialogue system , we propose a framework   that consists of two chat bots in different roles , and   leverage both supervised learning and unsupervised   reinforcement learning , as illustrated in Figure 2 .   The unsupervised reinforcement learning enables   the system to be ﬁne - tuned on other text corpuswhere no annotation or dialogue data is required .   2.1 Model Architecture   Given a passage P , the conversation between   Teacher botXand Student bot Ycan be denoted as   a sequence of turns C={U , U, ... ,U , U } ,   whereNis the number of turns in the conversation .   In order to mimic our use case , Teacher bot has   access toPwhereas Student bot does not .   Teacher bot Xaims at transmitting the infor-   mation inPto the student . At the nturn ,   Xtakes as input Pand the conversation history   H={U , U, ... ,U , U } , and outputs   U. Teacher bot Xadopts DoHA ( Prabhumoye   et al . , 2021 ) , a pre - trained model for document-   grounded text generation , and is tuned in super-   vised phase and unsupervised self - play phase .   In order to ﬁne - tune Teacher bot Xwith rein-   forcement learning on full conversations , as a prac-   tical approach , we train a Student bot Yto carry   on conversations with X. Student bot Ytakes the   conversation history H={U , U, ... ,U}as   input , and output U. It adopts BART ( Lewis et al . ,   2020 ) model .   2.2 Phase 1 : Supervised Pre - Tuning   This phase trains Teacher bot Xto initialize and   carry out conversations based on a given passage P ,   and trains Student bot Yto respond appropriately   toX. To this end , we pre - tune both XandYon the   Wizard of Wikipedia ( WoW ) dataset . WoW was   chosen as the pre - tuning dataset because of its two   characteristics : 1 ) Open - domain : WoW contains   conversations on a broad range of topics and do-   mains across Wikipedia , thus the pre - tuned Teacher4782and Student bots have greater potentials to be suc-   cessfully transferred to other domains during the   ﬁne - tuning stage ; 2 ) Content - grounded : in WoW ,   the teaching bot ’s utterances are grounded on pas-   sages , which is similar to our task . We present the   gold passage to Teacher bot directly , though , differ-   ent from the WoW ’s original setting ( Dinan et al . ,   2019 ) where Teacher bot searches a large corpus   for supporting passages .   We optimize the maximum - likelihood objective   for Teacher bot by minimizing the following loss :   L=−/summationdisplay / summationdisplaylog(p(x|x, ... ,x , H , P ) )   whereNis the total number of turns in the conver-   sation,{x, ... ,x}is Teacher bot ’s response at   thenturn , Mis the number of words in U.   The loss function for Student bot , L , is simi-   lar toL , with the exception of not including a   passagePas input .   L=−/summationdisplay / summationdisplaylog(p(y|y, ... ,y , H ) )   where{y, ... ,y}is Student bot ’s response at   thenturn .   2.3 Phase 2 : Unsupervised Self - Play   Fine - tuning   In this phase , we aim at improving Teacher bot ’s   ability to present informative and coherent conver-   sations . This is achieved by reinforcement learn-   ing on Teacher bot with the help of Student bot ,   and could be applied in a novel target domain   even where dialogue dataset is absent . We adopt   a self - play approach , i.e. we let Teacher bot and   Student bot chat with each other over a passage   in the target domain to generate multiple turns   of conversations . In this ﬁne - tuning phase , we   keep Student bot frozen , and reward Teacher bot   when the generated conversation achieves higher   scores . In order to reduce the variance of the gra-   dient estimate , we apply self - critic reinforcement   learning ( Rennie et al . , 2017 ) . Speciﬁcally , at   each turn , we let Teacher bot generate two sep-   arate utterances : 1 ) U , which is sampled from   the model , i.e. x∼p(x|x, ... ,x ) , and   2)U , which is obtained by greedy search , i.e.   x= arg maxp(x|x, ... ,x ) . We optimizethe model by minimizing the following RL loss :   L=−/summationdisplay(R(U)−R(U))/summationdisplaylog(p(x   |x, ... ,x , H , P ) )   whereR()is the reward function , which we will   cover in Section 2.4 , and Pis a passage from the   target domain corpus .   If not taking into account language modeling ,   optimizing RL loss alone would lead Teacher bot   to generate inarticulate and even grammatically   incorrect utterances . To keep the ﬂuency of Teacher   bots , we optimize a combined loss Lconsisting   of RL lossLon the new target domain data and   MLE lossLon the pre - tuning dialogue dataset ,   so the language style acquired during the pre - tuning   phase would not get lost during RL ﬁne - tuning :   L = γL+ ( 1−γ)L   whereγ∈(0,1)is a scaling factor accounting   for the emphasis on LandL. We note that   whileLshould be obtained on an annotated   content - grounded dialogue dataset ( e.g. WoW ) ,   Lcould be obtained on any target domain passage   corpus even without dialogue data . This enables   our approach to be transferred to an unsupervised   text corpus .   2.4 Reward Functions   2.4.1 Coverage   We deﬁne the coverage reward of a Teacher bot ’s   utteranceUas :   R = R(P , H + U)−R(P , H )   where Rouge(P , H)is the Rouge-1 F1 ( Lin , 2004 )   score of the conversation history Hto the input pas-   sageP. Intuitively , this function favors utterances   that cover more information in the passage and   have less overlap with the conversation history .   2.4.2 Coherence   Dialogue coherence datasets We explore neural   coherence scoring models trained on two open-   domain dialogue coherence datasets :   1.WoW - coherence dataset We reuse the WoW   dataset to heuristically build a dialogue coherence   classiﬁcation dataset . Speciﬁcally , for each multi-   turn dialogue in WoW , we label the ground truth   response to its conversation history as coherent re-   sponse , and all later responses in the same dialogue   asincoherent responses.47832.InferConv dataset ( Dziri et al . , 2019 ) This   is an open - domain dialogue coherence classiﬁca-   tion dataset built from PersonaChat conversational   data ( Zhang et al . , 2018b ) . The dataset casts a   response as the hypothesis and the conversation   history as the premise , thus convert dialogue co-   herence evaluation into an NLI task . The dataset   classiﬁes the relationship between the response and   the conversation history into three categories : en-   tailed , neutral andcontradict . Table 2 summarizes   statistics of these datasets .   Coherence scoring models Based on the same   pre - trained model BERT ( Devlin et al . , 2019 ) , we   train two different coherence scoring models on   the two dialogue coherence classiﬁcation datasets   respectively . Both models take the concatenation   ( with [ SEP ] ) of the conversation history and a can-   didate response as input , and minimize the cross   entropy loss between the predicted label and the   gold label . We use different methods to attain the   coherence reward Rfrom the two models .   For model WoW - coherence , we deﬁne the coher-   ence reward with softmax :   R = e   e+e   whereoandoare the logits for coherent andin-   coherent labels in the output layer . For model Infer-   Conv , we observe some responses labeled as neu-   tralare appropriate responses but are not closely   related to conversation history ( e.g. “ That ’s in-   teresting ! ” ) , we thus heuristically assign constant   scoress , sandsas coherence reward R   when the response is predicted as entailed , neu-   tralandcontradict . In the remainder of the paper ,   we use WoW - coherence as the default coherence   model , and compare it with InferConv in Section 4 .   2.4.3 Mixed Reward   The coverage and coherence rewards are combined   with a hyper - parameter β , yielding the ﬁnal reward :   R = βR+ ( 1−β)R   3 Experimental Settings   We proceed by describing our datasets , comparison   systems and evaluation metrics . We then show the   performance of our proposed approach compared   to state - of - the - art in § 4 .   3.1 Datasets   Wizard of Wikipedia ( Dinan et al . , 2019 ) contains a   total of 22,311 human - human conversations crowd-   sourced via Amazon ’s Mechanical Turk . The con-   versations are grounded in Wikipedia passages cov-   ering a wide range of topics : e - book , toga party ,   armadillo , etc . Both Teacher and Student bots are   pre - tuned on the WoW dataset during Phase 1 . Dif-   ferent from WoW ’s original setting , we present the   gold passage to the Teacher bot directly , instead of   searching a large corpus for supporting passages .   This allows us to focus less on retrieval and more   on creating a Teacher bot to deliver informative and   attentive dialogues .   We consider knowledge sources of various sorts   as Teacher bot ’s target domain during ﬁne - tuning .   CNN / DailyMail contains a large collection of on-   line news articles with an average of 781 tokens   per article ( See et al . , 2017 ) . The full content of the   article can not be conveyed in a short conversation .   Thus , we use the ﬁrst 130 tokens of each article as   a supporting passage , assuming it covers the most   important content of the news article .   Academic papers have become an omnipresent   source of knowledge . We create our own dataset   containing papers published in recent years ( 2017 –   2021 ) at major venues , including ACL , EMNLP ,   NAACL , EACL , Findings and ICLR conferences .   Similarly , we use paper abstracts as supporting pas-   sages instead of full articles . Moreover , we include   Wikipedia passages from the WoW dataset , with-   out conversations , as another source of knowledge .   The CNN - DM , Paper Abstracts andWikipedia   datasets are used in Phase 2 of unsupervised self-   play ﬁne - tuning . Statistics of these datasets are   summarized in Table 1 .   3.2 Comparison Models   Our baseline Teacher bot builds on the state - of - the   art content - grounded dialogue generation model:4784   DHA(Prabhumoye et al . , 2021 ) . It includes two   improvements to the architectures of pre - trained   encoder - decoder models ( Lewis et al . , 2020 ): build-   ing context - driven representation of the supporting   document , and enabling document - headed atten-   tion to acquire information from the document .   DHAhas demonstrated strong performance in   document - grounded generation . All DHAmod-   els are pre - tuned on the WoW dataset .   OurF Teacher bot is created to converse in   an informative and coherent manner . It extends   DHA by incorporating both coverage and coher-   ence rewards in unsupervised self - play ﬁne - tuning .   Additionally , we ablate F model by removing   each of the two rewards : + Cuses only the cov-   erage reward for ﬁne - tuning , i.e. setting β= 1 in   our reward function ( § 2.4 ) . + Cutilizes only the   WoW - coherence reward , i.e. setting β= 0 . Please   refer to appendix for more implementation details   and hyper - parameters .   3.3 Evaluation Metrics   We investigate a wide range of metrics to evaluate   Teacher bot ’s performance . Objective metrics mea-   sure the content coverage and coherence of Teacher   bot ’s utterances . Subjective metrics , devised with   human - in - the - loop , provide a holistic evaluation of   a conversation , focusing on its overall effectivenessand various aspects of linguistic quality .   Objective Metrics . Teacher bot converses with   Student bot over a passage for three turns . That is ,   Teacher bot initiates the dialogue and provides two   responses to Student bot . We objectively evaluate   Teacher bot ’s utterances in terms of information   coverage and coherence as follows .   •R ( Lin , 2004 ) is one of the most widely   used metrics for measuring information coverage .   We consider three variants in this study : R-1,R-2   andR - L , which respectively measure the overlap   of unigrams , bigrams and the longest common sub-   sequence between the given passage and Teacher   bot ’s utterances .   •QAandQAare two variants of Sum-   maQA ( Scialom et al . , 2019 ) , a question answering-   based evaluation metric . If a conversation is rich   in information , it could be used as a surrogate for   the passage to answering important questions . To   this end , SummaQA generates Cloze - style ques-   tions from a passage by masking out entities , then   employs a QA system to answer those questions   based on a conversation . A higher QA performance   suggests the conversation has better coverage . Par-   ticularly , QAreports the F1 score for question   answering ; QAmeasures the conﬁdence of   the QA system in predicting answers .   •WoW - Coherence andInferConv are neural   coherence scoring models ( § 2.4.2 ) repurposed for   evaluation . These models quantitatively assess if   Teacher bot has produced a coherent response given   the conversation history , or not .   •DPRprovides a new perspective on dia-   logue coherence evaluation ( Zhang et al . , 2018a ) . It   draws on the Dense Passage Retriever model ( DPR ;   Karpukhin et al . , 2020 ) to predict if a Teacher bot ’s   response is relevant to Student bot ’s input . A higher   relevance score means the input and response share   the same topic , suggesting a coherent conversation .   Subjective Metrics . We recruit 24 human evalua-   tors to interact with Teacher bots . Each evaluator   is asked to converse with bots over four passages .   For each passage , the evaluator chats with four dif-   ferent Teacher bots for three turns , where Teacher   bot initiates the conversation and responds twice   to the evaluator ’s input . We randomly select 48   passages for evaluation , i.e. , 16 passages from each   of the three test sets . To evaluate conversations   produced from Paper Abstracts , we require evalu-   ators , 8 in total , to be either PhD students or have4785   obtained a PhD degree . For fair comparison , we   shufﬂe and hide the order of Teacher bots presented   to evaluators . Human evaluators were suggested to   feed the same or similar inputs across Teacher bots   on the same passage whenever possible . Through-   out the conversation , the passage was not shown   to the evaluators . After the conversation , human   evaluators were asked to complete the following   evaluation tasks :   •QA : Five sentences are randomly se-   lected from each passage and one important entity   is masked out in each sentence . The evaluators are   presented with each corrupted sentence and asked   if the sentence could be recovered by referencing   the conversation with Teacher bot . We report the   ratio of sentences that could be correctly recovered .   •Linguistic Quality : We ask human evaluators   to rate each conversation along three dimensions :   Coherence : Does Teacher bot provide coherent re-   sponses to the evaluator ’s input ? Readability : Are   Teacher bot ’s utterances easy to read , containing no   grammatical or semantic errors ? Overall Quality :   How will the conversation score in terms of infor-   mativeness , coherence , readability and all aspects   considered ? The scoring rubric provided to human   evaluators is shown in Table 3 .   4 Objective Results   Results on Test Sets . Table 4 presents objective   evaluation results obtained for various Teacher bots   on three test sets : CNN - DM , Paper Abstracts and   Wikipedia . We observe that our F Teacher bot   is able to substantially outperform the baseline sys-   temDHAon all datasets and across all objective   metrics . It strikes a ﬁne balance between delivering   information - rich conversations and ensuring those   conversations are coherent and attentive . Further ,   we ﬁnd that optimizing a single reward , whether it   be coverage or coherence , produces suboptimal re-   sults . For instance , + Ctends to produce longer   utterances than other variants . It improves infor-   mation coverage , but yields low coherence scores ,   leading to a performance even inferior to the base-   lineDHA . Our ﬁndings suggest that it is impor-   tant for the reinforcer Lto learn with both cover-   age or coherence rewards .   Trading off Coverage for Coherence . In Fig-   ure 4 , we plot the learning curves of coverage and   coherence scores when the reinforcer adopts a sin-   gle reward ( + C,+C ) or both ( F ) . We   useRandRto approximate coverage and   coherence scores . These plots are generated using   50 validation instances from the Paper Abstracts   dataset . We observe that with only the coverage   reward ( + C ) , Teacher bot tends to aggressively   copy content from the passage , while disregarding   the conversation history . This inevitably leads to   incoherent conversations . Conversely , + Ccan   improve on coherence , but falls short on deliver-   ing informative conversations . Finally , our F   Teacher bot trades off coverage for substantially   higher coherence , thus achieving a signiﬁcant im-   provement over the baseline DHA model .   Trading off Coverage for Coherence . We are cu-   rious to know the amount of information brought   by each utterance produced by Teacher bot . To   this end , we deﬁne information gain IG(·)as the4786   improvement of ROUGE scores brought by an ut-   teranceU :   IG(U ) = R ( P , H + U)−R ( P , H ) ,   wherePis the supporting passage , and Hrepre-   sents the conversation history . We consider three   R variants , R-1,R-2 andR - L , respectively .   Figure 3 illustrates the gain of information for each   of the three turns . The average R gain is re-   ported for each turn , using conversations produced   for the Paper Abstracts dataset . We observe that   there is a general tendency across turns that infor-   mation gain is decreasing . This is in part because   that at the beginning of a conversation , Teacher   bot has no constraints regarding content selection ,   it could rephrase any content selected from the   supporting passage to initiate a dialogue . In subse-   quent turns , Teacher bot has to exercise caution in   response generation considering both the conversa-   tion history and overall coherence of the conversa-   tion . Consequently , we ﬁnd that the average length   of the utterances also decreases in subsequent turns .   A Comparison of Coherence Scoring Models .   We compare two Teacher bots ﬁne - tuned only with   coherence reward from different coherence scoring   models ( i.e. WoW - coherence andInferConv ) . We   demonstrate their objective results in Table 5 . Ac-   cording to the results , Teacher bot ﬁne - tuned with   InferConv achieves slightly better coverage metrics .   However , in terms of coherence metrics , Teacher   bot ﬁne - tuned with WoW - coherence model gener-   ally achieves better performance . Based on thisobservation , WoW - coherence scoring model better   measures coherence in conversations .   5 Subjective Results   We demonstrate the subjective evaluation results in   Table 4 and have the following observations :   1.For question answering , + Cachieves the   best performance on all three datasets . This again   proves that the coverage reward helps make the   conversation more informative ;   2.For coherence scores , + Cachieves the best   performance on Wikipedia . However , on the other   two datasets it was outperformed by F.   3.For readability scores , on CNN - DM andPa-   per Abstracts , + Cachieves the lowest perfor-   mance while + Cachieves the highest .   4.For overall scores , F demonstrates the   best performance . This suggests F delivers   conversations that are more balanced in coverage ,   coherence and readability .   5.ThePaper Abstracts corpus is the most chal-   lenging among all the test corpora , as our Teacher   bots generally show worse performance in coher-   ence , readability and overall scores . We found pas-   sages in Paper Abstracts contain volumes of pro-   fessional vocabularies thus are more complicated   for people to understand . In addition , it ’s also more   difﬁcult for Student bot to respond appropriately   during self - play ﬁne - tuning ( See examples in ap-   pendix ) . As a result , transferring Teacher bots to   this domain is more challenging .   A Case Study . We show a few Teacher bots ’ re-   sponses to users in Table 6 . After analyzing the   cases , we have the following observations :   1.The coverage reward encourages Teacher bots   to directly copy content from the input passage,4787   while the coherence reward encourages abstrac-   tively generating new content : As shown in Exam-   ple A , + Cdirectly extracts a part of the original   passage as response , regardless of the user ’s ques-   tion , while + CandF abstractively rewrite   the response to make it more coherent .   2.Putting too much weight on coherence re-   ward could make Teacher bot become so abstrac-   tive that it misrepresents the original passage and   lead to incoherence and semantic / grammar errors .   ( See + C ’s response in example A and C ) This   explains + C ’s low coherence and readability   scores on CNN - DM andPaper Abstracts . This ob-   servation suggests the necessity to carefully choose   the weight for coherence rewards , and to coupling   coherence reward with coverage reward , which   could make the chat bot less abstractive.3.Generally , user utterances could be classiﬁed   into two categories : Information - seeking queries   which request certain information ( e.g. the user ’s   utterance in example A ) ; Open statements which   do not have speciﬁc requests ( e.g. the user ’s ut-   terance in example B ) . We found evaluators tend   to give high coherence scores to response to open   statements , as they could be addressed by a wider   range of responses .   6 Related Work   Content - Grounded Dialogue Generation Con-   tent grounded dialogue generation is the task of   using the information provided in external con-   tent ( e.g. a passage , etc . ) to guide dialogue   generation . Compared to previous research , our   task has the following novelties . 1 ) Compared to   content - grounded information - retrieval - oriented di-   alogue such as doc2dial ( Feng et al . , 2020 ) and   ABCD ( Chen et al . , 2021 ) where the chat bot re-   sponds to user query in a passive way , we expect   our chat bots to convey knowledge proactively .   2 ) Compared to chit chat - oriented dialogue such   as ( Zhou et al . , 2018 ; Dinan et al . , 2019 ; Komeili   et al . , 2021 ; Xu et al . , 2021 ) , our task is more   focused on extensive conversation in a particular   topic , and aims at helping the end user acquire   knowledge or information from a given passage . 3 )   Contrasted to chat bots that are applied in a single   domain ( Zhou et al . , 2018 ; Moghe et al . , 2018 ; Xu   et al . , 2019 ) , our chat bot could be transferred to   other domains through self - talk based ﬁne - tuning .   Another line of research works focus on content   grounded text generation models ( Prabhumoye   et al . , 2021 ; Zhao et al . , 2020 ) . Compared with or-   dinary text generation models ( e.g. BART ( Lewis   et al . , 2020 ) ) , these models are speciﬁcally de-   signed to model external content as an additional   input , and achieve better performance on con-   tent grounded dialogue generation tasks includ-   ingCMU DoG ( Zhao et al . , 2020 ) and Wizard   of Wikipedia ( Dinan et al . , 2019 ) .   RL in Text Generation Reinforcement learning   has been applied in various natural language gener-   ation tasks , including image caption ( Rennie et al . ,   2017 ) , automatic summarization ( Paulus et al . ,   2018 ) , machine translation ( Kang et al . , 2020 ) and   poem generation ( Yang et al . , 2019 ) . Speciﬁcally ,   when applying reinforcement learning in dialogue   generation ( Li et al . , 2016 ; Zhao et al . , 2019 ; Shi   et al . , 2019 ; Yamazaki and Aizawa , 2021 ; Liu et al . ,47882020 ) , self - play is often used to enable scoring   multi - turn dialogues . Compared to previous dia-   logue generation research using RL and self - play ,   our two - phase framework enables transferring the   teacher bot to other domains by optimizing a mixed   reward of coverage and coherence .   Educational Dialogue Systems and Conversa-   tional QA There have been research works ap-   plying dialogue systems for educational purposes .   Some chat bots are for language practice . ( Ruan   et al . , 2021 ; Stewart and File , 2007 ; Huang et al . ,   2017 ) Others are specially designed for education   in a single domain or task , e.g. moral educa-   tion ( Peng et al . , 2019 ) , educational debate ( Yuan   et al . , 2008 ) . Compared with previous educational   dialogue systems , our system is for information   acquisition without domain restriction . Our task   is also related to conversational question answer-   ing ( CQA ) , e.g. ( Ko ˇciský et al . , 2018 ; Rajpurkar   et al . , 2016 ; Joshi et al . , 2017 ; Zellers et al . , 2018 ) .   However , most existing CQA systems passively   response to user queries in single turn conversa-   tions , while our system actively engage with users   in multi - turn conversations .   7 Conclusion   We propose an information - acquisition - oriented di-   alogue system that transfers information and knowl-   edge in passages to users through conversations .   An unsupervised self - talk approach is introduced   leveraging novel rewards to enable Teacher bots   to deliver informative and attentive conversations .   Experiments with automatic and human evalua-   tions demonstrate the effectiveness of our approach .   Some interesting future directions include extend-   ing the conversations to be based on a set of docu-   ments and specializing our dialogue systems for in   speciﬁc domain , e.g. patient education .   8 Ethical Considerations   Our models are pre - tuned on Wizard of Wikipedia   dataset and ﬁne - tuned on three corpora : Wikipedia ,   CNN - DailyMail andPaper - Abstracts ( Abstracts   of papers from ACL , EMNLP , NAACL , EACL ,   Findings and ICLR submissions from 2017 to   2021 ) . All the datasets used in this paper are   publicly available . Moreover , we did not use   full - length Wikipedia or CNN - Daily Mail news   articles in our experiments , but tailored versions   of 100 - 150 words . This is because a full length   Wikipedia / CNN - Daily Mail article may contain too   much content to be covered in a short conversation . As described in ( Maynez et al . , 2020 ; Kryscin-   ski et al . , 2020 ; Lebanoff et al . , 2020 ; Zhou et al . ,   2021 ) , current state of the art neural conditional text   models can output hallucinated content unfaithfully   to the input text , which impedes the safe deploy-   ment of the models . We note that our Teacher bots   may also generate utterances that are not supported   by the input passage.4789References4790479147929 Appendix   9.1 Implementation Details   9.1.1 Pre - tuning   Key hyper - parameters for both models .   • source max len : 1024   • target max len : 128   • batch size : 8   • train epoch : 3   • learning rate : 2e-5   •Both Teacher and Student bots adopt the ini-   tialized weights of bart - base from hugging-   face .   •Both DoHA models and BART models are   based on the implementation presented in the   DoHA paper ( Prabhumoye et al . , 2021 ) .   Note we initialize our models with bart - base in-   stead of bart - large as our self - play ﬁne - tuning is   very computational intensive and time consuming .   With our current setting , the self - play ﬁne - tuning   takes about 2.5 days on one single NVIDIA Tesla   V100 GPU .   9.1.2 Self - Play Reinforced Fine - Tuning   Mixed Loss . Optimizing our mixed loss is   achieved by intermittently minimizing the MLE   loss for aabatches and then minimizing the RL   loss forbbatches . In other word , the parameter γin   Section 2.3 is determined by aandb . Speciﬁcally ,   in each ﬁne - tuning step , the parameters of Teacher   bot are updated for 4 times , once over an RL batch ,   3 times over MLE batches .   Key Hyper - Parameters . We apply the same set of   basic hyper - parameters for all Teacher bots during   all ﬁne - tuning process :   • MLE batch size : 8   • RL batch size : 5   • maximum coverage score : 0.5   • train epoch : 3   • learning rate : 1e-6   • Fine - tuning steps : 10,000   •βfor three reinforced Teacher bots : F-   β= 0.7 , + C - β= 1.0 , + C - β= 0.0   9.2 Coherence Scoring Models   Both WoW - coherence andInferConv are trained   based on bert - base - cased .   Key hyper - parameters for both models .   • max length : 256   • batch size : 32   • learning rate 2e-5   • epochs 3ForInferConv classiﬁcation model , the constant   scores ares= 1.0,s= 0.2ands= 0.0   The WoW - coherence andInferConv classiﬁca-   tion model achieves 82.1 % and 88.4 % accuracy on   respective test sets .   All other hyper - parameter settings for Teacher   bot , Student bot and coherence scoring models are   based on the system ’s default setting . All our ex-   periments were run on servers with Nvidia A100   and V100 GPUs .   9.3 Dataset Details   The passages in train / validation / test set of our   Wikipedia corpus are randomly sampled from pas-   sages in train / validation / test set of WoW respec-   tively . Similarly , passages in train / validation / test   set of our CNN - DM corpus are randomly sam-   pled from the train / validation / test set of the orig-   inalCNN - DM dataset . For Paper Abstracts , we   randomly distribute all collected paper abstracts   into train / validation / test sets .   9.4 Rewards function Details   During our exploration , we have explored multiple   variations of coverage reward functions :   1.Reward score is gained at the end of each   turn of conversation , and it is calculated the   ROUGE score improvement of the teacher   bot ’s utterance ( As we applied in the paper ) ;   2.Reward score is gained at the end of each   turn of conversation , and it is calculated   the ROUGE score improvement of both the   teacher bot and student bot ’s utterances ;   3.Reward score is gained at the end of the entire   conversation , and is calculated as the ROUGE   score improvement of all teacher bot ’s utter-   ances .   According to our experience , the performance   of 1 is similar than 2 . Speciﬁcally , we notice in   most cases , our student bot contribute marginally in   terms of ROUGE score improvement . In addition ,   in practice , we observe 1 shows better performance   than 3 .   9.5 Human Evaluation Details   All human evaluation passages are collected ran-   domly , the length of the Wikipedia andCNN pas-   sages are tailored to 100 - 150 words to conform to   the length of passages in the ﬁne - tuning stage . Dur-   ing conversations , we suggest our evaluators to use4793utterances relevant to the topic , so that their utter-   ances could be appropriately addressed by Teacher   bots referencing content in the passage . All human   evaluators we recruit have at least a bachelor de-   gree , each evaluator is rewarded with a $ 20 gift   card for participation .   The evaluation results are collected using Google   Colab , an example evaluation page is avail-   able through this link ( Personal information are   anonymized during reviewing stage ) .   Note that in our human evaluation , Teacher bot   chat about a passage with two evaluators . We use   Pearson correlation coefﬁcient to measure Inter-   annotator agreement , the average Pearson correla-   tion coefﬁcient among each pair of evaluators is   0.16 , implying weakly positive correlation . We   note different evaluators use different utterances to   chat with the same Teacher bot , thus it is reason-   able that the conversation with the same Teacher   bot over the same passage have large variance .   9.6 Examples   We provide more examples of conversations in self-   play in Table 7 and human evaluations in Table 8.479447954796