  Rui Li , Xu Chen , Chaozhuo Li , Yanming Shen , Jianan Zhao ,   Yujing Wang , Weihao Han , Hao Sun , Weiwei Deng , Qi Zhang , Xing XieDalian University of Technology , Renmin University of China , Microsoft Research Asia , Université de Montréal , Microsoft   xu.chen@ruc.edu.cn , cli@microsoft.com   Abstract   Embedding models have shown great power   in knowledge graph completion ( KGC ) task .   By learning structural constraints for each train-   ing triple , these methods implicitly memorize   intrinsic relation rules to infer missing links .   However , this paper points out that the multi-   hop relation rules are hard to be reliably mem-   orized due to the inherent deficiencies of such   implicit memorization strategy , making em-   bedding models underperform in predicting   links between distant entity pairs . To allevi-   ate this problem , we present Vertical Learn-   ing Paradigm ( VLP ) , which extends embed-   ding models by allowing to explicitly copy tar-   get information from related factual triples for   more accurate prediction . Rather than solely   relying on the implicit memory , VLP directly   provides additional cues to improve the gen-   eralization ability of embedding models , espe-   cially making the distant link prediction sig-   nificantly easier . Moreover , we also propose   a novel relative distance based negative sam-   pling technique ( ReD ) for more effective op-   timization . Experiments demonstrate the va-   lidity and generality of our proposals on two   standard benchmarks . Our code is available at   https://github.com/rui9812/VLP .   1 Introduction   Knowledge graphs ( KGs ) structurally represent hu-   man knowledge as a collection of factual triples .   Each triple ( h , r , t ) represents that there is a rela-   tionrbetween head entity hand tail entity t. With   the massive human knowledge , KGs facilitate a   myriad of downstream applications ( Xiong et al . ,   2017 ) . However , real - world KGs such as Freebase   ( Bollacker et al . , 2008 ) are far from complete ( Bor-   des et al . , 2013 ) . This motivates substantial re-   search on the knowledge graph completion ( KGC )   task , i.e. , automatically inferring missing triples . Figure 1 : Learning paradigm of embedding models .   As an effective solution for KGC , embedding   model learns representations of entities and rela-   tions with pre - designed relation operations . For   example , TransE ( Bordes et al . , 2013 ) represents   relations as translations between head and tail en-   tities . RESCAL ( Nickel et al . , 2011 ) , DistMult   ( Yang et al . , 2015 ) and ComplEx ( Trouillon et al . ,   2016 ) model the three - way interactions in each   triple . RotatE ( Sun et al . , 2019 ) , QuatE ( Zhang   et al . , 2019 ) and DualE ( Cao et al . , 2021 ) repre-   sent relations as rotations in different dimensions .   Rot - Pro ( Song et al . , 2021 ) further introduces the   orthogonal projection for each relation .   Essentially , embedding models learn structural   constraints for every factual triple during the train-   ing period . For example , for each training triple   ( h , r , t ) , TransE constrains that the head embed-   dinghplus the relation embedding requals the   tail embedding t. Such single - triple constraints   empower embedding models to implicitly perceive   ( i.e. , memorize ) the high - order entity connections   and intrinsic relation rules ( Sun et al . , 2019 ) . As   shown in Figure 1 , by imposing the structural con-   straints ( e.g. , h+r = tin TransE ) on the five   training triples , embedding models can memorize   the entity connection ( x , r∧r , z)and the rela-   tion rule r∧r→r . In this way , the missing link6335   ( x , r , z ) can be inferred at test time without any ex-   plicit prompt . We refer to this single - triple learning   paradigm as Horizontal Learning Paradigm ( HLP ) ,   since the relation rules are implicitly induced by   the horizontal paths between head and tail entities .   However , this paper shows that the HLP - based   embedding models are hard to reliably memorize   the multi - hop relation rules , which is attributed to   inevitable single - triple bias and high - demanding   memory capacity . The unreliable multi - hop rela-   tion rules in the implicit memory can not serve as   rational basis for prediction , leading to the inferior   performance of embedding models in predicting   links between distant entity pairs . This brings us a   question : is there a general paradigm for embed-   ding models to alleviate this problem of HLP and   achieve superior performance ?   We give an affirmative answer by presenting Ver-   tical Learning Paradigm ( VLP ) , which endows em-   bedding models with the ability to explicitly con-   sult related factual triples ( i.e. , vertical references )   for more accurate prediction . Specifically , to an-   swer ( h , r , ? ) , VLP first selects Nrelevant refer-   ence queries in the training graph , and then treats   their ground - truth entities as the reference answers   for embedding models to jointly predict the target t.   This learning process can be viewed as an explicit   copy strategy , which is different from the implicit   memorization strategy of HLP , making it signifi-   cantly easier to predict distant links . Moreover , to   effectively optimize the models , we further propose   a novel Relative Distance based negative sampling   technique ( ReD ) , which can generate more infor-   mative negative samples and reduce the toxicity of   false negative samples . Note that VLP and ReD are   both general techniques and can be widely applied   to various embedding models . Our contributions   are summarized as follows :   •We show that existing embedding models un-   derperform in predicting links between dis-   tant entity pairs , since they are hard to reliably   memorize the multi - hop relation rules.• We present a novel learning paradigm named   VLP , which can empower embedding mod-   els to leverage explicit references as cues for   more accurate prediction .   •We further propose a new relative distance   based negative sampling technique named   ReD for more effective optimization .   •We conduct in - depth experiments on two stan-   dard benchmarks , demonstrating the validity   and generality of the proposed techniques .   2 Preliminaries   To elicit our proposal from a general paradigm per-   spective , we give a bird ’s eye view of existing em-   bedding models in this section . We first review the   problem setup of KGC task . Afterwards , we sum-   marize a generalized score function of embedding   models and describe how the models learn to pre-   dict new links ( i.e. , horizontal learning paradigm ) .   2.1 Problem Setup   Given the entity set Eand relation set R , a knowl-   edge graph can be formally defined as a collec-   tion of factual triples D={(h , r , t ) } , in which   head / tail entities h , t∈ Eand relation r∈ R. KGC   task aims to infer new links by answering a query   ( h , r,?)or ( ? , r , t ) . As an effective tool for this   task , embedding model learns representations of   entities and relations to measure each candidate ’s   plausibility with a pre - designed score function .   2.2 Generalized Score Function   Based on a series of previous works ( Nickel et al . ,   2011 ; Bordes et al . , 2013 ; Wang et al . , 2014 ; Lin   et al . , 2015 ; Yang et al . , 2015 ; Trouillon et al . , 2016 ;   Sun et al . , 2019 ; Gao et al . , 2020 ; Song et al . , 2021 ) ,   we summarize a generalized score function ( GSF )   of embedding models . To facilitate presentation ,   we only describe the query case of ( h , r , ? ) , while   ( ? , r , t)can be similarly conduced.6336Given a query ( h , r,?)and a candidate answer t ,   GSF first maps the head embedding h∈Xto the   query embedding q∈Xwith a relation - specific   linear transformation :   q = Wh+b , ( 1 )   where X∈ { R , C}is the embedding space , dand   dare the embedding dimensions of entities and   relations , W∈Xandb∈Xdenote the   relation - specific projection matrix and bias vector .   Then , GSF uses another linear function to gen-   erate the answer embedding k∈Xfrom the tail   embedding t∈X :   k = Wt , ( 2 )   where W∈Xdenotes the relation trans-   formation matrix for tail projections .   Finally , the plausibility score of the triple ( h , r , t )   is calculated by a similarity function g :   score = g(q , k ) . ( 3 )   By combining the above three steps , we formally   define the generalized score function fas follows :   f(h , r , t ) = g(Wh+b , Wt).(4 )   With different choices of W , b , Wandg ,   GSF can be instantiated as specific score functions   of existing models . Table 1 exhibits several popular   methods and their corresponding GSF settings .   2.3 Horizontal Learning Paradigm   With the pre - defined score functions , embedding   models commonly follow the horizontal learning   paradigm , which constructs the single - edge con-   straints to implicitly memorize high - order entity   connections and intrinsic relation rules .   Take RotatE to process the triples in Figure 1 as   an example . By imposing the rotation constraints   on three triples ( a , r , b),(b , r , c)and(a , r , c ) , Ro-   tatE is able to perceive a two - hop entity connection   and further induce a two - hop relation rule :         b = a ◦ r   c = b ◦ r   c = a ◦ r⇒r = r ◦ r. ( 5 )   Similarly , the high - order connection can also be   captured by constraining ( x , r , y)and(y , r , z ):   /braceleftigg   y = x ◦ r   z = y ◦ r⇒z = x ◦ r ◦ r. ( 6 )   Finally , by combining Equation ( 5 ) and ( 6 ) , RotatE   is capable of inferring the missing link ( x , r , z ) .3 Motivation   The motive of our work originates from an obser-   vation that embedding models underperform in pre-   dicting links between distant entity pairs ( refer to   Appendix A for more details ) . Since the effective-   ness of embedding models is largely determined   by the ability to learn intrinsic relation rules ( Sun   et al . , 2019 ; Song et al . , 2021 ; Li et al . , 2022 ) , such   inferior performance reveals that the models are   hard to memorize the multi - hop relation rules . We   attribute this deficiency to the multi - hop bias accu-   mulation andhigh - demanding memory capacity in   the implicit memorization strategy of HLP .   Multi - hop Bias Accumulation The HLP - based   embedding models implicitly perceive the multi-   hop relation rules by constraining each training   edge as shown in Section 2.3 . Nevertheless , the   single - edge constraints inevitably have biases dur-   ing the optimization , which will accumulate with   the increase of relation hops . This bias accumula-   tion makes the memorized relation rules unreliable ,   leading to the deficient generalization ability for   link prediction between distant entities . Concretely ,   considering the single - edge biases , the rule learn-   ing process in Equation ( 5 ) can be rewritten as :         b = a ◦ r ◦ ϵ   c = b ◦ r ◦ ϵ   c = a ◦ r ◦ ϵ⇒r = r ◦ r ◦ ϵ,(7 )   where ϵ=ϵ ◦ ϵ ◦ ϵis the cumulative bias .   Note that ϵis triple - dependent , which makes it   intractable for other queries , e.g. , ( x , r,?)in Figure   1 , to rely on this rule for prediction .   High - demanding Memory Capacity The HLP-   based models essentially learn the general rules   from the relation paths between head and tail en-   tities . With the increase of path length , the quan-   tity of different paths ( or rules ) expands exponen-   tially ( Wang et al . , 2021 ) . This requires intensive   memory to memorize the whole crucial relation   rules . However , the modeling capacity of embed-   ding models is insufficient to meet this requirement .   Since these models constrain basic edges to form   long - range paths following the bottom - up design   of HLP , they are more inclined to memorize the   low - order rules and forget the high - order rules .   Design Goal We seek to develop a general tech-   nique to alleviate the " Hard to Memorize " problem   of existing embedding models.6337   A straight - forward strategy is to directly extract   and process the enclosing subgraph between head   and tail entities ( Teru et al . , 2020 ) , which can avoid   the multi - hop bias accumulation . However , such a   sophisticated procedure needs to be executed once   for each candidate triple , which brings enormous   training and test time costs . For example , GraIL   ( Teru et al . , 2020 ) takes about 1 month to infer   on the full FB15k-237 test set ( Zhu et al . , 2021 ) .   Moreover , the enclosing subgraph extraction is also   constrained by the path length , severely harming   the performance of link prediction .   Therefore , this paper aims to propose a general   framework which can : ( 1 ) alleviate the deficiency   of HLP ; ( 2 ) enjoy the merits of validity and gener-   ality with tractable computational costs .   4 Methodology   4.1 Vertical Learning Paradigm   Inspired by the notion that “ to copy is easier than to   memorize ” ( Khandelwal et al . , 2020 ) , we propose a   vertical learning paradigm for KGC task . Different   from the implicit memorization strategy of HLP ,   VLP provides embedding models with the ability   to reference related triples as cues for prediction ,   which can be viewed as an explicit copy strategy .   More concretely , we present the overall pipeline   of VLP in Figure 2 . Given a query ( h , r , ? ) , the   procedure of predicting tail tcan be divided into   reference query selection , reference graph construc-   tion and reference answer aggregation .   Reference Query Selection For the input query   q= ( h , r , ? ) , the VLP - based models first select N   entity - relation pairs ( h , r)in the training graph as   the reference queries { q } , which can providerelevant semantics for prediction . For example ,   to answer ( Jill Biden , lives_in , ? ) , we can refer-   ence the answer - known query ( Joe Biden , lives_in ,   ? ) for target information , since Joe Biden andJill   Biden are highly related . One intuitive way for the   reference selection is to choose the top- kentities in   terms of the cosine similarity between hand all en-   tities involved in relation rduring the optimization .   Nevertheless , this approach incurs high computa-   tional costs and is intractable . Numerically , the   time complexity of such similarity calculation is   O(nd ) , where nis the number of r - involved   entities and n≈ |E| ≫ din the worst case .   In this work , inspired by the small world princi-   ple ( Newman , 2001 ; Liben - Nowell and Kleinberg ,   2007 ) , in which related individuals are connected   by short chains ( e.g. , Joe Biden andJill Biden are   directly connected by the marriage relationship ) ,   we introduce the graph distance based approach   for efficient reference query selection . Specifically ,   we select N r - involved entities { h}closest to   hin terms of their relative graph distance ( i.e. , the   shortest path length on the training graph ) . The cor-   responding ground - truth targets tof the reference   queries q= ( h , r,?)are referred as reference   answers . In this way , VLP - based models can pre-   retrieve Nrelated references for every input query ,   thus incurring no additional computational cost for   training and inference .   Reference Graph Construction After the ef-   ficient reference retrieval , we construct an edge-   attributed reference graph to integrate the selected   Nreference queries and their corresponding an-   swers with the input query . As shown in Figure 2 ,   the input query qis regarded as the central node ,   and the reference answers tare treated as the N   neighbors . VLP - based models aims to leverage the   explicit reference answers for prediction . However ,   since there is no guarantee that tis the same as the   target tail t , it is unreasonable to directly copy t   without any modification . For example , to answer   ( England , capital_is , ? ) , we can not directly copy   the answer of ( France , capital_is , ? ) .   Therefore , we introduce the query similarity   sas the edge attribute between qandt . By con-   sidering the query differences , VLP - based models   are able to adaptively copy the reference answers .   For example , to answer the input ( England , capi-   tal_is , ? ) , we can adjust the target information from   Paris in terms of the difference between ( France ,   capital_is , ? ) and the input query.6338Reference Answer Aggregation With the con-   structed reference graph , VLP - based models learn   to explicitly gather target information from neigh-   bor answers for prediction . Specifically , based on   the generalized functions summarized in Section   2.2 , the central node embedding qand neighbor   node embedding kcan be defined as :   q = Wh+b ,   k = Wt.(8 )   The edge embedding s(i.e . , query similarity   embedding ) can be further defined as :   s = q−q   = W(h−h).(9 )   Then , combining the neighbor nodes and edge at-   tributes , VLP - based models aggregate the reference   answers to generate the final embedding t :   t = σ(W[t , q ] ) ,   t=1   N / summationdisplay(Wk+Ws),(10 )   where σ(·)is a nonlinear activation function ( e.g. ,   tanh ) , [ · , · ] is the concatenate operation , W ,   W andWare shared projection matrices .   The output tshould be close to the target tail em-   bedding tin the latent space , whose score can be   revealed by the cosine similarity :   f(h , r , t ) = tt   ∥t∥∥t∥(11 )   We highlight that the VLP ’s aggregating strategy   in Equation ( 10 ) differs from GNN - based methods   ( Vashishth et al . , 2020 ; Bansal et al . , 2019 ; Shang   et al . , 2019 ; Schlichtkrull et al . , 2018 ) . For each   query ( h , r , ? ) , regardless of whether the reference   query is a neighbor of hin the training graph , VLP-   based models can directly attend to the reference   answer throughout the entire training set .   Score Function For each triple ( h , r , t ) in the test   sets , to alleviate the deficiency of HLP and predict   more accurately , we integrate the vertical score f   with the horizontal score fto form the final score   function fwith a weight hyper - parameter λ :   f(h , r , t ) = f(h , r , t ) + λf(h , r , t ) . ( 12 )   Note that VLP can be widely applied to various   embedding models , since the reference aggregation   is designed on the generalized score function . Complexity Analysis Compared with the vanilla   embedding models , the VLP - based models only   bring a few additional parameters , i.e. , the shared   aggregation matrices in Equation ( 10 ) . Therefore ,   the VLP - based models have the same space com-   plexity as the HLP - based models , i.e. , O(|E|d ) . In   the aspect of time cost for processing single triple ,   the time complexity of vanilla embedding mod-   els is O(dd ) , derived from the generalized score   function in Equation ( 4 ) . The VLP - based models   require the same computation for each reference ,   which produces the time complexity of O(Ndd ) .   Such computation is tractable since a small N(no   more than 8) is enough for VLP - based models to   achieve high performance in the experiments .   4.2 Optimization   During training , we jointly optimize fandfby a   two - component loss function with coefficient α :   L = L+αL. ( 13 ) .   For the former one , we use the cross - entropy   between predictions and labels as training loss :   L=−/summationdisplayylogp , ( 14 )   where pandyare the i - th components of pandy ,   respectively ; p∈Ris calculated by applying the   softmax function to the " 1 - to - All " ( Lacroix et al . ,   2018a ) results of f;y∈Ris the one - hop vector   that indicates the position of true label .   For the later one , negative sampling has been   proved quite effective in extensive works ( Song   et al . , 2021 ; Sun et al . , 2019 ) . Formally , for a posi-   tive triple ( h , r , t ) , we first sample a set of entities   { t}(or{h } ) based on the pre - sampling   weights pto construct negative triples ( h , r , t)(or   ( h , r , t ) ) . With these samples , a negative sampling   loss is designed to optimize embedding models :   L=−/summationdisplayp(h , r , t ) logσ(−f(h , r , t)−γ )   −logσ(γ+f(h , r , t ) ) , ( 15 )   where γis a pre - defined margin , σis the sigmoid   function , ldenotes the number of negative samples ,   ( h , r , t)is a negative sample against ( h , r , t ) . Im-   portantly , p(h , r , t)is the post - sampling weight ,   which determines the proportion of ( h , r , t)in the   current optimization.6339   As shown in Figure 3 , recent works ( Song et al . ,   2021 ; Chao et al . , 2021 ; Gao et al . , 2020 ; Sun et al . ,   2019 ) utilize the self - adversarial technique ( Self-   Adv ) , in which the pre - sampling weights follow a   uniform distribution and the post - sampling weights   increase with the negative scores . Differently , in   this work , we propose a new approach named ReD   based on the relative distance , which can draw   more informative negative samples and reduce the   toxicity of false negative samples .   For the pre - sampling weights , considering the   deficiency of embedding models as described in   Section 3 , the distant entities are usually hard to   be predicted as the target answer . It reveals a ra-   tional priori , i.e. , distant entities are more likely   to form easy ( meaningless ) negative triples . This   inspires us to sample more hard ( informative ) neg-   ative triples based on the relative graph distance   d. As shown in Figure 3 , the pre - sampling weight   in ReD decreases with the increase of graph dis-   tance between head and tail entities . Formally , for a   training query ( h , r , ? ) , we pre - sample entities tto   construct negatives from the following distribution :   p(h , r , t ) = exp−αd(h , t )   /summationtextexp−αd(h , t),(16 )   where αis the pre - sampling temperature , d ( · , · )   outputs the relative graph distance between two   entities . Note that the calculation of d(·,·)is a   one - time preprocessing step , which will not bring   additional training overhead .   For the post - sampling weights , Self - Adv assigns   greater weights to high scoring negative triples in   Equation ( 15 ) , which makes the optimization focus   more on hard negatives . However , this monotoni-   cally increasing strategy ignores the issue of false   negatives , since triples with higher scores are more   likely to be correct . A more rational posteriori   is that the easy negatives are underscored and thefalse negatives are overscored . In this work , we   use the relative latent distance between the posi-   tive and negative samples to determine whether the   negative score is too low or too high . Specifically ,   ReD defines the post - sampling weights as a distri-   bution that first rises and then falls as the negative   score increases . As shown in Figure 3 , if the nega-   tive score is significantly greater than ( or less than )   the positive score , this negative sample is more   likely to be false ( or easy ) , and thus be assigned a   small weight in the Equation 15 . Formally , based   on the positive score c = f(h , r , t ) and negative   score n = f(h , r , t ) , the post - sampling weight   in ReD is defined as :   p(h , r , t ) = expw(h , r , t)/summationtextexpw(h , r , t ) ,   w(h , r , t ) = /braceleftigg   αn , n≤c+τ   αc−αm , n > c+τ ,   m = n−c−τ , ( 17 )   where αandαare the post - sampling tempera-   tures . By combining the sampling weights in Equa-   tion ( 16 ) and ( 17 ) , ReD is able to generate and   process higher quality negatives for optimization .   5 Experiment   5.1 Experimental Setup   Datasets We evaluate our proposal on two   widely - used benchmarks : WN18RR ( Dettmers   et al . , 2018 ) and FB15k-237 ( Toutanova and Chen ,   2015 ) . More details can be found in Appendix B.   Baselines To verify the effectiveness and general-   ity of our proposal , we combine the proposed tech-   niques with three representative embedding models   DistMult ( Yang et al . , 2015 ) , ComplEx ( Trouillon   et al . , 2016 ) and RotatE ( Sun et al . , 2019 ) . For   performance comparison , we select a series of em-   bedding models as baselines in Table 2 .   Implementation Details We fine - tune the hyper-   parameters with the grid search on the validation   sets . Please see Appendix C for more details .   5.2 Main Results   The experimental results are reported in Table 2 .   Compared to DistMult , ComplEx and RotatE , all   three VLP - based versions achieve consistent and   significant improvements on both datasets . For   example , on WN18RR and FB15k-237 datasets ,   RotatE - VLP outperforms RotatE with 2.2%and6340   2.4%absolute improvements in MRR , respectively .   Such obvious gains reveal that the vertical contexts   generally inject valuable information into the em-   bedding models for more accurate prediction .   Moreover , one can further see that ComplEx-   VLP and RotatE - VLP perform competitively with   the SOTA baselines . Specifically , RotatE - VLP sur-   passes all the baselines in terms of most metrics   over both datasets ; ComplEx - VLP also achieves   promising performance on FB15k-237 compared   with the baselines . The superior performance fur-   ther confirms the effectiveness of our proposal .   5.3 Fine - grained Performance Analysis   Performance on Distance Splits Table 3 reports   the performance of three VLP - based models on the   distance splits defined in Appendix A. One can ob-   serve that : ( 1 ) the VLP - based embedding models   outperform the vanilla models across all the dis-   tance splits ; ( 2 ) the VLP models achieve greater   relative improvement on the split with larger d.   For example , as dincreases from 1to4 , RotatE-   VLP achieves 0.5%,4.3%,20.6%and22.0%rela-   tive improvements over RotatE on the MRR metric ,   respectively . This reveals that the explicit vertical   contexts can significantly alleviate the limitations   of memory strategy in the embedding models .   Performance on Each Relation To verify the   modeling capacity of our proposal from a fine-   grained perspective , we explore the performance of   VLP - based models on each relation of WN18RR   following ( Zhang et al . , 2019 ) . As shown in Ta-   ble 4 , compared to RotatE and QuatE , RotatE - VLP   surpasses them on all the 11 relation types , confirm-   ing that the explicit reference aggregation brings   superior modeling capacity .   Performance on Mapping Properties Table 5   exhibits the performance of our proposal on differ-   ent relation mapping properties ( Sun et al . , 2019 )   in FB15k-237 . We observe that RotatE - VLP con-   sistently outperforms RotatE across all RMP types .   Such advanced performance owes to the powerful   modeling capability of the explicit copy strategy.6341   5.4 Impact of Reference Quantity   VLP aggregates target information from Nrefer-   ences pre - selected before training . We investigate   the impact of Non the performance ( MRR ) of   VLP - based models . Figure 4 shows the results   on WN18RR dataset . As expected , all three VLP-   based models with more vertical references achieve   better performance than the ones with fewer refer-   ences , since the aggregation of sufficient references   brings the superior modeling capacity . Moreover ,   we can observe that the models can achieve high   performance with Nless than 10 , making the com-   putation tractable as discussed in Section 4.1 .   5.5 Ablation Study of ReD   To explore the effectiveness of the proposed ReD ,   we conduct ablation studies on the pre - sampling   and post - sampling parts of the three VLP - based   models . Table 6 shows the detailed results . We can   observe that the removal of any part reduces the   performance , which demonstrates that ReD makes   the model focus more on meaningful negative sam-   ples for more effective optimization . Moreover , we   also integrate ReD with original embedding models   to verify the generality of this technique . Please   refer to Appendix D for more results .   6 Related Work   Embedding models can be roughly categorized into   distance based models and semantic matching mod-   els ( Chao et al . , 2021 ) .   Distance based models use the Euclidean dis-   tance to measure the plausibility of each triple . A   series of work is conducted along this line , such as   TransE ( Bordes et al . , 2013 ) TransH ( Wang et al . ,   2014 ) , TransR ( Lin et al . , 2015 ) , RotatE ( Sun et al . ,   2019 ) , PairRE ( Chao et al . , 2021 ) , Rot - Pro ( Song   et al . , 2021 ) , ReflectE ( Zhang et al . , 2022 ) and so   on . TransE and RotatE are the most representative   distance - based models , which represent relations as   translations and rotations , respectively . Semantic   matching models utilize multiplicative functions   to score each triple , including RESCAL ( Nickel   et al . , 2011 ) , DistMult ( Yang et al . , 2015 ) , Com-   plEx ( Trouillon et al . , 2016 ) , QuatE ( Zhang et al . ,   2019 ) , DualE ( Cao et al . , 2021 ) and so on . Typi-   cally , RESCAL ( Nickel et al . , 2011 ) defines each   relation as the tensor decomposition matrix . Dist-   Mult ( Yang et al . , 2015 ) simplifies the relation   matrices to be diagonal for preventing overfitting .   However , existing embedding models essentially   follow the horizontal learning paradigm , underper-   forming in predicting links between distant entities .   Moreover , some advanced techniques are pro-   posed to improve embedding models , such as graph   encoders ( Schlichtkrull et al . , 2018 ; Shang et al . ,   2019 ; Vashishth et al . , 2020 ; Wang et al . , 2022 ) and   regularizers ( Lacroix et al . , 2018b ) . Note that our   proposals are orthogonal to these techniques , and   one can integrate them for better performance .   7 Conclusion   In this paper , we present a novel learning paradigm   named VLP for KGC task . VLP can be viewed as   an explicit copy strategy , which allows embedding   models to consult related triples for explicit refer-   ences , making it much easier to predict distant links .   Moreover , we also propose ReD , a new negative   sampling technique for more effective optimization .   The in - depth experiments on two datasets demon-   strate the validity and generality of our proposals.6342Limitations   Although our proposal enjoys the advantages of   validity and generality , there are still two major   limitations . First , VLP can not directly generalize   to the inductive setting , since VLP is defined based   on the score functions of transductive embedding   models . One potential direction is to design an   inductive reference selector for emerging entities .   Second , how to efficiently select more helpful refer-   ences for prediction is still an open challenge . We   expect future studies to mitigate these issues .   Acknowledgements   This work was supported in part by the National   Natural Science Foundation of China under Grant   62276044 , and also Sponsored by CAAI - Huawei   MindSpore Open Fund .   References6343   A Experimental Observation   The motive of our work originates from an experi-   mental observation , which shows that embedding   models underperform in predicting links between   distant entity pairs . Specifically , according to the   relative graph distance dbetween head and tail   entities of each test triple , we divide the test sets of   WN18RR and FB15k-237 into four splits . Three   representative embedding models ( DistMult , Com-   plEx and RotatE ) are tested on each split .   Figure 5 summarizes the detailed MRR results   and split ratios on the two datasets . We can observe   that all three embedding models achieve promising   results in link prediction between close entities ,   while the performance drops significantly in the   prediction between distant entities . For example ,   on the split where d= 1 in WN18RR , RotatE   achieves excellent performance ( MRR of 0.986 ) ,   while on the split where d= 2 , the performance   of RotatE decreases by about 62 % ( MRR of 0.375 ) .   B Datasets   Table 7 summarizes the detailed statistics of two   benchmark datasets . WN18RR ( Dettmers et al . ,6344   2018 ) and FB15k-237 ( Toutanova and Chen , 2015 )   datasets are subsets of WN18 ( Bordes et al . , 2013 )   and FB15k ( Bordes et al . , 2013 ) respectively with   inverse relations removed . WN18 is extracted from   WordNet ( Miller , 1995 ) , a database featuring lex-   ical relations between words . FB15k is extracted   from Freebase ( Bollacker et al . , 2008 ) , a large - scale   KG containing general knowledge facts .   C Implementation Details   We use Adam ( Kingma and Ba , 2015 ) as the op-   timizer and fine - tune the hyperparameters on the   validation dataset . The hyperparameters are tuned   by the grid search , including batch size b , embed-   ding dimension d , negative sampling temperatures   { α } , loss weight λand fixed margin γ . The   hyper - parameter search space is shown in Table 8 .   D Embedding Models with ReD   To verify the generality of the proposed negative   sampling technique ReD , we integrate ReD with   three representative embedding models ( i.e. , Dist-   Mult , ComplEx and RotatE ) for KGC task . As   shown in Table 9 , compared to Self - Adv , the em-   bedding models combined with ReD achieve better   performance on both datasets , since ReD guaran-   tees more informative negative samples from both   pre - sampling and post - sampling stages.6345ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 8 after the Conclusion section   /squareA2 . Did you discuss any potential risks of your work ?   Not applicable . Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   No response .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   No response .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   No response .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   No response .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   No response .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   No response .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   No response.6346 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   No response .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   No response .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.6347