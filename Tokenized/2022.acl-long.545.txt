  Puyuan Liu , Chenyang Huang , Lili Mou   Dept . Computing Science , Alberta Machine Intelligence Institute ( Amii )   University of Alberta , Canada   { puyuan , chuang8}@ualberta.ca , doublepower.mou@gmail.com   Abstract   Text summarization aims to generate a short   summary for an input text . In this work , we   propose a Non - Autoregressive Unsupervised   Summarization ( NAUS ) approach , which does   not require parallel data for training . Our   NAUS Ô¨Årst performs edit - based search to-   wards a heuristically deÔ¨Åned score , and gener-   ates a summary as pseudo - groundtruth . Then ,   we train an encoder - only non - autoregressive   Transformer based on the search result . We   also propose a dynamic programming ap-   proach for length - control decoding , which is   important for the summarization task . Ex-   periments on two datasets show that NAUS   achieves state - of - the - art performance for unsu-   pervised summarization , yet largely improving   inference efÔ¨Åciency . Further , our algorithm is   able to perform explicit length - transfer sum-   mary generation .   1 Introduction   Text summarization is an important natural lan-   guage processing ( NLP ) task , aiming at generating   concise summaries for given texts while preserving   the key information . It has extensive real - world   applications such as headline generation ( Nenkova   et al . , 2011 ) . In this paper , we focus on the set-   ting of sentence summarization ( Rush et al . , 2015 ;   Filippova et al . , 2015 ) .   State - of - the - art text summarization models are   typically trained in a supervised way with large   training corpora , comprising pairs of long texts and   their summaries ( Zhang et al . , 2020 ; Aghajanyan   et al . , 2020 , 2021 ) . However , such parallel data are   expensive to obtain , preventing the applications to   less popular domains and less spoken languages .   Unsupervised text generation has been attracting   increasing interest , because it does not require par-   allel data for training . One widely used approachis to compress a long text into a short one , and to   reconstruct it to the long text by a cycle consis-   tency loss ( Miao and Blunsom , 2016 ; Wang and   Lee , 2018 ; Baziotis et al . , 2019 ) . Due to the in-   differentiability of the compressed sentence space ,   such an approach requires reinforcement learning   ( or its variants ) , which makes the training difÔ¨Åcult   ( Kreutzer et al . , 2021 ) .   Recently , Schumann et al . ( 2020 ) propose an   edit - based approach for unsupervised summariza-   tion . Their model maximizes a heuristically deÔ¨Åned   scoring function that evaluates the quality ( Ô¨Çuency   and semantics ) of the generated summary , achiev-   ing higher performance than cycle - consistency   methods . However , the search approach is slow   in inference because hundreds of search steps are   needed for each data sample . Moreover , their ap-   proach can only select words from the input sen-   tence with the word order preserved . Thus , it is   restricted and may generate noisy summaries due   to the local optimality of search algorithms .   To address the above drawbacks , we propose   a Non - Autoregressive approach to Unsupervised   Summarization ( NAUS ) . The idea is to perform   search as in Schumann et al . ( 2020 ) and , inspired   by Li et al . ( 2020 ) , to train a machine learning   model to smooth out such noise and to speed up the   inference process . Different from Li et al . ( 2020 ) ,   we propose to utilize non - autoregressive decoders ,   which generate all output tokens in parallel due to   our following observations :   Non - autoregressive models are several times   faster than autoregressive generation , which is im-   portant when the system is deployed .   The input and output of the summarization task   have a strong correspondence . Non - autoregressive   generation supports encoder - only architectures ,   which can better utilize such input ‚Äì output cor-   respondence and even outperform autoregressive   models for summarization .   For non - autoregressive models , we can de-7916sign a length - control algorithm based on dynamic   programming to satisfy the constraint of output   lengths , which is typical in summarization applica-   tions but can not be easily achieved with autoregres-   sive models .   We conducted experiments on Gigaword head-   line generation ( Graff et al . , 2003 ) and DUC2004   ( Over and Yen , 2004 ) datasets . Experiments show   that our NAUS achieves state - of - the - art perfor-   mance on unsupervised summarization ; especially ,   it outperforms its teacher ( i.e. , the search approach ) ,   conÔ¨Årming that NAUS can indeed smooth out the   search noise . Regarding inference efÔ¨Åciency , our   NAUS with truncating is 1000 times more efÔ¨Åcient   than the search approach ; even with dynamic pro-   gramming for length control , NAUS is still 100   times more efÔ¨Åcient than search and several times   more efÔ¨Åcient than autoregressive models . Our   NAUS is also able to perform length - transfer sum-   mary generation , i.e. , generating summaries of dif-   ferent lengths from training .   2 Approach   In our approach , we Ô¨Årst follow Schumann et al .   ( 2020 ) and obtain a summary by discrete search   towards a heuristically deÔ¨Åned objective function   ( ¬ß 2.1 ) . Then , we propose a non - autoregressive   model for the summarization task ( ¬ß 2.2 ) . We   present the training strategy and the proposed   length - control algorithm in ¬ß 2.3 .   2.1 Search - Based Summarization   Consider a given source text x= ( x;x ; : : : ; x ) .   The goal of summarization is to Ô¨Ånd a shorter text   y= ( y;y ; : : : ; y)as the summary .   Our work on unsupervised summarization fol-   lows the recent progress of search - based text gener-   ation ( Liu et al . , 2020 , 2021a ; Kumar et al . , 2020 ) .   Schumann et al . ( 2020 ) formulate summarization   as word - level extraction ( with order preserved ) , and   apply edit - based discrete local search to maximize   a heuristically designed objective .   SpeciÔ¨Åcally , the objective function considers   two aspects : ( 1 ) a language Ô¨Çuency score f(y ) ,   given by the reciprocal of a language model ‚Äôs   perplexity ; and ( 2 ) a semantic similarity score   f(y;x ) , given by the cosine embeddings . The   overall objective combines the two aspects as   f(y;x ) = f(y)f(y;x)(1 )   where   is a weighting hyperparameter . Interestedreaders are referred to Schumann et al . ( 2020 ) for   the details of the scoring function .   Further , the desired summary length can be spec-   iÔ¨Åed as a hard constraint , achieved by searching   only among sentences of the correct length . Sup-   pose the desired summary length is T , the approach   selectsTrandom words from the input , and max-   imizes the scoring function ( 1)by changing the   selection and non - selection of two words .   A greedy hill - climbing algorithm determines   whether the change is accepted or not . In other   words , a change is accepted if the score improves ,   or rejected otherwise . Such a process continues   until a ( possibly local ) optimum is found .   A pilot analysis in Schumann et al . ( 2020 ) shows   that words largely overlap between a source text   and its reference summary . This explains the high   performance of such a word extraction approach ,   being a state - of - the - art unsupervised summariza-   tion system and outperforming strong competitors ,   e.g. , cycle consistency ( Wang and Lee , 2018 ; Bazi-   otis et al . , 2019 ) .   2.2 Non - Autoregressive Model for   Summarization   Despite the high performance , such edit - based   search has several drawbacks . First , the search   process is slow because hundreds of local search   steps are needed to obtain a high - quality summary .   Second , their approach only extracts the original   words with order preserved . Therefore , the gener-   ated summary is restricted and may be noisy .   To this end , we propose a Non - Autoregressive   approach to Unsupervised Summarization ( NAUS )   by learning from the search results . In this way ,   the machine learning model can smooth out the   search noise and is much faster , largely alleviat-   ing the drawbacks of search - based summarization .   Compared with training an autoregressive model   from search ( Li et al . , 2020 ) , non - autoregressive   generation predicts all the words in parallel , further   improving inference efÔ¨Åciency by several times .   Moreover , a non - autoregressive model enables   us to design an encoder - only architecture , which is   more suited to the summarization task due to the   strong correspondence between input and output ,   which can not be fully utilized by encoder ‚Äì decoder   models , especially autoregressive ones .   SpeciÔ¨Åcally , we propose to use multi - layer   Transformer ( Vaswani et al . , 2017 ) as the non-   autoregressive architecture for summarization.7917   Each Transformer layer is composed of a multi-   head attention sublayer and a feed - forward sub-   layer . Additionally , there is a residual connection   in each sublayer , followed by layer normalization .   LetX2Rbe the representation at the   nth layer , where Tis the number of words and d   is the dimension . Specially , the input layer Xis   the embeddings of words . Suppose we have hat-   tention heads . The output of the ithhead in thenth   attention sublayer is A= softmax   V ,   whereQ , K , andVare matrices calculated by   three distinct multi - layer perceptrons ( MLPs ) from   X;dis the attention dimension .   Multiple attention heads are then concatenated :   A= Concat    A;:::;A   W   whereW2Ris a weight matrix .   Then , we have a residual connection and layer   normalization by   A= LayerNorm    X+A   ( 2 )   Further , an MLP sublayer processes A , followed   by residual connection and layer normalization ,   yielding the nth layer ‚Äôs representation   X= LayerNorm A+ MLP ( A)   ( 3 )   The last Transformer layer Xis fed to   softmax to predict the words of the summary in a   non - autoregressive manner , that is , the probability   at thetth step is given by softmax(Wx ) , where   xis thetth row of the matrix XandWis   the weight matrix .   It is emphasized that , in the vocabulary , we in-   clude a special blank token  , which is handled by   dynamic programming during both training and in-   ference ( ¬ß 2.3 ) . This enables us to generate a shorter   summary than the input with such a multi - layerTransformer .   Our model can be thought of as an encoder-   only architecture , differing from a typical encoder ‚Äì   decoder model with cross attention ( Vaswani et al . ,   2017 ; Baziotis et al . , 2019 ; Zhou and Rush , 2019 ) .   Previously , Su et al . ( 2021 ) propose a seemingly   similar model to us , but put multiple end - of-   sequence ( EOS ) tokens at the end of the generation ;   thus , they are unable to maintain the correspon-   dence between input and output . Instead , we allow   blank tokens scattering over the entire sentence ;   the residual connections in Eqns ( 2)and(3)can   better utilize such input ‚Äì output correspondence for   summarization .   2.3 Training and Inference   In this section , we Ô¨Årst introduce the Connectionist   Temporal ClassiÔ¨Åcation ( CTC ) training . Then , we   propose a length - control decoding approach for   summary generation .   CTC Training . The Connectionist Temporal   ClassiÔ¨Åcation ( CTC , Graves et al . , 2006 ) algorithm   allows a special blank token in the vocabulary ,   and uses dynamic programming to marginalize out   such blank tokens , known as latent alignment ( Sa-   haria et al . , 2020 ) . In addition , non - autoregressive   generation suffers from a common problem that   words may be repeated in consecutive steps ( Gu   et al . , 2018 ; Lee et al . , 2018 ) ; thus , CTC merges   repeated words unless separated by . For example ,   the sequence of tokens aaabb is reduced to the   textaab , denoted by  (aaabb ) = aab .   Concretely , the predicted likelihood is marginal-   ized over all possible Ô¨Ållings of  , i.e. , all possible   token sequences that are reduced to the groundtruth   text :   P(yjx ) = XP(wjx ) ( 4)7918whereP(wjx)is the probability of generating a   sequence of tokens w. Although enumerating every   candidate infw :  (w ) = ygis intractable , such   marginalization fortunately can be computed by   dynamic programming in an efÔ¨Åcient way .   Let  = PP(wjx)be the   marginal probability of generating yup to the   sth decoding slot . Moreover ,  is deÔ¨Åned to be   the probability that wis all , thus not having   matched any word in y. The  variable can be   further decomposed into two terms  =  +   , where the Ô¨Årst term is such probability with   w= , and the second term w6=. Apparently ,   the initialization of  variables is   = P(w=jx ) ( 5 )   = P(w= yjx ) ( 6 )   = 0;8t1 ( 7 )   = 0;8t>1ort= 0 ( 8)   Eqn . ( 7)is because , at the Ô¨Årst prediction slot , the   empty token does not match any target words ;   Eqn . ( 8)is because the predicted non- Ô¨Årst token   must match exactly the Ô¨Årst target word .   The recursion formula for  is   =  P(w=jx )   since the newly predicted token with probabil-   ityP(w=jx)does not match any target word ,   inheriting  .   The recursion formula for  is   = 8   > > <   > > :     +     P(w= yjx ) ;   ify= y     +     P(w= yjx ) ;   otherwise   Here , wis not , so we must have w= y , having   the predicted probability P(w= yjx ) .   Ify= y , then we have two sub - cases : Ô¨Årst ,   wis reduced to ywithw=separat-   ing two repeating words in y , having probability   ; or second , wis reduced to ywith   w= y6= , having probability  , which   implies we are merging wandw .   Ify6= y , wis reduced to either y   ory . In the Ô¨Årst case , wcan be either or   non- , given by  =  +  .   In the second case , we must have w6= , which   has a probability of  .   Finally ,  is the marginal probability in   Eqn . ( 4 ) , as it is the probability that the entire gen-   erated sequence matches the entire target text .   The CTC maximum likelihood estimation is to   maximize the marginal probability , which is equiv-   alent to minimizing the loss    . Since the   dynamic programming formulas are differentiable ,   the entire model can be trained by backpropagation   in an end - to - end manner with auto - differentiation   tools ( such as PyTorch ) .   Length - Control Inference . Controlling output   length is the nature of the summarization task , for   example , displaying a short news headline on a mo-   bile device . Moreover , Schumann et al . ( 2020 )   show that the main evaluation metric ROUGE   ( Lin , 2004 ) is sensitive to the summary length , and   longer summaries tend to achieve higher ROUGE   scores . Thus , it is crucial to control the summary   length for fair comparison .   We propose a length - control algorithm by dy-   namic programming ( DP ) , following the nature of   CTC training . However , our DP is an approximate   algorithm because of the dependencies introduced   by removing consecutive repeated tokens . Thus ,   we equip our DP with a beam search mechanism .   We deÔ¨Åne Bto be a set of top- Bsequences   withspredicted tokens that are reduced to twords .   Bis constructed by three scenarios .   First , the blank token is predicted for the sth   generation slot , and thus the summary length tre-   mains the same , shown by the blue arrow in Fig-   ure 2 . This yields a set of candidates   B=   b:b2B 	  ( 9 )   whererefers to string / token concatenation .   Second , a repeated word is predicted for the sth   generation slot , i.e. , bfor a subsequence bof   lengths 1 . In this case , the summary length talso   remains the same , also shown in the blue arrow in   Figure 2 . This gives a candidate set   B=   bb : b2B 	  ( 10 )   Third , a non-  , non - repeating word wis gener-   ated , increasing the summary length from t 1to7919 t , shown by the red arrow in Figure 2 . This gives   B= top   bw : b2B;w6= ;   w6= b 	  ( 11 )   where topselects the best Belements by the   probabilityP(wjx ) .   Based on the three candidates sets , we select   top - Bsequences to keep the beam size Ô¨Åxed :   B= top(B[B[B ) ( 12 )   where the sequences are ranked by their predicted   joint probabilities .   Theorem 1 . ( 1 ) If repeating tokens are not merged ,   then the proposed length - control algorithm with   beam sizeB= 1 Ô¨Ånds the exact optimum B   being the most probable length- Tsentence given   bySprediction slots . ( 2 ) If we merge repeating   tokens predicted by CTC - trained models , the above   algorithm may not be exact .   Appendix A presents the proof of the theorem   and provides a more detailed analysis , showing   that our length - control algorithm , although being   approximate inference , can generate a summary of   the desired length properly . Compared with trun-   cating an overlength output , our approach is able   to generate more Ô¨Çuent and complete sentences .   Also , our length - control algorithm is different from   conventional beam search , shown in Appendix C.   3 Experiments   3.1 Setup   Datasets . We evaluated our NAUS model on Giga-   word headline generation and DUC2004 datasets .   The headline generation dataset ( Rush et al . ,   2015 ) is constructed from the Gigaword news cor-   pus ( Graff et al . , 2003 ) , where the Ô¨Årst sentence   of a news article is considered as input text and   the news title is considered as the summary . The   dataset contains 3.8M/198K/1951 samples for train-   ing / validation / test . Based on the analysis of the   training size in Appendix B , we used 3 M samples   for training NAUS .   It should be emphasized that , when NAUS learns   from search , we only use the input of the training   corpus : we perform search ( Schumann et al . , 2020 )   for each input , and train our NAUS from the search   results . Therefore , we do not utilize any labeled   parallel data , and our approach is unsupervised .   Moreover , we considered two settings with de-   sired summary lengths of 8 and 10 , following Schu - mann et al . ( 2020 ) . Our NAUS is trained from   respective search results .   The DUC2004 dataset ( Over and Yen , 2004 ) is   designed for testing only with 500 samples , where   we also take the Ô¨Årst sentence of an article as the   input text . Our NAUS is transferred from the above   headline generation corpus . Based on the length   of DUC2004 summaries , we trained NAUS from   search results with 13 words , also following Schu-   mann et al . ( 2020 ) for fair comparison .   Evaluation Metrics . We evaluated the quality   of predicted summaries by ROUGE scores(Lin ,   2004 ) , which are the most widely used metrics   in previous work ( Wang and Lee , 2018 ; Baziotis   et al . , 2019 ; Zhou and Rush , 2019 ) . SpeciÔ¨Åcally ,   ROUGE - nevaluatesn - gram overlap between a   predicted summary and its reference summary ;   ROUGE - L , instead , measures the longest common   sequence between the predicted and reference sum-   maries .   Different ROUGE variants are adopted in previ-   ous work , depending on the dataset . We followed   the standard evaluation scripts and evaluated head-   line generation by ROUGE F1 ( Wang and Lee ,   2018 ; Baziotis et al . , 2019 ; Schumann et al . , 2020 )   and DUC2004 by Truncate ROUGE Recall ( Dorr   et al . , 2003 ; West et al . , 2019 ) .   In addition to summary quality , we also eval-   uated the inference efÔ¨Åciency of different meth-   ods , as it is important for the deployment of deep   learning models in real - time applications . We   report the average inference time in seconds for   each data sample , and compare the speedup with   Schumann et al . ( 2020 ) ‚Äôs search approach , which   achieves ( previous ) state - of - the - art ROUGE scores .   Our experiments were conducted on an i9 - 9940X   CPU and an RTX6000 graphic card . Appendix B   presents additional implementation details .   3.2 Results and Analyses   Main Results . Table 1 presents the performance of   our model and baselines on the Gigaword headline   test set . For a fair comparison , we categorize all   approaches by average summary lengths of ~8 and   ~10 into Groups A and B , respectively .   The Lead baseline extracts the Ô¨Årst several words   of the input sentence . Despite its simplicity , the   Lead approach is a strong summarization baseline   adopted in most previous work ( F√©vry and Phang ,   2018 ; Baziotis et al . , 2019).7920   Wang and Lee ( 2018 ) utilize cycle consis-   tency ( Miao and Blunsom , 2016 ) for unsupervised   summarization ; the performance is relatively low ,   because the cycle consistency loss can not ensure   the generated text is a valid summary . Zhou and   Rush ( 2019 ) perform beam search towards a step-   by - step decomposable score of Ô¨Çuency and contex-   tual matching . Both are unable to explicitly control   the summary length : in a fair comparison of length   10 ( Group B , Table 1 ) , their performance is worse   than the ( previous ) state - of - the - art approach ( Schu-   mann et al . , 2020),which performs edit - based   local search .   Our NAUS approach follows Schumann et al .   ( 2020 ) , but trains a non - autoregressive model fromsearch results . We consider two settings for con-   trolling the summary length : truncating longer   summaries and decoding with our proposed length-   control algorithm . Both of our variants outperform   Schumann et al . ( 2020 ) by 1.21‚Äì2.73 in terms of the   total ROUGE score ( Rows 5‚Äì6 & 13‚Äì14 , Table 1 ) .   As mentioned , Schumann et al . ( 2020 ) only extract   original words with order preserved , yielding noisy   sentences . Our NAUS , as a student , learns from the   search - based teacher model and is able to smooth   out its noise . This is a compelling result , as our   student model outperforms its teacher .   Regarding inference efÔ¨Åciency , our NAUS   method with truncating is more than 1300 times   faster than Schumann et al . ( 2020 ) , because we   do not need iterative search . Even with dynamic   programming and beam search for length control ,   NAUS is still over 100 times faster . This shows our   NAUS is extremely efÔ¨Åcient in inference , which is   important for real - time applications .   Although the efÔ¨Åciency of Wang and Lee ( 2018 )   and Zhou and Rush ( 2019 ) is not available , we   still expect our approach to be a few times faster   ( despite our higher ROUGE scores ) because their   models are autoregressive . By contrast , our NAUS   is non - autoregressive , meaning that it predicts all   words simultaneously . We will provide a con-   trolled comparison between autoregressive and non-   autoregressive models in Table 3 .   Table 2 shows the results on the DUC2004   dataset . The cycle - consistency approach ( Bazio-7921   tis et al . , 2019 ; West et al . , 2019 ) does not per-   form well on this dataset , outperformed by an   early rule - based syntax tree trimming approach ( Za-   jic et al . , 2004 ) and the state - of - the - art edit - based   search ( Schumann et al . , 2020 ) .   The performance of our NAUS model is con-   sistent with Table 1 , outperforming all previous   methods in terms of the total ROUGE score , and   being 100‚Äì1000 times faster than the search ap-   proach ( Schumann et al . , 2020 ) .   In general , the proposed NAUS not only achieves   state - of - the - art ROUGE scores for unsupervised   summarization , but also is more efÔ¨Åcient when de-   ployed . Results are consistent on both datasets ,   demonstrating the generality of our NAUS .   In - Depth Analyses . We conduct in - depth anal-   yses on the proposed NAUS model in Table 3 . Due   to the limit of time and space , we chose the Giga-   word headline generation as our testbed . All the   autoregressive ( AR ) and non - autoregressive ( NAR )   variants learn from the search output of our replica-   tion ( Rows 2 & 11 ) , where we achieve very close   results to those reported in Schumann et al . ( 2020 ) .   We Ô¨Årst tried vanilla encoder ‚Äì decoder NAR   Transformer ( Rows 4 & 13 , Gu et al . , 2018 ) , where   we set the number of decoding slots as the de-   sired summary length ; thus , the blank token and   the length - control algorithm are not needed . As   seen , a vanilla NAR model does not perform well , and CTC largely outperforms vanilla NAR in both   groups ( Rows 5‚Äì6 & 14‚Äì15 ) . Such results are   highly consistent with the translation literature ( Sa-   haria et al . , 2020 ; Chan et al . , 2020 ; Gu and Kong ,   2021 ; Qian et al . , 2021 ; Huang et al . , 2022 ) .   The proposed encoder - only NAUS model out-   performs encoder ‚Äì decoder ones in both groups in   terms of the total ROUGE score , when the sum-   mary length is controlled by either truncating or   length - control decoding ( Rows 8‚Äì9 & 17‚Äì18 ) . Pro-   foundly , our non - autoregressive NAUS is even bet-   ter than the autoregressive Transformer ( Rows 3   & 12 ) . We also experimented with previous non-   autoregressive work for supervised summariza-   tion ( Su et al . , 2021)in our learning - from - search   setting . Although their approach appears to be   encoder - only , it adds end - of - sequence ( EOS ) to-   kens at the end of the generation , and thus is unable   to utilize the input ‚Äì output correspondence . Their   performance is higher than vanilla NAR models ,   but lower than ours . By contrast , NAUS is able to   capture such correspondence with the residual con-   nections , i.e. , Eqns . ( 2)and(3 ) , in its encoder - only   architecture .   Generally , the efÔ¨Åciency of encoder - only NAR   ( without length - control decoding ) is ~2 times faster   than encoder ‚Äì decoder NAR and ~20 times faster   than the AR Transformer .   Further , our length - control decoding improves   the total ROUGE score , compared with truncating ,   for both encoder ‚Äì decoder CTC and encoder - only   NAUS models ( Rows 6 , 9 , 15 , & 18 ) , although its   dynamic programming is slower . Nevertheless , our   non - autoregressive NAUS with length control is   ~200 times faster than search and ~3 times faster   than the AR Transformer .   Additional Results . We present additional re-   sults in our appendices :   C. Analysis of Beam Search   D. Case Study   E. Human Evaluation   F. Length - Transfer Summarization79224 Related Work   Summarization systems can be generally catego-   rized into two paradigms : extractive and abstrac-   tive . Extractive systems extract certain sentences   and clauses from input , for example , based on   salient features ( Zhou and Rush , 2019 ) or feature   construction ( He et al . , 2012 ) . Abstraction systems   generate new utterances as the summary , e.g. , by   sequence - to - sequence models trained in a super-   vised way ( Zhang et al . , 2020 ; Liu et al . , 2021b ) .   Recently , unsupervised abstractive summariza-   tion is attracting increasing attention . Yang et al .   ( 2020 ) propose to use the Lead baseline ( Ô¨Årst sev-   eral sentences ) as the pseudo - groundtruth . How-   ever , such an approach only works with well-   structured articles ( such as CNN / DailyMail ) . Wang   and Lee ( 2018 ) and Baziotis et al . ( 2019 ) use cycle   consistency for unsupervised summarization . Zhou   and Rush ( 2019 ) propose a step - by - step decompos-   able scoring function and perform beam search for   summary generation . Schumann et al . ( 2020 ) pro-   pose an edit - based local search approach , which   allows a more comprehensive scoring function and   outperforms cycle consistency and beam search .   Our paper follows Schumann et al . ( 2020 ) but   trains a machine learning model to improve efÔ¨Å-   ciency and smooth out search noise . Previously ,   Li et al . ( 2020 ) Ô¨Åne - tune a GPT-2 model based   on search results for unsupervised paraphrasing ;   Jolly et al . ( 2022 ) adopt the search - and - learning   framework to improve the semantic coverage for   few - shot data - to - text generation . We extend pre-   vious work in a non - trivial way by designing a   non - autoregressive generator and further proposing   a length - control decoding algorithm .   The importance of controlling the output length   is recently realized in the summarization commu-   nity . Baziotis et al . ( 2019 ) and Su et al . ( 2021 )   adopt soft penalty to encourage shorter sentences ;   Yang et al . ( 2021 ) and Qi et al . ( 2021 ) control the   summary length through POS tag and EOS predic-   tions . None of these studies can control the length   explicitly . Song et al . ( 2021 ) is able to precisely   control the length by progressively Ô¨Ålling a pre-   determined number of decoding slots , analogous to   the vanilla NAR model in our non - autoregressive   setting .   Non - autoregressive generation is originally pro-   posed for machine translation ( Gu et al . , 2018 ; Guo   et al . , 2020 ; Saharia et al . , 2020 ) , which is later   extended to other text generation tasks . Wisemanet al . ( 2018 ) address the table - to - text generation   task , and model output segments by a hidden semi-   Markov model ( Ostendorf et al . , 1996 ) , simulta-   neously generating tokens for all segments . Jia   et al . ( 2021 ) apply non - autoregressive models to   extractive document - level summarization . Su et al .   ( 2021 ) stack a non - autoregressive BERT model   with a conditional random Ô¨Åeld ( CRF ) for abstrac-   tive summarization ; since the summary is shorter   than the input text , their approach puts multiple   end - to - sequence ( EOS ) tokens at the end of the   sentence , and thus is unable to utilize the strong   input ‚Äì output correspondence in the summarization   task . Yang et al . ( 2021 ) apply auxiliary part - of-   speech ( POS ) loss and Qi et al . ( 2021 ) explore   pretraining strategies for encoder ‚Äì decoder non-   autoregressive summarization . All these studies   concern supervised summarization , while our pa-   per focuses on unsupervised summarization . We   adopt CTC training in our encoder - only architec-   ture , allowing blank tokens to better align input   and output words , which is more appropriate for   summarization .   5 Conclusion   In this work , we propose a non - autoregressive un-   supervised summarization model ( NAUS ) , where   we further propose a length - control decoding al-   gorithm based on dynamic programming . Exper-   iments show that NAUS not only archives state-   of - the - art unsupervised performance on Gigaword   headline generation and DUC2004 datasets , but   also is much more efÔ¨Åcient than search methods   and autoregressive models . Appendices present ad-   ditional analyses and length - transfer experiments .   Limitation and Future Work . Our paper fo-   cuses on unsupervised summarization due to the   importance of low - data applications . One limita-   tion is that we have not obtained rigorous empirical   results for supervised summarization , where the   developed model may also work . This is because   previous supervised summarization studies lack   explicit categorization of summary lengths ( Yang   et al . , 2020 ; Qi et al . , 2021 ) , making comparisons   unfair and problematic ( Schumann et al . , 2020 ) .   Such an observation is also evidenced by Su et al .   ( 2021 ) , where the same model may differ by a few   ROUGE points when generating summaries of dif-   ferent lengths . Nevertheless , we have compared   with Su et al . ( 2021 ) in our setting and show the su-   periority of the NAUS under fair comparison . We7923plan to explore supervised summarization in future   work after we establish a rigorous experimental   setup , which is beyond the scope of this paper .   6 Acknowledgments   We thank Raphael Schumann for providing valu-   able suggestions on the work . We also thank the   Action Editor and reviewers for their comments dur-   ing ACL Rolling Review . The research is supported   in part by the Natural Sciences and Engineering   Research Council of Canada ( NSERC ) under grant   No . RGPIN2020 - 04465 , the Amii Fellow Program ,   the Canada CIFAR AI Chair Program , a UAHJIC   project , a donation from DeepMind , and Compute   Canada ( www.computecanada.ca ) .   References7924   A Proof of Theorem 1   Theorem 1 . ( 1 ) If repeating tokens are not merged ,   then the proposed length - control algorithm with   beam sizeB= 1 Ô¨Ånds the exact optimum B   being the most probable length- Tsentence given   bySprediction slots . ( 2 ) If we merge repeating   tokens predicted by CTC - trained models , the above   algorithm may not be exact .   Proof . [ Part ( 1 ) ] This part concerns a variant of our   decoding algorithm , which only removes the blank   tokenbut does not merge consecutive repeated   tokens to a single word , i.e. , Eqn . ( 10 ) is removed .   We denote this by   , for example ,  (aaabb ) =   aaabb , as opposed to  (aaabb ) = aabin our   algorithm . We now show that , based on   , our   dynamic programming algorithm in ¬ß 2.3 with beam   sizeB= 1is an exact inference algorithm .   We deÔ¨Åne  = maxP(bjx ) ,   wherejjdenotes the length of a sequence . In7925other words ,  is the maximum probability of s   tokens that are reduced to twords .   According to the deÔ¨Ånition , we have   = P(w=jx ) ( 13 )   = maxP(wjx ) ( 14 )   = 0 fors > t ( 15 )   In(13 ) ,  refers to the probability of one to-   ken that is reduced to zero words , in which case   the Ô¨Årst predicted token can only be the blank to-   ken , corresponding to Eqn . ( 9)withs= 1 and   t= 0 . Likewise ,  is the maximum probability   of one token that is reduced to one word . Thus ,   it is the probability of the most probable non- to-   ken , corresponding to Eqn . ( 11 ) with s= 1 and   t= 0 . Eqn . ( 15 ) asserts that fewer tokens can not   be reduced to more words ; it is used for mathe-   matical derivations , but need not to be explicitly   implemented in our algorithm in ¬ß 2.3 .   The recursion variable  is computed by   = maxn   P(w=jx ) ;   maxP(wjx)o   ( 16 )   In other words , the variable  can inherit   with a predicted blank token  , corresponding to   Eqn . ( 9 ) ; or it can inherit  with a predicted   non-token , corresponding to Eqn . ( 11 ) . Specially ,   ift= 0 , then the second term has  unde-   Ô¨Åned , and thus is ignored in the max operation .   We need the max operator to take the higher   probability in the two cases , since  is the max-   imum probability of stokens being reduced to t   words . This corresponds to Eqn . ( 12 ) with beam   sizeB= 1 .   To sum up , our inductive calculation guaran-   tees that  is the exact maximum probability of   maxP(bjx)for the desired length   TwithSgeneration slots ; our algorithm ( if not   merging repeating tokens ) gives the correspond-   ingBasargmaxP(bjx)under the same con-   straints , concluding the proof of Part ( 1 ) .   [ Part ( 2 ) ] CTC training merges consecutive re-   peated tokens to a single word , unless separated by   the blank token (Graves et al . , 2006 ) . Since our   model is trained by CTC , we should adopt this rule   in inference as well . We show in this part that our   algorithm , with beam size B= 1 , may not yield   the exact optimum with an example in Table 4 .   We consider generating a sentence of two words   from the two prediction slots , i.e. , S = T= 2 .   Apparently , the optimal sequence is ‚Äú I like ‚Äù with   probability 0:390:9 = 0:351 . However , the al-   gorithm would predict B = f‚Äúlike‚Äùgbecause   ‚Äú like ‚Äù is the most probably token in the Ô¨Årst slot .   Then , our algorithm will give B = f‚Äúlike I‚Äùg ,   because it has to select a non - repeating token based   on  , yielding a non - optimal solution .   It is noted that , if we do not merge repeating   tokens as in   , our algorithm will give the exact   optimum ‚Äú like like ‚Äù in the above example . This   shows that merging consecutive repeated tokens   requires the decoding algorithm to correct early   predictions , and thus , our dynamic programming   becomes an approximate inference . Nevertheless ,   our algorithm is able to generate a sequence of   the desired length properly ; its approximation hap-   pens only when the algorithm compares more rep-   etitions with fewer s versus more s with fewer   repetitions . Such approximation is further allevi-   ated by beam search in our dynamic programming .   Therefore , the proposed length - control algorithm is   better than truncating a longer sentence ; especially ,   our approach generates more Ô¨Çuent and complete   sentences .   B Implementation Details   Our NAUS had a Transformer encoder as the ba-   sic structure , generally following the settings in   Vaswani et al . ( 2017 ): 6 encoder layers , each hav-   ing 8 attention heads . The dimension was 512 for   attention and 2048 for feed - forward modules .   Our training used a batch size of 4 K tokens ,   with a maximum of 200 K updates . We used Adam   with  = ( 0:9;0:98 ) . In general , the learning rate   warmed up to 5e-4 in the Ô¨Årst 10 K steps , and then   decayed to 1e-9 with the inverse square - root sched-   ule , except that we Ô¨Ånd the maximum learning rate   of 1e-4 worked better for headline generation with   the summary length of 8 . We set the ` weight de-   cay to 0.01 . Our length - control decoding algorithm7926   had a beam size of 6 . More details can be found in   our repository ( Footnote 1 ) .   Our NAUS training is based on Schumann et al .   ( 2020 ) ‚Äôs prediction on the input of the Gigaword   headline generation training set . We show perfor-   mance against the number of training samples in   Figure 3 . As seen , NAUS outperforms its search   teacher even with a small set of 0.1 million sam-   ples . The performance saturates as the number of   samples increases . Based on this analysis , we used   3 million samples from the 3.8 million Gigaword   training set to train our NAUS models .   Each reported number in Tables 1‚Äì3 were aver-   aged over 10 independent runs , whereas the results   in Table 7 ( Appendix F ) were based on a single run   due to the limited time .   C Analysis of Beam Search   As mentioned , our length - control decoding algo-   rithm involves beam search within its dynamic pro-   gramming , because the algorithm does not Ô¨Ånd   the exact optimum when it merges repeating words .   We analyze the effect of the beam size in our length-   control algorithm .   In addition , we compare our approach with CTC   beam search ( Graves et al . , 2006).Typically , a   CTC - trained non - autoregressive model can be de-   coded either greedily or by beam search . The   greedy decoding Ô¨Ånds the most probable token at   each step , i.e. , w= argmaxP(wjx ) , and re-   duces the tokens to a sentence by  (w;;w ) ,   whereTis the number of decoding steps .   The CTC beam search algorithm searches for   the most likely sentence by marginalizing all   token sequences that are reduced to y , i.e. ,   argmaxPP(wjx ) .   We show results in Figure 4 , where we chose 10-   word Gigaword headline generation as the testbed   with our NAUS model ( Group B , Table 1 ) . Notice   that CTC beam search does not control the output   length , and for fair comparison , we truncated its   generated summaries . This also shows that our   novel decoding approach and CTC beam search   are distinct algorithms .   As seen in Figure 4a , the beam search does play   a role in our length - control algorithm . When the   beam enlarges from 1 to 6 , the performance ( or-   ange solid line ) increases by 1.2 points in R , the   difference of total ROUGE in comparison with   Schumann et al . ( 2020 ) under our replication ( Row   10 , Table 1 ) . However , further increasing the beam   size does not yield additional performance gain .   This is consistent with previous literature in autore-   gressive generation ( Meister et al . , 2020 ) , which   also suggests a beam size of 5‚Äì7 is the best in   their applications . In terms of the efÔ¨Åciency ( Fig-   ure 4b ) , a larger beam size monotonically increases   the inference time . However , the overhead of beam   search is relatively small in our dynamic program-   ming , and thus we chose a beam size of 6 in our   experiments .   Our length - control algorithm signiÔ¨Åcantly out-   performs CTC beam search ( dashed blue lines ) in   terms of both R and efÔ¨Åciency . Especially , CTC   beam search is three times slower , and degrades   more signiÔ¨Åcantly than our length - control decoding   when the beam size increases .   D Case Study   We show in Table 6 example summaries generated   by our NAUS with truncating and length - control   decoding , as well as the previous state - of - the - art   method ( Schumann et al . , 2020 ) . We observe that   NAUS without length control generates slightly   longer summaries , and if truncated , the output may   be incomplete ; by contrast , our length - control algo-7927   rithm can generate a Ô¨Çuent and complete sentence   of the desired length by dynamic programming .   Compared with Schumann et al . ( 2020 ) , our NAUS   ( length control ) generates a more informative sum-   mary that includes the main clause ( united nations   condemned ) , which also appears in the reference   summary .   E Human Evaluation   We conducted human evaluation with a focus on   truncating and length - control decodings . This is   because truncating may generate incomplete sen-   tences , which can not be adequately evaluated by   automatic metrics as their ROUGE scores are close .   SpeciÔ¨Åcally , we invited three human annotators   to compare the two decoding algorithms for NAUS   on 50 randomly selected samples , in the setting of   Group B , Table 1 ( Gigaword headline generation   with a target length of 10 ) . The annotation was   conducted in a pairwise manner in terms of overall   quality and Ô¨Çuency / completeness ; average results   ( wins / loses / ties ) are shown in Table 4 . It should be   mentioned that our annotation was strictly blind :   the samples of two systems were presented in ran-   dom order and annotators did not know which sys-   tem generated a sample .   As seen , our length - control decoding algo-   rithm largely outperforms the truncating approach   in terms of both the overall quality and Ô¨Çu-   ency / completeness . The results are statistically   signiÔ¨Åcant ( p - values<0:01 ) in a one - sided bino-   mial test . This veriÔ¨Åes that length - control decoding   is important for summarization , as truncating yields   incomplete sentences , which are inadequately re-   Ô¨Çected by ROUGE scores .   F Length - Transfer Summary Generation   In the main paper , we present results where our   NAUS is trained on search outputs ( Schumann   et al . , 2020 ) that have the same length as the infer-   ence target . This follows the common assumption   in machine learning that training and test samples   are independently identically distributed .   In this appendix , we show the performance of   length - transfer summary generation , where the pre-   diction has a different length from that of training .   We denote such a model by NAUS , referring to   training with iwords and testing for jwords .   As seen in Groups A & B in Table 7 , NAUS   with length transfer is slightly worse than NAUS   trained on the correct length , which is understand-   able . Nevertheless , length - transfer decoding still   outperforms the search teacher and other baselines .   Moreover , we consider the third setting in Schu-   mann et al . ( 2020 ) , where the target length is 50 %   of the input . Since it takes time to obtain pseudo-   groundtruths given by the edit - based search , we   would directly transfer already trained NAUS mod-   els to this setting by our length - control decoding .   Results are shown in Group C , Table 7 . We ob-   serve NASUis better than NASU ,   which makes much sense because the latter has   a larger gap during transfer . Remarkably , both   NASUand NASUoutperform Schu-   mann et al . ( 2020 ) and other baselines , achieving   new state - of - the - art unsupervised performance on   this setting as well .   We further compare with Su et al . ( 2021 ) , who   use a length penalty to encourage short summaries .   However , their length control works in the statisti-   cal sense but may fail for individual samples . More-   over , such a soft length penalty can not generate   longer summaries than trained . Even in the setting   of10!8 , their generates summaries are slightly   longer than required , while the performance de-   grades much more considerably than NAUS .   These results show that our novel length - control   decoding algorithm is not only effective when gen-   erating summaries of similar length to the train-   ing targets , but also generalizes well to different7928   desired summary lengths without re - training . In   general , our NAUS is an effective and efÔ¨Åcient un-   supervised summarization system with the ability   of explicit length control.7929