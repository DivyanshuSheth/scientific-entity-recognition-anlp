  Tiberiu SoseaHongli ZhanJunyi Jessy LiCornelia CarageaDepartment of Computer Science , University of Illinois ChicagoDepartment of Linguistics , The University of Texas at Austin   { tsosea2,cornelia}@uic.edu { honglizhan,jessy}@utexas.edu   Abstract   Understanding what leads to emotions during   large - scale crises is important as it can provide   groundings for expressed emotions and sub-   sequently improve the understanding of ongo-   ing disasters . Recent approaches ( Zhan et al . ,   2022 ) trained supervised models to both de-   tect emotions and explain emotion triggers   ( events and appraisals ) via abstractive summa-   rization . However , obtaining timely and quali-   tative abstractive summaries is expensive and   extremely time - consuming , requiring highly-   trained expert annotators . In time - sensitive ,   high - stake contexts , this can block necessary   responses . We instead pursue unsupervised sys-   tems that extract triggers from text . First , we   introduce C ET - EXT , augmenting ( Zhan   et al . , 2022 ) ’s abstractive dataset ( in the con-   text of the COVID-19 crisis ) with extractive   triggers . Second , we develop new unsuper-   vised learning models that can jointly detect   emotions and summarize their triggers . Our   best approach , entitled Emotion - Aware Pager-   ank , incorporates emotion information from   external sources combined with a language un-   derstanding module , and outperforms strong   baselines . We release our data and code at .   1 Introduction   Language plays a central role in social , clinical ,   and cognitive psychology ( Pennebaker et al . , 2003 ) ,   and social media presents a gold mine for such   analysis : people turn to social media to share ex-   periences around challenges in their personal lives   and seek diagnosis , treatment , and emotional sup-   port for their conditions ( Choudhury and De , 2014 ;   Gjurkovi ´ c and Šnajder , 2018 ) . During crises , such   as natural disasters or global pandemics , large - scale   analysis of language on social media — both how   people feel andwhat ’s going on in their lives to   lead to these feelings — can have a profound im-   pact on improving mental health solutions as wellFigure 1 : An example post from C ET - EXT anno-   tated with emotion triggers . The highlighted sentences   represent triggers of the tagged emotions .   as helping policymakers take better - informed deci-   sions during a crisis .   Recent work ( Zhan et al . , 2022 ) taps into this   broad challenge by jointly detecting emotions and   generating a natural language description about   what triggers them ( triggers include both objec-   tive events and subjective appraisals of those   events ( Ellsworth and Scherer , 2003 ; Moors et al . ,   2013 ) ) . Trigger explanation is formulated as a su-   pervised , abstractive summarization task that is   emotion - specific . Unlike generic summarization   however , due to the high cognitive load to pro-   vide judgments for each emotion , obtaining human-   written summaries for this task is time - consuming   and requires significant annotator training . This   results in small , domain - specific datasets that are   difficult to scale — especially in the face of new   crisis events where the timing of such analysis is   often pivotal .   This work instead takes a fully unsupervised   approach such that we do not rely on any labeled   data , thus becoming agnostic to distributional shifts   in domain or types of crisis , and robust for time-9550critical events . We posit that emotion triggers can   be summarized effectively in an extractive manner   where unsupervised methods are well - suited ; we   thus tackle the challenge of simultaneous emotion   prediction and trigger extraction .   For this new task , we first introduce C ET-   EXT , augmenting Zhan et al . ( 2022 ) ’s C ET   with manually annotated extractive summaries cor-   responding to each of their abstractive summaries .   The result is a dataset of 1,883Reddit posts about   the COVID-19 pandemic , manually annotated with   7fine - grained emotions ( from C ET ) and their   corresponding extractive triggers ( Figure 1 ) . For   every emotion present in a post , our annotators   highlight sentences that summarize the emotion   triggers , resulting in 6,741extractive summaries   in total . Qualitative analyses of the dataset indicate   good agreement among the annotators , and follow-   up human validations of the annotations also re-   veal high correctness . C ET - EXT provides   an ideal test bed to facilitate the development of   extractive ( supervised or unsupervised ) techniques   for the tasks of emotion detection and trigger sum-   marization in crisis contexts .   We propose Emotion - Aware PageRank ( EAP ) ,   a novel , fully unsupervised , graph - based approach   for extractive emotion trigger summarization from   text . The core of our method is to decompose   the traditional PageRank ( Page et al . , 1999 )   ranking algorithm into multiple biased PageRanks   ( Haveliwala , 2003 ) , one for each emotion . To bias   our model towards various emotions , our approach   harnesses lexical information from emotion lex-   icons ( Mohammad and Turney , 2013 ; Mohammad ,   2018 ) . Critically , unlike previous graph - based un-   supervised approaches ( Mihalcea and Tarau , 2004 ;   Liu et al . , 2010 ; Gollapalli and Caragea , 2014 ; Flo-   rescu and Caragea , 2017 ; Patel and Caragea , 2021 ;   Singh et al . , 2019 ) , which represent the text as a   bag - of - words or word embeddings , EAP incorpo-   rates a language understanding module leveraging   large language models to ensure that the summaries   for an emotion are coherent in the context of that   emotion . Results on our C ET - EXT indicate   the effectiveness of our EAP , which significantly   pushes the Rouge - L score of our summaries by an   average of 2.7%over strong baselines .   Our contributions are as follows : 1)We in-   troduce C ET - EXT , a manually annotated   benchmark dataset for the task of emotion detec-   tion and trigger summarization . 2)We proposeEmotion - Aware PageRank , a variation of PageR-   ank that combines a language understanding mod-   ule and external emotion knowledge to generate   emotion - specific extractive summaries . 3)We carry   out a comprehensive set of experiments using nu-   merous baselines to evaluate the performance on   C ET - EXT and show that our proposed EAP   significantly outperforms strong baselines .   2 Background and Related Work   Emotion Tasks . Most of the prior work on emo-   tions on social media focuses solely on detecting   emotions or emotional support from text ( Wang   et al . , 2012 ; Biyani et al . , 2014 ; Abdul - Mageed and   Ungar , 2017 ; Khanpour et al . , 2018 ; Khanpour and   Caragea , 2018 ; Demszky et al . , 2020 ; Desai et al . ,   2020 ; Sosea and Caragea , 2020 ; Adikari et al . ,   2021 ; Calbi et al . , 2021 ; Kabir and Madria , 2021 ;   Beck et al . , 2021 ; Mohammed Abdulla et al . , 2019 ;   Sosea and Caragea , 2021 ; Hosseini and Caragea ,   2021a , b ; Saakyan et al . , 2021 ; Ils et al . , 2021 ; Sosea   et al . , 2022 ; Sosea and Caragea , 2022a , b ) . Our task   is directly related to emotion cause extraction ( Gao   et al . , 2015 ; Gui et al . , 2016 ; Gao et al . , 2017 )   which focused on identifying phrase - level causes   from Chinese news or micro - blogs , which are dis-   tinct from the spontaneous writing on social media .   In our context , similar to the work of Zhan et al .   ( 2022 ) , what triggers an emotion includes both   what happened and how the writer appraised the   situation . A major difference of our work from   Zhan et al . ( 2022 ) is that we consider extractive   summaries instead of abstractive and take a fully   unsupervised perspective , eliminating the reliance   on labeled data . For a comprehensive overview of   C ETintroduced by Zhan et al . ( 2022 ) , refer   to Appendix § A.   Unsupervised Extractive Summarization . Ex-   tractive summarization aims to condense a piece of   text by identifying and extracting a small number   of important sentences ( Allahyari et al . , 2017 ; Liu   and Lapata , 2019 ; El - Kassas et al . , 2021 ) that pre-   serve the text ’s original meaning . The most popular   approaches in unsupervised extractive summariza-   tion leverage graph - based approaches to compute a   sentence ’s salience for inclusion in a summary ( Mi-   halcea and Tarau , 2004 ; Zheng and Lapata , 2019 ) .   These methods represent sentences in a document   as nodes in an undirected graph whose edges are   weighted using sentence similarity . The sentences   in the graph are scored and ranked using node cen-9551trality , computed recursively using PageRank ( Page   et al . , 1999 ) . In contrast , our EAP considers words   instead of sentences as nodes in the graph and em-   ploys multiple separate biased PageRanks ( Haveli-   wala , 2003 ) to compute an emotion - specific score   for each word , which is combined with a sentence-   similarity module to produce one sentence score   per emotion , indicating the salience of the sen-   tences under each emotion .   3 Dataset Construction   Since there is no annotated data for extractive   emotion triggers summarization in crisis contexts ,   we first bridge this gap by extending C ET ,   Zhan et al . ( 2022 ) ’s abstractive - only dataset with   extractive trigger summaries . Doing so ( a)cre-   ates benchmark data for extractive systems ; ( b )   allows in - depth analyses to understand how and   when emotion triggers are expressed on social me-   dia . This will also create a parallel abstractive-   extractive dataset for future research . We name our   new dataset C ET - EXT ( C ET{extrac-   tive , extension } ) .   Annotating Emotion Triggers . Given a post   from C ETannotated with an emotion e , we   ask annotators to highlight sentences in the post   that best describe the trigger for e. An overview   of our annotation scheme can be viewed in Ap-   pendix § B. We recruit both undergraduate stu-   dents ( in a Linguistics department ) as well as pre-   qualified crowd workers ( from the Amazon Me-   chanical Turk ) for this task . Each post is anno-   tated by two annotators . We monitor the annotation   quality and work with the annotators during the full   process . Similar to C ET , the test set is anno-   tated by undergraduate students .   Benchmark Dataset . We follow the benchmark   setup in Zhan et al . ( 2022 ) with 1,200examples   for training , 285examples for validation , and 398   examples for testing . If two annotators highlight   different sentences as triggers for the same emotion ,   we consider both sets of sentences as the gold sum-   maries and evaluate them using multi - reference   ROUGE . We anonymize C ET - EXT . Note   that since we explore unsupervised methods , the   training set is notused in our summarization mod-   els . Nevertheless , we emphasize that while the fo-   cus of this work is the unsupervised setup , we hope   thatC ET - EXT can spur further research into   both supervised and unsupervised methods , hence   we maintain the splits in Zhan et al . ( 2022 ) . For   completeness , we carry out experiments in a fully   supervised setup in Appendix § F.   Human Validation . We validate the annotated   extractive summaries of emotion triggers in   C ET - EXT through inspections from third-   party validators on the Amazon Mechanical Turk   crowdsourcing platform . A subset of our training   data including 300randomly selected examples   which contain annotations of extractive summaries   of emotion triggers are validated . Given an anno-   tated extractive trigger summary , we first ask the   validators whether the summary leans towards the   annotated emotion . It yes , we ask the validator to   further point out if the trigger — rather than the   emotion itself — is present in the summary . The   percentage of examples that validators confirm for   the two steps is shown in Table 1 . Overall , the hu-   man validation results showcase moderately high   correctness in the annotations of C ET - EXT ,   considering the subjective nature of our task .   Inter - Annotator Agreement . We measure the   inter - annotator agreement between two extractive   trigger summaries for the same emotion in a post ,   as shown in Table 2 . Results show that , within the   examples where we find emotion overlaps , 29.9 %   of the extractive summaries of triggers for the   same emotion share completely identical annota-   tions from both annotators , and 25.6 % have partial   sentence - level overlaps . In total , we find overlaps9552in55.5 % of the summaries , and the experts who   were responsible for the test set ( 65.8 % ) have more   overlapping summaries than the crowd workers   who were responsible for the training and valida-   tion sets ( 52.3 % ) . Furthermore , the average Fleiss ’   kappa ( Fleiss , 1971 ; Randolph , 2005 ) is 0.89across   all the emotions in C ET - EXT . This suggests   substantial agreement among our annotators .   In addition , we also employ automatic metrics   including self - BLEU ( with smoothing methods 1 )   and self - ROUGE to capture the overlap between   annotators ’ summaries . To establish a baseline , we   report these metrics between the annotators ’ work   and a randomly selected sentence from the original   post . We repeat this process five times . Results   reveal that both the self - BLEU and self - ROUGE   of our annotations significantly outperform that of   the random baseline ( as shown in Table 2 ) . We   also observed higher values of these measures for   student annotators compared with crowd workers .   ( c.f . Appendix § D ) . These results indicate strong   accordance among our annotators .   Dataset Statistics . Here we elaborate on the   overview of C ET - EXT . On average , there are   1.35sentences ( std.dev = 0.79 ) consisting of 32.54   tokens ( std.dev = 20.68 ) per extractive summary   of emotion trigger in C ET - EXT . As shown   in Figure 2 , when broken down into unique trig-   gersentences , fear has the most trigger sentences   in the dataset , closely followed by anticipation .   On the other hand , trust has the lowest number   of trigger sentences . This can be attributed to the   calamitous nature of the domain of our dataset . Be-   sides , unlike generic news summarization ( Fabbri   et al . , 2021 ) , the emotion - trigger extractive summa-   rization task is notlead - based . This is manifested   through our scrutiny of the position of emotion trig-   ger sentences in the original posts ( Figure 6 and   Figure 7 , Appendix § E ) , where a large number of   triggers cluster in the later parts of the post .   Additional analyses of C ET - EXT can be   found in Appendix § E.   Emotion Explicitness . To examine the explic-   itness of emotions in the extractive summaries   of emotion triggers , we apply EmoLex ( Moham-   mad and Turney , 2013 ) , an English lexicon for the   Plutchik-8 primary emotions . Specifically , for the   extractive summaries of triggers to a certain emo-   tione , we measure the average ratio of e ’s words   in EmoLex being present in the sentence - level lem-   matized summaries . The results are presented in   Figure 2 . Interestingly , we notice that sadness is   the most explicit emotion in the annotated extrac-   tive summaries of triggers in our dataset , while   anger is the most implicit one .   4 Unsupervised Extractive   Summarization   In this section we introduce Emotion - Aware Pager-   ank ( EAP ) , our fully unsupervised , graph - based ,   emotion trigger extractive summarization method   that incorporates information from emotion lexi-   cons to calculate a biased PageRank score of each   sentence in a post . EAP then fuses this score with   an additional similarity - based sentence - level score   that ensures the summary for a specific emotion e   does not diverge in meaning from other summaries   of the same emotion e. We show an overview of   our model architecture in Figure 3 .   Task Formulation . LetPbe a Reddit post . Pis   composed of an ordered sequence of nsentences :   P={s , s , ... , s } . Generic extractive summa-   rization aims to output an ordered set of sentences   SwithS⊂Pthat captures the essence of post   P. In our emotion trigger summarization , how-   ever , we aim to generate multiple extractive sum-   maries conditioned on the expressed emotions . To   this end , we are interested in a set of summaries   S={S , S , ... , S}where mis the total   number of emotions present in PandSis the   summary of the triggers that lead to the expression   of emotion ewithS⊂P. Note that Pusually   conveys a subset of emotions , in which case the   summaries for the emotions that are not present in   text are empty.9553   Graph Construction . We build an undirected   graph G= ( V , E ) , where Vis vocabulary set of   words . To build Vwe employ various process-   ing and filtering techniques . First , we only select   nouns , adjectives , verbs , adverbs and pronouns and   remove any punctuation . Next , we stem all the   selected words to collapse them in a common base   form . Finally , we remove infrequent words which   appear less than 20times in the entire training set .   The remaining words form the vocabulary V. A   pair of words ( w , w)∈Edefines an edge be-   tween wandwand the operator β(w , w)de-   notes the weight of edge ( w , w ) . We compute   the weight of an edge in our graph using word   co - occurences in windows of text . Given a win-   dow size of ws , we say that two words wand   wco - occur together if the number of words be-   tween them in text is less than ws . We build a   co - occurence matrix Cof size |V| × |V|from the   documents in our training set where Cis the num-   ber of times words wandwco - occur together .   Using Cwe simply define the weight of an edge   as :   β(w , w ) = 2×C(C+C)(1 )   Intuitively , the more frequently two words co - occur   together , the higher the weight of the edge between   them becomes .   Emotion Decomposition . In PageRank , the im-   portance or relevance R(w)of an arbitrary word   wis computed in an iterative fashion using the   following formula : R(w ) = λβ(w , w)R(w ) + ( 1 −λ)1   |V|   ( 2 )   where |.|is the set size operator and λis the damp-   ing factor , a fixed value from 0to1which measures   the probability of performing a random jump to any   other vertex in the graph . The idea of PageRank   is that a vertex or word is important if other im-   portant vertices point to it . The constant term   is called a random jump probability and can be   viewed as a node preference value , which in this   case assigns equal weights to all the words in the   graph , indicating no preference .   In this current formulation , the PageRank model   calculates the weights of words irrespective of the   expressed emotion . We claim that for our purpose   words should bear different importance scores in   different emotion contexts . For example , the word   agony should have a higher importance in the con-   text of sadness orfear than in the context of joy .   To this end , we propose to decompose the text   into multiple components , one for each emotion ,   where the relevance of a word differs from compo-   nent to component . Biased PageRank ( Haveliwala ,   2003 ) is a variation of PageRank where the sec-   ond term in Equation 2 is set to be non - uniform ,   which can influence the algorithm to prefer par-   ticular words over others . We propose to run a   separate biased PageRank for each emotion and   leverage a custom importance function i(w)that   yields high values for words that are correlated   with an emotion eand low values otherwise . For-9554mally , the relevance computation for the PageRank   corresponding to emotion ebecomes :   R(w ) = λβ(w , w)R(w)+(1−λ)i(w )   N   ( 3 )   where Nis a normalization factor such that= 1 . Since the model prefers those   vertices with higher random jump probabilies , us-   ing an accurate importance function i(w)for emo-   tionecan lead to accurate relevance scores in the   context of e. We define this function using the   NRC emotion intensity ( Mohammad , 2018 ) lex-   icon . EmoIntensity associates words with their   expressed emotions and also indicates the degree   of correlation between a word and a particular emo-   tion using real values from 0to1 . For example ,   outraged has an intensity for anger of 0.964while   irritation has an intensity of 0.438 . In our context ,   assigning importance values using intensity is ap-   propriate since a sentence containing high intensity   words for an emotion eis more likely to be rel-   evant in the context of ecompared to a sentence   containing lower intensity words . Denoting the set   of words in EmoIntensity correlated with emotion   ebyI , all words w∈ Ialso come with intensity   value annotations denoted by int(w ) . Therefore ,   we define the importance function as :   i(w ) =   int(w)if w ∈ I   c if w ∈V\ I(4 )   where cis a constant that we find using the valida-   tion set . Since our summaries are at the sentence   level , we simply score a sentence sas the average   relevance of its words :   R(s ) = R(w )   |s|(5 )   Encoding the meaning . A major drawback of   prior graph - based approaches is that they exclu-   sively represent the input as a bag - of - words , ignor-   ing the structure of text . We propose to solve this   drawback by introducing a language model - based   component to encode the meaning of a sentence .   Our component is based on the assumption that a   sentence sthat is highly relevant for an emotion e   should be similar in meaning to other sentences s   relevant to e. We capture this property by scoring   each sentence based on its similarity with other   important ( i.e. , in the context of e ) sentences . Weleverage the popular Sentence - BERT ( Reimers and   Gurevych , 2019 ) model , which produces mean-   ingful sentence embeddings that can be used in   operations such as cosine similarity . Given a sen-   tence s , let sbe its embedding and sim(s , s )   be the cosine similarity between the embeddings   of sentences sands . Denoting by Tthe set of   sentences in the entire dataset , we score sin the   context of emotion eas follows :   M(s ) = sim(s , s)∗ R(s )   |T |(6 )   Intuitively , M(s)yields high values if sis similar   in meaning to sentences relevant in the context of   emotion e.   Constructing the Summaries . Given a post P=   { s , s , ... , s } , we first combine the meaning and   the relevance scores into a final , sentence level ,   per - emotion score , which we use to score every   sentence sinPalong all the emotions :   F(s ) = R(s)∗M(s ) ( 7 )   We use this per - emotion score to rank the sentences   in the post P. For an emotion e , we only select   the sentences swhere F(s ) > t to be part of   the final summary for e.tis a threshold value that   we infer using our validation set . Note that given   P , we compute the score Ffor every emotion e.   In the case that none of the sentences in Pexceed   the threshold for a particular emotion , we consider   that the emotion is not present in the post ( i.e. , we   do not generate a summary ) .   5 Experiments and Results   In this section , we first introduce our emotion-   agnostic and emotion - specific baselines . Next , we   present our experimental setup and discuss the re-   sults obtained by EAP against the baselines .   Emotion - agnostic baselines . We explore two   standard heuristic baselines , namely 1)Extracting   the first sentence in the post ( 1sent ) and 2)Ex-   tracting the first three sentences in the post ( 3sent ) .   Next , we design three graph centrality measure-   based methods : 3)PacSum ( Zheng and Lapata ,   2019 ) , 4)PreSum ( Liu and Lapata , 2019 ) and word-   level 5)TextRank ( Mihalcea and Tarau , 2004 ) .   Note that these methods are emotion - oblivious and   the generated summary will be identical for differ-   ent emotions.9555   Emotion - specific baselines . We first employ two   lexical - based methods : 6)EmoLex - we use the   EmoLex ( Mohammad and Turney , 2013 ) lexicon   to identify lexical cues that indicate the expression   of emotions . If a sentence contains a word that   is associated with an emotion e , we consider the   sentence to express e. The final summary for econ-   tains all sentences expressing e.7)EmoIntensity   - we leverage the NRC Affect Intensity Lexicon   ( Mohammad , 2018 ) to build a more fine - grained   approach of identifying if a sentence expresses an   emotion or not . For each sentence and emotion ,   we calculate the average emotion word intensity   and compare it to a pre - defined threshold t. If the   average intensity for eis higher than twe label the   sentence with e.tis a tunable parameter that we   select based on our validation set performance .   Finally , we leverage models trained on emotion   detection datasets to build our emotion - specific   summaries . For a post P , we use our model to   make predictions on each sentence in Pand build   summaries by concatenating sentences that express   the same emotions . We mainly experiment with   a model trained on the 8)GoEmotions ( Demszky   et al . , 2020 ) dataset .   Experimental Setup . We carry out our experi-   ments on an Nvidia A 5000 GPU . We use the Hug-   gingFace Transformers ( Wolf et al . , 2019 ) library   for our Sentence - BERT implementation and wewill make the code for our methods and data avail-   able for reasearch purposes . We report the per-   formance in terms of Rouge-2 and Rouge - L ( Lin ,   2004 ) to evaluate the summarization performance .   Additionally , we also calculate the performance in   terms of F1 and show the results in Appendix I.   We provide extensive details about the hyperparam-   eters used in EAP and the baselines , such as our   various thresholds and constants in Appendix § G.   Results . We show the results obtained in Table 3 .   First , we note that emotion - specific approaches out-   perform the emotion - oblivious methods consider-   ably . Notably , EmoIntensity outperforms PacSum   by an average of 1.1%in Rouge-2 . Among the   emotion - specific baselines , EmoIntensity , which   uses the intensity of emotion words to extract rel-   evant sentences for a particular emotion obtains   good performance , outperforming the EmoLex   method by 5.1%Rouge-2 on disgust and 3.3%on   fear . This result emphasizes that having a degree   of association between a word and an emotion ( i.e. ,   the intensity ) is a stronger signal than the plain   word - emotion association in our emotion - based ex-   tractive summarization context .   EAP consistently yields the highest results   both in terms of Rouge-2 and Rouge - L compared   to the other approaches . Concretely , we obtain   an average improvement of 2.7%in Rouge - L and   2.5%in Rouge-2 score over our strongest EmoIn-   tensity baseline . For example , on anger and joy we   see improvements in Rouge-2 of 1.7%and6.3 %   respectively . Moreover , our emotion - aware PageR-   ank considerably outperforms TextRank ( Mihalcea   and Tarau , 2004 ) by as much as 5.5%Rouge - L and   4.5%Rouge-2 on average .   Emotion Detection . While EAP shows strong   results in our emotion trigger summarization ex-   periments , we want to evaluate our approach in   a traditional emotion detection task . To this end,9556   we ask how well EAP can detect emotions at the   post level . Given a post P , we label the post with   emotion eif we identify any sentence s∈Pas   a summary for e. If no sentence is selected to be   included in the summary , we consider that EAP   does not predict e.   We show the results obtained in Table 4 , where   we compare EAP to lexical methods ( EmoLex and   EmoIntensity ) and a domain adaptation method ,   which trains a BERT ( Devlin et al . , 2019 ) model on   the GoEmotions dataset ( Demszky et al . , 2020 ) .   We observe that EAP consistently outperforms   prior work on all the emotions by an average of   0.9%in F1 score . Notably , we see 1.5%improve-   ments in F1 on fear and 1.9%on anticipation .   Ablation Study . We perform a thorough ablation   study to tease apart and analyze the components   lead to the success of EAP . First , we analyze the   influence of emotion intensity on the performance   of the model . Here , we slightly modify the impor-   tance function from Equation 4 to a constant value .   Instead of using the variable int(w)we use a con-   stant value cwhere c > c. Intuitively , we still   bias the model towards a particular emotion e , how-   ever , every word associated with eweighs equal in   this ablated version of EAP . We denote this modifi - cation of the algorithm by -int . Second , we remove   themeaning score Mfrom our algorithm and use   only the word - based relevance R. This approach   is denoted by -sim . We also analyze the behaviour   of EAP when removing both components .   We show the results obtained in Table 5 . Re-   moving emotion intensity leads to a performance   degradation of 1%in Rouge - L while the lack of   our similarity module decreases the performance by   1.2%in Rouge - L. Removing both further decreases   the performance by 2.9%in Rouge-2 . These re-   sults emphasize that both similarity and intensity   are core components of EAP and both consistently   contribute to its success .   Anecdotal Evidence . To offer additional insights   into our EAP , we provide anecdotal evidence in Fig-   ure 4 , where we show a post expressing both joy   and fear . We indicate for each word both its rele-   vance for joy and for fear . Additionally , we show   the meaning score for each sentence and emotion .   Interestingly , we observe that the scores produced   by our model are very relevant . For instance , pro-   tection has a very large value for joy of 0.531and a   very small value of 0.076forfear . Along the same   lines , worried has a relevance of 0.523forfearand   0.074for joy . The similarity scores are also accu-9557rate . For example , glad I am fully vaccinated has a   score for joy of 0.463,9times as large of the score   of the same sentence for fear . We show additional   analysis on the effect of the most relevant terms on   EAP performance in Appendix § H.   6 Conclusion   We introduce C ET - EXT , a new benchmark   dataset composed of 1,883Reddit posts annotated   for the task emotion detection and extractive trigger   summarization in the context of the COVID-19   pandemic . Our proposed Emotion - Aware Pagerank   approach yields strong results on our datasets ,   consistently outperforming prior work in an   unsupervised learning context . In the future , we   plan to study abstractive trigger summarization   from an unsupervised point of view to bridge   the gap between the extractive and abstractive   summarization performance .   Limitations   Since our EAP builds its graph representation from   social media data , our method may carry inductive   biases rooted in this type of data . Moreover , note   that the scope of our study is limited to English   social media posts and our approach does not con-   sider inputs larger than 512tokens . Therefore using   our approach in long document summarization may   be challenging . Finally , the general applicability   of EAP in a different domain is highly dependent   on the existence of high - quality lexicons for the   domain in question , which may not be available .   Acknowledgements   This research was partially supported by National   Science Foundation ( NSF ) grants IIS-1912887 ,   IIS-2107487 , ITE-2137846 , IIS-2145479 , IIS-   2107524 , IIS-2107487 . We thank Jamie Pen-   nebaker for useful discussions and comments . We   also thank our reviewers for their insightful feed-   back and comments .   References9558955995609561A C ET   Zhan et al . ( 2022 ) was the first to introduce the com-   bined labeling of both emotions and ( abstractive )   summaries of their triggers on the domain of spon-   taneous speech ( i.e. , Reddit posts ) . They presented   C ET , a corpus of 1,883Reddit posts man-   ually annotated with 7emotions ( namely anger ,   anticipation , joy , trust , fear , sadness , and disgust )   as well as abstractive summaries of the emotion   triggers described in the post . The posts are cu-   rated from r / COVID19_support , a sub - Reddit for   people seeking community support during COVID-   19 . To ensure the diversity of the data distribution ,   C ETconsists of Reddit posts from two dif-   ferent timelines ( before and during the Omicron   variant ) . The posts in C ETare lengthy and   emotionally rich , with an average of 156.4tokens   and2.46emotions per post . C ETserves as an   ideal dataset to spur further research on capturing   triggers of emotions in long social media posts .   Nevertheless , the combined labeling of emotions   and free - form abstractive summarization of their   triggers is difficult and time - consuming as it re-   quires annotators to comprehend the document in   depth . This fails to meet the time - sensitivity re-   quirement in the face of major crises like COVID-   19 . Our work instead proposes to generate an   extractive summarization of emotion triggers and   studies the task of emotion detection and trigger   summarization from an unsupervised learning per-   spective , which is robust to domain variations and   beneficial in boosting understanding in time - critical   periods .   B Annotation Scheme of C ET - EXT   The process of collecting annotations for   C ET - EXT is shown in Figure 5 . Given a   post and its annotations containing emotion e   from C ET , we ask annotators to highlight   sentences in the post that best describe the trigger   for emotion e. Rather than selecting text that   expresses the emotion itself , we specifically   instruct annotators to extract the events and how   people make sense of the events that lead to   the expression of the emotion . We use detailed   examples provided by Zhan et al . ( 2022 ) to help   our annotators better interpret the definition of   emotion triggers .   C Crowd Workers   Both groups of annotators for C ET - EXT   come from the United States . The crowd workers   are recruited from the Amazon Mechanical Turk   crowdsourcing platform , with restrictions that their   locale is the US and that they have completed 500 +   HITs with an acceptance rate of at least 95 % . The   undergraduate students are hired from a university   in the United States .   D Inter - annotator Agreement Among   Undergraduate Students and Crowd   Workers   As shown in Table 6 , the inter - annotator perfor-   mance of the undergraduate students consistently   exceeds the crowd workers .   E Additional Analyses of C ET - EXT   Trigger Positions . We examine the position of   the emotion trigger sentences in the original posts .   The sentence - level distribution of the annotated   triggers is reported in Figure 6 . Results reveal   that the trigger sentences spread evenly across the   posts , with a large number of triggers clustering   in the later parts of the post . This means that the   emotion - trigger extractive summarization task is   notlead - based , unlike generic news summariza-   tion ( Fabbri et al . , 2021 ; Sebastian et al . , 2019 ) .   This is especially true for anticipation , as demon-   strated in Figure 7.9562   Trigger Components . In addition to the explic-   itness of emotion triggers , we also examine the   syntactic components of the extractive summaries   of emotion triggers . Results are shown in Figure   8 . We observe that nouns and verbs take up the   majority of triggers , closely followed by the use of   pronouns .   Pronoun Distributions . Psycho - linguistic stud-   ies reveal that the analysis of function words such   as pronouns can disclose psychological effects of   life experiences and social processes ( Campbell   and Pennebaker , 2003 ; Tausczik and Pennebaker ,   2010 ; Pennebaker et al . , 2014 ; Seraj et al . , 2021 ;   Singh et al . , 2018 ) . Specifically , overusing the first-   person singular pronouns may imply a high level   of self - involvement , whereas the increased use of   other pronouns may signify improvement of social   engagement ( Cohn et al . , 2004 ; Simmons et al . ,   2008 ; Kumari and Singh , 2017 ) .   We evaluate the percentage of personal pronoun   usage per annotated emotion trigger sentence . In   particular , we discover an inverse correlation be-   tween first - person singular pronouns ( e.g. , I , me ,   my , mine , myself ) and second - person pronouns   ( e.g. , you , your , yours , yourself , yourselves ) . We   provide the average percentage of the personal pro-   nouns per emotion trigger in Figure 9 . Further   statistical tests reveal negative Pearson correlations   between the percentage distribution of first - person   singular pronouns and second - person pronouns in   each emotion ( with substantial significance in all   7emotions ; shown in Table 7 ) . We note that when   expressing negative emotions such as sadness and   fear , authors used more first - person singular pro-   nouns in triggers . On the other hand , authors used   more second - person pronouns in expressing the   triggers for positive emotions like joyandtrust .   The inverse correlation between first - person singu-   lar pronouns and second - person pronouns suggests   more self - involvement in negative emotions and   more social engagement in positive emotions in   C ET - EXT .   Topical Variations . To better interpret the an-   notated emotion triggers , we train a multi - class   bag - of - words logistic regression model to predict   the emotion label of each annotated extractive emo-   tion trigger sentence . The trained model ’s weights   pertaining to each class of emotions are then ex-   tracted to locate the tokens that are most indicative   of each emotion . The multi - class logistic regres-   sion model achieved a micro F1 score of 0.33after9563   training and evaluating on our benchmark dataset .   The most indicative tokens associated with each   emotion are reported in Table 8 .   Connections to C ET.To understand the   ties between C ET - EXT andC ET , we   measure the self - BERTScore between the ex-   tractive summaries of triggers from C ET-   EXT and the abstraction summaries of triggers   from C ET . Results reveal that the average   BERTScore F1 is 0.872between the extractive and   abstractive summaries , indicating strong correla-   tions between the two datasets .   Same Triggers for Different Emotions . The sta-   tus of overlapping trigger sentences for different   emotions is shown in Figure 10 . Specifically , we   measure the percentage of sentences that are trig-   gers for an emotion ithat are also triggers for emo-   tionjin C ET - EXT .   F Supervised Extractive Summarization   Although our focus is exclusively on unsupervised   approaches to eliminate the reliance on labeled   data , we note that Covid - EXT can be a suitable   benchmark for developing supervised methods as   well . In this section , we compare two supervised   methods against our unsupervised EAP . We experi-   ment with two methods for emotion trigger extrac-   tion . 1)First , we experiment with the BART - FT-   JOINT ( Zhan et al . , 2022 ) model which is trained   to jointly predict emotions and their summary . We   train this model on the training set of Covid - EXT   in a supervised manner . Second we employ a sim-   ple2)BERT ( Devlin et al . , 2019 ) classifier that is   trained in a supervised manner to detect emotions   at sentence level . We consider as positive exam-   ples the sentences that are included in the summary,9564   and negative examples the rest of the sentences .   Note that we train 7different models , one for each   emotion .   We show the results obtained in Table 9 . We ob-   serve that BART - FT - JOINT outperforms our EAP   considerably by 1.5%in Rouge - L score . How-   ever , we see that the BERT - based approach is much   closer to the performance of the unuspervised EAP ,   outperforming it by less than 1%in Rouge - L and   F1 .   G Hyperparameters   In this section we detail the values of the hyper-   parameters used and the search space considered   in the development of our EAP . First in terms of   the constant cin Equation 4 , we experiment with   values in the range 0.1→0.5but observed that   0.1works well . We mentioned that the minimum   frequency of a word necessary for selection in our   vocabulary Vis20 . We also experimented with   other values ranging from 5to50 . The threshold t   from Equation 7 is emotion - specific and inferred   using the validation set . We experiment with values   between 0.2and0.7and observed that 0.35works   well in general .   H Model Analysis   To offer additional insights into our approach , we   show in Figure 11 an analysis on the effect of the   top relevant terms on the performance of EAP . For   each emotion , we experiment with completely drop-   ping the top kmost relevant terms ( i.e. , words ) in   the graph , with kranging from 1to40and report   the average performance obtained . This analysis   can be seen as a way to measure the reliance of   EAP and the top relevant words . We observe that   the performance drops considerably while drop-   ping the first 28terms and the starts to plateau .   I Extractive Summarization Results in   terms of F1   In Table 10 we present the performance on extrac-   tive summarization in terms of F1 . While Rouge   captures the overlap between extracted summaries   and human references at word level , F1 measures   the number of extracted sentences from the post   that are correctly part of the gold summary ( human9565references ) . Specifically , we compute F1 as if we   dealt with a traditional classification problem . For   every emotion , the sentences belonging to the trig-   ger summaries are positive examples , and all the   other sentences are negative examples . If our EAP   model selects a sentence that does not appear in the   trigger summary , we view it as a false positive . On   the other hand , if our EAP model does not extract   a sentence which belongs to the trigger summary ,   we count it as a false negative . We calculate F1 as   the harmonic mean between precision and recall.95669567ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Limitations Section   /squareA2 . Did you discuss any potential risks of your work ?   Limitations Section   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3   /squareB1 . Did you cite the creators of artifacts you used ?   Section 3   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Section 5   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Not applicable . Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 3   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3   C / squareDid you run computational experiments ?   Section 5   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Section 5 + Appendix G9568 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5 + Appendix   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Section 5 experimental setup   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Section 3   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Section 3   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   This data collection is reviewed and exempted by the IRB board of our institution   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix C9569