  Pradip Pramanick andChayan Sarkar   Robotics & Autonomous Systems   TCS Research , India   { pradip.pramanick,sarkar.chayan}@tcs.com   The usage of automatic speech recognition   ( ASR ) systems are becoming omnipresent rang-   ing from personal assistant to chatbots , home , and   industrial automation systems , etc . Modern robots   are also equipped with ASR capabilities for inter-   acting with humans as speech is the most natural   interaction modality . However , ASR in robots faces   additional challenges as compared to a personal as-   sistant . Being an embodied agent , a robot must   recognize the physical entities around it and there-   fore reliably recognize the speech containing the   description of such entities . However , current ASR   systems are often unable to do so due to limita-   tions in ASR training , such as generic datasets and   open - vocabulary modeling . Also , adverse condi-   tions during inference , such as noise , accented , and   far - field speech makes the transcription inaccurate .   In this work , we present a method to incorporate   a robot ’s visual information into an ASR system   and improve the recognition of a spoken utterance   containing a visible entity . Specifically , we propose   a new decoder biasing technique to incorporate the   visual context while ensuring the ASR output does   not degrade for incorrect context . We achieve a   59 % relative reduction in WER from an unmodi-   fied ASR system .   1 Introduction   Spoken interaction with a robot not only increases   its usability and acceptability , it provides a natural   mode of interaction even for a novice user . The   recent development of deep learning - based end - to-   end automatic speech recognition ( ASR ) systems   has achieved a very high accuracy ( Li , 2021 ) as   compared to traditional ASR systems . As a result ,   we see a huge surge of speech - based interfaces for   many systems including robots . However , the accu-   racy of any state - of - the - art ASR gets significantly   impacted based on the dialect of the speaker , dis-   tance of the speaker from the microphone , ambient   noise , etc . , particularly for novel and low - frequencyFigure 1 : A simple pipeline of speech interface for   human - robot interaction . Figure 2 : RobustSpeech Interface ( RoSI ) for embod-   ied agents with shallow fusion biasing using dynamic   biasing vocabulary .   vocabularies . These factors are often predominant   in many robotic applications . This not only results   in poor translation accuracy but also impacts the   instruction understanding and task execution capa-   bility of the robot .   Figure 1 depicts a typical scenario where an   agentuses an ASR to translate audio input to text .   Then , it detects the set of objects in its vicinity us-   ing an object detector . Finally , it matches the object   mentioned in the command and the objects detected   in the vicinity ( grounding ) to narrow down the tar-   get object before execution . If the audio translation   is erroneous , the grounding can fail , which leads to   failure in task execution . For example , in Figure 1 ,   even though the user mentioned “ pink pillow ” , the   translation was “ bink illow ” , which results in fail-   ure in task grounding .   There has been an increasing interest in con-   textual speech recognition , primarily applied to   voice - based assistants ( Williams et al . , 2018 ; Pun-   dak et al . , 2018 ; Chen et al . , 2019 ; He et al . , 2019;1946Gourav et al . , 2021 ; Le et al . , 2021 ) . However ,   incorporating visual context into a speech recog-   nizer is usually modeled as a multi - modal speech   recognition problem ( Michelsanti et al . , 2021 ) , of-   ten simplified to lip - reading ( Ghorbani et al . , 2021 ) .   Attempts to utilize visual context in robotic agents   also follow the same approach ( Onea t˘a and Cucu ,   2021 ) . Such models always require a pair of speech   and visual input , which fails to tackle cases where   the visual context is irrelevant to the speech .   In contrast , we consider the visual context as   a source of dynamic prior knowledge . Thus , we   bias the prediction of the speech recognizer to in-   clude information from the prior knowledge , pro-   vided some relevant information is found . There   are two primary approaches to introducing bias   in an ASR system , namely shallow and deep fu-   sion . Shallow fusion based approaches perform   rescoring of transcription hypotheses upon detec-   tion of biasing words during beam search ( Williams   et al . , 2018 ; Kannan et al . , 2018 ) . Class - based lan-   guage models have been proposed to utilize the   prefix context of biasing words ( Chen et al . , 2019 ;   Kang and Zhou , 2020 ) . Zhao et al . 2019 further   improved the shallow - fusion biasing model by in-   troducing sub - word biasing and prefix - based acti-   vation . Gourav et al . 2021 propose 2 - pass language   model rescoring with sub - word biasing for more   effective shallow - fusion .   Deep - fusion biasing approaches use a pre - set   biasing vocabulary to encode biasing phrases into   embeddings that are applied using an attention-   based decoder ( Pundak et al . , 2018 ) . This is fur-   ther improved by using adversarial examples ( Alon   et al . , 2019 ) , complex attention - modeling ( Chang   et al . , 2021 ; Sun et al . , 2021 ) , and prefix disam-   biguation ( Han et al . , 2021 ) . These approaches can   handle irrelevant and empty contexts but are less   scalable when applied to subword units ( Pundak   et al . , 2018 ) . Furthermore , a static biasing vocabu-   lary is unsuitable for some applications , including   the one described in this paper . Recent works pro-   pose hybrid systems , applying both shallow and   deep fusion to achieve state - of - the - art results ( Le   et al . , 2021 ) . Spelling correction models are also   included for additional accuracy gains ( Wang et al . ,   2021 ; Leng et al . , 2021 ) .   In this article , we propose a robust speech in-   terface pipeline for embodied agents , called RoSI ,   that augments existing ASR systems ( Figure 2 ) .   Using an object detector , a set of ( natural language)phrases about the objects in the scene are generated .   A biasing vocabulary is built using these generated   captions on the go or it can be pre - computed when-   ever a robot moves to a new location . Our main   contributions are twofold .   •We propose a new shallow fusion biasing algo-   rithm that also introduces a non - greedy prun-   ing strategy to allow biasing at the word level   using sub - word level information .   •We apply this biasing algorithm to develop   a speech recognition system for a robot that   uses the visual context of the robot to improve   the accuracy of the speech recognizer .   2 Background   We adopt a connectionist temporal classifica-   tion ( CTC ) ( Graves et al . , 2006 ) based model-   ing in the baseline ASR model in our experi-   ments . A CTC based ASR model outputs a se-   quence of probability distributions over the tar-   get vocabulary y={y , . . . , y}(usually char-   acters ) , given an input speech signal with length L ,   x={x , . . . , x } , L > T , thus computing ,   P(y|x ) = /productdisplayP(y|x ) . ( 1 )   The output sequence with the maximum like-   lihood is usually approximated using a beam   search ( Hannun et al . , 2014 ) . During this beam   search decoding , shallow - fusion biasing proposes   rescoring an output sequence hypothesis contain-   ing one or more biasing words ( Hall et al . , 2015 ;   Williams et al . , 2018 ; Kannan et al . , 2018 ) . As-   suming a list of biasing words / phrases is avail-   able before producing the transcription , a rescoring   function provides a new score for the matching hy-   pothesis that is either interpolated or used to boost   the log probability of the output sequence hypothe-   sis ( Williams et al . , 2018 ) ,   s(y ) = logP ( y|x)−λlogB ( y ) , ( 2 )   where B(y)provides a contextual biasing score of   the partial transcription yandλis a scaling factor .   A major limitation of this approach is ineffec-   tive biasing due to the early pruning of hypothe-   sis . To enable open - vocabulary speech recognition ,   ASR networks generally predict sub - word unit la-   bels ( e.g. , character ) instead of directly predicting   the word sequence . However , as the beam search1947keeps a fixed number of candidates in each time-   stepi∈L , lower ranked hypotheses that contain   incomplete biasing words , are pruned by the beam   search before the word boundary is reached .   To counter this , biasing at the sub - word units   ( grapheme / word - piece ) by weight pushing has been   proposed in ( Pundak et al . , 2018 ) . Biasing at the   grapheme level improves recognition accuracy than   word level biasing for speech containing bias terms ,   but the performance degrades severely for general   speech where the bias context is irrelevant . Sub-   sequently , two improvements are proposed - i ) bi-   asing at the word - piece level ( Chen et al . , 2019 ;   Zhao et al . , 2019 ; Gourav et al . , 2021 ) and ii ) utiliz-   ing known prefixes of the biasing terms to perform   contextual biasing ( Zhao et al . , 2019 ; Gourav et al . ,   2021 ) . Although word - piece biasing shows less   degradation than grapheme level on general speech ,   it performs worse than word level biasing when   using high biasing weights ( Gourav et al . , 2021 ) .   Also , prefix - context based biasing is ineffective   when a biasing term is out of context . Moreover , a   general problem with the weight - pushing based ap-   proach as compared to the sub - word level biasing   is that they require a static / class - specific biasing vo-   cabulary to work , usually compiled as a weighted   finite state transducer ( WFST ) . Also it requires   costly operations to be included with the primary   WFST - based decoder . However , for frequently   changing biasing vocabulary , e.g. , changing with   the agent ’s movement , frequent re - compiling and   merging of the WFST is inefficient .   Therefore , we propose an approach to retain the   benefits of word - level biasing for general speech ,   also preventing early pruning of partially match-   ing hypotheses using a modified beam search al-   gorithm . During beam search , our algorithm al-   locates a fixed portion of the beam width to be   influenced by sub - word level look - ahead , which   does not affect the intrinsic ranking of the other   hypotheses . This property is not guaranteed in   weight - pushing ( Chen et al . , 2019 ; Gourav et al . ,   2021 ) that directly performs subword - level bias-   ing . Moreover , we specifically target transcribing   robotic instructions that usually include descrip-   tions of everyday objects . Thus the words in the   biasing vocabulary are often present in standard   language models ( LM ) in contrast to existing bias-   ing models that focus on biasing out - of - vocabulary   ( OOV ) terms such as person names . We also uti-   lize this distinction , by incorporating an n - gramlanguage model to contextually scale the biasing   score . We describe our shallow - fusion model in   Section 4.2 .   3 System Overview   In this section , we present an overview of the em-   bodied agent that executes natural language instruc-   tions , as depicted in Figure 3 . Given a speech input ,   the agent also captures an image from its ego - view   camera . The dynamic context extraction module   extracts the visual context from the captured image   before producing the transcription of the speech   input . Firstly , a dense image captioning model   predicts several bounding boxes of interest in the   image and generates a natural language description   for each of them . Given the dense captioned im-   age , a bias target prediction model predicts a list of   biasing words to be used in speech recognition .   The list of biasing words / phrases is compiled   into a prefix tree ( trie ) that is used by the beam   search decoder to prevent the pruning of partially   matched hypotheses . The trie is dynamically cre-   ated with the agent ’s movement that captures a new   image . The acoustic model processes the speech   input to produce a sequence of probability distri-   butions over a character vocabulary . We use the   Wav2Vec2 ( Baevski et al . , 2020 ) for acoustic mod-   eling of the speech . This sequence is decoded into   the transcription using a modified beam search de-   coding algorithm . During the beam search , the   visual context that is represented using the bias-   ing trie is used to produce a transcription that is   likely to contain the word(s ) from the visual con-   text . We describe our biasing approach in detail in   Section 4.2 .   Given the transcribed instruction , the task under-   standing & planning module performs task type   classification , argument extraction , and task plan-   ning . We use conditional random field ( CRF ) based   model as proposed in our earlier works ( Pramanick   et al . , 2019 , 2020 ) . Specifically , the transcribed   instruction is passed through a task - crf model that   labels tokens in the transcription from a set of task   types . Given the output of task - crf , an argument - crf   model labels text spans in the instruction from a   set of argument labels . This results in an annotated   instruction such as ,   [ Take ] [ the pink pillow ] .   To perform a high - level task mentioned in the in-   struction ( e.g. , taking ) , the agent needs to perform   a sequence of basic actions as produced by the1948   task planner . The predicted task type is matched   with a pre - condition and post - condition template ,   encoded in PDDL ( Ghallab et al . , 1998 ) . The tem-   plate is populated by the prediction of the argument-   crf . Finally , a heuristic - search planner ( Hoffmann   and Nebel , 2001 ) generates the plan sequence .   4 Visual Context Biasing   The probability distribution sequence produced by   the acoustic model can be sub - optimally decoded in   a greedy manner , i.e. , performing an argmax com-   putation each time - step and concatenating the char-   acters to produce the final transcription . However ,   a greedy - decoding strategy is likely to introduce   errors in the transcription that can easily avoided   by using beam search ( Baevski et al . , 2020 ) . In   the following , we propose an approach to further   reduce transcription errors by exploiting the em-   bodied nature of the agent .   4.1 Dynamic Context Extraction   Upon receiving a speech input , the embodied agent   captures an ego - view image . We process the image   to identify the prior information about the envi-   ronment that could be present in the speech . To   do so , we detect all the objects in the image and   generate a textual description for each . The object   descriptions are further processed using a bias tar-   get prediction network to extract a dynamic biasing   vocabulary . As the embodied agent performs a dis-   crete action ( such as moving or rotating ) , a new   ego - view is captured , updating the visual context .   Transcribing any new speech input after the action   is executed , would be biased using the new context.4.1.1 Dense Captioning   We utilize a dense image captioning network ,   DenseCap ( Yang et al . , 2017 ) to generate rich re-   ferring expressions of the objects that includes self-   attributes such as color , material , shape , etc . , and   various relational attributes . The DenseCap model   uses a Faster R - CNN based region proposal net-   work to generate arbitrarily shaped bounding boxes   that are likely to contain objects - of - interest ( Yang   et al . , 2017 ) . The region features are produced by a   convolutional network to predict object categories .   The region features are further contextualized and   fed into a recurrent network ( LSTM ) to generate   descriptions of the proposed regions . We use a   pre - trained model for our experiments . The model   is trained on the Visual Genome dataset ( Krishna   et al . , 2017 ) containing approximately 100,000 real-   world images with region annotations , making the   model applicable to diverse scenarios .   4.1.2 Bias Target Prediction   One could simply extract the tokens from the gen-   erated captions and consider the set of unique to-   kens ( or n - grams ) as the biasing vocabulary . How-   ever , a large biasing vocabulary could result in a   performance degradation of the ASR system in   case of irrelevant context , as shown in prior exper-   iments ( Chen et al . , 2019 ; Kang and Zhou , 2020 ) .   Therefore , we propose a more efficient context ex-   traction approach , where we explicitly label the   generated captions using the bias target prediction   network . Given a caption as sequence of word   { w , . . . , x } , the bias target prediction network   predicts a sequence of labels { l , . . . , l}from the   set of symbols S={B - B , I - B , O } , which denotes that1949the word is at the beginning , inside and outside of   a biasing phrase . We model the bias target predic-   tor as a lightweight BiLSTM - based network . We   encode the given word sequence using pre - trained   GloVe embeddings ( Pennington et al . , 2014 ) ; thus   producing an embedding sequence { e , . . . , e } .   We further obtain a hidden representation hfor   each word wby concatenating the two hidden   representations produced by the forward and back-   word pass of the LSTM network . his fed to a   feed - forward layer with softmax to produce a prob-   ability distribution over S ,   h= [ L− →S TM ( e,− →h);L← −S TM ( e,← −h ) ]   l= argmaxP(l|FNN ( h ) ) .   4.2 Beam Search Adaptation   To enable word - level biasing while preventing early   pruning of grapheme - level biasing candidates , we   modify the generic beam search algorithm . More   specifically , we modify the three general steps of   beam search decoding as shown in Figure 3 . An   overview of the modified beam search decoder is   shown in Algorithm 1 . The algorithm accepts a se-   quence of pre - computed logits after applying soft-   max and returns the top - N ( N is the beam width )   transcriptions along with their log - probabilities . In   the following , we describe its primary components   in detail .   4.2.1 Sampling function   In each time step of the beam search , existing hy-   potheses ( partial transcription at time t ) are ex-   tended with subword units from the vocabulary .   Assuming the probability distribution over the vo-   cabulary at time tisA , this would generally result   in the generation of c = N× |A|candidates ,   where Nis the beam width and |A|is a constant   denoting the dimension of the vocabulary . Please   note that initially , i.e. , at t= 1 , the hypothesis set   is empty . Thus only |A|candidates are generated ,   extending from beginning - of - sequence ( < BOS > )   tokens . The sampling function selects a subset of   vocabulary to extend the hypotheses at time tas ,   S(A , C ) = { a:/summationdisplayP(A)≈C},(3 )   where Cis a hyper - parameter and P(A)is the   probability of the iindex in the probability dis-   tribution at time t. Essentially , Sstarts pruningitems from the vocabulary at time twhen the cu-   mulative probability reaches C. Although a similar   pruning strategy has been applied previously for   decreasing decoding latency Amodei et al . ( 2016 ) ;   Baevski et al . ( 2020 ) , we find that such a sampling   strategy is essential to our biasing algorithm . By   optimizing Con development set , the decoder can   be prevented from generating very low - scoring can-   didates , acting as a counter to over - biasing . This   is particularly effective in irrelevant context , i.e. ,   when none of the predicted biasing words are pro-   nounced in the speech input .   4.2.2 Contextual rescoring   After the generation of candidates , the score of   each candidate is computed in the log space ac-   cording to the CTC decoding objective ( Hannun   et al . , 2014 ) . We develop a rescoring function R   that modifies the previously computed sequence   score according to pre - set constraints . We apply R   at the word boundary in each candidate , defined in   the following ,   where Vis a word - vocabulary obtained from the   set of unigrams of a n - gram language model , B   is the dynamic biasing trie , cis the rightmost to-   ken in a candidate c , andγ , δare hyper - parameters .   The term c∈Bdenotes the rightmost token   ( word ) in candidate cis a complete path in B.S   is the log probability of the character sequence in   c , which is approximated using the CTC decod-   ing equation and interpolated using a n - gram word   level language model ( lm ) score ( Hannun et al . ,   2014 ) ,   S = log(P(c|x)P(c|c)|c| ) ,   ( 5 )   where P is the sequence probability computed   from the output of wav2vec2 , Pis the same n-   gram LM and |c|denotes the word count in candi-   datec . The parameter αis scaling factor and βis   a discounting factor to normalize the interpolation   with the sequence length .   In eq . 4 , the rescoring function modifies the base   sequence score according to the availability of cer-   tain contextual information . When the candidate   ends in a word that is not OOV , it is likely that   the language model has already provided a contex-   tual boosting score ( eq . 5 ) . Therefore we compute1950the biasing score boost by simply scaling the lm-   interpolated score using the unconditional unigram   score of c.   The rescoring function is similar to the unigram   model in ( Kang and Zhou , 2020 ) , but with two im-   portant distinctions . Firstly , we remove the class-   based language modeling and propose simply us-   ing the unigram log - probability of a word - based   language model . Thus we do not require any class-   based LM training . Secondly , we propose reducing   the score of an OOV , which is not a biasing word by   the parameter δ . This is based on the assumption   that any OOV proposed by the ASR is less likely   to be correct if it is not already in the biasing vo-   cabulary . While the ASR is still a open - vocabulary   system , i.e. , it can produce OOV words , we impose   a soft , conditional lexicon constraint on the decoder .   This is also different from previously proposed hard   lexicon constraint , applied unconditionally ( Han-   nun et al . , 2014 ) . For the third condition in eq . 4 ,   we simply boost the score of a OOV in biasing vo-   cabulary by a fixed amount , i.e. , γ , as we do not   have a contextual score for the same .   4.2.3 Bias - aware pruning   As the rescoring function for word - level biasing   is applied at the word boundary , candidates can   be pruned early without bias being applied , which   could be otherwise completed in a valid word from   the trie and rescored accordingly . We attempt to   prevent this by introducing a novel pruning strat-   egy . We define a rescoring likelihood function ψ   that scores candidates to be pruned according to the   beam width threshold . As shown in Algorithm 1 ,   we divide the candidates into a forward and a prun-   able set . The forward set represents the set of can-   didates ranked according to their sequence scores ,   after applying R.   The original beam search decoding algorithm   makes a locally optimal decision ( greedy ) at this   point to simply use the forward set as hypotheses   for the next time - step and discard the candidates   inprunable . Instead , we formulate ψ , which has   access to hypothetical , non - local information of   future time - steps from B , to take a non - greedy   decision . The rescoring likelihood ( in log space )   score calculates the probability of a candidate be-   ing rescored at a subsequent stage of the beam   search . We approximate its value by the following   interpolation ,   ψ(S ) = S+σlog / parenleftigtn   1 + nl / parenrightig   , ( 6)Algorithm 1 : Bias - aware pruning for beam   search decoding .   where tn(traversed nodes ) is the nodes traversed   so far in B , nl(nodes to leaf ) is the minimum   no of nodes to reach a leaf node in B , and σis a   scaling factor , optimized as a hyper - parameter .   Essentially , we calculate the rescoring likelihood   as a weighted factor of - i ) how soon a rescoring   decision can be made ; which is further approxi-   mated by the ratio of character nodes traversed and   minimum nodes left to complete a full word - path   inBand ii ) the candidate ’s score which approxi-   mates the joint probability of the candidate ’s char-   acter sequence , given the audio input . In a special   case when σis set to zero , ψsimply represents the   ranking by S. Thus , we compute the rescoring   likelihood for candidates in the prunable set and   sort in a descending order . Finally , we swap the   bottom- kcandidates in the forward set with top- k   candidates in the prunable set . The rescoring likeli-   hood is used only to select and prevent pruning of   a subset of the candidates , but it does n’t change the   sequence scores in any way . Thus , any rescoring   due to bias is still applied at the word - level.19515 Experiments   5.1 Data   To perform speech recognition experiments , we   have collected a total of 1050 recordings of spo-   ken instructions given to a robot . To collect the   recordings , we first collect a total of 233 image-   instruction pairs . The images are extracted from   a photo - realistic robotic simulator ( Talbot et al . ,   2020 ) and two volunteers wrote the instruction for   the robot for each given image . We recorded the   written instructions spoken by three different speak-   ers ( one female , two males ) . All the volunteers   are fluent but non - native English speakers . We   additionally recorded the instructions using two   different text - to - speech ( TTS ) models , produc-   ing speech as a natural female speaker . The male   speaker and the TTS models each produced 233   recordings of instructions and the female speaker   recorded 118 instructions . We divide the dataset   into validation and test splits . The validation set   contains 315 instruction recordings ( ≈30 % ) and   the test set contains 735 recordings .   5.2 Baselines   We compare our approach with several baselines   as described in the following . All the baselines   use a standard CTC beam search decoder imple-   mentation , with modified scoring functions ( in   bias - based models only ) as described below .   •Base - This is a pre - trained wav2vec2 model   that we use as the baseline acoustic model .   Specifically , we use the wav2vec2 - large vari-   ant ( Baevski et al . , 2020 ) , fine - tuned on Lib-   riSpeech ( Panayotov et al . , 2015 ) .   •Base + LM - We use a 3 - gram , word vocab-   ulary , language model trained on LibriSpeech   text ( Panayotov et al . , 2015 ) . The language   model probabilities are interpolated using eq . 5 .   •Base + WB - We use a Word - level Biasing ap-   proach that rescores LM - interpolated scores on   word boundaries . This is similar to the word-   level biasing described in ( Williams et al . , 2018 ) ,   but we use a fixed boost for bias .   •Base + WB- We use Word - level Bias   with contextual rescoring using unigram log-   probability from LM . This is similar to ( Kang   and Zhou , 2020 ) , but instead of class - based esti-   mation we use word - based LM probability , i.e. ,   by setting δ= 0 in eq . 4 .   5.3 Optimization   We optimize all hyper - parameters ( except for the   beam width N ) for our system and the baselines   with the same validation set . We use a Bayesian op-   timization toolkitand perform separate optimiza-   tion experiments for the baselines and our model ,   with a word error rate ( WER ) minimization ob-   jective . We use the same bounds for the common   parameters , set the same random seed for all the   models , and run each optimization experiment for   50 trials . The optimal hyper - parameter values and   the corresponding search spaces for our model are   shown in Table 1 .   5.4 Main results   We primarily use WER metricfor evaluation . We   also show a relative metric , namely word error rate   reduction ( WERR ) ( Leng et al . , 2021 ) . Addition-   ally , we use a highly pessimistic metric , namely   transcription accuracy ( TA ) , which computes an   exact match accuracy of the entire transcription .   We show the results of the experiments on the test   set in Table 2 .   Without any modification , the base ASR system   produces an WER around 21 % and TA around 54 % .   Even though predicting every word correctly is not1952equally important for task prediction and ground-   ing , still these numbers in general do not represent   the expected accuracy for a practical ASR setup in   a robot . The LM interpolation reduces the WER by   12 % , but TA is slightly decreased by 0.75 % . Using   word - level bias ( Base+WB ) results in a significant   reduction in WER ( 25 % ) and improvement in TA   ( 28 % relative ) . Compared to Base+LM , WERR in   Base+WB is 15 % and TA improvement is around   30 % relative . This again shows that even word-   level biasing can effectively improve the ASR ’s   accuracy , provided the biasing vocabulary can be   predicted correctly . The Base+WBmodel per-   forms slightly better than Base+WB , the WERR im-   proving 26 % compared to Base and 1 % compared   to Base+WB . Improvement in TA w.r.t . Base+WB   is also minimal , i.e. , 0.2 % . To analyze this , we   calculated the percentage of the biasing words that   are OOV ’s in the test set , which is found to be   9.5 % . Thus , due to lack of many OOV - biasing   terms , the fixed boost part of the rescoring model in   Base+WBwas not triggered as much and model   could not significantly discriminate between the   scores of OOV and non - OOV biasing terms .   Our beam search decoder achieves a WER of   8.48 , significantly outperforming the best base-   line , i.e. , Base+WBin both WER ( 45 % relative   WERR ) and TA ( 7.4 % relative improvement ) met-   rics . Compared to the unmodified ASR model , the   improvements are substantial , 59.3 % in WERR and   38 % in TA . We perform more ablation experiments   on our algorithm in the following Section .   5.4.1 Other ablation experiments   Sampling and pruning function We experiment   with no sampling , i.e. , setting C= 1 in the sam-   pling function S(eq . 3 ) and setting σ= 0 in the   rescoring likelihood equation ( eq . 6 ) . The results   are listed in Table 3 . The first experiment shows   that the sampling function is effective in our biasing   algorithm . Although without it , we achieve 47.29 %   WERR from the Base model , which is much higher   than other baselines . The second experiment shows   that the rescoring likelihood formulation improves   the biasing effect , 53.53 % vs. 59.28 % WERR , and   improves the TA metric by 2.5 % .   Anti - context results Biasing models make a   strong assumption of the biasing vocabulary being   relevant to the speech , i.e. , a subset of the biasing   vocabulary is pronounced in the speech . While   this is often true when giving instructions to an   embodied agent , it is important to evaluate biasing   models against adversarial examples to measure   degradation due to biasing . In the anti - context set-   ting , we deliberately remove all words from the   dynamic biasing vocabulary that are present in the   textual instruction . We show the the results of anti-   context experiments in Table 4 . We find that our   model has the least relative degradation compared   to previous results in valid contexts . Even though   all the biasing models are applied at word - level ,   prior experiments has shown that using sub - word   level biasing often results in much worse degrada-   tion ( Zhao et al . , 2019 ; Gourav et al . , 2021 ) . Also ,   the relative degradation for all variants of our model   are lower than the baselines . More importantly , the   absolute WER and TA values for our model are still   much better than the unmodified ASR and other   baselines , even in incorrect context .   5.4.2 Qualitative results   We show some examples from the dataset in Ta-   ble 5 . In the first example , the word redis a homo-   phone of read , and refrigerator has long sequence   of characters . Our model correctly transcribes both   the words by utilizing the correctly predicted visual   context . In the second example , frisbee is a chal-   lenging word for ASR , which is also transcribed   correctly using the biasing vocabulary . In the third   example , rowis incorrectly predicted as roar by   both models . As the caption generator ( Yang et al . ,   2017 ) ca n’t predict abstract concepts , e.g. , row , con-   sistently , the visual context could not be utilized   by our model . In the last example , our model cor-   rectly predicts stack but incorrectly predicts books   asbook , as the visual context suggests that book is   likely to be spoken.1953   6 Conclusion   In this article , we have presented a method to utilize   contextual information from an embodied agent ’s   visual observation in its speech interface . In partic-   ular , we have designed a novel beam search decod-   ing algorithm for efficient biasing of a speech recog-   nition model using prior visual context . Our experi-   ments show that our biasing approach improves the   performance of a speech recognition model when   applied to transcribing spoken instructions given   to a robot . We also find that our approach shows   less degradation than other approaches when the   extracted visual context is irrelevant to the speech .   Even in adversarial context , the accuracy of our sys-   tem is well beyond the accuracy of the unmodified   speech recognition model .   Limitations   We have proposed a shallow - fusion biasing system   that can be used in an embodied agent for improv-   ing accuracy in recognition of speech targeted to   the agent , using visual context acquired by the em-   bodied nature of the agent . The system ’s efficacy is   naturally limited by the assumption that the speech   is relevant to the visual context and the context can   be extracted accurately . The first factor is a gen-   eral limitation of any biasing system . However , we   show that our system does not degrade too much if   the context is irrelevant to the speech .   As for the problem of context extraction , the   system would also be less effective if the object   detector / caption generator can not correctly predict   the object name or describe the object as a caption .   One way to counter this would be to additionally   use a static prior knowledge , e.g. , the entire vo-   cabulary of an object detector or a pre - collectedsemantic map , instead of dynamic visual context   extraction . The context also need not be visual only ,   e.g. , one could bias using verbs ( go , pick , bring ,   etc . ) commonly used in instructions .   We designed the system , specifically the beam   search algorithm to be particularly effective when   applied to embodied agents . While parts of the al-   gorithm such as the pruning function are applicable   to generic biasing problems , some parts such as the   conditional lexicon constraint and our experiments   in general are mostly limited to the robotics do-   main . For example , the average biasing vocabulary   size in the test dataset is 10.9 , which is typical for   the described scenario . But we did not extensively   test the algorithm ’s scalability to very large lists of   biasing words .   Acknowledgments   We thank the anonymous reviewers for their valu-   able feedback . We thank Ruchira Singh , Ruddra   dev Roychoudhury and Sayan Paul for their help   with the dataset creation . We are thankful to Swar-   nava Dey for having helpful discussions .   References19541955A Appendix   A.1 Acoustic modeling   We use the Wav2Vec2 ( Baevski et al . , 2020 ) for   acoustic modeling of the speech . It transforms raw   audio input into a contextual feature representation   that can be further processed to predict labels from   a sub - word level vocabulary - grapheme , phoneme   or wordpiece . We have chosen grapheme units for   a relatively simple algorithm design , but our ap-   proach is equally applicable to phoneme or word-   piece vocabulary . The model encodes raw audio   waveform using several convolutional layers . The   output of the final convolution layer is quantized   and fed to a Transformer block that produces con-   textualized feature representation for the discrete   time - steps CF , . . . , CF , considering the entire   sequence ( Baevski et al . , 2020 ) . The network is pre-   trained in an self - supervised manner using 53.2k   hours of unlabeled audio . To perform speech recog-   nition , the pre - trained feature extractor CFis fed to   a linear layer with softmax and fine - tuned on anno-   tated data from the LibriSpeech dataset ( Panayotov   et al . , 2015 ) , with the CTC loss function ( Graves   et al . , 2006 ) . The softmax produces a probability   distribution over a character vocabulary in each   time - step .   A.2 Language model   We use a 3 - gram English language modeltrained   on LibriSpeech data . More specifically , we use   the3 - gram.pruned.1e-7 variant of the language   model in all of our experiments . We use the kenlm   library for querying and calculating probabilities   from language model .   A.3 Computing infrastructure & runtime   All of our experiments are performed in a laptop   with an 11generation Intel i-5 processor with 2.6   GHz frequency , having 8 physical cores and 16   GB main memory . The operating system used was   Ubuntu 20.04 . We show the average run time de-   coding using our algorithm and different baselines   in Table 6 . Please note that because of the sam-   pling function , a lesser number of candidates are   generated in each time step , compared to the Base   decoder . As a result , our decoding is actually faster .   However , without sampling ( which we do not ad-   vocate ) , an additional latency of ( 76.7 - 40.5)=36.2   ms on average is incurred for a single run.1956   A.4 Validation results   We report the WER and TA on the validation set in   Table 7 . These results are found by the separately   optimized hyper - parameter search on the validation   set .   A.5 Hyper - parameters for other models   We show the optimal hyper - parameters for all base-   lines and other variants of our model in Table 8 .   All models use beam width N= 100 . The optimal   hyper - parameters for our final model is shown in   the main paper , in Table 1 .   A.6 Dataset creation details   Adding to description in Section 5.1 , we manu-   ally drove a robotic agent in 5 different rooms inthe simulator ( Talbot et al . , 2020 ) to obtain the   images . We pre - processed the images by generat-   ing captions using ( Yang et al . , 2017 ) , followed by   running the bias - target prediction Bi - LSTM model .   Two volunteers from our organization agreed to   write the instructions . We asked the volunteers to   look at both the given image and the pre - processed   biasing words and write instructions for the robot   that includes one or more of the objects in the bias-   ing words . We manually checked and removed any   instruction that does n’t contain at least one word   from the biasing word list and thus ensured the   context is always valid . For recording , we have   given the instructions ( without image / biasing lists )   to three volunteers and asked them to record the   instructions using their mobile phones . We have   collected and converted all recordings to 16KHz   wav format . We have not excluded any record-   ings from the collected set . Additionally , we have   used two TTS models to generate recordings , as   described in Section 5.1.1957