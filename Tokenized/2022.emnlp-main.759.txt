  Sewon MinXinxi LyuAri HoltzmanMikel Artetxe   Mike LewisHannaneh HajishirziLuke ZettlemoyerUniversity of WashingtonMeta AIAllen Institute for AI   { sewon,alrope,ahai,hannaneh,lsz}@cs.washington.edu   { artetxe,mikelewis}@meta.com   Abstract   Large language models ( LMs ) are able to in-   context learn — perform a new task via infer-   ence alone by conditioning on a few input - label   pairs ( demonstrations ) and making predictions   for new inputs . However , there has been lit-   tle understanding of how the model learns and   which aspects of the demonstrations contribute   to end task performance . In this paper , we   show that ground truth demonstrations are in   fact not required — randomly replacing labels in   the demonstrations barely hurts performance on   a range of classification and multi - choce tasks ,   consistently over 12 different models including   GPT-3 . Instead , we find that other aspects of   the demonstrations are the key drivers of end   task performance , including the fact that they   provide a few examples of ( 1 ) the label space ,   ( 2 ) the distribution of the input text , and ( 3 ) the   overall format of the sequence . Together , our   analysis provides a new way of understanding   how and why in - context learning works , while   opening up new questions about how much can   be learned from large language models through   inference alone .   1 Introduction   Large language models ( LMs ) have shown impres-   sive performance on downstream tasks by simply   conditioning on a few input - label pairs ( demonstra-   tions ) ; this type of inference has been referred to as   in - context learning ( Brown et al . , 2020 ) . Despite in-   context learning consistently outperforming zero-   shot inference on a wide range of tasks ( Zhao et al . ,   2021 ; Liu et al . , 2021 ) , there is little understanding   ofhow it works and which aspects of the demon-   strations contribute to end task performance .   In this paper , we show that ground truth demon-   strations are in fact not required for effective in-   context learning ( Section 4 ) . Specifically , replac-   ing the labels in demonstrations with random labels   barely hurts performance in a range of classifica-   tion and multi - choice tasks ( Figure 1 ) . The resultFigure 1 : Results in classification ( top ) and multi - choice   tasks ( bottom ) , using three LMs with varying size . Re-   ported on six datasets on which GPT-3 is evaluated ; the   channel method is used . See Section 4 for the full results .   In - context learning performance drops only marginally   when labels in the demonstrations are replaced by ran-   dom labels .   is consistent over 12 different models including the   GPT-3 family ( Radford et al . , 2019 ; Min et al . ,   2021b ; Wang and Komatsuzaki , 2021 ; Artetxe   et al . , 2021 ; Brown et al . , 2020 ) . This strongly   suggests , counter - intuitively , that the model does   notrely on the input - label mapping in the demon-   strations to perform the task .   Further analysis investigates which parts of   demonstrations actually docontribute to the perfor-   mance . We identify possible aspects of demonstra-   tions ( e.g. , the label space and the distribution of   the input text ) and evaluate a series of variants of   the demonstrations to quantify the impact of each   ( Section 5 ) . We find that : ( 1 ) the label space and   the distribution of the input text specified by the   demonstrations are both key to in - context learn-   ing ( regardless of whether the labels are correct   for individual inputs ) ; ( 2 ) specifying the overall   format is also crucial , e.g. , when the label space   is unknown , using random English words as la-   bels is significantly better than using no labels ; and11048(3 ) meta - training with an in - context learning objec-   tive ( Min et al . , 2021b ) magnifies these effects — the   models almost exclusively exploit simpler aspects   of the demonstrations like the format rather than   the input - label mapping .   In summary , our analysis provides a new way   of understanding the role of the demonstrations in   in - context learning . We empirically show that the   model ( 1 ) counter - intuitively does not rely on the   ground truth input - label mapping provided in the   demonstrations as much as we thought ( Section 4 ) ,   and ( 2 ) nonetheless still benefits from knowing the   label space and the distribution of inputs specified   by the demonstrations ( Section 5 ) . We also include   a discussion of broader implications , e.g. , what we   can say about the model learning at test time , and   avenues for future work ( Section 6 ) .   2 Related Work   Large language models have been key to strong per-   formance in a wide range of downstream tasks ( De-   vlin et al . , 2019 ; Radford et al . , 2019 ; Liu et al . ,   2019 ; Raffel et al . , 2020 ; Lewis et al . , 2020 ) . While   finetuning has been a popular approach to transfer   to new tasks ( Devlin et al . , 2019 ) , it is often imprac-   tical to finetune a very large model ( e.g. ≥10B pa-   rameters ) . Brown et al . ( 2020 ) propose in - context   learning as an alternative way to learn a new task .   As depicted in Figure 2 , the LM learns a new task   via inference alone by conditioning on a concatena-   tion of the training data as demonstrations , without   any gradient updates .   In - context learning has been the focus of signif-   icant study since its introduction . Prior work pro-   poses better ways of formulating the problem ( Zhao   et al . , 2021 ; Holtzman et al . , 2021 ; Min et al . ,   2021a ) , better ways of choosing labeled exam-   ples for the demonstrations ( Liu et al . , 2021 ; Lu   et al . , 2021 ; Rubin et al . , 2021 ) , meta - training   with an explicit in - context learning objective ( Chen   et al . , 2021 ; Min et al . , 2021b ) , and learning to   follow instructions as a variant of in - context learn-   ing ( Mishra et al . , 2021b ; Efrat and Levy , 2020 ;   Wei et al . , 2022a ; Sanh et al . , 2022 ) . At the   same time , some work reports brittleness and over-   sensitivity for in - context learning ( Lu et al . , 2021 ;   Zhao et al . , 2021 ; Mishra et al . , 2021a ) .   Relatively less work has been done to understand   why in - context learning works . Xie et al . ( 2022 )   provide theoretical analysis that in - context learn-   ing can be formalized as Bayesian inference that   uses the demonstrations to recover latent concepts .   Razeghi et al . ( 2022 ) show that in - context learn-   ing performance is highly correlated with term fre-   quencies in the pretraining data . To the best of our   knowledge , this paper is the first that provides an   empirical analysis that investigates why in - context   learning achieves performance gains over zero - shot   inference . We find that the ground truth input - label   mapping in the demonstrations has only a marginal   effect , and measure the impact of finer - grained as-   pects of the demonstrations .   3 Experimental Setup   We describe the experimental setup used in our   analysis ( Section 4 and 5 ) .   Models . We experiment with 12 models in to-   tal . We include 6 language models ( Table 1 ) , all   of which are decoder - only , dense LMs . We use   each LM with two inference methods , direct and   channel , following Min et al . ( 2021a ) . The sizes   of LMs vary from 774 M to 175B. We include the11049   largest dense LM ( GPT-3 ) and the largest publicly   released dense LM ( fairseq 13B ) at the time of con-   ducting experiments . We also include MetaICL ,   which is initialized from GPT-2 Large and then   meta - trained on a collection of supervised datasets   with an in - context learning objective , and ensure   that our evaluation datasets do not overlap with   those used at meta - training time .   Evaluation Data . We evaluate on 26 datasets ,   including sentiment analysis , paraphrase detection ,   natural language inference , hate speech detection ,   question answering , and sentence completion ( full   list and references provided in Appendix A).All   datasets are classification and multi - choice tasks .   We use these datasets because they ( 1 ) are true   low - resource datasets with less than 10 K train-   ing examples , ( 2 ) include well - studied bench-   marks from GLUE ( Wang et al . , 2018 ) and Super-   GLUE ( Wang et al . , 2019a ) , and ( 3 ) cover diverse   domains including science , social media , finance ,   and more .   Other Details . We use k= 16 examples as   demonstrations by default for all experiments in   the paper , unless otherwise specified . Examples   are sampled at uniform from the training data .   We choose a set of ktraining examples using   5 different random seeds and run experiments 5   times . For fairseq 13B and GPT-3 , due to lim-   ited resources , we experiment with a subset of 6datasetsand 3 random seeds . We report Macro-   F1for classification tasks and Accuracy for multi-   choice tasks . We compute per - dataset average over   seeds , and then report macro - average over datasets .   We use the minimal templates in forming an in-   put sequence from an example . We refer to Ap-   pendix B for more details . All experiments are   reproducible from github.com/Alrope123/   rethinking - demonstrations .   4 Ground Truth Matters Little   4.1 Gold labels vs. random labels   To see the impact of correctly - paired inputs and   labels in the demonstrations — which we call the   ground truth input - label mapping — we compare the   following three methods .   No demonstrations is a typical zero - shot method   that does not use any labeled data . A prediction   is made via argmaxP(y|x ) , where xis the test   input and Cis a small discrete set of possible labels .   Demonstrations w/ gold labels are used in a typi-   cal in - context learning method with klabeled ex-   amples ( x , y) ... (x , y ) . A concatenation of k   input - label pairs is used to make a prediction via   argmaxP(y|x , y ... x , y , x).11050   Demonstrations w/ random labels are formed   with random labels , instead of gold labels from   the labeled data . Each x(1≤i≤   k ) is paired with ˜ythat is randomly sam-   pled at uniform from C. A concatenation of   ( x,˜y) ... (x,˜y)is then used to make a predic-   tion via argmaxP(y|x,˜y ... x,˜y , x ) .   Results are reported in Figure 3 . First , using the   demonstrations with gold labels significantly im-   proves the performance over no demonstrations ,   as it has been consistently found in much of prior   work ( Brown et al . , 2020 ; Zhao et al . , 2021 ; Liu   et al . , 2021 ) . We then find that replacing gold la-   bels with random labels only marginally hurts   performance . The trend is consistent over nearly   all models : models see performance drop in the   range of 0–5 % absolute . There is less impact in   replacing labels in multi - choice tasks ( 1.7 % on av-   erage ) than in classification tasks ( 2.6 % absolute ) .   This result indicates that the ground truth input-   label pairs are not necessary to achieve perfor-   mance gains . This is counter - intuitive , given that   correctly paired training data is critical in typical   supervised training — it informs the model of the ex-   pected input - label correspondence required to per-   form the downstream task . Nonetheless , the mod-   elsdoachieve non - trivial performance on the down-   stream tasks . This strongly suggests that the mod-   els are capable of recovering the expected input-   label correspondence for the task ; however , it is not   directly from the pairings in the demonstrations .   It is also worth noting that there is particularly   little performance drop in MetaICL : 0.1–0.9 % ab-   solute . This suggests that meta - training with an   explicit in - context learning objective actually en-   courages the model to essentially ignore the input-   label mapping and exploit other components of the   demonstrations ( more discussion in Section 5.4 ) .   In Appendix C.2 , we provide additional results   showing that ( 1 ) selecting random labels from a   true distribution of labels ( instead of a uniform   distribution ) reduces the gap even further , and ( 2 )   the trends may depend on the dataset , although the   overall trend is consistent over most datasets .   4.2 Ablations   For additional ablations , we experiment with 5 clas-   sification and 4 multi - choice datasets .   Does the number of correct labels matter ? To   further examine the impact of correctness of la-   bels in the demonstrations , we conduct an ablation   study by varying the number of correct labels in the   demonstrations . We evaluate “ Demonstrations w/   a% correct labels ” ( 0≤a≤100 ) which consist   ofk×a/100correct pairs and k×(1−a/100 )   incorrect pairs ( see Algorithm 1 in Appendix B ) .   Here , a= 100 is the same as typical in - context   learning , i.e. , demonstrations w/ gold labels .   Results are reported in Figure 4 . Model perfor-   mance is fairly insensitive to the number of correct   labels in the demonstrations . In fact , always us-   ing incorrect labels significantly outperforms no-11051   demonstrations , e.g. , preserving 92 % , 100 % and   97 % of improvements from using the demonstra-   tions with MetaICL in classification , MetaICL in   multi - choice , and GPT - J in multi - choice , respec-   tively . In contrast , GPT - J in classification sees   relatively significant performance drop with more   incorrect labels , e.g. , nearly 10 % drop in perfor-   mance when always using incorrect labels . Still ,   always using incorrect labels is significantly better   than no demonstrations .   Is the result consistent with varying k?We   study the impact of the number of input - label pairs   ( k ) in the demonstrations . Results are reported in   Figure 5 . First , using the demonstrations signifi-   cantly outperforms the no demonstrations method   even with small k(k= 4 ) , and performance drop   from using gold labels to using random labels is   consistently small across varying k , in the range of   0.8–1.6%.Interestingly , model performance does   not increase much as kincreases when k≥8 , both   with gold labels and with random labels . This is   in contrast with typical supervised training where   model performance rapidly increases as kincreases ,   especially when kis small . We hypothesize that   larger labeled data is beneficial mainly for super-   vising the input - label correspondence , and other   components of the data like the example inputs ,   example labels and the data format are easier to   recover from the small data , which is potentially a   reason for minimal performance gains from larger   k(more discussion in Section 5 ) .   Is the result consistent with better templates ?   While we use minimal templates by default , we   also explore manual templates , i.e. , templates that   are manually written in a dataset - specific manner ,   taken from prior work ( details in Appendix B ) . Fig-   ure 6 shows that the trend — replacing gold labels   with random labels barely hurting performance —   holds with manual templates . It is worth noting   that using manual templates does not always out-   perform using minimal templates .   5 Why does In - Context Learning work ?   Section 4 shows that the ground truth input - label   mapping in the demonstrations has little impact to   performance gains from in - context learning . This   section further examines what other aspects of the   demonstrations lead to good performance of in-   context learning .   We identify four aspects of the demonstrations   ( x , y) ... (x , y)that potentially provide learning   signal ( depicted in Figure 7 ) .   1.The input - label mapping , i.e. , whether each   input xis paired with a correct label y.   2.The distribution of the input text , i.e. , the   underlying distribution that x ... xare from .   3.The label space , i.e. , the space covered by   y ... y.   4.The format — specifically , the use of input-   label pairing as the format .   As Section 4 does for the input - label mapping ,   we design a series of variants of the demonstrations   that quantify the impact of each aspect in isolation   ( Section 5.1–5.3 ) . We then additionally discuss the   trend of the models meta - trained with an in - context   learning objective ( Section 5.4 ) . For all experi-   ments , models are evaluated on five classification11052   and four multi - choice datasets as in Section 4.2 .   See Appendix B and Table 4 for implementation   details and example demonstrations , respectively .   5.1 Impact of the distribution of the input text   We experiment with OOD demonstrations which   include out - of - distribution ( OOD ) text instead of   the inputs from unlabeled training data . Specif-   ically , a set of ksentences { x}are ran-   domly sampled from an external corpus , and re-   place x ... xin the demonstrations . This variant   assesses the impact of the distribution of the input   text , while keeping the label space and the format   of the demonstrations .   Results . Figure 8 shows that using out - of-   distribution inputs instead of the inputs from the   training data significantly drops the performance   when Channel MetaICL , Direct GPT - J or Channel   GPT - J are used , both in classification and multi-   choice , by 3–16 % in absolute . In the case of Di-   rect GPT - J in multi - choice , it is even significantly   worse than no demonstrations . Direct MetaICL   is an exception , which we think is the effect of   meta - training ( discussion in Section 5.4 ) .   This suggests that in - distribution inputs in the   demonstrations substantially contribute to perfor-   mance gains . This is likely because conditioning on   the in - distribution text makes the task closer to lan-   guage modeling , since the LM always conditioned   on the in - distribution text during training .   5.2 Impact of the label space   We also experiment with demonstrations w/ ran-   dom English words that use random English   words as labels for all kpairs . Specifically , we   sample a random subset of English words Cwhere|C|=|C| , and randomly pair ˜y∈ C   withx . This variant assesses the impact of the   label space , while keeping the distribution of the   input text and the format of the demonstrations .   Results . Based on Figure 9 , direct models and   channel models exhibit different patterns . With di-   rect models , the performance gap between using   random labels within the label space and using ran-   dom English words is significant , ranging between   5–16 % absolute . This indicates that conditioning   on the label space significantly contributes to per-   formance gains . This is true even for multi - choice   tasks where there is no fixed set of labels — we   hypothesize that multi - choice tasks still do have   a particular distribution of the choices ( e.g. , ob-   jects like “ Bolts ” or “ Screws ” in the OpenBookQA   dataset ) that the model uses .   On the other hand , removing the output space   does not lead to significant drop in the channel   models : there is 0–2 % drop in absolute , or some-   times even an increase . We hypothesize that this is   because the channel models only condition on the   labels , and thus are not benefiting from knowing   the label space . This is in contrast to direct models   which must generate the correct labels .   5.3 Impact of input - label pairing   Section 5.1 and 5.2 focus on variants which keep   the format of the demonstrations as much as possi-   ble . This section explores variants that change the   format . While there are many aspects of the format ,   we make minimal modifications to remove the pair-   ings of inputs to labels . Specifically , we evaluate   demonstrations with no labels where the LM is   conditioned on the concatenation of x ... x , and11053   demonstrations with labels only where the LM is   conditioned on the concatenation of y ... y. These   ablations provide the no - format counterparts of the   ‘ demonstrations with random English words ’ and   ‘ demonstrations with OOD inputs ’ , respectively .   Results . Based on Figure 10 , removing the for-   mat is close to or worse than no demonstrations ,   indicating the importance of the format . This is   likely because conditioning on a sequence of input-   label pairs triggers the model to mimic the overall   format and complete the new example as expected   when the test input is given .   More interestingly , keeping the format plays a   significant role in retaining a large portion of per-   formance gains by only using the inputs or only   using the labels . For instance , with Direct MetaICL ,   it is possible to retain 95 % and 82 % of improve-   ments from in - context learning ( demonstrations   with gold labels ) by simply sampling random sen - tences from a corpus and randomly pairing them   with the label set ( ■ in Figure 10 ) in classification   and multi - choice , respectively . Similarly , with the   channel models , it is possible to retain 82 % , 87 % ,   86 % and 75 % of improvements from in - context   learning by simply pairing each input from the un-   labeled training data with a random English word   ( ■ in Figure 10 ) in MetaICL classification , GPT-   J classification , MetaICL multi - choice and GPT - J   multi - choice , respectively . For all of these cases ,   removing inputs instead of using OOD inputs , or   removing labels instead of using random English   words is significantly worse , indicating that keep-   ing the format of the input - label pairs is key .   5.4 Impact of meta - training   Different from other models , MetaICL is trained   with an in - context learning objective , in line with   recent work that uses multi - task training on a11054large collection of supervised datasets ( called meta-   training ) for generalization to new tasks ( Agha-   janyan et al . , 2021 ; Khashabi et al . , 2020 ; Wei   et al . , 2022a ; Sanh et al . , 2022 ) . We aim to better   understand the role of this meta - training in relation   with our findings by closely examining the result of   MetaICL . In particular , we observe that the patterns   we see so far are significantly more evident with   MetaICL than with other models . For instance , the   ground truth input - label mapping matters even less ,   and keeping the format of the demonstrations mat-   ters even more . There is nearly zero influence of   the input - label mapping and the input distribution   in Direct MetaICL , and the input - label mapping   and the output space in Channel MetaICL .   Based on this observation , we hypothesize that   meta - training encourages the model to exclu-   sively exploit simpler aspects of the demonstra-   tions and to ignore others . This is based on our   intuition that ( 1 ) the input - label mapping is likely   harder to exploit , ( 2 ) the format is likely easier to   exploit , and ( 3 ) the space of the text that the model   is trained to generate is likely easier to exploit than   the space of the text that the model conditions on .   6 Discussion & Conclusion   In this paper , we study the role of the demonstra-   tions with respect to the success of in - context learn-   ing . We find that the ground truth input - label map-   ping in the demonstrations matters significantly   less than one might think — replacing gold labels   with random labels in the demonstrations only   marginally lowers the performance . We then iden-   tify a series of aspects in the demonstrations and   examine which aspect actually contributes to per-   formance gains . Results reveal that ( 1 ) gains are   mainly coming from independent specification of   the input space and the label space , ( 2 ) the models   can still retain up to 95 % of performance gains by   using either the inputs only or the label set only if   the right format is used , and ( 3 ) meta - training with   an in - context learning objective magnifies these   trends . Together , our findings lead to a set of   broader indications about in - context learning , as   well as avenues for future work .   Does the model learn at test time ? If we take   a strict definition of learning : capturing the input-   label correspondence given in the training data , then our findings suggest that LMs do not learn   new tasks at test time . Our analysis shows that the   model may ignore the task defined by the demon-   strations and instead use prior from pretraining .   However , learning a new task can be interpreted   more broadly : it may include adapting to specific   input and label distributions and the format sug-   gested by the demonstrations , and ultimately get-   ting to make a prediction more accurately . With   this definition of learning , the model does learn   the task from the demonstrations . Our experiments   indicate that the model does make use of aspects of   the demonstrations and achieve performance gains .   Capacity of LMs . The model performs a down-   stream task without relying on the input - label corre-   spondence from the demonstrations . This suggests   that the model has learned the ( implicit notion of )   input - label correspondence from the language mod-   eling objective alone , e.g. , associating a positive   review with the word ‘ positive ’ . This is in line   with Reynolds and McDonell ( 2021 ) who claim   that the demonstrations are for task location and   the intrinsic ability to perform the task is obtained   at pretraining time .   On one hand , this suggests that the language   modeling objective has led to great zero - shot ca-   pacity , even if it is not always evident from the   naive zero - shot accuracy . On the other hand , this   suggests that in - context learning may not work on   a task whose input - label correspondence is not al-   ready captured in the LM . This leads to the research   question of how to make progress in NLP problems   that in - context learning does not solve : whether   we need a better way of extracting the input - label   mappings that are already stored in the LM , a bet-   ter variant of the LM objective that learns a wider   range of task semantics , or explicit supervision   through fine - tuning on the labeled data .   Connection to instruction - following models .   Prior work has found it promising to train the model   that reads the natural language description of the   task ( called instructions ) and performs a new task   at inference ( Mishra et al . , 2021b ; Efrat and Levy ,   2020 ; Wei et al . , 2022a ; Sanh et al . , 2022 ) . We   think the demonstrations and instructions largely   have the same role to LMs , and hypothesize that our   findings hold for instruction - following models : the11055instructions prompt the model to recover the capac-   ity it already has , but do not supervise the model to   learn novel task semantics . This has been partially   verified by Webson and Pavlick ( 2022 ) who showed   that the model performance does not degrade much   with irrelevant or misleading instructions . We leave   more analysis on instruction - following models for   future work .   Significantly improved zero - shot performance .   One of our key findings is that it is possible to   achieve nearly k - shot performance without using   any labeled data , by simply pairing each unlabeled   input with a random label and using it as the demon-   strations . This means our zero - shot baseline level   is significantly higher than previously thought .   Future work can further improve the zero - shot per-   formance with relaxed assumptions in access to the   unlabeled training data .   Limitation   Effect of types of tasks and datasets . This paper   focuses on the tasks from established NLP bench-   marks that have realnatural language inputs . Syn-   thetic tasks with more limited inputs may actually   use the ground truth labels more , as observed by   Rong ( 2021 ) .   We report macro - level analysis by examining the   average performance over multiple NLP datasets ,   but different datasets may behave differently . Ap-   pendix C.2 discusses this aspect , including find-   ings that there are larger gaps between using the   ground truth labels and using the random labels   in some dataset - model pairs ( e.g. , in the most   extreme case , nearly 14 % absolute on the finan-   cial_phrasebank dataset with GPT - J ) . Since the first   version of our paper , Kim et al . ( 2022 ) showed   that using negated labels substantially lowers the   performance in classification . We believe it is   important to understand to what extend the model   needs the ground truth labels to successfully per-   form in - context learning .   Extensions to generation . Our experiments are   limited to classification and multi - choice tasks . We   hypothesize that ground truth output may not be   necessary for in - context learning in the open - settasks such as generation , but leave this to future   work . Extending of our experiments to such tasks   is not trivial , because it requires a variation of the   output which has incorrect input - output correspon-   dence while keeping the correct output distribution   ( which is important based on our analysis in Sec-   tion 5 ) .   Since the first version of our paper , Madaan and   Yazdanbakhsh ( 2022 ) conducted a similar analy-   sis with the chain of thought prompting ( Wei et al . ,   2022b ) which generates a rationale to perform com-   plex tasks such as math problems . Madaan and   Yazdanbakhsh ( 2022 ) show that , while simply us-   ing a random rationale in the demonstrations ( e.g. ,   pairing with a rationale from a different example )   significantly degrades the performance , other types   of counterfactual rationales ( e.g. , wrong equations )   do not degrade the performance as much as we   thought . We refer to Madaan and Yazdanbakhsh   ( 2022 ) for more discussions on what aspects of the   rationale matter or do not matter .   Acknowledgements   We thank Gabriel Ilharco , Julian Michael , Ofir   Press , UW NLP members and anonymous review-   ers for their comments in the paper . This research   was supported by NSF IIS-2044660 , ONR N00014-   18 - 1 - 2826 , a Sloan fellowship and gifts from AI2 .   References11056110571105811059A Full Datasets   We include 26 datasets as follows : fi-   nancial_phrasebank ( Malo et al . , 2014 ) ,   poem_sentiment ( Sheng and Uthus , 2020 ) ,   medical_questions_pairs ( McCreery et al . , 2020 ) ,   glue - mrpc ( Dolan and Brockett , 2005 ) , glue-   wnli ( Levesque et al . , 2012 ) , climate_fever ( Diggel-   mann et al . , 2020 ) , glue - rte ( Dagan et al . , 2005 ;   Bar - Haim et al . , 2006 ; Giampiccolo et al . ,   2007 ; Bentivogli et al . , 2009 ) , superglue-   cb ( de Marneffe et al . , 2019 ) , sick ( Marelli et al . ,   2014 ) , hate_speech18 ( de Gibert et al . , 2018 ) ,   ethos - national_origin ( Mollas et al . , 2020 ) , ethos-   race ( Mollas et al . , 2020 ) , ethos - religion ( Mollas   et al . , 2020 ) , tweet_eval - hate ( Barbieri et al . , 2020 ) ,   tweet_eval - stance_atheism ( Barbieri et al . , 2020 ) ,   tweet_eval - stance_feminist ( Barbieri et al . , 2020 ) ,   quarel ( Tafjord et al . , 2019a ) , openbookqa ( Mi-   haylov et al . , 2018 ) , qasc ( Khot et al . , 2020 ) , com-   monsense_qa ( Talmor et al . , 2019 ) , ai2_arc ( Clark   et al . , 2018 ) , codah ( Chen et al . , 2019 ) , superglue-   copa ( Gordon et al . , 2012 ) , dream ( Sun et al . ,   2019 ) , quartz - with_knowledge ( Tafjord et al . ,   2019b ) , quartz - no_knowledge ( Tafjord et al . ,   2019b ) . The choice of datasets is made following   low - resource datasets in Min et al . ( 2021b ) , with   the exact same set of k - shot train data using 5   random seeds . We use the HuggingFace version   of the data ( Lhoest et al . , 2021 ) and use the   development data for evaluation , following Ye   et al . ( 2021 ) . See Table 2 for statistics .   B Experimental Details   Example template We follow Ye et al . ( 2021 ) ;   Min et al . ( 2021b ) ; Logan IV et al . ( 2021 ) in us-   ing the minimal format to transform the input to a   sequence ( e.g. a concatenation of multiple inputs )   and using the label words from each dataset as it is .   We also explore manual templates taken from prior   work ( Holtzman et al . , 2021 ; Zhao et al . , 2021 ) as   reported in Section 4.2 , although we find that using   these templates is not consistently better than using   minimal templates . We thus run main experiments   with minimal templates . Example templates are   provided in Table 3 .   Format of the demonstrations We follow the   standard of each model for formatting the demon-   strations , either from exploration in prior work or   the example code provided in the official tutorial .   For GPT-2 , we separate the input and the label ,   and each demonstration example with a space . For   MetaICL , GPT - J and GPT-3 , we separate the input   and the label with a newline ( \n ) , and each demon-   stration example with three newlines . For fairseq   models , we use a newline to separate the input and   the label as well as each demonstration example .   Details in variants of the demonstrations For   “ demonstrations w/ a% accurate labels ” ( 0≤   a≤100 ) , we use k×a/100correct pairs and   k×(1−a/100 ) incorrect pairs in a random order ,   as described in Algorithm 1 . For “ OOD demon-   strations ” , we use CC - News ( Nagel , 2016 ) as an   external corpus . We consider the length of the text   during sampling , so that sampled sentences have   similar length to the test input . For “ demonstrations   with random English words ” , we use pypi.org/   project / english - words for the set of En-11060   Algorithm 1 Forming the demonstrations with an   accuracy of a% .   glish words , which consists of 61,569 words .   Table 4 provides a list of example demonstra-   tions for each method used in Section 5 .   C More Experimental Results   C.1 Gold labels vs. random labels   Figure 11 shares the same interface as Figure 3 , but   all models are evaluated on 3 classification and 3   multi - choice datasets and are thus comparable to   each other .   C.2 Random labels from true distribution of   labels & Task breakdown   In Section 4 , random labels are sampled from the   label space from a uniform distribution . We ex-   periment with another variant of demonstrations in   the classification tasks , where labels are randomly   sampled from the true distribution of labels on the   training data . This may have large impact if labels   are far from uniform on the training data . Results   indicate that performance drop from using goldlabels is further reduced compared to using uni-   formly random labels : with Channel MetaICL , the   gap is reduced from 1.9 % to 1.3 % absolute , and   with Channel GPT - J , the gap is reduced from 5.0 %   to 3.5 % absolute .   Figure 12 shows performance gap between using   gold labels and using random labels per dataset . We   find that the trend that the gap is smaller than pre-   viously thought is consistant across most datasets .   Nonetheless , there are a few outlier datasets where   performance gap is non - negligible , such as finan-   cial_phrasebank and a few hate speech detection   datasets . Future work may investigate on which   tasks the model makes more use of the correctly   paired training data .   C.3 More variants of the demonstrations   We explored demonstrations with a con-   stant label where all labels in the demon-   strations are replaced with a constant text ,   “ answer ” . Specifically , a prediction is made via   argmaxP(y|x , answer ... x , answer , x ) .   This can be viewed as another way to remove the   impact of the label space while keeping the impact   of the distribution of the input text . However ,   results are consistently worse than the results   of demonstrations with random English labels .   We think this is because constant labels actually   change the format of the demonstrations , since   they can be viewed as part of a separator between   different demonstration examples .   We also explored demonstrations with the test   input where all inputs in the demonstrations are   replaced with the test input , each paired with a ran-11061dom label . Specifically , a prediction is made via   argmaxP(y|x,˜y ... x,˜y , x ) , where ˜y(1≤   i≤k ) is randomly sampled at uniform from C.   This variant is seemingly a reasonable choice given   that it satisfies the condition that the inputs in the   demonstrations come from the same distribution   as the test input ( since they are identical ) , and us-   ing random labels is as good as using gold labels .   Nonetheless , we find that this variant is signifi-   cantly worse than most other methods with demon-   strations . We think this is because using the con-   stant input for all demonstration example signifi-   cantly changes the format of the sequence , since the   input can be viewed as part of a separator between   different demonstration examples.110621106311064