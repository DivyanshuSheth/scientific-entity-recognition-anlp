  Shanqing CaiSubhashini VenugopalanKatrin Tomanek   Ajit Narayanan Meredith Ringel Morris Michael P. Brenner   Google Research   { cais,vsubhashini}@google.com   Abstract   Motivated by the need for accelerating text en-   try in augmentative and alternative communi-   cation ( AAC ) for people with severe motor im-   pairments , we propose a paradigm in which   phrases are abbreviated aggressively as primar-   ily word - initial letters . Our approach is to   expand the abbreviations into full - phrase op-   tions by leveraging conversation context with   the power of pretrained large language mod-   els ( LLMs ) . Through zero - shot , few - shot , and   ﬁne - tuning experiments on four public conver-   sation datasets , we show that for replies to the   initial turn of a dialog , an LLM with 64B pa-   rameters is able to accurately expand over 70 %   of phrases with abbreviation length up to 10 ,   leading to an effective keystroke saving rate of   up to 77 % on these expansions . Including a   small amount of context in the form of a single   conversation turn more than doubles abbrevia-   tion expansion accuracies compared to having   no context , an effect that is more pronounced   for longer phrases . Additionally , the robust-   ness of the models against typo noise can be   enhanced through ﬁne - tuning on noisy data .   1 Introduction   The prevalent paradigm of text entry on computing   devices is sequential typing of characters . Word   completion and prediction can theoretically save   up to 40 - 50 % keystrokes when 3 - 5 predictions   are provided ( Trnka and McCoy , 2008 ; Fowler   et al . , 2015 ) . This reduces the motor and cogni-   tive demand of entering text , especially on devices   where typing is difﬁcult , e.g. , phones . In AAC   use cases such as eye - gaze keyboards for severely   motor - impaired individuals , the cost per keystroke   is so high that there is a desire to save as many   keystrokes as possible . Gaze - typing requires the   user to precisely control the direction and timing of   gaze for each keystroke , resulting in an extremely   low text - entry speed of 8 - 10 words per minute and   severely limiting real - time communication ( Waller ,   2019 ) . A text - entry paradigm with substantially   higher keystroke saving rate ( KSR ) can reduce mo-   tor demand and thereby beneﬁt AAC usage in real-   time communication .   One potential paradigm is " SMS language " ,   a spontaneously - evolved system for saving   keystrokes in which each word is abbreviated as   a single letter , such as in the well - known abbrevi-   ations sgforsounds good andttylfortalk to you   later ( Anjaneyulu , 2013 ) . SMS language features   a high KSR ( 75 - 80 % ) , but is limited by its small   closed set of common phrases of mostly six words   or shorter . Its abbreviation scheme is not applied to   longer or less frequent phrases because such abbre-   viations would be hard for the recipient to decipher .   For example , the abbreviation iipitb is highly am-   biguous and may represent many possible phrases ,   e.g. , it is pouring in the bay andit is pretty in the   backyard ( see Figure 1 for more examples ) . Some   existing AAC systems support abbreviation expan-   sion ( e.g. , Tobii ) , but are limited by hardcoded,1261closed phrase sets .   The current study is based on the insight that   although decoding open - set phrases from abbre-   viations is hard without context due to ambiguity ,   providing conversational context signiﬁcantly con-   strains the space of likely phrases as shown by   the example in Fig.1 ( it is playing in the back-   yard ) . Hence we propose a high - KSR abbrevia-   tion scheme that focuses on conversational scenar-   ios . We apply this scheme to three existing dialog   datasets and create datasets for abbreviation expan-   sion ( AE ) .   This allows us to study whether LLMs , trained   on web text including conversational data , can en-   able AE and beneﬁt from added context . We take a   64B parameter LLM and compare zero - shot , few-   shot , and ﬁne - tuning performance on the AE task .   Additionally , we simulate typing noise to study   tolerance of the approach to typos . The main con-   tributions of our work are :   1.Demonstrating the potential of abbreviation   expansion using LLMs aided by conversational   context for highly - abbreviated text entry , while   measuring the effects of different amounts of con-   text and different dialog turns .   2.Describing a high - KSR abbreviation scheme ,   a method for simulating typing noise , and conver-   sation datasets based on these .   3.Comparing zero - shot , few - shot , and model   ﬁne - tuning approaches for the AE task and their   tolerance to typo noise .   2 Related Work   Abbreviation expansion for text entry . Previ-   ous research on aiding text entry through AE used   abbreviation schemes such as using only content   words ( Demasco and McCoy , 1992 ) , discarding   certain vowels and consonants ( Shieber and Nelken ,   2007 ) , and ﬂexible letter saving schemes ( Pini et al . ,   2010 ; Adhikary et al . , 2021 ; Gorman et al . , 2021 ) .   Spontaneous abbreviations schemes primarily omit   vowels , repeating consonants , last characters , and   spaces , and lead to modest KSR ( e.g. , 25 - 40 %   in Willis et al . 2005 , and 21 % in Adhikary et al .   2021 . ) The low KSR of such schemes can be at-   tributed to the implicit need for a human reader   to decode the phrases without signiﬁcant cogni-   tive burden . N - gram models and neural language   models ( LMs ) have been applied to expanding ab-   breviations for these relatively low - KSR schemes .   By using LSTM models and context , Gorman et al.(2021 ) achieve a word error rate of 1.5 % . Adhikary   et al . ( 2021 ) report a 24.2 % top-5 sentence error   rate decoding abbreviations using an RNN to aug-   ment an n - gram LM . Our presented approach is a   step towards using automation and context to ex-   pand abbreviations at a higher KSR that is close to   that of SMS language .   Large language model prompting and ﬁne-   tuning . Our approach builds on prior work on   LLMs including few - shot prompting , ﬁne - tuning ,   and conversation models ( Raffel et al . , 2019 ;   Brown et al . , 2020 ; Adiwardana et al . , 2020 ;   Roller et al . , 2020 ) . We focus primarily on few-   shot prompting ( Brown et al . , 2020 ) and ﬁne-   tuning ( Ruder , 2021 ) . Few - shot prompting uses a   text description of a task along with a small number   of examples for the task in the input text in order   to elicit desired task responses from an LLM . In   thezero - shot scenario , no examples are provided .   Prompting involves no updates to the model pa-   rameters . Model ﬁne - tuning requires more data   compared to prompting , but often leads to higher   task accuracy than prompt engineering ( e.g. , Austin   et al . 2021 ; Lester et al . 2021 ) . For our AE task ,   data for ﬁne - tuning can be synthesized from exist-   ing conversation datasets based on an abbreviation   scheme ( Sec . 3 ) . Thus , we explore both prompting   and ﬁne - tuning and compare their performance .   Assisting text entry with context . Textual con-   texts have been exploited to aid email writing ( Kan-   nan et al . , 2016 ; Chen et al . , 2019 ) . For text en-   try in AAC , Wisenburn and Higginbotham ( 2008 )   demonstrated that providing noun phrases from a   conversation partner ’s speech as selection options   increases text - entry speed by 36.7 % . Adhikary et al .   ( 2019 ) concluded that with currently - attainable ac-   curacy of ASR , partner speech can be valuable in   improving language modeling for AAC text entry .   Shen et al . ( 2022 ) used a ﬁne - tuned GPT-2 model   ( Radford et al . , 2019 ) to expand bags of keywords   into full phrases in conversational contexts based   on the ConvAI2 dataset ( Dinan et al . , 2020 ) and   reported a KSR of 77 % at a word error rate thresh-   old of 0.65 . Our current study differs from the   previous studies in the following aspects . First , we   provide an abbreviation scheme to allow greater   user control over the exact phrase structure and   wording . Second , we performed detailed quantita-   tive analysis of the combined predictive power of   state - of - the - art LLMs and context awareness.12623 Methodology   Abbreviation Scheme . Our abbreviation   scheme differs from previous studies in that we   optimize for KSR and do not expect a human   reader to be able to easily decode the abbreviations .   Additionally , it offers the beneﬁt that each given   phrase is mapped to a ﬁxed abbreviation . The   detailed rules for abbreviating phrases are :   1.Each word is abbreviated as its initial letter ,   unless the word contains an apostrophe ( i.e. , con-   traction ) , in which case the word is split at the   apostrophe and the initial letters from the splits are   taken ( e.g. , ca n’t – > ct ) . This prevents abbrevia-   tions that are otherwise identical but semantically   opposite ( e.g. , canvs.can’t ) .   2 . All letters in the abbreviation are lowercase .   3.Arabic numerals in a sentence are preserved   ( e.g. , see you at 10 o’clock – > sya10oc ) .   4.Sentence-ﬁnal punctuation are removed . Mid-   sentence punctuation and special characters ( e.g. , #   and$ ) are preserved to help constrain the structure   of the sentence ( e.g. , OK , but be quick . – > o , bbq ) .   3.1 Datasets for context - aware AE   We study modiﬁed versions of existing dialog   datasets , which we converted for the context - aware   AE task . We also describe how we simulate typos .   Datasets . Table 1 summarizes the four datasets .   We use their original train / dev / test splits in our ex-   periments . The Turk Dialogues dataset ( Vertanen ,   2017 ) consists of crowd - sourced dialogs , each of   which is exactly six turns in length . The dataset has   typos and grammatical errors . We manually cor-   rect these and refer to the corrected dataset as Turk   Dialogues Corrected ( TDC ) .We use three more   datasets , DailyDialog ( Li et al . , 2017 ) , a dataset of   everyday conversations ; the Cornell Movie Dia-   logues ( CMD ) ( Danescu - Niculescu - Mizil and Lee ,   2011 ) based on movie scripts , and the Turk AAC   dataset ( TAC ) ( Vertanen and Kristensson , 2011 ) .   For evaluation on out - of - domain dialogs , we use   theTaskMaster-1 Self Dialogs ( TMSD ) dataset   ( Byrne et al . , 2019 ) , a corpus of dialogs written by   crowdworkers for task - oriented scenarios such as   ordering pizza . TMSD is used only for evaluation   and not for training or validation of the models . For   DailyDialog , we remove 228 dialogues from thetest split that are duplicate with conversations in the   train split ( see Appendix A ) , which leads to what   we call the DailyDialog Corrected ( DDC ) dataset .   No correction is applied to the other datasets . The   TAC dataset contains only isolated phrases without   any conversational - turn context . Hence we use it   only for training . In all of our experiments , we com-   bine data from the training splits of all four datasets   when ﬁne - tuning models . We perform evaluations   on the TDC , DDC , CMD , and TMSD datasets . The   TDC dataset is chosen as our primary benchmarks   because of its strict six - turn dialog structure .   Modiﬁcations for the AE task . The above-   mentioned datasets are typically used to study   dialog generation . For our scenario , we con-   vert each turn of the conversation in these   datasets into the following canonical format :   Context : { Content of the contextual turn }   Shorthand : { Abbreviation of next turn }   Full : { Expanded content of next turn }   Context : { Would you like to sit down ? }   Shorthand : { n , imfsu }   Full : { No , I ’m ﬁne standing up }   For the AE task , the context consists of one or   more previous dialog turns . When context is absent   ( e.g. , for the opening turn ) , the context part is   omitted . For a multi - turn dialog , the n(1 - based )   example contains the ﬁrst ( n - 1 ) dialog turns as the   context as well as the shorthand and the full form   of the nturn . Thus , a 6 - turn conversation yields   six examples for the AE task . When multiple   sentences are present in a single turn , we use only   the ﬁrst sentence for expansion ; when a turn is   used as context , all available sentences are used .   Table 2 shows examples generated from all six   turns of a dialog from TDC . Each dialog in the   TDC , DDC , and CMD datasets yields several   examples covering different amount of context .   We create only 0 - context - turn examples for the   TAC dataset since it contains only isolated phrases .   Text - entry noise in AE datasets . As with our   AE scheme , the introduction of noise to the datasets   is also motivated by the AAC text entry use case ,   and in particular eye - gaze typing , which is error   prone ( Feit et al . , 2017 ) . Here , misclicks occur   frequently and must be taken into account when   designing a gaze - driven text entry system . In order   to simulate the noise , we model eye - gaze typing   as uncorrelated 2D Gaussian distributions around   the intended key ( Azenkot and Zhai , 2012 ) . To1263   simulate noise in the abbreviation input , we use a   simpliﬁed rectangular - grid qwerty keyboard layout   with 30 keys arranged in three rows and 10 columns .   The keys are 1×1squares with no gaps in between .   The keystrokes for an intended key are drawn from   2D Gaussian distribution centered on the center of   the intended key and standard deviations denoted   σequal in the two spatial dimensions . To model   different levels of noise , we use three values of   σ : 0.0 ( i.e. , no - typo baseline ) , 0.3 , and 0.5 , which   corresponds to 0 % , 13 % , and 44 % character error   rates , respectively . Examples with simulated typos   are shown in Table 2 .   3.2 Large Language Model   One of our goals is to test whether zero - shot and   few - shot prompting of LLMs are effective at the   AE task without the need for supervised ﬁne - tuning .   Prompting is the method of eliciting desired task-   speciﬁc responses from an LLM by including anatural - language description of the task and/or   input - output examples of the task in the input string   for an LLM , without altering the model ’s weights   ( Brown et al . , 2020 ) . Zero- and few - shot prompting   differ in whether any examples are included in the   prompt to the LLM . For this , we use a decoder-   only Transformer language model ( Vaswani et al . ,   2017 ) from the LaMDA ( Thoppilan et al . , 2022 )   family of models . Our experiments are based   on the 64B parameter model , unless otherwise   speciﬁed . This model has 32Transformer layers ,   withd = 8192 , d= 65536 , h= 128 ,   dk = dv= 128 . The model was pre - trained on   2.97B public web documents , Wikipedia , and di-   alogs . The training data was tokenized with the   SentencePiece vocabulary ( Kudo and Richardson ,   2018 ) of size 32K. We call this the BaseLLM .   We also developed ﬁne - tuned versions of this   model for the AE task . The ﬁne - tuning uses ex-   amples in the format as shown in Table 2 . Since   the BaseLLM is a decoder only model , and we   use both the context and abbreviation as triggers   to the model during inference , we modify the loss   to only be calculated on the tokens of the AE tar-   get , i.e. the full form to be predicted in the pair   of curly brackets after " Full : " . For both training   and inference , we split the characters in the ab-   breviation with spaces to force SentencePiece to1264use per - character IDs . We tunetwo models , FT-   LLM on the combined AE datasets without typos ,   andFTnoise - LLM on the version with simulated   typos . Both use early stopping on a dev set consist-   ing of combined examples from the dev splits of   TAC and TDC ( Table 1 ) .   4 Experiments   Models . We use and compare the following mod-   els in our different experiment settings .   Look - Up Table ( LUT ) . As a straight - forward ,   non - ML baseline , we compile a dictionary of   375,298 sentence - level abbreviations from the train   splits of the datasets in Table 1 . Each abbreviation   maps to one or more phrases with their frequencies ,   leading to 447,249 unique abbreviation - sentence   pairs . During evaluation , we map the query abbrevi-   ation to the top-5 expansion phrases ( by frequency )   by using the dictionary and breaking ties randomly .   BaseLLM ( from Sec . 3.2 ) . We study the   BaseLLM in the zero - shot andfew - shot ( specif-   ically 4 - shot ) settings . The four examples are   selected from the train split of the TDC dataset ( see   Appendix B ) . We quantify the variability of the   model on a sets of 856 4 - example sequences from   the train split of the TDC dataset . The best per-   forming one on the dev set is denoted BaseLLM .   FTnoise - LLM tuned on simulated typos with   noise levelσ= 0.3(see Appendix C ) , and FT-   LLM tuned on AE data without noise as described   in Sec . 3.2 are additional models we compare to .   T5 encoder - decoder . For comparison with   smaller models , we use the T5 encoder - decoder   small ( 60 M ) , large ( 770 M ) , and 3Bparameter   models ﬁne - tuned on AE data without noise , iden-   tical to FT - LLM .   We evaluate the ﬁne - tuned models in the set-   ting without any explicit natural language instruc-   tions ( denoted “ no instr . ” ) unless mentioned oth-   erwise . For all models , we perform random sam-   pling with temperature=1.0 over the top_k=40 can-   didates with the highest logits at each step . We   decode 128 samples for each abbreviation unless   otherwise speciﬁed . For each model and evaluation   setting we report the standard deviations ( SDs ) of   metrics over 3 repeated runs .   Studies . For the BaseLLM , we study the vari-   ance in performance based on the prompt selection .   For all the models , we sample multiple responses   for each query , hence we study the effect of number   of responses sampled on AE accuracy and latency .   We also compare the performance of the models   with varying amounts of conversation context and   with no context . To study the effect of typos , we   compare the performance of the models on the   noise induced AE dataset . To measure the impact   of model size on accuracy and latency , we also ﬁne-   tune and evaluate performance of the decoder - only   LaMDA models with fewer than 64B parameters ,   speciﬁcally 4B , 8B , and 27B parameters . All these   models were trained on the same data , so that the   model size consitutes the only difference .   Evaluation . We only evaluate on conversation   queries with abbreviation length ≤10charac-   ters . This encompasses the majority ( 85 % ) of   the dialog turns from the original dataset ( Table   3 ) . Where applicable , we prepend the following   natural - language instruction to the model input for   the AE task : " Given previous turn(s ) of conversa-   tion and acronym of reply , write the full phrase . "   Before calculating performance metrics , we ﬁl-   ter the model ’s responses : we remove sentence-   ﬁnal punctuation , standardize whitespace to one   space , lower - case , de - duplicate , and ﬁlter for pre-   cise match of the abbreviation . The responses that   pass the ﬁltering are sorted by descending count .   For evaluation with noise , we do ﬁltering to allow   matches to nearby characters on the keyboard .   Metrics . Accuracy measures whether any re-   sponse expansion exactly matches the ground truth   ( with standardized letter - casing and whitespace ,   and discarded ﬁnal punctuation ) . Additionally , we   measure BLEU score ( Papineni et al . , 2002 ) us-   ing the SacreBLEU library ( Post , 2018 ) as a more   ﬁne - grained metric for the similarity between AE   options and the ground truth . For both metrics , we   report performance in the top-5 responses after they   are sorted based on frequency .   Key Stroke Savings ( KSR ) measures the num-1265   ber of saved keystrokes compared to the full length   of the phrase . Note , however , that AE succeeds   only for a subset of the cases , while for others the   top-5 options do not contain the intended phrase .   Hence we compute two types of KSR :   KSR , computed on all phrases , is deﬁned as   whereL andLare the character lengths   of the abbreviation and full phrase , respectively . In   other words , if a phrase has a matching option in the   top-5 , we calculate the KSR as the percentage of   keypresses saved by using the abbreviation . If the   ground truth is not in top-5 , we add a penalty term   ( L ) to account for the need to enter the phrase   by starting anew character - by - character , leading to   anegative KSR.KSRis calculated by averaging   over all phrases in an experiment . KSR , is   calculated by averaging over only the subset of   phrases with exact matches and uses the ﬁrst case   in Equation 1 .   5 Results   We present the main results comparing the models   on all datasets in Table 4 and then highlight results   from speciﬁc experiments .   The accuracy of LLMs at expanding word-   initial abbreviations is enhanced by ﬁne - tuning .   Table 4 compares the performance of all the mod-   els on the abbreviation expansion ( AE ) task . The   data shown in the table are for AE on the 2turn   of a dialog that utilizes the 1turn as the context ,   which focuses on our main hypothesis regarding   the effect of context on AE .   It ’s noteworthy that the BaseLLM , which has   seen just four examples in its prompt ( unlike the   other models ) , shows performance that exceeds   the look - up table ( LUT ) baseline in many cases , demonstrating the versatility of LLMs . The higher   scores of the LUT on DailyDialogs ( DDC ) and Cor-   nell Movie Dialogues ( CMD ) datasets are indica-   tive of the high percentage of similar phrases in the   train and test sets of the datasets . Unsurprisingly ,   the ﬁne - tuned models ( FT - LLM , FTnoise - LLM ,   and T5 models ) far outperform even the best 4 - shot   BaseLLM , achieving 74 - 77 % top-5 exact - match   accuracy on the TDC and DDC datasets in the ab-   sence of typo noises . The accuracies are lower on   the CMD dataset ( comprised of movie scripts . ) The   out - of - domain evaluation on the TaskMaster Self   Dialogs ( TMSD ) dataset also showed accuracies   lower than the TDC and DDC datasets , but higher   than the results from the CMD dataset .   Fine - tuning and tolerance to noise . For condi-   tions that involve simulated typo noise in the ab-   breviation input , FTnoise - LLM shows superior per-   formance compared to other models ( see the col-   umn " TDC - test + noise " in Table 4 . ) Interestingly ,   the performance of the BaseLLMdoesn’t drop as   much as any of the ﬁne - tuned models - T5 or FT-   LLM - in this setting . However , while FT - LLM still   outperforms BaseLLM on the noisy abbreviations ,   the smaller T5 models fail to do so .   Context is critical for AE accuracy Figure 3   show how the AE accuracy of FT - LLM varies when   different amounts of context from previous turns   of the conversation are provided . Compared to   having no context ( dash - dotted curve ) , including   just one previous turn of context ( dashed curve )   approximately doubles accuracy . Using the full   context ( all dialog turns from the 1to the ( n-   1 ) , solid curve ) leads to further improvements   indicating that prior turns carry useful information   for the AE task .   Compared to the 1turn , AE under no context   on subsequent turns ( 2 - 6 ) shows signiﬁcantly   worse accuracy . This is due to the fact that the   ﬁrst turn consists of conversation starters that are   easier to predict without context . Overall , irrespec-   tive of context , the accuracy of AE decreases as1266   the number conversation turns increases , indicating   increasing difﬁculty in predicting the full phrases   from the abbreviation as the dialogs progress . How-   ever , including full context during inference still   achieves accurate expansions for 60 % -70%of the   cases on the later turns .   Effect of context is more pronounced on   longer abbreviations . When performance is   sliced by the abbreviation length ( Figure 4 ) , accu-   racy without context decreases sharply and nearly   monotonically with increasing abbreviation length ,   regardless of whether it ’s the opening turn or the   2nd turn . With context however , the accuracy   remains higher and decreases more slowly with   abbreviation length , extending the approximately   80 % or higher accuracy into longer phrase lengths .   The variability and usefulness of few - shot   prompts decreases after model tuning . Here   we focus on how much the LLM beneﬁts fromAcc.@top-5 BaseLLM FT - LLM   4 - shot prompt 31.71 ±4.83 74.43 ±1.79   0 - shot prompt 37.10 ±1.38 77.10 ±0.38   No instr . 14.00 ±1.01 76.65 ±1.06   prompting before and after ﬁne - tuning . The ﬁrst   row of Table 5 compares AE accuracies from   different 4 - shot prompts on the TDC dataset for   BaseLLM and FT - LLM . We use the 856 example   abbreviation - expansion pairs from the train split of   the TDC dataset , using four conversation examples   for the prompt at a time . The BaseLLM shows a   large variance in performance depending on the   selected examples in the prompt by as much as   SD = 4.83 . The best 4 - shot prompt for BaseLLM   outperforms the 0 - shot prompt , despite the fact that   the average 4 - shot prompt accuracy is lower . There-   fore for BaseLLM we report the results from the   best 4 - shot prompt ( BaseLLM ) . By contrast , the   ﬁne - tuned model ( FT - LLM ) shows signiﬁcantly   lower prompt - related variance ( SD = 1.79 ) in   addition to a 2.3 - fold increase in the mean accu-   racy . Moreover , FT - LLM is able to perform the AE   task with only a natural - language prompt without   examples ( 0 - shot prompt ) and even without any   instruction ( “ No instr . ” ) at average accuracies that   are more than 1 SD above that of 4 - shot prompting .   The “ No instr . ” setting is attractive due to its sim-   plicity ( no need to search for or hand - engineer a   prompt ) and reduced latency ( due to shorter input   preﬁx lengths ) . Given these results , we use the   “ No instr . ” as the default setting and for all other   experiments on FT - LLM and FTnoise - LLM.1267   Increasing number of decoded samples im-   proves accuracy at the cost of latency . Latency   is important for interactive text - entry applications .   During sampled decoding , the LLMs generate 128   continuations of length 16 tokens for a batch of   preﬁx length 256 with a median latency of 0.568 s   ( interquartile range : 0.16 s ) .   This latency is close to typical dwell time of   eye - gaze keyboards ( Majaranta and Räihä , 2007 )   and hence could be acceptable for the eye - gaze   typing use cases . Figure 5 shows the effect of in-   creasing the number of continuations sampled from   the LLMs . As expected , increasing sample count   from 128 to 2048 improves top-5 accuracy for both   BaseLLM * ( with 4 - shot prompts ) and FT - LLM ( no   instr . ) . Improved accuracy comes at the cost of in-   creased latency . BaseLLM beneﬁts signiﬁcantly   more from increasing sample count than FT - LLM .   Comparison of model sizes Figure 5 also com-   pares ﬁne - tuned models of different sizes ( 4B , 8B ,   27B , and 64B ) . With model ﬁne - tuning , the ac-   curacy increases monotonically with increasing   number of parameters . Interestingly , even with the   fewest samples ( 128 ) , ﬁne - tuned models of all sizes   outperform the larger ( 64B ) model under few - shot   learning . Amongst the encoder - decoder T5 models   ( Table 4 ) larger models signiﬁcantly outperform   smaller ones . As observed for the decoder - only   models , the smaller ﬁne - tuned T5 models outper-   form the few - shot BaseLLM in almost all cases   except when the input consists of typos .   Keystroke saving rates . KSR can be considered   as a proxy measure of usability of the approach   for AAC use - cases . KSR values are in the   range of 73 - 77 % for the 1st and 2nd turns of di-   alogs in the TDC and DDC datasets ( Table 6 ) , indi-   cating that our proposed AE scheme does indeed   lead to high KSRs . Values of KSRare lower ,   reﬂecting the penalties for when a perfect match   is not achieved . However , with context , KSR   approaches 50 % and is higher compared to no con-   text ( 20%-37 % ) . Note that KSRis extremely   conservative as it does not consider ( a ) the possi-   bility of using the information already contained in   the abbreviation to " recover from AE failure " ( e.g. ,   by letting the user specify a word and invoke the   LLM again ) or ( b ) the fact that word completion   and prediction may still be utilized even if the user   falls back to sequential text entry .   Fine - tuning with noise improves typo tolerance .   Figure 6 compares the AE accuracies of LLMs   ﬁne - tuned with and without noise ( FTnoise - LLM   and FT - LLM ) . While both models show decreasing   AE accuracies with increasing amounts of typos ,   FTnoise - LLM is much more robust showing lesser   drop in performance . Further , on noise - free inputs   ( σ=0 ) , FTnoise - LLM shows only slight accuracy   deterioration compared to FT - LLM . We also ﬁnd   that typo tolerance , for both FT - LLM and FTnoise-   LLM , is more pronounced with context than with-   out .   Cross - domain generalization . We use the   TMSD dataset to compare and evaluate the   performance of models on conversation domains   not seen in training . In Table 4 we can observe   that few - shot prompting does fall behind the   simple Look - Up Table baseline on DDC and CMD   datasets . However , when we evaluate the models   on cross - domain TMSD dataset of dialogs we can   observe that the ﬁne - tuned and few - shot models do   generalize better to unseen domains and perform   better than the baseline look - up .   6 Discussion   Qualitative analysis of AE failures . As indi-   cated by the relatively high BLEU scores in Table 41268(>80 % ) , there are many expansions in the top-5   options that are " near misses " . Appendix Table   7 shows a few examples of such near misses , in   which the options differ from the the ground - truth   phrase by only a semantically - similar word ( e.g. ,   “ yes ” vs. “ yeah ” , “ head out ” vs. “ head over ” . ) Fu-   ture studies need to investigate the frequency and   UX acceptability of such near - miss AE options .   But their existence implies that exact - match ac-   curacy reported above slightly underestimates the   practical effectiveness of the models . Another cat-   egory of AE failures involve phrases that contain   certain proper nouns . The last four examples in Ta-   ble 7 show such cases in which the model correctly   expands all the words but a proper noun . When   such errors occur , the model tends to predict more   common proper nouns , which is likely a reﬂection   of the higher frequency of the predicted nouns in   the model ’s pre - training and ﬁne - tuning datasets .   The beneﬁt of AE relative to sequential text en-   try . Word completion and prediction incur scan-   ning cost : users scan the options in order to deter-   mine whether any of them match their intention ,   which has a detrimental effect on speed that needs   to be overcome by the high quality of the options   ( Trnka et al . , 2009 ) . Although the speed of AE-   based text entry remains to be quantiﬁed in future   studies , we point out that : ( 1 ) AE removes over-   head of scanning for options in between keystrokes ,   ( 2 ) there are fewer characters to examine or correct   when typing , both of which may offer speed - ups in   addition to the higher KSR afforded by AE .   Although the current study is motivated by and   focuses on the AAC use case , our paradigm of ab-   breviated text entry may be applicable to text input   on touch screens as well . The AE approach of the   current study can be regarded as a variation of con-   textual prediction of user text ( Kannan et al . , 2016 ;   Chen et al . , 2019 ) that affords greater ﬂexibility in   message content at the trade - off of requiring spec-   iﬁcation of the message with a small number of   keystrokes .   Future directions . We found ﬁne - tuning to be   signiﬁcantly better than prompting in terms of ( a )   accuracy ( for both scenarios with and without typo-   noise ) and also ( b ) exhibit lower latency as we   achieve better results with fewer samples . Future   work should investigate the differences in laten-   cies between the encoder - decoder architecture and   decoder - only models . For training efﬁciency , in - stead of ﬁne - tuning , it will also be worth investigat-   ing strategies such as prompt tuning ( Lester et al . ,   2021 ) that continue to keep the model frozen , but   learn some additional parameters for the task .   Even in the best case scenario models can fail   to ﬁnd accurate expansionsamong the top-5 op-   tions . Recovering from such failures is important   for AAC use cases . Future studies should con-   sider options for partial speciﬁcations of one or   more words or selection of some words from the   available options . Once the recovery from failure   is proven in ofﬂine analysis , user studies are re-   quired to validate and quantify the actual beneﬁt   of the AE text - entry paradigm in lab and real - life   settings . Integration with UI approaches is also   an essential direction , e.g. , speeding up eye - gaze   typing such as cascading dwell time and dwell - free   paradigms ( Mott et al . , 2017 ; Kristensson and Ver-   tanen , 2012 ) .   7 Conclusion   In this work we proposed a high - KSR form   of abbreviation expansion to dramatically save   keystrokes for severely - disabled users . We use it to   synthesize three datasets for the AE task . Based on   extensive experiments using few - shot prompting   and model tuning we demonstrate that across the   datasets , ﬁne - tuned LLMs can accurately predict   expansions for 48 - 77 % of phrases that are replies   to initial turns of dialogs and exhibit KSRs in the   range of 73 - 77 % for the correctly predicted expan-   sions , thus pointing at a promising direction for   future user studies of contextual and abbreviated   text entry based on LLMs . Models evaluated with   conversation context show signiﬁcantly higher ac-   curacy than without , thus supporting our hypothesis   that context is the key to effective abbreviated text   entry in conversational settings . Furthermore , ﬁne-   tuning with simulated typos substantially improves   tolerance to noise in abbreviation .   8 Acknowledgements   We would like to thank Shumin Zhai and Michael   Terry for feedback on a draft of this work , Yanping   Huang for pointers on model inference , as well as   James Stout , Bob MacDonald , Julie Cattiau , and   Maarten Bosma for their support . We are grateful   to Team Gleason for their active involvement and   feedback in the development of this work.12699 Ethical Considerations , Limitations ,   and Societal Impact   Accelerating augmentative and alternative com-   munication ( AAC ) can enhance quality of life of   people with extremely limited mobility by facili-   tating increased social participation and indepen-   dence ( Caligari et al . , 2013 ) . While the beneﬁts of   AE may be large for this population , we note that   this approach may have risks .   The primary risk of AE is errors in expansions   that substantially misrepresent the intent of the   speaker in a way that might cause harm to them-   selves or others ( e.g. , failure to correctly convey   critical health information , insertion of offensive   language . ) The abbreviation expansions may also   reﬂect biases in the underlying language model   ( e.g. , perpetuating stereotypes by more frequently   suggesting male pronouns than female , Weidinger   et al . 2021 . )   A more subtle risk is when expansions miss the   ground - truth phrase closely ( see Table 7 ) , which   may accurately convey content but reduce the   speaker ’s sense of autonomy and authentic self-   expression . Prior work ( e.g. , Kane et al . 2017 ) has   shown that people with ALS highly value AAC that   preserves and facilitates authentic identity expres-   sion . Providing speakers with multiple AE options   to choose from and requiring user conﬁrmation be-   fore voicing an expansion are design options that   can mitigate these risks . Model ﬁne - tuning to im-   prove safety or personalization to the end - user ’s   communication style are additional risk - mitigation   approaches .   Beyond enhancing communication speed , an-   other intended beneﬁt of AE is the potential to   reduce fatigue associated with gaze - based AAC by   reducing keystrokes ; however , a risk of our sys-   tem is that if errors in AE are frequent for a given   user ( perhaps due to eye tracker miscalibration or   long - tail abbreviation use ) then these savings could   be outweighed by the need to correct errors , inad-   vertently increasing fatigue . User studies to bet-   ter understand error rates in practice , as well as   future work designing interfaces to simplify AE   error correction , are important for minimizing this   risk . Similarly , our abbreviations scheme ’s simple   design based on ﬁrst letters aims to minimize cog-   nitive load ; however , user studies with the target   population using instruments such as NASA ’s Task   Load Indexwould be required to verify that AEdoes not cognitively strain end - users .   References127012711272Appendix   A Removal of duplicate dialogs from the   DailyDialog dataset   We observed that the DailyDialog dataset ( Li et al . ,   2017 ) contains a signiﬁcant number of dialogs in   its dev ( validation ) and test splits that are identical   or nearly identical to the dialogs found in its train   split . We determined two dialogs to be duplicate   by using the following criterion :   1.If both dialogs consist of the same number   of turns and the corresponding turns are all   identical ( case - insensitive ) , or   2.If both dialogs consist of the same number   of turns and there are three or more turns at   which both dialogs contain identical text ( case-   insensitive ) .   See the ﬁle daily_dialog_deduplications.csv in   Supplemental Data for a list of the 177 dialogs in   the dev split and the 228 dialogs in the test splits   that are found to be duplicates with the train split   and hence are removed from our DailyDialog Cor-   rected ( DDC ) dataset .   B 4- shot examples for BaseLLM   We select four consecutive dialogues from the 859   examples from train split of the TDC dataset ( Verta-   nen , 2017 ) while varying the starting conversation ,   which yields 859−4 + 1 = 856 different 4 - shot   prompt sets .   C Tuning on noisy data vs. accuracy   Preliminary experiments have shown that σ= 0.3   is a good trade - off between accuracy gains on noisy   data and losses on non - noisy data .   D Model ﬁne - tuning details   Our model ﬁne - tuning uses the AdaFactor opti-   mizer ( Shazeer and Stern , 2018 ) . The nominal   batch size 16 is made more efﬁcient through ex-   ample packing ( Raffel et al . , 2019 ) , leading to an   average effective batch size of approximately 200   examples under a maximum sequence length of   1024 tokens . We used TPUv3s ( Jouppi et al . , 2018 )   with a conﬁguration of 4x8 for the LLM ﬁne - tuning .   Our ﬁne - tuning recipe applies a constant , low learn-   ing rate of 5×10and a dropout rate of 0.2 , which   helps to prevent early overﬁtting . Early stopping is   based on a dev set consisting of combined examplesfrom the dev splits of the TAC and TDC datasets .   We ﬁnd the best checkpoint after 2100 and 1800   training steps for the FT - LLM and FTnoise - LLM   models , respectively , which amounts to approxi-   mately 1 - 1.2 epochs of training . We ran a small   set of hyperparameter tuning experiments , varying   batch size , learning rate and dropout and chose the   best setting based on the TAC + TDC dev set .   E Computation cost   Fine - tuning of the 64B LLM uses TPU v3 with a   4x8 conﬁguration , i.e. , 32 TPUs . FT - LMM and   FTnoise - LLM are each trained for approximately   2100 and 1800 steps , respectively . The training   time is approximately 3 hours . This leads to a   model ﬁne - tuning budget of 32 x 3 = 96 TPU *   hour per model .   Evaluation and inference on the 64B LLM uses   TPU v3 with a 4x4 conﬁguration , i.e. , 16 TPUs .   Each example ( batch size = 128 samples ) takes   0.653 s. This leads to 16×0.568/128= 0.071   TPU×second per sample .   F Splitting characters in abbreviations .   Pilot experiments showed the importance of pro-   grammatically inserting spaces between characters   in the abbreviations . Since the vocabulary used by   the LaMDA models is fairly large ( 32k entries ) ,   unless we enforce character - level splitting , subse-   quences of multiple characters in many abbrevia-   tions will be combined into spurious tokens , lead-   ing to slightly reduced AE accuracy .   G Recovery from failure - analysis   In the best scenario of replying to a question , the   ﬁne - tuned LLM is capable of predicting the correct   phrase expansion approximately 81 % of the times   with top-5 options and sufﬁcient sampling ( Figure   5 ) . Hence the model will fail to ﬁnd the correct   expansion at least 19 % of the cases .   H Inference latencies of different   LaMDA model sizes   In Figure H we compare the latencies during infer-   ence time for the decoder - only models of different   sizes . Compared to the 4B model , the 27B model   shows 1.5x latency , while the 64B model shows   2.2x latency . While the latency increase is quite   signiﬁcant , this analysis shows that we can not sub-   stitute the 64B model with a smaller model ( e.g. ,12731274   by increasing the number of samples ) in a way that   improves latency without signiﬁcantly harming the   AE accuracy ( compare the AE accuracies in Fig-   ure 5.)1275