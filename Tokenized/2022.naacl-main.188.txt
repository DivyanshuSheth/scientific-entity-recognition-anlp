  Hadas OrgadSeraphina Goldfarb - TarrantYonatan BelinkovTechnion – Israel Institute of TechnologyUniversity of Edinburgh   orgad.hadas@cs.technion.ac.il s.tarrant@ed.ac.uk   belinkov@technion.ac.il   Abstract   Common studies of gender bias in NLP focus   either on extrinsic bias measured by model per-   formance on a downstream task or on intrinsic   bias found in models ’ internal representations .   However , the relationship between extrinsic   and intrinsic bias is relatively unknown . In   this work , we illuminate this relationship by   measuring both quantities together : we debias   a model during downstream fine - tuning , which   reduces extrinsic bias , and measure the effect   on intrinsic bias , which is operationalized as   bias extractability with information - theoretic   probing . Through experiments on two tasks   and multiple bias metrics , we show that our   intrinsic bias metric is a better indicator of de-   biasing than ( a contextual adaptation of ) the   standard WEAT metric , and can also expose   cases of superficial debiasing . Our framework   provides a comprehensive perspective on bias   in NLP models , which can be applied to deploy   NLP systems in a more informed manner .   1 Introduction   Efforts to identify and mitigate gender bias in Nat-   ural Language Processing ( NLP ) systems typically   target one of two notions of bias . Extrinsic evalua-   tion methods and debiasing techniques focus on the   bias reflected in a downstream task ( De - Arteaga   et al . , 2019 ; Zhao et al . , 2018 ) , while intrinsic   methods focus on a model ’s internal representa-   tions , such as word or sentence embedding geom-   etry ( Caliskan et al . , 2017 ; Bolukbasi et al . , 2016 ;   Guo and Caliskan , 2021 ) . Despite an abundance   of evidence pointing towards gender bias in pre-   trained language models ( LMs ) , the extent of harm   caused by these biases is not clear when it is not   reflected in a specific downstream task ( BarocasFigure 1 : Our proposed framework . Black arrows mark   forward passes , red arrows mark things we measure . We   first ( a ) train a model on a downstream task , then ( b )   train another model on the same task using a debiased   dataset , and finally ( c ) measure intrinsic bias in both   models and compare .   et al . , 2017 ; Kate Crawford , 2017 ; Blodgett et al . ,   2020 ; Bommasani et al . , 2021 ) . For instance , while   the word embedding proximity of “ doctor ” to “ man ”   and “ nurse ” to “ woman ” is intuitively normatively   wrong , it is not clear when such phenomena would   lead to downstream predictions manifesting in so-   cial biases . Recently , Goldfarb - Tarrant et al . ( 2021 )   have shown that debiasing static embeddings in-   trinsically is not correlated with extrinsic gender   bias measures , but the nature of the reverse relation-   ship is unknown : how are extrinsic interventions   reflected in intrinsic representations ? Furthermore ,   Gonen and Goldberg ( 2019a ) demonstrated that a   number of intrinsic debiasing methods applied to   static embeddings only partially remove the bias   and that most of it is still hidden within the embed-2602ding . Complementing their view , we examine ex-   trinsic debiasing methods , as well as demonstrate   the possible harm this could cause . Contrary to   their conclusion , we do not claim that these debias-   ing methods should not be trusted , as long as they   are utilized with care .   Our goal is to gain a better understanding of the   relationship between a model ’s internal represen-   tations and its extrinsic gender bias by examining   the effects of various debiasing methods on the   model ’s representations . Specifically , we fine - tune   models with and without gender debiasing strate-   gies , evaluate their external bias using various bias   metrics , and measure intrinsic bias in the represen-   tations . We operationalize intrinsic bias via two   metrics : First , we use CEAT ( Guo and Caliskan ,   2021 ) , a contextual adaptation of the widely used   intrinsic bias metric WEAT ( Caliskan et al . , 2017 ) .   Second , we propose to use an information - theoretic   probe to quantify the degree to which gender can be   extracted from the internal model representations .   Then , we examine how these intrinsic metrics corre-   late with a variety of extrinsic bias metrics that we   measure on the model ’s downstream performance .   Our approach is visualised in Figure 1 .   We perform extensive experiments on two down-   stream tasks ( occupation prediction and corefer-   ence resolution ) ; several debiasing strategies that   involve alterations to the training dataset ( such as   removing names and gender indicators , or balanc-   ing the data by oversampling or downsampling ) ;   and a multitude of extrinsic bias metrics . Our anal-   ysis reveals new insights into the way language   models encode and use information on gender :   •The effect of debiasing on internal represen-   tations is reflected in gender extractability ,   while not always in CEAT . Thus , gender ex-   tractability is a more reliable indicator of gen-   der bias in NLP models .   •In cases of high gender extractability but low   extrinsic bias metrics , the debiasing is super-   ficial , and the internal representations are a   good indicator for this : The bias is still present   in internal representations and can be restored   by retraining the classification layer . There-   fore , our proposed measuring method can help   in detecting such cases before deploying the   model .   •The two tasks show different patterns of cor-   relation between intrinsic and extrinsic bias . The coreference task exhibits a high correla-   tion . The occupation prediction task exhibits a   lower correlation , but it increases after retrain-   ing ( a case of superficial debiasing ) . Gender   extractability shows higher correlations with   extrinsic metrics than CEAT , increasing the   confidence in this metric as a reliable measure   for gender bias in NLP models .   2 Methodology   In this study , we investigate the relationship be-   tween extrinsic bias metrics of a task and a model ’s   internal representations , under various debiasing   conditions , for two datasets in English . We perform   extrinsic debiasing , evaluate various extrinsic and   intrinsic bias metrics before and after debiasing ,   and examine correlations .   Dataset . LetD={X , Y , Z}be a dataset con-   sisting of input data X , labels Yand protected   attributes Z.This work focuses on gender as the   protected attribute z. In all definitions , FandM   indicate female and male gender , respectively , as   the value of the protected attribute z.   Trained Model . The model is optimized to solve   the downstream task posed by the dataset . It can   be formalized as f ◦ g : X→R , where g(·)is   the feature extractor , implemented by a language   model , e.g. , RoBERTa ( Liu et al . , 2019 ) , f(·)is   the classification function , and Yis the set of the   possible labels for the task .   2.1 Bias Metrics   Each bias evaluation method described in the lit-   erature can be categorized as extrinsic or intrinsic .   In all definitions , rindicates the model ’s output   probabilities .   2.1.1 Extrinsic Metrics   Extrinsic methods involve measuring the bias of a   model solving a downstream problem . The extrin-   sic metric is a function :   E(X , Y , R , Z)∈R   The output represents the quantity of bias mea-   sured ; the further from 0 the number is , the larger   the bias is . Our analysis comprises a wide range2603of extrinsic metrics , including some that have been   measured in the past on the analyzed tasks ( Zhao   et al . , 2018 ; De - Arteaga et al . , 2019 ; Ravfogel et al . ,   2020 ; Goldfarb - Tarrant et al . , 2021 ) and some that   have never been measured before , and shows our   results apply to many of them . For illustration ,   we will consider occupation prediction , a common   task in research on gender bias ( De - Arteaga et al . ,   2019 ; Ravfogel et al . , 2020 ; Romanov et al . , 2019 ) .   The input xis a biography and the prediction yis   the profession of the person described in it . The   protected attribute zis the gender of that person .   Performance gap . This is the difference in per-   formance metric for two different groups , for in-   stance two groups of binary genders , or a group of   pro - stereotypical and a group of anti - stereotypical   examples . We measure the following metrics : True   Positive Rate ( TPR ) , False Positive Rate ( FPR ) , and   Precision . In occupation prediction , for instance ,   the TPR gap for each profession yexpresses the   difference in the percentage of women and men   whose profession is yand are correctly classified   as such . We also measure F1 of three standard   clustering metrics for coreference resolution . Each   such performance gap captures a different facet of   gender bias , and one might be more interested in   one of the metrics depending on the application .   We compute two types of performance gap met-   rics : ( 1 ) the sum of absolute gap values over all   classes ; ( 2 ) the Pearson correlation between the   performance gap for a class and the percentage of   women in that class . For instance , if yis a pro-   fession , we measure the correlation between per-   formance gaps and percentages of women in each   profession . The two metrics are closely related but   answer slightly different questions : the sum quanti-   fies how a model behaves differently on different   genders , and the correlation shows the relation of   model behaviour to social biases ( in the world or   the data ) without regard to actual gap size .   Statistical metrics . For breadth of analysis , we   examine three additional statistical metrics ( Baro-   cas et al . , 2019 ) , which correspond to different no-   tions of bias . All three are measured as differences   ( d ) between two probability distributions , and we   then obtain a single bias quantity per metric by   summing all computed distances.•Independence : d / parenleftbig   P(r|z = z ) , P(r)/parenrightbig   ∀z∈   { F , M } . For instance , we measure the difference   between the distribution of model ’s predictions   on women and the distribution of all predictions .   Independence is stronger as the prediction ris   less correlated with the protected attribute z. It   is measured with no relation to the gold labels .   •Separation : d / parenleftbig   P(r|y = y , z = z ) , P(r|y = y)/parenrightbig   ∀y∈ Y , z∈ { F , M } . For instance , we mea-   sure the difference between the distribution of a   model ’s predictions on women who are teachers   and the distribution of predictions on all teachers .   It encapsulates the TPR and FPR gaps discussed   previously , and can be seen as a more general   metric .   •Sufficiency : d / parenleftbig   P(y|r = r , z = z ) , P(y|r = r)/parenrightbig   .   For instance , we measure the difference between   the distribution of gold labels on women classi-   fied as teachers by the model and the distribu-   tion of gold labels on all individuals classified   as teachers by the model . Sufficiency relates to   the concept of calibration in classification . A dif-   ference in the classifier ’s scores for men and for   women indicates that it might be penalizing or   over - promoting one of the genders .   2.1.2 Intrinsic Metrics   Intrinsic methods are applied to the representation   obtained from the feature extractor . These meth-   ods are independent of any downstream task . The   intrinsic metric is a function :   I(g(X),Z)∈R   Compression . Our main intrinsic metric is the   compression of gender information evaluated by a   minimum description length ( MDL ) probing clas-   sifier ( V oita and Titov , 2020 ) , trained to predict   gender from the model ’s representations . Probing   classifiers are widely used for predicting various   properties of interest from frozen model represen-   tations ( Belinkov and Glass , 2019 ) . MDL probes   were proposed because a probe ’s accuracy may be   misleading due to memorization and other issues   ( Hewitt and Liang , 2019 ; Belinkov , 2021 ) . We use   the MDL online code , where the probe is trained in   timesteps , on increasing subsets of the training set ,   then evaluated against the rest of it . Higher com-   pression indicates greater gender extractability .   CEAT . We also measure CEAT ( Guo and   Caliskan , 2021 ) , which is a contextualized version2604of WEAT ( Caliskan et al . , 2017 ) , a widely used   bias metric for static word embeddings . WEAT de-   fines sets XandYof target words , and sets Aand   Bof attribute words . For instance , AandBcontain   males and females names , while XandYcontain   career and family related words , respectively . The   bias is operationalized as the geometric proximity   between the target and attribute word embeddings ,   and is quantified in CEAT by the Combined Effect   Size ( CES ) and a p - value for the null hypothesis of   having no biased associations . For more informa-   tion on CEAT refer to Appendix A.4.3 .   2.2 Debiasing Techniques   We debias models by modifying the downstream   task ’s training data before fine - tuning . Scrub-   bing ( De - Arteaga et al . , 2019 ) removes first names   and gender - specific terms ( “ he ” , “ she ” , “ husband ” ,   “ wife ” , “ Mr ” , “ Mrs ” , etc . ) . Balancing subsamples   or oversamples examples such that each gender is   equally represented in the resulting dataset w.r.t   each label . Anonymization ( Zhao et al . , 2018 ) re-   moves named entities . Counterfactual Augmenta-   tion ( Zhao et al . , 2018 ) involves replacing male   entities in an example with female entities , and   adding the modified example to the training set .   As some of these are dataset / task - specific , we give   more details in the following section .   3 Experiments   In each experiment , we fine - tune a model for a   downstream task . For training , we use either the   original dataset or a dataset debiased with one of   the methods from Section 2.2 . Figure 2 presents   examples of debiasing methods for the two down-   stream tasks . We measure two intrinsic metrics by   probing that model ’s inner representations for gen-   der extractability ( as measured by MDL ) and by   CEAT , and test various extrinsic metrics . The rela-   tion between one intrinsic and one extrinsic metric   becomes one data point , and we repeat over many   random seeds ( for both the model and the probe ) .   Further implementation details are in appendix A.   3.1 Occupation Prediction   The task of occupation prediction is to predict a   person ’s occupations ( from a closed set ) , based on   their biography . We use the Bias in Bios dataset   ( De - Arteaga et al . , 2019 ) . Regardless of the train-   ing method , the test set is subsampled such that   each profession has equal gender representation .   Model . Our main model is a RoBERTa model   ( Liu et al . , 2019 ) topped with a linear classifier ,   which receives the [ CLS ] token embedding as in-   put and generates a probability distribution over the   professions . In addition , we experiment with train-   ing a baseline classifier layer on top of a frozen ,   non - finetuned RoBERTa . We also replicate our   RoBERTa experiments with a DeBERTa model ( He   et al . , 2020 ) , to verify that our results are are not   model specific and hold more broadly .   Debiasing Techniques . Following De - Arteaga   et al . ( 2019 ) we experiment with scrubbing the   training dataset . Figure 2 shows an example biog-   raphy snippet and its scrubbed version . We also   conduct balancing ( per profession , subsampling   and oversampling to ensure an equal number of   males and females per profession ) , which has not   previously been used on this dataset and task .   Metrics . We measure all bias metrics from Sec-   tion 2.1 except for F1 .   Probing . The probing dataset for this task is the   test set , and the gender label of a single biography   is the gender of the person described in it . We probe   the [ CLS ] token representation of the biography . In   addition to the models described above , we mea-   sure baseline extractability of gender information   from a randomly initialized RoBERTa model .   3.2 Coreference Resolution   The task of coreference resolution is to find all tex-   tual expressions referring to the same real - world   entities . We train on Ontonotes 5.0 ( Weischedel   et al . , 2013 ) and test on the Winobias challenge   dataset ( Zhao et al . , 2018 ) . Winobias consists of   sentence pairs , pro- and anti - stereotypical variants ,   with individuals referred to by their profession . For   example , “ The physician hired the secretary be-2605   cause he / she was busy . ” is pro / anti - stereotypical ,   based on US labor statistics . A coreference sys-   tem is measured by the performance gap between   the pro- and anti - stereotypical subsets .   Model . We use the model presented in Lee et al .   ( 2018a ) with RoBERTa as a feature extractor .   Debiasing Techniques . Following Zhao et al .   ( 2018 ) , we apply anonymization ( denoted as Anon )   and counterfactual augmentation ( CA ) on the train-   ing set . These techniques were used jointly in pre-   vious work ; we examine each individually as well .   Metrics . Following Zhao et al . ( 2018 ) , we mea-   sure the F1 difference between anti- and pro-   stereotypical examples . We also interpret the task   as a classification problem , and measure all met-   rics from Section 2.1 . For more details refer to   Appendix A.4.2 .   Probing . We probe the representation of a pro-   fession word as extracted from Winobias sentences , after masking out the pronouns . We define a pro-   fession ’s gender as the stereotypical gender for this   profession . To prevent memorization by the probe —   given the small number of professions — the dataset   is sorted so that professions are gradually added to   the training set , so a success on the validation set   is on previously unseen professions .   4 Results   Tables 1a and 1b present intrinsic and extrinsic   metrics for RoBERTa models on the occupation   prediction and coreference resolution tasks , respec-   tively . We present a representative subset of the   measured metrics that demonstrate the observed   phenomena ; full results are found in Appendix B.   The DeBERTa model results are consistent with   the RoBERTa model trends .   4.1 Compression Reflects Debiasing Effects   As shown in the tables , compression captures dif-   ferences in models that were debiased differently .   CEAT , however , can not differentiate between oc-   cupation prediction models . For example , in occu-   pation prediction ( Table 1a ) the compression rate2606varies significantly between a non - debiased and a   debiased model via scrubbing and oversampling ,   while CEAT detects no difference between the mod-   els . In coreference resolution ( Table 1b ) , both com-   pression and CEAT are able to identify differences   between the non - debiased model and the others ,   such as CA , which has both a lower compression   and CEAT effect . But the CEAT effect sizes are   small ( below 0.5 ) , which implies no bias , in con-   trast to the extrinsic metrics .   4.2 High Gender Extractability Implies   Superficial Debiasing   Extrinsic and intrinsic effects of debiasing . In   occupation classification ( Table 1a ) , somewhat sur-   prisingly , subsampling the training data has the   strongest effect on extrinsic metrics , but not on   compression rate . Scrubbing reduces both intrinsic   and extrinsic metrics , although its effect on extrin-   sic metrics is limited compared to subsampling .   Training with oversampling caused less reduction   in extrinsic bias metrics . A consequence of over-   sampling is that some metrics are less biased , but   compression rates are increased , so gender infor-   mation is more accessible . The effectiveness of   subsampling over other metrics is further discussed   in appendix C. In coreference resolution ( Table 1b ) ,   while both CA and CA with anonymization reduced   gender extractability as well as external bias met-   rics , anonymization alone increased intrinsic bias   without affecting external bias metrics significantly .   Debiasing without fine - tuning . As the effect on   extrinsic bias did not match the effect on intrinsic   bias in several cases , we examined the role of the   classification layer . We trained a model for occupa-   tion prediction without fine - tuning the underlying   RoBERTa model . Training on a subsampled dataset   also reduced the extrinsic metrics ( 0.15 , 0.03 , 0.20 ,   and 0.31 , respectively , on TPR gaps Pearson , FPR   gaps sum , separation sum , and sufficiency sum ) .   Detailed results of this experiment can be found in   Appendix B. Since no updates were made to the   LM , the internal representations could not be debi-   ased , thus the debiasing observed in this model can   only be superficial .   Retraining the classification layer . Fine - tuning   of both tasks revealed that lower extrinsic metrics   did not always lead to lower compression . Does   this indicate cases where the debiasing process is   only superficial , and the internal representations   remain biased ? To test this hypothesis , we froze thepreviously fine - tuned LM ’s weights , and retrained   the classification layer . We used the original ( non-   debiased ) training set for retraining .   Tables 1a and 1b also compare extrinsic metrics   before and after retraining . All models show bias   restoration , due to the classification layer being   trained on the biased dataset . The amount of bias   restored varies between models in a way that is   predictable by the compression metric .   In the occupation prediction task , comparing Be-   fore and After numbers in Table 1a , the model   fine - tuned using a scrubbed dataset — which has the   lowest compression rate — displays the least bias   restoration , confirming that the LM absorbed the   process of debiasing . The model fine - tuned on sub-   sampled data has higher extrinsic bias after retrain-   ing . Hence , the debiasing was primarily cosmetic ,   and the representations within the LM were not   debiased . The model fine - tuned on oversampled   data — which has the highest compression — has the   highest extrinsic bias ( except for FPR ) , even though   this was not true before retraining .   In coreference resolution , comparing Before and   After numbers in Table 1b , models with the least   extrinsic bias ( CA and CA+Anon ) are also least   biased after retraining . Compression rate predicted   this ; these models also had lower compression rates   than non - debiased models . Interestingly , the model   fine - tuned with an anonymized dataset is the most   biased after retraining , consistent with its high com-   pression rate relative to the other models . As with   subsampling and oversampling in occupation pre-   diction , anonymization ’s ( lack of ) effect on extrin-   sic metrics was cosmetic ( compare None and Anon   in Before block , Table 1b ) . Anonymization actu-   ally had a biasing effect on the LM , which was   realized after retraining .   We conclude that compression rate is a useful in-   dicator of superficial debiasing , and can potentially   be used to verify and gain confidence in attempts   to debias an NLP model , especially when there is   little or no testing data .   4.3 Correlation between Extrinsic and   Intrinsic Metrics   Table 2 shows correlations between compression   rate and various extrinsic metrics before and after2607Occupation Classification Coreference Resolution   RCompression RCEAT RCompression RCEAT   Metric Before After Before After Before After Before After   F1 diff ( pro−anti ) - - - - 0.821 0.709 0.246 0.005   TPR gap ( P ) 0.046 0.304 0.042 0.049 0.222 0.006 0.008 0.012   TPR gap ( S ) 0.049 0.449 0.022 0.036 0.817 0.752 0.297 0.003   FPR gap ( P ) 0.001 0.120 0.008 0.002 0.021 0.054 0.002 0.000   FPR gap ( S ) 0.353 0.046 0.079 0.001 0.844 0.773 0.263 0.004   Precision gap ( P ) 0.032 0.173 0.000 0.000 0.068 0.038 0.019 0.000   Precision gap ( S ) 0.174 0.529 0.000 0.021 0.849 0.774 0.268 0.006   Independence gap ( S ) 0.251 0.382 0.050 0.005 0.778 0.732 0.355 0.001   Separation gap ( S ) 0.066 0.165 0.046 0.009 0.835 0.776 0.261 0.005   Sufficiency gap ( S ) 0.202 0.567 0.040 0.034 0.825 0.753 0.287 0.002   retraining . In occupation prediction , certain extrin-   sic metrics have a weak correlation with compres-   sion rate , while others do not . Except one metric   ( FPR gap sum ) , the compression rate and the extrin-   sic metric correlate more after retraining . Figure 3   illustrates this for TPR - gap ( Pearson ) . The increase   is due to superficial debiasing , especially by sub-   sampling data , which prior to retraining had low   extrinsic metrics and relatively high intrinsic met-   rics . This shows that correlation between extrinsic   metrics and compression rate for certain metrics   is stronger than it appeared before retraining . It is   unsurprising that CEAT does not correlate with any   extrinsic metrics , since CEAT could not distinguish   between different models .   Coreference resolution shows stronger correla-   tions between compression rate and extrinsic met - rics , but low correlations between Pearson metrics .   We further discuss cases of no correlation in ap-   pendix D. Correlations decrease after retraining ,   but metrics that were highly correlated remain so   ( > 0.7after retraining ) . The correlations are visu-   alized for F1 difference metrics in Figure 4 . CEAT   and extrinsic metrics correlate much less than com-   pression rate ( Table 2 ) . Our results are in line with   those of Goldfarb - Tarrant et al . ( 2021 ) , who found   a lack of correlation between extrinsic metrics and   WEAT , the static - embedded version of CEAT .   Given that recent work ( Goldfarb - Tarrant et al . ,   2021 ; Cao et al . , 2022 ) questions the validity of   intrinsic metrics as a reliable indicator for gender   bias , the compression rate provides a reliable al-   ternative to current intrinsic metrics , by offering   correlation to many extrinsic bias metrics.2608   5 Related Work   There are few studies that examine both intrinsic   and extrinsic metrics . Previous work by Goldfarb-   Tarrant et al . ( 2021 ) showed that debiasing static   embeddings intrinsically is not correlated with ex-   trinsic bias , challenging the assumption that intrin-   sic metrics are predictive of bias . We examine the   other direction , exploring how extrinsic debiasing   affects intrinsic metrics . We also extend beyond   their work to contextualized embeddings , a wider   range of extrinsic metrics , and a new , more effec-   tive intrinsic metric based on information - theoretic   probing . A contemporary work by Cao et al . ( 2022 )   measured the correlations between intrinsic and   extrinsic metrics in contextualized settings across   different language models . In contrast , our work   examines the correlations across different versions   of the same language model by fine - tuning it using   various debiasing techniques .   Studies that inspect extrinsic metrics include ei-   ther a challenge dataset curated to expose differ-   ences in model behavior by gender , or a test dataset   labelled by gender . Among these datasets are Wino-   bias ( Zhao et al . , 2018 ) , Winogender ( Rudinger   et al . , 2018 ) and GAP ( Webster et al . , 2018 ) for   coreference resolution , WinoMT ( Stanovsky et al . ,   2019 ) for machine translation , EEC ( Kiritchenko   and Mohammad , 2018 ) for sentiment analysis ,   BOLD ( Dhamala et al . , 2021 ) for language gen-   eration , gendered NLI ( Sharma et al . , 2020 ) for   natural language inference and Bias in Bios ( De-   Arteaga et al . , 2019 ) for occupation prediction .   Studies that measure gender bias intrinsically   in static word or sentence embeddings measure   characteristics of the geometry , such as the prox - imity between female- and male - related words to   stereotypical words , or how embeddings cluster   or relate to a gender subspace ( Bolukbasi et al . ,   2016 ; Caliskan et al . , 2017 ; Gonen and Goldberg ,   2019b ; Ethayarajh et al . , 2019 ) . However , metrics   and debiasing methods for static embeddings do   not apply directly to contextualized ones . Several   studies use sentence templates to adapt to contex-   tual embeddings ( May et al . , 2019 ; Kurita et al . ,   2019 ; Tan and Celis , 2019 ) . This templated ap-   proach is difficult to scale , and lacks the range of   representations that a contextual embedding offers .   Other work extracts embedding representations of   words from natural corpora ( Zhao et al . , 2019 ; Guo   and Caliskan , 2021 ; Basta et al . , 2019 ) . These   studies often adapt the WEAT method ( Caliskan   et al . , 2017 ) , which measures embedding geometry .   None measure the effect of the presumably found   “ bias ” on a downstream task .   There is a growing conversation in the field   ( Barocas et al . , 2017 ; Kate Crawford , 2017 ; Blod-   gett et al . , 2020 ; Bommasani et al . , 2021 ) about the   importance of articulating the harms of measured   bias . In general , extrinsic metrics have clear , in-   terpretable impacts for which harm can be defined .   Intrinsic metrics have an unclear effect . Without   evidence from a concrete downstream task , a found   intrinsic bias is only theoretically harmful . Our   work is a step towards understanding whether in-   trinsic metrics provide valuable insights about bias   in a model .   6 Discussion and Conclusions   This study examined whether bias in internal repre-   sentations is related to extrinsic bias . We designed2609a new framework in which we debias a model on   a downstream task , and measure its intrinsic bias .   We found that gender extractability from internal   representations , measured by compression rate via   MDL probing , reflects bias in a model . Compres-   sion was much more reliable than an alternative   intrinsic metric for contextualised representations ,   CEAT . Compression correlated well — to varying   degrees — with many extrinsic metrics . We thus   encourage NLP practitioners to use compression   as an intrinsic indicator for gender bias in NLP   models . When comparing two alternative models ,   a lower compression rate provides confidence in a   model ’s superiority in terms of gender bias . The   relative success of compression over CEAT may   be because the compression rate was calculated on   the same dataset as the extrinsic metrics , whereas   CEAT was measured on a different dataset not nec-   essarily aligned with a specific downstream task .   The use of a non - task - aligned dataset is a common   strategy among other intrinsic metrics ( May et al . ,   2019 ; Kurita et al . , 2019 ; Basta et al . , 2021 ) . An-   other possible explanation is that compression rate   measures a more focused concept , namely the gen-   der information within the internal representations .   CEAT measures proximity among embeddings of   general terms that may include other social contexts   that do not directly relate to gender ( e.g. a female   term like ‘ lady ’ or ‘ Sarah ’ contains information   about not just gender but class , culture , formality ,   etc , and it can be hard to isolate just one of these   from the rest ) .   Our results show that when a debiasing method   reduces extrinsic metrics but not compression , it   indicates that the language model remains biased .   When such superficial debiasing occurs , the debi-   ased language model may be reapplied to another   task , as in Jin et al . ( 2021 ) , resulting in unexpected   biases and nullifying the supposed debiasing . Our   findings suggest that practitioners of NLP should   take special care when adopting previously debi-   ased models and inspect them carefully , perhaps   using our framework . Our results differ from those   of Mendelson and Belinkov ( 2021a ) , who found   that the debiasing increases bias extractability as   measured by compression rate . However , they stud-   ied different , non - social biases , that arise from spu-   rious or unintended correlations in training datasets   ( often called dataset biases ) . In our case , some   debiasing strategies increase intrinsic bias while   others decrease it . Future work could investigatewhy debiasing affects extractability differently for   these two types of biases .   Our work also highlighted the importance of the   classification layer . Using a debiased objective ,   such as a balanced dataset , the classification layer   can provide significant debiasing . This holds even   if the internal representations are biased and the   classifier is a single linear layer , as shown in the   occupation prediction task . Bias stems in part from   internal LM bias and in part from classification   bias . Practitioners should focus their efforts on   both parts when attempting to debias a model .   We used a broader set of extrinsic metrics than   is typically used , and found that the bias metrics   behaved differently : some decreased more than oth-   ers after debiasing , and they correlated differently   with compression rate . Debiasing efforts may not   be fully understood by testing only a few extrin-   sic metrics . However , compression as an intrinsic   bias metric can indicate meaningful debiasing of   internal model representations even when not all   metrics are easily measurable , since it correlates   well with many extrinsic metrics .   A major limitation of this study is the use of gen-   der as a binary variable , which is trans - exclusive .   Cao and Daumé III ( 2020 ) made the first steps   towards inclusive gender bias evaluation in NLP ,   revealing that coreference systems fail on gender-   inclusive text . Further work is required to adjust   our framework to non - binary genders , potentially   revealing insights about the poor performance of   NLP systems in that area .   Acknowledgements   This research was supported by the ISRAEL SCI-   ENCE FOUNDATION ( grant No . 448/20 ) and by   an Azrieli Foundation Early Career Faculty Fellow-   ship . We also thank Kate McCurdy and Andreas   Grivas for comments on early drafts , the members   of the Technion CS NLP group for their valuable   feedback , and the anonymous reviewers for their   useful suggestions .   References2610261126122613A Implementation Details   We used RoBERTa in all models ( base size , 120 M   parameters ) . We use following random seeds in   all repeated experiments : 0 , 5 , 11 , 26 , 42 , 46 , 50 ,   63 , 83 , 90 . Our code was implemented mainly   using the Python libraries Pytorch ( Paszke et al . ,   2019 ) , Transformers ( Wolf et al . , 2020 ) , Sklearn   ( Pedregosa et al . , 2011 ) , and the experiments were   logged using Wandb ( Biewald , 2020 ) .   A.1 Occupation Classification   We fine - tuned a RoBERTa - base model with a lin-   ear classification layer on top . Training was done   for 10 epochs at a learning rate of 5e-5 , batch size   of 64 . The input to RoBERTa was the biography   tokens , which is limited to the first 128 tokens . The   resulting [ CLS ] token embedding is fed to the clas-   sifier to predict the occupation . The probing task   involves using the same [ CLS ] token and training   the probing classifier to predict the gender of the   person in the biography . The experiments without   fine - tuning included either a pre - trained or a pre-   viously fine - tuned RoBERTa . We first extracted   the pre - trained RoBERTa ’s embeddings of tokens   from the [ CLS ] and then trained a linear classi-   fier on them . The learning rate was 0.001 and the   batch size was 64 . We trained the classification   layer with pre - trained RoBERTa on 300 epochs ,   but with fine - tuned RoBERTa , 10 epochs were suf-   ficient . For all training processes , the epoch with   the greatest validation accuracy was saved . Fine-   tuning took 7 hours on a GeForce RTX 2080 Ti   GPU . Bias in Bios contains almost 400k biogra-   phies , and we obtain validation ( 10 % ) and test set   ( 25 % ) by splitting with Scikit - learn ’s ( Pedregosa   et al . , 2011 ) test_train_split with our random seeds .   A.2 Coreference Resolution   We use the implementation of Xu and Choi ( 2020 ) ,   a model that was introduced by Lee et al . ( 2018b )   and has been adopted by many coreference resolu-   tion models . Coreference resolution is the process   of clustering different mentions in a text that refer   to the same real - world entities . The task is solved   by detecting mentions through text spans and then   predicting for each pair of spans if they represent   the same entity . The span representations were ex-   tracted with a RoBERTa model , which is fine - tuned   throughout the training process , except in the re-   training experiment . Fine - tuning took 3 hours on   an NVIDIA RTX A6000 GPU . Ontonotes 5.0 has625k sentences and we use the standard validation   and test splits .   A.3 Probing Classifier   We use the MDL probe ( V oita and Titov , 2020 ) im-   plementation by Mendelson and Belinkov ( 2021b ) .   In all experiments , we use a linear probe and train   it with a batch size of 16 and a learning rate of   1e-3 . The timestamps used , meaning the accumu-   lating fractions of data that the probe is trained on ,   are 2.0 % , 3.0 % , 4.4 % , 6.5 % , 9.5 % , 14.0 % , 21.0 % ,   31.0 % , 45.7 % , 67.6 % , 100 % .   A.4 Metrics   A.4.1 Fairness - Based Metrics Implementation   All three statistical fairness metrics measure the   difference between two probability distributions ,   where this difference describes a notion of bias .   We calculate Independence and Separation via   Kullback – Leibler ( KL ) divergence , using the Al-   lenNLP implementation ( ) . We calculate Sufficiency via   Wasserstein distance instead , which is motivated   by Kwegyir - Aggrey et al . ( 2021 ) . In this case , we   can not use KL divergence , since there are some   classes that do not occur in model predictions for   both male and female genders . This causes the   probability distributions to not have the same sup-   port , and KL divergence is unbounded . Wasserstein   distance lacks the requirement for equal support .   A.4.2 Classification Metrics Interpretation in   Winobias   Winobias datasets contain pairs of stereotypical and   anti - stereotypical sentences . The stereotypes are   derived from the US labor statistics ( for instance , a   profession with a majority of males is stereotypi-   cally male ) . Since coreference resolution is viewed   as a clustering problem , it is usually measured via   clustering evaluation metrics . Coreference resolu-   tion is commonly measured as the average F1 score   of these , and the same is true for Winobias . Nev-   ertheless , coreference resolution is accomplished   by making a prediction for each pair of mentions ,   so it can be seen as a classification task . Wino-   bias can be viewed as a simpler task than general   coreference resolution , as it contains exactly two   mentions of professions and one pronoun , which   refers to exactly one profession . Therefore , we re-   frame it as a classification problem . In a Winobias   sentence with two professions xandy , as well as a   pronoun p , where pis referring to x , a true positive2614would be to cluster xandptogether , while a false   positive would be to cluster yandptogether . Our   classification metrics are derived based on these   definitions . For instance , the TPR gap for pro-   fession “ teacher ” , which is a stereotypical female   occupation , is the TPR rate on pro - stereotypical   sentences ( with a female pronoun ) minus the TPR   rate on anti - stereotypical sentences ( with a male   pronoun ) .   A.4.3 CEAT   The Word Embedding Association Test ( WEAT )   developed by ( Caliskan et al . , 2017 ) is a method   for evaluating bias in static word embeddings . The   test is defined as follows : given two sets of target   words X , Y(e.g . , ’ executive ’ , ’ management ’ , ’ pro-   fessional ’ and ’ home ’ , ’ parents ’ , ’ children ’ ) and   two sets of attribute words ( e.g. , male names and   female names ) , and using ⃗ wto represent the word   embedding for word w , the effect size is :   ES = means(x , A , B)−means(y , A , B )   where   s(x , A , B ) =   meancos(⃗x,⃗a)−meancos(⃗x,⃗b )   std - devs(w , A , B )   In essence , the effect size measures how differ-   ent are the distances between the embedding vec-   tors of each target group and the attribute groups .   Specifically , if s(x , A , B)>0,⃗xis more simi-   lar to attribute words Band vice versa . For in-   stance , a larger effect size is observed if target   words Xare more similar to attribute words Aand   target words Yare more similar to attribute words   B.|ES|>0.5and|ES|>0.8are considered   medium and large effect sizes , respectively ( Rice   and Harris , 2005 ) . The null hypothesis holds that   there is no difference between the two sets of target   words in terms of their relative similarity to the   two sets of attribute words , indicating that there are   no biased associations . Statistical significance is   defined by the p - value of WEAT , which reflects the   probability of observing the effect size under the   null hypothesis .   Since a word can take on a great variety of vector   representations in a contextual setting , ESvaries   according to the sentences used to extract word   representation . Thus , to adopt WEAT to contextu-   alized representations , the Combined Effect Size   ( CES ) ( Guo and Caliskan , 2021 ) is derived as thedistribution of WEAT effect sizes over many possi-   ble contextual word representations :   CES ( X , Y , A , B ) = /summationtextvES / summationtextv   where ESdenotes the WEAT effect size of the i’th   choice of word representations from a large corpus ,   andvis the inverse of the sum of in - sample vari-   anceVand between - sample variance in the distri-   bution of random - effects . As in Guo and Caliskan   ( 2021 ) , the representation for each word is derived   from 10,000 random sentences extracted from a   corpus of Reddit comments .   The combined effect size of each of the models   is examined on WEAT stimulus 6 , which contains   target words of career / family and attribute words   of male / female names . This was the only one that   detected bias on a pre - trained RoBERTa ( CES close   to 0.5 and p < 0.05 ) . The points that we kept in   our analysis are those where p < 0.05 , which make   up 90 % of the points in occupation prediction and   95 % of the points in coreference resolution .   B Full Results   In this section we provide the full results of a   RoBERTa model trained on the downstream task .   Table 3 presents results for the occupation pre-   diction task after fine - tuning , Table 4 presents the   retrained model results .   Figure 5 illustrates the correlations between ex-   trinsic metrics and compression rate before and   after retraining .   Table 5 presents the complete results for the oc-   cupation prediction task of the model trained with-   out fine - tuning , meaning that the RoBERTa model   is the pretrained version from Liu et al . ( 2019 )   and only the classification layer was updated . Sub-   sampling the dataset has significant debiasing ef-   fects , which suggests that this debiasing method   can achieve low extrinsic bias even when internal   bias exists . The Pearson correlation on precision ex-   hibits a different behavior . It makes sense nonethe-   less : precision is computed as TP\(TP+FP ) . A   biased model will assign more examples of a spe-   cific profession to a specific gender ( which aligns   with the percentage of biographies of this profes-   sion with this gender on the training set ) , increasing   bothTPandFPand decreasing precision . The   results on the coreference resolution task align with   the results of occupation prediction.2615Table 6 presents the results using a DeBERTa   model ( He et al . , 2020 ) for the occupation classi-   fication task . The trends are similar to those of   RoBERTa , with the same metrics showing an in-   crease , no change , or decrease in correlation after   re - training , suggesting a general trend in the behav-   ior of these metrics in relation to internal model   representations .   Table 7 displays the results on a finetuned model   for the coreference resolution task and Table 8 dis-   plays the retraining results .   Figure 6 shows the correlations between com-   pression rate and extrinsic metrics before and after   the retraining.2616Debiasing Strategy   Metric None Oversampling Subsampling Scrubbing   Compression 4.121 ±1.238 8.522 * ±2.354 3.568 ±1.516 1.699 * ±0.138   Accuracy 0.861±0.005 0.852 * ±0.004 0.861±0.003 0.851 * ±0.003   TPR gap ( P ) 0.763 ±0.071 0.729 ±0.067 0.319 * ±0.114 0.704 * ±0.068   TPR gap ( S ) 2.391 ±0.257 2.145 * ±0.220 1.598 * ±0.273 2.019 * ±0.262   FPR gap ( P ) 0.591 ±0.052 0.491 * ±0.059 0.087 * ±0.094 0.552 ±0.063   FPR gap ( S ) 0.075 ±0.010 0.085 * ±0.011 0.030 * ±0.006 0.057 * ±0.007   Precision gap ( P ) -0.880 ±0.031 -0.855 ±0.115 -0.299 * ±0.215 -0.815 * ±0.040   Precision gap ( S ) 3.621 ±0.337 3.401 ±0.667 1.549 * ±0.229 2.590 * ±0.279   Independence gap ( S ) 0.009 ±0.002 0.008 ±0.002 0.001 * ±0.000 0.005 * ±0.001   Separation gap ( S ) 0.327 ±0.051 0.305 ±0.030 0.204 * ±0.032 0.296 ±0.053   Sufficiency gap ( S ) 9.451 ±1.945 8.324 * ±1.537 1.218 * ±0.330 4.930 * ±0.927   Debiasing Strategy   Metric None Oversampling Subsampling Scrubbing   Compression 4.121 ±1.238 8.522 ±2.354 3.568 ±1.516 1.699 ±0.138   Accuracy 0.859 ±0.004 0.856 ±0.003 0.853 ±0.003 0.854 ±0.003   TPR gap ( P ) 0.777 ±0.047 0.813 * ±0.040 0.704 * ±0.075 0.714 * ±0.068   TPR gap ( S ) 2.482 ±0.238 2.593 * ±0.240 2.164 * ±0.284 1.989 * ±0.227   FPR gap ( P ) 0.596 ±0.041 0.603 ±0.047 0.602 ±0.041 0.536 * ±0.038   FPR gap ( S ) 0.073 ±0.008 0.068 * ±0.007 0.081 * ±0.012 0.059 * ±0.005   Precision gap ( P ) -0.877 ±0.027 -0.891 * ±0.023 -0.889 * ±0.035 -0.817 * ±0.058   Precision gap ( S ) 3.710 ±0.251 3.996 * ±0.272 3.555 * ±0.598 2.703 * ±0.255   Independence gap ( S ) 0.009 ±0.002 0.010 * ±0.002 0.009 ±0.003 0.005 * ±0.001   Separation gap ( S ) 0.334 ±0.050 0.328 ±0.048 0.300 * ±0.049 0.274 * ±0.041   Sufficiency gap ( S ) 9.701 ±1.305 10.908 * ±1.354 8.370 * ±2.558 5.239 * ±0.7982617Debiasing Strategy   Metric None Oversampling Subsampling Scrubbing   Accuracy 0.824 ±0.003 0.815 * ±0.005 0.831 * ±0.001 0.807 * ±0.003   TPR gap ( P ) 0.839 ±0.011 0.443 * ±0.053 0.158 * ±0.156 0.814 ±0.029   TPR gap ( S ) 3.088 ±0.192 1.545 * ±0.177 1.621 * ±0.088 3.154 ±0.332   FPR gap ( P ) 0.598 ±0.016 0.369 * ±0.029 0.067 * ±0.050 0.550 * ±0.012   FPR gap ( S ) 0.087 ±0.004 0.041 * ±0.004 0.027 * ±0.003 0.112 * ±0.005   Precision gap ( P ) -0.872 ±0.028 -0.427 * ±0.074 -0.161 * ±0.162 -0.853 ±0.019   Precision gap ( S ) 3.811 ±0.253 1.736 * ±0.108 1.551 * ±0.195 3.907 ±0.184   Independence gap ( S ) 0.014 * ±0.002 0.001 * ±0.000 0.000 * ±0.000 0.022 * ±0.001   Separation gap ( S ) 0.336 * ±0.044 0.214 * ±0.038 0.203 * ±0.024 0.432 * ±0.048   Sufficiency gap ( S ) 12.019 * ±1.721 2.105 * ±0.576 1.478 * ±0.394 13.798 * ±0.966   RCompression RCEAT   Metric Before After Before After   TPR gap ( P ) 0.023 0.120 0.051 0.006   TPR gap ( S ) 0.000 0.200 0.036 0.098   FPR gap ( P ) 0.083 0.153 0.121 0.149   FPR gap ( S ) 0.055 0.013 0.009 0.021   Precision gap ( P ) 0.002 0.135 0.046 0.031   Precision gap ( S ) 0.024 0.362 0.026 0.103   Independence gap ( S ) 0.034 0.084 0.0 0.054   Separation gap ( S ) 0.000 0.117 0.008 0.009   Sufficiency gap ( S ) 0.016 0.250 0.046 0.0422618261926202621Debiasing Strategy   Metric None Anon CA Anon + CA   Compression 1.984 ±0.101 2.073 * ±0.102 1.502 * ±0.075 1.540 * ±0.098   F1 ( Ontonotes test ) 76.406 ±0.165 76.538 ±0.176 77.187 * ±0.071 77.246 * ±0.230   F1 diff ( pro−anti ) 6.631 ±1.013 7.256 ±0.846 2.302 * ±0.466 2.422 * ±0.714   TPR gap ( P ) 0.654 ±0.069 0.710 * ±0.047 0.607±0.082 0.627 ±0.100   TPR gap ( S ) 4.884 ±0.698 4.870 ±0.509 2.041 * ±0.228 2.014 * ±0.286   FPR gap ( P ) 0.602 ±0.036 0.620 ±0.056 0.572±0.078 0.629 ±0.107   FPR gap ( S ) 0.120 ±0.015 0.128 ±0.011 0.050 * ±0.006 0.049 * ±0.007   Precision gap ( P ) -0.549 ±0.051 -0.571 ±0.052 - 0.491 * ±0.081 -0.569 ±0.122   Precision gap ( S ) 3.080 ±0.275 3.266 ±0.264 1.421 * ±0.181 1.390 * ±0.216   Independence gap ( S ) 0.027 ±0.008 0.025 ±0.004 0.004 * ±0.001 0.004 * ±0.001   Separation gap ( S ) 1.247 ±0.150 1.344 ±0.137 0.537 * ±0.061 0.557 * ±0.070   Sufficiency gap ( S ) 8.684 ±1.883 8.816 ±1.544 1.673 * ±0.354 1.557 * ±0.384   Debiasing Strategy   Metric None Anon CA Anon + CA   Compression 1.984 ±0.065 2.073 * ±0.104 1.502 * ±0.081 1.540 * ±0.079   F1 ( Ontonotes test ) 76.40 * ±0.16 76.48 * ±0.22 76.72 * ±0.15 76.91 * ±0.19   F1 diff ( pro−anti ) 6.072 ±0.789 7.417 * ±1.280 3.674 * ±0.599 2.858 * ±0.382   TPR gap ( P ) 0.635±0.053 0.688 * ±0.052 0.679 * ±0.062 0.654 ±0.049   TPR gap ( S ) 4.561 ±0.414 5.143 * ±0.713 2.590 * ±0.420 2.178 * ±0.201   FPR gap ( P ) 0.579±0.046 0.637 * ±0.055 0.620 * ±0.070 0.692 * ±0.075   FPR gap ( S ) 0.113 ±0.011 0.126 * ±0.016 0.063 * ±0.010 0.052 * ±0.004   Precision gap ( P ) -0.512 ±0.060 -0.581 * ±0.057 -0.550 * ±0.083 -0.632 * ±0.098   Precision gap ( S ) 2.943 ±0.215 3.221 * ±0.384 1.690 * ±0.242 1.446 * ±0.146   Independence gap ( S ) 0.022 ±0.003 0.026 * ±0.006 0.006 * ±0.002 0.004 * ±0.001   Separation gap ( S ) 1.188 ±0.114 1.336 * ±0.175 0.670 * ±0.111 0.594 * ±0.057   Sufficiency gap ( S ) 7.350 ±0.914 8.655 * ±1.726 0.2.401 * ±0.610 1.653 * ±0.2942622262326242625Female Male   Words Words   husband , women , chief , companies   gender , listed , computer ,   practices , nurse , applications ,   specializes , md , accepts ,   children , known , doctors ,   ba , child , npi , sports ,   reading , families , philosoph ’ ,   location , place , problems , rating ,   affiliated , family , no , systems ,   experiences , theory , practicing ,   spanish , software ,   love , justice security , major   Female Male   Words Words   husband , women , holds , emergency ,   midwife , providing vanderbilt , forces ,   book , includes , registered , mental ,   joining , faculty assistant , president   C Why is scrubbing not as effective as   subsampling ?   The debiasing method of subsampling significantly   reduced external biases in the occupation predic-   tion task . Although compression rates show that   scrubbing reduced more gender information , sub-   sampling outperforms it as a debiasing method . We   find that in spite of the scrubbing , a probe is able   to correctly identify the gender from an internal   representation with 68.8 % accuracy compared to   90.7 % on the original , non - scrubbed data . This   means that although the scrubbing process reduces   extrinsic bias significantly , gender information is   still embedded in the [ CLS ] token embeddings .   To investigate the source of gender information   after scrubbing , we use logistic regression ( LR)model to predict the gender from the Bag - of - Words   of the scrubbed biographies . We perform an itera-   tive process for automatic extra scrubbing : in each   iteration we ( 1 ) train a LR model for gender predic-   tion ( 2 ) scrub the n most significant words for each   gender according to the LR weights . The most rel-   evant words among 5 seeds of training with n=10   words scrubbed per iteration are displayed in Table   9 . The model learns indirect correlations to gender   in the absence of explicit gendered words . Because   the significant words are related to male- or female-   dominated professions , we conducted the process   on a specific profession . Table 10 presents the most   significant words for biographies of nurses . There   are differences in wording even between females   and males in the same profession . The results of   this study are in line with the results of other studies   that have been conducted on the way biographies   are written for men and women ( Wagner et al . ,   2016 ; Sun and Peng , 2021 ) .   Subsampling is therefore more effective even   when gender information is present since it pre-   vents the model from learning correlations between   gender information and a profession whereas scrub-   bing only attempts to remove gender indicators   without removing correlations . On the other hand ,   it is possible that oversampling is less effective for   debiasing since seeing more non - unique examples   an unrepresented group encourages learning corre-   lations .   D A closer look into no - correlation cases   D.1 Occupation Prediction   Although compression has the ability to identify   bias in most cases , some metrics still show little or   no correlation with compression rate . These results   suggest that gender information comprises only   one facet of embedded bias in the representations .   Other factors that may influence these metrics are   not considered or measured , such as the connection   between a name and a profession .   For example , as can be see in Tables 3 and 4 ,   LMs finetuned on subsampled data have the largest   FPR gaps after retraining , despite being the least   biased before retraining , while those finetuned on   oversampled data have the next - to - lowest FPR gaps   after retraining . The information encoded in the   internal representations may have been encoded   in a manner that allowed the classification layer   to exhibit a smaller FPR gap when trained on a   balanced dataset . However , when the classification2626layer was retrained on biased training data , it used   the same features to make biased predictions .   D.2 Coreference Resolution   The cases where there is no correlation between   our intrinsic metric and an extrinsic metric are the   cases where the metric is based on Pearson corre-   lation . Unlike occupation prediction , coreference   resolution seems to exhibit no correlation between   those metrics and compression rate . These metrics   are computed as the Pearson correlation between a   performance gap for a specific profession and the   percentage of women in that profession , however   the percentages are computed differently in each   task : in occupation prediction , the percentages are   computed from the train set , focusing on the rep-   resentation each gender has in the data . In Wino-   bias , the percentages are taken from the US labor   statistics , and are unrelated to the training dataset   statistics . We note that the two statistics can be dif-   ferent - the real - world representation of women in a   profession does not have to be equal to their repre-   sentation in written text ( Suresh and Guttag , 2021 ) .   We thus decided to test what happens if we change   the statistics used in Winobias to dataset statistics ,   but Ontonotes 5.0 has very little representation to   each profession and the statistics extracted from   it would not be reliable . We thus took a different   approach and computed the Pearson correlations   for occupation prediction with real world statistics   instead of dataset statistics . To do this , we mapped   the professions appearing in this dataset to pro-   fessions from the US labor statistics , and dropped   those who could no be mapped ( 6 out of 29 of the   professions which is 21.4 % ) . We then repeated   all experiments on the Pearson metrics using these   statistics . Figure 7 shows the results . Correlations   are very different when computed with respect to   real - world statistics . TPR - gap has no correlation at   all although it had with training data statistics , the   correlation for FPR - gap after retraining exists but   is negative , and the correlation with precision - gap   does not exist after retraining . We thus conclude   that the Pearson metrics are less reliable as they are   heavily dependent on the statistics with respect to   which they are calculated.26272628