  Nuo Chen , Linjun Shou , Jian Pei , Ming Gong , Bowen Cao   Jianhui Chang , Daxin Jiang , Jia LiHong Kong University of Science and Technology ( Guangzhou ) ,   Hong Kong University of Science and TechnologySTCA , Microsoft , Beijing , Duke University , USAPeking University , China   chennuo26@gmail.com , jialee@ust.hk   Abstract   Currently , learning better unsupervised sen-   tence representations is the pursuit of many   natural language processing communities . Lots   of approaches based on pre - trained language   models ( PLMs ) and contrastive learning have   achieved promising results on this task . Experi-   mentally , we observe that the over - smoothing   problem reduces the capacity of these pow-   erful PLMs , leading to sub - optimal sentence   representations . In this paper , we present   aSimple method named Self - Contrastive   Learning ( SSCL ) to alleviate this issue , which   samples negatives from PLMs intermediate lay-   ers , improving the quality of the sentence rep-   resentation . Our proposed method is quite   simple and can be easily extended to vari-   ous state - of - the - art models for performance   boosting , which can be seen as a plug - and-   play contrastive framework for learning un-   supervised sentence representation . Exten-   sive results prove that SSCL brings the supe-   rior performance improvements of different   strong baselines ( e.g. , BERT and SimCSE )   on Semantic Textual Similarity and Transfer   datasets . Our codes are available at https :   //github.com / nuochenpku / SSCL .   1 Introduction   Learning effective sentence representations is a   long - standing and fundamental goal of natural lan-   guage processing ( NLP ) communities ( Hill et al . ,   2016 ; Conneau et al . , 2017 ; Kim et al . , 2021 ; Gao   et al . , 2021 ; You et al . , 2022 ) , which can be applied   to various downstream NLP tasks such as Seman-   tic Textual Similarity ( Agirre et al . , 2012 , 2013 ,   2014 , 2015 , 2016 ; Cer et al . , 2017 ; Marelli et al . ,   2014 ) and information retrieval ( Xiong et al . , 2021 ;   Li et al . , 2022 ) . Compared with supervised sen-   tence representations , unsupervised sentence repre-   sentation learning is more challenging because of   lacking enough supervised signals . Figure 1 : Inter - layer cosine similarity of sentence rep-   resentations computed from SimCSE and Ours . We   calculate the sentence representation similarity between   two adjacent layers on STS - B test set . In this exam-   ple , we extend our methods of SimCSE by utilizing the   penultimate layer as negatives .   In the context of unsupervised sentence repre-   sentation learning , prior works ( Devlin et al . , 2018 ;   Lan et al . , 2020 ) tend to directly utilize large pre-   trained language models ( PLMs ) as the sentence   encoder to achieve promising results . Recently , re-   searchers point that the representations from these   PLMs suffer from the anisotropy ( Li et al . , 2020 ;   Su et al . , 2021 ) issue , which denotes the learned   representations are always distributed into a narrow   one in the semantic space . More recently , several   works ( Giorgi et al . , 2021 ; Gao et al . , 2021 ) prove   that incorporating PLMs with contrastive learning   can alleviate this problem , leading to the distribu-   tion of sentence representations becoming more   uniform . In practice , these works ( Wu et al . , 2020a ;   Yan et al . , 2021a ) propose various data augmenta-   tion methods to construct positive sentence pairs .   For instance , Gao et al . ( 2021 ) propose to leverage   dropout as the simple yet effective augmentation   method to construct positive pairs , and the corre-   sponding results are better than other more complex   augmentation methods .   Experimentally , aside from the anisotropy and   tedious sentence augmentation issues , we observe3552   a new phenomenon also makes the model sub-   optimized : Sentence representation between two   adjacent layers in the unsupervised sentence en-   coders are becoming relatively identical when the   encoding layers go deeper . Figure 1 shows the   sentence representation similarity between two ad-   jacent layers on STS - B test set . The similarity   scores in blue dotted line are computed from Sim-   CSE ( Gao et al . , 2021 ) , which is the state - of - the-   art PLM - based sentence model . Obviously , we   can observe the similarity between two adjacent   layers ( inter - layer similarity ) is very high ( almost   more than 0.9 ) . Such high similarities refer to that   the model does n’t acquire adequate distinct knowl-   edge as the encoding layer increases , leading to   the neural network validity and energy ( Cai and   Wang , 2020 ) decreased and the loss of discrimina-   tive power . In this paper , we call this phenomenon   as the inter - layer over - smoothing issue ( Tang et al . ,   2022 ) .   Intuitively , there are two factors could result in   the above issue : ( 1 ) The encoding layers in the   model are of some redundancy ; ( 2 ) The training   strategy of current model is sub - optimized , making   the deep layers in the encoder can not be optimized   effectively . For the former , the easiest and most   reasonable way is to cut off some layers in the en-   coder . However , this method inevitably leads to   performance drop . As presented in Table 1 , the   performance of SimCSE decreases from 76.85 %   to 70.45 % when we drop the last two encoder lay-   ers . Meanwhile , almost none existing works have   delved deeper to alleviate the over - smoothing issue   from the latter side .   Motivated by the above concerns , we present   a new training paradigm based on contrastive   learning : Simple contrastive method named Self-   Contrastive Learning ( SSCL ) , which can signif-   icantly improve the performance of learned sen-   tence representations while alleviating the over-   smoothing issue . Simply Said , we utilize hidden   representations from intermediate PLMs layers as   negative samples which the final sentence represen-   tations should be away from . Generally , our SSCL   has several advantages : ( 1 ) It is fairly straightfor - ward and does not require complex data augmenta-   tion techniques ; ( 2 ) It can be seen as a contrastive   framework that focuses on mining negatives effec-   tively , and can be easily extended into different sen-   tence encoders that aim for building positive pairs ;   ( 3 ) It can further be viewed as a plug - and - play   framework for enhancing sentence representations .   As presented in Figure 1 , ours ( red dotted line ) that   extend of SimCSE with employing the penultimate   layer sentence representation as negatives results   in a large drop in the inter - layer similarity between   last two adjacent layers ( 11 - th and 12 - th ) , showing   SSCL makes inter - layer sentence representations   more discriminative . Results in Table 1 show ours   also could result in better sentence representations   while alleviating the inter - layer over - smoothing   issue .   We show SSCL brings superior performance im-   provements in 7 Semantic Textual Similarity ( STS )   and 7 Transfer ( TS ) datasets . Experimentally , we   apply our method on two base encoders : BERT and   SimCSE . And the resulting models achieve 15.68 %   and 1.65 % improvements on STS tasks , separately .   Then , extensive in - depth analysis and probing tasks   are further conducted , revealing SSCL could im-   prove PLMs ’ capability to capture the surface , syn-   tactic and semantic information of sentences via   addressing the over - smoothing problem . Besides   of these observations , another interesting finding is   that ours can keep comparable performance while   reducing the sentence vector dimension size signif-   icantly . For instance , SSCL even obtains better   performances ( 62.42 % vs. 58.83 % ) while reducing   the vector dimensions from 768 to 256 dimensions   when extending to BERT - base . In general , the con-   tributions of this paper can be summarized as :   •We first observe the inter - layer over-   smoothing issue in current state - of - the - art un-   supervised sentence models , and then propose   SSCL to alleviate this problem , producing su-   perior sentence representations .   •Extensive results prove the effectiveness of   the proposed SSCL on Semantic Textual Sim-   ilarity and Transfer datasets .   •Qualitative and quantitative analysis are in-   cluded to justify the designed architecture and   look into the representation space of SSCL.35532 Background   In this section , we first review the formulation of   the over - smoothing issue in PLMs from the per-   spective of the intra - layer andinter - layer . Then   we discuss the difference of over - smoothing and   annisotropy problems .   2.1 Over - smoothing   Recently , Shi et al . ( 2022 ) point intra - layer over-   smoothing issue in PLMs from the perspective of   graph , which denotes different tokens in the input   sentence are mapped to quite similar representa-   tions . It can be observed via measuring the sim-   ilarity between different tokens in the same sen-   tence , named token - wise cosine similarity . Given a   sentence X={x , x , ... , x } , token - wise cosine   similarity of Xcan be calculated as :   TokSim = 1   m(m−1)/summationdisplayxx   ∥x∥∥x∥(1 )   where m is the number of tokens , x , xare the   representations of x , xfrom PLMs and ∥ · ∥is   the Euclidean norm .   In this paper , we argue that the over - smoothing   issue also also exists in inter - layer level , which   refers to sentence representations from adjacent   PLMs layers are relatively identical . In detail , inter-   layer over - smoothing means the sentence represen-   tations from adjacent layers have high similarity ,   which can be measured by inter - layer similarity :   SetSim = ss   ∥s∥∥s∥(2 )   where sandsdenote sentence representations   ofXfrom two adjacent layers ( i - th and i+1 - th ) in   PLMs .   In summary , the over - smoothing issue can di-   vided into two folds : inter - layer andintra - layer . In   this paper , we aim at alleviating the over - smoothing   issue from the perspective of inter - layer , improv-   ing the sentence representations . Surprisingly , we   find alleviating over - smoothing in inter - layer also   can alleviate the intra - layer over - smoothing issue   to some extent , which is discussed in Section 5.3 .   2.2 Over - smoothing vs. Anisotropy   Currently , the anisotropy issue is widely studied to   improve sentence representations from PLMs . Ad-   mittedly , despite over - smoothing and anisotropy   are related concepts , they are nonetheless com-   pletely diverse . As described in ( Li et al . , 2020;Su et al . , 2021 ) , the anisotropy problem refers to   the distribution of learnt sentence representations   in the semantic space is always constrained to a   certain area . As illustrated in ( Shi et al . , 2022 ) ,   " over - smoothing ” can be summarized as the token   uniformity problems in BERT , which denotes to-   ken representations in the same input sentence are   highly related that is defined as intra - layer over-   smoothing in this paper . Moreover , we extend the   concept of over - smoothing issue to the inter - layer ,   which refers there is a significant degree of similar-   ity between sentence representations from neigh-   bouring neural network layers . Experimentally , the   over - smoothing problem can cause one sentence   to have a greater token - wise similarity or nearby   layers in PLMs to have a higher sentence represen-   tation similarity , while anisotropy makes all pairs   of sentences in the dataset achieve a relatively iden-   tical similarity score . Obviously , over - smoothing   is different from the anisotropy issue . Therefore ,   we distinguish these two concepts in the paper .   3 Methodology   In this section , we first introduce the traditional   contrastive methods for learning unsupervised sen-   tence representation . Then , we describe the pro-   posed method SSCL for building negatives and   briefly illustrate how to extend SSCL of other con-   trastive frameworks .   3.1 Traditional Contrastive Methods   Considering learning unsupervised sentence rep-   resentation via contrastive learning needs to con-   struct plausible positives or negatives , traditional   contrastive methods ( e.g. , word deletion , dropout )   tends to utilize data augmentation on training data   to build positives . In detail , given a sentence collec-   tion : X={X } . Subsequently , we can utilize   some data augmentation methods : f(·)on each   X∈ X to construct the semantically related posi-   tive sample X = f(X)(e.g . , dropout , word shuf-   fle and deletion ) , as shown in Figure 2 ( a ) . Then ,   lethandhdenote the PLMs ( e.g. , BERT ) last   layer sentence representations of XandX , the   contrastive training objective for ( h , h)with a   mini - batch of Npairs can be formulated as :   L=−logexp(Ψ(h , h)/τ)/summationtextexp(Ψ(h , h)/τ)(3 )   where Ψ ( , ) denotes the cosine similarity function ,   τis temperature . Notice that , these methods focus3554   on mining positive examples while directly utilize   in - batch negatives during training . Thereafter , we   introduce SSCL to build useful negatives , and thus ,   can be seen as complementary to previous methods .   3.2 SSCL   SSCL is free from external data augmentation pro-   cedures which utilizes hidden representations from   PLMs intermediate layers as negatives . In this pa-   per , we treat the last layer representation as the   final sentence representation which is needed to   optimize . Concretely , we collect the intermediate   M - th layer sentence representation in PLMs , which   is regarded as the negatives of last layer represen-   tation and named as h , as shown in Figure 2 ( b ) .   Hence , we obtain the negative pairs ( h , h ) . As   aforementioned , we also treat has the positive   sample which obtained from any data augmenta-   tion method . Subsequently , the training objective   Lcan be reformulated as follows :   where the first term in the denominator refers to   the original in - batch negatives , and the second term   denotes the intermediate negatives . Through these   methods , SSCL makes the last layer representa-   tion of PLMs more discriminative from the pre-   vious layers via easily enlarging the number of   negatives , and thus , alleviating the over - smoothing   issue . Clearly , our approach is rather straightfor-   ward and can be simply implemented into these   conventional contrastive techniques.4 Experiments   4.1 Evaluation Datasets   We conduct our experiments on 7 Semantic Textual   Similarity ( STS ) tasks and 7 Transfer tasks ( TR ) .   Following the common setting , SentEval toolkit is   used for evaluation purposes .   Semantic Textual Similarity We evaluate our   method on the following seven STS datasets : STS   12 - 16 ( Agirre et al . , 2012 , 2013 , 2014 , 2015 , 2016 ) ,   STS - B ( Cer et al . , 2017 ) and SICK - R ( Marelli   et al . , 2014 ) . And Spearman ’s correlation coef-   ficient is used as evaluation metric of the model   performance .   Transfer We evaluate our models on the fol-   lowing transfer tasks : MR ( Pang and Lee , 2005 ) ,   CR ( Hu and Liu , 2004 ) , SUBJ ( Pang and Lee ,   2004 ) , MPQA ( Wiebe et al . , 2005 ) , SST-2 ( Socher   et al . , 2013 ) , TREC ( V oorhees and Tice , 2000 ) and   MRPC ( Dolan and Brockett , 2005 ) . Concretely ,   we also follow the default settings in ( Gao et al . ,   2021 ) to train each sentence representation learning   method .   4.2 Implementation Details   We use the same training corpus from ( Gao et al . ,   2021 ) to avoid training bias , which consists of   one million sentences randomly sampled from   Wikipedia . In our SSCL implement , we select   BERT ( base and large version ) as our backbone   architecture because of its typical impact . τis set   to 0.05 and Adam optimizer is used for optimiz-   ing the model . Experimentally , the learning rate   is set to 3e-5 and 1e-5 for training BERTand3555   BERT models . The batch size is set to 64 and   max sequence length is 32 . It is worthwhile to no-   tice we utilize average pooling over input sequence   token representation and [ CLS ] vector to obtain   sentence - level representations , separately . More   concretely , we train our model with 1 epoch on a   single 32 G NVIDIA V100 GPU . For STS tasks ,   we save our checkpoint with best results on STS - B   development set ; For Transfer tasks , we use the   average score of 7 seven transfer datasets to find   the best checkpoint .   4.3 Results   Baselines We compare our methods with the fol-   lowing baselines : ( 1 ) naive baselines : GloVe aver-   age embeddings ( Pennington et al . , 2014 ) , Skip-   thought and BERT ; ( 2 ) strong baselines based   on BERT : BERT - flow ( Li et al . , 2020 ) , BERT-   whitening ( Su et al . , 2021 ) , IS - BERT ( Zhang et al . ,   2020 ) , CT - BERT ( Carlsson et al . , 2021 ) , Con-   sert ( Yan et al . , 2021b ) and SimCSE . For a fair   comparison , we extend SSCL to BERT and Sim-   CSE , separately . When extending to BERT ( SSCL-   BERT ) , we do n’t add any augmentation methods to   construct positives ; Extending to SimCSE ( SSCL-   SimCSE ) means we utilize dropout masks as the   way of building positives . STS tasks Table 2 reports the results of methods   on 7 STS datasets . From the table , we can observe   that : ( 1 ) Glove embeddings outperforms BERT ,   indicating the anisotropy issue has the negative   impact of BERT sentence representations ; ( 2 )   SSCL - BERT(cls./avg . ) surpasses BERT   ( cls./avg . ) by a large margin ( 62.42 % vs. 46.70 % ,   62.56 % vs. 52.54 % ) , showing the effectiveness of   our proposed SSCL ; ( 3 ) SSCL - SimCSEboosts   the model performance of SimCSE(77.90 %   vs. 76.25 % ) , representing SSCL can easily extend   of other contrastive model which can be seen as   a plug - and - play framework . Results also prove   incorporating negatives in contrastive learning is   essential for obtaining better sentence representa-   tions . Similar results can be observed in the large   version of the above models .   Transfer tasks Table 3 includes the main re-   sults on 7 transfer datasets . From the ta-   ble , we can draw a conclusion that our model   SSCL - BERT / SSCL - BERT outperforms   BERT / BERT on seven datasets , proving   the effectiveness of ours . Meanwhile , SSCL-   SimCSE / SSCL - SimCSE also shows a sub-   stantial model performance boost when com-   pared with SimCSE / SimCSE . For exam-   ple , SSCL - SimCSE improves SimCSE to   88.88 % ( 87.17 % ) , suggesting its effectiveness.3556   5 Analysis   In this section , we first conduct qualitative experi-   ments via probing tasks to analyse the structural of   the resulting representations ( Table 4 ) , including   syntactic , surface and semantic . Then , we explore   adequate quantitive analysis to verify the effective-   ness of SSCL , such as the negative sampling strat-   egy , strengths of SSCL in reducing redundant se-   mantics ( vector dimension ) and etc . Subsequently ,   we further provide some discussions on SSCL ,   likechicken - and - egg issue . In the Appendix B ,   we show the strength of SSCL in fasting conver-   gence speed ( Figure 6 ) , and conduct discussions :   whether improvements of resulting model are   indeed from SSCL or just more negatives ( Table   7 ) .   5.1 Qualitative Analysis   Representation Probing In this component , we   aim to explore the reason behind the effectiveness   of the proposed SSCL . Therefore , we conduct some   probing tasks to investigate the linguistic structure   implicitly learned by our resulting model repre - sentations . We directly evaluate each model using   three group sentence - level probing tasks : surface   task probe for Sentence Length ( SentLen ) , syn-   tactic task probe for the depth of syntactic tree   ( TreeDepth ) and the semantic task probe for co-   ordinated clausal conjuncts ( CoordInv ) . We re-   port the results in Table 4 , and we can observe   our models significantly surpass their original base-   lines on each task . Specially , SSCL - BERT and   SSCL - SimCSE improve the baselines ’ ( BERT and   SimCSE ) ability of capturing sentence semantic   ( 60.18 % vs. 50 % , 42.1 % vs. 34 % ) and surface   ( 75.3 % vs. 67 % , 88.5 % vs. 80 % ) by a large mar-   gin , which are essential to improve model sentence   representations , showing the reason of ours per-   form well on both STS and Transfer tasks .   5.2 Quantitive Analysis   Negative Sampling Strategy From the descrip-   tion in Section 3 , we can raise an intuitive question :   Which single layer is most suitable for building   negatives in SSCL ? Hence , we conduct a series of   experiments to verify the effect of intermediate lay-   ers with { 0,1,2,3,4,5,6,7,8,9,10,11 } , results   illustrated in Figure 3 ( a ) . In the figure , layer - index   0 represents original SimCSE , and layer - index 1 - 11   represents corresponding transformer layers . We   can observe that our model SSCL - SimCSE ob-   tains the best result 77.80 % while utilizing 11 - th   layer representation as negatives . The reason be-   hind this phenomenon can be explained that SSCL   makes the PLMs more distinguishable between last   layer and previous layers , and thus alleviating over-   smoothing issues . More specifically , this effect will   be more obvious when utilizing 11 - th layer repre-3557   sentation as negatives , helping the model achieve   best result .   Progressive SSCL Intuitively , we also can stack   several intermediate layers to construct more nega-   tives in our SSCL implement . Thus , we stack pre-   vious several layers for building negatives which   named Progressive SSCL . We visualize the results   in Figure 3 ( b ) , and the stack transformer layers   range 0 to 11 . Stacking 0 layer represents original   SimCSE , and Stacking 1 - 11 layers means we stack   last 1 - 11 layers representation to construct nega-   tives . For example , stacking 2 layers represents   utilizing 11 - th and 10 - th transformer layers to form   negatives . From the figure , we can draw the fol-   lowing conclusion : ( 1 ) Progressive SSCL slightly   outperforms SSCL , showing incorporating more   negatives can help improve the model performance ;   ( 2 ) Progressive SSCL with 2 layers can lead the   best model performance ( 77.90 % ) , indicating using   11 - th and 10 - th transformer layers to construct neg-   atives can further make the token representations   of last layer become more distinguishable .   Vector Dimension From the above analysis and   experimental results , we can observe SSCL can   help the PLMs achieve sufficient sentence - level   semantic representations . Therefore , we conduct   experiments to verify whether our methods need   high vector dimensions ( e.g. , 768 ) to maintain cor-   responding results . We report results of BERT ,   SSCL - BERT and SSCL - SimCSE with different   vector dimensions in the Table 5 . First , we can   observe that BERT performance keeps dropping   when word vector dimension reducing , indicating   the high vector dimension is essential for main - taining BERT performance . Then , we also find   SSCL - BERT and SSCL - SimCSE still achieve com-   parable performances with smaller vector dimen-   sions , showing our method can reduce redundant   information in the resulting sentence - level repre-   sentations , and thus lower dimensions is enough   for SSCL models obtaining competitive results . It   is worthwhile to notice that SSCL - BERT achieves   better performances when the vector dimension de-   creased . For example , SSCL - BERT improves the   model results from 58.83 % to 62.42 % when vector   dimensions reduced from 768 to 256 .   Impact of τIntuitively , it is essential to study   the sensitivity analysis of the temperature τin con-   trastive learning . Thereafter , we conduct additional   experiments to verify the effectiveness of τon opti-   mizing the model . We test the model performances   withτ∈ { 0.001,0.01,0.05,0.1 } . From the Table   6 , we observe the different τindeed brings perfor-   mance improvements or drops of both models , and   ours achieve best results when τ= 0.05 .   5.3 Discussion on SSCL   Chicken - and - egg issue As mentioned in Sec-   tion 1 , our methods effectively alleviate the over-   smoothing problem in sentence - level . In this com-   ponent , we also utilize TokSim in Eq.1 to conduct   quantitative analysis to verify whether SSCL could   alleviate the over - smoothing problem in intra - layer   level . We calculate TokSim for each sample from   STS - B ( Cer et al . , 2017 ) test set with SimCSE and   our resulting model SSCL - SimCSE . For compari-   son , both models are initialized from BERT stacked   with 12 transformer blocks . As shown in the Fig-3558   Modelτ   0.001 0.01 0.05 0.1   SimCSE 74.82 75.33 76.25 72.24   SSCL75.77 77.40 77.90 74.12   ure 4 , TokSim is low from the first few layers ,   showing token representations are highly distin-   guishable . However , TokSim becomes higher with   layers getting deeper . Concretely , TokSim of the   last layer from SimCSE is larger than 90 % . There-   after , ours has a obvious TokSim drop in the last   few layers ( 11 and 12 ) , proving our method allevi-   ates the over - smoothing issue in both sentence level   and token level while improving the model perfor-   mances ( Figure 4 ( b ) ) . This is because sentence   representations are frequently obtained via adding   aggregation methods ( e.h . , mean pooling and max   pooling ) over the token representations , resulting   in an entangled relationship ( Mohebbi et al . , 2021 ) .   Therefore , alleviating over - smoothing in sentence   representation could eliminate over - smoothing at   token - level to some extent .   Visualization As shown in the Figure 5 ( a ) , we   showcase the token representation similarities pro-   duced by SimCSE ( Gao et al . , 2021 ) . Obviously ,   we can observe each token representation in the   sentence is very close to each other . Nevertheless ,   the token representations within the same sentence   should be discriminative even if the sentence struc-   ture is simple in the ideal setting ( as shown in the   Figure 5 ( b ) ) . As aforementioned , such high sim-   ilar token representations may confuse the model   to capture global and reasonable sentence - level un-   derstanding , leading to sub - optimized sentence rep-   resentations . Nevertheless , our SSCL - SimCSE can   alleviate this problem from the inter - layer perspec-   tive while making token representations in the sen-   tence more discriminative , as seen in Figure 5 ( b ) .   6 Conclusion   In this paper , we explore the over - smoothing prob-   lem in unsupervised sentence representation . Then ,   we propose a simple yet effective method named   SSCL , which constructs negatives from PLMs in-   termediate layers to alleviate this problem , lead-   ing better sentence representations . The proposed   SSCL can easily be extended to other state - of - the-   art methods , which can be seen as a plug - and - play   contrastive framework . Experiments on seven STS   datasets and seven Transfer datasets prove the effec-3559tiveness of our proposed method . And qualitative   analysis indicates our method improves the result-   ing model ’s ability of capturing the semantic and   surface . Also quantitative analysis shows the pro-   posed SSCL not only reduces redundant semantics   but also fasts the convergence speed . As an ex-   tension of our future work , we will explore other   methods to improve the unsupervised sentence rep-   resentation quality .   Limitations   The main contributions of this paper are towards   tackling over - smoothing issue for learning unsu-   pervised sentence representation . The proposed ap-   proach is fairly basic and may simply be extended   to improve the performance of other state - of - the - art   models . More broadly , we anticipate that the cen-   tral idea of this study will provide insights to other   research communities seeking to improve sentence   representation in an unsupervised setting . Admit-   tedly , the proposed strategies are restricted with   unsupervised training , and biases in the training   corpus also may influence the performance of the   resulting model . These concerns warrant further   research and consideration when utilizing this work   to build unsupervised retrieval systems .   Acknowledgement   This research was supported by NSFC Grant No .   62206067 , and Guangzhou - HKUST(GZ ) Joint   Funding Scheme 2023A03J0673 .   References356035613562A Related Work   A.1 Unsupervised Sentence Representation   Unsupervised sentence representation learning has   gained lots of attention , which is considered to   be one of the most promising areas in natural lan-   guage understanding . Thanks to remarkable results   achieved by PLMs , quite a few works ( Devlin et al . ,   2018 ; Lan et al . , 2020 ) tended to directly use the   output of PLMs , obtaining the sentence - level rep-   resentation via [ CLS ] token - based representation   or leveraging pooling methods ( e.g. , mean - pooling   and max - pooling ) . Recently , some works ( Li et al . ,   2020 ; Su et al . , 2021 ; Shi et al . , 2022 ) found that   there are anistropy and over - smoothing problems   ( Gao et al . , 2022 ) in BERT ( Devlin et al . , 2018 )   representations . Facing these challenges , Su et al .   ( 2021 ) introduced whitening methods to obtain   isotropic sentence embedding distribution . More   recently , Shi et al . ( 2022 ) proposed to alleviate over-   smoothing problem via graph fusion methods . In   this paper , we design a novel and simple approach   to improve the quality of sentence representations ,   making them more uniform while alleviating the   over - smoothing problem from a new perspective .   A.2 Contrastive Learning   During the past few years , contrastive learning   ( Hadsell et al . , 2006 ) has been proved as an ex-   tremely promising approach to build on learning   effective representations in different contexts of   deep learning ( Chen et al . , 2021a , 2022 ; Gao et al . ,   2021 ; Chen et al . , 2021b ; You et al . , 2021 ; You   et al . ; Chen et al . , 2023 ) . Concretely , contrastive   learning objective aims at pulling together semanti-   cally close positive samples ( short for positives ) in   a semantic space , and pushing apart negative sam-   ples ( short for negatives ) . In the context of learn-   ing unsupervised sentence representation , Wu et al .   ( 2020b ) proposed leveraging several sentence - level   augmentation strategies to construct positives , ob-   taining a noise - invariant representation . Recently ,   Gao et al . ( 2021 ) designed a simple method named   SimCSE for constructing positives into contrastive   learning via using dropout ( Srivastava et al . , 2014 )   as noise . In detail , Gao et al . ( 2021 ) passed the each   sentence into the PLMs twice and obtained posi-   tives by applying random dropout masks in the rep-   resentations from last layer of PLMs . Subsequently ,   Wang et al . ( 2022 ) extended of SimCSE to for-   mulate a new contrastive method called MixCSE ,   which continually constructing hard negatives via   mixing both positives and negatives . However , it   is still limited to the specific framework . In this   paper , we focus on mining hard negatives for learn-   ing unsupervised sentence representation without   complex data augmentation methods and not lim-   ited to some specific frameworks . Accordingly , we   propose SSCL , a plug - and - play framework , which   can be extended to various state - of - the - art models .   B More Analysis   B.1 Convergence Speed   Moreover , we report the convergence speed of Sim-   CSE and our resulting model : SSCL - SimCSE in   the Figure 6 . From the figure , we can observe   that SimCSE and SSCL - SimCSE both obtain their   best results before the training ends . And SSCL-   SimCSE manages to maintain an absolute lead of   5%-15%over SimCSE during the early stage of   training , showing our methods not only speed the   training time and achieves superior performances .   Concretely , SSCL - SimCSE achieves its best per-   formances with only 1500 steps iteration . That is ,   our model can fast the convergence speed greatly ,   and thus , save the time cost .   B.2 Discussion on More Negatives   As illustrated in Eq.4 , our SSCL enlarges the size of   mini - batch negatives from N pairs to 2N pairs . In-   tuitively , there is a question : whether the improve-   ments of the resulting model are from SSCL ? Or   the model can achieve such results via just enlarg-   ing the batch size to get more in - batch negatives .   To answer this question , we conduct additional ex-   periments , as shown in Table 7 . When enlarging the3563   batch size from 64 to 128 , SimCSE still achieves   comparable performances rather than obtaining ob-   vious improvements like SSCL - SimCSE . In other   words , simply expanding in - bath negatives can not   effectively lead to better sentence representations ,   that is , the performance boost of SSCL - simCSE   indeed comes from our method.3564ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Left blank .   /squareA2 . Did you discuss any potential risks of your work ?   Left blank .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Left blank .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Left blank .   /squareB1 . Did you cite the creators of artifacts you used ?   Left blank .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Left blank .   C / squareDid you run computational experiments ?   Left blank .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Left blank.3565 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Left blank .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Left blank .   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Left blank.3566