  Wenzheng ZhaoYuanning CuiWei HuState Key Laboratory for Novel Software Technology , Nanjing University , ChinaNational Institute of Healthcare Data Science , Nanjing University , China   wzzhao.nju.cs@gmail.com , yncui.nju@gmail.com , whu@nju.edu.cn   Abstract   Continual relation extraction ( RE ) aims to learn   constantly emerging relations while avoiding   forgetting the learned relations . Existing works   store a small number of typical samples to   re - train the model for alleviating forgetting .   However , repeatedly replaying these samples   may cause the overfitting problem . We con-   duct an empirical study on existing works and   observe that their performance is severely af-   fected by analogous relations . To address this   issue , we propose a novel continual extraction   model for analogous relations . Specifically , we   design memory - insensitive relation prototypes   and memory augmentation to overcome the   overfitting problem . We also introduce inte-   grated training and focal knowledge distillation   to enhance the performance on analogous rela-   tions . Experimental results show the superiority   of our model and demonstrate its effectiveness   in distinguishing analogous relations and over-   coming overfitting .   1 Introduction   Relation extraction ( RE ) aims to detect the relation   between two given entities in texts . For instance ,   given a sentence “ Remixes of tracks from Persona 5   were supervised by Kozuka and original composer   Shoji Meguro ” and an entity pair ( Persona 5 , Shoji   Meguro ) , the “ composer ” relation is expected to be   identified by an RE model . Conventional RE task   assumes all relations are observed at once , ignoring   the fact that new relations continually emerge in the   real world . To deal with emerging relations , some   existing works ( Wang et al . , 2019 ; Han et al . , 2020 ;   Wu et al . , 2021 ; Cui et al . , 2021 ; Zhao et al . , 2022 ;   Zhang et al . , 2022 ; Hu et al . , 2022 ; Wang et al . ,   2022 ) study continual RE . In continual RE , new   relations and their involved samples continually   emerge , and the goal is to classify all observed rela-   tions . Therefore , a continual RE model is expected   Table 1 : Results of our empirical study . We divide all   relations into three groups according to their maximum   similarity to other relations . “ Accuracy ” indicates the   average accuracy ( % ) of relations after the model fin-   ishes learning . “ Drop ” indicates the average accuracy   drop ( % ) from learning the relation for the first time to   the learning process finished .   to be able to learn new relations while retaining the   performance on learned relations .   Existing works primarily focus on storing and   replaying samples to avoid catastrophic forgetting   ( Lange et al . , 2022 ) of the learned relations . On   one hand , considering the limited storage and com-   putational resources , it is impractical to store all   training samples and re - train the whole model when   new relations emerge . On the other hand , replay-   ing a small number of samples every time new   relations emerge would make the model prone to   overfit the stored samples ( Verwimp et al . , 2021 ;   Lange et al . , 2022 ) . Moreover , existing works sim-   ply attribute catastrophic forgetting to the decay   of previous knowledge as new relations come but   seldom delve deeper into the real causation . We   conduct an empirical study and find that the severe   decay of knowledge among analogous relations is   a key factor of catastrophic forgetting .   Table 1 shows the accuracy and accuracy drop   of two existing models on the FewRel ( Han et al . ,   2018 ) and TACRED ( Zhang et al . , 2017 ) datasets .   CRL ( Zhao et al . , 2022 ) and CRECL ( Hu et al . ,   2022 ) are both state - of - the - art models for contin-   ual RE . All relations in the datasets are divided1162into three groups according to the maximum co-   sine similarity of their prototypes to other relation   prototypes . A relation prototype is the overall rep-   resentation of the relation . We can observe that   the performance on relations with higher similar-   ity is poorer , which is reflected in less accuracy   and greater accuracy drop . Given that a relation   pair with high similarity is often analogous to each   other , the performance on a relation tends to suffer   a significant decline , i.e. , catastrophic forgetting ,   when its analogous relations appear . For example ,   the accuracy of the previously learned relation “ lo-   cation ” drops from 0.98 to 0.6 after learning a new   relation “ country of origin ” . Therefore , it is im-   portant to maintain knowledge among analogous   relations for alleviating catastrophic forgetting . See   Appendix A for more details of our empirical study .   To address the above issues , we propose a novel   continual extraction model for analogous relations .   Specifically , we introduce memory - insensitive re-   lation prototypes and memory augmentation to re-   duce overfitting . The memory - insensitive relation   prototypes are generated by combining static and   dynamic representations , where the static represen-   tation is the average of all training samples after   first learning a relation , and the dynamic representa-   tion is the average of stored samples . The memory   augmentation replaces entities and concatenates   sentences to generate more training samples for   replay . Furthermore , we propose integrated train-   ing and focal knowledge distillation to alleviate   knowledge forgetting of analogous relations . The   integrated training combines the advantages of two   widely - used training methods , which contribute to   a more robust feature space and better distinguish   analogous relations . One method uses contrastive   learning for training and generates prototypes for   relation classification , while the other trains a linear   classifier . The focal knowledge distillation assigns   high weights to analogous relations , making the   model more focus on maintaining their knowledge .   Our main contributions are summarized below :   •We explicitly consider the overfitting problem   in continual RE , which is often ignored by pre-   vious works . We propose memory - insensitive   relation prototypes and memory augmentation   to alleviate overfitting .   •We conduct an empirical study and find that   analogous relations are hard to distinguish and   their involved knowledge is more easily to beforgotten . We propose integrated training and   focal knowledge distillation to better distin-   guish analogous relations .   •The experimental results on two benchmark   datasets demonstrate that our model achieves   state - of - the - art accuracy compared with ex-   isting works , and better distinguishes analo-   gous relations and overcomes overfitting for   continual RE . Our source code is available at   https://github.com/nju-websoft/CEAR .   2 Related Work   Continual learning studies the problem of learn-   ing from a continuous stream of data ( Lange et al . ,   2022 ) . The main challenge of continual learning is   avoiding catastrophic forgetting of learned knowl-   edge while learning new tasks . Existing contin-   ual learning models can be divided into three cate-   gories : regularization - based , dynamic architecture ,   and memory - based . The regularization - based mod-   els ( Li and Hoiem , 2016 ; Kirkpatrick et al . , 2016 )   impose constraints on the update of parameters   important to previous tasks . The dynamic architec-   ture models ( Mallya and Lazebnik , 2018 ; Qin et al . ,   2021 ) dynamically extend the model architecture   to learn new tasks and prevent forgetting previous   tasks . The memory - based models ( Lopez - Paz and   Ranzato , 2017 ; Rebuffi et al . , 2017 ; Chaudhry et al . ,   2019 ) store a limited subset of samples in previous   tasks and replay them when learning new tasks .   In continual RE , the memory - based models   ( Wang et al . , 2019 ; Han et al . , 2020 ; Wu et al . ,   2021 ; Cui et al . , 2021 ; Zhao et al . , 2022 ; Zhang   et al . , 2022 ; Hu et al . , 2022 ) are the mainstream   choice as they have shown better performance for   continual RE than others . To alleviate catastrophic   forgetting , previous works make full use of relation   prototypes , contrastive learning , multi - head atten-   tion , knowledge distillation , etc . EA - EMR ( Wang   et al . , 2019 ) introduces memory replay and the   embedding aligned mechanism to mitigate the em-   bedding distortion when training new tasks . CML   ( Wu et al . , 2021 ) combines curriculum learning   and meta - learning to tackle the order sensitivity   in continual RE . RP - CRE ( Cui et al . , 2021 ) and   KIP - Framework ( Zhang et al . , 2022 ) leverage re-   lation prototypes to refine sample representations   through multi - head attention - based memory net-   works . Additionally , KIP - Framework uses exter-   nal knowledge to enhance the model through a   knowledge - infused prompt to guide relation proto-1163   type generation . EMAR ( Han et al . , 2020 ) , CRL   ( Zhao et al . , 2022 ) , and CRECL ( Hu et al . , 2022 )   leverage contrastive learning for model training .   Besides , knowledge distillation is employed by   CRL to maintain previously learned knowledge .   ACA ( Wang et al . , 2022 ) is the only work that   considers the knowledge forgetting of analogous   relations ignored by the above works and proposes   an adversarial class augmentation strategy to en-   hance other continual RE models . All these models   do not explicitly consider the overfitting problem   ( Lange et al . , 2022 ; Verwimp et al . , 2021 ) , which   widely exists in the memory - based models . As far   as we know , a few works ( Wang et al . , 2021 ) in   other continual learning fields have tried to reduce   the overfitting problem and achieve good results .   We address both the problems of distinguishing   analogous relations and overfitting to stored sam-   ples , and propose an end - to - end model .   3 Task Definition   A continual RE task consists of a sequence of tasks   T={T , T , . . . , T } . Each individual task is a   conventional RE task . Given a sentence , the RE   task aims to find the relation between two entities in   this sentence . The dataset and relation set of T∈   Tare denoted by DandR , respectively . D   contains separated training , validation and test sets ,   denoted by D , DandD , respectively .   Rcontains at least one relation . The relation sets   of different tasks are disjoint .   Continual RE aims to train a classification modelthat performs well on both current task Tand   previously accumulated tasks ˜T=/uniontextT. In   other words , a continual RE model is expected to   be capable of identifying all seen relations ˜R=/uniontextRand would be evaluated on all the test sets   of seen tasks ˜D=/uniontextD.   4 Methodology   4.1 Overall Framework   The overall framework is shown in Figure 1 . For a   new task T , we first train the continual RE model   onDto learn this new task . Then , we select   and store a few typical samples for each relation   r∈R. Next , we calculate the prototype pof   each relation r∈˜Raccording to the static and dy-   namic representations of samples . We also conduct   memory augmentation to provide more training   data for memory replay . Note that the augmented   data are not used for prototype generation . Finally ,   we perform memory replay consisting of integrated   training and focal knowledge distillation to alle-   viate catastrophic forgetting . The parameters are   updated in the first and last steps . After learning T ,   the model continually learns the next task T.   4.2 New Task Training   When the new task Temerges , we first train the   model on D. We follow the works ( Cui et al . ,   2021 ; Zhao et al . , 2022 ; Zhang et al . , 2022 ; Hu   et al . , 2022 ) to use the pre - trained language model   BERT ( Devlin et al . , 2019 ) as the encoder.1164Given a sentence xas input , we first tok-   enize it and insert special tokens [ E]/[E]and   [ E]/[E]to mark the start / end positions of head   and tail entities , respectively . We use the hidden   representations of [ E]and[E]as the represen-   tations of head and tail entities . The representation   ofxis defined as   h= LayerNorm / parenleftbig   W[h;h ] + b / parenrightbig   , ( 1 )   where h , h∈Rare the hidden represen-   tations of head and tail entities , respectively . d   is the dimension of the hidden layer in BERT .   W∈Randb∈Rare two trainable pa-   rameters .   Then , we use a linear softmax classifier to calcu-   late the classification probability of xaccording to   the representation h :   P(x;θ ) = softmax ( Wh ) , ( 2 )   where θdenotes the model when learning T.   W∈Ris the trainable parameter of the   linear classifier .   Finally , the classification loss of new task train-   ing is calculated as follows :   ( 3 )   where P(r|x;θ)is the probability of input x   classified as relation rby the current model θ.y   is the label of xsuch that if y = r , δ= 1 ,   and 0 otherwise .   4.3 Memory Sample Selection   To preserve the learned knowledge from previous   tasks , we select and store a few typical samples   for memory replay . Inspired by the works ( Han   et al . , 2020 ; Cui et al . , 2021 ; Zhao et al . , 2022 ;   Zhang et al . , 2022 ; Hu et al . , 2022 ) , we adopt the   k - means algorithm to cluster the samples of each   relation r∈R. The number of clusters is defined   as the memory size m. For each cluster , we select   the sample whose representation is closest to the   medoid and store it in the memory space M. The   accumulated memory space is ˜M=/uniontextM.   4.4 Memory - Insensitive Relation Prototype   A relation prototype is the overall representation   of the relation . Several previous works ( Han et al . ,   2020 ; Zhao et al . , 2022 ; Hu et al . , 2022 ) directlyuse relation prototypes for classification and sim-   ply calculate the prototype of rusing the average   of the representations of its typical samples . But ,   such a relation prototype is sensitive to the typi-   cal samples , which may cause the overfitting prob-   lem . To reduce the sensitivity to typical samples ,   Zhang et al . ( 2022 ) propose a knowledge - infused   relation prototype generation , which employs a   knowledge - infused prompt to guide prototype gen-   eration . However , it relies on external knowledge   and thus brings additional computation overhead .   To alleviate the overfitting problem , we first   calculate and store the average representation of   all training samples after first learning a relation .   This representation contains more comprehensive   knowledge about the relation . However , as we can-   not store all training samples , it is static and can not   be updated to adapt to the new feature space in the   subsequent learning . In this paper , the dynamic   representation of typical samples is used to fine-   tune the static representation for adapting the new   feature space . The memory - insensitive relation   prototype of relation ris calculated as follows :   p= ( 1−β)p+β   |M|/summationdisplayh,(4 )   where p is the average representation of all   training samples after learning relation rfor the   first time , and βis a hyperparameter .   4.5 Memory Augmentation   The memory - based models ( Wang et al . , 2019 ; Han   et al . , 2020 ; Cui et al . , 2021 ; Zhao et al . , 2022 ;   Zhang et al . , 2022 ; Hu et al . , 2022 ) select and store   a small number of typical samples and replay them   in the subsequent learning . Due to the limited mem-   ory space , these samples may be replayed many   times during continual learning , resulting in over-   fitting . To address this issue , we propose a mem-   ory augmentation strategy to provide more training   samples for memory replay .   For a sample xof relation rinM , we ran-   domly select another sample x̸=xfrom M.   Then , the head and tail entities of xare replaced   by the corresponding entities of xand the new   sample , denoted by x , can be seen as an addi-   tional sample of relation r. Also , we use sentence   concatenation to generate training samples . Specif-   ically , we randomly select another two samples x   andxfrom ˜M\Mand append them to the end   ofxandx , respectively . Note that xandx1165are not the typical samples of relation r. Then , we   obtain two new samples of relation r , denoted by   xandx . The model is expected to still   identify the relation rthough there is an irrelevant   sentence contained in the whole input . We conduct   this augmentation strategy on all typical samples in   ˜M , but the augmented data are only used for train-   ing , not for prototype generation , as they are not   accurate enough . Finally , the overall augmented   memory space is ˆM , and|ˆM|= 4|˜M| .   4.6 Memory Replay   4.6.1 Integrated Training   There are two widely - used training methods for   continual RE : Han et al . ( 2020 ) ; Zhao et al . ( 2022 ) ;   Hu et al . ( 2022 ) use contrastive learning for training   and make predictions via relation prototypes ; Cui   et al . ( 2021 ) ; Zhang et al . ( 2022 ) leverage the cross   entropy loss to train the encoder and linear clas-   sifier . We call these two methods the contrastive   method and the linear method , respectively .   The contrastive method contributes to a better   feature space because it pulls the representations of   samples from the same relation and pushes away   those from different relations , which improves the   alignment and uniformity ( Wang and Isola , 2020 ) .   However , its prediction process is sensitive to the   relation prototypes , especially those of analogous   relations that are highly similar to each other . The   linear classifier decouples the representation and   classification processes , which ensures a more task-   specific decision boundary . We adopt both con-   trastive and linear methods to combine their merits :   L = L+L , ( 5 )   whereLandLdenote the losses of the con-   trastive and linear methods , respectively .   In the contrastive method , we first leverage two-   layer MLP to reduce dimension :   z= Norm / parenleftbig   MLP ( h)/parenrightbig   . ( 6 )   Then , we use the InfoNCE loss ( van den Oord   et al . , 2018 ) and the triplet loss ( Schroff et al . , 2015 )   in contrastive learning :   ( 7)where zis the low - dimensional prototype of rela-   tionr.y= arg maxz·zis the most   similar negative relation label of sample x.τis   the temperature parameter . µandωare hyperpa-   rameters .   At last , the relation probability is computed   through the similarity between the representations   of test sample and relation prototypes :   P(x;θ ) = softmax ( z·Z ) , ( 8)   where Zdenotes the matrix of prototypes of all   seen relations .   In the linear method , a linear classifier obtains   the relation probability similar to that in the new   task training step . The loss function is   ( 9 )   4.6.2 Focal Knowledge Distillation   During the continual training process , some emerg-   ing relations are similar to other learned relations   and are difficult to distinguish . Inspired by the fo-   cal loss ( Lin et al . , 2020 ) , we propose the focal   knowledge distillation , which forces the model to   focus more on analogous relations .   Specifically , we assign a unique weight for each   sample - relation pair , according to the classifica-   tion probability of the sample and the similarity   between the representations of sample and rela-   tion prototype . Difficult samples and analogous   sample - relation pairs are assigned high weights .   The weight wfor sample xand relation ris   s = exp / parenleftbig   sim(h , p)/τ / parenrightbig   /summationtextexp / parenleftbig   sim(h , p)/τ / parenrightbig ,   ( 10 )   w = s / parenleftbig   1−P(y|x;θ)/parenrightbig , ( 11 )   where pis the prototype of relation r.sim ( · )   is the similarity function , e.g. , cosine . τis the   temperature parameter and γis a hyperparameter .   Withw , the focal knowledge distillation loss   is calculated as follows :   a = wP(r|x;θ ) , ( 12 )   ( 13)1166where P(r|x;θ)denotes the probability of   sample xpredicted to relation rby the previous   model θ .   The focal knowledge distillation loss is com-   bined with the training losses of contrastive and   linear methods . The overall loss is defined as   L = L+λL+λL , ( 14 )   where L andL are the focal knowledge   distillation losses of contrastive and linear methods ,   respectively . λandλare hyperparameters .   4.7 Relation Prediction   After learning task T , the contrastive and linear   methods are combined to predict the relation label   of the given test sample x :   ( 15 )   where P(x;θ)andP(x;θ)are the probabili-   ties calculated by the contrastive and linear meth-   ods , respectively . αis a hyperparameter .   5 Experiments and Results   In this section , we report the experimental results   of our model . The source code is accessible online .   5.1 Datasets   We conduct our experiments on two widely - used   benchmark datasets :   •FewRel ( Han et al . , 2018 ) is a popular RE   dataset originally built for few - shot learning .   It contains 100 relations and 70,000 samples   in total . To be in accord with previous works   ( Cui et al . , 2021 ; Zhao et al . , 2022 ) , we use 80   relations each with 700 samples ( i.e. , in the   training and validation sets ) , and split them   into 10 subsets to simulate 10 disjoint tasks .   •TACRED ( Zhang et al . , 2017 ) is a large - scale   RE dataset having 42 relations and 106,264   samples . Following the experiment setting of   previous works , we remove “ no_relation ” and   divide other relations into 10 tasks .   5.2 Experiment Setting and Baseline Models   RP - CRE ( Cui et al . , 2021 ) proposes a completely-   random strategy to split all relations into 10 subsets   corresponding to 10 tasks , and accuracy on all ob-   served relations is chosen as the evaluation metric , which is defined as the proportion of correctly pre-   dicted samples in the whole test set . This setting   is widely followed by existing works ( Zhao et al . ,   2022 ; Zhang et al . , 2022 ; Hu et al . , 2022 ) . For a   fair comparison , we employ the same setting and   obtain the divided data from the open - source code   of RP - CRE to guarantee exactly the same task se-   quence . Again , following existing works , we carry   out the main experiment with a memory size of 10   and report the average result of five different task   sequences . See Appendix B for the details of the   hyperparameter setting .   For comparison , we consider the following base-   line models : EA - EMR ( Wang et al . , 2019 ) , EMAR   ( Han et al . , 2020 ) , CML ( Wu et al . , 2021 ) , RP - CRE   ( Cui et al . , 2021 ) , CRL ( Zhao et al . , 2022 ) , CRECL   ( Hu et al . , 2022 ) and KIP - Framework ( Zhang et al . ,   2022 ) . See Section 2 for their details .   5.3 Results and Analyses   5.3.1 Main Results   Table 2 shows the results of all compared baselines   in the main experiment . The results of EA - EMR ,   EMAR , CML , and RP - CRE are obtained from the   RP - CRE ’s original paper , and the results of other   baselines are directly cited from their original pa-   pers . We additionally report the standard deviations   of our model . Based on the results , the following   observations can be drawn :   Our proposed model achieves an overall state - of-   the - art performance on the two different datasets   for the reason that our model can reduce overfitting   to typical samples and better maintain knowledge   among analogous relations . Thus , we can conclude   that our model effectively alleviates catastrophic   forgetting in continual RE .   As new tasks continually emerge , the perfor-   mance of all compared models declines , which   indicates that catastrophic forgetting is still a major   challenge to continual RE . EA - EMR and CML do   not use BERT as the encoder , so they suffer the   most performance decay . This demonstrates that   BERT has strong stability for continual RE .   All models perform relatively poorer on TA-   CRED and the standard deviations of our model on   TACRED are also higher than those on FewRel .   The primary reason is that TACRED is class-   imbalanced and contains fewer training samples   for each relation . Therefore , it is more difficult and   leads to greater randomness in the task division.1167   5.3.2 Ablation Study   We conduct an ablation study to validate the ef-   fectiveness of individual modules in our model .   Specifically , for “ w/o FKD ” , we remove the focal   knowledge distillation loss in memory replay ; for   “ w/o LM ” or “ w/o CM ” , the model is only trained   and evaluated with the contrastive or linear method ;   for “ w/o MA ” , we only train the model with origi-   nal typical samples in memory replay ; and for “ w/o   DP ” or “ w/o SP ” , we directly generate relation pro-   totypes based on the average of static or dynamic   representations .   The results are shown in Table 3 . It is observed   that our model has a performance decline without   each component , which demonstrates that all mod-   ules are necessary . Furthermore , the proposed mod-   ules obtain greater improvement on the TACRED   dataset . The reason is that TACRED is more dif-   ficult than FewRel , so the proposed modules are   more effective in difficult cases .   5.3.3 Influence of Memory Size   Memory size is defined as the number of stored   typical samples for each relation . For the memory-   based models in continual RE , their performance is   highly influenced by memory size . We conduct an   experiment with different memory sizes to compare   our model with CRL and CRECL for demonstrat-   ing that our model is less sensitive to memory size .   We re - run the source code of CRL and CRECL   with different memory sizes and show the results   in Figure 2 . Note that we do not compare with KIP-   Framework because it uses external knowledge to   enhance performance , which is beyond our scope .   In most cases , our model achieves state - of-   the - art performance with different memory sizes ,   which demonstrates the strong generalization of   our model . However , our model does not obtain   the best performance on TACRED with memory   size 15 because the overfitting problem that we   consider is not serious in this case . In fact , as   the memory size becomes smaller , the overfitting   problem is getting worse , and analogous relations   are more difficult to distinguish due to the limited   training data samples . From Figures 2(a ) , ( b ) , ( e),1168   and ( f ) , our model has greater advantages when   the memory size is small , which indicates that our   model can better deal with the overfitting problem   in continual RE .   We also observe that the performance of each   model declines due to the decrease of memory size ,   which demonstrates that memory size is a key fac-   tor in the performance of continual RE models .   From Figures 2(d ) and ( h ) , the performance dif-   ference between different memory sizes is smaller .   Thus , we draw the conclusion that our model is   more robust to the change of memory size .   5.3.4 Performance on Analogous Relations   One strength of our model is to distinguish anal-   ogous relations for continual RE . We conduct an   experiment to explore this point . Specifically , we   select relations in the former five tasks which have   analogous ones in the latter tasks , and report the   accuracy and drop on them in Table 4 . We con-   sider that two relations are analogous if the simi-   larity between their prototypes is greater than 0.85 .   As aforementioned , knowledge of the relations is   more likely to be forgotten when their analogous   relations emerge . Thus , all compared models are   challenged by these relations . However , the perfor-   mance of our model is superior and drops the least ,   which shows that our model succeeds in alleviating   knowledge forgetting among analogous relations .   5.3.5 Case Study   We conduct a case study to intuitively illustrate the   advantages of our model . Figure 3 depicts the vi-   sualization result . It is observed that the relations   analogous in semantics ( e.g. , “ mouth of the wa-   tercourse ” and “ tributary ” ) have relatively similar   relation prototypes , which reflects that our model   learns a reasonable representation space . More-   over , we see that the discrimination between similar   relation prototypes ( e.g. , “ director ” and “ screen-   writer ” ) is still obvious , which reveals that our   model can distinguish analogous relations . Please   see Appendix C for the comparison with CRECL .   6 Conclusion   In this paper , we study continual RE . Through   an empirical study , we find that knowledge de-   cay among analogous relations is a key reason for   catastrophic forgetting in continual RE . Further-   more , the overfitting problem prevalent in memory-   based models also lacks consideration . To this end ,   we introduce a novel memory - based model to ad-   dress the above issues . Specifically , the proposed   memory - insensitive relation prototypes and mem-   ory augmentation can reduce overfitting to typical1169   samples . In memory replay , the integrated train-   ing and focal knowledge distillation help maintain   the knowledge among analogous relations , so that   the model can better distinguish them . The ex-   perimental results on the FewRel and TACRED   datasets demonstrate that our model achieves state-   of - the - art performance and effectively alleviates   catastrophic forgetting and overfitting for continual   RE . In future work , we plan to explore whether our   model can be used in few - shot RE to help distin-   guish analogous relations .   7 Limitations   Our model may have several limitations : ( 1 ) As   a memory - based model , our model consumes ad-   ditional space to store typical samples and static   prototypes , which causes the performance to be   influenced by the storage capacity . ( 2 ) Although   we propose memory - insensitive relation prototypes   and memory augmentation , our model still relies   on the selection of typical samples . The selected   samples of low quality may harm the performance   of our model . ( 3 ) The recent progress in large lan-   guage models may alleviate catastrophic forgetting   and overfitting , which has not been explored in this   paper yet .   Acknowledgments   This work was supported by the National Natu-   ral Science Foundation of China ( No . 62272219 )   and the Collaborative Innovation Center of Novel   Software Technology & Industrialization . References11701171A More Results of Empirical Study   As mentioned in Section 1 , we conduct an empiri-   cal study to explore the causation of catastrophic   forgetting and find that the knowledge among anal-   ogous relations is more likely to be forgotten . As   a supplement , we further report more results of   our empirical study . Table 5 shows the average   change of maximum similarity when the accuracy   on relations suffers a sudden drop . Note that the   number of relations greater than a 40 % drop of   CRECL on the TACRED dataset is quite small ,   thus the result may not be representative . It is   observed that , if the maximum similarity of a re-   lation to others obviously increases , its accuracy   suddenly drops severely , which indicates that there   tends to be a newly emerging relation analogous   to it . In short , we can conclude that a relation may   suffer catastrophic forgetting when its analogous   relations appear . This also emphasizes the impor-   tance of maintaining knowledge among analogous   relations .   B Implementation Details   We carry out all experiments on a single NVIDIA   RTX A6000 GPU with 48 GB memory . Our imple-   mentation is based on Python 3.9.7 and the version   of PyTorch is 1.11.0 .   We find the best hyperparameter values through   grid search with a step of 0.1 except 0.05 for ωand   0.25 for γ . The search spaces for various hyper-   parameters are α∈[0.2,0.8 ] , β∈[0.1,0.5 ] , µ∈   [ 0.1,1.0 ] , ω∈[0.05,0.25 ] , γ∈[1.0,2.0]andλ ,   λ∈[0.5,1.5 ] . Besides , we fix τandτto 0.1   and 0.5 , respectively . The used hyperparameter   values are listed below:•For FewRel , α= 0.5,β= 0.5,τ= 0.1 ,   µ= 0.5,ω= 0.1,τ= 0.5,γ= 1.25 ,   λ= 0.5,λ= 1.1 .   •For TACRED , α= 0.6,β= 0.2,τ= 0.1 ,   µ= 0.8,ω= 0.15,τ= 0.5,γ= 2.0 ,   λ= 0.5,λ= 0.7 .   C Case Study of Our Model and CRECL   To intuitively illustrate that our model can better   distinguish analogous relations , we conduct a com-   parison to CRECL based on the case study in Sec-   tion 5.3.5 . As depicted in Figure 4 , it is true for   both our model and CRECL that if the relations   are dissimilar in semantics , the similarity between   their prototypes is low . However , we can observe   that our model learns relatively dissimilar proto-   types among analogous relations ( e.g. , lighter color   between “ director ” and “ screenwriter ” ) , which   demonstrates that our model can better distinguish   analogous relations .   D Comparison with ACA   As aforementioned in Section 2 , Wang et al . ( 2022 )   propose an adversarial class augmentation ( ACA )   strategy , aiming to learn robust representations   to overcome the influence of analogous relations .   Specifically , ACA utilizes two class augmentation   methods , namely hybrid - class augmentation and   reversed - class augmentation , to build hard nega-   tive classes for new tasks . When new tasks arrive ,   the model is jointly trained on new relations and   adversarial augmented classes to learn robust ini-   tial representations for new relations . As a data   augmentation strategy , ACA can be combined with   other continual RE models . Therefore , we conduct   an experiment to explore the performance of our   model with ACA .   We re - run the source code of ACA and report   the results of RP - CRE + ACA , EMAR + ACA , and   our model + ACA in Table 6 . Compared with the   original models , both EMAR and RP - CRE gain im-   provement , which demonstrates the effectiveness   of ACA in learning robust representations for anal-   ogous relations . However , as we also explicitly   consider the knowledge forgetting of analogous   relations , there exist overlaps between ACA and   our model . Thus , the performance of our model   declines when combined with ACA . We leave the   combination of our model and other augmentation   methods in future work.1172E Performance on Dissimilar Relations   We further conduct an experiment to explore the   performance on dissimilar relations . We consider   that relations with the highest similarity to other   relations lower than 0.7 are dissimilar relations . As   shown in Table 7 , our model achieves the best accu-   racy on dissimilar relations . We attribute this to the   better representations it learns through integrated   training . However , our model does not always ob-   tain the smallest drop as it focuses on alleviating   the forgetting of analogous relations . Overall , from   the results in Tables 4 and 7 , we can conclude that   our model achieves the best accuracy on both anal-   ogous and dissimilar relations as well as the least   drop on analogous relations.1173ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   Section 7 .   /squareA2 . Did you discuss any potential risks of your work ?   No , our paper is a foundational research .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   Section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Sections 4 and 5 .   /squareB1 . Did you cite the creators of artifacts you used ?   Sections 4 and 5 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   The artifacts that we use are all public .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Section 5 .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   The datasets that we use are all public   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   The artifacts that we use are all public .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 5 .   C / squareDid you run computational experiments ?   Section 5 and Appendix B.   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix B.1174 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Section 5 and Appendix B.   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Section 5 .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix B.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Not applicable . Left blank .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Not applicable . Left blank .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Not applicable . Left blank .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   Not applicable . Left blank .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Not applicable . Left blank.1175