  Md Rashad Al Hasan Rony , Liubov Kovriguina , Debanjan Chaudhuri ,   Ricardo Usbeck , Jens LehmannUniversity of Bonn , University of Hamburg , Fraunhofer IAIS Dresden   { rashad.rony,liubov.kovriguina,jens.lehmann}@iais.fraunhofer.de   { lehmann,d.chaudhuri}@uni-bonn.de   ricardo.usbeck@uni-hamburg.de   Abstract   Evaluating Natural Language Generation   ( NLG ) systems is a challenging task . Firstly ,   the metric should ensure that the generated   hypothesis reﬂects the reference ’s semantics .   Secondly , it should consider the grammatical   quality of the generated sentence . Thirdly , it   should be robust enough to handle various   surface forms of the generated sentence .   Thus , an effective evaluation metric has to   be multifaceted . In this paper , we propose   an automatic evaluation metric incorporating   several core aspects of natural language un-   derstanding ( language competence , syntactic   and semantic variation ) . Our proposed metric ,   RoMe , is trained on language features such as   semantic similarity combined with tree edit   distance and grammatical acceptability , using   a self - supervised neural network to assess   the overall quality of the generated sentence .   Moreover , we perform an extensive robustness   analysis of the state - of - the - art methods and   RoMe . Empirical results suggest that RoMe   has a stronger correlation to human judgment   over state - of - the - art metrics in evaluating   system - generated sentences across several   NLG tasks .   1 Introduction   Automatic generation of ﬂuent and coherent nat-   ural language is a key step for human - computer   interaction . Evaluating generative systems such as   text summarization , dialogue systems , and machine   translation is challenging since the assessment in-   volves several criteria such as content determina-   tion , lexicalization , and surface realization ( Liu   et al . , 2016 ; Dale and Mellish , 1998 ) . For assess-   ing system - generated outputs , human judgment is   considered to be the best approach . Obtaining hu-   man evaluation ratings , on the other hand , is both   expensive and time - consuming . As a result , devel-   oping automated metrics for assessing the quality   of machine - generated text has become an active   area of research in NLP.The quality estimation task primarily entails   determining the similarity between the reference   and hypothesis as well as assessing the hypoth-   esis for grammatical correctness and naturalness .   Widely used evaluation metrics such as BLEU ( Pap-   ineni et al . , 2002 ) , METEOR ( Banerjee and Lavie ,   2005 ) , and ROUGE ( Lin , 2004 ) which compute   the word - overlaps , were primarily designed for   evaluating machine translation and text summa-   rization systems . Word - overlap based metrics , on   the other hand , are incapable of capturing the hy-   potheses ’ naturalness and ﬂuency . Furthermore ,   they do not consider the syntactic difference be-   tween reference and hypothesis . In a different line   of research , word mover distance ( WMD ) ( Kus-   ner et al . , 2015 ) , BERTScore ( Zhang et al . , 2020a )   and MoverScore ( Zhao et al . , 2019 ) compute word   embedding based similarity for evaluating system-   generated texts . Although these metrics employ the   contextualized representation of words , they do not   take the grammatical acceptability of the hypoth-   esis and the syntactical similarity to the reference   into account .   To address these shortcomings , we propose   RoMe , an automatic and robust metric for eval-   uating NLG systems . RoMe employs a neural clas-   siﬁer that uses the generated sentence ’s grammati-   cal , syntactic , and semantic qualities as features to   estimate the quality of the sentence . Firstly , it cal-   culates the earth mover ’s distance ( EMD ) ( Rubner   et al . , 1998 ) to determine how much the hypothesis   differs from the reference . During the computa-   tion of EMD , we incorporate hard word alignment   and soft - penalization constants to handle various   surface forms of words in a sentence , such as re-   peated words and the passive form of a sentence .   Secondly , using a semantically enhanced tree edit   distance , the difference in syntactic structures be-   tween the reference and hypothesis sentences is   quantiﬁed . Thirdly , the metric incorporates a bi-   nary classiﬁer to evaluate the grammatical accept-5645ability of the generated hypotheses . Finally , the   scores obtained from the preceding steps are com-   bined to form a representation vector , which is   subsequently fed into a self - supervised network .   The network produces a ﬁnal score , referred to as   RoMe ’s output which represents the overall quality   of the hypothesis statement .   We investigate the effectiveness of our proposed   metric by conducting experiments on datasets from   various domains of NLG such as knowledge graph   based language generation dataset ( KELM ( Agar-   wal et al . , 2021 ) ) , dialogue datasets ( Eric et al . ,   2017 ; Chaudhuri et al . , 2021 ) , the WebNLG   2017 challenge dataset ( Shimorina et al . , 2018 ) ,   structured data to language generation dataset   ( BAGEL ( Mairesse et al . , 2010 ) and SFHO-   TEL ( Wen et al . , 2015 ) ) . The capability of existing   metrics to handle various forms of text has lately   become a matter of debate in the NLP community   ( Ribeiro et al . , 2020 ; Novikova et al . , 2017 ; Liu   et al . , 2016 ) . Hence , we conduct an extensive ro-   bustness analysis to assess RoMe ’s performance   in handling diverse forms of system - generated sen-   tences . To verify our claim , we design the analysis   based on the text perturbation methods used in   CHECKLIST ( Ribeiro et al . , 2020 ) and adversarial   text transformation techniques from TextFooler ( Jin   et al . , 2020 ) and TextAttack ( Morris et al . , 2020 ) .   Empirical assessment on benchmark datasets and   the robustness analysis results exhibit that RoMe   can handle various surface forms and generate an   evaluation score , which highly correlates with hu-   man judgment . RoMe is designed to function at the   sentence level and can be used to evaluate English   sentences in the current version of the implemen-   tation . In the future versions , we plan to extend   RoMe by including more languages . We released   the code and annotation tool publicly .   2 Preliminaries   2.1 Earth Mover ’s Distance   The Earth Mover ’s Distance ( EMD ) estimates the   amount of work required to transform a probabil-   ity distribution into another ( Rubner et al . , 1998 ) .   Inspired by the EMD , in NLP the transportation   problem is adopted to measure the amount of work   required to match the system generated hypothesis   sentence with the reference sentence ( Kusner et al . ,   2015 ; Zhao et al . , 2019 ) . Let us deﬁne the refer-   ence asR = fr;r;:::;rgand the hypothesis as   H = fh;h;:::;hg , whererandhindicates the   i - th andj - th word of the reference and hypothe-   sis , respectively . The weight of the word rand   hare denoted as mandnrespectively . Then ,   the total weight distribution of RandHism = Pmandn = Pn , respectively . Here ,   the sentence - level and normalized TF - IDF score   of a word is considered as the word ’s weight . For-   mally , EMD can be deﬁned as :   wheredis the distance between the words rand   hin the space andF(H;R)is a set of possible   ﬂows between the two distributions that the system   tries to optimize . In Equation 1 , EMD ( H;R )   denotes the amount of work required to match the   hypothesis with the reference . The optimization is   done following four constraints :   The ﬁrst constraint indicates that each ﬂow must be   non - negative . The second constraint limits the total   weights ﬂowing from rto less than or equal to   m. Similarly , the third constraint restricts the total   weights ﬂowing from hto less than or equal to   n. The ﬁnal constraint indicates that the total ﬂow   of weights must be equal to the minimum weight   distribution . Figure 1 depicts the EMD for a given   hypothesis - reference pair.56462.2 Syntactic Similarity and Tree Edit   Distance   In computational linguistics , dependency and con-   stituency trees are used to represent syntactic de-   pendencies between words in a sentence . Unlike   the constituency tree , a dependency tree can repre-   sent non - adjacent and non - projective dependencies   in a sentence , which frequently appear in spoken   language and noisy text . That leads us to prefer   dependency trees over constituency trees for evalu-   ating NLG output .   Formally , a dependency tree is a set of nodes     = fw;w;:::;wgand a set of dependency   linksG = fg;g;:::;gg , wherewis the imagi-   nary root node and gis an index into   represent-   ing the governor of w. Every node has exactly   one governor except for w , which has no gover-   nor ( Hall and Novák , 2010 ) . Syntactic similarity   between a pair of dependency trees can be esti-   mated using several methods , such as graph cen-   tralities and Euclidean distances ( Oya , 2020 ) . In   our work , we exploit the Tree Edit Distance ( TED )   algorithm ( Zhang and Shasha , 1989 ) to estimate   syntactic similarity between reference and hypothe-   sis . TED is typically computed on ordered labeled   trees and can thus be used to compare dependency   trees . The editoperations performed during the   comparison of parsed dependency trees include   Change , Delete , and Insert .   Let us consider TandTbe the parsed de-   pendency trees of the hypothesis and reference ,   respectively . The operations required to transform   one tree into another are visualized in Figure 2 .   In TED , an exact match between the nodes of the   compared trees is performed to decide if any edit   operation is required . In this work , the syntactic   difference between hypothesis and reference is de-   termined by the output of TED , which speciﬁes the   total number of edit operations.3 RoMe   In RoMe , a neural network determines the ﬁnal   evaluation score given a reference - hypothesis pair .   The network is trained to predict the evaluation   score based on three features : semantic similar-   ity computed by EMD , enhanced TED , and the   grammatical acceptability score . We explain these   features in the following subsections .   3.1 Earth Mover ’s Distance Based Semantic   Similarity   During the computation of EMD , we employ hard   word alignment andsoft - penalization techniques   to tackle repetitive words and passive forms of a   sentence . We compute a distance matrix and a ﬂow   matrix as described below and ﬁnally obtain EMD   utilizing Equation 1 .   Hard Word Alignment . We ﬁrst align the word   pairs between reference and hypothesis based on   their semantic similarities . The alignment is per-   formed by computing all paired cosine similarities   while taking word position information into ac-   count , as in ( Echizen - ya et al . , 2019 ) . In contrast to   ( Echizen - ya et al . , 2019 ) , we use contextualized   pre - trained word embedding from the language   model ALBERT ( Lan et al . , 2020 ) . ALBERT uses   sentence - order prediction loss , focusing on mod-   eling inter - sentence coherence , which improves   multi - sentence encoding tasks . The word align-   ment score is computed as follows :   where~ rand ~ hdenote the contextualized word   embedding of randh , respectively . The ﬁrst   part of the right side of the equation computes the   cosine similarity between ~ rand ~ h , and the second   part calculates the relative position information as   proposed in ( Echizen - ya et al . , 2019 ) .   Figure 3 depicts a matrix of word alignment   scores generated on an example pair of sentences .   This alignment strategy fails to handle repetitive   words where a word from the hypothesis may get   aligned to several words in the reference ( see Fig-   ure 4 ) . To tackle such cases , we restrict the word   alignment by imposing a hard constraint . In the   hard constraint , we prevent the words in the hypoth-   esis from getting aligned to multiple words in the   reference as illustrated by the dotted arrows in Fig-   ure 4 . We denote the resulting set of hard - aligned   word pairs asA.5647   Transport Distance . A distance matrixDis re-   quired to compute the ﬁnal EMD score . For each   aligned pair ( r;h)2Awhere >  ,   the distance between randhis computed as fol-   lows :   d= 1:0 ~ r~h   k~ rkk ~ hke ( 4 )   whered2 D andis a conﬁdence threshold   found via hyper - parameter search ,   2[ 1;0)is   a soft - penalization constant . For all the non - hard-   aligned pairs and aligned pairs with value less than    , the distance dreceives a maximum value of   1.0 . Intuitively , a lower value of dimplies that   the word needs to travel a shorter distance in the   transportation problem of EMD . In Equation 4 ,   e works as a penalty where a higher   position difference multiplied with the negative   constant   will results in low dscore . The role of    is explained below .   Soft - penalization . Existing metrics often im-   pose hard penalties for words with different or-   der than the reference sentence ( Zhao et al . , 2019 ;   Echizen - ya et al . , 2019 ) . For instance , sentences   phrased in the passive form obtain a very low score   in those metrics . Addressing this issue , we intro-   duce a soft - penalization constant   =  in   Equation 4 to handle the passive form of a sentence   better . Let us consider a reference , " Shakespearehas written Macbeth " and the passive form of the   sentence as hypothesis , " The Macbeth is written   by Shakespeare " . The word Shakespeare appears   at the beginning of the reference and at the end   of the hypothesis , thus the position difference is   larger . In such scenario ,   imposes a lower penalty   as it divides the position difference by the length   max(p;q ) .   Finally , following the optimization constraints   of Equation 2 , we obtain the transportation ﬂow   F(H;R ) . For the optimized ﬂow f2F(H;R ) ,   the ﬁnal equation of EMD is as follows :   The semantic similarity between hypothesis and ref-   erence is denoted as F= 1:0 EMD . The nor-   malized value of EMD is used to calculate F.   3.2 Semantically Enhanced TED   To estimate the difference between the syntactic   structures of reference and hypothesis , we extend   the TED algorithm ( Zhang and Shasha , 1989 ) . The   original TED algorithm performs edit operations   based on an exact match between two nodes in the   dependency trees of hypothesis and reference . In   this work , we modify the TED algorithm and com-   pute a word embedding - based cosine similarity to   establish the equivalence of two nodes . Two nodes   are considered equal , if the cosine similarity of their   embedding representations exceeds the threshold   . This allows the semantically enhanced TED to   process synonyms and restricts it from unnecessary   editing of similar nodes . We call the resulting algo-   rithm TED - SE . The normalized value of TED - SE   is denoted asF. We compute TED - SE over the   lemmatized reference and hypothesis since lemma-   tized text exhibits improved performance in such   use cases ( Kutuzov and Kuzmenko , 2019 ) . The   lemmatizer and dependency parser from Stanza ( Qi   et al . , 2020 ) are utilised to obtain the tree represen-   tation of the text . Further details are provided in   Appendix A.1 .   3.3 Grammatical Acceptability Classiﬁcation   Linguistic competence assumes that native speak-   ers can judge the grammatical acceptability of a   sentence . However , system - generated sentences   are not always grammatically correct or acceptable .   Therefore , we train a binary classiﬁer on the Cor-   pus of Linguistic Acceptability ( CoLA ) ( Warstadt5648et al . , 2019 ) , predicting the probability that the hy-   pothesis is grammatically acceptable . CoLA is a   collection of sentences from the linguistics liter-   ature with binary expert acceptability labels con-   taining over 10k examples ( Warstadt et al . , 2019 ) .   The classiﬁer is based on BERT - large ( Devlin et al . ,   2019 ) and trained to optimize binary cross - entropy   loss . A text sequence is fed as input and as out-   put , the classiﬁer produces the class membership   probability ( grammatically acceptable , grammati-   cally unacceptable ) . The model achieves an accu-   racy of 80.6 % on the out - of - domain CoLA test set   ( Warstadt et al . , 2019 , p. 8) . We denote the score   from the classiﬁer as the feature F , which is used   to train a neural network ( see § 3.4 ) .   3.4 Final Scorer Network   A feed - forward neural network takes the previously   computed features as input and learns a function   f(F;F;F)in the ﬁnal step , yielding a ﬁnal   output score in the [ 0;1]interval . The output score   is regarded as the overall quality of the hypoth-   esis . Following a self - supervised paradigm , the   network is trained on artiﬁcially generated training   samples from the KELM dataset ( Agarwal et al . ,   2021 ) . KELM contains knowledge - grounded natu-   ral sentences . We randomly choose 2,500 sentence   pairs from the KELM dataset and generate 2,500   more negative samples by randomly augmenting   the sentences using TextAttack ( Morris et al . , 2020 )   and TextFooler ( Jin et al . , 2020 ) . Following a sim-   ilar approach , we additionally generate 1,000 test   sentence pairs from the KELM dataset . Overall , we   then have 5,000 training and 1,000 test examples .   The network is a simple , two - layered feed - forward   network optimized with stochastic gradient descent   using a learning rate of 1e-4 .   4 Experiments and Analysis   4.1 Data   To assess RoMe ’s overall performance , ﬁrst , we   benchmark on two language generation datasets ,   BAGEL ( Mairesse et al . , 2010 ) and SFHO-   TEL ( Wen et al . , 2015 ) , containing 404 and 796   data points , respectively . Each data point contains   a meaning representation ( MR ) and a system gen-   erated output . Human evaluation scores of these   datasets are obtained from ( Novikova et al . , 2017 ) .   Furthermore , we evaluate dialogue system ’s out-   puts on Stanford in - car dialogues ( Eric et al . , 2017)containing 2,510 data points and the soccer dia-   logue dataset ( Chaudhuri et al . , 2019 ) with 2,990   data points . Each data point of these datasets in-   cludes a user query , a reference response , and a   system response as a hypothesis . Three different   system outputs are evaluated for each dialogue   dataset . We use the human annotated data pro-   vided by ( Chaudhuri et al . , 2021 ) . Moreover , we   evaluate the metrics on the system generated out-   puts from the WebNLG 2017 challenge ( Shimorina   et al . , 2018 ) .   Finally , to conduct robustness analysis , we ran-   domly sample data points from KELM ( Agarwal   et al . , 2021 ) and perturb them with adversarial text   transformation techniques . Three annotators par-   ticipated in the data annotation process ( two of   them are from a Computer Science and one from   a non - Computer Science background ) , where they   annotated the perturbed data . We provided the an-   notators with an annotation tool which displays the   reference sentence and the system output for each   data point . The annotators were asked to choose   a value from a range of [ 1,3 ] , for each of the cate-   gories : Fluency , Semantic Correctness , and Gram-   matical correctness . In this case , the values stand   for 1 : poor , 2 : average , and 3 : good . The overall   inter - annotator agreement score , is 0.78 . The   annotation tool and its interface are discussed in   detail in Appendix A.2 .   4.2 Hyper - parameter Settings   We use= 0:60and= 0:65 in § 3.1 . Best values   are found by a hyper - parameter search from a range   of [ 0,1.0 ] with an interval of 0.1 . RoMe obtained   the best result by utilizing ALBERT - large ( Lan   et al . , 2020 ) model with 18 M parameters and 24   layers . Furthermore , we use the English word em-   bedding of dimension 300 to obtain results from   Fasttext ( Bojanowski et al . , 2017 ) throughout the   paper . As the grammatical acceptability classiﬁer ,   we train a BERT - base model with 110 M parame-   ters and 12 layers . The hidden layer size is 768   with a hidden layer dropout of 0.1 . A layer norm   epsilon of 1e-12 was used for layer normalization .   GELU ( Hendrycks and Gimpel , 2016 ) was used as   the activation function . We use a single GPU with   12GBs of memory for all the evaluations .   4.3 Baselines   We select both the word - overlap and embedding-   based metrics as strong baselines . For the experi-   ment and robustness analysis we choose BLEU ( Pa-5649   pineni et al . , 2002 ) , METEOR ( Banerjee and   Lavie , 2005 ) , BERTScore ( Zhang et al . , 2020a )   and MoverScore ( Zhao et al . , 2019 ) . We evaluate   the metrics on the sentence level to make a fair   comparison .   4.4 Results   Table 1 shows the performance of different metrics   on data to language generation datasets ( BAGEL   and SFHOTEL ) . In both the BAGEL and SFHO-   TEL , a meaning representation ( MR ) , for instance   inform(name=’hotel drisco’,price_range=’pricey ’ )   is given as a reference sentence , where the sys-   tem output is : the hotel drisco is a pricey ho-   tel , in this case . Although , RoMe outperformed   the baseline metrics in evaluating the informative-   ness , naturalness andquality score , the correlation   scores remain low with regard to human judgment .   This is because the MR , which is not a natural   sentence , is the reference statement in this sce-   nario . For all the experiments , we take the nor-   malized human judgement scores . We ﬁrstly eval-   uate our model using Fasttext ( Bojanowski et al . ,   2017 ) word embedding . We notice a signiﬁcant im-   provement in results when we replace the Fasttext   embedding with contextualized word embeddingobtained from BERT ( Devlin et al . , 2019 ) . Fur-   thermore , we experiment with multiple language   models and ﬁnally , we reach to our best performing   model with ALBERT - large ( Lan et al . , 2020 ) . In   all the experiments , we report the results of RoMe ,   using ALBERT - large ( Lan et al . , 2020 ) . In Ta-   ble 1 , WMD and SDM refer to word mover distance   and sentence mover distance , respectively , used in   MoverScore . We report the results of WDM and   SMD from ( Zhao et al . , 2019 ) .   Table 4 demonstrates the evaluation results   on dialogue datasets . We evaluated the system-   generated dialogues from three dialogue sys-   tem models : Mem2Seq ( Madotto et al . , 2018 ) ,   GLMP ( Wu et al . , 2019 ) , and DialoGPT ( Zhang   et al . , 2020b ) . In case of in - car dataset , all the non-   word - overlap metric achieved a better correlation   score than the word - overlap based metrics . This is   because generated responses in dialogue systems   are assessed based on the overall semantic meaning   and correctness of the information . Overall , RoMe   achieves stronger correlation scores on both in - car   and soccer dialogue datasets in evaluating several   dialogue system outputs .   Finally , we investigate the outputs of nine dis-   tinct systems that competed in the WebNLG 20175650   competition and report the correlation scores in   Table 6 . Although RoMe achieves the best cor-   relation in most of the cases , we notice a com-   parable and in some cases better results achieved   by the MoverScore ( Zhao et al . , 2019 ) . A corre-   lation graph is plotted in Figure 5 to investigate   the metrics ’ performance correlations further . The   graph is constructed from RoMe and baseline met-   rics ’ scores on the BAGEL dataset . As observed   from the correlation graph , we can infer that our   proposed metric , RoMe correlates highly with the   MoverScore . However , since RoMe handles both   the syntactic and semantic properties of the text   it achieved better results in all the datasets across   different NLG tasks .   4.5 Ablation Study   We conduct an ablation study to investigate the   impact of the RoMe ’s components on its overall   performance . Table 5 exhibits the incremental im-   provement in Spearman ’s correlation coefﬁcient ,   that each of the components brings to the metric .   We randomly choose 100 system - generated dia-   logue utterances from the dialogue datasets , sincethey frequently contain sentences in passive form   and repetitive words . The correlation of standard   EMD with the human judgement is denoted as   " RoMe score with EMD " . Inclusion of semantic   word alignment ( EMD ) and soft - penalization   ( EMD ) further improved the correlation score .   The classiﬁer was not used until this point in the   ablation since there was just one score . Moreover ,   the correlation score improved signiﬁcantly when   the semantically enhanced TED and grammatical   acceptability were introduced as features in addi-   tion to the EMD score to a neural classiﬁer . We   hypothesize that the inclusion of language features   related to grammar and syntactic similarity helped   the neural network achieve better performance .   4.6 Qualitative Analysis   RoMe is developed in a modular fashion , so it may   be used to generate scores for semantic similarity ,   syntactic similarity , and grammatical acceptabil-   ity separately . Table 2 shows the component - wise   score and the ﬁnal score of RoMe on three example   data points . In the ﬁrst example , RoMe demon-   strates its ability of capturing similar sentences5651   by obtaining high score . The scores from several   components in the second example demonstrate   RoMe ’s ability to handle passive form . The ﬁnal   example in Table 2 demonstrates that RoMe penal-   izes sentence with repetitive word .   Table 3 shows the performance of the three base-   lines and RoMe in handling erroneous cases . Al-   though the ﬁrst example contains a completely dif-   ferent hypothesis and the second case with repeti-   tive hypothesis both BERTScore and MoverScore   exhibit high score . On the contrary , BLEU score is   unable to handle such scenarios . However , by ob-   taining low scores , RoMe demonstrates its ability   to understand such cases better .   4.7 Robustness Analysis   In this section , we design ﬁve test cases to   stress the models ’ capabilities . For the analysis   purpose , we randomly sample data points from   KELM ( Agarwal et al . , 2021 ) ( cases 1 , 2 , and 4 )   and BAGEL ( Mairesse et al . , 2010 ) ( cases 3 and 5 ) .   The annotators annotate the sampled data points   on the following criteria : ﬂuency , semantic correct-   ness , grammatical correctness .   Case 1 : Entity replacement . We perform invari-   ance test ( INV ) from ( Ribeiro et al . , 2020 ) to check   the metrics ’ NER capability in assessing the text   quality . In this approach , we replace the entities   present in the text partially or fully with other enti-   ties in the dataset . For instance , " The population of   Germany " gets transformed to " The population of   England " .   Case 2 : Adjective replacement . Similar to the   entity replacement , in this case we choose 100 data   points from KELM that contain adjective in them .   Then we replace the adjectives with a synonym   and an antonym word to generate two sentences   from a single data point . For instance , the adjective   different is replaced with unlike andsame . At the   end of this process , we obtain 200 data points . Case 3 : Random word replacement . The   words in different positions in the text are replaced   by a generic token AAA following the adversarial   text attack method from ( Morris et al . , 2020 ) , in   this case . For instance , the sentence , " x is a cheap   restaurant near y " is transformed into " x is a cheap   restaurant AAA AAA " . We select the greedy search   method with the constraints on stop - words modi-   ﬁcation from the TextAttack tool . This approach   generates repetitive words when two consecutive   words are replaced .   Case 4 : Text transformation . We leverage   TextFooler ( Jin et al . , 2020 ) to replace two words   in the texts by similar words , keeping the semantic   meaning and grammar preserved .   Case 5 : Passive forms . In this case , we   randomly choose 200 data points from the   KELM ( Agarwal et al . , 2021 ) dataset where the   system generated responses are in passive form .   From the results of robustness analysis in Ta-   ble 7 , it is evident that almost all the metrics obtain   very low correlation scores with respect to human   judgment . Word - overlap based metrics such as   BLEU and METEOR mostly suffer from it . Al-   though RoMe achieves higher correlation scores   in most of the cases , there are still scope for im-   provement in handling the ﬂuency of the text better .   Text perturbation techniques used to design the test   cases often generate disﬂuent texts . In some cases ,   the texts ’ entities or subjects get replaced by words   from out of the domain . From our observation , we   hypothesize that handling keywords such as entities   may lead to a better correlation score .   5 Related Work   A potentially good evaluation metric is one that cor-   relates highly with human judgment . Among the   unsupervised approaches , BLEU ( Papineni et al . ,   2002 ) , METEOR ( Banerjee and Lavie , 2005 ) and   ROUGE ( Lin , 2004 ) are the most popular evalua-   tion metrics traditionally used for evaluating NLG5652systems . Although these metrics perform well in   evaluating machine translation ( MT ) and summa-   rization tasks , ( Liu et al . , 2016 ) shows that none of   the word overlap based metrics is close to human   level performance in dialogue system evaluation   scenarios . In a different line of work , word embed-   ding based metrics are introduced for evaluating   NLG systems ( Mikolov et al . , 2013 ; Matsuo et al . ,   2017 ) . Several unsupervised automated metrics   were proposed that leverage EMD ; one of them   is word mover ’s distance ( WMD ) ( Kusner et al . ,   2015 ) . Later , ( Matsuo et al . , 2017 ) proposed an   evaluation metric , incorporating WMD and word-   embedding , where they used word alignment be-   tween the reference and hypothesis to handle the   word - order problem . Recently , ( Echizen - ya et al . ,   2019 ) introduced an EMD - based metric WE_WPI   that utilizes the word - position information to tackle   the differences in surface syntax in reference and   hypothesis .   Several supervised metrics were also proposed   for evaluating NLG . ADEM ( Lowe et al . , 2017 )   uses a RNN - based network to predict the human   evaluation scores . With the recent development of   language model - based pre - trained models ( Zhang   et al . , 2020a ) proposed BERTScore , which uses   a pre - trained BERT model for evaluating various   NLG tasks such as machine translation and im-   age captions . Recently , ( Zhao et al . , 2019 ) pro-   posed MoverScore , which utilizes contextualized   embedding to compute the mover ’s score on word   and sentence level . A notable difference between   MoverScore and BERTScore is that the latter relies   on hard alignment compared to soft alignments in   the former . Unlike the previous methods , RoMe   focuses on handling the sentence ’s word repeti-   tion and passive form when computing the EMD   score . Furthermore , RoMe trains a classiﬁer by   considering the sentence ’s semantic , syntactic , and   grammatical acceptability features to generate the   ﬁnal evaluation score .   6 Conclusion   We have presented RoMe , an automatic and ro-   bust evaluation metric for evaluating a variety of   NLG tasks . The key contributions of RoMe in-   clude 1 ) EMD - based semantic similarity , where   hard word alignment andsoft - penalization tech-   niques are employed into the EMD for tackling   repetitive words and passive form of the sentence ,   2)semantically enhanced TED that computes thesyntactic similarity based on the node - similarity   of the parsed dependency trees , 3 ) grammatical   acceptability classiﬁer , which evaluates the text ’s   grammatical quality , and 4 ) robustness analysis ,   which assesses the metric ’s capability of handling   various form of the text . Both quantitative and qual-   itative analyses exhibit that RoMe highly correlates   with human judgment . We intend to extend RoMe   by including more languages in the future .   Acknowledgements   We acknowledge the support of the following   projects : SPEAKER ( BMWi FKZ 01MK20011A ) ,   JOSEPH ( Fraunhofer Zukunftsstiftung ) , OpenGPT-   X ( BMWK FKZ 68GX21007A ) , the excellence   clusters ML2R ( BmBF FKZ 01 15 18038 A / B / C ) ,   ScaDS.AI ( IS18026A - F ) and TAILOR ( EU GA   952215 ) .   References565356545655A Appendix   A.1 Dependency Tree Representation for   Tree Edit Distance Calculation   This section describes the process of parsing a de-   pendency tree from a sentence , followed by con-   verting the dependency tree to the adjacency list   for computing TED - SE . Let us consider a refer-   ence statement " the aidaluna is operated by aida   cruises which are located at rostock . " and a hy-   pothesis , " aida cruises , which is in rostock , oper-   ates aidaluna . " . First , a dependency tree is parsed   utilizing the Stanza dependency parser ( Qi et al . ,   2020 ) and then converted to an adjacency list . The   adjacency list contains a key - value pair oriented   data structure where each key corresponds to a   node ’s index in the tree , and the value is a list of   edges on which the head node is incident . Figure 6   demonstrates the dependency trees and their corre-   sponding adjacency lists for the given reference and   hypothesis . List of nodes and adjacency lists are   then fed into the TED - SE algorithm to calculate se-   mantically enhanced tree edit distance as described   in § 3.2 .   A.2 Annotation Tool   For all the annotation processes , we use the annota-   tion tool shown in Figure 7 . The tool is developed   using Python programming language . Annotators   can load their data into the tool in JSON format by   selecting the Load Raw Data button . An example   annotation step is shown in Figure 7 . The reference   and hypothesis sentences are displayed in differ-   ent text windows . The annotators were asked to   annotate the data based on Fluency , Semantically   correctness andGrammar . Annotators can choose   a value on a scale of [ 1,3 ] for each category , from   the corresponding drop - down option . Finally , the   annotated text can be saved for evaluation using   thesave button , which saves the annotated data in   JSON format.56565657