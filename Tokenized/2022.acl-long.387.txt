  Md Rashad Al Hasan Rony , Liubov Kovriguina , Debanjan Chaudhuri ,   Ricardo Usbeck , Jens LehmannUniversity of Bonn , University of Hamburg , Fraunhofer IAIS Dresden   { rashad.rony,liubov.kovriguina,jens.lehmann}@iais.fraunhofer.de   { lehmann,d.chaudhuri}@uni-bonn.de   ricardo.usbeck@uni-hamburg.de   Abstract   Evaluating Natural Language Generation   ( NLG ) systems is a challenging task . Firstly ,   the metric should ensure that the generated   hypothesis reÔ¨Çects the reference ‚Äôs semantics .   Secondly , it should consider the grammatical   quality of the generated sentence . Thirdly , it   should be robust enough to handle various   surface forms of the generated sentence .   Thus , an effective evaluation metric has to   be multifaceted . In this paper , we propose   an automatic evaluation metric incorporating   several core aspects of natural language un-   derstanding ( language competence , syntactic   and semantic variation ) . Our proposed metric ,   RoMe , is trained on language features such as   semantic similarity combined with tree edit   distance and grammatical acceptability , using   a self - supervised neural network to assess   the overall quality of the generated sentence .   Moreover , we perform an extensive robustness   analysis of the state - of - the - art methods and   RoMe . Empirical results suggest that RoMe   has a stronger correlation to human judgment   over state - of - the - art metrics in evaluating   system - generated sentences across several   NLG tasks .   1 Introduction   Automatic generation of Ô¨Çuent and coherent nat-   ural language is a key step for human - computer   interaction . Evaluating generative systems such as   text summarization , dialogue systems , and machine   translation is challenging since the assessment in-   volves several criteria such as content determina-   tion , lexicalization , and surface realization ( Liu   et al . , 2016 ; Dale and Mellish , 1998 ) . For assess-   ing system - generated outputs , human judgment is   considered to be the best approach . Obtaining hu-   man evaluation ratings , on the other hand , is both   expensive and time - consuming . As a result , devel-   oping automated metrics for assessing the quality   of machine - generated text has become an active   area of research in NLP.The quality estimation task primarily entails   determining the similarity between the reference   and hypothesis as well as assessing the hypoth-   esis for grammatical correctness and naturalness .   Widely used evaluation metrics such as BLEU ( Pap-   ineni et al . , 2002 ) , METEOR ( Banerjee and Lavie ,   2005 ) , and ROUGE ( Lin , 2004 ) which compute   the word - overlaps , were primarily designed for   evaluating machine translation and text summa-   rization systems . Word - overlap based metrics , on   the other hand , are incapable of capturing the hy-   potheses ‚Äô naturalness and Ô¨Çuency . Furthermore ,   they do not consider the syntactic difference be-   tween reference and hypothesis . In a different line   of research , word mover distance ( WMD ) ( Kus-   ner et al . , 2015 ) , BERTScore ( Zhang et al . , 2020a )   and MoverScore ( Zhao et al . , 2019 ) compute word   embedding based similarity for evaluating system-   generated texts . Although these metrics employ the   contextualized representation of words , they do not   take the grammatical acceptability of the hypoth-   esis and the syntactical similarity to the reference   into account .   To address these shortcomings , we propose   RoMe , an automatic and robust metric for eval-   uating NLG systems . RoMe employs a neural clas-   siÔ¨Åer that uses the generated sentence ‚Äôs grammati-   cal , syntactic , and semantic qualities as features to   estimate the quality of the sentence . Firstly , it cal-   culates the earth mover ‚Äôs distance ( EMD ) ( Rubner   et al . , 1998 ) to determine how much the hypothesis   differs from the reference . During the computa-   tion of EMD , we incorporate hard word alignment   and soft - penalization constants to handle various   surface forms of words in a sentence , such as re-   peated words and the passive form of a sentence .   Secondly , using a semantically enhanced tree edit   distance , the difference in syntactic structures be-   tween the reference and hypothesis sentences is   quantiÔ¨Åed . Thirdly , the metric incorporates a bi-   nary classiÔ¨Åer to evaluate the grammatical accept-5645ability of the generated hypotheses . Finally , the   scores obtained from the preceding steps are com-   bined to form a representation vector , which is   subsequently fed into a self - supervised network .   The network produces a Ô¨Ånal score , referred to as   RoMe ‚Äôs output which represents the overall quality   of the hypothesis statement .   We investigate the effectiveness of our proposed   metric by conducting experiments on datasets from   various domains of NLG such as knowledge graph   based language generation dataset ( KELM ( Agar-   wal et al . , 2021 ) ) , dialogue datasets ( Eric et al . ,   2017 ; Chaudhuri et al . , 2021 ) , the WebNLG   2017 challenge dataset ( Shimorina et al . , 2018 ) ,   structured data to language generation dataset   ( BAGEL ( Mairesse et al . , 2010 ) and SFHO-   TEL ( Wen et al . , 2015 ) ) . The capability of existing   metrics to handle various forms of text has lately   become a matter of debate in the NLP community   ( Ribeiro et al . , 2020 ; Novikova et al . , 2017 ; Liu   et al . , 2016 ) . Hence , we conduct an extensive ro-   bustness analysis to assess RoMe ‚Äôs performance   in handling diverse forms of system - generated sen-   tences . To verify our claim , we design the analysis   based on the text perturbation methods used in   CHECKLIST ( Ribeiro et al . , 2020 ) and adversarial   text transformation techniques from TextFooler ( Jin   et al . , 2020 ) and TextAttack ( Morris et al . , 2020 ) .   Empirical assessment on benchmark datasets and   the robustness analysis results exhibit that RoMe   can handle various surface forms and generate an   evaluation score , which highly correlates with hu-   man judgment . RoMe is designed to function at the   sentence level and can be used to evaluate English   sentences in the current version of the implemen-   tation . In the future versions , we plan to extend   RoMe by including more languages . We released   the code and annotation tool publicly .   2 Preliminaries   2.1 Earth Mover ‚Äôs Distance   The Earth Mover ‚Äôs Distance ( EMD ) estimates the   amount of work required to transform a probabil-   ity distribution into another ( Rubner et al . , 1998 ) .   Inspired by the EMD , in NLP the transportation   problem is adopted to measure the amount of work   required to match the system generated hypothesis   sentence with the reference sentence ( Kusner et al . ,   2015 ; Zhao et al . , 2019 ) . Let us deÔ¨Åne the refer-   ence asR = fr;r;:::;rgand the hypothesis as   H = fh;h;:::;hg , whererandhindicates the   i - th andj - th word of the reference and hypothe-   sis , respectively . The weight of the word rand   hare denoted as mandnrespectively . Then ,   the total weight distribution of RandHism = Pmandn = Pn , respectively . Here ,   the sentence - level and normalized TF - IDF score   of a word is considered as the word ‚Äôs weight . For-   mally , EMD can be deÔ¨Åned as :   wheredis the distance between the words rand   hin the space andF(H;R)is a set of possible   Ô¨Çows between the two distributions that the system   tries to optimize . In Equation 1 , EMD ( H;R )   denotes the amount of work required to match the   hypothesis with the reference . The optimization is   done following four constraints :   The Ô¨Årst constraint indicates that each Ô¨Çow must be   non - negative . The second constraint limits the total   weights Ô¨Çowing from rto less than or equal to   m. Similarly , the third constraint restricts the total   weights Ô¨Çowing from hto less than or equal to   n. The Ô¨Ånal constraint indicates that the total Ô¨Çow   of weights must be equal to the minimum weight   distribution . Figure 1 depicts the EMD for a given   hypothesis - reference pair.56462.2 Syntactic Similarity and Tree Edit   Distance   In computational linguistics , dependency and con-   stituency trees are used to represent syntactic de-   pendencies between words in a sentence . Unlike   the constituency tree , a dependency tree can repre-   sent non - adjacent and non - projective dependencies   in a sentence , which frequently appear in spoken   language and noisy text . That leads us to prefer   dependency trees over constituency trees for evalu-   ating NLG output .   Formally , a dependency tree is a set of nodes     = fw;w;:::;wgand a set of dependency   linksG = fg;g;:::;gg , wherewis the imagi-   nary root node and gis an index into   represent-   ing the governor of w. Every node has exactly   one governor except for w , which has no gover-   nor ( Hall and Nov√°k , 2010 ) . Syntactic similarity   between a pair of dependency trees can be esti-   mated using several methods , such as graph cen-   tralities and Euclidean distances ( Oya , 2020 ) . In   our work , we exploit the Tree Edit Distance ( TED )   algorithm ( Zhang and Shasha , 1989 ) to estimate   syntactic similarity between reference and hypothe-   sis . TED is typically computed on ordered labeled   trees and can thus be used to compare dependency   trees . The editoperations performed during the   comparison of parsed dependency trees include   Change , Delete , and Insert .   Let us consider TandTbe the parsed de-   pendency trees of the hypothesis and reference ,   respectively . The operations required to transform   one tree into another are visualized in Figure 2 .   In TED , an exact match between the nodes of the   compared trees is performed to decide if any edit   operation is required . In this work , the syntactic   difference between hypothesis and reference is de-   termined by the output of TED , which speciÔ¨Åes the   total number of edit operations.3 RoMe   In RoMe , a neural network determines the Ô¨Ånal   evaluation score given a reference - hypothesis pair .   The network is trained to predict the evaluation   score based on three features : semantic similar-   ity computed by EMD , enhanced TED , and the   grammatical acceptability score . We explain these   features in the following subsections .   3.1 Earth Mover ‚Äôs Distance Based Semantic   Similarity   During the computation of EMD , we employ hard   word alignment andsoft - penalization techniques   to tackle repetitive words and passive forms of a   sentence . We compute a distance matrix and a Ô¨Çow   matrix as described below and Ô¨Ånally obtain EMD   utilizing Equation 1 .   Hard Word Alignment . We Ô¨Årst align the word   pairs between reference and hypothesis based on   their semantic similarities . The alignment is per-   formed by computing all paired cosine similarities   while taking word position information into ac-   count , as in ( Echizen - ya et al . , 2019 ) . In contrast to   ( Echizen - ya et al . , 2019 ) , we use contextualized   pre - trained word embedding from the language   model ALBERT ( Lan et al . , 2020 ) . ALBERT uses   sentence - order prediction loss , focusing on mod-   eling inter - sentence coherence , which improves   multi - sentence encoding tasks . The word align-   ment score is computed as follows :   where~ rand ~ hdenote the contextualized word   embedding of randh , respectively . The Ô¨Årst   part of the right side of the equation computes the   cosine similarity between ~ rand ~ h , and the second   part calculates the relative position information as   proposed in ( Echizen - ya et al . , 2019 ) .   Figure 3 depicts a matrix of word alignment   scores generated on an example pair of sentences .   This alignment strategy fails to handle repetitive   words where a word from the hypothesis may get   aligned to several words in the reference ( see Fig-   ure 4 ) . To tackle such cases , we restrict the word   alignment by imposing a hard constraint . In the   hard constraint , we prevent the words in the hypoth-   esis from getting aligned to multiple words in the   reference as illustrated by the dotted arrows in Fig-   ure 4 . We denote the resulting set of hard - aligned   word pairs asA.5647   Transport Distance . A distance matrixDis re-   quired to compute the Ô¨Ånal EMD score . For each   aligned pair ( r;h)2Awhere >  ,   the distance between randhis computed as fol-   lows :   d= 1:0 ~ r~h   k~ rkk ~ hke ( 4 )   whered2 D andis a conÔ¨Ådence threshold   found via hyper - parameter search ,   2[ 1;0)is   a soft - penalization constant . For all the non - hard-   aligned pairs and aligned pairs with value less than    , the distance dreceives a maximum value of   1.0 . Intuitively , a lower value of dimplies that   the word needs to travel a shorter distance in the   transportation problem of EMD . In Equation 4 ,   e works as a penalty where a higher   position difference multiplied with the negative   constant   will results in low dscore . The role of    is explained below .   Soft - penalization . Existing metrics often im-   pose hard penalties for words with different or-   der than the reference sentence ( Zhao et al . , 2019 ;   Echizen - ya et al . , 2019 ) . For instance , sentences   phrased in the passive form obtain a very low score   in those metrics . Addressing this issue , we intro-   duce a soft - penalization constant   =  in   Equation 4 to handle the passive form of a sentence   better . Let us consider a reference , " Shakespearehas written Macbeth " and the passive form of the   sentence as hypothesis , " The Macbeth is written   by Shakespeare " . The word Shakespeare appears   at the beginning of the reference and at the end   of the hypothesis , thus the position difference is   larger . In such scenario ,   imposes a lower penalty   as it divides the position difference by the length   max(p;q ) .   Finally , following the optimization constraints   of Equation 2 , we obtain the transportation Ô¨Çow   F(H;R ) . For the optimized Ô¨Çow f2F(H;R ) ,   the Ô¨Ånal equation of EMD is as follows :   The semantic similarity between hypothesis and ref-   erence is denoted as F= 1:0 EMD . The nor-   malized value of EMD is used to calculate F.   3.2 Semantically Enhanced TED   To estimate the difference between the syntactic   structures of reference and hypothesis , we extend   the TED algorithm ( Zhang and Shasha , 1989 ) . The   original TED algorithm performs edit operations   based on an exact match between two nodes in the   dependency trees of hypothesis and reference . In   this work , we modify the TED algorithm and com-   pute a word embedding - based cosine similarity to   establish the equivalence of two nodes . Two nodes   are considered equal , if the cosine similarity of their   embedding representations exceeds the threshold   . This allows the semantically enhanced TED to   process synonyms and restricts it from unnecessary   editing of similar nodes . We call the resulting algo-   rithm TED - SE . The normalized value of TED - SE   is denoted asF. We compute TED - SE over the   lemmatized reference and hypothesis since lemma-   tized text exhibits improved performance in such   use cases ( Kutuzov and Kuzmenko , 2019 ) . The   lemmatizer and dependency parser from Stanza ( Qi   et al . , 2020 ) are utilised to obtain the tree represen-   tation of the text . Further details are provided in   Appendix A.1 .   3.3 Grammatical Acceptability ClassiÔ¨Åcation   Linguistic competence assumes that native speak-   ers can judge the grammatical acceptability of a   sentence . However , system - generated sentences   are not always grammatically correct or acceptable .   Therefore , we train a binary classiÔ¨Åer on the Cor-   pus of Linguistic Acceptability ( CoLA ) ( Warstadt5648et al . , 2019 ) , predicting the probability that the hy-   pothesis is grammatically acceptable . CoLA is a   collection of sentences from the linguistics liter-   ature with binary expert acceptability labels con-   taining over 10k examples ( Warstadt et al . , 2019 ) .   The classiÔ¨Åer is based on BERT - large ( Devlin et al . ,   2019 ) and trained to optimize binary cross - entropy   loss . A text sequence is fed as input and as out-   put , the classiÔ¨Åer produces the class membership   probability ( grammatically acceptable , grammati-   cally unacceptable ) . The model achieves an accu-   racy of 80.6 % on the out - of - domain CoLA test set   ( Warstadt et al . , 2019 , p. 8) . We denote the score   from the classiÔ¨Åer as the feature F , which is used   to train a neural network ( see ¬ß 3.4 ) .   3.4 Final Scorer Network   A feed - forward neural network takes the previously   computed features as input and learns a function   f(F;F;F)in the Ô¨Ånal step , yielding a Ô¨Ånal   output score in the [ 0;1]interval . The output score   is regarded as the overall quality of the hypoth-   esis . Following a self - supervised paradigm , the   network is trained on artiÔ¨Åcially generated training   samples from the KELM dataset ( Agarwal et al . ,   2021 ) . KELM contains knowledge - grounded natu-   ral sentences . We randomly choose 2,500 sentence   pairs from the KELM dataset and generate 2,500   more negative samples by randomly augmenting   the sentences using TextAttack ( Morris et al . , 2020 )   and TextFooler ( Jin et al . , 2020 ) . Following a sim-   ilar approach , we additionally generate 1,000 test   sentence pairs from the KELM dataset . Overall , we   then have 5,000 training and 1,000 test examples .   The network is a simple , two - layered feed - forward   network optimized with stochastic gradient descent   using a learning rate of 1e-4 .   4 Experiments and Analysis   4.1 Data   To assess RoMe ‚Äôs overall performance , Ô¨Årst , we   benchmark on two language generation datasets ,   BAGEL ( Mairesse et al . , 2010 ) and SFHO-   TEL ( Wen et al . , 2015 ) , containing 404 and 796   data points , respectively . Each data point contains   a meaning representation ( MR ) and a system gen-   erated output . Human evaluation scores of these   datasets are obtained from ( Novikova et al . , 2017 ) .   Furthermore , we evaluate dialogue system ‚Äôs out-   puts on Stanford in - car dialogues ( Eric et al . , 2017)containing 2,510 data points and the soccer dia-   logue dataset ( Chaudhuri et al . , 2019 ) with 2,990   data points . Each data point of these datasets in-   cludes a user query , a reference response , and a   system response as a hypothesis . Three different   system outputs are evaluated for each dialogue   dataset . We use the human annotated data pro-   vided by ( Chaudhuri et al . , 2021 ) . Moreover , we   evaluate the metrics on the system generated out-   puts from the WebNLG 2017 challenge ( Shimorina   et al . , 2018 ) .   Finally , to conduct robustness analysis , we ran-   domly sample data points from KELM ( Agarwal   et al . , 2021 ) and perturb them with adversarial text   transformation techniques . Three annotators par-   ticipated in the data annotation process ( two of   them are from a Computer Science and one from   a non - Computer Science background ) , where they   annotated the perturbed data . We provided the an-   notators with an annotation tool which displays the   reference sentence and the system output for each   data point . The annotators were asked to choose   a value from a range of [ 1,3 ] , for each of the cate-   gories : Fluency , Semantic Correctness , and Gram-   matical correctness . In this case , the values stand   for 1 : poor , 2 : average , and 3 : good . The overall   inter - annotator agreement score , is 0.78 . The   annotation tool and its interface are discussed in   detail in Appendix A.2 .   4.2 Hyper - parameter Settings   We use= 0:60and= 0:65 in ¬ß 3.1 . Best values   are found by a hyper - parameter search from a range   of [ 0,1.0 ] with an interval of 0.1 . RoMe obtained   the best result by utilizing ALBERT - large ( Lan   et al . , 2020 ) model with 18 M parameters and 24   layers . Furthermore , we use the English word em-   bedding of dimension 300 to obtain results from   Fasttext ( Bojanowski et al . , 2017 ) throughout the   paper . As the grammatical acceptability classiÔ¨Åer ,   we train a BERT - base model with 110 M parame-   ters and 12 layers . The hidden layer size is 768   with a hidden layer dropout of 0.1 . A layer norm   epsilon of 1e-12 was used for layer normalization .   GELU ( Hendrycks and Gimpel , 2016 ) was used as   the activation function . We use a single GPU with   12GBs of memory for all the evaluations .   4.3 Baselines   We select both the word - overlap and embedding-   based metrics as strong baselines . For the experi-   ment and robustness analysis we choose BLEU ( Pa-5649   pineni et al . , 2002 ) , METEOR ( Banerjee and   Lavie , 2005 ) , BERTScore ( Zhang et al . , 2020a )   and MoverScore ( Zhao et al . , 2019 ) . We evaluate   the metrics on the sentence level to make a fair   comparison .   4.4 Results   Table 1 shows the performance of different metrics   on data to language generation datasets ( BAGEL   and SFHOTEL ) . In both the BAGEL and SFHO-   TEL , a meaning representation ( MR ) , for instance   inform(name=‚Äôhotel drisco‚Äô,price_range=‚Äôpricey ‚Äô )   is given as a reference sentence , where the sys-   tem output is : the hotel drisco is a pricey ho-   tel , in this case . Although , RoMe outperformed   the baseline metrics in evaluating the informative-   ness , naturalness andquality score , the correlation   scores remain low with regard to human judgment .   This is because the MR , which is not a natural   sentence , is the reference statement in this sce-   nario . For all the experiments , we take the nor-   malized human judgement scores . We Ô¨Årstly eval-   uate our model using Fasttext ( Bojanowski et al . ,   2017 ) word embedding . We notice a signiÔ¨Åcant im-   provement in results when we replace the Fasttext   embedding with contextualized word embeddingobtained from BERT ( Devlin et al . , 2019 ) . Fur-   thermore , we experiment with multiple language   models and Ô¨Ånally , we reach to our best performing   model with ALBERT - large ( Lan et al . , 2020 ) . In   all the experiments , we report the results of RoMe ,   using ALBERT - large ( Lan et al . , 2020 ) . In Ta-   ble 1 , WMD and SDM refer to word mover distance   and sentence mover distance , respectively , used in   MoverScore . We report the results of WDM and   SMD from ( Zhao et al . , 2019 ) .   Table 4 demonstrates the evaluation results   on dialogue datasets . We evaluated the system-   generated dialogues from three dialogue sys-   tem models : Mem2Seq ( Madotto et al . , 2018 ) ,   GLMP ( Wu et al . , 2019 ) , and DialoGPT ( Zhang   et al . , 2020b ) . In case of in - car dataset , all the non-   word - overlap metric achieved a better correlation   score than the word - overlap based metrics . This is   because generated responses in dialogue systems   are assessed based on the overall semantic meaning   and correctness of the information . Overall , RoMe   achieves stronger correlation scores on both in - car   and soccer dialogue datasets in evaluating several   dialogue system outputs .   Finally , we investigate the outputs of nine dis-   tinct systems that competed in the WebNLG 20175650   competition and report the correlation scores in   Table 6 . Although RoMe achieves the best cor-   relation in most of the cases , we notice a com-   parable and in some cases better results achieved   by the MoverScore ( Zhao et al . , 2019 ) . A corre-   lation graph is plotted in Figure 5 to investigate   the metrics ‚Äô performance correlations further . The   graph is constructed from RoMe and baseline met-   rics ‚Äô scores on the BAGEL dataset . As observed   from the correlation graph , we can infer that our   proposed metric , RoMe correlates highly with the   MoverScore . However , since RoMe handles both   the syntactic and semantic properties of the text   it achieved better results in all the datasets across   different NLG tasks .   4.5 Ablation Study   We conduct an ablation study to investigate the   impact of the RoMe ‚Äôs components on its overall   performance . Table 5 exhibits the incremental im-   provement in Spearman ‚Äôs correlation coefÔ¨Åcient ,   that each of the components brings to the metric .   We randomly choose 100 system - generated dia-   logue utterances from the dialogue datasets , sincethey frequently contain sentences in passive form   and repetitive words . The correlation of standard   EMD with the human judgement is denoted as   " RoMe score with EMD " . Inclusion of semantic   word alignment ( EMD ) and soft - penalization   ( EMD ) further improved the correlation score .   The classiÔ¨Åer was not used until this point in the   ablation since there was just one score . Moreover ,   the correlation score improved signiÔ¨Åcantly when   the semantically enhanced TED and grammatical   acceptability were introduced as features in addi-   tion to the EMD score to a neural classiÔ¨Åer . We   hypothesize that the inclusion of language features   related to grammar and syntactic similarity helped   the neural network achieve better performance .   4.6 Qualitative Analysis   RoMe is developed in a modular fashion , so it may   be used to generate scores for semantic similarity ,   syntactic similarity , and grammatical acceptabil-   ity separately . Table 2 shows the component - wise   score and the Ô¨Ånal score of RoMe on three example   data points . In the Ô¨Årst example , RoMe demon-   strates its ability of capturing similar sentences5651   by obtaining high score . The scores from several   components in the second example demonstrate   RoMe ‚Äôs ability to handle passive form . The Ô¨Ånal   example in Table 2 demonstrates that RoMe penal-   izes sentence with repetitive word .   Table 3 shows the performance of the three base-   lines and RoMe in handling erroneous cases . Al-   though the Ô¨Årst example contains a completely dif-   ferent hypothesis and the second case with repeti-   tive hypothesis both BERTScore and MoverScore   exhibit high score . On the contrary , BLEU score is   unable to handle such scenarios . However , by ob-   taining low scores , RoMe demonstrates its ability   to understand such cases better .   4.7 Robustness Analysis   In this section , we design Ô¨Åve test cases to   stress the models ‚Äô capabilities . For the analysis   purpose , we randomly sample data points from   KELM ( Agarwal et al . , 2021 ) ( cases 1 , 2 , and 4 )   and BAGEL ( Mairesse et al . , 2010 ) ( cases 3 and 5 ) .   The annotators annotate the sampled data points   on the following criteria : Ô¨Çuency , semantic correct-   ness , grammatical correctness .   Case 1 : Entity replacement . We perform invari-   ance test ( INV ) from ( Ribeiro et al . , 2020 ) to check   the metrics ‚Äô NER capability in assessing the text   quality . In this approach , we replace the entities   present in the text partially or fully with other enti-   ties in the dataset . For instance , " The population of   Germany " gets transformed to " The population of   England " .   Case 2 : Adjective replacement . Similar to the   entity replacement , in this case we choose 100 data   points from KELM that contain adjective in them .   Then we replace the adjectives with a synonym   and an antonym word to generate two sentences   from a single data point . For instance , the adjective   different is replaced with unlike andsame . At the   end of this process , we obtain 200 data points . Case 3 : Random word replacement . The   words in different positions in the text are replaced   by a generic token AAA following the adversarial   text attack method from ( Morris et al . , 2020 ) , in   this case . For instance , the sentence , " x is a cheap   restaurant near y " is transformed into " x is a cheap   restaurant AAA AAA " . We select the greedy search   method with the constraints on stop - words modi-   Ô¨Åcation from the TextAttack tool . This approach   generates repetitive words when two consecutive   words are replaced .   Case 4 : Text transformation . We leverage   TextFooler ( Jin et al . , 2020 ) to replace two words   in the texts by similar words , keeping the semantic   meaning and grammar preserved .   Case 5 : Passive forms . In this case , we   randomly choose 200 data points from the   KELM ( Agarwal et al . , 2021 ) dataset where the   system generated responses are in passive form .   From the results of robustness analysis in Ta-   ble 7 , it is evident that almost all the metrics obtain   very low correlation scores with respect to human   judgment . Word - overlap based metrics such as   BLEU and METEOR mostly suffer from it . Al-   though RoMe achieves higher correlation scores   in most of the cases , there are still scope for im-   provement in handling the Ô¨Çuency of the text better .   Text perturbation techniques used to design the test   cases often generate disÔ¨Çuent texts . In some cases ,   the texts ‚Äô entities or subjects get replaced by words   from out of the domain . From our observation , we   hypothesize that handling keywords such as entities   may lead to a better correlation score .   5 Related Work   A potentially good evaluation metric is one that cor-   relates highly with human judgment . Among the   unsupervised approaches , BLEU ( Papineni et al . ,   2002 ) , METEOR ( Banerjee and Lavie , 2005 ) and   ROUGE ( Lin , 2004 ) are the most popular evalua-   tion metrics traditionally used for evaluating NLG5652systems . Although these metrics perform well in   evaluating machine translation ( MT ) and summa-   rization tasks , ( Liu et al . , 2016 ) shows that none of   the word overlap based metrics is close to human   level performance in dialogue system evaluation   scenarios . In a different line of work , word embed-   ding based metrics are introduced for evaluating   NLG systems ( Mikolov et al . , 2013 ; Matsuo et al . ,   2017 ) . Several unsupervised automated metrics   were proposed that leverage EMD ; one of them   is word mover ‚Äôs distance ( WMD ) ( Kusner et al . ,   2015 ) . Later , ( Matsuo et al . , 2017 ) proposed an   evaluation metric , incorporating WMD and word-   embedding , where they used word alignment be-   tween the reference and hypothesis to handle the   word - order problem . Recently , ( Echizen - ya et al . ,   2019 ) introduced an EMD - based metric WE_WPI   that utilizes the word - position information to tackle   the differences in surface syntax in reference and   hypothesis .   Several supervised metrics were also proposed   for evaluating NLG . ADEM ( Lowe et al . , 2017 )   uses a RNN - based network to predict the human   evaluation scores . With the recent development of   language model - based pre - trained models ( Zhang   et al . , 2020a ) proposed BERTScore , which uses   a pre - trained BERT model for evaluating various   NLG tasks such as machine translation and im-   age captions . Recently , ( Zhao et al . , 2019 ) pro-   posed MoverScore , which utilizes contextualized   embedding to compute the mover ‚Äôs score on word   and sentence level . A notable difference between   MoverScore and BERTScore is that the latter relies   on hard alignment compared to soft alignments in   the former . Unlike the previous methods , RoMe   focuses on handling the sentence ‚Äôs word repeti-   tion and passive form when computing the EMD   score . Furthermore , RoMe trains a classiÔ¨Åer by   considering the sentence ‚Äôs semantic , syntactic , and   grammatical acceptability features to generate the   Ô¨Ånal evaluation score .   6 Conclusion   We have presented RoMe , an automatic and ro-   bust evaluation metric for evaluating a variety of   NLG tasks . The key contributions of RoMe in-   clude 1 ) EMD - based semantic similarity , where   hard word alignment andsoft - penalization tech-   niques are employed into the EMD for tackling   repetitive words and passive form of the sentence ,   2)semantically enhanced TED that computes thesyntactic similarity based on the node - similarity   of the parsed dependency trees , 3 ) grammatical   acceptability classiÔ¨Åer , which evaluates the text ‚Äôs   grammatical quality , and 4 ) robustness analysis ,   which assesses the metric ‚Äôs capability of handling   various form of the text . Both quantitative and qual-   itative analyses exhibit that RoMe highly correlates   with human judgment . We intend to extend RoMe   by including more languages in the future .   Acknowledgements   We acknowledge the support of the following   projects : SPEAKER ( BMWi FKZ 01MK20011A ) ,   JOSEPH ( Fraunhofer Zukunftsstiftung ) , OpenGPT-   X ( BMWK FKZ 68GX21007A ) , the excellence   clusters ML2R ( BmBF FKZ 01 15 18038 A / B / C ) ,   ScaDS.AI ( IS18026A - F ) and TAILOR ( EU GA   952215 ) .   References565356545655A Appendix   A.1 Dependency Tree Representation for   Tree Edit Distance Calculation   This section describes the process of parsing a de-   pendency tree from a sentence , followed by con-   verting the dependency tree to the adjacency list   for computing TED - SE . Let us consider a refer-   ence statement " the aidaluna is operated by aida   cruises which are located at rostock . " and a hy-   pothesis , " aida cruises , which is in rostock , oper-   ates aidaluna . " . First , a dependency tree is parsed   utilizing the Stanza dependency parser ( Qi et al . ,   2020 ) and then converted to an adjacency list . The   adjacency list contains a key - value pair oriented   data structure where each key corresponds to a   node ‚Äôs index in the tree , and the value is a list of   edges on which the head node is incident . Figure 6   demonstrates the dependency trees and their corre-   sponding adjacency lists for the given reference and   hypothesis . List of nodes and adjacency lists are   then fed into the TED - SE algorithm to calculate se-   mantically enhanced tree edit distance as described   in ¬ß 3.2 .   A.2 Annotation Tool   For all the annotation processes , we use the annota-   tion tool shown in Figure 7 . The tool is developed   using Python programming language . Annotators   can load their data into the tool in JSON format by   selecting the Load Raw Data button . An example   annotation step is shown in Figure 7 . The reference   and hypothesis sentences are displayed in differ-   ent text windows . The annotators were asked to   annotate the data based on Fluency , Semantically   correctness andGrammar . Annotators can choose   a value on a scale of [ 1,3 ] for each category , from   the corresponding drop - down option . Finally , the   annotated text can be saved for evaluation using   thesave button , which saves the annotated data in   JSON format.56565657