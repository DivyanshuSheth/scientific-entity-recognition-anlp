  Ife AdebaraAbdelRahim ElmadanyMuhammad Abdul - MageedAlcides Alcoba InciarteDeep Learning & Natural Language Processing Group , The University of British ColumbiaDepartment of Natural Language Processing & Department of Machine Learning , MBZUAI   { ife.adebara@,a.elmadany@,muhammad.mageed@,alcobaaj@mail.}ubc.ca   Abstract   Language identification ( LID ) is a crucial pre-   cursor for NLP , especially for mining web   data . Problematically , most of the world ’s   7000 + languages today are not covered by   LID technologies . We address this pressing   issue for Africa by introducing AfroLID , a   neural LID toolkit for 517African languages   and varieties . AfroLID exploits a multi - domain   web dataset manually curated from across 14   language families utilizing five orthographic   systems . When evaluated on our blind Test   set , AfroLID achieves 95.89F - score . We also   compare AfroLID to five existing LID tools   that each cover a small number of African   languages , finding it to outperform them on   most languages . We further show the util-   ity of AfroLID in the wild by testing it on   the acutely under - served Twitter domain . Fi-   nally , we offer a number of controlled case   studies and perform a linguistically - motivated   error analysis that allow us to both show-   case AfroLID ’s powerful capabilities and limi-   tations .   1 Introduction   Language identification ( LID ) is the task of identi-   fying the human language a piece of text or speech   segment belongs to . The proliferation of social   media have allowed greater access to multilingual   data , making automatic LID an important first step   in processing human language appropriately ( Tjan-   dra et al . , 2021 ; Thara and Poornachandran , 2021 ) .   This includes applications in speech , sign language ,   handwritten text , and other modalities of language .   It also includes distinguishing languages in code-   mixed datasets ( Abdul - Mageed et al . , 2020 ; Thara   and Poornachandran , 2021 ) . Unfortunately , for the   majority of languages in the world , including most   African languages , we do not have the resources   for developing LID tools . Figure 1 :   This situation has implications for the future   NLP technologies . For instance , LID has facili-   tated development of widely multilingual models   such mT5 ( Xue et al . , 2021 ) and large multilin-   gual datasets such as CCAligned ( El - Kishky et al . ,   2020 ) , ParaCrawl ( Esplà et al . , 2019 ) , WikiMa-   trix ( Schwenk et al . , 2021 ) , OSCAR ( Ortiz Suárez   et al . , 2020 ) , and mC4 ( Xue et al . , 2021 ) which   have advanced research in NLP . Comparable re-   sources are completely unavailable for the major-   ity of the world ’s 7000 + today , with only poor   coverage of the so - called low - resource languages   ( LR ) . This is partly due to absence of LID tools ,   and impedes future NLP progress on these lan-   guages ( Adebara and Abdul - Mageed , 2022 ) . The   state of African languages is not any better than   other regions : Kreutzer et al . ( 2021 ) perform a man-   ual evaluation of 205datasets involving African   languages such as those in CCAligned , ParaCrawl ,   WikiMatrix , OSCAR , and mC4 and show that at1958least 15corpora were completely erroneous , a sig-   nificant fraction contained less than 50 % of correct   data , and 82corpora were mislabelled or used am-   biguous language codes . These consequently affect   the quality of models built with these datasets . Al-   abi et al . ( 2020 ) find that 135 K out of 150 K words   in the fastText embeddings for Yorùbá belong to   other languages such as English , French , and Ara-   bic . New embedding models created by Alabi   et al . ( 2020 ) with a curated high quality dataset out-   perform off - the - shelf fastText embeddings , even   though the curated data is smaller .   In addition to resource creation , lack ( or poor   performance ) of LID tools negatively impacts pre-   processing of LR languages since LID can be a   prerequisite for determining , e.g. , appropriate tok-   enization . ( Duvenhage et al . , 2017a ) . Furthermore ,   some preprocessing approaches may be necessary   for certain languages , but may hurt perforrmance   in other languages ( Adebara and Abdul - Mageed ,   2022 ) . Developing LID tools is thus vital for all   NLP . In this work , we focus on LID for African   languages and introduce AfroLID .   AfroLID is a neural LID tool that covers 517   African languages and language varietiesacross   14language families . The languages covered be-   long to 50African countries and are written in   five diverse scripts . We show the countries cov-   ered by AfroLID in Figure 1 . Examples of the   different scripts involved in the 517 languages   are displayed in Figure 2 . To the best of our   knowledge , AfroLID supports the largest subset   of African languages to date . AfroLID is also us-   able without any end - user training , and it exploits   data from a variety of domains to ensure robust-   ness . We manually curate our clean training data ,   which is of special significance in low resource   settings . We show the utility of AfroLID in the   wild by applying it on two Twitter datasets and   compare its performance with existing LID tools   that cover any number of African languages such   as CLD2 ( McCandless , 2010 ) , CLD3 ( Salcianu   et al . , 2018 ) , Franc , LangDetect ( Shuyo , 2010 ) ,   and Langid.py ( Lui and Baldwin , 2012 ) . Our re-   sults show that AfroLID consistently outperforms   allother LID tools for almost all languages , and   serves as the new SOTA for language identification   for African languages .   To summarize , we offer the following main con-   tributions :   1.We develop AfroLID , a SOTA LID tool for   517African languages and language varieties .   To facilitate NLP research , we make our mod-   els publicly available .   2.We carry out a study of LID tool performance   on African languages where we compare our   models in controlled settings with several   tools such as CLD2 , CLD3 , Franc , LangDe-   tect , and Langid.py .   3.Our models exhibit highly accurate perfor-   mance in the wild , as demonstrated by ap-   plying AfroLID on Twitter data .   4.We provide a wide range of controlled   case studies and carry out a linguistically-   motivated error analysis of AfroLID . This al-   lows us to motivate plausible directions for   future research , including potentially beyond   African languages .   The rest of the paper is organized as follows :   In Section 2 we discuss a number of typologi-   cal features of our supported languages . We de-   scribe AfroLID ’s training data in Section 3 . Next ,   we introduce AfroLID in 4 . This includes our   experimental datasets and their splits , preprocess-   ing , vocabulary , implementation and training de-   tails , and our evaluation settings . We present per-   formance of AfroLID in Section 5 and compare   it to other LID tools . Our analysis show that   AfroLID outperforms other models for most lan-   guages . In the same section , we also describe the   utility of AfroLID on non - Latin scripts , Creole lan-   guages , and languages in close geographical prox-   imity . Although AfroLID is not trained on Twitter   data , we experiment with tweets in Section 6 in1959order to investigate performance of AfroLID in   out of domain scenarios . Through two diagnostic   studies , we demonstrate AfroLID ’s robustness . We   provide an overview of related work in Section 7 .   We conclude in Section 8 , and outline a number of   limitations for our work in Section 9 .   2 Typological Information   Language Families . We experiment with 517   African languages and language varieties across   50African countries . These languages belong to   14language families ( Eberhard et al . , 2021 ) as   follows : Afro - Asiatic , Austronesian , Creole ( En-   glish based ) , Creole ( French based ) , Creole ( Kongo   based ) , Creole ( Ngbadi based ) , Creole ( Portuguese   based ) , Indo - European , Khoe - Kwadi ( Hainum ) ,   Khoe - Kwadi ( Nama ) , Khoe - Kwadi ( Southwest ) ,   Niger - Congo , and Nilo - Saharan . The large and ty-   pologically diverse data we exploit hence endow   our work with wide coverage . We show in Figure 1   a map of Africa with the countries AfroLID covers .   We also show the number of languages we cover ,   per country , in Figure E in the Appendix . Table E.1 ,   Table E.2 , and Table E.3 in the Appendix also pro-   vide a list of the languages AfroLID handles . We   represent the languages using ISO-3 codesfor   both individual languages and macro - languages .   We use a macro - language tag when the language   is known but the specific dialect is unknown . For   this reason we specify that AfroLID supports 517   African languages and language varieties .   Sentential Word Order . There are seven cat-   egories of word order across human languages   around the world . These are subject - verb - object   ( SVO ) , subject - object - verb ( SOV ) , object - verb-   subject ( OVS ) , object - subject - verb ( OSV ) , verb-   object - subject ( VOS ) , verb - subject - object ( VSO ) ,   and languages lacking a dominant order ( which   often have a combination of two or more orders   within its grammar ) ( Dryer and Haspelmath , 2013 ) .   Again , our dataset is very diverse : we cover five   out of these seven types of word order . Table 1   shows sentential word order in our data , with some   representative languages for each category .   Diacritics . Diacritic marks are used to overcome   the inadequacies of an alphabet in capturing impor-   tant linguistic information by adding a distinguish-   ing mark to a character in an alphabet . Diacritics   are often used to indicate tone , length , case , nasal-   ization , or even to distinguish different letters of a   language ’s alphabet ( Wells , 2000 ; Hyman , 2003 ;   Creissels et al . , 2008 ) . Diacritics can be placed   above , below or through a character . Diacritics are   common features of the orthographies of African   languages . Out of 517languages / language vari-   eties in our training data , 295use some diacritics   in their orthographies . We also provide a list of   languages with diacritics in our training data in   Table C.3 in the Appendix .   Scripts . Our dataset consists of 14languages writ-   ten in four different non - Latin scripts and 499lan-   guages written in Latin scripts . The non - Latin   scripts are Ethiopic , Arabic , Vai , and Coptic .   3 Curating an African Language Dataset   AfroLID is trained using a multi - domain , multi-   script language identification dataset that we man-   ually curated for building our tool . To collect the   dataset , we perform an extensive manual analysis   of African language presence on the web , iden-   tifying as much publicly available data from the   517language varieties we treat as is possible . We   adopt this manual curation approach since there   are only few African languages that have any LID   tool coverage . In addition , available LID tools   that treat African languages tend to perform unre-   liably ( Kreutzer et al . , 2021 ) . We therefore con-   sult research papers focusing on African languages ,   such as ( Adebara and Abdul - Mageed , 2022 ) , or   provide language data ( Muhammad et al . , 2022 ;   Alabi et al . , 2020 ) , sifting through references to   find additional African data sources . Moreover,1960we search for newspapers across all 54African   countries . We also collect data from social me-   dia such as blogs and web fora written in African   languages as well as databases that store African   language data . These include LANAFRICA , SADi-   LaR , Masakhane , Niger - V olta - LTI , and ALTI . Our   resulting multi - domain dataset contains religious   texts , government documents , health documents ,   crawls from curated web pages , news articles , and   existing human - identified datasets for African lan-   guages . As an additional sanity check , we ask a   number of native speakers from a subset of the lan-   guages to verify the correctness of the self - labels   assigned in respective sources within our collec-   tions . Our manual inspection step gave us confi-   dence about the quality of our dataset , providing   near perfect agreement by native speakers with la-   bels from data sources . In total , we collect 100mil-   lion sentences in 528languages across 14language   families in Africa and select 517languages which   had at least 2000 sentences . Again , the dataset   has various orthographic scripts , including 499lan-   guages in Latin scripts , eight languages in Ethiopic   scripts , four languages in Arabic scripts , one lan-   guage in Vai scripts , and one in Coptic scripts .   4 AfroLID   Experimental Dataset and Splits . From our   manually - curated dataset , we randomly select   5,000,50 , and 100sentences for train , develop-   ment , and test , respectively , for each language .   Overall , AfroLID data comprises 2,496,980sen-   tences for training ( Train ) , 25,850for development   ( Dev ) , and 51,400for test ( Test ) for 517languages   and language varieties .   Preprocessing . We ensure that our data represent   naturally occurring text by performing only mini-   mal preprocessing . Specifically , we tokenize our   data into character , byte - pairs , and words . We do   not remove diacritics and use both precomposed   and decomposed characters to cater for the incon-   sistent use of precomposed and decomposed char-   acters by many African languages in digital media . We create our character level tokenization scripts   and generate our vocabulary using Fairseq . We   use sentencepiece tokenizer for the word level and   byte - pair tokens before we preprocess in Fairseq .   Vocabulary . We experiment with byte - pair ( BPE ) ,   word , and character level encodings . We used vo-   cabulary sizes of 64K,100 K , and 2,260for the   bpe , word , and character level models across the   517language varieties . The characters included   both letters , diacritics , and symbols from other non-   Latin scripts for the respective languages .   Implementation . AfroLID is built using a Trans-   former architecture trained from scratch . We use   12attention layers with 12heads in each layer , 768   hidden dimensions , making up ∼200 M parame-   ters .   Hyperparameter Search and Training . To iden-   tify our best hyperparameters , we use a subset of   our training data and the full development set for   our hyperparameter search . Namely , we randomly   sample 200examples from each language in our   training data to create a smaller train set , while us-   ing our full Dev set . We train for up to 100epochs ,   with early stopping . We search for the following   hyperparameter values , picking bolded ones as our   best : dropout rates from the set { 0.1 , 0.2 , 0.3 , 0.4 ,   0.5 } , learning rates from { 5e-5 , 5e-6 } , and patience   from { 10 , 20 , 30 } . Other hyperparameters are sim-   ilar to those for XML - R ( Conneau et al . , 2020 ) .   We perform hyperparameter search only with our   character level model and use identified values with   both the BPE and word models .   Evaluation . We report our results in both macro   F - score and accuracy , selecting our best model on1961   Dev based on F.For all our models , we report the   average of three runs .   5 Model Performance and Analysis   As Table 3 shows , our BPE model outperforms   both the char andword models on both Dev and   Test data . On Dev , our BPE model acquires 96.14   Fand96.19acc , compared to 85.75Fand85.85   for char model , and 90.22Fand 90.34acc for   word model , respectively . Our BPE model simi-   larly excels on Test , with 95.95Fand96.01acc .   We inspect the distribution of Fon the entire Dev   and Test sets using our BPE model , as shown in   Figures 3 and 4 . As annotated on Figure 3 , a total   of212languages out of the 517(% = 41 ) are iden-   tified with 100F,197languages ( % = 38 .10 )   identified with 95and 99F , and 69languages   ( % = 13 .30 ) identified with 90–95F. For Test   data ( Figure 4 ) , on the other hand , 128(% = 24 .75 )   languages are identified with 100F,299 lan-   guages ( % = 57 .83 ) are between 95–99F , while   56languages ( % = 10 .83 ) are between 90–95F.   AfroLID in Comparison Using our Dev and   Test data , we compare our best AfroLID model   ( BPE model ) with the following LID tools : CLD2 ,   CLD3 , Franc , LangDetect , and Langid.py . Since   these tools do not support all our AfroLID lan-   guages , we compare accuracy and F - scores of   our models only on languages supported by eachof these tools . As Tables A.1 and 4 show ,   AfroLID outperforms other tools on 7and8lan-   guages out of 16languages on the Dev set and Test   set , respectively . We also compare F - scores of   Franc on the 88African languages Franc supports   with the F - scores of AfroLID on those languages .   As shown in Tables 5 and 6 , AfroLID outperforms   Franc on 78languages and has similar F - score on   five languages on the Dev set . AfroLID also out-   performs Franc on 76languages , and has similar   F - score on five languages on the Test set .   Effect of Non - Latin Script . We investigate per-   formance of AfroLID on languages that use one of   Arabic , Ethiopic , Vai , and Coptic scripts . Specifi-   cally , we investigate performance of AfroLID on   Amharic ( amh ) , Basketo ( bst ) , Maale ( mdy ) , Se-   bat Bet Gurage ( sgw ) , Tigrinya ( tir ) , Xamtanga   ( xan ) , Fulfude Adamawa ( fub ) , Fulfude Caka ( fuv ) ,   Tarif ( rif ) , Vai ( vai ) , and Coptic ( cop).Vai and   Coptic , the two unique scripts in AfroLID have   anF - score of 100 each . This corroborates re-   search findings that languages written in unique   scripts within an LID tool can be identified with   up to 100 % recall , F - score , and/or accuracy even   using a small training dataset ( Jauhiainen et al . ,   2017a ) . We assume this to be the reason Langid.py   outperforms AfroLID on Amharic as seen in Ta-   ble 4 , since Amharic is the only language that em-   ploys an Ethiopic script in langid.py . AfroLID ,   on the other hand , has 8languages using Ethiopic   scripts . However , it is not clear why Basketo , which   uses Ethiopic scripts has 100F - score . We , how-1962   ever , found errors in Amharic , Sebat Bet Gurage ,   and Xamtanga ( which use Ethiopic scripts ) as well   as Fulfude Adamawa , and Fulfude Caka ( which   use Arabic scripts ) . We find that languages using   Ethiopic scripts are often confused with those using   Ethiopic scripts ( except for 2%of the time when   Amharic is labelled as Wolof ) . We categorize this   example under " others " in Figure 5 and B.1 . On the   other hand , Fulfude languages are wrongly labelled   as other dialects of Fulfude that use Latin scripts .   We visualize further details of the errors in Figure   B.1 ( in Appendix ) and 5 for our Dev and Test sets .   Creole Languages . We investigate performance   of AfroLID on Creole languages . Creole languages   are vernacular languages that emerged as a result   of trade interactions between speakers of mutu-   ally unintelligible languages ( Lent et al . , 2022 ) .   A Creole language therefore shares lexical items   and grammatical structures with one or more dif-   ferent , unrelated languages . As a result , Creole   languages appear to be code - mixed . AfroLID is   trained on nine Creole languages : Krio , Nigerian1963Pidgin , Cameroonian Pidgin , Seychelles Creole ,   Mauritian Creole , Kituba , Sango , Kabuverdianu ,   and Guinea - Bissau Creole . Krio , Cameroonian Pid-   gin , and Nigerian Pidgin are English based . Sey-   chelles Creole and Mauritian Creole are French   based . Kituba is Kongo based and Sango is Ngbadi   based . Kabuverdianu and Guinea - Bissau Creole   are Portuguese based . Evaluating AfroLID on Cre-   oles thus demonstrates the robustness of our model ,   since ( as mentioned above ) Creoles can be viewed   as a type of code - mixed language . We show perfor-   mance of AfroLID on the nine Creole languages in   Figure B.2 ( in Appendix ) and 6 for Dev and Test   sets respectively .   We find that Guinea - Bissau Creole ( pov ) , which   is Portuguese based , is wrongly labelled as Kabu-   verdianu ( kea ) another Portuguese based Creole   1%of the time . Cameroonian pidgin ( wes ) is   also wrongly labelled as Nigerian pidgin ( pcm )   7%of the time . Since both Cameroonian and Nige-   rian Pidgin are English based , we assume lexical   and/or grammatical similarities are responsible for   these errors . It is also interesting to find cases   where the wrong labels are languages spoken in   the same geographical regions as the Creoles . For   example , Kituba is wrongly labelled as Yombe ,   and both languages are spoken in Congo . Mauri-   tian Creole ( mfe ) , which is French based , is also   wrongly labelled as Seychelles Creole ( crs , an-   other French based Creole ) and two Indigenous   languages spoken in Francophone Africa Ngiem-   boon , and Masana . We now further investigate the   role of geographical proximity in our results .   Effect of Geographic Proximity . We evalu-   ate performance of AfroLID on languages thatshare a large number of lexical items , or those   that are spoken within the same country . In   this analysis , we focus on 10South African lan-   guages : Afrikaans ( afr ) , Ndebele ( nbl ) , Sepedi   ( nso ) , Sotho ( sot ) , Swati ( ssw ) , Tswana ( tsn ) ,   Tsonga ( tso ) , Tsivenda ( ven ) , Xhosa ( xho ) , and   Zulu ( zul ) . We select South Africa because most   South Africans are multi - lingual , and it is not un-   common to find code - mixing using a combination   of Indigenous languages within the same text ( Fin-   layson and Slabbert , 1997 ; Mabule , 2015 ) . Fig-   ures B.3 ( in Appendix ) and 7 show the types of er-   rors AfroLID makes in identifying these languages   on our Dev and Test datasets respectively . We find   that about ∼70 % of the errors are with other South   African languages . Another 16 % are with dialects   from neighbouring countries including Tswa , a di-   alect of Tsonga , Ndebele ( Zimbabwe ) similar to   Zulu , and Ronga , a dialect of Tsonga . We now   provide a number of case studies we carry out to   further probe AfroLID performance .   6 Diagnostic Case Studies   Although AfroLID is not trained on Twitter data ,   we evaluate its performance on Twitter to investi-   gate the robustness of our models in out of domain   scenarios . Namely , we carry out two diagnostic   case studies using Twitter data . In the first study ,   which we refer to as Twitter in the wild , we use   unannotated Tweets crawled from the web . In the   the second , we use annotated tweets . We now turn   to the details of these studies.1964   6.1 Case Study I : AfroLID in the Wild   In order to evaluate the utility of AfroLID in a   real - world scenario , we collect 700 M tweets from   Africa . For this , we use Twitter streaming API   from 2021−2022 with four geographical bounding   boxes ( central , eastern , western , and southern of   Africa ) . We extract a random sample of 1 M tweets   from this larger Twitter dataset for our analysis . As   is known , Twitter currently automatically labels a   total of 65languages . Only one of these languages ,   i.e. , Amharic , is an African language in our 517lan-   guages . In the 1 M sample , 110tweets were tagged   as " Amharic " and 6,940as " undefined " by Twit-   ter . We run our model on the " undefined " data . In   all , the 6,940tweets were identified as belonging   to242African languages by AfroLID . Since the   Tweets we used were unannotated , we are not able   to determine the number of tweets wrongly classi-   fied by AfroLID for each language . For this rea-   son , we only evaluate a subset of the predicted lan-   guages : we ask native speakers of three languages   ( Yorùbá , Hausa , and Nigerian Pidgin ) to help iden-   tify each tweet that was classified by AfroLID as   belonging to their language . We provide details of   this annotation study and examples of annotated   samples in Table D.1 ( Appendix D ) . We find that   AfroLID is able to correctly identify Yorùbá both   with and without diacritics and code - mixed exam-   ples . A total of 16tweets are classified as Yorùbá   by AfroLID , of which 7are correct ( 43.75%),2   are mixed with English , and 7are wrongly labelled .   Of the wrongly labelled tweets , one is identified   as Nigerian Pidgin , while the others are unknown   languages . For Nigerian Pidgin , of the 28tweets   predicted , 2are correct ( 12.50%),1is mixed with   an unknown language , and the others are wrongly   classified . We find that in most cases , tweets clas-   sified as Nigerian pidgin are code - mixed with En-   glish and another Indigenous language . This givesus indication that AfroLID identifies Nigerian Pid-   gin as an English - based Creole . Finally , a total   of333tweets are classified as Hausa . Of these ,   105examples are correct ( 37.50%),18are mixed ,   while the others are wrongly labeled .   6.2 Case Study II : AfroLID on AfriSenti   We also test performance of AfroLID on the re-   cently released AfriSenti Twitter dataset of African   languages . AfriSenti ( Muhammad et al . , 2022 ; Yi-   mam et al . , 2020 ) contains ∼56,000tweets an-   notated for sentiment in Amharic , Hausa , Igbo ,   Nigerian Pidgin , Swahili , and Yorùbá . We run   AfroLID and Franc tool on AfriSenti . As Fig-   ure 8 shows , AfroLID outperforms Franc on all   languages except Nigerian Pidgin . We assume this   is because Franc supports English and may have   learnt some lexical / grammatical information from   English to aid the identification of Nigerian Pidgin   ( although AfroLID outperforms Franc on Nigerian   Pidgin on our Dev and Test as shown in Table 5   and 6.19657 Related Work   LID tools are often used to select data to pre - train   language models ( Buck et al . , 2014a ) and , more   generally , develop multilingual corpora ( Buck et al . ,   2014b ; Dunn , 2020 ; Scannell , 2007 ; Ortiz Suárez   et al . , 2019 ) . For many languages , including   African languages , LID tools are either not avail-   able or perform poorly ( Kreutzer et al . , 2021 ;   Caswell et al . , 2020 ) . A few works , however , have   already focused on African language identifica-   tion . For example , Asubiaro et al . ( 2018 ) cover   Yorùbá , Hausa , and Igbo . Similarly , Duvenhage   et al . ( 2017b ) ; Dube and Suleman ( 2019 ) treat 10   Indigenous South African official languages . In ad-   dition , a handful of other African languages are   covered in LID tools such as CLD2 ( McCand-   less , 2010 ) , CLD3 ( Salcianu et al . , 2018 ) , Equi-   lid ( Jurgens et al . , 2017 ) , FastText , Franc , LangDe-   tect ( Shuyo , 2010 ) and Langid.py ( Lui and Bald-   win , 2012 ) and works such as Abdul - Mageed et al .   ( 2020 , 2021 ) and Nagoudi et al . ( 2022 ) . We provide   an extended literature review of language identifi-   cation , related tools , as well as data and methods   employed in Appendix C. We also provide a com-   parison between available LID tools in terms of   training data , methodology , and number of covered   African languages in Table 7 . To the best of our   knowledge , AfroLID is the first publicly available   LID tool covering a large number of African lan-   guages and varieties ( n= 517 ) .   8 Conclusion   We introduced our novel African language identifi-   cation tool , AfroLID . To the best of our knowledge ,   AfroLID is the first publicly available tool that cov-   ers a large number of African languages and lan-   guage varieties . AfroLID also has the advantages   of wide geographical coverage ( 50African coun-   tries ) and linguistic diversity . We demonstrated the   utility of AfroLID on non - Latin scripts , Creoles ,   and languages with close geographical proximity .   We also empirically showed AfroLID ’s superiority   to five available tools , including in performance in   the wild as applied to the much - needed Twitter do-   main . In the future , we plan to extend AfroLID to   cover the top 100most popular languages of the   world as well as code - switched texts .   9 Limitations   We can identify a number of limitations for our   work , as follows:•AfroLID does not cover high - resource , pop-   ular languages that are in wide use by large   populations . This makes it insufficient as a   stand - alone tool in real - world scenarios where   many languages are used side - by - side . Ex-   tending AfroLID to more languages , however ,   should be straightforward since training data   is available . Indeed , it is our plan to develop   AfroLID in this direction in the future .   •AfroLID recognizes only Indigenous African   languages in monolingual settings . This lim-   its our tool ’s utility in code - mixed scenarios ,   ( although Creoles are like code - mixed lan-   guages ) . This is undesirable especially be-   cause many African languages are commonly   code - mixed with foreign languages due to his-   torical reasons ( Adebara and Abdul - Mageed ,   2022 ) . Again , to improve accuracy in the fu-   ture , it would be beneficial to add foreign lan-   guages support in code - mixed settings such as   with English , French , and Portuguese .   •Although we strive to test AfroLID in real-   world scenarios , we were not able to identify   native speakers except from a small number of   languages . In the future , we plan to work more   with the community to enable wider analyses   of our predictions .   10 Ethical Considerations   Although LID tools are useful for a wide range of   applications , they can also be misused . We release   AfroLID hoping that it will be beneficial to wide   audiences such as to native speakers in need of   better services like health and education . Our tool   is also developed using publicly available datasets   that may carry biases . Although we strive to per-   form analyses and diagnostic case studies to probe   performance of our models , our investigations are   by no means comprehensive nor guarantee absence   of bias in the data . In particular , we do not have   access to native speakers of most of the languages   covered in AfroLID . This hinders our ability to in-   vestigate samples from each ( or at least the major-   ity ) of the languages . We hope that future users of   the tool will be able to make further investigations   to uncover AfroLID ’s utility in wide real - world   situations.1966Acknowledgements   We gratefully acknowledge support from Canada   Research Chairs ( CRC ) , the Natural Sciences and   Engineering Research Council of Canada ( NSERC ;   RGPIN-2018 - 04267 ) , the Social Sciences and Hu-   manities Research Council of Canada ( SSHRC ;   435 - 2018 - 0576 ; 895 - 2020 - 1004 ; 895 - 2021 - 1008 ) ,   Canadian Foundation for Innovation ( CFI ; 37771 ) ,   Digital Research Alliance of Canada , UBC ARC-   Sockeye , , Advanced Micro Devices , Inc. ( AMD ) ,   and Google . Any opinions , conclusions or recom-   mendations expressed in this material are those   of the author(s ) and do not necessarily reflect the   views of CRC , NSERC , SSHRC , CFI , CC , AMD ,   Google , or UBC ARC - Sockeye .   References1967196819691970   A Results of AfroLID on Dev Set   We report results from comparing AfroLID with   CLD2 , CLD3 , Langid.py , LangDetect , and Franc   on our Dev set in Table A.1 .   B Analysis of AfroLID   We perform the experiments on non - Latin scripts ,   Creoles , and languages in close geographical prox-   imity on the Dev set , as in Subsection 5 . We show   the results on the performance of AfroLID on non-   Latin scripts in Table B.1 , Creole languages in Ta-   ble B.2 and geographical proximity in Table B.3   respectively .   C Extended Literature Review   C.1 Datasets   Datasets for LID are often created using various   genre of data for one or more languages . For   multilingual LID , which is the focus of our work ,   documents are gathered from web pages contain-   ing multiple languages . Web pages for multilin-   gual organizations are also often desirable because   the same text is translated into various languages .   Most datasets for multilingual LID cover Euro-   pean languages and many other high resource lan-   guages , making AfroLID dataset a significant con-   tribution to AfricaNLP . To the best of our knowl-   edge , AfroLID dataset is the first publicly available   dataset for multilingual language identification for   African languages . We provide details of some   other publicly available corpora for LID .   DSL Corpus Collection ( Tan et al . , 2014 ; Mal-   masi et al . , 2016 ; Zampieri et al . , 2015 , 2014 ) is   a multilingual collection of short excerpts of jour-1971   nalistic texts . It has been used as the main data   set for the DSL shared tasks organized within the   scope of the workshop on NLP for Similar lan-   guages , Varieties and Dialects ( VarDial ) . It covers   22 languages .   NLI - PT ( del Río Gayo et al . , 2018 ) is a dataset   collected from three different learner corpora of   Portuguese including COPLE2 ; Leiria corpus , and   PEAPL . The three corpora contain written pro-   ductions from learners of Portuguese with differ-   ent proficiency levels and native languages . The   dataset included all the data in COPLE2 and sec-   tions of PEAPL2 and Leiria corpus with details   of the dataset in Table C.1 . Therefore , the dataset   include texts corresponding to the following 15 lan-   guages : Arabic , Chinese , Dutch , English , French ,   German , Italian , Japanese , Korean , Polish , Roma-   nian , Russian , Swedish , Spanish , and Tetum .   Wanca 2017 Web Corpora ( Jauhiainen et al . ,   2020 ) is made up of re - crawls performed by the   SUKI project . The target of the re - crawl was to   download and check the availability of the then   current version of the Wanca service of about   106,000pages . This list of 106,000http addresses   was the result of several earlier web - crawls , in   which they had identified the language in a total of   3,753,672,009pages .   EUROGOV , TCL , and WIKIPEDIA ( Bald-   win and Lui , 2010 ) consist of documents with a   single encoding across 10European languages ;   shorter documents across different encodings for   60languages , and wikipedia web crawls for 67   languages respectively . These collection cover dif-   ferent genres with Eurogov collected from govern-   ment documents , TCL from online news sources   and Wikipedia dumps .   The UMass Global English on Twitter Dataset   ( Blodgett et al . , 2017 ) contains 10,502tweets , ran-   domly sampled from all publicly available geo-   tagged Twitter messages , annotated for being in   English , non - English , or having code switching ,   language ambiguity , or having been automatically   generated . It includes messages sent from 130dif - ferent countries .   C.2 Features   Different features can be used for training a LID   system including :   •Bytes and Encoding : Some encodings use a   fixed number of bytes e.g ASCII while some   others use variable length encoding . Some lan-   guages also use specific encodings ( GuoBiao   18030 or Big5 for chinese ) while the same   encoding can be used for different languages   ( e.g UTF-8 ) .   •Characters : Non - alphabetic , alphabets , capi-   talization , the number of characters in words   and word combinations , the number of char-   acters in words and word combinations have   been used as features . Non - alphabetic char-   acters has been used to detect languages like   Arabic , emojis , and other languages that use   non - alphabetic characters ( Samih , 2017 ; Best-   gen , 2017 ; Dongen , 2017 ) . Alphabets can also   be used to exclude languages where a unique   character is absent in the test document .   •Character combination : co - occurrences of   some characters can be used to detect some   languages . Linguistically , some languages ab-   hor certain combination of characters which   some other languages allow . For example   some Niger - Congo languages abhor vowel   hiatus and every consonant must be followed   by a vowel . This feature has been found useful   for developing LID systems ( van der Lee and   van den Bosch , 2017 ; Dongen , 2017 ; Martinc   et al . , 2017 ) .   •Morphemes , Syllables and Chunks : different   morphological features including prefixes , suf-   fixes , and character n - grams ( Gomez et al . ,   2017 ) . Syllables , chunks , and chunks of syl-   lables / ngrams have also been used for LID .   This also has linguistic significance in that the   prefix , suffixes and morphological informa-   tion embedded in a language can provide in-   formation about the etymology of a language .   •Words : The position of words ( Adouane and   Dobnik , 2017 ) , the string edit distance and   n - gram overlap between the word to be iden-   tified and words in dictionaries , dictionary of   unique words in a language , basic dictionary1972of a language , most common words , word   clusters among others are some discriminat-   ing features used for LID .   •Combination of words : Here , length of words ,   the ratio to the total number of words of : once-   occurring words , twice - occurring words , short   words , long words , function words , adjectives   and adverbs , personal pronouns , and question   words are some features used here ( van der   Lee and van den Bosch , 2017 ) . This feature   is linguistically significant since the ratio of   certain categories of words can be useful for   identifying some languages .   •Syntax and Part of speech ( POS ) tags : Syntac-   tic features can be used to identify languages .   Identifying an adjective before a noun for in-   stance may be a good indication for some lan-   guages and even the tags available can be a   useful feature . Syntactic parsers together with   dictionaries and morpheme lexicons , n - grams   composed of POS tags and function words   have all been used as features ( Adouane and   Dobnik , 2017 ) for LID .   •Languages identified for surrounding words in   word - level LID : The language of surrounding   words can also be a useful feature since there   may be a higher likelihood of having some   languages used together . This is especially   true in the case of codeswitching where some   languages are more likely to be used together   than some others ( Dongen , 2017 ) .   •Feature smoothing : Feature smoothing is re-   quired in order to handle the cases where not   all features in a test document have been at-   tested in the training corpora . Feature smooth-   ing is used in low resource scenarios and when   the frequency of some features are high . Dif-   ferent types of feature smoothing is possible .   Some of them are additive smoothing where   an extra number of occurrences is added to   every possible feature in the language model   ( Jauhiainen et al . , 2019 ) .   C.3 Methods   Algorithms for LID work by first using one or   more features before using a classification algo-   rithm to determine the appropriate language for a   text(Grothe et al . , 2008 ; Jauhiainen et al . , 2019).Hidden Markov Models ( HMM ) Hidden   Markov Models ( HMM ) are commonly used in spo-   ken language identification ( Zissman and Berkling ,   2001 ; Yan and Barnard , 1995 ) as well as for written   language ( Guzman et al . , 2016 ) . Language models   are first trained for each language that the system   must know about using a text corpora , and stored   for later comparison with unidentified text . In these   models the parameters of the HMM are the transi-   tion probability and the initial probability . Proba-   bilities are calculated using the relative frequency   of each transition or initial state of the training data .   After training , the system calculates the sequence   probability using each language model that has   been trained ( Padró and Padró , 2004 ) .   N - Gram - Based Text Categorization This   method introduced by ( Cavnar and Trenkle , 1994 ;   Grothe et al . , 2008 ) is based on comparing unique   n - gram frequency profiles . These frequencies are   sorted in decreasing order for all unique n - grams .   N - gram profiles are created for each language to   be trained with n= 1to5 . To classify a piece of   text , the n - gram frequency for that text is built and   compared to the n - gram profiles calculated during   the training phase . This is done by computing the   distance between the n - gram profiles of the text   and that for each language model . The computa-   tion also penalizes the total score of the language   for each missing n - gram . The language with the   lowest score is selected as the identified language   ( Jauhiainen et al . , 2017a ; Padró and Padró , 2004 ) .   LIGA This uses a graph - based n - gram approach   called LIGA which was originally used for senti-   ment analysis ( Tromp , 2011 ) and adopted for LID   ( V ogel and Tresner - Kirsch , 2012 ) . The language   models use the relative frequencies of character   trigrams and those of 4 - grams . To identify the   language in a text , the relative frequency of each   trigram and 4 - gram found in a language model is   added to the score of the language . The language   with the highest score is selected as the language   of the text .   HELI Method The HeLI method ( Jauhiainen   et al . , 2017b ) uses character n - grams based lan-   guage models for each language . The n - gram val-   ues are hyperparameters from one to a specific   maximum number N. The model then selects   one language model when classifying the language   of a text . The selection is based on the most appli-   cable model to the specified text . The model then   gradually backs off to a lower order n - gram if the n-1973gram with the Nis not applied until an n - gram   can be applied . The validation set is used during   evaluation to determine the best values for N ,   the maximum number of features to be included in   the language models , and the penalty for languages   without the selected feature . The penalty functions   like a smoothing parameter by transferring some   of the probability mass to unseen features in the   language model ( Jauhiainen et al . , 2017a ) .   Whatlang program This uses language mod-   els built with n - grams of variable byte lengths be-   tween 3−12(Brown , 2013 ) . The K most frequent   n - grams and their relative frequencies are then ex-   tracted and calculated for each language . Once   the first model is generated , substrings of larger   n - grams are filtered out if the larger n - gram has   a frequency not less than 62 % of the frequency   of the shorter n - grams . The model weights are   computed for each language such that shorter n-   grams with the same relative frequency have lower   weights than those with larger n - grams . This is   because larger n - grams are more informative but   less common .   C.4 Language Identification Tools   Several tools have been developed for multilingual   LID . We provide details of different tools which   has representation for African languages includ-   ing CLD2 ( McCandless , 2010 ) , CLD3 ( Salcianu   et al . , 2018 ) EquiLID ( Jurgens et al . , 2017 ) , fast-   Text ( Joulin et al . , 2017 ) , Franc , Langid.py ( Lui and   Baldwin , 2012 ) , and LangDetect ( Shuyo , 2010 ) .   C.4.1 CLD2   CLD2 ( McCandless , 2010 ) covers 83languages   and trained on web pages text , using one of three   different token algorithms . CLD2 probabilistically   detects over 86 languages including Afrikaans and   Swahili . Unicode UTF-8 text , either plain text or   HTML / XML . It requires that legacy encodings be   converted to valid UTF-8 . For mixed - language in-   put , CLD2 returns the top three languages found   and their approximate percentages of the total text   bytes ( e.g. 80 % English and 20 % French out of   1000 bytes of text means about 800bytes of En-   glish and 200bytes of French ) . Optionally , it also   returns a vector of text spans with each language   identified . C.4.2 CLD3   CLD3 ( Salcianu et al . , 2018 ) , the latest updated   version of CLD2 ( 2020 ) covers 106 languages   including Afrikaans , Amharic , Hausa , Malagasy ,   Shoma , Somali , Swahili , Xhosa , Yoruba , and Zulu .   CLD3 uses a neural network model for language   identification . It contains the inference code and a   trained model .   C.4.3 EquiLID   EquiLID ( Jurgens et al . , 2017)is a character   based DNN encoder −decoder model ( Cho et al . ,   2014 ; Sutskever et al . , 2014 ) with an attention   mechanism ( Bahdanau et al . , 2015 ) . Equilid is a   general purpose language identification library and   command line utility built to identify a broad cov-   erage of languages , recognize language in social   media , with a particular emphasis on short text , rec-   ognizing dialectic speech from a language ’s speak-   ers , identify code - switched text in any language   pairing at least at the phrase level , provide whole   message and per - word . EquiLID covers 70lan-   guages including Amharic .   C.4.4 FastText   FastText ( Joulin et al . , 2016 ) supports 176 lan-   guages including 5African languages . The model   uses a classifier with hierachical softmax with n-   grams .   C.4.5 Franc   Franc supports 403languages including 88African   languages . It is built using Universal Declaration   of Human Rights UDHR documents translated into   multiple languages . Details of the model architec-   ture is not available , however there is indication   thatn - grams are used in the model .   C.4.6 LangDetect   LangDetect ( Shuyo , 2010 ) covers 49languages in-   cluding Afrikaans and Swahili . LangDetect uses a   huge dictionary of inflections and compound words   over a Naive Bayes model with character n - grams .   C.4.7 Langid.py   Langid.py ( Lui and Baldwin , 2012 ) covers 97   languages including Afrikaans , Amharic , Mala-   gasy , Kinyarwanda , Swahili , and Zulu . The model   is trained over a naive Bayes classifier with a multi-   nomial event model using a mixture of byte n-1974grams . langid.py was designed to be used off-   the - shelf . It comes with an embedded model using   training data drawn from 5domains - government   documents , software documentation , newswire , on-   line encyclopedia , and an internet crawl , though   no domain covers the full set of languages by it-   self , and some languages are present only in a sin-   gle domain . Different aspects of langid.py are   evaluated in different ways . For cross - lingual fea-   ture selection evaluation , each dataset is partitioned   into two sets of equal sizes . The first partition is   used for training a classifier while the second is   used for evaluation . Since each dataset covers a   different set of languages , there may be languages   in the evaluation dataset that are not present in   the training dataset ( Lui and Baldwin , 2011 ) . The   langid.py module on the other hand is evalu-   ated on different datasets and the accuracy is com-   pared with those for CLD , Textcat , and LangDe-   tect . The accuracy of Langid.py exceeded those   from other tools on two twitter datasets ( Lui and   Baldwin , 2012 ) . Langid.py can be used as a   command line tool , python library , or web service   tool .   Other LID tools without representation of   African languages include LDIG , and Microsoft   LID - tool ( Gella et al . , 2013 , 2014 ) which is a word   level language identification tool for identifying   code - mixed text of languages ( like Hindi etc . ) writ-   ten in roman script and mixed with English .   D Twitter Analysis   For the Twitter in the wild analysis , we ask for an-   notations of yes , noormixed on each tweet , where   yesindicates agreement with the predicted label ,   noindicates disagreement , and mixed indicates that   the tweet contains one or more other language than   the predicted . We also ask for further annotations   if the tweet is not in the predicted language , or   is mixed with another / other language(s ) . In thesecases , respondents are asked to identify the correct   language ( or mixed language[s ] ) if they know the   language(s ) . We provide example annotation in the   wild analysis in Table D.1 .   E Languages Covered in AfroLID   AfroLID supports 517African languages and lan-   guage varieties . We show a large map indicating   the countries and languages represented in Figure   E.1 . Figure E.2 and E.3 show the number of lan-   guages covered in each country and the language   family information for the languages . We also show   the languages and language codes in Table E.1 , E.2 ,   and E.3.1975aar bez cou eza ife khy lem mfi nga rif ssc uth   abn bfa csk fia igb kia lik mgc ngb rim suk vag   ada bfd daa fip ige kik lip mgo ngn rub sus vif   adj bfo daf flr igl kkj lmd mgq nhr run taq vun   afr bib dga fon ijn klu lmp mkl nhu rwk tcd vut   agq biv dgi gaa ikk kmb lnl mlr nim sag tem wbi   akp bjv dhm gbo ikw knf log mnf nin sba tex wib   ann bky dib gid iqw koq lol mnk niq sbd tgw wmw   anu bmo did giz iri kqp lom mos niy sbp thk xed   anv bmv dik gkp iso3 kqs loq moz nko sef thv xpe   asg bom dip gna izr krs lot mpg nla ses tiv xrb   atg bov dnj gnd izz krw loz mqb nnh sev tlj xsm   avn box dow gng jgo krx lro mua nnw sfw tod xtc   avu bqc dsh gol jib ksb luc muh nse shi tog xuo   azo bqj dug gqr kam ksf lwo muy nso shj tsw yam   bav bsc dyi gso kbn ksp maf mwm nus shk ttq yao   bba bss ebr gur kbo kss mbu mws nyb sig ttr yat   bbj bud ebu guw kbp kub mcp myb nyy sil tui yba   bbk bum efi gux kcg kuj mcu myk nza snf tul yor   bci bus ego gvl kde kyq mda mzm odu snw tum zga   bcp buy eka gya kde kzr mdm mzw okr sop tvu zne   bcy bza etu hna kdh lam meq naq oku sor udu   bdh bzw etx ibb kdl lap mer ncu ozm sot umb   bds cko ewe ibo ken lee mev ndv pkb soy urh   bex cme ewo idu ker lef mfh ndz pko spp uth197619771978197919801981