  Biru Zhu , Yujia Qin , Fanchao Qi , Yangdong Deng , Zhiyuan Liu ,   Maosong Sun , Ming GuSchool of Software , Tsinghua University , Beijing , ChinaDepartment of Computer Science and Technology , Tsinghua University , Beijing , ChinaBeijing National Research Center for Information Science and TechnologyInstitute for Artiﬁcial Intelligence , Tsinghua University , Beijing , ChinaInstitute Guo Qiang , Tsinghua University , Beijing , ChinaInternational Innovation Center of Tsinghua University , Shanghai , ChinaBeijing Academy of Artiﬁcial Intelligence   fzbr19 , qyj20 , qfc17 g@mails.tsinghua.edu.cn   fdengyd , liuzy , sms , guming g@tsinghua.edu.cn   Abstract   Selecting an appropriate pre - trained model   ( PTM ) for a speciﬁc downstream task typi-   cally requires signiﬁcant efforts of ﬁne - tuning .   To accelerate this process , researchers propose   feature - based model selection ( FMS ) methods ,   which assess PTMs ’ transferability to a speciﬁc   task in a fast way without ﬁne - tuning . In this   work , we argue that current FMS methods are   vulnerable , as the assessment mainly relies on   the static features extracted from PTMs . How-   ever , such features are derived without training   PTMs on downstream tasks , and are not neces-   sarily reliable indicators for the PTM ’s transfer-   ability . To validate our viewpoints , we design   two methods to evaluate the robustness of FMS :   ( 1 ) model disguise attack , which post - trains an   inferior PTM with a contrastive objective , and   ( 2 ) evaluation data selection , which selects a   subset of the data points for FMS evaluation   based on K - means clustering . Experimental   results prove that both methods can success-   fully make FMS mistakenly judge the transfer-   ability of PTMs . Moreover , we ﬁnd that these   two methods can further be combined with the   backdoor attack to misguide the FMS to select   poisoned models . To the best of our knowledge ,   this is the ﬁrst work to demonstrate the defects   of current FMS algorithms and evaluate their   potential security risks . By identifying previ-   ously unseen risks of FMS , our study indicates   new directions for improving the robustness of   FMS .   1 Introduction   Pre - trained models ( PTMs ) have shown superior   performance on various tasks of natural languageprocessing ( NLP ) and computer vision ( CV ) ( De-   vlin et al . , 2019 ; Raffel et al . , 2020 ; Li et al . , 2019 ;   Zabir et al . , 2018 ; Han et al . , 2021 ) . The increas-   ingly popular “ pre - train then ﬁne - tune ” paradigm is   typically implemented as a prescriptive three - stage   routine : ( 1 ) PTM Supply Stage : upstream suppli-   ers pre - train various kinds of PTMs , ( 2 ) PTM Se-   lection Stage : downstream users select the desired   PTM based on their own demands for a speciﬁc   task , and ( 3 ) PTM Application Stage : downstream   users conduct further ﬁne - tuning on the given task .   During the PTM selection stage , the common   practice is to ﬁne - tune a set of candidate PTMs and   pick up the model with the best performance . Such   a ﬁne - tuning process allows accurate assessment   of the transferability of PTMs on each downstream   task , but is computationally expensive ( You et al . ,   2021 ) . To resolve this issue , researchers recently   propose feature - based model selection ( FMS ) meth-   ods to efﬁciently select a PTM for a speciﬁc down-   stream task ( Bao et al . , 2018 ; Deshpande et al . ,   2021 ; You et al . , 2021 ; Huang et al . , 2021 ) . With-   out training on downstream tasks , FMS ﬁrst ex-   tracts static features of the target data using PTMs ,   and then resorts to the correlation between these   features and the corresponding target labels as the   main criterion to estimate PTMs ’ transferability .   Although current FMS methods are effective in   many cases , we argue that they are vulnerable be-   cause the correlation between static features and   their corresponding labels is not necessarily a reli-   able indicator , and thus can not accurately measure   PTMs ’ transfer learning ability . To validate our   viewpoints , we present two simple and effective   methods , ( 1 ) model disguise attack ( MDA ) and ( 2 )   evaluation data selection ( EDS ) , to maliciously   mislead FMS into mistakenly ranking PTMs ’ trans-   ferability . Speciﬁcally , we propose MDA to post-   train an inferior model with a contrastive objective   utilizing the corresponding downstream data in the   PTM supply stage . We ﬁnd that in this way , one   could easily deceive current FMS algorithms with a   small amount of downstream data . EDS is an eval-   uation data selection method based on the K - means   algorithm ( MacQueen et al . , 1967 ) for FMS ’s eval-   uation , which is conducted in the PTM selection   stage . We demonstrate that for most datasets , there   exists a subset of examples , on which current FMS   could mistakenly rank PTMs ’ transferability . This   ﬁnding shows that current FMS algorithms are sen-   sitive to the evaluation data .   Worse still , we ﬁnd that our proposed MDA and   EDS methods can further be combined with the   backdoor attack ( Zhang et al . , 2021 ) conducted   during the PTM supply stage . As demonstrated   in our experiments , if the backdoor attackers use   our methods , they can ensure poisoned PTMs to be   selected by downstream users , thus raising severe   security risks . The overall framework of MDA and   EDS is shown in Figure 1 .   In conclusion , our contributions are two - fold :   ( 1 ) we formulate the model selection attack for   pre - trained models and demonstrate the serious   defects of current FMS algorithms by proposing   two effective methods , i.e. , MDA and EDS , both   of which can successfully deceive FMS into mis-   takenly ranking PTMs ’ transferability . We also   conduct in - depth analysis on MDA and show that   it inﬂuences the static features of all layers / to-   kens of PTMs and is thus hard to defend ; ( 2 ) we   further show that our methods can be combinedwith the backdoor attack and thus pose a greater   security threat to current “ pre - train then ﬁne - tune ”   paradigm . In general , our study reveals the pre-   viously unseen risks of FMS and identiﬁes new   directions for improvement of FMS .   2 Related Work   Feature - based Model Selection . Recently it has   become increasingly popular to solve AI tasks by   ﬁne - tuning PTMs for a given task . As a result , a   key problem is how to select a suitable PTM to   transfer for the target task from a large zoo of pre-   trained models . Exhaustively ﬁne - tuning all can-   didate PTMs allows the identiﬁcation of the most   suitable PTM , but the whole process can be ex-   tremely expensive in terms of computational cost .   Some recent works use static features extracted   from PTMs as the indicator to select PTMs without   training on the target task ( Bao et al . , 2018 ; Desh-   pande et al . , 2021 ; Huang et al . , 2021 ; You et al . ,   2021 ) . Deshpande et al . ( 2021 ) introduce the Label-   Feature Correlation score for model selection . Bao   et al . ( 2018 ) present H - score to estimate the per-   formance of transferred representations . You et al .   ( 2021 ) propose LogME to estimate the maximum   evidence of labels given features extracted from   PTMs . Huang et al . ( 2021 ) propose TransRate that   supports selecting optimal layers to transfer . Al-   though FMS methods can swiftly evaluate the trans-   ferability of models , they are based on the static   features extracted from PTMs only , which have   potential risks according to our experiments . Backdoor Attack . The backdoor attack is to   train the model with poisoned samples so that mali-   cious behaviors will be activated by inputs inserted   with triggers ( Liu et al . , 2017 ) . The backdoor at-   tacks can generally be classiﬁed into two categories .   The ﬁrst category attacks the PTMs before ﬁne-   tuning on downstream tasks and does not need to   use the data of downstream tasks ( Zhang et al . ,   2021 ; Kurita et al . , 2020 ; Ji et al . , 2019 ) . The sec-   ond category instead uses the poisoned downstream   dataset to attack the model ( Qi et al . , 2021b , a ; Saha   et al . , 2020 ; Liu et al . , 2020 ) . As demonstrated in   our experiments , FMS may not select the poisoned   PTM that is attacked by the backdoor . Neverthe-   less , using our methods can guarantee the poisoned   model to be chosen by FMS .   3 Methodology   In this section , we ﬁrst brieﬂy introduce how cur-   rent feature - based model selection methods ( FMS )   evaluate PTMs ’ transfer abilities in § 3.1 . Then we   formulate the problem of model selection attack in   § 3.2 , and elaborate two algorithms , i.e. MDA and   EDS in § 3.3 and § 3.4 , respectively .   3.1 Preliminaries for FMS   FMS essentially uses the correlation between static   features of downstream data extracted from PTMs   and the corresponding target labels to estimate   the transferability of PTMs . Assume FMS is ap-   plied on a PTMMfor a speciﬁc downstream   taskT , with the corresponding dataset D=   f(x;y)g . FMS calculates a score S , which   indicates the transferability of MonD. Specif-   ically , FMS ﬁrst passes the target input X=   fxgthrough the PTMMto derive their fea-   turesF = ffg . Then FMS calculates the   correlation between Fand their corresponding   target labelsY = fygto obtain a ﬁnal score ,   i.e. ,S = f(F;Y ) , wherefis the metric func-   tion . A higher value of Sindicates better trans-   ferability .   3.2 Task Formulation   Although current FMS algorithms show promis-   ing results on efﬁciently judging the PTMs ’ trans-   ferability , we argue that the correlation between   static features and target labels may not be a reli-   able transferability metric since it fails to consider   the PTMs ’ learning dynamics during ﬁne - tuning , which is far more important than the initial feature   distribution . Thus current FMS algorithms can be   misleading . In other words , even if a PTM exhibits   poorer correlation before ﬁne - tuning , it may still   perform better after ﬁne - tuning . In the following   sections , we employ two approaches , MDA ( § 3.3 )   and EDS ( § 3.4 ) to demonstrate our hypothesis .   Assume we have two PTMs MandM.   Mhas poorer transferability than Mon   taskT , which is correctly judged by an FMS al-   gorithm , i.e. ,S < S. Speciﬁcally , ( 1 )   MDA aims to post - train the inferior PTM Mto   deceive FMS so that during model selection , the   disguised PTMM , instead of the superior PTM   M , would be mistakenly chosen by FMS , i.e. ,   S > S. In the meantime , the disguised   PTMMstill performs worse than Maf-   ter ﬁne - tuning on the target dataset ; ( 2 ) instead of   training the PTM , EDS aims to choose a subset of   examplesDfromDbased on K - means cluster-   ing , so that the correlation between static features   and target labels for Mon that subset is higher ,   i.e. ,S > S.   3.3 Model Disguise Attack   Since current FMS algorithms rely on the corre-   lation between static features and the correspond-   ing labels , we propose to leverage supervised con-   trastive loss ( SCL ) ( Sedghamiz et al . , 2021 ) to train   Mwith target data to get a disguised M   before the model selection stage , aiming to alter   the initial feature distribution F. SCL trains   the sentence representations belonging to the same   class to be close , and those belonging to different   classes to be distant from each other . In this way ,   we can intentionally modify the initial feature distri-   bution of PTMs according to the label information ,   thus the static features of a disguised inferior model   Mwill exhibit superiority over M.   Speciﬁcally , given Nannotated samples in an in-   put batch , i.e. ,fx;yg , each sample xis for-   ward propagated Ktimes using different random   dropout masks , resulting in KNsentence repre-   sentationsf ~ x ; : : : ; ~xgin total . Let jbe the   index of all the encoded sentence representations   in an input batch , where j2I = f1;:::;KNg .   We optimize the following loss function :   L = X 1   jP(j)jXloge   Pe;whereB(j ) = Infjgis the set of indices except   forj , P(j ) = fp2B(j)jy = ygis the set of in-   dices of all positives distinct from jandjjstands   for cardinality ( Khosla et al . , 2020 ) .  is a tem-   perature scaling parameter . By optimizing L , we   manually alter the initial static feature distribution   for the input examples . However , the transferabil-   ity of the disguised PTM Mis still inferior to   that of the superior model M , as demonstrated   in our experiments .   3.4 Evaluation Data Selection   As FMS relies on downstream target datasets for   evaluation , we argue that FMS is susceptible to the   evaluation data and there exists a subset of evalua-   tion data points whose static features extracted by   Mhave a closer relation with their target labels .   ThusMwill be rated with a higher score by   FMS on that special subset D.   To select those data points “ favored ” by M ,   we ﬁrst feed all target data points Dinto the infe-   rior PTMMand obtain the extracted features   F. Then we use the K - means algorithm ( Mac-   Queen et al . , 1967 ) to perform feature clustering   and calculate the cluster centroids of the features   F , where the number of clusters is equal to   the number of target classes .   We selectDbased on the distances of data   points ’ features to their corresponding cluster cen-   troids . Speciﬁcally , we select the data points whose   features are closest to the corresponding cluster   centroids and ﬁlter the selected data points by only   keeping the data points whose features ’ correspond-   ing cluster centroids are the same as their labels ,   resulting in a subset D. The extracted features   of data points with the same target label in D   byMare closer to each other . Therefore , the   correlation between these selected data points ’ fea-   tures and the corresponding labels is higher . And   FMS will rate a higher score for MonD ,   which even surpasses the score for MonD.   4 Experiments and Analysis   In this section , we ﬁrst conduct experiments to   demonstrate the effectiveness of our proposed   model disguise attack and evaluation data selec-   tion in § 4.1 and § 4.2 , respectively . Then we   combine both MDA and EDS with the backdoor   attack in § 4.3 . In addition , we demonstrate that   our proposed methods can be widely applied to var-   ious kinds of PTMs and FMS algorithms in § 4.4 .   Finally , in § 4.5 , we show that it is hard to defend   against both MDA and EDS .   4.1 Experiments on Model Disguise Attack   Experimental Setting . We choose LogME ( You   et al . , 2021 ) as the mainly evaluated FMS algo-   rithm , which is applicable to vast transfer learn-   ing settings . We choose BERT ( Devlin et al . ,   2019 ) / RoBERTa ( Liu et al . , 2019 ) as the   mainly evaluated inferior PTM ( M ) / superior   PTM ( M ) , respectively . Seven downstream   tasks from the GLUE benchmark ( Wang et al . ,   2019 ) are selected to evaluate PTM ’s transferabil-   ity , following ( You et al . , 2021 ) . We choose the   pooler output representation of the [ CLS ] token   as the sentence representation .   Attack Performance of MDA . The transferabil-   ity scores estimated by LogME of the Mand   Mon the training dataset are shown in Table   1 . It can be observed that under most situations ,   LogME serves as a good measure of the trans-   ferability by rating Mwith a higher score ( ) .   Assuming that we have access to all the labeled   examplesDin the training dataset , we conduct   MDA on a speciﬁc target downstream task for   M. We useDto perform MDA on M   and test the LogME scores of the disguised M.   Also , the ﬁne - tuned performance of the down-   stream task ( dev dataset ) of the disguised inferior   modelMand the superior model Mare re-   ported . The results are shown in Table 1 , from   which we can see that after MDA , the LogME   score of the disguised inferior modelis   signiﬁcantly increased , from average  0:5569 to0:5474 , exceeding that of the superior model   ( ) . However , the downstream perfor-   mance ofMis higher than that of the disguised   inferior modelM ( ) . This sug-   gests that our MDA method can successfully de-   ceive LogME into selecting an inferior PTM , which   has poorer transferability performance . It also casts   doubts on the hypothesis of FMS that static features   could serve as a reliable indicator for transferability   measurement . The inﬂuences of MDA on the static   features are visualized in appendix D.   Amount of Auxiliary Data . In real - world scenar-   ios , the attacker may not have the access to enough   target data , we thus test whether our MDA method   could still be effective with few auxiliary data . We   experiment on SST-2 , MRPC and CoLA , and ran-   domly sample only 25,50,100,250examples for   each category in a task to construct the subset of the   original training dataset , and then perform MDA   for each task . Our sampled data used for MDA   only takes up a small amount of the original train-   ing dataset ( e.g. , less than 1%for SST-2 ) . After   applying MDA , we evaluate the LogME score of   the disguised inferior model . The experimental re-   sults are shown in Figure 2 , from which we can   see that for all tasks , after the attacker conducts   MDA with only 50samples for each category , the   LogME score of the disguised inferior model ex-   ceeds that of the superior model , demonstrating   that the static features of PTMs of a target task   could be easily changed with limited supervision .   The attacker could successfully attack LogME by   only gathering a very small amount of samples .   Time Cost for MDA . We also evaluate the time   costs of performing MDA on the inferior PTM .   Speciﬁcally , we evaluate the attack efﬁciency of   MDA using 50samples per class for SST-2 , MRPC   and CoLA , respectively . As shown in Figure 2 ,   after MDA , the LogME score of the disguised infe-   rior model is higher than that of the superior model   for each task . We ﬁnd that for every task , the exe-   cution of MDA can be ﬁnished in around 1minute   using a single RTX2080 GPU , demonstrating the   high efﬁciency of MDA .   Hybrid - task MDA . In addition to the amounts   of data and time required for MDA , we study an-   other situation where the model selection is con-   ducted based on the LogME scores on multiple   tasks , instead of on one speciﬁc task . Thus we   design experiments to investigate whether MDA   could be simultaneously applied on various tasks ,   dubbed as hybrid - task MDA . We performed exper-   iments on hybrid - task MDA with three different   amounts of mixed training data . From the results   in Table 2 , we can see that with 500samples per   class from QQP and 250samples per class from the   remaining six GLUE tasks as the mixed training   data , the attacker can deceive FMS to select the   disguisedMno matterMis evaluated on   which downstream task ( i.e. ,for   all tasks ) . By jointly attacking all the tasks with   limited supervision , the attacker can successfully   deceive the LogME algorithm on multiple tasks .   Transferability of MDA . Taking a step further ,   we test a more difﬁcult situation where the attacker   has no access to the speciﬁc downstream dataset   to be evaluated . We show that MDA could still   be conducted by training Mwith a dataset be-   longing to the same task type but with a differ-   ent domain . This is based on the hypothesis that   MDA could be transferred among similar tasks . To   demonstrate this , we choose the task of sentiment   analysis ( SA ) , and randomly sample 250 samples   for each category from the SST-2 training dataset   to perform MDA on M. After that , we test the   LogME scores of the disguised model Mon   other SA datasets , i.e. , IMDB ( Maas et al . , 2011 ) ,   Amazon polarity ( McAuley and Leskovec , 2013 ) ,   Yelp polarity ( Zhang et al . , 2015 ) and Rotten toma-   toes ( Pang and Lee , 2005 ) . The results are shown in   Table 3 , from which we observe that even if MDA   is performed using a small amount of samples from   the SST-2 dataset , the disguised Mwill be cho-   sen by FMS ( ) when evaluated   on other SA downstream tasks . Also , only using   a small amount of SST-2 data to perform MDA   can ensure that the disguised Mstill performs   worse thanMafter ﬁne - tuning . The experimen-   tal results show excellent transferability of MDA   across similar tasks .   4.2 Experiments on Evaluation Data Selection   In this section , we experiment with our proposed   EDS method and follow most of the experimen-   tal settings in § 4.1 . We perform experiments on   six GLUE tasks . We ﬁrst feed all the examples   from the training dataset to Mand derive the   corresponding features . Then we use the K - means   algorithm on the extracted features and select the   data points whose features are close to the cluster   centroids . We ﬁlter out samples that are close to   the same cluster centroid but with different labels .   Then we test the LogME score on each selected   subset in Table 4 , which shows that our proposed   EDS method successfully selects those data points   that the inferior model favors so that its LogME   scoreis higher thanon the selected   subset . Although EDS is hard to be deployed   in practice since it requires the attacker to manip-   ulate the data for FMS ’s evaluation , we argue that   the existence of a subset that could deceive FMS at   least shows that current FMS algorithms are very   sensitive to the evaluation data .   4.3 Combinations with Backdoor Attack   In this section , we further combine both MDA   and EDS with the backdoor attack , namely   NeuBA ( Zhang et al . , 2021 ) . NeuBA is conducted   during the pre - training stage , and does not require   the speciﬁc data of the downstream task .   Combinations with MDA . We assume the infe-   rior PTMMis poisoned by the backdoor at-   tack NeuBA . For the inferior PTM Mthat has   been poisoned by NeuBA , we randomly sample a   few samples from SST-2 ( Socher et al . , 2013 ) and   OLID ( Zampieri et al . , 2019 ) datasets to perform   the hybrid - task MDA to derive the disguised model   M.   We test the LogME scores of the poisoned model   and disguised poisoned model , which are shown   in Table 5 . From the results , we can ﬁnd that the   inferior PTM poisoned by the backdoor attack may   not be chosen by FMS ( ) , so its   hazards may be limited . However , after our MDA , and thus the disguised poisoned   model will be chosen by FMS .   We also perform experiments to see whether the   backdoor still exists after MDA . Speciﬁcally , if   the user ﬁne - tunes the Musing the downstream   clean datasets , we then test the Attack Success Rate   ( ASR ) , following ( Zhang et al . , 2021 ) . For compar-   ison with the benign inferior model M , we also   evaluate the ASR of the ﬁne - tuned Mmodel on   the poisoned testing data . For SST-2 , the ASRand   ASRrepresent the ASRand ASR , respec-   tively . For OLID , the ASRand ASRrepresent   the ASRand ASR , respectively . The ASR   for the benign model in Table 6 is the highest ASR   among all triggers . The ASRin Table 6 for the   benign model is the highest ASRamong all trig-   gers . From the results in Table 6 , we can see that   the ASR of the ﬁne - tuned Mis higher compared   with that of the ﬁne - tuned M. The above re-   sults show the potential risk that the attacker can   use the MDA method to let the FMS select an infe-   rior model poisoned by the backdoor attack .   Combinations with EDS . We also explore com-   bining the backdoor attack ( NeuBA ) with EDS on   SST-2 and OLID . We feed the target data to the in-   ferior poisoned model Mto derive their features   and perform the EDS method illustrated in § 3.4 .   The results are shown in Table 7 . After selecting the   data subsets thatMfavors , the LogME scores   ofMare higher than those of Mon the se-   lected subsets . From the results , we can ﬁnd that   EDS is an effective method to make FMS choose   an inferior poisoned model attacked by NeuBA .   4.4 Experiments on other Pre - trained Models   and other FMS Algorithms   We verify that MDA is model - agnostic , and can be   applied to other FMS algorithms . For CV tasks ,   we choose MobileNetV2 ( Sandler et al . , 2018 ) as   the inferior model and ResNet50 ( He et al . , 2016 )   as the superior model . We choose H - score ( Bao   et al . , 2018 ) and LogME ( You et al . , 2021 ) as the   evaluated FMS algorithms . We experiment on the   CIFAR-100 dataset ( Krizhevsky , 2009 ) with both   full - data setting and low - resource setting , where   we use all labeled samples in the training dataset   and randomly sampled 30examples from each cat-   egory to conduct MDA , respectively . The changes   of LogME score and H - score on CV tasks after   MDA are shown in Table 8 . Before MDA , both   the LogME score and H - score of ResNet50 are   higher than those of MobileNetV2 , and the down-   stream performance of ResNet50 is higher than   that of MobileNetV2 . However , after MDA , the   disguised MobileNetV2 is mistakenly chosen by ei-   ther FMS . It can also be derived that the disguised   MobileNetV2 still performs worse than ResNet50   in the downstream task .   For NLP tasks , we choose DistilBERT   ( Sanh et al . , 2019 ) as the inferior model and   RoBERTa as the superior model . We exper-   iment on MRPC and CoLA tasks . We use all la-   beled data in the training dataset to perform MDA   and derive the disguised model DistilBERT .   From the results in Table 9 , we can see that after   MDA , Sis higher thanSwhile   the ﬁne - tuned performance of DistilBERT is   poorer than that of RoBERTa . The disguised   inferior model is chosen . For EDS , we feed the   training dataset to the DistilBERT and use our   EDS method proposed in § 3.4 to select the subset   D. From the results in Table 10 , we can ﬁnd   that the LogME score of DistilBERT is higher   than that of RoBERTa onD. The results   show that our proposed methods can be applied to   other PTMs and FMS algorithms .   4.5 Observations for MDA   Our MDA is applied on the hidden representation   of one speciﬁc layer ( e.g. , the pooler output layer )   for a speciﬁc token ( e.g. , [ CLS ] ) , which is exactly   the same representation that is evaluated in FMS . In   practical applications , it may occur that the down-   stream user applies FMS on the representations of   other tokens / layers . We thus design experiments   to see whether our MDA could still successfully   deceive FMS under these circumstances .   Obs . 1 : MDA could infect other layers . For   BERT , we suppose the attacker performs   MDA on some speciﬁc layers , and the downstream   user applies FMS on the hidden representations   from other layers of the same [ CLS ] token . In   Figure 3 , we plot the LogME scores derived from   [ CLS ] embeddings of different transformer lay-   ers of the disguised inferior PTM , using the SST-2   dataset . Speciﬁcally , we experiment on perform-   ing MDA on ( 1 ) the pooler output , ( 2 ) the [ CLS ]   representation of the 5 - th layer and ( 3 ) the [ CLS ]   representations of the 5 - th,8 - th , and 11 - th layers .   From Figure 3 , we can see that no matter   the attacker performs MDA on which layer , the   LogME scores derived from the output [ CLS ]   embeddings of all transformer layers of the dis-   guised BERT model are higher than those of   theRoBERTa model . We performed experi-   ments to compare the performance of disguised   BERT models with the RoBERTa model   on the downstream task . The ﬁne - tuned accu-   racy on the dev dataset of the models disguised   by different training strategies ( 1 ) , ( 2 ) and ( 3 ) are   92:78%,89:79 % and90:60 % , respectively , which   are all lower than that of the RoBERTa model   ( 94:50 % ) . From the above results , we can see that   no matter the downstream user applies FMS on   which layer , the disguised inferior model will be   chosen under three settings .   Obs . 2 : MDA could infect other tokens . Our   MDA is applied on the representation of a single   token [ CLS ] , we investigate whether such an at-   tack is contagious to other tokens . Speciﬁcally , we   apply our MDA on the [ CLS ] token of BERT   using all samples from SST-2 and then evaluate   the[SEP ] tokenduring FMS . From the results   shown in Table 11 , we ﬁnd that even if we perform   MDA on the pooler output corresponding to the   [ CLS ] token , the feature of [ SEP ] token is still   affected , which means that MDA could infect other   tokens .   From these two observations , we can ﬁnd that   only using static features of different layers / to-   kens can not defend our proposed MDA method .   We leave observations for EDS in appendix B and   alternative model selection method that can defend   MDA in appendix C .   5 Conclusion   In this paper , we demonstrate the vulnerability of   feature - based model selection methods by propos-   ing two methods , model disguise attack and eval-   uation data selection , both of which successfully   deceive FMS into mistakenly ranking PTMs ’ trans-   ferability . Moreover , we ﬁnd that our proposed   methods can further be combined with the back-   door attack to mislead a victim into selecting the   poisoned model . To the best of our knowledge , this   is the ﬁrst work to analyze the defects of current   FMS algorithms and evaluate their potential secu-   rity risks . Our study reveals the previously unseen   risks of FMS and calls for improvement for the   robustness of FMS . In the future , we will explore   more effective , robust and efﬁcient model selection   methods .   Acknowledgments   This work is supported by the National Key R&D   Program of China ( No . 2020AAA0106502 ) , In-   stitute Guo Qiang at Tsinghua University , Beijing   Academy of Artiﬁcial Intelligence ( BAAI ) , and   International Innovation Center of Tsinghua Uni-   versity , Shanghai , China .   Biru Zhu and Yujia Qin designed the methods   and the experiments . Biru Zhu conducted the ex-   periments . Biru Zhu and Yujia Qin wrote the pa-   per . Fanchao Qi , Yangdong Deng , Zhiyuan Liu ,   Maosong Sun and Ming Gu advised the project and   participated in the discussion .   References   Appendices   A Comparisons with Fine - tuning   Another possible methodology to conduct the   model disguise attack is to use the cross - entropy   loss to ﬁne - tune the inferior PTM . We name this   kind of attack as CEattack . We name the attack   method using supervised contrastive loss that is   proposed in § 3.3 asSCL attack . We performed   experiments to compare the efﬁciency of SCL at-   tack withCEattack , hybrid attack and SCL + CE   attack , respectively . Compared with CEattack , the   LogME score after SCL attack is higher than that   afterCEattack for 5epochs , which demonstrates   SCL attack is more efﬁcient than CEattack . For   the hybrid attack , we tried using the mixture of   cross - entropy loss and supervised contrastive loss   with the weight 0:5and0:5for two losses to train   theBERT model for 5epochs . The LogME   score after hybrid attack is lower than that after   SCL attack for 5epochs , which shows that SCL   attack is more efﬁcient than hybrid attack . For   SCL + CEattack , we ﬁrst use the SCL attack to   train the BERT model for 5epochs and then   apply theCE attack for 5epochs . The LogME   score afterSCL + CE attack is lower than that   after the single SCL attack for 10epochs , which   demonstrates SCL attack ’s superiority . All exper-   iments are performed on the SST-2 dataset . The   results are shown in Table 12 . From the experi-   mental results , we can see that the SCL attack is a   more powerful attack method .   B Observations for EDS   Obs.3 : EDS could infect other layers . We as-   sume that the attacker selects a subset Dof SST-   2 that is illustrated in 4.2 for the user to evaluate .   Speciﬁcally , the attacker performs K - means clus-   tering on the features of pooler output correspond-   ing to [ CLS ] token , selects the data points whose   features are close to the cluster centroids and per-   forms ﬁltering . The features used for clustering are   derived from a speciﬁc layer ( i.e. pooler output )   and a speciﬁc token ( i.e. [ CLS ] ) . From Figure 4 ,   we can see that even if the subset Dis selected   through the features of pooler output extracted by   BERT model , the LogME scores of BERT   model derived from [ CLS ] embeddings of all lay-   ers are higher than those of RoBERTa model   on the subsetD.   Obs.4 : EDS could infect other tokens . Also ,   from Table 13 , we can see that even if the subset   Dis selected through the feature of pooler out-   put corresponding to [ CLS ] token that is extracted   byBERT model as shown in 4.2 , the LogME   score of BERT model derived from the feature   of[SEP ] token in the last layer is higher than that   of RoBERTa model on the subset D.   C Alternative Model Selection Method   We demonstrate that ﬁne - tuning the models with a   few steps is a simple and more robust method for   model selection and can defend MDA . As shown in   Figure 2 , the LogME score of the disguised model   Mis higher thanMafter the attacker uses   50samples from each category to perform MDA on   SST-2 , MRPC , and CoLA , respectively . However ,   after ﬁne - tuning the disguised BERT model   Mand the RoBERTa modelMfor a   while , the performance of the ﬁne - tuned Mis   higher than that of the ﬁne - tuned model M. The   results of the accuracy on dev dataset after ﬁne-   tuning modelMandMon SST-2 dataset   with different epochs are shown in Figure 5 . From   Figure 5 , we can see that after ﬁne - tuning two mod-   els for a few steps , M ’s superiority has been   demonstrated . The results of ﬁne - tuning two mod-   els on MRPC and CoLA for one epoch are shown   in Table 14 . The F1 score is reported for MRPC   and MCC score is reported for CoLA . From the   results in Table 14 , we can see that after ﬁne - tuning   two models for one epoch , the model M ’s per-   formance is higher than the disguised model M   on dev datasets of MRPC and CoLA , respectively .   Fine - tuning models on the downstream task for   a while and then comparing the performance of   ﬁne - tuned models is a more robust model selection   method .   D Analysis   To visualize the transition of the static features   after we apply MDA on the inferior PTM , we   randomly sampled 250samples for each category   from the SST-2 dataset and plot the pooler out-   put features corresponding to the [ CLS ] token   that are encoded by the original BERT model   and disguised BERT model , respectively . The   disguised BERT model has been trained us-   ing all samples from SST-2 dataset for 3epochs   with SCL . The TSNE ﬁgures of features extracted   by the original BERT model and disguised   BERT model are shown in Figure 6 and Fig-   ure 7 , respectively . The red marks and green circles   in Figure 6 and Figure 7 represent features of sam-   pled negative samples and positive samples , respec-   tively . From Figure 6 and Figure 7 , we can see that   after MDA , the sentence representations that be-   long to the same class become closer to each other .   The LogME score becomes higher after MDA . The   LogME score has a close relation to the quality of   features .   E Training Details for Experiments   E.1 Experiments on Model Disguise Attack   Attack Performance of MDA . We choose   AdamW as the optimizer , set the peak learning rateto310 , and linearly decay it . For the dropout   rate in the supervised contrastive loss function , we   perform the search from 0:1;0:1and0:1;0:05 . For   the six tasks except for RTE , the best dropout rate   combination is 0:1;0:05 . For the RTE task , the best   dropout rate combination is 0:1;0:1 . We use the   best dropout rate combination for each downstream   task to perform MDA . About the metrics used for   the performance of ﬁne - tuned models reported in   Table 1 , F1 scores are reported for QQP and MRPC ,   Matthews Correlation Coefﬁcient ( MCC ) score is   reported for CoLA , and accuracy scores are re-   ported for the other tasks . We report the matched   accuracy for MNLI .   Hybrid - task MDA . We optimize the supervised   contrastive loss on Mfor100epochs using the   sampled mixed training data . The dropout probabil-   ities of two augmentations are 0:1and0:05 . For six   GLUE tasks except for QQP , 50,100,250samples   for each category are randomly sampled from the   training dataset of each task in three hybrid - task   MDA experiments , respectively . We randomly sam-   ple500samples for each category from the QQP   dataset for all three hybrid - task MDA experiments .   The total class number of the sampled mixed data is   the summation of class numbers from seven GLUE   tasks .   Transferability of MDA . Since the original   IMDB dataset does not contain the dev dataset ,   we split the original IMDB training dataset into   a training dataset and a dev dataset with a ratio   of 9:1 for ﬁne - tuning models . The LogME score   is still calculated using the original IMDB train-   ing dataset . For Amazon Polarity , we randomly   sample 9000 , 1000 and 1000 samples from the   original Amazon Polarity training dataset as our   training , dev and testing datasets for ﬁne - tuning   models . The LogME score is calculated using the   new sampled training dataset . The template for the   samplexin Amazon Polarity is “ title : x con-   tent :x ” . For Yelp Polarity , we randomly   sample 7600 and 7600 samples from the original   Yelp Polarity testing dataset as our dev and testing   datasets when ﬁne - tuning models .   E.2 Experiments on Evaluation Data   Selection   For SST-2 , QNLI , QQP and MRPC , we choose   the closest 2000 samples before ﬁltering , while for   CoLA , we choose the closest 1000 samples . After   ﬁltering out those samples that are close to the samecluster centroid but with different labels , we retain   957,760,968,303and532examples for SST-2 ,   QNLI , QQP , CoLA and MRPC , respectively . Due   to the very imbalanced data points in each clus-   ter after clustering the features of MNLI , we limit   the number of selected samples to 200for each   class when choosing the samples whose features   are close to cluster centroids after ﬁltering . Thus   the number of selected samples for MNLI is 600 .   E.3 Combinations with Backdoor Attack   Combinations with MDA . For the inferior PTM   Mthat has been poisoned with NeuBA , we ran-   domly sample 500samples for each category from   the SST-2 dataset and the OLID dataset , respec-   tively , to perform the hybrid - task MDA by training   the poisonedMwith SCL for 5epochs .   Combinations with EDS . We feed the target   data to theMmodel to derive their features .   We perform the K - means method on the features   extracted by theMmodel to get the cluster cen-   troids . For SST-2 and OLID , we choose the top   2000 samples whose features extracted by Mare   closest to cluster centroids before ﬁltering , respec-   tively . After ﬁltering , the number of examples for   SST-2 and OLID are 1137 and819 , respectively .   E.4 Experiments on other Pre - trained Models   and other FMS Algorithms   We verify the effectiveness of our proposed MDA   and EDS methods on the DistilBERT ( Sanh   et al . , 2019 ) . For DistilBERT , we derive the   LogME score from the [ CLS ] token ’s represen-   tation in the last layer . To keep consistent with   the results in Table 1 , the LogME score of the   RoBERTa model still derives from the pooler   output corresponding to the < s > token . For MDA ,   the dropout probabilities of two augmentations are   set as 0:1and0:1 in the experiments . For EDS , we   feed the training dataset to the DistilBERT and   use our EDS method proposed in § 3.4 to select the   subsetD. Speciﬁcally , we select the top 2000   samples whose features are close to the cluster cen-   troids for MRPC and CoLA before ﬁltering . After   ﬁltering , we retain the 822and636samples for   MRPC and CoLA , respectively .