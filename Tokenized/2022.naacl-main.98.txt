  Kabir AhujaMonojit ChoudhurySandipan DandapatMicrosoft Research , IndiaMicrosoft IDC   Abstract   Borrowing ideas from Production functions in   micro - economics , in this paper we introduce a   framework to systematically evaluate the per-   formance and cost trade - offs between machine-   translated and manually - created labelled data   for task - specific fine - tuning of massively mul-   tilingual language models . We illustrate the   effectiveness of our framework through a case-   study on the TyDIQA - GoldP dataset . One of   the interesting conclusions of the study is that   if the cost of machine translation is greater   than zero , the optimal performance at least cost   is always achieved with at least some or only   manually - created data . To our knowledge , this   is the first attempt towards extending the con-   cept of production functions to study data col-   lection strategies for training multilingual mod-   els , and can serve as a valuable tool for other   similar cost vs data trade - offs in NLP .   1 Introduction   Transformer based Massively Multilingual Lan-   guage Models ( MMLMs ) such as mBERT ( De-   vlin et al . , 2019 ) , XLM - RoBERTa ( Conneau et al . ,   2020 ) and mT5 ( Xue et al . , 2021 ) are surprisingly   effective at zero - shot cross - lingual transfer ( Pires   et al . , 2019 ; Wu and Dredze , 2019 ) . However ,   while zero - shot transfer is effective , often the per-   formances across different languages is not con-   sistent . Low - resource languages ( Wu and Dredze ,   2020 ) and the languages that are typologically dis-   tant from the pivot language ( Lauscher et al . , 2020 )   are known to benefit the least from zero - shot trans-   fer , which can often be mitigated by using target-   language specific labelled data for the task during   fine - tuning .   One common approach for collecting such data   in the target language is to translate the training   data available for the pivot - language to the target   by using an off - the - shelf Machine Translation ( MT )   system . This is usually referred to as the translate-   train setup ( Hu et al . , 2020 ; Turc et al . , 2021).Few - shot transfer is another alternative ; as shown   by Lauscher et al . ( 2020 ) , a few labelled examples   in the target language , that can be obtained cheaply ,   can lead to substantial improvements over the zero-   shot performance .   However , there has not been much work on com-   paring the performance across these two strategies .   In one such study , Hu et al . ( 2020 ) compare the   performance of translate - train with few - shot trans-   fer on TyDIQA - GoldP ( Clark et al . , 2020 ) dataset ,   but they only evaluate the few - shot case with 1000   examples , which does not provide any insight into   how the performance varies with increasing dataset   sizes for these two approaches . Additionally , there   are trade - offs related to the data acquisition costs   as well . The cost per training instance is expected   to be much smaller for an MT - based approach than   manual translation or labeling of examples . How-   ever , depending on the nature of task , language ,   and quality of the MT output , the amount of data   required to achieve the same performance through   these two approaches can be drastically different .   More importantly , fine - tuning the MMLMs with   a combination of the data from the two strategies   could be the cheapest alternative for achieving a tar-   get accuracy , which , to the best of our knowledge ,   has not been explored yet .   Inspired by the above observations and gaps , in   this paper , we ask the following question : Given a   pre - determined budget to fine - tune a multilingual   model on a task for which some data is available in   a pivot language , what is the best achievable accu-   racy on a target language by ( a ) training the model   on the pivot - language data , ( b ) different amounts   of machine - translated and ( c ) manually - collected   data in the target language . Solving this requires   an understanding of the exact nature of the perfor-   mance and cost trade - offs between the two kinds of   target language datasets and their relative costs of   acquisition , apart from factors such as the amount   of pivot language data , the task , the MMLM , and1369the languages concerned .   This problem of modeling and measuring the   trade - offs between different input factors and   their costs is well - studied in the field of micro-   economics . A sophisticated machinery has been   developed in the form of Production Functions and   allied analytical methods ( Miller and Blair , 2009 ;   Cobb and Douglas , 1928 ) , in order to solve the   following generic problem : with the best available   technology , how are the inputs to a production pro-   cess ( eg . Labor andCapital ) related to its output ,   that is the quantity of goods produced . In this paper ,   we adapt this framework to address the aforemen-   tioned question of MMLM fine - tuning trade - offs .   The key contributions of our work are threefold .   1 . We extend the idea of production functions to   performance functions that model the relationship   between input data sizes and performance of a sys-   tem ; we propose a possible analytical form for this   function and derive the performance trends and op-   timal data collection strategies under fixed costs .   2 . We illustrate the usefulness of this framework   through a case study on a Q&A task – TyDIQA –   GoldP ( Clark et al . , 2020 ) and systematically study   the various trade - offs for 8 languages . 3 . Our study   provides several important insights such as ( a ) if   the cost of MT data creation is non - zero , then the   optimal performance under a fixed budget is al-   ways achieved with either only manually - created   data or a combination of the two ; ( b ) the ratio of the   two datasets for the least cost combination usually   remains constant at different levels of performance .   To the best of our knowledge , this is the first   work that applies the idea of production func-   tions to analyze the cost - performance trade - offs of   MMLM fine - tuning . The proposed framework can   be extended to a multitude of NLP problems where   the trade - offs similar to the ones discussed above ,   are common ( e.g. , pre - training vs. fine - tuning data ) .   To encourage reproducible research , we have made   our code , the performance data , and a detailed list   of the results publicly available .   2 Theoretical Foundations   One of the foundational pillars of neoclassical eco-   nomics is the idea of Production Functions . Simply   put , a production function is a mathematical for-   malization of the relationship between the output   of a firm ( industry , economy ) and the inputs that   have been used in obtaining it ( Khatskevich andPranevich , 2018 ; Miller and Blair , 2009 ) . A multi-   factor production function is defined as a map   Q : x→f(x),∀x∈R(1 )   where Q∈Ris the quantity of output , nis   the number of the inputs , the non - negative func-   tionfis continuously differentiable for all x=   ( x , . . . , x)when x∈R. A sophisticated and   extensive set of analytical machinery has been de-   veloped over the years in microeconomics theory   that allows one to closely model and analyze not   only the relationship between the inputs and out-   putsof a firm , but also the interdependence be-   tween the inputs ( i.e. , xs ) . Thus , one can effi-   ciently compute and clearly visualize the various   trade - offs and optimal configurations of the produc-   tion system .   Production functions have been extensively used   to model and study systems as diverse as educa-   tion ( Bettinger et al . , 2020 ; Bowles , 1970 ) , envi-   ronment ( Lu et al . , 2019 ; Halicioglu and Ketenci ,   2018 ) , sustainability ( Yankovyi et al . , 2021 ) , cogni-   tion ( Todd and Wolpin , 2003 ) and of course , differ-   ent types of industries ( Husain et al . , 2016 ; Batiese ,   1992 ) . Along similar lines , in this work we de-   velop the concept of Performance Function that   models the performance of an MMLM given the   amount of translated and manually labeled data .   In this section , we begin by formalizing the nota-   tions and defining some key concepts from micro-   economics , appropriately adapted to our context .   Then we present the functional form of the per-   formance function , and discuss certain practical   constraints and assumptions that we will make in   our formulation .   2.1 Notation and Definitions   Consider a multilingual model Mpre - trained on a   set of languages L , which is to be fine - tuned for a   taskT , for which Plabelled examples are available   in a pivot language p∈ L. Some or all of the   Ppivot language examples can be automatically   translated to a target language l∈ L through an   MT system to obtain T(≤P)examples . Further ,   letMbe the amount of examples for lthat have   been labelled or translated manually .   Definition 1 Performance Function , Π =   π(T , M|l , p , P , M , T ) , denotes the best possible   performance ( as per the current state - of - the - art ) of1370a system in language lfor a task T , that has been   built on top of a pre - trained MMLM M , Plabelled   examples in language p̸=l , Ttranslated exam-   ples by an MT system , and Mmanually created   examples .   Here , Π∈[0,1]is any appropriate and accepted   measure of performance , such as accuracy or F1-   score . To simplify the notation we will often drop   the given conditions from the equation and denote   Π = π(T , M ) . The conditional factors , whenever   not obvious from the context , will be explicitly   stated . Note that TandMare respective equiva-   lents of KandLof the neoclassical Capital - Labor   production functions . Capital investment in tech-   nology or mechanization is similar to machine -   translated data , whereas manual dataset creation   would require investment on labor .   Definition 2 Total cost of operation ( or simply   the cost ) , κ(T , M ) = κ(T ) + κ(M ) , is the total   cost of procuring translated and manually created   datasets for lfor the task T.   We further assume that the translation and man-   ual collection costs are scalar multiples of the unit   costs , i.e. κ(T ) = cTandκ(M ) = cM ,   where c>0is the cost of translating a single ex-   ample from Pinto language lautomatically , while   c>0is the cost of collecting one training exam-   ple in lmanually . Therefore ,   C = κ(T , M ) = cT+cM ( 2 )   Usually , c > c. Also , note that we are ignor-   ing the costs of pivot data collection and computa-   tional costs of pre - training and fine - tuning , partly   because we are interested in studying the trade-   off between TandM. Also , Pis useful for any   target language , and therefore , the amortized cost   of creating Ptends to zero as the number of tar-   get languages increases . Similarly the amortized   cost of pre - training tends to zero as the number   of tasks grows . The task - specific training cost is   proportional to training data - size , P+T+M , and   therefore , can be partially consumed in candc .   Definition 3 Isoperf curves are the contours of   the performance function that represent the rela-   tionship between TandMfor a fixed performance   value Π.   Definition 4 Isocost curves are the contours of the   cost function that represent the different possible   combinations of TandMthat result in equal over-   all costs .   Both isoperfs and isocosts are drawn on a T - M   diagram ( K - L diagram in micro - economics ) , which   is illustrated in Fig . 1 . The x and y axis represent   the input factors T and M , respectively . The or-   ange curves are the hypothetical isoperfs , known   asisoquants in economics . As the name suggests ,   each point on these curves represents T - M combi-   nations that result in the same ( iso ) performance   ( perf ) , denoted in the diagram by Π , Π , etc . Intu-   itively , it can be seen that two isoperfs never inter-   sect ; as we move towards right and up , Πincreases   because either TorMor both increase . Thus ,   Π < Π < Π < Π. The origin , T= 0 , M= 0 ,   represents an isoperf corresponding to the zero - shot   performance on lwhenMis fine - tuned only on P.   The blue lines represent the isocosts . Consid-   ering the nature of the cost function defined , the   isocost curves will be straight lines parallel to each   other with slope −. Like isoperfs , the cost of   operation increases for the isocosts as we move   towards right and top in the T - M diagram .   Definition 5 Least Cost Operating Point on an   isoperf refers to the ( possibly multiple ) point where   the total cost of operation is lowest for a given   performance .   Under the assumption of smooth and convex isop-   erfs , the isocost corresponding to the least cost   of operation will be a tangent to the isoperf , and   the optimal allocation of the TandMis given by   thepoint of tangency . The isocosts shown in Fig . 11371correspond to the least cost curves for respective   isoperfs , and the points of tangency are represented   by the points E1 , E2 , etc .   Definition 6 Expansion path is a path connect-   ing the point of tangency of different isoperf and   isocost curves , tracing out the cost minimizing com-   bination of the data resources with increasing per-   formance and costs .   Expansion paths are important in determining re-   source allocations strategies . For instance , when   a higher budget is available for dataset expansion   in a particular language , should one invest more in   translation or in manually collected data ? And how   does this equation change in the long run , as the   system moves towards higher performances ?   Thus , isoperfs and isocosts when studied col-   lectively can help determine the allocation of the   amount of translation and manual data for a desir-   able performance value that minimizes the cost of   operation .   2.2 Selecting a Functional Form for π   In production analysis , one of the difficult prob-   lems is to decide on the functional form of the pro-   duction function that can on one hand accurately   represent the input - output relationship , and on the   other , is amenable to close - formed analysis ( Griffin   et al . , 1987 ) . Clearly , a linear production function   would be an inappropriate choice for π(T , M ) , as   TandMare not perfect substitutes of each other .   A popular choice in such case is the Cobb - Douglas   performance function ( Cobb and Douglas , 1928 ) ,   which is of the form TM . However , the two   datasets do not have multiplicative , but rather an   additive effect . Therefore , we propose the follow-   ing performance function :   Π = π(T , M ) = a+aT+aM(3 )   where a , a , a≥0and0≤α , α≤1 . The   positive coefficients of the input factors are moti-   vated by assuming that under a reasonable transla-   tion and manual annotation quality , the addition of   data from these sources should not hurt the zero-   shot performance which is given by a(when   T = M= 0 ) . Bounding the exponents below   1 ensures that the performance is not allowed to in-   crease linearly with increasing data in one of these   sources , as we always see diminishing returns with   respect to data for any machine learning model . The commonly used training setups can be ob-   tained as special cases of the above equation . The   translate - train setup , can be obtained by setting   T = PandM= 0 in the equation , giving Π=   a+aP. Similarly , Π = a+akgives   the few - shot setup with kexamples . We denote this   functional form as AMUE ( Additive Model with   Unequal Elasticities ) .   The expression for tangency point can be derived   by setting dM / dT |to the slope of the isocost ,   −c / c , which gives the following equation for   theexpansion path .   M=/parenleftbiggcaα   caα / parenrightbiggT ( 4 )   Thus , M / T ( also called the labor - to - capital ratio )   increases with performance if α > α , remains   fixed when α = α , and decreases with perfor-   mance when α < α . Similarly , the ratio of costs   of acquiring manually created data to translated   data , Mc / Tcis proportional to aM / aT ,   which is the ratio of the contributions of the two   datasets to the performance Π.   More often than not , actual production systems   are too complex to be modeled accurately with sim-   ple functional forms . We expect a similar situation ,   where AMUE might be well suited for modeling   and visualizing the trends . However , to obtain the   actual operating cost and expansion path that are   practically useful , one would need to model the   behavior of the performance function more accu-   rately . To this end , we also experiment with Gaus-   sian Process Regression ( GPR ) for defining the   performance function . As we shall see in the next   section , GPR is able to fit the data more effectively ,   though we shall stick to AMUE as the two show   identical trends and the latter also allows us to gain   deeper insights and richer visualizations .   2.3 Some Practical Considerations   Definition 7 Cost Ratio , defined as c= ,   is the relative cheapness of the translation data ,   when compared to the cost of obtaining a manually   created data point .   We expect the cost ratio to be much smaller than 1 .   However , both translation and manual annotation   costs vary according to the complexity ( in case of   translation , just the lengths of sentences ) of the   task at hand . cmight also vary with the choice   of the target language l , while ccan be assumed   to be uniform across the languages supported by1372the commercial MT systems like Google or Bing .   In the experiments for our case study , we calculate   the expansion paths for different values of cto   systematically study the nature of the trade - offs   between the two sources of data .   Realizable region : The forms of the performance   function as well as cost function defined above do   not place any constraint on the values that the input   factors , i.e. TandM , can take , which means that   the amount of data can be increased indefinitely in   order to improve the performance . However , we   are aware that the amount of translated data is up-   per bounded by the amount of pivot data available ,   i.e. T≤P. While this constraint can be explic-   itly worked out into the equations ( by replacing T   withmin(T , P ) ) , we stick to the original forms to   preserve the smoothness of AMUE . Instead , we   define a realizable region R : T≤P , and if a   tangency point lies outside Rwe explicitly search   for the minimum cost point on the part of the isop-   erf curve that lies in the realizable region . Note   that , in such cases the isocost curves correspond-   ing to the minimum cost point will no longer be   tangents to the corresponding isoperfs , and will   usually lie at the boundary between the realizable   and non - realizable regions .   3 Case - Study on TyDiQA - GoldP   In order to understand the efficacy of the proposed   framework , we conduct a case - study on a popular   multilingual Question Answering task ( cf . T ) us-   ing TyDiQA - GoldP ( Clark et al . , 2020 ) dataset and   consider mBERT as the MMLM M. In the follow-   ing subsections , we provide the details of the task   and training setup for generating the performance   Πfor different combination of the input factors ,   the procedure for estimating the parameters of the   performance functions , and the findings .   3.1 Task and Dataset   We consider the Minimum Answer Span Task from   the Typologically Diverse Question Answering   benchmark or TyDiQA - GoldP for conducting the   experiments . The choice of this particular dataset   stems from two main properties of the benchmark .   First , question - answering tasks are amenable to   translation . Secondly , TyDiQA - GoldP is com-   prised of manually labelled datasets for nine ty-   pologically diverse languages . This enables us to   study the effect of different amounts of manually-   created data Mon the performance of the MMLM.The amount of Mvaries significantly from lan-   guage to language with 1.6k examples for Korean   to 15k examples in Arabic . 3.7k examples are   available for English which we shall consider as   the pivot language pin all the experiments . We use   Azure Translatorto obtain the translated data Tin   eight target languages . The answer span alignment   between English and the translated languages are   obtained based on the technique described in Hu   et al . ( 2020 ) . We measure the performance Πas the   average F1 - score between the predicted and actual   answer - spans for the test examples .   3.2 Fine - tuning Setup   We fine - tune mBERT on the TyDiQA - GoldP   dataset with different values of the input factors ,   TandM , for each target language , along with the   amount of English pivot data , P. Different values   ofTare chosen by translating 0 % , 10 % , 40 % ,   70 % or 100 % of the English pivot data . Eleven   different values in the range [ 0,|D|](Dis the size   of the available training data in l ) and seven val-   ues between 0 and 3.7k are selected for Mand   P , respectively . Considering eight different target   languages , this results in 3080 different fine - tuning   configurations . In each configuration , we use 3   different random seeds and train for 5 epochs with   a learning rate of 2e-5 and a batch size of 32 . The   models are also jointly trained . We use XTREME   repository ( Hu et al . , 2020 ) and the Hugging Face   Transformer Library ( Wolf et al . , 2020 ) to conduct   all our experiments .   3.3 Parameter Estimation of the Performance   Function   Upon estimating the performance values for the   various fine - tuning configurations , we formulate   the parameter estimation for the performance func-   tions πas a regression task , with TandMas   inputs and Πas the output . we use a Non Lin-   ear Least Squares algorithm ( Levenberg , 1944 ) to   fit the AMUE functional form ( cf . Equation ( 5 ) ) ,   while specifying the bounds on the function param-   eters . For GPR , we use an RBF Kernel added with   a White Kernel to model the noise in the observa-   tions , and the kernel parameters are optimized us-   ing L - BFGS - B optimization algorithm ( Byrd et al . ,   1995 ) with 10 restarts . Note that , we fit different1373l a α a α   P= 3696   ar 3.7e-01 1.9e-07 2.0e+00 2.2e-01   bn 5.8e-04 6.9e-01 2.3e+00 3.0e-01   fi 7.4e-02 3.9e-01 1.2e+00 3.0e-01   i d 2.5e-13 2.5e-01 1.2e+00 2.9e-01   ko 2.6e-15 2.1e-03 1.5e+00 2.6e-01   ru 7.8e-13 5.6e-01 7.1e-01 3.5e-01   sw 5.2e-02 4.2e-01 1.1e+00 3.7e-01   te 5.1e-19 2.5e-01 1.2e+01 1.5e-01   P= 2000   ar 1.7e-01 2.9e-01 2.9e+00 2.1e-01   bn 9.9e-01 1.2e-01 1.9e+00 3.4e-01   fi 9.4e-02 4.6e-01 1.6e+00 3.0e-01   i d 4.0e-01 1.2e-01 1.5e+00 3.0e-01   ko 3.0e-13 4.1e-01 1.6e+00 2.8e-01   ru 5.8e-03 6.5e-01 1.1e+00 3.4e-01   sw 9.2e-02 4.3e-01 1.2e+00 3.7e-01   te 1.6e-01 3.0e-01 1.2e+01 1.5e-01   performance functions for each combination of l   andP. Additionally , we also conducted several   experiments with other functional forms including   Cobb - Douglas , linear , log - linear and polynomial   functions ( > 1 degree ) which either showed higher   margins of error or over - fitting .   3.4 Results   First , we evaluate how well the two proposed per-   formance functions are able to predict the perfor-   mance for different fine - tuning configurations . For   this , we split the 3080 different training configura-   tions into training ( 80 % ) and test ( 20 % ) sets . The   test root mean squared error ( RMSE ) and coeffi-   cient of determination ( r ) values for AMUE and   GPR were found to be 5.84 , 0.90 and 2.43 , 0.98   respectively . Thus , both the models can fit the data   reasonably well , though as expected , GPR provides   a better fit . Check Appendix for more details .   Expansion Paths : Table 1 shows the estimated   values of the AMUE parameters for different lan-   guages and pivot sizes . For all the languages , a   is greater than aby at least an order of magnitude ,   meaning that the manually collected data ends up   having a significantly higher contribution towards   the model ’s performance . For P= 2000 , wesee comparatively higher values of a(though still   < a ) . This indicates that the machine - translated   data might be more beneficial when there is a   paucity of training data available in the pivot lan-   guage , and thus a lower zero - shot performance to   begin with .   ForP= 3696 , Arabic , Indonesian and Korean   hasα > αand therefore , the corresponding   expansion curves ( Eqn 4 ) will have an increasing   M / T ratio with increasing Π. On the other hand   for Swahili , Telugu and Finnish , α < α , and   hence the expansion curves will bend towards the   x - axis in the T - M diagram , indicating a declining   M / T ratio . In such cases , as we continue to in-   crease the performance at the minimum cost , the   optimum strategy would be to collect higher and   higher amount of translation data as compared to   manually labelled data .   However , notice that the αandαare close   to each other for majority of the cases resulting in   nearly linear expansion paths , a situation that is   often encountered in economics whenever the pro-   duction function is homogenous . We did not start   with a homogenity assumption on π(M , T ) ; rather ,   the estimated parameters indicate so . This has two   interesting implications : 1 ) M / T remains nearly   uniform at the different levels of performance ; 2 )   the slope of the expansion path is approximately   ( ) ( by setting α = αin Eqn 4 ) , mean-   ing if the cost ratiois greater than , the op-   timal strategy would be to collect more manually   labelled data ( since > 1 by definition ) and   vice - versa . Thus , by just looking at the value of   these parameters we can gain key insights about   the optimal data allocation strategies .   These strategic insights can also be clearly visu-   alized through the isoperf , isocost and expansion   path curves on the T - M diagrams , as shown in Fig .   2 . Due to paucity of space , we show the diagrams   for two languages – Swahili ( sw ) and Telugu ( te ) –   with two different cost ratios for the former ( Fig . 2a   and 2b ) , and two different pivot sizes for the latter   ( Fig . 2c and 2d ) . Refer appendix ( 6,7 , 8 , 9 ) for rest .   Forc= 0.1,l = sw ( Fig . 2a ) , the expan-   sion path follows a straight line roughly with a   slope ( ) = 3.2 . This indicates that even   though Mis 10 times more expensive than T , the   optimal allocation policy is to still collect about   thrice as much amount of MasT. However , for   c= 0.01 , which is less than , the slope of the   expansion path drops to ≈0.08 , as demonstrated1374   by the theoretical expansion path on the right side   of the M = Tline in Fig . 2b . This suggests that   we can rely on collecting a higher amount of trans-   lation data to increase the performance in this case   because the manually collected data is much more   expensive . As we move to the performance values   > 70 , we reach at the boundary of the realizable   region ( marked by translucent gray rectangle ) , and   can no longer keep on collecting more translation   data to increase the performance as by definition   T≤P. Beyond this point , to increase the perfor-   mance , collecting higher amounts of manual data   becomes inevitable .   For Telugu , we study the effect of two differ-   ent values of Pand keep cfixed at 0.1 . At   P= 3696 , the isoperfs are nearly parallel to x-   axis with the expansion path lying along the line   T= 0 ( Fig . 2c ) , which is expected as≈0   in this case ( see Table 1 ) . This particular expan-   sion path indicates that data obtained by translating   English examples into Telugu does not have any no-   table performance improvement , though demands   additional cost . The optimal strategy in this case   is to only collect manually annotated data . This   is not entirely surprising ; the translate - train setupin Hu et al . ( 2020 ) also shows low F1 - scores for Tel-   ugu than the zero - shot setup . Interestingly , when   P= 2000 ( Fig . 2d ) , Tprovides non - trivial perfor-   mance gains . The expansion curve is bent slightly   to the left of the M = Tline , similar to Fig . 2a .   This trend of higher a / afor lower Pis observ-   able for all languages ( Table 1 ) .   Performance and Cost Trade - off : Fig . 3 plots   the cost vs the performance value traced out by   the expansion paths for the 8 target languages . To   calculate the total cost , we assume c= 0.007 ,   which was estimated according to the standard   translator Pricing offered by Azure , and consider   c= 0.01 . For all the languages , we observe a   declining slope as we increase the value of C. Thus ,   it becomes increasingly more expensive to improve   the performance of the models as we move to the   higher values of Π(law of diminishing returns ) .   Comparing AMUE isoperfs with GPR isoperfs : 1375   Figure 4 displays the isoperfs and the correspond-   ing optimum isocosts obtained using AMUE and   GPR based performance functions . As can be ob-   served , both functions predict similar trends across   their isoperfs ; however , as expected , the curves are   shifted due to different margin of errors for the two   models .   4 Discussion and Conclusion   In this work , we have proposed a micro - economics   inspired framework to study the performance and   cost trade - offs between manually annotated and   machine - translated data for training multilingual   models , and demonstrated its efficacy through a   case - study on the TyDiQA - GoldP dataset . The key   findings from this case - study are : 1 . Some amount   of manually collected data in a target language is   crucial to attain optimal performance at minimum   cost irrespective of how much cheaply MT data canbe procured , as long as the cost is non - zero . 2 . The   ratio of manually collected and machine - translated   data at least cost operating point remains nearly uni-   form at the different levels of performance 3 . The   usefulness of translated data is higher when the   amount of pivot language data is less . There are   several other insights that can be drawn from the   T - M diagrams and other plots , which could not be   presented here due to the paucity of space .   This work can be expanded in several ways . In   the current work we considered a single - pivot and   single - target case . Generalizing this to the case   where the model is allowed to be trained on multi-   ple pivot languages and then be evaluated on multi-   ple targets is of considerable interest . This implies   extension to multiple - output production functions   with multiple ( > 2 ) input factors .   Here , we have not considered the effect of mul-   tiple technology on the isoperfs . For our problem ,   multiple technologies may correspond to the dif-   ferent MMLMs such as mBERT , XLMR and mT5 ,   different MT systems , and even different training   curricula . Identifying the optimal allocation policy   considering the presence of such multiple techno-   logical alternatives would be an interesting exercise .   In particular , it will be interesting to explore the   impact of translation quality on the trade - offs . An   important limitation of the current framework is   that it presumes availability of certain amounts of   M and T datasets such that the performance func-   tion can be estimated . However , in practice , one   would like to understand the trade - offs before col-   lecting the data . Recently , Srinivasan et al . ( 2021 )   showed that it is possible to predict the zero - shot   and few - shot performance of MMLMs for differ-   ent languages using linguistic properties and their   representation in the pre - training corpus . Under-   standing if there exists a similar dependence of the   performance trade - offs with the linguistic proper-   ties of different languages can help us generalize   our framework to the new languages without the   need for explicit data collection .   Finally , we believe that performance function-   based analysis can be applied to a multitude of   three - way trade - offs among technology , cost and   data that are commonly encountered in the NLP   world . The economics of language data can be a   new direction of study with important practical and   theoretical applications.1376Acknowledgements   We would like to thank Shanu Kumar and the LIT-   MUS team at Microsoft for their valuable inputs   and feedback over the course of this work . We are   also grateful to the anonymous reviewers for their   constructive comments .   References13771378A Appendix   A.1 Derivations   Here we derive the expression for the curve traced   by the expansion path as given in equation 4 . As de-   scribed in section 2.2 AMUE performance function   is given by :   π(T , M ) = a+aT+aM   Setting π(T , M ) = Πi.e . a constant value , we   can obtain an analytic expression for the isoperf   curves from this functional form , which is given   by :   M=/parenleftbiggΠ−a−aT   a / parenrightbigg(5 )   Since the expansion path is the locus of the   points of tangency between isoperf and isocost   curves , we can compute the slope of the isoperf   curve and set them equal to each other . The slope   for isoperf curve can be computed as :   M=/parenleftbiggΠ−a−aT   a / parenrightbigg   αMdM   dT=−αa   aT   dM   dT=−αa   αaT   M   The slope of the isocost curve is simply − ,   equating them we get :   c   c = αa   αaT   M   M = αac   αacT   M=/parenleftbiggcaα   caα / parenrightbiggT   A.2 Training Setup   We typically run the fine - tuning experiments on   NVIDIA - P100 GPUs with 16 GB of memory . A   fine - tuning job with 3 random seeds typically takes   2 hours to run on the specified compute . Having   access to 64 of such GPUs we ran multiple jobs in   parallel . For fitting performance functions and do-   ing analysis on expansion paths CPU only compute   of Intel(R ) Xeon(R ) CPU E5 - 2690 was utilized .   We use mBERT configuration bert - base-   multilingual - cased for fine - tuning , which supports   104 languages and has around 178 million   parameters . A.3 Goodness of Fit   Table 2 shows the train and test RMSE and rfor   GPR and AMUE . For training set we also compute   the errors corresponding to different fine - tuning   setups like translate - train , few - shot etc , which in-   dicates that our models can accurately fit different   regions of the performance landscape . The point   is again illustrated in Figure5 which compares the   predictions of AMUE and GPR with the actual F1-   scores for different values of the amount of manual   data ( i.e. M ) , keeping T , P , and pas fixed.137913801381138213831384