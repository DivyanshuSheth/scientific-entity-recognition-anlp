  Prathyusha Jwalapuramand Shafiq Jotyand Xiang LinNanyang Technological University , SingaporeSalesforce Research Asia , Singapore   { jwal0001,srjoty,linx0057}@ntu.edu.sg   Abstract   Given the claims of improved text generation   quality across various pre - trained neural mod-   els , we consider the coherence evaluation of   machine generated text to be one of the prin-   cipal applications of coherence models that   needs to be investigated . Prior work in neu-   ral coherence modeling has primarily focused   on devising new architectures for solving the   permuted document task . We instead use a   basic model architecture and show significant   improvements over state of the art within the   same training regime . We then design a harder   self - supervision objective by increasing the ra-   tio of negative samples within a contrastive   learning setup , and enhance the model further   through automatic hard negative mining cou-   pled with a large global negative queue encoded   by a momentum encoder . We show empiri-   cally that increasing the density of negative   samples improves the basic model , and using   a global negative queue further improves and   stabilizes the model while training with hard   negative samples . We evaluate the coherence   model on task - independent test sets that resem-   ble real - world applications and show signifi-   ca nt improvements in coherence evaluations of   downstream tasks .   1 Introduction   Coherence is a property of a well - written text that   makes it different from a random set of sentences :   sentences in a coherent text are connected in sys-   tematic ways such that each sentence follows nat-   urally from previous ones and leads into the fol-   lowing ones ( Halliday and Hasan , 1976 ; Grosz and   Sidner , 1986 ) . Coherence models ( Barzilay and   Lapata , 2005 ) that can distinguish a coherent text   from incoherent ones have a wide range of applica-   tions in language generation , summarization , and   coherence assessment tasks such as essay scoring   and sentence ordering . With recent advancements in neural methods ,   claims of fluency in summarization ( Liu et al . ,   2017 ; Celikyilmaz et al . , 2018 ) , language modeling   ( Radford et al . , 2019 ; Brown et al . , 2020 ) , response   generation ( Zhang et al . , 2020 ; Hosseini - Asl et al . ,   2020 ) and human parity in machine translation   ( Hassan et al . , 2018 ) have led to calls for finer-   grained discourse - level evaluations ( Läubli et al . ,   2018 ; Sharma et al . , 2019 ; Popel et al . , 2020 ) , since   traditional metrics such as BLEU and ROUGE   are unable to measure text quality and readabil-   ity ( Paulus et al . , 2018 ; Reiter , 2018 ) . Coherence   models that can evaluate machine - generated text   have become the need of the hour .   A majority of coherence models proposed op-   timize their learning objectives on the permuted   document task using the Penn Treebank ( WSJ ) cor-   pus . An original article is considered a ‘ positive ’   sample of a coherent document , while a permuta-   tion of its sentences is considered a ‘ negative ’ or   incoherent sample ( see Appendix A.1 for an ex-   ample ) . Models are usually trained in a pairwise   ranking fashion to distinguish the two .   The basic entity - grid model proposed by Barzi-   lay and Lapata ( 2005 , 2008 ) was extended to in-   corporate entity - specific features ( Elsner and Char-   niak , 2011 ) , multiple ranks ( Feng and Hirst , 2012 ) ,   and coherence relations ( Lin et al . , 2011 ; Feng   et al . , 2014 ) . Their neural extensions have also   been proposed ( Nguyen and Joty , 2017 ; Mohi-   uddin et al . , 2018 ) . More recent state - of - the - art   models like the Transferable Neural model ( Xu   et al . , 2019 ) consider coherence at a local level   by training a forward and backward model only   on adjacent sentences , in addition to generative   pre - training of the sentence encoders . The Uni-   fied Coherence model ( Moon et al . , 2019 ) uses   bi - linear layer and lightweight convolution - pooling   in a Siamese framework to capture discourse rela-   tions and topic structures , along with an explicit   language model loss to capture syntactic patterns.6044Mohiuddin et al . ( 2021 ) recently tested these   state - of - the - art models by conducting coherence   evaluations on the WSJ permuted document task ,   machine translation , summarization and next utter-   ance ranking tasks . They found that while mod-   els performed well on the permuted document   task , when tested off - the - shelf , models general-   ized poorly to downstream evaluation tasks . They   call for more comprehensive evaluations of coher-   ence models . Pishdad et al . ( 2020 ) also reached a   similar conclusion . They retrained several neural   coherence models for tasks analogous to coherence   modeling such as detecting connective substitution   and topic switching . They found that performance   on the permuted document task is only partially   indicative of coherence modeling capabilities .   In light of these recent findings , our aim is   to propose a coherence model that generalizes   well to downstream tasks . We train our model   purely through self - supervision , without tailoring   the model architecture specifically to the permuted   document task or any other form of supervision .   Li and Jurafsky ( 2017 ) point out that coherence   models are exposed to a limited number of inco-   herent samples in the pairwise setup , since only   a small sample of all possible incoherent permu-   tations of a document are used to train models .   Learning with more negatives can better maximize   the mutual information between their representa-   tions ( van den Oord et al . , 2018 ) . By using a con-   trastive learning ( Gutmann and Hyvärinen , 2010 )   setup , where each ‘ positive ’ document is compared   with multiple ‘ negative ’ documents , we increase   the proportion of negative samples that the model   is exposed to , and show that the coherence model   shows significant improvements in performance .   Wu et al . ( 2020 ) show that the difficulty of the   negative samples used for contrastive training can   strongly influence model success for visual repre-   sentation learning . Guided by this principle , we   train the model with automatically mined hard neg-   atives , coupled with a large global negative queue   encoded by a momentum encoder ( He et al . , 2019 ) .   In summary , our contributions are :   •A neural coherence model trained purely through   well - designed self - supervision tasks that general-   izes well to downstream applications .   •Evaluation on multiple independent test sets that   are more indicative of real - world performance of   the coherence model .   •Empirical results demonstrating that increase inthe density and quality of negative samples leads   to better generalization for coherence models .   2 Datasets   To ensure that our coherence model is useful for   evaluation in downstream applications , we use a   selection of task - independent test sets that cover a   variety of domains and genres , including machine   generated text from summarization systems and   language models . Following Pishdad et al . ( 2020 ) ,   we also evaluate the models on a commonsense   reasoning narrative dataset . We train ( and validate )   the coherence models on standard WSJ data , while   using the rest as “ independent ” test sets to indicate   the generalizability of the trained models . All eval-   uations on downstream tasks are conducted in a   pairwise setting to enable a fair comparison .   2.1 Training Data   •WSJ The Wall Street Journal ( WSJ )   corpus consists of news articles divided   into 1240/138/1053 documents for train-   ing / development / testing in the standard setup . We   exclude documents with < 4 sentences and truncate   them to a maximum length of 600 tokens . To   maximally utilize documents which are otherwise   truncated due to GPU memory constraints , we   partition documents with 20 + sentences into   blocks of 10 sentences and consider each block as   a separate positive document . This increases the   number of coherent ‘ documents ’ that we can use   to generate a larger training set . Moon et al . ( 2019 )   use 20 permutations of a document for training ;   since their setup is pairwise , it means the original   positive document is repeated 20x . We regenerate   the permuted documents similarly , sampling a   larger set of permutations for our contrastive   learning setup . This gives us 46,522instances of   positive and corresponding negative documents for   training and 4,522instances for development . We   use the original pairwise test set used by Moon   et al . ( 2019 ) with 20,411pairs for testing .   2.2 Machine Generated Texts   •S E Fabbri et al . ( 2020 ) conduct a   manual coherence evaluation of the summaries gen-   erated by 16different summarization systems for6045100 source articles based on the CNN / DailyMail   ( Hermann et al . , 2015 ) dataset . Likert - style coher-   ence ratings from 3expert annotators are available   for each summarized text . We adapt this to the pair-   wise setting by creating pairs of summaries from ev-   ery system for each unique source article . The sum-   mary with the higher average coherence rating is   designated as the positive document , while the sum-   mary with the lower rating is the negative document   for that pair . This results in    ×100 = 12 , 000   pairs for evaluation .   •LMLM To cover a wider variety of machine   generated text , we generated texts from various   language models using prompts taken from the val-   idation and test sets of the WritingPrompts dataset   ( Fan et al . , 2018 ) . Four language models were   chosen for this purpose : GPT2 - Small , GPT2 - XL ,   CTRL and GPT3 . The continuations produced by   these models for each prompt were truncated at ap-   proximately 150tokens and paired together . Using   these texts , we conducted a user study on Ama-   zon Mechanical Turk . Workers were instructed   about the concept of coherence and shown exam-   ples of coherent and incoherent texts . Given the   prompt , they were asked to choose the more coher-   ent text out of two given language model outputs ;   they were also given an option to choose neither   in case the texts were equally coherent / incoherent   ( see Appendix A.3 for more details such as the   study interface ) . After removing the samples with   low agreements and ties , a total of 1,046pairs with   judgments from 3annotators each were collected .   The Krippendorff ’s alpha coefficient ( Krippendorff ,   2011 ) between the annotators was 0.84 . We calcu-   late the agreements of the coherence model ranking   with these judgments , designated LMLM .   2.3 Curated Test Sets   •INSDShen et al . ( 2021 ) propose a sentence   intrusion detection task in order to test the coher-   ence modeling capabilities of pre - trained language   models . Incoherent documents are created by sub-   stituting a sentence from a document with another   sentence from a different document , ensuring that   the replacement sentence is similar to the original   document to make the task sufficiently hard . We   adapt their task to the pairwise setting by pairing   the original coherent and the corrupted incoher-   ent document , giving us 7,168instances from their   CNN test set ( INSD - CNN ) and 3,666instances   from their Wikipedia test set ( INSD - W ) forevaluation . Shen et al . ( 2021 ) also create a hand-   crafted linguistic probe test set , where incoherence   is manually inserted based on a range of linguistic   phenomena ; we use this test set for analysis ( § 4 ) .   •S C The S C dataset   ( created from RS ( Sharma et al . , 2018 ) )   consists of a short narrative - style text with two pos-   sible endings , one of which is implausible . The test   set labels are not public so we use the validation   set . We designate the text with the correct ending   as the positive document and the text with the in-   correct ending as the negative document , resulting   in a total of 1,571pairs for evaluation .   3 Methodology   3.1 Model Architecture   Previous work on coherence modeling proposed   elaborate architectures to capture various aspects   of coherence ( see § 1 ) . However , our key hypothesis   is that large - scale pre - trained models are expres-   sive enough to model coherence given the right   self - supervision . Effective bi - directional encoding   through large Transformer networks ( Vaswani et al . ,   2017 ) can consider longer language context , while   language modeling objectives enforce syntactic and   local coherence patterns in the model .   In our work , we adopt XLNet ( Yang et al . ,   2019 ) as the backbone model . It is trained us-   ing a permuted language modeling objective , in   which the expected log - likelihood of a sequence   with respect to all permutations of the factoriza-   tion order is maximized . This allows the modeling   of bi - directional context , while maintaining the   auto - regressive property and avoiding the pretrain-   finetune discrepancy . In addition , XLNet also in-   corporates segment recurrence ( or memory ) and   the relative encoding scheme of Transformer - XL   ( Dai et al . , 2019 ) , which makes it effective in mod-   eling longer text sequences . This makes it suitable   for our purpose of coherence modeling .   Given a document Dwith nsentences   ( s , s , . . . , s)as input , our model uses the rep-   resentations obtained through XLNet ( parameter-   ized by ϕ ) to assign a coherence score to the model .   Specifically , for each sentence swithktokens   ( w , w. . . w ) , XLNet maps each token wto its   vector representation v∈Rwhere dis the dimen-   sion of the embedding . In addition , the complete   inputDis also mapped to a document representa-   tionz∈R(i.e . ,the representation of the [ ] 6046token ) . We simply add a linear layer to convert   document representation zto obtain the final co-   herence score : f(D ) = wz+b , where wand   bare the weight and bias of the linear layer with   θ={ϕ,w , b}being the entire parameter set of the   model ( see the upper part of Figure 1 ) .   3.2 Margin - based Pairwise Ranking   Setup . Traditionally , coherence model training   has been done in a pairwise ranking setup . In this   setup , the model is trained to score the coherent   or positive document higher than the incoherent or   negative document , using a pairwise ranking loss   ( Collobert et al . , 2011 ) defined as follows :   L= max0 , τ−f(D ) + f(D)(1 )   where f(D)is the coherence score of the positive   document , f(D)is the coherence score of the   negative document and τis the margin .   Baselines . We compare our models against   all three versions of the Local Coherence   Discriminator or LCD model ( Xu et al . , 2019 ):   ( i)LCD - G , that uses GloVe ( Pennington et al . ,   2014 ) representations , ( ii)LCD - I , that uses In-   ferSent ( Conneau et al . , 2017 ) representations , and   ( iii)LCD - L , that uses representations from an   RNN - based language model trained on the train-   ing data . We also compare against the Unified   Coherence model or UNC ( Moon et al . , 2019 ) ,   which is the previous SOTA on the WSJ permuted   document task . Results from evaluation of existing   coherence models by Pishdad et al . ( 2020 ) and Mo-   hiuddin et al . ( 2021 ) indicate that UNC and LCD   are the best - performing models ( see Appendix A.4   for a full comparison ) . We retrain their models with   our training data for comparison . In addition , to   ascertain the contribution of the pre - trained XLNet   embeddings , we train our pairwise model without   fine - tuning the representations , i.e. ,only the score-   producing linear layer weights wandbare trained   on the pairwise ranking task .   Results . The results for the baseline models are   given in Table 1 ( see top five rows ) . We see that   despite accuracies of more than 90 % on the WSJ   permuted document task , the LCD models perform   only a little above a random baseline of 50 % on   most of the independent test sets , with LCD - G   being the best generalizing model out of the three . Similarly , despite a relatively high performance on   theWSJ test set ( 94.11 % ) , UNC ’s performance   on the independent test sets is quite poor , even   failing to do better than the random baseline of   50 % in two out of five cases . Both the LCD and   UNC models have slightly better success on the   INSD - CNN dataset , which is the same domain   ( news ) as the training data , with the UNC model   reaching 67.21 % accuracy . Our XLNet - Pairwise   model trained without fine - tuning the representa-   tions ( No FT ) performs no better than the baseline   models . This shows that both the LCD - G and the   UNC models are in fact strong baselines despite   using GloVe and ELMo ( Peters et al . , 2018 ) pre-   trained representations respectively .   Our fully - trained XLNet - Pairwise model not   only outperforms the UNC model on the standard   WSJ permuted document task , but also signifi-   cantly outperforms all baseline models on the in-   dependent test sets , showing an absolute improve-   ment of 15 - 20 % on the S E , INSD-   CNN , INSD - Wand the S C   datasets . On LMLM , the UNC model has a bet-   ter performance ; we suspect that its explicit condi-   tional language modeling loss might provide an ad-   ditional advantage for this particular task . Overall ,   our results are consistent with observations from   Mohiuddin et al . ( 2021 ) that show poor generaliz-   ability in the previous SOTA model .   3.3 Contrastive Learning   Setup . In pairwise ranking , each positive sample   is only compared to one negative at a time . Con-   trastive learning ( Gutmann and Hyvärinen , 2010 )   makes it general , where a single positive sample   can be compared to multiple negatives , which can   be particularly useful in the permuted document   task where the number of possible incoherent sam-   ples per coherent document can be very large . The   number of negatives considered and their quality   can affect model performance ( Arora et al . , 2019 ) .   Wu et al . ( 2020 ) show that contrastive loss max-   imizes a lower bound on the mutual information   between representations . A larger number of neg-   atives increases the tightness of the bound ; learn-   ing with more negatives can better maximise the   mutual information . We train our model with a   margin - based contrastive loss defined as :   ( 2)6047   where f(D)is the coherence score of the positive   document , f(D ) , · · · , f(D)are the scores of   theNnegative documents , and τis the margin .   Training . We use the same training data as the   baseline models to train our contrastive model ; the   positive documents remain the same , while we use   5 negative documents per instance ( instead of only   1 in the pairwise setup ) . Effectively , the model   sees the same number of positive or coherent doc-   uments , but five times as many negative samples   during training compared to the pairwise setting .   Appendix A.5 gives the full set of hyperparameters .   Results . From the results in Table 1 , we see that   the contrastive model ( second to last row ) further   improves the results across all the independent test   sets ; the results on the LMLM dataset also im-   prove , surpassing the UNC model performance .   Although the improvement on the WSJ permuted   document task is small , the improvement in the   generalizability of the model is more significant .   3.4 Hard Negative Mining   It has been shown that the difficulty of the negative   samples used for contrastive training can strongly   influence model success ( Wu et al . , 2020 ; Huang   et al . , 2020 ) . We therefore automatically mine hard   negatives during training . For the permuted docu-   ment task , we can take advantage of the fact that the   negative sample space can be huge ; for a document   withnsentences , the candidate pool of permuta-   tions has n!−1incoherent documents from which   we can mine hard negatives . For the problem of   dense text retrieval , Xiong et al . ( 2020 ) find global   hard negatives by computing document encodings   using a recent checkpoint to build an asynchronousindex of the entire corpus , and sampling negative   documents from the index . However , the huge can-   didate pool for permuted documents also makes it   infeasible to mine global negatives in our case .   Instead , we perform local negative sample rank-   ing . For each positive instance in the training data ,   we sample a larger number of permuted documents   ( h ) per instance than we need for training ( i.e. ,   h > N ) . We score these negative documents using   the model updated thus far and use the highest rank-   ing negatives for training . Specifically , the model   is first trained with xinstances ( xis a hyperparam-   eter ) of data , by using 5 negative samples randomly   chosen out of h. The updated model is then used   to score all the hnegative samples each for another   set of xinstances from the training data . The scores   of the hnegative samples are ranked and the top   scoring 5 negative samples for each instance are   used to train the model for the next xgradient steps .   This process is repeated throughout training ; the   model therefore iteratively mines harder and harder   negative samples as it improves . See Algorithm 1   in Appendix A.2 for the pseudocode .   In practice however , we find that using hard neg-   ative samples directly leads to instability in model   training ( see § 4.1 ) . We therefore use hard negative   training in combination with a momentum encoder ,   which we describe in the next subsection .   3.5 Hard Negatives with Momentum Encoder   While increasing the number of negative samples   per instance has been shown to be effective for   constrastive learning , resource constraints can limit   the number of negatives that can be considered per   instance . One solution is to consider other posi-   tive instances in the same training batch as nega-6048   tives ( Karpukhin et al . , 2020 ; Chen et al . , 2020 ) .   However , it is not suitable for the permuted docu-   ment task since the negatives are instance - specific .   While a permuted document is still independently   incoherent , training with permutations of other doc-   uments will not provide the same cues for coher-   ence modeling as the original self - supervision .   Another solution is to maintain a large global   queue of negative samples that are independent of   the current training instance . During training , neg-   ative samples ( specifically , their representations )   from the latest batch are enqueued to build a queue   upto some size l. As training continues , the nega-   tive samples from the oldest batch are dequeued to   accommodate newer samples . However , represen-   tations of the documents will evolve through train-   ing as the model parameters get updated ; this will   make the negative samples in the queue inconsis-   tent with each other and the training instances in the   current batch . Moreover , the issue of mismatched   self - supervision with negatives that are permuted   versions of other documents still remains .   Momentum Encoder . To address these issues ,   we add an auxiliary momentum encoder ( He et al . ,   2019 ) , which is also XLNet ( Yang et al . , 2019 ) .   Figure 1 shows the overall architecture . Keeping   the base contrastive setup the same ( the upper part ) ,   we add an additional contrastive objective based   on representations from the momentum encoder .   Specifically , we re - encode the positive and nega-   tive samples through the momentum encoder ; the   negative samples thus encoded are used to build the   queue . We train the model to promote the similaritybetween the positive representations from the mo-   mentum encoder and the positive representations   from our base encoder over the similarity with the   negative samples from the queue , Q. Specifically ,   we define a momentum loss Las : ( 3 )   where zandzare the positive representations   from the base encoder ( ϕ ) and the momentum en-   coder ( ϕ ) respectively , q , . . . , qindexed by jare   the negative representations from ϕin the queue ,   andτis the margin . The momentum encoder ϕis   updated based on the base encoder ϕas :   ϕ←µ∗ϕ+ ( 1−µ)∗ϕ ( 4 )   where µ∈[0,1)is the momentum coefficient ; only   ϕis updated through backpropagation . Our full   model is trained with a combination of the original   contrastive objective ( Eq . 2 ) and the momentum   encoded contrastive similarity objective ( Eq . 3 ):   L = λL+ ( 1−λ)L ( 5 )   where λis a weighting hyperparameter . Note that   the momentum encoder can be considered as a tem-   poral ensemble model consisting of exponential-   moving - average versions of the base model . Due   to this , the gradients from the momentum loss ( Eq .   3 ) also help in stabilising the overall training ( § 4).6049Length Invariance . In the permuted document   task , both the positive and the negative samples   have the same number of sentences . This is not   necessarily the case for downstream applications .   To incorporate length invariance into our model , we   encode a random contiguous slice of the positive   document through the momentum encoder ϕ.   The global negatives queue Qis constructed   from the mined hard negative samples used for   training . Our model is therefore trained to rely   not only on comparative coherence cues from the   traditional permuted document setup , but also to   recognize more independent cues for coherence   through the global queue , which is additionally   enhanced by incorporating length invariance and   automatically mined hard negative samples .   Training . We train the model with the same train-   ing data , this time sampling h= 50 negativesper   instance for hard negative ranking , and setting the   training steps ( or instances ) x= 200 . We use a   queue size of l= 1000 and set our momentum   coefficient µ= 0.9999999 , with loss weighting   parameter λ= 0.85 . Due to GPU memory con-   straints ( 24 GB , Quadro RTX 6000 ) , we train our   model with a batch size of 1 . See Appendix A.5   for the full set of hyperparameters .   Results . The results in Table 1 ( last row ) show   that our momentum encoder model with hard nega-   tive mining outperforms all previous models across   the independent testsets . This improvement comes   despite a very similar performance on the WSJ   test set ; we believe that our model truly improves   in generalizability without overfitting to the per-   muted document task . The improvements on the   out - of - domain test sets , particularly on LMLM   and S C , support this conclusion .   4 Analysis   4.1 Hard Negative Training   We only train our complete model ( i.e. ,base con-   trastive plus momentum model ) by mining hardnegative samples ( § 3.5 ) , because we find that train-   ing the base contrastive model directly with hard   negatives leads to instability during training . Fig-   ure 2a plots development set accuracies of our base   model trained with and without hard negative min-   ing , and our complete model trained with hard   negative mining ( evaluated every 1000 steps ) . As   seen in the figure , the contrastive model displays   significant volatility when trained with hard nega-   tives only , while the complete model is quite stable .   This is inline with the finding of Xuan et al . ( 2020 )   who show that training with the hardest negative   samples leads to bad local minima . This can be   explained with the gradient analysis of such neg-   atives which have a larger gradient norm ( Xiong   et al . , 2020 ) , resulting in abrupt gradient steps . The   momentum encoder being a temporal ensemble of   the base models has a regularizing effect , address-   ing this issue and leading to stable and improved   results ( see § 3.5 ) .   4.2 Effects of Hyperparameters   Number of Ranked Negatives . Figure 2b shows   the results across the test sets for different num-   bers of negative samples considered for ranking   ( h ) during hard negative mining . We see that in-   creasing the number of negatives considered im-   proves results across the board , with results on out-   of - domain test sets LMLM andS C   showing particular improvement .   Momentum Coefficient . Figure 2c shows the   variation in the model performance across the test   sets for different values of the momentum coeffi-   cientµ. We see that apart from a slight drop on the   INSD - Wdataset at µ= 0.9999999 , overall   an increasing µvalue leads to better generalization   on the independent test sets , presumably due to a   more consistent global negative queue .   Queue Size . Figure 2d shows the variation in   model performance across different test sets for   various sizes of the global negative queue Q. We   see that while increasing the queue size generally   leads to an improvement in scores , at high queue   sizes the improvement is limited to test sets from   the same domain ( WSJ , S EandINSD-   CNN ) , and the model ’s generalizability is affected .   4.3 Effects of Varying Task & Dataset   So far , we have reported the results of training   our model on the permuted document task using   documents from the WSJ corpus as was done by6050   most prior work ( Elsner and Charniak , 2011 ; Moon   et al . , 2019 ) . We now test the effectiveness of other   datasets , by varying the task itself and by using a   different dataset for the permuted document task .   Sentence Intrusion . As described in § 2.3 , Shen   et al . ( 2021 ) propose a sentence intrusion task to   test coherence modeling capabilities of pre - trained   language models . We adapt their dataset to the   pairwise setting by pairing the original coherent   document ( positive ) with the corrupted ( negative )   document ; setting aside 10 % of the data for de-   velopment gives us 25,852 positive - negative train-   ing pairs for INSD - CNN and 41,135 pairs   forINSD - W. We train our pairwise ( § 3.2 )   model on this task . From the results in Table 2 ( first   two rows ) , we see that the performance on the same   domain / task ( as the training ) and the performance   on the LMLM dataset is high , but the models   trained on this task generalize poorly to the other   independent test sets . Permuted Document Task with INSD.We   train our model on the permuted document task us-   ing the INSDdatasets . We generate 52,607 and   66,679 positive - negative pairs for INSD - CNN   andINSD - Wrespectively by sampling per-   mutations , similar to our training data ( see § 2.1 ) ,   and train our pairwise model with this data . Specif-   ically for machine generated texts , results in Ta-   ble 2 show that the sentence intrusion task training   does better on the LMLMdataset . On the other   hand , the permuted document task training does   better on S E. This could be because the   documents in S E are summaries of the   same source article and therefore similar in content   ( detecting incoherence through permutations might   help here ) , while the text generated by language   models even for the same prompt tends to differ   in content more significantly ( detecting intruder   sentences might help here ) . Additionally , the per-   formance of our WSJ model on the INSD - CNN6051   andINSD - Wdatasets is comparable to the   performance of the respective in - domain pairwise   models , while outperforming both the other models   on the S C dataset . Overall , the model   trained on the WSJ permuted document task gen-   eralizes well .   4.4 Linguistic Probe Analysis   Shen et al . ( 2021 ) create 8 hand - crafted linguistic   probe test sets by manually modifying words in co-   herent texts based on various linguistic phenomena ,   ensuring the incoherent text produced as a result   remains syntactically correct . Except for the words   targeted by the probe , the rest of the text remains   identical . Each test set has 100 samples each .   We evaluate the best performing LCD - G , UNC   and our full models on these test sets . The results   are shown in Table 3 along with some examples   from the dataset . The LCD - G model has mixed   success across the test sets . The UNC model has   the most success with the tense agreement test set   and is moderately successful on the pronoun test   sets . We see that our model has perfect accuracy   on all pronoun - related test sets and near - perfect ac-   curacy on the tense agreement test set . This shows   that our model is indeed capturing the discourse-   level phenomena that constitute coherence . Where   our model falters is in cases which may requirecommonsense knowledge , such as identifying that   6.7 wins is not possible . Overall , our model is quite   successful in detecting several kinds of incoher-   ence .   5 Conclusion   We show empirically that increasing the ratio and   quality of negative samples improves the general-   izability of the coherence model . We also test our   model on a wide - ranging collection of independent   test sets that resemble downstream applications ,   including machine generated text , on which our   model significantly outperforms the previous SOTA   model . Our work thus also sets a new evaluation   standard for future research in coherence model-   ing . We open source our code base to encourage   research in a new paradigm of coherence modeling .   Acknowledgements   We would like to thank the Senior Area Chairs of   ACL 2022 for evaluating our paper on its merits ,   and the reviewers and meta - reviewer of ARR for   their reviews . We would also like to thank our   colleagues Mathieu Ravaut and Han Cheol Moon   for their valuable inputs.6052Ethics Statement   Data   A description of the data pre - processing is provided   in § 2.1 . Datasets that we created will be open-   sourced . In the case of the WSJ dataset , the data is   licensed for use only to members by the Linguistic   Data Consortium . Consequently , we only release   scripts to generate the data we use and not the data   itself . We highlight however that the permuted   document self - supervision task that we train on is   independent of the dataset used and the task can   be reproduced on any other corpus ; see also § 4.3 .   All other datasets we use are licensed freely for   academic use .   Annotation of LMLMDataset   We conduct a user study to collect pairwise co-   herence judgments on our language model output   dataset . As part of our crowd - sourced user study   on Amazon Mechanical Turk to collect these coher-   ence judgements , we do not collect any personal   information from the participants . Based on the av-   erage time spent to perform the tasks , participants   were paid the equivalent of 16 USD per hour for   their work . The annotation instructions and inter-   face provided to the participants are included in   Appendix A.3 .   One potential issue is that the language model   output that we generate from prompts may lead   to malicious text generation by the models . We   flagged the task to warn the workers that there   may be potentially offensive content , and manu-   ally checked the final dataset post curation .   Applicability Across Languages   All our experiments are conducted using data for   the English language . However , as coherence and   discourse relations in text are a universal concept ,   and our training data is automatically generated ,   we expect the permuted document task to be easily   extensible to other languages .   References605360546055A Appendix   A.1 WSJ Permuted Document Task   The examples for the permuted document task on   the WSJ data are shown in Table 5 .   A.2 Hard Negative Ranking Pseudocode   The pseudocode for our hard negative mining   through local sample ranking is given in Algo-   rithm 1 .   A.3 LMLMUser Study   The instructions and the interface provided to the   workers in the user study comparing pairs of lan-   guage model outputs is given in Figure 3 . Workers   were restricted to the native English speaking re-   gions of Canada , United Kingdom and the United   States and could only participate in our task if they   had completed > 10,000HITs with a > 98 % ac-   ceptance rate . Each task was estimated to take 2   minutes , and workers were paid the equivalent of   16 USD per hour .   A.4 Comparison of Existing State - of - The - Art   Coherence Models   We report the results obtained by Pishdad et al .   ( 2020 ) and Mohiuddin et al . ( 2021 ) on their evalu-   ation tasks for SOTA neural coherence models in   Table 6 .   A.5 Hyperparameters   The hyperparameters used in our experiments are   given in Table 4.6056   Algorithm 1 Local Negative Sample Ranking   Require : Training data Din which each instance consists of a positive document and hnegative   documents , model θInitialize empty hard negative array ˆDfor each instance ∈Dprocedure HN R ( θ , D ) Partition the dataset into sets of xinstances D. . . D fori= 1 . . .rdo ifi==0 then ▷No hard negatives for first iteration forj= 1 . . . x do Randomly sample Nnegatives from Dand store in ˆD Train θwith ( D,ˆD ) forj= 1 . . . x do Score all the hnegative documents in D SortDin descending order of scores GetNtop scoring negative documents and store in ˆD ▷Store hard negatives for the next iteration60576058As reported by Pishdad et al . ( 2020 )   Task Dataset UNC Mesgar and Strube ( 2018 )   Permuted Document Visual Storytelling 88.42 82.25   Permuted Document ROCStories 94.80 89.55   Permuted Document Dialogue 97.21 90.79   Permuted Document HellaSwag 83.92 69.38   Permuted Document PDTB 92.85 61.96   Connective Substitution PDTB 96.46 84.99   Topic Switching Visual Storytelling 92.10 64.81   Topic Switching ROCStories 94.62 67.85   Topic Switching Dialogue 71.74 68.41   Topic Switching PDTB 70.89 52.33   As reported by Mohiuddin et al . ( 2021 )   Task Dataset UNC LCD   Permuted Document WSJ 93.19 91.77   Abstractive Summarization ( Agr . ) CNN 0.68 0.55   Extractive Summarization ( Agr . ) DUC 0.35 0.38   Machine Translation ( Agr . ) WMT 0.77 0.78   ( Trained ) Machine Translation ( Agr . ) WMT 0.83 0.756059