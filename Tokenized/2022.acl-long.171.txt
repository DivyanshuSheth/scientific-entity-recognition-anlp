  Songlin Yang , Kewei Tu   School of Information Science and Technology , ShanghaiTech University   Shanghai Engineering Research Center of Intelligent Vision and Imaging   { yangsl,tukw}@shanghaitech.edu.cn   Abstract   Constituency parsing and nested named entity   recognition ( NER ) are similar tasks since they   both aim to predict a collection of nested and   non - crossing spans . In this work , we cast   nested NER to constituency parsing and pro-   pose a novel pointing mechanism for bottom-   up parsing to tackle both tasks . The key idea   is based on the observation that if we traverse   a constituency tree in post - order , i.e. , visit-   ing a parent after its children , then two con-   secutively visited spans would share a bound-   ary . Our model tracks the shared boundaries   and predicts the next boundary at each step   by leveraging a pointer network . As a result ,   it needs only linear steps to parse and thus   is efﬁcient . It also maintains a parsing con-   ﬁguration for structural consistency , i.e. , al-   ways outputting valid trees . Experimentally ,   our model achieves the state - of - the - art perfor-   mance on PTB among all BERT - based mod-   els ( 96.01 F1 score ) and competitive perfor-   mance on CTB7 in constituency parsing ; and   it also achieves strong performance on three   benchmark datasets of nested NER : ACE2004 ,   ACE2005 , and GENIA .   1 Introduction   Constituency parsing is an important task in natural   language processing , having many applications in   downstream tasks , such as semantic role labeling   ( Fei et al . , 2021 ) , opinion mining ( Xia et al . , 2021 ) ,   among others . Named entity recognition ( NER ) is   a fundamental task in information extraction and   nested NER has been receiving increasing attention   due to its broader applications ( Byrne , 2007 ) .   Constituency parsing and nested NER are simi-   lar tasks since they both aim to predict a collection   of nested and non - crossing spans ( i.e. , if two spans   overlap , one must be a subspan of the other ) . Fig.1Figure 1 : ( a ) an example non - binary constituency tree .   ( b ) an example sentence with nested named entities .   We show the span and pointing representations .   shows example span representations of both tasks .   The difference between the two tasks is that the   collection of spans form a connected tree in con-   stituency parsing , whereas they form several tree   fragments in nested NER . However , we can add   a node that spans the whole sentence to connect   all tree fragments in nested NER to form a tree .   Because of the similarity , there are some previous   studies adapting methods from the constituency   parsing literature to tackle nested NER ( Finkel and   Manning , 2009 ; Wang et al . , 2018 ; Fu et al . , 2021 ) .   In this work , we focus on constituency parsing , but   our proposed method tackles nested NER as well .   The two main paradigms in constituency pars-2403ing are span - based and transition - based methods .   Span - based methods ( Stern et al . , 2017 ; Kitaev and   Klein , 2018 ; Zhang et al . , 2020 ; Xin et al . , 2021 ,   inter alia ) decompose the score of a constituency   tree into the scores of constituent spans and use   chart - based algorithms for inference . Built upon   powerful neural encoders , they have obtained state-   of - the - art results . However , they suffer from the   high inference time complexity of exact algorithms   or error propagation of top - down approximate al-   gorithms . In contrast , transition - based methods   ( Dyer et al . , 2016 ; Cross and Huang , 2016 ; Liu and   Zhang , 2017 , inter alia ) conduct a series of local   actions ( e.g. , shift and reduce ) to build the ﬁnal   parse in linear steps , so they enjoy lower parsing   time complexities . However , they suffer from the   error propagation and exposure bias problems .   Recently , Nguyen et al . ( 2021a ) propose   a sequence - to - sequence ( seq2seq ) model with   pointer networks ( Vinyals et al . , 2015a ) . They   cast constituency parsing to a top - down splitting   problem . First , they use neural encoders to obtain   span representations , similar to span - based meth-   ods . Then they feed input parent span representa-   tions into the neural decoder recursively following   the order shown in Fig . 2(a)—which amounts to   pre - order traversal — to output a series of splitting   points ( i.e. , boundaries ) via pointer networks , so   that each parent span is split into two child spans .   Notably , Nguyen et al . ( 2020 ) propose a similar   top - down pointing mechanism , but they design a   chart - based parsing algorithm instead of adopting   seq2seq modeling , and has been shown underper-   forming Nguyen et al . ( 2021a ) . Thanks to seq2seq   modeling , Nguyen et al . ( 2021a ) ’s model achieves a   competitive parsing performance with a lower pars-   ing complexity compared with span - based meth-   ods .   However , their model has two main limitations .   First , when generating each constituent , its subtree   features can not be exploited since its subspans have   not been realized yet ( Liu and Zhang , 2017 ) . Thus   it is difﬁcult for the model to predict the splitting   point of a long span due to a lack of its subtree in-   formation , which exacerbates the error propagation   problem and undermines the parsing performance .   Second , since each parent span can only be split   into two , their parsing algorithm can only ouput   binary trees , thus needing binarization .   In this work , we devise a novel pointing mecha-   nism for bottom - up parsing using ( almost ) the same   seq2seq backbone as Nguyen et al . ( 2021a ) . Our   model is able to overcome the two aforementioned   limitations of Nguyen et al . ( 2021a ) . The main idea   is based on the observation that if we traverse a con-   stituency tree in post - order ( i.e. , visiting a parent   after its children ) , two consecutively visited con-   stituent spans would share a boundary . Fig . 2(b )   shows an example : the right boundary of 1is   also the left boundary of 2and the right bound-   ary of 5is also the right boundary of 6 . Based   on this observation , we propose to use a cursor to   track the shared boundary boundaries and at each   step , leverage a pointer network to predict the next   boundary for generating the next constituent span   and update the cursor to the right boundary of the   new span . Our model generates one span at each   step , thus needing only linear steps to parse a sen-   tence , which is efﬁcient . Besides , our model can   leverage rich subtree features encoded in the neural   decoder to generate parent constituent spans , which   is especially helpful in predicting long spans . Fi-   nally , our model can output n - ary trees , enabling di-   rect modeling of the original non - binary parse tree   structures in treebanks and eliminating the need for   binarization.2404We conduct experiments on the benchmarking   PTB and CTB for constituency parsing . On PTB ,   we achieve the state - of - the - art performance ( 96.01   F1 score ) among all BERT - based models . On CTB ,   we achieve competitive performance . We also ap-   ply our method to nested NER and conduct exper-   iments on three benchmark datasets : ACE2004 ,   ACE2005 , and GENIA . Our method achieves com-   parable performance to many tailored methods of   nested NER , beating previous parsing - based meth-   ods . Our contributions can be summarized as the   following :   •We propose a novel pointing mechanism for   bottom - up n - ary tree parsing in linear steps .   •Our model achieves the state - of - the - art result   on PTB in constituency parsing . We further   show its application in nested NER where it   achieves competitive results .   2 Methods   2.1 Preprocessing   It is known that constituency parsing can be re-   garded as a top - down splitting problem where par-   ent spans are recursively split into pairs of subspans   ( Stern et al . , 2017 ; Shen et al . , 2018 ; Nguyen et al . ,   2020 , 2021a ) . However , this formulation can out-   put binary trees . We make an extension to cast   constituency parsing as top - down segmentation ,   i.e. , parent spans are segmented into 2 subspans   recursively , for the sake of outputting n - ary trees .   To this end , we add some ; spans ( we do not al-   low two adjacent;spans to eliminate ambiguities )   so that each span is either a bottommost span or   can be segmented by its subspans . For instance ,   in Fig 2 , 3is a bottom - most span , and 7can   be segmented by 2,3and 6 . We always   include the whole - sentence span in order to cast   other tasks , e.g. , nested NER , to constituency pars-   ing . We also collapse unary chains to atomic labels   in constituency parsing , e.g. , S->VP!S+VP .   2.2 Parsing conﬁguration   A problem of seq2seq constituency parsers is how   to maintain structural consistency , i.e. , outputting   valid trees . To solve this problem , our pointing   system maintains a parsing conﬁguration , which is   a quadruple ( c;A;p;S ) where :   •c : index of the cursor .   •A : set of indices of all candidate boundaries.•p : the left boundary of the lastly created span ,   which is needed to maintain A.   •S : set of generated spans .   We can see from Fig . 3 that in the beginning ,   the cursorclies at 0 . At each step , cpoints   to another boundary afromAto form a span   ( min(c;a);max(c;a ) ) . There are two cases :   •c < a : a new bottom - most span is generated .   •a < c : several consecutive spans are merged   into a larger span . It is worthy to note that   we can merge > = 2 spans in a single step ,   which allows our model to perform n - ary tree   parsing .   In the ﬁrst case , the new bottom - most span can   combine with the very previous span to form a   larger span whose left boundary is p , so we push p   back toA(except for the case that p = null ) . In   the later case , the very previous span is a subspan of   the new span and thus pcannot be pushed back . In   both cases , all indices min(c;a)i < max(c;a )   are removed from Adue to the post - order gener-   ation restriction ; pis updated to min(c;a)andc   is updated to max(c;a ) . The process stops when   the whole - sentence span is generated . Table 1 for-   malises this process .   Oracle . The oracle pointing representations   shown in Fig.1 can be generated by running a post-   order traversal of the tree ( e.g. , Fig.2 ) and for each   traversed span , pointing the cursor from its bound-   ary shared with the previous span to its other bound-   ary . If we do not allow two consecutive ; spans ,   the oracle is unique under our pointing system ( we   give a proof in Appendix A.1 by contradiction ) .   2.3 Model   Given a sentence w = w;:::;x , we add < bos >   ( beginning of sentence ) as wand < eos > ( end   of sentence ) as w. The oracle is fq !   p;yg , whereyis the span label and we   usel= min(q;p)andr= max(q;p)to de-   note the left and right boundary of the i - th span ,   respectively .   Encoder . We feed the sentence into BERT ( De-   vlin et al . , 2019 ) and for each word w , we   use the last subtoken emebedding of the last   layer as its dense representations x. Then we   feedx;:::;xinto a three - layer bidirectional2405Initial conﬁguration ( c;A;p;s ) = ( 0;f1;2;:::;ng;null ; ;)   Goal ( 0;n)2S   Pointing action Input Output Precondition   L- -a ( c;A;p;S ) ) ( c;Anfa;:::;c 1g;a;S[f(a;c)g ) 0a < c   R - -a ( c;A;p;S ) ) ( a;A[fpgnfc;:::;a 1g;c;S[f(c;a)g)c < an ,   LSTM ( Hochreiter and Schmidhuber , 1997 ) ( BiL-   STM ) to obtain c;:::;c , wherec= [ f;g ]   andfandgare the forward and backward hid-   den states of the last BiLSTM layer at position i   respectively .   Boundary and span representation . We use   fencepost representation ( Cross and Huang , 2016 ;   Stern et al . , 2017 ) to encode the i - th boundary lying   betweenxandx :   b= [ f;g ]   then we represent span ( i;j)as :   h = MLP(b b )   Decoder . We use a unidirectional one - layer   LSTM network as the decoder :   d = LSTM ( d;h;E);t2(1)wheredis the hidden state of the LSTM decoder   at time stept , Eis the label embedding matrix , ; is   the concatenation operation . For the ﬁrst step , we   feed a randomly initialized trainable vector dand   a special < START > embedding into the decoder to   obtaind .   Pointing score . We use a deep biafﬁne function   ( Dozat and Manning , 2017 ) to estimate the pointing   scoresof selecting the i - th boundary at time step   t :   d = MLP(d )   b = MLP(b )   s=   b ; 1Wd   where MLP and MLP are multi - layer   perceptrons ( MLPs ) that project decoder states   and boundary representations into k - dimensional   spaces , respectively ; W2R.2406Label score . For a newly predicted span , we feed   the concatenation of the span representation and   the decoder state into another MLP to calculate the   label scoree :   H = MLP([d;b b ] )   e = HE   Note that we reuse the label embedding matrix   from Eq . 1 to facilitate parameter sharing .   Training objective . The training loss is decom-   posed into the pointing loss and the labeling loss :   L = L + L   L =  XlogexpfsgPexpfsg   L =  Xlogexpfeg   Pexpfeg   wherejLjis the number of labels . Note that in   the pointing loss we normalize over all boundaries   instead of only accessible boundaries , because we   ﬁnd it performs better in our preliminary experi-   ments .   Parsing . Our model follows the description in   the previous subsection for parsing . For each   time stept , it selects the highest - scoring acces-   sible boundary to generate the span , then selects   the highest - scoring label of the generated span , and   updates the parsing conﬁguration ( Table 1 ) .   3 Experiment setup   3.1 Data setup   Constituency parsing . We conduct experiments   on Penn Treebank ( PTB ) 3.0 ( Marcus et al . , 1993 )   and Chinese Treebank ( CTB ) ( Xue et al . , 2005 ) .   Many previous researchers report that the results on   CTB5.1 are unstable and of high variance ( Zhang   et al . , 2020 ; Yang and Deng , 2020 ) . So we follow   the suggestion of Zhang et al . ( 2020 ) to conduct   experiments on CTB7 instead of CTB5.1 for more   robust evaluation as CTB7 has more test sentences   and has a higher annotation quality . We use the   standard data splits for both PTB and CTB .   Nested NER . We conduct experiments on three   benchmark datasets : ACE2004 ( Doddington et al . ,   2004 ) , ACE2005 ( Walker et al . , 2006 ) , and GENIA(Kim et al . , 2003 ) . We use the same data prepro-   cessing as Shibuya and Hovy ( 2020 ) .   3.2 Evaluation   We report labeled recall / precision / F1 scores based   onEVALBfor constituency parsing ; span - level   labeled recall / precision / F1 scores for nested NER .   All reported results are averaged over three runs   with different random seeds .   3.3 Implementation details   We use " bert - large - cased " ( Devlin et al . , 2019 )   for PTB , ACE2004 and ACE2005 ; " bert - chinese-   based " for CTB ; and " biobert - large - cased - v1.1 "   ( Lee et al . , 2020 ) for GENIA . We use no other   external resources ( e.g. , predicted / gold POS tags ,   external static word embedding ) . The hidden size   of LSTM is set to 1000 for both the encoder and the   decoder . We add dropouts in LSTM / MLP layers .   The dropout rate is set to 0.33 . The hidden and out-   put sizes of all MLPs are set to 500 . The value of   gradient clipping is set to 5 . The number of training   epochs is set to 10 for PTB , CTB , GENIA ; 50 for   ACE2004/2005 . We use Adam ( Kingma and Ba ,   2015 ) as the optimizer with  = 0:9and  = 0:9 .   The maximal learning rate is set to 5e 5for BERT   and2:5e 3for all other components . We use the   ﬁrst10 % epochs to linearly warmup the learning   rates of each components to their maximum value   and gradually decay them to zero for the rest of   epochs . We batch sentences of similar lengths to   make full use of GPUs and the number of tokens   in a single batch is set to 3000 .   4 Main result   On both PTB and CTB , we ﬁnd incorporating   Ein Eq . 1 leads to a slightly inferior perfor-   mance ( -0.02 F1 score on PTB and -0.05 F1 score   on CTB ) , so we report results without this input   feature .   Table 2 shows the results on PTB test set . Our   method achieves 96.01 F1 score , outperforming the   method of Nguyen et al . ( 2021a ) by 0.31 F1 and   having the same worst - case O(n)parsing time   complexity as theirs . It also outperforms all span-2407   Model P R F   Zhang et al . ( 2020 ) [ S ] 91.73 91.38 91.55   Ours[Q ] 91.66 91.31 91.49   based methods , obtaining the state - of - the - art per-   formance among all BERT - based models while en-   joying a lower parsing complexity .   Table 3 shows the results on CTB7 . Our method   obtains 91.49 F1 score , which is comparable to   the method of Zhang et al . ( 2020 ) but has a lower   complexity ( worst - case O(n)vs . O(n ) ) .   Table 4 shows the results on three benchmark   dataset on nested NER . We ﬁnd that incorporating   Eis important , leading to +0.67 F1 score and   +0.52 F1 sore on ACE2004 and ACE2005 , respec-   tively . Although our method underperforms two   recent state - of - the - art methods : Shen et al . ( 2021 )   and Tan et al . ( 2021 ) , we ﬁnd it has a competitive   performance to other recent works ( Wang et al . ,   2021 ; Yan et al . , 2021 ; Fu et al . , 2021 ) . The most   comparable one is the method of Fu et al . ( 2021 ) ,   which belongs to parsing - based methods as ours .   They adapt a span - based constituency parser to   tackle nested NER using the CYK algorithm for   training and inference . Our model outperforms   theirs by 0.34 F1 and 0.13 F1 scores on ACE2004   and ACE2005 and has a similar performance to   theirs on GENIA , meanwhile enjoying a lower in-   ference complexity .   5 Analysis   Error analysis . As we discussed previously ,   bottom - up parsing can make use of the subtree   features when predicting parent spans , so it is ex-   pected to have higher F1 scores on longer spans .   To verify this , we plot Fig . 4 to show the changes   of F1 scores with different constituent span lengths   on the PTB test set . We can see that our method   consistently outperforms the method of ( Nguyen   et al . , 2021a ) on all span lengths , but our advantage   is most prominent for spans of length > 30 , which   veriﬁes our conjecture . In Fig . 5 , we can see that   when a constituent has multiple children ( > 3 ) , our   method performs much better than that of ( Nguyen   et al . , 2021a ) , which validates the beneﬁt of n - ary   tree parsing . An intuitive explanation of this ben-   eﬁt is that our method predicts n - ary branching   structures in a single step , whereas theirs needs   multiple steps , which is more error - prone.2408Model ACE2004 ACE2005 GENIA   P R F P R F P R F   Shibuya and Hovy ( 2020 ) 84.71 83.96 84.33 82.58 84.29 83.42 79.92 76.55 78.20   Wang et al . ( 2020 ) 86.08 86.48 86.26 83.95 85.39 84.66 79.45 78.94 79.19   Wang et al . ( 2021 ) 86.27 85.09 85.68 85.28 84.15 84.71 79.20 78.16 78.67   Fu et al . ( 2021 ) 86.7 86.5 86.6 84.5 86.4 85.4 78.2 78.2 78.2   Xu et al . ( 2021 ) 86.9 85.8 86.3 85.7 85.2 85.4 80.3 78.9 79.6   Yan et al . ( 2021 ) 87.27 86.41 86.84 83.16 86.38 84.74 78.57 79.3 78.93   Shen et al . ( 2021 ) 87.44 87.38 87.41 86.09 87.27 86.67 80.19 80.89 80.54   Tan et al . ( 2021 ) 88.46 86.10 87.26 87.48 86.64 87.05 82.31 78.66 80.44   Ours 86.60 87.28 86.94 84.61 86.43 85.53 78.08 78.26 78.16   w.o . Ein Eq.1 85.66 86.88 86.27 83.75 86.31 85.01 78.46 77.97 78.22   Effect of beam search . We also tried beam   search but observed very slight improvement or   even worse performance ( e.g. , +0.05 F1 score on   PTB and -0.03 F1 score on CTB when we use a   beam size 20 ) . Hence we report all results using   greedy decoding for simplicity . This suggests that   greedy decoding can yield near - optimal solutions ,   indicating that our model is less prone to the error   propagation problem .   Effect of training loss . As discussed in Sec . 2.3 ,   we ﬁnd that explicitly considering the structural   consistency constraints when normalizing is harm-   ful ( -0.12 F1 score on PTB , -0.10 F1 score on CTB ) .   We speculate that not enforcing the constraints dur-   ing training can help the model to learn the con-   straints implicitly , which is helpful for the model   to generalize better on the unseen test set . Notably ,   Nguyen et al . ( 2021a ) also adopt this strategy , i.e. ,   normalizing over all boundaries .   Speed . Similar to Nguyen et al . ( 2021a ) , the train-   ing process ( i.e. , teacher forcing ) can be fully par-   allelized without resorting to structured inference ,   which could be compute - intensive or hard to par-   allelize . On PTB , it takes only 4.5 hours to train   the model using BERT as the encoder with a single   Titan V GPU . As for parsing , our method has the   same parsing complexity as Nguyen et al . ( 2021a ) ,   i.e. , worst - case O(n ) . Table 5 shows the speed   comparison on parsing the PTB test set ( we report   values based on a single Titan V GPU and not using   BERT as encoder following Nguyen et al . ( 2021a ) ) .   We report the average number of pointing actions   in Appendix A.2 .   6 Related Work   Constituency parsing . There are many methods   to tackle constituency parsing , such as transition-   based methods ( Dyer et al . , 2016 ; Cross and Huang ,   2016 ; Liu and Zhang , 2017 ; Yang and Deng ,   2020 ) , span - based methods ( Stern et al . , 2017 ; Ki-   taev and Klein , 2018 ; Kitaev et al . , 2019 ; Zhang   et al . , 2020 ; Wei et al . , 2020 ; Nguyen et al . , 2020 ;   Xin et al . , 2021 ) , sequence - to - sequence ( seq2seq)-   based methods ( Vinyals et al . , 2015b ; Fernández-   González and Gómez - Rodríguez , 2020 ) , sequence-   labeling - based methods ( Gómez - Rodríguez and Vi-   lares , 2018 ; Vilares et al . , 2019 ; Kitaev and Klein ,   2020 ) , among others .   Our work belongs to the category of seq2seq-   based methods . Previous seq2seq models linearize   constituency trees into bracket sequences ( Vinyals   et al . , 2015b ) or shift - reduce action sequences   ( Ma et al . , 2017 ; Fernández - González and Gómez-   Rodríguez , 2020 ) . However , they may produce   invalid outputs and their performance lags behind   span - based methods . Recently , seq2seq models lin-2409earize constituency trees into sequences of spans   in pre - order ( Nguyen et al . , 2021a ) or in in - order   ( Wei et al . , 2021 ) . Our method generates sequences   of spans in post - order instead , which has the advan-   tage of utilizing rich subtree features and perform-   ing direct n - ary tree parsing .   Binarization is de facto in constituency parsing ,   but there is a recent trend toward n - ary parsing .   Previous span - based methods adopt either explicit   binarization ( Zhang et al . , 2020 ) or implicit bina-   rization ( Stern et al . , 2017 ; Kitaev and Klein , 2018 ) .   Although the implicit binarization strategy elimi-   nates the need for binarization in training , it can   only output binary trees during decoding . Xin et al .   ( 2021 ) propose an n - ary - aware span - based method   by deﬁning semi - Markov processes on each par-   ent span so that the transition scores of adjacent   sibling child - spans are explicitly considered in pars-   ing . Fernández - González and Gómez - Rodríguez   ( 2019 ) ; Yang and Deng ( 2020 ) propose novel tran-   sition systems to model n - ary trees . Our method   outputs n - ary trees without the need for binariza-   tion via a novel pointing mechanism .   Parsing with pointer networks . Pointer Net-   works ( Vinyals et al . , 2015a ) are introduced to   the parsing literature by Ma et al . ( 2018 ) and   quickly become popular in various parsing sub-   tasks because they are ﬂexible to predict various   trees / graphs and can achieve very competitive per-   formance . Ma et al . ( 2018 ) linearize a dependency   tree in a top - down depth-ﬁrst and inside - out man-   ner and use a pointer network to predict the lin-   earized dependency tree , which is then extended   by Lin et al . ( 2019 ) to discourse parsing . Liu et al .   ( 2019 ) add shortcuts between the decoder states   of the previously generated parents / siblings to the   current decoder states in both dependency and dis-   course parsing . Fernández - González and Gómez-   Rodríguez ( 2019 ) propose a left - to - right depen-   dency parser that predicts the heads of each word   autoregressively , and later , they propose right - to-   left and outside - in variants ( Fernández - González   and Gómez - Rodríguez , 2021a ) . They also adapt   the left - to - right dependency parser to semantic de-   pendency parsing ( which predicts acyclic graphs   instead of trees ) ( Fernández - González and Gómez-   Rodríguez , 2020 ) , discontinuous constituency   parsing ( by treating discontinuous constituency   trees as augmented dependency trees ) ( Fernández-   González and Gómez - Rodríguez , 2020 ) , and joint   dependency and constituency parsing ( Fernández - González and Gómez - Rodríguez , 2020 ) . They use   a pointer network to reorder the sentence to re-   duce discontinuous constituency parsing to contin-   uous constituency parsing ( Fernández - González   and Gómez - Rodríguez , 2021b ) . Nguyen et al .   ( 2021a , b ) cast ( discourse ) constituency / RST pars-   ing as conditional splitting and use pointer net-   works to select the splitting points . Zhou et al .   ( 2021 ) propose an action - pointer network for AMR   parsing .   Nested NER . There are also many methods to   tackle nested NER , such as hypergraph - based meth-   ods ( Lu and Roth , 2015 ; Katiyar and Cardie , 2018 ;   Wang and Lu , 2018 ) , sequence - labeling - based   methods ( Shibuya and Hovy , 2020 ; Wang et al . ,   2021 ) , parsing - based methods ( Finkel and Man-   ning , 2009 ; Wang et al . , 2018 ; Fu et al . , 2021 ) ,   layered methods ( Fisher and Vlachos , 2019 ; Wang   et al . , 2020 ; Luo and Zhao , 2020 ) , span - based   methods ( Yu et al . , 2020 ; Li et al . , 2021 ) , object-   detection - based methods ( Shen et al . , 2021 ; Tan   et al . , 2021 ) etc .   Our work belongs to the category of parsing-   based methods . Finkel and Manning ( 2009 ) insert   named entities into a constituency tree and use a   discriminative parser ( Finkel et al . , 2008 ) for learn-   ing and prediction . Wang et al . ( 2018 ) adapt a   shift - reduce transition - based parser to output a con-   stituency forest instead of a constituency tree for   nested NER . Fu et al . ( 2021 ) adapt a span - based   neural TreeCRF parser , treat nested named enti-   ties as the observed parts of a partially - observed   constituency tree and develop a masked inside algo-   rithm to marginalize all unobserved parts for maxi-   mizing the probability of the observed named enti-   ties . Our method has a better performance as well   as a lower time complexity than Fu et al . ( 2021 ) .   Recently , Lou et al . ( 2022 ) extend the work of Fu   et al . ( 2021 ) , casting nested NER to lexicalized   constituency parsing for leveraging headword in-   formation . They achieve a higher performance at   the cost of a higher parsing complexity , i.e. , O(n ) .   7 Discussion and future work   In the deep learning era , global optimization on   trees becomes less important in both training and   decoding . Teng and Zhang ( 2018 ) show that a   span - based model trained with a local span clas-   siﬁcation loss performs well in conjunction with   CYK decoding . Wei et al . ( 2020 ) ; Nguyen et al .   ( 2020 ) show that top - down greedy decoding per-2410forms comparably . In this work we have shown that   greedy decoding works well . Thus it would also be   a fruitful direction to design more powerful neural   decoders which can leverage more subtree informa-   tion and can maintain structural consistency . Also ,   it is a fruitful direction to devise more powerful   span representations .   8 Conclusion   In this work we have presented a novel pointing   mechanism and model for bottom - up constituency   parsing , which allows n - ary tree parsing in linear   steps . Experiments on multiple datasets show the   effectiveness of our methods in both constituency   parsing and nested NER .   Acknowledgments   We thank the anonymous reviewers for their con-   structive comments . This work was supported by   the National Natural Science Foundation of China   ( 61976139 ) .   References2411241224132414   A Appendix   A.1 Uniqueness of oracle   If there are two oracles oandooutputting   the same tree . Their parsing conﬁguration is   ( c;A;p;S)and(C;A;p;S ) , respectively .   Assume that the ﬁrst kth pointing actions of o   andoare the same ( so they share the same cur-   sorc ) and thek+ 1th action is ( c!a;y)and   ( c!a;y)respectively . We enumerate all possi-   bilities :   •a < c < a , then ( a;c)exists inS.c   would be updated to a , so thereafter the end-   point of the generated span is a , thus   ( a;c)cannot exist in Ssincec < a.   •a < c < a , then ( a;c)exists inS. Similar   to the previous case , we can conclude that   ( a;c)cannot exist in S.   •a < a < c , then ( a;c)exists inS , but   a62Afor all remaining steps , thus ( a;c )   can not exist in S.   •a < a < c. This is similar to the previous   case .   •c < a < a , then ( a;c)exists inS , but   a62Afor the remaining steps , thus ( a;c )   can not exist in S.   •c < a < a. This is similar to the previous   case .   Hence there is exact one oracle and we have proved   it by contradiction .   A.2 Number of actions   The system of Nguyen et al . ( 2021a ) needs exact   n 1actions to parse a length- nsentence . While   our model requires 2n 1actions in the worst   case because we generate one span at each step and   there are at most 2n 1spans if the corresponding   constituency tree is a full binary tree . So there   is a concern that our model needs twice time to   parse . Empirically , since the constituency trees   in the treebank are not full binary trees in most   cases , we need less than 2n 1steps to parse . Fig .   6 shows the number of actions needed to parse   with different sentence lengths in PTB training set .   The red line is y = x 1and the green line is   y= 2x 1 . In average , our method needs 1.13   actions per token , Nguyen et al . ( 2021a ) needs 0.96   action per token . So , our method is around 20 %   slower than theirs . Fig . 7 shows the case in nested   NER . We only need 0.40 action per token since   the spans in nested NER is more sparse than that2415 in constituency parsing . Our method is expectedly   faster than other parsing - based methods in nested   NER , such as the transition system of Wang and Lu   ( 2018 ) , which needs at least one action per word ;   and the span - based method of Fu et al . ( 2021 ) ,   which needs cubic time for CYK parsing.2416