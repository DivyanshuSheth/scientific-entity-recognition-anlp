  Bohong Wu , Zhuosheng Zhang , Jinyuan Wang , Hai ZhaoDepartment of Computer Science and Engineering , Shanghai Jiao Tong UniversityKey Laboratory of Shanghai Education Commission for Intelligent Interaction   and Cognitive Engineering , Shanghai Jiao Tong University   chengzhipanpan@sjtu.edu.cn,zhangzs@sjtu.edu.cn   steve_wang@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn   Abstract   Training dense passage representations via   contrastive learning has been shown effective   for Open - Domain Passage Retrieval ( ODPR ) .   Existing studies focus on further optimizing   by improving negative sampling strategy or   extra pretraining . However , these studies   keep unknown in capturing passage with   internal representation conÔ¨Çicts from improper   modeling granularity . SpeciÔ¨Åcally , under our   observation that a passage can be organized   by multiple semantically different sentences ,   modeling such a passage as a uniÔ¨Åed dense   vector is not optimal . This work thus   presents a reÔ¨Åned model on the basis of a   smaller granularity , contextual sentences , to   alleviate the concerned conÔ¨Çicts . In detail ,   we introduce an in - passage negative sampling   strategy to encourage a diverse generation   of sentence representations within the same   passage . Experiments on three benchmark   datasets verify the efÔ¨Åcacy of our method ,   especially on datasets where conÔ¨Çicts are   severe . Extensive experiments further present   good transferability of our method across   datasets .   1 Introduction   Open - Domain Passage Retrieval ( ODPR ) has   recently attracted the attention of researchers for   its wide usage both academically and industrially   ( Lee et al . , 2019 ; Yang et al . , 2017 ) . Provided   with an extremely large text corpus that composed   of millions of passages , ODPR aims to retrieve   a collection of the most relevant passages as the   evidences of a given question .   With recent success in pretrained language   models ( PrLMs ) like BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) , dense retrieval   techniques have achieved signiÔ¨Åcant better resultsthan traditional lexical based methods , including   TF - IDF ( Ramos et al . , 2003 ) and BM25 ( Robertson   and Zaragoza , 2009 ) , which totally neglect   semantic similarity . Thanks to the Bi - Encoder   structure , dense methods ( Lee et al . , 2019 ; Guu   et al . , 2020 ; Karpukhin et al . , 2020 ) encode   the Wikipedia passages and questions separately ,   and retrieve evidence passages using similarity   functions like the inner product or cosine similarity .   Given that the representations of Wikipedia   passages could be precomputed , the retrieval speed   of dense approaches could be on par with lexical   ones .   Previous approaches often pretrain the Bi-   Encoders with a specially designed pretraining   objective , Inverse Cloze Task ( ICT ) ( Lee et al . ,   2019 ) . More recently , DPR ( Karpukhin et al . , 2020 )   adopts a simple but effective contrastive learning   framework , achieving impressive performance   without any pretraining . Concretely , for each   question q , several positive passages pand hard   negative passages pproduced by BM25 are   pre - extracted . By feeding the Bi - Encoder with   ( q ; p ; p)triples , DPR simultaneously maximizes   the similarity between the representation of qand   corresponding p , and minimizes the similarity   between the representations of qand all p.   Following such contrastive learning framework ,   many researchers are seeking further improvements   for DPR from the perspective of sampling strategy   ( Xiong et al . , 2020 ; Lu et al . , 2020 ; Tang et al . ,   2021 ; Qu et al . , 2021 ) or extra pretraining ( Sachan   et al . , 2021 ) , or even using knowledge distillation   ( Izacard and Grave , 2021 ; Yang et al . , 2021 ) .   However , these studies fail to realize that there   exist severe drawbacks in the current contrastive   learning framework adopted by DPR . Essentially ,   as illustrated in Figure 1 , each passage pis   composed of multiple sentences , upon which   multiple semantically faraway questions can be   derived , which forms a question set Q=1062   fq ; q ; : : : ; qg . Under our investigation , such a   one - to - many problem is causing severe conÔ¨Çicting   problems in the current contrastive learning   framework , which we refer to as Contrastive   ConÔ¨Çicts . To the best of our knowledge , this is   the Ô¨Årst work that formally studies the conÔ¨Çicting   problems in the contrastive learning framework of   dense passage retrieval . Here , we distinguish two   kinds of Contrastive ConÔ¨Çicts .   Transitivity of Similarity The goal of the con-   trastive learning framework in DPR is to maximize   the similarity between the representation of the   question and its corresponding gold passage . As   illustrated in Figure 2 , under Contrastive ConÔ¨Çicts ,   the current contrastive learning framework will   unintendedly maximize the similarity between   different question representations derived from the   same passage , even if they might be semantically   different , which would possibly be the cause of   the low performance on SQuAD ( Rajpurkar et al . ,   2016 ) for DPR(SQuAD has an average of 2.66   questions per passage ) .   Multiple References in Large Batch Size   According to Karpukhin et al . ( 2020 ) , the   performance of DPR highly beneÔ¨Åts from large   batch size in the contrastive learning framework .   However , under Contrastive ConÔ¨Çicts , one passage   could be the positive passage pof multiple   questions ( i.e. the question set Q ) . Therefore ,   a large batch size will increase the probability   that some questions of Qmight occur in the same   batch . With the widely adopted in - batch negative   technique ( Karpukhin et al . , 2020 ; Lee et al . , 2021 ) ,   suchpwill be simultaneously referred to as both   the positive sample and the negative sample for   every qinQ , which is logically unreasonable .   Since one - to - many problem is the direct cause   of both conÔ¨Çicts , this paper presents a simple but   effective strategy that breaks down dense passage   representations into contextual sentence level ones ,   which we refer to as Dense Contextual Sentence   Representation ( DCSR ) . Unlike long passages , it   is hard to derive semantically faraway questions   from one short sentence . Therefore , by modeling   ODPR in smaller units like contextual sentences ,   we fundamentally alleviate Contrastive ConÔ¨Çicts   by solving the one - to - many problem . Note that   we do not simply encode each sentence separately .   Instead , we encode the passage as a whole and use   sentence indicator tokens to acquire the sentence   representations within the passage , to preserve   the contextual information . We further introduce   the in - passage negative sampling strategy , which   samples neighboring sentences of the positive one   in the same passage to create hard negative samples .   Finally , concrete experiments have veriÔ¨Åed the   effectiveness of our proposed method from both   retrieval accuracy and transferability , especially on   datasets where Contrastive ConÔ¨Çicts are severe .   Contributions ( i ) We investigate the defects of the   current contrastive learning framework in training   dense passage representation in Open - Domain   Passage Retrieval . ( ii ) To handle Contrastive   ConÔ¨Çicts , we propose to index the Wikipedia   corpus using contextual sentences instead of   passages . We also propose the in - passage negative   sampling strategy in training the contextual   sentence representations . ( iii ) Experiments show   that our proposed method signiÔ¨Åcantly outperforms   original baseline , especially on datasets where   Contrastive ConÔ¨Çicts are severe . Extensive   experiments also present better transferability of1063our DCSR , indicating that our method captures the   universality of the concerned task datasets .   2 Related Work   Open - Domain Passage Retrieval Open - Domain   Passage Retrieval has been a hot research topic   in recent years . It requires a system to extract   evidence passages for a speciÔ¨Åc question from   a large passage corpus like Wikipedia , and is   challenging as it requires both high retrieval   accuracy and speciÔ¨Åcally low latency for practical   usage . Traditional approaches like TF - IDF   ( Ramos et al . , 2003 ) , BM25 ( Robertson and   Zaragoza , 2009 ) retrieve the evidence passages   based on the lexical match between questions and   passages . Although these lexical approaches meet   the requirement of low latency , they fail to capture   non - lexical semantic similarity , thus performing   unsatisfying on retrieval accuracy .   With recent advances of pretrained language   models ( PrLMs ) like BERT ( Devlin et al . , 2019 ) ,   RoBERTa ( Liu et al . , 2019 ) , a series of neural   approaches based on cross - encoders are proposed   ( Vig and Ramea , 2019 ; Wolf et al . , 2019 ) .   Although enjoying satisfying retrieval accuracy ,   the retrieval latency is often hard to tolerate in   practical use . More recently , the Bi - Encoder   structure has captured the researchers ‚Äô attention .   With Bi - Encoder , the representations of the corpus   at scale can be precomputed , enabling it to   meet the requirement of low latency in passage   retrieval . Lee et al . ( 2019 ) Ô¨Årst proposes to   pretrain the Bi - Encoder with Inverse Cloze Task   ( ICT ) . Later , DPR ( Karpukhin et al . , 2020 )   introduces a contrastive learning framework to train   dense passage representation , and has achieved   impressive performance on both retrieval accuracy   and latency . Based on DPR , many works make   further improvements either by introducing better   sampling strategy ( Xiong et al . , 2020 ; Lu et al . ,   2020 ; Tang et al . , 2021 ; Qu et al . , 2021 ) or   extra pretraining ( Sachan et al . , 2021 ) , or even   distilling knowledge from cross - encoders ( Izacard   and Grave , 2021 ; Yang et al . , 2021 ) .   Our method follows the contrastive learning   research line of ODPR . Different from previous   works that focus on either improving the quality   of negative sampling or using extra pretraining ,   we make improvements by directly optimizing the   modeling granularity with an elaborately designed   contrastive learning training strategy . Contrastive Learning Contrastive learning re-   cently is attracting researchers ‚Äô attention in all   area . After witnessing its superiority in Computer   Vision tasks ( Chen et al . , 2020 ; He et al . , 2020 ) ,   researchers in NLP are also applying this technique   ( Wu et al . , 2020 ; Karpukhin et al . , 2020 ; Yan   et al . , 2021 ; Giorgi et al . , 2021 ; Gao et al . ,   2021 ) . For the concern of ODPR , the research   lines of contrastive learning can be divided into   two types : ( i ) Improving the sampling strategies   for positive samples and hard negative samples .   According to ( Manmatha et al . , 2017 ) , the quality   of positive samples and negative samples are   of vital importance in the contrastive learning   framework . Therefore , many researchers seek   better sampling strategies to improve the retrieval   performance ( Xiong et al . , 2020 ) . ( ii ) Improving   the contrastive learning framework . DensePhrase   ( Lee et al . , 2021 ) uses memory bank like MOCO   ( He et al . , 2020 ) to increase the number of in-   batch negative samples without increasing the GPU   memory usage , and models retrieval process on   the phrase level but not passage level , achieving   impressive performance .   Our proposed method follows the second   research line . We investigate a special phe-   nomenon , Contrastive ConÔ¨Çicts in the contrastive   learning framework , and experimentally verify   the effectiveness of mediating such conÔ¨Çicts by   modeling ODPR in a smaller granularity . More   similar to our work , Akkalyoncu Yilmaz et al .   ( 2019 ) also proposes to improve dense passage   retrieval based on sentence - level evidences , but   their work is not in the research line of contrastive   learning , and focuses more on passage re - ranking   after retrieval but not retrieval itself .   3 Methods   3.1 Contrastive Learning Framework   Existing contrastive learning framework aims to   maximize the similarity between the representa-   tions of each question and its corresponding gold   passages .   Suppose there is a batch of nquestions ,   ncorresponding gold passages and in total k   hard negative passages . Denote the questions   in batch as q ; q ; : : : ; q , their corresponding   gold passages as gp ; gp ; : : : ; gp , and hard   negative passages as np ; np ; : : : ; np . Two   separate PrLMs are Ô¨Årst used separately to   acquire representations for questions and passages1064   fh ; h ; : : : ; h ; h ; : : : ; h ; h ; : : : g. The   training objective for each question sample qof   original DPR is shown in Eq ( 1 ):   L(q ; gp; ; gp ; np; ; np ) =    loge ( )   Pe()+Pe ( )   ( 1 )   Thesim()could be any similarity operator that   calculates the similarity between the question   representation hand the passage representation   h.   Minimizing the objective in Eq ( 1 ) is the same   as ( i ) maximizing the similarity between each h   andhpair , and ( ii ) minimizing the similarity   between hand all other h(i6 = j ) and h.   As discussed previously , this training paradigm will   cause conÔ¨Çicts under current contrastive learning   framework due to ( i ) Transitivity of Similarity , and   ( ii ) Multiple References in Large Batch Size .   3.2 Dense Contextual Sentence   Representation   The cause of the Contrastive ConÔ¨Çicts lies in one-   to - many problem , that most of the passages are   often organized by multiple sentences , while these   sentences may not always stick to the same topic ,   as depicted in Figure 1 . Therefore , we propose to   model passage retrieval in a smaller granularity , i.e.   contextual sentences , to alleviate the occurrence of   one - to - many problem .   Since contextual information is also important in   passage retrieval , simply breaking down passages   into sentences and encoding them independently   is infeasible . Instead , following ( Beltagy et al . ,2020 ; Lee et al . , 2020 ; Wu et al . , 2021 ) , we insert a   special < sent > token at the sentence boundaries in   each passage , and encode the passage as a whole to   preserve the contextual information , which results   in the following format of input for each passage :   [ CLS ] < sent > sent < sent > sent ... [ SEP ]   We then use BERT ( Devlin et al . , 2019 )   as encoder to get the contextual sentence   representations by these indicator < sent > tokens .   For convenience of illustration , taking a give query   qinto consideration , we denote the corresponding   positive passage in the training batch as p , which   consists of several sentences :   P = fp ; p ; : : :p ; : : :p ; pg   Similarly , we denote the corresponding BM25   negative passage as :   N = fn ; n ; : : : n ; : : : n ; ng   Here ( )means whether the sentence or   passage contains the gold answer . We reÔ¨Åne the   original contrastive learning framework by creating   sentence - aware positive and negative samples . The   whole training pipeline is shown in the left part of   Figure 3 .   3.2.1 Positives and Easy Negatives   Following Karpukhin et al . ( 2020 ) , we use BM25   to retrieve hard negative passages for each question .   To build a contrastive learning framework based on   contextual sentences , we consider the sentence that   contains the gold answer as the positive sentence   ( i.e.p ) , and randomly sample several negative   sentences ( random sentences from N ) from a1065BM25 random negative passage . Also , following   ( Karpukhin et al . , 2020 ; Lee et al . , 2021 ) , we   introduce in - batch negatives as additional easy   negatives .   3.2.2 In - Passage Negatives   To handle the circumstance where multiple   semantically faraway questions may be derived   from one single passage , we hope to encourage the   passage encoder to generate contextual sentence   representations as diverse as possible for sentences   in the same passage . Noticing that not all   the sentences in the passage contain the gold   answer and stick to the topic related to the given   query , we further introduce in - passage negatives   to maximize the difference between contextual   sentences representations within the same passage .   Concretely , we randomly sample one sentence that   does not contain the gold answer ( i.e. a random   sentence fromP = fPg ) . Note that a positive   passage might not contain such sentence . If it   does not exist , this in - passage negative sentence   is substituted by another easy negative sentence   from the corresponding BM25 negative passage   ( a random sentence from N ) . These in - passage   negatives function as hard negative samples in our   contrastive learning framework .   3.3 Retrieval   For retrieval , we Ô¨Årst use FAISS ( Johnson et al . ,   2019 ) to calculate the matching scores between the   question and all the contextual sentence indexes .   As one passage has multiple keys in the indexes ,   we retrieve top 100k(kis the average number   of sentences per passage ) contextual sentences for   inference . To change these sentence - level scores   into passage - level ones , we adopt a probabilistic   design for ranking passages , which we refer to as   Score Normalization .   Score Normalization After getting the scores   for each contextual sentences to each question   by FAISS , we Ô¨Årst use a Softmax operation   to normalize all these similarity scores into   probabilities . Suppose one passage Pwith several   sentences s ; s ; : : : ; s , and denote the probability   for each sentence that contains the answer as   p ; p ; : : : ; p , we can calculate the probability   thatthe answer is in passage Pby Equation 2 .   HasAns ( P ) = 1 Y(1 p ) ( 2 )   We then re - rank all the retrieved passages by   HasAns ( P ) , and select the top 100 passages for   evaluation in our following experiments .   4 Experiments   4.1 Datasets   OpenQA Dataset OpenQA ( Lee et al . , 2019 )   collects over 21 million 100 - token passages from   Wikipedia to simulate the open - domain passage   corpus . OpenQA also collects question - answer   pairs from existing datasets , including SQuAD   ( Rajpurkar et al . , 2016 ) , TriviaQA ( Joshi et al . ,   2017 ) , Natural Questions ( Kwiatkowski et al . ,   2019 ) , WebQuestions ( Berant et al . , 2013 ) and   TREC ( Baudi≈° and ≈†ediv ` y , 2015 ) .   We experiment our proposed method on SQuAD ,   TriviaQA and NQ . For the previously concerned   Contrastive ConÔ¨Çicts problem , we also analyze the   existence frequency of the conÔ¨Çicting phenomenon   for each dataset . We count the number of   questions for each passage , i.e , the times that this   passage is referred to as the positive sample . The   corresponding results are shown in Table 1 . From   this table , we can see that of all three datasets we   choose , SQuAD is most severely affected by the   Contrastive ConÔ¨Çicts problem , that many passages   occur multiple times as the positive passages for   different questions . These statistics are consistent   with the fact that DPR performs the worst on   SQuAD , while acceptable on Trivia and NQ .   4.2 Training and Implementation Details   Hyperparameters In our main experiments ,   we follow the hyperparameter setting in DPR   ( Karpukhin et al . , 2020 ) to acquire comparable   performance , i.e. an initial learning rate of 2e-5 for   40 epochs on each dataset . We use 8 Tesla V100   GPUs to train the Bi - Encoder with a batch size of   16 on each GPU .   Extra Cost Although we are modeling passage   retrieval in a totally different granularity , our   method adds little extra computation overhead1066   compared to DPR . For model complexity , our   proposed method adopts exactly the same model   structure as DPR does , meaning that there are no   additional parameters introduced . For training   time , the negative sentences in our method are   randomly sampled from the negative passage in   DPR . Therefore , the extra time burden brought   by our method is only caused by the sampling   procedure , which is negligible .   Training Settings To have a comprehensive   comparison with DPR , we train DCSR under three   different settings . ( i ) Single , where each dataset is   both trained and evaluated under their own domain .   ( ii)Multi , where we use a combination of the NQ ,   Trivia and SQuAD datasets to train a universal Bi-   Encoder , and evaluate its performance on the test   sets of all three datasets . ( iii ) Adversarial Training ,   which is a simple negative sampling strategy . We   Ô¨Årst use the original dataset to train a DPR or   DCSR checkpoint , and use such checkpoint to   acquire semantically hard negative passages from   the whole Wikipedia corpus .   4.3 Main Results on Passage Retrieval   Table 2 shows our main results on OpenQA .   For the Single setting , ( i ) Consistent with the core   aim of this paper that our proposed sentence - aware   contrastive learning solves Contrastive ConÔ¨Çicts ,   DCSR achieves signiÔ¨Åcantly better results than   DPR especially on the dataset that is severely   affected by Contrastive ConÔ¨Çicts . For example ,   on the SQuAD dataset , our method achieves 10.9 %   performance gain on the Top-20 metric , and 7.1 %   performance gain on the Top-100 metric . ( ii )   For datasets that are less affected by Contrastive   ConÔ¨Çicts , like NQ and Trivia , we still achieve slight   performance gain on all metrics .   For the Multi setting , DPR on Trivia and   SQuAD suffers from a signiÔ¨Åcant performance   drop compared to Single setting , while our model is   only slightly affected . It indicates that our proposed   sentence - aware contrastive learning not only solves   theContrastive ConÔ¨Çicts , but also captures the   universality of datasets from different domains .   4.4 Incorporated with Negative Sampling   Different from other frontier researches which   mainly devote themselves either to investigating   better negative sampling strategies , like ANCE   ( Xiong et al . , 2020 ) , NPRINC ( Lu et al . , 2020 ) ,   etc . , or to extra pretraining ( Sachan et al . , 2021 ) ,   or to distilling knowledge from cross - encoders   ( Izacard and Grave , 2021 ; Yang et al . , 2021 ) , our   proposed method directly optimizes the modeling   granularity in DPR . Therefore , our method could   be naturally incorporated with these researches and   achieve better results further . Due to computational   resource limitation , we do not intend to replicate   all these methods , but use adversarial training as   an example . Following ANCE ( Xiong et al . , 2020),1067   we conduct experiments on NQ and Trivia to show   the compatibility of our method , listed in Table 3 .   With such a simple negative sampling strategy , our   DCSR achieves comparable results with its DPR   counterpart .   4.5 Ablation Study   To illustrate the efÔ¨Åcacy of the previously proposed   negative sampling strategy , we conduct an ablation   study on a subset of OpenQA Wikipedia corpus .   We sample 1/20 of the whole corpus , which   results in a collection of 1.05 million passages   in total . As reference , we reproduce DPR and   also list their results in Table 4 . We compare   the following negative sampling strategies of our   proposed method .   + 1 BM25 random In this setting , we randomly   sample ( i ) one gold sentence from the positive   passage as the positive sample , and ( ii ) one negative   sentence from the negative passage as the negative   sample per question .   + 2 BM25 random In this setting , we randomly   sample ( i ) one gold sentence from the positive   passage as the positive sample , and ( ii ) two   negative sentences from two different negative   passages as two negative samples per question .   + 1 in - passage & + 1 BM25 random In this   setting , we randomly sample ( i ) one gold sentence   from the positive passage as the positive sample ,   ( ii ) one negative sentence from the positive passage   as the Ô¨Årst negative sample , and ( iii ) one negative   sentence from the negative passage as the second   negative sample per question .   Ablations of Negative Sampling Strategy The   results are shown in Table 4 . ( i ) Under the   circumstance where only 1.05 million passages   are indexed , variants of our DCSR generally   perform signiÔ¨Åcantly better than DPR baseline ,   especially on NQ dataset ( over 1 % improvement   on both Top-20 and Top-100 ) and SQuAD   dataset ( 8.0 % improvement on Top-20 and 4.9 %   improvement on Top-100 ) , which veriÔ¨Åes the   effectiveness of solving Contrastive ConÔ¨Çicts . ( ii )   Further , we found that increasing the number of   negative samples helps little , but even introduces   slight performance degradation on several metrics .   ( iii ) The in - passage negative sampling strategy   consistently helps in boosting the performance   of nearly all datasets on all metrics , especially   on the SQuAD dataset , which is consistent with   our motivation for in - passage negatives , which is   to encourage a diverse generation of contextual   sentence representations within the same passage   in solving the one - to - many problem .   Ablations of Training Data The results are shown   in Table 5 . ( i ) We Ô¨Årst directly use the augmented   adversarial training dataset provided by DPR   ( marked as DPR - hard ) and train our DCSR , having   achieved even better results on the NQ dataset . This   augmented dataset is sub - optimal for our model , as   these hard negative samples are passage - speciÔ¨Åc ,   while our model prefers sentence - speciÔ¨Åc ones . ( ii )   We then use our previous best DCSR checkpoint   to retrieve a set of sentence - speciÔ¨Åc hard negatives   ( marked as DCSR - hard ) and train a new DCSR ,   which achieves further performance gain on both   metrics on NQ dataset.1068   5 Discussion   In this section , we discuss the transferability   difference and the inÔ¨Çuence of Wikipedia corpus   size on both DPR and our DCSR . More discussions   from different aspects are presented in the   Appendices , including ( i ) Validation accuracy on   dev sets in Appendix A , which is also a strong   evidence of alleviating Contrastive ConÔ¨Çicts . ( ii )   Error analysis for SQuAD in Appendix B , which   further shows the generalization ability of our   method . ( iii ) Case study in Appendix C , which   discusses the future improvement of DCSR .   5.1 Transferability   To further verify that our learned DCSR is more   suitable in Open - Domain Passage Retrieval , espe-   cially under the Contrastive ConÔ¨Çicts circumstance ,   we conduct experiments to test the transferability   between DPR and our DCSR . Similarly , instead of   running such experiments on the entire Wikipedia   corpus , we sample 1/20 of the corpus , which results   in a collection of 1.05 million passages in total .   We test the transferability result from SQuAD to   Trivia and from NQ to Trivia , as compared to Trivia ,   both SQuAD and NQ suffer more from Contrastive   ConÔ¨Çicts . The results are shown in Table 6 .   From Table 6 , when compared to DPR , our   model enjoys signiÔ¨Åcantly better transferability . In   both scenarios , DPR shows over 2 % performance   gap in all metrics of the transferability tests ,   indicating that our method performs much better   in generalization across the datasets . This   phenomenon once again conÔ¨Årms our theorem , that   by modeling passage retrieval in the granularity of   contextual sentences , our DCSR well models the   universality across the datasets , and shows much   better transferability than DPR .   5.2 Corpus Size   In our extensive experiments , we further found out   that our method can achieve overwhelming better   performance than DPR on smaller corpus . In this   experiment , we take the Ô¨Årst 0.1 million , the Ô¨Årst   1.05 million andall passages from the original   Wikipedia corpus , and conduct dense retrieval on   these three corpora varied in size . The statistic   results are shown in Table 7 .   From Table 7 , Ô¨Årst of all , our model achieves   better performance than DPR in all settings , where   such improvement is more signiÔ¨Åcant in smaller   corpus . On the setting where only 0.1 million   passages are indexed in the corpus , our model   achieves over 2.0 % exact improvement on all   metrics on both NQ and Trivia . We speculate this   is because of the following two strengths of our   method .   The alleviation of Contrastive ConÔ¨Çicts , which   we have analyzed previously .   Modeling passage retrieval using contextual   sentences enables a diverse generation of indexes .   Some sentences may not be the core aim of their   corresponding passages , but can still be the clue   for some questions .   Secondly , we can discover that the performance   gap between DPR and DCSR is decreasing when   the size of Wikipedia corpus increases . This is   because with the expansion of indexing corpus,1069many questions that can not be solved in the   small corpus setting may Ô¨Ånd much more closely   related passages in the large corpus setting , which   gradually neutralizes the positive effect brought   by the second strength of our proposed method   discussed above . Still , our model achieves better   performance under the full Wikipedia setting on all   datasets and all metrics .   6 Conclusion   In this paper , we make a thorough analysis on the   Contrastive ConÔ¨Çicts issue in the current open-   domain passage retrieval . To well address the   issue , we propose an enhanced sentence - aware   conÔ¨Çict learning method by carefully generating   sentence - aware positive and negative samples .   We show that the dense contextual sentence   representation learned from our proposed method   achieves signiÔ¨Åcant performance gain compared   to the original baseline , especially on datasets   with severe conÔ¨Çicts . Extensive experiments   show that our proposed method also enjoys better   transferability , and well captures the universality in   different datasets .   References107010711072A Validation Accuracy   One may argue that the improvement of DCSR   might be due to the expansion of indexing corpus   ( which we have discussed in previous sections ) ,   but not the alleviation of Contrastive ConÔ¨Çicts . In   this section , we present the validation accuracy   comparison during the training process between   DPR and our DCSR , which is a strong evidence   that DCSR well handles the problem of Contrastive   ConÔ¨Çicts .   Under 8 V100 GPUs with a batch size of 16 on   each GPU , the validation process could be viewed   as a tiny retrieval process for both DPR and DCSR .   To maintain a similar validation environment for   fair comparison , we use the +1 BM25 random   version of DCSR , which results in 8 * 16=128   questions and 2 * 8 * 16=256 contextual sentences   in one batch . Therefore , the validation process   could be interpreted as retrieving the most relevant   contextual sentence for each question in a corpus   of 256 sentences . Under such a validation task , the   size of the indexing corpus is restricted to the same   for both DPR and DCSR .   The result is shown in Figure 4 . For both Trivia   and NQ , DCSR performs consistently better than   DPR with a small accuracy margin . On SQuAD ,   especially , our DCSR can achieve higher validation   accuracy than DPR with only one single epoch ,   and achieves nearly 20 % Ô¨Ånal validation accuracy   improvement . This phenomenon further veriÔ¨Åes   that improvement of DCSR is also achieved by   improving the training strategy which alleviates   Contrastive ConÔ¨Çicts , but not only the expansion   of the indexing corpus .   B Error Analysis for SQuAD   Although achieving overwhelmingly better per-   formance on SQuAD than DPR , our DCSR on   SQuAD still lags far behind its counterparts on NQ   or Trivia . Interestingly , we found that the results on   SQuAD dev sets are pretty good and comparable   to the results on NQ or Trivia . The results of both   DPR and DCSR on dev set and test set performance   are shown in Table 8 .   By analyzing the training instances , we observe   that there exists a severe distribution bias problem   in SQuAD : SQuAD - dev and SQuAD - train share a   great number of positive passages . In fact , almost   all positive passages in the SQuAD - dev could also   be found in SQuAD - train . Of all 7921 questions   that have at least one positive passage containing   the answer in SQuAD - dev , 7624 ( 96.25 % ) of these   passages ‚Äô titles could be found in the positive   passages of SQuAD - train . More surprisingly , 6973   ( 88.03 % ) of these passages are shared between   SQuAD - train and SQuAD - dev . However , this   feature is exactly what SQuAD - test does not have ,   resulting in relatively poor performance . But again ,   this phenomenon reveals another strength of our   DCSR , that it enjoys better generalization ability   than DPR , thus is more robust in practical use .   C Case Study   To analyze the retrieval performance difference   between DPR and DCSR , we especially focus   on the different Top 1 predictions on SQuAD .   We count the number of winning times for each   baseline , where DCSR signiÔ¨Åcantly outperforms   DPR ( 893 vs. 161 ) , shown in Figure 5 .   C.1 DCSR winning cases   On the question Who was the NFL Commissioner   in early 2012 ? , the strengths of our DCSR are listed   as follows .   Capability of utilizing contextual informa-   tion . The key phrase 2012 andNFL is faraway   from Commisioner Roger Goodell , while our   DCSR is still capable of capturing such distant   contextual information .   Locating the exact sentence of the answer .   This is an obvious feature of DCSR , as we   are modeling on the granularity of contextual   sentences .   On the contrary , due to Contrastive ConÔ¨Çicts ,   the question encoder of DPR is severely affected   that it can not generate Ô¨Åne - grained question   representation . Therefore , on this question , DPR   can only Ô¨Ånd out one key phrase commissioner ,   falling into a totally wrong prediction.1073   C.2 DCSR losing cases   On the question Super Bowl 50 decided the NFL   champion for what season ? , our DCSR has already   found a contextual sentence that is very close to the   given question , with several key phrases detected .   However , this contextual sentence is actually a low-   quality index , as it suddenly reaches the end of   the passage . This is caused by the brute force   segmentation strategy of OpenQA , which focuses   on the passage level and restricts the length of each   passage to 100 . In this paper , we perform sentence   split directly on these broken passages , which as a   result breaks down many sentences into low - quality   indexes , affecting the Ô¨Ånal retrieval performance .   We do not intend to reÔ¨Åne the splition strategy to   have a fair comparison with DPR , and leave it forfuture investigation.1074