  Byung - Doh Oh   Department of Linguistics   The Ohio State University   oh.531@osu.eduWilliam Schuler   Department of Linguistics   The Ohio State University   schuler.77@osu.edu   Abstract   Transformer - based large language models are   trained to make predictions about the next word   by aggregating representations of previous to-   kens through their self - attention mechanism . In   the field of cognitive modeling , such attention   patterns have recently been interpreted as em-   bodying the process of cue - based retrieval , in   which attention over multiple targets is taken   to generate interference and latency during re-   trieval . Under this framework , this work first   defines an entropy - based predictor that quan-   tifies the diffuseness of self - attention , as well   as distance - based predictors that capture the   incremental change in attention patterns across   timesteps . Moreover , following recent studies   that question the informativeness of attention   weights , we also experiment with alternative   methods for incorporating vector norms into   attention weights . Regression experiments us-   ing predictors calculated from the GPT-2 lan-   guage model show that these predictors deliver   a substantially better fit to held - out self - paced   reading and eye - tracking data over a rigorous   baseline including GPT-2 surprisal . Addition-   ally , the distance - based predictors generally   demonstrated higher predictive power , with ef-   fect sizes of up to 6.59 ms per standard devia-   tion on self - paced reading times ( compared to   2.82 ms for surprisal ) and 1.05 ms per standard   deviation on eye - gaze durations ( compared to   3.81 ms for surprisal ) .   1 Introduction   Much work in broad - coverage sentence processing   has focused on studying the role of expectation op-   erationalized in the form of surprisal ( Hale , 2001 ;   Levy , 2008 ) using language models ( LMs ) to de-   fine a conditional probability distribution of a word   given its context ( Smith and Levy , 2013 ; Goodkind   and Bicknell , 2018 ) . Recently , this has included   Transformer - based language models ( Wilcox et al . ,   2020 ; Merkx and Frank , 2021 ; Oh et al . , 2022).Figure 1 : The predictors and attention weight formu-   lations examined in this work . The entropy - based pre-   dictor ( i.e. NAE ) quantifies the diffuseness of atten-   tion over previous tokens at a given timestep , while the   distance - based predictors ( i.e. ∆NAE , MD , EMD ) cap-   ture the change in attention patterns across consecutive   timesteps ( top row : weights at ‘ journey , ’ bottom row :   weights at ‘ to ’ ) . These predictors can be calculated from   attention weights formulated using different methods   ( i.e. A - W , A - N , ARL - N ) .   However , expectation - based accounts have em-   pirical shortcomings , such as being unable to fully   account for garden - path effects ( van Schijndel and   Linzen , 2021 ) or predict the timing of delays in   certain constructions ( Levy et al . , 2013 ) . For this   reason , some research has begun to focus on the   effects of memory and attention using predictors   calculated from language model representations .   For example , Ryu and Lewis ( 2021 ) recently drew   connections between the self - attention patterns of   Transformers ( Vaswani et al . , 2017 ) and cue - based   retrieval models of sentence comprehension ( e.g.   Lewis et al . , 2006 ) . Their attention entropy , which   quantifies the diffuseness of the attention weights   over previous tokens , showed patterns that are con-   sistent with similarity - based interference observed   during the processing of subject - verb agreement .   However , these results relied on identifying one at-   tention head specialized for the nsubj dependency ,   and an aggregated version of this predictor was not   very strong in predicting naturalistic reading times   in the presence of a surprisal predictor ( Ryu and   Lewis , 2022 ) .   This work therefore defines and evaluates several9324memory- and attention - based predictors derived   from the self - attention patterns of the Transformer-   based GPT-2 language model ( Radford et al . , 2019 )   on two naturalistic datasets , in the presence of a   strong GPT-2 surprisal baseline . First , normalized   attention entropy expands upon Ryu and Lewis ’s   ( 2021 ) attention entropy by re - normalizing the at-   tention weights and controlling for the number of   tokens in the previous context . Additionally , three   distance - based predictors that quantify the shift in   attention patterns across consecutive timesteps are   presented , based on the idea that the reallocation   of attentional focus entails processing difficulty .   Moreover , motivated by work on interpreting   large language models that question the connec-   tion between attention weights and model predic-   tions ( e.g. Jain and Wallace , 2019 ) , the norm - based   analysis of the transformed vectors ( Kobayashi   et al . , 2020 , 2021 ) is newly applied to GPT-2 in   this work to inform alternative formulations of   attention weights . For example , it has been ob-   served that while large language models tend to   place high attention weights on special tokens   ( e.g. < |endoftext| > of GPT-2 ) , these tokens con-   tribute very little to final model predictions as their   ‘ value ’ vectors have near - zero norms ( Kobayashi   et al . , 2020 ) . Attention weight formulations that   incorporate the norms of the transformed vectors   should therefore alleviate the over - representation of   such special tokens and represent the contribution   of each token more accurately .   Results from regression analyses using these pre-   dictors show significant and substantial effects in   predicting self - paced reading times and eye - gaze   durations during naturalistic reading , even in the   presence of a robust surprisal predictor . Addition-   ally , alternative formulations of attention weights   that incorporate the norms of the transformed vec-   tors are shown to further improve the predictive   power of these predictors .   2 Background   This section provides a mathematical definition of   the self - attention mechanism underlying the GPT-2   language model and describes alternative norm-   based formulations of attention weights .   2.1 Masked Self - Attention of GPT-2   Language Models   GPT-2 language models ( Radford et al . , 2019 ) use   a variant of a multi - layer Transformer decoder pro - posed in Vaswani et al . ( 2017 ) . Each decoder layer   consists of a masked self - attention block and a feed-   forward neural network :   x = FF(LN(o+x))+(o+x),(1 )   where x∈Ris the ith input representation at   layer l , FFis a two - layer feedforward neural net-   work , LNis a vector - wise layer normalization   operation , and ois the output representation from   the multi - head self - attention mechanism , in which   Hmultiple heads simultaneously mix representa-   tions from the previous context . This output o   can be decomposed into the sum of representations   resulting from each attention head h :   o=/summationdisplayV / bracketleftbiggX   1 / bracketrightbigg   a , ( 2 )   where X= [ x , ... , x]∈Ris the se-   quence of layer - normalized input representations   leading up to xfrom the previous context , and   x = LN(x)is the layer - normalized ver-   sion of x. Vrepresents the head - specific value-   output transformation , anda∈Ris the vec-   tor of attention weights :   a = SM / parenleftbigg / parenleftbig   K / bracketleftbiggX   1 / bracketrightbigg / parenrightbigQ / bracketleftbiggx   1 / bracketrightbigg   √d / parenrightbigg   ,   ( 3 )   where QandKrepresent the head - specific   query and key transformations respectively , and   d = d / H is the dimension of each attention head .   LNis a vector - wise layer normalization op-   eration ( Ba et al . , 2016 ) that first standardizes   the vector and subsequently conducts element-   wise transformations using learnable parameters   c , b∈R :   LN(y ) = y−m(y )   s(y)⊙c+b , ( 4 )   where α∈ { in , out},m(y)ands(y)denote the   elementwise mean and standard deviation respec-   tively , and ⊙denotes a Hadamard product.9325   2.2 Weight- and Norm - Based Analysis of   Previous Context   Previous work that has studied the inner mecha-   nism of Transformer models has focused on ana-   lyzing the relative contribution of each token to its   final prediction . As a measure that quantifies the   ‘ strength ’ of contribution , attention weights from   the self - attention mechanism have been most com-   monly used . Similarly to recent work in cognitive   modeling ( e.g. Ryu and Lewis , 2021 ) , this work   also evaluates predictors calculated from attention   weights ( A - W ) .   a = a ( 5 )   While analysis using attention weights is com-   mon , the assumption that attention weights alone   reflect the contribution of each token disregards   the magnitudes of the transformed input vectors ,   as pointed out by Kobayashi et al . ( 2020 ) . As an   alternative , they proposed a norm - based analysis of   the self - attention mechanism , which quantifies the   contribution of each token as the norm of the trans-   formed vector multiplied by the attention weight .   In this work , in order to quantify the relative con-   tribution of each token in the previous context , the norms of the transformed vectors are normal-   ized across the sequence , resulting in ‘ norm - aware ’   weights that are comparable to attention weights   ( A - N ) .   a=(6 )   More recently , Kobayashi et al . ( 2021 ) showed   that the residual connection and the layer normal-   ization operation ( RLN ; RL ) that follow the   self - attention mechanism can also be conducted   before aggregating representations over token po-   sitions . Motivated by this observation , the vector   norms that take into consideration these subsequent   operations are also examined in this work . Simi-   larly to A - N , the norms are normalized across   the sequence to yield weights that are comparable   ( ARL - N ):   a= ,   ( 7 )   where g(·)incorporates the residual connection   ( + x ) and layer normalization ( LN ) of Eq . 1 .   Following the assumption that the residual con-   nection serves to ‘ preserve ’ the representation at   position i(Kobayashi et al . , 2021 ) and that it is   distributed equally across heads , xis added to   the representation at position iof each head after   dividing it by the number of heads :   g(Y)= ,   ( 8)   where δis a Kronecker delta vector consisting of a   one at element jand zeros elsewhere , and cand   brefer to the learnable parameters of LN .   3 Entropy- and Distance - Based   Predictors From Attention Patterns   Given the different formulations of self - attention   weights , entropy - based predictors that quantify the9326diffuseness of self - attention and distance - based pre-   dictors that capture the incremental change in atten-   tion patterns across timesteps can be defined . The   first predictor defined in this work is normalized   attention entropy ( NAE ):   NAE = , ( 9 )   where π∈ { W , N , RL - N } . This is similar to the at-   tention entropy proposed by Ryu and Lewis ( 2021 )   as a measure of interference in cue - based recall   attributable to uncertainty about the target , with   two notable differences . First , NAE controls for   the number of tokens in the previous context by   normalizing the entropy by the maximum entropy   that can be achieved at timestep i. Furthermore ,   NAE also uses weights over x , ... , xthat   have been re - normalized to sum to 1 , thereby ad-   hering closer to the definition of entropy , in which   the mass of interest sums to 1 .   In addition to NAE , distance - based predictors   are defined for capturing effortful change in at-   tention patterns across timesteps . However , as it   currently remains theoretically unclear how this   distance should be defined , this exploratory work   sought to provide empirical results for different   distance functions . The first is ∆NAE , which quan-   tifies the change in diffuseness across timesteps :   ∆NAE = |NAE−NAE|(10 )   As with NAE , this predictor is insensitive to how   the attention weights are reallocated between to-   kens in the previous context to the extent that the   overall diffuseness remains unchanged .   The second distance - based predictor is Manhat-   tan distance ( MD ) .   MD = ||a−a|| ( 11 )   MD directly measures the magnitude of change   in attention weights over all tokens at timestep i.   MD is less sensitive to the linear distance between   tokens and therefore makes it consistent with the   predictions of McElree et al . ( 2003 ) , who found   that processing speed was not influenced by theamount of intervening linguistic material in the   formation of a dependency .   Finally , the Earth Mover ’s Distance ( EMD ; Rub-   ner et al . , 2000 ) is applied to quantify the shift in at-   tention weights . EMD is derived from a solution to   the Monge - Kantorovich problem ( Rachev , 1985 ) ,   which aims to minimize the amount of “ work ” nec-   essary to transform one histogram into another .   Formally , let P={(p , w ) , ... , ( p , w)}be   the first histogram with mbins , where prepre-   sents the bin and wrepresents the weight of the   bin;Q={(q , w ) , ... , ( q , w)}the second his-   togram with nbins ; and D= [ d]the distance   matrix where dis the ground distance between   binspandq . The problem is to find an optimal   flowF= [ f ] , where frepresents the flow be-   tween pandq , that minimizes the overall work .   W(P , Q , F ) = /summationdisplay / summationdisplaydf ( 12 )   Once the optimal flow is found , the EMD is   defined as the work normalized by the total flow .   EMD ( P , Q ) = /summationtext / summationtextdf / summationtext / summationtextf(13 )   To quantify the minimum amount of work neces-   sary to ‘ transform ’ the attention weights , the EMD   between attention weights at consecutive timesteps   is calculated using EMD ( a , a ) . The   ground distance is defined as d = in order   to control for the number of tokens in the previous   context . EMD can be interpreted as being consis-   tent with Dependency Locality Theory ( Gibson ,   2000 ) in that reallocating attention weights to to-   kens further away in the context incurs more cost   than reallocating weights to closer tokens .   Code for calculating all of the predictors from   GPT-2 under the different attention weight formu-   lations is publicly available at https://github .   com / byungdoh / attn_dist .   4Experiment 1 : Evaluation of Predictors   on Human Reading Times   In order to evaluate the contribution of the entropy-   and distance - based predictors , regression models   containing commonly used baseline predictors , sur-   prisal predictors , and one predictor of interest were9327fitted to self - paced reading times and eye - gaze du-   rations collected during naturalistic language pro-   cessing . In this work , we adopt a statistical proce-   dure that directly models temporal diffusion ( i.e. a   lingering reponse to stimuli ) by estimating con-   tinuous impulse response functions and controls   for overfitting by assessing the external validity of   these predictors through a non - parametric test on   held - out data .   4.1 Response Data   The first experiment described in this paper used   the Natural Stories Corpus ( Futrell et al . , 2021 ) ,   which contains self - paced reading times from 181   subjects that read 10 naturalistic stories consisting   of 10,245 words . The data were filtered to exclude   observations for sentence - initial and sentence - final   words , observations from subjects who answered   fewer than four comprehension questions correctly ,   and observations with durations shorter than 100   ms or longer than 3000 ms . This resulted in a   total of 770,102 observations , which were subse-   quently partitioned into a fit partition of 384,905   observations , an exploratory partition of 192,772   observations , and a held - out partition of 192,425   observations . The partitioning allows model selec-   tion ( e.g. making decisions about baseline predic-   tors and random effects structure ) to be conducted   on the exploratory partition and a single hypothe-   sis test to be conducted on the held - out partition ,   thus obviating the need for multiple trials correc-   tion . All observations were log - transformed prior   to regression modeling .   Additionally , the set of go - past durations from   the Dundee Corpus ( Kennedy et al . , 2003 ) also pro-   vided the response variable for regression modeling .   The Dundee Corpus contains eye - gaze durations   from 10 subjects that read 67 newspaper editori-   als consisting of 51,501 words . The data were   filtered to remove unfixated words , words follow-   ing saccades longer than four words , and words at   sentence- , screen- , document- , and line - starts and   ends . This resulted in a total of 195,507 observa-   tions , which were subsequently partitioned into a   fit partition of 98,115 observations , an exploratory   partition of 48,598 observations , and a held - out   partition of 48,794 observations . All observations   were log - transformed before model fitting.4.2 Predictors   For each dataset , a set of baseline predictors that   capture basic , low - level cognitive processing were   included in all regression models .   •Self - paced reading times ( Futrell et al . , 2021 ):   word length measured in characters , index of   word position within each sentence ;   •Eye - gaze durations ( Kennedy et al . , 2003 ): word   length measured in characters , index of word   position within each sentence , saccade length ,   whether or not the previous word was fixated .   In addition to the baseline predictors , two surprisal   predictors were also included in all regression mod-   els evaluated in this experiment . The first is un-   igram surprisal as a measure of word frequency ,   which was calculated using the KenLM toolkit   ( Heafield et al . , 2013 ) with parameters estimated on   the English Gigaword Corpus ( Parker et al . , 2009 ) .   The second is surprisal from GPT-2 Small ( Radford   et al . , 2019 ) , which is trained on ∼8B tokens of the   WebText dataset . Surprisal from the smallest GPT-   2 model was chosen because it has been shown to   be the most predictive of self - paced reading times   and eye - gaze durations among surprisal from all   variants of GPT-2 ( Oh et al . , 2022 ) .   Finally , the entropy- and distance - based pre-   dictors defined in Section 3 were calculated   from the attention patterns ( i.e. a where   π∈ { W , N , RL - N } ) of heads on the topmost   layer of GPT-2 Small . Contrary to previous stud-   ies that analyzed the attention patterns of all layers ,   this work focuses on analyzing those of the topmost   layer out of the concern that the attention patterns   of lower layers are less interpretable to the extent   that they perform intermediate computations for   the upper layers . Since the topmost layer generates   the representation that is used for model prediction ,   the attention patterns from this layer are assumed   to reflect the contribution of each previous token   most directly . Subsequently , the by - head predictors   were aggregated across heads to calculate by - word   predictors . This assumes that each attention head   contributes equally to model prediction , and is also   consistent with the formulation of multi - head self-   attention in Eq . 2 .   To calculate surprisal as well as the entropy- and   distance - based predictors , each story of the Natu-   ral Stories Corpus and each article of the Dundee   Corpus was tokenized according GPT-2 ’s byte - pair   encoding ( BPE ; Sennrich et al . , 2016 ) tokenizer9328   and was provided as input to the GPT-2 model .   In cases where each story / article did not fit into   a single context window , the second half of the   previous context window served as the first half of   a new context window to calculate predictors for   the remaining tokens . Additionally , when a single   word wwas tokenized into multiple subword to-   kens , negative log probabilities of subword tokens   corresponding to wwere added together to cal-   culate S(w ) = −logP(w|w ) . Similarly ,   the entropy- and distance - based predictors for such   subword tokens were also added together .   4.3 Procedures   To evaluate the predictive power of each predictor   of interest , a baseline regression model contain-   ing only the baseline predictors and fullregres-   sion models containing one predictor of interest   on top of the baseline regression model were first   fitted to the fit partition of each dataset . In or-   der to control for the confound of temporal diffu-   sion , continuous impulse response functions ( IRFs )   were estimated using the statistical framework of   continuous - time deconvolutional regression ( CDR ;   Shain and Schuler , 2021 ) . The predictors of interest   were centered , and all regression models included   a by - subject random intercept .   As a preliminary analysis , the predictive power   of different predictors was compared on the ex-   ploratory partition by calculating the increase in   log - likelihood ( ∆LL ) to the baseline regressionmodel as a result of including the predictor , fol-   lowing recent work ( Goodkind and Bicknell , 2018 ;   Wilcox et al . , 2020 ; Oh et al . , 2021 ) . Subsequently ,   based on the preliminary exploratory results , the   predictive power of one best entropy - based pre-   dictor and that of one best distance - based predic-   tor were evaluated on the held - out partition of   both datasets . More specifically , statistical signifi-   cance testing was conducted using a paired permu-   tation test ( Demšar , 2006 ) of the difference in by-   item squared error between the baseline regression   model and the respective full regression models .   4.4 Results   The results in Figure 3 show that across both cor-   pora , most of the entropy- and distance - based pre-   dictors make a notable contribution to regression   model fit under all attention formulations . Given   that the baseline model contains strong predictors   such as unigram surprisal and GPT-2 surprisal , this   may suggest their validity as predictors of compre-   hension difficulty . The exception to this is the EMD   predictor , which did not show an increase in likeli-   hood under A - Won the Natural Stories Corpus   and under ARL - N on the Dundee Corpus . As   EMD is sensitive to how the ground distance d   between bins is defined ( i.e. how costly it is to real-   locate attention weights across input positions ) , a   more principled definition of dmay make EMD   a more robust predictor . Across the two corpora ,   A - N+NAE andARL - N+MD appear to   be the most predictive among the entropy- and   distance - based predictors respectively.9329Additionally , the NAE and ∆NAE predictors   showed different trends across the two corpora ,   where NAE contributed to stronger model fit than   ∆NAE on the Natural Stories Corpus , while the op-   posite trend was observed on the Dundee Corpus .   In contrast to various surprisal predictors that have   shown a very similar trend in terms of predictive   power across these two corpora ( Oh et al . , 2022 ) ,   these two predictors may shed light on the sub-   tle differences between self - paced reading times   and eye - gaze durations . Finally , incorporating vec-   tor norms into attention weights ( i.e. A - Nand   ARL - N ) generally seems to improve the pre-   dictive power of these predictors , which provides   support for the informativeness of input vectors   in analyzing attention patterns ( Kobayashi et al . ,   2020 , 2021 ) .   Table 1 presents the effect sizes of A-   N+NAE andARL - N+MD on the held - out   partition of the Natural Stories Corpus and the   Dundee Corpus , which were derived by calculat-   ing how much increase in reading times the re-   gression model would predict at average predictor   value given an increase of one standard deviation .   On both datasets , ARL - N+MD appears to be   a strong predictor of reading times , which con-   tributed to significantly lower held - out errors . The   entropy - based A - N+NAE predictor showed   contrasting results across the two corpora , showing   a large effect size on the Natural Stories Corpus but   not on the Dundee Corpus . This is consistent with   the differential results between NAE and ∆NAE   on the exploratory partition of the two corpora   and may hint at differences between self - paced   reading times and eye - gaze durations . In terms   of magnitude , the two predictors showed large ef-   fect sizes on the Natural Stories Corpus , which   were more than twice that of GPT-2 surprisal . On   the Dundee Corpus , however , the effect size of   ARL - N+MD was much smaller compared to   that of GPT-2 surprisal .   5 Experiment 2 : Do NAE and MD   Independently Explain Reading Times ?   The previous experiment revealed that on the Natu-   ral Stories Corpus , the select entropy- and distance-   based predictors from the attention patterns of GPT-   2 contributed to significantly higher regressionCorpus Predictor Effect Size   Natural   Stories 6.87 ms 2.56 ms 6.59 ms 2.82 ms   Dundee N / A 4.22 ms 1.05 ms 3.81 ms   model fit . Although they showed similarly large   effect sizes on the held - out partition , the two pre-   dictors may independently explain reading times ,   as they are defined to quantify different aspects of   attention patterns . The second experiment exam-   ines this possibility following similar procedures   as the previous experiment .   5.1 Procedures   In order to determine whether the effect of one pre-   dictor subsumes that of the other , a CDR model   including both A - N+NAE and ARL-   N+MD was first fit to self - paced reading times   of the fit partition of the Natural Stories Corpus .   The CDR model followed the same specifications ,   random effects structure , and baseline predictors as   described in Experiment 1 . Subsequently , the fit of   this regression model on the held - out partition was   compared to those of the two regression models   that contain only one of the two predictors from the   previous experiment . More specifically , the ∆LL   as a result of including the predictor(s ) of interest   were calculated for each regression model , and sta-   tistical significance testing was conducted using a   paired permutation test of the difference in by - item   squared error between the new ‘ A - N+NAE   & ARL - N+MD ’ regression model and the re-   spective ‘ A - N+NAE ’ and ‘ ARL - N+MD ’   regression models , which allowed the contribution   of each predictor to be analyzed.9330   5.2 Results   The results in Figure 4 show that the regres-   sion model including both A - N+NAE and   ARL - N+MD achieves higher ∆LL on the   held - out partition of the Natural Stories Corpus   compared to regression models including only   one of these predictors . Moreover , the differ-   ence in by - item squared error between the ‘ A-   N+NAE & ARL - N+MD ’ regression model   and the ‘ ARL - N+MD ’ regression model was   statistically significant ( p < 0.001 ) . In con-   trast , the significance between the ‘ A - N+NAE   & ARL - N+MD ’ regression model and the   ‘ A - N+NAE ’ regression model was not sta-   tistically significant ( p= 0.236 ) . This indicates   that the entropy - based A - N+NAE contributes   to model fit over and above the distance - based   ARL - N+MD and also subsumes its effect in   predicting reading times .   6 Correlation Analysis Between   Predictors and Syntactic Categories   6.1 Procedures   In order to shed more light on the predictors newly   proposed in this work , a series of correlation anal-   yses were conducted . First , Pearson correlation   coefficients were calculated between the entropy-   and distance - based predictors as well as the sur-   prisal predictors to examine the similarity between   predictors and the influence of different attention   weight formulations . Subsequently , the most pre-   dictive A - N+NAE andARL - N+MD pre-   dictors were analyzed with a focus on identifying   potential linguistic correlates . This analysis used   a version of the Natural Stories Corpus and theDundee Corpus that had been manually annotated   using a generalized categorial grammar annotation   scheme ( Shain et al . , 2018 ) .   6.2 Results   The correlation matrix in Figure 5 shows that   within the same predictor , the different attention   formulations did not make a very large difference ,   with ‘ intra - predictor ’ correlation coefficients at   above 0.85for most predictors . An exception to   this trend was EMD , which showed a correlation   coefficient of 0.74on the Natural Stories Corpus   and0.72on the Dundee Corpus between A-   W+EMD andARL - N+EMD . Such difference   is to be expected , as EMD is the most sensitive to   the change of location in attention weights among   the distance - based predictors . This is also consis-   tent with the exploratory regression results in Fig-   ure 3 , where the ∆LL of EMD predictors varied   the most as a function of different attention weight   formulations .   Additionally , the norm - based attention formula-   tions seem to bring the entropy - based NAE closer   to the distance - based predictors in terms of correla-   tion coefficients . On both corpora , A - N+NAE   andARL - N+NAE show stronger correlations   to distance - based predictors than A - W+NAE .   Interestingly , the highest correlation coefficient   between NAE and a distance - based predictor is   observed between A - N+NAE andARL-   N+MD at0.90on the Natural Stories Corpus and   0.91on the Dundee Corpus , which were the two   strongest predictors identified in Experiment 1 .   Such high correlation partially explains the results   of Experiment 2 , in which the influence of A-   N+NAE subsumed that of ARL - N+MD .   Finally , the entropy- and distance - based predic-   tors showed moderate correlation to unigram sur-   prisal at around 0.5on both corpora . With regard   to GPT-2 surprisal , these predictors showed weak   correlation at around 0.3on the Natural Stories   Corpus , and around 0.4on the Dundee Corpus . To-   gether with the regression results , this further sug-   gests that the proposed predictors capture a mecha-   nistic process that is distinct from the frequency or   predictability of the word .   An analysis of the predictors according to syntac-   tic categories showed that ARL - N+MD may   be sensitive to the transition between the subject   constituent and the predicate constituent . Its aver-   age values for nouns in different contexts presented9331   Corpus Nouns Count Mean   Natural   StoriesEnd of subj . NP 1,051 19.024   Other 3,098 16.520   DundeeEnd of subj . NP 4,496 18.982   Other 15,763 16.661   in Table 2 show that on both corpora , nouns occur-   ring at the end of subject NPs were associated with   a greater shift in attention patterns . This is con-   sistent with the intuition that transitions between   constituents entails cognitive effort during incre-   mental processing .   7 Discussion and Conclusion   This work builds on recent efforts to derive   memory- and attention - based predictors of pro-   cessing difficulty to complement expectation - based   accounts of sentence processing . Based on the   observation that the self - attention patterns of   Transformer - based language models can be inter-   preted as embodying the process of cue - based   retrieval , an entropy - based predictor that quanti-   fies the diffuseness of self - attention was first de-   fined . Moreover , based on the idea that realloca-   tion of attention may incur processing difficulty , distance - based predictors that capture the incremen-   tal change in attention patterns across timesteps   were defined . Regression results using these pre-   dictors calculated from the GPT-2 language model   showed that these entropy- and distance - based pre-   dictors deliver a substantially better fit to self - paced   reading and eye - tracking data over a strong base-   line including GPT-2 surprisal .   To our knowledge , this work is the first to re-   port robust effects of Transformer attention - based   predictors in predicting reading times of broad-   coverage naturalistic data . This provides support   for Ryu and Lewis ’s ( 2021 ) observation that the   self - attention mechanism of Transformers embod-   ies the process of cue - based retrieval , and further   suggests that representations that exhibit similarity-   based interference can be learned from the self-   supervised next - word prediction task . Additionally ,   the strength of the distance - based predictors fur-   ther demonstrates the potential to bring together   expectation- and memory - based theories of sen-   tence processing under a coherent framework .   Acknowledgments   We thank the reviewers for their helpful comments .   This work was supported by the National Science   Foundation grant # 1816891 . All views expressed   are those of the authors and do not necessarily re-   flect the views of the National Science Foundation.9332Limitations   The connection between attention patterns of   Transformer - based language models and human   sentence processing drawn in this work is based   on a model trained on English text and data from   human subjects that are native speakers of English .   Therefore , the connection made in this work may   not generalize to other languages . Additionally , al-   though the alternative formulations of self - attention   weights resulted in stronger predictors of process-   ing difficulty , they are more computationally ex-   pensive to calculate as they rely on an explicit de-   composition of the matrix multiplication operation ,   which are highly optimized in most packages .   Ethics Statement   Experiments presented in this work used datasets   from previously published research ( Futrell et al . ,   2021 ; Kennedy et al . , 2003 ) , in which the proce-   dures for data collection and validation are out-   lined . As this work focuses studying the possible   connection between attention patterns of large lan-   guage models and human sentence processing , its   potential negative impacts on society seem to be   minimal .   References9333   A CDR Implementation Details   The continuous - time deconvolutional regression   ( CDR ) models used in this work were fitted using   variational inference to estimate the means and vari-   ances of independent normal posterior distributions   over all model parameters assuming an improper   uniform prior . Convolved predictors used the three-   parameter ShiftedGamma impuse response func-   tion ( IRF ) kernel :   f(x;α , β , δ ) = β(x−δ)e   Γ(α)(14 )   Posterior means for the IRF parameters were   initialized at α= 0.2,β= 0.5 , and δ=−0.2 ,   which defines a decreasing IRF with a peak cen-   tered at t= 0that decays to near - zero within about   1 s. Models were fitted using the Adam optimizer   ( Kingma and Ba , 2015 ) with Nesterov momentum   ( Nesterov , 1983 ; Dozat , 2016 ) , a constant learning   rate of 0.001 , and minibatches of size 1,024 . For   computational efficiency , histories were truncated   at 128 timesteps . Prediction from the network used   an exponential moving average of parameter iter-   ates with a decay rate of 0.999 , and the models   were evaluated using maximum a posteriori esti-   mates obtained by setting all IRF parameters to   their posterior means .   For the baseline regression predictors , the ‘ index   of word position within each sentence ’ predictors   were scaled , and the ‘ word length in characters ’   and ‘ saccade length ’ predictors were both centered   and scaled.9334