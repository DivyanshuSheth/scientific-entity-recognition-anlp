/sort - amount - down   Max Müller - EbersteinandRob van der GootandBarbara PlankDepartment of Computer Science , IT University of Copenhagen , DenmarkCenter for Information and Language Processing ( CIS ) , LMU Munich , Germany   mamy@itu.dk , robv@itu.dk , bplank@cis.lmu.de   Abstract   Making an informed choice of pre - trained lan-   guage model ( LM ) is critical for performance ,   yet environmentally costly , and as such widely   underexplored . The field of Computer Vi-   sion has begun to tackle encoder ranking , with   promising forays into Natural Language Pro-   cessing , however they lack coverage of linguis-   tic tasks such as structured prediction . We   propose probing to rank LMs , specifically for   parsing dependencies in a given language , by   measuring the degree to which labeled trees   are recoverable from an LM ’s contextualized   embeddings . Across 46 typologically and ar-   chitecturally diverse LM - language pairs , our   probing approach predicts the best LM choice   79 % of the time using orders of magnitude less   compute than training a full parser . Within this   study , we identify and analyze one recently pro-   posed decoupled LM — RemBERT — and find it   strikingly contains less inherent dependency in-   formation , but often yields the best parser after   full fine - tuning . Without this outlier our ap-   proach identifies the best LM in 89 % of cases .   1 Introduction   With the advent of massively pre - trained language   models ( LMs ) in Natural Language Processing   ( NLP ) , it has become crucial for practitioners to   choose the best LM encoder for their given task   early on , regardless of the rest of their proposed   model architecture . The greatest variation of LMs   lies in the language or domain - specificity of the   unlabelled data used during pre - training ( with ar-   chitectures often staying identical ) .   Typically , better expressivity is expected from   language / domain - specific LMs ( Gururangan et al . ,   2020 ; Dai et al . , 2020 ) while open - domain settings   necessitate high - capacity models with access to as   much pre - training data as possible . This tradeoff is   difficult to navigate , and given that multiple special-   ized LMs ( or none at all ) are available , practitioners   often resort to an ad - hoc choice . In absence of im - mediate performance indicators , the most accurate   choice could be made by training the full model   using each LM candidate , however this is often   infeasible and wasteful ( Strubell et al . , 2019 ) .   Recently , the field of Computer Vision ( CV ) has   attempted to tackle this problem by quantifying   useful information in pre - trained image encoders   as measured directly on labeled target data without   fine - tuning ( Nguyen et al . , 2020 ; You et al . , 2021 ) .   While first forays for applying these methods to   NLP are promising , some linguistic tasks differ   substantially : Structured prediction , such as pars-   ing syntactic dependencies , is a fundamental NLP   task not covered by prior encoder ranking methods   due to its graphical output . Simultaneously , perfor-   mance prediction in NLP has so far been studied as   a function of dataset and model characteristics ( Xia   et al . , 2020 ; Ye et al . , 2021 ) and has yet to examine   how to rank large pools of pre - trained LMs .   Given the closely related field of probing , in   which lightweight models quantify task - specific   information in pre - trained LMs , we recast its ob-   jective in the context of performance prediction   and ask : How predictive is lightweight probing at   choosing the best performing LM for dependency   parsing ? To answer this question , we contribute :   •An efficient encoder ranking method for struc-   tured prediction using dependency probing   ( Müller - Eberstein et al . , 2022 ; DP ) to   quantify latent syntax ( Section 2 ) .   •Experiments across 46 typologically and ar-   chitecturally diverse LM + target language   combinations ( Section 3 ) .   •An in - depth analysis of the surprisingly low   inherent dependency information in Rem-   BERT ( Chung et al . , 2021 ) compared to its   high fine - tuned performance ( Section 4).1296   2 Methodology   Probing pre - trained LMs is highly related to en-   coder ranking in CV where the ease of recover-   ability of class - differentiating information is key   ( Nguyen et al . , 2020 ; You et al . , 2021 ) . This ap-   proach is more immediate than existing NLP per-   formance prediction methods which rely on fea-   turized representations of source and target data   without actively ranking encoders ( Xia et al . , 2020 ;   Ye et al . , 2021 ) . As most experiments in NLP are   conducted using a limited set of LMs — often a sin-   gle model — without strong prior motivations , we   see LM ranking as a critical task on its own .   While probes for LMs come in many forms , they   are generally characterized as lightweight , min-   imal architectures intended to solve a particular   task ( Hall Maudslay et al . , 2020 ) . While non - linear   models such as small multi - layer perceptrons are of-   ten used ( Tenney et al . , 2019 ) , there have been crit-   icisms given that their performance highly depends   on the complexity of their architecture ( Hewitt and   Liang , 2019 ; V oita and Titov , 2020 ) . As such ,   we rely on linear probes alone , which have the   benefit of being extremely lightweight , closely re-   sembling existing performance prediction methods   ( You et al . , 2021 ) , and allow for statements about   linear subspaces contained in LM latent spaces .   DP ( Müller - Eberstein et al . , 2022 ; vi-   sualized in Figure 1 ) is a linear formulation for   extracting fully labeled dependency trees based on   the structural probe by Hewitt and Manning ( 2019 ) .   Given contextualized embeddings of dimensional-   ityd , a linear transformation B∈Rwithb≪d   ( typically b= 128 ) maps them into a subspace in   which the Euclidean distance between embeddings   corresponds to the number of edges between the   respective words in the gold dependency graph .   In our formulation , we supplement a linear trans-   formation L∈R(with l= number of depen-   dency relations ) which maps each embedding to asubspace in which the magnitude of each dimen-   sion corresponds to the likelihood of a word and its   head being governed by a certain relation .   By computing the minimum spanning tree in B   and then finding the word with the highest root   likelihood in L , we can determine the direction-   ality of all edges as pointing away from the root .   All remaining edges are labeled according to the   most likely non - root class in L , resulting in a fully   directed and labeled dependency tree .   Note that this approach differs substantially from   prior approaches which yield undirected and/or   unlabeled trees ( Hewitt and Manning , 2019 ; Kul-   mizev et al . , 2020 ) or use pre - computed edges and   non - linear classifiers ( Tenney et al . , 2019 ) . D-   P efficiently computes the full target metric   ( i.e. labeled attachment scores ) instead of approxi-   mate alternatives ( e.g. undirected , unlabeled attach-   ment scores or tree depth correlation ) .   3 Experiments   Setup We investigate the ability of DP   to select the best performing LM for dependency   parsing across nine linguistically diverse treebanks   from Universal Dependencies ( Zeman et al . , 2021 ;   UD ) which were previously chosen by Smith et al .   ( 2018 ) to reflect diverse writing systems and mor-   phological complexity ( see Appendix A ) .   For each target language , we employ three multi-   lingual LMs — mBERT ( Devlin et al . , 2019 ) , XLM-   R ( Conneau et al . , 2020 ) , RemBERT ( Chung et al . ,   2021)—as well as 1–3 language - specific LMs re-   trieved by popularity from HuggingFace ’s Model   Hub ( Wolf et al . , 2020 ) , resulting in a total of 46   LM - target pair setups ( see Appendix C ) .   For each combination , we train a DP   to compute labeled attachment scores ( LAS ) , hy-   pothesizing that LMs from which trees are most   accurately recoverable also perform better in a fully   tuned parser . To evaluate the true downstream per-   formance of a fully - tuned model , we further train   a deep biaffine attention parser ( BAP ; Dozat and   Manning , 2017 ) on each LM - target combination .   Compared to full fine - tuning , DP only op-   timizes the matrices BandL , resulting in the ex-   traction of labeled trees with as few as 190k instead   of 583 M trainable parameters for the largest Rem-   BERT model ( details in Appendix B ) .   We measure the predictive power of probing for   fully fine - tuned model performance using the Pear-   son correlation coefficient ρas well as the weighted1297   Kendall ’s τ(Vigna , 2015 ) . The latter metric cor-   responds to a correlation coefficient in [ −1,1]and   simultaneously defines the probability of choosing   the better LM given a pair as , allowing us to   quantify the overall quality of a ranking .   Results Comparing the LAS of DP ’s   lightweight predictions against full BAP fine-   tuning in Figure 2 , we see a clear correlation as   the probe correctly predicts the difficulty of pars-   ing languages relative to each other and also ranks   models within languages closely according to their   final performance . With a τof .58 between scores   ( p < 0.001 ) , this works out to DP select-   ing the better performing final model given any   two models 79 % of the time . Additionally , LAS is   slightly more predictive of final performance than   unlabeled , undirected attachment scores ( UUAS )   withτ= .57 to which prior probing approaches   are restricted ( see Appendix C ) .   Given a modest ρof .32 ( p < 0.05 ) , we sur-   prisingly also observe a single strong outlier to   this pattern , namely the multilingual RemBERT   ( Chung et al . , 2021 ) decoupled LM architecture .   While DP consistently ranks it low as it   can not extract dependency parse trees as accurately   as from the BERT and RoBERTa - based architec-   tures , RemBERT actually performs best on four   out of the nine targets when fully fine - tuned in   BAP . Excluding monolingual LMs , it further out-   performs the other multilingual LMs in seven out   of nine cases . As it is a more recent and distinc-   tive architecture with many differences to the most   commonly - used contemporary LMs , we analyze   potential reasons for this discrepancy in Section 4 .   Excluding RemBERT as an outlier , we find sub-   stantially higher correlation among all other mod-   els : ρ= .78 and τ= .78 ( p < 0.001 ) . This means   that among these models , fully fine - tuning the LM   for which DP extracts the highest scores ,   yields the better final performance 89 % of the time .   In practice , learning DP ’s linear trans-   formations while keeping the LM frozen is multiple   orders of magnitude more efficient than fully train-   ing a complex parser plus the LM ’s parameters .   As such , linear probing offers a viable method for   selecting the best encoder in absence of qualita-   tive heuristics or intuitions . This predictive perfor-   mance is furthermore achievable in minutes com-   pared to hours and at a far lower energy budget ( see   Appendices B and C ) .   4 Probing Decoupled LMs   Considering DP ’s high predictive perfor-   mance across LMs with varying architecture types ,   languages / domains and pre - training procedures ,   we next investigate its limitations : Specifically ,   which differences in RemBERT ( Chung et al . ,   2021 ) lead to it being measured as an outlier with   seemingly low amounts of latent dependency infor-   mation despite reaching some of the highest scores   after full fine - tuning . The architecture has 32 lay-   ers and embeddings with d= 1152 , compared to   most models ’ 12 layers and d= 768 . It accom-   modates these size and depth increases within a   manageable parameter envelope by using smaller   input embeddings with d= 256 . While choosing   different dfor the input and output embeddings   is not possible in most prior models due to both   embedding matrices being coupled , RemBERT de-   couples them , leading to a larger parameter budget   and less overfitting on the masked language model-   ing pre - training task ( Chung et al . , 2021).1298   Layer - wise Probing Prior probing studies have   found dependency information to be concentrated   around the middle layers of an LM ( Hewitt and   Manning , 2019 ; Tenney et al . , 2019 ; Fayyaz et al . ,   2021 ) . Using EN - EWT ( Silveira et al . , 2014 ) ,   we evaluate whether this holds for RemBERT ’s   new architecture . Figure 3 confirms that both de-   pendency structural and relational information are   most prominent around layer 17 of 32 as indi-   cated by UUAS and relation classification accuracy   ( RelAcc ) respectively . Combining the structural   and relational information in DP similarly   leads to a peak of the LAS at the same layer while   decreasing with further distance from the center .   Across all target languages , we next investigate   whether probing a sum over the embeddings of all   layers weighted by α∈Rcan boost extraction   performance in RemBERT . The heavier weighting   of middle layers by α , visible in Figure 4 , reaf-   firms a concentration of dependency information   in the center . Contrasting probing work on prior   models ( Tenney et al . , 2019 ; Kulmizev et al . , 2020 ) ,   using all layers does not increase the retrievable de-   pendencies , with LAS differences ±1 point . This   further confirms that there is not a lack of depen-   dency information in any specific layer , but that   there is less within the encoder as a whole .   Frozen Parsing Our probing results show that   linear subspaces in RemBERT contain less depen-   dency information than prior LMs . However , D-   P ’s parametrization is kept intentionally sim - ple and may therefore not be capturing non - linearly   represented information that is useful during later   fine - tuning . To evaluate this hypothesis , we train   a full biaffine attention parsing head , but keep the   underlying LM encoder frozen . This allows us to   quantify the performance gains which come from   inherent dependency information versus later task-   specific fine - tuning .   Table 1 confirms our findings from DP   and shows that despite RemBERT outperforming   mBERT and XLM - R when fully fine - tuned , it   has substantially lower LAS across almost all lan-   guages when no full model fine - tuning is applied .   This leads us to conclude that there indeed is less in-   herent dependency information in the newer model   and that most performance gains must be occurring   during task - specific full fine - tuning .   Given that DP extracts dependency   structures reliably from LM architectures with dif-   ferent depths and embedding dimensionalities ( e.g.   RoBERTawith 24 layers and d= 1024 versus   RuBERTwith 3 layers and d= 312 ) as well as   varying tokenization , optimization and pre - training   data , the key difference in RemBERT appears to   be embedding decoupling . The probe ’s linear for-   mulation is not the limiting factor as the non - linear ,   biaffine attention head also produces less accurate   parses when the LM ’s weights are frozen . Our   analyses thus suggest that RemBERT ’s decoupled   architecture contains less dependency information   out - of - the - box , but follows prior patterns such as   consolidating dependency information towards its   middle layers and serving as strong initialization   for parser training .   Lastly , RemBERT ’s larger number of tunable   parameters compared to all other LM candidates   may provide it further capacity , especially after full   fine - tuning . As our probing methods are deliber-   ately applied to the frozen representations of the en-   coder , it becomes especially important to consider   the degree to which these embeddings may change   after updating large parts of the model . Taking   these limitations into account , the high correlations   with respect to encoder ranking nonetheless enable   a much more informed selection of LMs from a   larger pool than was previously possible .   5 Conclusion   To guide practitioners in their choice of LM en-   coder for the structured prediction task of depen-   dency parsing , we leveraged a lightweight , linear1299DP to quantify the latent syntactic infor-   mation via the labeled attachment score . Evaluat-   ing 46 pairs of multilingual / language - specific LMs   and nine typologically diverse target treebanks , we   found DP to not only be efficient in its   predictions , with orders of magnitude fewer train-   able parameters , but to also be accurate 79–89 %   of the time in predicting which LM will outper-   form another when used in a fully tuned parser .   This allows for a substantially faster iteration over   potential LM candidates , saving hours worth of   compute in practice ( Section 3 ) .   Our experiments further revealed surprising in-   sights on the newly proposed RemBERT architec-   ture : While particularly effective for multilingual   dependency parsing when fully fine - tuned , it con-   tains substantially less latent dependency informa-   tion relative to prior widely - used models such as   mBERT and XLM - R. Among its architectural dif-   ferences , we identified embedding decoupling to   be the most likely contributor , while added model   capacity during fine - tuning may also improve final   performance . Our analyses showed that despite   containing less dependency information overall ,   RemBERT follows prior findings such as structure   and syntactic relations being consolidated towards   the middle layers . Given these consistencies , per-   formance differences between decoupled LMs may   be predictable using probes , but in absence of simi-   lar multilingual LMs using decoupled embeddings   this effect remains to be studied ( Section 4 ) .   Overall , the high efficiency and predictive power   of ranking LM encoders via linear probing as well   as the ease with which they can be analyzed — even   when they encounter their limitations — offers im-   mediate benefits to practitioners who have so far   had to rely on their own intuitions when making   a selection . This opens up avenues for future re-   search by extending these methods to more tasks   and LM architectures in order to enable better in-   formed modeling decisions .   Acknowledgements   We would like to thank the NLPnorth group for   insightful discussions on this work , in particular   Elisa Bassignana and Mike Zhang . Thanks also to   ITU ’s High - performance Computing team . Finally ,   we thank the anonymous reviewers for their helpful   feedback . This research is supported by the Inde-   pendent Research Fund Denmark ( Danmarks Frie   Forskningsfond ; DFF ) grant number 9063 - 00077B.References1300130113021303   Appendices   A Treebanks   Table 2 lists the nine target treebanks based on   the set by Smith et al . ( 2018 ): AR - PADT ( Haji ˇc   et al . , 2009 ) , EN - EWT ( Silveira et al . , 2014 ) , FI-1304TDT ( Pyysalo et al . , 2015 ) , GRC - PROIEL ( Eck-   hoff et al . , 2018 ) , HE - HTB ( McDonald et al . ,   2013a ) , KO - GSD ( Chun et al . , 2018 ) , RU - GSD   ( McDonald et al . , 2013b ) , SV - Talbanken ( McDon-   ald et al . , 2013a ) , ZH - GSD ( Shen et al . , 2016 ) . We   use these treebanks as provided in Universal Depen-   dencies v2.9 ( Zeman et al . , 2021 ) . DP and   BAP are trained on each target ’s respective train-   ing split and are evaluated on the development split   as this work aims to analyze general performance   patterns instead of state - of - the - art performance .   B Experiment Setup   DP is implemented in PyTorch v1.9.0   ( Paszke et al . , 2019 ) and uses language models   from the Transformers library v4.13.0 and the as-   sociated Model Hub ( Wolf et al . , 2020 ) . Following   the structural probe by Hewitt and Manning ( 2019 ) ,   each token which is split by the LM encoder into   multiple subwords is mean - pooled . Similarly , we   follow the original hyperparameter settings and   set the structural subspace dimensionality to b=   128 and use embeddings from the middle layer of   each LM ( Hewitt and Manning , 2019 ; Tenney et al . ,   2019 ; Fayyaz et al . , 2021 ) . The structural loss is   computed based on the absolute difference of the   Euclidean distance between transformed word em-   beddings and the number of edges separating the   words in the gold tree ( see Hewitt and Manning ,   2019 for details ) . The relational loss is computed   using cross entropy between the logits and gold   head - child relation . Optimization uses AdamW   ( Loshchilov and Hutter , 2018 ) with a learning rate   of10which is reduced by a factor of 10 each   time the loss plateaus . Early stopping is applied   after three epochs without improvement and a max-   imum of 30 total epochs . With the only trainable   parameters being the matrices BandL , the model ’s   footprint ranges between 51k and 190k parameters .   BAP For the biaffine attention parser ( Dozat and   Manning , 2017 ) we use the implementation in the   MaChAmp framework v0.3 ( van der Goot et al . ,   2021 ) with the default training schedule and hyper-   parameters . The number of trainable parameters   depends on the LM encoder ’s size and ranges be-   tween 14 M and 583M.   Analyses For our analyses in Sections 3 and 4 we   further make use of numpy v1.21.0 ( Harris et al . ,   2020 ) , SciPy v1.7.0 ( Virtanen et al . , 2020 ) and   Matplotlib v3.4.3 ( Hunter , 2007).Training Details Models are trained on an   NVIDIA A100 GPU with 40GBs of VRAM and   an AMD Epyc 7662 CPU . BAP requires around 1   h ( ±30 min ) . DP can be trained in around   15 min ( ±5 min ) with the embedding forward op-   eration being most computationally expensive . The   models use batches of size 32 and are initialized   using the random seeds 692 , 710 and 932 .   Reproducibility In order to ensure reproducibil-   ity and comparability with future work , we re-   lease our code and token - level predictions at   https://personads.me/x/naacl-2022-code .   C Detailed Results   Tables 3–11 list exact LAS and standard deviations   for each experiment in Section 3 ’s Figure 2 in ad-   dition to the HuggingFace Model Hub IDs of the   LMs used in each of the 46 setups as well as their   number of layers , embedding dimensionality dand   total number of parameters . In addition , Figure   5 shows UUAS for all setups , equivalent to only   probing structurally ( Hewitt and Manning , 2019 )   for unlabeled , undirected dependency trees.130513061307