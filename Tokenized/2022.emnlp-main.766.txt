  Yingxiu Zhao , Yinhe Zheng , Zhiliang Tian , Chang Gao ,   Jian Sun , Nevin L. ZhangThe Hong Kong University of Science and Technology , Hong KongAlibaba Group , China , The Chinese University of Hong Kong , Hong Kong   Abstract   Lifelong learning ( LL ) is vital for advanced   task - oriented dialogue ( ToD ) systems . To ad-   dress the catastrophic forgetting issue of LL ,   generative replay methods are widely employed   to consolidate past knowledge with generated   pseudo samples . However , most existing gen-   erative replay methods use only a single task-   specific token to control their models . This   scheme is usually not strong enough to con-   strain the generative model due to insufficient   information involved . In this paper , we pro-   pose a novel method , prompt conditioned VAE   for lifelong learning ( PCLL ) , to enhance gen-   erative replay by incorporating tasks ’ statistics .   PCLL captures task - specific distributions with   a conditional variational autoencoder , condi-   tioned on natural language prompts to guide   the pseudo - sample generation . Moreover , it   leverages a distillation process to further con-   solidate past knowledge by alleviating the noise   in pseudo samples . Experiments on natural   language understanding tasks of ToD systems   demonstrate that PCLL significantly outper-   forms competitive baselines in building life-   long learning models . We release the code and   data at GitHub .   1 Introduction   Task - oriented dialogue ( ToD ) systems are of great   importance in advanced AI applications ( Zhang   et al . , 2020b ; Dai et al . , 2020 , 2021 ; He et al . ,   2022a , b , c ) . However , most existing ToD systems   are developed under the assumption that the data   distribution remains unchanged ( Zhu et al . , 2022 ) .   Unless the entire system is retrained , this setup may   not be realistic when the ToD system deployed in   practice needs to support new features and pro-   vides more services over time based on user de-   mands . Without incurring the high cost of retrain-   ing , Lifelong Learning ( LL ) is able to acquire newknowledge continuously while preserving previ-   ously learned knowledge ( Delange et al . , 2021 ) .   Hence , it ’s crucial to equip natural language under-   standing ( NLU ) modules , the vital components of   ToD systems , with the lifelong learning ability .   The main issue for lifelong learning is catas-   trophic forgetting ( McClelland et al . , 1995 ; Parisi   et al . , 2019 ) , which refers to the phenomenon that a   model forgets previously learned tasks when learn-   ing new tasks . Various approaches have been pro-   posed to alleviate this issue ( Schwarz et al . , 2018 ;   Aljundi et al . , 2018 ; Rusu et al . , 2016 ; Aljundi   et al . , 2017 ) . The replay - based methods are among   the most effective and widely used ones ( Rebuffi   et al . , 2017 ; Shin et al . , 2017 ; Dai et al . , 2022 ) .   The main idea of replay - based methods is to re-   train samples or representations from already seen   tasks when learning new tasks ( Mundt et al . , 2020 ) .   Some methods explicitly store previously seen real   samples for replaying ( experience replay ) ( Rebuffi   et al . , 2017 ; Chaudhry et al . , 2019 ) . However , this   setting will be infeasible when data from previous   tasks is unavailable due to data security concerns .   Other methods try to generate pseudo samples us-   ing a generative model ( generative replay ) . This   variant relieves the burden of storing previously   seen data and has been widely adopted in previ-   ous studies ( Delange et al . , 2021 ; Shin et al . , 2017 ;   Kemker and Kanan , 2018 ) .   The key to generative replay is to produce   pseudo samples to approximate the real data distri-   bution of previous tasks . Intuitively , higher quality   pseudo samples can better preserve learned tasks   and lead to less forgetting in LL . However , the gen-   eration of pseudo samples for each seen task in   previous studies ( Sun et al . , 2020 ; Chuang et al . ,   2020 ) is usually controlled by a single task - specific   token . It has been observed that this scheme is usu-   ally insufficient to constrain the PLM ( Sun et al . ,   2020 ) , due to limited information involved . Conse-   quently , the generated pseudo samples suffer from11153problems such as not being fluent or not corre-   sponding well to the designated task . Moreover ,   those special tokens are only introduced in the   fine - tuning stage of the PLM . This enlarges the   gap between pre - training and fine - tuning of the   PLM ( Gu et al . , 2022 ) and harms the quality of   the generated pseudo samples . In addition , gener-   ated noisy pseudo samples may degenerate the LL   performance .   To address the above issues , we propose a novel   method , Prompt Conditioned V AE for Lifelong   Learning ( PCLL ) , to enhance generative replay on   NLU tasks of ToD systems . To impose strong con-   trol over the pseudo - sample generation , PCLL ex-   plicitly models latent task - specific distributions us-   ing a conditional variational autoencoder ( CV AE )   ( Kingma and Welling , 2014 ; Zhao et al . , 2017 ) .   Then it incorporates the corresponding task statis-   tics to guide the generation of pseudo samples . To   reduce the gap between pretraining and finetuning ,   we construct natural language prompts to unify dif-   ferent NLU tasks while being specific to each task .   These prompts not only contain meaningful seman-   tics compared to special tokens , but also serve as   conditions to assist CV AE in capturing task distri-   butions . Moreover , PCLL employs a knowledge   distillation scheme to alleviate the impact of noisy   pseudo samples during the replay process . Leverag-   ing the above strategies , PCLL can generate high-   quality pseudo samples that better approximate the   real distributions of previous tasks while tackling   the aforementioned issues .   We validate our method on NLU tasks of ToD   systems including both intent detection and slot fill-   ing . The results indicate that our approach gener-   ates high - quality pseudo samples and significantly   outperforms competitive baselines . Our main con-   tributions are as follows ,   ( 1)We propose a novel method , PCLL , to enhance   generative replay for building lifelong NLU mod-   ules of ToD systems .   ( 2)Conditioned on prompts , PCLL models latent   task distributions with CV AE to guide the pseudo-   sample generation and leverages knowledge distil-   lation to further avoid forgetting .   ( 3)Our extensive experiments and comprehensive   analyses demonstrate the superior performance of   PCLL and the high quality of its generated samples.2 Related Work   2.1 Lifelong Learning   There are generally three categories of LL methods :   Regularization - based Methods aim to strike a   balance between protecting already learned tasks   while granting sufficient flexibility for a new task   ( Mundt et al . , 2020 ) . Some methods ( Schwarz   et al . , 2018 ; Aljundi et al . , 2018 ; Zenke et al . , 2017 ;   Ebrahimi et al . , 2019 ) impose constraints on the   modification of important weights . Other methods   introduce a distillation loss to constrain predicted   features of the LL model . ( Li and Hoiem , 2017 ;   Dhar et al . , 2019 ; Rannen et al . , 2017 ) . However ,   these additional regularization terms may down-   grade the model performance ( Parisi et al . , 2019 ) .   Architecture - based Methods dedicate model   parameters for each task to prevent forgetting ( De-   lange et al . , 2021 ) . Some studies ( Fernando et al . ,   2017 ; Serrà et al . , 2018 ; Hu et al . , 2018 ) use   static architectures and rely on task specific infor-   mation to route through the architecture ( Mundt   et al . , 2020 ) , while other studies ( Rusu et al . , 2016 ;   Aljundi et al . , 2017 ; Zhai et al . , 2020 ; Madotto   et al . , 2021 ; Ke et al . , 2021 ; Geng et al . , 2021 ;   Zhao et al . , 2022b ) dynamically grow the architec-   ture in the LL training process . However , these   methods either require capacity allocation for tasks   at the beginning or are not feasible when model ex-   pansion is prohibited with limited resources ( Sun   et al . , 2020 ) .   Replay - based Methods aim to preserve pre-   vious knowledge by replaying data from learned   tasks . One line of studies ( Rebuffi et al . , 2017 ;   Chaudhry et al . , 2019 ; Lopez - Paz and Ranzato ,   2017 ; Mi et al . , 2020 ; Han et al . , 2020 ; Liu et al . ,   2021b ) keeps a small number of real samples from   old tasks for replaying . However , these methods   are unpractical when data from old tasks are un-   available . Another line of studies ( Shin et al . , 2017 ;   Kemker and Kanan , 2018 ; Xiang et al . , 2019 ) uti-   lizes a generative model to reproduce pseudo sam-   ples or representations from old tasks .   In this paper , we focus on improving generative   replay , as it does not require allocating extra param-   eters or model capacity and can be used with any   LL model . Specifically , Sun et al . ( 2020 ) propose a   general framework LAMOL for lifelong language   learning to replay pseudo samples of previous tasks .   Chuang et al . ( 2020 ) improve LAMOL by training   an extra teacher model before learning each new   task , however , this increases the burden of the LL11154process . Kanwatchara et al . ( 2021 ) freeze critical   parameters in LAMOL based on rationales , but   those rationales are not always available for NLP   tasks . All these previous works do not take task   statistics into consideration , whereas our PCLL   method incorporates the information of tasks ’ dis-   tributions to enhance generative replay .   2.2 Prompt - based Learning in NLP   Prompt - based learning has been found to be   more effective than typical finetuning to use PLM   ( Schick and Schütze , 2021 ) . With prompts , we can   convert various downstream tasks to a unified lan-   guage modeling task ( Brown et al . , 2020 ; Schick   and Schütze , 2021 ) . Prompts can be either manu-   ally designed ( Petroni et al . , 2019 ; Yu et al . , 2019 )   or generated automatically ( Shin et al . , 2020 ; Jiang   et al . , 2020 ; Gao et al . , 2021 ) . Some recent stud-   ies employ prompt tuning on continual learning   for dialogue state tracking ( Zhu et al . , 2022 ) and   few - shot learning ( Qin and Joty , 2022 ) .   3 Methodology   3.1 Problem Definition   We aim to build an LL model to learn a stream of   NLU tasks sequentially T={t}in dialogue   systems , where Tcan be infinite potentially . For   each task t , a set of samples D={(x , y ) }   are drawn from its underlying data distribution .   Here , xdenotes the input utterance , and yde-   notes the output label of NLU . In intent detection   tasks , yis the intent label of x ; in slot filling   tasks , yis the slot - value pairs contained in x.   Our objective is to learn a model that can perform   well on all seen tasks and forget as little as possible .   3.2 Overview   We start with a brief overview of our proposed   PCLL method for generative replay ( See Fig . 1 ) .   PCLL consists of two components : an LM - based   task solver to solve NLU tasks ( Fig . 3 ) and a CV AE-   based generator ( Fig . 2 ) to generate pseudo samples   with the help of task - specific latent distributions .   For the first task , PCLL is initialized with PLMs   along with other parameters randomly initialized .   Before learning a new task t , we first use the PCLL   model trained on previous tasks to generate pseudo   samples for each of the learned tasks T. Then   we interleave these pseudo samples with the train-   ing data in Dand continue to train PCLL . In this   way , the model can learn the new task twhile con-   solidating the knowledge of past tasks .   In the following sections , we first illustrate how   PCLL learns the current task ( Sec . 3.3 , 3.4 ) . Then   we describe the pseudo - sample generation process   ( Sec . 3.5 ) , and finally , we introduce a knowledge   distillation process to further improve the LL per-   formance ( Sec . 3.6 ) .   3.3 LM - based Task Solver   Following recent studies ( Sun et al . , 2020 ; Chuang   et al . , 2020 ) , PCLL unifies different NLU tasks into   a language modeling ( LM ) task and implements a   task solver based on a PLM . Different from pre-   vious studies that introduce randomly initialized   special tokens in the fine - tuning stage ( Sun et al . ,   2020 ) , we construct task - specific natural language   prompts for the solver . These prompts carry rich   semantic information to alleviate the mismatch be-   tween fine - tuning and pre - training of PLM .   For each input - output pair ( x , y)from task t ,   our task solver is a LM that takes a prompt g(x )   as an input and predicts y. Specifically , g(x)is   constructed as g(x ) = g⊕x⊕g , where g   andgare prompt prefix and postfix designed for   taskt , respectively , and ⊕means the concatenation   of word tokens . For instance , if the task tis an   intent detection task , we design g(x)as : “ For an   utterance from the ID task , x has the   following intent ” , where “ ID ” represents the   task name of t. After serializing the output yinto a   token sequence , we can obtain a natural language   sentence by simply concatenating g(x)withy . We   list detailed examples in Appendix B.1 . Then the   PLM ffor the current task tis optimized on the   concatenated sentence   g(x , y ) = g⊕x⊕g⊕y , ( 1 )   by maximizing the following objective ( see Fig . 3 ):   L= log p(g(x , y ) ) + λlogp(y|g(x ) ) ,   in which the first term learns to decode the con-   structed sentence given the start token [ BOS ] , and11155the second term learns to predict the output yaf-   ter reading the prompt g(x).λis a scalar used to   balance these two terms .   3.4 Prompt Conditioned V AE Generator   To construct high - quality pseudo - samples , PCLL   leverages a CV AE module to build a pseudo - sample   generator so that it can incorporate tasks ’ statistics   to guide the generation of pseudo samples . The   CV AE module captures task - specific latent distri-   butions by taking utterances as the input , condi-   tioned on prefix prompts , and reconstructing the   input during training .   Specifically , given an input utterance xin task   t , we assume a random variable zcaptures the la-   tent distribution over x. We define a conditional   distribution as p(x , z|t ) = p(x|z , t)p(z|t ) , where   we approximate p(z|t)andp(x|z , t)using deep   neural networks with parameters ϕandθ , respec-   tively . We refer to p(z|t)as the prior network and   p(x|z , t)as the decoder . To reconstruct x , a latent   variable zis first sampled from p(z|t)and then x   is decoded through p(x|z , t ) .   In this study , we assume the prior of zto be a   multivariate Gaussian distribution with a diagonal   covariance matrix , and introduce a recognition net-   work q(z|x , t)to approximate the intractable true   posterior p(z|x , t ) . The goal of CV AE is to max-   imize the conditional log - likelihood logp(x|t ) = /integraltext   p(x|z , t)p(z|t)dz . Employing variational infer-   ence , we can get the following evidence lower   bound ( ELBO ) ( Zhao et al . , 2017 ) to maximize :   where βis a scalar to balance the reconstruction   termLand the Kullback – Leibler ( KL ) diver-   gence term Land is adjusted by a cyclic anneal-   ing schedule ( Fu et al . , 2019 ) to alleviate the van-   ishing latent variable issue ( Bowman et al . , 2016 ) .   CV AE Implementation . When implementing   each network in Eq.2 , we use the prompt pre-   fixgto represent the task tbecause gin-   volves the task name that can exclusively identify t.   Fig . 2 shows the overall architecture of our PCLL   model , in which we use an unidirectional trans-   former ( Vaswani et al . , 2017 ) to encode the con-   catenated sentence g⊕xinto hidden representa-   tions . Then an attention - average block ( Fang et al . ,2021 ) is introduced to pool the hidden representa-   tions of gandg⊕xto single vectors , which   are further fed into a prior network p(z|t)and   recognition network q(z|x , t)respectively . Next ,   the reparametrization trick ( Kingma and Welling ,   2014 ) is used to obtain latent variables zfrom the   prior and posterior distributions . Then zis injected   to the decoder p(x|z , t)by adding to each token   embedding ( word embedding and position embed-   ding , elementwisely ) of the prompt ( Fang et al . ,   2021 ; Li et al . , 2020 ) .   In PCLL , the decoder p(x|z , t)shares the same   parameters with the PLM - based task solver f.   This allows us to inherit the advantage of PLM   and leverage a unified model to solve each task and   generate pseudo samples simultaneously .   3.5 Pseudo Sample Generation   Generating pseudo samples for learned tasks in-   volves two steps : ( 1 ) PCLL generates a pseudo   input utterance xguided by a latent task distribu-   tion using the CV AE - based generator . Specifically ,   for each seen task t,(t < t ) , the model samples a   latent variable zfrom the prior network p(z|t )   with the constructed prompt prefix gas the input .   Then the decoder takes zandg , and decodes   them into the pseudo input xusing top - k sampling   ( Holtzman et al . , 2019 ) . ( 2 ) PCLL generates the   output yassociated with xusing the solver ( i.e. ,   following Fig . 3 ) .   3.6 Knowledge Distillation   Previous generative replay approaches indistin-   guishably interleave pseudo data with the current   task ’s training data . However , this naive approach   hurts the model performance since these pseudo   data may contain noise and may drift from the real   data distribution . In this study , we utilize a knowl-   edge distillation ( KD ) ( Hinton et al . , 2015 ) process   to prevent our model from being affected by these   noisy pseudo data .   When training on a new task t , we treat the   model obtained on previous tasks Tas a fixed   teacher model f. For each input - output pair   ( x , y)in the pseudo data , fis distilled on the   generated pseudo data to the current model f(i.e . ,   serves as the student model ) by maximizing the11156   token - level distillation objective :   L=   +   where g(x , y ) < landy < l refers to the token   sequence before the l - th token in g(x , y)andy ,   respectively . Vrepresents the vocabulary set .   Similarly , when training the CV AE module , we   replace the reconstruction term L of in Eq . 2   with a distillation objective :   L = Ep(v|z , t , x)×   logp(v|z , t , x ) ,   and thus we maximize the following objective over   the pseudo data L = L−βL.   Using the above KD strategy , the distributions   produced by the teacher model contain richer   knowledge compared to one - hot labels ( Hinton   et al . , 2015 ) . These distributions constrain the stu-   dent model ( i.e. , f ) by preventing its weights from   drifting too far when learning new tasks , thereby   mitigating forgetting in lifelong learning .   Fig.1 illustrates the training process of PCLL .   Specifically , when learning a new task t , we op-   timize PCLL on training samples of twith the   following objective : L+L . For pseudo   samples of previous tasks t,(t < t ) , we optimizethe loss   L = α(L+L ) + ( 1−α)(L+L ) ,   where α∈[0,1]is a scalar used to adjust knowl-   edge distillation terms .   4 Experiments   4.1 Datasets   We evaluate the PCLL method on intent detection   and slot filling based on public NLU benchmarks :   For intent detection , we collect six datasets that   carry intent annotations : HWU ( Liu et al . , 2019 ) ,   BANKING ( Casanueva et al . , 2020 ) , CLINC ( Lar-   son et al . , 2019 ) , SNIPS ( Coucke et al . , 2018 ) ,   AITS ( Hemphill et al . , 1990 ) , and TOP ( Gupta   et al . , 2018 ) . The dataset TOP is divided into three   disjoint subsets TOP - S1 , TOP - S2 , and TOP - S3 ,   and these three subsets along with the other five   datasets are regarded as separate LL tasks to in-   crease the total number of tasks for sequential train-   ing . Finally , we have eight tasks to be learned   sequentially for this intent detection experiment .   For slot filling , we adopt five datasets that   provide slot labels : SNIPS , AITS , DSTC ( Ras-   togi et al . , 2020 ) , MIT - MOVIE , and MIT-   RESTAURANT . Each dataset above is regarded   as a separate LL task , and thus five tasks are learned   in lifelong slot filling experiments . More descrip-   tions about datasets are in Appendix A.   4.2 Implementation Details   We use the pretrained 12 - layer GPT2 model ( Rad-   ford et al . , 2019 ) to initialize the encoder and de-   coder of our CV AE model . The prior network and   the recognition network are both set to be a 2 - layer   MLP with hidden size of 128 . When learning a new   taskt , PCLL balances the training data of tand   pseudo samples by generating γNpseudo samples   for previously learned tasks . γis the sampling ratio11157andγis set to 0.2 in our experiment following Sun   et al . ( 2020 ) . Each task for intent detection and slot   filling is trained for 5 and 10 epochs , respectively .   We train PCLL on six random permutations of the   task order . See Appendix B.2 and B.3 for more   details .   4.3 Baselines   We compare PCLL with the following baselines :   Fine - tune directly fine - tunes the model on the task   stream without preventing catastrophic forgetting ;   EWC ( Schwarz et al . , 2018 ) and MAS ( Aljundi   et al . , 2018 ) are two regularization methods that   mitigate forgetting by penalizing changes of impor-   tant parameters for learned tasks ; LAMOL - g and   LAMOL - t ( Sun et al . , 2020 ) are two variants of the   generative replay method LAMOL that control the   generation of pseudo samples either using a global   special token ( LAMOL - g ) or task - specific special   tokens ( LAMOL - t ) ; L2KD ( Chuang et al . , 2020 )   improves LAMOL by assigning an extra teacher for   each new task to perform knowledge distillation ;   ER(Rolnick et al . , 2019 ) preserves previously seen   real samples for replay to prevent forgetting . We   also consider some architecture - based baselines :   HAT ( Serrà et al . , 2018 ) creates a task - based hard   attention during training ; CTR ( Ke et al . , 2021 )   inserts continual learning plug - ins into BERT to   mitigate forgetting and encourage knowledge trans-   fer;Adapter ( Madotto et al . , 2021 ) builds residual   adapter for each task independently . Since works   in Liu et al . ( 2021b ) and Qin and Joty ( 2022 ) are   specially designed for dialogue state tracking and   few - shot learning , respectively , we do not consider   them as our baselines .   Besides the above baselines , we further eval-   uate the model performance when all tasks are   trained simultaneously in a multitask learning set-   ting ( Multi ) , which is often seen as an upper bound   of LL . For fair comparisons , all baselines are im-   plemented following either the settings of Sun et al .   ( 2020 ) , or their own reported settings . For ER , we   store 1 % of previously seen samples in memory   following the setting of Madotto et al . ( 2021 ) .   4.4 Evaluation Metrics   We use the accuracy score , and macro - averaged   F1 score ( Coope et al . , 2020 ) to evaluate the per-   formance of intent detection and slot filling tasks ,   respectively . Moreover , we consider access to a test   set for each of the Ttasks to learn in the LL process ,   and define Ras the test score of the task jafter   finishing learning the task i. We follow previous   studies Lopez - Paz and Ranzato ( 2017 ) ; Chaudhry   et al . ( 2018a ) to use the following two metrics to   evaluate the performance of LL : ( 1 ) Average Score   ( Score ) is defined as the average test score of all T   tasks after the LL process : Score = /summationtextR.   ( 2)Learning Curve Area ( LCA ) is the area un-   der the Zcurve , which captures the model ’s per-   formance on all Ttasks ( Chaudhry et al . , 2018b ) .   Specifically , Zis the average score for all seen   tasks at the training step b. Here , high Score and   high LCA are preferred for a good LL model .   4.5 Main Results   Table 1 shows the performances of our model   PCLL and all the baselines . Our method PCLL sig-   nificantly outperforms all baselines by a large mar-   gin on both intent detection and slot filling tasks .   To better understand the LL process , we also plot   the curve of the average score for all the models   when trained using the same task order ( see Fig . 4 ) .   From those results , we can observe that :   ( 1 ) Regularization - based methods ( EWC and MAS )   suffer from serious catastrophic forgetting , consis-   tent with the observation of Madotto et al . ( 2021 ) .   ( 2 ) Generative replay methods LAMOL - g ,   LAMOL - t , and L2KD alleviate the forgetting issue   to some extent . However , replaying real samples   ( i.e. , ER ) performs much better . This indicates   that the quality of samples used for replaying is   critical to addressing catastrophic forgetting , which   matches our motivation to improve generative11158   replay by generating high - quality pseudo samples .   Our method PCLL achieves higher performance   than ER , indicating that PCLL can generate   high - quality pseudo samples under the guidance of   task distributions . Our analyses in Sec . 5.3 further   prove this claim .   ( 3 ) Architecture - based methods HAT , CTR , and   Adapter achieve good performance . However ,   PCLL still outperforms these baselines . This fur-   ther validates the effectiveness of PCLL . Note that   replay - based methods such as PCLL can be used   together with these architecture - based methods to   further improve the LL performance .   ( 4 ) From Fig 4 , we can notice that when switching   to new tasks , PCLL retains more knowledge about   previous tasks ( less performance degradation ) com-   pared to the baselines . This suggests that PCLL   has a better ability to consolidate knowledge and   mitigate catastrophic forgetting for LL .   4.6 Ablation Studies   We conduct ablation studies to verify the effective-   ness of each proposed component in PCLL . ( 1 )   w/o Latent means no latent distribution is modeled   for each task , i.e. , the CV AE model in Section 3.4   is removed , and pseudo samples are generated by   directly feeding the prompt prefix into the LM f   without incorporating task - specific statistics . ( 2 )   w/o Task ID means no task indicators are involved   in the prompts . In other words , we design a task-   independent prompt prefix by replacing the task   ID with a general description “ current task ” ( see   Appendix B.1 for more details ) . In this way , the   CV AE model degenerates to a V AE model that cap-   tures a global latent space for all tasks . ( 3 ) w/o KD   means that the knowledge distillation process in   Section 3.6 is not applied .   From Table 2 , we can see that : ( 1 ) Capturing   task - specific latent distributions and incorporatingthem in the pseudo - sample generation process is   crucial for building better LL models ( w/o Latent ) .   ( 2 ) Using task - specific prompts helps to generate   high - quality pseudo samples , thereby improving   the LL performance ( w/o Task ID ) . ( 3 ) The pro-   posed knowledge distillation process does mitigate   the effects of noisy pseudo - samples and is bene-   ficial for consolidating previously learned knowl-   edge to prevent forgetting ( w/o KD ) .   5 Analyses   5.1 Soft Prompts vs. Manual Prompts   We conduct analyses on soft prompts by replac-   ing manually designed prompts with soft tokens in   PCLL . Specifically , the prompt prefix gand post-   fixgin Eq . 1 are replaced by several randomly   initialized task - specific soft ( learnable ) tokens ( Liu   et al . , 2021a ) . We also vary the lengths of these   soft prompts to analyze their behaviors .   Results in Table 3 show that : ( 1 ) Longer prefix   prompts ( i.e. more parameters guiding the pseudo-   sample generation ) generally lead to better LL per-   formance ; ( 2 ) Longer postfix prompts may not al-   ways lead to better LL performance . This may be   because the postfix prompts are less important than   prefix prompts since they do not participate in the   pseudo - sample generation . Longer postfix prompts   may bring in more noise , degenerating the perfor-   mance ; ( 3 ) Using manual prompts in PCLL outper-   forms all its soft - prompt variants even though some   soft prompts are much longer than manual prompts .   This justifies our claim that manual prompts carry-   ing rich semantic information help to alleviate the   mismatch between fine - tuning and pre - training of   PLM and capture tasks ’ distributions , and thus mit-   igate catastrophic forgetting in lifelong learning .   5.2 Manual Prompts   Different Designs . We validate different designs   of manual prompts in PCLL . Specifically , we im-   plement five different prompt templates with dif-11159   ferent lengths ( Appendix B.4 ) . We observe that   different manual prompts yield almost the same   performance . This indicates that our method is ro-   bust to the design of manual prompts . ( See Table 8   in the Appendix ) .   Visualization of Attentions . We provide the vi-   sualization of the attention scores over several man-   ual prompts employed by PCLL . High attention   scores of task names in Fig . 6 indicate that the task   indicators play an important role in our manually   designed prompts ( see Appendix B.5 ) .   5.3 Qualities of Pseudo Samples   We validate the quality of pseudo samples gener-   ated by PCLL and all our generative replay base-   lines on intent detection tasks . We use the distinct   score Dist - n ( Li et al . , 2016 ) to measure the propor-   tion of unique n - grams in the generated pseudo   samples ’ inputs ( n=1,2,3,4 ) . Higher Dist - n in-   dicates more diverse generated pseudo samples ,   which is usually preferred because diverse samples   help to approximate task distributions . As shown in   Table 4 , PCLL can generate more diverse pseudo   samples compared to other generative replay meth-   ods . This demonstrates that pseudo samples con-   structed by our method are closer to real samples .   Further , we measure whether the generated   pseudo samples can restore the distribution of real   samples by visualizing samples ’ feature space with   t - SNE ( Van der Maaten and Hinton , 2008 ) . As   shown in Fig . 7 , pseudo samples generated by   PCLL are clustered in a similar pattern compared   to real samples , while those of LAMOL - t are scat-   tered in the feature space . It shows that the pseudo   samples generated by PCLL share closer distribu-   tion with the real samples compared to our base-   lines ( see Appendix B.6 for more details ) .   zdimension Score LCA   32 90.00 88.27   128 90.25 88.82   256 90.10 88.30   512 90.04 88.26   5.4 Analyses of Latent Variables   To further analyze the behavior of the pseudo sam-   ple generator , we visualize the latent space captured   by the recognition network on slot filling tasks .   Specifically , for each sample in the test dataset , we   extract a latent variable zfrom its posterior distribu-   tion and use the t - SNE algorithm ( Van der Maaten   and Hinton , 2008 ) to visualize these variables in   2D space . It can be seen from Figure 5 that the la-   tent spaces of different tasks are well clustered and   clearly separated . This indicates that the latent vari-   ablezis able to capture task - specific knowledge   among learned tasks .   We also analyze the influence of dimensions for   latent variable z. The results are listed in Table 5 .   We can notice that when we select the dimension   ofzas 128 , it can reach the best performance . This   phenomenon is reasonable , when the dimension of   zis small , it may not catch enough information to   model the task distribution ; when the dimension   is large , it may contain some noisy information ,   leading to poorer performance .   5.5 Influence of Sampling Ratio γ   We analyze the influence of the sampling ratio γ   ( ranging from 0.01 to 1.0 ) on the performance of   PCLL . The results in Table 11 indicate that PCLL   is more effective in improving the LL performance   when considering a small number of pseudo sam-   ples ( See more details in Appendix B.7).11160   5.6 Case Study   We present several pseudo samples generated by   PCLL and LAMOL in Table 6 on the BANKING   task for intent detection ( see more cases in Ap-   pendix C ) . We can observe that : ( 1 ) Compared to   LAMOL , pseudo samples produced by PCLL are   closer to real samples from the BANKING dataset ;   ( 2 ) Some samples generated by LAMOL are incon-   sistent with the task : LAMOL generates samples   for the weather domain , which is not related to the   BANKING task ; ( 3 ) LAMOL may also generate   unmatched inputs and outputs in pseudo samples   ( last line in Table 6 ) . These observations verify our   claim that a single task - specific token is too weak   to constrain the PLM , and our method PCLL helps   to generate high - quality pseudo samples that are   consistent with each task .   5.7 Analyses of Forgetting for PCLL   We provide more fine - grained analyses for the for-   getting issue based on findings when learning with   our proposed method PCLL . In Appendix D , we   carry out the analyses from the following four as-   pects : ( 1 ) unbalanced classes in some tasks , ( 2 )   conflicted label spaces for different tasks , ( 3 ) noisy   pseudo labels for generated samples and ( 4 ) thediversity of pseudo samples created by PCLL .   6 Conclusion   In this paper , we propose PCLL to enhance genera-   tive replay for addressing catastrophic forgetting of   lifelong learning in building NLU modules of ToD   systems . To construct high - quality pseudo samples ,   PCLL captures task - specific distributions with a   prompt conditioned V AE to guide the generation   of pseudo samples . Empirical results on two NLU   tasks and extensive analyses demonstrate the su-   perior performance of PCLL and the high quality   of its generated pseudo samples . Currently , we do   not consider lifelong learning in the low - resource   setting where only limited labeled data are avail-   able . In the future , we will extend our framework   to lifelong few - shot learning .   Limitations   Here are some limitations of our work :   •We have not investigated lifelong learning in the   low - resource setting where only limited labeled   data are available . In future works , we will con-   sider combining PCLL with meta - learning ( Zhao   et al . , 2022a ) to extend our framework to a life-   long few - shot learning setting . We will also ex-   tend previous studies by using unlabeled data   ( Zhang et al . , 2020a ; Zhao et al . , 2022b ) to build   lifelong learning dialogue models .   •We have not considered architecture - based meth-   ods for lifelong learning . However , our   method PCLL can be readily combined with   the architecture - based approach by leverag-   ing parameter - efficient modules ( e.g. , Adapter   ( Houlsby et al . , 2019 ; Zhang et al . , 2021 ) , LoRA   ( Hu et al . , 2021 ) ) into the model architecture to   further mitigate the catastrophic forgetting issue .   We will explore this direction in the future .   Ethical Considerations   All our experiments are conducted on public avail-   able datasets . All metrics used in our paper are   automatic and do not need manual labor . There are   no direct ethical concerns in our study .   Acknowledgement   Research on this paper was supported by Alibaba   Group through Alibaba Research Intern Program   and Hong Kong Research Grants Council ( Grant   No . 16204920).11161References11162111631116411165A Details of Datasets   We list the statistics of datasets for the intent de-   tection and slot filling in Table 7 and give detailed   descriptions as follows .   •ATIS consists of audio recordings and cor-   responding manual transcripts about humans   asking for flight information on automated air-   line travel inquiry systems . The data consists   of 17 unique intent categories .   •BANKING contains 13,083 utterances re-   lated to banking domain with 77 different fine-   grained intents .   •CLINC contains 10 domains ( e.g. , travel ,   kitchen , utility , etc . ) and 150 different intent   classes .   •DSTC consists of slot annotations spanning 4   domains ( buses , events , homes , rental cars ) .   •HWU includes 64 intents spanning 21 do-   mains ( e.g. , alarm , music , IoT , news , calendar ,   etc . )   •MIT_RESTAURANT is a semantically   tagged training and test corpus in BIO format .   •MIT_MOVIE is a semantically tagged train-   ing and test corpus in BIO format . We choose   “ eng ” corpus for implementation which con-   sists of simple queries .   •TOP is a dataset of 44 K utterances where   each utterance is annotated with a hierarchical   semantic representation .   •SNIPS contains crowdsourced queries dis-   tributed among 7 user intents of various com-   plexity .   B Experiment Details   B.1 Prompt Examples of NLU Tasks   We provide some detailed examples for inputs and   outputs of the model with the designed prompts   in PCLL . For intent detection , when we train on   “ BANKING ” task , an input utterance xof the lan-   guage model ( LM ) for a sample is modified as   “ For an utterance from the BANKING task ,   “ I already have one of your cards , how do   I link them ? ” has the following intent ” ,   the output of LM yis its corresponding intent an-   notation : “ Card linking ” . For the ablation study   ofw / o Task ID , the prompt of the above sample   becomes “ For an utterance from the current   task , “ I already have one of your cards ,   how do I link them ? ” ” .   For slot filling , when we train on the “ MIT-   RESTAURANT ” task , an input utterance xis   “ Does the Casanova restaurant at Kendall   Square offer a fixed price menu ? ” of LM is   modified as “ In the MIT - RESTAURANT task , if   there are any slots and values , what are   they in this sentence : “ Does the Casanova   restaurant at Kendall Square offer a fixed   price menu ? ” ? Answer : ” , the output ylo-   cating the contained slot - value pairs is modified   as “ Restaurant name : Casanova ; Location :   Kendall Square . ” . Here , different slot - value   pairs are formatted as “ slot : value ” separated   with “ ; ” . If the input xdoes not contain any slot-   value pairs , we use the sentence “ No slot in this   sentence . ” as the output y.   B.2 Different Task Orders   We list the six random permutations of tasks that   we use to implement all competing methods in   Table 10 .   B.3 Model Implementation Details   We use a pre - trained GPT2 model ( Radford et al . ,   2019 ) as the initialization for the encoder and de-   coder of CV AE in PCLL . We set the maximum   context length as 256 . Our model contains a total   number of 240 M parameters . We train all compet-   ing methods on 1 Tesla - V100 GPU and it takes   around 6 to 10 hours to train all the tasks . More-   over , the training and testing batch sizes are set   to 64 . The maximum learning rate is 5e−5 , the   Adam optimizer is used with parameters β= 0.9,11166β= 0.98andϵ= 1e−8 . The number of cy-   cles for the cyclic annealing schedule is set to 4   in each epoch . When generating pseudo samples ,   the maximum decoded sequence length is set to 96 .   For baselines implementations , we use BERT to   implement HAT and CTR , and choose GPT-2 as   the backbone model for other baselines ( LAMOL ,   L2KD , ER , Adapter , EWC , MAS , Finetune ) .   Score LCA   Prompt1 ( 12 tokens ) 90.25 88.82   Prompt2 ( 13 tokens ) 89.94 88.78   Prompt3 ( 4 tokens ) 90.34 88.75   Prompt4 ( 11 tokens ) 90.05 88.50   Prompt5 ( 28 tokens ) 89.20 88.22   B.4 Analysis of Manual Prompts Designs   We list five different manual templates as the de-   signed prompts of intent detection in Table 9 , where   Prompt1 is the one we use in Table 1 . Let IDrefers   the task name , xrefers the input utterance and y   means the intent of x.   B.5 Analysis of Prompt Attention   We provide the visualization of the attention scores   over several samples employed with our designed   prompts for intent detection tasks . Specifically , the   attention score on each prompt token is calculated   using the averaged attention it receives when gen-   erating the output prediction . From the following   Fig 6 , we can notice that the task names do con-   tain meaningful information to be attended to when   generating predictions .   B.6 Analysis of Pseudo - sample Quality   We analyze the quality of generated pseudo sam-   ples with PCLL and other generative replay - based   baselines . Specifically , we first fine - tune a pre-   trained BERT ( Devlin et al . , 2019 ) model using   these observed real samples to construct a task clas-   sifier . This classifier can determine the task identity   of a given sample , and it reaches an accuracy of   98.67 % on a hold - out test set . The fine - tuned BERT   is used to extract the representation vector of each   sample , and the t - SNE algorithm ( Van der Maaten   and Hinton , 2008 ) is used to map these vectors into2 - dimensions . For a specific task orderin LL ,   we gather pseudo samples generated when learning   the last task and visualize the feature space of these   samples . Note that the last task , ATIS , is not shown   in Fig . 7 since there is no need to replay the last   task .   B.7 Analysis of Sampling Ratio   Table 11 shows the results on intent detection tasks .   It can be seen that generating more pseudo samples   helps to improve the LL performance . Besides ,   the performance gain slows down as the sampling   ratioγexceeds 0.2 , i.e. , generating 5 times more   pseudo samples from γ= 0.01toγ= 0.05yields   10.48 absolute improvement on the Score metric ,   while increasing γfrom 0.2 to 1.0 only yields 1.63   absolute improvement .   C Case Study   We present more generated pseudo samples from   PCLL and LAMOL along with real samples in   Table 12 . For intent detection , we list real and   pseudo samples from HWU tasks ; for slot filling ,   we list those samples from MIT - RESTAURANT   and DSTC tasks in Table 12 .   D Analyses of Forgetting   We provide more fine - grained analyses for the for-   getting issue .   •Classes with fewer samples are easier to be   forgotten . Some tasks ( e.g. , ATIS , TOP , MIT-   MOVIE ) have unbalanced classes . These minor   classes that only occupy a small portion of train-   ing samples are less likely to appear in pseudo   samples used for replay . For example , the in-   tent “ meal ” only takes 0.13 % of the training sam-   ples for ATIS , and there are barely any pseudo   samples generated for this intent when replaying.11167   Without these pseudo samples , the model is more   likely to forget these minor classes .   •Different tasks may have partially overlapping   data distributions and conflicted label spaces ,   i.e. , some tasks may assign different labels to   the same set of utterances . For example , in the   CLINC dataset , the utterance “ transfer funds to   the other account ” is assigned with a label of   “ transfer ” ; however , in the BANKING dataset ,   the same utterance is assigned with a label of   “ transfer into account ” . These conflicted label   spaces may confuse the model , resulting in incor-   rectly labeled pseudo samples .   •Noisy pseudo labels created by generative replay   may lead to error accumulation , which will down-   grade the performances of previously learned   tasks .   •The diversity of generated pseudo samples for   previous tasks tends to decrease as more replay   times are performed , and these less diversified   pseudo samples lead to more forgetting .   Specifically , we conduct analyses on lifelong in-   tent detection tasks with the following task order   ( CLINC , SNIPS , TOP_S3 , BANKING , TOP_S2 ,   HWU , TOP_S1 , ATIS ) . We compare the diversityof pseudo - samples for the first task ( i.e. , CLINC )   generated at different replay moments : ( 1 ) af-   ter learning the first task , ( 2 ) after learning three   subsequent tasks , and ( 3 ) after learning eight sub-   sequent tasks ( i.e. , after the last task ’s learning ) .   In Table 13 , we use the distinct scores ( Li et al . ,   2016 ) to measure the diversity of pseudo samples .   We can notice that as we learn more tasks , the   diversity of pseudo samples for the first learned   task decreases . Therefore , replaying less diverse   pseudo samples leads to performance degrada-   tion on previous tasks ( i.e. , forgetting of previous   tasks).1116811169