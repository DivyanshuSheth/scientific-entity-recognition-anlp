  Kazushi Kondo , Saku Sugawara , Akiko AizawaThe University of Tokyo , National Institute of Informatics   kkkazu@g.ecc.u-tokyo.ac.jp , { saku,aizawa}@nii.ac.jp   Abstract   In this study , we create a CConS ( Counter-   commonsense Contextual Size comparison )   dataset to investigate how physical common-   sense affects the contextualized size compari-   son task ; the proposed dataset consists of both   contexts that fit physical commonsense and   those that do not . This dataset tests the abil-   ity of language models to predict the size re-   lationship between objects under various con-   texts generated from our curated noun list and   templates . We measure the ability of several   masked language models and generative mod-   els . The results show that while large language   models can use prepositions such as “ in ” and   “ into ” in the provided context to infer size rela-   tionships , they fail to use verbs and thus make   incorrect judgments led by their prior physical   commonsense .   1 Introduction   Humans possess physical commonsense regarding   the behavior of everyday objects . Physical com-   monsense knowledge is relevant to their physical   properties , affordances , and how they can be ma-   nipulated ( Bisk et al . , 2020 ) . While a significant   amount of physical commonsense can be expressed   in language ( Forbes and Choi , 2017 ; Bisk et al . ,   2020 ) , direct sentences describing facts such as   “ people are smaller than houses ” rarely appear be-   cause of reporting bias ( Gordon and Van Durme ,   2013 ; Ilievski et al . , 2021 ) . Recent language mod-   els have succeeded in tasks that do not require con-   textual reasoning , such as size comparison and pre-   diction of event frequency ( Talmor et al . , 2020 ) .   However , what about inferences that are context-   dependent ? Whether a language model can make   correct inferences in various contexts is impor-   tant because physical reasoning is highly context-   dependent ( Ogborn , 2011 ) . Several studies on con-   textual physical reasoning ( Forbes et al . , 2019 ;   Bisk et al . , 2020 ; Aroca - Ouellette et al . , 2021;Figure 1 : Examples of contexts that do or do not ac-   cord with ordinary commonsense . Humans can imagine   the situation and make correct inferences , but language   models are drawn to commonsense and make incorrect   judgments . The example images are generated by Mid-   journey ( https://midjourney.com ) .   Zellers et al . , 2021 ) have been conducted to pro-   duce datasets that assess the ability to recognize   physical situations described in writing . Without   context , however , these datasets may be answered   by commonsense .   Humans also can reason in ways that differ from   simply using commonsense . For instance , if the   context “ there is a house inside a light bulb . ” is   provided , humans can still imagine the situation   and reason that the bulb must be larger than the   house . In other words , commonsense is just a   sweeping generalization , and reasoning about con-   text must be independent of commonsense . This   reasoning with defeasibility , which reflects the abil-   ity to reason logically without relying only on com-   monsense , seems to have been overlooked in the   study of language models compared to the acqui-   sition of commonsense . Previous investigations   of contextual physical reasoning ( Aroca - Ouellette   et al . , 2021 ; Yu et al . , 2022 ) failed to distinguish   physical reasoning from the simple use of physical   commonsense . To appropriately measure physi-   cal reasoning ability , we must use contexts that go603against commonsense to rule out the possibility   that the model is overconfident in physical com-   monsense .   In this study , we investigate the behavior of the   language model concerning physical commonsense   given the context of a situation that contradicts   commonsense . We choose the size comparison   task despite various possible domains of physical   commonsense ( Ilievski et al . , 2021 ) . The task is   one of the easiest physical commonsense reason-   ing tasks for language models ( Forbes and Choi ,   2017 ; Goel et al . , 2019 ) , and it is also easy to add   a context to change the relationship between sizes .   For example , in this study , the context is a sentence   that implies a size relationship , such as “ < obj1 >   contains < obj2 > . ”   For this purpose , we created a new dataset ,   CConS ( Counter - commonsense Contextual Size   comparison ) . This dataset contains 1,112 sen-   tences generated from 139 templates and tests the   ability of language models to infer the size relation-   ship between objects using a cloze - style prompt .   Figure 1 shows the size comparison examples with   or without contexts that ( do not ) agree with ordi-   nary commonsense . Our experiments using recent   language models show that GPT-3(text - davinci-   003 ) ( Brown et al . , 2020 ) correctly reasons in con-   text when it is consistent with commonsense , yield-   ing 85 % accuracy . In contrast , even GPT-3 can only   show poor performance ( 41 % accuracy ) for exam-   ples that contradict commonsense . This suggests   that the models may not effectively distinguish be-   tween physical commonsense and inferences based   on contexts , leading to incorrect predictions . Nev-   ertheless , when prepositions hint at the relation-   ships , the accuracy rate exceeded 55 % , even for   counter - commonsense examples . In summary , our   counter - commonsense examples reveal the differ-   ence in influence between prepositions and verbs   in contextualized physical reasoning .   The contributions of this study are as follows :   1.We create a dataset that assesses size com-   parison ability more precisely by contrasting   examples that conform to physical common-   sense with ones that do not .   2.We show that physical commonsense prevents   measuring the language models ’ ability of   contextual physical reasoning.3.We demonstrate that even large models per-   form poorly when making inferences that vi-   olate physical commonsense . Specifically ,   they struggle to infer size relations implied   by verbs and can infer only when prepositions   indicate .   2 Related Works   Size Comparison Task The size comparison   task , which previous studies ( Yang et al . , 2018 ;   Goel et al . , 2019 ) investigated since the earlier lin-   guistic representations , such as GloVe ( Pennington   et al . , 2014 ) or ELMo ( Peters et al . , 2018 ) , is one of   the easiest physical common - sense inference tasks   for language models ( Forbes and Choi , 2017 ; Goel   et al . , 2019 ) . While there are many prior studies   ( Elazar et al . , 2019 ; Zhang et al . , 2020 ) on this   topic , VerbPhysics ( Forbes and Choi , 2017 ) is the   most similar to this study in that it focuses on the   relationship between sizes and verbs . There are   also some other approaches , such as methods that   extract external knowledge ( Elazar et al . , 2019 ) ,   filling - masks ( Talmor et al . , 2020 ) , or generate im-   ages ( Liu et al . , 2022 ) . These results suggest that   the commonsense of comparing object size is en-   coded in recent language models . However , these   studies do not consider the context that might influ-   ence the results of size comparisons .   Defeasible Reasoning According to Koons   ( 2022 ) , defeasible reasoning is an argument that is   rationally persuasive but not completely valid as   a deduction . This defeasible reasoning is similar   to the subject of this study in that it involves the   recognition that commonsense and assumptions in   a given context are not entirely correct propositions .   Therefore , this study can be seen as an investiga-   tion into whether a language model can capture   commonsense as defeasible reasoning . The cre-   ation of a dataset dealing with defeasible reasoning   has been discussed by Rudinger et al . ( 2020 ) and   Allaway et al . ( 2022 ) . Our study is similar to All-   away et al . ( 2022 ) in that it generates sentences that   violate the context by fitting words to a template .   However , this study differs in that we also generate   examples contrary to commonsense for measuring   the actual performance of the language model as   well as the differences from the ordinary case .   3 Dataset Creation   In this study , we create 139 templates and auto-   matically generate 1,112 examples . Table 1 lists604   examples of these templates .   Designing Template We focus on the compre-   hensiveness of verb phrases while designing tem-   plates to ensure that the choice of verbs is not arbi-   trary . Therefore , we extract 139 verb phrases that   indicate size relationships from the Oxford 5000   dictionaryand manually assemble simple sen-   tences . For example , the statement “ < obj1 > beats   < obj2 > ” is not included in this template because   this statement is not informative enough to deter-   mine a size relation .   Moreover , in comparing sizes , we also notice   not only verbs but the usage of prepositions such   as “ in ” or “ into ” may provide clear clues about the   size relationships . Therefore , we select templates   that contain only examples with these prepositions   and distinguish them as easy templates from those   that do not as hard templates . In subsequent ex-   periments , we also investigate the effect of this   difference on the behavior of the language model .   Restriction on Noun If nouns are arbitrarily in-   serted , the resulting sentences may be nonsensical   or impossible for a human to imagine . For example ,   we choose not to include the sentence “ the stone   threw the dog ” because it is beyond imagination .   We place restrictions on the nouns used in the   sentence templates by defining tags to avoid this   nonsense . A single placeholder can have con-   straints ( multiple tags ) . There are 18 types of tags ,   including “ have_hands , ” “ box , ” and “ portable . ”   Tags are manually determined to abstract the prop-   erties of verb phrases . We also use the Oxford   5000 dictionary to obtain a list of nouns referring   to physical objects . One of the nouns that satisfy   all constraints is randomly selected from a list of   195 nouns and inserted .   Generating Sentences The template tags are   replaced with the corresponding nouns to generatethe context , and the questions asking for size   comparisons are combined . For example , the   contextualized question text provided to the   masked language models is as follows :   “ ”   Contexts and questions are used to generate input   for each of the masked language models and gen-   erative models . We classify generated sentences   to the Ordinary or Counter - Commonsense ( CCom-   mon ) subset based on whether the size relationship   between objects indicated by the template accords   commonsense .   4 Experiment   Task Definition We measure the ability of   masked language models and generative models   to recognize size relationships by providing sen-   tences for each architecture . These sentences are   generated from templates ( Section 3 ) . We also see   how the language model ’s behavior changes when   context sentences follow or do not follow a general   common - size relationship .   Comparison Aspects We investigate how lan-   guage models create physical reasoning without   being biased by their prior physical commonsense .   1.How do the physical reasoning results of the   language model change when contexts are   consistent or inconsistent with commonsense ?   2.How does the performance of a language   model change when comparing an easy   dataset that contains certain prepositions that   hint at size relationships with a hard dataset   that does not ?   Model Settings In this study , BERT ( Devlin   et al . , 2019 ) , RoBERTa ( Liu et al . , 2019 ) , and AL-   BERT ( Lan et al . , 2020 ) are used to assess the   performance of the masked language models . We   also investigate how the size of the model affects605physical reasoning . We choose T0 ( Sanh et al . ,   2022 ) and GPT-3(text - davinci-003 ) to evaluate the   performance of the generative model .   According to Talmor et al . ( 2020 ) , RoBERTa-   Large outperforms BERTs and RoBERTa - Base in a   no - context size comparison task . Proceeding from   this analysis we attempt to detect whether com-   monsense influences physical reasoning by giving   examples contrary to commonsense as context .   Tasks Format Details The tasks are performed   by inputting sentences according to the format de-   fined for each of the models , as follows .   Format for Masked Language Models   WithContext : « context » In this   situation , the size of < obj1 >   is probably much [ MASK ] than the   size of < obj2 > .   WithoutContext : The size of < obj1 >   is probably much [ MASK ] than the   size of < obj2 > .   The candidates for [ MASK ] are “ larger , ” “ big-   ger , ” “ smaller , ” and “ shorter . ” If the sum of the   probabilities of the first two options exceeds 0.5 ,   language models predict that obj1 is larger than   obj2 . Therefore , the language model always makes   binary decisions .   Format for Generative Models   WithContext : « context » Which is   bigger in this situation , < obj1 >   or < obj2 > ?   WithoutContext : Which is bigger in   general , < obj1 > or < obj2 > ?   « context » is a sentence generated from   templates .   Human Evaluation We ask crowdworkers to per-   form the same size comparison task to measure the   accuracy of humans in this task . Thus , we can test   the validity of the automatically generated ques-   tions . The crowdworkers are given the same con-   text and make a choice that is larger . ( See Appendix   B for details . ) Five crowdworkers are assigned to   each question . We use some intuitive examples ,   such as “ < obj1 > contains < obj2 > , ” which are pro-   vided for qualification , and exclude those who get   such examples wrong or choose the same answer   for all examples .   5 Result and Analysis   Tables 2 and 3 exhibit the performance of the   language model on our datasets . GPT-3 outper-   forms other models in Ordinary and NoCon setups .   RoBERTa - Large and ALBERT - XXLarge show bet-   ter reasoning ability than the other masked lan-   guage models in the Ordinary dataset . However , for   the CCommon dataset , the performance of the pre-   trained language model decreases , particularly in   ALBERT - XXLarge . This result suggests that com-   monsense built into the model hinders its ability to   make accurate judgments . Other models struggle   to capture size relationships . These results with-   out context ( NoCon ) are generally consistent with   the findings of a previous investigation of the no-   context size comparison task conducted by Talmor   et al . ( 2020 ) .   In some CCommon examples , BERT performs   better than RoBERTa . This may be because BERT   is less equipped with commonsense , allowing it to   make simpler judgments without being influenced .   Impact of Prepositions Prepositions did not sig-   nificantly impact the prediction for the masked lan-   guage models in the Ordinary dataset . However ,   there is a significant difference in the correct re-   sponse rates in the CCommon dataset . RoBERTa-   Large performs well in easy data , regardless of   whether the context defies commonsense . This re-   sult indicates that RoBERTa - Large recognizes the   connection between the prepositions and size re-   lationships . The ALBERT - XXLarge model does   not perform well for the CCommon dataset , even   if the setting is easy ; therefore , we consider that it   merely answers according to commonsense rather   than making inferences . In short , context is not   useful for ALBERT when the prepositions do not606   provide direct hints .   GPT-3 uses prepositions more effectively than   other models and performs better on the Easy   dataset , while the model struggles to answer the   CCommon dataset in the hard setting . This result   means GPT-3 learns commonsense well but can not   make physical logical inferences .   6 Conclusion   We develop a method providing a counter-   commonsense context to measure physical reason-   ing ability . Our proposed contextualized physical   commonsense inference dataset reveals that current   language models can partially predict size relations   but do not perform as well as humans in contexts   that contradict commonsense . These judgments are   possible to a limited extent in the presence of cer-   tain prepositions such as “ in ” and “ into . ” While we   focused on size comparison tasks in this study , the   importance of context in physical reasoning is not   limited to this task . Increasing the size and scope of   the datasets for contextual commonsense inference   is necessary to build language models that more   closely resemble humans and differentiate between   general commonsense and the facts at hand .   Limitations   The main limitation of our method is that it requires   human effort to increase the variety of templates ,   which makes it difficult to create large datasets .   Using templates to generate data reduces the time   required to create data manually , but the need for   human labor remains an obstacle . To resolve this ,   the templates themselves need to be generated auto - matically , although the tags that constrain the nouns   also need to be generated automatically , which is a   difficult problem .   Acknowledgment   We would like to thank anonymous reviewers for   their valuable comments and suggestions . This   work was supported by JST PRESTO Grant Num-   ber JPMJPR20C4 and JSPS KAKENHI Grant   Number 21H03502 .   References607608   A Experiment Details   We used a language model published on hugging   face Transformers ( Wolf et al . , 2020 ) except GPT-   3 under MIT ( RoBERTa ) or Apache-2.0 ( BERT ,   ALBERT , T0 , T0++ ) license . For GPT-3 , the Ope-   nAI API ( text - davinci-003 ) is used . All of these   models are designed to solve downstream natural   language tasks . Table 4 lists the paths for accessing   the models via hugging face .   We use a GPU Tesla V100 - PCIE-32 GB . The   total computation time was 1 hour for the masked   language models and 2 hours for the generative   models .   B Human Evaluation Details   We evaluate human accuracy in a size comparison   task using Amazon Mechanical Turk . We providethe following instructions and let the crowdwork-   ers choose their answers : We calculate the reward   as $ 15 per hour . Figure 2 shows the instructions   for the contextualized size comparison task . The   choices are virtually two - option questions , except   “ I ca n’t imagine the situation , ” etc . Figure 3 shows   the instructions for the non - contextualized size   comparison task . The choices are “ obj1”,“obj2 , ”   and “ N / A ( can not determine ) . ”   No personal information is obtained . Crowd-   workers live in the United Kingdom , the United   States , and Canada . By accepting Amazon Me-   chanical Turk ’s participation agreement , crowd-   workers consent to the collection and use of non-   personal data for research purposes.609610ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   7   /squareA2 . Did you discuss any potential risks of your work ?   The paper is about the simple task of comparing the sizes of two objects , and we believe there is no   such risk .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1 and Abstract   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   Section 3,4 , Appendix A   /squareB1 . Did you cite the creators of artifacts you used ?   Section 1,4   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Appendix A   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Appendix A   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Section 3   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Section 3 , Appendix A   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Section 3   C / squareDid you run computational experiments ?   4 , Appendix A , B   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Appendix A611 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   4 , Appendix A   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   No response .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Appendix A   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   4 , Appendix C   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   Appendix C   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   Appendix C   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   Appendix C   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   The task is a simple task of comparing the sizes of two objects and obviously does not pose any   problems with user safety , health , or personal information .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   Appendix C612