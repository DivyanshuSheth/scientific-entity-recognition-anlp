  EunJeong Hwang , Jay - Yoon Lee , Tianyi Yang , Dhruvesh Patel , Dongxu Zhang   & Andrew McCallum   College of Information and Computer Science , University of Massachusetts Amherst   { ehwang , jaylee , tianyiyang , dhruveshpate ,   dongxuzhang,mccallum}@cs.umass.edu   Abstract   To understand a story with multiple events ,   it is important to capture the proper relations   across these events . However , existing event   relation extraction ( ERE ) frameworks regard   it as a multi - class classiÔ¨Åcation task , and do   not guarantee any coherence between differ-   ent relation types . For instance , if a phone   linedied after storm , then it is evident that   thestorm happened before the died . Current   frameworks of event relation extraction do not   guarantee this anti - symmetry and thus enforce   it via a constraint loss function ( Wang et al . ,   2020 ) . In this work , we propose to modify   the underlying ERE model to guarantee coher-   ence by representing each event as a box rep-   resentation ( BERE ) without applying explicit   constraints . Our experiments show that BERE   has stronger conjunctive constraint satisfaction   while performing on par or better in terms   ofFcompared to previous models with con-   straint injection .   1 Introduction   A piece of text can contain several events . In order   to truly understand this text , it is vital to understand   the subevent and temporal relationships between   these events.(Mani et al . , 2006a ; Chambers and Ju-   rafsky , 2008 ; Yang and Mitchell , 2016 ; Araki et al . ,   2014 ) . Both temporal as well as subevent relation-   ships between events satisfy transitivity constraints .   For instance , in the paragraph , ‚Äú There was a storm   in Atlanta in the night . All the phone lines were   dead the next morning . I was not able to callfor   help . ‚Äù , the event marked by dead occurs after storm   and the event calloccurs after dead . Hence , by tran-   sitivity , a sensible model should predict that storm   occurs before call . In general , predicting the re-   lationships between different events in the same   document , such that these predictions are coherent ,   is a challenging task ( Xiang and Wang , 2019).While previous works utilizing neural methods   provide competitive performances , these works em-   ploy multi - class classiÔ¨Åcation per event - pair inde-   pendently and are not capable of preserving logical   constraints among relations , such as asymmetry   and transitivity , during training time ( Ning et al . ,   2019 ; Han et al . , 2019a ) . To address this problem   Wang et al . ( 2020 ) introduced a constrained learn-   ing framework , wherein they enforce logical co-   herence amongst the predicted event types through   extra loss terms . However , since the coherence is   enforced in a soft manner using extra loss terms ,   there is still room for incoherent predictions . In   this work , we show that it is possible to induce co-   herence in a much stronger manner by representing   each event using a box ( Dasgupta et al . , 2020 ) .   We propose a Box Event Relation Extraction   ( BERE ) model that represents each event as a prob-   abilistic box . Box embeddings ( Vilnis et al . , 2018 )   were Ô¨Årst introduced to embed nodes of hierar-   chical graphs into Euclidean space using hyper-   rectangles , which were later extended to jointly   embed multi - relational graphs and perform logical   queries ( Patel et al . , 2020 ; Abboud et al . , 2020 ) . In   this paper , we represent an event complex using   boxes ‚Äì one box for each event . Such a model en-   forces logical constraints by design ( see Section   3.2 ) . Consider the example in Figure 1 . Event dead   ( e ) follows event storm ( e ) , indicating eis child   ofe . Boxes can represent these two events as sep-   arate representations and by making eto contain   the boxe , which not only preserve their seman-   tics , but also can infer its antisymmetric relation   that eventeis a parent of event e. However , the   previous models based on pairwise - event vector   representations have no real relation between repre-   sentations ( e;e)and(e;e)that can guarantee   the logical coherence .   Experimental results over three datasets , HiEve ,   MATRES , and Event StoryLine ( ESL ) , show that   our method improves the baseline ( Wang et al . ,2352020 ) by 6.8 and 4.2 Fpoints on single task and   by 0.95 and 3.29 Fpoints on joint task over sym-   metrical dataset . Furthermore , our BERE model   decreases conjunctive constraint violation rate by   8588 % on a single - task models compared to plain   vector model , and by 38 % on joint - task model com-   pared to constraint - injected vector model . We show   that handling antisymmetric constraints , that ex-   ist among different relations , can satisfy the inter-   wined conjunctive constraints and encourage the   model towards a coherent output across temporal   and subevent tasks .   2 Background   Task description Given a document consist-   ing of multiple events e;e;:::;e , we wish   to predict the relationship between each event   pair(e;e ) . We denote by r(e;e)the relation   between event pair ( e , e ) . Its values are de-   Ô¨Åned in the label space { P -C , C -   P , C , N } for subevent relation-   ship ( HiEve ) and { B , A , E ,   V } for temporal relationship ( MATRES ) .   Both subevent and temporal relationships have four   similar - category relationship labels where the Ô¨Årst   two labels , ( P -C , C -P ) and   ( B , A ) hold reciprocal relationship , the   third label ( C andE ) occurs when it is   hard to tell which of the Ô¨Årst two labels that event   pair should be classiÔ¨Åed to . Lastly , the last label   ( N andV ) represents a case when an   event pair is not related at all .   Box embeddings A boxb = Q[b;b ]   such thatbRis characterized by its min and   max endpoints b;b2R , withb < b8i .   In the probabilistic gumbel box , these min and max   points are taken to be independent gumbel - max   and gumbel - min random variables , respectively .   As shown in Dasgupta et al . ( 2020 ) , if bandc   are two such gumbel boxes then their volume and   intersection is given as :   wherel(x;y ;  ) =  log(e+e ) ,  is the tem-   perature , which is a hyperparameter , and   is theEuler - Mascheroni constant .   Logical constraints We deÔ¨Åne symmetry and   conjunction constraints of relations . Symmetry   constraints indicate the event pair with Ô¨Çipping or-   ders will have the reversed relation . For example ,   ifr(e;e ) = P -C ( B ) , then   ~r(e;e ) = C -P ( A ) . Given any   two events , eande , the symmetry consistency is   deÔ¨Åned as follows :   ^r(e;e)$~r(e;e ) ( 1 )   whereris the relation between events , the Eis the   set of all possible events and the Ris the set of   relations , in which symmetry constraints hold .   Conjunctive constraints refer to the constraints   that exist in the relations among any event triplet .   The conjunctive constraint rules indicate that given   any three event pairs , ( e;e);(e;e);and(e;e ) ,   then the relation of ( e;e)has to fall into the con-   junction set speciÔ¨Åed based on ( e;e)and(e;e )   pairs ( see Appendix Table 6 ) . The conjunctive con-   sistency can be deÔ¨Åned as :   where theEis the set of all possible events , rand   rare any possible relations exist in the set of all   relationsR , ris the relation , which is speciÔ¨Åed by   randrbased on conjunctive induction table , and   Dis the set of all possible relations , in which r   andrhave no conÔ¨Çicts in between . The full expla-   nation on symmetry and conjunction consistency   can be found in Wang et al . ( 2020 ) .   3BERE model   In this section , we present the proposed box model   BERE for event - event relation extraction . As de-   picted in Figure 1 , the proposed model encodes   each event eas a boxbinRbased one ‚Äôs   contextualized vector representation h. As de-   scribed in ¬ß 3.1 , the relation between ( e;e)is   then predicted using conditional probability scores   P(bjb ) = V ol(b\b)=V ol(b),P(bjb ) =   V ol(b\b)=V ol(b)deÔ¨Åned on box space . Lastly ,   ¬ß 3.2 describes loss function used to learn the pa-   rameters of the model.236   ( B)(A)(C )   Vector ( e , e ) Vector ( e , e )   BERE(e , e ) BERE(e , e )   Parent - ChildChild - Parent CoRef NoRel   3.1 Inference rule on conditional probability   Notice that given two boxes bandb , a higher   value ofP(bjb)(resp . P(bjb ) ) implies that box   bis contained in b(resp.bcontained in b ) .   Moreover , other than complete containment in ei-   ther direction , there are other two prominent con-   Ô¨Ågurations possible , i.e. one where b , boverlap   but none contains the other , and the one where   b , bdo not overlap . It is possible to capture   all four conÔ¨Ågurations by comparing the values   ofP(bjb)andP(bjb)with a threshold . Fig-   ure 1(B ) states our classiÔ¨Åcation rule formulated   based on this observation . With this formula-   tion we have the desired symmetry constraint , i.e. ,   r(e;e ) = P -C()r(e;e ) =   C -P , satisÔ¨Åed by design .   3.2 Loss functions for training   BCE loss As we require two dimensions of scalar   P(bjb)andP(bjb)to classifyr(e;e ) , and for   ease of notation , we deÔ¨Åne our label space with   2 - dimensional binary variable yas shown in   Figure1(b ) . Where y = I(P(bjb) ) and   y = I(P(bjb) ) whereI()stands for   indicator function . Now given batch B , BCE loss   ( L ) is deÔ¨Åned as :   Pairwise loss Motivated from previous papers   using pairwise features to characterize relations ,   we also incorporate a pairwise box into our learn-   ing objective , and only in learning time , to en-   courage relevant boxes to be concentrated together . For the event - pair representation , two contextu-   alized event embeddings ( h;h)are combined   as[h;h;h  h]where  represents element-   wise multiplication . Then , a multi - layer perceptron   ( MLP ) is used to transform pairwise vectors to   box representations b. The pairwise features we   use here are similar to ( Zhou et al . , 2020 ) except   that we do not use subtraction in order to preserve   symmetry between pairwise features of ( e;e)and   ( e;e ) , i.e.b = b. For two related events , we   enforce the intersection of corresponding boxes   b\bto be inside the pairwise box . For irrelevant   event pairs such as having N orV , their   intersection and pairwise boxes are forced to be   disjoint . The pairwise loss Lis deÔ¨Åned as :   whereRis a set of irrelevant relations , such as   N andV , andRstands for comple-   ment set of R , i.e. all the set of relations that   indicates two events have some relation .   In the remainder of the paper , BERE refers to a   model trained with loss LandBERE - p refers to   a model trained with two losses L;Lcombined .   4 Experiments   In this section , we describe datasets , baseline meth-   ods , and evaluation metrics . Lastly , we provide   experimental results and a detailed analysis of logi-   cal consistency .   4.1 Experimental Setup   Datasets Experiments are conducted over three   asymmetrical event relation extraction corpus,237   HiEve ( Glava≈° and ≈†najder , 2014 ) , MATRES ( Ning   et al . , 2018 ) , and Event StoryLine ( ESL ) ( Caselli   and V ossen , 2017 ) . Table 1 shows a brief summary   of dataset statistics . HiEve consists of 100 articles   and the narratives in news stories are represented   as event hierarchies . The annotations include   subevent and coreference relations . MATRES is a   four - class temporal relation dataset , which contains   275 news articles drawn from a number of different   sources . Event StoryLine ( ESL ) corpus is a dataset   that contains 258 news documents and includes   event temporal and subevent relations . The ESL   dataset is deÔ¨Åned differently compared to HiEve   and MATRES , so we mapped the ESL labels into   the labels in HiEve similar to ( Wang et al . , 2020 )   as shown in Table 2 .   For creating symmetrical dataset , we augment   P -C andC -P ( B   and A ) pairs by their reversed relations   C -P andP -C ( A and   B ) , respectively .   Baseline We compare our BERE , BERE - p   against the state - of - the - art event - event relation ex - traction model proposed by ( Wang et al . , 2020 ) .   This model utilizes RoBERTa with frozen parame-   ters and further trains BiLSTM to represent text in-   puts into vector h(fore ) and then further utilizes   MLP to represent pairwise representation vfor   ( e;e ) . Givenv , vector model ( Vector ) simply   computes softmax over projected logits to produce   probability for every possible relations . On top of   this , as ( Wang et al . , 2020 ) showed that constraint   injection improves performance , we also compare   with the constraint - injected model ( Vector - c ) .   For a fair comparison , we utilize the same   RoBERTa + BiLSTM + MLP architecture for pro-   jecting event to box representation .   Metrics Following the same evaluation setting in   previous works , we report the micro- Fscore of   all pairs , except V pairs , on MATRES ( Han   et al . , 2019b ; Wang et al . , 2020 ) . On HiEve and   ESL , the micro- Fscore of P -C and   C -P pairs is reported ( Glava≈° and ≈†na-   jder , 2014 ; Wang et al . , 2020 ) .   4.2 Results and Discussion   Impact of pairwise box , Table 3 We Ô¨Årst show   the results of the BERE andBERE - p with and with-   out pairwise loss . The model with pairwise loss   shows about 2.8 Fpoint improvement on HiEve   and 1 Fpoint improvement on MATRES . It in-   dicates that promoting the relevant event pairs to   mingle together in the geometrical space is helpful   and it is particularly useful when most of the rela-   tion extraction model encodes individual sentences   independently .   Vector - based vs. Box - based , Table 4 Table 4   shows a comparison of our box approach to the   baseline with the ratio of symmetric and conjunc-   tive constraint violations . Our approach clearly   outperforms the baseline methods on symmetric   evaluation with a gain of 6.79 , 4.26 , and 9.34 F   points on the single task over HiEve , MATRES ,   and ESL datasets , respectively and with a gain of   0.95 and 3.29 Fpoints on the joint task over HiEve   and MATRES . The performance gains from asym-   metrical to symmetrical datasets with BERE - p are   much larger compared to the increase of Vector s.   This demonstrates the BERE - p successfully cap-   tures symmetrical relations , while previous vec-   tor models do not . In addition , it is noteworthy   that our method without constrained learning ex-   celsVector - c , which is trained with constrained   learning . This suggests that the inherent ability to238   model symmetrical relations helps satisfy the in-   tertwined conjunctive constraints , thus producing   more coherent results from a model . See Appendix   E for constraint violation statistics for asymmetric   dataset .   Constraint Violation Analysis , Table 8 ( Ap-   pendix ) We analyze constraint violations for   each label from both HiEve and MATRES . For   label pairs from the same dataset , our approach   excels in almost every cases . For label pairs across   datasets , our approach also shows fewer or similar   levels of violation . This further indicates , with-   out explicitly injecting constraints into objectives ,   our model can persist logical consistency among   different relations .   5 Ablation Study   We conduct additional experiments to see whether   Vector trained with the augmented symmetri-   cal dataset will affect the conclusion of BERE - p .   The results in Table 5 reconÔ¨Årm the BERE - p ‚Äôs su-   perior ability in handling constraints with better   performance , while Vector requires signiÔ¨Åcantly   longer training time due to the extended training   dataset with worse performance . We also note that   training Vector with the augmented symmetrical   dataset does not help with conjunctive constraint   violations ( 6:17!6:70 ) , although it reduces sym-   metrical constraint violations ( 24:08!12:01).6 Conclusion   We propose a novel event relation extraction   method that utilizes box representation . The pro-   posed method projects each event to a box represen-   tation which can model asymmetric relationships   between entities . Utilizing this box representation ,   we design our relation extraction model to han-   dle antisymmetry between events of ( e;e)and   ( e;e)which previous vector models were not ca-   pable of . Through experiments on three datasets ,   we show that the proposed method not only free of   antisymmetric constraint violations but also have   drastically lower conjunctive constraint violations   while maintaining similar or better performance   inF. Our model shows that box representation   can provide coherent classiÔ¨Åcation across multi-   ple event relations and opens up future research   for box representations in event - to - event relation   classiÔ¨Åcation .   7 Acknowledgement   This work is based upon work supported in part   by the Center for Data Science and the Center   for Intelligent Information Retrieval , and in part   by the National Science Foundation under Grant   Nos . 1763618 and 1514053 , and in part by the   Chan Zuckerberg Initiative under the project ‚Äú Sci-   entiÔ¨Åc Knowledge Base Construction ‚Äù . Some of   the work reported here was performed using high   performance computing equipment obtained under   a grant from the Collaborative R&D Fund managed   by the Massachusetts Technology Collaborative .   References239240241A Hyperparameters   We utilize 768 dimensional pretrained RoBERTa   model to compute word embeddings for events .   models are trained for 100 epochs with AMSGrad   optimizer and the learning rate is set to be 0.001 .   On HiEve and ESL , we sample N in trainset   using downsample ratio , which is Ô¨Åxed to 0.015 ,   and the downsample ratio for valid and test sets is   Ô¨Åxed to 0.4 . This is to encourage the models to   learn and evaluate all types of relations that exist   in the datasets when N overwhelmingly rep-   resents the dataset . We use three weights , ; ;   and , to balance our three learning objectives L ,   L , andL(see Section 3.2 and Appendix B ) , in   which the weights are selected between 0.1 and 1 .   A threshold for HiEve is selected between -0.4   and -0.3 and a threshold for MATRES is chosen   between -0.7 and -0.6 . We use wandb ( Biewald ,   2020 ) tool for efÔ¨Åcient hyperparameter tuning .   B Conjunctive Consistency Loss   With consistency requirements on conjunctive   relations over temporal and subevent datasets   ( as shown in Table 6 ) , we incorporate the   loss function introduced by ( Wang et al . , 2020 )   into our box model to handle conjunctive con-   straints . Three events are grouped into three   pairs , ( e1;e2);(e2;e3)and(e1;e3 ) , and the re-   lation score for each class is calculated based on   conditional probabilities and its binary logits . With   the relation labels deÔ¨Åned for each class ( see Sec-   tion 3.2 ) , the relation score , r(e;e ) , is calculated   as :   r = ylogP(bjb ) + ylogP(bjb)(2 )   wherey = I(P(bjb) ) andy=   I(P(bjb) ) andyandyare the Ô¨Årst   and second binary logits in relation label , respec-   tively . Using this relation score , we now deÔ¨Åne the   loss function for modeling conjunction constraints :   L = X   jLj+X   jLj ; ( 3 )   where the two transitivity losses are deÔ¨Åned as   Table 7 presents the results of BERE - p com-   bined with the above learning objective , denoted as   BERE - c . Compared to the results from BERE - p , BERE - c shows a signiÔ¨Åcantly smaller ratio of   constraint violations than BERE - p , while sacri-   Ô¨Åcing Fby2 point from the performance with   BERE - p .   C Vector model architecture   Refer to Figure 2 for architecture of previous vector   models . charged killed   D Detailed analysis on conjunctive   constraint violation   Constraint Violation Analysis , Table 8 We   further break down constraint violations for each   label on HiEve and MATRES . The comparison   of constraint violations between the vector model   with constrained learning ( Vector - c ) and the   box model without constrained learning ( BERE - p )   is shown in Table 8 . " n / a " refers to no predictions   and this frequently appears on C andE   due to their sparsity in the corpus . Our approach   shows a smaller ratio of constraint violations in   most of the categories , with only a few exceptions .   2nd and 3rd quadrants ( HiEve ! MATRES and   MATRES!HiEve ) stand for cross - category ,   while 1st and 4th quadrants ( HiEve ! HiEve   and MATRES!MATRES ) stand for the same-   category . Interestingly , our approach without any   injected constraints shows a smaller or similar   ratio to Vector - c in the cross - category as well   as in the same - category . We calculated r=   total # of cross - category const - violations   total # of cross - category event tripletsand   r = total # of same - category const - violations   total # of same - category event triplets ,   where const - violations refers to constraint vi-   olations.rforVector - c is 6.26 % and for   BERE - p is 4.55 % and rforVector - c is 0.05 %   and for BERE - p is 0.017 % . This conÔ¨Årms the242   effectiveness of having boxes in handling logical   consistency among different relations .   E Symmetric and conjunctive constraint   violations over origianl data   Table 9 shows the Fand symmetry and con-   junctive constraint violation results over original   dataset . The results of symmetry and conjunctive   constraint violations conÔ¨Årm our expectation and   exhibit a similar observation from Table 4.F Related Work   F.1 Event - Event Relation Extraction   This task has been traditionally modeled as a pair-   wise classiÔ¨Åcation task with hand - engineered fea-   tures and early attempts applied conventional ma-   chine learning methods , such as logistic regressions   and SVM ( Mani et al . , 2006b ; Verhagen et al . ,   2007 ; Verhagen and Pustejovsky , 2008 ) . Later   works utilized a structured learning ( Ning et al . ,   2017 ) and neural methods to characterize relations .   The neural methods have been shown effective and   ensure logical consistency on relations through in-   ference step ( Dligach et al . , 2017 ; Ning et al . , 2018 ,   2019 ; Han et al . , 2019a ) . More recent works pro-   posed a constrained learning framework , which fa-   cilitates constraints during training time ( Han et al . ,   2019b ; Wang et al . , 2020 ) . Motivated by these   works , we propose a box model to automatically   handle inherent constraints without heavily relying   on constrained learning across two different tasks .   F.2 Box Embeddings   Box embeddings ( Vilnis et al . , 2018 ) were intro-   duced as a shallow model to embed nodes of hier-   archical graphs into euclidean space using hyper-   rectangles , which were later extended to jointly   embed multi - relational graphs and perform logical   queries ( Patel et al . , 2020 ; Abboud et al . , 2020 ) .   Recent works have successfully used box represen-   tations in conjunction with neural networks to rep-   resent input text for tasks like entity typing ( Onoe   et al . , 2021 ) , multi - label classiÔ¨Åcation ( Patel et al . ,   2022 ) , natural language entailment ( Chheda et al . ,   2021 ) , etc . In all these works , the input is rep-243   resented using a single box by transforming the   output of the neural network into a hyper - rectangle .   In this paper , we take this a step forward by rep-   resenting the input event complex using multiple   boxes . Our single box model represents each even   in an input paragraph using a box and the pairwise   box model adds on top of these , one box each for   every pair of events ( see section 3.2).244