  Haolin DengYanan ZhangYangfan ZhangWangyang Ying   Changlong YuJun Gao Wei WangXiaoling Bai   Nan YangJin MaXiang ChenTianhua ZhouTencentHKUSTTsinghua UniversityUSTC   hldeng028@gmail.com , { yananzhang , devinbai}@tencent.com   Abstract   Event extraction ( EE ) is crucial to downstream   tasks such as new aggregation and event knowl-   edge graph construction . Most existing EE   datasets manually define fixed event types and   design specific schema for each of them , fail-   ing to cover diverse events emerging from the   online text . Moreover , news titles , an im-   portant source of event mentions , have not   gained enough attention in current EE re-   search . In this paper , We present Title2Event , a   large - scale sentence - level dataset benchmark-   ing Open Event Extraction without restrict-   ing event types . Title2Event contains more   than 42,000 news titles in 34 topics collected   from Chinese web pages . To the best of our   knowledge , it is currently the largest manually-   annotated Chinese dataset for open event ex-   traction . We further conduct experiments on Ti-   tle2Event with different models and show that   the characteristics of titles make it challenging   for event extraction , addressing the significance   of advanced study on this problem . The dataset   and baseline codes are available at https://   open-event-hub.github.io/title2event .   1 Introduction   Event extraction ( EE ) is an essential task in infor-   mation extraction ( IE ) , aiming to extract structured   event information from unstructured plain text . Ex-   tracting events from news plays an important role   in tracking and analyzing social media trending ,   and facilitates various downstream tasks including   information retrieval ( Basile et al . , 2014 ) , news rec-   ommendation system ( Raza and Ding , 2020 ) and   event knowledge graph construction ( Gottschalk   and Demidova , 2018 ; Yu et al . , 2020 ; Gao et al . ,   2022 ) . Figure 1 shows an example of extracting   events from multiple news titles . Based on the   extracted events , news reporting the same event   could be aggregated and sent to users to provide   comprehensive views from different sources . Figure 1 : An example of event extraction on news titles   where all factual events are extracted . Similar events   are identified ( in blue color ) and could be used in aggre-   gating relevant news .   Event extraction can be categorized into two   levels : sentence - level EE and document - level EE .   Sentence - level EE identifies event entities and at-   tributes in a single sentence ( Ahn , 2006 ) , while   document - level EE aims to extract entities of the   same event scattered across an article ( Sundheim ,   1992 ) . In scenarios such as news aggregation ,   human - written news titles often preserve the core   information of the news event , while news arti-   cles may contain too many trivial details . There-   fore , performing sentence - level EE on news titles   is more efficient than document - level EE on news   articles to aggregate relevant news .   However , most EE models trained on traditional   sentence - level datasets could not reach ideal per-   formance when extracting events from titles ( Chen   et al . , 2015 ; Nguyen and Nguyen , 2019 ; Wadden   et al . , 2019 ; Du and Cardie , 2020 ; Li et al . , 2020a ;   Liu et al . , 2020 ; Lu et al . , 2021 ; Lou et al . , 2022 ) .   On the one hand , these models request predefined   event types and a specific schema for each of them .   Each event schema consists of manually designed   argument roles such as event trigger , person , time ,   and location . Then the extraction of events will6511   be decomposed into sub - tasks of extracting each   argument role separately . Despite the success in   traditional EE , the manual design of specific event   schema is costly and time - consuming , and the lim-   ited predefined event types could not handle a great   variety of events emerging from the Internet where   most news titles nowadays are derived from . On   the other hand , extracting events from Chinese   titles could be more challenging than traditional   sentence - level EE such as the ACE 2005 bench-   mark . This is because some unique writing styles   are observed in news titles on Chinese social me-   dia , as shown in Figure 2 . First , the writing of   many titles does not strictly obey the correct gram-   mar . For example , some titles will omit the agent   when describing an action for brevity , while others   may place the action before the first mention of the   agent for emphasis . Second , the role overlap prob-   lem , i.e. , the same entity may play different roles   in multiple events , usually occurs when the events   in the text have certain associations with each other .   Although there are about 10 % events in ACE 2005   having this problem , it has not gained enough re-   search attention for quite a long time ( Yang et al . ,   2019 ) . However , the role overlap problem is muchmore commonly observed in news titles , and thus   becomes an issue that can not be ignored . Finally ,   due to the diverse coverage of news reports , there   are some cases in which the EE models have to rely   on certain domain knowledge ( e.g. rules and terms   in sports ) for correct event understanding . All these   characteristics of titles bring additional challenges   to event extraction , demanding EE models of the   greater capability of text understanding .   Considering the significance of title event ex-   traction and a lack of corresponding benchmarks ,   we present Title2Event , a dataset with more than   42,000 Chinese titles collected from the Internet .   In general , Title2Event has the following important   features :   1.It formulates title event extraction as an   open event extraction ( OpenEE ) task with-   out any predefined event type or specific   schema . Instead , it follows the formula-   tion of open information extraction ( Ope-   nIE ) ( Zhou et al . , 2022 ) and defines an event   as a ( Subject , Predicate , Object ) triplet .   Then , the EE models are required to extract   all event triplets in a given title . The biggest   difference between OpenEE and OpenIE is   that OpenEE is event - centric , which means6512only triplets of events are to be extracted .   2.It is a large - scale , high - quality dataset . Ti-   tle2Event consists of 42,915 news titles in 34   domains collected from Chinese web pages ,   along with 70,947 manually annotated event   triplets containing 24,231 unique predicates .   We write detailed annotation guidelines and   conducted two rounds of expert review for   quality control . To the best of our knowledge ,   Title2Event is currently the largest manually   annotated Chinese dataset for OpenEE .   3.It is the first sentence - level dataset with a spe-   cial focus on titles with its unique values and   challenges that little attention has been paid to .   We believe Title2Event could further facilitate   current EE research in real - world scenarios .   We experiment with different methods on Ti-   tle2Event and analyze their performance to address   the challenges of this task .   2 Related Work   Event Extraction Datasets . Automatic Content   Extraction ( ACE 2005 ) ( Doddington et al . , 2004 )   is one of the most widely - used corpora in event   extraction . It contains 599 documents with 8 event   types , 33 event subtypes , and 35 argument roles   in English , Arabic and Chinese ( Li et al . , 2021b ) .   TAC KBP 2017is a dataset of the event track-   ing task in KBP which contains 8 event types and   18 event subtypes in English , Chinese and Span-   ish . MA VEN ( Wang et al . , 2020 ) collects 4,480   Wikipedia documents , 118,732 event mention in-   stances and constructs 168 event types . Despite   the large scale , MA VEN merely focuses on event   triggers without annotating event arguments . All   of the above datasets manually define event types   and schema , struggling to handle newly emerging   event types in real - world applications .   Open Information Extraction . Open informa-   tion extraction ( OpenIE ) aims to extract facts in   the form of relational tuples from unstructured   text without restricting target relations , reliev-   ing human labor of designing complex domain-   dependent schema ( Niklaus et al . , 2018 ) . Due   to the release of large - scale OpenIE benchmarks   such as OIE2016 ( Stanovsky and Dagan , 2016 ) andCaRB ( Bhardwaj et al . , 2019 ) , neural OpenIE ap-   proaches become popular ( Zhou et al . , 2022 ) . Exist-   ing neural OpenIE models can be categorized into   sequence tagging models ( Stanovsky et al . , 2018 ;   Kolluru et al . , 2020a ; Zhan and Zhao , 2020 ) and   generative sequence - to - sequence models ( Cui et al . ,   2018 ; Kolluru et al . , 2020b ) . We adopt the formu-   lation of OpenIE and represent events as triplets   since the event mentions in news titles tend to be   brief without complex substructures .   Chinese Event Extraction . Chinese event extrac-   tion can be regarded as a special case of EE due to   its unique linguistic properties and challenges ( Li   et al . , 2021b ) . However , the resources of Chinese   EE data are relatively scarce and lack sufficient cov-   erage comparing with EE data in English , which   greatly hinders existing research ( Zeng et al . , 2016 ;   Lin et al . , 2018 ; Ding et al . , 2019 ; Xiangyu et al . ,   2019 ; Xu et al . , 2020 ; Cui et al . , 2020 ) . Apart   from multilingual datasets with Chinese corpora   such as ACE 2005 and TAC KBP 2017 , Chinese   Emergency Corpus ( CEC)collects 6 types of com-   mon emergency events . Doc2EDAG ( Zheng et al . ,   2019 ) and FEED ( Li et al . , 2021a ) are two Chinese   financial EE datasets built upon distant supervi-   sion . DuEE ( Li et al . , 2020b ) is a document - level   EE dataset with 19,640 events categorized into 65   event types , collected from news articles on Chi-   nese social media . Compared with DuEE , our Ti-   tle2Event dataset is larger in scale and does not   restrict event types .   3 Dataset Construction   This section describes the process of data collection   and annotation details .   3.1 Data Collection   We broadly collect Chinese web pages from Jan-   uary to March 2022 using the web crawler logs of   the search engine of Tencent as well as a proven   business tool to select web pages containing event   mentions ( most of them are from news websites ) .   Afterwards , the titles of the selected web pages are   extracted and automatically tagged with our prede-   fined topics , and titles containing toxic contents are   all removed . To ensure the diversity of events , we   conduct data sampling every ten days during the   crawling period , reducing the occurrence of events   belonging to the top frequently appeared topics6513to make the distribution of topics more balanced .   Eventually , around 43,000 instances are collected .   3.2 Annotation Framework   Annotation Standard . We summarize some es-   sential parts of our annotation standard . In general ,   we expect each event could be represented by a   ( Subject , Predicate , Object ) triplet where the   subject and object could be viewed as the argu-   ment roles of the event triggered by the predicate .   Multiple event triplets may be extracted from a sin-   gle title , and they may have some overlaps . How-   ever , the predicate of an triplet is considered as the   unique identifier of an event , thus multiple triplets   of a single title will not share the same predicate .   Some important specifications are listed below :   1 ) We define event as an action or a state of   change which occurs in the real world . Some state-   ments such as policy notifications or some subjec-   tive opinions are not considered as events . Also , if   an title is not clearly expressed , or is concatenated   by several unrelated events ( e.g. news round - up ) ,   then it should be labeled as " invalid " by annotators .   2 ) We find the identification of predicates in Chi-   nese is complex , so we specify some rules to unify   them . First , if an event tends to emphasize the   state change of the subject , e.g. “ 南阳大桥通   车 ” ( Nanyang Bridge opens to traffic ) , then the   predicate will be labeled as “ 通车 ” ( open - to - traffic )   instead of “ 通 ” with “ 车 ” as the object . Second ,   for phrases with serialized verbs and dual objects ,   we integrate the direct target of the action ( i.e. the   Patient ) into the predicate expression while tak-   ing the indirect patient ( i.e. the Affectee ) ( Thomp-   son , 1973 ) as the object of the event . For exam-   ple , in “ 送孩子去学校 ” ( send kids to school ) the   predicate will be labeled as “ 送去学校 ” ( send - to-   school ) with “ 孩子 ” ( kids ) as the object . Moreover ,   we find the colon ( " ： " ) frequently plays the role   of predicate in titles , representing the meaning of   " say " , " announce " or " require " , etc . We view this   as a feature of news titles and allow annotators to   label it as the predicate .   3 ) We expect the fine - grained annotations of ar-   gument roles , which are intact yet not redundant .   All determiners and modifiers of entities are kept   only if they largely affect the understanding of   events . All triplets are required to have a subject   and a predicate , while the object could be omitted   as in the original text . Crowdsourced Annotation . We cooperate with   crowdsourcing companies to hire human annota-   tors . After multi - rounds of training in three weeks ,   27 annotators are selected . We pay them ￥1 per   instance . Meanwhile , four experts are participated   in two rounds of annotation checking for quality   control . For each instance , a human annotator is   asked to write all expected event triplets indepen-   dently . To reduce the annotation difficulty , we pro-   vide some auxiliary information along with the raw   title , including the tokenization outputs , to help an-   notators quickly capture the entities and concepts   present in the titles . Note that we do not force anno-   tators to strictly obey the tokenization outputs , as   we find that many of them do not match the desired   granularity of triplet elements under our criteria .   Instead , the annotation is conducted in a < text ,   label > pair paradigm rather than a token - level tag-   ging paradigm . Moreover , we provide automatic   extraction outputs as references . During the ini-   tial phase , we design an unsupervised model to   extract triplets . After 20,000 labeled instances are   collected , we train a better sequence tagging model   for the rest of annotation process . Both models   are introduced in Section 5 . Meanwhile , as titles   often contain some domain knowledge which the   annotators may not be familiar with , we allow them   to refer to search engines . To ensure the quality , we   also allow them to label an instance as " not sure " if   they are not confident enough . The crowdsourced   annotation is conducted in batches . Every batch of   annotated instances undergoes two rounds of qual-   ity checking before being integrated into the final   version of our dataset . We also develop a browser-   based web application to accelerate the annotation   process , see Appendix A.   First - round Checking . Each time the crowd-   sourced annotation of a batch is completed , it is   sent to four experts to check whether they meet the   requirements of our annotation standard . Instances   which do not pass the quality check will be sent   back for revision , attached with the specific rea-   sons for rejection . This process repeats until the   acceptance rate reaches 90 % .   Second - round Checking . Each batch of anno-   tated instances passing the first - round checking is   sent to the authors for dual check . The authors will   randomly check 30 % of the instances and send un-   qualified instances back to the experts along with   the reasons for rejection . Slight adjustments on an-   notation standard also take place in this phase . This6514   process repeats until the acceptance rate reaches   95 % .   Our annotation process encourages positive in-   teractions among the authors , the experts and the   crowdsourced annotators , which effectively helps   the annotators to understand the annotation stan-   dard and provide timely feedback .   4 Data Analysis on Title2Event   This section describes the statistics and characteris-   tics of Title2Event from various perspectives . Ta-   ble 1 shows the overview of the dataset .   Topic Distribution . The titles in the dataset can   be categorized into 34 topics , 24 of which contain   more than 100 instances . Figure 3 lists the distribu-   tion of instances belonging to different topics , see   Appendix A for detailed statistics .   Event Distribution . As shown in Table 1 , most   of the titles contain more than one event , and the   maximum number of events per title is six . We   further investigate the distribution of instances con-   taining different numbers of triggers ( i.e. predicates   for Title2Event ) , and compare our dataset with the   ACE2005 Chinese dataset ( denoted as ACE05 - zh)as shown in Figure 4 . It can be observed that   the phenomenon of multiple events per instance   is more common in Title2Event compared with   ACE05 - zh , which brings additional challenges in   event extraction .   Predicate Distribution . We also investigate the   distribution of predicates in Title2Event . Figure 5   shows the distribution of the 30 most frequent pred-   icates in the dataset .   Challenge Distribution . We further analyze to   what extent are the observed challenges described   in Figure 2 covered in Title2Event . To do this,6515we randomly sample 1,000 instances and manu-   ally annotate 1 ) Whether the instance omits or   inverts some event arguments which makes it-   self not strictly obeying the grammatical norms .   2 ) Whether there ’s a text span appearing at multiple   events of the instance . ( 3 ) Whether some domain   knowledge is crucial in understanding the instance   that without these knowledge one might not cor-   rectly identify the event arguments . The annotation   result shows that 9.70 % of sampled instances are   observed with unconventional writing , 21.50 % in-   stances have role overlap problem ( 10 % for ACE   2005 for comparison ) , and 2.80 % instances re-   quires domain knowledge for correct event under-   stating . We believe such statistics are a good iden-   tification of the challenging nature of Title2Event .   5 Methods   Formally , given a sequence of tokens S= <   w , w , . . . , w > , Open EE aims to output a list of   triplets T= < t , t , . . . , t > where each triplet   t= < s , p , o > represents an event occurred   inSands , p , odenote the subject , predicate   and object of the event respectively . The object   of an event could be empty , and the total number   of events per sentence mis not fixed . Open EE can   also be aligned with traditional EE task formulation   by considering the predicate as the event trigger as   well as a unique event type , while the subjects and   objects both taken as event arguments .   Based on the task formulation , we first imple-   ment an unsupervised method using an existing   toolkit . Then , we split the task into trigger ex-   traction and argument extraction , and implement   different supervised methods on them .   5.1 Unsupervised Method   Since the formulation of Open EE is similar to   some traditional tasks such as dependency pars-   ing ( DP ) and semantic role labeling ( SRL ) , we   investigate the performance of existing triplet ex-   traction methods on Open EE . Each title will be   segmented and tokenized first , then the extraction is   conducted as a token - wise sequence - labeling task .   Each token will first be labeled by a SRL module on   whether it belongs to a semantic role which appears   in one of the S - P - O , S - P , P - O semantic tuples . If   not , it will be relabeled by a DP module on whether   it appears in a syntactical tuple of the above struc-   tures . The entire method is implemented using the   LTP toolkit ( Che et al . , 2020).5.2 Trigger Extraction   Since the number of triggers per sentence is neither   fixed nor given as input , we adopt a token - level   sequence tagging model to extract all event trig-   gers in a given sentence based on the inductive bias   that event triggers ( i.e. , predicates ) will not over-   lap with each other ( see Section 3.2 ) . Sequence   tagging model requires a set of tags where each   tag , aligned with a token , represents a part of an   event element ( i.e. , a triplet element ) or a non - event   element . Then , the model learns the probability   distributions of tags for each given sentence , and   outputs triplets based on the predicted tags . We   adopt the BIO tagging scheme where a token is   tagged B - trg(I - trg ) if it is at the beginning of   ( inside ) the itrigger , or Oif it is outside any trig-   ger . The subscript is used to distinguish between   different triggers as they might be discontinuous   tokens . Since Title2Event is not annotated on token-   level ( see 3.2 ) , we perform automatic tagging by   locating each annotated event element at the source   sentence to get its offset . We use BERT ( Devlin   et al . , 2019 ) as the sentence encoder to get the   contextualized representations of tokens , and each   token representation will be fed to a classification   layer to compute the probability distribution of the   tags .   5.3 Argument Extraction   Argument extraction models take the source sen-   tence and the given triggers as input and output the   arguments of each given trigger respectively . Due   to the role overlap problem , a token might appear   in multiple event arguments and thus has multiple   tags , which does not match the common setting of   sequence tagging task . Therefore , we iterate over   the extracted triggers and extract the arguments of   each event trigger separately . We implement three   methods for argument extraction .   Sequence Tagging . The first method is a token-   level sequence tagging model similar to the trigger   extraction model , which also uses BIO tagging   scheme for subject and object tokens . During each   forward process , to specify the current trigger , we   adopt the method proposed by Yang et al . ( 2019 ) .   Specifically , the input of BERT encoder is the sum   of WordPiece embeddings , position embeddings   and segment embeddings , and we set the segment   ids of current trigger tokens being one while others   being zero to explicitly encode the current trigger.6516Methods Trigger Ex . Argument Ex . Triplet Ex .   P R F1 P R F1 P R F1   Unsuper . 21.0 32.0 25.4 12.0 15.5 13.5 4.5 6.8 5.4   SeqTag 69.5 69.9 69.7 50.8 51.2 51.0 41.1 41.3 41.2   ST - SpanMRC - - - 60.1 54.9 57.4 44.5 44.8 44.7   ST - Seq2SeqMRC - - - 57.9 58.6 58.2 49.8 50.1 49.9   Span MRC . The second method is a span - level   tagging model which formulates argument extrac-   tion as a machine reading comprehension ( MRC )   task , inspired from Du and Cardie ( 2020 ) and Liu   et al . ( 2020 ) . For each given sentence as well as   a specified trigger , the subject and object are ex-   tracted separately by prepending a question , e.g.   “ 动作 < trigger > 的主体是 ？ ” ( What is the sub-   ject of < trigger > ) ? , into the sentence to form a   context like " [ CLS ] question [ SEP ] sentence   [ SEP ] " , then the model is asked to extract the an-   swer span from the context for the given question   by predicting a start position and an end position .   We also use BERT as the context encoder .   Seq2Seq MRC . The third method is a sequence-   to - sequence MRC model with same the ques-   tion design as Span MRC . However , instead   of extracting the answer spans from the con-   text , it directly generates a sequence of tokens   as the output with the given context by maxi-   mizing the conditional probability P(Y |S ) = /producttextp(y|y , y , . . . , y;S ) , where Y= <   y , . . . , y > is the golden answer . We adopt   mT5 ( Xue et al . , 2021 ) , a multilingual text - to - text   transformer model as the context encoder as well   as the answer decoder .   6 Experiments   We conduct experiments on Title2Event with the   methods described in Section 5 and analyze their   performance .   6.1 Evaluation Metrics   We adapt the evaluation metrics used in previous   works on traditional EE tasks ( Li et al . , 2021b )   to Open EE . We first define the matching criteria :   an event trigger or argument is correctly identified   if it exactly matches the golden answer , and an   event triplet is correctly identified only if all of its   elements are correctly identified . We then compute   the precision ( P ) , recall ( R ) , and F1 - score ( F1 ) for   trigger extraction , argument extraction and triplet   extraction respectively .   6.2 Evaluation Model   We summarize all the models we implement for   experiments here :   Unsuper . The unsupervised triplet extraction   method implemented by the LTP toolkit using the   Chinese - ELECTRA - small ( Cui et al . , 2021 ) model .   SeqTag . A pipeline tagging - based model consist-   ing of a trigger extractor and an argument extractor ,   both are based on the token - level sequence tagging   model using BERT - base - Chinese as the encoder ,   and are trained separately . During inference , the   argument extractor predicts the arguments based   on the triggers predicted by the trigger extractor .   ST - SpanMRC . A pipeline model using a token-   level sequence tagging model as the trigger extrac-   tor , and a span - level MRC model as the argument   extractor , both are based on BERT - base - Chinese .   ST - Seq2SeqMRC . A pipeline model which re-   places the argument extractor with a sequence - to-   sequence MRC model using mT5 - base .   6.3 Overall Experimental Results   Table 2 shows the results of all Open EE methods   experimented on Title2Event . It can be observed   that : 1 ) For trigger extraction , the sequence tagging   model significantly outperforms the unsupervised6517   model . 2 ) For argument extraction and triplet ex-   traction , ST - Seq2SeqMRC outperforms the other   tagging - based models . A large part of the reason is   that the unconventional writing styles of titles make   it difficult to locate token - level tags or span off-   sets in the source text , while sequence - to - sequence   models are free from these restrictions .   6.4 Analysis on Error Propagation   Table 3 shows the results of argument extraction   with predicted triggers and with golden triggers .   All three models ’ performance improve by approx-   imately 20 % if provided with golden triggers , indi-   cating the huge impact of correct triggers on argu-   ment extraction and the urgent need to alleviate the   propagating error brought by pipeline architecture   in future works .   6.5 Analysis on Multiple Event Extraction   Figure 4 shows that containing multiple events per   instance is an important feature of Title2Event , thus   we further investigate the models ’ performance on   multiple event extraction , as shown in Figure 6 . We   can see that as the number of events per instance in-   creases , all models on trigger extraction , argument   extraction , and triplet extraction show a decrease in   performance , which indicates that multiple events   per instance brings additional challenges to open   event extraction .   6.6 Analysis on Different Topics   We also investigate the results of trigger extraction   and argument extraction on different topics of Ti-   tle2Event , see Appendix A for details . It can be   observed that the F1 - scores of " Weather " are higher   than other topics , probably because news titles on   weather ( forecast ) usually have a fixed template   which makes extraction easier.65186.7 Analysis on Error Cases   We summarize three typical challenges observed in   Title2Event in Section 1 . Here , we analyze some   error cases of the model outputs to further demon-   strate the issues . Figure 7 ( a ) shows an error output   in trigger extraction , where the given title is un-   conventionally written by concatenating two predi-   cates . As a result , SegTag is unable to distinguish   the two different predicates . Figure 7 ( b ) shows an   instance with multiple events and all the models   mix up the argument roles . Figure 7 ( c ) shows   a sport news title , without the background that   Real Madrid andPSG are both football clubs , none   of the models properly understand the event that   PSG is defeated by Real Madrid . All of the above   cases clearly address the challenges present in Ti-   tle2Event , which are also common in real - world   scenarios , and require advanced study to be better   solved .   7 Conclusions   In this paper , we present Title2Event , a Chinese ti-   tle dataset benchmarking the task of open event   extraction . To the best of our knowledge , Ti-   tle2Event is the largest manually - annotated Chi-   nese dataset for sentence - level event extraction . We   experiment with different methods and conduct de-   tailed analysis to address the challenges observed   in Title2Event , which are rather scarce in exist-   ing datasets yet common in real - world scenarios .   We believe Title2Event could further facilitate ad-   vanced research in event extraction .   Limitations   We summarize the limitations of Title2Event as   follows :   Evaluation Metrics . We make Title2Event a   benchmark for open event extraction with a hope   that it could evaluate the performance of domain-   general EE models . We adapt the formulation of   Open IE and represent events in a universal triplet   format while adopting traditional EE metrics which   is based on exact match . However , we observe   that the narrative of events in Chinese titles are   extremely diverse . To unify them into the triplet   format without losing the core event information ,   we design detailed annotation guidelines which re-   sults in the fact that the a large amount of triplet   elements are text spans instead of one or two to-   kens which is common in traditional EE datasets   such as ACE 2005 . Therefore , using exact matchin Title2Event might be too strict for model outputs   which are just one or two tokens different from   the golden text span . We leave the design of fine-   grained evaluation approaches to future work .   Methods . Some characteristics of Title2Event   such as unfixed number of events per instance   and the role overlap problem bring difficulties to   the model design . We adopt a pipeline architec-   ture which suffers from the error propagation prob-   lem as discussed in Section 6.4 . We also adapt   some end - to - end models in traditional EE such as   TEXT2EVENT proposed by Lu et al . ( 2021 ) to our   Open EE benchmark , but find the performance is   unexpectedly poor . We conduct preliminary analy-   sis and find that the length of text span in triplets ( as   mentioned above ) as well as the relatively complex   linearized event structures ( largely due to the mul-   tiple events per instance issue ) are the potential   factors of the limited performance . Therefore , we   do not provide a good end - to - end model as base-   line , which might make the model comparison in   Section 6 less comprehensive . However , we hope   that future works could pay more attention to the   design of text - to - structure models except from tra-   ditional tagging - based models .   Ethics Statement   As Title2Event is an Open EE dataset which   broadly collects contents of various categories on   the Internet , keeping the corpus without bias is ex-   tremely difficult . However , we put large efforts in   cleaning the toxicity of data . First , all crawled web   paged are automatically removed if they contain   toxic contents using an existing system . During   annotation , all instances will be dual checked by   the human annotators and manually deleted if not   passing the check . Moreover , in our annotation   standard , we ask annotators to label only factual   events while ignoring all subjective opinions , as we   hope Title2Event could be factual and unbiased .   References6519652065216522A Appendix   Annotation Tool . Figure 8 shows a screenshot of   our annotation web page . The raw title are given   with auxiliary information , the annotators will first   determine whether to abandon this case as well as   is this case easy to annotate or not . Then , they will   type all plausible events in the text boxes following   our annotation guidelines .   Topic List . Table 4 lists all 34 topics and their   corresponding number of instances .   Topic Count   社会(Society ) 12,341   财经(Finance ) 6,539   体育(Sports ) 5,033   时事(Current Events ) 4,499   科技(Technology ) 2,965   娱乐(Entertainment ) 1,685   教育(Education ) 1,451   汽车(Cars ) 1,319   天气(Weather ) 1,013   军事(Military ) 712   旅游(Travel ) 659   房产(Real Estate ) 647   三农(Agriculture ) 520   文化(Culture ) 501   综艺(Variety Shows ) 412   游戏(Games ) 396   电影(Movies ) 348   健康(Health ) 344   电视剧(TV Series ) 233   历史 ( History ) 220   音乐(Music ) 159   科学(Science ) 147   生活(Life ) 118   美食(Food ) 117   情感(Sentiment ) 95   育儿(Childcare ) 73   时尚(Fashion ) 60   宠物(Pets ) 57   职场(Career ) 54   曲艺(Folk Art ) 41   动漫(Animation ) 34   摄影(Photography ) 24   搞笑(Funny News ) 12   其它(Others ) 11Results on Different Topics . Figure 9 shows   the F1 - scores of trigger extraction ( using SeqTag   model ) and argument extraction with golden trig-   gers ( using SeqTag , SpanMRC , and Seq2SeqMRC   models ) on the top-10 topics in Title2Event .   Hyper - parameter Settings in Training . For all   models , we use the batch size of 32 and train them   for 30 epochs on the training set of Title2Event . All   models are trained on a single Tesla A100 GPU . We   use the linear learning rate scheduler and AdamW   as the optimizer . For models based on Bert - base-   Chinese , we set the learning rate to be 5e-5 ; For   models based on mT5 - base , we set the learning rate   to be 1e-4 . All supervised models are implemented   using the Huggingface - transformers library.65236524