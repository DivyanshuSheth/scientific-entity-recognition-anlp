  Hailin Chen , Amrita Saha , Shafiq Joty , Steven C.H. HOINanyang Technological University , SingaporeSalesforce Research   { hailin001 , srjoty}@ntu.edu.sg   { amrita.saha , shoi}@salesforce.com   Abstract   Machine learning models usually assume i.i.d   data during training and testing , but data and   tasks in real world often change over time .   To emulate the transient nature of real world ,   we propose a challenging but practical task :   text classification in - the - wild , which introduces   different non - stationary training / testing stages .   Decomposing a complex task into modular   components can enable robust generalisation   under such non - stationary environment . How-   ever , current modular approaches in NLP do   not take advantage of recent advances in pa-   rameter efficient tuning of pretrained language   models . To close this gap , we propose , a label - modular prompt tuning   framework for text classification tasks . In , the input prompt consists of a   sequence of soft label prompts , each encod-   ing modular knowledge related to the corre-   sponding class label . In two of most formidable   settings , outperforms rele-   vant baselines by a large margin demonstrating   strong generalisation ability . We also conduct   comprehensive analysis to validate whether the   learned prompts satisfy properties of a modular   representation .   1 Introduction   While NLP research has received a significant   boost in performance by employing large - scale pre-   trained language models ( PLMs ) , finetuning an   entire dedicated model for each task is not always   practical or even feasible , especially as model size   continues to grow . To alleviate this , recently there   has been increased interest in parameter - efficient   methods such as Adapter ( Houlsby et al . , 2019 )   and Prompt - based tuning methods ( Lester et al . ,   2021 ; Qin and Eisner , 2021 ; Liu et al . , 2021 ; Li   and Liang , 2021 ) . When training on a downstream   task , these methods keep the PLM frozen and onlyFigure 1 :   update a small set of parameters that are added to   the network . Among these methods,(Lester et al . , 2021 ) proposes tunable prompts   – a sequence of learnable soft tokens , and show   impressive results , even competitive to full - model   finetuning with a large PLM .   While these prompt - based methods have proven   quite effective on popular benchmarks , these stan-   dard tasks typically assume only independently and   identically distributed ( i.i.d ) data during training   and testing . However , practical cognitive tasks in   the real world are usually more complex involving   changing contexts or non - stationary environments .   Talking about what is next for NLP , Kathleen McK-   eown in a recent interview ( source ) said : “ Most   models are static . But the world changes every   minute , every second . Dealing with a dynamic   world is a new area that ’s up and coming . ”   Our work in this paper particularly concerns text   classification settings where a model is trained on1677sequence of tasks and evaluated on an arbitrary   subset of seen labels of interest . We formalize this   as a novel text classification in - the - wild task ( de-   fined in § 3 ) , which emulates the transient learning   environment of real world , e.g. , for a service re-   quiring classification , the label set might gradually   change over time to include new labels or remove   obsolete ones . Such scenarios typically result in a   sequence of non - stationary low - resource training   and evaluations over different label sets ( e.g. , train   on { chemistry , physics } and { basketball , football }   in succession and then test on { physics , football } ) .   This requires handling non - stationary data dis-   tribution which humans are quite adept at , partly   because we can decompose a complex task in a   modular fashion ( Berwick et al . , 2013 ) . For exam-   ple , when learning to classify objects , we acquire   modular knowledge exclusive to each class . This   allows us to robustly classify irrespective of any   label space manipulations such as label omission   or learning over new label spaces . This notion   of modularity at the level of each class label is   what we call label modularity and is a desirable   quality for NLP models to generalize to practical   non - stationary classification settings .   Contemporary modular model designs for com-   plex NLP tasks typically use a routing network or   programmer ( Cases et al . , 2019 ; Rosenbaum et al . ,   2019 ; Khot et al . , 2021 ; Corona et al . , 2021 ; Jiang   and Bansal , 2019 ; Liu et al . , 2019 ; Hu et al . , 2018 ;   Gupta et al . , 2020 ; Andreas et al . , 2016 ) which   learns a meaningful decomposition of the task into   sub - tasks and executes it by applying a chain of   specialised modules designed for each sub - task .   For classification tasks , this can entail learning a   specialized module for each label in the target label-   space . While there has been limited research on   utilizing modular architectures for PLMs ( Andreas   et al . , 2016 ; Chen et al . , 2020 ) , the main research   gap that we explore in this work is that of a mod-   ular design of parameter efficient tuning of large   PLMs , in particular , .   Although can be considered mod-   ular at task level in that it learns soft - prompts for   each task to support multitasking , it is not able to   learn modular decomposition within a particular   task . For non - modular designs like   or model - finetuning , text classification in - the - wild   is challenging to handle , as it requires combining   partial information from different label spaces . In   contrast , a label - modular approach should learn ex - clusive knowledge for each label and generalise to   any subset of the label set . We thus postulate two   main objectives of a label - modular model :   Objective 1 . Separable Label Representation :   Each class label should have its own representation   which compactly encodes the information from the   data belonging to that label .   Objective 2 . Prediction over Controllable Label   Space : Models should perform robustly over any   subset of the learnt label space during inference .   To meet these objectives , we propose a modu-   lar design of Prompt Tuning – Label - modular   Prompt Tuning ( ) . It decomposes   the prompt sequence into label - modular compo-   nents called label prompts , each encoding specific   knowledge corresponding to a class label . Thus   in each forward pass , we can select desired label   prompts to construct the input prompt , based on the   target label - set . To ensure that the learned knowl-   edge is encoded in a modular fashion during train-   ing , we introduce a novel subset - invariant loss over   dynamic label - sets .   To evaluate generalizability of   we construct some practical scenarios of text classi-   fication in - the - wild . We train in multiple stages   over non - overlapping label spaces and evaluate   the model on label - sets that ( i ) correspond to each   training stage ( stage - specific ) , ( ii ) is accumulated   over all learned labels ( stage - agnostic ) , and ( iii )   is comprised of labels across multiple training   stages ( stage - fused ) . We show an example of on stage - fused NER setting in Figure 1 .   The stage - agnostic and stage - fused settings are   the most challenging scenarios for typical fine-   tuned or prompt - tuned models , and specifically on   those settings we find that outper-   forms all relevant baselines by a significant mar-   gin . This alludes towards its ability to learn robust   prompt representations that is generalizable to dif-   ferent non - stationary learning environments .   We further empirically justify that indeed showcases modular properties   by analyzing its behavior when either the ground   truth or other random labels are removed from the   input or the order of label prompts is permuted .   2 Related Work   We now review methods from the literature that are   relevant to ours from different perspectives . First ,   many parameter efficient tuning methods have been   proposed such as Adapter ( Houlsby et al . , 2019),1678 ( Lester et al . , 2021 ) ,   ( Li and Liang , 2021 ) , BitFit ( Zaken et al . , 2022 ) ,   LoRA ( Hu et al . , 2021 ) and COMPACTER ( Ma-   habadi et al . , 2021a ) . They either finetune a param-   eter subset or introduce new layers or embeddings .   Transfer learning over prompts ( Vu et al . , 2022 ) and   Adapters ( Pfeiffer et al . , 2021 ) and multi - tasking   over the latter ( Mahabadi et al . , 2021b ) have also   been explored , but they are not comparable to ours   due to difference in task / problem settings .   Second , continual learning ( CL ) methods are rel-   evant as they also have sequential training stages .   Architecture based CL methods adjust model archi-   tecture for each task ( Chen et al . , 2016 ; Rusu et al . ,   2016 ; Mallya et al . , 2018 ) , but require task identi-   ties for inference . Regularization based CL meth-   ods restrain updating parameters critical to previous   tasks ( Kirkpatrick et al . , 2017 ; Zenke et al . , 2017 ;   Aljundi et al . , 2018 ) . Memory based CL methods   retain key examples from prior tasks ( Lopez - Paz   and Ranzato , 2017 ; Chaudhry et al . , 2019 ; de Mas-   son d’Autume et al . , 2019 ; Zhu et al . , 2022 ) while   Memory generator models learn to generate and   use pseudo - data from prior tasks ( Sun et al . , 2020 ;   Qin and Joty , 2021 ) . These methods are not com-   parable to ours as we do not use any memory or   pseudo memory in our sequential training .   Third , modular networks have been shown to   perform well on out - of - domain data ( Kirsch et al . ,   2018 ; Ruder et al . , 2019 ; Alet et al . , 2018 ) and miti-   gate forgetting in continual learning ( Ostapenko   et al . , 2021 ) . Apart from routing network ap-   proaches introduced in § 1 , mixture - of - experts   ( MoE ) selects a soft subset of modules based   on model input ( Shazeer et al . , 2017 ; Fedus   et al . , 2021 ; Goyal et al . , 2021 ) . Similar to ours ,   Kudugunta et al . ( 2021 ) ; Rajendran et al . ( 2017 ) ;   Ponti et al . ( 2021 ) ; Ostapenko et al . ( 2021 ) con-   sider task level routing and support new tasks by   combining learned modules . By contrast , utilises parameter efficient finetuning   of large PLMs and support low resource settings .   3 Methodology   In this section , we first formally define the prob-   lem and subsequently present our   model , introduce subset invariant loss and explain   the framework under text classification in - the - wild .3.1 Problem Definition   Single stage text classification Assume a sin-   gle text classification domain ( or dataset ) D. Let   ( X , Y)∼ D be a sample , where X={x }   represents a text input sequence of length Land   Y={y}represents the corresponding classifi-   cation label name of length M(in tokens ) . Let Ω   denote the set of all possible class labels of inter-   est , for which we have ∀(X , Y)∼ D , cls(Y)⊆Ω.   Note that cls(Y)is a mapping which returns the   class label(s ) in Y. In case of single class classi-   fication , cls(Y)returns { Y } . In case of sequence   labelling which is token - level classification , cls(Y )   returns the set of all unique target tags in Y.   Text classification in - the - wild Assume a se-   quence of ntext classification stages with the cor-   responding training datasets D={D , ... , D } .   Each stage represents a different task in tempo-   ral dimension , with ( X , Y)∼ Ddenoting a   sample at the k - th training stage and Ωdenoting   the set of all possible class labels for D. Simi-   larly , the testing could consist of msuch datasets   D= ( D , ... , D)withΩdenoting the set of   possible class labels for D. For classification in-   the - wild , we examine three challenging yet very   practical settings . First , when m= 1 andΩ=   ∪{Ω } , we have one test dataset covering all   seen labels . We refer it as stage - agnostic testing   as the test label can come from any of the training   stages ( or tasks ) . Second , when m = nandΩ=   Ω,∀j={1 , ... , n } , we have one test dataset cor-   responding to each training stage with the same   label set . We denote this setting as stage - specific   testing as each test set evaluates the model ’s perfor-   mance on a particular task in which it was trained .   Finally , a more challenging setting where m > 1   andΩ/∈ { Ω , ... , Ω},∀j={1 , ... , m } , rather   Ω∈ P(∪{Ω})− ∪{P(Ω ) } , where   P(S)denotes the power - set of a given set S. That   is , the label set of a test stage does not correspond to   any one training stage , but is composed of partial la-   bel sets from multiple training stages ( or tasks ) . We   refer it as stage - fused testing . Note that the stage-   agnostic and stage - specific scenarios are closely   related to continual learning ( Thrun , 1996 ) , though   the latter considers access to task - id instead of intra-   task information ( i.e. , task label set ) .   3.2 Soft Prompt Tuning   LetX={x , ... , x}be an input text sequence ,   where xis the t - th token , and Mbe a pretrained1679   language model . The input text is mapped to a   sequence of embeddings H={h , ... , h}with   h∈R. A soft prompt is a sequence of Ntunable   tokens T={p , ... , p}withp∈R , that is con-   catenated with the text embedding as the final input   toM:¯H={T ⊕ H}={p , ... , p , h , ... , h } .   The model prediction is defined as P(Y|¯H;M ) =   P(Y|T , X;M ) . During training , Mis kept frozen   and only Tis updated .   3.3 Label Modular Prompt Model   We visualise the architecture of   ( Lester et al . , 2021 ) and our proposed in Figure 2 . In contrast to , ’s prompt consists of a se-   quence of label prompts , where each label prompt   contains the corresponding label name and a se-   quence of tunable soft tokens similar to soft prompt .   Formally , we denote l = e⊕ { p , ... , p}as la-   bel prompt for label k , where eis the embedding   of label k ’s text or sequence of token - embeddings   for multi - token labels , ⊕denotes concatenation   andmis the number of tunable tokens per label   prompt . The final input prompt is T=⊕l   withSbeing the set of labels of interest .   Prompt formulation The key mechanism of is prompt formulation { R , S } →   T , where Rdenotes the learned representation   space of all labels prompts . In , vari-   ables SandRdo not exist and the model training   tunesTdirectly . In , given Sas a   set of class labels of interest , we select the corre-   sponding label prompts representation from Rand   concatenate these to form the final input prompt   T. The training loss is back - propagated through   Y→ T → Rto learn the soft label prompts . Subset invariant loss The prompt formulation   { R , S } → T aims to achieve Objective 2 : pre-   diction over controllable label space ( § 1 ) . In sin-   gle domain setting , Ωis the set of all possible   class labels during training as defined by Section   3.1 . However fixing Sto a constant Ωthroughout   training will make the model susceptible to data dis-   crepancy between train and inference as Ω̸= Ω.   Thus to ensure Objective 2 , we propose to vary S   during training . We first uniformly sample the size   ofS,|S|from{1 , . . . , ( |Ω| −1)}and then ran-   domly choose |S|labels from Ωto construct S.   Such sub - sampling of Ωencourages a fair explo-   ration of different lengths of prompt sequences as   input during training , thus enabling representations   to be robust to a dynamic Ωat inference . For   each training instance , with probability pwe fix   S= Ωand vary Sas above with ( 1−p)chance .   We refer such sampling process as S∼ˆS. The   subset invariant loss is then defined as :   where 1is the Indicator function ; 1= 1   ifcls(Y)⊆S , otherwise 0 . According to Objec-   tive 1 , we expect our model to make predictions   grounded by the relevant label prompts . When S   does not contain ground truth class label(s ) in Y ,   the model should not be able to predict Yas output .   Thus we set the loss to be zero when cls(Y)⊈S   to avoid encouraging ungrounded predictions .   3.4 Classification in - the - wild with   So far , we have introduced ´ s func-   tionality under a single domain classification set-   ting . To verify our Objective 2 , we wish to exam-   ine it under text classification in - the - wild defined in1680Algorithm 1 Text Classification in - the - wild with .   § 3.1 . Given training datasets D={D , ... , D } ,   the model is trained on each dataset Dsequen-   tially , and then evaluated on three classification in-   the - wild testing settings . The pseudocode of under the continual learning setting is   given in Algorithm 1 . Note that Rin step 3 de-   notes label prompt representation of labels in Ω ,   i.e. ,R:={l∈R|k∈Ω}andRis simi-   larly defined as R:={l∈R|m∈ ∪Ω } .   Label prompt transfer In step 3 , for learning the   label prompt representation Rat any training   stage i , we first aim to transfer the label - modular   knowledge , Rlearned over the previous train-   ing stages through prompt initialization . This is   a unique learning characteristic that is facilitated   by our label - modular architecture and allows the   model to exploit semantic relatedness between la-   bels across training stages when initializing the   label prompt representation . Intuitively , if ‘ bistro ’   ∈Ωand ‘ restaurant ’ ∈Ω , then initializing the   label prompt representation of ‘ restaurant ’ with the   knowledge encoded in the learned label prompt   representation of ‘ bistro ’ should be helpful to the   model . To compute the similarity between labels   landlwithj∈Ωandk∈Ω , we use per-   token average cosine similarity sim(e , e)based   on the embeddings of the label texts . For each   label j∈Ω , we select the top- Kmost simi-   lar labels Ω⊂Ω. We then initialize l   by averaging the top- Ksimilar label prompt rep-   resentations , weighted by their normalized sim-   ilarity score : l←/summationtextαl , where   α = sim(e , e)//summationtextsim(e , e ) .   This method is similar in spirit to ( Vu et al . , 2022 ) ,   which shows good transfer for task level prompts   with training overheads , while we transfer at a finer - grained level over label prompts with no overheads .   4 Experiments   In this section , we first introduce datasets used   and data construction process ( § 4.1 ) followed by   relevant baselines ( § 4.2 ) , evaluation methods ( § 4.3 )   and implementation details ( § 4.4 ) . Through our   experiments , we target three research questions :   1.Can consolidate knowledge   over multi - stage training ? →answered in § 4.5   with stage - agnostic setting   2.Can adapt to dynamic label   space at inference ? →answered in § 4.6 with   stage - fused setting   3.How competitive is in stage-   specific setting ? →answered in § 4.7   Additionally , we perform ablations ( § 4.8 ) and quan-   titative and qualitative analysis ( § 4.9-§4.10 ) to ver-   ify modular properties of .   4.1 Tasks and Datasets   We conduct experiments on three types of NLP   tasks : News Domain Classification on Huffpost-   News ( Misra , 2018 ) , Name Entity Recognition   ( NER ) on fewNERD ( Ding et al . , 2021 ) and Rela-   tion Extraction ( RE ) on FewRel ( Han et al . , 2018 ) .   We formulate all tasks as a text - to - text problem , as   defined in § 3.1 . For News Domain Classification   and NER , we construct target text following Qin   and Joty ( 2021 ) . For RE , we concatenate the origi-   nal text and entities with a seperator ’ | ’ as the input   sequence , and use the relation type as the target .   ( Example data can be found at Appendix A )   For HuffpostNews , we subsample 100 shots per   class for training and validation and split it into   5 stages of disjoint labels . For FewNERD and   FewRel , we subsample 50 shots for training and   validation and split into 4 and 5 stages , respec-   tively . For testing , we subsample 200 , 50 , and   50 shots per class for HuffpostNews , FewNERD   and FewRel , respectively . The total number of   labels for { HuffpostNews , FewNERD , FewRel } is   { 41,64,80 } respectively , and resulting label size per   stage is { 8 - 9,16,16 } respectively .   For stage - specific testing , we follow the stages   defined for training and construct a correspond-   ing test data for each stage . For stage - agnostic   testing , we combine stage - specific test data for cur-   rent stage and all previously seen stages to con-   struct the test data . For stage - fused testing , we   construct label - sets for each fused stage such that1681it is not a subset of any single prior training stage ,   but rather contains labels from ‘ all ’ prior training   stages . We construct { 5,4,5 } fused stages for { Huff-   postNews , FewNERD , FewRel } . We conduct 5 ran-   domised trials with different data sampling and   experiment seed for all of the above settings .   4.2 Baselines   We use T5 - large ( Raffel et al . , 2020 ) as the back-   bone PLM for all methods , and consider the follow-   ing baselines to compare with our :   • ( Finetune ) , which tunes all param-   eters of the backbone PLM .   •(i ) ( PT ) from § 3.2 , ( ii ) PT-   An extension of PT to continual learning ( CL )   setting , which trains separate PT models for each   stage and concatenates the learned soft - prompts   during inference , based on the test label - set .   •Adapter , a parameter efficient tuning alternative   introduced in ( Houlsby et al . , 2019 ) , which in-   serts light adapter layers into the backbone PLM   and only tune them .   As text classification in - the - wild overlaps with con-   tinual learning , we also compare with versions of   the above baselines that use architecture - agnostic   methods and settings relevant to the latter .   •Online regularization based methods : ( i ) A scal-   able online version of EWC ( Kirkpatrick et al . ,   2017 ) proposed in ( Schwarz et al . , 2018 ) , and   ( ii ) Online MAS ( Aljundi et al . , 2018 ) . These   methods measure each parameter ’s importance to   previous tasks by fisher information , and restrict   updating previously important parameters when   learning a new task , to mitigate catastrophic for-   getting .   •Multitask model , which involves training on all   stages simultaneously , not sequentially . This is   infact an oracle method for stage - agnostic test-   ing and can be considered as an upper bound of   memory - based methods in continual learning .   4.3 Evaluation Methods   For all the three NLP tasks , we consider an exact   match as a correct prediction and report accuracy   for News Classification and RE , and compute F1-   score over the BIO format for the NER task . By   default , we do not apply any other post - processing   or verbalizer , though these are orthogonal methods   that can be separately used to enhance any of the   discussed models . In the stage - fused setting , weapply constrained decoding similar to ( Cao et al . ,   2021 ) to selected baselines , marked by special indi-   cator * ( e.g. , Finetune ) . For ,   we use all seen label prompts for stage - agnostic   testing and specific set of label prompts for stage-   specific and stage - fused testing . Since other base-   lines do not have label - level modularity , for stage-   agnostic and stage - fused testing , we use the check-   point after the final stage and for stage - specific test-   ing we take their checkpoints after each training   stage . We show average performance in the main   paper and relegate detailed results to Appendix C.   4.4 Implementation Details   We set the learning rate to 0.5 for   and and 5e-5 for   and Adapter , using Adafactor ( Shazeer and Stern ,   2018 ) optimizer . We adopt implementation of   Adapter from OpenDelta ( Hu , 2022 ) and use the de-   fault bottleneck dimension of 24 . For online EWC   and MAS , we report best results obtained over dif-   ferent regularization constant . For all methods , we   set maximum training epochs to 256 for Fuffpost-   News and FewNERD , and to 512 for FewRel . For , the number of soft tokens per la-   bel prompt is set to 10 , the selection probability pis   set to 50 % and number of label transfer candidates   K in § 3.4 is set to 3 .   4.5 Results on Stage - agnostic Setting   In Table 1 , we show the stage - agnostic testing   results . We observe that across all three tasks , significantly outperforms all other   baselines by a large margin . This empirically jus-   tifies that is indeed able to dy-   namically combine the label - specific knowledge   learned across different training stages in order   to infer over the unseen combined label - space .   Amongst the baselines , performs rel-   atively better , while the limited trainable parame-   ters make the parameter efficient models more sus-   ceptible to catastrophic forgetting . For CL methods ,   MAS improves and   by 4 % and 8 % on average respectively , but fails   on Adapter . EWC is less effective in addressing   forgetting across all baselines .   Also note that the PTextension is able to im-   prove by 10 - 20 % over vanila PT . This shows that   soft prompts , behaving like language tokens , have   a compositional nature and can be concatenated to   support multi - tasking . , in addi-   tion to exploiting this implicit language prior , also1682   explicitly imposes subset - invariant loss to adapt   to dynamic label spaces , further boosting stage-   agnostic performance by 14%-18 % over PT .   4.6 Results on Stage - fused Setting   We present results on our novel stage - fused setting   in Table 2 . We observe that none of the baselines   are capable of handling this setting , as is evident   from their abysmal performance across all testing   stages . In absence of any label - modular represen-   tation , they are unable to utilize any information   about the desired label - space . On the other hand , not only outperforms all baselines   by an average margin of 37.5 % , it also achieves 4%-   14 % better performance than the oracle multi - task on News Classification and NER .   We select the top performing baselines in this   setting and apply constrained decoding to them   ( marked with * ) , which improves their performance   by 20%-30 % on News and RE , 2%-4 % on NER .   However , still outperforms these   baselines by 14%-27 % . This significant improve-   ment is evident of the fact that ,   by learning label - modular representations , can ef-   fectively combine partial knowledge from different   training stages and condition the PLM on any target   set of label prompts . This allows it to seamlessly   adapt to dynamic unseen label spaces , without ap-   plying any post - processing or verbalizer .   Note that while PTis able to combine knowl-   edge from multiple training stages to support stage-   agnostic testing , it fails to extract and consolidate   specific knowledge corresponding to only the target   label - set , across different stages .   4.7 Results on Stage - specific Setting   While has proved to be particu-   larly successful in handling the challenging non-   stationary settings of stage - agnostic and stage-   fused evaluations , we now want to see how com-   petitive it is under stage - specific settings . From the   results in Table 3 , we see that the average stage-   specific performance of is com-   parable to vanila on the three tasks .   Note that while MAS regularization boosts stage-   agnostic performance somewhat for   and , it infact degrades their stage-   specific performance by 10%-40 % . Similarly ap-   plying EWC regularization fails to improve over   the vanila models in this setting while also proving   less effective on stage - agnostic evaluation . This   shows the lack of robustness of these techniques   across the different non - stationary settings . But is able to achieve state - of - the - art   in stage - agnostic and stage - fused settings while   remaining comparable to in stage-   specific evaluation . Besides , ( Lester et al . , 2021 )   showed that the performance gap between and will gradually close as   the size of backbone PLMs scales up . We posit that , being an extension of can similarly benefit from scaling - up of the   PLM , but we leave this as future work owing to   resource limitations.1683   4.8 Ablation Study   We now analyze the contribution of different com-   ponents of towards its SoTA per-   formance . From the results in Table 4 , we see that   in stage - agnostic setting , both label prompt transfer   and subset invariant loss provide a boost , though   the role of the former is seemingly more signifi-   ca nt . On the contrary , removing subset invariant   loss has a more debilitating effect on stage - fused   performance . This evinces that subset invariant   loss is indeed critical in learning label modular rep-   resentations . This is essential to the stage - fused   evaluation which needs to extract and dynamically   re - compose label - specific knowledge .   4.9 Quantitative Analysis   Apart from achieving SoTA , does   possess the desirable characteristics of a modu-   lar model ? According to Algorithm 1 , setS= Ωduring inference . We ex-   periment with different strategies of input prompt   construction including dropping label prompt(s ) ei-   ther corresponding to ground truth label(s ) or one   other random label , and permuting the default order   of label prompts ; see Table 5 for the results .   Indeed we observe that dropping the ground   truth label prompt during inference degrades the   mean performance by 57%-82 % while dropping   any other random label prompt boosts performance   slightly . This strongly demonstrates the label   grounding property of , i.e. the   knowledge of a class label is exclusively embedded   in its corresponding label prompt .   also shows low sensitivity to the order of label   prompts during inference - a yet another favourable   property of label modular models   4.10 Qualitative Analysis   Revisiting Figure 1 presented in § 1 , we observe that is able to predict correctly on a   testing regime that is unseen during training , by ex-   tracting and consolidating label specific knowledge   from multiple training stages . More example pre-   dictions are shown in Figure 3 ( and Appendix B ) ,   which indicate that is able to ex-   ploit in - context learning over label - prompts to gen-   eralize to unseen label - combinations during infer-   ence . For example , tags “ Gilbert ”   as politician as he was “ a delegate to ” a government .   In the same spirit , wrongly tags   “ Bert Bell ” and “ Rozelle ” as athletes ( true label be-   ingperson_other ) because they are associated with   the sports league “ NFL ” . Such qualitative findings   demonstrate ’s capabilities to learn   label modular representations and integrate them   dynamically during inference .   5 Conclusion   In this paper , we have proposed ,   a novel label modular prompt tuning framework1684for text classification in - the - wild . Extensive ex-   periments show that is able to   consolidate knowledge learned during sequential   training stages for stage - agnostic testing and ex-   tract and recompose knowledge for stage - fused test-   ing , while maintaining competitive performance in   stage - specific settings . We have also conduct anal-   ysis to show that has desirable   modular properties of label grounding , low order   sensitivity and in - context learning . Being the first   work on modular parameter efficient tuning , we   hope for it to spur more research in this area in   future towards solving a wider range of tasks under   more general non - stationary settings .   Limitations   In this section , we discuss limitations and potential   future work towards extending to   a more generalised method for wider applicability .   On Scalability In , the input   prompt Tgrows in proportion to |S| , the size of   label set of interest . This limits   from supporting huge label set ( e.g. , thousands   of labels ) as transformers can only condition on   a bounded - length context . With long range trans-   formers like Longformer ( Beltagy et al . , 2020 ) ,   Performer ( Choromanski et al . , 2021 ) and LongT5   ( Guo et al . , 2021 ) coming into vogue , this issue   is somewhat mitigated . Regardless of that , one   potential solution is to formulate a hierarchical ver-   sion of , which is similar in spirit   to hierarchical softmax ( Morin and Bengio , 2005 ) .   Hierarchical takes multiple steps   for prediction , with each step to predict labels in a   specific hierarchy level .   Another potential solution is to treat all label   prompts as memory units from which the model   learns to select relevant ones for a given data in-   stance , in the spirit of ( Wu et al . , 2022 ) .   On generation tasks As shows   SoTA performance and good modular characteris-   tics for text classification in - the - wild , it is appealing   to extend it to other tasks like Question Answering   ( QA ) , Machine Reading Comprehension ( MRC )   and Summarization . However , it is non - trivial for to incorporate these tasks as their   target texts are unstructured without clear class la-   bels . One potential solution is to instead consider   attributes or properties of target texts , which are   also conditioning factors ( e.g. , formality , concise - ness , topics , aspects , sentiment for summarization ) .   With such definitions , it will be interesting to check   if framework can achieve good   generalisation and conditional generation on text   generation in - the - wild .   References168516861687A More Details on Dataset Construction   In this section , we provide more complementary de-   tails to § 4.1 . Table 6 shows examples of text input   and target for three datasets . For Huffpost News   and FewRel , we subsample exactly 200,50 exam-   ples per label class . For FewNERD , as it can have   multiple label types per example , we subsample at   50 examples per label class .   B More Qualitative Examples   Similar to § 4.10 , we show another example of on stage - fused NER in Figure 4 and   more example predictions in Figure 5 . These ad-   ditional examples strengthen the conclusions in   § 4.10 . In the second example in Figure 5 , tags " Big Twin Sauce " as a product   food while its ground truth tag is product other . We   can see considers the context as   the entity is associated with a restaurant . Similarly ,   in the third example , " Kobo Touch " is actually a   hardware reader and its ground truth tag is prod-   uct other . However , such world knowledge is not   available and tags it as a software   based on the context of " eBooks " and libraries .   C Detailed Results on Stage - agnostic and   Stage - specific Settings   Table 7 shows detailed stage - agnostic results .   Dcorresponds to multi - stage training over   domains { 1 , .. , k}and testing over the aggregated   domains . For baselines , we use their checkpoints   after k - th stage for evaluating D. For , we use the final checkpoint . Table 8 ,   9 show detailed stage - fused results . D ’ denotes   k - th task fused stage.168816891690