  Olga PelloniAnastassia ShaitarovaTanja SamardzicText Group , URPP Language and Space , Department of Computational Linguistics , University of Zurich , Switzerland   Abstract   Pre - trained multilingual models , such as   mBERT , XLM - R and mT5 , are used to im-   prove the performance on various tasks in low-   resource languages via cross - lingual transfer .   In this framework , English is usually seen   as the most natural choice for a transfer lan-   guage ( for fine - tuning or continued training of   a multilingual pre - trained model ) , but it has   been revealed recently that this is often not   the best choice . The success of cross - lingual   transfer seems to depend on some properties   of languages , which are currently hard to ex-   plain . Successful transfer often happens be-   tween unrelated languages and it often can-   not be explained by data - dependent factors . In   this study , we show that languages written in   non - Latin and non - alphabetic scripts ( mostly   Asian languages ) are the best choices for im-   proving performance on the task of Masked   Language Modelling ( MLM ) in a diverse set of   30 low - resource languages and that the success   of the transfer is well predicted by our novel   measure of Subword Evenness ( SuE ) . Transfer-   ring language models over the languages that   score low on our measure results in the low-   est average perplexity over target low - resource   languages . Our correlation coefficients ob-   tained with three different pre - trained multilin-   gual models are consistently higher than all the   other predictors , including text - based measures   ( type - token ratio , entropy ) and linguistically   motivated choice ( genealogical and typological   proximity ) .   1 Introduction   Since pre - trained multilingual models became   available , the most common approach to NLP tasks   on low - resource languages has been cross - lingual   transfer learning . After initial tests on cross - lingual   ability within the languages covered by the multi-   lingual models ( Pires et al . , 2019 ; Wu and Dredze ,   2020 ; Libovický et al . , 2020 ) , a more recent line   of research shows that new languages ( not seen inFigure 1 : Best cross - lingual transfer results mentioned   in previous work . The size of each circle is proportional   to the number of mentions of a given transfer - target pair .   English is the best transfer language for European target   languages . For target languages outside of Europe , the   best transfer languages are hard to predict .   the training set ) are best processed by continued   training or fine - tuning ( depending on the task ) of   a pre - trained multilingual model on one transfer   language , even if the transfer language is differ-   ent from the target language . As a matter of fact ,   a single transfer language can help improve the   performance on many target languages ( Lin et al . ,   2019 ; Lauscher et al . , 2020 ; Turc et al . , 2021 ) . This   insight is especially important for low - resource lan-   guages , for which only test data is typically avail-   able . Multilingual models are usually fine - tuned or   ( additionally ) trained on a high - resource language   and the resulting weights are applied as zero- or   few - shot transfer to a target low - resource language   ( Tunstall et al . , 2022 ) .   It seems that injecting a bias towards some par-   ticular linguistic traits , while slightly “ forgetting ”   some others , makes the multilingual models adapt   better to a new language . An evident but hard ques-   tion arises here : Which languages are especially   well - suited to serve as transfer languages for many   low - resource target languages ? This is the first7428research question that we address in our study .   Most research on cross - lingual transfer regards   English as the most natural choice for any target   language due to the abundance of training data .   However , more recent studies reveal that other lan-   guages are often better choices . For instance , Rus-   sian turns out to be a good transfer language for   Thai , Arabic , Swahili , and Chinese ( Turc et al . ,   2021 ) . It is hard to see why this happens as these   languages are neither related nor similar by any   of the known criteria . Our own review of several   studies on cross - lingual transfer , summarized in   Figure 1 , shows that English is the best transfer lan-   guage when European languages are the target . For   other target languages , the choice of the transfer   languages is anything but clear . English is rarely   the best choice , while Russian , Chinese , German ,   Greek , or Arabic seem to help in many cases .   This observation leads us to the second ques-   tion that we address in our study : Which linguistic   traits should be used to improve the performance   of pre - trained multilingual models ? In other words ,   what are the traits that make some languages more   suitable for transfer ?   To answer these questions , we test language   models on a highly diverse set of low - resource lan-   guages , whose geographical distribution is shown   in Figure 2 . We follow the current cross - lingual   transfer learning framework in order to train the   models . We consider three popular multilingual   pre - trained models and 19 high - resource languages   as potential transfer languages . In these experi-   ments , we look for the best transfer languages , but   also , more importantly , for a strong predictor of   the transfer results across all target low - resource   languages .   As a predictor , we propose a novel text - based   measure , Subword Evenness ( SuE ) , a parameter   that describes the differences in the length of sub-   word units ( one value per language ) . The moti-   vation for looking into the properties of subword   units to assess the suitability of a given language   to be a transfer language comes from the fact that   language models are trained over the output of sub-   word tokenization . We know that subword splits   can depend on the properties of a language . For   example , Finnish is known for its regular subword   structure ( morphology ) , which is expected to give   more even subword tokens , while other languages   might have less subword regularity , leading to more   uneven splits . Our measure shows the preference   for even splits in a given language . An example   of a relatively even split would be co - work - ing →   set of lengths [ 2 , 4 , 3 ] compared to an uneven split   co - working →set of lengths [ 2 , 7 ] . A detailed   explanation of the measure is in Section 3 .   The main finding of our work is that the prop-   erty of Subword Evenness matters for successful   transfer to diverse non - Indo - European languages .   Transfer languages that score low on our measure   give the lowest average perplexity over target low-   resource languages . Compared to other text - based   statistics and alternative linguistically motivated   choices , our measure is the best predictor of trans-   fer results .   The code and data used for this work   are publicly available in the repository   https://github.com/olgapelloni/subword_evenness .   2 Related Work   The main idea behind cross - lingual transfer learn-   ing is that both similarities and differences be-   tween languages can be exploited for improving   model performances . In early work on multilin-   gual models , similarities between languages were   exploited to increase training data , for instance in   multilingual syntactic parsing ( Zeman and Resnik ,   2008 ; Snyder et al . , 2009 ) and pivot - based statisti-   cal machine translation ( Paul et al . , 2013 ) . Cross-   linguistic differences , as a form of distant supervi-   sion , have been shown to help the disambiguation   of lexical meaning ( van der Plas and Tiedemann ,   2006 ) and part - of - speech ( POS ) tags ( Snyder et al . ,   2008 ) . Neural models made this idea even more   attractive , leading to many proposals on how to   share some parameters across languages , for in-   stance through cascading and multi - task learning   ( p. 218–227 , Goldberg , 2017 ; Ruder , 2017 ) . With7429Transformer - based models ( Vaswani et al . , 2017 ) ,   the transfer approach has proven to be widely appli-   cable and successful , as shown by the 2018 model   ULMFiT ( Howard and Ruder , 2018 ) .   Previous analyses of multilingual Transformer-   based models have shown that the representations   produced during training are multilingual but the   language - specific information is preserved ( Pires   et al . , 2019 ; Wu and Dredze , 2019 ; Libovický et al . ,   2020 ; Wang et al . , 2020b ) . Forgetting some of the   language - specific information has been helpful for   the task of question - answering ( Yang et al . , 2021 ) .   Wu and Dredze ( 2019 ) show a positive correlation   between the number of shared tokens and trans-   ferability between languages , but this finding has   not been confirmed in later studies . K et al . ( 2020 )   do not find a strong influence of lexical overlap on   successful language transfer . Pires et al . ( 2019 ) add   that the shared tokens help to create cross - lingual   representations , but language transfer success de-   pends more on the structural similarity between   languages .   Studies on low - resource languages demonstrate   how a lack of resources impacts performance ( Wu   and Dredze , 2020 ; Wang et al . , 2020a ; Goyal et al . ,   2021 ) . Ruder et al . ( 2021 ) show that the perfor-   mance of XLM - R is lower on low - resource lan-   guages than on high - resource ones . Moreover , per-   formance is lower on languages with non - Latin   scripts , such as Hebrew , Japanese , Thai or Chinese .   The current processing workflows often include   continued training of a multilingual model on a   single high - resource transfer language and then ap-   plying the resulting weights in a few- or zero - shot   manner to many low - resource languages . Focus-   ing on the question of which transfer languages   give good results over multiple target languages ,   we count the mentions of the best transfer pairs in   several previous studies ( Ruder et al . , 2021 ; Turc   et al . , 2021 ; Vázquez et al . , 2021 ; Hu et al . , 2020 ;   Lauscher et al . , 2020 ; Lin et al . , 2019 ; Paul et al . ,   2013 ) . The counts are plotted in Figure 1 , showing   an interesting asymmetry between European and   other languages . When European languages are   the target of transfer , English seems to be the best   transfer language . This is in line with the findings   on the XTREME benchmark for evaluating cross-   lingual transfer ( Hu et al . , 2020 ) , which led the   authors to conclude that English is the most com-   mon and the most universal choice for a transfer   language . Paul et al . ( 2013 ) found that English asa pivot language in statistical machine translation   works well in approximately half of the observed   language pairs ( 22 Indo - European and Asian lan-   guages ) . Our review shows that English is not the   best choice when non - European languages are the   target .   One approach to choosing a transfer language is   to rely on structural similarity , which is measured   using grammar features from the URIEL database   ( Littell et al . , 2017 ) . Lauscher et al . ( 2020 ) find that   transfer is better in language pairs that are closer   in the URIEL vector space regarding POS tagging   and syntactic dependency parsing . Other factors ,   such as data size , are better predictors on the tasks   of question answering and inference . de Vries et al .   ( 2022 ) find that surface string similarity is the best   predictor for POS tagging . Shaitarova and Rinaldi   ( 2021 ) develop their own linguistic typology based   on negation constructions , which helps to choose a   better transfer language for negation scope resolu-   tion . Lin et al . ( 2019 ) suggest aggregating various   types of linguistic features , including geographic   location . Aggregated measures select better trans-   fer candidates than single features .   We depart from previous research in terms of   both data and methods . We work with a much big-   ger and more diverse sample of languages than any   previous studies . The main methodological novelty   of our work is the proposed text - based parameter ,   which captures an interesting subword feature of   good transfer languages .   3Subword Evenness ( SuE ) as a Language   Parameter   Defining formal properties of languages relevant   to NLP is still a challenging task . Mielke et al .   ( 2019 ) , for instance , notice that language model   sentence surprisal scores ( aggregated at the level   of a language ) differ across languages , but do not   manage to identify any properties of a language   that would predict the surprisal . When it comes   to transfer learning , previous research shows little   agreement on which languages should be chosen   for cross - lingual transfer and why .   The full workflow for calculating the SuE score   ( one value per language ) is shown in Figure 3 . We   first apply a subword tokenization algorithm to split   words in an unsupervised fashion . We then convert   each segmented word W = w , w , . . . win the   data into a sequence of integers L = l , l , . . . l ,   where nis the number of subword segments in7430   each word and each integer lrepresents the length   of the subword segment wmeasured in Unicode   characters . We then map each sequence of integers   to a single integer L→UI(unevenness index ) ,   where UI = max(L)−min(L)is the difference   between the maximum length ( longest segment )   and the minimum length ( shortest segment ) in the   sequence . The values of UIare plotted against   the values of word length measured in Unicode   characters ( the x - axis in Figure 3 ) , resulting in a   distribution , which then undergoes a Kernel Den-   sity Estimation ( KDE)analysis for identifying the   shape of the density region .   We aim to describe the whole distribution with a   single parameter by approximating the shape of the   density area with two lines and measuring the angle   between them . The first line ( intercepting the x-   axis on the left side in Figure 3 ) is a natural bound   of the distribution f(x ) = x−2determined by the   fact that the difference between any two subword   segments can not be greater than the length of the   whole word . The second line is a linear function   fitted to two points at the edge of the density area :   max(x)andmax(y),g(x ) = kx+b . Finally , the   measure of Subword Evenness ( Equation 1 ) is the   upper angle in the triangle formed by these two   lines and the x - axis .   SuE = 180 ° − |arctan 1 | − |arctan k|(1 )   For Equation 1 , we first calculate the inclination   of each intersecting line using the formula m=   tan(θ ) , where mis the slope of the line and θis   the inclination , i.e. the angle between the x - axis   and the line ( Larson and Hostetler , 2007 , p. 430 ) .   The slope of the first line f(x)equals 1 , the slope   of the second line g(x)equals k. In order to get the   value of the inclination θfromtan(θ ) , we apply theinverse trigonometric function arctan(tan ( θ ) ) =   θ(Larson and Hostetler , 2007 , p. 109 , 193 ) . We   use absolute values in order to work with the angles   measured in degrees . Once we know the inclination   of both lines ( their angles to the x - axis ) , we can   find the upper angle inside the triangle . Since all   angles in a triangle sum up to 180 ° , we subtract   both inclinations from 180 ° to find the upper angle .   The upper angle shows the preference for Subword   Evenness , thus called SuE . The higher the value of   this angle , the higher the preference towards even   subword splits in longer words .   There are many methods that can be used to per-   form unsupervised subword tokenization . The most   widely used methods are Byte - Pair Encoding ( BPE ,   Gage , 1994 ) implemented by Sennrich et al . ( 2016 )   and also in the SentencePiece library ( Kudo and   Richardson , 2018 ) , WordPiece ( Wu et al . , 2016 )   and the SentencePiece Unigram model . The out-   put of all these algorithms is highly dependent on   the hyperparameters that determine the size of the   resulting subword vocabulary . However , there are   currently no general criteria to determine the value   of these hyperparameters ( Mielke et al . , 2021 ) .   We follow the method of Gutierrez - Vasques et al .   ( 2021 ) , which is motivated by information theoretic   properties of segmented text in many languages :   the BPE algorithm is run until the text redundancy   value reaches its minimum . This criterion is inde-   pendent of any particular NLP task and is cross-   lingually aligned , which is important for our study .   Cross - lingual alignment allows us to normalize , to   a certain degree , the difference in writing systems :   minimum redundancy is reached faster in alpha-   betic scripts and slower in syllabic and logographic   scripts . For reference , we extract the SuE measure   using several other segmentation algorithms and   hyperparameters . These results can be found in   Table 9 in the Appendix.74314 Data and Experiments   We obtain our training and test data from the Text   Data Diversity Sample ( TeDDi , Moran et al . , 2022 ) .   The TeDDi corpus contains texts representing the   languages included in the 100 - language sample ,   published by the World Atlas of Language Struc-   tures ( WALS , Haspelmath et al . , 2005 ) . This sam-   ple is compiled by experts in linguistic typology   with the goal of representing overall linguistic di-   versity : language families , geographical areas and   typological features . Following this sample , the   TeDDi corpus maximizes linguistic diversity and   also covers different textual genres for languages   with more available resources ( Gutenberg Project ,   the Parallel Bible Corpus ( Mayer and Cysouw ,   2014 ) , the OpenSubtitles corpus ( Lison and Tiede-   mann , 2016 ) and Universal Declaration of Human   Rights   For the set of transfer languages ( training set ) ,   we select 19 languages which contain at least   one million tokens : Basque ( eus ) , English ( eng ) ,   Finnish ( fin ) , French ( fra ) , German ( deu ) , Greek   ( ell ) , Hebrew ( heb ) , Hindi ( hin ) , Indonesian ( ind ) ,   Japanese ( jpn ) , Korean ( kor ) , Mandarin ( cmn ) , Per-   sian ( pes ) , Russian ( rus ) , Spanish ( spa ) , Tagalog   ( tgl ) , Thai ( tha ) , Turkish ( tur ) , Vietnamese ( vie ) .   We balance the genres when possible and cap the   length of the sampled text to 1 M tokens per lan-   guage . We use the scikit - learn library ( Pedregosa   et al . , 2011 ) to shuffle each dataset and split it into   train ( 80 % ) and validation ( 20 % ) sets .   For the set of target languages ( test set ) , we de-   fine the threshold of at least 100 K tokens available   per language . There are 30 such languages : Alam-   blak , Amele , Apurina , Arabic ( Egyptian ) , Ara-   pesh ( Mountain ) , Barasano , Burmese , Chamorro ,   Daga , Fijian , Georgian , Guarani , Hausa , Jakaltek ,   Kewa , Khalkha , Khoekhoe , Lango , Malagasy , Ma-   pudungun , Mixtec ( Chalcatongo ) , Oromo ( Harar ) ,   Quechua ( Imbabura ) , Sango , Sanuma , Swahili ,   Wichi , Yagua , Yaqui , Yoruba . We fix the size of   the test sets to be 100 K tokens per language . The   chosen languages belong to 23 different language   families and are spoken in 5 geographical areas   ( Figure 2 ) .   4.1 Pre - trained Models   We work with three popular multilingual Trans-   former models in our experiments : mBERT ( De-   vlin et al . , 2018 ) , XLM - RoBERTa ( Conneau et al . ,   2019 ) and mT5 ( Xue et al . , 2021 ; Raffel et al . ,   2019 ) . The pre - training objectives for all three   models include the Masked Language Modeling   ( MLM ) objective which is also the objective of our   trained models . These models were pre - trained on   vast amounts of multilingual data such as Com-   monCrawl for XLM - R and mT5 and Wikipedia   for mBERT . We employ the base variants of the   models ( Table 1 ) .   Note that we use the pre - trained multilingual   models only as the starting point for our own mono-   lingual training . This is currently the best method   for obtaining high performance on many test lan-   guages .   4.2 Continued Training and Testing   We continue training the three pre - trained mod-   els on our set of transfer languages . We keep the   same hyperparameters for mBERT and XLM - R   and train them for 5 epochs with a learning rate   3e-5 and maximum sequence length 256 . In order   to maintain the batch size of 128 with infrastruc-   tural constraints , we set the batch size to 4 and   use gradient accumulation after 32 steps . For mT5 ,   we use an available T5 architecture from Hugging-   face ( Wolf et al . , 2019 ) which allows continued7432   training of the language model in an unsupervised   fashion . We load the mT5 - base checkpoint and   continue training following the early stopping ap-   proach as in Turc et al . ( 2021 ) , where we evaluate   each model every 200 steps and stop training at   2000 steps . Only the best identified checkpoint is   kept as a final model . As a result , mT5 is trained for   approximately the same number of epochs ( 3–5 ) as   mBERT and XLM - R , making the results compara-   ble .   All models are re - trained 5 times with 5 different   seeds . Thus , we obtain 285 fine - tuned models ( 5   models ×19 training languages ×3 architectures )   plus 3 pre - trained models without continued train-   ing as baselines . We test the baseline and additional   trained models on all 30 test languages in a zero-   shot fashion . We evaluate all the models measuring   the perplexity , which is an exponentiated average   of negative log likelihood of a sequence . Lower   perplexity means better performance with the cho-   sen transfer language . We report the average results   across 5 seeds for each language pair . Only 4 out of 30 target languagesin our datasets   are included in the pre - training of all three pre-   trained models : Burmese , Georgian , Malagasy ,   and Swahili , making our experiments almost com-   pletely zero - shot .   5 Results and Discussion   The first general outcome of our experiments is the   ranking of transfer languages according to the aver-   age perplexity score on the target languages shown   in Table 2 . There is considerable overlap between   the three sets of top 5 languages ( one set per pre-   trained model).In addition , we note that English   is never in the top 5 , which confirms observations   from previous work summarized in Figure 1 . An   interesting observation regarding these results con-   cerns the scripts : all of the best transfer languages   are written in a non - Latin script , in contrast to the   general preference for Latin scripts ( e.g. over 70 %   of languages in the TeDDi sample are written in a   Latin script ) .   The second general outcome is the ranking of   transfer languages according to the SuE measure   shown in Table 3 . There is considerable overlap be-   tween the top languages in the two rankings ( com-   paring Table 3 with Table 2 ) . To quantify this de-   pendence , we perform correlation tests.74335.1 Transfer language SuE correlates with   average target language perplexity   Figure 4 shows the main finding of our study : a rel-   atively high correlation score between the Subword   Evenness ( SuE ) measure and the transfer - learning   performance per transfer language , measured as   language model perplexity averaged over all 30   target languages . This correlation means that trans-   ferring from languages with uneven subword splits   ( low SuE ) leads to better performance ( lower per-   plexity ) on a diverse set of target languages . In   other words , adding information from languages   whose words are split unevenly to a pre - trained mul-   tilingual model is more helpful for the performance   on many target languages than adding information   from more regular languages whose subwords are   evenly split .   While we find this correlation with all three pre-   trained models , we notice that the coefficients differ .   For instance , the Pearson correlation is 0.56 in the   case of XLM - R , 0.40 for mBERT and only 0.18   for mT5 . These differences are well aligned with   the baseline performance and transfer gains . For   example , the baseline perplexity of mT5 is much   lower than the other two models , the gains are con-   sequently smaller and the correlation is weaker .   However , with only three observations , we can not   know whether this alignment is due to chance .   Another pattern that appears in the plots is the   grouping of languages according to the script , with   non - alphabetic scripts being more associated with   low SuE values . This might point to other potential   explanations for the observed correlation such as ,   for instance , the fact that words tend to be shorter in   non - alphabetic scripts leading to smaller angles . To   exclude such factors , we perform tests with several   other measures and compare them with SuE.   5.2 SuE vs. other text - based measures   Table 4 shows the correlation coefficients obtained   with SuE compared to other , simpler text - based   measures . We include the mean word length in this   analysis to see whether the known impact of the   script on the word length may explain the observed   patterns in cross - lingual transfer . As for type - token   ratio ( TTR ) and unigram entropy , we include them   as indicators of language complexity , which is also   studied in previous work .   While the mean word length indeed shows a   moderate correlation with the transfer performance   ( likely due to the differences in scripts ) , SuE comes   out as a much better predictor . TTR and unigram   entropy are only weakly correlated , which can be   seen as an indirect replication of the negative re-   sults of Mielke et al . ( 2019 ) . It is reasonable to   expect that more complex languages ( high TTR   and entropy indicate higher complexity ) could be   more helpful for non - Indo - European languages ( of-   ten complex themselves ) , but we do not observe   this . Subword Evenness ( SuE ) explains the trans-   fer success much better , probably by capturing a   text property that is more relevant to the model ’s   performance.7434   5.3 SuE vs. genealogical and typological   proximity   Other than text - based measures , typical predictors   of transfer learning studied in previous work are   measures of language similarity extracted from lin-   guistic databases . To see how SuE compares with   such measures , we analyse changes in complexity   when SuE is replaced by linguistic measures . We   do not perform correlation tests in these cases be-   cause linguistic measures are not available for all   the languages in our experiments .   Table 5 shows the comparison between SuE and   genealogical proximity . For this comparison , we   look for pairs of languages where both the transfer   and the target language belong to the same family   according to the genealogical hierarchy presented   in WALS ( Haspelmath et al . , 2005 ) . Since our   corpus maximizes linguistic diversity , most of the   languages in our study do not have such close rela-   tives . Nevertheless , we identify a sub - sample of 8   target languages whose relatives are among transfer   languages . For each of these languages , we check   whether the language model ’s perplexity is reduced   more if the related language is used for transfercompared to one of the SuE top 5 languages ( with   lowest SuE ) , which are not related to the target lan-   guage . The positive scores in Table 5 mean that   at least one of the SuE top 5 languages is always   a better transfer language than the genealogically   closest language in our sample for all the target   languages which are tested .   Table 6 shows the comparison between SuE and   typological proximity for the target languages for   which we could extract reliable feature vectors   from the URIEL database ( Littell et al . , 2017 ) . To   assess the typological proximity , we use syntactic ,   phonological and inventorial features . Again , we   obtain mostly positive scores , meaning that at least   one of the SuE top 5 languages is almost always   a better option than the typologically closest lan-   guage in our sample . There are two cases where   the language chosen according to the typological   proximity performed better : English as a transfer   for Burmese and Japanese as a transfer for Khalkha   ( Mongolian ) . The case of Khalkha is quite special ,   since this language appears to be hard to model :   its perplexity is among the highest values across   the three models . On the other hand , Japanese is   actually among the good predictors according to   SuE , just not in the strict top 5 ( Table 3 ) . In case of   transfer between English and Burmese , we can see   that the change in perplexity is rather low ( -0.07 ) ,   and generally English does not appear to be a good   transfer language for any other languages in our   experiments .   Our results show that proximity to the transfer   language in terms of genealogical or typological   properties is often not the best criterion for choos-   ing a good transfer language when working with   low - resource languages . At least one transfer lan-   guage that we identify as most suitable ( top 5 low-   est SuE values ) gets consistently better ( lower ) per-   plexity scores than genealogically or typologically   close languages . Sometimes the best transfer lan-   guage overlaps between all three choice methods ,   but our measure SuE is still more consistent , and   does not depend on the test low - resource language   and does not need any linguistic annotation ( purely   data - driven ) .   6 Stability of SuE across different   corpora   Additionally , we check how corpus size and change   of data can influence our measure of SuE. A system-   atic study on the corpus dependence would exceed7435   the scope of this paper , so we provide only several   comparisons here .   We measured SuE on the full TeDDi corpus and   compared the results with the SuE values obtained   on a balanced sample from the TeDDi corpus with   1 million tokens ( Table 7 ) . Then , we used the Aalto   MorphoChallenge corpus ( Kurimo et al . , 2010 ) ,   which provides large amounts of data in English ,   Finnish and Turkish coming mostly from Europarl   ( Koehn , 2005 ) in order to check SuE on data from   a different source . We compare the obtained values   to the two TeDDi versions in Table 8 .   Table 7 demonstrates that the differences in lan-   guages with Latin scripts are smaller than in lan-   guages written in non - Latin scripts . Nevertheless ,   average difference across all languages is about   5 degrees , which is rather low . The correlation   between the values of the TeDDi sample with 1   million tokens and the full TeDDi is high , we get a   0.7 Pearson coefficient correlation ( p - value = .002 ) .   Table 8 shows that fluctuations range between 2and 12 degrees with no apparent correlation to the   data size .   While these experiments show that the rankings   of SuE values remain generally stable after chang-   ing data source or size , we find that the best predic-   tions are found when using the balanced 1 million   tokens sample from the TeDDi corpus .   7 Conclusion   In this study , we tackled the question of what   languages are preferable for cross - lingual transfer   when modelling diverse low - resource languages ,   and what motivates this selection . Our experiments   on the task of masked language modelling ( MLM )   with three multilingual pre - trained Transformer-   based models show that there is a small set of gen-   erally good transfer languages : Japanese , Greek ,   Hebrew , Thai , and Russian . What is common to   these languages is the fact that all of them are writ-   ten in a non - Latin script . However , the script alone   is not the best general predictor of transfer perfor-   mance . We show that the best predictor of the lan-   guage model perplexity on a wide range of target   low - resource languages is the Subword Evenness   ( SuE ) score of transfer languages , which we have   presented in this paper . Most of the languages that   repeatedly come out as good transfer languages   have low SuE scores .   Our results are largely in line with the observa-   tions about good transfer languages made in previ-   ous work . In addition to providing new evidence   confirming previous findings on a very diverse sam-   ple of languages ( 19 transfer and 30 target lan-   guages ) , we identify the strongest predictor of the   observed performances up to now . The proposed   SuE measure is a better predictor for transfer lan-   guages than other text - based measures ( mean word   length , type - token ratio , unigram entropy ) as well   as genealogical and typological proximity . With   this finding , we make a further step towards explain-   ing why continued training on transfer languages   is helpful for modelling low - resource languages .   Acknowledgements   This research is supported by the Swiss National   Science Foundation ( SNSF ) grant 176305.7436Limitations   Our focus on low - resource languages limits to a   certain degree the generalization of our findings .   While our data represents a carefully designed lan-   guage sample , the decisions made by the authors   of the sample are arbitrary , which means that our   samples are not random and the finding might   not generalize to all languages . For example , our   sample of target languages does not include any   Indo - European languages , such as Germanic or Ro-   mance low - resource languages . These languages   have been studied before and it has been shown that   the best choice for them is transferring from a ge-   nealogically related rich - resource language ( Aepli   and Sennrich , 2021 ) . It might be interesting to see   how our proposed measure would compare with   other measures in these cases , but this would re-   quire a different study design , which we leave for   future work .   Another limitation of our work concerns the data   size thresholds that we use to divide the languages   into low - resource ( target ) and high - recourse ( trans-   fer ) languages . In our experiments , the size of   100 K tokens is assigned to the low - resource group .   We took this decision taking into account two crite-   ria . First , we wanted to have reasonable models for   comparing the performance and we judged this size   reasonable . Second , we identified this threshold by   checking how many test languages we could still   draw out of the TeDDi sample , and the number of   30 ( 1/3 of TeDDi ’s languages ) seems to be a rela-   tively good test size . While this procedure might   leave some of the really low - resource languages   out of our sample , we still cover a wide variety   of languages for which at least some texts ( 100 K   tokens is approximately the size of a shorter novel )   are available .   Finally , our experiments are limited to one task   ( masked language modelling ) and the results might   not generalize to tasks requiring annotated data .   We note that , despite this limitation , our results are   largely in line with previous work on various tasks .   References743774387439A Results with alternative subword tokenization algorithms7440B Detailed language model perplexity scores74417442C Details on the previous work744374447445