  Lan Jiang , Hao Zhou , Yankai Lin , Peng Li , Jie Zhou , Rui JiangMOE Key Laboratory of Bioinformatics , Center for Synthetic and Systems Biology ,   Department of Automation , BNRist , Tsinghua University , ChinaPattern Recognition Center , WeChat AI , Tencent Inc. , ChinaGaoling School of Artificial Intelligence , Renmin University of China , Beijing , ChinaBeijing Key Laboratory of Big Data Management and Analysis Methods , Beijing , ChinaInstitute for AI Industry Research ( AIR ) , Tsinghua University , China   jiangl20@mails.tsinghua.edu.cn   Abstract   Even though the large - scale language models   have achieved excellent performances , they   suffer from various adversarial attacks . A   large body of defense methods has been pro-   posed . However , they are still limited due to   redundant attack search spaces and the inabil-   ity to defend against various types of attacks .   In this work , we present a novel fine - tuning   approach called RObustSEletive fine - tuning   ( ROSE ) to address this issue . ROSE conducts   selective updates when adapting pre - trained   models to downstream tasks , filtering out in-   valuable and unrobust updates of parameters .   Specifically , we propose two strategies : the   first - order and second - order ROSE for select-   ing target robust parameters . The experimen-   tal results show that ROSE achieves signifi-   ca nt improvements in adversarial robustness   on various downstream NLP tasks , and the en-   semble method even surpasses both variants   above . Furthermore , ROSE can be easily in-   corporated into existing fine - tuning methods   to improve their adversarial robustness further .   The empirical analysis confirms that ROSE   eliminates unrobust spurious updates during   fine - tuning , leading to solutions corresponding   to flatter and wider optima than the conven-   tional method . Code is available at https :   //github.com / jiangllan / ROSE .   1 Introduction   Recently , the discipline of fine - tuning large - scale   pre - trained language models has gained promi-   nence , achieving remarkable performances across   various natural language processing benchmarks .   However , recent studies ( Ribeiro et al . , 2020 ; Jin   et al . , 2020 ; Nie et al . , 2020 ; Lin et al . , 2021 ; Jiang   et al . , 2022 ) have highlighted the lack of adver-   sarial robustness in models fine - tuned on specificdownstream tasks , i.e. , adapted models are vul-   nerable to various types of adversarial attacks . A   majority of adversarial examples fool models via   character or word - level perturbations , either those   tokens that are n’t found in training sets or super-   ficial cues attached to labels ( Li et al . , 2021b ; Le   et al . , 2022 ) . The vulnerability of adapted mod-   els can be attributed to their tendency to capture   shallow and spurious patterns when fine - tuning on   downstream tasks , instead of utilizing the general   linguistic knowledge they have learned during the   pre - training stage ( Sagawa et al . , 2020 ; Warstadt   et al . , 2020 ; Gouk et al . , 2021 ; Dong et al . , 2021 ) .   To address this issue , various defense methods   have been proposed , including adversarial train-   ing ( Goodfellow et al . , 2015 ; Zhu et al . , 2020 ;   Ivgi and Berant , 2021 ) , adversarial data augmen-   tation ( Zhang et al . , 2019 ; Zheng et al . , 2020 ; Si   et al . , 2021 ) and so on . Adversarial training and   adversarial data augmentation stand to provide the   most promising performance among all the defense   methods . They enhance adversarial robustness   by re - training models with additional adversarial   data generated either via human - crafting or by con-   ducting projected gradient ascent on the benign   examples . In essence , these methods prevent mod-   els from learning misleading features by covering   more diverse training data . However , they are lim-   ited to practice as they often require prohibitively   large attack search spaces , and are not generally   applicable to different types of attacks .   In this work , we present an attack - agnostic and   model - agnostic defense method called RObust   SElective Fine - Tuning ( ROSE ) to address these   challenges from a learning perspective . ROSE is   a novel fine - tuning method that conducts robust   updates selectively during the fine - tuning stage .   The intuition behind our method is straightforward :   only robust and informative updates of parameters   should be conducted . While the improper ones ,   which make the fine - tuned model capture superfi-2886   cial cues and overfit the training data of the down-   stream task , should be filtered out .   Specifically , we propose two strategies in re-   sponse to the above challenges : first - order ROSE   and second - order ROSE . Our first - order ROSE em-   ploys adversarial perturbations to clarify parame-   ters that are robust against slight perturbation in   the hidden space , enabling models to cope with   superficial patterns among examples with similar   semantics in the current step . Our second - order   ROSE allows models to counter superficial patterns   across examples with different semantics along the   fine - tuning process by smoothing the optimization   trajectory . We also propose an ensemble method to   aggregate the benefits of the above two strategies .   ROSE distinguishes parameters based on robust   criteria at each step in the backward process , then   performs robust updates while freezing the remain-   ing parameters .   Figure 1 illustrates the loss landscapes of solu-   tions found by different fine - tuning strategies ( pre-   trained initial , vanilla fine - tuning , overfitting and   ROSE - tuning ) on specific tasks . ROSE leads to so-   lutions corresponding to broader and flatter optima   compared to traditional fine - tuning methods , which   implies that it achieves better adversarial robust-   ness as found in Goodfellow and Vinyals ( 2015 ) .   Moreover , our probing experiment illustrates that   ROSE prefers deeper linguistic features rather than   shallow lexical ones during fine - tuning . The above   empirical analysis confirms the inner working of   ROSE . ROSE allows for more robust solutions by   masking out unreliable and spurious updates when   fine - tuning models on downstream tasks .   We conduct extensive experiments to evaluate   the effectiveness of ROSE . We compare ROSE to   several strong defense methods . The results show   that ROSE exhibits superior adversarial robustnesson challenging examples , and achieves compara-   ble or even better benign performance on several   benchmarks . ROSE is generic and can be incor-   porated with existing methods to further enhance   their adversarial robustness .   2 Methodology   In this section , we will introduce our method in   detail . The key motivation of our method is to se-   lect parameters that carry stable information for   downstream tasks during fine - tuning . Specifically ,   the vanilla fine - tuning updates all the parameters   in the backward process , while ROSE only updates   the robust and informative ones . To identify robust   parameters , we propose two robustness criteria and   corresponding selective fine - tuning strategies : first-   order ROSE ( Section 2.1 ) and second - order ROSE   ( Section 2.2 ) . Furthermore , we explore an ensem-   ble robust selective fine - tuning method ( Section   2.3 ) , which aggregates the benefits of the above   two strategies . The overall training algorithm of   ROSE when applied to AdamW ( Loshchilov and   Hutter , 2019 ) is shown in Algorithm 1 .   2.1 First - order ROSE   Our first - order ROSE aims to select parameters   that are insensitive to first - order perturbation in the   hidden space . We employ adversarial inputs to dis-   tinguish robust parameters . Different from the con-   ventional virtual adversarial examples generated   via PDG - based methods , ROSE adopts dropout to   generate adversarial perturbation with little over-   head cost . We follow the method used in Gao et al .   ( 2021 ) ; Liang et al . ( 2021 ) , which passes the same   input to the model twice in the forward process   with different dropout , and obtains two outputs   correspondingly . Then in the backward process ,   ROSE only updates parameters that are insensitive2887Algorithm 1 ROSE for AdamW Optimizer   given α= 0.001 , β , β∈[0,1 ) , ϵ= 10 , λ∈Rinitialize time step t←0 , parameter vector θ∈R , first moment vector m←0 , second   moment vector v←0 , learning rate η∈Rrepeat t←t+ 1 g← ∇L(θ ) r← ∥∇L(θ)∥ ▷calculate first - order risks r← |(1−β)∥g∥/∥m∥−1| ▷calculate second - order risks M←CalculateMask ( r , r ) ▷calculate mask g←M⊙g+ ( J −M)⊙m ▷update gradients m←βm+ ( 1−β)g v←βv+ ( 1−β)g ˆm←m/(1−β ) ˆv←v/(1−β ) θ←θ−η / parenleftbigg   ˆm/(√ˆv+ϵ ) + λθ / parenrightbigg   ⊙M ▷update weightsuntil stopping criterion is metreturn optimized parameters θ   to the difference between the two outputs .   Formally , we denote an initial pre - trained model   asθ , the two probability distributions produced   with different dropout at the t - step as P , P , and   the Kullback - Leibler ( KL ) divergence between   them is defined as follows :   L = D(P∥P ) + D(P∥P ) . ( 1 )   In the backward process , first - order ROSE filters   out parameters which incline to learn superficial   cues between similar examples . The potential risk   rof the i - th parameter in model is computed as   theℓnorm of the gradient with regard to L :   r=∥(∇L)(θ)∥. ( 2 )   Then we sort the magnitude of the sensitivity from   r , r,···,rinto r , r,···,r in   ascending order .   Given the upper threshold c(e.g . ,60 % ) for   robust parameters , the mask Mcan be derived as :   M=/braceleftigg   1i / n≤c ,   0otherwise.(3 )   Note that , only the classification loss will be   used to update the weights of models , while gra-   dients with regard to Lwill be discarded after   calculating masks.2.2 Second - order ROSE   Our second - order ROSE smooths the optimization   trajectory to prevent models from learning spu-   rious patterns between different groups of data   points along the fine - tuning process . More pre-   cisely speaking , our second - order ROSE selects   and tunes parameters that are less aggressively   updated to avoid overfitting on spurious patterns .   A straightforward idea is to calculate the second   derivatives of the classification loss as the second-   order risks . Unfortunately , it requires prohibitive   computation and storage costs . Thus we employ a   stochastic gradient - based optimizer like AdamW   to approximate this solution .   Formally , we denote the softmax cross - entropy   loss at the t - step as L , and the first momentum   of optimizer as m. Then the second - order risk   rof the i - th parameter in the model is defined as   the relative magnitude between current gradients g   and the exponential moving average m , which   is computed as :   r=/vextendsingle / vextendsingle / vextendsingle / vextendsingleα∥g∥   ∥m∥−1 / vextendsingle / vextendsingle / vextendsingle / vextendsingle , ( 4 )   where αis a scaling coefficient . In AdamW , α=   ( 1−β ) , and βis the momentum factor .   Similar to our first - order ROSE , we sort the mag-   nitude of the second - order risks rin ascending or-   der and calculate the second - order mask Mwith   Eq . 3.28882.3 Ensemble ROSE   Since our first - order and second - order ROSE em-   phasize different kinds of robust parameters , we   then propose an ensemble method to aggregate the   benefits of the above two mechanisms .   At the t - step , we first calculate both the first-   order risks r,···,rwith Eq . 2 and second-   order risks r,···,rwith Eq . 4 . Then we   sort both of them in ascending order . Given upper   thresholds candc , we can compute the first-   order and second - order masks as : MandM ,   respectively . Finally , the ensemble mask Mat   t - step is computed as :   M = γM+ ( 1−γ)M , ( 5 )   where γ∈(0,1)is a scaling coefficient hyper-   parameter to control the weight of two masks .   3 Experiment   3.1 Datasets   We demonstrate the effectiveness of our method   using four tasks from GLUE ( Wang et al . , 2019 )   and AdvGLUE ( Wang et al . , 2021b ) benchmarks .   The General Language Understanding Evaluation   ( GLUE ) is a widely - used benchmark , including 9   natural language understanding tasks . The Adver-   sarial GLUE ( AdvGLUE ) is a robustness bench-   mark based on GLUE , covering 14prevalent ad-   versarial textual attack methods . The AdvGLUE   adopts careful systematic annotations to curate   high - quality and reliable adversarial examples . We   do not employ automatic adversarial attack algo-   rithms to evaluate the adversarial robustness , due   to their tendency to produce incorrect or puzzling   adversarial examples ( Li et al . , 2021a ) .   SST-2 ( Socher et al . , 2013 ) is a sentiment classi-   fication task with single - sentence inputs , which is   collected from movie reviews .   RTE ( Bentivogli et al . , 2009 ) is a natural lan-   guage inference task aggregated from a series of   textual entailment challenges , originating from   news articles and Wikipedia content .   QNLI ( Rajpurkar et al . , 2016 ) is also an infer-   ence task . Given the context sentence and corre-   sponding question , this task is to determine whether   it provides the answer .   QQP(Chen et al . , 2018 ) is a widely used bench-   mark involving detecting semantic similarity . Anannotated binary label indicates whether two texts   of each pair are semantically same or not .   3.2 Baselines   We adopt pre - trained checkpoints of RoBERTa   and RoBERTa ( Liu et al . , 2019 ) as the basis   of our experiments . Besides the vanilla fine - tuning   method , we select several suitable baselines for   comparison including :   R - Drop ( Liang et al . , 2021 ) is a generic regular-   ization strategy , which assures consistency between   two outputs obtained by different dropout . ROSE   borrows the idea of dropout twice , but does not   fine - tune all parameters to constrain the divergence   between two outputs .   CHILD - TUNING(Xu et al . , 2021 ) is a fine-   tuning technique , which only updates the most in-   formative subset of parameters of large pre - trained   models during the backward process . Although   both CHILD - TUNING and ROSE mask out gradi-   ents in the backward process , the specific parame-   ters they update are completely different .   SMART ( Jiang et al . , 2020 ) is an adversarial   training approach , contains a regularization module   induced by smoothness and a optimization module   inspired from Bregman proximal point method .   FreeLB ( Zhu et al . , 2020 ) is an adversarial train-   ing approach built on the top of language models ,   which improves higher invariance in word embed-   ding space and reduces the adversarial risk sur-   rounding examples .   3.3 Experimental Settings   Our implementation of ROSE is based on Hugging-   face library(Wolf et al . , 2020 ) . Batch size for   RTE is set to 16 , and for other tasks it is set to 32 .   Dropout rates are all set to 0.1 . We carry out grid   search of learning rate ∈ { 1e−5,2e−5,···,1e−   4}and upper threshold ∈ { 10%,20%,···,90 % } .   For ROSE - ensemble , we simply set γ= 0.5 in   Eq . 5 . The maximum number of epochs is set to   10 . For the replication study , we report the average   accuracy over 5random seeds on the GLUE and   AdvGLUE development sets after fine - tuning the   pre - trained models on the corresponding GLUE   training data .   For all the baselines , we either perform grid   search or adopt the parameter combination pro-   vided by the official codebase to select the best pa-   rameters . Similarly , we report the average results2889   on two benchmarks over 5random seeds using the   same evaluation schema .   3.4 Main Results   We compare ROSE - First , ROSE - Second , and   ROSE - Ensemble to all baselines on SST-2 , RTE ,   QNLI , and QQP tasks from GLUE and AdvGLUE   benchmarks . The overall results are summarized in   Table 1 . We observe that :   ( 1 ) Our proposed ROSE substantially improves   the robustness of fine - tuned pre - trained language   models , while maintaining competitive benign per-   formances to previous methods . Despite the effec-   tiveness of ROSE - First and ROSE - Second varies   on tasks and model size , both of them surpass   the existing methods . ROSE - Ensemble aggre-   gates the advantages of first - order and second-   order strategies , providing the strongest adversar-   ial robustness . In particular , ROSE - Ensemble   outperforms vanilla RoBERTa model by   8.04 % average score . ROSE - Ensemble beats   RoBERTa by4.29 % on average .   ( 2 ) ROSE consistently outperforms CHILD-   TUNINGand R - Drop , which both share some   similarities with our method . CHILD - TUNING ,   which masks out the most inessential parameters in   the backward process , shows the worst robustness   on most datasets . R - Drop uses dropout to regu-   larize the output of models . Results indicate that   R - Drop improves robustness on a number of tasks , but it is not competitive with strong defense meth-   ods . We will explore the effectiveness of our robust   selection strategies further in Section 3.5 .   ( 3 ) Our method also surpasses the two strong   baselines SMART and FreeLB , which employ the   most prevalent adversarial training idea to improve   the robustness of pre - trained models . For instance ,   ROSE - Ensemble enhances performance by up   to2.75 % average score over SMART . ROSE-   Ensemble gains 2.15 % average score improvement   compared to FreeLB with RoBERTa . Further-   more , SMART and FreeLB are both inefficient and   heavily correlated with model structure , while our   ROSE does not suffer from these issues .   3.5 Extensions of ROSE to Existing Method   ROSE is a generic method and can be easily in-   corporated into other well - recognized methods . In   this section , we incorporate ROSE into R - Drop and   examine whether it is still effective . Since the opti-   mization objective of R - Drop is a weighted sum of   softmax cross - entropy loss and KL - divergence , we   decouple them from the aggregated loss and use   them to perform our first - order and second - order   mask calculations , respectively . Noted that , in the   backward process we still use gradient calculated   with regards to the aggregated loss to update , which   is different from the ROSE process .   We primarily adopt the best parameter combi-   nations from the main experiments in Section 3.4,2890   including the learning rates and upper thresholds .   We follow the settings from R - Drop for other pa-   rameters . We conduct experiments using both   RoBERTa and RoBERTa .   Results are shown in Table 2 . Generally , ROSE   improves the adversarial robustness of R - Drop   by a large margin , and maintains competitive be-   nign performances at the same time . For example ,   ROSE - First promotes the adversarial robust-   ness of R - Drop on QNLI task from 28.92 % to   39.46 % . R - Drop patched with ROSE - Second wit-   nesses an improvement on QQP task from 44.80 %   to50.26 % using RoBERTa . Notably , our   ROSE - Ensemble outperforms R - Drop by roughly 3   points on average for both model sizes . The above   results indicate that when incorporated into exist-   ing methods , ROSE can enhance their adversarial   robustness even further .   3.6 Effect of Scalar γ   Further , we investigate the impact of the scaling   coefficient γin our ROSE - Ensemble . Here we   vary the γ∈ { 0.1,0.3,0.5,0.7,0.9}and conduct   experiments on four tasks , where γ= 0.5is the   current setting . We adopt the setting from Section   3.4 for other parameters .   The results are presented in Table 3 . It is shown   that the best - balanced choice is γ= 0.5 , but ROSE - Ensemble can stably improve the robustness using   other γ . Furthermore , ROSE achieves more sub-   stantial performance when applied to pre - trained   language models of greater complexity .   4 Analysis   In this section , we conduct further analysis to reveal   the inner working of ROSE .   4.1 Two - dimensional Loss Visualization   The loss landscape ( Goodfellow and Vinyals , 2015 ;   Li et al . , 2018a ) is a valid and effective indica-   tor to characterize the property of neural networks .   It has been empirically demonstrated that flatter   and wider optima correlate well with better robust-   ness . We plot and compare two - dimensional loss   landscapes of the solutions found by vanilla fine-   tuning and our ROSE . Visualizations on various   tasks show that our ROSE generally leads to flatter   and wider optima , thus improving the adversarial   robustness of adapted models .   Letθdenote the parameters of a model fine-   tuned on downstream tasks . Then the two-   dimensional loss curve of model can be plotted   with the function :   f(α , β ) = L(θ+αδ+βη ) , ( 6 )   whereLis the loss function , and α , β are scalar val-   ues.δ , ηare direction vectors randomly sampled   from Gaussian distribution , which denote two direc-   tion vectors in the parameter space corresponding   to the two axes of the loss surface . To remove   the scaling effect of neural nets , we follow the   filter - wise normalization in Li et al . ( 2018a ) , which   scales the δ , ηto the same norm as parameters by∥θ∥,∥θ∥. We set the range of both αand   βto[−0.25,0.25]and uniformly sample 51points2891   for each axis . Since the parameter space is high-   dimensional , experimental results confirm the two   directions δandηare divergent and orthogonal to   each other . We plot and compare the loss surfaces   of models with vanilla fine - tuning and ROSE on   four tasks .   The visualizations are shown in Figure 2 . We   can observe that ROSE has a significant influence   on the smoothness of the loss landscapes across   all datasets . Models fine - tuned with ROSE pro-   vide wider and less dense loss contours than vanilla   fine - tuning , which shows that they are more robust   against noisy perturbations . Specifically , ROSE-   First finds solutions with wider bottoms , and ROSE-   Second leads to solutions with less dense loss con-   tour . This indicates that ROSE - First and ROSE-   Second succeed in defense of local and globalperturbations , respectively . Additionally , ROSE-   Ensemble is shown to have both of these features ,   demonstrating that it aggregates the benefits of the   two strategies discussed above . Appendix A.1 pro-   vides an additional one - dimensional visualization .   4.2 Probing Preference for Different Features   We then employ the probing task from ( Warstadt   et al . , 2020 ) to test whether models fine - tuned with   ROSE prefer linguistic rather than superficial fea-   tures . In the probing experiment , a model is first   trained on ambiguous data which equally supports   both linguistic and superficial cues , and then tested   on disambiguating data which supports only the   linguistic cues . The preference of models for fea-   tures is measured through Matthews correlation   scores between predictions and labels on test sets.2892The models are shown a systematic preference for   linguistic features if the score is 1 , and complete re-   liance on superficial cues if the score is −1 . There-   fore a higher score shows a stronger preference for   linguistic features . We select two representative   experiments gotten by pairing the linguistic feature   Syntactic construction with two surface features   Lexical content andLength . For each probing task ,   we report results of adapted models with different   fine - tuning strategies on 5random seeds .   Results are plotted in Figure 3 . We can observe   that , compared to vanilla fine - tuned models , ROSE-   tuned models show a stronger preference for lin-   guistic features than any superficial features . This   indicates that ROSE successfully enables models   to extract deeper linguistic knowledge during fine-   tuning , instead of adopting spurious cues from the   training data of downstream tasks .   5 Related Work   Adversarial training is the most effective and   promising strategy to improve the adversarial ro-   bustness of models . Existing adversarial training   methods usually employ PDG - based attacks to gen-   erate adversarial examples , and force models to   maintain the output on them ( Zhu et al . , 2020 ;   Liang et al . , 2018 ; Wang et al . , 2021c ) . Despite the   substantial improvements in robustness , adversar-   ial training often requires significant computational   and memory costs and fails to preserve the original   labels . Some works focus on constructing reliable   adversarial datasets ( Gardner et al . , 2020 ; Eger and   Benz , 2020 ) , which require huge human annotation   and only work for a single task . By contrast , our   proposed ROSE is much more efficient and only   employs such perturbations to select robust param - eters to tune , therefore , there is no need for reliable   adversarial examples .   Besides adversarial training methods , our work   also relates to a few works of regularization and op-   timization . In regularization , lots of methods have   been proposed , including L - penalty ( Schwarz   et al . , 2018 ; Li et al . , 2018b ; Chen et al . , 2020 ) ,   weight decay ( Kang et al . , 2016 ; Zhang et al . ,   2021 ) , Mixout regularization ( Lee et al . , 2020 ) ,   and so on . The general approach is augmenting   the vanilla optimizer with terms that indirectly or   directly penalize the aggressive updates . Although   these methods are exciting , the regularization is   often not scalable , and hard to transfer to another   model . Another line of work ( Wang et al . , 2021a ;   Dong et al . , 2021 ) attempts to address this issue   from an informative - theoretic perspective . In op-   timization , there has been some work proposed   recently to force optimization towards wide val-   leys ( Chaudhari et al . , 2017 ; Jiang et al . , 2020 ) .   Compared to these works , ROSE uses the simplest   idea by selecting parameters with the second - order   robustness in fine - tuning stage to smooth the opti-   mization trajectory . ROSE is more efficient and can   be incorporated into existing methods to improve   their adversarial robustness further .   Note that our method does not fall within the   realm of model compression . The target of model   compression is to obtain an efficient sub - network   with competitive performance , with typical ap-   proaches to abandon some parameters when mod-   els do inference . While ROSE aims to improve   the adversarial robustness of pre - trained language   models , which is done via conducting selective   parameters updates in the backward process .   6 Conclusion   In this work , we propose an attack - agnostic and   model - agnostic defense approach called ROSE ,   which selectively updates robust parameters dur-   ing the fine - tuning stage . We present first - order   ROSE which selects the parameters robust against   slight perturbation in the hidden space , second-   order ROSE which filters out aggressive updates ,   and ensemble ROSE which aggregates the benefits   of the above two strategies . Experimental results   show that both our ROSE - First and ROSE - Second   greatly improve the robust performance on vari-   ous NLP benchmarks , while ROSE - Ensemble is   even more effective . Besides , existing methods   achieve better robustness when incorporated with2893our ROSE . We also demonstrate empirically that   the effectiveness of ROSE can be attributed to the   wider and flatter solutions it finds than the conven-   tional fine - tuning methods . We hope ROSE could   motivate more defense works for language models .   Limitations   Although ROSE achieves superior adversarial ro-   bustness on four datasets , there are still two limita-   tions . First , there are some vital hyper - parameters   in ROSE , e.g.the scaling coefficient γ , which have   a great influence on the performance as shown in   Section 3.6 . We adopt grid search to select the   best parameters , which requires considerable GPU   resources . There is still a need for a more auto-   matic method . Once we further understand the   inner working mechanism of deep neural networks ,   such hyper - parameters could be calculated theoret-   ically . Second , due to the limitation of computa-   tional resources , we focus on fine - tuning in this   work , leaving applying ROSE to pre - training for   future work . We hope ROSE could provide a new   perspective for general defense work towards more   robust language models .   References28942895   A Appendix   A.1 One - dimensional Linear Interpolation   In order to investigate the robustness of our method ,   we present parametric one - dimensional visualiza-2896tion as described in ( Goodfellow and Vinyals ,   2015 ) , which plots the value of loss function along   the line connecting two different models . Let θ   andθindicate the parameters of these two models   respectively . Then we plot the function :   f(α ) = L((1−α)θ+αθ ) , ( 7 )   where αis a scalar value . We compare the weights   of models obtained by vanilla and ROSE - Ensemble   fine - tuning method on four tasks . In particular , for   α∈[−0.5,1.5 ] , we uniformly sample 51points   and plot the function f(α)and superimpose the   classification accuracy .   Figure 4 shows the visualization of fine - tuned   models with different strategies . Compared with   vanilla fine - tuning , we can observe that our ROSE   provides wider and flatter curves . The result indi-   cates that solutions obtained by ROSE tend to be   more robust .   A.2 Effectiveness of Dropout   In order to investigate the effectiveness of dropout   in ROSE - First , For two outputs produced by differ-   ent dropout , we inspect the ratio ( % ) of prediction   labels that are not consistent with each other on the   first200,600,2000 and4000 steps .   From Table 4 we can see that , dropout generates   adversarial examples as expected with striking lowStep 200 600 2000 4000   SST-2 12.69 7.80 5.25 4.45   RTE 13.30 9.44 5.12 -   QNLI 18.44 11.61 7.70 6.32   QQP 12.31 9.58 7.35 6.45   cost . The ratio decreases fast at the beginning and   comes to stable finally , which indicates that ROSE-   First succeeds to improve the robustness of models   against such perturbation over the training process.2897