  Songyang Zhang , Linfeng Song , Lifeng Jin , Haitao Mi , Kun Xu ,   Dong YuandJiebo LuoUniversity of Rochester , Rochester , NY , USA   szhang83@ur.rochester.edu , jluo@cs.rochester.eduTencent AI Lab , Bellevue , WA , USA   { lfsong,lifengjin,haitaomi,kxkunxu,dyu}@tencent.com   Abstract   Video - aided grammar induction aims to lever-   age video information for finding more accu-   rate syntactic grammars for accompanying text .   While previous work focuses on building sys-   tems for inducing grammars on text that are   well - aligned with video content , we investi-   gate the scenario , in which text and video are   only in loose correspondence . Such data can   be found in abundance online , and the weak   correspondence is similar to the indeterminacy   problem studied in language acquisition . Fur-   thermore , we build a new model that can bet-   ter learn video - span correlation without man-   ually designed features adopted by previous   work . Experiments show that our model trained   only on large - scale YouTube data with no text-   video alignment reports strong and robust per-   formances across three unseen datasets , despite   domain shift and noisy label issues . Further-   more our model yields higher F1 scores than   the previous state - of - the - art systems trained on   in - domain data .   1 Introduction   Grammar induction is a fundamental and long-   lasting ( Lari and Young , 1990 ; Clark , 2001 ; Klein   and Manning , 2002 ) problem in computational lin-   guistics , which aims to find hierarchical syntactic   structures from plain sentences . Unlike supervised   methods ( Charniak , 2000 ; Collins , 2003 ; Petrov   and Klein , 2007 ; Zhang and Clark , 2011 ; Cross and   Huang , 2016 ; Kitaev and Klein , 2018 ) that require   human annotated treebanks , e.g. , Penn Treebank   ( Marcus et al . , 1993 ) , grammar inducers do not rely   on any human annotations for training . Grammar   induction is attractive since annotating syntactic   trees by human language experts is expensive and   time consuming , while the current treebanks are   limited to several major languages and domains . Recently , deep learning models have achieved   remarkable success across NLP tasks , and neural   models have been designed ( Shen et al . , 2018b , a ;   Kim et al . , 2019a , b ; Jin et al . , 2018 ) for grammar   induction , which greatly advanced model perfor-   mance on induction with raw text . Recent efforts   have started to consider other useful information   from multiple modalities , such as images ( Shi et al . ,   2019 ; Jin and Schuler , 2020 ) and videos ( Zhang   et al . , 2021 ) . Specifically , Zhang et al . ( 2021 ) show   that multi - modal information ( e.g. motion , sound   and objects ) from videos can significantly improve   the induction accuracy on verb and noun phrases .   Such work uses curated multi - modal data publicly   available on the web , which all assume that the   meaning of a sentence needs to be identical ( e.g. ,   being a caption ) to the corresponding video or im-   age . This assumption limits usable data to several   small - scale benchmarks ( Lin et al . , 2014 ; Xu et al . ,   2016 ; Hendricks et al . , 2017 ) with expensive hu-   man annotations on image / video captions .   The noisy correspondence between form and   meaning is one of the main research questions   in language acquisition ( Akhtar and Montague ,   1999 ; Gentner et al . , 2001 ; Dominey and Dodane ,   2004 ) , where different proposals attempt to address   this indeterminacy faced by children . There has   been computational work incorporating such in-   determinacy into their models ( Yu and Siskind ,   2013 ; Huang et al . , 2021 ) . For modeling empir-   ical grammar learning with multi - modal inputs ,   two important questions still remain open : 1 ) how   can a grammar inducer benefit from large - scale   multi - media data ( e.g. , YouTube videos ) with noisy   text - to - video correspondence ? and 2 ) how can a   grammar inducer show robust performances across   multiple domains and datasets ? By using data with   only weak cross - modal correspondence , such as   YouTube videos and their automatically generated   subtitles , we allow the computational models to   face a similar indeterminacy problem , and exam-233ine how indeterminacy interacts with data size to   influence learning behavior and performance of the   induction models .   In this paper , we conduct the first investigation   on both questions . Specifically , we collect 2.4 mil-   lion video clips and the corresponding subtitles   from instructional YouTube videos ( HowTo100 M   Miech et al . 2019 ) to train multi - modal grammar   inducers , instead of using the training data from a   benchmark where text and video are in alignment .   We then propose a novel model , named Pre - Trained   Compound Probabilistic Context - Free Grammars   ( PTC - PCFG ) , that extends previous work ( Shi et al . ,   2019 ; Zhang et al . , 2021 ) by incorporating a video-   span matching loss term into the Compound PCFG   ( Kim et al . , 2019a ) model . To better capture the   video - span correlation , it leverages CLIP ( Miech   et al . , 2020 ) , a state - of - the - art model pretrained on   video subtitle retrieval , as the encoders for both   video and text . Compared with previous work   ( Zhang et al . , 2021 ) that independently extracts   features from each modality before merging them   using a simple Transformer ( Vaswani et al . , 2017 )   encoder , the encoders of our model have been pre-   trained to merge such multi - modal information ,   and no human efforts are needed to select useful   modalities from the full set .   Experiments on three benchmarks show that our   model , which is trained on noisy YouTube video   clips and no data from these benchmarks , produces   substantial gains over the previous state - of - the - art   system ( Zhang et al . , 2021 ) trained on in - domain   video clips with human annotated captions . Further-   more , our model demonstrates robust performances   across all three datasets . We suggest the limitations   of our model and future directions for improve-   ments through analysis and discussions . Code will   be released upon paper acceptance .   In summary , the main contributions are :   •We are the first to study training a grammar in-   ducer with massive general - domain noisy video   clips instead of benchmark data , introducing the   indeterminacy problem to the induction model .   •We propose PTC - PCFG , a novel model for un-   supervised grammar induction . It is simpler in   design than previous models and can better cap-   ture the video - text matching information .   •Trained only on noisy YouTube videos without   finetuning on benchmark data , PTC - PCFG re-   ports stronger performances than previous mod - els trained on benchmark data across three bench-   marks .   2 Background and Motivation   2.1 Compound PCFGs   A PCFG model in Chomsky Normal Form can be   defined as a tuple of 6 terms ( S , N , P , Σ , R , Π ) ,   where they correspond to the start symbol , the sets   of non - terminals , pre - terminals , terminals , produc-   tion rules and their probabilities . Given pre - defined   numbers of non - terminals and pre - terminals , a   PCFG induction model tries to estimate the proba-   bilities for all production rules .   The compound PCFG ( C - PCFG ) model ( Kim   et al . , 2019a ) adopts a mixture of PCFGs . Instead   of a corpus - level prior used in previous work ( Kuri-   hara and Sato , 2006 ; Johnson et al . , 2007 ; Wang   and Blunsom , 2013 ; Jin et al . , 2018 ) , C - PCFG im-   poses a sentence - specific prior on the distribution   of possible PCFGs . Specifically in the generative   story , the probability πfor production rule ris   estimated by model gthat assigns a latent variable   zfor each sentence σ , andzis drawn from a prior   distribution :   π = g(r , z;θ),z∼p(z ) . ( 1 )   where θrepresents the model parameters . The prob-   abilities for all three types of CFG rules are defined   as follows :   π = exp(uf([w;z]))/summationtextexp(uf([w;z ] ) ) ,   π = exp(u[w;z])/summationtext exp(u[w;z ] ) ) ,   π = exp(uf([w;z]))/summationtextexp(uf([w;z ] ) ) ,   ( 2 )   where A∈ N , BandC∈ N ∪ P , T∈ P , w∈Σ.   Bothwanduare dense vectors representing words   and all types of non - terminals , and fandfare   neural encoding functions .   Optimizing the C - PCFG model involves maxi-   mizing the marginal likelihood p(σ)of each train-   ing sentence σfor all possible z :   logp(σ ) = log / integraldisplay / summationdisplayp(t|z)p(z)dz(3 )   whereT(σ)indicates all possible parsing trees for   sentence σ . Since computing the integral over z234is intractable , this objective is optimized by max-   imizing its evidence lower bound ELBO ( σ;ϕ,θ ):   ELBO ( σ;ϕ , θ ) = E[logp(σ|z ) ]   −KL[q(z|σ)||p(z)],(4 )   where q(z|σ)is the variational posterior calcu-   lated by another neural network with parameters   ϕ. Given a sampled z , the log - likelihood term   logp(σ|z)is calculated via the inside algorithm .   The KL term can be computed analytically when   both the prior p(z)and the variational posterior   q(z|σ)are Gaussian ( Kingma and Welling , 2014 ) .   2.2 Multi - Modal Compound PCFGs   Multi - Modal Compound PCFGs ( MMC-   PCFG ) ( Zhang et al . , 2021 ) extends C - PCFG with   a model to match a video vwith a span cin a   parse tree tof a sentence σ . It extracts Mvisual   and audio features from a video vand encodes   them via a multi - modal transformer ( Gabeur   et al . , 2020 ) , denoted as Ψ={ψ } . The word   representation hof the ith word is computed by   BiLSTM . Given a particular span c = w , . . . , w ,   its representation cis the weighted sum of all   label - specific span representations :   c=/summationdisplayp(k|c , σ)f / parenleftigg   1   j−i+ 1 / summationdisplayh / parenrightigg   , ( 5 )   where { p(k|c , σ)|1≤k≤ |N| } are the phrasal   label probabilities of span c. The representation of   a span cis then correspondingly projected to M   separate embeddings via gated embedding ( Miech   et al . , 2018 ) , denoted as Ξ={ξ } . Finally   the video - text matching loss is defined as a sum   over all video - span matching losses weighted by   the marginal probability of a span from the parser :   s(v , σ ) = /summationdisplayp(c|σ)h(Ξ , Ψ),(6 )   where h(Ξ , Ψ)is a hinge loss measuring the   distances from video vto the matched and un-   matched ( i.e.span from another sentence ) span c   andcand the distances from span cto the matchedand unmatched ( i.e.another video ) video vandv :   ω(c ) = exp(uc)/summationtextexp(uc ) , ( 7 )   o(Ξ , Ψ ) = /summationdisplayω(c)cos(ξ , ψ ) , ( 8)   h(Ξ , Ψ ) = E[o(Ξ , Ψ)−o(Ξ , Ψ ) ) + ϵ ]   + E[o(Ξ , Ψ)−o(Ξ , Ψ ) + ϵ],(9 )   where Ξis a set of unmatched span expert em-   beddings of Ψ , Ψis a set of unmatched video   representations of Ξ,ϵis a positive margin , [ · ]=   max(0,·),{u}are learned weights , and the   expectations are approximated with one sample   drawn from the training data . During training , both   ELBO and the video - text matching loss are jointly   optimized .   2.3 Limitation and Motivation   Existing work on multi - modal grammar induction   aims at leveraging strict correspondence between   image / video and text for information about syntac-   tic categories and structures of the words and spans   in the text . However , such datasets are expensive to   annotate . Besides , the ambiguous correspondence   between language and real - world context , observed   in language acquisition , is not really reflected in   such training setups .   As a result , we believe that the previous work   fails to answer the following important questions :   1 ) how well a grammar inducer would perform   when it is trained only on noisy multi - media data ;   2 ) how the scale of training data would affect the   performance and cross - domain robustness ?   3 Training a Grammar Inducer with   Massive YouTube Videos   We make the first investigation into the above ques-   tions by leveraging massive video clips from in-   structional YouTube videos to train our grammar   inducer . Different from the benchmark data used   by previous work , the YouTube video clips do not   contain paired sentences . This section will first   introduce the method for generating noisy train-   ing instances ( video clip and sentence pairs ) from   YouTube videos ( § 3.1 ) , before describing a novel   grammar induction model ( § 3.2 ) with pre - trained   text and video encoders.235   3.1 Harvesting Training Instances from   YouTube Videos   Given a YouTube video , we would like to generate   a set of video clip and subtitle pairs Ω = { ( v , σ ) } ,   where each subtitle σis a complete sentence and is   aligned in time with its paired video clip v. To this   end , the YouTube API is chosen to obtain all sub-   titles of the video . But , our observation finds that   most obtained subtitles are not complete sentences ,   and in some cases , a complete sentence can last for   several continuous video fragments . Meanwhile ,   they do not contain any punctuation , which is a   key factor for sentence segmentation . As shown   in the top part of Figure 1 , we design an algorithm   that takes the following steps to find each complete   sentence and its corresponding video clip .   Sentence segmentation . In the first step , we try   to find complete sentences from the subtitles . We   first concatenate all subtitles from the same video   are concatenated into a very long sequence of to-   kens . Next , a punctuation restoration model(Tilk   and Alumäe , 2016 ) is adopted to insert punctuation   into the sequence . Lastly , sentences are segmented   based on certain punctuation ( e.g. , “ . ” , “ ? ” , “ ! ” ) .   Video clip extraction . In the second step , we   trim the corresponding video clips . Each raw sub-   title contains its start and an end times . We as-   sume each word within the raw subtitle occupies   equal time and record the start and end times foreach word . After that , given a complete sentence   σ = w , w , ... , w , we use the start time of its   first word wand the end time of its last word w   as the start and end times of σ . Lastly , we segment   a complete sentence σ ’s corresponding video clip   vbased on its start and end times .   3.2 Model : Pre - Trained Compound PCFGs   After harvesting large - scale sentence and video   pairs , the next step is to build a strong grammar   induction model that can benefit from them . In this   section , we introduce our Pre - Trained Compound   PCFGs ( PTC - PCFG ) model for unsupervised gram-   mar induction . As shown in the lower part of Fig-   ure 1 , the PTC - PCFG model composes of a video   encoder , a span encoder and a parsing model . Both   the video encoder and the span encoder are initial-   ized from the MIL - NCE model ( Miech et al . , 2020 ) ,   a pre - trained video - text matching model that takes   a simple design and has shown superior zero - shot   results on many video understanding tasks , such as   video retrieval , video question answering , etc . We   first introduce the pre - trained video and span en-   coders , before covering the training and inference   details of PTC - PCFG .   Video encoding . The first step is to encode a video   vto its representation v. To do this , we first seg-   ment vinto small video clips , where each video   clipvconsists of Tframes . Following Zhang et al .   ( 2021 ) , we sample Lvideo clips with equal interval   for efficiency . We use the video encoder from the   MIL - NCE model ( Miech et al . , 2020 ) as our video   encoder and only fine - tune its last fully connected236layer ffor efficiency . In more detail , for each   sampled video clip , we pre - compute the input of   fas its representation , denoted as { h } . Then   we feed them into fand average the output as its   representation v , denoted as ,   v = AvgPool ( { f(h ) } ) , ( 10 )   where AvgPool indicates average pooling .   Span encoding . The next step is to compute   a span representation cfor each particular span   c = w , . . . , w(1≤i < j ≤N)in sentence   σ = w , w , . . . , w. The pre - trained text encoder   of MIL - NCE consists of a word embedding layer   and two stacked fully connected layers , fandf .   Motivated by Zhao and Titov ( 2020 ) ; Zhang et al .   ( 2021 ) , we expect to learn |N|different span repre-   sentations , each is specified for one non - terminal   node . However , directly applying the pre - trained   text encoder is not feasible , since it has only one   output layer f. Therefore , we duplicate ffor   |N|times , denoted as { f } , and compose |N|   label - specific output layers . In more detail , we   first encode each word wwith the word embed-   ding layer , denoted as h. Then we feed the word   embeddings to f , ReLU , maximum pooling and   each label - specific output layer sequentially . we   also compute the probabilities of its phrasal labels   { p(k|c , σ)|1≤k≤ |N| } , as illustrated in Sec-   tion 2.1 . Lastly , the span representation cis the sum   of all label - specific span representations weighted   by the probabilities we predicted , denoted as :   τ = MaxPool ( ReLU(f(h ) ) )   c=/summationdisplayp(k|c , σ)f(τ),(11 )   where MaxPool is a maximum pooling operation   andReLU is a ReLU activation function .   Training . As shown in lower left of Figure 1 , we   optimize both the video - text matching loss and   evidence lower bound during training . We first   compute the similarity between a video clip vand a   particular span cvia dot product and then compute   a triplet hinge loss as following ,   h(v , c ) = E[c·v−c·v+ϵ ]   + E[c·v−c·v+ϵ ] , ( 12 )   where ϵis a positive margin , [ · ]= max(0,·),v   is a clip from a different video and cis a span froma different sentence . The video - text matching loss   is correspondingly defined as ,   s(v , σ ) = Σp(c|σ)h(v , c ) , ( 13 )   where p(c|σ)is the probability of a particular span   cbeing a syntactic phrase . Finally , the overall loss   function is composed by the ELBO and the video-   text matching loss :   L(ϕ , θ ) = /summationdisplay−ELBO ( σ;ϕ , θ ) + αs(v , σ ) ,   ( 14 )   where αis a constant balancing these two terms .   Inference . During inference , given a sentence σ ,   we predict the most likely tree twithout accessing   videos , as shown in the lower right of Figure 1 .   Since computing the integral over zis intractable ,   we estimate twith the following approximation ,   t= arg max / integraldisplayp(t|z)p(z|σ)dz   ≈arg maxp(t|σ,µ(σ)),(15 )   where µ(σ)is the mean vector of the variational   posterior q(z|σ ) , and tis obtained by the CYK   algo . ( Cocke , 1969 ; Younger , 1967 ; Kasami , 1966 ) .   4 Experiments   4.1 Datasets   Following previous work , we evaluate all systems   on three benchmarks ( i.e. , DiDeMo , YouCook2 and   MSRVTT ) . Instead of training on benchmark data ,   our models are trained on the data harvested from   HowTo100 M dataset . Below shows more details   about these datasets :   DiDeMo ( Hendricks et al . , 2017 ) contains 10k   unedited personal Flickr videos . Each video is   associated with roughly 3 - 5video - sentence pairs .   There are 32 994 , 4 180 and4021 video pairs in the   training , validation and testing sets .   YouCook2 ( Zhou et al . , 2018 ) contains 2000   long untrimmed YouTube videos from 89cook-   ing recipes . The procedure steps for each video are   annotated with temporal boundaries and described   by imperative English sentences . There are 8 913 ,   969and3 310 video - sentence pairs in the training ,   validation and testing sets .   MSRVTT ( Xu et al . , 2016 ) contains 10k generic   YouTube videos accompanied by 200k captions an-   notated by paid human workers . There are 130 260 , 2379 940 and59 794 video - sentence pairs in the train-   ing , validation and testing sets .   HowTo100 M ( Miech et al . , 2019 ) is a large - scale   dataset of 136million video clips sourced from   1.22 M narrated instructional web videos depict-   ing humans performing more than 23k different   visual tasks . Noted that there are 404videos in   HowTo100 M exists in YouCook2 , we exclude these   videos during training .   4.2 Evaluation   We discard punctuation , lowercase all words , re-   place numbers with a special token and ignore   trivial single - word and sentence - level spans dur-   ing testing following Kim et al . ( 2019a ) . Besides ,   we follow previous work ( Shi et al . , 2019 ; Zhang   et al . , 2021 ) by using a state - of - the - art constituency   parser ( Benepar Kitaev et al . 2019 ) to obtain the   reference trees for evaluation . Following Shi et al .   ( 2020 ) ; Zhang et al . ( 2021 ) , all models are run 5   times for 1epoch with different random seeds . For   each model , we report the averaged sentence - level   F1 ( S - F1 ) and corpus - level F1 ( C - F1 ) of its runs on   each testing set .   4.3 Implementation Details   We use Spacyfor tokenization and keep sentences   with fewer than 40words for training due to the   limited computational resources . Each video is de-   coded at 16fps and L= 8video clips are sampled   in total , where each clip contains T= 16 frames .   We train baseline models , C - PCFG and MMC-   PCFG with the same hyper - parameters suggested   by Kim et al . ( 2019a ) and Zhang et al . ( 2021 ) . The   parsing model of PTC - PCFG has the same hyper-   parameter setting as C - PCFG and MMC - PCFG   ( Please refer their papers for details ) . The constant   αis set to 1 . We select the top 20 000 most com-   mon words in HowTo100 M as vocabulary for all   datasets . All baseline methods and ours are op-   timized by Adam ( Kingma and Ba , 2015 ) with a   learning rate of 0.001,β= 0.75andβ= 0.999 .   All parameters ( except the video - text matching   model in PTC - PCFG ) are initialized with Xavier   uniform initializer ( Glorot and Bengio , 2010 ) . All   our models in experiments are trained for 1epoch   with batch size of 32 , without finetuning on thetarget dataset .   4.4 Main Results   Figure 2 - 4 compare our proposed PTC - PCFG ap-   proach with recently proposed state - of - the - art mod-   els : C - PCFG ( Kim et al . , 2019a ) and MMC-   PCFG(Zhang et al . , 2021 ) . To pinpoint more   fine - grained contributions , we also train these mod-   els on HowTo100 M data .   The effectiveness of HowTo100M. We find that   C - PCFG achieve better performance when they   are trained with more instances from HowTo100 M   than the original in - domain training sets , where the   largest improvements are +18.1%,+21.7%and   +1.4%S - F1 scores on DiDeMo , YouCook2 and   MSRVTT , respectively . These results indicate   that grammar inducers are generally robust against   the instances with noisy text - video correspondence .   As the results , learning from noisy YouTube videos   can benefit model ’s overall performance and its   generalization ability across multiple domains .   The effectiveness of PTC - PCFG . Comparing C-   PCFG , MMC - PCFG and PTC - PCFG trained on   different amount of HowTo100 M data , we found   that PTC - PCFG achieves the best performances in   all three datasets . It can further improve S - F1 to   +6.3%on DiDeMo , +16.7%on YouCook2 and   +2.8%on MSRVTT . This demonstrates the effec-   tiveness of the PTC - PCFG model . In particular ,   utilizing the video and span encoders pre - trained   on a relevant tasks ( e.g. , video retrieval ) can benefit   unsupervised grammar induction .   Performance comparison over data scale . On   DiDeMo and MSRVTT , we observe that PTC-   PCFG achieves the best performance with 592k   HowTo100 M training samples , and further increas-   ing the number of training instances does not   improve the parsing performance on these two   datasets . In contrast , the performance gain of PTC-   PCFG on YouCook2 further increases with increas-   ing training data . The reason can be that the domain   of HowTo100 M is closer to YouCook2 ( both are in-   structional videos ) than the other two datasets . Fu-   ture work includes adding data from other sources   to the whole training set more domain generic.238   4.5 Cross - dataset Evaluation   We evaluate the robustness of models across dif-   ferent datasets , as shown in Table 1 . Comparing   MMC - PCFG trained on in - domain datasets ( Row   1 - 3 ) , we can observe that MMC - PCFG trained on   MSRVTT achieves the best overall performance ,   while MMC - PCFG trained on YouCook2 is the   worst . We believe this is due to the different num-   ber of training instancesand the domain gap be-   tween different datasets . Comparing Rows 1 - 4 , we   can observe that the MMC - PCFG model trained   on HT(592k ) ( Row 4 ) is the best or the second   place regarding C - F1 and S - F1 compared with its   variants trained on in - domain datasets ( Rows 1 - 3 ) .   This demonstrates that the our processed video-   text training instances are abundant , rich in con-   tent and can serve for general purpose . Compar-   ing Rows 4and5 , PTC - PCFG outperforms MMC-   PCFG in both C - F1 and S - F1 in all three datasets   and has smaller variance . This demonstrate that our   model can leverage pre - trained video - text matching   knowledge and learn consistent grammar induction .   4.6 Effectiveness of Pre - Training   In this section , we explore how different pre-   trained video and text encoders can affect the   parsing performance , and the results are shown   in Table 2 . In particular , we study different239   video encoders , including the S3D - based en-   coder from MIL - NCE ( Miech et al . , 2020 ) ( MIL-   NCE ) , the multi - modal video encoder from MMC-   PCFG ( Zhang et al . , 2021 ) ( MM ) and the CLIP   model for image - text pre - training ( Radford et al . ,   2021 ) ( CLIP ) . We also investigate various text en-   coders , including an LSTM encoder with random   initialization ( Zhang et al . , 2021 ; Zhao and Titov ,   2020 ) , a pre - trained TinyBERT ( Jiao et al . , 2020 )   model , the text encoder from MIL - NCE ( Miech   et al . , 2020 ) , and the text encoder from CLIP ( Rad-   ford et al . , 2021 ) .   Comparing Rows 1with2 , we can observe that   MM is better than the video encoder of MIL - NCE   regarding C - F1 and S - F1 on all three datasets , as   MM provides more comprehensive video features .   By comparing row 1with3 , we can also observe   that TinyBERT , which is distilled from BERT ( De-   vlin et al . , 2019 ) , outperforms the randomly ini-   tialized LSTM encoder . However , both MM and   TinyBERT are independently trained only on vi-   sion or language tasks , where the vision - language   correspondences are not considered during pre-   training . Therefore , we further investigate the   encoders jointly pre - trained on large scale multi-   media datasets , including the video - text match-   ing model MIL - NCE ( Row 4 ) and the image - text   matching model CLIP ( Row 5 ) . We can observe   that by leveraging both video and text encoders in   MIL - NCE can improve the parsing performance   by a large margin on all three datasets . On the   other hand , CLIP does not perform well , since it is   designed for static images and other multi - modal   information ( e.g. , motion ) is ignored .   4.7 Qualitative Analysis   In figure 5 , we visualize a parser tree predicted   by the best run of C - PCFG trained on MSRVTT ,   MMC - PCFG trained on MSRVTT , MMC - PCFG   trained on HT(296k ) and PTC - PCFG trained on   HT(296k ) , as well as its reference tree . We can   observe that C - PCFG trained on MSRVTT fails at   noun phrase “ a lady ” , while MMC - PCFG trained   on MSRVTT succeeds . MMC - PCFG can be further   improved by training on HT(296k ) , however , fails   at noun phrase “ the groceries she had kept in her   refrigerator ” . Our PTC - PCFG can leverage the pre-   trained matching knowledge and make the correct   prediction .   5 Related Work   Grammar Induction has a long and rich history in   the computational linguistics . Earlier work ( Shen   et al . , 2018a , b ; Drozdov et al . , 2019 ; Kim et al . ,   2019a ; Jin et al . , 2019 ; Yang et al . , 2021a , b ) on   grammar induction with pure unsupervised learn-   ing showed promising results . Instead of learning   purely from text , recent work improved the pars-   ing performance with paired images ( Shi et al . ,   2019 ; Zhao and Titov , 2020 ) or videos ( Zhang   et al . , 2021 ) . However , they are all limited to small240benchmarks and specified for a few domains . In   contrast , our work leverages massive noisy video-   subtitle pairs from YouTube without any manual   annotations .   Video Retrieval has been a hot topic in the com-   puter vision field for many years . Earlier ap-   proaches focused on model design ( Gabeur et al . ,   2020 ; Zhang et al . , 2019 ) , while more recent ap-   proaches ( Radford et al . , 2021 ; Miech et al . , 2020 )   focused on the pre - training on a large scale dataset   and demonstrated superior zero - shot results on   many downstream tasks . These models are sim-   ple in design and provide representative features   with less human effort in annotations . In this work ,   we demonstrate that unsupervised grammar induc-   tion can also benefit from the pre - trained video - text   model .   6 Conclusion   In this paper , we have investigated how massive in-   structional YouTube video and subtitle pairs can im-   prove grammar induction . We have also proposed   a new model that leverages the latest advances   in multi - modal pre - training to learn better video-   span correlation . Experiments on three benchmarks   demonstrate superior and robust performances of   our model over previous systems . We leave explor-   ing other pre - trained video - text matching models   and more publicly available data ( e.g. , YouTube   videos from other domains and TV shows ) in fu-   ture work .   7 Limitations   Although our model faces a similar indeterminacy   problem like children do , and results show that   induction works even with noisy correspondence ,   there are a few factors which prevent this result   from being directly applied to language acquisition .   Our models only use instructional video and do   not have the capability to interact with the world ,   both of which are unrealistic for human language   learners . The complexity of the PCFG induction   algorithm we use is cubic to the number of syn-   tactic categories , therefore potentially limits the   usefulness of larger amounts of data , where finer   subcategories may be learned . Algorithms such as   in Yang et al . ( 2021b ) could be used in conjunction   with multimodal inputs to examine this issue .   Following previous work , our experiments are   only conducted on English video - text datasets .   However , our framework is general for grammarinduction in many languages . Since our training   instances are originally collected from Internet and   are uploaded by users , the dataset itself might have   misinformation . Meanwhile , training a model on a   large - scale dataset could have high cost in energy   and carbon emission . We list our computational   cost of our experiments in Appendix B.   References241242243A Video Processing Details   MIL - NCE . Following the implementationof MIL - NCE , we extract 1feature per second from video   encoder ’s last fully connected layer . All videos are decoded at 16frames per second ( fps ) .   MM . We list the details of object , action , scene , OCR and face feature extraction as below :   •ResNeXt ( Xie et al . , 2017 ) . We use the ResNeXt101 version implemented by torchvision . Videos   are decoded at 1fps and we extract 1feature per second from the last fully connected layer .   •SENet ( Hu et al . , 2018 ) . We use the SENet154 version implemented by Cadene . Videos are decoded   at1fps and we extract 1feature per second from the last fully connected layer .   • I3D(Carreira and Zisserman , 2017 ) . We use the I3D_8x8_R50 version implemented by SlowFast .   We decode videos at 4fps and extract 1feature per 2seconds from the last fully connected layer .   •S3DG ( Miech et al . , 2020 ) . We use the implementation from HERO . Videos are decoded at 30fps   and we extract 1feature per 1.5seconds from the global averaged pooling layer .   •R2P1D ( Tran et al . , 2018 ) . We use the r2plus1d_34 version implemented by torchvision . we decode   videos at 16fps and extract 1feature per 2seconds .   •Scene . We use densenet161 ( Huang et al . , 2017 ) implemented by CSAILVision . Videos are   decoded at 1fps and we extract 1feature per second from the last fully connected layer .   •OCR . We use the text detector PANet ( Wang et al . , 2019 ) and the text recognizer seg_r31 implemented   by MMOCR . we decode videos at 0.5fps and extract 1feature per 2seconds .   •Face . We use face detector MTCNN ( Zhang et al . , 2017 ) and face recognizer ( Schroff et al . , 2015 )   implemented by FaceNet . We decode videos at 1fps and extract 1feature per second .   CLIP . Following the implementationof CLIP , we extract 1video feature per second from ViT - B/32 ’s   last fully connected layer . All videos are decoded at 1fps .   B Computational Cost   All our models are trained on 2 32 GB V 100GPUs . The approximate time cost for each run of different   model is listed in Table 1 . For each model , we run 5times with different random seeds in parallel . During   training , the video encoder , the span encoder and C - PCFG are involved , which contains 76.6 M parameters   in total . During inference , since only C - PCFG is involved , there are 23.0 M parameters in total.244C Performance Comparison - Full Tables   We compare the performances of different models trained on different datasets . The full experiment   results are demonstrated in Table 2 - 4 . LBranch , RBranch and Random represent left branching tree , right   branching tree and random tree , respectively . In addition to C - F1 and S - F1 , we also evaluate the recall of   each model on different phrase types , including NP , VP , PP , SBAR , ADJP and ADVP . All numbers are   shown in percentage ( % ) .245246247