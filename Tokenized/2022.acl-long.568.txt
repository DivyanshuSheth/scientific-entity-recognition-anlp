  Leshem Choshen , Guy Hacohen , Daphna Weinshall , Omri Abend   Department of Computer Science   Department of Brain Sciences   Hebrew University of Jerusalem   Abstract   The learning trajectories of linguistic phe-   nomena in humans provide insight into lin-   guistic representation , beyond what can be   gleaned from inspecting the behavior of an   adult speaker . To apply a similar approach to   analyze neural language models ( NLM ) , it is   ﬁrst necessary to establish that different mod-   els are similar enough in the generalizations   they make . In this paper , we show that NLMs   with different initialization , architecture , and   training data acquire linguistic phenomena in   a similar order , despite their different end per-   formance . These ﬁndings suggest that there   is some mutual inductive bias that underlies   these models ’ learning of linguistic phenom-   ena . Taking inspiration from psycholinguis-   tics , we argue that studying this inductive bias   is an opportunity to study the linguistic repre-   sentation implicit in NLMs .   Leveraging these ﬁndings , we compare the rel-   ative performance on different phenomena at   varying learning stages with simpler reference   models . Results suggest that NLMs exhibit   consistent “ developmental ” stages . Moreover ,   we ﬁnd the learning trajectory to be approxi-   mately one - dimensional : given an NLM with   a certain overall performance , it is possible to   predict what linguistic generalizations it has al-   ready acquired . Initial analysis of these stages   presents phenomena clusters ( notably morpho-   logical ones ) , whose performance progresses   in unison , suggesting a potential link between   the generalizations behind them .   1 Introduction   Children present remarkable consistency in their   patterns of language acquisition . They often ac-   quire linguistic phenomena in a similar order ( Kuhl   et al . , 1992 ; Ingram , 1989 ) , and make similar gen-   eralizations and over - generalizations ( Kuczaj II ,   1977 ; Pinker , 1995 ) . This consistency provides an   important starting point for linguistic study . Forexample , arguments in favor of single or dual sys-   tem accounts of morphological representation are   often backed by computational models of children   learning trajectories ( e.g. , Rumelhart and McClel-   land , 1986 ; Pinker and Prince , 1988 ; Kirov and   Cotterell , 2018 ) . In this paper , we embrace this   program for the study of computational language   models , investigating learning trajectories .   The representations that language models ( LM )   acquire have been studied extensively , including   studying their learning dynamics to improve train-   ing ( see § 6 ) . However , very little work aimed at   drawing connections between the training dynam-   ics and the learned representations . In this work   we adopt a behavioral approach , thus revealing that   NLMs share learning trajectories and generalize   in similar ways during training . This implies that   studying trajectories of NLMs is worthwhile , in the   sense that results on one architecture or size are   expected to be reproducible by others .   These ﬁndings call for a characterization of these   trajectories , a new and promising territory for re-   search . We take ﬁrst steps to explore these direc-   tions , emphasizing their potential beneﬁt to a better   future understanding of what models learn .   Speciﬁcally , we train NLMs on next - word pre-   diction , but evaluate and compare them by tracking   their performance on grammar learning in English ,   using the BLIMP dataset ( See 2.1 ) . BLIMP is a   dataset that consists of 67 K minimal pairs , where   each pair includes a grammatically correct and   a grammatically erroneous sentence . NLMs are   tested for their ability to assign higher probability   to the correct one . See example in Table 1 , and   details of our experimental methodology in § 2 .   We begin ( § 3 ) by establishing that NLMs learn   grammatical phenomena in a consistent order . We   evaluate NLMs at different time points along their   training , showing that the performance on linguis-8281   tic phenomena across initializations is highly cor-   related . We further ﬁnd many similarities in the set   of examples that they correctly classify .   Still , models of different architectures learn at a   different pace , and hence can not be directly com-   pared at identical time points . In § 3.3 , we over-   come this by re - scaling the timeline . We then   show that despite architectural differences , NLMs   present highly correlated performance trajectories .   In § 3.4 , we further demonstrate that even the choice   of training data has minor inﬂuence on the results .   Finally , in § 3.5 we show that the learning dynam-   ics essentially follows a single dimension . Namely ,   where the average performance is similar , success   on linguistic phenomena is also similar .   We proceed by analyzing the early stages of   learning in § 4 . We ﬁnd that , at ﬁrst , NLMs rely   mostly on local cues and not on word order . They   thus resemble bag - of - words models over a window   of the preceding tokens . Later stages seem to drift   further away from bag - of - words models toward   n - gram models , and with time seem to be more   sensitive to structural cues . We also ﬁnd evidence   that some latent features that the model learns may   not be related to linguistic phenomena .   Finally , in § 5 we take the ﬁrst steps in catego-   rizing linguistic phenomena by their learning tra-   jectories . We identify links between their repre-   sentations by ﬁnding phenomena that progress in   unison . For example , we ﬁnd that morphological   phenomena are mostly learned at similar stages .   Of particular interest are cases where performance   decreases with time , which may suggest either over-   generalization or biases in the BLIMP challenges .   2 Experimental Setup   2.1 The BLIMP Dataset   We use BLIMP ( Warstadt et al . , 2019 ) to assess   the extent to which generalizations are made by   the NLMs . BLIMP includes 67 grammatical chal-   lenges categorized into 13 super - phenomena ( e.g. ,   island - related or quantiﬁers ) comprising of 4 broad   ﬁelds ( e.g. , Syntax , Semantics ) . Each challenge   consists of 1 K minimal pairs of sentences . A mini - mal pair contains a sentence and a near - duplicate   distractor that incorporates an error on a particular   linguistic phenomenon , i.e. , only the phenomenon   in question is changed between the sentences in a   pair ( see Table 1 ) . Each challenge includes pairs   with the same linguistic phenomenon .   2.2 Training   LM details : as training multiple GPT2 instances   ( Radford et al . , 2019 ) is computationally demand-   ing , we train smaller NLMs . Following Turc et al .   ( 2019 ) , we trained 1 instance of GPT2 ( width   768,12layers , 8attention heads ) and 4 instances of   GPT2(width 512,4layers , 4attention heads ) ,   with different random seeds .   Similarly , we train a small TransformerXL ( Dai   et al . , 2019 ) , XL ( width 512,4layers , 8at-   tention heads ) and a full - sized one ( width 4096 ,   18layers , 16attention heads ) . We stop the full   model after 600 K steps , while the perplexity re-   mained high . We use it for comparison to the   early stages of learning of TransformerXL . All   models ’ hyperparameters can be found in App . § B.   We also use the results of the fully trained GPT2 ,   TransformerXL , LSTM and human performance   reported in Warstadt et al . ( 2019 ) .   In § 4 , we compare NLMs with simpler models .   To this end , we create two GPT2variations ,   denoted BOW andWindow-5 . BOW replicates   GPT2 , but relies only on bag of words . This is   achieved by removing the positional weights , and   replacing the attention weights with a simple av-   erage . Window-5 similarly ignores the positions ,   and additionally only attends to the last 5 words .   Note that both are unidirectional LMs and consider   only previously predicted words at each step .   Unless explicitly stated otherwise ( as in § 3.4 ) ,   all models were trained on the WikiBooks dataset   ( Zhu et al . , 2015 ) , which contains the English   Wikipedia ( 2:1Bwords ) and BookCorpus ( 854 M   words ) . This dataset resembles BERT ’s train-   ing data ( Devlin et al . , 2019 ) , except that current   Wikipedia is used . Additionally , we trained models   on the following datasets : English openSubtitles   ( Lison and Tiedemann , 2016 ) , newsCrawl ( Barrault   et al . , 2019 ) , GigaWord ( Napoles et al . , 2012 ) , and8282a sample of openWebText ( 3B words ; Gokaslan   and Cohen , 2019 ) – a replication of GPT2 dataset .   Throughout this paper , we report Pearson corre-   lation . Using Spearman correlation leads to qualita-   tively similar conclusions . When multiple models   are correlated against each other , their average pair-   wise correlation is reported .   3 The Learning Order of NLMs   In this section , we examine various aspects of   NLMs , generally showing that their learning trajec-   tories are similar .   We evaluate network similarity by adopting a   behavioral approach . Accordingly , networks are   viewed as functions , whose latent features manifest   themselves only by their inﬂuence on the network ’s   behavior . Latent features are the unobserved causes   of the measured behavior . Consequently , parame-   ters , activation patterns and representations can be   completely different among similar models . This is   unlike the approaches employed by Williams et al .   ( 2018 ) ; Saphra and Lopez ( 2019 ) ; Liu et al . ( 2021 ) ,   which analyze internal representations directly .   To formalize the above notion , let Ldenote a   checkpoint , the language model Lat timet . Let   pv(L)denote its performance vector – the accu-   racy obtained by Lon each BLIMP challenge p :   pv(L ) = [ acc(L;p)]2R(1 )   Timetis measured in training steps or perplex-   ity . The trajectory of the performance vector as a   function oftreﬂectsL ’s training dynamics .   Given this behavioral deﬁnition , we focus on   comparing the relative strength of models . Similar-   ity between models is thus measured as the corre-   lation between their performance vectors . Hence ,   models are similar if they rank phenomena in the   same way . On the other hand , models of the same   average performance can be dissimilar : consider   two models that agree on everything except nouns .   One generates only feminine nouns and the other   plural nouns . The models ’ average performance   is similar , but due to their biases , they are correct   on different challenges . This dissimilarity suggests   that the models rely on different latent features .   3.1 Consistent Order of Learning   We begin by showing that models produced by   different initializations learn the same phenomena ,   in the same order . In terms of our deﬁnitions above ,   this may imply that despite converging to differentparameter values , the learned latent features and   the generalization patterns made are similar .   In order to examine the hypothesis empirically ,   we compute the correlation between 4 random ini-   tializations ( Fig . 1 ) . Results conﬁrm the hypothesis ,   the correlation between GPT2instances is ex-   tremely high . It is already high after 10 K steps , and   remains high throughout training . We note that the   correlation at step 0 is 0 ( not shown ) , and that after   10 K warm - up steps the network ’s ability as a LM   is still poor . For example , perplexity is 10.9 after   10 K steps and 6.7 after 70 K steps .   3.2 Effects of Architecture   Next , we show that different architectures also   present similar trajectories . As the learning pace is   not comparable across models , computing correla-8283tion in ﬁxed and identical intervals is not informa-   tive . Instead , we choose tto be the perplexity on   the development set , comparing models at the same   performance level . TransformerXL is not directly   comparable as perplexity requires the vocabulary   to be the same .   Following this paradigm , we see that GPT2   and GPT2are highly correlated ( > 0:9 ) , present-   ing similar learning order throughout training . Ob-   serving the trajectories per challenge qualitatively ,   we see that they align very well ( cf . Fig . 2 and App .   § A , § C ) . TransformerXL also seems to share the   general tendencies of the GPT2 architectures .   Interestingly , we see that models behave simi-   larly not only in terms of relative performance , but   also at the example level ( binary decision per min-   imal pair ) . We ﬁnd that GPT2 and GPT2   have an average agreement of = 0:83(Fleiss   et al . , 1969 ) . This implies strong consistency in   the order of learning of different examples also   within phenomena . Henceforth , we focus on the   phenomena - level as it is more interpretable , lend-   ing itself more easily to characterization . We dis-   cuss per - example similarity further in App . § D.   3.3 Comparison to Off - the - shelf Models   So far , we have observed the common trajectories   presented by NLMs that are trained in parallel . We   proceed to compare trajectories of one model to   other models ’ performance vectors at a single point   of interest in their learning , i.e. a checkpoint ’s per-   formance vector . This allows us to analyze how   similarities evolve , rather than whether two trajec-   tories are synced . We compare fully trained off - the-   shelf NLMs with the trajectory of GPT2(Fig .   3a ) and GPT2 ( App . § E ) .   The observed similarity to off - the - shelf models   is high ( 0.6 - 0.8 ) , implying that NLMs in general   share tendencies and biases . Moreover , similarity   increases until the point of same performance and   then ( when relevant ) decreases . This suggests that   the small NLM approaches off - the - shelf tendencies   as it improves and stops somewhere on the same   trajectory of generalizations ( cf . § 3.5 ) . Further-   more , we ﬁnd considerable correlation with the   performance levels of humans on the different chal-   lenges , but still , all NLMs correlate better with our   model than humans correlate with it .   These results present a curious order imposed on   the NLMs . Both GPT2and GPT2 ( App .   § E ) are more similar to the LSTM model than toTransformerXL , and even less similar to GPT2 .   Interestingly , our models are more similar to an   RNN and a model with a different architecture ,   than to a larger model with the same architecture .   Thus , it seems that the architecture type can not   explain the similarities in the relative order . We   further examine this issue in the next section .   3.4 Effect of Training Data   This section examines the possibility that the simi-   larities reported in Fig . 3a can simply be explained   by the similarity in the NLM ’s training data . More   speciﬁcally , since the ranking by model similar-   ity reported above ﬁts the similarity between the   training sets that the models were trained on , we   view it as a potential confound and attempt to con-   trol for it . Our training data ( WikiBooks ) consists   mostly of Wikipedia and so do the LSTM ’s and   TransformerXL ’s training sets , which are trained   on earlier versions of Wikipedia and WikiMatrix   ( Schwenk et al . , 2019 ) respectively . GPT2 , on the   other hand , is trained on openWebText , which con-   sists of scraped web pages .   To tease apart the effect of training data , we   trained 3 additional GPT2 instances over   the openWebText , openSubtitles and newsCrawl   datasets . Results ( Fig . 1 ) show that the dataset has   more effect on the correlation than initialization .   Hence , the choice of training data does affect the   learning trajectory , but its effect decreases with   training ( correlation gets higher with more training   steps ) . We also recompute the correlations from   § 3.3 after training GPT2on the same data as   GPT2 ( App . § F ) , and ﬁnd that the relative   order between the NLMs remains the same , with   GPT2 being the least similar .   We conclude that while the training data affects   the learned generalizations , it only very partially   explains the observed similarities between NLMs .   3.5 One Dimension of Learning   Based on the ﬁndings of the previous sub - sections ,   we hypothesize that current NLMs all learn in a   similar order , where the effect of training data and   architecture is secondary . In other words , training   time , size and efﬁciency may affect what a model   has learned , but not its learning order . This implies   that stronger models may improve performance ,   but still follow a similar learning trajectory . If this   hypothesis is correct , models should be most simi-   lar to models with the same performance ; similarity   should drop as the gap in performance widens.8284   Controlled comparison supports this hypothe-   sis . Fig . 3b presents the correlation of GPT2   training trajectory with several static checkpoints   taken during GPT2 training . We observe that   at the point in which the average performance   of GPT2is closest to that of the checkpoint ,   the correlation peaks , and then decreases again   as GPT2surpasses the checkpoint in average   performance . So overall correlation peaks when   average performance is most similar . Note that de-   spite the different network sizes and convergence   rates , the correlation ’s maximal value is very high   ( higher than 0.9 ) .   Further experiments show similar trends . Fig . 3a   presents a similar investigation , albeit with more   varied architectures and training datasets . Here too   the maximum correlation is obtained around the   point of most similar performance .   3.6 Comparison to 5 - gram   NLMs are most similar to other NLMs with the   same performance . However , when compared to   non - neural LMs , this is no longer the case .   More speciﬁcally , we compare GPT2 to   two 5 - gram LMs trained on the same dataset as   the NLMs ( WikiBooks ) and another ( GigaWord )   dataset . Results are shown in Fig . 4 , which is qual-   itatively different from Fig . 3a . Here , similarity in   performance implies neither high correlation , nor   the point of highest similarity . This serves both as a   sanity check to our methodology , and as a reminder   of model biases : In general , models may have dif-   ferent biases and tendencies , regardless of overallperformance . In our case , it seems that NLMs share   biases between them that are not necessarily shared   with other LMs .   While not the main purpose of the analysis , our   comparison reveals other noteworthy trends . For   example , 5 - gram LMs trained on different corpora   have different correlations to the GPT2trajec-   tory . This is further discussed in App . § G.   3.7 Discussion   We ﬁnd that the order of learning is surprisingly   stable across architectures , model sizes and train-   ing sets . Therefore , given a new NLM , the order8285 in which it will learn linguistic phenomena can be   predicted by another model that achieves a similar   average accuracy . When considering non - neural   LMs , this observation does not always hold : in-   herently different architectures ( such as 5 - grams )   have very different trajectories . Hence , future mod-   els with very different induced biases may present   different orders .   4 Phases of Learning   Having established that different NLMs learn in   a consistent order , we investigate the emerging   learning trajectory by comparing it with simpler   reference models . Our goal is to identify distinct   learning phases that characterize NLM ’s training .   Setup . We compare GPT2to fully trained   LMs ( same as § 3.3 ) , as well as to a variety of met-   rics . For each metric mwe compute the average   score over each example for each of the 67 sets   E[m(p)]2R. The results are replicated   with GPT2 and TransformerXL and lead to   similar conclusions ( see App . § E ) .   Sentence - level Metrics . First , we consider two   sentence - level metrics : sentence length ( in tokens )   and syntactic depth . Assuming a sentence parse   tree , the depth is the longest path from a word to   the root . Sentence length is often considered to   be a source of challenge for infants ( Brown , 1973 )   and networks ( Neishi and Yoshinaga , 2019 ) , re-   gardless of the sentence ’s complexity . Syntactic   depth ( Yngve , 1960 ) is a measure used to assess   how cognitively complex a sentence is . We leave   the question of which measure of linguistic com-   plexity ( Szmrecsányi , 2004 ) correlates best with   the trajectory exhibited by NLMs to future work .   Our results ( Fig . 5 ) show that neither sentence-   level metric ( length and syntactic depth ) can predictwell what is difﬁcult for the model . This is not   surprising , as both measures only capture sentence   complexity at a general level , and are not directly   related to the linguistic phenomenon that is being   tested . We do see that the syntactic depth starts   off as a worse predictor of the NLM performance   and ends as a better one . We provide a different   perspective on this initial learning phase , before   and after that switch , later in this section .   Next , we compare the performance vector with   task difﬁculty for humans , as reported in the orig-   inal BLIMP paper . We observe that correlation is   fairly high after a sufﬁcient number of steps . In   fact , the network becomes more similar to humans   as it improves : at the beginning , the network relies   on different features than humans , but with time   more of the hurdles are shared . However , correla-   tion saturates at a mid - range correlation of under   0.5 . This suggests that the network ( partially ) relies   on features that are not used by human annotators .   These may be valid generalizations not tested by   BLIMP , or erroneous ones that are still beneﬁcial   to reduce the score on the task it was trained on ( cf .   McCoy et al . , 2019 ) . We revisit this issue in § 5 .   Comparison with Limited Context and Local-   ity . Our methodology opens the door to examine   other potential biases of LMs . We now do so , start-   ing with context and locality .   We consider models that take into account dif-   ferent scopes of context : unigram , and 2 - 5 gram   LMs that can exploit the order of preceding words .   We argue that the correlation between NLMs and   n - gram LMs may indicate that features based on   limited context are also employed by NLMs .   Surprisingly , the unigram model , which does n’t   use context , perfectly classiﬁes 7 phenomena ,   achieves 98.1 % accuracy on 1 , and completely fails   ( 0 % accuracy ) on 8 . This suggests that high accu-   racy on some syntactic and semantic challenges   ( as deﬁned by BLIMP ) can be achieved by simple   heuristics . Note , however , that the NLMs we test   are not trained towards any speciﬁc phenomena   and are not ﬁne - tuned in any way . Hence , NLMs   can only attain heuristics or biases ( generalization   errors ) which are beneﬁcial in general , not ones   speciﬁc to our test challenges .   While NLMs initially present a strong corre-   lation with the unigram model , this correlation   quickly drops ( see Fig . 5 ) . From the outset ,   GPT2succeeds on 6 of the 8 phenomena that   are classiﬁed well by unigrams , and 4 of the 8 that8286the unigram model utterly fails on . Interestingly ,   for 3 of the other phenomena on which the unigram   failed , GPT2initially achieves 0 % accuracy   ( chance level is 50 % ) , but its accuracy does climb   during training ( e.g. , see App . § A ) . We conclude   that , as expected , the NLM acquires a bias towards   predicting frequent words early in training , but that   this bias is weighed in against other ( contextual )   considerations later on in training .   Comparing different scopes of context , our re-   sults ( Fig . 6 and App . § E ) show that through-   out training , the network presents high correlation   withn - gram models . From a certain point onward ,   the network becomes more similar to the bi - gram   model than to the other n - gram LMs . We also   note that similarity peaks early on , but with time   the correlation decreases . This may suggest that   initially , the NLMs acquire grammatical behavior   that resembles a Markov model , or even a bi - gram   model . Only later does the network rely more on   global features . This is in line with our earlier ﬁnd-   ings , which show an increasing correlation with   syntactic depth as compared to sentence length .   At the very beginning , NLMs often generate one   word repetitions ( e.g. , " the " Fu et al . , 2020 ) . This   seems to be at odds with our ﬁnding that grammar   learning already begins at this early stage . How-   ever , while frequency may dictate the most proba-   ble predictions , comparing two options that differ   only slightly may prove to depend more on context ,   as our results indicate .   Limited Context and Word Order . By compar-   ing NLMs to n - grams , we examined the effect of   context within a ﬁxed window size . Now we ex - amine the effect of word order , within a window   and in general . To this end , we create two ablated   GPT2models . BOW is agnostic to the order be-   tween preceding tokens , while Window-5 is similar   but relies only on 5 tokens ( details in § 2 ) .   Our results suggest that initially , the identity of   the preceding words is more important than their   order . Both BOW and Window-5 better correlate   with our NLM than the n - gram models . Later on ,   this trend reverses and the n - grams , that do ex-   ploit word order , become better correlated . Fur-   thermore , the correlation with Window-5 is sig-   niﬁcantly smaller than with BOW at later stages   of learning , suggesting that the network gradually   learns to rely on more context ( cf . Saphra and   Lopez , 2019 ) .   5 Classifying the Learning Trajectories   To understand the latent features learned by NLMs ,   we categorize linguistic phenomena through the   lens of their learning trajectories . We ask whether   linguistically similar phenomena are learned in a   similar fashion , and whether what is learned simi-   larly is deﬁned by linguistic terms .   We inspect linguistic categories by comparing   the learning trajectories of their phenomena . In the   Morphology ﬁeld , we ﬁnd that they display similar   gradual curves , ultimately reaching high perfor-   mance ( median accuracy 0:85 , see Fig . 7a ) . This   may indicate that some latent features learned are   morphological , and affect performance on almost   all ’ Morphology ’ phenomena .   Syntax - semantics phenomena also present   unique behavior : their scores plateau near chance   performance ( see Fig 7b ) , suggesting that the   learned features are insufﬁcient to correctly repre-   sent phenomena in this ﬁeld . The other ﬁelds , " se-   mantics " and " syntax " ( Figs 7c,7d ) , do not present   prototypical learning curves , suggesting that they   are too broad to correspond to a single learning pat-   tern . This , in turn , may suggest that they do not all   correspond to a well - deﬁned set of latent features .   Next , we follow the reverse direction and cluster   the learning curves of GPT2 . We use spectral   clustering with 10 clusters and sklearn default pa-   rameters , by projecting the learning curves into a   normalized Laplacian and applying k - means . Intu-   itively , learning curves with similar values along   the principal directions , are clustered together .   Other clustering methods show similar results .   The clusters ( Fig . 8 and App . § H ) reﬂect several8287   learning proﬁles , some more expected than oth-   ers . For some , accuracy improves as learning pro-   gresses ( see Fig . 8a ) . Some are barely learned , and   accuracy remains at near - chance level ( see Fig . 8b ) .   Perhaps more surprisingly , some clusters deterio-   rate , and accuracy drops to nearly 0as learning   progresses ( see Fig . 8c ) . Notably , some challenges   are quite easy – NLMs instantly reach perfect ac-   curacy ( see Fig . 8d ) , while some are confusing   – NLMs performance is worse than chance ( see   Fig . 8c ) . In the latter cases , the NLMs presumably   learn unrelated , harmful generalizations .   When inspecting the emerging clusters , many   ( but not all , see Fig . 8b ) contain a shared promi-   nent ﬁeld , but often varied super - phenomena ( see   Fig . 8a ) . Thus , while the categorization in BLIMP   reﬂects a common linguistic organization of gram-   matical phenomena , from the perspective of learn-   ing trajectories only few of the super - phenomena   in BLIMP show consistent behavior . We cautiously   conclude that there is some discrepancy between   the common linguistic categorization of grammati-   cal phenomena and the categorization induced by   the learning trajectories of NLMs . An interesting   direction for future work would therefore be the   development of a theory that can account for the   patterns presented by NLMs ’ learning trajectories .   We manually inspect a few phenomena with   strong initial performance that then deteriorates .   We ﬁnd that some of these challenges are solvableby a simple rule , easily learnable by an n - gram   model . For example , in " principle A case 1 " , al-   ways preferring subjective pronouns ( e.g. , " she " or   " he " ) over reﬂexive ones ( e.g. , " himself " , " herself " )   is sufﬁcient to obtain a perfect score , and preferring   " not ever " over " probably / fortunately ever " solves   " sentential negation NPI licensor present " . The fact   that NLM performance deteriorates , ﬁts our ﬁnding   that nascent NLMs resemble an n - gram model .   6 Related Work   Characterizing what networks learn is a long-   standing challenge . Recently , studies suggested   methods to analyze trained models such as prob-   ing ( Tenney et al . , 2019 ; Slobodkin et al . , 2021 ) ,   analyzing attention heads ( V oita et al . , 2019 ; Ab-   nar and Zuidema , 2020 ) and neurons ( ﬁnding also   correlations across epochs ; Bau et al . , 2018 ) and   assessing the extent to which LMs represent syntax   ( van Schijndel et al . , 2019 ) . Other works compare   outputs , like us , to assess network generalizations   ( Choshen and Abend , 2019 ; Ontan’on et al . , 2021 ) ,   look for systematic biases ( Choshen and Abend ,   2018 ; Stanovsky et al . , 2019 ) or evaluate character-   istics of outputs ( Gehrmann et al . , 2021 ; Choshen   et al . , 2020 ) . McCoy et al . ( 2020 ) ﬁne - tuned BERT   and tested generalizations on the adversarial dataset   HANS ( McCoy et al . , 2019 ) , ﬁnding models to   make inconsistent generalizations . Their results   differ from ours , but so is their setup , which in-8288volves ﬁne - tuning for inference .   Characterizing the features learned by networks   according to the order in which examples and phe-   nomena are learned is a relatively new topic . Re-   cently , Hacohen et al . ( 2020 ) ; Hacohen and Wein-   shall ( 2021 ) ; Pliushch et al . ( 2021 ) showed that   classiﬁers learn to label examples in the same or-   der . While their focus was on computer vision , it   provided motivation for this work . Other studies   use learning dynamics as a tool , rather than a topic   of study . They choose training examples ( Toneva   et al . , 2018 ) , categorize examples ( Swayamdipta   et al . , 2020 ) or characterize the loss - space ( Xing   et al . , 2018 ) . Little research on NLM learning dy-   namics and generalization types was previously   conducted .   Perhaps the closest to this work is Saphra and   Lopez ( 2019 ) , which compared LSTM represen-   tations with 3 types of linguistic tagger outputs ,   ﬁnding that correlation is low and that later in train-   ing , more context is used . The latter is reminiscent   of our ﬁndings in § 4 .   In parallel work , Liu et al . ( 2021 ) probe models   during training . They show that , early in train-   ing , information required for linguistic classiﬁ-   cations is found somewhere in the layers of the   model . Our work supports their ﬁndings by show-   ing that grammar learning experiments conducted   with one model are likely to replicate on another .   Our methodology differs from theirs in requiring   the information the model learnt to manifest itself   in behavior rather than to be extractable with a   dedicated classiﬁer .   Studying the trajectories of language learning   is a mostly untapped area in NLP , but is a long-   established ﬁeld of research in linguistics and psy-   chology . Such lines of research study topics such   as acquisition of phonemes ( Kuhl et al . , 1992 ) , mor-   phology ( Marcus et al . , 1992 ) , complex construc-   tions ( Gropen et al . , 1991 ; Qing - mei , 2007 ) and   innate learning abilities ( Tomasello , 2003 ) . Con-   siderable computational work was also done on   constructing models that present similar learning   trajectories to those of infants ( McClelland and   Rumelhart , 1981 ; Perfors et al . , 2010 ; Abend et al . ,   2017 , among many others ) .   Our work suggests that the generalizations   NLMs make are coupled with the bottom - line per-   formance . This gives a new angle and opens av-   enues of research when combined with previous   work about bottom - line performance . For exam - ple , the bottom - line performance of small models   could predict the performance of larger models   ( Ivgi et al . , 2022 ) . In such cases , the type of gener-   alizations made might also be predicted from the   smaller models .   Our work is also closely related to ﬁelds such as   curriculum learning ( Bengio et al . , 2009 ; Hacohen   and Weinshall , 2019 ) , self - paced learning ( Kumar   et al . , 2010 ; Tullis and Benjamin , 2011 ) , hard data   mining ( Fu and Menzies , 2017 ) , and active learning   ( Krogh and Vedelsby , 1994 ; Hacohen et al . , 2022 ;   Ein - Dor et al . , 2020 ) . In these ﬁelds , the order in   which data should be presented to the learner is   investigated . On the other hand , in our work , we   study the order of the data in which the learner   is learning – which may shed some light on the   advancement of such ﬁelds .   7 Summary and Conclusions   We showed that NLMs learn English grammatical   phenomena in a consistent order , and subsequently   investigated the emerging trajectory . Our ﬁndings   suggest that NLMs present consistent and infor-   mative trends . This ﬁnding suggests a path for   studying NLMs ’ acquired behavior through their   learning dynamics , as a useful complementary per-   spective to the study of ﬁnal representations .   Future work will consider the impact of addi-   tional factors , architectures and learning phases   that appear only later in training . We hope that this   work will increase the afﬁnity between the knowl-   edge and methodologies employed in developmen-   tal studies , and those used for studying NLMs . Our   goal is to obtain a better understanding of what   makes linguistic generalization complex or simple   to learn , for both humans and NLMs .   Acknowledgments   We thank Prof. Inbal Arnon for her helpful discus-   sions . This work was supported in part by the Israel   Science Foundation ( grant no . 2424/21 ) , by a grant   from the Israeli Ministry of Science and Technol-   ogy , and by the Gatsby Charitable Foundations.8289References829082918292A Per challenge Graphs   We include behaviours of each model trained over   the main dataset used ( Wikipedia and books ) on   each BLIMP challenge by perplexity . In general ,   accuracy is similar despite different initialization   and size of the GPT2 models . TransformerXL   shows a similar trend , despite the uncomparable   Perplexity . We supply several examples here and   leave the rest to the data accompanying this paper .   B Details on experimental settings   We include further settings to ensure reproduciblity   of the results . Parameters shared by all the trained   NLMs include 32Ktokens in the vocabulary , 5   10learning rate , max gradient norm of 1 , Adam   optimizer ( Kingma and Ba , 2015 ) , and 10Kwarm-   up steps . TransformerXl vocabulary is kept to its   default . All other parameters , including GPT2   size parameters , are the defaults according to the   HuggingFace transformers library .   Our 2 - 5 grams are KenLM ( Heaﬁeld , 2011 )   trained on WikiBooks . A second 5 - gram model   trained on GigaWord corpus ( Graff et al . , 2003 ) , as   reported by BLIMP . The Uni - gram LM is deﬁned   according to the frequency of a word in WikiBooks .   Sentence probability is normalized by the number   of words , which is helpful for the rare cases where   the minimal pairs are of different lengths .   C Correlation during training   We see that tendencies during training are not only   similar between instances of the same architec-   ture but also between different architectures . On   comparable stages of learning , the GPT2and   GPT2 correlate well ( > 0.9 ) with respect to   their performance vectors . We present the cor-   relations of GPT2compared to GPT2 in   Fig . 12 . We ﬁnd the two learn in a similar order   throughout their training .   We manually compare the results to Trans-   formerXL . Qualitatively , observing the trajectories   per challenge ( Trajectories are found in Supp . § A   and the supplied data ) of TransformerXL , it seems   to share the general tendencies of the GPT2 archi-   tectures . However , reaching a lower stage of train-   ing , it never improves on some challenges ( e.g. ,   determiner - noun agreement ) .   D Models are consistent on per example   level   We compute the binary score of every example   by each model . We reframe the question as an   annotator agreement problem and ask whether the   models agree on the right answer for each example.8293   Framed this way , the methodology is clear . We   compute Fleiss kappa ( Fleiss et al . , 1969 ) and ﬁnd   the per example correlation . The full results per   step and challenge are added as a supplemental   ﬁle . The average overall kappa is 0.83 , models not   only agree on the order of learning phenomena but   also on the order of learning examples within each   per - phenomenon ( if learnt at all ) . While there are   phenomena with lower and higher agreement , there   are only two phenomena in the range of 0.5 - 0.6   agreement . Meaning even the most different ones   have high example correlation and there is little   variance between models to explain .   Our main aim in this work is to compare models   acquisition . However , we see the per example order   of acquisition as less informative , unless we can   cluster or name the examples learnt . The reason to   choose the phenomena was to extract such names ,   and we hence focus in our work on them .   Note , that consistency per example was shown   before in the scope of computer vision ( Hacohen   et al . , 2020 ) . However , a critical difference is that   they deal with classiﬁcation and check whether   which examples are learnt ﬁrst . We however , aim   to ask about generalization , given that you learn   one task ( language modelling ) , what type of gen-   eralizations do you make , tested on another . For   example , while learning to predict the next word ,   the network understands after X steps that the verb   should be in agreement with the subject .   E Reproducing with other models   We provide the GPT2 correlation with other   models and with various metrics and models in Fig .   13 and 14 respectively . We also supply the average   BLIMP accuracies of the models we trained in Fig .   15.8294E.1 Results mainly replicate in   TransformerXL   We replicate the same experiment over the train-   ing of the TransformerXL instance . The Trans-   formerXL seems to reach a lower stage of learning ,   probably due to the vast vocabulary and model .   The model replicates some of the general no-   tions seen on GPT2 . It correlates most with   simpler models , then with humans and then with   global features . At ﬁrst , sentence length makes a   sentence more challenging than its actual structure ,   5 window BOW starts as more relevant than BOW   over all the sentence .   We do see that the overall graph is quite straight .   With that , the increase in correlation with humans   is quite small , the BOW models do n’t drop and the   evidence of relying on more abstract knowledge in   late stages is less apparent . This might be expected ,   as we know the network reached an early step on   the performance scale .   F Reproducing with other data   As comparison to the correlations with our main   model , we provide the correlations of GPT2   trained on OpenWebText with the two 5 - gram mod-   els , one on WikiBooks and one on Giga word ( Fig .   17 ) . We see that the higher resemblence to Wiki-   Book trained model is kept despite being trained   on the same data , but the difference is lower at the   beginning and more stable . It might be the case   that over reliance on the speciﬁc data is shown at   those ﬁrst steps where the difference is large , but it   would require further evidence .   We also compare the model to several other   trained models in Fig . 18.8295 G 5 - grams notes   The gap between the correlation with the two 5-   grams decreases during the ﬁrst 50 K steps or so ,   and then remains constant . This suggests that   the choice of a dataset is more important during   early NLM training . Because , at the beginning   the network learn generalizations which are more   common to counts of one ( huge , general domain )   dataset than another , and this effect diminishes .   Possibly , this is because at this point NLMs rely   more on word identity , rather than on abstract gen-   eralizations , that are shared to a greater extent   across corpora ( see § 4 ) . We observe that the 5-   gram trained on WikiBooks correlates better with   GPT2 , even when GPT2is not trained on it   ( not reported ) . We can not offer a simple explana-   tion for this trend .   H Clustering BLIMP   We include the learning curves of GPT2on   BLIMP dataset , clustered according to ﬁelds   ( Fig . 19 ) „ super - phenomena ( Fig . 20 ) , and the spec-   tral clustering ( Fig . 21 ) . Due to restrictions on ap-   pendix ﬁles the ﬁgures are found in corresponding   folders in the supplied data.82968297