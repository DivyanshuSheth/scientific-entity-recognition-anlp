  Xiao Long , Liansheng Zhuang , Li Aodi , Shafei Wang , Houqiang LiUniversity of Science and Technology of China , Hefei 230026 , ChinaPeng Cheng Laboratory , Shenzhen 518000 , China   longxiao1997@mail.ustc.edu.cn ; lszhuang@ustc.edu.cn   Abstract   Query embedding ( QE)—which aims to embed   entities and first - order logical ( FOL ) queries   in a vector space , has shown great power in   answering FOL queries on knowledge graphs   ( KGs ) . Existing QE methods divide a com-   plex query into a sequence of mini - queries   according to its computation graph and per-   form logical operations on the answer sets of   mini - queries to get answers . However , most   of them assume that answer sets satisfy an in-   dividual distribution ( e.g. , Uniform , Beta , or   Gaussian ) , which is often violated in real appli-   cations and limit their performance . In this pa-   per , we propose a Neural - based Mixture Proba-   bilistic Query Embedding Model ( NMP - QEM )   that encodes the answer set of each mini - query   as a mixed Gaussian distribution with multi-   ple means and covariance parameters , which   can approximate any random distribution ar-   bitrarily well in real KGs . Additionally , to   overcome the difficulty in defining the closed   solution of negation operation , we introduce   neural - based logical operators of projection ,   intersection and negation for a mixed Gaus-   sian distribution to answer all the FOL queries .   Extensive experiments demonstrate that NMP-   QEM significantly outperforms existing state-   of - the - art methods on benchmark datasets . In   NELL995 , NMP - QEM achieves a 31 % relative   improvement over the state - of - the - art .   1 Introduction   Answering FOL queries on KGs is a fundamental   problem in KGs . It aims to find answer entities   of given complex queries using operators includ-   ing existential quantification ( ∃ ) , conjunction ( ∧ ) ,   disjunction ( ∨ ) , and negation ( ¬ ) , which has at-   tracted great attention from both academic and in-   dustry recently ( Sun et al . , 2019 ; Li et al . , 2017 ;   Dalvi and Suciu , 2007 ; Dong et al . , 2020 ) .   Early methods transform a FOL query into a   computation graph like Figure 2(a ) , where eachnode represents a set of entities and each edge rep-   resents a logical operation . Then , these methods   traverse the KG according to the computation graph   to identify the answer set ( Lin et al . , 2018 ; Guo   et al . , 2018 ; Saxena et al . , 2020 ; Sun et al . , 2019 ) .   However , this type of approach suffers two major   challenges . First , it has difficulties identifying the   correct answers when some links are missing in   KGs . Second , it needs to traverse all the intermedi-   ate entities on reasoning paths , which may lead to   exponential computation costs .   Recently , query embedding ( QE ) methods have   attracted much attention and keep them close   enough ( Hamilton et al . , 2018 ; Ren et al . , 2020 ;   Guu et al . , 2015 ; Zhang et al . , 2021 ; Amayuelas   et al . , 2021 ) . These methods divide a query into a   sequence of mini - queries according to its computa-   tion graph , and perform logical operations on the   answer set of each mini - query to get the answer .   Existing QE methods differ in their way of model-   ing answer sets and logical operations , which can   be divided into geometry - based and distribution-   based categories . Geometry - based models repre-   sent each answer set as a “ region ” ( e.g. , box or   cone ) in Euclidean spaces , while distribution - based   models usually leverage an individual distribution   ( e.g. , Beta or Gaussian ) to represent each answer   set . Compared with early methods , QE methods do   not need to track all intermediate entities , and can   use the nearest neighbor search in the embedding   space to quickly discover answers .   Though having achieved promising performance ,   most existing QE methods assume that answer sets   satisfy an individual distribution . For example ,   geometry - based models ( Ren et al . , 2020 ; Zhang   et al . , 2021 ) assume that answer sets satisfy a uni-   form distribution in a region , and distribution - based   models assume that answer sets satisfy the Gaus-   sian distribution ( Choudhary et al . , 2021 ) or Beta   distribution ( Ren and Leskovec , 2020 ) . In the real   KGs , distributions of answer sets are often very3001complicated . As shown in the left of Figure 1 , the   embedding vectors of the answer set to the query   ( US , Location_Contains , ? ) in NELL995 ( Xiong   et al . , 2017 ) are scattered across two class cen-   ters : city - related center containing ( e.g. , New York ,   Joplin , Pittsburgh ) and college - related center con-   taining ( e.g. , Cambridge , Yale University , Brooklyn   College ) . Similarly , shown in the right of Figure 1 ,   the embedding vectors of the answer set to the   query ( Sport_Baseball , Plays_Sport_reverse , ? ) in   Freebase ( Toutanova and Chen , 2015 ; Bollacker   et al . , 2008 ) are scattered across three class cen-   ters : pitcher - related center , coach - related center ,   and catcher - related center , which indicate different   positions among baseball players . Obviously , it is   difficult to model the answer set of each query with   only one individual distribution , which limits the   performance of existing methods .   Inspired by the above insights , this paper   proposes a Neural - based Mixture Probabilistic   Query Embedding Model ( NMP - QEM ) , which   uses mixed Gaussian distribution with multiple   means and covariance parameters to encode the   answer set of each query . Since mixed Gaussian   distributions can approximate any random distri-   bution arbitrarily well ( Wu and Perloff , 2007 ) , the   proposed NMP - QEM can model more complicated   answer sets ’ distribution in real KGs . Furthermore ,   due to the unboundedness of Gaussian random vari-   ables , the closed form of the negation operators   of mixed Gaussian distributions is hard to define ,   which prevents answering the queries with nega-   tion . To tackle this problem , we propose neural-   based logical operators of projection , intersection   and negation for mixed Gaussian distributions , so   that the proposed NMP - QEM can answer all the   FOL queries . To summarize , the contributions of   our works are as follows :   •We propose a novel NMP - QEM to answer   FOL queries on KGs . To the best of our knowl-   edge , NMP - QEM is the first attempt that mod-   els answer sets of queries with mixture dis-   tributions instead of individual distributions ,   which makes it more suitable for real KGs .   •To handle all the FOL queries , we propose   neural - based logical operators of projection ,   intersection , negation for mixed Gaussian dis-   tributions , which has difficulty in defining the   closed solution of negation operation .   •Extensive experiments on three popular bench - mark datasets demonstrate that NMP - QEM   achieves superior performances in answering   FOL queries on KGs and significantly outper-   forms state - of - the - art baselines . In NELL995 ,   NMP - QEM even achieves up to 31 % relative   improvement over the state - of - the - art .   2 Related Work   In this section , we briefly review the related work .   To answer complex logical queries on KGs , path-   based methods ( Lin et al . , 2018 ; Guo et al . , 2018 )   start from anchor entities and require traversing the   intermediate entities on the path , which leads to   exponential computation cost . Query embedding   models are another line of work . Roughly speaking ,   existing query embedding methods can be divided   into two categories −geometry - based models and   distribution - based models .   Geometry - based models usually represent answer   sets as " regions " e.g. , points , boxes , or cones in Eu-   clidean spaces , and then design set operations upon   them . For example , Query2Box ( Ren et al . , 2020 ;   Dasgupta et al . , 2020 ) represents entities as points   and queries as boxes . If a point is inside a box , then   the corresponding entity is the answer to the query .   Geometric shapes provide a natural and easy way   to represent sets and logical relations among them .   To better handle the queries with negation , recently   ( Zhang et al . , 2021 ) use the closure of cones to   design the negation operation . While ( Amayuelas   et al . , 2021 ) directly uses MLP to define the nega-   tion operation and embed the query and the entities   in the same vector space with single - point vectors .   Distribution - based models usually embed entities3002and queries as an individual distribution e.g. , Beta ,   Gaussian . ( Ren and Leskovec , 2020 ) uses Beta   distributions defined on the [ 0,1]interval to model   entities , queries , and design neural logical opera-   tors that support full first - order logic . ( Choudhary   et al . , 2021 ) encodes entities as a multivariate Gaus-   sian density with mean and covariance parameters ,   and defines the closed logical operations upon the   Gaussian distribution . Although it defines the logi-   cal operators for mixture input during chaining the   queries , this model can not deal with the negation   operation . Besides , it needs to calculate the inverse   of the large matrix and solve a linear system , which   requires huge memory and is very time - consuming .   Although these QE models make significant   progress in answering FOL queries on KGs , they   overlook that the distribution of answer sets in real   KGs is often complicated . Thus , the proposed   NMP - QEM models answer sets of queries with   mixed Gaussian distributions instead of individual   distributions , in order to approximate any distribu-   tions well in real applications .   3 Preliminaries   In this section , we first give the formal definition   of answering FOL queries on KGs .   A KG can be denoted as G= ( V;R ) , where   v∈ V represents an entity , and r∈ R is a binary   function r : V × V → { True ; False } , indicating   whether the relation rholds between a pair of en-   tities or not . The goal of answering FOL queries   on KGs is to answer First - Order Logical ( FOL )   Queries on KGs . We can define them as follows :   Definition 1 ( First - order logic queries ) A first-   order logic query qis formed by an anchor entity   setV⊆ V , an unknown target variable Vand a se-   ries of existentially quantified variables V , . . . , V.   In its Disjunctive Normal Form ( DNF ) ( Davey and   Priestley , 2002 ) , it is written as a disjunction of   conjunctions :   q[V ] = V · ∃V , . . . V : c∨c∨ · · · ∨ c(1 )   Specifically , crepresents a conjunctive query of   one or several literals : c = e∧ · · · ∧ e. And   the literals represent a relation or its negation :   e = r(v , v)or¬r(v , v)where v , vare enti-   ties and r∈ R.   Computation Graph . Given a query , we represent   the reasoning procedure as a computation graph   ( see Figure 2 a for an example ) , of which nodesrepresent entity sets and edges represent logical op-   erations over entity sets . We map edges to logical   operators according to the following rules .   Projection Operator P.Given a set of entities   S ⊂ V and a relational function r∈ R , the   projection operator Poutputs all adjacent entities   ∪N(v , r)where N(v , r)is the set of entities   such that r(v , v ) = True for all v∈N(v , r ) .   Intersection Operator I.Given nsets of enti-   tiesS , S , . . . , S , the intersection operator Iper-   forms set intersection to obtain ∩S.   Negation Operator N.Given an entity set S ⊂ V ,   Ngives ¯S = V\S .   It is worth noting that the union operation is un-   necessary , as ( Ren et al . , 2020 ) shows that a union   operator becomes intractable in distance - based met-   rics , and can be implemented using intersection and   negation .   4 Methods   Overall , the architecture of the NMP - QEM and the   corresponding computation graph are shown in Fig-   ure 2 . To get answers , the complex query is divided   into a sequence of mini - queries according to its   computation graph . Then we use the neural - based   operators to embed the entity and do logical oper-   ations between the answer set of each mini - query .   In this query , the basketball players of America   may play different positions ( e.g. , center , pg ) and   players who win the NBA championship may also   come from different countries ( e.g. , Spain , France ) .   In the following subsections , we will introduce the   details of the proposed method .   4.1 Mixture Probabilistic Embedding for   Entities and Queries   Firstly , we introduce how the NMP - QEM embeds   entities and queries by which the output obey a   mixture of Gaussian distributions . We assume   that for the answer set V , there are Ksemantic   centers . And the answer set is modeled by   V=/summationtextωN(µ,Σ ) , where a diagonal   matrix is used instead of full covariance matrix   of Gauss density function in consideration of   computational cost . The learnable parameters   ω∈R,µ∈RandΣ∈Rindicate the   icomponent ’s weights , semantic position and   spatial query area of answer sets , respectively .   Finally , for the convenience of calculation , we con-   catenate three parameters and mask them as : W=   [ [ µ;ω ] , . . . , [ µ;ω ] , [ Σ ; 0 ] , . . . , [ Σ ; 0 ] ] ∈3003   R , where [ · ; · ] indicates concatenation   on the sequence length dimension . Next , each   source node representing an anchor entity v∈ V   is a vector in R , and we regard it as the K   identical components and then concatenate it into   the same shape as W∈R. Thus , we   unify the dimensions of anchor entities and answer   sets ’ embeddings to facilitate subsequent logical   operations .   4.2 Neural - based Operators   To answer a query using the computation graph ,   we need projection operators and logical operators   for the mixture probabilistic embedding . Next , we   describe the design of these operators used in com-   putation graphs , which include relation projection   P , intersection Iand negation N. As discussed   before , the union can be implemented using in-   tersection and negation . Accordingly , we do not   define this operator .   Projection Operator P : The goal of Pis to map   an anchor entity or an answer set to another answer   set . Therefore , we design a probabilistic projec-   tion operator Pthat maps one mixture probabilis-   tic embedding Wto another mixture probabilistic   embedding Wgiven the relation type r. We then   learn a transformation neural network for each rela-   tion type r , which is implemented as a multi - layer   perceptron ( MLP ):   W = MLP(W ) ( 2 )   Noticing that the output Wis also the mixture of   Gaussian components , we must apply different acti-   vation functions to satisfy the following restrictions .   The parameters of different component ’s weightsωshould satisfy ω>0and / summationtextω= 1 . And   the diagonal Σshould be positive semidefinite . So ,   we use the SoftMax function and Relu function to   activate these parameters :   ω = exp(ω)/summationtextexp(ω),Σ = Relu(Σ ) ( 3 )   Intersection Operator I : Given ninput embed-   dings{W , . . . , W } , the goal of neural - based in-   tersection operator Iis to calculate the mixture   probabilistic embedding W that represents the   intersection of the distributions . Here we consider   the intersection operation between two input em-   beddings { W , W } , since Ninput embeddings   can be transformed into a pairwise intersection .   TheWcontains KGaussian components and the   key of the Iis how to make sufficient intersections   between KGaussian components . Thus , we model   Iby taking the attention mechanism ( Vaswani   et al . , 2017 ) . Additionally , considering that the   intersection satisfies commutative law , we design a   symmetric attention mechanism as follows :   I{W , W}=SoftMax ( QK   √m)V+   + SoftMax ( QK   √m)V(4 )   where Q= ( q,···,q)∈R ,   K= ( k , . . . , k)∈R ,   V= ( v , . . . , v)∈R.   The input embeddings Ware reshaped as   ˆW= [ [ µ;ω ; Σ ; 0 ] , . . . , [ µ;ω ; Σ ; 0 ] ] ∈3004R , and the Q , K , Vare calculated by :   Q=ˆW¯W ,   K=ˆW¯W , ( 5 )   V=ˆW¯W ,   where α= 1,···,n . The ¯W∈R,¯W∈   R,¯W∈Rare parameter matrices .   Observing the Equation 4 , it is easy to find that :   I{W , W}=I{W , W } . This result also guar-   antees the commutative law of intersection I.   Negation Operator N : Finally , we require a   neural - based negation operator Nthat takes mix-   ture probabilistic embedding Was input and pro-   duces an embedding of the N(W ) . This operator   should reverse in the sense that regions of high   probability in Whave low probability in N(W )   and vice versa . And we use a multi - layer MLP   to model this transformation in distribution .   N(W ) = MLP(W ) ( 6 )   After the transformation in distribution , we also   need to activate the parameters in N(W)like Equa-   tion 3 , to satisfy the constraints of wandΣ.   4.3 Training Objective   The goal is to jointly train the neural - based logical   operators and the entity embeddings . Our training   objective is to minimize the distance between the   entity embeddings and query embeddings , while   maximizing the distance from the query to incor-   rect random entities , which can be done via neg-   ative samples . Equation 7 expresses this training   objective in mathematical terms .   L=−logσ(γ−Dist(v;q))−   −/summationdisplay1   klogσ(Dist(v;q)−γ ) ) ( 7 )   where qis the query , v∈ [ [ q ] ] is an answer of query   ( the positive sample ) ; v/∈ [ [ q ] ] represents a random   negative sample ; γ > 0is a fixed margin , and σ ( · )   is the sigmoid function . Both the margin and the   number of negative samples kremain as hyperpa-   rameters of the model .   Distance : When defining the training objective ,   we need to specify the distance between the embed-   dings of entity v : Eand the query q : W. We use   theLdistance between source entity ’s embeddingEand weighted µof query q :   Dist(v , q ) = |E−/summationdisplayW[ω]·W[µ]|(8 )   where W[ω]andW[µ]denote the ωpart and   µpart of mixture probabilistic embedding W , re-   spectively .   5 Experiments   In this section , we first introduce the experiment   settings including datasets , baselines and evalu-   ation protocols . Secondly , we compare NMP-   QEM with competitive models on answering FOL   queries over KGs and demonstrate its superiority .   Thirdly , we reduce training queries to four types   ( 1p/2i/2in / inp ) to observe the further generaliza-   tion of the model . Then , we analyze the impact of   two important parameters on the proposed model .   Finally , we do the case study to exploit how the   NMP - QEM models the answer sets in NELL995 .   5.1 Experiment Setup   We adopt the commonly used experimental settings   in ( Ren and Leskovec , 2020 ) . The only difference   is that we used the WN18RR instead of the FB15K.   For the reason that ( Dettmers et al . , 2018 ) raises   WN18 and FB15 K suffer test leakage through in-   verse relations problem . So they put forward the   FB15K-237 , WN18RR −a subset of FB15k and   WN18 , where inverse relations are removed . Ac-   cordingly , there is no need to use FB15 K when   FB15K-237 is already used .   Datasets and Queries : We use three datasets :   FB15k-237 , NELL995 and WN18RR . To obtain   the queries from the datasets and their ground truth   answers , we used the same query structures in   ( Ren and Leskovec , 2020 ) . The training and vali-   dation queries consist of five conjunctive structures   ( 1p/2p/3p/2i/3i ) and five structures with nega-   tion ( 2in/3in / inp / pni / pin ) . Please refer to Ap-   pendix A.1 for more details about the datasets and   query structures .   Evaluation Protocols : For each non - trivial answer   vof a test query q , we rank it against non - answer   entities V\ [ [ q ] ] . We denote the rank as rand then   calculate the Mean Reciprocal Rank ( MRR ) and   Hit@N. The definitions of these protocols are pro-   vided in Appendix A.3 . Higher MRR and Hit@N   indicate a better performance .   Baselines : We compare NMP - QEM against six   state - of - the - art models , including GQE ( Hamilton3005   et al . , 2018 ) , Query2Box ( Q2B ) ( Ren et al . , 2020 ) ,   BETAE ( Ren and Leskovec , 2020 ) , ConE ( Zhang   et al . , 2021 ) , MLP ( Amayuelas et al . , 2021 ) and   PREM ( Choudhary et al . , 2021 ) . GQE , Q2B and   PREM are trained only on five conjunctive struc-   tures as they can not model the queries with nega-   tion .   Parameter Settings : For a fair comparison , we   assign the best dimension to the embeddings of the   six methods . We list the hyperparameters , architec-   tures and more details in Appendix A.2   5.2 Main Results   In this subsection , we compare the MRR and   HIT@1 of the NMP - QEM with all the baselines   on three datasets . We run our model five timeswith different random seeds and report the average   performance . Due to the limited space , the HIT@1   performance and error bars of the performance are   shown in Table 9 and Table 10 in Appendix B.   The performance of all models is reported in Ta-   ble 1 and Table 2 . From the results , we have the   following observations . Overall , NMP - QEM sig-   nificantly outperforms compared models . For the   EPFO ( ∃,∧,∨ ) queries , on average , NMP - QEM   obtains 1.0 % ( 4.2%relative ) , 8.4 % ( 30.8 % rela-   tive ) , and 3.3 % ( 9.9 % relative ) improved MRR   over the best models on FB15k-237 , NELL995 and   WN18RR respectively , which demonstrates the su-   periority of NMP - QEM on the whole . NMP - QEM   also gains an impressive improvement on queries   ( ip / pi/ 2u / up ) , which are not in the training data .   These results show the superior generality ability   of NMP - QEM . Additionally , for the queries with   negation ( 2in/3in / inp / pin / pni ) , NMP - QEM ob-   tains 0.5 % ( 7.2%relative ) , 8.4 % ( 39.0 % relative ) ,   and 0.8 % ( 2.9 % relative ) improved MRR over   the best baselines on three datasets respectively .   Finally , we refer the reader to Table 7 and Ta-   ble 8 in Appendix B for Hit@1 results . NMP-   QEM also achieves better performance than six   baselines on Hit@1 . Furthermore , we found that   NMP - QEM achieves a better improvement on   NELL995 and WN18RR than FB15K-237 , which   may be caused by the entity - relationship ratio . In   NELL995 and WN18RR , the entity - relationship   ratio ( entity / relationship ) are 317 and 3681 , re-3006Model 2p 3p 3i pi ip 2u up 3 in pin pni A VG Descent Rate   BetaE 7.5 7.0 40.2 19.5 9.2 11.4 7.0 7.5 2.4 3.0 11.5 ↓15.1 %   ConE 8.4 7.2 43.6 21.1 9.4 14.1 7.3 9.5 3.4 3.9 12.7 ↓16.4 %   MLP 6.4 5.3 40.5 20.5 10.4 10.9 5.3 9.3 2.8 1.8 11.3 ↓24.1 %   NMP - QEM 9.6 7.5 44.0 22.6 12.7 14.9 7.4 9.6 3.1 4.9 13.6 ↓15.0 %   spectively , while in FB15K-237 , the ratio is only   61 . This phenomenon shows that with a relatively   high entity - relationship ratio , the answer sets may   have more semantic centers , which can be modeled   better by NMP - QEM . We will discuss in detail the   impact of the semantic center ’s number Kon the   model in the following sections .   5.3 Generalization of the models   In this subsection , we reduce training queries to   four types to observe the further generalization of   the model . Since in the previous 5.2 experiments ,   the model improves less on FB15K-237 . Thus , we   conduct more experiments on FB15K-237 . Previ-   ous work ( Ren and Leskovec , 2020 ) uses 10 types   of queries for training as shown in Figure 6 in Ap-   pendix A. Then , they evaluate the model ’s gener-   alization ability using queries with logical struc-   tures that the model has never seen during training ,   which includes ( ip / pi/ 2u / up)for evaluation . To   further explore the model ’s generalization , we use   ( 1p/2i/2in / inp ) queries for training and test the   remaining 10 types of queries . We evaluate the   MRR of the models and the results are shown in   Table 3 . We can find that although these ten types   of queries did not appear in the training set , our   model still achieve better performance nearly on   all types compared with other methods . Moreover ,   compared with the results in section 5.2 , which has   more query types in the training set , NMP - QEM   drops a relatively small proportion of average MRR ,   while other methods drop more on average MRR .   These results demonstrate that NMP - QEM can gen-   eralize beyond query structures better than other   models .   5.4 Influence of hyperparameters   As mentioned above , a good number of semantic   centers Kmay bring the model a higher perfor-   mance boost . Accordingly , in this subsection , we   conduct two experiments to analyze the impact of   important parameter Kon the proposed model .   Firstly , we reduce the number of semantic cen - ters to 1 which also means the model has only   one Gaussian component . And we compare its   average MRR results on all types of queries with   those under the best Kof the model . From Fig-   ure 3 , we can find that no matter which datasets   we choose , the model with multiple semantic cen-   ters always performs better than those with a single   center . And this result further illustrates that the an-   swer set of the query is more inclined to have mul-   tiple semantic centers . Additionally , we can find   that on datasets with a higher entity - relationship ra-   tio ( NELL995 and WN18RR ) , the improvement is   more significant than the FB15K-237 . This result   also shows that with a higher entity - relationship ra-   tio , the answer set may have more semantic centers ,   which makes it modeled better .   Secondly , we change the number of semantic   centers Kin the range of { 2 , 4 , 6 , 8 , 10 , 20 } to   observe the average MRR results on all types of   queries . From Figure 4 , we can find that for dif-   ferent datasets , the appropriate Kis also different .   The value of Kis related to the entity - relationship   ratio . Specifically , for the FB15K-237 , the entity-   relationship ratio is relatively small , and the best   Kis only about 4 . While for the WN18RR and   NELL995 , the entity - relationship ratio is relatively   large , and the best Kare 6 and 10 respectively .   These results show that appropriate Kis of great   significance to the model ’s performance . If Kis3007   too large , there are more parameters to be trained ,   which may degrade the performance . If Kis too   small , the NMP - QEM can not model the high   entity - relationship ratio dataset with many seman-   tic centers .   5.5 Case Study   Finally , we explore how the NMP - QEM models   answer sets in the test sample . We provide an   example from NELL995 in Figure 5 . The FOL   query is " List the province of city Tazewell in the   USA that proxy for " , and the process of answer-   ing this question can be divided into four parts .   We show each part ’s semantic center of the an-   swer set and their weight ωat the bottom of   the figure . Firstly , the answer set1 of the ques-   tionsubpartof _ reverse ( USA , V ) are the differ-   ent continents in the USA , and they have six se-   mantic centers ( different continents ) since the Kof   the model is six on NELL995 . Additionally , the six   centers have similar weights ω , which means the   probability of these components is similar . The sec-   ond part is to answer located _ in(Tazewell , V ) ,   and the answer set2 has five similar components re-   lated to Illinois . The sum of their weights reaches   0.885 , which is close to the truth that Tazewell is   located in Illinois . Then , we intersect two answer   sets above . The intersected set also has five simi-   lar components related to Illinois , and the sum of   their weights reaches 0.784 . Finally , we project the   intersected set to obtain the final answers . We can   observe that most answers are cities in Illinois ,   and the result given in red is the incorrect answer .   From this example , we find that NMP - QEM can   capture the semantic component in the answer set   effectively , and the intersection operation can cap-   ture the same component of two sets , which helps   NMP - QEM model the FOL queries better .   6 Conclusions   In this paper , we propose a novel query embed-   ding model NMP - QEM for answering FOL queries   over knowledge graphs . The proposed NMP - QEM   models the answer set of queries with mixed Gaus-   sian distributions instead of individual distributions ,   which enables it to approximate any distributions   well in real applications . Furthermore , to model   the logic operators in FOL queries , we propose the   neural - based logical operators of projection , inter-   section and negation for mixed Gaussian distribu-   tions , which are hard to be used to define the closed   solution of negation operator . Extensive experimen-   tal results show a significant performance improve-   ment compared to other state - of - the - art methods   built for this purpose .   7 Limitations   In this section , we discuss the limitations of the   proposed model . As mentioned above , ( Amayue-   las et al . , 2021 ) used the vector to represent relation   and defines the projection as the concatenation of   the relation vector and query embedding . ( Ren3008et al . , 2020 ; Zhang et al . , 2021 ) used few param-   eters to represent the projection operators . Com-   pared with these methods , our model may have   more parameters and need more time during the   training . This is because we define the neural - based   projection operators Pfor each relation . So , if the   amount of relation is too much , the parameters   will increase accordingly . The Table 11 in Ap-   pendix B shows an example . But compared with   PREM ( Choudhary et al . , 2021 ) which needs to   calculate the inverse of the big matrix and solve a   linear system , NMP - QEM uses less training time   and memory . So , in the future , we will consider   how to decrease the parameters in the projection op-   erators P , which will reduce memory consumption   during training and improve training speed .   Acknowledgement   This work was supported in part by Next Gener-   ation AI Project of China No.2018AAA0100602 ,   in part to Dr. Liansheng Zhuang by NSFC under   contract No . U20B2070 and No.61976199 , and in   part to Dr. Houqiang Li by NSFC under contract   No.61836011 .   References3009   Appendix   A More Details about Experiments   In this section , we show more details about experi-   ments that are not included in the main text due to   the limited space .   A.1 Query Generation and Statistics   Statistics about the datasets can be found in   Table 4 . To obtain the queries from the datasets   and its ground truth answers , we consider the 9   query basic structures ( without negation ) from   Query2box ( Ren et al . , 2020 ) and additional 5   query structures ( Ren and Leskovec , 2020 ) with   negation to include FOL queries as shown in   Figure 6 . Given the 3 datasets , 3 graphs are created :   G⊆ G⊆ Gfor training , testing and   validation , respectively . Therefore the generated   queries are also : [ [ q ] ] ⊆ [ [ q ] ] ⊆ [ [ q ] ] .   Thus , we evaluate and tune hyperparameters   on [ [ q ] ] \ [ [ q ] ] and report the results on   [ [ q ] ] \ [ [ q ] ] . We always evaluate on queries   and entities that were not part of the already seen   dataset used before .   As summarized in Figure 6 , our training and   evaluation queries consist of the 5 conjunctivestructures ( 1p/2p/3p/2i/3i ) and also 5 structures   with negation ( 2in/3in / inp / pni / pin ) . Further-   more , we also evaluate the model ’s generalization   ability which means answering queries with logical   structures that the model has never seen during   training . We further include ( ip / pi/ 2u / up)for   evaluation .   For the experiments , we have used the   train / valid / test set of queries - answers used   in BetaE ( Ren and Leskovec , 2020 ) . This   query - generation system differs from the original   in the fact that it limits the number of possible   answers to a specific threshold since some queries   in Query2box ( Ren et al . , 2020 ) had above 5000   answers , which is unrealistic . And , for dataset   WN18RR , we adopt the same generation process .   The average number of queries is shown in Table 5 .   A.2 Experimental Details   We implement our code using Pytorch . For the   baselines , we have used the implementation of   the baselines and the testing framework from   Query2Box , GQE , BetaE , MLP , ConE and PREM .   For a fair comparison with the models in the pa-   per , we have selected the same hyperparameters   listed in the paper . For our method , we fine-   tune the hyperparameters including number of   embedding dimensions from { 200,400,800 } , and   the learning rate from { 1e,7e,5e,1e } ,   batch size from { 128,256,512 } , negative sam-   ple size from { 32,64,128 } , number of semantic   centers Kfrom{4,8,12,20}and the margin γ   from{10,20,30,40,50,60,70 } . For the datasets   FB15K-237 and NELL995 , we cite results in BE-   TAE , ConE and MLP for comparison . But for   dataset WN18RR , we conduct a new experiment us-   ing the best hyperparameters in their paper . Notic-   ing that , PREM uses queries different from Be-   taE , which pointed out that there are unrealistic   queries ( Ren and Leskovec , 2020 ) . So , for a fair   comparison , we conduct new experiments with   PREM on the three datasets . We list the hyper-   parameters of each model in Table 6 . Every single   experiment is run on a single NVIDIA Tesla V100   GPU , and we run each method for 400k iterations .   A.3 Evaluation Metrics   Given the rank of answer entities v∈ V to a non-   trivial test query q , we compute the evaluation met-   rics defined according to Equation 9 below . Then,3010   Dataset Entities Relations Training Edges Val Edges Test Edges Total Edges   FB15K-237 14,505 237 272,115 17,526 20,438 310,079   NELL995 63,361 200 114,213 14,324 14,267 142,804   WN18RR 40,493 11 86,835 3,034 3,134 93,003   Queries Training Validation Test   Dataset 1p/2p/3p/2i/3i2in/3in / inp / pni / pin 1p others 1p others   FB15K-237 149,689 14,968 20,101 5000 22,812 5000   NELL995 107,982 10,798 16,927 4000 17,034 4000   WN18RR 86,830 16,830 12,830 3000 12,830 3000   Models embedding dim learning rate batch size negative sample size margin   GQE 800 0.0005 512 128 30   Q2B 400 0.0005 512 128 30   BetaE 400 0.0005 512 128 60   ConE 800 0.0001 512 128 30   MLP 800 0.0001 512 128 24   PREM 400 0.0001 512 128 24   NMP - QEM 400 0.00005 512 128 203011   we average all queries with the same query format .   Metric ( q ) = /summationtextf ( rank(v ) ) )   | [ [ q ] ] \ [ [ q ] ] |(9 )   where vis the set of answers V⊂ [ [ q ] ] \ [ [ q ] ] ,   f is the specific metric function and rank ( v )   is the rank of answer entities returned by the model .   In our experiments , we use the flowing two f   functions .   Mean Reciprocal Rank ( MRR ) : It is a statistic   measure used in Information Retrieval to evaluate   the systems that returns a list of possible responses   ranked by their probability to be correct . Given an   answer , this measure is defined as the inverse of   the rank for the first correct answer , averaged over   the sample of queries Q. Equation 10 express this   measure formally .   MRR = 1   |Q|/summationdisplay1   rank(10 )   where rankrefers to the rank position of the first   correct answer for the i−thquery .   Hits rate at K ( H@K ) : This measure considers   how many correct answers are ranked above K. It   directly provides an idea of how the algorithm is   performing . Equation 11 defines this metric mathe-   matically .   H@K= 1 ( 11)B More Experimental Results   In this section , we give more experimental results   that are not included in the main text due to the   limited space .   We show in Table 7 and Table 8 the Hit@1 results   of the three methods for answering FOL queries .   Our method still shows a significant improvement   over the six baselines in all three datasets .   To evaluate the performance of NMP - QEM , we   run the model five times . We report the error bars   of these results . Table 9 and Table 10 show the   error bar of NMP - QEM ’s MRR results on all FOL   queries . Overall , the standard variances are small ,   which demonstrates that the performance of NMP-   QEM is stable .   Additionally , Table 11 shows the numbers of pa-   rameters for some models on FB15K-237.3012   Dataset 1p 2p 3p 2i 3i pi ip 2u up A VG   FB15K-23746.2 12.9 11.3 35.0 47.8 25.6 15.1 15.0 10.9 24.4   NELL99568.8 23.9 17.8 47.0 55.0 31.1 26.0 29.6 20.7 35.6   WN18RR53.1 24.3 14.1 68.5 86.6 38.2 19.2 12.7 13.2 36.6   Dataset 2 in 3 in inp pin pni A VG   FB15K-2376.8 11.7 8.2 5.5 4.7 7.4   NELL99510.0 9.2 12.9 4.8 7.4 8.9   WN18RR24.2 68.2 19.8 12.0 16.3 28.1   Models Q2B ConE MLP PREM NMP - QEM   Parameter   Numbers682,280,0 238,904,01 247,920,00 262,764,800 198,126,7763013