  Yi Zhu , Zhaoqing Zhu , Bingqian Lin , Xiaodan Liang , Feng Zhao , Jianzhuang LiuHuawei Noah ’s Ark Lab , University of Science and Technology of China , Sun Yat - sen University   zhu.yee@outlook.com , zhaoqingzhu@mail.ustc.edu.cn , bingqianlin@126.com ,   xdliang328@gmail.com , fzhao956@ustc.edu.cn , liu.jianzhuang@huawei.com   Abstract   Conventional visual relationship detection mod-   els only use the numeric ids of relation labels   for training , but ignore the semantic correlation   between the labels , which leads to severe train-   ing biases and harms the generalization ability   of representations . In this paper , we introduce   compact language information of relation la-   bels for regularizing the representation learn-   ing of visual relations . Specifically , we propose   a simple yet effective visual Relationship pre-   diction framework that transfers natural lan-   guage knowledge learned from Contrastive   Language- Image Pre - training ( CLIP ) models   to enhance the relationship prediction , termed   asRelCLIP . Benefiting from the powerful   visual - semantic alignment ability of CLIP at   image level , we introduce a novel Relational   Contrastive Learning ( RCL ) approach that ex-   plores relation - level visual - semantic alignment   via learning to match cross - modal relational   embeddings . By collaboratively learning the   semantic coherence and discrepancy from rela-   tion triplets , the model can generate more dis-   criminative and robust representations . Exper-   imental results on the Visual Genome dataset   show that RelCLIP achieves significant im-   provements over strong baselines under full   ( providing accurate labels ) and distant supervi-   sion ( providing noise labels ) , demonstrating its   powerful generalization ability in learning rela-   tionship representations . Code will be available   athttps://gitee.com/mindspore/models/   tree / master / research / cv / RelCLIP .   1 Introduction   Visual relationship detection needs to predict the re-   lation label between a pair of localized objects ( e.g.   “ man carrying bag ” ) . Based on such relationships   we can construct a structural representation ( i.e. ,   scene graph ) that regards the visual concepts within   a scene as a whole and could benefit many down-   stream reasoning tasks , such as image retrieval ( QiFigure 1 : Overview of RelCLIP which adapts Con-   trastive Language - Image Pretraining ( CLIP ) models to   enhance the visual - semantic alignment for relationship   learning via a novel Relational Contrastive Learning   ( RCL ) approach . RCL matches the cross - modal re-   lationship embeddings in both subject - predicate and   predicate - object levels and improves relationship dis-   crimination with the help of language information of   relation labels .   et al . , 2017 ; Johnson et al . , 2015 ) , vision question   answering ( Antol et al . , 2015 ) , and visual common-   sense reasoning ( Zellers et al . , 2019 ) .   Existing visual relationship detection models   ( Yao et al . , 2021 ; Guo et al . , 2021 ; Chen et al . ,   2019 ; Zellers et al . , 2018 ) learn relation represen-   tations based on the visual information of object   pairs under the supervision of numeric ids of rela-   tion labels . The learned representations often suffer   from the highly changeful visual appearances of   the instances from the same relation category as   well as the fine - detailed visual difference between   relation classes . As shown in Fig . 1 , the representa-   tion of the sample “ girl carrying surfboard ” is very   close to that of “ man riding surfboard ” in the visual   embedding space , which may cause undesirable   misclassification of relations .   To alleviate this , we introduce language informa-   tion of relation labels which is more compact than   visual information to capture the semantic correla-   tions among the labels , and regularize the represen-   tation learning of visual relationships via exploring   visual - semantic alignment at relation level . By do-4800ing this , we can learn more robust and discrimina-   tive relationship representations . Specifically , we   propose a Relationship detection framework that   adapts Contrastive Language- Image Pretraining   ( CLIP ) models ( Radford et al . , 2021 ) to enhance   the visual - semantic alignment during relationship   learning , termed as RelCLIP . The CLIP model   is pre - trained on 400 million image - text pairs har-   vested from the Web , and is proved to be good at   image - level visual - semantic alignment . During the   representation learning of CLIP , the objects within   an image are considered to be independent , and it is   difficult to capture the interaction between objects ,   e.g. , visual relationships , so CLIP models can not   be directly applied to visual relation detection . For   example , as shown in Fig . 2 , CLIP features only   focus on individual objects but fail to activate the   area that indicates the relationship between two   objects . In contrast , our RelCLIP can effectively   focus on the interactive regions between objects   that intuitively indicates the relations .   To achieve relation - level visual - semantic align-   ment , we develop a novel Relational Contrastive   Learning ( RCL ) approach to match the visual   and language embeddings of relationships , since   naively using existing contrastive learning methods   ( Chen and He , 2021 ; He et al . , 2020 ; Xie et al . ,   2021 ; Li et al . , 2020 ) for relationship triplets ( e.g.   subject - predicate - object ) may cause two severe is-   sues . First , directly comparing the triplets may   lead to trivial comparisons among all possible com-   binations of labels of subject , object and predi-   cate , which is quite inefficient and greatly harms   the robustness of the model . Second , the synony-   mous relationships ( e.g. , “ man riding bike ” and   “ person riding bike ” ) and the less informative nega-   tive ones ( e.g. “ man riding bike ” and “ bird sitting   on branch ” ) may be contrasted inappropriately and   inefficiently . To address these issues , our RCL 1 )   decouples the triplet level comparison into dual   contrastive objectives which compare the same re-   lation instance in subject - predicate andpredicate-   object levels to reduce the amount of comparisons ;   2 ) adopts a new semantic - aware active sampling   strategy that excludes synonymous relationships   and includes informative negative samples accord-   ing to their semantic meaning .   Extensive experiments on the Visual Genome   dataset show that RelCLIP achieves significant   improvements over strong baselines trained using   whether human - annotated labels from full super-   vision or noisy labels from distant supervision ,   demonstrating its powerful generalization ability   in learning relationship representations .   2 Related Work   Visual Relationship Detection . Visual Relation-   ship Detection ( Li et al . , 2017 ; Zellers et al . , 2018 ;   Lu et al . , 2016 ; Tang et al . , 2019 ) has raised wide   concern in the computer vision community for its   potential benefits that would be brought to down-   stream visual reasoning tasks ( Johnson et al . , 2018 ;   Yang et al . , 2019 ; Shi et al . , 2019 ) . Early works   tend to detect objects and pairwise relationships   independently , which overlooks the rich visual con-   text and may lead to sub - optimal performance ( Lu   et al . , 2016 ; Zhuang et al . , 2017 ; Zhang et al . , 2017 ;   Zhu and Jiang , 2018 ) . Later , many works have ex-   plored the message passing for context propagation   and feature refinement ( Xu et al . , 2017 ; Zellers   et al . , 2018 ; Dai et al . , 2017 ) . At the same time ,   some works also have noticed some connections be-   tween objects and pairwise relationships , and kept   their cooperative relationship ( Zhang et al . , 2019 ;   Li et al . , 2022 ) . More recently , Yao et al . ( Yao et al . ,   2021 ) explores a novel visual distant supervision   that retrieves possible relation labels from com-   monsense knowledge bases for object pairs . Nev-   ertheless , most visual relationship detection works   typically trained the relationship prediction models   under the supervision of numeric ids of predicate   labels , which usually suffer from the highly diverse   visual object appearances of the same predicate . To   break the limitation for better relation detection , we   resort to the learned natural language knowledge4801 in cross - modal pre - training models for capturing   rich semantic dependencies among predicates and   objects in this paper .   Cross - modal Pre - training . Recently , there has   been a surging interest in employing cross - modal   pre - training ( Su et al . , 2020 ; Chen et al . , 2020 ;   Li et al . , 2020 , 2019 ; Tan and Bansal , 2019 ; Lu   et al . , 2019 ) for improving the performance of   downstream tasks , such as Visual Question An-   swering ( VQA ) ( Antol et al . , 2015 ) , Visual Com-   monsense Reasoning ( VCR ) ( Zellers et al . , 2019 ) ,   and Referring Expression Comprehension ( Hamil-   ton et al . , 2017 ) . LXMERT ( Tan and Bansal , 2019 )   and ViLBERT ( Lu et al . , 2019 ) are two pioneer-   ing works , which rely on two Transformers to en-   code image and text modalities with a third Trans-   former built on top for multi - modal fusion . Unlike   this two - stream architecture , single - stream archi-   tectures where two modalities are directly fused in   the early stage are further designed in some recent   works , such as VL - BERT ( Su et al . , 2020 ) , Visu-   alBERT ( Li et al . , 2019 ) , UNITER ( Chen et al . ,   2020 ) and Unicoder - VL ( Li et al . , 2020 ) . Con-   trastive Language - Image Pretraining ( CLIP ) ( Rad-   ford et al . , 2021 ) is a recently proposed cross - modal   pretrained model that uses 400 million image - text   pairs collected from the web . It is shown to have the   outstanding ability for object - level visual - semantic   alignment and improving numerous downstream   tasks . Powered by this , our method adapts the CLIP   model to enhance the relation - level visual - semantic   alignment for learning more discriminative relation-   ship representations .   3 Methodology   3.1 Preliminaries   Problem Setup . Given a scene image I , visual rela-   tionship detection models detect a set of visual rela-   tionships in the form of a triple token ⟨a , a , a⟩ ,   where a , a∈ A are the class labels of two ob-   jectsiandjlocalized by bounding boxes band   b , and a∈ Ais the class label of the predi-   cate that connects the object pair . For each image   I , we employ the Faster R - CNN framework ( Ren   et al . , 2015 ) to extract a set of Mobject propos-   alsB={b},b= [ x , y , w , h]with(x , y )   being the coordinate of the top left corner and w   andhbeing the width and the height of the bound-   ing box . We take each pair of objects as a relation   proposal b= Union ( b , b)if there is an over-   lap between the object boxes . Union ( · , · ) denotesthe bounding box of the relationship which is the   joint box of two objects . We can obtain visual rep-   resentations of object and relation proposals via   performing ROI Pooling on spatial feature maps   generated from the visual backbone .   Pre - training Model . RelCLIP adapts a recent   successful cross - modal pretraining model called   Contrastive Language - Image Pre - training ( CLIP )   ( Radford et al . , 2021 ) to enhance visual - semantic   alignment during relationship learning . CLIP con-   sists of a text encoder T(·)and an image encoder   V ( · ) , which are pre - trained on 400 million image-   text pairs harvested from the web . The text encoder   is a Transformer as in ( Vaswani et al . , 2017 ) . The   base architecture of the image encoder can be either   a Convolutional Neural Network ( CNN ) ( He et al . ,   2016 ) or a Vision Transformer ( ViT ) ( Dosovitskiy   et al . , 2021 ) . Considering that relationships are   usually associated with larger image regions than   an object and require much more visual context ,   we choose the ViT version as it is good at captur-   ing long - range dependencies within an image . The   pre - trained image - text encoders are integrated in   a plug - and - play manner , and their parameters are   fixed during training .   3.2 Cross - modal Relation Embedding   Visual Embedding . We reshape the final trans-   former states as a 2D spatial feature map V(I )   for image I. We denote the ROI Pooling layer   asf(·)which takes the image feature maps and   the bounding boxes as the inputs and outputs a   feature vector . Besides , we also introduce object   and relation feature adaptors ( denoted as h ( · )   andh ( · ) ) to project the pre - trained visual embed-   dings to the domain of visual relationship detec-   tion . Each adaptor consists of a fully connected   layer and two multi - head attention layers . Given   a pair of objects iandj , we calculate the relation   representation v∈Ras :   v = h([f(V(I),b);p ] ) , ( 1 )   where Dis the dimension of the visual embed-   dings , pis position embedding . [ · ; · ] denote fea-   ture concatenation . Similarly , we can get the object   features vandvbased on boxes bandb .   Language Embedding . Existing visual relation-   ship prediction models only use numeric ids of rela-   tion labels , causing severe biases toward the noise   and harming the generalization ability . Here we in-   troduce compact language information of relation4802   labels for regularizing the representation learning   of visual relationships . We adopt the prompt engi-   neering ( Radford et al . , 2021 ) to extract language   embeddings of each relationship token ⟨a,ˆa , a⟩ ,   ˆa∈ A , as shown in Fig . 3 . The resulting   text embedding of the relationship is denoted as   t∈R. Similarly , we can get object text em-   beddings t , t∈Rby filling the sentence tem-   plates using object labels . Dis the dimension of   the text embedding .   3.3 Relational Contrastive Learning   Conventional contrastive learning methods may en-   counter two serious problems when directly ap-   plied to compare relationship instances . First , the   large quantity of subject - predicate - object combi-   nations will lead to extremely trivial comparisons   thus reducing the efficiency of contrastive learn-   ing . Second , the undesirable comparisons among   synonymous relationships will introduce semantic   ambiguity and harm the robustness of relationship   discrimination . To address these issues , we propose   Relational Contrastive Learning ( RCL ) , which 1 )   decouples the triplet level comparison into two tu-   ples for reducing the number of comparisons and 2 )   introduces a new semantic - aware active sampling   strategy for excluding synonymous relations and   the less informative ones .   Cross - modal relation embeddings . We decoupleeach relation instance into subject - predicate and   predicate - object levels and compare the same in-   stances at both levels , respectively . To generate   embeddings at each level , we exclude the informa-   tion of the subject or object by subtracting their   features alternatively from the feature of the rela-   tionship triplet vas :   ∆v=/vextenddouble / vextenddoublev−v / vextenddouble / vextenddoubleW ,   ∆v=/vextenddouble / vextenddoublev−v / vextenddouble / vextenddoubleW,(2 )   where ∥·∥stands for the L2 - normalization . W ,   W∈Rare learnable weight matrices .   Similarly , the text embeddings are extracted as :   ∆t=/vextenddouble / vextenddoublet−t / vextenddouble / vextenddoubleW ,   ∆t=/vextenddouble / vextenddoublet−t / vextenddouble / vextenddoubleW,(3 )   where W , W∈Rare learnable weight   matrices . The cross - modal relation embeddings   are extracted at both the subject - predicate and   predicate - object levels as ( ∆v,∆t ) and ( ∆v ,   ∆t ) , respectively .   Learning Objectives . CLIP is proved to be good   at expressing visual concepts for benefiting object   recognition tasks while still struggling to capture   the semantic relations between objects , as shown in   Fig . 2 . RelCLIP takes the advantage of CLIP ’s   powerful ability of image - level visual - semantic   alignment and explores relation - level alignment via4803RCL . RCL aims to match the cross - modal embed-   dings of relations and pull the samples of the same   relation instance together while pushing the differ-   ent ones away . It maximizes the cosine similarity of   the cross - modal embeddings of Npositive pairs in   the batch while minimizing the cosine similarity of   the embeddings of the N−Nnegative pairs . For   mathematical simplicity , we here denote the cross-   modal relation embeddings ( v , t ) as the inputs of   RCL objectives . We adopt the cross - modal version   of the InfoNCE loss ( van den Oord et al . , 2018 )   which is minimized to lead the encoders to max-   imally preserve the mutual information between   the correctly matched pairs . The image - to - text and   text - to - image contrastive losses are defined as :   l=−logexp(⟨v , t⟩/τ )   /summationtextexp(⟨v , t⟩/τ ) ,   l=−logexp(⟨t , v⟩/τ )   /summationtextexp(⟨t , v⟩/τ),(4 )   where⟨·,·⟩represents the cosine similarity between   two vectors . Qis the actively sampled dictionary   that provides informative negative samples for the   positive sample according to their semantic mean-   ing . The cross - modal objective of RCL is then   computed as a weighted combination of the two   losses averaged over all possible image - text pairs   in each mini - batch :   L(v , t ) = 1   N / summationdisplay(γl+(1−γ)l),(5 )   where γ∈[0,1]is a scalar weight . RCL integrates   two contrastive objectives regarding a same relation   instance at subject - predicate andpredicate - object   levels . We instantiate L(v , t)with ( ∆v,∆t )   and ( ∆v,∆t ) to form RCL loss :   L = L(∆v,∆t)/bracehtipupleft / bracehtipdownright / bracehtipdownleft /bracehtipupright+L(∆v,∆t)/bracehtipupleft / bracehtipdownright / bracehtipdownleft /bracehtipupright.(6 )   Together with L , our model is also optimized   by two multi - class cross - entropy losses for object   classification and predicate classification .   Semantic - aware Active Sampling . Given a query   sample k , our method actively constructs its neg-   ative sampling dictionary Qby ranking the rest   samples according to their semantic meaning ( e.g. ,   word2vec embeddings ( Pennington et al . , 2014 ) ) .   To exclude the synonymous relationships from the   negative samples , we remove from Qthe sampleswhose similarity scores are higher than a threshold   ϵ. To pay more attention to the most informative   samples , we assign each of the remaining samples   with a weight which is the normalized similarity   score with the query sample . We construct the   sampling dictionaries QandQfor the dual   contrastive objectives of RCL as :   Q={(∆v,∆t;w)|η≤w } , w=⟨e , e⟩ ,   Q={(∆v,∆t;w)|η≤w } , w=⟨e , e⟩ ,   ( 7 )   where ηis the threshold for excluding the less in-   formative samples , eandeare word embed-   dings for the word tokens of subject - predicate and   predicate - object for the n - th relation sample .   Our semantic - aware active sampling strategy   constructs dynamic sampling dictionaries for the   same relationship in each of the contrastive ob-   jectives , which further improves the diversity of   negative samples . As is shown in Fig . 4 , for the   positive sample “ person lying on bed ” , the sample   “ man lying on grass ” will not be selected as a neg-   ative sample at the subject - predicate level . While   in the predicate - object level , it is selected as an   informative negative sample since it is closer to   the positive sample than the other samples in the   semantic space .   3.4 Visual Relationship Detection   Prediction . As is shown in Fig . 3 , we regard v   andvas the learned object features and classify   them to predict the final labels of the object iandj .   Similarly , the learned relation feature vis used to   predict final relation label between object iandj .   And the corresponding object proposals detected by   RPN are employed as our bounding box prediction .   Supervision . RelCLIP is compatible and effective   when trained with different types of supervision sig-   nals , e.g. , full supervision and distant supervision   ( Yao et al . , 2021 ) . Conventional visual relationship   detection models require full supervision where all   training samples are elaborately annotated . To al-   leviate the burden of manual annotations , distant   supervision is proposed to automatically generate   relation labels from the commonsense knowledge   base . The knowledge contains a huge amount of   relationship triplets parsed from the Conceptual   Caption dataset ( Sharma et al . , 2018 ) . Given an ob-   ject pair ⟨a , a⟩ , we can retrieve possible relations   from the knowledge base and get a multi - hot label   for training , as shown in Fig . 3 . In RCL , all the4804   possible labels are fused according to their weights   to form the language embedding . Training mod-   els under distant supervision is more challenging   than full supervision since the retrieved labels may   introduce noise .   4 Experiments   4.1 Settings   Datasets . We evaluate our method on the popular   large - scale Visual Genome ( VG ) ( Krishna et al . ,   2017 ) benchmark including approximately 108k   images . Following previous works ( Chen et al . ,   2019 ; Yao et al . , 2021 ) , we employ the data split   which contains the most frequent 150 object cate-   gories and 20 well - defined predicates . The refined   predicate schemes defined by Chen et al . ( Chen   et al . , 2019 ) remove synonyms and super - sets from   the 50 predicates in the Visual Genome dataset . Tasks . We follow three conventional tasks ( Zellers   et al . , 2018 ; Chen et al . , 2019 ; Chen et al . , 2019 ;   Yao et al . , 2021 ) to evaluate the proposed SGG   model : 1 ) Predicate classification ( PredCls ) , which   predicts the predicate labels given a ground truth   set of object boxes and object labels , 2 ) Scene   graph classification ( SGCls ) , where both the object   classes and the relation type of each object pair are   predicted given the ground - truth object bounding   boxes . 3 ) Scene graph detection ( SGDet ) , which   only takes the original image as input and sequen-   tially predicts the object bounding boxes , the object   labels and the relationships between object pairs .   Metrics . We use the following metrics which are   under graph constraint to evaluate the performance   of the proposed approach : ( 1 ) Recall@K ( R@K )   ( Lu et al . , 2016 ) , which computes the fraction of   relationship hits in the top K confident relationship   predictions , ( 2 ) mean Recall@K ( mR@K ) ( Tang   et al . , 2019 ; Chen et al . , 2019 ) , which takes the   average R@K of all predicate classes to give a fair   performance measure for both head and tail classes .   Implementation Details . We implement our4805   model using the MindSpore Lite tool ( MindSpore ) .   We train our model with the SGD optimizer for   20,000 iterations with image batch size 12 . The   initial learning rate is 0.001 . The dimension values   DandDof the cross - modal embeddings are both   set to 512 . We use the pre - trained CLIP model that   is publicly available . The dimension of the visual   and language embeddings DandDare set to 512 .   The sampling threshold ϵfor splitting the positive   and negative samples is set to 0.7 .   4.2 Visual Relationship Detection   Compared Baselines . We compare our method   with strong baselines under distant , and full super-   vision . The distant supervised models are trained   using a set of relation candidates retrieved from the   commonsense knowledge collected on Conceptual   Caption ( Sharma et al . , 2018 ) . The candidates are   image - agnostic and each of them has equal proba-   bility . To further enhance the distant supervision   ( KB ) , VisualDS ( Yao et al . , 2021 ) also introduces   an external semantic signal ( EXT ) which leverages   CLIP models to assign probability scores to the   relation candidate sets regarding the image content .   We adopt two fully supervised methods as strong   baselines , i.e. , Motif ( Zellers et al . , 2018 ) which   is a widely used scene graph model and BA - SGG   ( Guo et al . , 2021 ) is one of the most recent models .   Numerical Results . In Tab . 1 , we compare our   approach with existing baselines of visual rela-   tionship detection under different types of super-   vision signals . Our RelCLIP achieves consistent   improvements over prior arts ( e.g. , VisualDS ( Yao   et al . , 2021 ) and BA - SGG ( Guo et al . , 2021 ) ) on   all sub - tasks under both distant and full supervi-   sion , demonstrating that RelCLIP can learn more   robust and discriminative representations for visualrelationships . Though distant supervision provides   many noise labels , RelCLIP can still learns dis-   criminative representations from a small amount   of correctly labeled samples with the help of the   language knowledge of relation labels .   Human Evaluation . Since there are amounts of   unsatisfactory annotations in the VG dataset as   shown in Fig . 5 , the Recall metric is insufficient to   comprehensively evaluate the accuracy of predic-   tions . We ask 10 people to evaluate the precision   of our predicted relations . Predictions from 200   images are selected and the results are reported   as mean Precision@10 and 20 , see Tab . 2 . When   comparing with RelCLIP ( No RCL ) , the precision   ( P@10/20 ) of improves nearly 10 % , which further   demonstrates the excellent ability of RelCLIP in   recognizing visual relationships .   4.3 Ablation Study   In this section , we evaluate the effectiveness of   the training objectives , sampling strategies , and   pretraining knowledge under the most challenging   setting of distant supervision .   Effect of relational contrastive objectives . We   develop variants of RCL to verify the effective-   ness of dual contrastive objectives . The experi-   mental results on PredCls are presented in Tab . 3 .   First , we replace our Dual Contrast objectives with   Unary Contrast which solely contrasts relationship   instances at predicate level and find that the perfor-   mance drops . This is because the Unary Contrast   fails to capture the semantic dependencies between   predicates and objects . Then , we use Triple Con-   trast where relationship instances are contrasted at   subject - predicate - object level . The results on Re-   call and mean Recall are lower than dual contrast ,   which indicates that our dual contrast can better   capture the complicated and cluttered dependen-   cies between the predicate and objects .   Effect of pretraining models . We replace Rel-   CLIP ’s vision encoder with an ImageNet1 K pre-   trained backbone . As shown in Tab . 4 , RelCLIP   achieves higher performance when using the same   pretrained backbone as VisualDS , and the perfor-4806   mance can be further improved when using CLIP   models , which demonstrates the ability of RelCLIP   in adapting cross - modal pretraining knowledge .   Effect of semantic - aware active sampling . We   build three baselines for validating the effect of our   sampling strategy to construct the effect negative   sampling dictionary . 1 ) The No Sampling baseline   disables the whole sampling functionality during   model training . 2 ) The No Del. Syn . baseline pre-   served the synonymous relation instances . 3 ) The   No weight baseline removes the synonymous rela-   tionships but disables the sample weighting which   suppresses the effect of less informative samples .   Compared with Ours that excludes both the synony-   mous instances and the less informative ones , the   performances of the ablation baselines above drop ,   indicating the effectiveness of our semantic - aware   active sampling strategy , see Tab . 5 .   Effect of the threshold ϵof sampling . We set the   threshold value ϵto 0.3 , 0.5 , 0.7 , and 0.9 for train-   ing RelCLIP models and the results on the PredCls   task are shown in Tab . 6 . When the threshold is   set to 0.7 , we can obtain the highest performance .   If we use a larger threshold ( e.g. , 0.9 ) , more syn-   onymous negative samples will be included in the   sampling dictionary and the performance drops . If   we use a smaller threshold ( e.g. , 0.3 and 0.5 ) , some   informative samples will be removed undesirably   and thus lead to performance drop .   5 Visualization   Case Study . In Fig . 6 , we compare the accuracy of   “ riding ” of RelCLIP with and without RCL on Re-   call@100 . When RCL is disabled , the performance   drops by a significant margin of 5 % , and a large   proportion of the “ riding ” failures are misclassified   as “ carrying ” . As shown in the two failure cases ,   RelCLIP with RCL can well capture the minor dif-   ferences of interactions between man and board via   comparing the similarity and discrepancy across re-   lation instances , thus correctly distinguish “ riding ”   and “ carrying ” .   Prediction Examples . We show some visualiza-   tion examples in Fig . 5 . Though trained with distant   supervision , RelCLIP can correctly detect relation-4807ships ( in green ) . Surprisingly , numerous reasonable   relationships ( in blue ) are also predicted by Rel-   CLIP , even though they are missed in the human   annotations , e.g. , in Fig . 5(a ) , “ cat sitting on bed "   can be obviously observed by humans , but is not   labeled in GT . Furthermore , we observed that our   failure cases ( in orange ) are more precise than GTs   from the human perspective , e.g. , in Fig . 5(d ) , the   relationship between “ man-3 ” and “ sidewalk ” is   more like “ walking on ” rather than “ standing on ” .   6 Conclusion   In this work , we develop RelCILP , a simple yet ef-   fective visual relationship detection framework that   successfully adapts cross - modal pre - training for   improving relationship discrimination . To achieve   this , we propose Relational Contrastive Learning   ( RCL ) that not only enables efficient comparisons   via decoupling triplet - level contrast into subject-   predicate andpredicate - object levels , but also en-   sures effective comparisons via a novel semantic-   aware sampling strategy . By collaboratively learn-   ing the semantic coherence and discrepancy from   relationship instances , the model can generate more   discriminative and robust relationship representa-   tions . Our RelCLIP significantly outperforms ex-   isting methods under both full and distant super-   vision , demonstrating the effectiveness and gener-   alization ability of relational contrast powered by   cross - modal pretraining models .   Limitations   The limitations of RelCLIP can be summarized   as follows : First , Relational Contrastive Learn-   ing ( RCL ) contrasts the visual and language em-   beddings of relationships within each mini - batch ,   which makes RCL sensitive to batch size and may   introduce bias into the training of RelCLIP . In our   future work , we will try to introduce a global mem-   ory bank for achieving cross - batch comparison .   Second , RelCLIP can learn visual relationships   under distant supervision without any annotations .   However , the performance gap between distant and   full supervision is still large , because distant super-   vision may inevitably introduce noisy labels . The   quality of distant supervision depends on the com-   monsense knowledge base , so the quality and ca-   pacity of the knowledge base may affect the perfor-   mance of RelCLIP to some extent . Third , RelCLIP   is implemented following the two - stage framework   where the visual relationship detection problem isdecoupled into object detection and relationship   classification . The powerful knowledge of CLIP   is integrated to enhance the discrimination of rela-   tionships , but it is not used to improve the object   detector . So the performance of RelCLIP is still   limited by the accuracy of object detection . In fu-   ture works , we will explore how CLIP models can   be used to simultaneously improve both the object   detector and the relation classifier .   Acknowledgement   We gratefully acknowledge the support of Mind-   Spore , CANN ( Compute Architecture for Neu-   ral Networks ) , and Ascend AI Processor used for   this research . This work was supported in part   by National Key R&D Program of China under   Grant 2020AAA0109700 , Guangdong Outstand-   ing Youth Fund under Grant 2021B1515020061 ,   GuangDong Basic and Applied Basic Research   Foundation under Grant 2022A1515011835 , and   Anhui Provincial Natural Science Foundation un-   der Grant 2108085UD12 . We also acknowledge   the support of GPU cluster built by MCC Lab of   Information Science and Technology Institution ,   USTC .   References480848094810