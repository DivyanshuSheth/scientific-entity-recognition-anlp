  Neha Srikanth   Department of Computer Science   University of Maryland , College Park   nehasrik@umd.eduRachel Rudinger   Department of Computer Science   University of Maryland , College Park   rudinger@umd.edu   Abstract   When strong partial - input baselines reveal ar-   tifacts in crowdsourced NLI datasets , the per-   formance of full - input models trained on such   datasets is often dismissed as reliance on spuri-   ous correlations . We investigate whether state-   of - the - art NLI models are capable of overrid-   ing default inferences made by a partial - input   baseline . We introduce an evaluation set of   600 examples consisting of perturbed premises   to examine a RoBERTa model ’s sensitivity to   edited contexts . Our results indicate that NLI   models are still capable of learning to condi-   tion on context — a necessary component of in-   ferential reasoning — despite being trained on   artifact - ridden datasets .   1 Introduction   Natural language inference ( NLI ) is integral to   building systems that are broadly capable of lan-   guage understanding ( White et al . , 2017 ) . In a   traditional NLI setup , models are provided with a   premise as context and a corresponding hypothe-   sis . They must then determine whether the premise   entails , contradicts , or is neutral in relation to the   hypothesis ( Giampiccolo et al . , 2007 ) .   Researchers have shown that many NLI datasets   contain statistical biases , or “ annotation artifacts ”   ( Gururangan et al . , 2018 ; Herlihy and Rudinger ,   2021 ) that systems leverage to correctly predict   entailment . To diagnose such artifacts in datasets   and provide a stronger alternative to majority - class   baselines , Poliak et al . ( 2018 ) introduced partial-   input baselines , a setting in which models are pro-   vided parts of NLI instances to predict an entail-   ment relation . Poliak et al . ( 2018 ) , Gururangan   et al . ( 2018 ) , and others posit that datasets contain-   ing artifacts may in turn produce models that are   incapable of learning to perform true reasoning .   In this paper , we re - examine such dismissal of   the reasoning capabilities of models trained on   datasets containing artifacts . While a competi-   tive partial - input baseline is sufficient to cast doubtFigure 1 : Given NLI and δ - NLI instances that partial-   input baselines correctly label , we edit their context to   induce a different gold label . We use the edited instances   to probe a full - input model ’s ability to leverage context   to deviate from the predicted partial - input label .   on the inferential capabilities of full - input models   trained on the same data , it is premature to con-   clude that full - input models are not capable of such   reasoning at all . As a thought experiment , we imag-   ine training a human to learn a task from a set of   examples containing artifacts that allow them to   cheat . If we force them to learn to perform the task   by removing relevant context , they must rely on   the artifacts to accurately perform the task . How-   ever , provided all the data , they may still learn to   perform the task the “ right way ” .   Through two sets of experiments , we investi-   gate whether NLI models are able to condition on   the full input despite learning from artifact - ridden   datasets . Section 3 investigates whether additional   context strengthens a full - input model ’s confidence   in the correct label , despite a partial - input model ’s   correct prediction . In Section 4 , we introduce an   evaluation set to probe whether full - input models   are sensitive to changes in context that flip the gold   label in examples containing artifacts . We make   our evaluation set and code publicly available .   Our results indicate that full - input models are   still successfully learning to utilize context , over-   riding strong signal in the partial input . With this4753finding , we argue that while partial - input baselines   are still a useful tool , they do not license the conclu-   sion that models trained on datasets with artifacts   do not learn to leverage context from the full input .   2 Background   Here we describe the two related tasks of natural   language inference ( NLI ) and defeasible NLI ; two   corresponding datasets ( SNLI and δ - NLI , respec-   tively ) ; and annotation artifacts present in these   datasets .   Task Definitions . Natural language inference   ( Giampiccolo et al . , 2007 ; MacCartney , 2009 ; Bow-   man et al . , 2015 ) is the task of determining whether   apremise sentence ( P ) entails a hypothesis sen-   tence ( H ) . That is , given P , would a human con-   clude His true ( entailment ) , false ( contradiction ) ,   or neither ( neutral ) .   When His neutral , the task of defeasible natural   language inference ( Rudinger et al . , 2020 ) asks   whether a third update sentence ( U ) makes H   more likely to be true ( Uis astrengthener ) orless   likely to be true ( Uis aweakener ) . In the following   example , Hisneutral given P(NLI task ) , and Uis   aweakener given PandH(defeasible NLI task ) .   In this work , we adopt the terms context ( C ) and   target ( T ) for clarity when describing partial - input   baselines for the two tasks . In NLI , C = Pand   T = H ; in defeasible NLI , C= ( P , H ) , and   T = U. Thus , for either task , the partial - input   baseline we use looks only at Tand ignores C.   SNLI . SNLI ( Bowman et al . , 2015 ) is the first   large - scale English NLI dataset , containing 570 K   labeled P - Hpairs . In SNLI , premises are derived   from image captions ( Young et al . , 2014 ) , and hy-   potheses for each label ( entailment , neutral , con-   tradiction ) are elicited from crowdsource workers   who are shown a premise .   δ - NLI . For the task of defeasible NLI , Rudinger   et al . ( 2020 ) introduce the δ - NLI dataset , which   consists of extensions to three pre - existing English   natural language reasoning datasets : SNLI ( Bow-   man et al . , 2015 ) , ATOMIC ( Sap et al . , 2019 ) , and   SOCIAL - CHEM-101 ( Forbes et al . , 2020 ) . To   extend each dataset , instances ( e.g. , P - Hpairs)were presented to crowdworkers who then wrote   an update sentence ( U ) to strengthen or weaken the   given hypothesis ( as described in Task Definitions ) .   The resulting binary classification task is to predict   whether an update is a strengthener or weakener ,   given a ( P , H , U ) triple . In this work , we focus   our evaluation on δ - SNLI , the SNLI portion of the   δ - NLI dataset .   Artifacts and Partial - Input Baselines . Gururan-   gan et al . ( 2018 ) and Poliak et al . ( 2018 ) observe   that the crowdsourcing protocols adopted by Bow-   man et al . ( 2015 ) and others lead to the creation   of data with annotation artifacts that enable partial-   input baselines ( e.g. , hypothesis - only baselines ) to   perform well above a majority - class baseline . For   SNLI , Poliak et al . ( 2018 ) report that an InferSent   ( Conneau et al . , 2017 ) hypothesis - only baseline sur-   passes a majority baseline by 35 points . A similar   effect is observed by Rudinger et al . ( 2020 ) in the   δ - NLI data , with an update - only RoBERTa ( Liu   et al . , 2019 ) model achieving 15 points above a   majority baseline .   3 Experiment 1 : Context in NLI   An essential component of true reasoning is learn-   ing to leverage all parts of an example ’s input to   make a determination of entailment . Strong partial-   input models demonstrate that full - input models   do not necessarily need to utilize context to make   correct predictions . However , we explore whether   they do at all , and how access to such context shifts   a full - input model ’s confidence in the correct label .   If , upon supplying C , a model strengthens its con-   fidence in its prediction , we may conclude that it   utilizes both CandTduring inference as intended .   Experimental Setup . We finetune two sets of   RoBERTa ( Liu et al . , 2019 ) models ( a partial - input   and a full - input model ) on the train splits of SNLI   andδ - NLI ( see Appendix A for dataset sizes ) . We   utilize roberta - base from the Hugging Face   library ( Wolf et al . , 2020 ) , and finetune each model   for two epochs . Appendix B further details our   training setup . We then run inference on the test   splits from SNLI and δ - SNLI ( the SNLI portion   ofδ - NLI ) using each pair of models . Table 1 re-   ports accuracy of all models on the corresponding   test splits . We calibrate RoBERTa models post - hoc   using temperature scaling ( Guo et al . , 2017 ) as sug-   gested by Desai and Durrett ( 2020 ) , and examine   confidence shifts in the correct label to understand4754   SNLI δ - SNLI   Partial - Input ( C ) 0.70 0 .65   Full - Input ( C , T ) 0.91 0 .82   whether full - input models utilize context at all .   As shown in Fig . 2 , we plot an ordered pair of   each model ’s confidence in the correct label for   examples in the test splits , with the partial - input   model ’s confidence along the x - axis and the full-   input model ’s confidence along the y - axis . Density   around the diagonal would indicate no change in   confidence .   Evidenced by the density above the diagonal in   Figures 2a and 2b , full - input models ( i.e access to   bothCandT ) are more confident in the correct   label than partial - input models . While this behav-   ior may seem unsurprising , partial - input baselines   illustrate that models may show confidence in the   correct label without needing to condition on con-   text at all . Our results hint that full - input models   may be successfully learning to leverage additional   context instead of overgeneralizing on artifacts in   the target . To probe this behavior directly , we intro - duce an evaluation set crafted by editing contexts   in the following section .   4 Experiment 2 : Context Editing   We investigate a model ’s ability to leverage con-   text despite the presence of artifacts by exploring   how sensitive full - input models are to changes in   non - target components of the input . We present an   example modification scheme , illustrated in Figure   1 , in which we edit context sentences from exam-   ples where a model correctly predicts the label l   from the target Talone . Namely , while holding T   constant , we introduce an edited context sentence   Cthat induces a different label l̸=lon the new   ( C , T)pair . Using this scheme , we construct an   evaluation set of 600 examples sourced from SNLI   andδ - NLI .   Example Subselection . We select SNLI and δ-   SNLI test examples to edit by running the partial-   input RoBERTa models from Section 3 and full-   input bag - of - words ( BoW ) models , implemented   viafasttext with a maximum of 4 - grams   ( Joulin et al . , 2017 ) . See Appendix B for train-   ing details on the bag - of - words models . We select   examples to edit for which either the partial - input   model or the BoW model predicted the correct la-   bel . This identifies the subset of examples likeliest   to contain artifacts in T , lexical or otherwise .   Editing SNLI Examples . For a given SNLI ex-   ample ( P , H , l ) and a new predefined target label4755   l , we edit P , creating a modified SNLI example   ( P , H , l ) . For each of the six directional pairs of   labels ( e.g. , entailment →contradiction ) , we ran-   domly sample 50 examples from the subset to edit ,   resulting in 300 examples evenly distributed across   label pairs .   Editing δ - SNLI Examples . Given a δ - SNLI in-   stance ( P , H , U , l ) , we edit Hwhile holding Pand   Uconstant to induce a new label l̸=l , resulting   in a modified example ( P , H , U , l ) . We edit 300   examples total , turning 150 strengthener examples   intoweakener examples , and vice - versa .   Our final evaluation set consists of 600 examples   containing edited context - target pairs split evenly   across SNLI and δ - SNLI . Figure 4 shows exam-   ples of editing contexts from both datasets . All   examples were manually edited by one author and   independently validated by another . During valida-   tion , we hide both the landl , and ask the annotator   to label the text pair . Using Cohen ’s Kappa ( Co-   hen , 1960 ) , we obtain an agreement measure of   κ= 0.78andκ= 0.76for SNLI and δ - SNLI   examples in our test set respectively , indicating   substantial agreement ( Artstein and Poesio , 2008 ) .   Results . Using the same full - input RoBERTa   models trained in Section 3 , we run inference on   our edited evaluation set . Tables 2 and 3 show   model performance stratified by original label l   and target label l. Our results show that full - input   models are in fact sensitive to context modifications   despite the presence of artifacts in T , consistently   achieving above 70 % accuracy on edited examples .   Thus , we conclude that these models are not over-   generalizing on artifacts in the instance , learning   to condition on context for prediction .   Analyzing Post - Edit Model Confidence . Sim-   ilar to the analysis in Section 3 , we inspect shifts   in confidence upon editing contexts to shed more   light on a full - input model ’s utilization of C. For   δ - SNLI examples , we plot ordered pairs in Fig . 3   of a full - input model ’s confidence in the correct   label pre - edit , and its confidence in the same label   post - edit ( i.e the confidence in the now - incorrect   label ) . The majority of mass is under the diago-   nal , indicating that our model is indeed sensitive to   changes in context . The green bottom - left quadrant   delineates ideal performance ( correct before andaf-   ter editing examples ) . We attribute the small cluster   of examples in the blue quadrant ( previously highly   confident in the correct label and subsequently re-   mained confident , but in the wrong label ) to strong ,   non - lexical artifacts overriding additional signal   from the context . Appendix C visualizes shifts in   SNLI examples using simplex plots to accommo-   date the ternary label .   Lexical Model Performance . We do not retrain   any of the models on edited examples prior to evalu-   ating with our edited test set , precluding them from   picking up on any newly - introduced non - lexical   artifacts . However , to validate the absence of triv-   ial lexical features that override the artifacts in T   and to ensure sufficient difficulty , we run full - input   BoW models on our edited evaluation set . A full-4756   inputfasttext model , often used for adversarial   filtering ( Zellers et al . , 2018 ) , achieves 16 % and   24.3 % accuracy on the SNLI and δ - SNLI portions   of our evaluation set respectively .   5 Related Work   In addition to work on partial - input baselines for   NLI ( Gururangan et al . , 2018 ; Poliak et al . , 2018 ;   Tsuchiya , 2018 ) , partial - input models have been   studied for story completion ( Cai et al . , 2017 )   and reading comprehension ( Kaushik and Lipton ,   2018 ) . Feng et al . ( 2019 ) observe that low - scoring   partial - input baselines do not preclude other arti-   facts and heuristics , while Glockner et al . ( 2018 )   and McCoy et al . ( 2019 ) demonstrate lexical and   syntactic examples of NLI heuristics . Adversarial   editing has been explored for non - NLI tasks as well   ( Jia and Liang , 2017 ; Ribeiro et al . , 2018 ) . Finally ,   adversarial filtering has been proposed as a means   of removing artifacts from datasets ( Zellers et al . ,   2018 ; Le Bras et al . , 2020 ) .   6 Discussion and Conclusion   While partial - input models are useful tools for anal-   ysis , often leveling fair criticism of datasets , our   results show it is hasty to conclude that models   trained on such datasets are not capable of rea-   soning . Even though high - scoring partial - input   baselines show that full - input models could ignore   context , our experiments show that they can lever-   age this context quite effectively .   We argue that artifacts do not necessarily spelldisaster for a model ’s reasoning capabilities . In   particular , our context - editing experiments identify   a set of instances that partial - input models fail ( by   design ) , but full - input models largely succeed at ,   displaying the capability of full - input models to   leverage context to overcome SNLI and δ - NLI ar-   tifacts in many , but not all , cases . Of course , we   do not deny that artifacts cananddolead to mod-   els with exploitable heuristics , as demonstrated by   Glockner et al . ( 2018 ) and McCoy et al . ( 2019 ) .   While we do not attempt to define the sufficient   conditions for a model to perform “ true inference , ”   we demonstrate that these models can and do meet   thenecessary condition of leveraging the full in-   put . Thus , we conclude that partial input baselines   should be understood as agnostic warning signs :   sufficient to conclude that full - input models might   not be leveraging critical context , but insufficient   to prove that they do n’t .   This raises a number of interesting questions for   follow - up work . If adversarial filtering ( Le Bras   et al . , 2020 ) can identify instances containing arti-   facts , is it beneficial to remove these instances from   the training set ? Or could they be edited to flip the   label and mitigate the spurious correlation ? The ed-   its we made in this work were done manually , but   another interesting question is whether these edits   could be made automatically or semi - automatically .   Having a more efficient way of producing these   examples would enable both rapid evaluation of   models trained on datasets with artifacts ( as in this   work ) , as well as expansion of training sets to pre-   emptively mitigate artifacts.4757References47584759Dataset Split Size   SNLItrain 550,152   valid 10,000   test 10,000   δ - NLItrain 200,694   valid 14,968   test 15,414   δ - SNLItrain 88,676   valid 1,785   test 1,837   A SNLI and δ - NLI Dataset Sizes   To train and tune our neural and lexical models , we   utilize the train and validation splits from SNLI and   δ - NLI . We include train / validation / test split sizes   in Table 4 . Our δ - NLI RoBERTa models were fine-   tuned on examples from all 3 portions of the δ - NLI   dataset ( ATOMIC , SNLI , and SOCIAL - CHEM-   101 ) , however for our evaluation set and analysis ,   we exclusively use the SNLI portion of the δ - NLI   dataset , abbreviated as δ - SNLI . We include the split   sizes for δ - SNLI in Table 4 as well .   B Model Training Setup   Neural Models . We use the Hugging Face library   to train all of our RoBERTa models . We utilize   roberta - base , which has 125 M trainable pa-   rameters . All models were trained on a single   NVIDIA 1080 - TI GPU . After tuning on the val-   idation set , all models were trained for two epochs   with a learning rate of 2e-5 and a batch size of 32 .   Tables 2 and 3 report best accuracy across five runs .   Lexical Models . We use the fasttext library   to implement our bag - of - words models . fast-   text is an off - the - shelf text classification library .   All lexical models were trained for 5 epochs with   4 - grams as the maximum length of word ngrams .   We use the default learning rate of 0.1 .   CVisualizing Distribution Shifts in SNLI   Edited Examples   We are able to visualize confidence distribution   shifts for δ - NLI before and after editing using   a 2D plane with ordered pairs , as in Figure 3 ,   due to the label set containing only two update   types — weakener and strengthener ( so , a probabil-   ity score > 0.5results in the predicted label ) . Sincethe SNLI label set consists of three labels ( entail-   ment , neutral , contradiction ) , we choose to visual-   ize shifts in the confidence distribution before and   after editing contexts via ternary plots . For each   directional label pair ( entailment →{neutral , con-   tradiction } , neutral→{entailment , contradiction } ,   contradiction →{entailment , neutral } ) , we plot a   heatmap of probabilities , or confidences , in each   of the three classes on the simplex with Gaussian   smoothing using python - ternary , a ternary   plotting package ( Harper , 2015 ) . Figures 5 , 6 , and   7 show these visualizations . We include these plots   mainly to help visualize information about the pre-   dicted labels of the incorrect examples ( Table 2   only reports the accuracy on finer - grained buck-   ets ) . We observe that for most classes of examples ,   the SNLI RoBERTa model utilizes the context , and   correctly predicts the new induced gold label . How-   ever , the { neutral→entailment } class of examples   in particular proved difficult for the model , as ev-   idenced by a large chunk of a mass remaining in   the neutral corner of the simplex .   D Dataset Limitations   In this work , we choose to explore the role of con-   text with respect to the SNLI and δ - NLI datasets .   In particular , the proven presence of strong artifacts   in SNLI made it an appealing dataset to explore a   model ’s behavior with respect to the utilization of   context . We chose to include δ - NLI in our analy-   sis , since ultimately , we ’d like reasoning systems   to operate in complex and dynamic contexts . The   ability to be sensitive to shifting contexts and under-   stand when default inferences should be overridden   by additional context ( i.e more nuanced inference )   is both central to our exploration and central to   the task of defeasible reasoning itself . Our evalu-   ation set does not include examples sourced from   other NLI datasets such as MultiNLI ( Williams   et al . , 2018 ) . It also does not contain datasets   across domains , such as MedNLI ( Romanov and   Shivade , 2018 ) . However , we note that while the   datasets may be different , others have shown ar-   tifacts present in such datasets ( i.e ( Herlihy and   Rudinger , 2021 ; Gururangan et al . , 2018 ) . Our goal   was to utilize datasets containing high amounts of   artifacts.4760476147624763