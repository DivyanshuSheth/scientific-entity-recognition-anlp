  Sanjay SubramanianWill MerrillTrevor Darrell   Matt GardnerSameer SinghAnna RohrbachUC BerkeleyNew York UniversityMicrosoft Semantic MachinesUC IrvineAllen Institute for AI ( AI2 )   { sanjayss,trevordarrell,anna.rohrbach}@berkeley.edu ,   wcm9940@nyu.edu , mattgardner@microsoft.com , sameer@uci.edu   Abstract   Training a referring expression comprehension   ( ReC ) model for a new visual domain requires   collecting referring expressions , and potentially   corresponding bounding boxes , for images in   the domain . While large - scale pre - trained mod-   els are useful for image classification across   domains , it remains unclear if they can be ap-   plied in a zero - shot manner to more complex   tasks like ReC. We present ReCLIP , a simple   but strong zero - shot baseline that repurposes   CLIP , a state - of - the - art large - scale model , for   ReC. Motivated by the close connection be-   tween ReC and CLIP ’s contrastive pre - training   objective , the first component of ReCLIP is a   region - scoring method that isolates object pro-   posals via cropping and blurring , and passes   them to CLIP . However , through controlled ex-   periments on a synthetic dataset , we find that   CLIP is largely incapable of performing spatial   reasoning off - the - shelf . Thus , the second com-   ponent of ReCLIP is a spatial relation resolver   that handles several types of spatial relations .   We reduce the gap between zero - shot baselines   from prior work and supervised models by as   much as 29 % on RefCOCOg , and on RefGTA   ( video game imagery ) , ReCLIP ’s relative im-   provement over supervised ReC models trained   on real images is 8 % .   1 Introduction   Visual referring expression comprehension ( ReC ) —   the task of localizing an object in an image given   a textual referring expression — has applications in   a broad range of visual domains . For example ,   ReC is useful for guiding a robot in the real world   ( Shridhar et al . , 2020 ) and also for creating natu-   ral language interfaces for software applications   with visuals ( Wichers et al . , 2018 ) . Though the   task is the same across domains , the domain shift   is problematic for supervised referring expression   models , as shown in Figure 1 : the same simple   Figure 1 : Predictions from ReCLIP ( cyan ) and   UNITER - Large ( Chen et al . , 2020 ) ( red ) for the same   referring expression on images from two visual domains .   UNITER - Large fails on the GTA ( video game ) domain ,   while ReCLIP selects the correct proposal in both cases .   Close - ups of the two GTA boxes are shown .   referring expression is localized correctly in the   training domain but incorrectly in a new domain .   Collecting task - specific data in each domain   of interest is expensive . Weakly supervised ReC   ( Rohrbach et al . , 2016 ) partially addresses this is-   sue , since it does not require the ground - truth box   for each referring expression , but it still assumes   the availability of referring expressions paired with   images and trains on these . Given a large - scale pre-   trained vision and language model and a method   for doing ReC zero - shot — i.e. without any addi-5198   tional training — practitioners could save a great   deal of time and effort . Moreover , as pre - trained   models have become more accurate via scaling ( Ka-   plan et al . , 2020 ) , fine - tuning the best models has   become prohibitively expensive – and sometimes in-   feasible because the model is offered only via API ,   e.g. GPT-3 ( Brown et al . , 2020 ) .   Pre - trained vision and language models like   CLIP ( Radford et al . , 2021 ) achieve strong zero-   shot performance in image classification across   visual domains ( Jia et al . , 2021 ) and in object de-   tection ( Gu et al . , 2021 ) , but the same success has   not yet been achieved in tasks requiring reason-   ing over vision and language . For example , Shen   et al . ( 2021 ) show that a straightforward zero - shot   approach for VQA using CLIP performs poorly .   Specific to ReC , Yao et al . ( 2021 ) introduce a zero-   shot approach via Colorful Prompt Tuning ( CPT ) ,   which colors object proposals and references the   color in the text prompt to score proposals , but   this has low accuracy . In both of these cases , the   proposed zero - shot method is not aligned closely   enough with the model ’s pre - training task of match-   ing naturally occurring images and captions .   In this work , we propose ReCLIP , a simple but   strong new baseline for zero - shot ReC. ReCLIP ,   illustrated in Figure 2 , has two key components : a   method for scoring object proposals using CLIP   and a method for handling spatial relations between   objects . Our method for scoring region proposals ,   Isolated Proposal Scoring ( IPS ) , effectively reduces   ReC to the contrastive pre - training task used by   CLIP and other models . Specifically , we propose   to isolate individual proposals via cropping and   blurring the images and to score these isolated pro-   posals with the given expression using CLIP.To handle relations between objects , we first   consider whether CLIP encodes the spatial infor-   mation necessary to resolve these relations . We   show through a controlled experiment on CLEVR   images ( Johnson et al . , 2017 ) that CLIP and another   pre - trained model ALBEF ( Li et al . , 2021 ) are un-   able to perform its pre - training task on examples   that require spatial reasoning . Thus , any method   that solely relies on these models is unlikely to   resolve spatial relations accurately . Consequently ,   we propose spatial heuristics for handling spatial   relations in which an expression is decomposed   into subqueries , CLIP is used to compute proposal   probabilities for each subquery , and the outputs for   all subqueries are combined with simple rules .   On the standard RefCOCO / g/+ datasets ( Mao   et al . , 2016 ; Yu et al . , 2016 ) , we find that ReCLIP   outperforms CPT ( Yao et al . , 2021 ) by about 20 % .   Compared to a stronger GradCAM ( Selvaraju et al . ,   2017 ) baseline , ReCLIP obtains better accuracy on   average and has less variance across object types .   Finally , in order to illustrate the practical value of   zero - shot grounding , we also demonstrate that our   zero - shot method surpasses the out - of - domain per-   formance of state - of - the - art supervised ReC mod-   els . We evaluate on the RefGTA dataset ( Tanaka   et al . , 2019 ) , which contains images from a video   game ( out of domain for models trained only on   real photos ) . Using ReCLIP and an object detector   trained outside the target domain , we outperform   UNITER - Large ( Chen et al . , 2020 ) ( using the same   proposals ) and MDETR ( Kamath et al . , 2021 ) by   an absolute 4.5 % ( relative improvement of 8 % ) .   In summary , our contributions include : ( 1 ) Re-   CLIP , a zero - shot method for referring expression   comprehension , ( 2 ) showing that CLIP has low5199zero - shot spatial reasoning performance , and ( 3 ) a   comparison of our zero - shot ReC performance with   the out - of - domain performance of state - of - the - art   fully supervised ReC systems .   2 Background   In this section , we first describe the task at hand   ( § 2.1 ) and introduce CLIP , the pre - trained model   we primarily use ( § 2.2 ) . We then describe two   existing methods for scoring region proposals using   a pre - trained vision and language model : colorful   prompt tuning ( § 2.3 ) and GradCAM ( § 2.4 ) .   2.1 Task description   In referring expression comprehension ( ReC ) , the   model is given an image and a textual referring   expression describing an entity in the image . The   goal of the task is to select the object ( bounding   box ) that best matches the expression . As in much   of the prior work on REC , we assume access to a   set of object proposals b , b , ... , b , each of which   is a bounding box in the image . Task accuracy is   measured as the percentage of instances for which   the model selects a proposal whose intersection-   over - union ( IoU ) with the ground - truth box is at   least 0.5 . In this paper , we focus on the zero - shot   setting in which we apply a pre - trained model to   ReC without using any training data for the task .   2.2 Pre - trained model architecture   The zero - shot approaches that we consider are   general in that the only requirement for the pre-   trained model is that when given a query con-   sisting of an image and text , it computes a score   for the similarity between the image and text . In   this paper , we primarily use CLIP ( Radford et al . ,   2021 ) . We focus on CLIP because it was pre-   trained on 400 M image - caption pairs collected   from the weband therefore achieves impressive   zero - shot image classification performance on a   variety of visual domains . CLIP has an image-   only encoder , which is either a ResNet - based ar-   chitecture ( He et al . , 2016 ) or a visual transformer   ( Dosovitskiy et al . , 2021 ) , and a text - only trans-   former . We mainly use the RN50x16 and ViT-   B/32 versions of CLIP . The image encoder takes   the raw image and produces an image representa-   tionx∈R , and the text transformer takes thesequence of text tokens and produces a text rep-   resentation y∈R. In CLIP ’s contrastive pre-   training task , given a batch of Nimages and match-   ing captions , each image must be matched with   the corresponding text . The model ’s probability   of matching image iwith caption jis given by   exp(βxy)/Pexp(βxy ) , where βis a   hyperparameter .   We now describe two techniques from prior work   for selecting a proposal using a pre - trained model .   2.3 Colorful Prompt Tuning ( CPT )   The first baseline from prior work that we consider   is colorful prompt tuning ( CPT ) , proposed by Yao   et al . ( 2021 ): they shade proposals with differ-   ent colors and use a masked language prompt in   which the referring expression is followed by “ in   [ MASK ] color ” . The color with the highest proba-   bility from a pre - trained masked language model   ( MLM ) ( VinVL ; ( Zhang et al . , 2021 ) ) is then cho-   sen . In order to apply this method to models like   CLIP , that provide image - text scores but do not of-   fer an MLM , we create a version of the input image   for each proposal , where the proposal is transpar-   ently shaded in red . Our template for the input text   is “ [ referring expression ] is in red color . ” Since we   have adapted CPT for non - MLM models , we refer   to this method as CPT - adapted in the experiments .   2.4 Gradient - based visualizations   The second baseline from prior work that we con-   sider is based on gradient - based visualizations ,   which are a popular family of techniques for un-   derstanding , on a range of computer vision tasks ,   which part(s ) of an input image are most impor-   tant to a model ’s prediction . We focus on the most   popular technique in this family , GradCAM ( Sel-   varaju et al . , 2017 ) . Our usage of GradCAM fol-   lows Li et al . ( 2021 ) , in which GradCAM is used   to perform weakly supervised referring expression   comprehension using the ALBEF model . In our   setting , for a given layer in a visual transformer ,   we take the layer ’s class - token ( CLS ) attention ma-   trixM∈R. The spatial dimensions hand   ware dependent on the model ’s architecture and   are generally smaller than the input dimensions5200of the image . Then the GradCAM is computed   asG = M⊙ , where Lis the model ’s output   logit ( the similarity score for the image - text pair )   and⊙denotes elementwise multiplication . The   procedure for applying GradCAM when the visual   encoder is a convolutional network is similar ; in   place of the attention matrix , we use the activa-   tions of the final convolutional layer . Next , we   perform a bicubic interpolation on Gso that it has   the same dimensions as the input image . Finally ,   we compute for each proposal b= ( x , y , x , y )   the scorePPG[i , j ] , where Ais the   area of the image and αis a hyperparameter , and   we choose the proposal with the highest score .   3 ReCLIP   ReCLIP consists of two main components : ( 1 ) a   region - scoring method that is different from CPT   and GradCAM and ( 2 ) a rule - based relation re-   solver . In this section , we first describe our region   scoring method ( § 3.1 ) . However , using controlled   experiments on a synthetic dataset , we find that   CLIP has poor zero - shot spatial reasoning perfor-   mance ( § 3.2 ) . Therefore , we propose a system that   uses heuristics to resolve spatial relations ( § 3.3 ) .   3.1 Isolated Proposal Scoring ( IPS )   Our proposed method , which we call isolated pro-   posal scoring , is based on the observation that   ReC is similar to the contrastive learning task with   which models like CLIP are pre - trained , except   that rather than selecting one out of several im-   ages to match with a given text , we must select   one out of several image regions . Therefore , for   each proposal , we create a new image in which   that proposal is isolated . We consider two methods   of isolation – cropping the image to contain only   the proposal and blurring everything in the image   except for the proposal region . For blurring , we   apply a Gaussian filter with standard deviation σ   to the image RGB values . Appendix A.2 provides   an example of isolation by blurring . The score for   an isolated proposal is obtained by passing it and   the expression through the pre - trained model . To   use cropping and blurring in tandem , we obtain   a score sandsfor each proposal and use   s+sas the final score . This can be viewed   as an ensemble of “ visual prompts , ” analogous to   Radford et al . ( 2021 ) ’s ensembling of text prompts .   3.2 Can we use CLIP to resolve spatial   relations ?   A key limitation in Isolated Proposal Scoring is   that relations between objects in different propos-   als are not taken into account . For example , in   Figure 2 , the information about the spatial rela-   tionships among the cats is lost when the proposals   are isolated . In order to use CLIP to decide which   object has a specified relation to another object ,   the model ’s output must encode the spatial relation   in question . Therefore , we design an experiment   to determine whether a pre - trained model , such   as CLIP , can understand spatial relations within   the context of its pre - training task . We generate   synthetic images using the process described for   the CLEVR dataset ( Johnson et al . , 2017 ) . These   scenes include three shapes – spheres , cubes , and   cylinders – and eight colors – gray , blue , green , cyan ,   yellow , purple , brown , red .   In the text - pair version of our tasks , using the   object attribute and position information associated   with each image , we randomly select one of the   pairwise relationships between objects – left , right ,   front , or behind – and construct a sentence fragment   based on it . For example : “ A blue sphere to the   left of a red cylinder . ” We also write a distractor   fragment that replaces the relation with its opposite .   In this case , the distractor would be “ A blue sphere   to the right of a red cylinder . ” The task , similar to   the contrastive and image - text matching tasks used   to pre - train these models , is to choose the correct   sentence given the image . As a reference point ,   we also evaluate on a control ( non - spatial ) task in   which the correct text is a list of the scene ’s objects   and the distractor text is identical except that one   object is swapped with a random object not in the   scene . For example , if the correct text is “ A blue   sphere and a red cylinder , ” then the distractor text   could be “ A blue sphere and a blue cylinder . ”   In the image - pair version of our tasks , we have a5201single sentence fragment constructed as described   above for the spatial and control ( non - spatial ) tasks   and two images such that only one matches the text .   Appendix B shows examples of these tasks .   CLIP ’s performance on these tasks is shown in   Table 1 . Similar results for the pre - trained model   ALBEF ( Li et al . , 2021 ) are shown in Appendix D.1   While performance on the control task is quite   good , accuracy on the spatial task is not so dif-   ferent from random chance ( 50 % ) . This indicates   that the model scores of image - text pairs largely do   not take spatial relations into account .   3.3 Spatial Relation Resolver   Since CLIP lacks sensitivity to spatial relations ,   we propose to decompose complex expressions   into simpler primitives . The basic primitive is a   predicate applying to an object , which we use CLIP   to answer . The second primitive is a spatial relation   between objects , for which we use heuristic rules .   Predicates A predicate is a textual property that   the referent must satisfy . For example , “ the cat ”   and “ blue airplane ” are predicates . We write P(i )   to say that object isatisfies the predicate P. We   model Pas a categorical distribution over objects ,   and estimate p(i ) = Pr [ P(i)]with the pre - trained   model using isolated proposal scoring ( § 3.1 ) .   Relations We have already discussed the impor-   tance of binary spatial relations like “ the cat to the   leftof the dog ” for the ReC task . We consider   seven spatial relations – left , right , above , below ,   bigger , smaller , and inside . We write R(i , j)to   mean that the relation Rholds between objects i   andj , and we use heuristics to determine the prob-   ability r(i , j ) = Pr [ R(i , j ) ] . For example , for left ,   we set r(i , j ) = 1 if the center point of box iis to   the left of the center point of box jandr(i , j ) = 0   otherwise . § C.1 describes all relation semantics .   Superlative Relations We also consider superla-   tives , which refer to an object that has some relation   to all other objects satisfying the same predicate ,   e.g. “ leftmost dog ” . We handle superlatives as a   special case of relations where the empty second ar-   gument is filled by copying the predicate specifying   the first argument . Thus , “ leftmost dog ” effectively   finds the dog that is most likely to the left of other   dog(s ) . Our set of superlative relation types is the   same as our set of relation types , excluding inside .   Semantic Trees Having outlined the semantic   formalism underlying our method , we can describe   it procedurally . We first use spaCy ( Honnibal and   Johnson , 2015 ) to build a dependency parse for the   expression . As illustrated in Figure 3 , we extract   a semantic tree from the dependency parse , where   each noun chunk becomes a node , and dependency   paths between the heads of noun chunks become   relations between entities based on the keywords   they contain . See § C.2 for extraction details . In   cases where none of our relation / superlative key-   words occur in the text , we simply revert to the   plain isolated proposal scoring method using the   full text .   In the tree , each node Ncontains a predicate P   and has a set of children ; an edge ( N , N)between   Nand its child Ncorresponds to a relation R.   For example , as shown in Figure 3 , “ a cat to the left   of a dog ” would be parsed as a node containing the   predicate “ a cat ” connected by the relation leftto its   child corresponding to “ a dog ” . We define π(i )   as the probability that node Nrefers to object i ,   and compute it recursively . For each node N , we   first set π(i ) = p(i)and then iterate through   each child Nand update π(i)as follows :   π(i)∝π(i)XPr   R(i , j)∧P(j)   ∝π(i)Xr(i , j)π(j ) .   The last line makes the simplifying assumption that   all predicates and relations are independent .   To compute our final output , we ensemble the   distribution πfor the root node with the output   of plain isolated proposal scoring ( with the whole   input expression ) by multiplying the proposal prob-   abilities elementwise . This method gives us a prin-   cipled way to combine predicates ( P ) with spatial   relational constraints ( R ) for each node N.5202   4 Experiments   4.1 Datasets   We compare ReCLIP to other zero - shot methods on   RefCOCOg ( Mao et al . , 2016 ) , RefCOCO andRe-   fCOCO+ ( Yu et al . , 2016 ) . These datasets use im-   ages from MS COCO ( Lin et al . , 2014 ) . RefCOCO   and RefCOCO+ were created in a two - player game ,   and RefCOCO+ is designed to avoid spatial rela-   tions . RefCOCOg includes spatial relations and   has longer expressions on average . For comparing   zero - shot methods with the out - of - domain perfor-   mance of models trained on COCO , we use Re-   fGTA ( Tanaka et al . , 2019 ) , which contains images   from the Grand Theft Auto video game . All re-   ferring expressions in RefGTA correspond to peo-   ple , and the objects ( i.e. people ) tend to be much   smaller on average than those in RefCOCO / g/+ .   4.2 Implementation Details   We use an ensemble of the CLIP RN50x16 and   ViT - B/32 models ( results for individual models   are shown in Appendix G ) . We ensemble model   outputs by adding together the logits from the   two models elementwise before taking the soft-   max . GradCAM ’s hyperparameter αcontrols theeffect of the proposal ’s area on its score . We se-   lectα= 0.5for all models based on tuning on the   RefCOCOg validation set . We emphasize that the   optimal value of αfor a dataset depends on the size   distribution of ground - truth objects . ReCLIP also   has a hyperparameter , namely the standard devi-   ation σ . We try a few values on the RefCOCOg   validation set and choose σ= 100 , as we show   in Appendix E.4 , isolated proposal scoring has lit-   tle sensitivity to σ . As discussed by ( Perez et al . ,   2021 ) , zero - shot experiments often use labeled data   for model selection . Over the course of this work ,   we primarily experimented with the RefCOCOg   validation set and to a lesser extent with the Ref-   COCO+ validation set . For isolated proposal scor-   ing , the main variants explored are documented in   our ablation study ( § 4.6 ) . Other techniques that we   tried , including for relation - handling , and further   implementation details are given in Appendix E.   4.3 Results on RefCOCO / g/+   Table 2 shows results on RefCOCO , RefCOCO+ ,   and RefCOCOg . ReCLIP is better than the other   zero - shot methods on RefCOCOg and RefCOCO   and on par with GradCAM on RefCOCO+ . How-   ever , GradCAM has a much higher variance in its   accuracy between the TestA and TestB splits of Re-   fCOCO+ and RefCOCO . We note that GradCAM’s5203hyperparameter α , controlling the effect of pro-   posal size , was tuned on the RefCOCOg validation   set , and RefCOCOg was designed such that boxes   of referents are at least 5 % of the image area ( Mao   et al . , 2016 ) . In the bottom portion of Table 2 , we   show that when this 5 % threshold , a prior on object   size for this domain , is used to filter proposals for   both GradCAM and ReCLIP , ReCLIP performs on   par with / better than GradCAM on TestA. ReCLIP ’s   spatial relation resolver helps on RefCOCOg and   RefCOCO but not on RefCOCO+ , which is de-   signed to avoid spatial relations .   4.4 Results on RefGTA   Next , we evaluate on RefGTA to compare our   method ’s performance to the out - of - domain accu-   racy of two state - of - the - art fully supervised ReC   models : UNITER - Large ( Chen et al . , 2020 ) and   MDETR ( Kamath et al . , 2021 ) .   Like ReCLIP , UNITER takes proposals as in-   put . We show results using ground - truth propos-   als and detections from UniDet ( Zhou et al . , 2021 ) ,   which is trained on the COCO , Objects365 ( Shao   et al . , 2019 ) , OpenImages ( Kuznetsova et al . , 2020 ) ,   and Mapillary ( Neuhold et al . , 2017 ) datasets . Fol-   lowing the suggestion of the UniDet authors , we   use the confidence threshold of 0.5 . MDETR does   not take proposals as input .   Table 3 shows our results . For methods that take   proposals ( all methods except MDETR ) , we con-   sider two evaluation settings using UniDet – DT - P ,   in which the detected proposals are filtered to have   only proposals whose predicted class label is “ per-   son ” , and DT , in which all detected proposals are   considered . ReCLIP ’s accuracy is more than 15 %   higher than the accuracy of UNITER - Large and   roughly 5 % more than that of MDETR . ReCLIP   also outperforms GradCAM by about 20 % , and the   gap is larger when all UniDet proposals are con-   sidered . ReCLIP w/o relations is 1 - 2 % better than   ReCLIP in the settings with ground - truth proposals   and filtered UniDet proposals . One possible reason   for this gap is that the objects of relations in the   expressions could be non - people entities . When   considering all UniDet proposals , the relation re-   solver in ReCLIP does not hurt accuracy much but   also does not improve accuracy significantly – an ad-   ditional challenge in this setting is that the number   of proposals is dramatically higher . Appendix F   shows qualitative examples of predictions on Re-   fGTA .   4.5 Using another Pre - trained Model   In order to determine how isolated proposal scor-   ing ( IPS ) compares to GradCAM and CPT on other   pre - trained models , we present results using AL-   BEF ( Li et al . , 2021 ) . ALBEF offers two methods   for scoring image - text pairs – the output used for   its image - text contrastive ( ITC ) loss and the out-   put used for its image - text matching ( ITM ) loss .   The architecture providing the ITC output is very   similar to CLIP – has only a shallow interaction be-   tween the image and text modalities . The ITM   output is given by an encoder that has deeper in-   teractions between image and text and operates   on top of the ITC encoders ’ output . Appendix D   provides more details . The results , shown in Ta-   ble 4 , show that with the ITC output , IPS performs   better than GradCAM , but with the ITM output ,   GradCAM performs better . This suggests that IPS   works well across models like CLIP and ALBEF   ITC ( i.e. contrastively pre - trained with shallow   modality interactions ) but that GradCAM may be   better for models with deeper interactions .   4.6 Analysis   Performance of IPS Our results show that   among the region scoring methods that we consider,5204   IPS achieves the highest accuracy for contrastively   pre - trained models like CLIP . Figure 4a gives in-   tuition for this — aside from an object ’s attributes ,   many referring expressions describe the local con-   text around an object , and IPS focuses on this local   context ( as well as object attributes ) .   Table 5 shows that using both cropping and blur-   ring obtains greater accuracy than either alone .   Error Analysis and Limitations Although Re-   CLIP outperforms the baselines that we consider ,   there is a considerable gap between it and super-   vised methods . The principal challenge in improv-   ing the system is making relation - handling more   flexible . There are several object relation types   that our spatial relation resolver can not handle ; for   instance , those that involve counting : “ the second   dog from the right . ” Another challenge is in deter-   mining which relations require looking at multiple   proposals . For instance , ReCLIP selects a proposal   corresponding to the incorrect noun chunk in Fig-   ure 4b because the relation resolver has no rule for   splitting an expression on the relation “ with . ” De-   pending on the context , relations like “ with ” may   or may not require looking at multiple proposals ,   so handling them is challenging for a rule - based   system .   In the RefCOCO+ validation set , when using de-   tected proposals , there are 75 instances for which   ReCLIP answers incorrectly but ReCLIP w/o re-   lations answers correctly . We categorize these in-   stances based on their likely sources of error : 4   instances are ambiguous ( multiple valid propos-   als ) , in 7 instances the parser misses the head noun   chunk , in 14 instances our processing of the parse   leads to omissions of text when doing isolated pro-   posal scoring ( e.g. in “ girl sitting in back , ” the   only noun chunk is “ girl , ” so this is the only text   used during isolated proposal scoring ) , 52 cases   in which there is an error in the execution of the   heuristic ( e.g. our spatial definition of a relation   does not match the relation in the instance ) . ( There   are 2 instances for which we mark 2 categories . )   The final category ( “ execution ” ) includes several   kinds of errors , some examples of which are shown   in Appendix F.   5 Related Work   Referring expression comprehension Datasets   for ReC span several visual domains , including   photos of everyday scenes ( Mao et al . , 2016 ;   Kazemzadeh et al . , 2014 ) , video games ( Tanaka   et al . , 2019 ) , objects in robotic context ( Shridhar   et al . , 2020 ; Wang et al . , 2021 ) , and webpages   ( Wichers et al . , 2018 ) .   Spatial heuristics have been used in previous   work ( Moratz and Tenbrink , 2006 ) . Our work is   also related to Krishnamurthy and Kollar ( 2013),5205which similarly decomposes the reasoning process   into a parsing step and visual execution steps , but   the visual execution is driven by learned binary   classifiers for each predicate type . In the super-   vised setting , prior work shows that using an ex-   ternal parser , as we do , leads to lower accuracy   than training a language module jointly with the   remainder of the model ( Hu et al . , 2017 ) .   There is a long line of work in weakly super-   vised ReC , where at training time , pairs of refer-   ring expressions and images are available but the   ground - truth bounding boxes for each expression   are not ( Rohrbach et al . , 2016 ; Liu et al . , 2019 ;   Zhang et al . , 2018 , 2020 ; Sun et al . , 2021 ) . Our   setting differs from the weakly supervised setting   in that the model is not trained at all on the ReC   task . Sadhu et al . ( 2019 ) discuss a zero - shot setting   different from ours in which novel objects are seen   at test time , but the visual domain stays the same .   Pre - trained vision and language models Early   pre - trained vision and language models ( Tan and   Bansal , 2019 ; Lu et al . , 2019 ; Chen et al . , 2020 )   used a cross - modal transformer ( Vaswani et al . ,   2017 ) and pre - training tasks like masked language   modeling , image - text matching , and image feature   regression . By contrast , CLIP and similar models   ( Radford et al . , 2021 ; Jia et al . , 2021 ) use a sepa-   rate image and text transformer and a contrastive   pre - training objective . Recent hybrid approaches   augment CLIP ’s architecture with a multi - modal   transformer ( Li et al . , 2021 ; Zellers et al . , 2021 ) .   Zero - shot application of pre - trained models   Models pre - trained with the contrastive objective   have exhibited strong zero - shot performance in im-   age classification tasks ( Radford et al . , 2021 ; Jia   et al . , 2021 ) . Gu et al . ( 2021 ) use CLIP can be   to classify objects by computing scores for class   labels with cropped proposals . Our IPS is different   in that it isolates proposals by both cropping and   blurring . Shen et al . ( 2021 ) show that a simple   zero - shot application of CLIP to visual question   answering performs almost on par with random   chance . Yao et al . ( 2021 ) describe a zero - shot   method for ReC based on a pre - trained masked lan-   guage model ( MLM ) ; we show that their zero - shot   results and a version of their method adapted for   models pre - trained to compute image - text scores   ( rather than MLM ) are substantially worse than   isolated proposal scoring and GradCAM.6 Conclusion   We present ReCLIP , a zero - shot method for refer-   ring expression comprehension ( ReC ) that decom-   poses an expression into subqueries , uses CLIP to   score isolated proposals against these subqueries ,   and combines the outputs with spatial heuristics .   ReCLIP outperforms zero - shot ReC approaches   from prior work and also performs well across vi-   sual domains : ReCLIP outperforms state - of - the - art   supervised ReC models , trained on natural images ,   when evaluated on RefGTA . We also find that CLIP   has low zero - shot spatial reasoning performance ,   suggesting the need for pre - training methods that   account more for spatial reasoning .   7 Ethical and Broader Impacts   Recent work has shown that pre - trained vision and   language models suffer from biases such as gen-   der bias ( Ross et al . , 2021 ; Srinivasan and Bisk ,   2021 ) . Agarwal et al . ( 2021 ) provide evidence that   CLIP has racial and other biases , which makes   sense since CLIP was trained on data collected   from the web and not necessarily curated carefully .   Therefore , we do not advise deploying our system   directly in the real world immediately . Instead ,   practitioners interested in this system should first   perform analysis to measure its biases based on pre-   vious work and attempt to mitigate them . We also   note that our work relies heavily on a pre - trained   model whose pre - training required a great deal of   energy , which likely had negative environmental   effects . That being said our zero - shot method does   not require training a new model and in that sense   could be more environmentally friendly than super-   vised ReC models ( depending on the difference in   the cost of inference ) .   8 Acknowledgements   We thank the Berkeley NLP group and Med-   hini Narasimhan for helpful comments . We   thank Michael Schmitz for help with AI2 in-   frastructure . This work was supported in part   by DoD , including DARPA ’s LwLL , and/or Se-   maFor programs , and Berkeley Artificial Intelli-   gence Research ( BAIR ) industrial alliance pro-   grams . Sameer Singh was supported in part by the   National Science Foundation grant # IIS-1817183   and in part by the DARPA MCS program under   Contract No . N660011924033 with the United   States Office Of Naval Research.5206References520752085209   A Visualization of Region - Scoring   Methods   A.1 Colorful Prompt Tuning ( CPT )   Figure 5 shows an example of the visual represen-   tation of a proposal using CPT - adapted .   A.2 Isolated Proposal Scoring ( IPS )   Figure 6 shows the blurred versions of the propos-   als for an image using σ= 100 .   B Synthetic Spatial Reasoning   Experiment   Figure 7 gives an example of the text - pairs version   of the synthetic tasks .   Figure 8 gives an example of the image - pairs   version of the synthetic tasks .   C Semantic Formalism   C.1 Relation Semantics   We use deterministic heuristics to compute the se-   mantics of the following six relations : left , right ,   above , below , bigger , and smaller . On the other   hand , we treat inside as a random variable , and use   heuristics to compute the value of its parameter .   ForR∈ { left , right , above , below } , we compute   R(i , j)by checking whether Rholds between the   center point of box iand box j. For example , if the   center point of iis to the left of the center point of   boxj , then left(i , j ) = 1 .   We compute bigger ( i , j)andsmaller ( i , j)sim-   ply by comparing the areas of boxes iandj . For   example , bigger ( i , j)checks that the area of box i5210is greater than the area of box j.   Finally , for R = inside , we parameterize r(i , j )   as the ratio between the are of the intersection of   boxes i , jcompared to the area of box i. Thus ,   unlike the other six deterministic rules , inside is   modeled as a random variable .   C.2 Relation Extraction   We identify noun chunks in the dependency parse   as predicates . We then extract relations by looking   for dependency paths between the heads of noun   chunks that contain the following keywords :   •left : “ left ” , “ west ”   •right : “ right ” , “ east ”   •above : “ above ” , “ north ” , “ top ” , “ back ” , “ be-   hind ”   •below : “ below ” , “ south ” , “ under ” , “ front ”   •bigger : “ bigger ” , “ larger ” , “ closer ”   •smaller : “ smaller ” , “ tinier ” , “ further ”   •inside : “ inside ” , “ within ” , “ contained ”   We extract superlative relations by looking for de-   pendency paths off the head of a noun chunk con-   taining the following keywords :   •left : “ left ” , “ west ” , “ leftmost ” , “ western ”   •right : “ right ” , “ rightmost ” , “ east ” , “ eastern ”   •above : “ above ” , “ north ” , “ top ”   •below : “ below ” , “ south ” , “ underneath ” ,   “ front ”   •bigger : “ bigger ” , “ biggest ” , “ larger ” ,   “ largest ” , “ closer ” , “ closest ”   •smaller : “ smaller ” , “ smallest ” , “ tinier ” , “ tini-   est ” , “ further ” , “ furthest ”   D Description of ALBEF   The ALBEF model has an image - only transformer   and a text - only transformer like CLIP but also has   a multi - modal transformer that operates on the out-   puts of these two transformers . ALBEF is pre-   trained with three losses : ( 1 ) an image - text con-   trastive ( ITC ) loss that works just like CLIP ’s and   uses the outputs of the image - only and text - only   transformers , ( 2 ) an image - text matching ( ITM )   loss – where the task is to decide whether a given   image - text pair match – which uses the outputs of   the multi - modal encoder , and ( 3 ) a masked lan-   guage modeling loss which uses the outputs of the   multi - modal encoder . We explore both the ITC and   ITM scores in our experiments . ALBEF was pre-   trained on roughly 15 M image - caption pairs from   conceptual captions ( Sharma et al . , 2018 ) , SBU   Captions ( Ordonez et al . , 2011 ) , COCO ( Lin et al . ,   2014 ) , and Visual Genome ( Krishna et al . , 2016 ) .   D.1 ALBEF Performance on Synthetic Spatial   Reasoning Experiment   Table 6 shows the zero - shot accuracy of ALBEF   ITM and ITC in the synthetic spatial reasoning   experiment described in § 3.2 .   E Implementation Details   E.1 Text prompt   For ALBEF , we pass the input expression directly   to the model , whereas for CLIP , when using Grad-   CAM and ReCLIP ( with or without relations ) , we   use the prefix “ a photo of ” following the authors ’   observations ( Radford et al . , 2021 ) . For CPT , the   prompt is given in § 2.3 .   E.2 Position embeddings   Both CLIP and ALBEF use fixed - size position em-   beddings , so either the input image must be resized   to fit the dimensions of the embeddings or the size   of the embeddings must be changed . For all mod-   els , we resize the image to match the model ’s vi-   sual input resolution . Resizing of images is done   via bicubic interpolation . Figure 9 shows the how   the performance of the GradCAM method varies   between resizing images and resizing embeddings –   for CLIP RN50x16 , there is very little difference ,   while for CLIP ViT - B/32 image resizing makes a   larger difference.5211   Hyperparameters Specifically , we evaluate each   value in the set { 0.2,0.4,0.6,0.8,1.0}and choose   the best . The chosen values are α= 0.8for CLIP   RN50x16 and ALBEF ITC and α= 1.0for CLIP   ViT - B/32 .   E.3 GradCAM Layer   For CLIP ViT - B/32 , we use the last layer of   the visual transformer for GradCAM . For CLIP   RN50x16 , we use output of layer 4 for GradCAM .   For ALBEF ITM , we use the third layer of the   multi - modal transformer for GradCAM ( following   Li et al . ( 2021 ) ) . For ALBEF ITC , we use the final   layer of the visual transformer for GradCAM .   E.4 Hyperparameter sensitivity   Figure 9 shows the sensitivity of the GradCAM   method to αfor the two CLIP models . We choose   α= 0.5for all models ( including ALBEF ) , which   results in the best accuracy for almost models .   For ViT - B/32 , α= 0.6yields slightly higher ac-   curacy by ( 0.1 % ) on the RefCOCOg validation   set . Figure 10 shows the sensitivity of the IPS   method to the blur standard deviation σfor the   CLIP RN50x16 model . As shown , the method has   little sensitivity to σabove σ= 20 .   E.5 Experimentation on validation set   As discussed by Perez et al . ( 2021 ) , research on the   zero - shot setting often uses labeled data for model   selection . Aside from variants of IPS documented   in our ablation study ( § 4.6 ) , we also experimented   on the RefCOCOg validation set ( and to a lesser   extent on the RefCOCO+ validation set ) with :   1.Drawing a rectangle around the proposal and   using an appropriate text prompt . Perfor-   mance was somewhat similar to CPT perfor-   mance .   2.Ensembling the original text prompt with a   text prompt having only the noun chunk of   the expression containing the head word . This   helped for IPS and is in a sense part of our   rule - based relation - handling .   3.Other techniques for handling superlatives .   For instance , we tried to compute Pr[P(i)∧V(¬P(j)∨(P(j)∧R(i , j ) ) ) ] . This   performed worse than our chosen technique   on the RefCOCOg validation set .   4.Invoking the parser and relation - handling   pipeline on all sentences rather than only those   containing one of the relation / superlative key-   words .   We also selected the relation types and keywords   based on these validation sets . Most of these pre-   liminary experiments were performed using the   area threshold mentioned in § 4.3 .   E.6 Description of Computing Infrastructure   We primarily used a machine with Quadro RTX   8000 GPUs , Google Cloud machines with V100   GPUs , and a machine with TITAN RTX and   GeForce 2080s . These machines used Ubuntu as   the operating system.5212E.7 Dataset Information   All datasets that we use are focused on English .   The COCO dataset can be downloaded from   https://cocodataset.org/#download .   The RefCOCO / g/+ datasets can be down-   loaded from https://github.com/   lichengunc / refer / tree / master / data .   The RefGTA dataset can be downloaded   from https://github.com/mikittt/   easy - to - understand - REG / tree/   master / pyutils / refer2 . The RefCOCOg   validation set has 4896 instances , the RefCOCOg   test set has 9602 instances , the RefCOCO+   validation set has 10758 instances , the RefCOCO+   TestA set has 5726 instances , the RefCOCO+   TestB set has 4889 instances , the RefCOCO   validation set has 10834 instances , the RefCOCO   TestA set has 5657 instances , the RefCOCO TestB   set has 5095 instances , the RefGTA validation set   has 17766 instances , and the RefGTA test set has   17646 instances .   F Qualitative Examples   Figure 12 shows qualitative examples for the Re-   fGTA validation set . Figure 11 shows examples of   the execution errors mentioned in the error analysis   in Section 4.6 .   G Additional Experiment Results   This section presents the full results on the   RefCOCOg / RefCOCO+/RefCOCO datasets , in-   cluding results without ensembling using CLIP   RN50x16 and ViT - B/32 models and results using   ground - truth proposals . Table 7 shows full results   on the RefCOCOg and RefCOCO+ datasets . Ta-   ble 8 shows full results on the RefCOCO dataset.521352145215