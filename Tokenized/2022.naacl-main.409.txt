  Luis F. Guzman - Nateras , Minh Van Nguyen and Thien Huu Nguyen   Department of Computer and Information Science   University of Oregon   { lfguzman,minhnv,thien}@cs.uoregon.edu   Abstract   In this work , we focus on Cross - Lingual Event   Detection where a model is trained on data   from a source language but its performance   is evaluated on data from a second , target , lan-   guage . Most recent works in this area have   harnessed the language - invariant qualities dis-   played by pre - trained Multi - lingual Language   Models . Their performance , however , reveals   there is room for improvement as the cross-   lingual setting entails particular challenges . We   employ Adversarial Language Adaptation to   train a Language Discriminator to discern be-   tween the source and target languages using   unlabeled data . The discriminator is trained   in an adversarial manner so that the encoder   learns to produce refined , language - invariant   representations that lead to improved perfor-   mance . More importantly , we optimize the ad-   versarial training process by only presenting the   discriminator with the most informative sam-   ples . We base our intuition about what makes   a sample informative on two disparate metrics :   sample similarity and event presence . Thus ,   we propose leveraging Optimal Transport as a   solution to naturally combine these two distinct   information sources into the selection process .   Extensive experiments on 8 different language   pairs , using 4 languages from unrelated fami-   lies , show the flexibility and effectiveness of   our model that achieves state - of - the - art results .   1 Introduction   Event Detection ( ED ) is an important sub - task   within the broader Information Extraction ( IE ) task .   Event detection consists of being able to identify   the words , commonly referred to as triggers , that   denote the occurrence of events in a sentence , and   classify them into a discrete set of event types .   For example , in the sentence “ Jamie bought a   car yesterday . ” , bought is considered the trigger of   a TRANSACTION : TRANSFER - OWNERSHIPevent type . It is a very well studied task in which   there have been lots of previous research efforts   that have recently been primarily deep learning-   based ( Nguyen and Grishman , 2015 ; Chen et al . ,   2015 ; Nguyen et al . , 2016a , b ; Sha et al . , 2018 ; Wad-   den et al . , 2019 ; Zhang et al . , 2019a ; Yang et al . ,   2019 ; Nguyen and Nguyen , 2019 ; Zhang et al . ,   2020 ; Liu et al . , 2020 ) .   Nonetheless , ED remains quite a challenging   task as the context in which a trigger occurs can   change its corresponding type completely . Further-   more , the same event might also be expressed by   entirely different words / phrases . Additionally , the   vast majority of the aforementioned efforts are lim-   ited to a monolingual setting — performing ED on   text belonging to a single language .   Alternatively , Cross - Lingual ED ( CLED ) pro-   poses the scenario of creating models that effec-   tively perform ED on data belonging to more than   one language , which brings about additional chal-   lenges . For instance , trigger words present in one   language might not exist in another one . An fre-   quent example of this phenomenon are verb con-   jugations where some tenses only exist in some   languages . Accurate verb handling is of particular   importance for the ED task as event triggers are   usually related to the verbs in a sentence . Some re-   cent work ( Majewska et al . , 2021 ) has attempted to   address this issue by injecting external verb knowl-   edge into the training process . Another similar   problematic issue for CLED are triggers with dif-   ferent meanings that are each distinct words in   different languages . For instance , the word “ juicio ”   in Spanish can either mean “ judgement ” or “ trial ”   in English , depending on the context .   A compelling approach to creating a cross-   lingual model is to use transfer learning which   carries the performance of a model trained on a   source language over onto a second target lan-   guage . The general idea is leveraging the existing   high - quality annotated data available for a high-5588resource language to train a model in a way that   allows it to learn the language - invariant charac-   teristics of the task at hand , ED in this case , so   that it also performs effectively on text from a sec-   ond language . Prior works on transfer learning for   CLED have relied on pre - trained Multilingual Lan-   guage Models ( MLMs ) , such as multilingual BERT   ( mBERT ) ( Devlin et al . , 2019 ) , to take advantage of   their innate language - invariant qualities . Yet , their   performance still shows room for improvement as   they sometimes struggle to handle the difficult in-   stances , unique to cross - lingual settings , mentioned   earlier . We identify a significant shortcoming of   previous CLED efforts in that they do not exploit   the abundant supply of unlabeled data : even though   MLMs are trained on immense amounts of it , unla-   beled data is not used when fine - tuning for the ED   task . It is our intuition that by integrating unlabeled   target - language data into the training process , the   model is exposed to more language context which   should help deal with issues such as verb variation   and multiple connotations .   As such , we propose making use of Adversar-   ial Language Adaptation ( ALA ) ( Joty et al . , 2017 ;   Chen et al . , 2018 ) to train a CLED model . The   key idea is to generate language - invariant repre-   sentations that are not indicative of language but   remain informative for the ED task . Unlabeled data   from both the source and target languages is used   to train a Language Discriminator ( LD ) network   that learns to discern between the two . The adver-   sarial part comes from the fact that the encoder   and discriminator are trained with opposing objec-   tives : as the LD becomes better at distinguishing   between languages , the encoder learns to generate   more language - invariant representations in an at-   tempt to foolthe LD . To the best of our knowledge ,   our work is the first one proposing the use of ALA   for the CLED task .   Nonetheless , contrary to past uses of ALA where   the same importance is given to all unlabeled sam-   ples , we recognize that such course of action is sub-   optimal as certain samples are bound to be more   informative for the discriminator than others . For   example , we would like to present the LD with the   samples that allow it to learn the fine - grained dis-   tinctions between the source and target languages ,   instead of relying on syntactic differences . More-   over , in the context of ED , we suggest it would be   beneficial for the LD to be trained with examples   containing events , instead of non - event samples , asthe presence of an event can then be incorporated   into the generated representations .   Hence , we propose refining the adversarial train-   ing process by only keeping the most informative   examples while disregarding less useful ones . Our   intuition as to what makes samples more informa-   tive for CLED is two - fold : First , we presume that   presenting the LD with examples that are too dif-   ferent makes the discrimination task too simple .   As mentioned previously , we would like the LD to   learn a fine - grained distinction between the source   and target languages which , in turn , improves the   language - invariance of the encoder ’s representa-   tions . Thus , we suggest presenting the LD with ex-   amples that have similar contextual semantics , i.e. ,   similar contextualized representations . Second , we   consider that sentences containing events should   provide an ED system with additional task - relevant   information when compared against non - event sam-   ples . Accordingly , we argue that event - containing   sentences should have a larger probability of being   selected for ALA training .   With these intuitions in mind , we propose Op-   timal Transport ( OT ) ( Villani , 2008 ) as a natural   solution to simultaneously incorporate both the sim-   ilarity between sample representations and the like-   lihood of the samples containing an event into a   single framework . Therefore , we cast sample selec-   tion as an OT problem in which we attempt to find   the best alignment between the samples from the   source and target languages .   For our experiments , we focus on the widely   used ACE05 ( Walker et al . , 2006 ) and ERE ( Song   et al . , 2015 ) datasets which , in conjuction , con-   tain event - annotations in 4 different languages : En-   glish , Spanish , Chinese , and Arabic . We work on   8 different language pairs by selecting different   languages as the source and target . Our proposed   model obtains new state - of - the - art results with con-   siderable performance improvements ( + 2 - 3 % in F1   scores ) over competitive baselines and previously   published results ( M’hamdi et al . , 2019 ) . We be-   lieve these results demonstrate our model ’s efficacy   and applicability at creating CLED systems .   The rest of this paper is organized as follows :   section 2 provides an thorough description of our   proposed model , section 3 presents and analyses   the results from our experiments , section 4 pro-   vides a brief review of related work , and section 5   includes our conclusions.55892 Model   2.1 Problem Definition   Following prior works ( M’hamdi et al . , 2019 ;   Majewska et al . , 2021 ) , we treat ED as a se-   quence labeling problem . Given a set Dof   word sequences w={w , w , ... , w , w }   and their corresponding label sequences y=   { y , y , ... , y , y } , we use an encoder net-   work Eto obtain a contextualized vector rep-   resentation of the words in the input sequence   h = E(w ) = { h , h , ... , h , h } . Using   such representations as input , a prediction network   Pcomputes a distribution over the set of possible   labels and is trained in a supervised manner using   the negative log - likelihood function L :   L=−/summationdisplay / summationdisplaylogP(y|h ) ( 1 )   In the cross - lingual transfer - learning setting , the   data used to train the model and the data on which   the model is tested come from different languages   known as the source andtarget , respectively . As   such , we deal with two datasets DandD. We   assume that we do not have access to the gold labels   of the target language y , other than to evaluate   our CLED model at testing time .   Our goal is to define a model able to generate   language - invariant word representations that are   refined enough so that cross - lingual issues , such   as the ones described in section 1 , are properly   handled .   2.2 Baseline Model   Here , we briefly describe the BERT - CRF model   proposed by M’hamdi et al . ( 2019 ) which was the   previous state - of - the - art and serves as our main   baseline . Using multilingual BERT ( mBERT , ( De-   vlin et al . , 2019 ) ) as its encoder , BERT - CRF gen-   erates robust , contextualized representations for   words from different languages . For words that are   split into multiple word - pieces , the average of the   representation vectors for all comprising sub - pieces   is used as the representation of the full word .   For classification purposes , instead of assigning   the labels of each token independently , BERT - CRF   uses a Conditional Random Field ( CRF ) ( Lafferty   et al . , 2001 ) layer on top of the prediction network   to better capture the interactions between the labelsequences . In summary , the contextualized rep-   resentation vectors hgenerated by the mBERT   encoder from the words in the sequence are then   fed to a CRF layer which finds the optimal label   sequence .   2.3 Adversarial Language Adaptation   The pre - trained versions of MLMs like mBERT or   XLM - RoBERTa ( Conneau et al . , 2019 ) generate   contextualized representations with a certain de-   gree of language - invariance . This can be confirmed   by their successful application in cross - lingual set-   tings ( M’hamdi et al . , 2019 ; Majewska et al . , 2021 ) .   However , a lingering issue is the difficulty of learn-   ing the nuances of the target language such as verb   variations that do not exist in the source language   used to train them . Majewska et al . ( 2021 ) , for   instance , propose to address this issue by injecting   external verb knowledge into the encoder via task-   specific adapter modules ( Pfeiffer et al . , 2020 ) .   It is our intuition , however , that these issues can   be mitigated by achieving a more refined level of   language - invariance in the word representations .   As such , we propose using Adversarial Language   Adaptation ( ALA ) ( Joty et al . , 2017 ) , a technique   used to create language - invariant models . The   ALA framework consists in including a Language   Discriminator ( LD ) whose purpose is to learn   language - dependent features and be able to differ-   entiate between the samples from either the source   or the target languages .   A fundamental characteristic of the ALA ap-   proach is its lack of requirements for annotated   data in the target language . As such , we can   use data from both DandD. An auxiliary   dataset D={(w , l ) , . . . , ( w , l)}is cre-   ated where wis a text sequence from either D   orD , and lis a language label . The cardinal-   ity of Dis|D|= 2 m , where mis equal to   the batch size . Text samples w. . . w∈ D ,   and samples w. . . w∈ D. As described   earlier , the encoder E receives the text sequences   and produces a sequence of contextualized repre-   sentations E(w ) = h={h , h , h , . . . , h }   where his the representation of the [ CLS ] token   added at the beginning of every input sequence .   In our work , the LD is a a simple Multi - Layer   Perceptron(MLP ) network that takes has input   and produces a single sigmoid output . It ’s trained   with the usual binary cross - entropy loss function   objective : LD = argminL(LD(h ) , l).5590As the LD learns to distinguish between the   source and target languages , we concurrently train   the encoder to “ fool ” the discriminator . In other   words , the encoder must learn to generate represen-   tations that are language - invariant enough that the   LD is unable to classify them while still remain-   ing predictive for event - trigger classification . We   optimize the following loss :   argmin / summationdisplay(L(C(h ) , y))−λL(LD(h , l ) )   ( 2 )   Where C refers to the CRF - based classifier network   andλis a hyperparameter .   Equation 2 is implemented by using a Gradient-   Reversal Layer ( GRL ) ( Ganin and Lempitsky ,   2015 ) which acts as the identity during the forward   pass , but reverses the direction of the gradients dur-   ing the backward pass . The first term in Equation 2   can , of course , only be applied for annotated data   from the source language .   The GRL is applied to the input vectors , h ,   of the LD . This way , the LD is being trained to   differentiate between the two languages while the   encoder is trained in the opposite direction , i.e. to   generate sequence representations that are harder   to discriminate .   2.4 Adversarial Training Optimization   ALA has already been shown to be effective at gen-   erating language - invariant models ( Joty et al . , 2017 ;   Chen et al . , 2018 ) . However , in regular ALA train-   ing , all samples in a batch , from both the source   and target domains , are treated equally . That is ,   all samples are used as examples for the discrim-   inator to learn how to better discern between the   two domains . We propose that ALA effectiveness   can be further improved by carefully selecting the   samples with which to train the discriminator . We   argue that some samples might be more informative   than others and that , by only using such informative   samples during training , better adaptation results   can be achieved .   We base our notion as to what makes a sam-   ple more informative on two factors . First , we   argue that presenting the LD with examples from   the source and target language that are too dissim-   ilar makes its task easier which , in turn , leads to   the LD not learning the fine - grained distinctions   between the languages . Instead , we propose us-   ing samples whose vector representations hareclose to each other in the embedding space . The   intuition for this being that , as representations cap-   ture the contextual semantics of the samples , closer   representations correspond to more similar exam-   ples . Second , we suggest that presenting the LD   with samples containing events should make the   encoder incorporate task - specific information into   its representations .   2.4.1 Optimal Transport   One challenge of using the two mentioned crite-   ria for the ALA sample selection process is that   they come with two different measures which are   hard to combine . To address this , we propose using   Optimal Transport ( OT ) ( Villani , 2008 ) as a natu-   ral way to combine these two metrics into a single   framework for sample selection . Optimal trans-   port is , in broad terms , the problem of finding out   the cheapest transformation between two discrete   probability distributions . It requires a cost function   to determine the cost of transforming a data point   in one distribution into a data point in the second   distribution . When the cost function is based on a   valid distance function , the minimum cost is known   as the Wasserstein distance . Formally , it solves the   following optimization problem :   π(s , t ) = min / summationdisplay / summationdisplayπ(s , t)C(s , t)ds dt   ( 3 )   s.t.s∼p(s)andt∼q(t )   where SandTare the two domains to be trans-   formed ; p(s)andq(t)are the probability distribu-   tions of SandT , respectively ; Cis a cost function   for mapping StoT , C(s , t ) : S × T −→ R ;   and finally , π(s , t)is the optimal joint distribution   over the set of all joint distributions / producttext(s , t ) . The   problem described by Equation 3 is , of course , in-   tractable . Therefore , we use instead the Sinkhorn   algorithm ( Cuturi , 2013 ) which is an entropy - based   relaxation of the discrete OT problem .   2.4.2 Problem Formulation   We formulate the OT problem as follows : the do-   mains SandTare defined as the representation   vectors of the text samples in either the source h   or the target hlanguages . We use the L2 distance   between these representations as the cost function :   C(h , h ) = ||h−h|| ( 4)5591To define the marginal probability distributions   p(s)andq(t)for the SandTdomains , we pro-   pose including an Event - Presence ( EP ) prediction   module and use its normalized likelihood scores as   the probability distributions for SandT. Thus , the   auxiliary dataset Dis augmented to include an   event - presence label efor each sample . Of course ,   this can only be done for samples in the source   language as the labels for the target - language data   are unavailable :   D={(w , l , e ) , . . . , ( w , l , e ) ,   ( w , l ) , . . . , ( w , l ) }   The EP module is then trained to optimize the   following loss :   EP = argminL(EP(h ) , e ) ( 5 )   where i < = m , i.e. , only using samples from the   source language .   The probability distributions p(s)andp(t)are   the computed as follows :   p(s ) = Softmax ( EP(h)|l==s ) ( 6 )   p(t ) = Softmax ( EP(h)|l==t ) ( 7 )   2.4.3 Sample Selection   We use the OT solution matrix π , where an entry   π(s , t)represents the optimal cost of transforming   data point s∈ S intot∈ T , to compute an the   overall similarity score vof a sample h∈ S to   the samples in the target domain Tby using the   average distance :   v=/summationtextπ(h , h )   m(8 )   Correspondingly , we compute an overall similarity   score vof each sample h∈ T to the samples in   the source domain S :   v=/summationtextπ(h , h )   m(9 )   Lastly , we select a fraction , hyperparameter γ , of   samples with the best similarity scores from both   the source and target languages , and only use these   selected samples during ALA training .   2.5 OACLED Model   We train our Optimized Adversarial Cross - Lingual   Event Detection ( OACLED ) model end - to - end   with the following loss objective :   L = CRF+αLD+βEP ( 10 )   where αandβare trade - off hyperparameters.3 Experiments   3.1 Datasets   We evaluate our model on the ACE05 ( Walker   et al . , 2006 ) dataset which includes annotated event-   trigger data in 3 languages : English , Chinese and   Arabic . To include an additional language in our   experiments , we also evaluate on the ERE dataset   which has annotated data in English and Spanish .   Note that the ACE05 and ERE datasets do not share   the same label set : ACE05 involves 33 distinct   event types while ERE involves 38 event types . We   follow the same data pre - processing and splits as in   previous work ( M’hamdi et al . , 2019 ) to ensure a   fair comparison . Table 1 presents the data statistics .   3.2 Hyper - parameters   We fine - tune the hyper - parameters for our OA-   CLED model using the development data . We ap-   ply the following values based on the fine - tuning   process :   • AdamW as the optimizer .   • 5 warm up epochs .   •A learning rate of 1efor the transformer   parameters and of 1efor the rest of the   parameters .   • A batch size of 16 .   •300 for the dimensionality of the layers in   feed - forwards networks .   •Aγ= 0.5for the percentage of samples used   in adversarial training.5592•Aλ= 0.001as the scaling factor of the GRL   layer .   •Anα= 1 andβ= 0.001as the trade - off   parameters of the LD loss and ED loss , re-   spectively .   •A dropout of 10 % for added regularization   during training .   3.3 Main Results   In our experiments , we work with 8 distinct   language pairs by selecting each of the avail-   able languages as either the source or target lan-   guage : English - Chinese , Chinese - English , English-   Arabic , Arabic - English , Chinese - Arabic , Arabic-   Chinese , English - Spanish , and Spanish - English .   The Chinese - Spanish , Spanish - Chinese , Arabic-   Spanish , and Spanish - Arabic language combina-   tions are unavailable due the previously mentioned   incompatibility between the event type sets in   ACE05 and ERE .   We compare our OACLED model against 3 rele-   vant baselines . First , the previous state - of - the - art   CLED model BERT - CRF ( M’hamdi et al . , 2019 )   as described in section 2.2 . Second , the mBERT-   2TA model ( Majewska et al . , 2021 ) which aims at   improving cross - lingual performance by incorpo-   rating language - independent verb knowledge via   task - specific adapters . And third , XLM - R - CRF   which is equivalent in all regards to BERT - CRF   except that it uses XLM - RoBERTa ( Conneau et al . ,   2019 ) as the encoder .   Table 2 and Table 3 show the results of our ex-   periments on the ACE05 and ERE datasets , respec-   tively . In all our experiments , we use the base trans-   former versions bert - base - cased andxlm - roberta-   base as the encoders , parameters are tuned on the   development data of the source language , and all   entries are the average of five runs .   From Tables 2 and 3 , it should be noted that   there is a substantial performance increase by per-   forming the trivial change of replacing mBERT   with XLM - RoBERTa as the encoder . Furthermore ,   our OACLED model clearly and consistently out-   performs the baselines for all language pairings ,   with the exception of the Chinese - Arabic pair . We   attribute this to the impaired performance of XLM-   RoBERTa as the encoder for that specific pair as   can be confirmed by the poor performance of the   XLM - R - CRF baseline on the same configuration .   Most importantly , OACLED ’s improvement over   the XLM - R - CRF baseline is present in every con-   figuration , which validates the effectiveness of our   optimized approach to ALA training .   3.4 Ablation Study   We identify 2 main components in our approach :   using ALA to create refined language - invariant rep-   resentations , and optimizing the adversarial train-   ing process by selecting a subset of samples cho-   sen with OT to incorporate our measures of infor-   mativeness into the sample selection process . Of   course , removing ALA training entirely restores   the model to the baseline . However , adversarial   training optimization via OT has various aspects   to it . In order to understand the contribution of   these aspects , we explore four different models :   OACLED - OT presents the effects of removing sam-   ple selection entirely and using all available sam-   ples to train the LD ; OACLED - L2 uses a constant   distance between the unlabeled samples instead the   standard L2 distance used in the Sinkhorn algo-   rithm ; OACLED - EP completely removes the EP   module and a uniform distribution is used as the   probability distributions for both languages ; finally ,   OACLED - ED - Loss keeps the EP module , but re-   moves its EPterm from Equation 10 . The per-   formance results of these models is presented in   Table 4 . In this and the following sections ( 3.5,55933.6.2 ) , we present the results of experiments us-   ing English as the sole source language as it is the   source language most ubiquitously used . We , how-   ever , found consistency in the displayed effects for   different source / target language configurations .   As expected , removing the sample selection   through OT leads to the worst performance drop .   This highlights the importance of selecting informa-   tive examples for the LD . Furthermore , removing   the cost function also hurts performance greatly ,   which shows that a proper distance function is   needed for the OT algorithm to work effectively .   While the effects of removing the EP module and   its corresponding loss term are not of the same   magnitude , they are still significant . These results   support our claim for the need and utility of all the   components in our approach , showing that their   inclusion is crucial in achieving state - of - the - art per-   formance .   3.5 Language Model Finetuning   The key contribution of our approach is to exploit   unlabeled data in the target language , which is usu-   ally abundant , by introducing it into the training   process to improve our model ’s language - invariant   qualities .   To confirm the utility of our approach , Table 5   contrasts our model ’s performance against a base-   line whose encoder has been finetuned with the   same unlabeled data using the standard masked   language model objective .   It can be observed that our model outperforms   the finetuned baseline in two out of the three target   languages . Additionally , the difference in perfor-   mance in those two instances is considerably larger(3.58 % and1.15 % ) , than the setting in which the   baseline performs better ( 0.13 % ) .   3.6 Analysis   3.6.1 Learned Representation Distances   First , we look at the distance between the sentence-   level representations hgenerated by the encoder   for different source / target language pairs . Figure 1   shows a plot of such distances using cosine distance   as the distance function .   When computing the correlation with the per-   formance results in Table 2 , we obtain a score   R=−0.6616 , meaning there is moderate nega-   tive correlation between the distance of the rep-   resentations and model performance , i.e. closer   representations lead to better performance .   Similarly , Table 6 shows a comparison of the   distances between the representations generated by   OACLED and those obtained by the XLM - R - CRF   baseline .   Cosine Distance   Source / Target Baseline OACLED   English / Chinese 3.64e-3 3.93e-6   English / Arabic 7.71e-2 2.08e-5   English / Spanish 5.4e-3 5.3e-6   Chinese / English 3.62e-3 3.87e-6   Arabic / English 4.16e-2 1.02e-5   Spanish / English 6.87e-3 1.49e-5   We observe that OACLED representations are   closer , by several orders of magnitude , than those   obtained by the baseline . This supports our claim   that our model ’s encoder generates more refined5594language - invariant representations than those ob-   tained by the default version of XLM - RoBERTa .   3.6.2 Access to Labeled Target Data   Previously , we discussed how a key feature of our   approach is that it does not require annotated data   in the target language and , instead , leverages the   use of unlabeled data which is readily available .   Nonetheless , we also explore the performance of   our model in the event that there exists a small   amount of annotated target data available . Figure   2 shows the results of our experiments when us-   ing different amounts of labeled target data during   training .   It can be observed that OACLED consistently   outperforms the baseline even when there is some   availability of annotated data . Additionally , perfor-   mance steadily increases as more and more data   is used . This conforms to expectations , and con-   firms that having labeled data in the target language   available for training is ultimately beneficial to the   model ’s performance .   3.6.3 Case Study   Next , we look into our model ’s predictions and   analyse instances where it outperforms the base-   line to exemplify the advantages of dealing withoptimized language - invariant representations . We   identify two important patterns .   First , our model seems to better classify events   in the target language that involve trigger words   that have distinct connotations that depend on con-   text . Specially those that are two distinct words   in the source language . For example , the Span-   ish word “ juicio ” can have two distinct meanings   that are different words in English : “ trial ” and   “ judgement ” . Our model correctly classifies it as   a JUSTICE : TRIAL - HEARING trigger in the sen-   tence “ Dos llamados a juicio fueron hechos por un   jurado federal investigador ” . Meanwhile , the base-   line fails to even recognize it as a trigger . Another   example is the word “ detenido ” , an adjective that   can mean both “ detained ” , in a criminal context ,   and“stopped ” , as in halted . Our model correctly   classifies it in the sentence “ Padilla no debería per-   manecer detenido durante meses alejado de otros   reos ” as a JUSTICE : ARREST - JAIL trigger while   the baseline fails to detect the event . We manually   identified 23 of these polysemous triggers in the   Spanishtest set and found that 19 ( 82.6 % ) were   correctly classified by our OACLED model versus   14 ( 60.8 % ) by the baseline ( 27.8%improvement ) .   Additionally , we found our model correctly clas-   sifies verb conjugation variants that do not exist in   the source language . For instance , our model cor-   rectly recognizes the words “ venderlos ” , “ vender ” ,   “ vendes ” , and “ vendedor ” ( variants of the   verb “ to buy ” ) as TRANSACTION : TRANSFER-   OWNERSHIP triggers whereas the baseline   incorrectly classifies them as being of the   TRANSACTION : TRANSFER - MONEY type . As   previously mentioned , Majewska et al . ( 2021 )   propose injecting external verb - knowledge into   the training to help with verb interpretation for   event extraction . Our empirical results , however ,   outperform their reports which appears to imply   that , at least for CLED , holistically learning the   language - invariant features shared between the tar-   get and source languages works better than inject-   ing language - specific verb knowledge .   We believe these findings illustrate how , by intro-   ducing additional context in the form of unlabeled   data , the model is able to learn fine - grained word   representations that better capture the semantics of   the words in the target language , and successfully   deal with difficult cross - lingual issues.55954 Related Work   Research efforts on monolingual ED are extensive   and varied . Hand - crafted , feature - based , language-   specific methods were the basis of early ED ap-   proaches ( Ahn , 2006 ; Ji and Grishman , 2008 ; Pat-   wardhan and Riloff , 2009 ; Liao and Grishman ,   2010a , b ; Hong et al . , 2011 ; McClosky et al . , 2011 ;   Li et al . , 2013 ; Miwa et al . , 2014 ; Yang and   Mitchell , 2016 ) . More recent efforts have primarily   made use of deep learning techniques such as con-   volutional neural networks ( Nguyen and Grishman ,   2015 ; Chen et al . , 2015 ; Nguyen et al . , 2016b ) ,   recurrent neural networks ( Nguyen et al . , 2016a ;   Sha et al . , 2018 ; Lai et al . , 2020 ) , graph convolu-   tional networks ( Nguyen and Grishman , 2018 ; Yan   et al . , 2019 ; Nguyen et al . , 2021a ) , adversarial net-   works ( Hong et al . , 2018 ; Zhang et al . , 2019b ) , and   pre - trained language models ( Wadden et al . , 2019 ;   Zhang et al . , 2019a ; Yang et al . , 2019 ; Zhang et al . ,   2020 ; Liu et al . , 2020 ; Pouran Ben Veyseh et al . ,   2021b , a ) .   Works on cross - lingual ED are not as prevalent   and generally make use of cross - lingual resources   employed to address the differences between lan-   guages such as bilingual dictionaries or parallel   corpora ( Muis et al . , 2018 ; Liu et al . , 2019 ) and ,   more recently , pre - trained multilingual language   models ( M’hamdi et al . , 2019 ; Hambardzumyan   et al . , 2020 ; Majewska et al . , 2021 ) . Unlike these   previous efforts , our method leverages unlabeled   data to further refine the language - invariant quali-   ties of the language models .   Adversarial Language Adaptation , inspired by   models in domain adaptation research ( Ganin and   Lempitsky , 2015 ; Naik and Rose , 2020 ; Ngo Trung   et al . , 2021 ) , has been successfuly applied at gener-   ating language - invariant models ( Joty et al . , 2017 ;   Chen et al . , 2018 ; Nguyen et al . , 2021b ) . Our   method improves upon these approaches optimiz-   ing the adversarial training process by selecting   the most informative examples from the unlabeled   data .   Additional examples of downstream applications   of cross - lingual learning are document classifica-   tion ( Holger and Xian , 2018 ) , named entity recog-   nition ( Xie et al . , 2018 ) and part - of - speech tag-   ging ( Cohen et al . , 2011 ) . For a thorough review   on cross - lingual learning , we refer the reader to   Pikuliak et al . ( 2021).5 Conclusion   We present OACLED , a new model for cross-   lingual event detection that learns fine - grained   language - invariant representations by optimiz-   ing the standard ALA training through optimal-   transport - based sample selection . Our model   achieves new state - of - the - art performance in our   experiments on 8 different language pairs which   demonstrate its robustness and effectiveness at gen-   erating refined language - invariant representations   that allow for better event detection results . Our   analysis of its intermediate outputs and predictions   confirm that OACLED ’s representations are indeed   closer to each other and that this proximity trans-   lates into better handling of difficult cross - lingual   instances . We also note that , while this work fo-   cuses on the event detection task , our proposed   optimization of the adversarial training process is   task independent and can be generalized to other   related IE tasks when leveraging ALA is deemed   beneficial .   Acknowledgement   This research has been supported by the Army Re-   search Office ( ARO ) grant W911NF-21 - 1 - 0112   and the NSF grant CNS-1747798 to the IU-   CRC Center for Big Learning . This research is   also based upon work supported by the Office   of the Director of National Intelligence ( ODNI ) ,   Intelligence Advanced Research Projects Activ-   ity ( IARPA ) , via IARPA Contract No . 2019-   19051600006 under the Better Extraction from Text   Towards Enhanced Retrieval ( BETTER ) Program .   The views and conclusions contained herein are   those of the authors and should not be interpreted   as necessarily representing the official policies , ei-   ther expressed or implied , of ARO , ODNI , IARPA ,   the Department of Defense , or the U.S. Govern-   ment . The U.S. Government is authorized to re-   produce and distribute reprints for governmental   purposes notwithstanding any copyright annotation   therein . This document does not contain technol-   ogy or technical data controlled under either the   U.S. International Traffic in Arms Regulations or   the U.S. Export Administration Regulations . We   thank the anonymous reviewers and Tracy King for   their helpful feedback.5596References559755985599