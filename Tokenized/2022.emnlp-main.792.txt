  Ciarán Byrne   ciaranby@gmail.comDanijela Horak   danijela.horak@gmail.com   Amandla Mabona   akvmabona@gmail.comKaro Moilanen   karohenrim@gmail.com   Abstract   Recent unsupervised topic modelling ap-   proaches that use clustering techniques on   word , token or document embeddings can ex-   tract coherent topics . A common limitation   of such approaches is that they reveal noth-   ing about inter - topic relationships which are   essential in many real - world application do-   mains . We present an unsupervised topic mod-   elling method which harnesses Topological   Data Analysis ( TDA ) to extract a topological   skeleton of the manifold upon which contextu-   alised word embeddings lie . We demonstrate   that our approach , which performs on par with   a recent baseline , is able to construct a network   of coherent topics together with meaningful   relationships between them .   1 Introduction   Unsupervised topic modelling is a standard tech-   nique for making sense of document collections .   While traditional approaches such as LDA ( Blei   et al . , 2003 ) rely on probabilistic models , the field   has recently moved towards clustering - based meth-   ods in which topic clusters are obtained via docu-   ment , word or token embeddings ( Thompson and   Mimno , 2020 ; Silburt et al . , 2021 ; Angelov , 2020 ;   Grootendorst , 2022 ) . Even though clustering can   yield interpretable topics , it typically discards infor-   mation about relationships between clusters , hence   making it harder to interpret clusters in global con-   texts .   In this work , we approach topic modelling as   a task to find regions on a manifold of contextu-   alised word embeddings which reflect a " topic " . To   this end , we apply Mapper - an algorithm from the   field of Topological Data Analysis ( TDA ) . Map-   per creates a graph whose topology reflects the   shape of the underlying data set and whose nodes   represent subsets of data points . In the case ofcontextualised word embeddings , we construct a   graph where each node is a cluster of tokens ( i.e.   a " topic " ) , and where connections between them   reflect the topology of the embedding manifold .   We use community detection techniques to demon-   strate that semantically related topics are connected   in the graph .   Our main contributions are the following :   1.We propose and evaluate a new method for   topic modelling which learns topics and rela-   tionships between them without any restric-   tions on graph structure . To the best of our   knowledge , our work is the first application of   TDA Mapper to the task of topic modelling .   2.To the best of our knowledge , we are the first   to use stability analysis for Mapper on a real-   world data set and problem . Unlike prior ap-   proaches which are computationally infeasi-   ble on large data sets , we propose a scalable   approach using separate stability scores for   both the graph topology and the clustering .   3.We define a new stability score via spectral   distance between Mapper graphs .   4.We use community detection techniques to   automatically identify regions of interest in   large Mapper graphs .   2 Related Work   The seminal work on unsupervised topic modelling   was Blei et al . 2003 who introduced Latent Dirich-   let Allocation ( LDA ) , a Bayesian generative model   of documents which assumes that the tokens in a   document are drawn from a mixture model whose   mixture components are interpreted as topics . Of   the many extensions to the classic LDA archetype   that have since been proposed , most relevant to   our present work are methods to model associa-   tions and relationships between topics , and the use11514of neural representations in general and contex-   tualised representations in particular . Correlated   topic models ( Blei and Lafferty , 2005 , 2007 ) are   LDA extensions that attempt to learn the structure   of topic associations within a document . The goal   of hierarchical topic models ( Griffiths et al . , 2003 ;   Wang and Blei , 2009 ; Blei et al . , 2010 ; Ghahra-   mani et al . , 2010 ; Zavitsanos et al . , 2011 ; Ahmed   et al . , 2013 ; Paisley et al . , 2014 ) is to learn a tree-   structured graph of topics by incorporating hierar-   chical non - parametric Bayesian priors into tradi-   tional topic models .   Several studies have combined topic modelling   with neural representations with a view to learn   better topics or representations . For example , amor-   tised variational inference with neural variational   posteriors ( Kingma and Welling , 2014 ) has been   investigated as a means to scale up inference on   probabilistic topic models and relax the conjugacy   assumptions which are required for tractable in-   ference in traditional topic models ( Srivastava and   Sutton , 2017 ) . Various variants of such models   have focused on neural extensions of correlated   ( Xun et al . , 2017 ; Liu et al . , 2019 ) and hierarchi-   cal ( Isonuma et al . , 2020 ) topic models although   they all use neural representations in the generative   model or variational posterior . Some studies have   also incorporated contextualised word embeddings   into topic models while still using neural proba-   bilistic generative models ( Bianchi et al . , 2021b , a ;   Hoyle et al . , 2020 ) .   The prior work most closely related to our pro-   posed method is the joint application of topic mod-   elling and contextualised word embeddings by   Thompson and Mimno , 2020 , Sia et al . , 2020 , and   Angelov , 2020 who induce topics via vector clus-   tering over word or document embeddings .   Our method differs from LDA and its extensions   in that we use TDA rather than probabilistic gen-   erative models to induce topics . Correlated topic   models and their neural extensions learn a flat topic   structure while adding scalar associations , whereas   our method induces a topic graph . In contrast to   hierarchical topics models and their neural exten-   sions which induce tree - structured topic graphs ,   our method induces an unrestricted graph . Unlike   our method , previous work on inducing topics from   contextualised word representations construct a flat   topic structure rather than a graph .   Also related to our work is TopoAct ( Rathore   et al . , 2021 ) which applies Mapper to the analysisof BERT word embeddings . Our work differs from   ibid . in that we focus specifically on topic model-   ling , and we follow a systematic hyperparameter   selection process through stability analysis .   3 Proposed Method   The manifold hypothesis states that real - world high-   dimensional data lie on a low - dimensional mani-   fold embedded in a high - dimensional space . Topic   modelling can be regarded as an endeavour to iden-   tify topologically meaningful regions of the word   representation manifold which contain homoge-   neous topics or words . Traditionally , it has been   approached as a clustering problem in that the rep-   resentation manifold is assumed to be a discon-   nected union of " topic " manifolds . However , such   an assumption is clearly limiting and not grounded   theoretically . One potential solution involves di-   mensionality reduction and direct manifold visu-   alisation . Unfortunately , most dimensionality re-   duction techniques capture only topology within   local neighbourhoods , and can not be relied upon   for inference regarding the global topology of the   manifold .   Our method of choice to address this problem   is TDA Mapper introduced in ( Singh et al . , 2007 )   ( also referred to as topological data visualisation or   topological clustering ) , a method that yields an ap-   proximation of a Reeb graph of a manifold ( Munch   and Wang , 2016 ) which captures the topology and   shape of the manifold . Reeb graphs are constructed   from a manifold in order to learn topological in-   variants and global structure . Even though they   lose some of the original topological structure of   the manifold , their low - dimensional invariants ( e.g.   connected components ) remain the same .   3.1 Overview of TDA Mapper   The TDA Mapper algorithm takes as input a set of   points and outputs a graph whose vertices are sub-   sets of points , and whose edges are defined between   vertices which have a non - empty intersection . The   following main steps are typically executed .   1.The data is projected to a lower dimension   using a " filter function " ( or " lens " ) f. This   can be any standard dimensionality reduction   function or even a domain - specific function   which captures some interesting property of   the data .   2.The projected space is covered with a set of11515overlapping subets ( U)where Iis an in-   dexing set .   3.Each set Uis " pulled back " into the original   high - dimensional space by taking its preim-   agef(U ) . The points in this " pull - back   set " are broken into clusters using a clustering   algorithm .   4.A graph is constructed by using each cluster as   a vertex and adding an edge between any two   clusters that have a non - empty intersection .   3.2 Hyperparameter Tuning for TDA Mapper   Model selection in TDA Mapper is non - trivial , the   main reason being the absence of ground truth la-   bels , analogous to what other unsupervised learn-   ing algorithms face . One model selection ap-   proach suitable for algorithms of this kind which   has recently gained traction in TDA is stability   analysis ( Belchí et al . , 2020 ; Lim and Yu , 2016 ;   V on Luxburg et al . , 2010 ) . Rather than configuring   clustering parameters up front and then optimising   an evaluation metric , stability analysis simply con-   strains clustering to return structures that are stable   under small perturbations of data . For example ,   letM(D)be a certain mathematical structure on   a data set Dwith parameters θwhere Mcould   be clustering , dimensionality reduction , TDA Map-   per , or some other unsupervised learning algorithm .   If there exists a distance measure to quantify the   similarity of the structures d(M , M ) , then we   can define the instability of Mfor the parameter   choice θas the expected distance between M(D )   andM(D ) , where DandDare two data sam-   ples obtained by the same data generation process .   More precisely ,   S(M , d ) =   2   n(n−1)/summationdisplay / summationdisplayd(M(D),M(D ) )   where Sdenotes the instability score , and Dare   independent samples from the data set D. Finally ,   the optimal set of parameters θfor structure Mis   chosen from the ones that have a low instability   scoreS. Note that the instability score should only   be used to rule out parameter choices that yield   high instability scores ; it alone can not be used for   parameter selection as some structures are stable   but not necessarily correct . It is crucial to choose a   distance function which best embodies the notionof similarity between mathematical structures M   in order to obtain meaningful results from stabil-   ity analysis . One such distance function for TDA   Mapper graphs was defined and studied in ( Belchí   et al . , 2020 ) . Unfortunately , their numerical match-   ing distance algorithm is prohibitively slow in our   use case . We accordingly define two alternative   distance metrics to capture two salient properties   of Mapper graphs . One is designed to capture sim-   ilarity amongst graph structures while the other   accounts for vertex ( or cluster ) similarity .   These concepts are defined formally as follows .   Definition 1 LetM(D)be a TDA Mapper graph   with a vertex set V={C , . . . , C}where C⊂   D ; and an edge set E={(C , C)|ifC∩C̸=   ∅}where θ= ( θ , θ , θ)are three groups of pa-   rameters pertaining to a filter function , cover , and   clustering algorithm , respectively .   The stability of Mapper graphs is then assessed   with respect to different choices of parameters θ ,   and the final parameter values are chosen from the   most stable regions of the landscape .   We further define two distance metrics on Map-   per graphs for stability analysis .   Definition 2 LetMandMbe two TDA Mapper   graphs with vertices V={C , . . . , C};V=   { C , . . . , C } ; and edges EandE , respectively .   Ifm̸=n , then empty set padding is added to the   smaller vertex set so that m = n. The distance   d / parenleftbig   M , M / parenrightbig   = min1   n / summationdisplay / vextendsingle / vextendsingleC △ C / vextendsingle / vextendsingle   where πruns over all permutations of the set   { 1,2 , . . . , n } , is called the matching distance and   quantifies the similarity of vertices between Map-   per graphs .   Definition 3 LetΛ = { λ , λ , . . . , λ},Λ=   { λ , λ , . . . , λ}be eigenvalues of the normalised   Laplacian defined on Mapper graphs M=   G(V , E)andM = G(V , E ) , respectively . The   spectral distance is defined within the distribu-   tion of the eigenvalues µ=/summationtextpδand   ν=/summationtextpδas their 1 - Wasserstein distance ,   i.e.   d / parenleftbig   M , M / parenrightbig   = /integraldisplayF(t)−F(t)dt   where FandFare CDFs for µandν.11516The spectral distance quantifies the similarity of   graph topologies amongst graphs ( Gu et al . , 2015 ) .   Lastly , let Θbe the search space for parameters   θ : Then the stable region of Θwith permissible   parameter choices is   Θ={θ∈Θ| S(M , d ) < ε   andS(M , d ) < ε }   where εandεare thresholds for distances that   are considered " large " and hence unstable .   4 Experiments   4.1 Data   We evaluated the proposed model on two text data   sets : 20 NewsgroupsandAG News . Descriptions   of these data sets are found in the Appendix . We   extract contextualised subword embeddings using   bert - base - uncased(Devlin et al . , 2019 ) ,   and use the last layer embeddings . When a docu-   ment exceeds 512 tokens ( cf . the max length for   BERT ) , we simply run the model on each block of   512 tokens . To obtain word embeddings , we take   the mean of the subword components . The doc-   uments are tokenised using spaCy , and BERT   subword tokens are aligned to spaCy tokens with   spacy - alignments .   Although pretrained language models can rep-   resent them , we decided to remove rare words on   the grounds of lighter compute requirements . Fol-   lowing Thompson and Mimno , 2020 , we remove   stopwords , skip punctuation and digits , and further   remove any tokens which occur in fewer than 5   documents or more than 25 % of the documents .   This yields a vocabulary with 14829 words for 20   Newsgroups and 12530 words for AG News . Note   that we only remove these tokens after word embed-   dings have been obtained since they are important   for downstream representations .   4.2 Methodology   We apply the Mapper algorithm to the resultant   data set of contextualised word representations .   For our filter function , we use UMAP ( Uniform   Manifold Approximation and Projection ) ( McInneset al . , 2018 ) . We reduce the data down to two di-   mensions via the default parameters for UMAP ’s   Python reference implementation .   For clustering , we use HDBSCAN , a density-   based clustering algorithm which automatically de-   termines the number of clusters in a set of points   ( Campello et al . , 2013 ) . The main parameter for   HDBSCAN is min_cluster_size , the small-   est number of points that can constitute a cluster ,   which we set to 15 .   4.3 Parameter Selection   Aside from the clustering and filter function , Map-   per requires a " cover " . We use the " balanced " cover   offered by the giotto - tdalibrary - this sim-   ply partitions the space into hypercubes but adjusts   their sizes so that each cover set contains a similar   number of data points .   The cover requires two parameters : ( i ) the num-   ber of intervals or bins and ( ii ) the percentage over-   lap . We perform a stability analysis to rule out   unstable parameter combinations whose topologi-   cal features are more likely to be artefacts . For the   number of intervals , we experiment with values in   the range between 5 and 50 in steps of 5 . For the   percentage overlap , we try values between 0.1and   0.3 in increments of 0.05 . We subdivide the data   sets into 3 samples , each containing two thirds of   the embeddings in the entire data set . Each pair of   subsamples overlaps by 50 % . We run Mapper on   each sample subset to generate 3 graphs for each   pair of parameters .   We compute an instability score for each param-   eter set as the average distance between all three   graphs . We conduct the stability analysis twice   using two separate metrics , namely 1 ) Matching   Distance ( Definition 2 ) to measure clustering sta-   bility ; and 2 ) Spectral Graph Distance ( Definition   3 ) to measure stability in the graph structure . Our   stability plots are shown in Figures 1 , 2 , 3 , and 4 .   Looking at the regions that appear stable under   both metrics , we are still left with multiple choices   for stable parameters . We further eliminated sets   of parameters that had too many topics or nodes   ( typically due to a high bin size ) .   We also ruled out some graphs which were   highly connected and therefore had uninteresting   structure . Ultimately this led us to choose a bin size11517of 20 for both data sets , and overlaps of 0.1and0.3   for20 Newsgroups andAG News , respectively .   4.4 Community Detection for Subgraphs   The resulting graphs both had one very large con-   nected component as well as a large number of   small components with only one or two nodes .   These disconnected nodes contained about 30 %   of tokens in the 20 Newsgroups data set and about   60 % of the AG News tokens . Since these nodes are   disconnected from the primary component of the   topological manifold , we treat them essentially as   noise and discard them from the rest of our analy-   sis .   Since the graph is large , exploring all areas of   it manually is cumbersome . Therefore , we used   a community detection algorithm to identify clus-   ters of nodes that are densely connected . We form   additional higher - level topics from these clusters   by taking the union of all tokens in the nodes in   scope . We report metrics at both the node- and at   the community - level .   For community detection , we use the label propa-   gation algorithm described in Raghavan et al . , 2007   viaiGraphwhich is adapted to consider edge   weights ( Csardi et al . , 2006 ) .   4.5 Baseline   We compare our work with two recent baselines .   As a first baseline , we chose Top2Vec(Angelov ,   2020 ) , a recent method based on document rep-   resentations and clustering . Following ibid . , we   build a Top2Vec model using Doc2Vec document   embeddings which we train for 400 epochs with   a window size of 15 . Secondly , we compare our   methods to BERTopic ( Grootendorst , 2022 ) , us-   ing pretrained Sentence - BERT ( SBERT ) embed-   dings . For all other parameters we use the default   settings in the BERTopic Python reference imple-   mentation .   4.6 Evaluation Metrics   We use three automated metrics to evaluate our   model with respect to topic coherence , diversity ,   and specificity . It is important to note , however ,   that automated evaluation of topic coherence is an   activate area of research , and that standard evalu-   ation metrics have well - known limitations : in par-   ticular , automated measures can detect differences11518between topic models in cases where human judge-   ments do not ( Hoyle et al . , 2021 ) . The primary   goal of our work is not to reach greater coherence   per se but rather to arrange topics in a meaning-   ful graph structure for which comparisons with   baselines through automated measures suffice . In   addition to reporting three standard automated eval-   uation measures , we also inspect some of our topics   within some newsgroup categories .   Firstly , we estimate topic coherence by tak-   ing the average NPMI ( Normalized Pointwise   Mutual Information ) ( Aletras and Stevenson ,   2013 ) between all pairs of words in a given   topic . We estimate word probabilities us-   ingwikitext-103 - raw - vl(Merity et al . ,   2016 ) as our reference corpus , with a sliding win-   dow of 10 .   Secondly , we report Mean Word Entropy ( MWE )   ( Thompson and Mimno , 2020 ) per topic as a mea-   sure of topic specificity representing the condi-   tional entropy of a word type given its topic , namely   −/summationtextP(w|z ) logP(w|z ) . There is no clear   optimal value for specificity but overly specific top-   ics will have few word types and a low conditional   entropy ( with a minimum value of 0 ) ; conversely ,   overly broad topics will exhibit high entropy ( max-   imum log of the vocabulary size ) . Since Top2Vec   does not directly output a distribution over words ,   we use the empirical unigram distribution for all   documents assigned to a particular topic .   Thirdly , since it is possible for a topic model to   duplicate the same coherent topic many times , we   also need a measure of topic diversity . We report   the proportion of words that are unique to one topic ,   p , accordingly .   5 Results   Table 1 summarises our coherence , diversity , and   specificity results . We can see that we achieve   slightly improved coherence for 20 Newsgroups   data set although Top2Vec has slightly higher co-   herence scores on AG News . Including the commu-   nity detection step significantly reduces topic speci-   ficity , as expected . The strong coherence scores   after community detection indicate that topics are   still coherent even when merged with their neigh-   bours . This demonstrates that the edges in the   graph connect topics which are indeed related . For   a full list of topics in our graphs , see Supplementary   Material .   5.1 Target Label Analysis   Both data sets have human topic annotations which   we use these to visualise the regions of the graph   that are associated with particular topics . To do this ,   we colour the nodes in the graph by the percentage   of its tokens that come from a particular category of   documents . Figure 5 show these plots for two cate-   gories from each data set . We observe that there are   regions in the graph which correlate with particular   categories . The strength of the correlation varies   depending on the category . For 20 Newsgroups ,   the effect is very strong for rec , sci , comp , and talk   Newsgroups but weak or non - existent for the misc ,   alt , and soc ones . Likely this just reflects that these   are much less frequent labels . For AG News , the   effect appears to be weaker , meaning that our topic   clusters are not as strongly related to the human   labels . This is not necessarily a bad thing since the   goal of topic modeling is to find unsupervised topic   classes . Plots for all categories can be found in the   Supplementary Material .   5.2 Part - of - Speech Effects   We run spaCy on the entire data set to assign part-   of - speech tags to each token , revealing clear re-   gions of the graph corresponding to VERB , NOUN ,   and ADJ tags ( Figures 6 and 7 ) . We do not plot   other word classes since they are relatively in-   frequent in the data set ( cf . filtering and pre-   processing in Section 4 ) . We make no claim as   to whether the observed correlation with part - of-   speech tags is beneficial since the exact definition   of what constitutes a useful topic is highly task-11519   and domain - dependent . However , our word class   clusters could motivate the application of TDA to   the recent field of " BERTology " to interpret emer-   gent linguistic structure across Transformer archi-   tectures ( Rogers et al . , 2020 ; Manning et al . , 2020 ) .   5.3 General Qualitative Observations   In this section , we qualitatively evaluate the types   of topics that can be extracted with our method .   For brevity , we use examples only from the 20   Newsgroups data set although similar phenomena   can be observed in the AG News topics which can   be found in the Supplementary Materials . Table   2 illustrates sample topic clusters for which we   provided a manual category label . The topics in   our graph are generally coherent and exhibit ap-   propriate middle - level specificity ( not too coarse ,   not too fine ) . Our graph discovered unambiguous   top - level newsgroup categories , as expected . For   example , rows 0 - 6 represent vanilla topics relevant   to computers , space , sports , and religion . A vari-   ety of subtler , more interesting clusters are note-   worthy in that they capture a variety of broader ,   yet coherent lexical senses both para- and syntag-   matically . Rows 7 - 10 , for example , denote logic   and argumentation , physical damage , law , possibil-   ity , and evidence . Some of the topics discovered   border on word sense disambiguation which goes   beyond typical , predominantly nominal topics ( as   subject headings ) . Consider ( i ) the clear and ac-   curate sense - level distinctions in rows 12 - 15 ; ( ii )   " program(s ) " qua computer software ( row 1 ) vs. ra-   dio shows ( row 24 ) ; and ( iii ) a non - trivial pattern in-   volving clusters made of intra - sense antonyms sub-   sumed under a relevant macrosense category ( rows   18 - 20 ) . Interestingly , we also see higher , discourse-   level phenomena such as interjectional ( and other )   discourse markers and particles ( row 21 ) , and gen-   eral , extralinguistic text structures ( rows 22 - 23 ) .   These patterns indicate that our method is sensi-   tive enough to make non - trivial topic distinctions11520   at multiple levels concurrently .   5.4 Topic Subgraphs   Topics extracted via community detection on the   Mapper graph can be used to further probe and con-   textualise any individual topic by examining the   subgraph to which it corresponds . Figures 8 & 9   show a subgraph from each of the two data sets .   For example , Figure 9 visualises aspects of the   Middle East conflict as discussed in the 20 News-   groups data sets - these include people , locations ,   and ethnicity as well as historical , racial , religious ,   geopolitical , and military themes . Figure 8 shows   different topics pertaining to the film industry ex-   tracted from AG News .   6 Conclusion   We propose an unsupervised topic modelling   method which leverages topological data analy-   sis ( TDA ) to extract a semantic topic graph from   a large unstructured document collection . Our ex-   perimental results demonstrate that our method is   able to detect topics on par with a recent baseline   while also exposing meaningful inter - topic rela-   tionships towards deeper topic interpretation . Our   experiments to date motivate future work involving   TDA to develop , for example , interactive visualisa-   tion tools for exploring rich relational topic graphs ,   and to study the interface between topological and   linguistic properties of topics .   7 Limitations   Our method makes use of pretrained language mod-   els to extract contextualised word representations .   Thus we can introduce biases from the pretraining   data set . Often these data sets , undergo little or no   curation meaning the biases can be harmful or un-   wanted . See ( Bender et al . , 2021 ) for a discussion .   This differs from traditional probablistic topic mod-   els which only depend on the data set that is being   explored .   Another limitation of our approach is the number   of different hyperparmeters required . Our stabil-   ity analysis approach does not uniquely determine   them all , and some heuristic selection was still nec-   essary . Further analysis of the interaction between   clustering , UMAP , and cover parameters is an im-   portant direction for future work .   The connections in our graph represent the topol-   ogy of the manifold of BERT embeddings . While   we have demonstrated that these connections cap-   ture a general notion of " relatedness " , we can not   necessarily interpret them as semantic relations .   Further exploration of the graph ’s edges will be   necessary in order to understand what types of in-   terpretable relations can be captured .   References115211152211523   A Data   The 20 Newsgroups data set contains 18846 En-   glish language posts categorised into thematic   newsgroups . We use the standard train - test split .   Table 3 summarises per - category document fre-   quencies in the training set . We remove email   addresses , headers , and subject lines .   TheAG News data set is constructed by assem-   bling titles and description fields of news articles   from four classes : " World " , " Sports " , " Business " ,   and " Sci / Tech " . Since the data set is large we ran-   domly select 30000 articles resulting in the cate-   gory frequencies in Table 4 .   B All Detected Topics   Tables 5 , 6 and 7 show all topics from the 20 News-   group data set and Tables 8 , 9 and 10 show all   topics from the AG News data set .   C Target Label Analysis   C.1 20 Newsgroups Target Label Graphs   Figures 10 - 16 show the regions of the graph asso-   ciated with particular newsgroups . Figure 17 shows   the entropy of the distribution of newsgroup tokens   for particular nodes . This is used as a measure   of " diversity " - nodes with high entropy will have   tokens that come uniformly from all newsgroup   categories .   C.2 AG News Target Label Graphs   Figures 18 - 21 show the regions of the graph as-   sociated with particular news categories . Figure   22 shows the entropy of the distribution of target   labels.20 Newsgroups Category # Documents   alt.atheism 480   comp.graphics 584   comp.os.ms-windows.misc 591   comp.sys.ibm.pc.hardware 590   comp.sys.mac.hardware 578   comp.windows.x 593   misc.forsale 585   rec.autos 594   rec.motorcycles 598   rec.sport.baseball 597   rec.sport.hockey 600   sci.crypt 595   sci.electronics 591   sci.med 594   sci.space 593   soc.religion.christian 599   talk.politics . guns 546   talk.politics.mideast 564   talk.politics.misc 465   talk.religion.misc 377   AG News Category # Documents   Business 2477   Sci / Tech 2662   Sports 2338   World 252311524115251152611527115281152911530115311153211533