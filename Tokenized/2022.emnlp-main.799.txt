  Sicheng YuJing JiangHao ZhangYulei NiuQianru SunLidong BingSingapore Management UniversityDAMO Academy , Alibaba GroupNanyang Technological UniversityColumbia University   scyu.2018@phdcs.smu.edu.sg , { jingjiang,qianrusun}@smu.edu.sg   hzhang26@outlook.com , yn.yuleiniu@gmail.com   l.bing@alibaba-inc.com   Abstract   Out - of - distribution ( OOD ) settings are used to   measure a model ’s performance when the dis-   tribution of the test data is different from that   of the training data . NLU models are known to   suffer in OOD settings ( Utama et al . , 2020b ) .   We study this issue from the perspective of   causality , which sees confounding bias as the   reason for models to learn spurious correlations .   While a common solution is to perform inter-   vention , existing methods handle only known   and single confounder ( Pearl and Mackenzie ,   2018 ) , but in many NLU tasks the confounders   can be both unknown and multifactorial . In   this paper , we propose a novel interventional   training method called Bottom - up Automatic   Intervention ( BAI ) that performs multi - granular   intervention with identified multifactorial con-   founders . Our experiments on three NLU tasks ,   namely , natural language inference , fact verifi-   cation and paraphrase identification , show the   effectiveness of BAI for tackling different OOD   settings .   1 Introduction   From the era of word embeddings ( Pennington   et al . , 2014 ) to pre - trained language models ( Devlin   et al . , 2019 ) , researchers of natural language under-   standing ( NLU ) have tried to push the performance   on benchmark datasets . Traditional settings as-   sume independent and identical distribution ( IID )   in training and testing splits . However , the IID set-   ting cloaks the vulnerability of neural models , i.e. ,   neural models tend to learn non - robust “ shortcut ”   patterns in the training data but fail to make robust   predictions on unseen samples . To evaluate the ro-   bustness of models , the out - of - distribution ( OOD )   setting draws the attention of the NLU community . Figure 1 : The proportions of entailment and non-   entailment samples with different percentages of lexical   overlap .   For example , the task of natural language infer-   ence ( NLI ) determines whether a hypothesis can   be entailed from a premise . We can observe that   the lexical overlap between the hypothesis and the   premise correlates with the entailment label on the   benchmark MNLI dataset ( Williams et al . , 2018 )   ( as shown in the top part of Figure 1 ) . McCoy   et al . ( 2019 ) proposed an OOD set named HANS   for NLI . As shown in the bottom part of Figure 1 ,   HANS does not have the correlation between lexi-   cal overlap and the entailment label . NLI models   that rely on the lexical overlap heuristic suffer from   a significant degradation on HANS ( Utama et al . ,   2020b ) .   Recently , causal inference has been adopted in   NLP to identify robust correlations by analyzing   reliable causal effects between variables ( Zhang11627   et al . , 2021 ; Nan et al . , 2021 ) . From the perspec-   tive of causality ( Pearl , 2009 , 2010 ) , the crux under   a model ’s vulnerability is confounding bias . We   summarize the causal relations behind NLU tasks   as a causal graph in Figure 2(a ) . Xrepresents the   input , e.g. , a pair of sentences for NLI , and Yrep-   resents a label to be predicted . X→Yrepresents   the desired relation for a robust NLU model , i.e. ,   how to predict the label with reliable understand-   ing of the input . X←C→Ydenotes a backdoor   path of some unreliable relation between XandY   confounded by the confounder C. Examples of C   include nature bias in the dataset ( Tang et al . , 2020 )   or crowdsourced workers preference ( Geva et al . ,   2019 ) . For instance , in NLI , Cmay represent the   degree of lexical overlap between the premise and   the hypothesis , which is correlated with the entail-   ment relation in the MNLI dataset ( see Figure 1 ) .   When crowdsourced workers are engaged to cre-   ate hypotheses for NLI , Ccould be the experience   level of a worker , with inexperienced workers more   likely to write simple sentences with straightfor-   ward meanings . As a result , these examples of C   will make XandYspuriously correlated .   A common solution of deconfounding is inter-   vention ( Pearl et al . , 2016 ; Pearl and Mackenzie ,   2018 ) , which aims to block the backdoor path   ( or spurious correlation ) by cutting off C→X   ( see Figure 2 ( b ) ) . The key idea is to stratify X   into different environments ( Arjovsky et al . , 2019 ;   Teney et al . , 2021 ) , i.e. , several subsets of train - ing data , according to the identified confounder .   Then the model is expected to make environment-   agnostic prediction . By doing so , we are con-   trolling Xand thus break the backdoor path by   D - Separation ( Koller and Friedman , 2009 ) . Fig-   ure 2(c ) depicts an example where the NLI training   data is stratified into several environments , e.g. ,   one with obvious trend of lexical overlap bias and   another does not . Then the NLI model is trained to   fit both environments .   However , the confounder Cis not always ob-   served . Furthermore , confounders can be multifac-   torial in NLU , e.g. , it may contain both inherent   dataset bias and artifacts from crowdsourced work-   ers . Both scenarios make intervention non - trivial .   In this paper , we propose BAI , a bottom - up auto-   matic intervention method , which can ( 1 ) identify   the unobserved confounder(s ) automatically , and   ( 2 ) perform multi - granular intervention to handle   multifactorial confounders . Inspired by Creager   et al . ( 2021 ) , the automatic stratifying mechanism   is realized by maximizing the difference between   data in different environments . We further pro-   pose a novel bottom - up intervention mechanism   that aims to address the multifactorial characteris-   tic of C. While most existing debiasing work only   considers a single bias , our bottom - up mechanism   enables the model to pick up different confounders   in two rounds of interventions . Specifically , based   on our preliminary experiments , we find that fine-   grained partition ( i.e. , partition with more environ-   ments ) results in smaller differences between envi-   ronments , making environment - agnostic learning   easier . Thus we start from a fine - grained partition .   We then move on to a coarse - grained partition to   further block the backdoor effect via Cand make   the learning environment - agnostic .   We apply BAI on three OOD benchmarks for   NLU tasks . The results show that our method out-   performs state - of - the - art methods , e.g. , achieving 7   percentage points of absolute gains from the previ-   ous best method under OOD setting of Quora Ques-   tion Pairs ( QQP ) ( Zhang et al . , 2019 ) , a benchmark   dataset for paraphrase identification .   Contributions : ( 1 ) we analyze the issue of NLU   vulnerability from the perspective of causality anal-   ysis ; ( 2 ) we propose a bottom - up automatic in-   tervention method to perform intervention for un-   observed and multifactorial confounders ; and ( 3)11628extensive experiments on three OOD benchmarks   demonstrate that our method outperforms state - of-   the - art methods .   2 Related Work   OOD Generalization . OOD settings have   been studied in recent years in NLU . To tackle   dataset bias , most existing work relies on in-   stance reweighting with a bias model for debiasing .   Specifically , these methods ( Cadene et al . , 2019 )   first design a bias model and then train a target   debiased model fused with the bias model . Train-   ing instances predicted correctly by the bias model   will be down - weighted in the training of the debi-   ased model . Early work mainly revolves around   different fusion methods ( He et al . , 2019 ; Clark   et al . , 2019 ; Utama et al . , 2020a ; Mahabadi et al . ,   2020 ) with known bias . Then researchers started   looking into unknown bias by designing the bias   model with heuristics , e.g. , a model trained with   very small amount of data ( Utama et al . , 2020b ) or   a model with only the bottom layers of the language   model ( Ghaddar et al . , 2021 ) .   However , instance reweighting based methods   rely on either prior knowledge of bias or heuristic   design of the bias model . Furthermore , it is pointed   out that such bias models may not be able to pre-   dict the main model ’s reaction of biased samples   and reweighting may waste data ( Amirkhani and   Pilehvar , 2021 ) . In contrast , our method is derived   from causal inference ( Pearl and Mackenzie , 2018 ) ,   which is not related to any form of reweighting .   Meanwhile , our method does not adopt any bias   model ( which requires carefully design or prior   knowledge of bias ) .   Causal Intervention . Causality inference ( Pearl ,   2009 , 2010 ) measures the causal effect between   variables and has been widely applied to various   scenarios , e.g. , social science ( Baron and Kenny ,   1986 ) , medical science ( Hall et al . , 1993 ) , and other   applications ( Niu et al . , 2021 ; Yu et al . , 2020 ; Niu   and Zhang , 2021 ) . Recently , causality inference is   introduced to the machine learning community and   intervention is one of the techniques in causality   inference . Intervention ( Pearl , 1993 ) helps to elim-   inate the effect of confounders ( Yang et al . , 2021 ;   Yue et al . , 2020 ; Qi et al . , 2020 ; Nan et al . , 2021 ;   Zhu et al . , 2022 ; Niu et al . , 2022 ) . Invariant Risk   Minimization ( IRM ) ( Arjovsky et al . , 2019 ) imple-   ments intervention by learning a model invariant   to different environments ( Arjovsky et al . , 2019;Wang et al . , 2021 ) . Although IRM has been widely   adopted in computer vision ( CV ) ( Krueger et al . ,   2021 ; Rosenfeld et al . , 2020 ; Creager et al . , 2021 ;   Liu et al . , 2021 ; Wang et al . , 2021 ; Teney et al . ,   2021 ) , to the best of our knowledge , our proposed   BAI is an initial work of applying IRM in NLU .   Our work is distinguishable from previous work   in two aspects . First , previous work in CV mainly   focuses on image with annotated background as   confounder while the confounder in NLU is more   abstract and vague . Second , our method is the first   work considering multiple partitions for handling   multifactorial confounders .   3 Method   3.1 Preliminaries   Causal Intervention is the core idea of this paper .   We formulate NLU tasks with a causal graph ( Pearl   and Mackenzie , 2018 ) , which illustrates the causal   relationships between variables with a directed   acyclic graph . As shown in Figure 2 , each node   represents a variable , e.g. , a pair of sentences or   a label for NLU tasks , and each directed edge de-   notes that the head node has direct effect on the tail   node .   Naïve model training , i.e. , empirical risk mini-   mization ( ERM ) ( Vapnik , 1991 ) , indiscriminately   learns both spurious correlation X←C→Yand   causal correlation X→Y. Specifically , by ap-   plying Bayes ’ rule on Figure 2(a ) , we can obtain :   P(Y|X ) = /summationdisplayP(Y|X , c)P(c|X ) , ( 1 )   where the bias is introduced via P(C|X ) . For ex-   ample , consider the NLI task . Let Xbe a pair of   two sentences ( premise and hypothesis ) and Ythe   entailment label . Let Crepresents the degree of   lexical overlap between the two sentences in X ,   and let candcdenote two situations : having   obvious lexical overlap and having little or no lexi-   cal overlap . Typically on IID training data of NLI ,   P(c|X)is larger than P(c|X ) , and thus P(c|X )   tends to dominate the overall term , P(Y|X ) . In   other words , model tends to learn P(Y|X)fromc   instead of X.   In contrast , causal intervention in Figure 2(b )   yields :   P(Y|do(X ) ) = /summationdisplayP(Y|X , c)P(c),(2)11629   where the do(X)denotes that intervention is con-   ducted on X. With dooperation , cis no longer   associated with Xand thus the model treats cand   cfairly subject to the prior distribution of C.   Invariant Risk Minimization ( Arjovsky et al . ,   2019 ) ( IRM ) is one of the popular tool for interven-   tion in deep neural networks . Given the stratified   environments , IRM targets at a robust model which   is invariant to environments . In our paper , we uti-   lize two versions of IRM .   Given the input X , model fand the partition of   environments E , the original version of IRM ( Ar-   jovsky et al . , 2019 ) minimizes the objective :   IRM=/summationdisplayXE(f(X ) , Y )   + λ · ∥∇XE(w·f(X ) , Y)∥ ,   ( 3 )   where Xdenotes the data in the environment of   eandXEdenotes cross - entropy loss . wis a fixed   dummy classifier . The second term measures the   optimality of wfor each environment to encourage   the model to make environment - invariant predic-   tions . This version of IRM is unstable due to the   second - order derivatives .   Another version of IRM ( Teney et al . , 2021 )   adopted in our paper initializes individual classifier   Wfor each environment ewhile all environments   share one feature extractor . Here we denote the   model for the environment easf = W ◦ Φwhere   Φis a feature extracter , e.g. , BERT . The correspond-   ing loss is written as :   IRM=/summationdisplayXE(f(X ) , Y ) + λ·Var(W ) .   ( 4 )   The second term is the variance of classifier   weights , which encourages optimal classifiers fordifferent environments to be close to each other .   3.2 Bottom - up Automatic Intervention   To implement intervention on NLU tasks with the   unobserved and multi - factorial confounder , we pro-   pose a Bottom - up Automatic Intervention ( BAI )   method using IRM . Figure 3 and Algorithm 1 ( in   Appendix ) show the overall pipeline of BAI . It   consists of two components : automatic stratifica-   tion andbottom - up intervention . The automatic   stratification component generates partition of en-   vironments by maximizing the difference between   data in different environments based on a reference   model . The bottom - up intervention component per-   forms intervention at two levels of granularity .   Automatic Stratification generates the partition   of environments with unobserved confounder . A   good partition is achieved when a reference model   behaves differently under different environments .   Inspired by Creager et al . ( 2021 ) , we first train   a reference model fthrough the naïve trained   BERT ( Devlin et al . , 2019 ) . Note the second term   of Eq . 3 is to make environment - invariant predic-   tion , that is , to minimize the difference of data   behavior across environments . Inversely , our goal   is to maximize the difference of data behavior by   magnifying the second term of IRM .   As shown in Figure 3(a ) , we initialize an envi-   ronment matrix M∈Rindicating the belong-   ing of each training sample to each environment ,   where DandNdenote the number of training data   and pre - defined environments , respectively . M   is the probability of i - th sample belonging to j-   th environment . IRMis not applicable since   the naïve trained reference model only has one   classifier . Thus we derive Mby fixing the refer-11630ence model fand maximizing the second term   of IRMas follows :   max / summationdisplay∥∇XE(w·f(X ) , Y)∥,(5 )   whereEis the partition of environments determined   byM. Note that max operation makes the back-   propagation of gradients from Minfeasible . To   address this issue , we deploy the Gumbel Softmax   trick ( Jang et al . , 2016 ) to re - formulate the discrete   sampling as :   E = g(M ) = Gumbel - Softmax ( M).(6 )   We term the environment matrix with nenviron-   ments as M. Specifically , we deploy automatic   stratifying to extract two environments matrices ,   i.e. , fine - grained Mand coarse - grained M   ( n > n ) , for bottom - up intervention .   Bottom - Up Intervention adopts multi - granular   partitions for intervention in a bottom - up fashion ,   to derive a robust model f. As shown in Fig-   ure 3(b ) , bottom - up intervention consists of two   rounds of intervention deployed by IRMdue to   its stability and scalability .   We first generate fine - grained partition Eand   coarse - grained partition EfromMandM   ( see Figure 3(b ) ) , where the number of environ-   ments in Eis larger than that in E. Second ,   we start from the fine - grained partition Eand   train the intervened robust model f. Similarly ,   we decompose f = W ◦ Φwhere Φis feature   extractor , e.g. , BERT , and Wis a set of learned   classifiers . We use Wto represent the classi-   fier exclusive to environment eandW{E } to de-   note the set of classifiers for Epartition , that is ,   W{E}={W|e∈ E}represents all classi-   fiers for partition E. The feature extractor and the   classifiers of Ein bottom fine - grained interven-   tion are optimized by :   min / summationdisplayXE(f(X ) , Y)+λ·Var(W ) ,   ( 7 )   Then we conduct the intervention of coarse-   grained partition E. To prevent the catastrophic   forgetting , i.e. , the intervention with new partition   may make the model forget the invariant property   on previous partition , we incorporate the idea from   continual learning ( Li and Hoiem , 2017 ; Rebuffi   et al . , 2017 ) . Specifically , we fix the parameter of   model fincluding the feature extractor and nclassifiers for E. Then we augment nclassifiers   for the new partition E , resulting in n+nclas-   sifiers . Here we only optimize the naugmented   classifiers during training as :   min / summationdisplayXE(f(X ) , Y)+λ·Var(W ) ,   ( 8)   where the first term is based on the new partition   Ewhile the second term computes the variance   of classifier weights across all n+nclassifiers .   Inference is based on the design of IRM(Teney   et al . , 2021 ) . Since we are not able to distinguish   which environment the input data belongs to , we   simply average the weight of n+nclassifiers   for inference :   ˆY = f(X ) = ¯W·Φ(X ) , ( 9 )   where ¯Wdenotes the mean weight of all classifiers .   The overall pipeline of BAI is summarized in   Algorithm 1 .   Algorithm 1 BAI TrainingInput : Dataset D , reference model fOutput : f = W ◦ ΦInitialize environment matrix M , MUpdate M , Mwith Eq . 5 and Eq . 6Initialize fforXinDdo Get environment e∈EofXfromM Update ΦandWwith Eq . 7end forforXinDdo Get environment e∈EofXfromM Update Wwith Eq . 8end for   4 Experiment   4.1 NLU Tasks and Benchmarks   We apply our method on three NLU tasks to eval-   uate the effectiveness of our method . Specifically ,   we train on the original training set and evaluate   on both the IID and the OOD evaluation sets . The   accuracy is reported for all the benchmark datasets .   Natural Language Inference aims to classify the   relationship between two sentences , i.e. , a premise   and a hypothesis , into three classes : “ entailment ” ,   “ contradiction ” and “ neutral ” . It has been ob-   served that NLI models may rely on the lexical   overlap bias ( McCoy et al . , 2019 ) . We adopt11631MethodMNLI FEVER QQP   IID OOD IID OOD IID OOD   Dev HANS Dev Symmetric Dev PAWS   Naïve Fine - tuning 84.5 62.4 85.6 63.1 91.0 33.5   Reweighting ( KB ) 83.5 69.2 84.6 66.5 89.5 50.8   Product - of - Expert ( KB ) 82.9 67.9 86.5 66.2 88.8 58.1   Learned - Mixin 84.0 64.9 83.1 64.9 86.6 56.8   Regularized - Confidence ( KB ) 84.5 69.1 86.4 66.2 89.0 36.0   Reweighting ( UB ) 82.3 69.7 87.1 65.5 85.2 57.4   Product - of - Expert ( UB ) 81.9 66.8 85.9 65.8 86.1 56.3   Regularized - Confidence ( UB ) 84.3 67.1 87.6 66.0 89.0 43.0   Forgettable Examples 83.1 70.5 87.1 67.0 89.0 48.8   Self - Debiasing 83.2 71.2 - - 90.2 46.5   EIIL 83.9 69.9 89.2 68.1 87.9 57.3   BAI ( Ours ) 82.3 72.7 90.1 69.1 84.2 65.0   MNLI ( Williams et al . , 2018 ) and HANS ( McCoy   et al . , 2019 ) as the IID and OOD sets , respectively .   Fact Verification also takes in a pair of sentences ,   i.e. , a claim and an evidence , and requires the   model to give the position of the evidence towards   the claim . The labels are “ support ” , “ refutes ” , and   “ not enough information ” . Fact verification models   often suffer from the claim - only bias ( Utama et al . ,   2020b ) . In this paper , we use FEVER ( Thorne   et al . , 2018 ) as the IID data and FEVER Symmet-   ric ( Schuster et al . , 2019 ) as the OOD data .   Paraphrase Identification identifies whether a   sentence is paraphrase of another sentence . A sen-   tence pair is labeled as “ duplicate ” if the two sen-   tences share the same semantic meaning , otherwise   “ non - duplicate ” . Similar to NLI , lexical overlap   bias exists in paraphrase identification . We use   QQP ( Wang et al . , 2018 ) in training as the IID set   and PAWS ( Zhang et al . , 2019 ) as the OOD set .   4.2 Implementation   BERT - base ( Devlin et al . , 2019 ) from Hugging-   Face ’s Transformers ( Wolf et al . , 2020 ) is deployed   as the feature extractor for fair and direct compari-   son with previous methods . The reference model   is also based on BERT - base which is the same as   in Devlin et al . ( 2019 ) , i.e. , one classifier layer on   top of BERT . For standard hyperparameters for thetraining of NLU model , we use the same configu-   ration as Utama et al . ( 2020a , b ) , i.e. ,3epochs of   training , learning rate of 5e−5for NLI and 2e−5   for fact verification and paraphrase identification .   Unlike previous methods ( Clark et al . , 2019 ; Grand   and Belinkov , 2019 ; Clark et al . , 2020 ; Sanh et al . ,   2020 ; Ghaddar et al . , 2021 ) which are directly eval-   uated on the OOD set , we only perform checkpoint   selection on the OOD set . We choose hyperpa-   rameters exclusive to our method according to the   analysis on the NLI task ( see RQ3 ) and deploy the   same configuration for the other two tasks to avoid   hyperparameter tuning . Specifically , we set the   learning rate to 1e−2for automatic stratification   to optimize the environment matrix , and n= 5   andn= 2for bottom - up intervention . We also   fixλto1e2 . Note the coarse - grained partition may   require multiple turns of training to achieve better   performance . The average results over 5runs with   different random seeds are reported .   4.3 Comparison with SOTAs   In this section , we compare our method with the   following baselines : Naïve Fine - tuning ( Devlin   et al . , 2019 ) directly fine - tunes the pre - trained   language model on the downstream NLU tasks ;   Reweighting ( Clark et al . , 2019 ) reweights each   training sample according to the confidence on bias11632model ; Product - of - Expert ( Hinton , 2002 ) trains   the robust model fused with the bias model by   sum of logits ; Learned - Mixin ( Clark et al . , 2019 )   utilizes a different fusion method . Regularized-   Confidence ( Utama et al . , 2020a ) enhances the   model in a knowledge distillation fashion ; Un-   known bias version methods in Utama et al .   ( 2020b ) adopt the bias model trained only with   a small number of data ; Forgettable Exam-   ples ( Yaghoobzadeh et al . , 2021 ) trains the model   with an additional round with the forgotten data ;   Self - Debiasing ( Ghaddar et al . , 2021 ) utilizes bot-   tom layers of model as the bias model ; EIIL ( Crea-   ger et al . , 2021 ) is the IRM method that inspired   this paper , which is originally applied to CV .   Table 1 summarizes the performance comparison   between BAI and the above SOTA methods . Over-   all , BAI achieves the top performance on all the   OOD sets . Specifically , BAI significantly outper-   forms naïve Fine - tuning by doubling the accuracy   on PAWS ( 65.0%vs.33.5 % ) , which demonstrates   that BAI with causality - theoretic basis is effective   for OOD generalization on NLU tasks . Also , BAI   surpasses SOTA methods with 6.9%gains over   previous best result on PAWS , which shows the su-   periority of BAI over reweighting based methods .   We also observe a trade - off between IID and   OOD on MNLI and QQP across most of the meth-   ods , i.e. , performance gains on OOD are achieved   with the sacrifice of IID performance . It is because   naïve fine - tuning fits IID training data well . Inter-   estingly , the IID test data of FEVER benefits from   debiasing methods , which suggests that the data   distribution of the IID test data may be different   from that of the training data .   4.4 Ablation Studies   In this section , we conduct extensive ablation stud-   ies to evaluate the components in our BAI and an-   swer the following research questions .   RQ1 : How does each component of BAI contribute   to the performance gains ?   Answer : We design four ablative settings : ( a ) Re-   placing the learned environment matrix with a ran-   domly initialized one ; ( b ) Removing the regularizer   term in Eq . 7 and 8 ; ( c ) Replacing bottom - up in-   tervention with single intervention , i.e. , removing   Eq . 8 . ( d ) Using the same number of classifiers on   naïve fine - tuning model as our BAI .   As reported in Table 2 , the settings ( a ) and ( d )   prove that the environment partition is vital in ourAblative Setting Dev HANS   Naïve FT 84.5 62.4   ( a ) Randomized Environment 84.0 62.4   ( b ) w/o Regularizer 83.0 66.8   ( c ) One Intervention 83.9 69.9   ( d ) Naive FT+Multiple Classifiers 84.4 62.6   Full Method 82.3 72.7   Stratifying Method Dev HANS   No Stratifying 84.5 62.4   ( 1 ) Domain Information 84.2 63.2   ( 2 ) Confidence 84.0 67.7   ( 3 ) Lexical Overlap 83.8 65.6   Automatic Stratifying ( Ours ) 83.9 69.9   method and the improvement of our method is not   from the added parameters . Result of ( b ) reveals   that both the regularizer term and the design of   one classifier for one environment contribute to   the gains in our method . Finally , the full method   with bottom - up intervention outperforms ( c ) , which   demonstrates the effectiveness of multi - granular   intervention .   RQ2 : Is there any other solution for stratification ?   Answer : Yes . We evaluate several alternative meth-   ods for partition on MNLI according to the attached   information of training samples : ( 1 ) Domain in-   formation , i.e. , “ fiction ” , “ governmnet ” , “ slate ” ,   “ telephone ” and “ travel ” ; ( 2 ) Confidence of predic-11633   tion ( Clark et al . , 2019 ) . We calculate the high-   est confidence or the options and the confidence   for the ground - truth label . All the samples are   grouped into environments by K - Means ( Hartigan   and Wong , 1979 ) according to the two confidence   scores ; ( 3 ) Prior knowledge of bias , i.e. , lexical   overlap bias in Figure 1 . We also group them into   different environments by K - Means . For fairness ,   we fix the number of environments as 5 , which   is the number of domains in the setting ( 1 ) . We   compare the above settings with our model trained   using only one intervention in Eq . 7 .   As summarized in Table 3 , the results show that   directly using domain information as basis for envi-   ronments stratifying has very few gains , i.e. ,0.8 % .   Although intervention based on domain informa-   tion is beneficial for every domain , such interven-   tion does not provide a good partition for debiasing   as the lexical bias still exists . Stratifying based on   confidence and lexical overlap shows considerable   improvements compared to that of no stratifying ,   which demonstrates the two factors are indeed re-   lated to the confounder of MNLI . Note that the   automatic stratifying method is designed for unob-   served confounder , which outperforms the simple   heuristics in settings ( 2 ) and ( 3 ) without using any   prior knowledge of bias .   RQ3 : How to set the number of environments ?   Answer : We first analyze the situation of only   one round of intervention and visualize the per-   formance trend in Figure 4 . Note that setting the   number of environments as one equals to naïveOrder & Combination Dev HANS   E→ E 81.7 70.1   E→ E 83.7 71.4   E→ E→ E 81.3 73.5   E→ E(Config in Table 1 ) 81.1 73.3   fine - tuning , i.e. , no stratification . Overall , there is   a trade - off in the results between Dev and HANS ,   i.e. , IID and OOD performances . This phenomenon   is particularly prominent in E. The reason is that   only one intervention forces the model to focus   on only one confounder . In this case , it forces the   model to pay much attention on the harder samples ,   i.e. , the confounder of crowdsourced worker pref-   erence , leading to significant performance drop on   dev set ( see RQ4 for more details).With the num-   ber of environments increasing , the gaps between   the environments are also smaller , i.e. , the OOD   performance of ten environments is close to that of   the naïve fine - tuning .   We further analyze the multiple interventions .   We conduct experiments with the number of inter-   ventions in different orders or combinations . The   experiment results are summarized in Table 4 . We   observe that applying the partition with two en-   vironments in the final intervention is better and   increasing the turns of intervention only brings11634MethodMNLI FEVER QQP   IID OOD IID OOD IID OOD   Naïve F.T. 87.3 69.8 87.1 68.6 90.8 38.4   BAI 85.3 76.7 91.2 72.9 83.5 70.2   marginal improvements . Thus , we simply fix   E→ Efor all tasks in our paper .   RQ4 : What is each environment like ?   Answer : Figure 5 inspects each environment in   two partitions , i.e. ,EandE , on MNLI and sum-   marizes the characteristic for each environment . E   can be regarded as a coarse variant of E , i.e. , the   first environment of Epartition combines four en-   vironments of E. We can see that both partitions   contain environments with distinct characteristics .   Efocuses more on crowdsourced worker prefer-   ence while Eshows each environment with more   diverse situation for the nature bias , i.e. , lexical   overlap bias .   We further investigate the crowdsourced worker   preference in E , i.e. , the difficulty of the sam-   ples in these two environments is distinguishable .   Samples in the second environment are more chal-   lenging compared to the first one . As depicted   in Figure 5 ( ii ) , reasoning of easy samples is   straightforward , i.e. ,nice versus not nice and   do not like . In contrast , hard examples re-   quire a deep understanding of the semantic mean-   ing . For instance , the hard samples with contra-   diction and entailment as labels expect the model   to have the ability to identify the current situation ,   e.g. ,no name for now , and the usual situation ,   e.g. ,name is usually mentioned in the   past . The above inspection reveals that BAI helps   to generate meaningful and multifactorial partition .   RQ5 : Whether BAI is model - agnostic ?   Answer : We apply the same hyperparameters and   partitions on BERT to RoBERTa ( Liu et al . , 2019 ) .   The results in Table 5 demonstrate that the pro-   posed BAI can be applied on more advanced lan-   guage model than BERT .   5 Conclusions   In this paper , we explore how to improve the robust-   ness of NLU models under OOD setting , and pro-   pose a bottom - up automatic intervention method   for debiasing . The experiment results demonstratethe superiority of our model over state - of - the - art   methods . In future work , we will consider two   improvements on BAI . First , we target at an end - to-   end framework for intervention and dynamic learn   the partition of environment for NLU tasks . Sec-   ond , we want to ease the trade - off effect between   IID and OOD sets .   6 Limitations   The limitations of this paper are twofold . First ,   the proposed method is only evaluated on natural   language understanding tasks . Thus the effective-   ness on natural language generation tasks and se-   quence labeling tasks is not guaranteed . Similarly ,   the optimal hyper - parameters for other tasks may   also differ from the selections stated in this paper .   Second , the performance trade - off ( see Table 1 ) is   non - negligible on the IID set compared to the OOD   set . It is not desirable when the model is applied to   the normal scenario , e.g. , the confounders provide   shortcuts for model inference .   Acknowledgments   The authors gratefully acknowledge the support by   the Lee Kuan Yew Fellowship awarded by Singa-   pore Management University .   References11635116361163711638