  Hongyuan Lu , Wai Lam , Hong Cheng , Helen M. Meng   The Chinese University of Hong Kong   { hylu,wlam,hcheng,hmmeng}@se.cuhk.edu.hk   Abstract   Incorporating personas information allows di-   verse and engaging responses in dialogue re-   sponse generation . Unfortunately , prior works   have primarily focused on self personas and   have overlooked the value of partner personas .   Moreover , in practical applications , the avail-   ability of the gold partner personas is often not   the case . This paper attempts to tackle these   issues by offering a novel framework that lever-   ages automatic partner personas generation to   enhance the succeeding dialogue response gen-   eration . Our framework employs reinforcement   learning with a dedicatedly designed critic net-   work for reward judgement . Experimental re-   sults from automatic and human evaluations   indicate that our framework is capable of gen-   erating relevant , interesting , coherent and infor-   mative partner personas , even compared to the   ground truth partner personas . This enhances   the succeeding dialogue response generation ,   which surpasses our competitive baselines that   condition on the ground truth partner personas .   1 Introduction   Building informative and engaging dialogue agents   ( Zhang et al . , 2020 ; Roller et al . , 2021 ) is a pop-   ular research direction within the area of natural   language processing . For the sake of engagement ,   diverse and consistent responses ( Song et al . , 2020 ,   2021 ) are important factors , and personas informa-   tion ( Zhang et al . , 2018 ) gives rise to both . There   are two types of personas , namely self persona   and partner persona . The former refers to a self   profile consisting of several sentences represent-   ing the dialogue agents . Such a persona allows   producing consistent responses rather than solely   relying on the personas that are randomly learned   and embedded in the model parameters ( Kim et al . ,   2020 ) . The latter refers to a profile that represents   the users . Leveraging such partner personas has   been empirically shown to be helpful for dialogue   response selection ( Gu et al . , 2021).Unfortunately , the existence of partner personas   suffers from the cold start ( Schein et al . , 2002 ;   Zhang et al . , 2014 ; Li et al . , 2021 ) at the begin-   ning of the conversation . Most of the works , if not   all , ( Li et al . , 2016b ; Mazaré et al . , 2018 ; Song   et al . , 2019 ; Gu et al . , 2019 ; Zhao et al . , 2019 ;   Madotto et al . , 2019 ; Liu et al . , 2020 ; Majumder   et al . , 2020 ; Wu et al . , 2020a ; Song et al . , 2020 )   have been either overlooking partner personas or   simply focusing on the impractical situation where   partner personas guarantee to exist . In contrast , our   work does not suffer from the practical issue when   partner personas are missing during inference , and   our proposed framework surpasses the baseline that   conditions on the ground truth partner personas .   To our knowledge , this is the first attempt to for-   mulate partner personas generation for improved   performance on the downstream dialogue response   generation . Our work is motivated by the under-   lying hypothesis that partner personas generation   is plausible given the self personas and dialogue   context . Automatic and human evaluation results   support the hypothesis and indicate that generated   personas are even more interesting than the ground   truth , which improves the downstream dialogue re-   sponse generation . This paper thus paves the way   to exploit partner personas generation ( PPG ) for   dialogue response generation ( DRG ) .   We propose a novel framework composed of   three major components , namely a personas gen-   erator , a dialogue response generator and a critic   network . The personas generator generates partner   personas , which the dialogue response generator   conditions on . We employ reinforcement learning   with a critic network that propagates the reward   back to the generators for joint training .   Prior works have investigated partner persona re-   trieval ( Zhang et al . , 2018 ; Song et al . , 2019 ) . The   human - constructed ground truth personas serve as   the upper bound for such retrieval - based systems ,   and we argue that the ground truth is not coherent5200and diverse enough . Interestingly , we observe that   the generative counterpart proposed in our frame-   work generates relevant , informative and coherent   partner personas , which further improves the suc-   ceeding dialogue response generation . It follows   another advantage that our framework does not   need an external database to retrieve from ( Madotto   et al . , 2020 ; Xu et al . , 2021 ) .   One close work to ours is a multi - task frame-   work for meta - learning ( Lee et al . , 2021 ) that uses   personas reconstruction as an auxiliary task to im-   prove response consistency . The differences are   that theirs does not differentiate between self per-   sonas and partner personas , while ours does . Theirs   indicates an improvement over personality con-   sistency , while ours report improvements for the   overall quality . We conduct an empirical compar-   ison with their model by reconstructing the part-   ner personas . Experimental results indicate that   such a multi - task model does not work well in our   problem setting . Very recently , Zhou et al . ( 2021 )   formulates personas generation as a Seq2Seq task   for improved downstream response generation via   multi - task learning . In contrast , our work leverages   reinforcement learning to jointly train the partner   personas generator and the response generator .   Automatic and human evaluation results indi-   cate that our framework can generate partner per-   sonas that are more diverse and interesting than the   ground truth partner personas and generate more   diverse and engaging responses than the baseline   conditioned on ground truth partner personas .   2 Related Work   2.1 Personalized Dialgoue Generation   Conditioning on personas helps to produce infor-   mative and engaging responses . The most well-   known multi - turn dialogue dataset conditioned on   personal profiles is P C ( Zhang et al . ,   2018 ) , in which two crowdsourcers converse and   find more about each other . The community has   proposed many methods to better utilize self per-   sonas . Mazaré et al . ( 2018 ) employs a pre - training   stage based on dedicatedly extracted large - scale   persona - based dialogues . Zhao et al . ( 2019 ) fuses   information in personas and dialogue context into   individual contextualized representations by attend-   ing to different parts of both . Gu et al . ( 2019 )   exploits the interaction between personas , dialoguecontext and response to improve retrieval - based   dialogue agents . Madotto et al . ( 2019 ) leverages   meta - learning with several dialogues of the current   speakers to enhance response personality . Welleck   et al . ( 2019 ) releases a dataset for measuring dia-   logue consistency . Song et al . ( 2020 ) employs a   multi - stage pipeline to improve response person-   ality by response rewriting . Lee et al . ( 2021 ) uses   multi - task learning for improved personality con-   sistency in the meta - learning scenario . Gu et al .   ( 2021 ) employs four different strategies for per-   sonas fusing to leverage both self persona . How-   ever , most of these works focus on exploiting self   personas rather than partner personas , and they as-   sume the existance of the gold partner personas .   2.2 User Profile Extraction   Li et al . ( 2014 ) leverages distant supervision to clas-   sify the spouse , education and job information from   user twitters . Wu et al . ( 2020b ) proposes a two-   staged profile extractor that extracts attributes be-   fore extracting the underlying relationship . Wang   et al . ( 2021 ) proposes to categorize the profile ex-   traction task into two different difficulties , namely   ‘ extraction ’ and ‘ inference ’ , and they leverage a   GPT - based generator to extract user profiles . These   works have formulated user profile extraction as a   classification task that conditions on an input sen-   tence , and they aim at better profile extraction . In   contrast , we propose to formulate personas genera-   tion to be conditioned dialogue input to be jointly   trained with response generation . While ground   truth personas serve as the upper bound for these   user profile extractors , we empirically demonstrate   that our reinforcement learning algorithm surpasses   the response model conditioned on the ground truth   partner personas . As supported by our human eval-   uation , we believe the underlying reason is that   our model can leverage pre - trained generators to   generate coherent and relevant partner personas .   2.3 Reinforcement Learning   Reinforcement learning ( RL ) , or specifically , pol-   icy gradient methods ( Williams , 1992 ) , have been   frequently adopted to both task - oriented dialogue   agents ( Roman Roman et al . , 2020 ; Deng et al . ,   2021 ) or open - domain chitchat agents ( Li et al . ,   2016c ; Saleh et al . , 2020 ) . It can either propagate   non - differentiable loss ( Cai et al . , 2019a ) or opti-   mize an expert reward such as ease of answering   ( Li et al . , 2016c ) . It also adopts a scenario where a   user simulator and a dialogue agent interact , and an5201   expert reward function can be defined to assign the   goodness to each response generated ( Roman Ro-   man et al . , 2020 ) .   3 Proposed Framework   We propose a novel framework composed of three   major components , namely a partner personas gen-   erator , a dialogue response generator and a rein-   forcement learning component with a critic net-   work . Figure 1 depicts the inference flow of our   setting . The input dialogue context with self per-   sona is first fed into the partner personas generator . The generated partner personas output is then con-   catenated with the dialogue context and the self   personas as the input into the dialogue response   generator . In the beginning , we train our partner   personas generator and dialogue response genera-   tor under supervised learning . In the training stage ,   we use the ground truth partner personas to train   the dialogue response generator , and we replace it   with generated partner personas in the inference   stage . After the supervised learning stage , the sec-   ond stage is a reinforcement learning stage which   jointly optimizes both partner personas generator   and dialogue response generator as depicted in Fig-   ure 2 to train the partner personas generator under   the reward signal that is relevant to dialogue re-   sponse generation as well as fine - tuning dialogue   response generator trained on the generated partner   personas . Particularly , we employ a dedicatedly   designed critic network that receives generated part-   ner personas and generated dialogue responses as   the input and output a reward that measures the   relevance between the generated personas and re-   sponses and propagates back to the generators .   3.1 Partner Personas Generation ( PPG )   A Seq2Seq neural network ( Sutskever et al . , 2014 )   is adopted as our partner personas generator for   the task of partner personas generation ( PPG ) . The   concatenation of dialogue context cand self per-   sonas sis fed as an input into the partner personas   generator . The personas generator then outputs an   approximated partner personas ˆpconditioned on   the input that maximises the following likelihood :   P(ˆp|s , c ) = /productdisplayP(ˆp|ˆp , ... , ˆp , s , c ) ,   where Trepresents the length of the generated part-   ner personas and ˆprepresents the word at the posi-   tiontthat has been inferenced .   For training , the ground truth partner personas p   is used and we train our generator to maximise the   likelihood P(p|s , c ) . We generate the complete   partner personas profiles in an one - off shot for all   the dialogue samples .   3.2 Dialogue Response Generation ( DRG )   We also adopt a Seq2Seq neural network for the   task of dialogue response generation ( DRG ) . Dur-   ing inference , the concatenation of dialogue context5202c , self personas s , and generated partner personas ˆp   is fed as an input into the dialogue response genera-   tor . The response generator then outputs a dialogue   response ˆrconditioned on the input , which max-   imises the conditional likelihood : P(ˆr|s,ˆp , c ) .   For training , the ground truth partner personas p   and the ground truth dialogue responses rare used .   3.3 Reinforcement Learning ( RL )   We employ a critic network to compute the rein-   forcement learning rewards for our generators . We   use a binary classifier as critic by extracting train-   ing instances ( s , r , L=1),(s , r , L=1 ) and   ( s , r , L=1 ) . Then we can derive two negative   samples as : ( s , r , L=0 ) and(s , r , L=0 ) .   Thereafter , we fine - tune on a binary classifier to be   used as our critic in RL on the training partition by   minimizing the binary cross - entropy loss :   −Llog(P(L|s , r))−(1−L ) log(1 −P(L|s , r ) ) ,   where the binary label Lindicates whether the re-   sponse is relevant to the personas .   We then use this classifier acting as a critic net-   work that outputs ˆL , conditioned on the generated   partner personas ˆpand generated response ˆr . The   predicted binary label ˆLis then converted to a re-   wardR.Ris a positive reward when ˆL=1 , andR   is a negative reward when ˆL=0 . We empirically   set the reward Rfor RL to { 1 , -1 } for both PPG   and DRG . We then update our RL agents with the   following gradients :   ∆θ=−R ▽ logP(ˆp|s , c )   for the partner personas generator ( PPG ) , and for   the dialogue response generator ( DRG ):   ∆θ=−R ▽ logP(ˆr|s,ˆp , c )   By formulating a reward that measures the rele-   vance between generated partner personas and gen-   erated dialogue response , we are motivated by the   following objectives :   •Further fine - tune the partner personas genera-   tor to generate personas that benefits the down-   stream dialogue response generation.•Further fine - tune the dialogue response gen-   erator trained with ground - truth partner per-   sonas to adapt to noisy partner personas gen-   erated by the partner personas generator .   As mentioned in Section 3.1 , the first motivation   is that we are generating the complete personas   profile . However , some of them can be irrelevant   and unhelpful for the next - turn dialogue response   generation . It could be challenging for the part-   ner personas generator alone to identify which per-   sonas could be helpful . Therefore , we design such   a reward to train the personas generator to learn   to generate a set of personas that is more helpful   for the downstream dialogue response generation .   Our second motivation is that the dialogue response   generator has not been exposed to the generated   partner personas . We would like to fine - tune the re-   sponse generator to mitigate the potential training-   inference discrepancy . Experimental results indi-   cate that our design empirically works well .   The previous work from Cai et al . ( 2019a ) em-   ployed critic network for RL loss backpropagation .   The major difference is that their critic is trained in   an adversarial manner ( Li et al . , 2018 ) to pick up   the gold response among other negative candidates .   Also , their critic network conditions only on the   dialogue response but not on the generated skele-   ton . In contrast , we aim for improved response   generation with a classifier conditioning on both   the generated personas and the generated response .   3.4 Evaluation Metrics   For both PPG and DRG , perplexity ( PPL ) is re-   ported to measure the intrinsic performance with   the ground truth output ( Roller et al . , 2021 ) . We   adopt well - known sequence evaluation metrics   weighted BLEU ( Papineni et al . , 2002 ) and F-   measure for ROUGE - L ( Lin , 2004 ) as the extrinsic   evaluations . For PPG , we also report Distinct - N   with N={1,2 } to measure the response diversity ( Li   et al . , 2016a ; Cai et al . , 2019b ; Gao et al . , 2019 )   with the ratio of distinct unigrams / bigrams against   total number of unigrams / bigrams generated .   4 Experimental Setup   4.1 Dataset   We conduct experiments on the P C   ( Zhang et al . , 2018 ) , the most well - known multi-   turn dialogue dataset conditioned on personas . We   follow the train / valid / test split from the PAI5203   platform ( Miller et al . , 2017 ) that contains about   65,000/7,800/7,500 instances respectively . Each   instance contains about 8 utterances on average   and about 4 traits for each of the self and partner   personas . We denote the dataset with this original   personas as P C- . Later the orig-   inal personas have been manually scrutinized by   rephrasing , generalizing or specializing , which we   denote as P C- . We apply the same   preprocessing operation to both datasets . To train   the critic for RL , we collected about 130,000 in-   stances from the train split with equally distributed   positive and negative samples .   4.2 Baselines and Comparison Models   E2E w/o Partner Personas This is an end - to - end   ( E2E ) response generator without partner persona .   E2E w/ Partner Personas in Training With par-   tial ground truth partner personas for training only .   E2E w/ Partner Personas in Training and In-   ference With ground truth partner personas for   training and inference . During early experiments ,   we found that feeding all traits yields lower perfor-   mance . Retrieving Top-3 relevant partner personas   using BM25 ( Robertson and Walker , 1994 ) yields   the best performance on the original personas .   GPT-2 This is a comparison model fine - tuned   onGPT-2 ( Radford et al . , 2019 ) . We build the   same three E2E systems described above , and the   best model is selected , the third one .   T T A comparison model built   with a Transformer - based model pre - trained on gen - eral domain corpus , which is then fine - tuned on   P C ( Wolf et al . , 2019 ) .   PCVAE This is a comparison model that em-   ploys a memory - augmented architecture incorpo-   rated with conditional variational autoencoder that   exploits persona information ( Song et al . , 2019 ) .   PAML This is a comparison model that lever-   ages several dialogues collected from the same   speaker to enhance response personality via meta-   learning ( Madotto et al . , 2019 ) . As the authors did   not conduct experiments on the P C- and no preprocessing scripts are provided for   the revised personas , we only report the results of   their model on the P C - only .   MTL w/ Personas Reconstruction This is a   multi - task learning ( MTL ) comparison model ( Lee   et al . , 2021 ) trained to maximise the objective :   αL+ ( 1−α)L ,   where Lrepresents the auxiliary PPG likeli-   hood , and Lrepresents the DRG likelihood . α   is weight tuned over the validation set , and both   tasks condition on dialogue context and self per-   sonas and share the same model parameters .   5 Results   5.1 Dialogue Response Generation Results   We build our baselines , the partner personas gen-   erator and the dialogue response generator based   on a state - of - the - art pre - trained dialogue model   D GPT ( Zhang et al . , 2020 ) for parameters5204   initialization . More implementation details can be   found in Appendx B.   The dialogue response generation results are pre-   sented in Table 1 . Our framework with reinforce-   ment learning attains the best over all the metrics   on both P C - andP C- . This supports the usefulness of our frame-   work , which generates reasonable personas and   effectively enhances the succeeding dialogue re-   sponse generation , through the use of RL .   Although T T attains a better   score on the PPL than the fine - tuned GPT-2 , GPT-   2have better extrinsic scores than T -   T .GPT-2 also has better overall scores   than the E2E baselines without the complete part-   ner personas . However , it is surpassed by the E2E   baseline with the complete partner personas during   training and inference .   The E2E baseline with the complete ground truth   partner personas attains better scores on all of the   metrics than our remaining baselines . Our frame-   work with RL succeeds the performance of such a   competitive baseline for both P C-   andP C- , indicating our proposed   framework ’s robustness against paraphrasal .   The multi - task learning comparison model ( Lee   et al . , 2021 ) produces less promising results . Con-   cretely , we postulate that the nature of PPG and   DRG largely differs . The textual format of partner   personas always initiates with first - person sentence   starters , while dialogue responses are more general ,   ranging from greetings to goodbyes . Therefore , it   could be hard to capture both in a single model .   5.2 Cold Start   Cold start is a common problem in recommender   systems ( Schein et al . , 2002 ; Zhang et al . , 2014 ; Li   et al . , 2021 ) . This also applies to dialogue systems ,   as the partner personas are commonly missing in   early turns . We conduct an analysis on the base-   lines and our framework when N turns are available   where N={1,2,3 } , using P C- . As   demonstrated in Figure 3 , all the methods attain a   better PPL when N increases , which indicates the   existence of the cold start . This is also the case for   the baseline with ground truth personas , and we   postulate that it fails to learn how to use partner   personas during cold start due to the lack of clues .   Our framework effectively mitigates the cold start   problem and attains the best among them for all N.   5.3 Case Study on Dialogue Response   Generation   Table 2 depicts the case study for response gener-   ation using P C- . In the first case,5205   our framework successfully recognizes that the   partner is asking specifically for metallica . It then   conditions on the generated personas to generate   a much more entailed response than the baseline .   The human response expresses negatively and thus   seems less engaging . In the second case , our frame-   work recognizes that the partner has a garden . It   then talks about the garden rather than the irrele-   vant response from the baseline that we postulate   is misled by the ‘ large ’ adjective in the dialogue   context . The human response is potentially sarcas-   tic if the partner is not joking , while our generation   does not have such issue . For the third case , the   baseline produces a response that could be poten-   tially offensive , which could be biased by the word   ‘ violent ’ in the dialogue context . In contrast , our   framework recognizes the identity of the partner   to generate a response without such an issue . The   human response tends to raise a new topic and is   less relevant . For the fourth case , we observe that   the annotator sometimes converses based on the   partner profile rather than his own traits . In this   case , the annotator ( Dialogue Context ) said that   he has many pets , which is not in his own traits   ( Gold Partner ) . Rather , his conversation partner   expressed his passion for animals in previous dia-   logue contexts . We postulate that the annotator at-   tempted to engage the conversation by conditioning   his partner personas and telling a relevant joke . Our   PPG can recognize this , which further tweaks the   model output to talk about dogs and cats rather thanthe dog only . These cases validate that leveraging   partner personas is beneficial , and our framework   can generate reasonable partner personas , which is   not even in the ground truth .   5.4 Partner Personas Generation Results   Table 4 presents the quality measurements of the   generated partner personas from our PPG with no   RL . We observe that our models have much higher   Distinct - N scores as the number of unique words   in the generated output is much higher than the   ground truth test personas . Compared to the ground   truth personas that are limited sets of traits , our gen-   erator can leverage the power of pre - trained models   for better diversity . The remaining metrics also re-   port reasonable scores , suggesting the plausbility to   formulate personas generation as a Seq2Seq task .   5.5 Case Study on Partner Personas   Generation   Table 3 presents generated partner personas using   P C- . As depicted , our PPG can   generate reasonable partner personas which are   relevant to the ground truth partner personas . It   sometimes gives a reasonable generation which is   even not in the ground truth partner personas . In the   first case , the generator successfully identifies the   partner as being an army ranger . It then becomes   rather positive than a violent person as given in   the ground truth personas . Conditioning on such   positive contents can give a positive response . In   the second case , it recognizes the partner as a gym   person , and imagines that the partner drinks protein   and life weights , which is not in the ground truth   personas . In the third case , the generator generates   coherent personas , saying that the partner would   drink beer and eat food while watching football ,   which is also not in the ground truth . We postulate   that personas could be semantically closer to each   other when they frequently co - occur in the training   set . Our PPG then tends to generate more coherent5206   personas by learning such semantical relationship .   Since our generated personas are relevant and co-   herent , we postulate it as the underlying reason why   our method gives a better generalization to DRG .   In contrast , as demonstrated by Table 3 , ground   truth personas tend to be more like discrete collec-   tions of traits . This could be the reason why some   of our generated partner personas could beat the   ground truth , which is also supported by our human   evaluation in Section 5.6 . This is a potential benefit   of our approach compared to sentence - level user   profile extraction ( Li et al . , 2014 ; Wu et al . , 2020b ;   Wang et al . , 2021 ) that is upper bounded by the   discrete ground truth . We present more examples   in Table 8 in the Appendix .   5.6 Human Evaluation   We hired experienced annotators who have degrees   relevant to English Linguistics to conduct eval-   uation on P C- . For both DRG   and PPG , we present a questionnaire composed   of 800 questions with randomly sampled 200 test   instances to three annotators who compare model   outputs under A / B testing . As in Zou et al . ( 2021 )   and ACUTE - Evals ( Li et al . , 2020 ) , annotators fol-   low the criteria which we present in Appendix D.   Table 5 presents the human evaluation results   on dialogue response generation . Our framework   trained under RL surpasses the E2E model that   leverages both training and inference ground truth   partner personas from all the aspects .   Table 6 presents the human evaluation results   on PPG . We observe that our PPG generates per-   sonas that are more coherent and interesting than   the ground truth , which align with the facts ob-   served in Section 5.4 and Section 5.5 indicating   that our generated partner personas are more coher-   ent and diverse .   5.7 Ablation Study   We conduct an ablation study on P C - as reported in Table 7 to present the perfor-   mance of our framework when one of the compo-   nents is frozen during RL . The result indicates that   our proposed framework yields the best result when   both of the components are actively trained under   RL . We also notice that scaling the RL reward for   either PPG or DRG by 10 leads to minor decrease   in the performance . Further scaling deteriorates the   quality of response generation .   6 Conclusion   Our novel framework incorporates partner personas   generation into dialogue response generation . It   effectively mitigates the problem that partner per-   sonas are not available in practical applications as   well as the cold start problem during early conver-   sation . The experimental results with both auto-   matic and human evaluation demonstrate that our   framework generates coherent , diverse , interesting   and engaging partner personas , even compared to   the ground truth partner personas . We employ re-   inforcement learning with a dedicatedly designed   critic network that boosts the response generation   by conditioning on the generated personas . Auto-   matic and human evaluation results indicate that   our response generator surpasses our competitive   baselines that condition on the ground truth partner   personas . Extensive case studies demonstrate that   our framework can generate satisfying dialogue   responses and partner personas.5207Acknowledgments   This research / paper was supported by the Center   for Perceptual and Interactive Intelligence ( CPII )   Ltd under the Innovation and Technology Commis-   sion ’s InnoHK scheme .   References520852095210   A Ethics Statement   The P C dataset used in this work is   well - known and widely used . In our view , there is   no known ethical issue with its usage . Large - scale   pre - trained models are also employed , but they are   widely known to be subject to potential problems   such as generating offensiveness context . With its   use , our partner personas generator could generate   unseen personas , which are also subject to potential   offensive generation . An offensiveness check can   be incorporated to alleviate this problem for actual   usage ( Baheti et al . , 2021 ) .   B Implementation Details   For supervised phase , we set Adam ( Kingma and   Ba , 2015 ) as our optimizer , with hyperparameters   η= 5e−4,β= 0.9,β= 0.999,ϵ= 1e−8 . The   models are fine - tuned for 2 epochs . For RL phase ,   we set Adam as our optimizer , with η= 5e−6 ,   β= 0.9,β= 0.999,ϵ= 1e−8 . We update the   model parameters every 20training instances and   validate the model performance every 50updates .   DistilBERT ( Sanh et al . , 2019 ) is used to initialize   the model parameters for the critic network . We   set Adam as our optimizer , with hyperparameters   η= 5e−6,β= 0.9,β= 0.999,ϵ= 1e−8 .   We fine - tune the critic for 1 epoch , and we freeze   it empirically during RL . All the experiments are   conducted based on the T library   from H ( Wolf et al . , 2020 ) .   C Analysis on Dialogue Reponse   Generation   We present the progressive change of the testing   perplexity for DRG and PPG on P C - in Figure 4.We observe that they improvesimultaneously , which supports our motivation to   use RL for joint training .   D Human Evaluation Criteria   •(Appropriateness ) : " Who is more appropri-   ate given the previous dialogue context ? "   •(Informativeness ) : " Who is more diverse in-   stead of null answers such as I do not know ? "   •(Engagingness ) : " Who would you prefer to   talk with for a long conversation ? "   •(Human - likeness ) : " Which speaker do you   think sounds more like a real person ? "   •(Coherence ) : " Which persona contains traits   that are more coherent to each other ? "   •(Interestingness ) : " Which persona is more   interesting and diverse ? "   The first four are from the existing work ( Li et al . ,   2019 ; Zou et al . , 2021 ) and we propose the last   two for evaluating PPG . We report the first four for   DRG , and we report the last four for PPG .   E Dataset Limitations   Our work uses an off - the - shelf persona - based con-   versational dataset P C ( Zhang et al . ,   2018 ) , which is collected and built by crowdsourc-   ing to converse based on a fake set of discrete traits .   There is no personal information and hence no   ethics concern , but this might result in limited use-   fulness as there could be discrepancies between the   collected samples and real - life conversation . It is   also more expensive to collect real data . However ,   P C has been widely used by the com-   munity as a standard dataset . Many well - known   persona - based datasets suffer from the same prob-   lem ( Urbanek et al . , 2019 ) as widely known .   Although Mazaré et al . ( 2018 ) proposed a use-   ful method to collect large - scale persona - based di-   alogue datasets by extracting persona from user   comments with classifiers trained on revised per-   sonas from P C which can improve the   model performance on P C. For legal   reasons , they did not release this dataset at the time   of writing . Similarly , Zheng et al . ( 2019 ) proposed   a persona - based dialogue dataset with diversified   traits , but it is not currently online readily available .   Zhong et al . ( 2020 ) has followed the approach   suggested by Mazaré et al . ( 2018 ) to build an em-   pathetic conversation dataset based on personas.5211   However , their main focus is to investigate the im-   pact of personas on empathetic dialogue generation .   Therefore , we choose to follow the community to   investigate our method on the most well - known   dataset , P C.   F Computing Infrastructure   We run all our experiments on a single NVIDIA TI-   TAN RTX with 24 GB GPU memory . Fine - tuningthe generators for 2 epochs as we have done on our   preprocessed P C train split consumes   about 3 - 4 hours . Fine - tuning our critic classifier   for 1 epoch consumes about 1 hour . Our RL phase   consumes about 15 hours to achieve the best vali-   dation loss before being early stopped . We report   averaged results from 3 runs for our dialogue re-   sponse generation and partner personas generation   results reported in Table 1 , Table 4 and Table 7.5212