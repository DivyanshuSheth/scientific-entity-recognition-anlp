  Eyup Halit Yilmaz and Cagri Toraman   Aselsan Research Center   Ankara , Turkey   { ehyilmaz,ctoraman}@aselsan.com.tr   Abstract   Supervised training with cross - entropy loss   implicitly forces models to produce probabil-   ity distributions that follow a discrete delta dis-   tribution . Model predictions in test time are   expected to be similar to delta distributions   if the classiﬁer determines the class of an in-   put correctly . However , the shape of the pre-   dicted probability distribution can become sim-   ilar to the uniform distribution when the model   can not infer properly . We exploit this obser-   vation for detecting out - of - scope ( OOS ) utter-   ances in conversational systems . Speciﬁcally ,   we propose a zero - shot post - processing step ,   called Distance - to - Uniform ( D2U ) , exploiting   not only the classiﬁcation conﬁdence score ,   but the shape of the entire output distribution .   We later combine it with a learning procedure   that uses D2U for loss calculation in the su-   pervised setup . We conduct experiments using   six publicly available datasets . Experimental   results show that the performance of OOS de-   tection is improved with our post - processing   when there is no OOS training data , as well as   with D2U learning procedure when OOS train-   ing data is available .   1 Introduction   Automated conversational systems have recently   received attention from the research community   ( Dopierre et al . , 2021 ; Mehri et al . , 2020 ; Qin et al . ,   2021 ) . In applications such as voice assistants , Spo-   ken Language Understanding ( Young et al . , 2013 )   aims to extract meaning from the user inputs , called   utterances , in order to process and execute desired   functionalities . The task of Intent Detection , or   Intent Classiﬁcation , aims to classify user utter-   ance into a set of system - identiﬁable intents . How-   ever , supervised training of such systems can only   cover a restricted set of classes , i.e. in - scope ( INS )   classes . To enhance user experience , the task of   Out - of - Scope ( OOS ) detection ( Lin and Xu , 2019a ;   Xu et al . , 2021 ; Zhan et al . , 2021 ; Shen et al . , 2021)Figure 1 : Sample output distributions of an INS classi-   ﬁer predicting the intent of an INS and OOS utterance .   Since OOS utterances do not belong to any intent , the   prediction gets closer to the uniform distribution .   distinguishes INS utterances from those that do not   belong to the scope of the classiﬁer with dedicated   model architectures and loss functions .   Existing methods in OOS detection utilize the   classiﬁer conﬁdence score for a given utterance   with thresholding to classify highly conﬁdent pre-   dictions as INS and lower conﬁdence predictions   as OOS . However , softmax classiﬁers suffer from   overconﬁdent predictions for OOS data ( Hendrycks   and Gimpel , 2017 ) , which makes it difﬁcult to ac-   curately determine a threshold value . Conﬁdence   loss ( Lee et al . , 2018 ) mitigates this by calculat-   ing the KL Divergence between model prediction   and the uniform distribution to decrease the conﬁ-   dence for OOS input . We adapt a similar idea to   the zero - shot setup with a novel post - processing   step and exploit it jointly in the supervised setup   with a learning procedure . The joint application of   supervised D2U learning and D2U post - processing   forms a novel OOS detection pipeline .   Figure 1 illustrates output probability distribu-   tions of a classiﬁer for predicting the intent class for   an INS and OOS utterance . The classiﬁer , trained   only on INS utterances , is confused when OOS   utterance is given . The model assigns closer proba-   bilities for different classes since there is no correct   class for this OOS utterance , hence the resulting   distribution gets closer to a uniform distribution2093   than a discrete delta distribution .   Based on this observation , we propose to mea-   sure the dissimilarity from or distance to the uni-   form distribution ( D2U ) . Statistical distance cal-   culations between the prediction and uniform dis-   tribution enable the decision boundary to be more   accurate . Figure 2 illustrates possible beneﬁts of us-   ing distance to the uniform distribution with cross-   entropy . The subplot at the left shows the distribu-   tion of the number of utterances according to their   Maximum Likelihood Estimate ( MLE ) score . The   subplot at the right shows the same distribution ac-   cording to cross - entropy score between prediction   probability and uniform distribution . A decision   boundary or threshold can be easily determined us-   ing D2U ’s cross - entropy as a post - processing step   without any OOS training data .   When OOS training data is available ( Larson   et al . , 2019 ) , D2U can be used as a loss function   to minimize the distance between OOS predictions   and the uniform distribution . Such a loss function   forces OOS predictions to be less conﬁdent , and   beneﬁt D2U post - processing further . To test our   hypothesis that D2U is a useful method for OOS   detection , we answer following research questions :   •RQ1 : Does the application of D2U as a post-   processing step on INS classiﬁer predictions in-   crease OOS detection performance when there is   no OOS training data ?   •RQ2 : Does incorporating D2U into the training   procedure as a particular loss function boost per-   formance when OOS training data is available ?   •RQ3 : Is the performance of OOS detection sig-   niﬁcantly improved by D2U over existing state-   of - the - art methods?2 Related Work   We divide OOS detection studies into three cate-   gories : ( i ) Conﬁdence - based , ( ii ) representation-   based , and ( iii ) distance - based methods .   2.1 Conﬁdence - based OOS detection   Threshold - based Methods Thresholding is a com-   mon approach in OOS detection ( Larson et al . ,   2019 ; Feng et al . , 2020 ; Zhang et al . , 2020 ) , which   reﬂects the intuition that a classiﬁer output is more   conﬁdent for a sample that follows its training dis-   tribution . The overconﬁdence problem of softmax   classiﬁers ( Hendrycks and Gimpel , 2017 ) , although   less apparent in Transformer - based ( Vaswani et al . ,   2017 ) models ( Hendrycks et al . , 2020 ) , hinders   threshold - based OOS detection performance .   Post - processing Methods The overconﬁdence   problem of softmax classiﬁers is tackled by post-   processing predictions . ODIN ( Liang et al . , 2018 )   and SofterMax ( Lin and Xu , 2019b ) apply tem-   perature scaling for enlarging the conﬁdence gap   between INS and OOS instances , since INS logits   are ideally further away on the positive axis of the   softmax input . Gangal et al . ( 2020 ) utilize likeli-   hood ratios with generative classiﬁers to distinguish   OOS predictions . Our method , D2U , employs a   conﬁdence - based post - processing method .   2.2 Representation - based OOS detection   Dedicated model architectures or loss functions   help represent utterances in a high - dimensional   space suitable for OOS detection . Large Margin   Cosine Loss ( LMCL ) ensures that INS intents are   tightly clustered ( Zeng et al . , 2021a ) , so that OOS   utterances are exposed for outlier detection algo-   rithms , such as Local Outlier Factor ( Lin and Xu ,   2019a ) . Intent class embeddings ( Cavalin et al . ,   2020 ) model OOS detection as a reverse dictionary   task by mapping intent classes and utterances to the   same space . Yilmaz and Toraman ( 2020 ) propose   a feature representation mechanism that uses KL   Divergence to capture the changes in model predic-   tions during sequential processing of utterances .   In order to mitigate data scarcity , Marek et al .   ( 2021 ) propose a method to generate OOS data   with Generative Adversarial Networks . GANs are   also utilized to generate high - dimensional repre-   sentations that are hard to distinguish from that of   real utterances , providing adversarial signals to the   INS classiﬁer which increases the robustness of the   model ( Zeng et al . , 2021b ; Liang et al . , 2021).20942.3 Distance - based OOS detection   Distances and divergences are useful tools in OOS   detection , since they provide a measure of dissimi-   larity that can distinguish INS and OOS samples .   Xu et al . ( 2020 ) utilize Euclidean and Mahalanobis   distances with generative classiﬁers to identify out-   liers with Gaussian Discriminative Analysis . Ma-   halanobis distance calculated using representations   from the intermediate layers of BERT ( Devlin et al . ,   2019 ) increases OOS detection performance ( Shen   et al . , 2021 ) . Lee et al . ( 2018 ) introduce the con-   ﬁdence loss in Computer Vision for GANs that   calculates KL Divergence between the training pre-   dictions for OOS samples and uniform distribution .   The idea of measuring the distance between pre-   diction distribution and uniform distribution is uti-   lized in different learning architectures ( Lee et al . ,   2018 ; Gangal et al . , 2020 ) , but not extensively stud-   ied for OOS intent detection . Besides , we explore   various distance metrics in zero - shot OOS detec-   tion and different distance - to - uniform training pro-   cedures in supervised setup .   3 Distance - to - Uniform OOS Detection   3.1 D2U post - processing for zero - shot setup   Supervised classiﬁers trained on INS data model   the ground truth labels with a discrete delta func-   tion that corresponds to the label , given as follows .   δ(x ) = /braceleftBigg   1,ifx = c   0,otherwise(1 )   For the data instance i , cis the ground - truth   label indicating the correct class . The cross - entropy   loss between softmax model output and discrete   delta function is given as follows .   L=−1   N / summationdisplayδ(x ) log ˆP(u ) ( 2 )   Here , ˆP(u)is the output probability distribu-   tion of the model for utterance uin a batch of N   utterances , and cis the correct class label for the   given utterance . This criterion implicitly forces   the model to generate conﬁdent predictions for a   given data point with maximal conﬁdence score   assigned to the ground - truth class label , and low   prediction scores for the other classes . When an   OOS utterance is given to an intent classiﬁer that   is trained using only INS data , the classiﬁer gets   confused , i.e. , the output probability distributionis more dissimilar to a delta distribution than what   an INS utterance would result in . In other words ,   output distributions of OOS samples get closer to   the uniform distribution than that of INS samples ,   an observation that we exploit for OOS detection .   The conventional methods for OOS detection   make use of a pre - determined threshold value on   the Maximum Likelihood Estimate ( MLE ) score   assigned to the predicted label , given as follows .   OOS ( u ) = /braceleftBigg   1,ifmax(ˆP(u))<θ   0,otherwise(3 )   Here , θis a pre - deﬁned threshold value between   0 and 1 , and max(ˆP(u))is the MLE score , which   considers only the conﬁdence and ignores the shape   of the distribution . We exploit the information con-   veyed by the shape of the entire prediction distri-   bution by ﬁrst calculating a distance between the   output distribution ˆPand the uniform distribution   Ubefore applying the threshold , given as follows .   OOS ( u ) = /braceleftBigg   1,ifdst(ˆP(u),U)<θ   0,otherwise(4 )   The distance determined by the dst(.)function   between ˆP(u)andUcan be calculated with var-   ious distance metrics . We experiment with geo-   metric distance calculations , such as Euclidean   distance and Cosine distance ; as well as statisti-   cal distance calculations , such as Jensen - Shannon   distance and symmetrized Kullback - Leibler diver-   gence . The distance value calculated by the dst ( . )   function can be intuitively interpreted as the level of   conﬁdence of the model . When the distance value   is low , the model is less conﬁdent and more con-   fused , since the output distribution assigns closer   scores for each class .   This is an architecture - agnostic zero - shot post-   processing step which can be generalized to any   classiﬁcation model trained with cross - entropy loss   with no need for OOS training data . OOS detec-   tion in test time is achieved by a function of the   prediction distribution given by D2U.   3.2 Distance metrics for post - processing   We examine a number of geometric and statistical   distance measures listed as follows .   •Bray Curtis Distance ( BC ): For two probability   distributions , uandv , the Bray Curtis distance is   given as / summationtext|u−v|//summationtext|u+v|.2095   •Canberra Distance ( Cbr ): Canberra distance be-   tweenuandvis / summationtext(|u−v|/(u+v ) ) .   •Cosine Distance ( Cos ): Derived from the Cosine   similarity , the Cosine distance is formulated as   1−(u·v/||u||||v||)where||.||is theLnorm .   •Euclidean Distance ( Euc ): The Euclidean dis-   tance between uandvis given as||u−v|| .   •Hellinger Distance ( Helng ): The Hellinger dis-   tance between uandvis||√u−√v||/√   2 .   •Cross - Entropy ( CE ): Cross - Entropy is a measure   of dissimilarity between distributions uandv   given as−/summationtextulogv .   •Symmetrized KL Divergence ( KL ): The sym-   metrized Kullback - Leibler divergence is given as   [ KL(u , v ) + KL(v , u)]/2whereKL(u , v ) = /summationtextulog ( u / v ) .   •Jenshen Shannon Distance ( JS ): JS distance be-   tweenuandvisKL(u , m)/2 + KL(v , m)/2   wheremis the mean of two distributions .   3.3 D2U training for supervised setup   When OOS training data is available , we modify   the ﬁne - tuning procedure as in Figure 3 , to increase   the similarity between OOS prediction and uniform   distribution . We use pretrained BERT ( Devlin et al . ,   2019 ) as the classiﬁer network . The loss function   for INS utterances , L , is still cross - entropy be-   tween true label and prediction , given as follows .   L=−1   N / summationdisplayδ(c ) log ˆP(u ) ( 5 )   For OOS utterances , the loss L , is calculated   against the uniform distribution , given as follows .   L=1   N / summationdisplaydst(ˆP(u),U ) ( 6 )   The total loss is the weighted average over a   batch of utterances containing Nnumber of INS   utterances and Nnumber of OOS utterances ,   given as follows .   L = NL+NL   N+N(7 )   As thedst(.)function in Equation 6 , we experi-   ment with differentiable functions ; such as cross-   entropy , KL divergence , and Sinkhorn distance ( Cu-   turi , 2013 ) , named as D2U - CE , D2U - KL , and D2U-   S , respectively . These functions treat the model   output and ground truth as probability distributions   and provide a differentiable measure . We do not   modify the loss calculation for INS utterances so   as not to affect the INS classiﬁcation performance .   Note that this architecture does not model the   OOS intent as a separate class . Therefore , post-   processing is applied as described in Section 3.1   in test time . Since the loss function incorporates   D2U into training , the performance gain by the   post - processing is expected to increase .   4 Experiments   4.1 Datasets   We use six publicly available intent classiﬁcation   datasets , some of which include labeled OOS data .   We give the main statistics of the datasets in Ta-   ble 1 . CLINC ( Larson et al . , 2019 ) is a dataset   with 150 INS intent classes targeting various do-   mains with curated OOS data . We use the OOS   split of CLINC to augment other existing intent   detection datasets that do not include labeled OOS   data ; which are ACID ( Acharya and Fung , 2020 ) ,   Banking ( Casanueva et al . , 2020 ) , HWU64 ( Liu   et al . , 2019 ) , and SNIPS ( Coucke et al . , 2018 ) . We   observe that HWU64 has many short and noisy ut-   terances , we therefore remove any utterances with   length less than or equal to three words .   TOP ( Gupta et al . , 2018 ) is an intent detection   dataset that generalizes conventional intent label-   ing with semantic parsing . The intent labels fol-   low a hierarchical structure with potentially many2096labels for an utterance . However , we take only   root intent class label into account to be consis-   tent with other datasets . The utterances with the   intent labels " UNSUPPORTED " and " UNSUP-   PORTED_NA VIGATION " are treated as OOS .   The variety of the number of classes , average   length ( number of words ) , and vocabulary size   provide a wide spectrum for understanding differ-   ent OOS detection scenarios . For instance , TOP   dataset can be considered a low resource setup   since the number of OOS utterances is signiﬁcantly   lower than INS utterances .   4.2 Evaluation metrics   To assess the performance of OOS detection , we   report the scores of Receiver Operating Curve   Area Under Curve ( ROC AUC ) , False Positive   Rate at 90 % OOS True Positive Rate ( FPR90 ) ,   and False Negative Rate at 90 % OOS True Nega-   tive Rate ( FNR90 ) using sklearn ( Pedregosa et al . ,   2011 ) . These metrics are independent of the thresh-   old value used for decision boundary , providing a   means of fair comparison . We also report weighted   OOS Recall and weighted OOS F1 based on the   threshold value that maximizes the Youden ’s J   statistic ( Youden , 1950 ) on a validation set .   Compared to Precision , Recall is arguably a   more critical performance metric for OOS detec-   tion ; since Recall considers Type II error , meaning   that OOS utterances are mislabeled as INS . In this   case , the voice assistant would execute a task that   the user does not intent to do . We argue that ROC   is a more generic measure that considers the per-   formances of varying thresholds , than Recall and   F1 considering only a ﬁxed threshold .   4.3 Baseline approaches   In the experiments , BERT ( Devlin et al . , 2019 )   with softmax layer is used as the classiﬁer network .   For RQ1 , the baseline zero - shot post - processing   approaches are listed below .   •MLE ( Hendrycks and Gimpel , 2017 ; Hendrycks   et al . , 2020 ): The conﬁdence score of a classiﬁer   trained only on INS data is used for thresholding .   •Softmax temperature scaling ( Temp ) ( Liang   et al . , 2018 ; Lin and Xu , 2019b ): As a modiﬁ-   cation to the MLE setup , the softmax input is   applied a temperature value of 10 .   •Standard deviation ( Stdev ) : We use the stdev   of the distribution before thresholding since OOS   predictions would have lower standard deviation .   •Entropy ( Ent ) ( Shen et al . , 2021 ): The entropy   of the prediction distribution , H(ˆP(u ) ) , is cal-   culated before applying the threshold , as follows .   OOS ( u ) = /braceleftBigg   1,ifH(ˆP(u))>θ   0,otherwise(8 )   For RQ2 , we use D2U zero - shot cross - entropy   post - processing ( D2U - zero ) as the baseline method ,   since we examine any improvement in supervised   setup over zero - shot . For RQ3 , we compare super-   vised D2U with the following baselines .   •Large Margin Cosine Loss ( LMCL ) ( Zeng   et al . , 2021b ): Cosine distance among INS class   centroids is increased up to a margin . We set the   margin as 0.35 , and scaling factor as 30 .   •Domain Regularization Module ( DRM ) ( Shen   et al . , 2021 ): DRM introduces domain logits for   regularization during INS training . We slightly   modify the design and apply sigmoid to domain   logits before dividing the classiﬁcation logits for   training stability .   •BERT - Binary ( Binary ) ( Devlin et al . , 2019 ):   The " bert - base - uncased " model ﬁne - tuned as a   binary classiﬁer for OOS detection .   •Entropy Regularization ( Reg . ) ( Zheng et al . ,   2020 ): Entropy of OOS predictions are maxi-   mized while minimizing INS training loss .   4.4 Experimental design   To avoid potential annotator - dependent effects as   noted by Larson et al . ( 2019 ) and comply with the   original splits , we modify 10 - fold leave - one - out   cross - validation as illustrated in Figure 4 . The vali-   dation splits are used to ﬁnd conﬁdence threshold   values for Recall and F1 calculations . We validate   statistically signiﬁcant differences in the average   performances of 10 - folds with the two - tailed paired   t - test at a 95 % interval with Bonferroni correction.2097   Note that the test splits do not overlap in order to   satisfy the independence criterion of t - test .   The experiments are designed with respect to   our research questions ( RQ 1 - 3 ) . First , we ﬁne-   tune a BERT classiﬁer ( Devlin et al . , 2019 ) using   huggingface implementation ( Wolf et al . , 2019 ) for   INS intent detection with cross - entropy loss , and   apply different D2U post - processing methods for   RQ1 . Then , we ﬁx the post - processing method ,   and examine the effect of supervised D2U training   for RQ2 . Lastly , we compare D2U with state - of-   the art baselines for RQ3 to assess the performance   gain of our method .   4.5 Experimental results   RQ1 : D2U in zero - shot setup . In Table 2 , we re-   port ROC AUC , FPR90 , and FNR90 scores for dif-   ferent post - processing methods applied to a BERT-   based INS classiﬁer with no OOS training data .   Our proposed method , D2U - zero , statistically sig-   niﬁcantly outperforms all baselines in all datasets   with respect to ROC AUC score . Using cross-   entropy for D2U - zero has better performance in   majority of cases , compared to other distance met-   rics . The reason for its success might be that cross-   entropy is the loss function used in the training   procedure of the model . In terms of FPR90 and   FNR90 , D2U - zero does not always outperform all   baselines . Though , the cases when baselines out-   perform are not statistically signiﬁcant . This shows   that the baseline methods can optimize FPR90 and   FNR90 individually but can not outperform D2U in   terms of ROC which considers Type I and Type II   errors simultaneously . Entropy ( Shen et al . , 2021 )   is a strong baseline that performs better than other   baselines with respect to all performance metrics .   RQ2 : D2U in supervised setup . Next , we   report the effect of D2U training on OOS detec-   tion in Table 3 . Since our concern here is to ob-2098   serve any improvement over zero - shot setup , we   ﬁx post - processing method as cross - entropy for all   methods due to its performance in the previous   experiment . The results show that using D2U as   a loss function statistically signiﬁcantly improves   the performance of D2U - zero in almost all cases .   KL divergence loss ( D2U - KL ) and Cross - Entropy   loss ( D2U - CE ) are effective D2U methods in all   datasets , except that Sinkhorn distance ( D2U - S )   is effective in CLINC dataset . The choice of loss   function is a hyperparameter that can be tuned ac-   cording to speciﬁc use cases and datasets .   RQ3 : D2U versus state - of - the - art . The per-   formances of state - of - the - art baseline OOS detec-   tion models , regardless of zero - shot or supervised ,   and D2U methods are compared in Table 4 , with   extensive results reported in the Appendix . MLE ,   softmax temperature ( Temp . ) , Entropy , LMCL , and   DRM are zero - shot OOS detection setups , whereas   entropy regularization ( Reg . ) and BERT - Binary   ( Binary ) are supervised setups . D2U statistically   signiﬁcantly outperforms most baselines , although   Binary is a strong baseline method that outperforms   D2U in ACID and Banking datasets and challenges   it in HWU64 and TOP , which is not statistically sig-   niﬁcant . The reason for this might be the prevalent   domain difference between INS and OOS utter-   ances in ACID , Banking and TOP datasets ; whichbelong to the insurance , banking , and navigation   applications , respectively . It causes a trivial detec-   tion for the BERT - based binary classiﬁer . HWU64   contains generic utterances like queries and ques-   tions which may coincide with the OOS split and   disturb the training process of D2U. The combi-   nation of D2U training and D2U post - processing   demonstrates its advantage in CLINC where INS   and OOS utterances span a wide spectrum .   5 Discussion   5.1 Domain analysis   To validate our hypothesis that domain - speciﬁc   datasets provide an advantage to the Binary method ,   we apply UMAP ( Becht et al . , 2019 ) dimension   reduction on the CLS embeddings of Binary and   D2U CE models and plot them in Figure 5 . It is ap-   parent that the OOS utterances are separated from   INS utterances when there is a clear domain differ-   ence as in ACID and Banking . However , when this   separation becomes fuzzy , Binary fails to properly   distinguish INS and OOS utterances as in CLINC .   There is also an overlapping set of INS and OOS   utterances in SNIPS for Binary .   In D2U - CE plots , the clusters of INS intents are   easily identiﬁable since the model is trained for   intent detection , however , OOS utterances do not2099   form a separate cluster . We see that the training   procedure does not necessarily enforce a cluster-   ing on the OOS embeddings since the model is   trained to output a uniform distribution for OOS in-   puts . D2U achieves competitive performance even   though there are overlapping embeddings of INS   and OOS utterances , highlighting the importance   of D2U post - processing in the supervised setup .   5.2 Qualitative analysis   We provide a qualitative analysis on the effect   of D2U training . We illustrate the model out-   put distributions for INS utterance " get me to   ritzville by 4 via the freeway . " belonging to the   " GET_DIRECTIONS " intent , and the OOS utter-   ance " how many skating rinks are available in the   south paciﬁc tomorrow at 10 " taken from the TOP   dataset in Figure 6 . We observe that the OOS utter-   ance results in an overconﬁdent prediction in the   BERT MLE model whereas the prediction distribu-   tion of D2U - CE is similar to uniform distribution .   5.3 INS performance   In Table 5 , we analyze if OOS detection models   deteriorate INS performance . MLE baseline does   not modify the training procedure . The results   show that INS classiﬁcation performance is not   dramatically deteriorated by the supervised models   including D2U in SNIPS and TOP , whereas it is   even improved in the remaining datasets . Although   D2U ’s INS performance is similar to other super-   vised models , D2U has better OOS performance   than others , as observed in Table 4 . The reason for   the increase in INS detection performance could   be the regularization signal provided by the OOS   loss as observed by Shen et al . ( 2021 ) . Note that   this effect becomes more apparent when domain   difference is prevalent ( in ACID and Banking ) .   We do not include Binary , which has no capabil-   ity of INS classiﬁcation . Binary has a challenging   OOS performance in Table 4 , but D2U has advan-   tage of showing state - of - the - art performances for   both INS and OOS detection .   5.4 Limitations   We acknowledge some limitations to our study . Ex-   cept for CLINC and TOP , the datasets we use are   augmented with the OOS data from CLINC . How-   ever , we argue that the majority of the data remains   OOS for other datasets since it is sampled from   Wikipedia ( Larson et al . , 2019 ) . Moreover , D2U   has effective performance on CLINC and TOP   datasets which are designed with OOS utterances .   We leave the selection of the distance metric in   post - processing and supervised learning as a hyper-   parameter of D2U. In the results , this might pro-   vide an advantage to D2U in comparisons since we   do not apply hyperparameter tuning for baselines .   However , we use default or suggested parameter   settings for baselines . We adopt transparency in   reporting the results that are also detailed in Ap-   pendix .   Zero - shot D2U post - processing emphasizes the   distinction between INS and OOS utterances when   conﬁdence score becomes misleading . However ,   D2U struggles in the ultimate case where an OOS   utterance is mapped to an INS class with ~100 %   conﬁdence ( see Figure 6 BERT MLE ) . Nonethe-   less , D2U suffers from such overconﬁdent predic-   tions less than existing methods ( see Table 2 ) .   5.5 Ethical considerations   We list a number of ethical concerns related to envi-   ronmental impact , explainability , and transparency   in this section . We employ BERT ﬁne - tuning with2100small modiﬁcations , therefore the environmental   impact can be considered small . Our work focuses   on well - known OOS detection task with established   use cases , therefore there would be no risk for un-   intended use . We use publicly available datasets   with licences suitable for academic research .   To assure explainability and transparency , we   report the length of utterances and domains of   datasets in Section 4.1 . We report the statistics   of datasets and ﬁguratively report the split strategy   used in the experiments in Section 4.1 . There are   two setups in our study . The zero - shot setting does   not include any training . In the supervised setup ,   the complexity of our method is quite similar to   regular ﬁne - tuning procedure of BERT . The thresh-   olding hyperparameter is decided by maximizing   the Youden ’s J statistic as explained in Section 4.2 .   In Section 4.3 , we also report the hyperparameters   of the baseline methods . We employ a modiﬁed   10 - fold cross - validation strategy as explained in   Section 4.4 and apply t - test with Bonferroni correc-   tion to all experimental results .   6 Conclusion   We propose an OOS detection pipeline with a dis-   tance calculation between classiﬁer prediction and   uniform distribution , called D2U. In the zero - shot   setup , D2U serves as an architecture - agnostic post-   processing step to emphasize the distinction be-   tween INS and OOS . In the supervised setup , we   bring closer OOS predictions to uniform distribu-   tion with a modiﬁed loss function . Experimental   results , supported by statistical tests , show that   D2U outperforms existing baselines in zero - shot ,   and has challenging performance in the supervised   setup . We plan to extend our study to different   architectures and deep learning tasks in the future .   References21012102   A Appendix   We report Receiver Operating Curve Area Under   Curve , False Positive Rate at 90 % OOS True Pos-   itive Rate , False Negative Rate at 90 % OOS TrueNegative Rate , weighted OOS Recall , and weighted   OOS F1 scores in Tables 6 , 7 , 8 , 9 , 10 respec-   tively . Different training procedures , baseline and   proposed , are reported in rows and different post-   processing methods , baseline and proposed , are   reported in columns .   Baseline training methods are BERT - based in-   scope classiﬁer ( MLE ) ( Larson et al . , 2019 ; Devlin   et al . , 2019 ) , Large Margin Cosine Loss ( LMCL )   ( Zeng et al . , 2021a ) , Domain Regularization Mod-   ule ( DRM ) ( Shen et al . , 2021 ) , entropy regular-   ization ( Reg . ) ( Zheng et al . , 2020 ) , and BERT-   binary classiﬁer ( Binary ) ( Devlin et al . , 2019 ) . Post-   processing methods are not applicable for Binary   training since it models OOS detection as a binary   classiﬁcation problem . Baseline post - processing   methods are Maximum Likelihood Estimate ( MLE )   ( Gangal et al . , 2020 ; Zhang et al . , 2020 ) , softmax   temperature ( Temp ) ( Liang et al . , 2018 ; Lin and   Xu , 2019b ) , standard deviation ( Stdev ) , and entropy   ( Ent ) ( Shen et al . , 2021).210321042105210621072108