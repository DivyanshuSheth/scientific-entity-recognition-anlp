  Michihiro Yasunaga Jure LeskovecPercy Liang   Stanford University   Abstract   Language model ( LM ) pretraining captures vari-   ous knowledge from text corpora , helping down-   stream NLP tasks . However , existing methods   such as BERT model a single document , failing   to capture document dependencies and knowl-   edge that spans across documents . In this work ,   we propose LinkBERT , an effective LM pretrain-   ing method that incorporates document links ,   such as hyperlinks . Given a pretraining corpus ,   we view it as a graph of documents , and create   LM inputs by placing linked documents in the   same context . We then train the LM with two   joint self - supervised tasks : masked language   modeling and our newly proposed task , docu-   ment relation prediction . We study LinkBERT   in two domains : general domain ( pretrained on   Wikipedia with hyperlinks ) and biomedical do-   main ( pretrained on PubMed with citation links ) .   LinkBERT outperforms BERT on various down-   stream tasks in both domains . It is especially   effective for multi - hop reasoning and few - shot   QA ( +5 % absolute improvement on HotpotQA   and TriviaQA ) , and our biomedical LinkBERT   attains new state - of - the - art on various BioNLP   tasks ( +7 % on BioASQ and USMLE ) . We   release the pretrained models , LinkBERT and   BioLinkBERT , as well as code and data .   1 Introduction   Pretrained language models ( LMs ) , like BERT and   GPTs ( Devlin et al . , 2019 ; Brown et al . , 2020 ) , have   shown remarkable performance on many natural   language processing ( NLP ) tasks , such as text   classification and question answering ( Raffel et al . ,   2020 ) , becoming the foundation of modern NLP   systems . By performing self - supervised learning   on text , such as masked language modeling ( Devlin   et al . , 2019 ) , LMs learn to encode various knowl-   edge from text corpora and produce informative   language representations for downstream tasks   ( Bosselut et al . , 2019 ; Bommasani et al . , 2021).Figure 1 :   However , existing LM pretraining methods typ-   ically consider a single document in each input con-   text ( Liu et al . , 2019 ; Joshi et al . , 2020 ) , and do not   model links between documents . This can pose lim-   itations because documents often have rich depen-   dencies with each other ( e.g. hyperlinks , references ) ,   and knowledge can span across documents . As a   simple example , in Figure 1 , the Wikipedia article   “ Tidal Basin , Washington D.C. ” ( left ) describes that   the basin hosts “ National Cherry Blossom Festival ” ,   and the hyperlinked article ( right ) reveals the back-   ground that the festival celebrates “ Japanese cherry   trees ” . Taken together , the hyperlink offers new ,   multi - hop knowledge “ Tidal Basin has Japanese   cherry trees ” , which is not available in the single ar-   ticle “ Tidal Basin ” alone . Acquiring such multi - hop   knowledge in pretraining could be useful for various   applications including question answering . In fact ,   document links like hyperlinks and references are   ubiquitous ( e.g. web , books , scientific literature ) ,   and guide how we humans acquire knowledge and8003   even make discoveries too ( Margolis et al . , 1999 ) .   In this work , we propose LinkBERT , an effective   language model pretraining method that incor-   porates document link knowledge . Given a text   corpus , we obtain links between documents such as   hyperlinks , and create LM inputs by placing linked   documents in the same context window , besides   the existing option of placing a single document or   random documents as in BERT . Specifically , as in   Figure 2 , after sampling an anchor text segment , we   place either ( 1 ) the contiguous segment from the   same document , ( 2 ) a random document , or ( 3 ) a   document linked from anchor segment , as the next   segment in the input . We then train the LM with two   joint objectives : We use masked language modeling   ( MLM ) to encourage learning multi - hop knowledge   of concepts brought into the same context by docu-   ment links ( e.g. “ Tidal Basin ” and “ Japanese cherry ”   in Figure 1 ) . Simultaneously , we propose a Doc-   ument Relation Prediction ( DRP ) objective , which   classifies the relation of the second segment to the   first segment ( contiguous , random , orlinked ) . DRP   encourages learning the relevance and bridging con-   cepts ( e.g. “ National Cherry Blossom Festival ” ) be-   tween documents , beyond the ability learned in the   vanilla next sentence prediction objective in BERT .   Viewing the pretraining corpus as a graph   of documents , LinkBERT is also motivated as   self - supervised learning on the graph , where DRP   and MLM correspond to link prediction and node   feature prediction in graph machine learning ( Yang   et al . , 2015 ; Hu et al . , 2020 ) . Our modeling approach   thus provides a natural fusion of language - based   and graph - based self - supervised learning .   We train LinkBERT on two domains : the general   domain , using Wikipedia articles with hyperlinks   ( § 4 ) , and the biomedical domain , using PubMed   articles with citation links ( § 6 ) . We then evaluate   the pretrained models on a wide range of down-   stream tasks including question answering , in bothdomains . LinkBERT consistently improves on base-   line LMs across domains and tasks . For the general   domain , LinkBERT outperforms BERT on MRQA   benchmark ( +4 % absolute in F1 - score ) as well as   GLUE benchmark . For the biomedical domain ,   LinkBERT exceeds PubmedBERT ( Gu et al . , 2020 )   and attains new state - of - the - art on BLURB biomed-   ical NLP benchmark ( +3 % absolute in BLURB   score ) and MedQA - USMLE reasoning task ( +7 %   absolute in accuracy ) . Overall , LinkBERT attains   notably large gains for multi - hop reasoning , multi-   document understanding , and few - shot question   answering , suggesting that LinkBERT internalizes   significantly more knowledge than existing LMs   by pretraining with document link information .   2 Related work   Retrieval - augmented LMs . Several works   ( Lewis et al . , 2020b ; Karpukhin et al . , 2020 ;   Oguz et al . , 2020 ; Xie et al . , 2022 ) introduce a   retrieval module for LMs , where given an anchor   text ( e.g. question ) , retrieved text is added to the   same LM context to improve model inference   ( e.g. answer prediction ) . These works show the   promise of placing related documents in the same   LM context at inference time , but they do not study   pretraining . Guu et al . ( 2020 ) pretrain an LM with   a retriever that learns to retrieve text for answering   masked tokens in the anchor text . In contrast ,   our focus is not on retrieval , but on pretraining a   general - purpose LM that internalizes knowledge   that spans across documents , which is orthogonal   to the above works ( e.g. , our pretrained LM could   be used to initialize the LM component of these   works ) . Additionally , we focus on incorporating   document links such as hyperlinks , which can offer   salient knowledge that common lexical retrieval   methods may not provide ( Asai et al . , 2020 ) .   Pretrain LMs with related documents . Several   concurrent works use multiple related documents8004to pretrain LMs . Caciularu et al . ( 2021 ) place doc-   uments ( news articles ) about the same topic into the   same LM context , and Levine et al . ( 2021 ) place sen-   tences of high lexical similarity into the same con-   text . Our work provides a general method to incor-   porate document links into LM pretraining , where   lexical or topical similarity can be one instance of   document links , besides hyperlinks . We focus on hy-   perlinks in this work , because we find they can bring   in salient knowledge that may not be obvious via   lexical similarity , and yield a more performant LM   ( § 5.5 ) . Additionally , we propose the DRP objective ,   which improves modeling multiple documents and   relations between them in LMs ( § 5.5 ) .   Hyperlinks and citation links for NLP . Hyper-   links are often used to learn better retrieval models .   Chang et al . ( 2020 ) ; Asai et al . ( 2020 ) ; Seonwoo   et al . ( 2021 ) use Wikipedia hyperlinks to train   retrievers for open - domain question answering .   Ma et al . ( 2021 ) study various hyperlink - aware   pretraining tasks for retrieval . While these works   use hyperlinks to learn retrievers , we focus on using   hyperlinks to create better context for learning   general - purpose LMs . Separately , Calixto et al .   ( 2021 ) use Wikipedia hyperlinks to learn multilin-   gual LMs . Citation links are often used to improve   summarization and recommendation of academic   papers ( Qazvinian and Radev , 2008 ; Yasunaga et al . ,   2019 ; Bhagavatula et al . , 2018 ; Khadka et al . , 2020 ;   Cohan et al . , 2020 ) . Here we leverage citation net-   works to improve pretraining general - purpose LMs .   Graph - augmented LMs . Several works aug-   ment LMs with graphs , typically , knowledge graphs   ( KGs ) where the nodes capture entities and edges   their relations . Zhang et al . ( 2019 ) ; He et al . ( 2020 ) ;   Wang et al . ( 2021b ) combine LM training with   KG embeddings . Sun et al . ( 2020 ) ; Yasunaga et al .   ( 2021 ) ; Zhang et al . ( 2022 ) combine LMs and graph   neural networks ( GNNs ) to jointly train on text and   KGs . Different from KGs , we use document graphs   to learn knowledge that spans across documents .   3 Preliminaries   A language model ( LM ) can be pretrained from   a corpus of documents , X={X } . An LM is   a composition of two functions , f(f(X ) ) ,   where the encoder ftakes in a sequence of tokens   X= ( x , x, ... ,x)and produces a contextualized   vector representation for each token , ( h , h, ... ,h ) .   The head ftypically uses these representations   to perform self - supervised tasks in the pretraining   step , and perform particular downstream tasks in   the fine - tuning step . We build on BERT ( Devlinet al . , 2019 ) , which pretrains an LM with the   following two self - supervised tasks .   Masked language modeling ( MLM ) . Given a   sequence of tokens X , a subset of tokens Y⊆X   is masked , and the task is to predict the original   tokens from the modified input . Yaccounts for   15 % of the tokens in X ; of those , 80 % are replaced   with [ MASK ] , 10 % with a random token , and 10 %   are kept unchanged .   Next sentence prediction ( NSP ) . The NSP task   takes two text segments(X , X)as input , and   predicts whether Xis the direct continuation of   X. Specifically , BERT first samples Xfrom the   corpus , and then either ( 1 ) takes the next segment   Xfrom the same document , or ( 2 ) samples X   from a random document in the corpus . The two   segments are joined via special tokens to form   an input instance , [ CLS ] X[SEP ] X[SEP ] ,   where the prediction target of [ CLS ] is whether X   indeed follows X(contiguous orrandom ) .   In this work , we will further incorporate docu-   ment link information into LM pretraining . Our   approach ( § 4 ) will build on MLM and NSP .   4 LinkBERT   We present LinkBERT , a self - supervised pretraining   approach that aims to internalize more knowledge   into LMs using document link information .   Specifically , as shown in Figure 2 , instead of   viewing the pretraining corpus as a set of documents   X={X } , we view it as a graph of documents ,   G= ( X , E ) , where E={(X , X)}denotes   links between documents ( § 4.1 ) . The links can   be existing hyperlinks , or could be built by other   methods that capture document relevance . We   then consider pretraining tasks for learning from   document links ( § 4.2 ): We create LM inputs by   placing linked documents in the same context   window , besides the existing options of a single   document or random documents . We use the MLM   task to learn concepts brought together in the con-   text by document links , and we also introduce the   Document Relation Prediction ( DRP ) task to learn   relations between documents . Finally , we discuss   strategies for obtaining informative pairs of linked   documents to feed into LM pretraining ( § 4.3 ) .   4.1 Document graph   Given a pretraining corpus , we link related docu-   ments so that the links can bring together knowledge   that is not available in single documents . We focus8005on hyperlinks , e.g. , hyperlinks of Wikipedia articles   ( § 5 ) and citation links of academic articles ( § 6 ) . Hy-   perlinks have a number of advantages . They provide   background knowledge about concepts that the doc-   ument writers deemed useful — the links are likely   to have high precision of relevance , and can also   bring in relevant documents that may not be obvious   via lexical similarity alone ( e.g. , in Figure 1 , while   the hyperlinked article mentions “ Japanese ” and   “ Yoshino ” cherry trees , these words do not appear in   the anchor article ) . Hyperlinks are also ubiquitous   on the web and easily gathered at scale ( Aghajanyan   et al . , 2021 ) . To construct the document graph , we   simply make a directed edge ( X , X)if there is   a hyperlink from document Xto document X.   For comparison , we also experiment with a docu-   ment graph built by lexical similarity between docu-   ments . For each document X , we use the common   TF - IDF cosine similarity metric ( Chen et al . , 2017 ;   Yasunaga et al . , 2017 ) to obtain top- kdocuments   X ’s and make edges ( X , X ) . We use k=5 .   4.2 Pretraining tasks   Creating input instances . Several works ( Gao   et al . , 2021 ; Levine et al . , 2021 ) find that LMs can   learn stronger dependencies between words that   were shown together in the same context during   training , than words that were not . To effectively   learn knowledge that spans across documents , we   create LM inputs by placing linked documents in   the same context window , besides the existing op-   tion of a single document or random documents .   Specifically , we first sample an anchor text segment   from the corpus ( Segment A ; X⊆X ) . For the   next segment ( Segment B ; X ) , we either ( 1 ) use   the contiguous segment from the same document   ( X⊆X ) , ( 2 ) sample a segment from a random   document ( X⊆Xwhere j̸=i ) , or ( 3 ) sample a   segment from one of the documents linked from Seg-   ment A ( X⊆Xwhere ( X , X)∈ E ) . We   then join the two segments via special tokens to form   an input instance : [ CLS ] X[SEP ] X[SEP ] .   Training objectives . To train the LM , we use   two objectives . We apply the MLM objective to   encourage the LM to learn multi - hop knowledge   of concepts brought together in the same context   by document links . We also propose a Document   Relation Prediction ( DPR ) objective , which clas-   sifies the relation rof segment Xto segment X   ( r∈{contiguous , random , linked } ) . By distinguish-   inglinked from contiguous andrandom , DRP en-   courages the LM to learn the relevance and existence   of bridging concepts between documents , besides   the capability learned in the vanilla NSP objective . To predict r , we use the representation of [ CLS ]   token , as in NSP . Taken together , we optimize :   L = L+L ( 1 )   = ( 2 )   where xis each token of the input instance , [ CLS ]   X[SEP ] X[SEP ] , andhis its representation .   Graph machine learning perspective . Our   two pretraining tasks , MLM and DRP , are also   motivated as graph self - supervised learning on the   document graph . In graph self - supervised learning ,   two types of tasks , node feature prediction and   link prediction , are commonly used to learn the   content and structure of a graph . In node feature   prediction ( Hu et al . , 2020 ) , some features of a node   are masked , and the task is to predict them using   neighbor nodes . This corresponds to our MLM   task , where masked tokens in Segment A can be   predicted using Segment B ( a linked document   on the graph ) , and vice versa . In link prediction   ( Bordes et al . , 2013 ; Wang et al . , 2021a ) , the task is   to predict the existence or type of an edge between   two nodes . This corresponds to our DRP task ,   where we predict if the given pair of text segments   are linked ( edge ) , contiguous ( self - loop edge ) , or   random ( no edge ) . Our approach can be viewed as   a natural fusion of language - based ( e.g. BERT ) and   graph - based self - supervised learning .   4.3 Strategy to obtain linked documents   As described in § 4.1 , § 4.2 , our method builds links   between documents , and for each anchor segment ,   samples a linked document to put together in the LM   input . Here we discuss three key axes to consider   to obtain useful linked documents in this process .   Relevance . Semantic relevance is a requisite   when building links between documents . If links   were randomly built without relevance , LinkBERT   would be same as BERT , with simply two options of   LM inputs ( contiguous orrandom ) . Relevance can   be achieved by using hyperlinks or lexical similarity   metrics , and both methods yield substantially better   performance than using random links ( § 5.5 ) .   Salience . Besides relevance , another factor to con-   sider ( salience ) is whether the linked document can   offer new , useful knowledge that may not be obvious   to the current LM . Hyperlinks are potentially more   advantageous than lexical similarity links in this   regard : LMs are shown to be good at recognizing   lexical similarity ( Zhang et al . , 2020 ) , and hyper-   links can bring in useful background knowledge that8006may not be obvious via lexical similarity alone ( Asai   et al . , 2020 ) . Indeed , we empirically find that using   hyperlinks yields a more performant LM ( § 5.5 ) .   Diversity . In the document graph , some docu-   ments may have a very high in - degree ( e.g. , many   incoming hyperlinks , like the “ United States ” page   of Wikipedia ) , and others a low in - degree . If we uni-   formly sample from the linked documents for each   anchor segment , we may include documents of high   in - degree too often in the overall training data , los-   ing diversity . To adjust so that all documents appear   with a similar frequency in training , we sample a   linked document with probability inversely propor-   tional to its in - degree , as done in graph data mining   literature ( Henzinger et al . , 2000 ) . We find that this   technique yields a better LM performance ( § 5.5 ) .   5 Experiments   We experiment with our proposed approach in the   general domain first , where we pretrain LinkBERT   on Wikipedia articles with hyperlinks ( § 5.1 ) and   evaluate on a suite of downstream tasks ( § 5.2 ) . We   compare with BERT ( Devlin et al . , 2019 ) as our base-   line . We experiment in the biomedical domain in § 6 .   5.1 Pretraining setup   Data . We use the same pretraining corpus used   by BERT : Wikipedia and BookCorpus ( Zhu et al . ,   2015 ) . For Wikipedia , we use the WikiExtractorto   extract hyperlinks between Wiki articles . We then   create training instances by sampling contiguous ,   random , orlinked segments as described in § 4 , with   the three options appearing uniformly ( 33 % , 33 % ,   33 % ) . For BookCorpus , we create training instance   by sampling contiguous orrandom segments ( 50 % ,   50 % ) as in BERT . We then combine the training   instances from Wikipedia and BookCorpus to train   LinkBERT . In summary , our pretraining data is   the same as BERT , except that we have hyperlinks   between Wikipedia articles .   Implementation . We pretrain LinkBERT of   three sizes , -tiny , -base and -large , following the   configurations of BERT ( 4.4 M parameters ) ,   BERT(110 M params ) , and BERT ( 340 M   params ) ( Devlin et al . , 2019 ; Turc et al . , 2019 ) . We   use -tiny mainly for ablation studies .   For -tiny , we pretrain from scratch with ran-   dom weight initialization . We use the AdamW   ( Loshchilov and Hutter , 2019 ) optimizer with   ( β , β ) = ( 0 .9,0.98 ) , warm up the learning rate   for the first 5,000 steps and then linearly decay it . We train for 10,000 steps with a peak learning rate   5e-3 , weight decay 0.01 , and batch size of 2,048   sequences with 512 tokens . Training takes 1 day   on two GeForce RTX 2080 Ti GPUs with fp16 .   For -base , we initialize LinkBERT with the   BERT checkpoint released by Devlin et al .   ( 2019 ) and continue pretraining . We use a peak   learning rate 3e-4 and train for 40,000 steps . Other   training hyperparameters are the same as -tiny .   Training takes 4 days on four A100 GPUs with fp16 .   For -large , we follow the same procedure as -base ,   except that we use a peak learning rate of 2e-4 . Train-   ing takes 7 days on eight A100 GPUs with fp16 .   Baselines . We compare LinkBERT with BERT .   Specifically , for the -tiny scale , we compare with   BERT , which we pretrain from scratch with the   same hyperparameters as LinkBERT . The only   difference is that LinkBERT uses document links   to create LM inputs , while BERT does not .   For -base scale , we compare with BERT , for   which we take the BERTrelease by Devlin et al .   ( 2019 ) and continue pretraining it with the vanilla   BERT objectives on the same corpus for the same   number of steps as LinkBERT .   For -large , we follow the same procedure as -base .   5.2 Evaluation tasks   We fine - tune and evaluate LinkBERT on a suite of   downstream tasks .   Extractive question answering ( QA ) . Given a   document ( or set of documents ) and a question as   input , the task is to identify an answer span from   the document . We evaluate on six popular datasets   from the MRQA shared task ( Fisch et al . , 2019 ):   HotpotQA ( Yang et al . , 2018 ) , TriviaQA ( Joshi   et al . , 2017 ) , NaturalQ ( Kwiatkowski et al . , 2019 ) ,   SearchQA ( Dunn et al . , 2017 ) , NewsQA ( Trischler   et al . , 2017 ) , and SQuAD ( Rajpurkar et al . , 2016 ) .   As the MRQA shared task does not have a public   test set , we split the dev set in half to make new   dev and test sets . We follow the fine - tuning method   BERT ( Devlin et al . , 2019 ) uses for extractive QA .   More details are provided in Appendix B.   GLUE . The General Language Understanding   Evaluation ( GLUE ) benchmark ( Wang et al . , 2018 )   is a popular suite of sentence - level classification   tasks . Following BERT , we evaluate on CoLA   ( Warstadt et al . , 2019 ) , SST-2 ( Socher et al . , 2013 ) ,   MRPC ( Dolan and Brockett , 2005 ) , QQP , STS - B   ( Cer et al . , 2017 ) , MNLI ( Williams et al . , 2017 ) ,   QNLI ( Rajpurkar et al . , 2016 ) , and RTE ( Dagan   et al . , 2005 ; Haim et al . , 2006 ; Giampiccolo8007   et al . , 2007 ) , and report the average score . More   fine - tuning details are provided in Appendix B.   5.3 Results   Table 1 shows the performance ( F1 score ) on   MRQA datasets . LinkBERT substantially outper-   forms BERT on all datasets . On average , the gain is   +4.1 % absolute for the BERTscale , +2.6 % for   the BERTscale , and +2.5 % for the BERT   scale . Table 2 shows the results on GLUE , where   LinkBERT performs moderately better than BERT .   These results suggest that LinkBERT is especially   effective at learning knowledge useful for QA tasks   ( e.g. world knowledge ) , while keeping performance   on sentence - level language understanding too .   5.4 Analysis   We further study when LinkBERT is especially   useful in downstream tasks . Improved multi - hop reasoning . In Table 1 ,   we find that LinkBERT obtains notably large   gains on QA datasets that require reasoning with   multiple documents , such as HotpotQA ( +5 % over   BERT ) , TriviaQA ( +6 % ) and SearchQA ( +8 % ) ,   as opposed to SQuAD ( +1.4 % ) which just has   a single document per question . To further gain   qualitative insights , we studied in what QA exam-   ples LinkBERT succeeds but BERT fails . Figure   3 shows a representative example from HotpotQA .   Answering the question needs 2 - hop reasoning :   identify “ Roden Brothers were taken over by Birks   Group ” from the first document , and then “ Birks   Group is headquartered in Montreal ” from the sec-   ond document . While BERT tends to simply predict   an entity near the question entity ( “ Toronto ” in the   first document , which is just 1 - hop ) , LinkBERT   correctly predicts the answer in the second docu-   ment ( “ Montreal ” ) . Our intuition is that because   LinkBERT is pretrained with pairs of linked docu-   ments rather than purely single documents , it better   learns how to flow information ( e.g. , do attention )   across tokens when multiple related documents   are given in the context . In summary , these results   suggest that pretraining with linked documents   helps for multi - hop reasoning on downstream tasks .   Improved understanding of document rela-   tions . While the MRQA datasets typically use   ground - truth documents as context for answering   questions , in open - domain QA , QA systems need to   use documents obtained by a retriever , which may   include noisy documents besides gold ones ( Chen   et al . , 2017 ; Dunn et al . , 2017 ) . In such cases , QA   systems need to understand the document relations   to perform well ( Yang et al . , 2018 ) . To simulate this   setting , we modify the SQuAD dataset such that   we prepend or append 1–2 distracting documents   to the original document given to each question .   Table 3 shows the result . While BERT incurs a large   performance drop ( -2.8 % ) , LinkBERT is robust to   distracting documents ( -0.5 % ) . This result suggests   that pretraining with document links improves   the ability to understand document relations and8008   relevance . In particular , our intuition is that the   DRP objective helps the LM to better recognize   document relations like ( anchor document , linked   document ) in pretraining , which helps to recognize   relations like ( question , right document ) in down-   stream QA tasks . We indeed find that ablating the   DRP objective from LinkBERT hurts performance   ( § 5.5 ) . The strength of understanding document   relations also suggests the promise of applying   LinkBERT to various retrieval - augmented methods   and tasks ( e.g. Lewis et al . 2020b ) , either as the   main LM or the dense retriever component .   Improved few - shot QA performance . We also   find that LinkBERT is notably good at few - shot   learning . Concretely , for each MRQA dataset , we   fine - tune with only 10 % of the available training   data , and report the performance in Table 4 . In this   few - shot regime , LinkBERT attains more signifi-   ca nt gains over BERT , compared to the full - resource   regime in Table 1 ( on NaturalQ , 5.4 % vs 1.8 % abso-   lute in F1 , or 15 % vs 7 % in relative error reduction ) .   This result suggests that LinkBERT internalizes   more knowledge than BERT during pretraining ,   which supports our core idea that document links   can bring in new , useful knowledge for LMs .   5.5 Ablation studies   We conduct ablation studies on the key design   choices of LinkBERT .   What linked documents to feed into LMs ? We   study the strategies discussed in § 4.3 for obtaining   linked documents : relevance , salience , and diversity . Table 5 shows the ablation result on MRQA datasets .   First , if we ignore relevance and use random doc-   ument links instead of hyperlinks , we get the same   performance as BERT ( -4.1 % on average ; “ random ”   in Table 5 ) . Second , using lexical similarity links   instead of hyperlinks leads to 1.8 % performance   drop ( “ TF - IDF ” ) . Our intuition is that hyperlinks   can provide more salient knowledge that may not be   obvious via lexical similarity alone . Nevertheless ,   using lexical similarity links is substantially better   than BERT ( +2.3 % ) , confirming the efficacy of   placing relevant documents together in the input   for LM pretraining . Finally , removing the diversity   adjustment in document sampling leads to 1 % per-   formance drop ( “ No diversity ” ) . In summary , our   insight is that to create informative inputs for LM   pretraining , the linked documents must be seman-   tically relevant and ideally be salient and diverse .   Effect of the DRP objective . Table 6 shows the   ablation result on the DRP objective ( § 4.2 ) . Re-   moving DRP in pretraining hurts downstream QA   performance . The drop is large on tasks with multi-   ple documents ( HotpotQA , TriviaQA , and SQuAD   with distracting documents ) . This suggests that   DRP facilitates LMs to learn document relations .   6 Biomedical LinkBERT ( BioLinkBERT )   Pretraining LMs on biomedical text is shown   to boost performance on biomedical NLP tasks   ( Beltagy et al . , 2019 ; Lee et al . , 2020 ; Lewis   et al . , 2020a ; Gu et al . , 2020 ) . Biomedical LMs   are typically trained on PubMed , which contains   abstracts and citations of biomedical papers . While   prior works only use their raw text for pretraining ,   academic papers have rich dependencies with each   other via citations ( references ) . We hypothesize   that incorporating citation links can help LMs learn   dependencies between papers and knowledge that   spans across them .   With this motivation , we pretrain LinkBERT on   PubMed with citation links ( § 6.1 ) , which we term   BioLinkBERT , and evaluate on biomedical down-   stream tasks ( § 6.2 ) . As our baseline , we follow and   compare with the state - of - the - art biomedical LM ,   PubmedBERT ( Gu et al . , 2020 ) , which has the same   architecture as BERT and is trained on PubMed .   6.1 Pretraining setup   Data . We use the same pretraining corpus used   by PubmedBERT : PubMed abstracts ( 21GB).We8009use the Pubmed Parserto extract citation links be-   tween articles . We then create training instances by   sampling contiguous , random , orlinked segments   as described in § 4 , with the three options appearing   uniformly ( 33 % , 33 % , 33 % ) . In summary , our pre-   training data is the same as PubmedBERT , except   that we have citation links between PubMed articles .   Implementation . We pretrain BioLinkBERT of   -base size ( 110 M params ) from scratch , following   the same hyperparamters as the PubmedBERT   ( Gu et al . , 2020 ) . Specifically , we use a peak learn-   ing rate 6e-4 , batch size 8,192 , and train for 62,500   steps . We warm up the learning rate in the first 10 %   of steps and then linearly decay it . Training takes   7 days on eight A100 GPUs with fp16 .   Additionally , while the original PubmedBERT   release did not include the -large size , we pretrain   BioLinkBERT of the -large size ( 340 M params )   from scratch , following the same procedure as   -base , except that we use a peak learning rate of 4e-4   and warm up steps of 20 % . Training takes 21 days   on eight A100 GPUs with fp16 .   Baselines . We compare BioLinkBERT with   PubmedBERT released by Gu et al . ( 2020 ) .   6.2 Evaluation tasks   For downstream tasks , we evaluate on the BLURB   benchmark ( Gu et al . , 2020 ) , a diverse set of biomed-   ical NLP datasets , and MedQA - USMLE ( Jin et al . ,   2021 ) , a challenging biomedical QA dataset .   BLURB consists of five named entity recog-   nition tasks , a PICO ( population , intervention ,   comparison , and outcome ) extraction task , three   relation extraction tasks , a sentence similarity task ,   a document classification task , and two question   answering tasks , as summarized in Table 7 . We   follow the same fine - tuning method and evaluation   metric used by PubmedBERT ( Gu et al . , 2020 ) .   MedQA - USMLE is a 4 - way multi - choice QA   task that tests biomedical and clinical knowledge .   The questions are from practice tests for the US   Medical License Exams ( USMLE ) . The questions   typically require multi - hop reasoning , e.g. , given   patient symptoms , infer the likely cause , and then   answer the appropriate diagnosis procedure ( Figure   4 ) . We follow the fine - tuning method in Jin et al .   ( 2021 ) . More details are provided in Appendix B.   MMLU - professional medicine is a multi - choice   QA task that tests biomedical knowledge and reason-   ing , and is part of the popular MMLU benchmark   ( Hendrycks et al . , 2021 ) that is used to evaluate mas-   sive language models . We take the BioLinkBERT   fine - tuned on the above MedQA - USMLE task , and   evaluate on this task without further adaptation .   6.3 Results   BLURB . Table 7 shows the results on BLURB .   BioLinkBERToutperforms PubmedBERT   on all task categories , attaining a performance   boost of +2 % absolute on average . Moreover ,   BioLinkBERT provides a further boost of +1 % .   In total , BioLinkBERT outperforms the previous   best by +3 % absolute , establishing new state - of-   the - art on the BLURB leaderboard . We see a trend   that gains are notably large on document - level tasks   such as question answering ( +7 % on BioASQ and8010   PubMedQA ) . This result is consistent with the   general domain ( § 5.3 ) and confirms that LinkBERT   helps to learn document dependencies better .   MedQA - USMLE . Table 8 shows the results .   BioLinkBERT obtains a 2 % accuracy boost   over PubmedBERT , and BioLinkBERT   provides an additional +5 % boost . In total , Bi-   oLinkBERT outperforms the previous best by +7 %   absolute , attaining new state - of - the - art . To further   gain qualitative insights , we studied in what QA   examples BioLinkBERT succeeds but the baseline   PubmedBERT fails . Figure 4 shows a representative   example . Answering the question ( left ) needs 2 - hop   reasoning ( center ): from the patient symptoms   described in the question ( leg swelling , pancreatic   cancer ) , infer the cause ( deep vein thrombosis ) ,   and then infer the appropriate diagnosis procedure   ( compression ultrasonography ) . We find that while   the existing PubmedBERT tends to simply predict   a choice that contains a word appearing in the   question ( “ blood ” for choice D ) , BioLinkBERT   correctly predicts the answer ( B ) . Our intuition is   that citation links bring relevant documents and   concepts together in the same context in pretraining   ( right),which readily provides the multi - hop   knowledge needed for the reasoning ( center ) . Com-   bined with the analysis on HotpotQA ( § 5.4 ) , our   results suggest that pretraining with document links   consistently helps for multi - hop reasoning across   domains ( e.g. , general documents with hyperlinks   and biomedical articles with citation links ) .   MMLU - professional medicine . Table 9 shows   the performance . Despite having just 340 M parame - ters , BioLinkBERT achieves 50 % accuracy on   this QA task , significantly outperforming the largest   general - domain LM or QA models such as GPT-3   175B params ( 39 % accuracy ) and UnifiedQA 11B   params ( 43 % accuracy ) . This result shows that   with an effective pretraining approach , a small   domain - specialized LM can outperform orders of   magnitude larger language models on QA tasks .   7 Conclusion   We presented LinkBERT , a new language model   ( LM ) pretraining method that incorporates docu-   ment link knowledge such as hyperlinks . In both   the general domain ( pretrained on Wikipedia with   hyperlinks ) and biomedical domain ( pretrained on   PubMed with citation links ) , LinkBERT outper-   forms previous BERT models across a wide range   of downstream tasks . The gains are notably large   for multi - hop reasoning , multi - document under-   standing and few - shot question answering , suggest-   ing that LinkBERT effectively internalizes salient   knowledge through document links . Our results sug-   gest that LinkBERT can be a strong pretrained LM   to be applied to various knowledge - intensive tasks .   Reproducibility   Pretrained models , code and data are available at   https://github.com/michiyasunaga/   LinkBERT .   Experiments are available at   https://worksheets .   codalab.org/worksheets/   0x7a6ab9c8d06a41d191335b270da2902e .   Acknowledgment   We thank Siddharth Karamcheti , members of the   Stanford P - Lambda , SNAP and NLP groups , as well   as our anonymous reviewers for valuable feedback .   We gratefully acknowledge the support of NSF   CAREER Award IIS-1552635 ; DARPA under Nos.8011HR00112190039 ( TAMI ) , N660011924033 ( MCS ) ;   Funai Foundation Fellowship ; Microsoft Research   PhD Fellowship ; ARO under Nos . W911NF-16 - 1-   0342 ( MURI ) , W911NF-16 - 1 - 0171 ( DURIP ) ; NSF   under Nos . OAC-1835598 ( CINES ) , OAC-1934578   ( HDR ) , CCF-1918940 ( Expeditions ) , IIS-2030477   ( RAPID ) , NIH under No . R56LM013365 ; Stanford   Data Science Initiative , Wu Tsai Neurosciences   Institute , Chan Zuckerberg Biohub , Amazon ,   JPMorgan Chase , Docomo , Hitachi , Intel , KDDI ,   Toshiba , NEC , Juniper , and UnitedHealth Group .   References8012801380148015   A Ethics , limitations and risks   We outline potential ethical issues with our work   below . First , LinkBERT is trained on the same   text corpora ( e.g. , Wikipedia , Books , PubMed )   as in existing language models . Consequently ,   LinkBERT could reflect the same biases and toxic   behaviors exhibited by language models , such as   biases about race , gender , and other demographic   attributes ( Sheng et al . , 2020 ) .   Another source of ethical concern is the use of   the MedQA - USMLE evaluation ( Jin et al . , 2021 ) .   While we find this clinical reasoning task to be an   interesting testbed for LinkBERT and for multi - hop   reasoning in general , we do not encourage users   to use the current models for real world clinical   prediction .   B Fine - tuning details   We apply the following fine - tuning hyperparameters   to all models , including the baselines .   MRQA . For all the extractive question answering   datasets , we use max_seq_length = 384 and a   sliding window of size 128if the lengths are longer   thanmax_seq_length .   For the -tiny scale ( BERT , LinkBERT ) ,   we choose learning rates from { 5e-5 , 1e-4 , 3e-4 } ,   batch sizes from { 16 , 32 , 64 } , and fine - tuning   epochs from { 5 , 10 } .   For -base ( BERT , LinkBERT ) , we   choose learning rates from { 2e-5 , 3e-5 } , batch sizes   from { 12 , 24 } , and fine - tuning epochs from { 2 , 4 } .   For -large ( BERT , LinkBERT ) , we   choose learning rates from { 1e-5 , 2e-5 } , batch sizes   from { 16 , 32 } , and fine - tuning epochs from { 2 , 4 } .   GLUE . We use max_seq_length = 128 .   For the -tiny scale ( BERT , LinkBERT ) ,   we choose learning rates from { 5e-5 , 1e-4 , 3e-4 } ,   batch sizes from { 16 , 32 , 64 } , and fine - tuning   epochs from { 5 , 10 } .   For -base and -large ( BERT , LinkBERT ,   BERT , LinkBERT ) , we choose learning   rates from { 5e-6 , 1e-5 , 2e-5 , 3e-5 , 5e-5 } , batch sizes   from { 16 , 32 , 64 } and fine - tuning epochs from 3–10.BLURB . We use max_seq_length = 512 and   choose learning rates from { 1e-5 , 2e-5 , 3e-5 , 5e-5 ,   6e-5 } , batch sizes from { 16 , 32 , 64 } and fine - tuning   epochs from 1–120 .   MedQA - USMLE . We use max_seq_length   = 512 and choose learning rates from { 1e-5 , 2e-5 ,   3e-5 } , batch sizes from { 16 , 32 , 64 } and fine - tuning   epochs from 1–6.8016