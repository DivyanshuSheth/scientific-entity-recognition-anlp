  Kiril Gashteovski , Mingying Yu , Bhushan Kotnis , Carolin Lawrence ,   Mathias Niepert , Goran Glava ˘sNEC Laboratories Europe GmbH , Heidelberg , GermanyUniversity of Mannheim , LMU Munich , GermanyUniversity of Stuttgart , Germany   firstname.lastname@neclab.eu   goran@informatik.uni-mannheim.de   Abstract   Intrinsic evaluations of OIE systems are car-   ried out either manually — with human evalu-   ators judging the correctness of extractions —   or automatically , on standardized benchmarks .   The latter , while much more cost - effective , is   less reliable , primarily because of the incom-   pleteness of the existing OIE benchmarks : the   ground truth extractions do not include all ac-   ceptable variants of the same fact , leading to   unreliable assessment of the models ’ perfor-   mance . Moreover , the existing OIE bench-   marks are available for English only . In this   work , we introduce BenchIE : a benchmark   and evaluation framework for comprehensive   evaluation of OIE systems for English , Chi-   nese , and German . In contrast to existing   OIE benchmarks , BenchIE is fact - based , i.e. ,   it takes into account informational equivalence   of extractions : our gold standard consists of   fact synsets , clusters in which we exhaustively   list all acceptable surface forms of the same   fact . Moreover , having in mind common   downstream applications for OIE , we make   BenchIE multi - faceted ; i.e. , we create bench-   mark variants that focus on different facets   of OIE evaluation , e.g. , compactness or mini-   mality of extractions . We benchmark several   state - of - the - art OIE systems using BenchIE   and demonstrate that these systems are signiﬁ-   cantly less effective than indicated by existing   OIE benchmarks . We make BenchIE ( data and   evaluation code ) publicly available .   1 Introduction   Open Information Extraction ( OIE ) is the task of   extracting relations and their arguments from natu-   ral language text in a schema - free manner ( Banko   et al . , 2007 ) . Consider the sentence " Sen. Mitchell ,   who is from Maine , is a lawyer . " ; an OIE system   is expected to extract the triples ( " Sen. Mitchell " ;   " is from " ; " Maine " ) and("Sen . Mitchell " ; " is " ; " a   lawyer " ) from the sentence . OIE systems are usedin many downstream tasks , including knowledge   graph ( KG ) population ( Gashteovski et al . , 2020 ) ,   open link prediction ( Broscheit et al . , 2020 ) , and   question answering ( Yan et al . , 2018 ) . These down-   stream tasks lend themselves as natural setups for   extrinsic OIE evaluation ( Mausam , 2016 ) . While   valuable in concrete applications , such extrinsic   evaluations do not measure the intrinsic correct-   ness of the extracted facts : for that purpose , several   benchmarks for intrinsic OIE evaluation have been   proposed ( Stanovsky and Dagan , 2016 ; Lechelle   et al . , 2019 ; Bhardwaj et al . , 2019 ) .   Automated benchmark evaluations are more fea-   sible ( i.e. , faster and cheaper ) than manual OIE   evaluations ( Hohenecker et al . , 2020 ) . The cur-   rent benchmarks , however , use scoring functions   that are based on approximate ( token - level ) match-   ing of system extractions against ground truth   facts , which seems to be substantially less reli-   able than human judgments of extraction correct-   ness ( Zhan and Zhao , 2020 ) . This primarily stems   from the incompleteness of existing OIE bench-   marks : the gold standard extractions do not in-   clude allacceptable surface realizations of the   same fact . Consider , for example , a sentence from   the recent evaluation framework CaRB ( Bhardwaj   et al . , 2019 ): “ Sen. Mitchell is conﬁdent he has   sufﬁcient votes to block such a measure with pro-   cedural actions ” ; with the gold triple extraction   ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ; “ sufﬁ-   cient votes to . . . procedural actions ” ) . Intuitively ,   a system extraction with a more concise object —   ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ; “ sufﬁcient   votes ” ) — could also be accepted , as it still captures   the same core piece of knowledge , and would ar-   guably be valuable in most downstream tasks .   To account for this , existing benchmarks credit   system extractions for per - slot lexical overlap with   gold extractions . Such scoring is overly lenient   and overestimates the systems ’ ability to extract   correct knowledge facts . Consider , e.g. , a system4472extraction ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ;   “ procedural actions ” ) for the above - mentioned sen-   tence . From the factual perspective , this extraction   is clearly incorrect ( Sen. Mitchell hasvotes , not   actions ) . However , the popular CaRB benchmark   with its token - level metrics would judge the extrac-   tion as having ( 1 ) perfect precision , since all ex-   tracted tokens can be found in corresponding slots   of a gold extraction and ( 2 ) high recall , as all of the   gold subject and predicate tokens as well as two   gold object tokens ( “ procedural ” and“actions ” )   are found within corresponding slots of the sys-   tem extraction ( Table 1 ) . Moreover , by providing   a single ground truth extraction per fact , existing   OIE benchmarks fail to acknowledge that different   downstream applications focus on different facets   ( i.e. , aspects ) of OIE extractions : e.g. , for text sum-   marization , one may prefer minimal extractions   ( Ponza et al . , 2018 ) , whereas knowledge base pop-   ulation beneﬁts from strict correctness of entities   in subject and object slots ( Lin et al . , 2020 ) .   In this work , we depart from lenient OIE evalua-   tions based on per - slot token overlaps and propose   BenchIE , a novel fact - centric andmulti - faceted   OIE evaluation framework and benchmark at the   core of which is the following question :   Does the system extraction express the same fact   ( i.e. , the same unit of knowledge ) as any of the   ground truth extractions ( and vice versa ) w.r.t . the   speciﬁc aspect of the OIE extraction that is of in-   terest for one or more downstream applications ?   Contributions . BenchIE advances the state of   the art in OIE evaluation in the following : ( 1)it   is the ﬁrst fact - centered approach to OIE evalu-   ation : to reliably answer the above question , we   exhaustively list all correct extractions of the same   fact . In contrast to existing benchmarks , BenchIE   speciﬁes complete sets of fact - equivalent extrac-   tions ( dubbed fact synsets ) , allowing us to avoid   error - prone evaluation based on token overlap mea-   sures ; ( 2)BenchIE is the ﬁrst multi - faceted OIE   benchmark , allowing to test systems for different   aspects of OIE extractions that may be relevant in   concrete downstream applications ; ( 3)BenchIE is   amultilingual benchmark , covering English , Chi-   nese , and German , and to the best of our knowledge   the ﬁrst with manually annotated ( i.e. , gold stan-   dard ) extractions in all languages;(4)ﬁnally , as afact - based and multi - faceted benchmark , BenchIE   allows us to perform what we believe to be the most   comprehensive proﬁling and comparative evalu-   ation of OIE systems . BenchIE portrays fact ex-   traction abilities of six state - of - the - art OIE models   much less favorably and points to their limitations   that can not be detected with existing benchmarks .   2 Matching Facts , Not Tokens   Most OIE systems extract ( subject , predicate , ob-   ject ) triples , with concepts as subjects and objects   and verb phrases ( VPs ) as predicates ( Banko et al . ,   2007 ; Stanovsky et al . , 2018 ; Lauscher et al . , 2019 ;   Gashteovski et al . , 2017 , 2019 ) , though systems   producing n - ary ( Akbik and Löser , 2012 ) , nested   ( Bhutani et al . , 2016 ) , and noun - mediated extrac-   tions ( Yahya et al . , 2014 ) also exist . Here we fol-   low the most common practice and focus on VP-   mediated facts . Our novel fact - based benchmark   and evaluation paradigm can , however , equally be   applied to other types of extractions ( e.g. , Friedrich   et al . ( 2022 ) used this fact - based concept for OIE to   create gold annotations for NE - Centric OIE triples ;   i.e. , triples where each argument is a named entity   and the relations could be either verb phrases or   noun phrases ) .   2.1 Fact Synsets   We introduce the general concept of a fact synset :   a set of allpossible extractions ( i.e. , different sur-   face forms ) for a given fact type ( e.g. , VP - mediated   facts ) that are instances of the same fact . E.g. , given   the input sentence from Table 2 , the extractions   ( “ Sen. Mitchell ” ; “ has sufﬁcient votes to block ” ;   “ such a measure ” ) and ( “ Sen. Mitchell ” ; “ has sufﬁ-   cient votes to block ” ; “ measure ” ) capture the same   fact and thus belong to the same fact synset .   Existing benchmarks fail to exhaustively list all   acceptable extractions for the same fact . This is   precisely why , in order to avoid penalizing sys-   tems for correct extractions that are not exactly   the same as the gold triples , they resort to le-   nient token - based performance measures prone   to two types of errors : ( 1 ) they punish correct   fact extractions that have limited lexical overlap   with the gold extraction of the same fact , e.g. ,   ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ; “ sufﬁ-   cient votes ” ) vs. ( “ Sen. Mitchell ” ; “ is conﬁdent he   has ” ; “ sufﬁcient votes to . . . procedural actions ” ) 4473   and ( 2 ) they reward incorrect extractions that have   high lexical overlap with a gold extraction , e.g. ,   ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ; “ proce-   dural actions ” ) vs. ( “ Sen. Mitchell ” ; “ is conﬁdent   he has ” ; “ sufﬁcient votes to block . . . with procedu-   ral actions ” ) .   To prevent this , BenchIE relies on exact match-   ingof system extractions against the gold fact   synsets . Further , some OIE systems ( over)generate   extractions of the same fact ; e.g. , ( “ Sen. Mitchell ” ;   “ has sufﬁcient votes to block ” ; “ such a measure ” )   and("Sen . Mitchell " ; " has sufﬁcient votes to block " ;   " measure " ) . Existing evaluation procedures do not   acknowledge the fact equivalence of extractions   and consequently reward OIE systems for multiply   extracting the same fact . Our evaluation based on   fact synsets directly remedies these shortcomings   of existing OIE benchmarks .   2.2 Annotation Process   English Benchmark . To make BenchIE compa-   rable to previous benchmarks , we annotate fact   synsets on a subset of sentences from CaRB ( Bhard-   waj et al . , 2019 ) . Because exhaustive annotation of   fact synsets is time consuming , we carried it on 300   ( out of 1,200 ) randomly sampled CaRB sentences .   To collect truly exhaustive fact synsets , two expertannotators independently labeled the selected 300   sentences in three rounds . ( 1)Each annotator ﬁrst   ( independently ) manually denoted every extraction   in which a VP - predicate connects two concepts .   The annotator then grouped the fact - equivalent   triples into fact synsets . To speed the annotation   process up , we developed a dedicated web - based   annotation tool AnnIE that facilitates the extraction   of VP - mediated triples ( e.g. , we color - code verbs   to indicate possible predicate heads ) and their clus-   tering into fact synsets;(2)The annotators then   carefully examined all gold extractions from the   original CaRB dataset and added those judged to be   correct , yet missing from the manually labeled fact   synsets from the previous step ; ( 3)Finally , each   annotator compared the extractions of all OIE sys-   tems in evaluation ( see § 4 ) against the BenchIE ’s   fact synsets ( i.e. , the result of the ﬁrst two steps ) .   Any system extraction not found in BenchIE was   carefully examined and — if judged to be correct —   added to the appropriate fact synset . Finally , the4474two annotators merged their independently created   annotations by discussing and jointly resolving the   disagreements . The overall annotation effort for   the English dataset amounted to 80 hours per an-   notator . English BenchIE contains 136,357 unique   gold extractions , grouped into 1,350 fact synsets .   For comparison , CaRB ( Bhardwaj et al . , 2019 ) lists   mere 783 gold triples for the same 300 sentences .   Table 2 shows fact synsets for an example sentence .   Inter - Annotator Agreement ( IAA ) . To validate   BenchIE ’s annotations , we measure the inter-   annotator agreement ( IAA ) between our two expert   annotators . To this end , we quantify the agree-   ment via recall at the fact level ( see § 2.3 for further   details ): for each annotator , we compute their fact-   level recall as the percentage of fact synsets of   the other annotator they cover with their extrac-   tions . We average the fact - level recalls of the   two annotators as the IAA score . We observed a   high IAA score of 0:79 . Upon manual inspection ,   we found that the annotators mostly agree on fact-   synset level ; most of the the disagreements are on   extractions level ( particularly , from marking the   optional tokens within an extraction ; see Appendix   A.1.3 for details about the optional tokens ) .   Chinese and German Benchmarks . Two bilin-   gual expert annotators – native in the target lan-   guage and ﬂuent in English ( ) – translated the   original 300 English sentences to Chinese ( )   and German ( ) , respectively . Then , to collect   exhaustive fact synsets inand , they fol-   lowed the same three annotation rounds described   for § 2.2 . Due to substantial ( primarily syntactic )   differences compared to , we adjusted the an-   notation guidelines for these languages ( see the   Appendix A.2 and A.3 for more details ) . The statis-   tics ( number of fact synsets and extractions ) of   theandbenchmarks are given in Table 3 .   Compared toBenchIE , thebenchmark con-   tains signiﬁcantly fewer fact synsets ( 994 compared   to 1,350 ) and more than two orders of magnitude   fewer extractions . The drastically smaller number   of extractions is primarily due to the lack of de-   terminers and articles in Chinese . Their frequent   occurrence in English combined with their neutral-   ity w.r.t . extractions ’ correctness results in many   mutually different yet fact - equivalent extractions .   The numbers for German are , expectedly , much   closer to those for English .   2.3 Evaluation Measure   We assume that BenchIE is ( 1 ) complete , i.e. , that   it contains ( a ) allVP - mediated facts expressed in   input sentences and ( b ) for each fact , its every ac-   ceptable extraction as well ; and ( 2 ) sound , i.e. , that   it does not contain any incorrect extraction that   would capture a fact not stated in the sentence .   Such a complete OIE gold standard enables not   only a more reliable evaluation of OIE systems by   means of exact matching , but also an evaluation   at the more meaningful level of knowledge facts ,   rather than at the level of individual triples .   Concretely , we consider a system extraction to   be correct if and only if it exactly matches some   gold extraction from some fact synset . The number   oftrue positives ( TPs ) is the number of fact synsets   ( i.e. , different facts ) covered by ( at least one of   the ) system extractions . This way , a system that   extracts Ndifferent triples of the same fact , will   be rewarded only once for the correct extraction   of the fact . BenchIE ’s false negatives ( FNs ) are   then , intuitively , fact synsets not covered by any   of the system extractions . Finally , each system   extraction that does not exactly match any gold   triple ( from any synset ) counts as a false positive   ( FP ) . We then compute Precision , Recall , and F   score ( as the ﬁnal score ) from TP , FP , and FN in   standard fashion .   3 Multi - Faceted OIE Benchmark   Different downstream applications care about dif-   ferent aspects of OIE extractions . For IE - based   text summarization and simpliﬁcation ( Ponza et al . ,   2018 ; Štajner and Glavaš , 2017 ) , e.g. , triples should   be minimal overall , across all slots ( i.e. , without   unnecessary tokens ) , but the exact token placement   across the slots ( e.g. , if a preposition is in the pred-   icate or object ) does not matter . For entity linking   and knowledge base population ( Lin et al . , 2020 ) ,   in contrast , the token placement between slots is   critical : a token that is not part of an entity , should   not be placed into subject or object . Acknowl-   edging this , we create three additional variants of   the English BenchIE , referred to as facets , each4475   corresponding to one aspect that is relevant in com-   mon OIE applications . This effort addresses re-   cent calls for multi - dimensional analysis of NLP   systems ( Ethayarajh and Jurafsky , 2020 ; Narayan   et al . , 2021 ) and is well - aligned with recent ef-   forts that create multi - faceted benchmarks for other   NLP tasks ( Liu et al . , 2021 ; Väth et al . , 2021 ) and   datasets ( Xiao et al . , 2022 ) .   3.1 BenchIE - E   The default , general - purpose BenchIE facet from   the previous section was designed to be some-   what tolerant to token distribution accross slots   ( see Appendix A.1.2 for details ): some tokens may   be placed in either the predicate or object ( e.g. ,   the preposition with in the synset fin Table 2 ) .   This enables a more ﬂexible comparison of OIE   systems that are designed for different purposes   ( i.e. , systems that produce slightly different token   placements are not punished ) and is in line with   prior work on intrinsic OIE evaluation , both auto-   matic ( Stanovsky and Dagan , 2016 ; Bhardwaj et al . ,   2019 ) and manual ( Fader et al . , 2011 ; Del Corro   and Gemulla , 2013 ; Gashteovski et al . , 2017 ) . Such   extraction ﬂexibility , however , may not be desirable   in tasks like automated KG construction ( Wolfe   et al . , 2017 ; Jiang et al . , 2019 ) or entity linking   ( Lin et al . , 2020 , 2021 ) . Angeli et al . ( 2015 ) show   empirically that extractions with wholesome enti-   ties and without additional tokens yield beneﬁts in   KG construction .   Since OIE is predominantly used for KG - related   tasks ( Weikum et al . , 2020 ) , it is paramount to   have an evaluation facet that imposes strict(er ) to-   ken boundaries on entity slots – subjects and ob-   jects . We thus create the entity facet of the bench-   mark ( BenchIE - E ) with this additional constraint   of wholesomeness of subject and object concepts .   BenchIE - E was constructed by one of our anno-   tators ( see § 2.2 ) by removing fromBenchIE ’s   fact synsets the extractions in which subject and/or   object was not a wholesome concept ( see Table 4).3.2 BenchIE - C   The default BenchIE facet ( § 2 ) compares OIE ex-   tractions against gold triples from fact synsets at the   slot level : to be judged correct , an extraction must   exactly match some gold triple in all slots . This   criterion , however , is overly strict if extractions   are to be used in applications like summarization   or simpliﬁcation ( Ponza et al . , 2018 ; Štajner and   Glavaš , 2017 ) , which commonly concatenate the   content of the slots . In this case , it does not matter   if a sequence of tokens occurs at the end of the   subject or beginning of the predicate ( analogously   for predicate and object ) . To reﬂect this , we intro-   duce the concatenation facet , BenchIE - C : for each   gold BenchIE triple , we create the gold BenchIE - C   utterance by simply concatenating the content of   the triple ’s slots ( see Table 4 ) .   3.3 BenchIE - M   Our third additional evaluation facet addresses the   aspect of minimality of OIE extractions ( Gash-   teovski et al . , 2017 ) . More compact extractions   can beneﬁt both text generation ( Ponza et al . , 2018 ;   Štajner and Glavaš , 2017 ) and KG - related tasks   ( Lin et al . , 2020 , 2021 ) . If two triples tandt   capture the same fact ( i.e. , are in the same fact   synset ) , tis considered more compact thant   if tokens of each tslot make a ( non - strict ) sub-   sequence of tokens in the corresponding tslot   ( Gashteovski , 2020).To allow for evaluation of   minimality , BenchIE - M triples contain only the   non - optional tokens ( denoted in square brackets   in Table 2 ) from the corresponding BenchIE triple .   Consequently , BenchIE - M fact synsets on average   contain many fewer extractions than the original   BenchIE synsets .   4 Fact - Level Evaluation   We ﬁrst compare BenchIE ’s fact - level evaluation   ( i.e. , default facet , § 2 ) against CaRB ’s token - level4476   scoring ( Bhardwaj et al . , 2019).Our quantita-   tive results conﬁrm our intuitions and observations   ( see Table 1 ): CaRB systematically and substan-   tially overestimates OIE systems ’ performance .   BenchIE , we argue , portrays the fact extraction   abilities of OIE systems more realistically .   4.1 Experimental Setup   OIE Systems . We tested six widely used OIE   systems that extract VP - mediated facts for ,   namely : ClausIE ( Del Corro and Gemulla , 2013 ) ,   Stanford OIE ( Angeli et al . , 2015 ) , MinIE ( Gash-   teovski et al . , 2017 ) , ROIE ( Stanovsky et al . , 2018 ) ,   OpenIE 6 ( Kolluru et al . , 2020 ) and MOIE ( Ro   et al . , 2020 ) . We additionally implemented the   following naive baseline ( Naive OIE ): each verb   ( detected using spaCy ’s POS - tagger ( Honnibal and   Montani , 2017 ) ) becomes the predicate , its entire   preceding sentence context becomes the subject   and succeeding context the object . Forand ,   we evaluated a supervised MOIE ( Ro et al . , 2020 )   model based on the multilingual BERT ( Devlin   et al . , 2019 ) , trained on a largedataset ( Zhan   and Zhao , 2020 ) and transferred ( zero - shot ) to tar-   get languages by means of its multilingual encoder .   Implicit and N - ary Extractions . Some OIE sys-   tems produce implicit extractions containing to-   kens that do not occur in the sentence . As   BenchIE does not contain implicit annotations , we   remove such extractions from the OIE systems ’   output , to avoid penalizing OIE systems for ex-   tracting fact types not covered by the benchmark .   To make CaRB directly comparable , we automati - cally remove all its implicit extractions too . ROIE   and MOIE produce N - ary extractions ( i.e. , more   than three slots ) , whereas BenchIE contains only   triples . We follow standard practice ( Del Corro and   Gemulla , 2013 ) and convert those extractions into   triples by concatenating the third and subsequent   slots into a single object .   4.2 Results and Discussion   Table 5 summarizes results of OIE systems on   BenchIE and CaRB . Across the board , BenchIE ’s   fact - level precision and recall are signiﬁcantly   lower than CaRB ’s respective precision and recall   computed on token level . On average , CaRB scores   the OIE systems higher than BenchIE by 14 per-   centage points for precision , 38 percentage points   for recall and 26 percentage points for the Fscore .   Precision . System ’s precision on BenchIE is lower   ( albeit not so drastically lower as recall ) than on   CaRB because BenchIE , as a complete benchmark ,   punishes incorrect facts , i.e. , extractions that can-   not be found in BenchIE ’s fact synsets . CaRB ,   on the other hand , rewards any token overlap that   the incorrectly extracted fact has against its gold   triple(s ) – in many cases such overlap is substan-   tial and CaRB consequently rewards the incorrect   fact with high precision . Consider , for example ,   the sentence from Table 1 and an incorrect fact ex-   traction ( “ Sen. Mitchell ” ; “ is conﬁdent he has ” ;   “ sufﬁcient actions ” ) ; on BenchIE , this extraction is   a false positive because it does not exist in any   of the four fact synsets it lists for the sentence .   CaRB , in contrast , rewards the extraction with per-   fect precision because all its tokens are accounted   for in the corresponding slots of its gold triple   ( “ Sen. Mitchell ” ; “ is conﬁdent he has " ; " sufﬁcient   votes to . . . actions ” ) .4477In an attempt to quantify how much CaRB over-   estimates fact - level precision with its token overlap   metric , we evaluated our Naive OIE baseline on   both CaRB and BenchIE . While BenchIE reﬂects   the poor quality of naive extractions with the near-   zero performance , CaRB estimates its precision to   be non - negligible ( 0.24 ) and even higher than that   of the Stanford ’s OIE system ( 0.17 ) . In contrast ,   BenchIE assigns much lower score to this baseline :   precision of 0.03—8 times less than CaRB ’s score .   Recall . While CaRB somewhat overestimates fact-   level precision of OIE systems , its overestimation   of their recall is much more drastic : all tokens of   its gold extractions that can be found in respec-   tive slots of a factually incorrect extraction of an   OIE system contribute to the system ’s recall . The   overestimation of CaRB ’s recall scores is best il-   lustrated by the fact that our naive baseline ( Naive   OIE ) obtains a score of 0.7 , better than any of the   six OIE systems under evaluation . In terms of re-   call , CaRB obviously rewards long extractions –   the longer the system extraction is , the more likely   it is to cover more tokens from gold standard ex-   tractions . Neural extractors OpenIE6 , ROIE , and   MOIE on average produce much longer extrac-   tions than rule - based systems like MinIE or Stan-   ford ( e.g. , on average , a ROIE extraction has 16   tokens , whereas Stanford extraction has 7.7 tokens ):   accordingly , CaRB rewards the neural systems with   much higher recall scores . BenchIE , on the other   hand , credits only the OIE extractions that cover   its fact synsets ( and only once per fact synset ) . Our   Naive OIE is , intuitively , highly unlikely to match   gold extractions from fact synsets and BenchIE re-   ﬂects this with a fact - level recall of only 2 % . Simi-   larly , BenchIE ’s recall scores reveal that the long   extractions of neural OIE systems very rarely cor-   respond to any acceptable variant of an expressed   fact ( e.g. , ROIE ’s fact - level recall is only 9 % ) .   Multilingual OIE . We evaluated MOIE ( as the   only multilingual model in our evaluation ) on the   Chinese and German versions of BenchIE . Quite   expectedly , the performance for Chinese and Ger-   man in target languages is below the source En-   glish performance . However , the drop due to the   zero - shot language transfer is , at ﬁrst glance – sur-   prisingly , much larger for German than for Chi-   nese : this goes against ﬁndings from other tasks ,   where transfer performance correlates with linguis-   tic proximity between the source and target lan-   guage ( Lauscher et al . , 2020 ) . MOIE ’s Chinese   performance is encouraging , as it surpasses the En-   glish performance of some of the other OIE mod-   els ( e.g. , its recall score is better than ROIE , and   its precision score is better than Stanford ’s ) . We   believe this is because ( a ) OIE is a highly syntac-   tic task ; and ( b ) Chinese language is syntactically   simple and has the same word order as English   ( SVO ) . German language , on the other hand , de-   spite overall linguistic proximity to English , has   a different word order ( SOV ; from generative per-   spective ) , with the main verb often appearing at the   very end of the sentence – this , we believe , is the   main cause of poor OIE transfer between English   and German . We believe BenchIE is a good starting   point for multilingual OIE evaluation : we subse-   quently created additional data for Arabic , Galician ,   and Japanese : see Kotnis et al . ( 2022 ) and Friedrich   et al . ( 2022 ) for details and further analyses .   5 Proﬁling OIE Systems with BenchIE   Token - based evaluation of existing OIE bench-   marks ( with real per - extraction scores in the range   [ 0;1 ] ) makes pinpointing of extraction error source   difﬁcult . This limits their usability in automatic   error analysis and system proﬁling . The fact that   previous work performed OIE error analyses man-   ually ( Fader et al . , 2011 ; Schneider et al . , 2017 )   conﬁrms this . BenchIE , in contrast , lists all accept-   able extractions and thus naturally lends itself to   reliable automatic error analysis and proﬁling .   5.1 Slot Errors   We carry out the analysis of errors per slots   on the default BenchIE facet ( § 2 ) , because it is   application - agnostic , unlike the additional facets   from § 3 . We observed that most of the errors in   all OIE systems stem from extracting the objects   ( see Figure 1 ) . For an SVO language like English,4478   correctly extracting subjects and predicates seems   substantially easier than correctly extracting ob-   jects . MinIE ( rule - based ) and ROIE ( neural ) have   higher shares of predicate mis - extractions . MinIE   post - processes ClausIE ’s triples by moving words   from objects to predicates . Since ClausIE most fre-   quently makes object errors , this effectively redis-   tributes those errors between predicates and objects   of MinIE ’s extractions .   Figure 1 , however , does not tell the whole story ,   as many extractions are erroneous in multiple slots .   For more detailed insights , we assign each incor-   rect extraction to one of seven error buckets : each   error bucket indicates one combination of extrac-   tion errors across the three slots . For example ,   the bucket ( 1;1;0)contains extractions that match   their closest gold triple in the subject and predi-   cate , but not object . The closest gold triple is the   one that matches the extraction in most slots .   The error - bucket analysis , summarized in Figure   2 , reveals that , across all systems , most extractions   with object errors actually have correct subjects and   predicates ( bucket ( 1;1;0 ) ) . MinIE deviates from   this pattern and produces also many extractions   with both incorrect object and predicate ( bucket   ( 1;0;0 ) ) or only bad predicate ( bucket ( 1;0;1 ) ) .   Expectedly , most extractions of our naive base-   line most often get only the predicate right ( bucket   ( 0;1;0 ) ) or all three slots wrong ( bucket ( 0;0;0 ) ) .   This further emphasizes how misleading current   token - based benchmarks can be – CaRB rewards   this baseline with very high recall ( see § 4 ) .   5.2 Bucketized Error Analysis   To understand where OIE systems fail systemat-   ically , we split the input sentences into buckets   and measured the performance of OIE systems perbucket . Based on preliminary qualitative error anal-   ysis , we chose bucketization according to some   linguistic properties of the sentences that produced   erroneous triples . In particular , we examine the   performance of OIE systems for sentence length ,   presence of conjunctions and case markers , since   these appeared to be the most common reasons for   failure . Note that BenchIE instances can be “ bucke-   tized ” according to an arbitrary dimension interest ,   lending itself to diverse future ﬁne - grained evalua-   tions and analyses of OIE systems ’ behaviour . In   general , we found that OIE systems exhibit weakest   performance on long sentences ( with more than 30   tokens ) as well as those that contain conjunctions   or have more than two case markers ( Figure 3 ) . For   a more detailed discussion , see Appendix C.   5.3 Multi - Faceted Evaluation   Finally , we proﬁle the OIE systems on our three   special benchmark facets ( § 3 ): BenchIE - E , -C and   -M. Figure 4 summarizes the performance of OIE   systems on these three facets .   BenchIE - C. Ignoring slot boundaries , this facet   is more lenient to OIE systems than the default   facet – BenchIE - C yields higher scores than the   regular BenchIE facet for allsystems . The gap   between the system ’s performance on BenchIE - C   and BenchIE effectively quantiﬁes how often the   system misplaces tokens between adjacent slots .   This gap is very small for Stanford OIE and MinIE   – this means that , for extractions with correct over-   all token span , they also distribute the tokens be-   tween the slots correctly . For downstream tasks   like text summarization , BenchIE - C results point   to ClausIE as the best choice . Interestingly , we   observed that CaRB ’s Precision for some systems   ( ClausIE and MinIE ) effectively matches their Pre-   cision on BenchIE - C ( see Figure 4 ) , which is an-   other indication that CaRB scores , in effect , neglect   precise token distributions across slots .   BenchIE - E. This facet is stricter than the default   BenchIE facet – it allows fewer token placement   variants in subject and object . For all OIE systems   theFBenchIE - E score is thus lower than the cor-   responding BenchIE score . MinIE and Stanford   OIE obtain very similar performance on BenchIE-   C , BenchIE ( default ) , and BenchIE - E : this means   that their extraction ( when correct in overall token   span ) most often have clean concepts in subject   and object . All neural systems and ClausIE ex-   hibit huge performance drops on BenchIE - E – this4479   means that their subject and object concept extrac-   tions are not clean , which makes these systems less   suitable for tasks like KG population and entity   linking . Out of the systems we evaluate , MinIE is   the best ﬁt for such downstream tasks .   BenchIE - M. This facet yields the lowest perfor-   mance for all systems , as it punishes extractions   with any unnecessary tokens . Expectedly , MinIE   – a system tailored to produce minimal extractions   – yields the best performance on this facet . But   even MinIE “ loses ” half of its performance when   minimality is enforced ( BenchIE vs. BenchIE - M ) .   This calls for more work on minimizing OIE extrac-   tions . Stanford OIE outperforms all systems except   MinIE , which renders it a good pick when extrac-   tion minimality is beneﬁcial for a downstream task .   Neural vs. Rule - Based Systems . Neural systems   underperform their rule - based counterparts on most   facets . This gap is most pronounced on BenchIE - E ,   whereas it is much smaller on BenchIE - C : these   observations strongly indicate that neural systems   struggle the most with correct distribution of to-   kens across the ( adjacent ) extraction slots . They   also do not attempt to remove the optional ( i.e. ,   unnecessary ) tokens , as indicated by extremely low   performance on BenchIE - M. On CaRB , however ,   these same neural systems yield the best perfor-   mance . Being trained and validated on datasetswith extractions similar to CaRB ’s , neural extrac-   tors seem to overﬁt to CaRB evaluation . Our fact-   based multi - faceted evaluation , however , reveals   that their extractions are far less likely to be useful   down the stream .   6 Conclusion   We introduced BenchIE : a benchmark for more re-   liable fact - level evaluation of OIE systems for En-   glish , Chinese and German . Unlike existing bench-   marks , BenchIE takes into account fact - level equiv-   alence of extractions : it consists of fact synsets that   contain allacceptable surface forms of the same   fact . Further , BenchIE is multi - faceted – it al-   lows to evaluate OIE extractions w.r.t . several as-   pects relevant in common downstream tasks . Our   experiments show that current benchmarks , with   incomplete gold standard and approximate token-   level matching , drastically overestimate fact extrac-   tion abilities of OIE systems . Currently , the limits   of BenchIE are its relatively small size ( 300 sen-   tences v.s. CaRB ’s 1,200 ) and its time - consuming   annotation process . A promising research direction   is the investigation of trade - off between the manual   effort and completeness of different OIE annota-   tion strategies . In this scenario , BenchIE is an ideal   point of reference : it can precisely quantify the   completeness of some larger ( non - exhaustive ) OIE   dataset created with limited or no manual effort.4480References448144824483A Appendix : Annotation Guidelines   A.1 Annotation Guidelines for English   A.1.1 General Principle   The annotator should manually extract verb-   mediated triples from a natural language sentence .   Each triple should represent two entities or con-   cepts , and the verb - mediated relation between them .   For example , from the input sentence " Michael Jor-   dan , who is a former basketball player , was born in   Brooklyn . " , there are three entities and concepts —   Michael Jordan , former basketball player and   Brooklyn — which are related as follows : ( " Michael   Jordan " ; " is " ; " former basketball player " ) and   ( " Michael Jordan " ; " was born in " ; " Brooklyn " ) .   Once the triple is manually extracted , it should   be placed into the correct fact synset ( see Sec-   tion A.1.2 ) .   A.1.2 Fact Synsets   Once a triple is manually extracted , the annotator   should place the triple into its corresponding fact   synset ( for more details about the concept of fact   synsets , refer to Section 2 ) . In case there is no ex-   isting fact synset for the manually extracted triple ,   the annotator should create one and place the triple   in that synset .   Coreference . The annotator should place extrac-   tions that refer to the same entity or concept under   the same fact synset . Consider the following input   sentence : " His son , John Crozie , was an aviation   pioneer . " ; the following triples should be placed in   the same fact synset :   •("His son " ; " was " ; " [ an]aviation pioneer " )   •("J. Crozie " ; " was " ; " [ an ] aviation pioneer " )   because " His son " and"John Crozie " refer to the   same entity .   Token placements within the slots . The annota-   tor should consider placing certain tokens in dif-   ferent slots , without damaging the meaning of the   fact . Consider the input sentence " Michael Jordan   was born in Brooklyn . " . There is one fact synset   ( f ) and its corresponding triples ( t ; tandt ):   ft:("M. J. " ; " was born in " ; " Brooklyn " )   t:("M. J. " ; " was born " ; " in Brooklyn " )   t:("M. J. " ; " was " ; " born in Brooklyn")Int , the preposition " in " is in the relation , while   intit is in the object . Likewise , the annotator   should allow for some ﬂexibility w.r.t . the verbs .   While the verbs and prepositions naturally belong   to the relation , some OIE systems were designed   with different goal in mind ; e.g. , to detect head   verbs as relations for detecting clauses within the   extractions ( Del Corro and Gemulla , 2013 ) or to ﬁt   SRL frames for predicates ( Stanovsky et al . , 2018 ) .   We do not want to penalize the OIE systems for   such design choices .   For BenchIE - E , however , this ﬂexibility of to-   ken placements is not allowed . In particular , for f   the annotator is allowed to only extract t , while   tandtshould not be listed . Note that this is   the only difference in the annotation guidelines be-   tween BenchIE - E and the standard BenchIE facet .   Passive voice . When possible , if an extraction   is in passive voice , the annotator should place its   active voice equivalent into the appropriate fact   synset . For instance , consider the sentence " The   ball was kicked by John . " ; then , the fact synset   should contain the following triples :   •("[The ] ball " ; " was kicked by " ; " John " )   •("John " ; " kicked " ; " [ The ] ball " )   Note that the opposite direction is not allowed .   If the sentence was " John kicked the ball . " , then   the annotator is not allowed to manually extract   the triple ( " [ The ] ball " ; " was kicked by " ; " John " )   because such extraction contains words that are   not originally found in the input sentence ( " was "   and"by " ) . These are so - called implicit extractions   and we do not consider them ( for details , see Sec-   tion A.1.8 of the appendix ) .   A.1.3 Optional Tokens   If possible , the annotator should label as optional   all tokens that can be omitted in an extraction   without damaging its semantics . Such tokens in-   clude determiners ( e.g. , a , the , an ) , honoriﬁcs ( e.g. ,   [ Prof. ] Michael Jordan ) or certain quantities ( e.g. ,   [ some ] major projects . The optional tokens are   marked with square brackets [ ] . In what follows ,   we show examples of considered optional token(s ) .   Determiners . Unless a determiner is a part of a   named entity ( e.g. , " The Times " ) , it is considered   as optional . For instance , the following triples are   considered to be semantically equivalent:4484•("Michael Jordan " ; " took " ; " the ball " )   •("Michael Jordan " ; " took " ; " ball " )   The annotator , therefore , should annotate   ( " Michael Jordan " ; " took " ; " [ the ] ball " ) , where   the optional token is in square brackets .   Titles . Titles of people are considered optional ;   e.g. , ( " [ Prof. ] Michael Jordan " ; " lives in " ; " USA " ) .   Adjectives . The annotator should label adjec-   tives as optional if possible . For example , in the   following triple , the adjective " smart " can be con-   sidered optional : ( " Albert Einstein " ; " was " ; " [ a ]   [ smart ] scientist " ) . Note that the annotator should   be careful not to label adjectives as optional if they   are essential to the meaning of the triple . For in-   stance , the adjective " cold " should not be labeled as   optional in the triple ( " Berlin Wall " ; " is [ infamous ]   symbol of " ; " [ the ] cold war " ) .   Quantities . Certain quantities that modify a   noun phrase can be considered as optional ; e.g. ,   ( " Mitsubishi " ; " has control of " ; " [ some ] major   projects " ) .   Words indicating some tenses . The annotator   can treat certain verbs that indicate tense as op-   tional . For instance , the word " has " in("FDA " ;   " [ has ] approved " ; " Proleukin " ) can be considered   as optional , since both VPs " have approved " and   " approved " contain the same core meaning .   Verb phrases . It is allowed for the annotator   to mark verb phrases as optional if possible ; e.g.   ( " John " ; " [ continues to ] reside in " ; " Berlin " ) .   A.1.4 Attribution Clauses   Extractions that indicate attribution of another core   piece of information should be placed in separate   fact synset , because they indicate a separate piece   of information with separate predicate . For exam-   ple , the core information of the sentence " Conspir-   acy theorists say that Barack Obama was born in   Kenya . " is that Barack Obama was born in Kenya .   As indicated by Mausam et al . ( 2012 ) , it is impor-   tant for OIE systems to extract the context about   the attribution of such information . Therefore , the   annotator should extract the core information —   the triple ( " Barack Obama " ; " [ was ] born in " ;   " Kenya " ) — in one fact synset , and the triples in-   dicating attribution — ( " Conspiracy theorists " ; " say   that " ; " Barack Obama was born in Kenya " ) — in   another . A.1.5 Incomplete Clauses   The annotator should not extract incomplete   clauses , i.e. , triples that lack crucial piece of in-   formation . Suppose there is the input sentence " He   was honored by the river being named after him " .   The following triple should not be manually ex-   tracted : ( " He " ; " was honored by " ; " [ the ] river " ) ,   but the following triples should be : ( " He " ; " was   honored by [ the ] river being named after " ; " him " )   and("[the ] river " ; " being named after " ; " him " ) .   A.1.6 Overly Complex Extractions   The annotators should not manually extract overly   speciﬁc triples , such that their arguments are com-   plex clauses . For instance , for the input sentence   " Vaccinations against other viral diseases followed ,   including the successful rabies vaccination by   Louis Pasteur in 1886 . " , the following triple should   not be extracted : ( " Vaccinations against other viral   diseases " ; " followed " ; " including the successful   rabies vaccination by Louis Pasteur in 1886 " ) be-   cause the object is a complex clause which does   not describe a single concept precisely , but rather   it is composed of several concepts .   A.1.7 Conjunctions   The annotator should not allow for conjunctive   phrases to form an argument ( i.e. , subject or object ) .   Such arguments should be placed into separate ex-   tractions ( and in separate fact synsets ) . Consider   the sentence " Michael Jordan and Scottie Pippen   played for Chicago Bulls . " . The annotator should   manually extract the following triples :   •("M. Jordan " ; " played for " ; " Chicago Bulls " )   •("S. Pippen " ; " played for " ; " Chicago Bulls " )   The annotator should not , however , extract   ( " M. J. and S. P . " ; " played for " ; " Chicago Bulls " ) .   A.1.8 Implicit Extractions   We focus on explicit extractions , which means that   every word in the extracted triple must be present   in the original input sentence . Therefore , implicit   extractions — i.e. , extractions that contain inferred   information with words not found in the sentence —   are not considered . One example implicit extrac-   tion is ( " Michael Jordan " ; " be " ; " Prof. " ) from   the input sentence " Prof. Michael Jordan lives in   USA . " , where the triple infers that Michael Jordan   is professor without being explicitly indicated in   the sentence ( i.e. , the word " be " is not present in   the input sentence , it is inferred).4485A.2 Annotation Guidelines ( Chinese )   The annotator should follow the same general prin-   ciples as with the English annotation guidelines   ( Section A.1 ) . Due to the language difference , we   slightly adapted the annotation guidelines for the   Chinese language . In what follows , we list those   differences .   A.2.1 Articles   Chinese language does not contain articles ( i.e. , " a " ,   " an " , " the " ) . Therefore , in the manual translation   of the sentences , there are no articles in the Chinese   counterparts .   A.2.2 Prepositional Phrases within a Noun   Phrase   Certain noun phrases with nested prepositional   phrase can not be translated directly into Chinese   the same way as in English . For example , suppose   we have the phrase " Prime Minister of Australia " .   In Chinese , the literal translation of this phrase   would be " Australia ’s Prime Minister " . For in-   stance , in the English annotations the sentence " He   was the Prime Minister of Australia " would have   two fact synsets :   f("He " ; " was [ the ] Pr . Min . of " ; " Australia " )   f("He " ; " was " ; " [ the ] Pr . Min . [ of Australia ] " )   This is because the fact synset frelates the con-   cepts " he " and"Australia " with the relation " was   [ the ] Prime Minister of " , while the second fact   synset relates the concepts " he " and"Prime Minis-   ter [ of Australia ] " with the relation " was " .   In Chinese language , however , the construction   offwould not be possible , because the phrase   " Prime Minister of Australia " can not be separated   into"Prime Minister " and"Australia " . Therefore ,   the golden annotation for this particular example   in Chinese would be only one fact synset : ( " He " ;   " was " ; " [ Australia ’s ] Prime Minister " ) , which is   equivalent with f.   A.3 Annotation Guidelines ( German )   In general , the annotators for German should fol-   low the same guidelines described in Section A.1   for English . In what follows , we describe the dif-   ferences which are speciﬁc for the German annota-   tions . A.3.1 Separable Verbs   Separable verbs ( e.g. , " aufstehen " ) in German con-   sist of a lexical core ( a verb ; " stehen " ) and a separa-   ble particle ( e.g. , a preposition ; " auf " ) . When used   in a sentence , separable verbs in German are split   in such manner that the separable particle goes to   the end of the sentence . Consider the following sen-   tence that contains the separable verb " aufstehen " :   " Ich stehe um 7 Uhr auf " . To accommodate the   verb - mediated relations , the annotator should ex-   tract the separable particle right after the separable   core within the predicate : ( " Ich " ; " stehe auf um " ;   " 7 Uhr " )   A.3.2 Modal Verbs   The modal verbs follow similar pattern as the sepa-   rable verbs . Namely , the modal verb has the main   predicate position within the sentence ( directly fol-   lowed by the subject ) , and the main verb that is   modiﬁed by the modal verb is at the end of the sen-   tence ; e.g. sentence " Imust go to work " and its Ger-   man counterpart " Ich muss zur Arbeit gehen " . Fol-   lowing the same guidelines for verb - mediated pred-   icates , the annotator should extract the modal verb   together with the main verb : ( " Ich " ; " muss gehen   zur " ; " Arbeit " ) .   A.3.3 Passive Voice   Consider the following English sentence written   in passive voice " The letters were sent through the   messenger " and its German counterpart " Die Briefe   wurden durch den Boten geschickt " . Following the   spirit of extractions with verb - mediated relations ,   the annotator should extract the following triple :   ( " [ Die ] Briefe " ; " wurden geschickt durch " ; " [ den ]   Boten " ) .   B Annotation Tool   To facilitate the annotation process , we developed a   web - based annotation tool : AnnIE ( Friedrich et al . ,   2022 ) . First , the annotator is given the input sen-   tence as a string along with its tokenized form ( Fig-   ure 5 ) . Then , the tool highlights the tokens of   interest that are candidates for the slots . In particu-   lar , we highlight the verbs in one color ( candidate   predicates ) and the nouns in another ( candidate   arguments ) .   Then , the annotator can select the tokens with   a UI and place them into slots . This forms one   annotated triple . Note that the annotator can also   annotate for optional tokens and phrases with the44864487   use of the mouse double click . Then , the annota-   tor can place the newly annotated triple in either   a new fact synset ( cluster ) or in an existing one   ( Figure 6 ) . For more details on the annotation tool ,   see ( Friedrich et al . , 2022 ) .   C Further Error Analysis   Based on preliminary qualitative error analysis , we   chose bucketization according to some linguistic   properties of the sentences that produced erroneous   triples . In particular , we examine the performance   of OIE systems for sentence length , presence of   conjunctions and case markers , since these ap-   peared to be the most common reasons for fail-   ure . Note that BenchIE allows for any type of   bucketization , which can be used for diverse set of   ﬁne - grained evaluation for future research on OIE .   C.1 Sentence Length   Sentence length is a feature that can affect the per-   formance of NLP systems for different tasks , in-   cluding relation extraction ( Alt et al . , 2020 ) and   named entity recognition ( Arora et al . , 2021 ) . To   evaluate how sentence length afffects performance   of OIE systems as well , we split the sentences into   three buckets : sentences shorter or equal than 20   tokens , between 21 and 30 tokens , and more than   30 tokens . The distribution of these buckets are   120 , 113 and 67 sentences respectively .   We observed that shorter sentences usually yield   the best performance for all OIE systems w.r.t . the   Fscore ( Figure 7a ) . An extreme example is   MinIE , which loses 26 percentage points from sen-   tences shorter than 20 tokens to sentences longer   than 30 tokens . Part of the reason why such sen-   tences are harder to handle is because they containmore complex linguistic structures , such as con-   junctions and case markers . Such sentences tend to   to produce overly complex extractions that contain   very complex structures in their arguments ( see   example extraction tin Table 6 ) .   C.2 Conjunctions   To examine the effect of the conjunctions on the   performance of OIE systems , we bucketized the   input sentences according to the dependency type   conj , which relates two conjunct words in a sen-   tence . In particular , we place the sentences with   no conjuncts in one bucket , and the sentences with   one or more conjuncts in another bucket . With such   bucketization , half of the sentences are in the ﬁrst   bucket , and half in the other . We observed that the   F1 score suffers when a sentence contains at least   one pair of conjuncts ( Figure 7b ) . This observa-   tion partially explains the observation from Sec-   tion 5.1 that OIE systems have troubles identifying   the objects correctly . In subsequent experiments ,   we observed that sentences with more than one con-   juncts worsen the scores further compared to the   sentences with one or no conjuncts . The triple tin   Table 6 is an example of such erroneous extraction .   Neural models seem to suffer the most due to the   conjuncts . For instance , MOIE loses more than   half of the F1 score points ( from 0.34 down to 0.16 )   when at least one conjunct is found in the sentence .   The exception for the neural systems is OpenIE   6 , which is more stable ( goes down from 0.29 to   0.23 ) . The reason is because OpenIE 6 was speciﬁ-   cally trained to handle conjunctions . Interestingly ,   ClausIE and MinIE — rule - based systems — lose ap-   proximately the same amount of F1 score points   as the neural OpenIE 6 . This indicates that neu-4488   ral models can be trained to handle conjunctions   similarly as rule - based systems , though there is   still room for improvement . We observed similar   behaviors for coordinated conjunctions .   C.3 Case Markers   In preliminary qualitative experiments , we found   that the objects are often overly speciﬁc because   they include phrases that should in principle not   be part of the expressed concept . Such excessively   speciﬁc phrases are usually prepositional phrases   or case markers . Consider , for example , the triple   tin Table 6 . The object in this triple is overly   speciﬁc and , thus , incorrect .   To quantify the effect of such case markers , we   bucketized the data according to the number of   the typed dependencies case that are found in the   input sentences . We observed that , as the number of   case dependencies increases , the performance of   OIE systems decreases ( Figure 7c ) . We observed   similar behavior for the number of prepositions   in a sentence . The rule - based system ClausIE is   very sensitive w.r.t . this property , while MinIE is   more stable . MinIE was built on top of ClausIE   and also focused on restructuring the output of   ClausIE , which is the likely reason why MinIE is   more robust w.r.t . the case markers . Neural systems   ( ROIE , OpenIE 6 and MOIE ) are very sensitive   to this property , since their performance is much   lower when we compare the buckets of 0 or 1 case   dependency and the buckets with more than 4 case   dependencies . D More Detailed Discussion on Related   Work   D.1 OIE Benchmarks   The currently existing benchmarks are based on   token - based scoring . The ﬁrst attempt to create   an OIE benchmark was OIE2016 ( Stanovsky and   Dagan , 2016 ) . The authors used a dataset from   another task — QA - SRL ( He et al . , 2015)—and au-   tomatically ported it to OIE . For scoring an OIE   triple , they follow the original task ’s guidelines   ( He et al . , 2015 ) and match only the grammatical   heads of each slot from the OIE triple with the ones   from the golden datasets . Such approach has many   drawbacks ( Zhan and Zhao , 2020 ) , because ( 1 ) ev-   ery error in the automatic porting transfers over to   the evaluation dataset ; ( 2 ) triples are incorrectly   ( and over - optimistically ) scored because it only   considers token - overlaps on grammatical heads ,   not the whole slots . Being crowdsourced , CaRB   ( Bhardwaj et al . , 2019 ) improves over OIE2016   by aggregating per - slot token - level precision and   recall scores between system and gold extractions   across the three slots ( subject , predicate , and ob-   ject ) . However , such approach is overly - lenient ,   as it allows for incorrect extractions to be scored   positively ( see examples in Table 1 ) . Subsequent   work followed similar evaluation procedures . For   instance , Dong et al . ( 2021 ) propose a dataset that   evaluates document - level OIE which uses the same   scoring procedures as CaRB .   D.2 Multi - faceted Evaluation   While having a reliable single - metric benchmark   is crucial for the progress of NLP , recent research   indicated that focusing on single metrics is some-4489what limited , because it does not provide further   insights that go beyond the averaged scores ( Etha-   yarajh and Jurafsky , 2020 ; Narayan et al . , 2021 ) .   In particular , Ethayarajh and Jurafsky ( 2020 ) argue   that single - metric scores ignore certain properties   of the evaluated NLP models . Such properties ,   however , could be relevant for practitioners or for   certain downstream tasks . As a consequence , the   ﬁnal evaluation score is computed at the expense   of other properties of the model . To allow such   multi - faceted evaluations , Liu et al . ( 2021 ) pro-   posed ExplainaBoard , which scores NLP systems   from several tasks across different facets , and Väth   et al . ( 2021 ) propose a multi - faceted benchmark   for visual question answering .   Due to the incompleteness of current OIE   benchmarks — and because of the peculiarity of the   task — no such multi - faceted evaluation for OIE   has been proposed . For each tested extraction , the   state - of - the - art benchmarks provide scores that are   in the interval of [ 0;1 ] . Such design is employed   because the benchmarks are incomplete , which , in   turn , makes it difﬁcult to do proper multi - faceted   evaluation . To tackle this issue , we propose a multi-   faceted evaluation that scores OIE systems across   several facets that are important for downstream   tasks ( see details in Section 3 ) .   D.3 Automatic Error Analysis   Producing automatic error analysis with current   benchmarks is not trivial because they are not ex-   haustive and do not provide crisp scores . For in-   stance , when there are scores within the interval   of[0;1]for each slot — as in CaRB — , it is hard   to say where exactly the error occurred . Previous   work on OIE performed error analysis manually   ( Fader et al . , 2011 ; Schneider et al . , 2017 ) , which   is very time - consuming and inefﬁcient . In contrast   to prior work , BenchIE is exhaustive benchmark   that provides crisp scores , which allows for auto-   matic per - slot error analysis . We discuss BenchIE ’s   automatic error analysis approach in Section 5.4490