  Yunshui LiBinyuan HuiZhichao YinMin YangFei HuangYongbin LiShenzhen Institute of Advanced Technology , Chinese Academy of SciencesUniversity of Chinese Academy of SciencesDAMO Academy , Alibaba GroupUniversity of Science and Technology of China   { ys.li , min.yang}@siat.ac.cn , { binyuan.hby , shuide.lyb}@alibaba-inc.com   Abstract   Perceiving multi - modal information and fulfill-   ing dialogues with humans is a long - term goal   of artificial intelligence . Pre - training is com-   monly regarded as an effective approach for   multi - modal dialogue . However , due to the lim-   ited availability of multi - modal dialogue data ,   there is still scarce research on multi - modal   dialogue pre - training . Yet another intriguing   challenge emerges from the encompassing na-   ture of multi - modal dialogue , which involves   various modalities and tasks . Moreover , new   forms of tasks may arise at unpredictable points   in the future . Hence , it is essential for designed   multi - modal dialogue models to possess suf-   ficient flexibility to adapt to such scenarios .   This paper proposes PaCE , a unified , struc-   tured , compositional multi - modal dialogue pre-   training framework . It utilizes a combination   of several fundamental experts to accommo-   date multiple dialogue - related tasks and can   be pre - trained using limited dialogue and ex-   tensive non - dialogue multi - modal data . Fur-   thermore , we propose a progressive training   method where old experts from the past can   assist new experts , facilitating the expansion of   their capabilities . Experimental results demon-   strate that PaCE achieves state - of - the - art results   on eight multi - modal dialog benchmarks .   1 Introduction   Enabling seamless communication between hu-   mans and machines is a long - standing goal of artifi-   cial intelligence research . The recent emergence of   chatGPThas increased confidence in the potential   for achieving this goal . Beyond the use of textual   language as a unique interface between humans   and machines , perceiving and utilizing multi - modal   information , especially visual information , has be - Figure 1 : An example of multi - modal dialogue , which   involves multiple tasks , including multi - modal intent   classification , multi - modal state tracking , multi - modal   dialog retrieval and response generation .   come a crucial capability known as multi - modal   dialogue ( Shuster et al . , 2020 ; Sun et al . , 2021 ) .   To facilitate the research on multi - modal dia-   logue , plenty of specific tasks and datasets have   emerged in the community ( Das et al . , 2017 ; Shus-   ter et al . , 2018 ; Feng et al . , 2022 ; Long et al . , 2023 ) .   However , the overall quantity of data is still lim-   ited . Furthermore , multi - modal dialogue presents a   greater challenge compared to traditional text - only   dialogue track ( Hui et al . , 2021 ; He et al . , 2022 ;   Si et al . , 2022 ) , as it involves the integration of   various modalities and more intricate task scenar-   ios . As shown in Figure 1 , the central tasks of   multi - modal dialogue include multi - modal intent   classification ( Zang et al . , 2021 ) , multi - modal dia-   logue retrieval ( Das et al . , 2017 ; Zang et al . , 2021),13402multi - modal dialogue state tracking ( Liao et al . ,   2021 ) , and multi - modal response generation ( Kot-   tur et al . , 2021 ) . Despite pre - training having be-   come the consensus for multi - task learning in ma-   chine learning ( Devlin et al . , 2018 ; Radford et al . ,   2019 , 2021 ) , the research on pre - training models   for multi - modal dialogue is an area that is yet to be   fully explored .   In this paper , we focus on building pre - trained   models of multi - modal dialogue . A key challenge   is to unify different modalities and task forms , and   make the best use of existing multi - modal dialog   and non - dialog data . A recent popular trend on   textual tasks is to build unified pre - trained founda-   tion models by multi - task learning , e.g. , T5 ( Raffel   et al . , 2020 ) . However , it attempts to mix all tasks   learned from scratch thus is difficult to control the   learning process , which is a completely black box .   Although the Mixture - of - Experts ( MoE ) ( Fedus   et al . , 2021 ; Du et al . , 2022 ) architecture attempts   to select independent experts for each input sample   through token - level routing , it lacks specific seman-   tics , i.e. , it is entirely unknown what the experts   are responsible for . We hope to find a new way to   handle many multi - modal dialog tasks simultane-   ously and combine existing concrete skills to learn   new tasks more efficiently .   To this end , we propose PaCE , a unified   multi - modal dialogue pre - training framework with   Progressive andCompositional Experts . First ,   we decompose complicated multi - modal dialogue   into fundamental sub - capabilities that could be   learned with specific data . Different from tradi-   tional MoE , each expert in PaCE is tailored to one   specific fundamental sub - capability of multi - modal   dialogue , including C , C , I ,   G andG .Second , we pro-   pose a progressive pre - training strategy to evolve   the model by controlling the combination of experts   in different pre - training phases . Specifically , in   stage I , we first train on multi - modal non - dialogue   data to obtain C , I , and G   experts . In stage II , we train the C expert ,   which is guided by the C expert on multi-   modal dialog data to learn the dependencies in con-   text . Furthermore , a dialogue G expert   is derived by adding a response generation task   based on the previously learned experts . Third , for   pre - training PaCE , we collect a multi - modal dialog   corpus with 1.4 million dialogs and a multi - modal   non - dialog corpus with 4 million samples . Once   the pre - training of PaCE is finished , we can flexibly   select different capability experts to solve a specific   downstream task .   As illustrated in Figure 2 , PaCE achieves state-   of - the - art performance across a broad range of   multi - modal dialogue benchmarks spanning four   diverse downstream tasks , i.e. , multi - modal in-   tent classification , multi - modal dialogue retrieval ,   multi - modal state tracking , and multi - modal re-   sponse generation This demonstrates that PaCE   not only possesses a flexible model architecture   but also exhibits adaptable training methodologies ,   resulting in remarkable performance .   2 Related Work   Pre - trained Vision - Language Models The pre-   training paradigm , with its successes in natural   language processing ( Devlin et al . , 2018 ; Radford   et al . , 2019 ) , has sparked a revolution in Multi-   modal Learning . ViLBERT ( Lu et al . , 2019 ) was   the first work to adapt the BERT - like architec-   ture for visual - language modeling , allowing for   learning joint representation of images and texts .   ViLT ( Kim et al . , 2021 ) constructed the vision mod-   ule in the same way as the text module with a   unified Transformer ( Vaswani et al . , 2017 ) , elim-   inating the need for resource - intensive image fea-   ture extraction and significantly accelerating the   model . CLIP ( Radford et al . , 2021 ) employed con-   trast learning to directly align images with natural   language texts , eliminating the constraints of pre-   defined image categories . ALIGN ( Jia et al . , 2021 )   and Florence ( Yuan et al . , 2021 ) further general-   ized this idea on noisier but larger image - text pairs .   These models have demonstrated the ability to learn   strong image and text representations for cross-13403modal alignment tasks . In addition , a number of   models ( Cho et al . , 2021 ; Wang et al . , 2021 , 2022 ;   Yu et al . , 2022 ; Alayrac et al . , 2022 ) employed   auto - regressive models to model the association   between images and texts , using a unified gener-   ation approach to construct the task in an end - to-   end manner . Although pre - trained vision - language   models have shown promising results , they mainly   focus on caption texts which are intrinsically dif-   ferent from human conversations ( Kulhánek et al . ,   2021 ) . To our best knowledge , the proposed PaCE   model is the first multi - modal dialogue pre - training   model .   Multi - Modal Dialogue Modeling Numerous ad-   vanced works have been proposed along with the   development of multi - modal dialogue datasets ( Das   et al . , 2017 ; Mostafazadeh et al . , 2017 ; Shuster   et al . , 2018 ; Zang et al . , 2021 ; Zheng et al . , 2021 ;   Kottur et al . , 2021 ; Liao et al . , 2021 ; Feng et al . ,   2022 ) . Several dialogue modeling works ( Qi et al . ,   2020 ; Lee et al . , 2021 ) have been conducted to   improve the performance of conversational agents   in image - grounded dialogue . Zang et al . ( 2021 )   proposed a dual - encoder model that utilized object   labels to encode image features so as to perform   a dialogue - based image retrieval task . Afterward ,   researchers ( Yang et al . , 2021 ; Chen et al . , 2021 )   explored enriching textual expressions of gener-   ated dialogue responses through associative vision   scenes . For textual response tasks , Zheng et al .   ( 2021 ) proposed a multi - modal dialogue genera-   tion model based on Seq2Seq architecture , which   was proved to be superior to the textual Seq2Seq   model . Lee et al . ( 2022 ) proposed a joint multi-   modal encoder - decoder model to incorporate visual   inputs . However , the above models have demon-   strated success in specific sub - tasks with a particu-   lar dataset , which can not meet the requirements   of a wide range of multi - modal dialogue tasks .   To address this challenge , we propose a unified   multi - modal dialogue pre - training model based on   a divide - and - conquer strategy , which can combine   different experts to complete a series of tasks .   3 Pre - training Data Construction   In this paper , we collect both multi - modal non-   dialogue and multi - modal dialogue data for PaCE   pre - training . The total statistics of our collected   pre - training corpora are shown in Table 1 .   Multi - modal Non - dialogue Data ( MultiNonDia-   log ) Similar to previous work ( Kim et al . , 2021 ) ,   we first collect four multi - model non - dialogue   datasets for image and text representation learning ,   including MSCOCO ( Lin et al . , 2014 ) , VG ( Kr-   ishna et al . , 2017 ) , SBU ( Ordonez et al . , 2011 ) and   GCC ( Sharma et al . , 2018 ) . In MultiNonDialog ,   each image is accompanied by one or more cap-   tions whose lengths are generally constrained to 20   tokens . Since GCC and SBU provide only image   URLs , we collect the images via the given URLs   which are still accessible .   Multi - modal Dialogue Data ( MultiDialog ) We   collect six existing multi - modal conversation cor-   pora ranging from online forum chatting logs ( Das   et al . , 2017 ; Shuster et al . , 2018 ; Zang et al . , 2021 ;   Feng et al . , 2022 ) to customer service conversa-   tions ( Liao et al . , 2021 ; Kottur et al . , 2021 ) and   build a large - scale multi - modal dialogue corpus .   To ensure that each conversation has at least one   corresponding image , we eliminate the text - only   conversations from the original datasets . In addi-   tion , to satisfy the requirements of the Stage II pre-   training , we use the BLIP model ( Li et al . , 2022b )   implemented by Li et al . ( 2022a ) to generate the   appropriate textual caption for each image . The   captions are constrained to 20 tokens .   4 Pre - training Method   Given a set of nmulti - modal dialogue samples   D={(U , R ) } , where UandRrepresent   the dialogue context and response , respectively .   Compared to traditional textual dialogue , both   U={u}andR=/braceleftbig   r / bracerightbigcan incorpo-   rate various types of information including textual   texts and visual images , where KandQare the   number of elements , and m∈ { t , v}denotes the   modality of U(orR ) . The notation tindicates   textual utterances , while vindicates visual images .   We devise a divide - and - conquer pre - training13404   strategy for multi - modal dialogue . Concretely , we   decompose complicated multi - modal dialogue into   five fundamental sub - capabilities and design five   corresponding experts ( i.e. , C , C ,   I , G , and G experts ) .   Then , we propose a progressive training strategy to   evolve the model by controlling the combination of   experts in different pre - training phases . Next , we   describe the input representation learning module ,   the divide - and - conquer pre - training strategy , the   pre - training objectives , and the fine - tuning process   in detail .   4.1 Input Representation Learning   The proposed model is designed to handle input   data from two modalities : visual representations   and textual representations .   Visual Representations The dialogue context   and response can be either visual or textual   data . We use Vision Transformer ( Dosovitskiy   et al . , 2020 ) to learn visual representations of   images . Formally , we process the visual image   v∈Rby dividing it into N = HW / P   patches v∈R ( ) , where Cis the number   of channels , ( H , W ) is the resolution of the in-   put image , and Pis the patch resolution . This   allows the model to extract meaningful features   from the image by considering it as a set of small   regions , rather than a single large array of pixels .   The image patches are then flattened into vectors   and processed by a linear projection using a weight   matrixW∈R()and a position embed-   dingW∈R , resulting in patch em-   bedding ¯v∈R , where Eis the dimension   of embedding . The position embedding is used to   add additional information about the position of thepatch in the image . Finally , we obtain the visual   representations Hafter summing patch embed-   dings and position embeddings .   Textual Representations The input text t∈   Ris embedded into a dense representation   ¯t∈Rby using a word embedding matrix   W∈Rand a position embedding matrix   W∈R , where |O|is the size of the   vocabulary , Lis the length of text , and Eis the   dimension of embedding . It is noteworthy that we   usually concatenate the context with the current ut-   terance to form the final textual input . The textual   representations can be denoted as H.   4.2 Divide - and - Conquer Pre - training   Strategy   We devise a novel pre - training strategy in a divide-   and - conquer manner . Specifically , we first divide   the complicated multi - model dialogue into several   sub - problems , which can be learned in an eas-   ier way . The solutions to the sub - problems are   then combined to give a solution to different down-   stream multi - modal dialogue tasks .   Multi - expert Architecture PaCE adopts an ex-   tension of the standard Transformer , which learns   multiple semantic experts instead of a single feed-   forward network ( FFN ) as in the original Trans-   former ( Bao et al . , 2021 ) . Concretely , the experts   share the information from both textual and visual   modalities through a multi - head attention mecha-   nism ( MSA ) , while each expert FFNhas its   own unique parameters to learn a different semantic   representation . Formally , the unique information ,   which is obtained by switching experts in each13405block , can be formulated as :   H = MSA ( LN(H ) ) + H   H = FFN(LN ( H ) ) + H(1 )   where H(l∈[1 , L ] ) represents the output rep-   resentation of the l-1 layer and Lis the number of   Transformer blocks . His the representation   of the k - th expert . The input representation could   be formulated as H= [ H , H ] . Here , MSA   and LN are the standard multi - head self - attention   and layer normalization , respectively .   Modality and Capability Experts As illustrated   in Figure 3 , we divide the complicated multi - modal   dialogue task into five easier sub - problems includ-   ingC modeling , C modeling , I- modeling , G , and G .   We design a semantic expert to solve each sub-   problem . These five experts can be divided into two   categories : modality experts ( C andI- experts ) and capability experts ( G ,   C M andG experts )   tailored for multi - modal dialogue . Ultimately , we   activate the modality and capability experts in a hi-   erarchical manner , with the bottom ( L−F)layers   activating only the modality experts and the top F   layers activating the capability experts , where Fis   a pre - defined hyper - parameter .   Experts Combination for Different Tasks We   propose a progressive cascade pre - training strategy   that solves different multi - modal dialogue tasks   by adaptively combining the solutions to the sub-   problems . We will introduce the details of progres-   sive cascade pre - training in Section 4.3 .   4.3 Pre - training Objectives   Our progressive cascade pre - training process con-   sists of three phases , each with a tailored pre-   training objective .   Stage I : Image - Text Matching In stage I , simi-   lar to ViLT ( Kim et al . , 2021 ) , we use non - dialogue   multi - modal data Dto learn the fundamental inter-   modal alignment , and this stage involves only three   experts , including the C expert , I   expert and G expert . As depicted in   Figure 3(a ) , following word and patch embeddings ,   the text and image are separately processed into   text and image representations by specialized C- andI experts . These representationsare then fused and fed into the G ex-   pert , yielding a unified representation of the image   and text . We then employ the representation of the   ‘ [ CLS ] ’ token from the expert output as the input   for a binary classification network to predict the   alignment between the current text and image . The   loss function of image - text matching is defined as :   L = ECE ( y , p(V , T ) ) ( 2 )   In addition to L , we also employ the MLM loss   Lin this stage for understanding unique tex-   tual modality . Concretely , following the method   of BERT , we randomly select tokens in the text   sequence and replace them with the [ MASK ] token .   The model is trained to predict these masked to-   kens using the context of the remaining unmasked   tokens and the visual clues . We adopt a masking   probability of 15 % . The final output vectors of the   masked tokens are then fed into a classifier over the   entire text vocabulary , with the training loss being   the cross - entropy loss .   L = ECE(y , p(V,ˆT ) )   ( 3 )   where ˆTis a masked text , Vis an original im-   age and p(V,ˆT)denotes the model ’s predicted   probability for the masked token ˆT.DandD   represent multi - modal non - dialogue and dialogue   data , respectively .   The joint loss in stage I can be formulated as :   L = L+L ( 4 )   Stage II : Image - Context Matching In stage II ,   we use multi - modal dialogue data Dto pre - train   PaCE , which aims to model dialogue context for   multi - modal dialogue tasks . At this stage , C- expert will be activated in addition to the   three experts from the first stage . Concretely , in   the second stage , the dialogue context Cis input   toC expert , the images Vare input to I- expert , and the corresponding image captions   Tare input to C expert . The loss function   of image - context matching is defined as :   L = ECE ( y , p(V , T , C ) )   ( 5 )   In addition , we use the C expert learned   in Stage I as a teacher to facilitate the learning of   C expert .   L=/vextenddouble / vextenddoubleH−H / vextenddouble / vextenddouble , ( 6 )   where HandHare the output of the   { L−F}th - layer of C expert and C   expert , respectively .   Besides , we also employ MLM loss in stage II   as defined in stage I , and the joint loss L in13406stage II could be formulated as :   L = L+L+L ( 7 )   Stage III : Generation Modeling The third stage   aims to enable the model to generate responses .   TheG expert is activated , and the input   to this expert is composed of the C expert   and the I expert . The loss function in stage   III is defined as follows :   L=−/summationdisplaylogp(C|V , C)(8 )   Here , we model generative capability by auto-   regression , i.e. , using past dialogue history C   and associated images Vto predict the current turn   Cof a dialogue .   4.4 Fine - Tuning on Downstream Tasks   Once the pre - training of PaCE is finished , we   perform fine - tuning on specific downstream tasks .   Thanks to our divide - and - conquer pre - training ap-   proach , we can flexibly select different capability   experts to solve a specific downstream task . Specif-   ically , for understanding tasks , including intent pre-   diction , dialog retrieval , and dialog state tracking ,   we activate C expert , I expert , and   G expert . For the generation task , i.e.   dialog state tracking , and response generation , we   activate the C expert , I expert , and   G expert .   5 Experiments   5.1 Downstream Datasets   To comprehensively evaluate our PaCE , we conduct   extensive experiments on seven datasets belonging   to four downstream tasks .   Multi - Modal Intent Prediction For multi-   modal intent prediction , PhotoChat ( Zang et al . ,   2021 ) and MMDialog ( Feng et al . , 2022 ) are se-   lected as benchmark datasets . This task aims to   identify the specific intent of the user in the multi-   modal context . More specifically , it predicts the   probability of photo sharing in the upcoming con-   versation turn .   Multi - Modal Dialog Retrieval For text - to-   image retrieval , we select PhotoChat ( Zang et al . ,   2021 ) as our benchmark dataset . It encompasses   12k dialogues , each accompanied by a user photo   exchanged during the conversation . The goal of this   task is to select the most appropriate photo given   the dialog context . For image - to - text retrieval , weselect Image - Chat ( Shuster et al . , 2018 ) to evaluate   our model , which consists of 202k dialogues over   202k images .   Multi - Modal Dialog State Tracking MM-   Conv ( Liao et al . , 2021 ) and SIMMC2.0 ( Kottur   et al . , 2021 ) datasets provide a good base for car-   rying out multi - modal dialog state tracking . The   MMConv dataset contains 5.1k dialogues collected   by enabling multi - modal conversations between   human - to - human role - playing pairs under real - life   traveling scenarios . In contrast , the SIMMC2.0   corpus includes 11,000 task - oriented dialogs in the   shopping domain that are grounded in immersive   and photo - realistic contexts .   Multi - Modal Response Generation Generating   appropriate responses for satisfactory task comple-   tion is the ultimate goal of task - oriented dialogue   agents . In this task , we selected MMConv ( Liao   et al . , 2021 ) and SIMMC2.0 ( Kottur et al . , 2021 )   as our benchmark datasets .   5.2 Experimental Setting   We use the bert - base - uncased tokenizer to tokenize   text inputs . We learn the textual embedding - related   parameters from scratch , instead of fine - tuning   them from pre - trained BERT . For all experiments ,   we use AdamW optimizer ( Loshchilov and Hutter ,   2017 ) with base learning rate of 10and weight   decay of 10 . The learning rate is warmed up for   10 % of the total training steps and is decayed lin-   early to zero for the rest of the training . We set the   total number of the Transformer layers L to 12 , with   the number of layers F for the top Transformer set   to 3 . We initialize the Transformer weights with the   pre - trained ViT ( Dosovitskiy et al . , 2020 ) . In the   pre - training process , we utilize 200 K steps , 25 K   steps , and 10 K steps , respectively , for the three   stages on 8 NVIDIA A100 GPUs with a batch size   of 4,096 .   5.3 Evaluation Metrics   For intent prediction , we adopt the F1 score as the   evaluation metric to measure the effectiveness of   our model , similar to previous work ( Zang et al . ,   2021 ) . For multi - modal dialog retrieval , we use   ranking - based evaluation metrics such as recall   natkincluding R@1 , R@5 and R@10 in ac-   cordance with prior studies ( Zang et al . , 2021 ;   Shuster et al . , 2018 ) . These metrics measure   whether the ground - truth textual or visual outputs13407   are ranked among the top k∈ { 1,5,10}po-   sitions among ncandidate elements . For multi-   modal dialogue state tracking , we report Categor-   ical , Non - categorical andoverall scores as eval-   uation metrics following ( Liao et al . , 2021 ) . To   measure the quality of response generation , we em-   ploy BLEU ( Papineni et al . , 2002 ) as the evaluation   metric for SIMMC2.0 . For MMConv , we report a   combined score ( Comb . ) , which is computed via   ( Inform + Success ) ×0.5+BLEU as an overall   evaluation measure as in ( Mehri et al . , 2019 ) .   5.4 Quantitative Comparison   As shown in Figure 2 and Table 2 , PaCE demon-   strates state - of - the - art performances across a wide   range of multi - modal dialogue tasks . Specifically ,   we have achieved a significant enhancement on the   PhotoChat and MMConv dataset , with an improve-   ment of 4.8 points in multi - modal dialog retrieval   and 21.2 points in multi - modal dialog state track-   ing , respectively . It is worth noting that PaCE has   a total parameter count of 338 million . In addition ,   since some experts may be idle during the execu-   tion of specific downstream tasks , the parameter   size will further decrease for specific downstream   tasks . Below , we provide a detailed analysis of the   results for each sub - task dataset .   Multi - Modal Intent Prediction For the Pho-   toChat dataset , we report the performances of   strong baselines as in ( Zang et al . , 2021 ) , including   ALBERT - base ( Lan et al . , 2019 ) , BERT ( Devlin   et al . , 2018 ) , T5 - base , and T5 - 3B ( Raffel et al . ,   2020 ) . For the MMDialog dataset , we adopt DE++ ,   Divter ( Feng et al . , 2022 ) , and ViLT ( Kim et al . ,   2021 ) as our baseline models . As shown in Table 3 ,   although some models such as T5 - 3B are much   larger than ours , our model still achieves the best   performance on all evaluation metrics .   Multi - Modal Dialog Retrieval For PhotoChat ,   we compare PaCE with strong baselines reported   in ( Zang et al . , 2021 ) , including BM25 ( Robert-   son et al . , 2009 ) , DE(Zang et al . , 2021 ) ,   VSE++ ( Faghri et al . , 2017 ) and SCAN ( Lee et al . ,   2018 ) . We also adapted VLMo ( Bao et al . , 2021 )   and ViLT ( Kim et al . , 2021 ) to perform multi - modal   dialog retrieval . The results on PhotoChat are re-   ported in Table 4 , PaCE achieves substantially bet-   ter performance than the best performing baselines .   For Image - Chat , we compare PaCE with TransRes-   Net152 ( Liao et al . , 2021 ) , VLMo and ViLT , and   report baseline results as in Table 5 . PaCE achieves   the best results for image - to - text dialog retrieval   with 3.0 improvement in terms of Sum .   Multi - Modal Dialog State Tracking For MM-   Conv dataset , we compare PaCE with DS-   DST(Zhang et al . , 2019 ) ; for SIMMC2.0 dataset ,   we compare PaCE with GPT-2 ( Radford et al . ,   2019 ) , MTN ( Le et al . , 2019 ) , BART - large and13408   BART - base ( Lewis et al . , 2019 ) . The results on   MMConv and SIMMC2.0 are reported in Table 6   and Table 7 , respectively . PaCE can achieve the   best results on most of the evaluation metrics . No-   tably , we observed that the PaCE achieves com-   petitive results at smaller parameter scales than   previous SOTA in SIMMC2.0 slot F1 .   Multi - Modal Response Generation For the re-   sponse generation task , we conduct experiments on   SIMMC2.0 and MMConv datasets . For MMConv ,   we adopt the strong baseline SimpleTOD ( Hosseini-   Asl et al . , 2020 ) implemented by ( Liao et al . ,   2021 ) . We summarize the experimental results of   SIMMC2.0 and MMConv in Table 7 and Table 8 ,   verifying the effectiveness of our model in both   discriminative and generative tasks .   5.5 Ablation Study   Effectiveness of Pre - training Objectives To   evaluate the effectiveness of each stage of pre-   training , we conduct an ablation study by remov-   ing Stage I pre - training ( PaCE ) , removingStage II pre - training ( PaCE ) , removing   Stage III pre - training ( PaCE ) , and remov-   ing both Stage II and Stage III ( PaCE ) .   For a fair comparison , the experimental setup of the   ablation study is consistent with that of the primary   experiments , utilizing the same hyper - parameters   and downstream fine - tuning strategy . The ablation   test results on PhotoChat and Image - Chat are pro-   vided in Table 9 . We can observe that image - text   matching ( Stage I ) and image - context matching   ( Stage II ) play the most important role in PaCE .   This is within our expectation since Stage I and   Stage II are the basis of the latter generation mod-   eling ( Stage III ) . It is no surprise that combining   all three stages achieves the best performance on   the experimental datasets . We also investigate the   impact of Lby removing it from Stage II pre-   training ( denoted as PaCE ) . We can ob-   serve that Lhas a significant impact on the per-   formance of PaCE in Stage II pre - training .   Effectiveness of Pre - training Data In addition ,   we also conduct an ablation study to verify the im-   pact of different pre - training data on PhotoChat   and Image - Chat datasets . We define the mod-   els that only use MultiNonDialog and MultiDi-   alog for pre - training as PaCE   and PaCE , respectively . The abla-   tion test results on PhotoChat and Image - Chat are   provided in Table 10 . We can observe that both   MultiNonDialog and MultiDialog pre - training cor-   pora contribute great performance improvement   to PaCE . This is within our expectation since the   MultiNonDialog data helps our model learn impres-   sive image - text representations and their alignment ,   while the MultiDialog data encourages PaCE to   capture the dialog context information.134096 Conclusion   In this paper , we proposed PaCE , a unified , struc-   tured , compositional multi - modal dialogue pre-   training framework , which adopted a divide - and-   conquer strategy . We first break down the com-   plicated multi - modal dialogue generation task into   several sub - capabilities , which could be learned   in an easier way . Then , the solutions to the sub-   capabilities were combined to obtain an effective   and efficient solution to each downstream multi-   modal dialogue task . Experimental results on   eight benchmark datasets demonstrated that PaCE   achieved new state - of - the - art performances .   Discussion   PaCE adopts a flexible model structure that decom-   poses complex multimodal dialogues into basic   sub - capabilities . As a result , it can be trained pro-   gressively on different data and exhibits excellent   expandability , making it applicable to new tasks .   An additional advantage is that it aligns well with   various attempts to enhance performance in terms   of interpretability . However , we believe that there   are still many aspects of PACE that are worth ex-   ploring . First is the exploration of incorporating   additional modalities and investigating whether the   self - attention layer can effectively handle a broader   range of modalities for a unified representation .   Another aspect worth exploring is the development   of a more efficient approach for adapting multi-   modal models to diverse downstream applications ,   eliminating the necessity to fine - tune all parameters   of the model . Furthermore , given the substantial   variations in the model networks employed for text   generation and image generation in contemporary   research , exploring the integration of multi - modal   generation into a unified framework is a worthwhile   endeavor .   Limitations   To better analyze the limitations of PaCE , we carry   out an analysis of the errors made by PaCE on the   PhotoChat and SIMMC2.0 test sets . We reveal sev-   eral reasons for the errors , which can be divided   into the following categories . First , since there are   many similar images in the datasets , PaCE fail to   distinguish some gold image from similar candi-   dates . This may be because we do not design an   explicit fine - grained reasoning module to capture   the details of images and texts . For example , for thecontext mentions “ I and my dad both have a cam-   era ” , our model can capture the entity “ camera ” ,   but fails to reason the fact that there should be two   cameras . One possible solution is to introduce a   deep reasoning and comprehension strategy to em-   power the model with excellent reasoning ability .   Second , due to the lack of fine - grained structural   understanding of the images , the sentences gener-   ated by PaCE suffer from identifying the relative   positions of entities . For example , PaCE may have   difficulties recognizing the fact that the right side   of a yellow shirt is black pants . This issue is partic-   ularly severe in SIMMC as there are many entities   in the pictures and spatial descriptions of entities   in the responses . One possible idea is to extract   the relative positions of objects mentioned in the   conversation as auxiliary data to guide the model ’s   generation .   Acknowledgements   Min Yang was partially supported by the Na-   tional Key Research and Development Pro-   gram of China ( 2022YFF0902100 ) , Shenzhen   Science and Technology Innovation Program   ( KQTD20190929172835662 ) , Shenzhen Basic   Research Foundation ( JCYJ20210324115614039   and JCYJ20200109113441941 ) , and NSFC ( no .   92270122 ) . This work was supported by Alibaba   Group through Alibaba Innovative Research Pro-   gram .   References134101341113412   A Case Study   To evaluate PaCE qualitatively , we choose two ex-   emplary conversations from PhotoChat and Image-   Chat test sets , and illustrate the retrieved responses   by PaCE in Figure 4 and Figure 5 . Our PaCE model   can retrieve highly relevant candidates to the con-   versation scenario . For the text - to - image ( T2I ) re-   trieval task , since the candidate images could be   quite similar , it is challenging to retrieve the exact   ground - truth image from the candidates . Although   PaCE may not obtain the ground - truth image , we   can still obtain the relevant candidate images.1341313414ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   We put the limitation after section 6 .   /squareA2 . Did you discuss any potential risks of your work ?   There are no potential risks in our work .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   We sumarize it in section 1 .   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   in Section 4 .   /squareB1 . Did you cite the creators of artifacts you used ?   in Section 2 .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Not applicable . Left blank .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Left blank .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   Not applicable . Left blank .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Not applicable . Left blank .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   In section 3 .   C / squareDid you run computational experiments ?   In section 5 .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   In section 5.213415 / squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   In section 5.2   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Not applicable . Left blank .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   In section 5.3   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.13416