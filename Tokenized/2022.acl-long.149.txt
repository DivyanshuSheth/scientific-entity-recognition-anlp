  Hai YeHwee Tou NgWenjuan HanDepartment of Computer Science , National University of SingaporeBeijing Institute for General Artificial Intelligence ( BIGAI ) , Beijing , China   { yeh,nght}@comp.nus.edu.sg   hanwenjuan@bigai.ai   Abstract   In conversational question answering ( CQA ) ,   the task of question rewriting ( QR ) in context   aims to rewrite a context - dependent question   into an equivalent self - contained question that   gives the same answer . In this paper , we are   interested in the robustness of a QR system to   questions varying in rewriting hardness or diffi-   culty . Since there is a lack of questions classi-   fied based on their rewriting hardness , we first   propose a heuristic method to automatically   classify questions into subsets of varying hard-   ness , by measuring the discrepancy between   a question and its rewrite . To find out what   makes questions hard or easy for rewriting , we   then conduct a human evaluation to annotate   the rewriting hardness of questions . Finally , to   enhance the robustness of QR systems to ques-   tions of varying hardness , we propose a novel   learning framework for QR that first trains a QR   model independently on each subset of ques-   tions of a certain level of hardness , then com-   bines these QR models as one joint model for   inference . Experimental results on two datasets   show that our framework improves the overall   performance compared to the baselines .   1 Introduction   In conversational question answering ( CQA ) ( Choi   et al . , 2018 ; Reddy et al . , 2019 ) , several sequential   questions need to be answered one by one given   a relevant article . To answer a question in CQA ,   we need to understand the historical context of   the question . For example , to answer the ques-   tion “ When did he begin writing these pieces ? ” ,   we need to know what herefers to in the conver-   sation context . In our work , we address question-   in - context rewriting ( QR ) , which aims to rewrite a   context - dependent question into an equivalent self-   contained question in CQA , e.g. , replacing hein the   Table 1 : One dialogue example from Elgohary et al .   ( 2019 ) including questions ( q ) and answers ( a ) and   certain rewrites ( q ) of the questions .   above example with its referent from the context .   The task is formulated as a text generation task that   generates the rewrite of a question given the origi-   nal question and its conversation context ( Elgohary   et al . , 2019 ) .   We are interested in how robust a QR system is   to questions with different rewriting hardness ( or   difficulty ) . As we can see from the examples in   Table 1 , rewriting the question qrequires only   replacing the pronoun heby its referent , which   usually appears in the conversation context , and   the model can identify the referent by attention   ( Luong et al . , 2015 ) . However , for the question q ,   to find the missing aside from clause , the model   needs to understand the entire conversation since   the question asks about other interesting aspects   about the article related to the topic of the entire   conversation . Understanding the whole context   will be challenging for the model . Can a QR model   still work well when rewriting the hard questions ?   In section 6.3 , our first study is on evaluating   the performance of a QR model under questions   varying in hardness . One issue in this process is2100that there is a lack of classified questions in dif-   ferent rewriting hardness . Though we can rely on   human labor to annotate the questions , it is expen-   sive and not scalable . Instead , we propose a simple   yet effective heuristic method to classify the ques-   tions automatically . We measure the discrepancy   between a question and its rewrite , where the larger   the discrepancy , the more difficult to rewrite the   question . The intuition is that if a question is very   dissimilar to its rewrite , more information has to   be filled into the rewrite , which means the question   is harder to rewrite . We specifically use the BLEU   score to measure the discrepancy , and lower scores   mean larger discrepancies . Using this method , we   then split the questions into three subsets : hard ,   medium , and easy , and evaluate the baseline sys-   tems using these subsets .   In order to verify the classified subsets and find   out what makes questions different in rewriting   difficulty , in section 6.3.2 , we further evaluate the   question characteristics in hard , medium , and easy   subsets through human evaluation . We first manu-   ally summarize the commonly used rules for rewrit-   ing questions from the training set , and then anno-   tate the questions using the labels of summarized   rewriting rules , followed by counting the number   of these rewriting rules used in these subsets .   Finally , to enhance the robustness of a QR model   to questions varying in difficulties , we propose   a novel learning framework in section 5 , where   we first separately train a QR model on each   hard , medium , and easy subset , and then com-   bine these models into a joint model for infer-   ence . Training one sole model on each subset is   to let the model better learn domain - specific in-   formation to deal with one specific type of ques-   tions ( hard / medium / easy ) . By combining the mod-   els together , we have a joint model capable of   rewriting questions differing in rewriting hardness .   Specially , we introduce adapters ( Houlsby et al . ,   2019 ) to reduce parameters when building pri-   vate models and we present sequence - level adapter   fusion and distillation ( SLAF and SLAD ) to ef-   fectively combine the private models into a joint   model .   Our contributions in this paper include :   •We are the firstto study the robustness of a   QR system to questions with varying levels of   rewriting hardness ;   •We propose an effective method to identify   questions of different rewriting hardness;•We manually annotate questions sampled   from the subsets with summarized rewriting   rules for validity and address what makes   questions hard or easy for rewriting ;   •We propose a novel QR framework by taking   into account the rewriting hardness .   We have the following observations in our paper :   •The baseline systems perform much worse on   the hard subset but perform well on the easy   subset ;   •We find that easy questions usually only re-   quire replacing pronouns but hard questions   involve more complex operations like expand-   ing special Wh * questions ;   •Experiments show that our QR learning frame-   work enhances the rewriting performance   compared to the baselines .   2 Related Work   Elgohary et al . ( 2019 ) created the QR dataset which   rewrites a subset of the questions from QuAC ( Choi   et al . , 2018 ) . Based on this dataset , some recent   work has studied this task and formulates QR as   a text generation task with an encoder - decoder ar-   chitecture ( Elgohary et al . , 2019 ; Kumar and Joshi ,   2016 ; Vakulenko et al . , 2020 ; Li et al . , 2019 ; Lin   et al . , 2020a ) .   The difficulty of answering a question given a   relevant document has been studied in the ques-   tion answering community ( Dua et al . , 2019 ; Wolf-   son et al . , 2020 ) . Sugawara et al . ( 2018 ) examine   12 reading comprehension datasets and determine   what makes a question more easily answered . Perez   et al . ( 2020 ) ; Min et al . ( 2019 ) ; Talmor and Be-   rant ( 2018 ) ; Dong et al . ( 2017 ) study how to make   a hard question more easily answered . However ,   there is no work to date that studies whether rewrit-   ing difficulties exist in QR and how to measure   the difficulties . Some other work is similar to QR   but focuses on other tasks such as dialogue track-   ing ( Rastogi et al . , 2019 ; Su et al . , 2019 ; Liu et al . ,   2020 ) and information retrieval ( V oskarides et al . ,   2020 ; Lin et al . , 2020b ; Liu et al . , 2019 ) .   Varying rewriting difficulties can result in mul-   tiple underlying data distributions in the QR train-   ing data . The shared - private framework has been   studied to learn from training data with multiple   distributions ( Zhang et al . , 2018 ; Liu et al . , 2017 ) .   One issue of the shared - private framework is pa-   rameter inefficiency when building private models   We use adapter tuning ( Rebuffi et al . , 2018 , 2017)2101to build the private models . Adapter tuning was   recently proposed for adapting a pre - trained lan-   guage model , e.g. , BERT ( Devlin et al . , 2019 ) , to   downstream tasks ( Pfeiffer et al . , 2020a , c ; Houlsby   et al . , 2019 ) , and its effectiveness has been verified   by previous work ( Bapna and Firat , 2019 ; Pfeiffer   et al . , 2020b ; Wang et al . , 2020 ; He et al . , 2021 ) .   We are the first to apply it to reduce model pa-   rameters in the shared - private framework . How to   combine the knowledge stored in multiple adapters   is also important . Pfeiffer et al . ( 2020a ) propose   adapter fusion to build an ensemble of adapters in   multi - task learning . We propose sequence - level   adapter fusion in our work .   3 Question - in - Context Rewriting   Question - in - context rewriting ( QR ) aims to gen-   erate a self - contained rewrite from a context-   dependent question in CQA . Given a conver-   sational dialogue Hwith sequential question   and answer pairs { q , a,···,q , a } , for a   question qfromHwith its history h=   { q , a,···,q , a } , we generate its rewrite   q. We define the labeled dataset D=   { q , h , q}which is a set of tuples of question   q , history h , and rewrite q. Following previous   work ( Elgohary et al . , 2019 ) , we model QR in an   encoder - decoder framework , by estimating the pa-   rameterized conditional distribution for the output   qgiven the input question qand history h. For   ( q , h , q)∈ D , we minimize the following loss   function parameterized by θ :   L = −logP(q|q , h;θ )   = −XX1{q = k}logP(q = k|q , q , h;θ )   ( 1 )   in which Tis the length of qand|V|is the vo-   cabulary size . Following Elgohary et al . ( 2019 ) , q   andhare concatenated into one sequence as the   input . All previous turns of the history informa-   tion are combined for learning . The choice of the   encoder - decoder framework can be LSTM ( Elgo-   hary et al . , 2019 ) , transformer ( Vakulenko et al . ,   2020 ) , or pre - trained language models ( Lin et al . ,   2020a ) . In our work , we build our model based   on the pre - trained language model BART ( Lewis   et al . , 2020).4 Difficulty of Question Rewriting   The difficulty of rewriting a question varies across   questions . We propose a simple yet effective heuris-   tic to formulate rewriting difficulty as the discrep-   ancy between a question and its rewrite . To gen-   erate a self - contained rewrite , we need to identify   relevant information from the conversation context   to incorporate it into the original question . We ob-   serve that if the discrepancy is large , we need to   identify more missing information from the con-   versation context which makes the rewriting task   more difficult .   In this work , we use BLEU score to measure   the discrepancy . BLEU has been widely used to   measure how similar two sentences are ( Papineni   et al . , 2002 ) . Given a question qand its rewrite q ,   we define the difficulty score zfor rewriting qas :   z = BLEU ( q , q ) ( 2 )   where the rewrite qis the reference and z∈[0,1 ] .   A low zscore indicates a larger discrepancy be-   tween qandq , making it more difficult to rewrite   qintoq . Besides BLEU , we also study the ef-   fectiveness of ROUGE , lengths of qandq , and   |q|/|q|in§6.5 to measure rewriting difficulty .   5 Difficulty - Aware QR with Adapters   Previous work on QR learns to rewrite questions   with only one shared model ( Elgohary et al . , 2019 ) ,   which can not adequately model all questions with   different rewriting difficulties . Instead of using   only one shared model , we propose a novel method   to classify a question into several classes by mea-   suring its rewriting difficulty ( § 5.1 ) , learn a private   model for each class ( § 5.2 ) , and finally combine   the private models for inference ( § 5.3 ) . Different   questions with varying rewriting difficulties result   in multiple data distributions in the training set .   By dividing the training data into several classes   with varying rewriting difficulties , we can better   learn the data distributions with the help of private   models ( Zhang et al . , 2018 ) .   5.1 Question Classification   We compute the difficulty score zof each question   in the dataset . We set score intervals and group   the questions with difficulty scores within the same   interval together . Specifically , we divide the origi-   nal dataset Dintomclasses : { D , D,···,D } .   Setting mto a large number ( e.g. , the number of   training samples ) can more accurately model the2102   data distribution of the training data , but at the   expense of data sparsity in each class such that a   private model can not be adequately trained .   5.2 Learning Private Models   After dividing the questions into mclasses , we   learn a private model for each class . By training on   each class of data , the private model can better learn   the domain - specific information . The common way   to use a pre - trained language model ( PLM ) such as   BART is to fine - tune the model on the downstream   task . However , doing so will require mtimes the   number of PLM parameters to build all private   models , where mis the number of classes . This   results in a large number of parameters , leading to   inefficiency .   To reduce the number of model parameters in   learning the private models , we introduce adapters   into the PLM . Adapters are light - weight neural   networks and are plugged into the PLM . When   adapting the PLM to downstream tasks , we only   need to update the parameters of the adapters but   keep the original parameters of the PLM frozen and   shared among all private models . Where to place   the adapters in the neural architecture will affect   the efficacy of adapters . As shown in Figure 1 ,   for each transformer layer in the encoder , we add   the adapters after the self - attention layer and feed-   forward layer . We further add the adapters after the   cross - attention layer in the decoder . Though our   model is built on BART , our proposed placement   of adapters can also be used in other PLMs , such   as T5 ( Raffel et al . , 2020 ) .   In Figure 1 , the adapter is a module with a stack   of two linear layers following Houlsby et al . ( 2019 ) .   Formally , given an input hidden vector xfrom the   previous layer , we compute the output hidden vec-   torxof the adapter as :   x = f(tanh ( f(x ) ) ) + x ( 3 )   where f(·)is the down - scale linear layer and f ( · )   is the up - scale linear layer . The hidden vector size   is smaller than the dimension of the input vector .   Learning a private model for one class only intro-   duces 5×Nadapters , where Nis the number of   layers in the encoder and decoder . The original pa-   rameters of the PLM are shared by all adapters , so   the number of parameters required when building   the private models can be much reduced .   5.3 Model Ensemble   After learning the private models for all classes , at   test time , we present the question to the correspond-   ing private model to generate its rewrite if we know   which class this question belongs to . However , it is   not possible to determine the difficulty score by cal-   culating the BLEU score between the question and   its rewrite since there is no gold - standard rewrite   for the question at test time . As such , we need   to combine the private models into one model for   inference . In this work , we propose two methods   to combine the private models , as explained below .   Sequence - level Adapter Fusion ( SLAF ) . After   dividing the training set into mclasses based on the   difficulty scores , we assign a difficulty label to each   class to obtain a set of class labels { l , l , · · · , l } .   We introduce a classifier to learn to predict the   difficulty label l , given a question qand its con-   versation history h. As shown in Figure 2 , during   inference , we obtain the logistic output from each   private model . The classifier generates the class   distribution to combine the logistic outputs for se-   quence generation .   By assigning a difficulty label to each question ,   we obtain the dataset D={q , h , q , l } . For   each training sample ( q , h , q , l)∈ D , we mini-2103mize the following loss function :   L=−log softmax Xαf(q , h;θ)   −logP(l|q , h;θ)(4 )   where fis the ith private model , αis the class   weight of the ith private model , and θis the pa-   rameter of the classifier . We jointly estimate the   conditional distribution for sequence generation   and the distribution for classification . In this pro-   cess , the private models are frozen and not updated .   We combine the vectors out of the private models to   calculate the vector fas the input for the classifier :   f=1   mXf ( q , h;θ ) ( 5 )   where fis the encoder of the ith private   model . For each private model , we average the to-   ken embeddings from the last layer of the encoder .   Sequence - level Adapter Distillation ( SLAD ) .   SLAF provides a way to combine the private mod-   els , but it is time - consuming during inference since   it requires each private model to compute its lo-   gistic output before combination . Another draw-   back is that the domain classifier in SLAF can not   generate the best class distributions at test time ,   causing non - optimal rewriting results by SLAF . As   shown in Figure 2 , to speed up inference and better   combine the private models , we distill the private   models into one shared model . We expect the stu-   dent model S(modeled by adapters ) to be able to   generate questions with different rewriting difficul-   ties . For each training sample ( q , h , q , l)∈ D ,   we define the knowledge distillation loss function   as follows :   L=−XXP{q = k|q , q , h;θ }   ×logP(q = k|q , q , h;θ )   ( 6 )   in which we approximate the output distribution of   the teacher private model lparameterized by θ   with the student model parameterized by θ . We   learn the student model with the following function :   L= ( 1−γ ) · L+γ · L(7 )   where Lis the same loss function in Eq . 1 ,   andγis a hyper - parameter . The private models are   fixed in the distillation process . Since we directly   distill the knowledge of the private models into a   shared model without the soft weights generated by   the domain classifier from SLAF , SLAD can bet-   ter combine the private models and achieve better   rewriting performance .   6 Experiments   6.1 Dataset   We conduct our experiments on ( Elgo-   hary et al . , 2019 ) and QRCC(Anantha et al . ,   2021 ) , which are designed for the task of ques-   tion rewriting in CQA . was created from   QuAC ( Choi et al . , 2018 ) , by rewriting a subset of   the questions by humans . The dataset consists of tu-   ples of question , conversation history , and rewrite .   QRCCanswers conversational questions within   large - scale web pages . Detailed data splits for the   two datasets are shown in Table 2 . We divide the   questions into hard , medium , and easy classes , and   the statistics are presented in Table 3 .   6.2 Setup   Model Settings . We build our models on the pre-   trained language model of BART ( Lewis et al . ,   2020 ) . Specifically , we use BART - base to initial-   ize our models . There are 6 transformer layers for   the encoder and decoder in BART - base . For our2104   adapter , we map the dimension of the input hidden   vector from 768 to 384 which is re - mapped to 768   for the output vector . The hidden vector size for   adapter tuning is the default value of 384 . Based on   BART - base , we need a total of 6×2 + 6×3 = 30   adapters for each private model . We set γto 0.5   in Eq . 7 for and 0.9 for QRCC.αfrom   Eq . 4 is set to 2 for both andQRCC .   When fine - tuning BART , we set the learning rate   to 1e-5 , and for adapter tuning , the learning rate is   1e-4 ( both values are tuned from { 1e-4 , 1e-5 } ) . We   use the validation set to keep the best model basedon the BLEU score . We implement our models   with HuggingFace ( Wolf et al . , 2019 ) and keep the   other default training settings . In , about   20 % of the questions can be rewritten by replac-   ing pronouns with their referents , so we carry out   pronoun replacement first for the questions ( if any )   before using BLEU scores to measure rewriting   difficulties . More details are given in Appendix A.   Baselines . We compare to the following baselines .   Sdenotes training only one shared model with all   the training data , which is commonly used in previ-   ous work ( Elgohary et al . , 2019 ; Lin et al . , 2020a ) .   By adapting BART , P - hard , P - medium , and P-   easy are the baselines that train private models on   the hard , medium , and easy classes respectively ,   using fine - tuning or adapter - tuning . Assuming that   rewriting difficulty labels are accessible for ques-   tions at test time ( i.e. , the oracle setting ) , Mix - gold   processes a question by the corresponding private   model using the difficulty label . SLAF andSLAD   denote sequence - level adapter fusion and adapter   distillation respectively for combining the private   models of P - hard , P - medium , and P - easy . SLAF-   uni . combines the private models with uniform   distributions . SLAF - pred predicts the class label   for the input and then chooses the corresponding   private model for generation . LSTM - S trains one   model using an LSTM - based Seq2Seq model with   copy mechanism ( See et al . , 2017 ) which was used   in Elgohary et al . ( 2019 ) .   Evaluation Metric . Following Elgohary et al .   ( 2019 ) , we use BLEUto obtain the results on hard ,   medium , and easy classes , and the three results are2105averaged to obtain the mean result .   6.3 Robustness Evaluation   6.3.1 Rewriting Difficulty   We first study rewriting difficulties across differ-   ent questions . Table 4 shows the results on hard ,   medium , and easy classes on .Each class   vs. Overall : Comparing to the overall results , the   rewriting performances of hard questions drop sub-   stantially , but are much higher on the easy class .   LSTM - S vs. BART - S : By comparing LSTM - S   to tuning on BART , LSTM - S achieves higher per-   formance on the easy class but much worse per-   formance on hard and medium classes . This is   probably because for easy questions , the model   only needs to copy some words from the context   and LSTM - S has an explicit copy mechanism to   achieve this goal but not BART . Since BART learns   a more complex model than LSTM - S , it can better   deal with harder questions .   We further divide the test set into ten classes in   Figure 3 , where the interval [ 0,1]is equally divided   into ten sub - intervals of size 0.1 . We find that when   zgets smaller , rewriting performance degrades , in-   dicating an increase in rewriting difficulty .   6.3.2 Human Evaluation   The above evaluation results show that our method   can effectively divide the questions into subsets   with different rewriting difficulties . Here , we con-   duct a human evaluation to evaluate the question   characteristics on these subsets for validity and see   what makes the questions hard or easy to rewrite .   Question Annotation . To find out what makes   the questions different , we first summarize the com-   monly used rewriting rules , which describe the op-   erations of translating a question into its rewrite .   6 rules are summarized from the training set of and presented in Table 5 . Different rules   account for different rewriting hardness for QR sys-   tems . For example , the rule of replace pronoun   is very simple since it only requires the model to   determine the pronoun to replace . However , rules   5 and 6 shown in the table will be much harder   because the model needs to understand the con-   versational history well , and the information to be   filled in is substantial .   Then we randomly select 50 examples from each   subset ( hard , medium , and easy ) from the test set   and annotate what rules in Table 5 are used for   each example . One question may have multiple   rewriting rules . More details are in Appendix B.   Results . We sum the number of each rewriting   rule in each subset and show the distributions of   rewriting rules for each subset in Figure 4 . The   three distributions are quite different . We find that :   •the easy subset mainly uses rule 1 for rewrit-   ing questions ;   •for medium and hard subsets , other rules are   used , such as rules 2 , 3 , and 4 which are more   complex than rule 1 ;   •the hard class uses more rules 2 , 3 , 5 , and 6   compared to the medium class , which demon-   strates that the hard class is more difficult than   the medium class .   Discussion . By knowing the characteristics of   each class of questions , we can optimize the model   architecture of private models accordingly . For   hard questions , we can add some rules to deal with   Wh * questions . For easy questions , LSTM - based   models seem to be good enough as Table 4 indi-   cates . In this work , we have shown that the ques-   tions vary in rewriting difficulties and to improve   the overall rewriting performance , we focus on the   ensemble method to combine the private models .   We leave optimizing the model architecture to fu-   ture work .   6.4 Question Rewriting   We report our results on question rewriting based   on andQRCC . From the results in Ta-   bles 6 and 7 , we first show the results of each class ,   then the mean performances are displayed . Mix-   gold , SLAF , SLAD vs. S : ( a ) Mix - gold , SLAF,2106   and SLAD are consistently better than S , which   demonstrates the effectiveness of learning private   models to model multiple underlying distributions .   ( b ) From the results on each class , SLAF and   SLAD can substantially enhance the performance   on medium and easy classes compared to S. ( c )   SLAD is more effective than SLAF and SLAD   is more efficient during inference . ( d ) We find   Mix - gold to be better than SLAF and SLAD , since   Mix - gold is an oracle model that uses the correct   difficulty label to select the private model for infer-   ence .   We find that by learning a private model for each   class , the performance on the corresponding class   can be consistently improved , which explains why   Mix - gold , SLAF , and SLAD can outperform S.   We also find that the sole private model can not   improve the overall rewriting performance of the   three classes , but SLAF and SLAD can outperform   S after model ensemble , which demonstrates the   necessity of combining the private models .   Model Ensemble . One question is whether the im-   provements of SLAF and SLAD simply come from   combing multiple models and whether applying   only one private model selected by the predicted   class label is better . As shown in Tables 6 and   7 , we find SLAF - uni . performs worse than SLAF   and SLAD , which demonstrates that the benefits   of SLAF and SLAD are not simply because of the   model ensemble , but class estimation also helps ( In   SLAD , class estimation lies in using gold class la-   bels of questions for knowledge distillation during   training ) . SLAF - pred can be regarded as an ensem-   ble method since it uses multiple private models   during inference . Compared to SLAF , SLAF - pred   uses one - hot class weights to combine the private   models . However , SLAF - pred performs worse than   SLAF , and the reason could be that classifying the   question into the corresponding class is nontrivial ,   wrong predictions will have much worse rewriting   results as the results of P - hard , -medium , -easy on   other classes indicate .   6.5 Further Analysis   Analysis of Rewriting Difficulty Measures . In   our work , we use BLEU to measure the discrep-   ancy between a question and its rewrite . We further   experiment with other methods to assess their ef-   fectiveness for difficulty measurement .   is evaluated here . As shown in Table 8 , we first   use the length of a question ( |q| ) , its rewrite ( |q| ) ,   and their ratio ( |q|/|q| ) to calculate a difficulty   score . After re - ranking the questions with a diffi-   culty score , we divide the ranked questions equally   into three classes . Interestingly , we find that |q|   works well . After analysis , we find that rewriting   short questions requires finding much missing in-   formation , which makes short questions hard ques-   tions . The |q|/|q|metric is not very useful , since   |q|/|q|can only measure the discrepancy in ques-   tion lengths , but does not necessarily measure their   semantic difference . |q|does not work for diffi-   culty measurement . Not surprisingly , the ROUGE   score is also useful in measuring discrepancy just   like BLEU .   Analysis of Learning Data Distribution . Ta-   bles 6 and 7 show that learning private models   can enhance performance on each class . We fur-   ther divide the data into eleven classes ( z∈[0,0.1 ] ,   ( 0.1,0.2],···,(0.9,1),1 ) and learn a private model   for each class . We build the private models using   LSTM - S , in which we first train a shared model   on the full training data , then fine - tune the shared   model on each class to obtain the private models .   Table 9 shows the BLEU scores where the score   in the ( i , j ) entry is obtained by training on class2107   iand testing on class j. On the whole , learning   private models can enhance the performance of the   corresponding class . With these private models ,   we can better model the data distributions , but how   to combine a large number of private models is a   challenge , since it is hard to train a classifier to   correctly predict so many class labels , which will   have some negative effects on the model ensemble .   Analysis of SLAF & SLAD . We plot the class   distributions of hard , medium , and easy classes in   Figure 5 . We find that in the hard class , the class   weights are almost equally distributed among the   private models , which means that the hard ques-   tions are difficult for classification . This result ex-   plains why SLAF performs worse than S for hard   questions in Tables 6 and 7 . We further study the   contribution of distillation in SLAD . In Figure 6 ,   on the whole , when γincreases , the contribution of   distillation decreases , and the performance drops ,   indicating that distillation is important for SLAD .   Case Study . We further show generated rewrit-   ing samples of various methods on in   Appendix C.   7 Conclusion   In this work , we study the robustness of a QR sys-   tem to questions varying in rewriting hardness . We   use a simple yet effective heuristic to measure the   rewriting difficulty . We further propose a novel   method to deal with varying rewriting difficulties .   Tested on andQRCC , we show the ef-   fectiveness of our methods .   Acknowledgments   This research is supported by the National Research   Foundation , Singapore under its AI Singapore Pro-   gramme ( AISG Award No : AISG - RP-2018 - 007   and AISG2 - PhD-2021 - 08 - 016[T ] ) . The computa-   tional work for this article was partially performed   on resources of the National Supercomputing Cen-   tre , Singapore ( https://www.nscc.sg ) .   References21082109   A Experimental Setup   We use HuggingFace ( Wolf et al . , 2019 )   to implement our model . We follow the   training script from to train the model . Models are   trained for 10 epochs . Batch size is selected from   { 10 , 16 , 32 } . Learning rate is selected from { 1e-5 ,   1e-4 } . We train 10 epochs for CANARD and 8   epochs for QRCC . The best model based on the   BLEU score on the validation set is kept . The   beam width for beam search is the default value of   4 .   For our QR framework , we first train a private   model for each class . For model ensemble , the   weights of the private models are frozen without   updating . On QRCC , to build the private models ,   on each class of data , we fine - tune the shared model   which is trained on all the training data , since we   find that this can enhance the final performance ,   but on , we do not see the improvement .   The learning rate of fine - tuning in this process is   1e-5 .   To pre - process the dataset , we only tokenize the   sentences . And we append the question and its   history context with “ ||| ” .   In , about 20 % of the questions can be   rewritten by replacing pronouns with their refer-   ents , so we carry out pronoun replacement first for   the questions ( if any ) before using BLEU scores to   measure rewriting difficulties . B Human Assessment for Rewriting   Rules   We first ask one annotator to summarize some com-   mon rewriting rules by looking at the training set   of ( Elgohary et al . , 2019 ) . When ac-   cessing the rewriting rules used for each question ,   the second annotator will rely on the summarized   rewriting rules for annotation . For each class , we   randomly select 50 questions from the test set for   annotation .   Case Study . Table 10 shows some annotated   results from the hard , medium , and easy classes .   C Case Study of Generated Rewrites   We further show some cases of generated rewrites   from various methods ( S , Mix - gold , SLAF , and   SLAD ) . We use adapter tuning to build these mod-   els . Tables 11 , 12 , and 13 show the generated   rewrites on hard , medium , and easy classes respec-   tively.2110211121122113