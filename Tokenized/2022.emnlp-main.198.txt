  Haejun LeeAkhil KediaJongwon Lee * Ashwin Paranjape   Christopher D. ManningKyoung - Gu Woo *   ♣ Samsung Research ♦ Samsung Electronics   ♠ Stanford University ♥ Growdle Corporation   { haejun82.lee , akhil.kedia , jay722.lee}@samsung.com   { ashwinp , manning}@cs.stanford.edu , epigramwoo@growdle.com   Abstract   Recent approaches to Open - domain Question   Answering refer to an external knowledge base   using a retriever model , optionally rerank pas-   sages with a separate reranker model and gen-   erate an answer using another reader model .   Despite performing related tasks , the mod-   els have separate parameters and are weakly-   coupled during training . We propose cast-   ing the retriever and the reranker as internal   passage - wise attention mechanisms applied se-   quentially within the transformer architecture   and feeding computed representations to the   reader , with the hidden representations pro-   gressively reﬁned at each stage . This allows   us to use a single question answering model   trained end - to - end , which is a more efﬁcient   use of model capacity and also leads to bet-   ter gradient ﬂow . We present a pre - training   method to effectively train this architecture   and evaluate our model on the Natural Ques-   tions and TriviaQA open datasets . For a ﬁxed   parameter budget , our model outperforms the   previous state - of - the - art model by 1.0 and 0.7   exact match scores .   1 Introduction   Open - domain Question Answering ( Open QA ) is a   knowledge - intensive task that ﬁnds the answer for   the given question from a large - scale knowledge   corpus that can easily surpass millions of docu-   ments . Thus , how to store and refer to the knowl-   edge at such scales is important in terms of both   performance and scalability for Open QA systems .   Traditional systems rely on information retrieval en-   gines such as Lucene . These score the relevance of   knowledge to a given query by lexical overlaps be-   tween them in a sparse representation space based   on TF - IDF or BM25 ( Chen et al . , 2017 ; Wang et al . ,   2018 ; Yang et al . , 2019 ) . However , recent advances   in neural language modeling have enabled two newlines of approach ; 1 ) referring to internal knowl-   edge parameterized in the model ( Brown et al . ,   2020 ; Petroni et al . , 2019 ; Roberts et al . , 2020 ) ,   and 2 ) referring to external knowledge retrieved by   matching query and knowledge in dense represen-   tation spaces ( Karpukhin et al . , 2020 ; Lee et al . ,   2019 ; Guu et al . , 2020 ; Lewis et al . , 2020 ; Izacard   and Grave , 2021b ) .   Despite the simplicity of the approach , para-   metric models have limitations such as a large   number of model parameters that require large   compute for both training and inference and non-   expandable knowledge without re - training . Their   implicit knowledge reference also makes it hard to   ﬁnd supporting knowledge and often results in hal-   lucinations ( Shuster et al . , 2021 ) . The current dense   retrieval models have advantages over parametric   models on these issues ( Karpukhin et al . , 2020 ;   Guu et al . , 2020 ; Lewis et al . , 2020 ) . But most re-   trieval models only have a weak coupling between   and separate parameters for the reader , reranker ( if   any ) , and retriever that limits these models from   achieving optimal end - to - end training and efﬁcient   use of the total model capacity .   In this paper , we propose a single language   model YONO ( You Only Need One model ) that   can refer to external knowledge via its internal at-   tention functions , which are trainable in a fully   end - to - end manner . We achieve this by generaliz-   ing the retrieval and reranking as internal passage-   wise attentions . At the lower retrieval layers , the   query and passages are separately encoded allow-   ing pre - computation of all the passage representa-   tions . Then passage - wise hard attention is applied   to retrieve initial relevant passages from the en-   tire knowledge base . While it would be optimal   to retrieve passages based on cross - attention be-   tween the query and all the passages ( Khattab and   Zaharia , 2020 ) , it is computationally intractable .   Hence , we approximate this attention by a passage-   wise hard - attention layer using decoupled query3047   and passage representations . The representations   of the initial relevant passages are further encoded   jointly with the query representation to compute   more expressive coupled representations . These   are used to select only the more relevant passages   using another passage - wise hard - attention in the   reranking layer . The representations of the ﬁnal   set of passages are then encoded by transformer   encoders for deeper representations that are fused   in the decoder to generate the answers .   We train this architecture fully end - to - end by   self - supervised pre - training and weakly supervised   ﬁne - tuning without passage labels .   Our contributions are twofold ;   •A single model that generalizes retrieval ,   reranking , and reading as internal attention   functions . We show that this model trained   end - to - end signiﬁcantly improves the retrieval   performance by leveraging a training signal   from the answer generation decoder to allow   better gradient ﬂow across the whole model   in Section 6.1 . It also achieves better utiliza-   tion of the model parameters , outperforming   a stand - alone reader with the same number   of parameters by 7.1 % and 3.2 % on NQ and   TQA respectively as shown in Section 6.2 .   •A method to train this architecture in a fully   end - to - end manner . We show our pre - training   method requires 51.5 % fewer pre - training to-   kens compare to the previous the state - of - the-   art approach in Section 7.2 .   2 YONO Architecture   As depicted in Figure 1 , we propose a single   encoder - decoder language model architecture con-   sisting of 3 components : Retrieval Layer , Rerank-   ing Layer , and Reading Layer.2.1 Retrieval Layer   This ﬁrst layer retrieves the top - N relevant passages   for a given query from the knowledge corpus using   query and passage representations independently   encoded with the ﬁrst Ktransformer encoder lay-   ers . The query is encoded with ‘ query : ’ preﬁx   while passages are encoded with ‘ title : ’ and ‘ con-   text : ’ preﬁxes following previous approaches ( Izac-   ard and Grave , 2021b ; Singh et al . , 2021 ) .   Passage - wise Disjoint Attention : LetqandP   be the ﬁrst tokens ’ representations of the query   and all the passages respectively encoded indepen-   dently . The disjoint attention scores are calculated   by the scaled dot - product attention scores ( Vaswani   et al . , 2017 ) between qandPas :   Q = LayerNorm ( qW )   K = LayerNorm ( PW )   score ( q , P ) = σ(QK//radicalbig   d)(1 )   whereW , W∈Rare learned linear projec-   tions and 1/√dis the scaling factor following   Vaswani et al . ( 2017 ) .   The top - N relevant passages Pwith the high-   estscore for a given query are selected and   passed to the next layer . In practice , we retrieve top-   N passages by indexing the pre - computed passage   representations Pusing Maximum Inner Product   Search tools ( MIPS ) such as FAISS ( Johnson et al . ,   2021 ) . During training , the index is iteratively   refreshed by the most recent model ’s representa-   tion following other neural retrieval approaches   ( Karpukhin et al . , 2020 ; Lee et al . , 2019 ; Guu et al . ,   2020 ; Izacard and Grave , 2021b ) .   2.2 Reranking Layer   We further narrow down the retrieved passages   by applying an additional passage - wise attention3048based on more expressive representations from the   joint encoding of query and passages .   Passage - wise Joint Attention : We concatenate   query and retrieved passage representations and en-   code them with cross - attention for more expressive   representations using the next Ltransformer en-   coder layers . Let hbe the encoded representations   of query and the npassage and Hbe the ﬁrst   token ’s representations of all encoded representa-   tions . We apply the second passage - wise attention   based onscore(q , P ) , obtained from H :   h = Transformer ( q⊕p ) ( 2 )   H= [ h , h , h, ... ,h ]   score(q , P ) = σ(LayerNorm ( H)W )   where⊕is the concatenation operation and W∈   Ris a learnt vector .   2.3 Reading Layer   After the retrieval and reranking layers , the ﬁnal   representations are fed to the reading layer . These   are further encoded using the remaining trans-   former encoder layers and fused in the decoder   for multi - passage reading , following the approach   of FiD ( Izacard and Grave , 2021b ) .   3 Training YONO   3.1 Training Objective   The whole model is always trained end - to - end by   leveraging a training signal from the ﬁnal answer   generation . Due to non - differentiability of the   passage - wise attentions , we combine additional   lossesL andL to the answer gen-   erationL as below :   L = L + L + L ( 3 )   Retrieval and Reranking Loss : The passage-   wise attention scores S andS for   retrieval and reranking layers are calculated by   Equation ( 1 ) and ( 2 ) using retrieved passages P   and in - batch negative passages Pas :   S = score ( q , P∪P )   S = score(q , P ) ( 4 )   In - batch negative passages Pare used to expand   S for more contrastive training signals . In-   batch negatives not used for S because   the joint representation is only calculated for the   retrieved passages , not the in - batch negatives . These attention scores are not differentiable   by the reader ’s generation loss because they are   only used to select top - N passages at retrieval and   reranking layers but not directly used in the answer   generation . Instead , we train score and   score to approximate the target scores ,   which following previous work ( Izacard and Grave ,   2021a ) are derived by accumulating the decoder ’s   attention scores across decoder layers and attention   heads over all encoded passage tokens as :   score ( P ) = ( 5)/summationdisplay / summationdisplay / summationdisplaySG(att(0,l , h , t ) )   NNN|p∈P   whereattis decoder attention matrices toward   encoded outputs , 0is an output token index , Nis   the number of layers , Nis the number of atten-   tion heads , Nis the number of tokens in a given   passage , and SGis a stop gradient function . The   gradient ﬂow back to the decoder ’s attention scores   is blocked by SGto train the decoder by L   only .   Using this scoring function , we get target scores   T andT . The scores of in - batch   negative passages Pare set to 0 .   T = score ( P)⊕(0|p∈P )   T = score ( P ) ( 6 )   Finally , the losses for training passage - wise at-   tention scores are obtained by KL - Divergence be-   tween target scores Tand attention scores Sas :   L = D(T / bardblS )   L = D(T / bardblS ) ( 7 )   In addition , we add a constant penalty γto   score of in - batch negative passages P   before applying a softmax of Equation ( 1 ) . This in-   ductive bias enforces the model to further decrease   scores of the random negative passages lower than   the lowest score of the retrieved passages .   Reading Loss : We use a conventional auto-   regressive language modeling loss for generating   an answeragiven a query qand retrieved passages   P :   L = −log / productdisplayp(a|a , q , P ) ( 8)30493.2 Pre - training Corpus   We ﬁrst pre - train our model to adapt the pre - trained   encoder - decoder architecture to the YONO archi-   tecture and provide initial retrieval performance for   ﬁne - tuning without passage labels . Inverse Cloze   Task ( ICT ) ( Lee et al . , 2019 ) and Masked Salient   Span ( MSS ) ( Roberts et al . , 2020 ; Guu et al . , 2020 )   are widely used tasks for pre - training . ICT uses   ‘ input - passage ’ pairs that have explicit supervision   for training passage - wise attention , but has no su-   pervision for the answer generation . On the other   hand , MSS trains the model by ‘ input - output ’ pairs   that have strong supervision for the answer genera-   tion , but no supervision for the retriever , requiring   additional warm - up training for retrieval such as   ICT . Thus , these tasks are sequentially applied to   pre - train the pipeline models to overcome their   limitations ( Guu et al . , 2020 ; Singh et al . , 2021 ) .   However , we train our single model architecture   with retrieval , reranking , and reading layers at the   same time using triples of ‘ input - passage - output ’   for pre - training . To provide such supervisions , we   extend a masked salient span task with explicit   passage labels .   Masked Salient Span with Passage Labels   ( MSS - P ): We ﬁrst pick one named entity and   mask all instances of this entity from the sentence .   We explicitly add a ground truth passage that con-   tains the masked named entity from 2 previous and   next passages except its original passage . We reﬁne   the data by simple heuristics ( using only pairs of   sentence and target passage that contain at least 1   common named entity other than the masked span ,   and selecting the target passage with the highest   number of common named entities when there are   multiple passages containing the masked span ) . In   this way , we generate 53 M triples from Wikipedia   passages in total .   3.3 Training Procedure   The model is pre - trained and ﬁne - tuned iteratively   to refresh retrieved passages for a better approxi-   mation of the distribution over all the passages .   We start the ﬁrst pre - training iteration using the   initial pre - training data , extracted by the MSS - P   method that has one ground - truth passage for each   query sentence . Note that with one positive pas-   sage per query , L is equivalent to the neg-   ative log - likelihood loss of predicting the positive   passage along with negative passages . However ,   L is 0 at this pre - training iteration anddoes not yield any training signal because it can   only learn from contrasting multiple retrieved pas-   sages .   From the second iteration , the model is trained   with 100 passages fetched from the retrieval layer .   We add the original ground truth passage to the re-   trieved passages to ensure that the model learns to   refer to the knowledge instead of implicitly mem-   orizing the answer in its internal parameters . We   do not ﬁlter more passages at the reranking layer   during training to compute score of Equa-   tion ( 5 ) to allow the reader to learn from the maxi-   mum number of passages . We pre - train the model   for several iterations until the performance of the   retrieval converges based on the recall metric .   After the pre - training , the model is then ﬁne-   tuned following the same procedure as that after   the ﬁrst iteration . To prevent an over-ﬁtting of the   reader due to the limited size of the ﬁne - tuning   data , we simply re - initialize the model with the pre-   trained YONO model after retrieval performance   converges following Izacard and Grave ( 2021a ) .   Note that the model is ﬁne - tuned by only weak-   supervision of question - answer pairs without gold   passages .   4 Experiments   4.1 Model Conﬁgurations   We primarily compare our model with baselines   that use 440 M parameters in total . To get a sin-   gle language model with 440 M parameters , we   initialize our model from the pre - trained T5 - large   ( Raffel et al . , 2020 ) discarding ﬁnal 18 decoder   layers . This results in our model with 24 encoder   and 6 decoder layers . The retrieval layer uses the   ﬁrst 12 encoder layers that uses 25 % fewer param-   eters than baselines ’ bi - encoders retrievers ( 165 M   vs 220 M ) . Since our reranking layer works on the   representations of the retrieval layer , we only al-   locate 4 encoder layers for reranking . The total   number of parameters used for retrieval and rerank-   ing is 220M. The remaining 220 M parameters are   allocated for the reading layer .   4.2 Training Details   At the ﬁrst pre - training iteration , the model is   trained with a batch of 800 question - passage-   answer triples for 100 K steps . From the second   iteration , we train the model for 1,250 steps per   iteration using a batch with 64 question - passages-   answer triplets , where each triplet is packed with3050   100 retrieved passages . In total , we run 42 ad-   ditional iterations after the ﬁrst iteration for pre-   training .   After pre - training , the model is ﬁne - tuned the   same way as pre - training , except it is trained for 1   epoch at every iteration .   The model is optimized with the Adam optimizer   ( Kingma and Ba , 2015 ) with a learning rate 10 .   The ﬁrst iteration takes 24 hours , and other iter-   ations take around 5 hours each including MIPS   indexing and passage refresh on 8 A100 GPUs .   The penalty γfor attention scores of the random   in - batch negative passages is set to 5 in all our   experiments .   We found that answer generation more easily   over-ﬁts compared to the retrieval during ﬁne-   tuning . To prevent this over-ﬁtting , the model   is once reinitialized from the pre - trained YONO   model at the 6iteration after the model achieves   acceptable recall on the downstream task .   4.3 Datasets   We evaluate our model with two standard open-   domain question answering datasets , Natural Ques-   tions ( Kwiatkowski et al . , 2019 ) and TriviaQA   ( Joshi et al . , 2017 ) following short answer sub-   sets processed by Lee et al . ( 2019 ) . Our external   knowledge base is built using the Wikipedia dump   from Dec. 20 , 2018 , where articles are split into   passages of 100 words without overlap which is thesame as datasets used in Karpukhin et al . ( 2020 ) ;   Izacard and Grave ( 2021b ) ; Singh et al . ( 2021 ) for   a fair comparison .   5 Evaluation   5.1 Retrieval Performance   Table 1 and Table 2 show overall performance of   our model and other baselines on Natural Questions   and TriviaQA test sets . Our retrieval layer achieves   the state - of - the - art recall@20/100 on Natural Ques-   tions regardless of model size even when compared   with models with more than 4x model parameters .   On TriviaQA , ours performs slightly worse than   the state - of - the - art models , FiD - KD ( Izacard and   Grave , 2021a ) and E2NR ( Sachan et al . , 2021 ) ,   which use passage labels during training or 4x more   parameters . Our approach achieves such perfor-   mance without using passage labels making rele-   vance for a larger range of applications that may   not have these annotations . These results also do   not use augmented data and as we show in Sec-   tion 6.3 it only gives slight improvements on the   end - to - end performance .   5.2 Reranking Performance   As shown in Table 1 , the reranking layer further   improves the recall of our retrieved passages by 3.8   and 5.5 absolute recall@5 on NQ and TQA respec-   tively when reranking 800 retrieved passages . This3051   is 2.3 and 3.4 absolute point improvements over   the previous state - of - the - art reranker model . Our   model achieves these recall performances using   only 55 M parameters which is only half the size of   the other reranker models . Similar to our retriever ,   our reranker does not require passage labels , unlike   other rerankers . This improvement in recall when   using the reranker persists even when reranking   only 200 passages .   5.3 End - to - end Performance   Our model achieves the best end - to - end perfor-   mance among the models of the same size on NQ   and irrespective of the model size on TQA as shown   in Table 2 . Our best scores improve EM scores by   0.7 points on both NQ and TQA respectively over   the previously best performing model of the same   size , EMDR(Singh et al . , 2021 ) . Using data aug-   mentation further boosts these improvements on   NQ by 0.3 as shown in Table 5 . Using reranking   also improves the end - to - end scores on TQA by   0.8 , a negligible improvement on NQ . We conjec-   ture that this may be due to the higher recall of the   retriever on NQ .   6 Ablation Studies   6.1 The Reader Loss on Retrieval   Performance   The retrieval layer is trained by signals from both   retrieval and reader losses . While the retrieval loss   directly trains the retrieval scores , the reader loss is   also a useful indirect training signal for the retriever .   This signal is a key advantage of our approach over   similar works such as Izacard and Grave ( 2021a ) ;   Singh et al . ( 2021 ) . We evaluate the performance   gain from the additional generation loss at the ﬁrst   pre - training iteration as shown in Table 3 . The   model trained with both losses shows signiﬁcant   improvement over the model trained with only the   retrieval loss . These relative gains are larger the   fewer the number of retrieved passages . This result   shows that reader ’s generation loss is very effective   for training the retriever . We evaluate after the   ﬁrst iteration because the reader loss is necessary   for training with multiple retrieved passages in the   following iterations .   6.2 Shared Representations on Reader   Performance   Our reading layer uses 220 M parameters but   shares representations encoded by its preceding   retrieval and reranking layers which use another   220 M parameters . To measure gains of the shared   representations , we compare our reader perfor-   mance with that of a stand - alone reader model that   uses 220 M parameters that is the same as our read-   ing layers . For a fair comparison , the stand - alone   reader model is pre - trained and ﬁne - tuned for the   same amount of training tokens using the data re-   trieved by our YONO retriever . Table 4 shows that   the reader model sharing representations outper-   forms the stand - alone reader by 7.1 % and 3.2 % on   NQ and TQA respectively.3052   6.3 Effectiveness of MSS - P pre - training   To show the effectiveness of our MSS - P pre-   training method , we evaluate this by ﬁne - tuning   our architecture without any pre - training using ini-   tial retrievals from DPR ( Karpukhin et al . , 2020 )   and ﬁne - tuning after the ﬁrst pre - training iteration .   We also compare our pre - training to that of the   additional data augmentation ( Mao et al . , 2021a ;   Ren et al . , 2021 ; Oguz et al . , 2021 ) . We generate   ‘ question - answer ’ pairs from a Wikipedia dump us-   ing a question and answer generation model trained   on the NQ dataset using the ASGen approach ( Back   et al . , 2021 ) . The model is further trained after the   pre - training by this augmented data for 12 more   iterations before ﬁne - tuning .   Table 5 shows retrieval , reranking , and read-   ing performance on Natural Questions and Triv-   iaQA test sets . Our MSS - P pre - training dramati-   cally boosts the performance of our architecture   by 4.8 and 13.3 EM points on NQ and TQA even   with only the ﬁrst pre - training iteration . Further   iterations of our pre - training improve EM by 1.7   and 2.9 EM points . The further data augmentation   pre - training improves performance on NQ consis-   tently but only slightly , while the improvements on   TQA are inconsistent , as the data was generated   by the model trained on NQ dataset . These results   clearly demonstrate that our simple self - supervised   MSS - P pre - training is strong enough to compete   favorably against sophisticated data augmentation   approaches .   7 Analysis   7.1 Computational Efﬁciency of Reranking   In many dense retrieval systems , a reranker is often   omitted due to functional overlaps with the reader   and computational overhead ( Guu et al . , 2020 ;   Lewis et al . , 2020 ; Singh et al . , 2021 ) . Thanks   to the shared representations across the reader and   reranker , our model can incorporate a reranking   function without signiﬁcantly more parameters or   computation . By dropping irrelevant passages early   at the reranking layer , we can achieve better com-   putational efﬁciency . Figure 2 shows exact match   scores for given N retrieved or reranked passages .   The model still achieves optimal EM performance   with only the top 20 reranked passages reranked   from 100 retrieved passages . Reranking 100 to 20   passages can reduce the inference computation by   27.4%without pre - computed passage representa-   tions , and 54.0%with pre - computed passage repre-   sentations without losing end - to - end performance .   7.2 Pre - training Efﬁciency   The number of pre - training tokens is an important   metric to measure the efﬁciency of the pre - training   objective . As shown in Table 6 , REALM ( Guu   et al . , 2020 ) and EMDR(Singh et al . , 2021 ) use   352B and 171B tokens in total respectively . In con-   trast , our method uses only 83B tokens , which is   76.4 % and 51.5 % less than the training tokens used   to train REALM and EMDRrespectively . Further-   more , the retrieval index is updated only 43 times   during our pre - training , while EMDRupdates the   index 164 times . This is a signiﬁcant reduction of3053the computation overhead for pre - training .   8 Related Works   Neural Retriever Augmented Language Model-   ing ( NRALM ): Augmenting language models   with neural retrieval has been shown to be very   effective , such as by retrieving nearest neighbor   words for LM tasks ( Khandelwal et al . , 2020 ; Yo-   gatama et al . , 2021 ) or Machine Translation ( Khan-   delwal et al . , 2021 ) . Dinan et al . ( 2019 ) proposed   a decomposed transformer for conversation tasks ,   which enabled pre - computation of the external   knowledge embeddings .   ORQA ( Lee et al . , 2019 ) proposed the ICT   task to pre - train a decomposed retriever , and   DPR ( Karpukhin et al . , 2020 ) enhanced this ap-   proach with in - batch negatives and hard negatives   to eliminate the pre - training . Synthetic Data Aug-   mentation is also commonly used , such as in DPR-   PAQ ( Oguz et al . , 2021 ) , PAIR ( Ren et al . , 2021 ) ,   Hu et al . ( 2021 ) . Per - token embeddings or multiple   embeddings were used in ColBERT ( Khattab et al . ,   2020 ) , ME - BERT ( Luan et al . , 2021 ) , Lin et al .   ( 2021 ) , Lee et al . ( 2021 ) .   Similar to our approach of re - ranker on top of a   shared retriever , PreTTR ( MacAvaney et al . , 2020 )   pre - computed term representations for all docu-   ments , and used these to run only the upper layers   of a transformer reranker model . Decoupled Trans-   former ( Elfdaeel and Peshterliev , 2021 ) also shares   the lower layers of a transformer encoder to serve   as a reranker , using the upper layers as a reader   and focuses on computationally efﬁcient rerank-   ing . Our approach extends these approaches by   also incorporating a retriever and a decoder in the   model .   E2E Optimization of NRALM : It is intractable   to re - compute the embeddings of the knowledge for   every weight update . REALM ( Guu et al . , 2020 )   and ANCE ( Xiong et al . , 2021a ) proposed async   index refresh to propagate updates to the index   to yield better negatives . TAS ( Hofstätter et al . ,   2021 ) and Xiong et al . ( 2021b ) used clustering of   embeddings for the same . RAG ( Lewis et al . , 2020 )   used DPR with BART generator to marginalize   over generated tokens , which is back - propagated   to the retriever . REALM++ ( Balachandran et al . ,   2021 ) added a re - ranker to REALM .   Similar to our work , Bruyn et al . ( 2020 ) and   TREAD ( Shuster et al . , 2021 ) utilize BART and   T5 reader ’s encoders as a retriever . In contrast tothese methods , our work has a uniﬁed pre - training   method to train all the components of the model .   Furthermore , our model also has an integrated   re - ranker , and the query and passage are cross-   encoded for more expressive representations .   Multi - passage Readers : Reading multiple pas-   sages at the same time is difﬁcult , as concatenating   multiple passages increases computation quadrati-   cally for transformers . Zhao et al . ( 2020 ) reduced   multiple passages and sentences to few via a knowl-   edge selector , which were then concatenated and   passed on to GPT ( Radford et al . , 2019 ) . FiD ( Izac-   ard and Grave , 2021b ) concatenated the encoded   representations of documents , which can then be at-   tended by the decoder , achieving large performance   gains . This approach was also applied in Rock-   etQA ( Qu et al . , 2021 ) . UnitedQA ( Cheng et al . ,   2021 ) and R2D2 ( Fajcik et al . , 2021 ) combine re-   sults from an ensemble of extractive and generative   readers , whereas PAQ ( Lewis et al . , 2021 ) directly   retrieves answers with an FiD fallback .   Similar to our work , both REALM ( Izacard and   Grave , 2021a ) and EMDR(Singh et al . , 2021 )   train the retriever with a signal from the reader .   Unlike these approaches , our model has shared   lower layers for more effective utilization of model   parameters and better end - to - end gradient ﬂow   across the whole model . Furthermore , our train-   ing methodology results in propagating the answer   generation loss of the retriever , which has a large   effect on performance as we show in Table 3 .   9 Conclusion   In this paper , we propose a novel language model   architecture that embeds the retriever and the   reranker as internal passage - wise attention mecha-   nisms and a training method to effectively train this   model . This singular model architecture efﬁciently   uses model capacity by cascading and sharing the   representations from retriever to reranker to the   reader leading to better gradient ﬂow for end - to-   end training . We evaluate our model on Natural   Questions and TriviaQA open datasets and for a   ﬁxed parameter budget , our model outperforms the   previous state - of - the - art model by 1.0 and 0.7 ex-   act match scores . We show detailed ablations and   analyses of each component of our approach . Our   future work is to conduct more experiments on   various knowledge - intensive tasks and extend this   model to match query and passage in multiple or   hierarchical representation spaces.3054Limitations   One caveat of sharing representation for multiple   tasks like retrieval , reranking , and reading is that   these show different over-ﬁtting tendencies during   ﬁne - tuning where the training data is limited . We   found that answer generation over-ﬁts more eas-   ily compared to the retrieval . Answer generation   relies on more expressive representation via cross   attention , which may make it easier to memorize   the output and hence make it more vulnerable to   over-ﬁtting . Furthermore , at the ﬁrst ﬁne - tuning   iteration , the model is trained by zero - shot retrieval   results from the pre - trained model that has a rela-   tively low recall rate and can harm the answer gen-   eration training . To refresh the over-ﬁtted answer   generation parameters , and to start from training   data with a high recall rate , we simply re - initialize   the model with the pre - trained YONO model after   a few ﬁne - tuning iterations . However , we believe   that this issue should be addressed carefully using   a more sophisticated solution . We further discuss   the over-ﬁtting issue and effect of re - initialization   in Appendix A.   References305530563057   Appendix   A Model Re - initialization during   Fine - tuning   To overcome over-ﬁtted reader parameters , we re-   fresh the model parameter using the pre - trained   YONO model at the ﬁne - tuning iteration where the   EM score starts to drop . Figure 3 shows retrieval and end - to - end perfor-   mance at each ﬁne - tuning iteration . The Exact   Match score drops at the 5iteration while the re-   trieval score keeps increasing . After re - initializing   the model before the 6iteration , the model re-   starts with a higher recall and EM score . However ,   the EM score drops again from the 10iteration   after achieving the best end - to - end performance ,   while the retrieval performance continues to im-   prove . We leave further approaches for preventing   over-ﬁtting of our model such as freezing the model   partially as future work .   B Effect of Pre - training Iterations on   Retrieval Performance   Figure 4 shows recall@N at each training stage   across the pre - training and ﬁne - tuning using Nat-   ural Question development set . The ﬁrst iteration   of the pre - training results in zero - shot recall@100   of67.0 % , which is further improved by additional   pre - training iterations to 71.8%recall@100 . These   zero - shot recall scores enable us to ﬁne - tune our   model without passage labels resulting in state - of-   the - art retrieval and reranking performance .   C Experiment Details   On Table 7 , 8 , and 9 , we provide all training details   and parameters used to conduct experiments on this   paper .   D Raw Values for Plots in Figures   In Table 10 and 11 , we provide raw values for plots   in Figure 2 and 4.3058   E Measures of Central Tendencies for   Results   To measure the sensitivity of our model to varying   seeds , we run 3 ﬁne - tunings of our model on NQ ,   and report the mean and standard errors on the   development set below , as shown in Figure 3 . The   model training seems stable with little variation   across runs . We did not run multiple instances of   pre - training as it is computationally expensive .   F Links to Source Code and Datasets   The source code is based on the original implemen-   tation of FiD ( Izacard and Grave , 2021b ) , which   can be found at their Github .   Data for the Wikipedia dump , Natural Questions ,   and TriviaQA can also be downloaded from FiD ’s   github using this script .   G Evaluation Metrics and Scripts   The evaluation script is based on the original FiD   script .   Exact Match - This is the average across all   examples of the per - example exact match score ,   which is 0 or 1 if all the words in the generated   answer exactly match the annotated answer after   unicode normalization by lower - casing , removing   punctuation and spaces .   Recall@N - Recall@N measures the percentage   of examples for which atleast one the top - N pas-3059sages contains a span that matches the annotated   answer as in Exact Match above .   H Dataset Statistics   Table 13 provides the statistics of our evaluation   datasets .   Dataset # Train # Dev # Test   Natural Questions 79 K 8.8 K 3.6 K   TriviaQA 79 K 8.8 K 11 K   I Computing Infrastructure   GPU model - 8x Nvidia A100 80 GB . CPU Model   - 2x AMD EPYC 7543 32 - Core Processor . RAM -   1000 GB . PyTroch version - 1.8.0+cu111 . Hugging-   face Transformers version - 3.0.2.3060