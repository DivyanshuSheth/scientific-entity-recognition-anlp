  Hadas Orgad Yonatan Belinkov   orgad.hadas@cs.technion.ac.il belinkov@technion.ac.il   Technion – Israel Institute of Technology   Abstract   Models trained on real - world data tend to imi-   tate and amplify social biases . Common meth-   ods to mitigate biases require prior informa-   tion on the types of biases that should be miti-   gated ( e.g. , gender or racial bias ) and the social   groups associated with each data sample . In   this work , we introduce BLIND , a method for   bias removal with no prior knowledge of the   demographics in the dataset . While training a   model on a downstream task , BLIND detects   biased samples using an auxiliary model that   predicts the main model ’s success , and down-   weights those samples during the training pro-   cess . Experiments with racial and gender biases   in sentiment classification and occupation clas-   sification tasks demonstrate that BLIND miti-   gates social biases without relying on a costly   demographic annotation process . Our method   is competitive with other methods that require   demographic information and sometimes even   surpasses them .   1 Introduction   Neural natural language processing ( NLP ) mod-   els are known to suffer from social biases , such as   performance disparities between genders or races   ( Blodgett et al . , 2020 ) . Numerous debiasing meth-   ods have been proposed in order to address this   issue , with varying degrees of success . A disadvan-   tage of these methods is that they require knowl-   edge of the biases one wishes to mitigate ( e.g. ,   gender bias ) ( Bolukbasi et al . , 2016 ; Zhao et al . ,   2018 ; De - Arteaga et al . , 2019 ; Maudslay et al . ,   2019 ) . Moreover , some methods require additional   annotations for identifying the demographics for   each sample in the data , such as the race of the   writer ( Han et al . , 2021a ; Liu et al . , 2021 ; Ravfogel   et al . , 2022 ; Shen et al . , 2022 ) . Some annotations   Figure 1 : Our proposed debiasing methods . In both   cases , an auxiliary classifier is trained to detect sam-   ples where demographic features may be used as short-   cuts and their importance to the main model is down-   weighted .   can be automatically collected from the data , while   others require manual annotations or expert knowl-   edge , which can be very costly . Thus , existing   methods are typically limited to a small number of   datasets and tasks . In this paper , we propose a new   debiasing method , BLIND – Bias remova LwIth   NoDemographics .   We see social bias as a special case of robustness   issues resulting from shortcut learning ( Geirhos   et al . , 2020 ) . Our goal is to down - weight samples   that contain demographic features that may be used8801as shortcuts in downstream tasks . We first consider   a case where we have demographic annotations   for every sample in the training set , and train a   demographics detector – an auxiliary classifier that   takes the main model ’s representations and predicts   the demographic attribute . Then , we down - weight   the importance of samples on which the classifier   is confident ( Figure 1a ) . To our knowledge , this is   the first work to consider demographic information   for re - weighting samples during training .   When we do not have demographic annotations ,   we make the following observation : The main   model has an easier job , or otherwise makes pre-   dictable mistakes , when demographic features are   used as shortcut features . Thus , we train a suc-   cess detector – another auxiliary classifier , which   takes the representations of the main model and   predicts its success on the task . A correct predic-   tion by the success detector means the main model   made a shallow decision , since it is possible pre-   dict its success or failure without access to the   main model ’s task labels . In such cases we expect   that the main model relies on simple , shortcut fea-   tures , and we use the success detector ’s confidence   to down - weight such samples in the training data   ( Figure 1b ) . We call this method BLIND .   In both cases , we adapt the debiased focal loss   ( DFL ; Karimi Mahabadi et al . , 2020 ) , originally   proposed for mitigating annotation artifacts , to   down - weight samples that the detectors predicted   correctly . In contrast to the original DFL work ,   which explicitly defined biases to mitigate , we de-   sign the bias detection mechanism in a general   manner such that the model ’s biases are learned   and mitigated dynamically during training .   We perform experiments on two English NLP   tasks and two types of social demographics : oc-   cupation classification with gender , and sentiment   analysis with race . Our methods successfully re-   duce bias , with BLIND sometimes succeeding in   cases where other methods that use demographic   information fail . Our analysis shows that BLIND   reduces demographic information in the model ’s in-   ternal representation , even though it does not have   access to it . Additionally , BLIND is particularly ef-   fective at mitigating bias due to its down - weighting   of easy training samples , rather than relying on de-   mographic information alone . This suggests that   BLIND may be more robust in mitigating bias than   other methods.2 Methodology   2.1 Problem Formulation   We consider general multi - class classification prob-   lems . The dataset D={x , y , z}consists of   triples of input x∈ X , label y∈ Y , and a pro-   tected attribute z∈ Z , which corresponds to a   demographic group , such as gender . zmight be   latent , meaning that it can not be accessed during   training or validation stages . Our goal is to learn   a mapping f :X →R , such that f , which   we call the main model , is robust to differences in   demographics as induced by z.   The robustness of a model is measured using   a variety of fairness metrics . A fairness metric   is a mapping from a model ’s predictions and the   protected attributes associated with each sample to   a numerical measure of bias : M : ( R , Z)→R.   The closer the absolute value is to 0 , the fairer the   model . The practical fairness metrics measured in   this work are described in Section 3.2 .   2.2 Debiased Focal Loss for Social Bias   Debiased focal loss was proposed by Karimi Ma-   habadi et al . ( 2022 ) to improve natural language   understanding models on out - of - distribution data .   The authors explicitly defined the biases they aim   to mitigate , and trained an auxiliary model on the   same task as the main model by feeding it with   biased features . We model biased samples differ-   ently : instead of learning the same downstream   task as the main model , our auxiliary model learns   a separate task that indicates bias .   The model fcan be written as a composition   of two functions : g , the text encoder , and h , the   classifier , such that f = h ◦ g. In our case , gis a   transformer language model such as BERT ( Devlin   et al . , 2018 ) , and his a linear classification layer .   Loss term . We use the DFL formulation to re-   weight samples that contain bias . To determine the   re - weighting coefficients , we need a separate model   that acts as a bias detector , f = h ◦ g. The next   two sub - sections define two options for the bias   detector , with and without using demographics .   The main and auxiliary models have parameters   θandθrespectively , and the parameters of the   encoder gare included in θ . The training loss is   defined as :   L(θ;θ ) = ( 1 )   /parenleftig   1−σ / parenleftbig   f(x;θ)/parenrightbig / parenrightig   log(σ(f(x;θ))8802for a single instance ( x , y , s ) , where σ(u ) =   e//summationtexteis the softmax function , and γis a   hyper - parameter that controls the strength of the   re - weighting . Here sis either the demographic   attribute zwhen we have it , or a success indicator   of the main model on x , as explained below . When   the bias detector assigns a high probability to s , the   contribution of this sample to the loss is decreased ,   and this effect is magnified by γ(γ= 0 restores   the vanilla cross - entropy loss ) . Both models are   trained simultaneously , but only the main model ’s   loss is back - propagated to the encoder g , avoiding   bias propagation from f.   2.3 Debiasing With Demographic Annotations   When demographic attributes are available , we   define bias as how easily demographic informa-   tion can be extracted from a sample . This strat-   egy aligns with the observation by Orgad et al .   ( 2022 ) that this measure correlates with gender   bias metrics . The bias detector is thus formulated   asf : g(X)→R , taking as input the repre-   sentations from gand predicting the demographic   attribute ; In other words , s:=zin the formulation   in Equation ( 1 ) . Figure 1a illustrates this approach .   By applying this method , samples in which the   demographics detector is successful in predict-   ing demographics ( σ(f(x))is high ) are down-   weighted , and difficult samples ( σ(f(x))is low )   are up - weighted . Intuitively , the main model is   encouraged to focus on samples with less demo-   graphic information , which reduces the learning of   demographics – task correlations .   Connection to adversarial learning . While the   concept of a demographics model may resemble   that used in work on debiasing via adversarial train-   ing ( Zhang et al . , 2018 ; Elazar and Goldberg , 2018 ;   Han et al . , 2021c ) , our DFL approach is fundamen-   tally different : rather than reversing gradients from   the demographics model to remove information   from the main model , we use the demographics   model to reweight the loss . Further discussion re-   garding adversarial learning can be found in Ap-   pendix A.   2.4 Debiasing Without Demographic   Annotations   One of the main weaknesses of other debiasing   methods in the field , including the method de-   scribed in Section 2.3 , is the requirement to collect   demographic annotations zper data point . Theseannotations may be expensive or impossible to ob-   tain . Additionally , these annotations often do not   address nuances , such as intersectional biases to   multiple groups ( e.g. , both gender and race ) , or non-   binary gender . We propose BLIND as a method for   reducing biases without demographic annotations .   In this setting , the auxiliary model fis trained   as a success detector . The success detector pre-   dicts , for each training sample , whether the main   model would be successful at predicting the correct   label for the main task . The success detector has   no knowledge of the original task . It is formulated   asf : g(X)→R , and sis defined as an in-   dicator function : s:=I.sis dynamic ,   and changes across different epochs of the training   process . BLIND is illustrated in Figure 1b .   We expect that if the success detector can predict   whether the main model is correct or incorrect on a   sample ( i.e. , σ(f(x)))is high ) , without knowing   the task at hand , then the sample contains some   simple but biased features , and thus should have re-   duced weight in the loss . This intuition aligns with   the claim that in the context of complex language   understanding tasks , all simple feature correlations   are spurious ( Gardner et al . , 2021 ) .   3 Experiments   3.1 Tasks and Models   We experiment with two English classification   tasks and bias types :   Sentiment Analysis and Race . We follow the   setting of Elazar and Goldberg ( 2018 ) , who used a   twitter dataset collected by Blodgett et al . ( 2016 ) to   study dialectal variation in social media . We use a   subset of 100k samples . As a proxy for the writer ’s   racial identity , each tweet is associated with a label   about whether it is written in African American En-   glish ( AAE ) or Mainstream US English ( MUSE ; of-   ten called Standard American English , SAE ) based   on the geo - location of the author . Elazar and Gold-   berg ( 2018 ) used emojis in the tweets to derive   sentiment labels for the classification task .   Occupation Classification and Gender . We use   the dataset by De - Arteaga et al . ( 2019 ) , a collec-   tion of 400 K biographies scraped from the internet   to study gender bias in occupation classification.8803The task is predicting one ’s occupation based on   a subset of their biography – without the first sen-   tence , which states the occupation . The protected   attribute is gender , and each instance is assigned   binary genders based on the pronouns in the text ,   which indicate the individual self - identified gender .   3.2 Metrics   As recommended by Orgad and Belinkov ( 2022 ) ,   who showed that different fairness metrics react   differently to debiasing methods , we measure mul-   tiple metrics to quantify bias in downstream tasks .   They can be separated to two main groups :   3.2.1 Performance gap metrics   These indicate the difference in performance be-   tween two demographic groups , such as females   versus males .   Absolute gap . For example , for gender we mea-   sure the True Positive Rate ( TPR ) gap for label yas   GAP = |TPR−TPR| . We also mea-   sure the False Positive Rate ( FPR ) and Precision   gaps . On a multi - class setting , we calculate the   absolute sum of gaps across the different labels of   the task ( denoted TPR ) . We also measure the root   mean square for TPR gap ( denoted TPR ) of   the gaps,/radicalig / summationtext(GAP ) , since it was   used in studies of other debiasing methods we com-   pare to ( Ravfogel et al . , 2020 , 2022 ) .   Gaps correlation with training statistics . When   feasible , we compute the Pearson correlation be-   tween the gap for each class and the training dataset   statistics for this class ( denoted TPR ) . For exam-   ple , the pearson correlation GAP and the per-   centage of female instances in class y , as appears   in the training set .   3.2.2 Statistical metrics   Another family of fairness metrics are statistical   metrics , which are measured on probability dis-   tributions . To describe these metrics , we use the   notation from Section 2 , and denote the model ’s   predictions with r.   Independence . Measures the statistical depen-   dence between the model ’s prediction and the   protected attributes , by measuring the Kull-   back – Leibler divergence between two distributions :   KL(P(r ) , P(r|z = z)),∀z∈ Z. We sum the val-   ues over zto achieve a single number that describes   the independence of the model . This metric doesnot consider the gold labels , and intutively just mea-   sures how much the model ’s behavior is different   on different demographics .   Separation . Measures the statistical dependence   between the model ’s prediction given the target   label and the protected attributes : KL(P(r|y=   y ) , P(r|y = y , z = z)),∀y∈ Y , z∈ Z. We sum   the values over yandzto achieve a single num-   ber . This metric is closely related to TPR and FPR   gaps , and intuitively measures if the model behaves   differently on each class and demographics .   Sufficiency . Measures the statistical dependence   between the target label given the model ’s predic-   tion and the protected attributes : KL(P(y|r=   r ) , P(y|r = r , z = z)),∀r∈ Y , z∈ Z. We sum   the values over randzto achieve a single number .   This metric is related to calibration in classification   and to precision gap , and can intuitively measure   if a model over - promotes or penalizes a certain   demographic group .   3.3 Training and Evaluating   We experiment with BERT ( Devlin et al . , 2018 )   and DeBERTa - v1 ( He et al . , 2020 ) based architec-   tures , where the transformer model is used as a   text encoder and its output and is fed into a linear   classifier . We fine - tune the text encoder with the   linear layer on the downstream task .   During training , we often use a temperature t   in the softmax function of the auxiliary model f ,   which we found to improve training stability . For   hyper - parameter search , since we are interested in   balancing the importance of task accuracy and fair-   ness , we adapt the ‘ distance to optimum ’ ( DTO )   criterion introduced by Han et al . ( 2021a ) . The   DTO calculation is explained in Appendix B. We   perform model selection without requiring a vali-   dation set with demographic annotations , by only   choosing the most radical hyper - parameters ( high-   estγand lowest t ) , while limiting the reduction   in accuracy ( see Appendix E ) . We chose 0.95 of   the maximum achieved accuracy on the task as a   threshold , per architecture . For more details on the   training and evaluation process , see Appendix C.   All of our models are tested on a balanced   dataset ( via subsampling ) , i.e. , each label contains   the same number of samples from each demo-   graphic group . This neutralizes bias in the test   dataset , allowing us to truly assess bias in the mod-   els , as suggested by Orgad and Belinkov ( 2022).88043.4 Baselines and Competitive Systems   We compare the following training methods :   DFL - demog ( ours ) DFL trained with demo-   graphic annotations , as described in Sec-   tion 2.3 .   BLIND ( ours ) DFL trained without demographic   annotations , as described in Section 2.4 .   Control To rule out any potential form of unin-   tended regularization in BLIND , a control   model is trained using random labels for the   auxiliary model . We expect this method to   have no significant debiasing effect .   Finetuned The same model architecture , opti-   mized to solve the downstream task without   any debiasing mechanism .   INLP ( Ravfogel et al . , 2020 ) A post - hoc method   to remove linear information from the neural   representation , by repeatedly training a linear   classifier to predict a property ( in our case ,   gender or race ) from the neural representation ,   and then projecting the neural representations   to the null space of the linear classifier .   RLACE ( Ravfogel et al . , 2022 ) The goal of this   method is also to linearly remove information   from the neural representations of a trained   model by utilizing a different strategy based   on a linear minimax game .   Scrubbing ( De - Arteaga et al . , 2019 ) A common   approach used to remove gender bias in the   occupation classification dataset , is to auto-   matically remove any gender indicators from   it , such as “ he ” , “ she ” , “ Mr. ” or “ Mrs. ” , and   names . We apply this method on the occupa-   tion classification task and also experiments   with combining it with our methods ( marked   as+Scrubbing ) .   FairBatch ( Roh et al . , 2021 ) This method adap-   tively selects minibatch sizes for improving   fairness , with three variations , targeting equal   opportunity , equalized odds , and demographic   parity . The method is designed on binary   tasks , thus we apply FairBatch to the senti-   ment classification task . For a fair compari-   son , we present the variation that achieved the   best fairness metrics we measured . JTT ( Liu et al . , 2021 ) Just Train Twice ( JTT ) is   a two - stage train - retrain approach that first   trains a model and then trains a second model   that upweights misclassified training samples .   It works without demographic annotations but   requires them for model selection . For a fair   comparison , we select the model that has the   closest accuracy to our BLIND method .   4 Results   In the main body of the paper , we report accu-   racy and a representative subset of fairness metrics .   The full set of fairness metrics is reported in Ap-   pendix D.   In Table 1a , we present the results of sentiment   classification with racial bias , and in Table 1b , re-   sults on occupation classification with gender bias .   As expected , the vanilla fine - tuning baseline yields   the best accuracy , but also the worst bias ( highest   fairness metrics ) , on both BERT and DeBERTa and   on both tasks .   4.1 Debiasing with Demographic Annotations   We first focus on DFL trained with a demographic   detector .   Sentiment classification . The auxiliary model is   trained to predict race . DFL leads to a statistically   significant reduction of bias compared to the fine-   tuned baseline in all metrics , with a minor drop in   accuracy ( 2−3%absolute ) . Compared to other   methods that use demographic attributes ( INLP ,   RLACE , FairBatch and JTT ) , DFL maintains bet-   ter or similar accuracy . On BERT , it also reduces   bias more . On DeBERTa , INLP and RLACE enjoy   a greater bias reduction , but suffer a decrease in ac-   curacy ( −14 % in INLP , −2.7%in RLACE ) , while   FairBatch suffers from both a decrease in accuracy   and less bias reduction , and JTT does not suffer   from accuracy reduction but reduces less bias than   DFL . We conclude that DFL is an effective method   to reduce bias in this setting while maintaining high   performance on the downstream task .   Occupation classification . When using demo-   graphic attributes ( here : gender ) , DFL leads to a   statistically significant reduction of bias according   to all metrics on both BERT and DeBERTa , with a   minor drop in accuracy ( 3 % ) . In contrast , INLP and   RLACE are much less effective in reducing bias8805   in this setting , with no significant difference from   the baseline on BERT and only partial improve-   ments on DeBERTa . Scrubbing is quite effective   in reducing bias while maintaining accuracy , but   it achieves a lesser degree of bias reduction than   DFL . When we combine DFL with scrubbing , we   find that it achieves an even greater bias reduction ,   surpassing all other methods , with only a minor   accuracy reduction compared to DFL . JTT reduces   bias in only one metric on both models .   Our conclusion is that DFL with demographics   is an effective tool for reducing bias , surpassing   other methods we compare to.4.2 Debiasing without Demographic   Annotations .   Next , we examine our method when there is no   access to demographic attributes ( BLIND ) , using a   success detector as a proxy for biased features .   Sentiment classification . Remarkably , we ob-   serve a statistically significant reduction of bias   compared to the fine - tuned model in BERT and   DeBERTa . Reduction in accuracy is minimal , as   before . Comparing DFL with and without demo-   graphics , the model trained with demographics pro-   duces lower fairness metrics , in both BERT and   DeBERTa . JTT , which also does not use demo-   graphics at training , is more effective than BLIND ;   however , it requires demographics for model selec-   tion . Additionally , the control model does not differ   statistically significantly from the vanilla model , in8806both accuracy and fairness metrics .   Occupation classification . As in the sentiment   classification task , debiasing without demographic   attributes ( BLIND ) tends to be less effective than   our variant for debiasing using demographic at-   tributes . Nevertheless , it is still successful in mit-   igating bias on some of the fairness metrics , even   surpassing other methods that use demographic   attributes ( −0.69 % in sufficiency , compared to   −0.55 % for INLP , −0.23 % for RLACE and no   significant reduction for JTT ) , while maintaining   a small reduction in performance ( −3 % ) . Once   again , the control model results are statistically   indistinguishable from those of the baseline . Ad-   ditionally , we find that combining BLIND with   scrubbing seems to not improve fairness on top of   the scrubbing method . Combining BLIND with   a method that has access to the bias we wish to   remove seems not helpful , at least in this case .   To summarize this part , while the results of DFL   without demographic attributes are behind those   of DFL that uses attributes and sometimes behind   other methods that use attributes , it is encouraging   to see a significant reduction in bias in this setting ,   that is sometimes even more effective than other   methods that use demographic attributes .   5 Analyses   5.1 Performance of the Success Detector   The success detector achieves an average accuracy   of85 % on occupation classification and 76 % on   sentiment classification with BERT . Moreover ,   we compute its Expected Calibration Error ( ECE )   and find that it is 0.03 on average for both occu-   pation classification and sentiment classification .   These results suggest that the success detector is   well - calibrated for both classification tasks and   achieves non - trivial accuracy , explaining its effec-   tiveness as a detector for biased samples .   5.2 Effect of debiasing on internal model   representations   To further understand why BLIND works , we inves-   tigate the internal representations of the debiased   models . Recently , the extractability of gender in-   formation from a model ’s internal representations   was found to be correlated with gender bias met-   rics ( Orgad et al . , 2022 ) . We therefore pose the   following question : How does debiasing with DFLaffect the neural representations of demographic   information ? Here , we focus on BERT .   To answer the above question , we train a probe   model , f , which predicts the protected attribute ,   either gender or race , from the main model ’s inter-   nal representations : f : g(X)→R. We then   report the ease at which the probe performs this   task using compression , measured by a minimum   description length ( MDL ) probing ( V oita and Titov ,   2020).Internal representations with a higher com-   pression indicate more accessible gender or racial   information . A detailed description on the imple-   mentation can be found in Appendix C.4 .   Figure 2 presents the probing results for both   tasks and on the two variations of debiasing : with   and without demographic attributes . Even though   some γs are noisy , there is a clear trend that the   accessibility of demographic information decreases   asγincreases . Surprisingly , applying BLIND   caused the models to encode less demographics   information even without information about the   protected attributes . This may explain why BLIND   is successful in reducing bias metrics associated   with these demographics , as well as suggesting that   other hidden characteristics may also be affected   by this debiasing process . These results align with   Orgad et al . ( 2022 ) , who found that the extractabil-   ity of gender in internal representations correlate   positively with various fairness metrics . However ,   our results are different from those of Mendelson   and Belinkov ( 2021 ) , who found a negative cor-   relation between robustness and biased features   extractability . Mendelson and Belinkov explicitly   modeled biased features for their debiasing pro-   cess , whereas we use demographics or success as   proxies for biased features , which might explain   the difference .   5.3 Bias detectors comparison : with and   without demographics   Recall that our method penalizes samples for which   the detector assigned a high probability to the cor-   rect label . The results indicated that using a demo-   graphic detector ( race / gender ) was more effective   than using a success detector . The two detector   models penalize samples differently , so we wish   to understand how the two differ and where they   agree . We compute the probability that each model8807   provided to the correct label ( details on compu-   tation in Appendix C.5).Here we present our   analysis of BERT . Results for DeBERTa are similar   ( Appendix G ) .   Table 2 summarizes which samples each detector   penalizes , defined as samples for which the detector   assigns a probability above 0.5 . We divide the   samples into two classes , depending on whether   the main model classified them correctly or not .   The table shows the percentage of samples that are   being penalized , out of all samples in this class .   We first note that the success detector ( left col-   umn ) is mostly penalizing samples where the main   model is correct ( 94 % and 95 % in sentiment and   occupation tasks , respectively ) , and much less sam-   ples where the main model is wrong ( 22 % and   33 % ) . Thus , the success detector reduces weights   on samples the model has already learned and clas - sified correctly , which could correspond to easier   samples that contain more bias . This reduces the   overall bias by preventing the main model from   over - fitting to these samples .   Looking at samples that both detectors penalize   ( right column ) , we observe that they are mostly   samples which the main model succeeds on ( 69 %   for sentiment and 87 % for occupation ) , suggesting   their importance for debiasing . However , when   observing what samples the demographics detector   penalized but the success detector did not ( middle   column ) , we find many failure samples ( 55 % for   sentiment and 66 % for occupation ) . In our exper-   iments , our method with demographics mitigated   bias better than the one without . The gap between   the methods might be because failure samples are   less penalized by the success detector , since the   success detector fails to correctly classify these   samples . Better debiasing might be achieved by de-   tecting failures in the main model more effectively ,   perhaps by using a stronger success detector .   6 Prior Work   Studies suggests a variety of ways for debiasing   NLP models from social biases on downstream   tasks , such as preprocessing the training data ( De-   Arteaga et al . , 2019 ; Zhao et al . , 2018 ; Han et al . ,   2021a ) , modifying the training process ( Elazar and   Goldberg , 2018 ; Shen et al . , 2022 ) , or applying   post - hoc methods to neural representations of a   trained model ( Ravfogel et al . , 2020 , 2022 ; Iskan-   der et al . , 2023 ) . All these methods , however , re-   quire that we define the bias we wish to operate8808upon , for example , gender bias . Additionally , many   of these methods require demographic annotations   per data instance , such as the gender of the writer   or the subject of the text . Webster et al . ( 2020 ) is an   exception , since it performs gender - debiasing by   modifying dropout parameters . Another exception   is JTT ( Liu et al . , 2021 ) , which improved worst-   group errors by training a model twice : first a stan-   dard ERM model , then a second model that up-   weights training samples misclassified by the first   model . The authors of these studies chose hyper-   parameters based on fairness metrics they wanted   to optimize , while we choose our hyper - parameters   without explicitly measuring fairness metrics . To   our knowledge , this is the first study to mitigate   social biases in NLP without assuming any prior   knowledge .   Other studies have focused on improving NLP   models robustness without prior knowledge of   bias issues , but without considering social bias .   Utama et al . ( 2020 ) and ( Sanh et al . , 2020 ) tackled   dataset biases ( a.k.a annotation artifacts ) in nat-   ural language understanding tasks , by training a   weak learner to identify biased samples and down-   weighting their importance . Weak learners are ei-   ther trained on a random subset of the dataset or   have a smaller capacity .   Regarding social fairness without demograph-   ics , Lahoti et al . ( 2020 ) proposed adversarially   reweighted learning , where an adversarial model   is trained to increase the total loss by re - weighting   the training samples . They used tabular , non-   textual data in their experiments . We consider non-   adversarial methods since adversarial training is   known to be difficult to stabilize . Hashimoto et al .   ( 2018 ) proposed a method for minimizing the worst   case risk over all distributions close to the empiri-   cal distribution , without knowledge of the identity   of the group . Coston et al . ( 2019 ) considered fair-   ness in unsupervised domain adaptation , where the   source or target domain do not have demographic   attributes , by proposing weighting methods . Han   et al . ( 2021b ) proposed debiasing via adversarial   training with only a small volume of protected la-   bels .   Focal loss ( Lin et al . , 2017 ) was proposed as   a method to address class imbalances by down-   weighting loss associated with well - classified sam-   ples . Raji ˇc et al . ( 2022 ) proposed using the original   focal loss ( Lin et al . , 2017 ) to improve robustness   in natural language inference , leading to improvedout - of - distribution accuracy . Debiased Focal Loss   ( DFL ) ( Karimi Mahabadi et al . , 2020 ) is a variant   of focal loss proposed to improve natural language   understanding models on out - of - distribution data .   7 Discussion and Conclusion   Even though BLIND led to bias reduction , it was   less effective than our method that used demo-   graphic annotations . Analysis showed that the suc-   cess detector is less accurate at classifying samples   that fail the main model . Additionally , the success   detector might be less focused than demographic-   based methods , but it might mitigate biases we   have not identified and can not measure without an-   notations . Thus , it would be interesting to see how   BLIND works on intersectional biases .   In sentiment analysis , BLIND reduced bias less   than JTT , which also does not use demographics   for training , but does for hyper - parameter search .   However , JTT was ineffective on the occupation   classification task , while BLIND was effective for   both tasks . The two tasks differ significantly , as   well as their data . For BERT and DeBERTa , pre-   training data is closer to biographies than tweets ,   so perhaps training for longer is beneficial for the   tweets data used in the sentiment classification task ,   and repeating samples in the training set ( as in JTT )   is similar to training for more steps . In any case ,   BLIND proved more reliable and generalizable in   reducing bias .   Our method has the potential for broader appli-   cations beyond demographic biases . While our pri-   mary focus was on mitigating demographic biases ,   the approach can be adapted to address other types   of biases by identifying relevant proxy indicators   via the success detector .   To summarize , we demonstrated the reduction of   racial and gender biases of NLP classification mod-   els , without any prior knowledge of those biases in   the data . These results suggest that we can poten-   tially apply BLIND to any dataset , which makes   bias reduction a much more feasible endeavor and   applicable to real - world scenarios .   8 Limitations   One limitation of this study is its scope , which   covers two downstream tasks and two types of de-   mographics ( race and gender ) . The binary gender   definition we used excludes other genders that do   not fall under male or female . In the case of race ,   we explored only African American race ( proxied8809by African American English ) , which excludes bi-   ases related to other races , and is a US - centric view   of racial bias . We did not investigate other types   of bias , such as religious bias . Furthermore , our   method was tested on datasets with short texts , and   it is unclear how it will perform on longer texts .   The experiments were conducted on datasets in En-   glish , and it is unclear how our method will work   on languages that are morphologically rich .   9 Ethics Statement   Through this study , we aim to reduce the barriers   of data collection in the effort of mitigating bias .   In situations where demographic information is   not available at all , or where its use could cause   privacy concerns , this method may be especially   useful . As with other bias mitigation methods , ap-   plying BLIND to the training process might create   a false sense of confidence in the model ’s bias , but   as we target scenarios without demographics , the   risk is greater as it may be harder to discover cases   where bias remains . We encourage practitioners of   NLP who use BLIND to identify potential biases   and harms experienced by individuals using their   system , and to define their fairness metrics accord-   ingly . In order to verify if the system is working   as expected according to the predefined fairness   metrics , we encourage collecting a small validation   set with demographic annotations .   Acknowledgements   This research was supported by the ISRAEL SCI-   ENCE FOUNDATION ( grant No . 448/20 ) and by   an Azrieli Foundation Early Career Faculty Fellow-   ship .   References88108811   A Connection of DFL to adversarial   training   As adversarial learning is a natural competing base-   line , we attempted to apply adversarial training to   our task setting at an initial step of the research ,   but found it to be highly unstable and impractical ,   which corroborates findings in the literature ( Ganin   et al . , 2016 ; Grand and Belinkov , 2019 ) . We also   examined the work of Elazar and Goldberg ( 2018 ) ,   who utilized adversarial learning for removing de-   mographic attributes from text inputs . However ,   they caution against relying on adversarial removal   for achieving fairness , as their results indicated that   demographic information could still be extracted by   classifiers of the same architecture . To address the   issue of attribute leakage more effectively , subse-   quent work ( Ravfogel et al . , 2020 , 2022 ) proposed   other methods that address demographic attribute   leakage better than adversarial training , which we   compare our method with .   B DTO   DTO Han et al . ( 2021a ) is measured as the L2-   distance from a utopia point , ( maxccuracy , 0 ) ,   where maxccuracy is the maximum accu-   racy achieved on the task in our experiments .   ( accuracy , fairness ) are the candidate points .   We computed fairness by averaging the various   fairness metrics measured in this study . In a prac-   tical application , the definition of the candidate   points and the utopia points should reflect the ap-   plication ’s current needs and priorities .   C Implementation Details   C.1 Fairness Metrics   For measuring the statistical fairness metrics ( in-   dependence , separation , sufficiency ) , we used   thefairness library , implemented by AllenNLP   ( https://github.com/allenai/allennlp ) .   FairBatch fairness metrics . FairBatch considers   three fairness metrics : equal opportunity , equalized   odds , and demographic parity . Equal opportunity is   closely related to the metric Independence , Equal-   ized odds is closely related to Separation and equal-8812ized opportunity is closely related to TPR gap . In   the main results table ( Table 1a ) we present the   variation that achieved the best metrics we display .   However , we found very little difference between   the results of the different variations of FairBatch .   C.2 Training and Evaluation   For the DFL loss , increasing the temperature tin-   creases the smoothness of the softmax result , re-   sulting in less radical regularization . In each ex-   periment , we grid - search γ∈ { 1,2,4,8,16}and   t∈ { 1,2,4,8}using the validation set .   We trained BERT ( Devlin et al . , 2018 ) and   DeBERTa - v1 ( He et al . , 2020 ) models , with one   linear classification layer on top of them . For   BERT we used the bert - base - uncased model by   Huggingface , which has 110 M parameters and for   DeBERTa we used the microsoft / debeta - base   from the Huggingface library , which has 1.5B pa-   rameters . We trained each model for 10 epochs —   which took around 5 - 6 hours for the occupation   classication task and 3 - 4 for the sentiment classifi-   cation task — and saved the best epoch based on the   validation accuracy .   We used the following GPUs for training BERT :   Geforce RTX 2080 Ti , TITAN Xp , GeForce GTX   1080 Ti , and the following GPUs for training De-   BERTa : A40 , RTX A4000 . Each experiment was   run using 5 different seeds : 0 , 5 , 26 , 42 , 63 . These   seeds were used to anchor the model ’s initializa-   tion , the data split , and any other randomness in the   code , and are considered as an input to our released   scripts . We used a 65 - 10 - 25 training - validation - test   split ratio for all tasks . Training was done with a   learning rate of 5e−5and an AdamW ( Loshchilov   and Hutter , 2018 ) optimizer . Both models were   trained by passing the biography / tweet through   the transformer model , obtaining the top [ CLS ]   token representation and feeding it into the classifi-   cation layer . The entire model was fine - tuned end-   to - end to optimize the cross entropy loss , while in   the DFL setting we added a weighting component .   The demographics / success detector ’s architecture   is a single linear classification layer optimized to   solve the appropriate classification task . It was   trained with a learning rate of 1e−3and an Adam   ( Kingma and Ba , 2015 ) optimizer .   Sentiment Classification . Our setup followed   the settings of Ravfogel et al . ( 2020 ) , where we   controlled how biased the training data is . We used   a subset of the original dataset with 100 K samples , and our training data was imbalanced such that   the “ happy ” sentiment class was composed of 70 %   AAE and 30 % MUSE , while the “ sad ” sentiment   was composed of 70 % MUSE and 70 % AAE . The   dataset was overall balanced : 50 % “ sad”/“happy ”   and 50 % AAE / MUSE .   Occupation Classification . We trained our mod-   els on the entire dataset , without any distribution   modifications .   For both tasks , data for validation and testing   was balanced such that each class had the same   demographic distribution .   C.3 Baselines and Competitive Systems   We ran INLP and RLACE on model representa-   tions extracted from a finetuned model without   DFL training . For INLP and RLACE , we used   the implementation of the authors and the same   hyper - parameters . For JTT , since the datasets were   different , we provide our own implementation but   searched over the same hyper - parameters , which   areλ∈ { 4,5,6}(the rate of multiplication of   repeated instances ) and T={1,2}(the epoch of   the model used to calculate failed instances ) . We   ran the scrubbing algorithm provided by the code   of De - Arteaga et al .. FairBatch was ran using the   code provided in the paper , for a training period   of 10 epochs where the best checkpoint is chosen   as the epoch with the best fairness metric being   optimized .   C.4 MDL probing : implementation details   Our MDL probe ( V oita and Titov , 2020 ) is based   on the implementation by Mendelson and Belinkov   ( 2021 ) . The linear probe is trained with a batch size   of 16 and a learning rate of 1e-3 in all experiments .   We used the following data accumulation fractions   to train the probe : 2.0 % , 3.0 % , 4.4 % , 6.5 % , 9.5 % ,   14.0 % , 21.0 % , 31.0 % , 45.7 % , 67.6 % , 100 % .   C.5 Bias detectors comparison :   implementation details   We wished to calculate for each model , the prob-   ability it assigned to the right label . To do that ,   we load the checkpoint of a BLIND - trained model   we are interested in , and then compute the main   model ’s predictions and the hidden representations   of each sample from a BLIND - trained model . Fol-   lowing that , we calculate the success detector ’s   and the demographics detector ’s predictions on the   extracted representations . For the demographics8813detector , we train another model for predicting de-   mographics from the extracted representations , to   simulate how a demographics detector would be-   have on the same representations .   D Full results   The full results can be found in Tables 5 , 6 , 7 and   8 .   E Effect of γ   Recall that the DFL loss ( Equation ( 1 ) ) uses γto   control how much importance to give to a biased   sample ; the higher γ , the less weight a biased sam-   ple receives in the loss , which should result in a   more biased model . Indeed , Figures 3 , 4 , 5 and   6 show that as γincreases ( moving from top to   bottom on the heatmaps ) , the bias metrics and ac-   curacy both tend to decrease .   Choosing the hyper - parameters blindly . As the   figures show , increasing γcauses the debiasing ef-   fect to be more aggresive , until a point that it is   collapsing and unable to train ( very low accuracy ) .   Increasing the temperature helps balancing this pro-   cess , where on the higher gammas a lower tempera-   ture mean more aggressive debiasing and thus less   bias . Based on this analysis , we conclude that it is   possible to make the model selection based on γ   andtalone , using the following logic : choose the   highest γand the lowest tfor which the accuracy   is above a tolerance threshold .   F Performance of Success Detector   Table 3 presents the full performance and calibra-   tion information of the success detector in both   tasks .   G Bias detectors comparison : with and   without demographics   Table 4 presents the results of bias detectors com-   parison for DeBERTa.881488158816881788188819ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   8   /squareA2 . Did you discuss any potential risks of your work ?   9   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   1   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We provide the link to our code in the abstract .   /squareB1 . Did you cite the creators of artifacts you used ?   3 and C   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   These are discussed in the git repo of our published code .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Yes - 3 . Our artifacts - our code is for a debiasing method , and we distribute it for that purpose ,   while discussing possible risks .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We measure social biases , so names and other identiﬁers are a crucial part of this phenomenon .   Additionally , the datasets we used are publicly available .   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Languages - sections 1 and 3 . Demographic groups are the main subject of the paper as we measure   biases .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Sections 3 and appendix C8820C / squareDid you run computational experiments ?   3 , 4   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   C   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   C   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   4   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   C   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.8821