  Fanshuang Kong , Richong Zhang , Xiaohui Guo , Samuel Mensah , Yongyi MaoSKLSDE , Beihang University , Beijing , ChinaZhongguancun Laboratory , Beijing , ChinaHangzhou Innovation Institute , Beihang University , Hangzhou , ChinaDepartment of Computer Science , University of Sheffield , UKSchool of Electrical Engineering and Computer Science , University of Ottawa , Canada   { kongfs,zhangrc,guoxh}@act.buaa.edu.cn   s.mensah@sheffield.ac.uk , ymao@uottawa.ca   Abstract   Overfitting is a common problem when there is   insufficient data to train deep neural networks   in machine learning tasks . Data augmenta-   tion regularization methods such as Dropout ,   Mixup , and their enhanced variants , are effec-   tive and prevalent , and achieve promising per-   formance to overcome overfitting . However ,   in text learning , most of the existing regular-   ization approaches merely adopt ideas from   computer vision without considering the im-   portance of dimensionality in natural language   processing . In this paper , we argue that the   property is essential to overcome overfitting   in text learning . Accordingly , we present a   saliency map informed textual data augmenta-   tion and regularization framework , which com-   bines Dropout and Mixup , namely DropMix , to   mitigate the overfitting problem in text learning .   In addition , we design a procedure that drops   and patches fine grained shapes of the saliency   map under the DropMix framework to enhance   regularization . Empirical studies confirm the   effectiveness of the proposed approach on 12   text classification tasks .   1 Introduction   Deep neural networks have shown its effective-   ness , achieving state - of - the - art result in many natu-   ral language processing ( NLP ) tasks . However ,   these models are likely to overfit training sets   with few samples due to the large number of pa-   rameters they contain . Regularization techniques ,   such as weighted decay ( Krogh and Hertz , 1992 ) ,   dropout ( Srivastava et al . , 2014 ) , and data augmen-   tation ( Goodfellow et al . , 2015 ; Zhang et al . , 2017 )   are powerful to overcome overfitting by making   slight modifications on the neural network or data   space to improve generlization .   Data augmentation methods have shown to be   useful for overcoming overfitting by increasing the   Table 1 : Illustration of an example Xgenerated by   SSMix . The darker the red ( or blue ) shade is , the more   salient the words in the positive ( or negative ) sentiment   decision . SSMix uses a Saliency Map ( Simonyan et al . ,   2013 ) to measure the saliency of words . SSMix sub-   stitute the least salient word “ is ” in Xwith the most   salient “ bad ” in X. The mixup ratio λ= 0.17as the   length of the salient words ( i.e. , “ bad ” ) is one out of six   words in X. Thus , the generated label is 17 % negative   or83 % positive .   training dataset size via adding augmented ver-   sions of already existing training data . For ex-   ample , Goodfellow et al . ( 2015 ) adds the worst   case adversarial sample into the training dataset ;   Mixup ( Zhang et al . , 2017 ) introduces new samples   by the linear interpolation of samples and their cor-   responding labels . The majority of these methods   are however designed for computer vision tasks ,   requiring adaptation for text learning . However ,   due to the discreteness and high - dimensional na-   ture of text data spaces , adapting these methods is   non - trivial .   Among the proposed data augmentation meth-   ods , Mixup ’s linearity inductive bias is succinct   and effective since it reduces the discreteness in-   between samples or the space in which new sam-   ples are generated . Thus , motivating researchers   to adapt this method for text learning ( Verma   et al . , 2019 ; Chen et al . , 2020 ; Zhang et al . , 2020 ;   Yoon et al . , 2021 ) . The current state - of - the - art SS-   Mix ( Yoon et al . , 2021 ) which builds upon Mixup ,   generates new samples by performing a saliency-   based span mixup on the input text rather than   its embedding vectors as like previous approaches   ( Verma et al . , 2019 ; Guo et al . , 2019 ) . As shown   in a text sentiment classification task ( Table 1 ) , SS-   Mix replaces the least salient span in Xwith the890most salient span in Xto generate the new sam-   pleXand uses a mixup ratio λto determine the   label X.   Unfortunately , three problems concurrently oc-   cur in SSMix which limits its ability to generate   expressive examples . Firstly , although SSMix has   a strong flexibility by exploring a larger synthetic   space to generate new samples , this space is in   fact high - dimensional as the input data consists   of words rather than embeddings . Besides , words   within this space is disordered and therefore may   lead to unpredictable sampling results . This leads   to the second and third problems , where there is a   potential of generating “ dirty ” examples or caus-   ing a label drift ( i.e. , newly added sample is catego-   rized under a wrong class ) , as it does not consider   the position of words or the sentence syntax struc-   ture in the mixup . For instance , Xcan be said   to be dirty . Besides being wrong grammatically ,   the generated example contains contrasting opinion   words ( i.e. , “ bad”,“delicious ” ) offering confusing   clues to determine its label .   To tackle the aforementioned issues , we pro-   pose a patch - aware DropMix framework , a data   augmentation method which takes advantage of   Dropout and Mixup , to generate new examples   based on the dimension and saliency of words in   texts . In contrast to SSMix , DropMix maps the   high - dimensional input data ( i.e. , words ) into a low-   dimensional space to encode the semantics of the   text input and mitigate the unpredictability of gener-   ated examples . Despite the problems encountered   by SSMix , empirical results indicate that sampling   from a large synthetic space may have some bene-   fits . This may be intuitively desirable , as the model   is not constrained to generate examples from only   the input text representations . To compensate for   this , DropMix applies Dropout on the embeddings   ofX(andX ) and replaces the dropped out units   with noise from X(andX ) to reduce the over-   dependence on the input features . However , since   the saliency of words in texts impacts the class de-   cision , the activation neurons in the features to be   dropped depends on the saliency with a Beta prior   ( or drop ratio ) . In other words , we present a gen-   eral patch - aware DropMix framework that drops   a patch surrounding the peak salient region of one   text input embedding and mixes it with the corre-   sponding patch of another text input embedding   to generate new examples . Under this framework ,   we develop other variants of DropMix that mixestext input embeddings using different fine - grained   patches based on the saliency and dimension of   words in text .   In a nutshell , we make the following contribu-   tions in this paper :   •We propose a unified framework DropMix ,   which leverages dropout and mixup for data   augmentation and regularization in natural lan-   guage processing tasks .   •Under this framework , we propose an efficient   fine grained patch selection for the mixup ,   with three other variants for the text augmen-   tation .   •Empirical studies on 12 commonly used   datasets in text classification confirms the ef-   fectiveness of our model .   2 Related Work   A variety of regularization techniques have been   proposed to solve overfitting in deep neural net-   works ( Zhang et al . , 2017 ; Yun et al . , 2019 ; Verma   et al . , 2019 ) . Some of the notable and recent meth-   ods include , Vanilla Mixup ( Zhang et al . , 2017 )   which linearly interpolates two input samples and   their associated labels to generate augmented ver-   sions for regularization . CutMix ( Yun et al . , 2019 )   cuts and pastes image patches to construct new ex-   amples , and mixes the labels in proportion to the   cut patch sizes . Manifold Mixup ( Verma et al . ,   2019 ) on the other hand interpolates the middle   hidden layers ’ feature maps . PatchUp ( Faramarzi   et al . , 2020 ) proposes a method of patches swap   or interpolation on the hidden layer feature maps .   SaliencyMix ( Uddin et al . , 2021 ) mixes a target im-   age with a saliency map ( Simonyan et al . , 2013 ) of   the source image that highlights important objects .   Although the majority of these approaches were   designed for vision tasks , they have been widely   adopted for NLP ( Guo et al . , 2019 ; Verma et al . ,   2019 ; Yoon et al . , 2021 ) .   In the context of text classification , Mixup regu-   larization variants including , wordMixup and sen-   Mixup ( Guo et al . , 2019 ) perform interpolation   on the word and sentence embeddings . Guo et al .   ( 2019 ) shows that Mixup can improve the accuracy   upon both CNN and LSTM sentence classification   models . MixText ( Chen et al . , 2020 ) extends the   hidden layer feature interpolation method to semi-   supervised text classification task . To overcome the891mixing difficulty in the discrete text input spaces ,   SeqMix ( Zhang et al . , 2020 ) performs token - level   interpolation in the embedding space and selects   a token closest to the interpolated embedding . SS-   Mix ( Yoon et al . , 2021 ) performs mixup operation   on the input text rather than hidden vectors as like   previous methods ( Guo et al . , 2019 ; Chen et al . ,   2020 ; Zhang et al . , 2020 ) . Specifically , SSMix syn-   thesizes a sentence while preserving the locality of   two original texts by relying on high saliency span-   based mixing . This work is closely related to our   methods , but we argue that the synthesis sanity of   its input word level mixup rather generates “ dirty ”   samples .   3 Preliminaries   Our proposed model , DropMix , builds upon no-   table works ( Srivastava et al . , 2014 ; Guo et al . ,   2019 ) relating to data augmentation for text   classification . Specifically , given a text x=   [ x , x , ... , x]withNwords , xis typically rep-   resented as low - dimensional features X∈R ,   generated through an encoder such as Bert ( De-   vlin et al . , 2018 ) . A classifier takes Xas input   and maps it to one or more class labels Y. To im-   prove the classification problem , conventional data   augmentation methods such as Dropout ( Srivastava   et al . , 2014 ) and Mixup ( Guo et al . , 2019 ) have   been utilized to encourage generalization . Here ,   we describe the basics of these data augmentation   methods as they are fundamental to our approach .   3.1 Dropout   Dropout ( Srivastava et al . , 2014 ) is a classical regu-   larization scheme which works by randomly drop-   ping inputs of a layer during training , which may   be the neurons of a neural network or the features   ( or units ) of the data sample . For text classifica-   tion , randomly dropping features of the data sample   refers to applying a dropout on the word embed-   ding layer to generate an augmented version of X ,   denoted as /tildewideXand formulated as follows :   /tildewideX = M⊙X,/tildewideY = Y   whereM∈Ris a mask matrix with entry   M∼Bernoulli ( p).pis a dropout ratio . The op-   erator⊙denotes an element - wise product . The new   label / tildewideYof / tildewideXremains unchanged . With Dropout ,   the over - dependence on the input features is re-   duced to encourage the learning model to general-   ize .   3.2 Mixup   Mixup is a data augmentation method that gener-   ates new training data ( /tildewideX,/tildewideY)by linearly interpo-   lating a pair of random inputs , XandX , and   their corresponding labels YandY.   /tildewideX= ( 1−λ)X+λX   /tildewideY= ( 1−λ)Y+λY(1 )   where λis beta - distributed with parameter α , de-   noted by λ∼Beta ( α , α ) . By generating such   samples , Mixup encourages a model to behave lin-   early in - between training samples to improve gen-   eralization . We refer the reader to the standard   source ( Guo et al . , 2019 ) for a detailed review on   Mixup .   4 DropMix for Text Augmentation   In this paper we combine the advantages of   Dropout and Mixup to develop a new approach   for data augmentation , namely , DropMix ( Fig . 1 ) .   Specifically , DropMix encourages a model to re-   duce the over - dependence on input units while be-   having non - linearly in - between training samples .   The idea is to dropout units in Xand replace their   positions with corresponding units in Xbased on   the saliency of the units . We define the augmenta-   tion approach as follows :   /tildewideX= ( 1−M)⊙X+M⊙X   /tildewideY= ( 1−λ)Y+λY(2)892where λ∼Beta ( α , α).M∈Ris con-   strained on λ , thus , λND units of Xis replaced   by the units in X.   Typically , the mask matrix Mis generated ran-   domly by setting units to 0 . This works for   Dropout because it significantly reduces the over-   dependence on input units without focusing on any   particular unit . However , our approach is more con-   cerned with the model ’s dependency on the saliency   on both inputs XandX. Hence , rather than ran-   dom sampling , we develop algorithms that identify   the salient units of Xto construct M. That is , the   resulting features /tildewideXbecomes a mixture of Xand   Xwhich considers the saliency of both inputs   into account .   4.1 Patch Selection and Mixing   In order to determine the salient units in X , we   use a saliency map ( Simonyan et al . , 2013 ) , denoted   asI∈R. Specifically , each unit in Iis   the magnitude of the derivative of classification   loss w.r.t its corresponding feature in X , found   through back - propagation . The saliency map ( or   saliency matrix ) highlights the important units that   affect the text classification decision .   We now aim to extract patches of the saliency   map for the mixup . To this end , we first use   sliding filter windows with size n×d , where   n∈ { 1,2 , ... , N}andd∈ { 1,2 , ... , D}and per-   form a convolution over the saliency map . Un-   der the assumption that we use a sliding stride of   1 , we obtain ( N−n+ 1 ) ( D−d+ 1 ) windows .   An L2 norm is then used as an activation func-   tion and applied on each window to highlight the   saliency of features . A saliency activation map   F∈Ris created by concate-   nating all the extracted windows .   At this point , we initialize a mask matrix M ,   which has the same size and dimension as X(or   X ) . The initialized Mis a matrix of ones . To   identify the units to be zeroed out , we relate it to   the feature activation mat F. More specifically ,   first note that each value in Frelates to a window   of size n×dinI. SinceX(orX ) also share   the same shape with I , we can infer that each   value in Falso relates to a window in X(or   X ) . The index mapping function ffromFto   Ican be defined as follows :   f(i , j ) = /uniondisplay / uniondisplay(p , q ) ( 3)where ( i , j)is an index of F,(p , q)is an index of   I. We identify the top- ksalient units in F , sort   them and collect their corresponding kwindows .   For each window , we set all the ndunits of Mcor-   responding to the window to zero . The patch used   in the mixup can now be extracted by applying the   mask ( or the mask ’s inverse ) on X(orX ) . The   new features /tildewideXcan now be generated , following   Eqn . ( 2 ) .   It should be noted that a unit in Imay be in-   volved with several patches . The number kis set   to ensure that the total number of units for replace-   ment is no more than λND . The relation between   kandλND is defined as follows :   λND = |/uniondisplayf(i , j)| ( 4 )   where Kis the set of top kwindows ’ indexes in   F.   To ensure the interpolation of features corre-   spond to the interpolation of associated labels , we   make an adjustment to the mixing of labels :   /tildewideY= ( 1−S   ND)Y+S   NDY ( 5 )   where Sis the sum of Melements , i.e. , the number   ofMelements containing one .   4.2 Window Size Setting   With regard to the window size setting , we are mo-   tivated by the assumption that the continuity of   words ( i.e. , words in the Ddimensional space ) in   text include context information which is signif-   icant for text classification . Therefore , nandd   should be in the set n∈ { 1,2 , ... , N } , d∈ { 1 , D } .   Our filtering window becomes a general formula-   tion of patch selection , which we refer to as general .   Thus , we refer to this model as DropMix - general   ( DropMix - G ) .   We introduce other filter window configurations :   unit , channel , word and develop the model vari-   ants DropMix - unit ( DropMix - U ) , DropMix - word   ( DropMix - W ) and DropMix - channel ( DropMix-   C ) .   DropMix - U ( n= 1 , d= 1 ): DropMix with a filter   window which convolves one unit in X.   DropMix - C ( n = N , d = 1 ): DropMix with a   filter window which convolves one dimension of   the embedding for all words from a sentence . That   means nfeatures of each dimension are replaced   or kept simultaneously.893DropMix - W ( n∈ { 1,2 , ... , N } , d =D ): Drop-   Mix with a filter window which convolves the   whole embedding of ncontinuous words . It has a   similar connotation to modelling with n - grams .   4.3 Loss Function   We adopt cross entropy to calculate the loss to op-   timize model parameters . Given a pair of inputs   ( X , Y)and(X , Y)for instance i , the loss   function can be defined as follows :   L=−(1−λ)/summationdisplayYlog(Y)−λ / summationdisplayYlog(Y )   ( 6 )   where Yis the label for the generated input /tildewideX.   5 Experiment   5.1 Dataset   To evaluate DropMix , we use 12 text classifica-   tion datasets from two sources : ( 1 ) 6 datasets se-   lected from the popular benchmark GLUE ( Wang   et al . , 2018 ): QQP , MNLI ( Williams et al . , 2018 ) ,   SST-2 ( Socher et al . , 2013 ) , QNLI ( Rajpurkar   et al . , 2016 ) , RTE ( Bentivogli et al . , 2009 ) and   MRPC ( Dolan and Brockett , 2005 ) . ( 2 ) 6 CNN   datasets used in the work of LeCun et al . ( 1998 ):   CR ( Hu and Liu , 2004 ) , MR ( Pang and Lee , 2005 ) ,   MPQA ( Esuli et al . , 2008 ) , SST-1 ( Socher et al . ,   2013 ) , Subj ( Pang and Lee , 2004 ) , TREC ( Li and   Roth , 2002 ) . Both GLUEand CNN datasets   can be downloaded from the Github link . Table 2   shows the statistics of all datasets . We use accuracy   as the evaluation metrics following the practice in   compared works ( Devlin et al . , 2018 ) .   5.2 Baseline   We implement and compare with a series of base-   line models , including Bert , Bert - Dropout , Bert-   Mixup , Bert - Dropout+Mixup and SSMix .   Bert in its original form includes a dropout layer .   To demonstrate the impact of data augmentation   strategies , we ablate the dropout layer in the origi-   nal Bert and report the performance .   Bert - Dropout is the officially published and exten-   sively utilized version of Bert ( Devlin et al . , 2018 ) .   Bert - Mixup replaces the Dropout layer in Bert-   Dropout with Mixup ( Guo et al . , 2019 ) .   Bert - Dropout+Mixup applies dropout on the in-   put features and applies a vanilla mixup to generate   the new data . This is different from our method   DropMix , which drops and mix corresponding fea-   tures based on the saliency of the input data .   SSMix ( Yoon et al . , 2021 ) is an enhanced model   of mixup inspired by CutMix and PuzzleMix ( Kim   et al . , 2020 ) .   Unfortunately , most of the image - based Mixup   models ( Kim et al . , 2020 ; Yun et al . , 2019 ; Uddin   et al . , 2021 ) can not be applied directly to text pro-   cessing tasks . SSMix on the other hand has been   successfully applied in text processing and it is now   the current SOTA in text regularization . We there-   fore exclude the image - based Mixup models but   compare with SSMix as well as our baselines .   Hereinafter , we omit ’ Bert- ’ Prefix from model   names for brevity .   5.3 Implementation Details   For comparisons , we exploit the pre - trained BERT-   base - uncased model from Hugging Face Trans-   formersand the source code from Github . Fol-   lowing the recommended parameter settings : max   sentence length Nis 128 , word embedding dimen-   sionDis 768 , batch size is 32 , epoch is 3 and   learning rate is 2e-5 , optimizer is AdamW with eps   1e-8 . For the specific parameters of regularization894   models , the dropout ratio pfor Dropout is set at   0.1 , the beta distribution parameter αis set at 0.1   for Mixup and α∈ { 0.025,0.05,0.1,0.25,0.5,1 }   for SSMix and DropMix . Unlike DropMix - U and   DropMix - C which have a determined window size ,   we set n∈[2,32]for DropMix - W and DropMix-   G. Since regularization methods reduce the speed   of convergence , we run multiple epochs ( 5 to 10   times ) for these regularization schemes . The pa-   rameter quantity of all regularization models are   equal to the original Bert ( Devlin et al . , 2018 ) . All   hyper - parameters are set manually and empirically   tuned on the development set during the training   process .   All experiments are completed on Tesla V100-   SXM2 - 32 GB GPU , costing about 1300s per epoch   for the medium scale dataset QNLI .   5.4 Text Classification Performance   We run our experiments five times using different   random seeds on all models . The average classifica-   tion accuracy and its standard deviation are shown   in Table 3.From these results , we find that DropMix outper-   forms other baseline models on all datasets , except   for MRPC . These results indicate that DropMix is   an effective scheme for text data augmentation . It is   also worth noting that SSMixstarts from the best   checkpoint of Dropoutand measures validation   accuracy every 500 steps . This proves to be use-   ful in achieving faster optimization . Nonetheless ,   DropMix still enjoys a better overall performance .   Among the four variants of DropMix , DropMix-   G generally achieves the best results , except on   QNLI and MRPC datasets . The results indicate   that DropMix - G is an efficient variant and choos-   ing fine grained patch on text representation is prac-   tical for data augmentation . Besides DropMix - G ,   DropMix - C achieves comparable results and out-   performs DropMix - U and DropMix - W in almost   all datasets except MR . This indicates DropMix-   C is a satisfying substitute to DropMix - G when   there is an efficiency requirement in the training   process . Furthermore , DropMix - C performs better   than DropMix - W. This demonstrates that channel   may be more important and appropriate than word895   ( DropMix - W ) for text augmentation .   We perform additional experiments to investi-   gate the model behaviour with different encoders ,   particularly , BERT - large - uncased and RoBERTa .   Table 4 shows the results of our experiments on   six datasets . Similar to the results produced via   the BERT - base - uncased encoder ( see Table 3 ) , we   find that both BERT - large - uncased and RoBERTa   leads to a better performance of DropMix variants   when compared to Dropout and Mixup . The results   indicate the efficiency of DropMix .   5.5 Performance on Sparse Data   To further analyze in sparse settings , we downsam-   ple50%,25%,10%,5%of the training data from   the 3 large datasets MNLI , QQP and QNLI . For the   sake of efficiency , we use DropMix - C in this exper-   iment . The classification accuracies are shown in   Figure 2 .   Results indicate that more training samples could   improve the performance of all models . On the   reduced datasets , we observe that these methodshave a certain regularization effect . DropMix - C is   better than other regularization models generally ,   and as the dataset decreases extremely to 10 % or   5 % , the performance gap between DropMix - C and   compared methods is enlarged . These observations   demonstrate the effectiveness of the DropMix - C on   even sparser datasets .   5.6 Fine Grained Patch Analysis   To discover the patch selection of DropMix - G , we   plot the saliency map of a sentence . We employ   a fixed λto determine the ratio of units to retain   for convenience . This is to ensure that for a given   sentence , the proportion of units to be replaced is   consistent in different epochs . We also set n= 2to   observe the effect of general in DropMix - G. Since   the saliency map has an extensive range of values   and it can not represent the result of the final patch ,   we plot the mask matrices as saliency map to some   extent . Figure 3 ( a ) illustrates the ‘ saliency map ’ of   the sentence “ like jaglom ’s films , some is honestly   affecting ” taken from SST-2.896   In this case example , words that affect the sen-   timent classification decision include the words ,   “ honestly ” and “ affecting ” . These words are easily   spotted when we use word , indicating its effective-   ness . Additionally , not all channels of these two   words are highlighted , which implies channel plays   a distinct role in the text representation . By com-   paring the results of general as against word and   channel to obtain the fine grained patch , we can   conclude that general is reasonable and effective   for text representation.5.7 Channel Importance Analysis   For a comprehensive study on the effectiveness of   channel , we construct an experiment on channel   selection on SST-2 . For positive and negative sam-   ples , we separately count the number of times that   each channel is selected ( highlighted in the saliency   map ) . For fair comparison , we divide the count by   the number of positive and negative samples . We   believe this value represents the importance of a   specific channel , call this the channel importance .   We first plot the channel importance of positive   and negative samples in the training set as shown   in Figure 3 ( b ) . It can be observed that channel   importance of positive and negative samples gain   peak values on different channels , which indicates   that channel plays a disparate role with different   sentiment polarity .   We further plot the difference between the chan-   nel importance in positive and negative to eliminate   the impact of irrelevant channels . As the decreas-   ing ordered difference illustrated in Figure 3 ( c ) ,   for all 768 channels ( from the 768 dimensions in   Bert Embeddings ) , around 100 positive channels   ( left of the figure ) and 100 negative channels ( right   of the figure ) achieves a relatively channel impor-   tance . We find that the selected positive channels   are more concentrated and have a higher value than   negative channels . To further investigate the under-   lying reason of this phenomenon , we find that neg-   ative sentences have richer patterns and are from   a more common vocabulary , such as “ little ” , “ too ”   and “ to ” . This means that more negative channels   are selected during the training . This observation   is also confirmed by the statistics in Table 5.897After obtaining the selected channels of positive   and negative , we further analyze which word con-   tributes most in a sentence of the selected channel .   Fixing the top 3 selected channels of positive sam-   ples , we compute the maximum gradient of each   word in this channel . Then we obtain the word   importance for each channel . We also conduct the   same procedure for negative samples . The top 3   channels for positive samples , negative samples   and their contributed words are shown in Table 5 .   Clearly , the listed contributed words are highly   related to their corresponding categories . This ex-   periment demonstrates the effectiveness of our pre-   sented model on channel .   5.8 CutMix vs. SSMix vs. DropMix   Our work is most closely related to the line of re-   search that adapt the mixup method ( Guo et al . ,   2019 ) . As such , we briefly discuss how these meth-   ods differ in architecture to gain a more in - depth   understanding of our proposed mechanism .   CutMix vs. SSMix CutMix cuts a patch of an im-   age and mixes it with a corresponding patch from   another image . SSMix is an adaptation of CutMix   to text classification . SSMix applies a saliency map   to determine the Mixup patch .   SSMix vs. DropMix SSMix disregards the posi-   tion and sentence structure of words , and simply re-   places unimportant word spans in Xwith salient   words in X. DropMix on the other hand consid-   ers the context of words as well as its structure by   mapping words into a low - dimensional space .   CutMix vs. DropMix CutMix cuts rectangular   patches on input images with 3 RGB channels .   Analogously , for a 2d sentence embedding matrix ,   DropMix - C uniformly cuts some specific dimen-   sion of latent word embedding , i.e. , a channel or   column vector patch . DropMix advocates and re-   gards channel mixing as suitable and important for   text synthesis and augmentation .   6 Conclusion   In this paper , we present a data augmentation frame-   work , DropMix , based on the saliency map which   combines Dropout and Mixup to facilitate regular-   ization in text learning . Our best results obtained   from our model variant DropMix - G indicates that   the features of the word embeddings are impor-   tant for patch selection and mixing . In general ,   our empirical results confirm the effectiveness of   DropMix.7 Limitation   DropMix - G has a dominant performance due to   the fine grained patch selection using an irregular   window size . This makes it computationally ex-   pensive . If there is an efficiency requirement for   an algorithm , DropMix - C is much more desirable   and it also achieves competitive performance with   DropMix - G.   8 Acknowledgements   This work was supported in part by the Na-   tional Key R&D Program of China under Grant   2021ZD0110700 , in part by the Fundamental Re-   search Funds for the Central Universities , in part by   the State Key Laboratory of Software Development   Environment . SM is supported by a Leverhulme   Trust Research Project Grant ( No . RPG-2020 - 148 ) .   References898899