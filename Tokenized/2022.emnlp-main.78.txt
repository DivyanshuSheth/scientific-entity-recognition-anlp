  Yue Yang , Wenlin Yao , Hongming Zhang , Xiaoyang Wang ,   Dong Yu , Jianshu ChenUniversity of Pennsylvania , Tencent AI Lab   Abstract   Large - scale pretrained language models have   made significant advances in solving down-   stream language understanding tasks . How-   ever , they generally suffer from reporting bias ,   the phenomenon describing the lack of explicit   commonsense knowledge in written text , e.g. ,   “ an orange is orange ” . To overcome this limita-   tion , we develop a novel approach , Z - LaVI , to   endow language models with visual imagina-   tion capabilities . Specifically , we leverage two   complementary types of “ imaginations ” : ( i ) re-   calling existing images through retrieval and   ( ii ) synthesizing nonexistent images via text - to-   image generation . Jointly exploiting the lan-   guage inputs and the imagination , a pretrained   vision - language model ( e.g. , CLIP ) eventually   composes a zero - shot solution to the original   language tasks . Notably , fueling language mod-   els with imagination can effectively leverage   visual knowledge to solve plain language tasks .   In consequence , Z - LaVI consistently improves   the zero - shot performance of existing language   models across a diverse set of language tasks .   1 Introduction   Large - scale Pretrained Language Models ( PLMs )   have achieved great success on various Natural   Language Understanding ( NLU ) tasks and even ex-   hibit impressive zero - shot capabilities without task-   specific fine - tunings ( Radford et al . , 2019 ) . And re-   cent research suggests that such ability improves by   further scaling up the model size ( e.g. , to hundreds   of billions of parameters ) and the amount of textual   pretraining data ( to TBs of raw texts ) ( Min et al . ,   2021 ; Brown et al . , 2020 ; Chowdhery et al . , 2022 ;   Kaplan et al . , 2020 ) . However , zero - shot language   learners solely trained on texts inevitably suffer   from human reporting bias . For example , people   tend not to write common or apparent things ( Grice ,   1975 ) , and the frequency of a certain textual state-   ment does not always correspond to their relativeFigure 1 : Our system endows language models with   two complementary types of visual imagination capabil-   ities : recalling existing images ( through retrieval ) and   synthesizing nonexistent images ( via image - to - text gen-   eration ) . They effectively alleviate the reporting bias   issue and improves the zero - shot performance for solv-   ingplain language tasks . We experiment with three   types of tasks : ( a ) Word Sense Disambiguation , ( b ) Sci-   ence Question Answering , and ( c ) Topic Classification .   likelihood in the world ( Gordon and Van Durme ,   2013 ) . Therefore , looking into other modalities to   supplement the textual information is crucial .   In this paper , we focus on incorporating vision   knowledge to facilitate the solution of plain lan-1186guage understanding tasks . Cognitive science has   demonstrated that the human vision system is cru-   cial to supplement , interact , and influence the lan-   guage system ( Dessalegn and Landau , 2013 ) . For   example , there exists a fast mapping between vi-   sion and language in the human language learning   process ( Altmann and Kamide , 2004 ) . Inspired by   this , we propose a visual imagination framework ,   Z - LaVI , to endow any PLMs ( e.g. , GPT , BERT ,   BART , etc . ) with visual imagination capabilities .   Specifically , we apply two different types of “ vi-   sual imaginations ” to the input texts . Given input   text , the first approach recalls existing images ( e.g. ,   through search engines ) , and the second one synthe-   sizes nonexistent images via text - to - image genera-   tion models ( e.g. , DALL - E ( Ramesh et al . , 2021 ) ) .   These two strategies mimic different types of hu-   man mental behaviors , i.e. , recalling past memories   and creative mental image construction . Interest-   ingly , we find that these two mechanisms are highly   complementary to each other . Our proposed visual   imagination module tends to rely more on recalling   when input texts are short because their correspond-   ing objects or scenes generally exist and are easy   to find . However , when input texts are long and   complex , the module is more inclined to create new   images . We develop a unified framework ( Figure 1 )   that exploits both types of imaginations along with   the original textual inputs to compose zero - shot   solutions to a broad set of downstream language   tasks . Note that our work differs from existing   multi - modal tasks such as VQA ( Antol et al . , 2015 ;   Wu et al . , 2017 ) or Visual Dialog , ( Das et al . , 2017 ) ,   which have both textual and visual inputs . Instead ,   we use visual imagination as machinery to facilitate   the ( zero - shot ) solution of pure language tasks .   We show that on a diverse set of language un-   derstanding tasks , Z - LaVI consistently improves   the performance of existing language models of   different sizes and architectures . In particular , our   Z - LaVI with SBERT can achieve a zero - shot F1   score of 87.5 % on the WSD task without fine-   tuning , even outperforming BERT - large , which is   fine - tuned with three examples per sense , by 2.3 % .   Z - LaVI also beats all existing zero - shot models on   four Science QA tasks and two Topic Classification   tasks by a large margin . Our analysis demonstrates   that Z - LaVI can complement language models and   significantly alleviate PLMs zero - shot prediction   errors by adaptively executing two visual imagina-   tion mechanisms - R and S .   2 Method   2.1 Task Formulation   To provide a zero - shot solution for language tasks   and solve them in a uniform way , we transform   different tasks into multi - choice questions , where   input stream xand candidate answers stream y∈ Y   are provided . The goal is to select the correct an-   swer from Y. In particular , for word sense disam-   biguation tasks , xis the instance sentence , and Y   are all possible word senses of the target word ; for   science question answering tasks , xis the question ,   andYare answer options ; for text classification   tasks , xis the input sentence , and Yis the pool of   categories . To make a prediction , the model needs   to estimate the plausibility of each tuple ( x , y)for   ally∈ Y and select the best answer ˆy .   ˆy = argmaxP(y|x ) . ( 1 )   2.2 Language Models for Zero - shot Tasks   We consider three main approaches for employing   language models to make zero - shot predictions on1187language tasks :   Prompt - based Approach ( Petroni et al . , 2019 ;   Schick and Schütze , 2021 ) treats Natural Language   Understanding tasks as a cloze test using prompts .   For example , we can format question - answering   tasks into :   “ Question : [ x ] ? The answer is [ y ] . ”   We convert the input ( x , y)into a sequence of   tokens W= ( w , ... , w , ... w , ... , w)via a   prompt , in which y= ( w , ... w).We apply au-   toregressive language models such as GPT ( Brown   et al . , 2020 ) to calculate the score :   Score(x , y ) = 1   −/summationtextlogP(w|W ) ,   where P(·)denotes the probability given by the   language model . Note that we adopt the stan-   dard token - length normalization to handle different   lengths of answer choices . Finally , we apply soft-   max to Score(x , y)to obtain the probability of   each candidate :   p(y|x ) = e   /summationtexte . ( 2 )   For the prompt - based approach , we select GPT-   Neo-1.3B/2.7B ( Black et al . , 2021 ) , GPT - J-6B   ( Wang and Komatsuzaki , 2021 ) and OPT-30B   ( Zhang et al . , 2022c ) as our models . The GPT-   Neo and GPT - J are trained on the Pile dataset ( Gao   et al . , 2020 ) , which contains 825 GB of English text   data . Besides Pile , OPT concatenates the training   data of RoBERTa ( Liu et al . , 2019 ) and PushShift   Reddit ( Baumgartner et al . , 2020 ) .   Natural Language Inference ( NLI ) Approach   ( Yin et al . , 2019 ) propose a textual entailment   framework for zero - shot text classification . The   NLI approach considers the input pair ( x , y)as a   ( premise , hypothesis ) pair to predict the probability   that the premise logically entails the hypothesis .   p(y|x ) = p(E |(x , y)).(3 )   Note that this approach requires language mod-   els to be fine - tuned on ( premise , hypothesis ) pairs .   Here we select RoBERTa - large ( Liu et al . , 2019 )   and BART - large ( Lewis et al . , 2020 ) fine - tuned on   Multi - genre NLI ( MNLI ) corpus , ( Williams et al . ,   2018 ) consisting of 433k sentence pairs . Latent Embedding Approach utilizes an off - the-   shelf feature encoder fto project the input tuple   ( x , y ) into a shared latent space and determines   their relevance based on a distance metric - cosine   similarity scores :   Score(x , y ) = cos(f(x ) , f(y ) ) . ( 4 )   Relevance scores are normalized with softmax   ( equation 2 ) to get the final probabilities .   We choose two state - of - the - art sentence en-   coders , i.e. , Sentence - BERT ( SBERT ) ( Reimers   and Gurevych , 2019 ) and SimCSE , ( Gao et al . ,   2021 ) as our latent embedding models . For   SBERT , we pick the all - mpnet - base - v2   checkpoint , which achieves the best performance   on 14 sentence embedding datasets . For Sim-   CSE , we choose the best fully unsupervised model   unsup - simcse - roberta - large .   2.3 Language with Visual Imagination   Visual Imagination aims to convert either xory   ( depending on the task ) in the textual input tuple   ( x , y)into an image . For WSD and QA tasks , we   imagine the candidate options y. While for topic   classification tasks , we imagine the instance sen-   tence x. Here we illustrate our method through the   example of imagining y. We propose two imagina-   tion mechanisms : 1 ) R and 2 ) S .   1)R : We use the text input to query Bing   Image Searchto recall the corresponding images .   We set a maximum number of images for each   query . When only limited images are available for   some queries , we download all of them .   2)S : We adopt DALL · E ( Ramesh   et al . , 2021 ) , a text - to - image generation model   pretrained on image - caption pairs , to synthesize   images . DALL · E constructs a codebook Vusing   a discrete variational autoencoder ( dV AE ) ( Rolfe ,   2016 ) to map the image into tokens concatenated   with the caption ’s text tokens . DALL · E models   the joint distribution over the text and image to-   kens with an autoregressive transformer . During   inference , DALL · E feeds the text tokens yinto the   transformer and generates a sequence of image to-   kens(v , v , ... , v ) , where an image token vis1188predicted based on the previous ones :   v = argmaxp(v|y , v ) , ( 5 )   in which , Vis the visual codebook . After we gen-   erate enough image tokens , we decode the tokens   into images by looking up the vectors in the dV AE   codebook to construct the pixels .   We iterate the S process multiple   times and combine with the images from R   to collect a set of Kimages { I|i= 1 , ... , K } for   each textual input y.   Vision - Text model for Zero - shot language tasks .   After transferring an input stream into images , we   modify a plain language task into a multimodal   task . Thus we can apply vision - text models to   solve the problems . We choose CLIP ( Radford   et al . , 2021 ) as our vision - text model , which is   pre - trained on 400 M image - caption pairs with the   contrastive learning strategy .   CLIP has a text encoder fand a visual en-   coder f , which can project text and image into   the shared latent space . Similar to the latent em-   bedding approach described in 2.2 , we aggregate   theKimages collected previously and use CLIP   to compute the relevance score of ( x , y ):   Score(x , y ) = 1   K / summationdisplaycos(f(x ) , f(I)),(6 )   and we obtain a probability distribution through   softmax ( over y ):   p(y|x ) = softmax ( Score(x , y)).(7 )   Ensemble Language and Vision Prediction . Our   system is designed for zero - shot tasks without la-   beled data to learn weights to ensemble the two   models . Therefore , we adopt a weighted sum as   the late fusion over the final output distributions of   the language and multi - modal models :   p(y|x ) = ( 1 −w)·p(y|x)+w·p(y|x),(8 )   where we design a heuristic function to calibrate   the weight wbased on the relative size between the   vision - text model and the language model :   w = sigmoid / parenleftbiggP   P / parenrightbigg   , ( 9 )   where PandPare the number of parameters   of the models . We hypothesize that when the lan-   guage model ’s size increases , it will encode more   knowledge and thus rely less on the vision model .   The number of parameters of each model and their   corresponding weight is listed in Table 8 .   3 Experimental Setup   3.1 Datasets   We evaluate our methods on six datasets of three   tasks . Table 1 shows dataset statistics .   CoarseWSD-20 ( Loureiro et al . , 2021 ) is a coarse-   grained WSD dataset built from Wikipedia . The   dataset consists of 20 nouns with 2 - 5 senses per   noun ( 53 senses in total ) . Each sense is associated   with a definition which is the first sentence on its   Wikipedia page . CoarseWSD guarantees that every   sense has test instances in the test set . On average ,   each sense has 192 test instances .   QASC ( Khot et al . , 2020 ) is a multi - hop , 8 - way   choice question answering dataset collected by de-   composing sentences about scientific facts . We re-   port the performance of the development set , which   contains 926 questions .   SciQA ( Welbl et al . , 2017 ) is a dataset of 4 - way   multiple - choice science exam questions spanning   from elementary to college - level covering chem-   istry , biology , physics , etc . We evaluate the devel-   opment set with 1,000 questions .   ARC ( Clark et al . , 2018 ) consists of 7,787 natural ,   grade - school level science questions . The ARC   dataset is split into easy ( ARC - E ) and challenge   ( ARC - C ) , where questions in the challenge set con-   tain the ones that simple retrieval or word correla-   tion methods can not answer correctly . We evaluate   the development sets of ARC - E and ARC - C , which   contain 570 and 299 questions , respectively .   AG News ( Zhang et al . , 2015 ) is a news topic clas-   sification dataset , and each sentence is associated1189with one of the four news types : word , sports , busi-   ness , and technology . We run our models on the   7,600 examples in the test set .   Situation ( Mayhew et al . , 2018 ) is a event - type   classification task . The dataset has 12 events : need   water , need infrastructure , crime violence , etc . The   original task on this dataset is multi - label classi-   fication and has an out - of - domain class . As the   multi - label prediction requires a fine - tuned thresh-   old to determine the predictions and is thus not   suitable for zero - shot models , we remove those ex-   amples with more than one label and ones with the   out - of - domain label , resulting in 1,789 instances .   3.2 Baselines   Aside from the zero - shot language models de-   scribed in the section 2.2 , we also evaluate on a   random baseline and compare with previous work .   For CoarseWSD-20 , we compare with the BERT-   large few - shot ( 1 - shot/3 - shot per sense ) results re-   ported in Loureiro et al . ( 2021 ) .   For QA tasks , we include the Information-   Retrieval ( IR ) solver ( Clark et al . , 2016 ) , which   combines the question and option as a query and   sends it to a search engine to check if they are ex-   plicitly written in some corpus . We also choose   SMLM ( Banerjee and Baral , 2020 ) as another base-   line - a RoBERTa - large model fine - tuned on triplets   extracted from knowledge graphs such as ATOMIC   ( Sap et al . , 2019 ) .   We compare topic classification with the TE-   wiki ( Ding et al . , 2022 ) , the state - of - the - art model   on zero - shot topic classification trained on a dataset   collected from Wikipedia .   3.3 Evaluation Metrics   We report the accuracy of all question - answering   and topic - classification datasets . For CoarseWSD-   20 , we compute each word ’s accuracy and F1 score   and take the mean score of all 20 words .   3.4 Implementation Details   Image Collection We adopt Bing Image Search to   R images . And for image S , we   utilize the newly released DALL · E - miniwhich   chooses VQGAN ( Esser et al . , 2021 ) as the image   encoder / decoder and BART ( Lewis et al . , 2020 ) as   the autoregressive transformer . For every textual   input , we obtain 100 images from each of the twomethods . The 200 images are sorted using CLIP   based on their similarity with the text input . We   preserve each text input ’s top-10 images ( K= 10 )   and feed them into the equation 6 to calculate the   vision - text probabilities .   Model Implementation The GPT - style and NLI-   based language models are built on top of the hug-   gingface API.For NLI models , we use the recently   released zero - shot classification pipeline . We use   the official release of SBERTand SimCSEto   implement the latent embedding approach . The   CLIP model is adapted from the OpenAI ’s public   repo , and we select the ViT / B32 as the image en-   coder . The experiments were run on 3 ×8 NVIDIA   V100 32 GB , which can generate 24 images in 5   seconds . The majority of the running time of our   model is image generation . In total , we employ   DALL·E - mini to generate approximately 1.8 M im-   ages which take around 104 hours .   4 Evaluation   4.1 Main Results   Z - LaVI boosts the performance of language   models . Table 2 , 3 and 4 show results on seven   datasets of three tasks . Each dataset has two results   columns : the original performance of the language   models and the ensembled performance by adding   our Z - LaVI model . We observe that in most cases ,   Z - LaVI consistently improves the performance of   different language models . Especially in the WSD   task , our Z - LaVI with SBERT can outperform the   BERT - large fine - tuned with 3 - shots of each sense .   Z - LaVI also significantly enhances the language   models on topic classification task where the best   language model with Z - LaVI beats the SOTA zero-   shot topic classification model TE - wiki by 2.8 % .   For science QA tasks , we can see Z - LaVI improves   on QASC , SciQ , and ARC - E , but it struggles on the   ARC - C , and adding Z - LaVI degrades the perfor-   mance of a few language models . This is because   the ARC - C questions are designed to be hard to an-   swer using retrieval or correlation , and Z - LaVI uses   CLIP , which is pre - trained on the image - text cor-   relation only . Figure 7 ( b ) shows an example that   needs multi - hop reasoning where Z - LaVI fails to   answer correctly.1190   Z - LaVI without language model is a strong base-   line . Surprisingly , we also find that Z - LaVI w/o   language model performs well on plain language   tasks . In some datasets , such as QASC , Coarse-   WSD , and topic classification tasks , Z - LaVI w/o   LM outperforms the language models without   fine - tuning on the downstream datasets ( e.g. , Sim-   CSE , GPT - Neo-1.3B/2.7B ) . This indicates that the   vision - text model pretraining on image - caption   pairs learns the knowledge that can be leveraged to   solve single modality tasks .   Ensembling two language models is not as good   as Z - LaVI . To verify the effectiveness of using   visual knowledge , we replace the visual imagi-   nation of Z - LaVI with another language model -   SimCSE . We select SimCSE here because Sim-   CSE is trained fully unsupervised and has the   same contrastive learning objective as CLIP . We   define the performance gain ( PG ) of model M   ( i.e. , SimCSE ) on top of model Mby com-   puting the relative improvement of the ensemble   model Ens(M , M)performance over the origi-   nal model Orig ( M ) .   PG(M , M ) = Ens(M , M)−Orig ( M )   Orig ( M )   ( 10 )   We include all the language models ( exclude Sim-   CSE ) in the set Mand calculate the average per-   formance gain on a dataset by :   avg - PG ( M ) = 1   |M|/summationdisplayPG(M , M)(11 )   For fair comparison , we fix the ensemble weight   w= 0.5 in equation ( 8)for both SimCSE and Z-   LaVI . We also include the Z - LaVI with dynamic   ensemble weight controlled by equation ( 8) . The   performance gain of SimCSE and Z - LaVI on all   six datasets is shown in Figure 3 . We observe1191   that Z - LaVI consistently has higher performance   gain than SimCSE across all datasets , demonstrat-   ing that the visual information provided by Z-   LaVI complements language models more hence   boosts more on performance . Additionally , Z-   LaVI with dynamic weights perform better than   simply setting the weight to 0.5 .   4.2 Analysis   Vision and Language models behave differently .   We define the overlap of correctly predicted exam-   ples between two models as :   overlap ( M , M ) = |S∩S|   |S|(12 )   where Sis the set of correctly predicted exam-   ples of model M. Figure 4 shows the overlap of   models ’ predictions in the Situation dataset . We   observe that Z - LaVI ( w/o LM ) has an obviously   smaller overlap with the other models , while dif-   ferent language models have a big mutual over-   lap . This difference explains the substantial perfor-   mance gain after exploiting visual imagination .   R vs. S .We ablate on the imag-   ination methods and compare the performance of   only using one of the methods . Table 5 demon-   strates the performance on each dataset with differ-   ent imagination methods . We can see that for the   dataset with short inputs for imagination ( e.g. , QA   tasks ) , R is better than S . This   is because short inputs of science QA datasets nor-   mally correspond to objects that exist in the real   world and are easy to find on the web , such as   mollusca andporifera shown in Figure 7 ( a ) . How-   ever , for queries with long sentences ( WSD and   Topic Classification ) , the text inputs are too spe-   cific to match any real photo . Hence S is   preferable . Figure 5 also indicates that the model   prefers to choose R images for short input   and tends to use S images when the in-   put contains more tokens . We also find that without   images , Z - LaVI has poor performance on all tasks ,   reflecting the necessity of imagination .   Performance vs. Image Quantities . We com-   bine R and S to imagine 200   image candidates . We wonder whether the num-1192   ber of imaginations impacts the Z - LaVI ’s perfor-   mance . Figure 6 reports Z - LaVI ’s performance on   CoarseWSD-20 versus the number of images . We   observe that Z - LaVI ’s F1 score increases with a   higher number of images . While the improvement   is marginal when the number is higher than 125 .   Z - LaVI supplements visual commonsense   knowledge . To further validate Z - LaVI helps to   mitigate reporting bias problems of language mod-   els , we conduct experiments on ViComTe ( Zhang   et al . , 2022a ) , which is a commonsense knowledge   dataset containing different types of properties for   over 5000 subjects , e.g. , the subject “ egg ” has the   property ( object ) “ oval ” . We investigate three rela-   tion types ( C , S , and M ) and   report the results on the test set ( see Table 7 for de-   tails ) . We select the BERT - large , and Oscar - large   ( Li et al . , 2020 ) as the baselines of which the results   are directly obtained from Zhang et al . ( 2022a ) .   For a fair comparison , we adopt the same set of   seven prompt templates provided by Zhang et al .   ( 2022a ) and report the average performance over   these prompts . Table 6 demonstrates the perfor-   mance of Z - LaVI with language models . We can   see Z - LaVI continue to consistently boost the per-   formance of language models and outperform the   baselines with significant margins . The results on   ViComTe indicate Z - LaVI is a promising way to   overcome the reporting bias of language models on   visual commonsense .   Qualitative Examples . Figure 8 shows some   qualitative examples from the two topic classifi-   cation datasets . We observe Z - LaVI can effec-   tively correct language models ’ prediction with   more straightforward visual signals . However , we   also notice that Z - LaVI fails on examples that can-   not be solved by correlation , e.g. , Z - LaVI wrongly   relates flooding with the situation of need water .   More examples are provided in the Appendix C.   5 Related Work   Visually Grounded Representation Learning   Several studies have focused on learning visually   grounded word or sentence representations . To   learn better word embeddings , Kiros et al . ( 2018 )   introduce Picturebook that encodes images as vec-   tors by querying all vocabulary words through   Google image search . Lazaridou et al . ( 2015 ) opti-   mize a multimodal skip - gram model , where visual1193   information is presented together with the corpus   contexts to produce word embeddings . Zablocki   et al . ( 2018 ) leverage the visual context of objects   to learn multimodal word embeddings . With re-   spect to visually grounded sentence embeddings ,   previous work develops several strategies to enrich   sentence representations with visual information ,   such as using the given sentence as captions to get   image features ( Kiela et al . , 2018 ) , capturing both   cluster information and perceptual information in   grounded space ( Bordes et al . , 2019 ) , or exploiting   multimodal contrastive learning objective ( Zhang   et al . , 2022b ) . Zhang et al . ( 2021 ) retrieves images   from COCO ( Lin et al . , 2014 ) as augmented visual   features for language models . Lu et al . ( 2022b ) aug-   ment sentence embeddings with VQGAN-(Esser   et al . , 2021 ) generated images and fine - tune them   on GLUE Benchmark ( Wang et al . , 2018 ) . Liu et al .   ( 2022 ) probes the spatial commonsense knowledge   ( sizes , positions ) of language models and vision-   language models through image generation .   Vision - Language Pretraining Models To connect   vision and language semantics , a line of work on   multimodal masked language models ( Li et al . ,   2019 ; Tan and Bansal , 2019 ; Lu et al . , 2019 ; Su   et al . , 2020 ) explores vision - language pretraining   and achieves SOTA fine - tuning performance on   multimodal benchmarks . Tsimpoukelli et al . ( 2021 )   freeze a language model parameters to generate   the appropriate caption by encoding each image   into the embedding space to inject visual knowl-   edge into PLMs . To retrain knowledge in both   vision and language pretrained models , Flamingo   ( Alayrac et al . , 2022 ) freezes both pretrained mod-   els and brings in additional model components to   do visually - conditioned autoregressive text gener-   ation . Tan and Bansal ( 2020 ) retrieve related im-   ages as vokens ( visualized tokens ) and then pro-   cess large language corpora ( e.g. , Wikipedia ) into   voken - prediction tasks . FLA V A ( Singh et al . , 2022 )   is an alignment model that pretrains on both uni-   modal and multimodal data while optimizing cross-   modal “ alignment ” objectives and multimodal fu-   sion objectives . Unified - IO ( Lu et al . , 2022a ) is a   general - purpose model which can perform a wide   range of vision , language , and multimodel tasks by   unifying the inputs and outputs as sequences .   6 Conclusion   In this paper , we propose a novel approach , Z - LaVI ,   to alleviate the reporting bias problem of pretrained   language models and enhance their zero - shot in-   ference ability . We develop two complementary   visual imagination mechanisms , i.e. , R that   aims to retrieve existing objects or scenes and S- that generates nonexistent ones . Experi-   ments on a wide range of language tasks show that   our approach can significantly outperform exist-   ing zero - shot language models , pointing towards   a promising direction to solve an unseen language   task with visual imagination.11947 Limitations   Our experiments apply DALL · E - mini for synthe-   sizing the images , but the quality and resolution of   the generated images are still low , which can be the   factor limiting Z - LaVI ’s performance . However ,   the recent breakthroughs of DALLE · E-2 ( Ramesh   et al . , 2022 ) , Imagen , ( Saharia et al . , 2022 ) and   the open - sourced Stable Diffusion ( Rombach et al . ,   2022 ) give us hope to obtain more realistic images   and thus further unleash the potential of Z - LaVI .   The negative results on ARC - C reveal the lack of   complex reasoning ability in the current zero - shot   vision - text model . At the same time , the success   of Flamingo ( Alayrac et al . , 2022 ) on few - shot   multi - modal tasks lets us sense the possibility of   applying the framework of Z - LaVI with these pow-   erful visual language models to solve broader lan-   guage tasks . We can foresee the bright future of our   method once these powerful resources are publicly   available . In this paper , we focus on the zero - shot   settings , and thus it is difficult to design a more   effective approach to ensemble the language and   vision without training data . However , when few-   shot examples are available , it is possible to learn a   mechanism to automatically calibrate the weights   of imagination depending on the input examples .   In addition , the image generation model is   trained on unfiltered data on the web , which may   leak personal information such as the human face ,   etc . The generation model may also be biased   toward stereotypes against minority groups . Fur-   thermore , compared to language models , our ap-   proach requires extra resources such as an image   search engine , pretrained text - to - image generation   model , etc . , which will increase the implementation   cost . Finally , we evaluated our method in English   datasets only , and we plan to incorporate other lan-   guages in the future with the help of multilingual   multi - modal models ( Huang et al . , 2021 ) .   References1195119611971198A Implementation Details   A.1 Prompt Selection   We use the intuitive prompt templates of the three   main tasks for each model shown in Table 9 , and   we do not tune the prompt for each dataset . For   ViComTe dataset , we reuse the original 7 prompts   provided by Zhang et al . ( 2022a ) .   A.2 Model Parameters   We include the number of parameters of all mod-   els we use in Table 8 . We also list the ensemble   weight wbased on the relative sizes between the   two models :   w = sigmoid / parenleftbiggP   P / parenrightbigg   = 1   1 + e(13 )   A.3 Other Details   Predicted scores of prompt - based approach . The   huggingface API can calculate the loss of the pro-   vided input , and we take the reciprocal of the loss   as the prediction score in practice .   Speed of Bing Image Search . The efficiency of   downloading images from Bing depends on your   internet speed and the API plan you select . In our   case , downloading 200 images of a query takes less   than 10 seconds . The script of Bing Image Search   is provided in our GitHub repo .   Requirements for computing resources . Our ap-   proach is designed for zero - shot scenarios and does   not involve training , thus most of the experiments   can be conducted on a single GPU with more than   20 GB of memory . The biggest challenge is to de-   ploy the OPT-30B , which requires 70 GB of mem-   ory . We successfully deploy OPT-30B using 4 P40   GPUs and released the code to implement OPT-   30B across multiple GPUs in our GitHub repo . B Additional Analysis   B.1 Relative Improvement of Each Word in   Word Sense Disambiguation   To quantify the ability of Z - LaVI to disambiguate   different words , we compute the relative improve-   ment ( RI ) of each word by comparing the F1 score   of SBERT with that of Z - LaVI :   RI = F1−F1   F1(14 )   As shown in Figure 10 , Z - LaVI improves the   F1 score of 16 words ( out of 20 ) , while for the   rest of four words ( pound , chair , digit , club ) , Z-   LaVI hurts the performance . Furthermore , we no-   tice those four words all contain abstract senses   ( see Figure 13 ) which are difficult to imagine , e.g. ,   pound has two senses- currency andmass , which   are both units and difficult to illustrate in images .   Based on this finding , we think future work can   design a dynamic ensemble method to calibrate the   weights conditioned on the input .   B.2 Overlap of Incorrectly Predicted   Examples   In Figure 4 , we show that Z - LaVI w/o LM has   a smaller overlap with other language models on   correctly predicted examples . To tell a complete   story of the different output distribution between Z-   LaVI and language models , we compute the over-   lap of incorrectly predicted examples shown in   Figure 9 . We can see that Z - LaVI has a smaller   overlap on incorrectly predicted ones.1199   B.3 R vs. S on ViComTe   We also ablate on the imagination methods on Vi-   ComTe shown in Table 11 . As mentioned before ,   R is preferred when the text input is short .   This finding still holds for ViComTe where the   text inputs are single words . We notice R   performs better than S except for the   C relation . We find the images from S- are more prototypical in colors than the   ones from R . ( See examples in Figure 15)C Qualitative Examples   Figure 11 demonstrates the qualitative examples   from the two topic classification dataset . Figure   12 shows the QA examples . Figure 13 , 14 show   theS image of each word sense from   CoarseWSD20 . Figure 15 demonstrates the im-   ages ( S andR ) of different sub-   jects from ViComTe . In addition , we release all the   S images generated by DALL · E - mini   which can be downloaded from our GitHub repo.1200120112021203