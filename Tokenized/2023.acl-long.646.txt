  Deokjae LeeJunYeong LeeJung - Woo Ha   Jin - Hwa KimSang - Woo LeeHwaran LeeHyun Oh Song   Abstract   The deployment of large - scale generative mod-   els is often restricted by their potential risk of   causing harm to users in unpredictable ways .   We focus on the problem of black - box red   teaming , where a red team generates test cases   and interacts with the victim model to dis-   cover a diverse set of failures with limited   query access . Existing red teaming methods   construct test cases based on human supervi-   sion or language model ( LM ) and query all   test cases in a brute - force manner without in-   corporating any information from past evalua-   tions , resulting in a prohibitively large number   of queries . To this end , we propose Bayesian   red teaming ( BRT ) , novel query - efficient black-   box red teaming methods based on Bayesian   optimization , which iteratively identify diverse   positive test cases leading to model failures   by utilizing the pre - defined user input pool   and the past evaluations . Experimental results   on various user input pools demonstrate that   our method consistently finds a significantly   larger number of diverse positive test cases   under the limited query budget than the base-   line methods . The source code is available   at https://github.com/snu-mllab/Bayesian-Red-   Teaming .   1 Introduction   Recently , generative models have demonstrated ex-   ceptional performance on a broad range of gen-   eration tasks , including open - domain dialogue ,   prompt continuation , and text - to - image generation ,   thanks to the rise of large - scale models such as   BlenderBot , Gopher , GPT-3 , PaLM , and Dall · E 2   ( Roller et al . , 2021 ; Rae et al . , 2021 ; Brown et al . ,   2020 ; Chowdhery et al . , 2022 ; Ramesh et al . , 2022 ) .   While utilizing large models in commercial sys-   tems can provide significant benefits , it also poses   a risk of unexpectedly causing harm to users , suchFigure 1 : Illustration of edit - based BRT . Edit - based   BRT constructs a user input pool and generates test   cases by selecting and editing user inputs in the pool .   Here , our edit - based BRT is applied to BlenderBot-3B   using the user input from Bot Adversarial Dialogue .   as the generation of offensive responses or NSFW   images ( Lee , 2016 ; Rando et al . , 2022 ) . Thus , it   is essential to identify and prevent these failures   before deployment to avoid severe ramifications to   society ( Xu et al . , 2021 ; Dinan et al . , 2019 ) .   The primary goal of red teaming is to identify   many diverse positive test cases which lead to   model failures ( Perez et al . , 2022 ) . Due to the   high computation cost of large models during in-   ference and the potential security risk of exposing   the model parameters , we consider the black - box   scenario in which the red team can only observe   the output of the victim model within a limited   query budget ( Rombach et al . , 2022 ; Dettmers et al . ,   2022 ; Tramèr et al . , 2016 ) . Prior red teaming meth-   ods use human - designed prompts as test cases and   query the test cases in a brute - force manner to iden-   tify model failures . These approaches usually re-   quire a prohibitively large number of queries to the   victim model as they do not utilize any information   from past evaluations during the red teaming pro-   cess ( Ribeiro et al . , 2020 ; Röttger et al . , 2021 ; Bar-11551tolo et al . , 2021 ; Garg et al . , 2019 ) . A recent work   proposes language model ( LM)-based red team-   ing methods , which construct a user input pool by   zero - shot generation method and utilize the user   input pool to generate test cases that are more likely   to be positive . However , LM - based red teaming   methods require access to victim model outputs of   entire user input pool , which is prohibitive in the   black - box setting ( Perez et al . , 2022 ) .   To this end , we propose Bayesian red teaming   ( BRT ) , query - efficient black - box red teaming meth-   ods based on Bayesian optimization ( BO ) . First ,   BRT constructs a user input pool that consists of   user inputs generated by human supervision or LM ,   e.g. , utterances from the dialogue dataset or zero-   shot generated utterances ( Figure 1 ) . Then , BRT   sequentially generates test cases that lead to diverse   positive test cases by choosing or editing user in-   puts from a pre - defined user input pool . For each   step , BRT utilizes past evaluations , to fit a Gaus-   sian Process ( GP ) model . Based on the GP model ,   BRT generates the next test case most likely to be   positive while encouraging diversity .   Our results demonstrate that BRT discovers a sig-   nificantly larger number of positive test cases with   higher diversity than the baseline methods under a   limited query budget on various generations tasks ,   open domain dialogue , prompt continuation , and   text - to - image generation . Notably , edit - based BRT   discovers more than 3x larger number of positive   test cases with higher diversity than the baseline   methods on the Bloom ZS user input pool against   BlenderBot-3B under a query limit of 20,000 .   2 Preliminaries   2.1 Problem Formulation   The main purpose of red teaming in this study is   to discover as many diverse positive test cases as   possible and detect diverse failure modes with max-   imized test coverage under a limited query access   ( Perez et al . , 2022 ) . We consider a victim model   G : U → O which generates an output o∈ O   for a given user input u∈ U. For a given victim   model G , a red teaming method constructs test   casesT ⊂ U and interacts with Gto identify pos-   itive test cases that lead to offensive outputs . To   measure the offensiveness of generated outputs , we   use a red team classifier R : U × O → [ −1,1 ]   which calculates the red team score R(u , o)rep-   resenting the offensiveness of the output ogiven a   user input u. Using the red team classifier R , wedefine offensive outputs and positive test cases .   Definition 1 . ( Offensive outputs and positive test   cases ) Let Gbe a victim model , and Rbe a red   team classifier . We define an output o = G(u)∈   Ois offensive if R(u , o)>0and safe otherwise .   We define a test case t∈ T ⊂ U is positive if its   corresponding output G(t)∈ O is offensive .   For open - domain dialogue model , such as Blender-   Bot , whose inputs and outputs are both texts , we   can use the Bot Adversarial Dialogue ( BAD ) clas-   sifier , which scores the offensiveness of a text , as   the red team classifier by R(u , o):= BAD ( u∥o )   where u∥odenotes the concatenation of two texts   uando(Roller et al . , 2021 ; Xu et al . , 2021 ) . Here ,   red team classifiers , such as the BAD classifier   or Perspective API , also can be used as the input   offensiveness classifier r : U → [ −1,1]which   scores the offensiveness r(u)of a user input u ,   e.g. ,r(u):= BAD ( u)(Gehman et al . , 2020 ) .   Similar to the offensiveness of outputs , we define   a user input u∈ U as offensive if r(u)>0and   safe otherwise . Table 1 shows examples of victim   models and their corresponding red team classifiers   for various tasks considered in this work .   We assume that the victim model and the red   team classifier are black - box . This means that the   red team has access to only the output of the victim   model and its red team score and has no knowledge   of the architecture or parameters of these models .   The objective of black - box red teaming is to gener-   ate diverse positive test cases as many as possible   within a limited query budget N. By Definition 1 ,   the set of positive test cases T⊂ T is formally   written as T={t∈ T | R(t , G(t))>0 } .   Hence , the problem can be formulated as   where Self - BLEUscore is a modified Self-   BLEU metric that measures the diversity of a text   set , which we describe in Section 2.2 , Nis the   query budget , and Dis the diversity budget for   Self - BLEUscore . Note that a lower value of   Self - BLEU(T)indicates that the positive test   cases are more diverse .   2.2 Evaluation Metric for Diversity   To compare the diversity of generated text sets con-   taining the same number of texts , Holtzman et al.11552   ( 2020 ) suggest Self - BLEU of a text set Vwhich   averages the BLEU score of each text t∈Vusing   all other texts in V\ { t}as references . A lower   Self - BLEU score indicates a more diverse text set .   This score is formulated as ( V ) = E [ ( t , V\ { t } ) ] ,   where Unif(V)is the uniform distribution on V ,   andBLEU ( t , V\ { t})is the BLEU score with text   tand a reference set V\{t}(Papineni et al . , 2002 ) .   However , red teaming methods may discover   a varying size of positive test cases . A common   workaround to compare the diversity of text sets of   different sizes is to evaluate the Self - BLEU score   ofk - subset sampled from each text set ( Perez et al . ,   2022 ) . This technique is equivalent to computing a   single - sample estimator for the average Self - BLEU   ofk - subsets of a text set , denoted by Self - BLEU ,   which can be written as(V):=E ( ( ) ) [ ( W ) ] .   We estimate the average Self - BLEU score of 100   sampled k - subsets of the positive test case set to   obtain an estimator with higher precision .   2.3 Bayesian Optimization   Bayesian optimization ( BO ) is a widely used op-   timization method for maximizing an expensive   black - box function f : A→Rby utilizing a surro-   gate statistical model that approximates f(Mockus   and Mockus , 1991 ; Frazier , 2018 ) . BO first eval-   uates random points for exploration , then repeats   the following steps :   1.Fit the parameters of a surrogate model given   evaluation history D={ˆx,ˆy = f(ˆx ) } .   2.Compute the acquisition function based on the   posterior given the evaluation history D.3.Evaluate the maximizer ˆx∈Aof the   acquisition function and append the pair   ( ˆx,ˆy = f(ˆx))to the evaluation   history .   Here , an acquisition function is a proxy score that   estimates the utility of evaluating a given point   for the purpose of maximizing f. After a certain   number of evaluations , BO returns the point with   the largest fas the solution .   Gaussian process ( GP ) and expected improve-   ment ( EI ) are commonly used as the surrogate   model and acquisition function for BO ( Osborne   et al . , 2009 ) . GP assumes that the prior of fon any   finite set X⊆Afollows a Gaussian distribution ,   i.e. ,f(X)∼ N(µ(X;η),Σ(X , X ; ψ))for a mean   function µ:A→Rand a covariance function   Σ : A×A→Rthat are parameterized by ηand   ψ , respectively . Given an evaluation history D , the   posterior of falso follows the Gaussian distribu-   tion with the posterior mean and variance as   E[f(X)|X , D ]   = Σ(X,ˆX)Σ(ˆX,ˆX)(ˆY−µ(ˆX ) ) + µ(X ) ,   Var[f(X)|X , D ]   = Σ(X , X ) −Σ(X,ˆX)Σ(ˆX,ˆX)Σ(ˆX , X ) ,   where ˆXandˆYdenote the concatenated vectors of   { ˆx}and{ˆy } , respectively ( Mackay , 1998 ) .   Based on the posterior mean and variance , we com-   pute the expected improvement , which is defined   asEI(x| D):=E[max ( f(x)−f,0)|x , D ] ,   where the reference term fis typically the largest   value of fevaluated so far ( Shahriari et al . , 2016 ) .   3 Methods : Bayesian Red Teaming   In this section , we describe BRT methods . We re-   formulate Equation ( 1 ) into the following sequence11553of relaxed optimization problems to construct the   test case set T={t , · · · , t}in a sequential   manner :   where λ > 0is diversity trade - off coefficient and   T={t , . . . , t}is the current test case set when   1≤n < N. In each step , we select the most prob-   able test case that maximizes Equation ( 2 ) based on   our GP surrogate model described in Section 3.1 .   We simplify the notation and denote the objec-   tive function of Equation ( 2 ) by L(u;T ) . Note   that Equation ( 2 ) is an unconstrained maximiza-   tion problem with the grey - box objective L(u;T )   which can be decomposed into a black - box term   f(u):=R(u , G(u))and a white - box term   g(u;T):=Self - BLEU({u } ∪ T ) . Here , the   value of the white - box term g(u;T)can change   each step as it depends on T. To capture this   change in the white - box term g(u;T ) , we model   the black - box term f(u)using a GP surrogate   model and estimate the posterior mean and variance   ofLby incorporating the actual value of white-   box function g(u;T)in each step . The posterior   mean and variance of Lfor a given evaluation   history D={(t , f(t))}can be obtained from   the posterior mean and variance of fcomputed   by its GP surrogate model and the actual value of   g(u;T)as follows :   E[L(u)|u , D ] = E[f(u)|u , D]−λg(u;T ) ,   Var[L(u)|u , D ] = Var [ f(u)|u , D ] . ( 3 )   Please refer to Appendix B for the derivation . Us-   ing the posterior mean and variance of Labove ,   we can compute the expected improvement score   EIofLfor a user input uas   EI(u| D ) = E[max(L(u)− L,0)|u , D ] ,   where we define the reference term Las   L:= max[min ( f(t),0)−λg(t;T ) ] .   However , the set of all possible user inputs Uis   prohibitively large to be considered as the search   space to maximize the EI score . To address this ,   we first construct a user input pool ˆUthat consists   of utterances from dialogue datasets or utteranceszero - shot generated from LM ( Dinan et al . , 2020 ;   Perez et al . , 2022 ) . Constructing such user input   pool sets up a feasible search space for BO and   provides enough utterances to guide the GP sur-   rogate model ( |U| ≫ | ˆU| ≫ N ) . We propose   BRT ( s ) andBRT ( e ) , a standard version and an   edit - based version of BRT , respectively . BRT ( s ) di-   rectly searches positive test cases in the user input   pool using a GP surrogate model that models the   black - box term f. BRT ( e ) extends the search space   to the ϵ-ball of ˆU , denoted by B(ˆU ) . We define   B(ˆU)as the set of all possible user inputs gener-   ated using at most ϵedit operations starting from   user inputs in ˆU. Specifically , BRT ( e ) uses word   replacement as the edit operation . Since BRT ( e )   has a substantially larger search space , it includes   editor GP for efficient exploration .   For the rest of the section , we first introduce our   GP surrogate model approximating the black - box   term f. Next , we present several techniques to   improve the scalability of BO . Finally , we outline   the overall algorithm of BRT methods .   3.1 GP Surrogate Model   To handle the discrete nature of texts , we extract   continuous features c(u)∈Rand use Single-   TaskGP of the BoTorch libraryon the continuous   feature space to model the black - box term f(u ) .   SingleTaskGP is a basic GP model suitable for ap-   proximating a single scalar function on the contin-   uous space ( Balandat et al . , 2020 ) . It employs the   Matern kernel with automatic relevance determi-   nation ( ARD ) as the covariance function ( Genton ,   2002 ) . The resulting covariance function between   two user inputs u , uis written by   Σ(u , u ) = σexp / parenleftigg / summationdisplay|c(u)−c(u)|   β / parenrightigg   ,   where σis a signal variance , νis a smoothness   parameter , and βis a length - scale parameter of the   i - th feature component . We maximize the posterior   probability of the evaluation history Dby fitting   the parameters . Please refer to Appendix C.2 for   more details .   3.2 Techniques for Scalable BO   Since inverting the covariance matrix has a com-   putational complexity of O(|D| ) , the process of   generic BOs can slow down significantly as the11554size of the evaluation history |D|increases ( Am-   bikasaran et al . , 2015 ) . To this end , we utilize the   Subset of Data ( SoD ) method , which samples a   subset Dof size Nby Farthest Point Cluster-   ing ( FPC ) and fits the GP model using the subset   D , following the practice of Lee et al . ( 2022 ) .   Additionally , instead of evaluating a single test case   in each step , we evaluate a batch of Ntest cases   for each step for further speedup . Specifically , we   construct the evaluation batch with a Determinantal   Point Process ( DPP ) to promote the diversity of the   batch during the batch selection ( Kulesza , 2012 ;   Kathuria et al . , 2016 ) . We include more details in   Appendix C.3 .   3.3 The Process of BRT Methods   3.3.1 Standard BRT : BRT ( s )   To efficiently identify offensive test cases from a   given user input pool , we use past evaluations to fit   aselector GP surrogate model for the black - box red   team score function f. Selector GP uses sentence   embedding as its continuous feature computed by a   pre - trained transformer , i.e. ,c(u):= emb ( u)∈R   ( Liu et al . , 2019 ; Reimers and Gurevych , 2019 ) .   The search step of BRT ( s ) begins by fitting selector   GP using Ntest cases randomly sampled from   the user input pool ˆU , where Nis the exploration   budget . It then repeatedly constructs a batch that   maximizes acquisition score EIbased on selector   GP fitted on a cumulative set of past evaluations .   To adhere to the diversity constraint , we adjust   the value of λadaptively based on the diversity of   the current positive test cases at each step . Algo-   rithm 1 of Appendix A.1 describes the procedure   ofBRT ( s ) .   3.3.2 Edit - Based BRT : BRT ( e )   BRT ( e ) aims to maximize EIin a larger search   spaceB(ˆU ) . However , it is impractical to compute   all acquisition scores in a brute - force manner . To   render the acquisition maximization process scal-   able , BRT ( e ) employs two GP surrogate models ,   namely selector GP andeditor GP , each serving a   slightly different function :   •Selector GP approximates the maximum   value of the function fover the set of   edited user inputs B({u } ) , denoted as   maxf(u),foru∈ˆU ,   •Editor GP directly approximates the function   value f(u)foru∈ B(ˆU ) .   By employing the selector GP and editor GP surro-   gate models , we divide the acquisition maximiza-   tion process into two stages . First , selector GP is   used to select the user input t∈ˆUthat is most   likely to contain the maximizer of the function fin   itsϵ-ball . Subsequently , the editor GP is utilized to   identify the edited user input t∈ B({t})that   maximizes the acquisition score in the ϵ-ball of the   selected user input t.   Unlike generic BOs , BRT ( e ) constructs the eval-   uation history Din a different way , using triplets of   the form ( t , t , f(t ) ) , where t∈ˆUis the user   input before edit , and t∈ B({t})is the test   case generated by editing t. For each iteration , we   fit selector GP using the data { ( t , f(t))}and   editor GP using { ( t , f(t ) ) } . Note that we   initialize the evaluation history DwithNtriplets   of the form ( t , t , f ( t))where t∈ˆUis a user input   randomly sampled from the user input pool .   For each word of a user input t∈ˆU , the can-   didate set for the word replacement is determined   using a pre - trained masked language model , adapt-   ing the protocol of Garg and Ramakrishnan ( 2020 ) .   Please refer to Algorithm 2 in Appendix A.2 for   the detailed procedure of BRT ( e ) .   3.3.3 Augmenting Feature with r   In practice , the cost of evaluating an input offen-   siveness classifier ris usually negligible com-   pared to querying a complex victim model G.   Table 2 demonstrates that a correlation exists be-   tween the input offensiveness scores and red team   scores for certain user input pools , suggesting that   the input offensiveness scores contain useful in-   formation for estimating the red team scores . We   thereby augment the continuous feature of selector   GP using an input offensiveness classifier as fol-   lows . Given a user input u∈ˆU , we concatenate   the sentence embedding and offensiveness score   of a user input to construct the continuous feature11555c(u):= emb ( u)⊕r(u)∈R , where a⊕b   denotes the concatenation of two vectors aandb .   BRT methods that use the augmented features are   denoted by BRT ( s+r ) andBRT ( e+r ) .   4 Experiments   We evaluate the red teaming performance of our   BRT methods on open - domain dialogue , prompt   continuation , and text - to - image generation tasks .   We first outline the user input pools , victim models ,   and baselines . Then , we report the performance of   BRT and the baseline methods .   4.1 Settings   4.1.1 Victim Models and User Input Pools   To show the versatility and effectiveness of BRT ,   we perform experiments on multiple user input   pools in various generation tasks . Table 1 outlines   the victim models and user input pools .   For the open - domain dialogue task , we red team   the chatbot models including BlenderBot ( BB)-   3B , GODEL - large , DialoGPT - large , and GPT-3.5   based chatbots ( Marv and Friend chat ) with the   Bot Adversarial Dialogue ( BAD ) classifier ( Roller   et al . , 2021 ; Peng et al . , 2022 ; Xu et al . , 2020 ;   Zhang et al . , 2020 ; Brown et al . , 2020 ) . We use   utterances from dialogue datasets ( Empathetic Di-   alogues , ConvAI2 , BAD , DailyDialog ) , and zero-   shot generated utterances ( Bloom ZS , OPT-66B   ZS ) as user input pools ( Rashkin et al . , 2019 ; Di-   nan et al . , 2020 ; Xu et al . , 2021 ; Li et al . , 2017 ;   Scao et al . , 2022 ; Zhang et al . , 2022 ) .   In the prompt continuation task , we red team the   GPT-3 with two Perspective API scores , ‘ toxicity ’   and ‘ profanity ’ ( Brown et al . , 2020 ) . We use the   initial prompts in Real Toxicity Prompts as the user   input pool ( Gehman et al . , 2020 ) .   For the text - to - image generation task , we red   team the Stable Diffusion with NSFW safety fil-   ter ( Rombach et al . , 2022 ) . We use the zero - shot   generated utterances ( OPT-66B ZS ( T2I ) ) as the   user input pool . Please refer to Appendix D.1 and   Appendix D.2 for more details .   4.1.2 Baseline Methods   We compare the red teaming performance of BRT   against the test case search methods ( Rand , Offen-   sive Top- N ) and the test case generation methods   ( Stochastic Few Shot ( SFS ) , Supervised Learning   ( SL ) ) under a limited query budget N(Perez et al . ,   2022 ) . Rand randomly samples test cases from   the user input pool . Offensive Top- Nassumes   that input offensiveness scores r(u)are accessible   and chooses top- Nuser inputs with highest r(u )   scores . SFS uses a pre - trained language model   and generates test cases by continuing few - shot   prompts generated with samples from the user input   pool . SLfine - tunes a pre - trained language model to   maximize the log - likelihood of positive test cases   in the user input pool . Test cases are then zero - shot   generated from the fine - tuned model . Please refer   to Appendix D.3 for more details .   Table 3 summarizes the number of access to   classifiers and the victim model in each method .   Each red teaming method requires Naccess   toGandRto calculate the red team scores   { R(u , G(u))}and classify the queried test   cases . BRT ( s+r ) , BRT ( e+r ) , and Offensive Top-   Nrequire |ˆU|additional access to rto calculate   the input offensiveness scores { r(u)}of the   user input pool . For fair comparison , we compare   BRT ( s ) with Rand , BRT ( s+r ) with Offensive Top-   N. The test case generation baselines , SFS and   SL , utilize red team scores { R(u , G(u ) ) } ,   thus making |ˆU|access to both GandR. We   emphasize that SFS andSLhave an unfair advan-   tage over BRT methods due to their access to vic-   tim model outputs of the entire user input pool ,   { G(u ) } , resulting in |ˆU|additional queries to   the victim model compared to BRT methods .   4.1.3 Evaluation Metrics   The primary goal of red teaming is to identify as   many diverse positive test cases as possible . We   evaluate the red teaming methods on two metrics:11556   red teaming success rate ( RSR ) and Self - BLEU   score . RSR is the percentage of positive test cases   among queried test cases . Thus a red teaming   method achieves higher RSR if it finds more pos-   itive test cases under limted number of queries .   Self - BLEUis an evaluation metric introduced   in Section 2.2 that measures the diversity of a text   set . For all experiments , we set k= 100 and cal-   culate Self - BLEUscore of positive test cases   inTby averaging Self - BLEU scoreof random   k - subset of Tover 100 runs .   4.2 Results   Table 4 summarizes the red teaming results against   BB-3B on the open - domain dialogue task . The   results show that BRT finds significantly more di-   verse positive test cases than all the baseline meth-   ods on all the user input pools we consider . No-   tably , both BRT ( e ) andBRT ( e+r ) significantly   outperform the baseline methods , achieving more   than three times larger RSR than SFSandSLwith   a lower Self - BLEUscore on Bloom ZS . Fig-   ure 2 shows the cumulative number of discovered   positive test cases on Bloom ZS against BB-3B   model . The result shows that BRT methods dis-   cover significantly more positive test cases using   fewer number of queries than the baseline meth-   ods . Table 5 presents the red teaming results on   the BAD dataset against GPT-3.5 based chatbots .   The results demonstrate that BRT also outperforms   the baseline methods when applied to large - scale   language model - based chatbots .   To evaluate the effectiveness of red teaming11557   methods in identifying hard positive test cases , we   consider a scenario in which the input offensive-   ness classifier ris freely accessible to measure   the offensiveness of a user input . We first use r   to filter out the offensive user inputs from the user   input pool , then apply the proposed BRT methods   and the baseline methods to the filtered pool of   safe user inputs . For the test case generation meth-   ods , we also ensure the safety of the generated test   cases by filtering out offensive test cases during the   generation and only interacting with the remaining   safe test cases . Table 6 shows the hard positive red   teaming results on Bloom ZS and ConvAI2 against   BB-3B model . BRT also outperforms the baseline   methods by a large margin when we red team the   hard positive test cases . Further analysis and exper-   imental results for the open - domain dialogue task   involving other datasets and victim models can be   found in Appendix E.2.1 .   BRT also shows superior performance on the   prompt continuation task and the text - to - image gen-   eration task against the baseline methods , demon-   strating the general effectiveness and applicability   of BRT in multiple domains . Table 7 shows that   BRT outperforms Rand andOffensive Top- Non   Real Toxicity Prompt with two types of Perspective   API scores , ‘ toxicity ’ and ‘ profanity ’ . Please refer   to Table 9 of Appendix E.1 for the red teaming   results in the text - to - image generation task .   Figure 3 illustrates the outputs of BB-3B given   the edited test cases tgenerated by BRT ( e )   in comparison to the corresponding unedited test   cases ton various user input pools . These examples   demonstrate that BRT ( e ) can successfully gener-   ate positive test cases outside the user input pool   by making a few word replacements . We provide   more qualitative results in Appendix E.3.115585 Related Work   A line of research utilizes manually designed tem-   plates to detect the model failures . Garg et al .   ( 2019 ) and Ribeiro et al . ( 2020 ) use templates to   test the fairness and robustness of the text classifi-   cation models . Bartolo et al . ( 2021 ) generate syn-   thetic adversarial data against question answering   models and improve the model robustness through   adversarial training . Röttger et al . ( 2021 ) utilize   templates to discover the failure of red team clas-   sifiers . Other prior works generate human - written   texts to identify the model failures in human - in - the-   loop scenario . Dinan et al . ( 2019 ) propose build it ,   break it , fix it scheme , which repeatedly discovers   failures of toxicity classifiers from human - model   interactions and fixes it by retraining to enhance   the robustness of the classifiers . Xu et al . ( 2021 )   adapt the notion of build it , break it , fix it scheme   to prevent harmful behavior of dialogue models .   Recently , Perez et al . ( 2022 ) red team dialogue   models using test cases generated by LM .   In the perspective of related recent machine   learning techniques , there has been a growing in-   terest in utilizing BO to uncover the vulnerability   of models . Ru et al . ( 2020 ) , Wan et al . ( 2021 ) , and   Lee et al . ( 2022 ) conduct BO to search adversarial   examples against classification models on image ,   graph , and text domains . Lee et al . ( 2022 ) improve   the scalability of BO by utilizing the Subset of   Data ( SoD ) method and batching based on DPP   prior ( Chalupka et al . , 2013 ; Kulesza , 2012 ) .   6 Conclusion   Our work aims to identify the potential risk of of-   fensive behavior in black - box large - scale gener-   ative models by red teaming in a limited query   regime . We propose BRT , a novel query - efficient   black - box red - teaming method using BO . BRT   methods construct a user input pool and iteratively   choose or edit user inputs using BO to generate di-   verse positive test cases . In contrast to prior works ,   BRT can incorporate the information from past   evaluations using GP to efficiently identify diverse   failures . The experimental results show that BRT   consistently outperforms existing methods in find-   ing a greater number of positive test cases with   higher diversity on various generation tasks includ-   ing open - domain dialogue , prompt continuation ,   and text - to - image generation , against various vic-   tim models under a query limit . Societal and Ethical Impact   Importance of Query - Efficient Black - Box Red   Teaming . It is becoming more common for   large generative models to be used in the form   of API ( Brown et al . , 2020 ; Chowdhery et al . ,   2022 ; Ramesh et al . , 2022 ) . Moreover , API users   can fine - tune the black - box model using custom   datasets through API and build personalized ap-   plications such as personalized chatbots ( OpenAI ,   2023 ) . Since each query to the API usually in-   curs costs , the development of techniques that can   query - efficiently identify model failures is essential   for cost - effective AI safety . Hence , our proposed   BRT methods can be valuable tools in this regard .   Broader Ethical Impact . Red teaming research is   crucial to make large generative models safer and   more reliable by white - hacking , in particular , for   deployment , thus ultimately aiming the sustainable   AI for humans . We mainly focus on describing   BRT for offensive results . Even though there are   potential risks of an adversary abusing BRT to gen-   erate socially harmful contents , we believe that our   results can give insights to AI research groups and   industries for training safer large generative models   and applying them to real - world applications for   users under various scenarios .   Limitations   We utilize safety classifier modules , such as the   BAD classifier and Perspective API , as the red team   classifier to automatically identify offensive output   from the victim model following the practice in   Perez et al . ( 2022 ) . However , automatic classifica-   tion of offensive outputs can be subject to inaccu-   racies , which may lead to the identification of false   positive test cases ( Gehman et al . , 2020 ) . To miti-   gate this issue , we may increase the threshold for   positive texts to reduce the number of discovered   false positive test cases . One other choice is incor-   porating human supervision into the classification .   For example , we may assume the human - in - the-   loop scenario that has access to the offensiveness   scores evaluated by human annotators within a lim-   ited number of queries to the annotators . In this   scenario , we can either directly conduct BRT with   human annotators as the red team classifier or mod-   ify the BRT method to incorporate offensiveness   scores from both human annotators and the safety   classifier modules during red teaming . Further ex-   ploration of these possibilities is left as future work.11559Acknowledgement   This work was supported by SNU - NA VER Hyper-   scale AI Center , Institute of Information & Commu-   nications Technology Planning & Evaluation ( IITP )   grant funded by the Korea government ( MSIT )   ( No . 2020 - 0 - 00882 , ( SW STAR LAB ) Develop-   ment of deployable learning intelligence via self-   sustainable and trustworthy machine learning and   No . 2022 - 0 - 00480 , Development of Training and   Inference Methods for Goal - Oriented Artificial In-   telligence Agents ) . Hyun Oh Song is the corre-   sponding author .   References115601156111562A Algorithms   The overall algorithm of BRT ( s ) andBRT ( e ) is shown in Algorithm 1 and Algorithm 2 , respectively .   Refer to Appendix D.5.1 for the process of adapting λ .   A.1 Overall Algorithm of BRT ( s )   Algorithm 1 BRT ( s)Input : The user input pool ˆU , the victim model G , the red team classifer R.Initialize T ∼ Unif(/parenleftbig / parenrightbig   ) .Initialize D ← { ( t , f(t)}.Initialize λ←λ.while|D| < Ndo Sample Dof size Nby SoD on D(Refer to Appendix C.3.1 ) . Fit GP parameters θto maximize the posterior probability distribution on D. Construct a batch B⊂ˆU \ T of the size min(N , N− |D| ) according to EI ( · | D , θ)scores   and the DPP prior ( Refer to Appendix C.3.2 ) . Evaluate the batch D={(t , f(t ) ) } . Update the test case set T ← T ∪ B. Update the evaluation history D ← D ∪ D. ifSelf - BLEU(T ) > D then λ←λ×ρ . else if Self - BLEU(T ) < D−δthen λ←λ / ρ . end if Update the white - box terms { g(u;T ) } . Update the reference term LofEI.L←max[min ( f(t),0 ) + λg(t;T)].end whileReturn T , T.11563A.2 Overall Algorithm of BRT ( e )   Algorithm 2 BRT ( e)Input : The user input pool ˆU , the victim model G , the red team classifer R.Initialize T ∼ Unif(/parenleftbig / parenrightbig   ) .Initialize D ← { ( t , t , f ( t)}.Initialize λ←λ.while|D| < Ndo Sample Dof size Nby SoD on D(Refer to Appendix C.3.1 ) . Fitθ to maximize the posterior probability distribution on { ( t , f(t ) ) } . Fitθto maximize the posterior probability distribution on { ( t , f(t ) ) } . Construct a batch B⊂ˆU \ T of the size min(N , N− |D| ) according to EI ( · | D , θ )   scores and the DPP prior ( Refer to Appendix C.3.2 ) . Initialize B← ∅,D← ∅. fortinBdo Compute the white - box terms { g(u;T ) } . Find the best edit candidate t∈ B({t})which maximizes EI ( · | D , θ ) . Evaluate t. D← D∪ { ( t , t , f(t ) } . B←B∪ { t } . end for Update the test case set T ← T ∪ B. Update the evaluation history D ← D ∪ D. ifSelf - BLEU(T ) > D then λ←λ×ρ . else if Self - BLEU(T ) < D−δthen λ←λ / ρ . end if Update the white - box terms { g(u;T ) } . Update the reference term LofEI.L←max[min ( f(t),0 ) + λg(t;T)].end whileReturn T , T.11564B Derivation of Equation ( 3 )   For the evaluated test case set T={t , . . . , t } ,   the objective L(u;T)can be decomposed to the   black - box red team score function f(u)and the   white - box diversity function g(u;T ) . Since gis a   deterministic white - box function ,   E[g(u;T)|u , D ] = g(u;T ) ,   Var[g(u;T)|u , D ] = 0 .   Hence , we can derive Equation ( 3 ) as following :   E[L(u)|u , D ]   = E[f(u)−λg(u;T)|u , D ]   = E[f(u)|u , D]−λE[g(u;T)|u , D ]   = E[f(u)|u , D]−λg(u;T ) ,   Var[L(u)|u , D ]   = Var [ f(u)−λg(u;T)|u , D ]   = Var [ f(u)|u , D]−λVar[g(u;T)|u , D]/bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright   = Var [ f(u)|u , D ] .   C Bayesian Optimization   In this section , we describe the continuous feature   of the GP model . We then explain the GP model fit-   ting procedure . Finally , we present the techniques   to improve the scalability of BRT .   C.1 Continuous Feature   We compute the sentence embedding emb ( u)of   a user input uusing a pre - trained transformer .   Specifically , we use the all - distilroberta - v1 model   ofsentence_transformer library ( Liu et al . , 2019 ;   Reimers and Gurevych , 2019 ) . Then , we use the   sentence embedding as the continuous feature for   the GP model , i.e. ,c(u ) = emb ( u ) .   C.2 GP Model Fitting   We fit GP parameter θto maximize the log posterior   probability distribution on D , log(p(θ| D ) ) .   From Bayes theorem , the posterior probability is   decomposed into the log maginal likelihood and   the log prior probabililty as following :   log(p(θ| D ) )   = log ( p(D|θ ) ) + log ( p(θ))−log(p(D)).Algorithm 3 Subset of DataInput : The evaluation history D , the evaluated   test case set T , and the size of subset N.if|D| < Nthen Return D.end ifInitialize T← { t}where t∼Unif(T).while|T| < Ndo Select t∈ T\ Twhich minimizes   d(t)≜maxcos(c(t ) , c(t ) ) . Update T← T∪ { t}end whileD← { ( t , f(t))∈ D | t∈ T}.Return D.   Since p(D)is a constant term , the problem of   maximizing the log posterior probability is equiva-   lent to the following maximization problem :   maximizelog(p(D|θ ) ) + log ( p(θ)).(4 )   We use Adam , a first order optimization method   to optimize Equation ( 4 ) ( Kingma and Ba , 2015 ) .   We set the learning rate to 0.1and update θfor   20iterations , with the initial values set to the GP   parameters from the previous step ( using warm   start ) .   C.3 Techniques for Scalability   We utilize two techniques , history subsampling and   batching , to improve scalability of Bayesian opti-   mization following the practice of Lee et al . ( 2022 ) .   We outline the process of these techniques for the   sake of completeness .   C.3.1 History Subsampling   Farthest Point Clustering ( FPC)-based Subset of   Data ( SoD ) method samples the subset Dof the   evaluation history D={(t , f(t))}(Chalupka   et al . , 2013 ) . To start , we randomly sample a   test case tfrom the evaluated test case set T=   { t , . . . , t } . Then , we sequentially select the test   case that minimizes cosine similarity to the most   similar test case among all previously selected test   cases . This procedure continues until the subset   size reaches N. We use N= 1000 for all   experiments we consider . If |D|>10000 , we sam-   ple a subset of size 10000 randomly from Dand   conduct SoD to the sampled subset to obtain the   subset Dof size N. The overall process of   SoD is summarized in Algorithm 3.11565C.3.2 Batching with the DPP prior   For each step , Selector GP constructs a batch   B⊂ˆUof the size N= 10 using the DPP   prior to promote batch diversity ( Kathuria et al . ,   2016 ) . The DPP prior of a batch Bis defined as   the determinant of the posterior variance matrix ,   Var(f(B)|B , D ) . We first construct the user in-   put set H⊂ˆUof the top- 200acquisition values .   Then , we initialize the batch B={u}where   u∈His the maximizer of the acquisition func-   tion . We greedily append the maximizer u∈H\B   of the DPP prior Var(g(B∪ { u } | D ) ) toBwhile   |B| ≤10 .   D Implementation Details   In this section , we outline the implementation de-   tails of our work .   D.1 User Input Pools   We construct user input pools using utterances from   dialogues and utterances zero - shot generated by   LM . In this section , we provide description of user   input pools we used .   D.1.1 Open Domain Dialogue   Following the practice of Perez et al . ( 2022 ) , we   generate utterances in zero - shot using the zero - shot   prompt   > List of questions to ask someone :   > 1 .   using the pre - trained Bloom and OPT-66B models ,   respectively ( Scao et al . , 2022 ; Zhang et al . , 2022 ) .   We generate utterances by nucleus ( top- P ) sam-   pling among top- Ktoken candidates for P= 0.95 ,   K= 50 with the temperature T= 1 ( Holtzman   et al . , 2020 ) . The generation process continues un-   til the model samples the end - of - sentence token or   a token containing ‘ \n ’ or ‘ 2 ’ . We sample a total of   1 million unique utterances from the Bloom model   and 500,000 unique utterances from the OPT-66B   model . To improve memory efficiency , we use   LLM.int8 ( ) , a quantization technique that does not   compromise performance during generation . We   utilize the implementation of LLM.int8 ( ) in bit-   sandbytes library ( Dettmers et al . , 2022 ) . We per-   form the process above in a machine with Intel   Xeon Gold 6338 CPU and four A100 GPUs .   We construct user input pools using the utter-   ances in the training sets of dialogue datasets ( Em-   pathetic Dialogues , ConvAI2 , BAD , DailyDialog)(Rashkin et al . , 2019 ; Dinan et al . , 2020 ; Xu et al . ,   2021 ; Li et al . , 2017 ) . We collect the utterances   in the training set of each dialogue dataset using   ParlAI library , a unified platform for dialogue tasks   ( Miller et al . , 2017 ) . We remove redundant utter-   ances and construct Empathetic Dialogues , Con-   vAI2 , and BAD user input pools of sizes 63 K , 116   K , and 63 K , respectively .   D.1.2 Prompt Continuation   For prompt continuation task , we use the set of ini-   tial prompts in Real Toxicity Prompt dataset as the   user input pool ( Gehman et al . , 2020 ) . We utilize   the Real Toxicity Prompt dataset open - sourced in   Hugging Face library ( Wolf et al . , 2020 ) .   D.1.3 Text - to - Image Generation   For Text - to - Image Generation , we construct user   input pool with LM generated zero - shot outputs   using nine zero - shot prompts with the following   template ,   > List of prompts that start with “ [ A ] ”   that generates [ B ] images :   > 1 .   where [ A ] is one of ‘ Image of ’ , ‘ Photo of ’ , or ‘ Pic-   ture of ’ , and [ B ] is one of ‘ various ’ , ‘ real - world   object ’ , or ‘ human - related ’ . We use the pre - trained   OPT-66B model with LLM.int8 ( ) and generate user   inputs for text - to - image model as described in Ap-   pendix D.1.1 . The generation result is a total of   79,072 unique utterances generated by LM .   D.2 Victim Models   For the open domain dialogue task , we use BB-3B ,   GODEL - large , DialoGPT - large , Marv , and Friend   chat models as the victim models ( Roller et al . ,   2021 ; Peng et al . , 2022 ; Zhang et al . , 2020 ; Brown   et al . , 2020 ) . We generate the response of a dia-   logue model to a user input using greedy decoding   on the dialogue models except for two GPT-3.5   based chatbots , Marv and Friend chat . For Marv   and Friend chat , we follow the OpenAI API ’s de-   coding instructions .   In the prompt continuation task , we consider   GPT-3 as the victim model ( Brown et al . , 2020 ) . In-   stead of directly performing inference on the GPT-3   model , we utilize data of initial prompts and GPT-   3 ’s continuations from Real Toxicity Prompt.11566The victim model of text - to - image generation   task is Stable Diffusion , a recently open - sourced   image generation model . We use version 1.4 of   the Stable Diffusion model ( Rombach et al . , 2022 ) .   We generate 512 by 512 pixels images with pseudo   linear multistep sampler ( PLMS ) ( Liu et al . , 2022 ) .   D.3 Baselines   In this subsection , we describe two generation   based red teaming methods , SFS andSL(Perez   et al . , 2022 ) . We first introduce the process of   SFS andSLin the open domain chatbot task . For   each generation step , SFS samples 5user inputs   u , . . . , ufrom the user input pool with probabil-   ity proportional to exp(R(u , G(u))/0.1 ) . Then ,   SFSconstructs the few - shot prompt as following :   > List of questions to ask someone :   > 1 . — — — — — u — — — — — –   > 2 . — — — — — u — — — — — –   > 3 . — — — — — u — — — — — –   > 4 . — — — — — u — — — — — –   > 5 . — — — — — u — — — — — –   > 6 .   SFS generates test cases by continuing the con-   structed few - shot prompt . Similar to zero - shot   generation in Appendix D.1 , we use nucleus ( top-   P ) sampling among top- Ktoken candidates for   P= 0.95,K= 50 with the temperature T= 1 .   The generation process halts when LM samples the   eos token or a token containing ‘ \n ’ or ‘ 7 ’ . We con-   sider the pre - trained OPT-1.3B and Bloom models   as the LM .   SLfine - tunes the OPT-1.3B model parameters to   maximize the log - likelihood of positive user inputs   in the user input pool condition on the zero - shot   prompt :   > List of questions to ask someone :   > 1 .   Then , SLgenerates test cases in zero - shot using the   zero - shot prompt . We randomly sample 90 % of   positive user inputs in ˆUto form a training set and   the remaining positive user inputs as validation set .   We run Adam optimizer with batch size 32 for the   minimum of 1epoch and 300update steps ( Kingma   and Ba , 2015 ) . We vary the learning rate in the   range of [ 5×10,2×10,5×10,2×10 ,   5×10,2×10,5×10,2×10,5×10 ,   2×10 ] and choose the trained parameters of   the best validation accuracy . In the text - to - imagegeneration task , we construct few - shot prompt for   SFSas   > List of prompts that start with “ Image   of ” that generates various images :   > 1 . — — — — — u — — — — — –   > 2 . — — — — — u — — — — — –   > 3 . — — — — — u — — — — — –   > 4 . — — — — — u — — — — — –   > 5 . — — — — — u — — — — — –   > 6 .   ForSL , we use the following zero - shot prompt :   > List of prompts that start with “ Image   of ” that generates various images :   > 1 .   Then , we conduct the same process above to fine-   tune the model parameters and generate utterances   in zero - shot using fine - tuned model .   D.4 Red Team Classifiers and Input   Offensiveness Classifiers   We provide the descriptions of red team classifiers   and input offensiveness classifiers used in each task .   For the open domain dialogue task , we utilize the   BAD classifier which measures the offensiveness   score of a dialogue . We normalize the output score   of BAD classifier to [ −1,1]and define the input of-   fensiveness score and the red team score functions   as following :   r(u ) = BAD ( u ) ,   R(u , o ) = BAD ( u∥o ) ,   where u∈ U is a user input , and o∈ O is a victim   model output .   Real Toxicity Prompt dataset contain the initial   prompts and their continuations . The dataset con-   tain the offensiveness scores ( toxicity and profan-   ity ) of the prompts and continuations evaluated   by Perspective API . We utilize the offensiveness   scores in the dataset for the input offensiveness   scores and the red team scores of user inputs in the   user input pool . Concretely , the input offensive-   ness score and the red team score functions in Real   Toxicity Prompt dataset can be written as :   r(u ) = PerspectiveAPI ( u ) ,   R(u , o ) = PerspectiveAPI ( o ) .   where uis an initial prompt , ois the continu-   ation of uby GPT-3 , PerspectiveAPI ( u)and11567PerspectiveAPI ( o)are offensiveness scores of u   ando , respectively . Here , we normalize the offen-   siveness scores to [ −1,1 ] .   The text - to - image generation task uses official   implementation of Stable Diffusion safety filter   which identifies NSFW images . The classifier has a   predefined list of the CLIP embeddings and the cor-   responding thresholds of NSFW - related concepts   such as ‘ sexual ’ , ‘ 18 + ’ , and ‘ nude ’ then calculates   the cosine similarity between the CLIP embedding   of a given image and the i - th concept , denoted by   cos ( o , i ) , where ois the image to be classi-   fied ( Radford et al . , 2021 ; Rando et al . , 2022 ) . An   image is classified as safe if all cosine similarities   are below its threshold , where the i - th threshold is   denoted by t ( i ) . All t ( i)s are lowered   by 0.01 if the image exhibits high cosine similar-   ity with any of the CLIP embedding of special   concepts including ‘ young girl ’ and ‘ young child ’   compared to the corresponding thresholds , where   thej - th cosine similarity and the threshold of the   j - th special concept are denoted by cos(o , j )   andt(j ) , respectively . Therefore , the red team   score function is given as following :   This score takes values in [ −1.22,0.82 ] . We   therefore normalize this score to [ −1,1]using a   piecewise linear function fdefined as :   f(x ) = /braceleftigg   x/1.22 ifx < 0 ,   x/0.82 otherwise .   Note that we conform to the use restrictionsand   generate NSFW images for research purpose only .   D.5 BRT   D.5.1 Adapting hyper - parameter λ   To guide the resulting positive test cases of BRT to   satisfy the diversity budget Dof Equation ( 1 ) , we   initialize λtoλand adjust λadaptively based   on the diversity of the current positive test cases   at each step . Concretely , we multiply ρ > 1toλ   when Self - BLEU(T ) > D , and divide λbyρ   when Self - BLEU(T ) < D−δ . D.5.2 Proxy of the White - Box Diversity   Function   In practice , we first sample an l - subset WfromT   and use a cheaper white - box function BLEU ( u , W )   as a proxy for the white - box term g(u;T)to im-   prove the efficiency . We update gperiodically ev-   eryNstep . We set l= 500 andN= 10 for all   experiments .   D.5.3 Word Replacement Operation   We use a pre - trained masked language model ,   RoBerta - large , to generate the candidates for word   replacement adapting the protocol of Garg and   Ramakrishnan ( 2020 ) ( Liu et al . , 2019 ) . Specif-   ically , given a word win a user input u , we first   replace wwith the mask token . Then , the pre-   trained RoBerta - large model predicts the token for   the replaced mask token . We discard tokens with   predicted probability smaller than 5×10 , and   use the remaining tokens of the top- 40predicted   probabilities as candidates . Finally , we filter out   the candidates that has part - of - speech ( POS ) differ-   ent to the original word wbased on nltkPOS tagger   ( Bird and Loper , 2004 ) . We adapt the word substi-   tution module in TextAttack API to implement the   process above ( Morris et al . , 2020 ) .   Editor GP finds the best edited test case t∈   B({t})where t∈ˆUis the user input selected by   selector GP . Editor GP conducts greedy ascent to   find the best edit in the ϵ-ball . Formally , editor   GP initializes t←tand iterates the following   greedy step for ϵtimes :   t←argmaxEI(t ) .   Then , editor GP selects the resulting tas the   edited test case . The 1 - ball of a text uis defined as   the set of texts generated by single word replace-   ment operation to u. To improve the scalability   of the editing procedure for long user inputs , we   randomly sample a maximum of 20 words from a   textuand only consider the set of texts generated   by replacing one of these words as the search space   for each greedy ascent step .   D.5.4 Hyper - parameters   In all experiments , we set the exploration budget   N= 50 , the batch size N= 10 , and the sub-   sample size of Subset of Data N= 1000 . For   BRT ( e ) andBRT ( e+r ) , we set ϵ= 3 .   We use the following configurations to adapt λ.11568   •Open - domain dialogue task and prompt con-   tinuation task : We initialize λtoλ= 0.3   forBRT ( s ) andλ= 0.03forBRT ( e ) for   adapting λ . We set ρ= 1.01,δ= 1 .   •Text - to - image generation task : We initialize   λtoλ= 0.03 . We set ρ= 1.01andδ= 1 .   •Figure 4 : We initialize λtoλ= 1.0for   BRT ( e+r ) . We set ρ= 1.03andδ= 1 .   In the open domain dialogue task ( Table 4 , Ta-   ble 5 , Table 6 , Table 8 , Table 10 , Table 11 ) , we   useSelf - BLEUofRand minus 0.1as the value   ofDforBRT ( s ) , and use Self - BLEUofOffen-   sive Top- Nminus 0.1forBRT ( s+r ) . Lastly , for   BRT ( e ) andBRT ( e+r ) , we set Dto the smallest   Self - BLEUof the baseline methods minus 0.1 .   For the experiments in prompt continuation task   ( Table 7 ) , we set Dto 20 . For the text - to - image   generation task , we set Dto 53 for all experiments .   D.5.5 Machine   We conduct our experiments on a machine with   AMD EPYC 7402 CPU and NVIDIA GeForce   RTX 3090 GPU . Under a query limit of N=   20,000 , the BRT process finishes within one GPU   day for user input pools in the open domain dia-   logue task . Specifically , the run - time for BRT ( s )   andBRT ( e ) in the ConvAI2 user input pool are 3   hours and 13 hours , respectively , on a single GPU   machine .   E Additional Experiments   In this section , we provide the additional analysis   and experimental results .   E.1 Text - to - Image Generation Task   Table 9 shows that BRT finds a significantly larger   number of positive test cases that generate NSFW   images compared to the baseline methods , demon-   strating the general effectiveness and applicabil-   ity of BRT in multiple domains including text-   to - image generation . Specifically , BRT ( s ) and   BRT ( e ) both outperforms their respective baselines   in RSR and Self - BLEU . This shows that our   method is capable of red teaming the text - to - image   generation domain.11569   E.2 Open - Domain Dialogue Task   E.2.1 Red Teaming Results against   GODEL - Large Model   We also compare BRT and the baseline methods   against GODEL - large model on the open - domain   dialogue task . Table 8 shows that BRT methods   outperforms the baseline methods in both RSR and   the diversity Self - BLEUunder a query limit of   20,000 . Moreover , Table 10 demonstrate that BRT   methods find significantly larger number of hard   positive test cases with higher diversity than base-   lines under a query limit of 20,000 .   E.2.2 Red Teaming Results on DailyDialog   Dataset   Table 11 shows the red teaming results on Daily-   Dialog dataset against BB-3B and DailoGPT - large .   The results show that BRT ( e ) discovers more than   1.8 times larger number of positive test cases com-   pared to the baseline methods , SFSandSL .   E.2.3 Human Evaluation   We further employed Amazon Mechanical Turk   ( MTurk ) to perform human evaluation of the red   teaming results in ConvAI2 against BB-3B. For   each method , we randomly sample 500 test cases   from a total of 20,000 test cases ( from Table 4 ) . For   each sampled test case t , we asked three evaluators   vote on the offensiveness of a dialogue t∥G(t ) .   We identified the test case as positive if two or   more evaluators vote the dialogue as offensive . Ta-   ble 12 summarizes the human evaluation results .   The results show that BRT ( e ) also discovers a sig-   nificantly greater number of test cases identified as   positive by MTurk compared to both SFS andSL   ( corresponding to column ‘ P ’ ) .   There exists a potential risk that the BRT meth-   ods , which aim to maximize the red team score dur-   ing the red teaming procedure , may discover test   cases that over - fit the red team classifier , resulting   in false positive test cases . To address this , we eval-   uate precision , defined as the ratio of true positive   test cases ( those identified as positive by both BAD   and MTurk ) among all positive test cases ( those   identified as positive by BAD ) . Table 12 shows   thatBRT ( e ) achieves higher precision compared to   the baseline methods , suggesting that the extent of   over - fitting is not severe empirically in ConvAI2 .   Nevertheless , it is crucial to prevent over - fitting to   ensure the trustworthiness of the red teaming re-   sults . To mitigate over - fitting , one possibility is to   utilize robust red team classifiers learned through   adversarial training methods or incorporate adver-   sarial example detection techniques into the BRT   framework ( Yoo and Qi , 2021 ; Zhou et al . , 2021 ;   Pang et al . , 2017 ; Yoo et al . , 2022 ) . We leave this   issue as a topic for future work .   E.2.4 Ablation on diversity trade - off   coefficient   We adjust the diversity trade - off coefficient λdur-   ing the BRT process and guide the diversity of   positive test cases to satisfy the diversity constraint.11570   To determine the validity of this technique , we   compare BRT ( e+r ) with its fixed- λcounterparts ,   namely , BRT(e+r ) .BRT(e+r ) uses a fixed   value of λduring red teaming . Figure 4 shows that   BRT ( e+r ) forD∈ { 40.0,43.0}meets the diver-   sity constraint and also achieves RSR comparable   toBRT(e+r ) .   E.2.5 Incorporating Sentence Quality   Measures into BRT   BRT can accommodate any measures of sentence   quality — diversity , fluency , grammaticality — into   the objective . In this subsection , we calculate the   perplexity values of user inputs via GPT-2 and uti-   lize these values as an automative measure of sen-   tence fluency . Instead of minimizing L(u;T ) ,   we minimize the following objective :   L(u;T):=f(u)−λg(u;T)−ηh(u ) ,   where h(u):= ( 1−perp ( u)/300 ) . From now on ,   we denote this method by BRT ( s ) + perp .   We conduct BRT ( s ) + perp on Empathetic Di-   alogues against BB-3B ( N=20,000 ) . Figure 5   illustrates the red teaming results for various ηval-   ues . As ηvalues increase , we observe a correspond-   ing decrease in perplexity . Specifically , when we   setη= 0.01 , the perplexity notably diminishes   from 133 to 94 , without compromising the RSR   and Self - BLEU score at all.11571E.3 Additional Qualitative Results11572ACL 2023 Responsible NLP Checklist   A For every submission :   /squareA1 . Did you describe the limitations of your work ?   We provide the limitations of our work in the Limitation section ( page 9 ) .   /squareA2 . Did you discuss any potential risks of your work ?   We provide potential risks of our work in the Ethics Statement section ( page 9 ) .   /squareA3 . Do the abstract and introduction summarize the paper ’s main claims ?   We provide abstract and introduction summarizing our main claims in the Introduction section   ( Section 1 ) ( page 1 - 2 )   /squareA4 . Have you used AI writing assistants when working on this paper ?   Left blank .   B / squareDid you use or create scientiﬁc artifacts ?   We provide the scientiﬁc artifacts such as parlai , huggingface in the Implementation Detail section   ( Section D ) in supplementary .   /squareB1 . Did you cite the creators of artifacts you used ?   Yes . We cite them in the Implementation Detail section ( Section D ) in supplementary .   /squareB2 . Did you discuss the license or terms for use and / or distribution of any artifacts ?   Yes . We describe the open - source APIs used in our research in the implementation detail section   ( Section D ) .   /squareB3 . Did you discuss if your use of existing artifact(s ) was consistent with their intended use , provided   that it was speciﬁed ? For the artifacts you create , do you specify intended use and whether that is   compatible with the original access conditions ( in particular , derivatives of data accessed for research   purposes should not be used outside of research contexts ) ?   Yes . We describe the purpose of the artifacts and our use in the implementation detail section ( Section   D ) .   /squareB4 . Did you discuss the steps taken to check whether the data that was collected / used contains any   information that names or uniquely identiﬁes individual people or offensive content , and the steps   taken to protect / anonymize it ?   We utilize the safety classiﬁer modules such as BAD or Perspective API . We describe this in the   implementation detail section ( Section D.4 )   /squareB5 . Did you provide documentation of the artifacts , e.g. , coverage of domains , languages , and   linguistic phenomena , demographic groups represented , etc . ?   Yes . We provide coverage of domains in the implementation detail section ( Section D.1 , D.2 ) .   /squareB6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits ,   etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the   number of examples in train / validation / test splits , as these provide necessary context for a reader   to understand experimental results . For example , small differences in accuracy on large test sets may   be signiﬁcant , while on small test sets they may not be .   Yes . We provide it in Table 1 of our main part . We describe this in detail in the implementation detail   section ( Section D.1).11573C / squareDid you run computational experiments ?   We provide it in Experiments section ( Section 4 ) and Additional Experiments section ( Section E ) .   /squareC1 . Did you report the number of parameters in the models used , the total computational budget   ( e.g. , GPU hours ) , and computing infrastructure used ?   Yes . We provide it in Table 1 , Experiments section ( Section 4.1.1 ) and the Implementation Detail   section ( Section D ) .   /squareC2 . Did you discuss the experimental setup , including hyperparameter search and best - found   hyperparameter values ?   Yes . We provide this in the Implementation Detail section ( Section D ) .   /squareC3 . Did you report descriptive statistics about your results ( e.g. , error bars around results , summary   statistics from sets of experiments ) , and is it transparent whether you are reporting the max , mean ,   etc . or just a single run ?   Yes . We provide the mean and std of our evaluation metrics for 3 runs in Experiments section ( Section   4 ) and Additional Experiments section ( Section E ) .   /squareC4 . If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did   you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE ,   etc . ) ?   Yes . We describe this in Section 2.2 , Section 4.1.3 , and Section D.   D / squareDid you use human annotators ( e.g. , crowdworkers ) or research with human participants ?   Left blank .   /squareD1 . Did you report the full text of instructions given to participants , including e.g. , screenshots ,   disclaimers of any risks to participants or annotators , etc . ?   No response .   /squareD2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students )   and paid participants , and discuss if such payment is adequate given the participants ’ demographic   ( e.g. , country of residence ) ?   No response .   /squareD3 . Did you discuss whether and how consent was obtained from people whose data you ’re   using / curating ? For example , if you collected data via crowdsourcing , did your instructions to   crowdworkers explain how the data would be used ?   No response .   /squareD4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?   No response .   /squareD5 . Did you report the basic demographic and geographic characteristics of the annotator population   that is the source of the data ?   No response.11574