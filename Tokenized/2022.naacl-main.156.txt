  Xiaochen Wang   University of North Carolina at   Chapel Hill   Chapel Hill , NC , USA   xcwang@email.unc.eduYue Wang   University of North Carolina at   Chapel Hill   Chapel Hill , NC , USA   wangyue@unc.edu   Abstract   As a fundamental task in natural language pro-   cessing , named entity recognition ( NER ) aims   to locate and classify named entities in unstruc-   tured text . However , named entities are always   the minority among all tokens in the text . This   data imbalance problem presents a challenge to   machine learning models as their learning ob-   jective is usually dominated by the majority of   non - entity tokens . To alleviate data imbalance ,   we propose a set of sentence - level resampling   methods where the importance of each train-   ing sentence is computed based on its tokens   and entities . We study the generalizability of   these resampling methods on a wide variety   of NER models ( CRF , Bi - LSTM , and BERT )   across corpora from diverse domains ( general ,   social , and medical texts ) . Extensive experi-   ments show that the proposed methods improve   performance of the evaluated NER models es-   pecially on small corpora , frequently outper-   forming sub - sentence - level resampling , data   augmentation , and special loss functions such   as focal and Dice loss .   1 Introduction   In natural language processing , named entity recog-   nition ( NER ) is an important task both on its own   and for numerous downstream tasks such as entity   linking and question answering . NER has an in-   herent data imbalance problem : named entities of   interest are almost always the minority among ir-   relevant ( Other type ) tokens in a text corpus . Table   1 shows the prevalent imbalanced nature of NER   corpora from multiple domains . As shown in Table   1,entity tokens ( tokens associated with any named   entity ) account for 3.9 - 16.6 % of all tokens in any   of these corpora . Within entity tokens , the most   frequent entity type may cover 2 - 200 times more   tokens than the least frequent entity type . At the   sentence level , 23 - 85 % sentences contain at leastone entity , suggesting that 15 - 77 % sentences con-   tain no entity at all .   Data imbalance is even more severe in real - world   bespoke NER tasks , which directly motivated this   work . For example , given full - text articles from   a medical subfield , domain experts may wish to   recognize only those concepts related to specific as-   pects of the subfield ( e.g. , symptoms and medicine   related to a specific disease ) . Compared to all to-   kens in the full text , extremely few tokens are anno-   tated with any entity type . Because domain experts   have limited availability , annotated corpus are usu-   ally small in such tasks . As a result , some rare   entity types may have less than 10 tokens across   the corpus . Such severe data imbalance and scarcity   makes many NER models suffer .   Data imbalance in NER challenges machine   learning - based models because their learning ob-   jective is dominated by entities of the majority type   ( Other ) , causing the model to be reluctant to predict   the types of interest . Various techniques have been   studied to tackle this challenge . Active learning   was applied to collect a more balanced dataset at   annotation time ( Tomanek and Hahn , 2009 ) . Spe-   cial loss functions including focal loss ( Lin et al . ,   2017 ) and Dice loss ( Li et al . , 2019 ) are proposed to   deal with data imbalance . Data augmentation was   shown to be effective by enriching entity - bearing   sentences through methods like segment shuffling   and mention replacement ( Dai and Adel , 2020 ; Is-   sifu and Ganiz , 2021 ; Wang and Henao , 2021 ) .   The classical method for alleviating data imbal-   ance is resampling ( upsampling the minority class   or downsampling the majority class ) and its close   relative , cost - sensitive learning ( assigning larger   weight to the minority class or smaller weight to   the majority class in the learning objective ) ( He   and Garcia , 2009 ) . A natural question is : Can we   apply resampling to address the data imbalance   problem in NER ? It turns out that unlike classi-   fication tasks , applying resampling to sequence2151Domain NER Corpus # of   Tokens # of   Entity   Types% of Least vs.   Most Freq . Type   Entity Tokens% of All   Entity   Tokens # of   Sent.% of Sent .   w/ Entities   Social WNUT 59,570 6 0.43 vs. 1.59 5.03 3,394 36.18   General GMB subset 66,161 8 0.03 vs. 4.00 15.03 2,999 85.40   Medical AnEM 71,697 11 0.03 vs. 1.08 3.91 2,815 35.38   Medical CADEC 121,307 5 0.21 vs. 6.65 15.76 5,719 58.86   General CoNLL 204,567 4 2.26 vs. 5.46 16.64 14,986 74.28   Medical n2c2 ADE 813,277 9 0.19 vs. 2.34 10.89 65,293 22.73   General OntoNotes 2,200,865 18 0.01 vs. 2.59 10.89 115,812 50.11   tagging tasks like NER is not straightforward . Re-   cent work attempted sub - sentence - level resampling   – dropping tokens from the majority class either   at random ( Akkasi , 2018 ) or using heuristic rules   ( Akkasi et al . , 2018 ; Akkasi and Varoglu , 2019 ;   Grancharova et al . , 2020 ) . These methods were   shown to perform well with shallow NER mod-   els – conditional random fields with local n - gram   and word shape features . However , sub - sentence-   level resampling inevitably destroy the structure of   complete sentences and distort the contextual infor-   mation around entities of interest . Complete sen-   tences are essential for state - of - the - art NER mod-   els based on contextual word representations , e.g. ,   deep Transformers ( Devlin et al . , 2018 ) . As shown   in our experiments , incomplete sentences gener-   ated by sub - sentence - level resampling often hurt   the performance of deep NER models .   In this paper , we propose sentence - level resam-   pling methods for NER , an underexplored problem   in this area . As sentences are the natural units   of data in NER , sentence - level resampling leaves   the contextual information intact in a natural sen-   tence needed by deep models like Transformers .   Since a sentence may contain a mixture of entities   whose types have different levels of rareness , tradi-   tional resampling method for imbalanced classifi-   cation ( e.g. , inverse probability resampling ) can not   be applied . Instead , we develop a set of methods   for computing integer - valued importance score for   a sentence based on its entity composition , and   resample the sentence accordingly . Experiments   show that our methods can improve performance of   a variety of NER models and are especially effec-   tive on tasks with small annotated corpora , whichis often seen in real - world bespoke NER tasks .   2 Related Work   2.1 Learning from Imbalanced Data   Class imbalance is a long - standing problem in   machine learning tasks , posing challenges to re-   searchers and practitioners in many domains ( King   and Zeng , 2001 ; Lu and Jain , 2003 ; He and Garcia ,   2009 ; Moreo et al . , 2016 ) . Classes in real - world   data often have highly skewed distribution , leading   to substantial gaps between majority and minority   classes . While the positive ( minority ) class is often   of interest , the lack of positive examples makes   classifiers conservative , i.e. , they incline to predict   all example as the negative ( majority ) class . This   often results in a low recall of the positive class .   Because only a small number of examples are pre-   dicted as positive , precision of the positive class   tends to be high or unstable . Such a low - recall ,   high - precision pattern often hurts the F1 - score , the   standard metric that emphasizes a balanced preci-   sion and recall ( Juba and Le , 2019 ) . This perfor-   mance pattern is observed not only in classification   tasks , but also in NER tasks where named entity   tokens are the minority compared to non - entity to-   kens ( Mao et al . , 2007 ; Kuperus et al . , 2013 ) .   Researchers have proposed various techniques   for imbalanced learning , including resampling and   cost - sensitive learning ( He and Garcia , 2009 ) . Both   aim to re - balance the representation of different   classes in the loss function , such that the classifier   is less conservative in making positive predictions .   In principle , by equating per - instance resampling   frequency with per - instance cost , resampling can   be implemented as cost - sensitive learning . How-2152ever , resampling can be applied to models that do   not support cost - sensitive learning , making it con-   veniently applicable to all models .   2.2 Resampling in Sequence Tagging Tasks   Resampling ( and cost - sensitive learning ) can be   conveniently used in classification and regression   tasks where a model makes pointwise predictions   ( a single categorical or scalar value ) . Each exam-   ple has a clearly defined sampling rate ( or cost )   according to its class label . However , in sequence   tagging tasks like NER ( more broadly , structured   prediction tasks ( BakIr et al . , 2007 ; Smith , 2011 ) ) ,   a model predicts multiple values for a sequence   ( or structured output ) . For sequence learning al-   gorithms such as linear - chain conditional random   fields , while the learning objective is formulated   at the sequence level , the evaluation metrics are   defined at the entity span level . This makes it non-   trivial to determine the sampling rate ( or cost ) for   a sequence that contains tokens from both majority   and minority entity types . Simply resampling enti-   ties by stripping surrounding context is problematic   as sequence tagging algorithms depend on context   to make predictions . Recent works proposed to ran-   domly or heuristically drop tokens from sentences   to re - balance NER data , which had success using   conditional random fields and shallow n - gram fea-   tures ( Akkasi , 2018 ; Akkasi and Varoglu , 2019 ;   Grancharova et al . , 2020 ) . However , these methods   distort the syntactic and semantic structure of com-   plete sentences , which may generate low - quality   data for models that are capable of capturing long-   distance linguistic dependencies ( e.g. BERT ) and   hurt performance of those models . In this work , we   focus on resampling strategies that leaves sentences   intact .   2.3 Loss Functions for Imbalanced Data   Recent literature proposed special loss functions   for tackling data imbalance , including focal loss   ( Lin et al . , 2017 ) and Dice loss ( Li et al . , 2019 ) .   They increase the cost of ‘ hard positives ’ where   the correct label has low predicted probability and   decrease the cost of ‘ easy negatives ’ where the cor-   rect label has high predicted probability . However ,   these loss functions do not fully address data im-   balance in NER . First , the formulation does not   always emphasize the loss of minority - class tokens   – majority - class tokens can also be hard to classify ,   and minority - class tokens can also be easy to clas-   sify . Second , these loss functions only work ontoken - wise prediction outputs . They can not work   on sequence - level outputs generated by conditional   random fields , which is commonly used in NER .   Our resampling methods can be seen as estimat-   ing sentence - level losses with explicit emphasis on   sentences containing minority - class tokens .   3 Resampling Strategy Design   For a sequence tagging task like NER , resampling   can not be as simple as what it is in classification   and regression tasks , in which data points can be   individually replicated , discarded , or synthesized .   In NER , named entities can not be resampled out   of context . The surrounding context of named enti-   ties – albeit tokens from the irrelevant Other type –   should be considered as well . Resampling named   entities with context is a double - edged sword : pre-   serving context will help NER models , but too   much context increases the amount of non - entity   tokens and aggravates the data imbalance problem .   The goal of sentence - level resampling is to find the   balance between too little and too much context ac-   companying named entities in complete sentences .   3.1 Sentence Importance Factors in NER   Intuitively , sentences that are worth resampling are   those that are more important towards constructing   a balanced NER dataset . We start by proposing   factors that influence the importance of a sentence   in resampling . These factors share the theoretical   foundation of retrieval functions in information   retrieval ( Fang et al . , 2004 ) . A retrieval function   evaluates the utility of a document with respect   to the query terms it contains . By direct analogy ,   sentence importance score measures the utility of a   sentence with respect to the entities it contains .   Count of entity tokens . Regardless of entity   types , a sentence containing more entity tokens   is more important than a sentence filled with non-   entity tokens . This factor mirrors term frequency   in retrieval functions ( Salton and Buckley , 1988 ) .   Rareness of entity type . The general idea of re-   sampling for minority classes says that the rarer an   entity type is , the more times we should resample   sentences containing this type of entity . This fac-   tor mirrors inverse document frequency in retrieval   functions ( Salton and Buckley , 1988 ) .   Density of tokens labeled as any entity . Includ-   ing too much context can aggravate the imbalance   problem . While the absolute count of entity tokens   matters , the density of entity tokens in a sentence2153(number of entity tokens compared to the length of   a sentence ) should also be concerned . The higher   the density , the more important a sentence . This   factor mirrors document length normalization in   retrieval functions ( Singhal et al . , 1996 ) .   Diminishing marginal utility . If one sentence   contains twice as many tokens with a specific en-   tity type as the other sentence with the same length ,   does that mean the first sentence is twice as impor-   tant as the second ? In reality , an entity may contain   numerous tokens , or a sentence may include multi-   ple entities of the same type . Twice as many tokens   from the same entity type may not offer twice as   much information ( for the same reason why too   many tokens from the Other type is not helpful ) .   Therefore , as the number of tokens from the same   entity type increases , they generate diminishing   marginal utility to a sentence . This factor mirrors   diminishing marginal gain of repeated query terms   in retrieval functions ( Fang et al . , 2004 ) .   3.2 Resampling Functions   Based on the above importance factors , we design   a suite of functions f∈Zto determine the num-   ber of times a sentence sshould be resampled in   a NER training set . These functions incorporate   progressively more factors discussed previously .   In a given corpus , let us denote the set of all   entity types except for the majority type Other as   T. Let c(t , s)be the count of tokens with entity   typet∈Tin sentence s. We define the resampling   function with respect to the smoothed count ( sC )   of all entity tokens as   f= 1 + /summationdisplayc(t , s ) . ( 1 )   Here,/summationtextc(t , s)is the total number of entity   tokens in sentence s. ‘ +1 ’ is to avoid removing   entity - less sentences from the training set , in remi-   niscence of add - one smoothing in empirical proba-   bility estimates . It guarantees that all training sen-   tences are resampled as least once . This smoothing-   like process maintains consistency between train-   ing and test sets . If the training set contains entity-   less sentences , it is highly likely that the test set   will contain entity - less sentences as well .   The next function incorporates entity rareness   factor . The rareness rof an entity type t∈Tis   measured as the self - information of the event that   any token carries this type :   r=−log / summationtextc(t , s )   N , where Sis the set of all sentences in the training   set , and therefore / summationtextc(t , s)is the total number   of tokens with entity type t. Nis number of all   tokens ( including Other tokens ) in the training set .   By introducing the rareness of an entity type we   propose another function called the smoothed re-   sampling incorporating count and rareness ( sCR ):   f= 1 +    /radicaligg / summationdisplayr·c(t , s)   . ( 2 )   Ceiling function ⌈·⌉ensures f∈Z. Square   root is to slow down the increase of fwhen an   entity type tis extremely rare ( when ris large ) .   According to the density factor in the previous   section , the length of sentence splays a role in   determining the density of entity tokens within a   sentence . Let lbe the length of sentence smea-   sured in number of tokens . We define the following   function called the smoothed resampling incorpo-   rating count , rareness , and density ( sCRD ):   f = 1 + /ceilingleftbigg / summationtextr·c(t , s)√l / ceilingrightbigg   . ( 3 )   We use√linstead of lto slow down the decrease   off when a sentence is too long .   Lastly , we incorporate the diminishing marginal   utility factor and propose a function called the nor-   malized and smoothed resampling incorporating   count , rareness , and density ( nsCRD ):   f = 1 + /ceilingleftigg / summationtextr·/radicalbig   c(t , s)√l / ceilingrightigg   .(4 )   Here,/radicalbig   c(t , s)applies a sublinear increasing   function on c(t , s)to implement the diminishing   marginal utility when a sentence contains many   tokens with the same type .   In summary , we proposed four functions for   determining resampling frequencies for each sen-   tence , representing four resampling methods .   4 Experimental Evaluation   Resampling should be a domain- and model-   agnostic strategy in tackling data imbalance . There-   fore , the goal of our experiments is to evaluate if   the proposed resampling methods are effective in   an extensive array of NER corpora and base models .   Towards this goal , we apply the four resampling   methods ( together with baseline methods ) on three   representative NER models ( each has two variants),2154and evaluate the resulting models on four corpora   from diverse domains .   4.1 Evaluation Metric   We use span - level strict - match macro - averaged F1   score as our main evaluation metric . Other is not   viewed as an entity type . Macro - averaged metrics   emphasize a balanced treatment of all entity types ,   which align with our main goal . See Appendix C   for micro - averaged and per - entity - type results .   4.2 Compared Methods   We compare the following baseline methods for   dealing with data imbalance in NER .   Original corpus : training data untreated .   Balanced undersampling : We implement the   algorithm proposed in ( Akkasi et al . , 2018 ) as a   representative of sub - sentence - level resampling .   Data augmentation : The data augmentation   techniques that includes alltransformations as pro-   posed in ( Dai and Adel , 2020 ) .   Focal loss ( Lin et al . , 2017 ) , Dice loss ( Li et al . ,   2019 ): We apply these loss functions on token-   wise predictions made by a softmax output layer .   Note that they are not applicable to sequence - level   predictions made by a CRF output layer .   sC , sCR , sCRD , nsCRD : the four resampling   methods proposed in this work .   4.3 NER Corpora   We select four corpora from different domains . The   first three are of small scale , representing bespoke   NER tasks in practice where entity types are task-   specific and annotation efforts are limited .   AnEM ( Ohta et al . , 2012 ): The Anatomical En-   tity Mention ( AnEM ) corpus consists of 500 doc-   uments selected randomly from citation abstracts   and full - text papers concerning both health and   pathological anatomy . With only 3.91 % entity to-   kens and 35.38 % sentences having any entity , this   is a very imbalanced corpus in Table 1 .   WNUT ( Derczynski et al . , 2017 ): This is a so-   cial domain corpus released in the 2017 Workshop   on Noisy User - generated Text ( W - NUT ) . It con-   tains noisy user - generated texts found in social me-   dia , online review , crowdsourcing , web forums ,   clinical records , and language learner essays . This   is another very imbalanced corpus in Table 1 .   GMB subset ( Bos et al . , 2017 ; Kaggle , 2018 ):   The Groningen Meaning Bank ( GMB ) corpus con-   sists of public domain English texts with corre-   sponding syntactic and semantic representations . The GMB subset is extracted from the larger GMB   2.0.0 corpus which is built specially for NER .   To test the generalizability of our methods , we   also include a standard NER benchmark dataset .   CoNLL ( Sang and De Meulder , 2003 ): The   CoNLL-2003 English news NER corpus .   For AnEM , WNUT , and CoNLL , we use their   pre - existing training / test split . For GMB subset ,   we use 3:1 training / test split .   4.4 Base NER Models and Variants   To comprehensively evaluate the combinations   of our upstream resampling strategies with many   downstream sequence tagging models , we select   the following models :   Shallow Model . We construct shallow NER   models that use pretrained word embeddings as   per - word feature vectors . We consider two variants :   one using a softmax output layer making token-   wise predictions ; the other using a CRF ( condi-   tional random fields ) output layer making sequence-   level predictions . Considering domains of the cor-   pora , we select embeddings trained on biomedical   literature ( Huang et al . , 2016 ) , tweets ( Glove-27B-   twitter-27B),and Wikipedia+news ( Glove-6B ) ,   for AnEM , WNUT , and datasets from general do-   main ( GMB subset and CoNLL ) , respectively . All   are 50 - dimensional embeddings . CrfSuiteis ap-   plied with default hyperparameters .   Bi - LSTM ( Bidirectional Long Short - Term   Memory ) . LSTM is a special recurrent neural net-   work architecture in which the vanishing gradient   problem can be effectively mitigated . Bi - LSTM   consists of two LSTMs taking inputs in both for-   ward and backward directions . Even though more   recent models ( e.g. , GPT-2 , BERT ) are shown to   outperform Bi - LSTM , it is still regarded as one   of the most prevalent tools for solving sequence   tagging problems . We implement two variants of   Bi - LSTM : one with a softmax output layer mak-   ing token - wise predictions ; the other with a CRF   decoding layer , to ensure the local consistency of   output tags . Different from the default hyperparam-   eters , batch size and number of epochs are set to   32 and 20 , respectively . Embeddings are used in   the same way as in the shallow models above .   BERT ( Bidirectional Encoder Representa-   tions from Transformers ) . BERT is widely re-21552156garded as the most significant improvement in nat-   ural language processing . Its outstanding capabil-   ity of learning contextualized word representations   makes it the representative of advanced NER model   in this work . Again , we implement two variants   of BERT : one with a softmax output layer making   token - wise predictions ; the other with a CRF de-   coding layer . Default hyperparameters are used .   More implementation details are in Appendix A.   4.5 Results and Discussion   Macro - averaged F1 - scores of different methods ap-   plied to four corpora and three base NER models   are reported in Tables 2 - 5 .   Our goal is notto compete with state - of - the-   art methods on these corpora . Instead , we aim to   present an interesting and underexplored problem   ( sentence - level resampling for NER ) and a set of   simple yet promising methods . In principle , our   proposed resampling methods are model - agnostic   and can provide an additional performance boost   for a variety of NER models . We observe the fol-   lowing trends in Tables 2 - 5 .   Overall performance of our resampling meth-   ods : Across Tables 2 - 5 , our methods ( sC , sCR ,   sCRD , nsCRD ) generally performed well , achiev-   ing the highest or second highest F1 - scores in al-   most every column ( except for the condition ‘ Shal-   low model , Softmax ’ on CoNLL ) . Although no   specific method consistently outperforms others   in every condition , it is clear that sentence - level   resampling is overall a promising approach to tack-   ling the data imbalance problem in NER . The best   resampling method depends on the specific base   model , output layer , and corpus used . Just as the   best hyperparameter values have to be empirically   determined , so could be the most suitable resam-   pling method . Fortunately , all our resampling meth-   ods are simple and straightforward , which allows   for convenient experimentation .   Shallow vs. deep models : We observe a clear   trend that shallow models using word embedding as   features and softmax / CRF as the output layer under-   perform deep models such as Bi - LSTM and BERT .   We view this as a sanity check . Bi - LSTMs and   BERT can learn word representations that account   for long - distance dependencies , and BERT should   be even more powerful with contextual word repre-   sentations pretrained on massive texts .   Softmax vs. CRF output layer : Using thesame base model , CRF output layer often ( but not   always ) outperforms softmax output layer . The   performance gap is larger on shallow models and   small corpora ( AnEM , WNUT , GMB ) than on deep   models and large corpus ( CoNLL ) . Indeed , Bi-   LSTM and BERT are capable of learning word   representations that account for long - distance word   dependencies , reducing the benefit of tag dependen-   cies offered by a CRF layer . Similar observation   was made by previous work ( Devlin et al . , 2018 ) .   An exception is the combination ( WNUT , Shallow   model ) , where the CRF layer suffered from severe   overfitting caused by noisy text and extremely im-   balanced data distribution in WNUT corpus .   Small vs. large corpus : On small corpora   ( AnEM , WNUT , GMB subset ) , our resampling   methods usually outperform the original corpus   baseline by a big margin . These benefits become   less salient on large corpus ( CoNLL ) . This implies   that our methods are especially effective when the   corpus is small and annotations are few . As corpus   size gets large , even rare entity types are covered by   many examples and therefore sufficiently trained .   Sub - sentence resampling and data augmen-   tation : Sub - sentence resampling ( balanced under-   sampling ) has large variance in its performance . In   some cases it gives the highest gain ( GMB subset ,   BERT model ) , and in other cases it performs worse   than just using the original corpus ( all corpora ,   Bi - LSTM models ) . It suggests that sub - sentence   resampling is highly sensitive to the corpus and   model choice . Data augmentation also shows high   variance in its performance . It gives the highest   gain on ( GMB subset , Bi - LSTM model ) , and per-   forms worse than the original corpus on ( WNUT ,   BERT model ) . Sentences generated by data aug-   mentation generally have correct syntax but garbled   semantics ( e.g. , one entity is replaced by another   same - type , out - of - context entity ) . The nonsensical   sentences may confuse NER models . In contrast ,   whole - sentence resampling methods give more sta-   ble improvements over the original corpus baseline   largely because they preserve the naturalness of   resampled sentences .   Focal loss and Dice loss : These loss functions   are applicable only on pointwise predictions made   by the softmax output layer . A major trend is that   their performance tend to be unreliable across sce-   narios . We attribute this behavior to the difficulty   in optimizing these losses . For shallow models ,   we optimize them by feeding gradients of either2157loss function ( see Appendix B for their derivation )   into a L - BFGS optimizer ( Liu and Nocedal , 1989 )   in Scikit - Learn . As shown in the ( Shallow model ,   Softmax ) column of GMB subset and CoNLL cor-   pora , the two loss functions ( especially the Dice   loss ) performed well . For deep models ( Bi - LSTM   and BERT ) , we rely on TensorFlow ’s automatic dif-   ferentiation and Adam gradient descent optimizer   ( Kingma and Ba , 2014 ) because manually deriving   gradients for deep models is infeasible . The two   loss functions sometimes give poor performance .   The Bi - LSTM model with Dice loss failed com-   pletely on AnEM ( F1 - score : 2.31 ) . A possible   explanation is that Dice loss is non - convex and it   may be difficult for first - order optimizers in current   deep learning toolkits ( e.g. Adam in TensorFlow )   to find high - quality local minima than second - order   methods like L - BFGS .   Precision and recall : To illustrate performance   changes in terms of precision and recall , Figure   1 visualizes the changes before and after resam-   pling as displacement vectors in precision - recall   plots with F1 - score contour lines . Some arrows are   pointing to the upper right corner of the plots , indi-   cating the associated methods improve F1 - score by   improving both precision and recall . Other arrows   point to the upper left , indicating the associated   methods increase recall at the sacrifice of precision .   In this case , most of our methods improve macro-   averaged precision and recall of the BERT model   on WNUT . See Appendix C.2 for more details .   4.6 Effect on Training Corpus Size   Table 6 shows the effect of training corpus size as   a result of resampling or data augmentation . These   factors are the average across four corpora .   The balanced undersampling method drops to-   kens from sentences , and therefore reduces training   corpus size . Data augmentation method doubles the   corpus size as many sentences are paraphrased into   multiple versions . Our proposed methods increases   corpus size by a slightly larger factor because sen-   tences that contain rare entity types are resampled   multiple times . Although increased training corpus   size leads to increased training time , note that our   methods are especially suitable for scenarios where   the annotated corpus is small and hence the training   time is still relatively short .   5 Conclusion and Future Work   Our proposed sentence resampling methods gener-   alize well across diverse NER corpora and models .   They enjoy the following advantages :   Model - agnostic : Since resampling only manip-   ulates datasets and not models , the proposed meth-   ods can be directly applied to any NER model ,   requiring no knowledge of its functioning or any   change to it . Resampling is also more convenient   than cost - sensitive learning as the latter still re-   quires changing the model training process .   Domain - agnostic : Compared with data pre-   processing methods such as data augmentation ,   sentence - level resampling methods are simple and   do not require domain- or language - specific ma-   nipulations such as synonym replacement , saving   practitioners from excessive data engineering.2158Note that data augmentation and sentence - level   resampling ( and resampling methods in general )   are complementary methods for improving NER   model training . Data augmentation improves the   semantic richness of training instances by expand-   ing the coverage of training data in the input fea-   ture space , while sentence - level resampling refines   theimportance weighting of training instances by   bridging the gap between the training objective   and evaluation metrics . Therefore , they work in   orthogonal directions . This points to a promising   direction for future work : to explore the two line of   methods in combination rather than in competition .   Various other avenues exist for future work .   First , further theoretical and empirical research can   explore more effective resampling functions that   deliver consistently better performance across cor-   pora and base NER models . Second , more corpora   and models can be examined under these resam-   pling strategies to evaluate their generalizability .   Third , the variance of performance in different sce-   narios may potentially relate to characteristics of   specific corpora . Future research may seek for   corpora - level statistics that can assist practitioners   in selecting the appropriate resampling methods .   Acknowledgments . The authors thank anonymous   reviewers for their useful comments . This work   is partially supported by the National Institute of   Health under the grant number R01DA053028 .   References21592160A Implementation Details   A.1 Software and Hardware Environment   All the deep learning models are implemented in   Tensorflow 1.12.0 environment .   Softmax regression ( or multinomial logistic re-   gression ) model is from Scikit - Learn package in   version 0.23.2 . The CRF model is implemented by   the package sklearn - crfsuite in version of 0.3.6 .   Data resampling and CRF training / evaluation   were performed on 2.60 GHz Intel CPUs and 8 GB   RAM . Bi - LSTM and BERT training / evaluation   were performed on GPUs ( GeForce GTX1080 8 GB   and Tesla V100 16 GB ) .   A.2 Hyperparameters for Machine Learning   Models   For shallow models and BERT , all hyperparameters   are set by default . For details of them , please see   documents of sklearn , crfsuite and BERT - NER .   For Bi - LSTM , we adjust a few of parameters as   there are some drawbacks of the default settings :   20 is not a commonly used number for batch size ,   and loss of Bi - LSTM model fails to converge under   some circumstances . So we set them to 32 and   20 , instead of default values 20 and 15 . Other   hyperparameters are applied according to default   settings .   For the fairness in the comparison , we do not   alter any hyperparameters while switching resam-   pling methods and loss functions without changing   dataset and models . We believe that it is totally   appropriate in the process of comparing , despite   that better performance of specific methods may   be obtained after tuning hyperparameters , which   beyond the scope of this exploring research .   A.3 Hyperparameters for Loss Functions   There are two hyperparameters in the focal loss   and Dice Loss , determining converging speed and   smooth degree . For focal loss , we set γto 2 , as   what authors of ( Lin et al . , 2017 ) recommend . Ac-   cording to ( Li et al . , 2019 ) , it is appropriate to set   γof Dice loss to 1 for the purpose of smoothing .   In our implementation of loss function in shallow   model , this setting is found effective . However ,   while using it in deep learning model , its effective-   ness can not be ensured . Hence , we adopt another   setting of γ= 10 in the tensor computing and   obtain better results compared with those obtained   with a larger γ . B Derivation of Loss Function Gradients   for Softmax Regression   When using the shallow model with softmax output   layer and focal / Dice loss functions , we optimize   the model parameters by the quasi - Newton method   L - BFGS provided by Python Scikit - Learn . This   approach requires us to provide the gradients of   current model parameters . Below we show our   derivation of these gradients .   Notations and Preliminaries . Scalar values are   denoted by non - bold , lowercase letters such as x.   Row vectors are denoted by bold , lowercase letters   such as x. Matrices are denoted by bold , uppercase   letters such as X.   Softmax regression has the following compo-   nents :   •Feature vector : x∈R , x =   [ x , · · · , x , · · · , x ] .   •Label vector : y∈ { 0,1},y =   [ y , · · · , y , · · · , y ] . If the ground truth is   thec - th class , 1≤c≤k , then y= 1 , and   y= 0ifj̸=c .   •Weight vector for the j - th class : w∈   R , w= [ w , · · · , w , · · · , w ] .   •Weight matrix W∈R , W =   [ w,···,w,···,w ] . “ ⊤ ” is the trans-   pose operation . wis the transpose of w ,   which is a column vector .   • Bias for the the j - th class : b∈R.   •Predicted probability vector : p∈[0,1],p=   [ p , · · · , p , · · · , p ] .   p= Pr ( y= 1|x ) ( 5 )   = exp(⟨w , x⟩+b)/summationtextexp(⟨w , x⟩+b)(6 )   ⟨w , x⟩is the inner product of vector wand   vector x.   One can verify that the partial derivative of p   with respect to w , the weight of the j - th class , the   i - th dimension , is the following :   ∂p   ∂w= [ 1{j = c } −p]px ( 7)2161B.1 Focal Loss Gradient   Suppose the ground truth is the c - th class for a   given example x.   L(x , y ) = −(1−p)logp ( 8)   ∂L(x , y )   ∂w(9 )   = −∂   ∂p[(1−p)logp]·∂p   ∂w(10 )   = −/bracketleftbigg   −γ(1−p)logp+(1−p )   p / bracketrightbigg   · ∂p   ∂w   ( 11 )   = −/bracketleftbigg   −γ(1−p)logp+(1−p )   p / bracketrightbigg   · [ 1{j = c } −p]px ( 12 )   = −[−γp(1−p)logp+ ( 1−p ) ]   · [ 1{j = c } −p]x ( 13 )   = a[p−1{j = c}]x ( 14 )   Here we set   a=−γp(1−p)logp+ ( 1−p)(15 )   to reduce notational clutter . ahas nothing to do   withiorj ; it only has to do with c , the index of   the ground truth label for the training example x.   When γ= 0,a= 1 . When γ > 0,adecreases   when pincreases from 0 to 1 . This means the   gradient for an easy example ( when pis close to   1 ) have a smaller magnitude than the gradient for a   hard example ( when pis close to 0 ) .   Generalizing the scalar gradient in Equation ( 14 )   to matrix gradient , we have   ∂L(x , y )   ∂W = a·x(p−y ) . ( 16 )   The shape of a·x(p−y)ism×k , the same   shape as W.   An important note is that here ais specific to   that single example x , which has ground truth la-   bely= 1 . If we have ndifferent training ex-   amples x,···,x , then every example will   have a different avalue : a , · · · , a. Let ’s   create a diagonal matrix A∈R , A=   diag(a , · · · , a ) .   If we have ntraining examples , then the fea-   ture matrix X∈R , the label matrix Y∈   { 0,1 } , and the predicted probability matrix   P∈[0,1 ] . We have :   ∂L(X , Y )   ∂W = XA(P−Y ) . ( 17)The shape of XA(P−Y)ism×k , the same   asW.   B.2 Dice Loss Gradient   Dice loss computes per - class F-1 scores . Suppose   the ground truth is the c - th class for a given exam-   plex .   L(x , y ) ( 18 )   = /summationdisplay / bracketleftbigg   1−1{c = j}γ+ 2p   γ+p+ 1   +1−1{c̸=j}γ   γ+p / bracketrightigg   ( 19 )   = 1−γ+ 2p   γ+p+ 1+/summationdisplay / bracketleftigg   1−γ   γ+p / bracketrightigg   ( 20 )   = k−γ+ 2p   γ+p+ 1−/summationdisplayγ   γ+p(21 )   Take gradient with respect to w , the weight of   thej - th class , the i - th dimension .   ∂L(x , y )   ∂w(22 )   = −∂   ∂p / bracketleftbiggγ+ 2p   γ+p+ 1 / bracketrightbigg   · ∂p   ∂w   −/summationdisplay∂   ∂p / bracketleftigg   γ   γ+p / bracketrightigg   · ∂p   ∂w(23 )   = −2(γ+p+ 1)−(γ+ 2p)2p   ( γ+p+ 1)·∂p   ∂w   −/summationdisplay−γ·2p   ( γ+p)·∂p   ∂w(24 )   = −2(γ+p+ 1)−(γ+ 2p)2p   ( γ+p+ 1 )   · [ 1{j = c } −p]px   −/summationdisplay−γ·2p   ( γ+p)·[1{j = j } −p]px   ( 25 )   = −2(1−p)(1 + γ+p)p   ( γ+p+ 1)·[1{j = c } −p]x   ( 26)2162+/summationdisplayγ·2p   ( γ+p)·[1{j = j } −p]x ( 27 )   = a[p−1{j = c}]x−/summationdisplayb[p−1{j = j}]x   ( 28 )   Here we set   a=2(1−p)(1 + γ+p)p   ( γ+p+ 1)(29 )   b = γ·2p   ( γ+p)(30 )   adepends on the ground truth label of example x.   bdepends on the current predicted probabilities   forx .   Generalizing the scalar gradient in Equation ( 28 )   to matrix gradient , we have   ∂L   ∂W(31 )   = xa(p−y )   −x   ···,/summationdisplayb[p−1{j = j } ] , · · ·   /bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright      ( 32 )   = xa(p−y)−xv ( 33 )   vis a vector specific to the example x.   If we have ntraining examples , then the fea-   ture matrix X∈R , the label matrix Y∈   { 0,1 } , and the predicted probability matrix   P∈[0,1 ] . We have :   ∂L(X , Y )   ∂W = XA(P−Y)−XV(34 )   where Vhas shape n×k , and the l - th row in matrix   Vis akdimensional vector computed in the same   manner as Equation ( 32 ) with respect to the l - th   training example , 1≤l≤n .C Additional Performance Analysis   C.1 Micro - averaged Metrics   In the main paper we reported macro - averaged F1   scores for each dataset . To provide a more com-   plete comparison of performance changes , here we   report micro - averaged F1 scores in Tables 7 - 10 .   Micro - averaged metrics lump together all named   entities without distinguishing their types , and   therefore the majority types have more influence   on these metrics than minority types . Overall , the   trend is consistent with the macro - averaged met-   rics . Sentence - level resampling methods tend to   deliver more stable gains and generally outperform   baseline methods .   C.2 Per - Entity - Type Metrics   To further examine the impacts of our method on   entity types , we also report per - entity - type preci-   sion , recall , and F1 scores for each dataset in Tables   11 - 14 . We compare the performance of using the   original corpus and a representative of our meth-   ods ( sCR ) . Red up - arrows ( ↑ ) means sCR obtains   better precision / recall / F1 score compared to using   the original corpus .   Here we observe that at the level of entity types ,   either precision and recall simultaneously improve   or drop , or precision improves at a slight cost of   recall . It is rare that recall improves at the cost of   precision ( only the GPE type in Table 13 ) . This   pattern indicates that the BERT - CRF model trained   on the original corpus has many ‘ false negatives ’   ( tagging entity tokens as “ other ” ) . In other words ,   the model is extremely reluctant to predict non-   other entity types . Our sentence - level resampling   methods encourage the model to correctly assign   entity types to more tokens . Another trend is that   improvements on small corpora ( AnEM , WNUT ,   GMB subset ) are more salient than on large corpus   ( CoNLL ) . Note that sentence resampling does not   necessarily favor minority entity types as all entity   types are very rare already , compared to the Other   tokens ( see the last column of Tables 11 - 14 , “ Token   % ” ) .216321642165