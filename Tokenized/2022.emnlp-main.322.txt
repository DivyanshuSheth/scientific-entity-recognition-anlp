  Chenghao Yang , Xuezhe MaUniversity of ChicagoUniversity of Southern California   yangalan1996@gmail.com , xuezhema@isi.edu   Abstract   Fine - tuning over large pretrained language   models ( PLMs ) has established many state-   of - the - art results . Despite its superior perfor-   mance , such fine - tuning can be unstable , re-   sulting in significant variance in performance   and potential risks for practical applications .   Previous works have attributed such instabil-   ity to the catastrophic forgetting problem in   the top layers of PLMs , which indicates itera-   tively fine - tuning layers in top - down manner is   a promising solution . In this paper , we first   point out that this method does not always   work out due to different convergence speeds   of different layers / modules . Inspired by this   observation , we propose a simple component-   wise gradient norm clipping method to adjust   the convergence speed for different compo-   nents . Experiment results demonstrate that our   method achieves consistent improvements in   terms of generalization performance , conver-   gence speed and training stability . The code-   base can be found at https://github.com/   yangalan123 / FineTuningStability .   1 Introduction   Fine - tuning over large pretrained language models   ( PLMs ) , which achieved remarkable performance   over various benchmarks , has become the de facto   paradigm for several current natural language pro-   cessing ( NLP ) systems . However , fine - tuning can   be unstable in terms of significant variance in met-   rics , resulting in even worse - than - random failed   models ( Devlin et al . , 2019 ; Lee et al . , 2019 ; Dodge   et al . , 2020 ; Mosbach et al . , 2020 ) .   Catastrophic forgetting ( Kirkpatrick et al . , 2017 )   during fine - tuning of PLMs is one common expla-   nation for this instability ( Lee et al . , 2019 ) , i.e. ,   PLMs may lose their rich domain - agnostic knowl-   edge acquired by language model pretraining in the   process of fine - tuning . Through layer - replacement   experiments between pretrained models and fine-   tuned models , Mosbach et al . ( 2020 ) further con - Figure 1 : Fine - tuning performance over time on RTE   datasets . Here we fine - tune over BERT - large - uncased   model ( Devlin et al . , 2019 ) . In each iteration , we train   the model for 3epochs and the errorbar is plotted based   on5different runs .   nected the catastrophic forgetting problem to the   optimization problem on top layers .   These findings give rise to a straightforward way   to enhance the fine - tuning stability : how about fine-   tuning the model from top to bottom to reduce the   parameter changes and hence mitigate the catas-   trophic forgetting problem ? This is reminiscent of   the gradual unfreezing ( Howard and Ruder , 2018 ) ,   which does layer - wise top - down fine - tuning and   unfreezing new layers only when the layers above   have been fine - tuned . Therefore , the newly un-   frozen layer would only be tuned for a slightly eas-   ier optimization problem at each iteration , leading   to much fewer changes to the parameters .   However , based on a comprehensive case study   of the gradual unfreezing method , we obtained em-   pirical observations beyond our expectations ( § 2 ) .   Our analysis further reveals a possible reason : dif-   ferent components ( e.g. , feed - forward networks at   different layers , fully connected matrices and bi-   ases at output layer ) converge at varying speeds .   Thus , components in upper layers , which have con-   verged to local optima , can not easily be fine - tuned4854with newly unfrozen parameters .   Based on this observation , we propose a simple   component - wise gradient clipping method to sta-   bilize the fine - tuning process ( § 3 ) . This method   achieves significant empirical improvements of   fine - tuning stability in terms of the variance and   the failed run percentage over three tasks ( § 4 ) . In   summary we make the following contributions :   1.We find that component - wise convergence   speed divergences is the key challenge in fine-   tuning stability , based on the case study of the   gradual unfreezing method .   2.Based on our observation , we propose a   new simple component - wise gradient clip-   ping method to help stabilize fine - tuning ,   which achieves empirical improvements of   fine - tuning stability over previous methods .   2 A Bitter Case Study : Layer - wise   Gradual Unfreezing   Mosbach et al . ( 2020 ) attributed the instability prob-   lem in fine - tuning process to the catastrophic for-   getting in the top layers , through a layer replace-   ment experiment between pretrained and fine - tuned   models . Following this empirical observation , the   instability problem might be mitigated if we can   minimize the edits to the pretrained model param-   eters , especially in top layers . This inspires us to   mitigate the instability problem via gradual unfreez-   ing ( GU , Howard and Ruder ( 2018 ) ) .   Specifically , suppose we are working with   a model Mwith Llayers parameterized by   { θ,1≤i≤L } . GU tunes MforLiterations .   Atk - th ( kstart from 0 ) iteration , we only tune a   subset of parameters R={θ , L−k≤i≤L } ,   where θ(L−k+ 1≤i≤L)is also tuned in   k−1 - th iteration . In each iteration , we tune the   parameter for Eepochs , where Eis large enough   for convergence . Detailed algorithm is shown in   Algorithm 1 at Appendix A.   Failure of Gradual Unfreezing From Fig . 1 ,   the accuracy of gradual unfreezing is significantly   worse than full fine - tuning , although it indeed   achieves smaller update to pretrained model pa-   rameters compared with full fine - tuning ( Fig . 2a).Convergence Racing between Parameters To   investigate the reason behind this unsatisfying per-   formance of GU , we plot the component - wise max-   imum update ( measured by component - wise maxi-   mum rooted mean squared difference , as different   layers can have multiple components with differ-   ent dimensions ) at each layer in Fig . 2b . Clearly ,   from the very beginning , the parameter updates for   both early - tuned parameters and newly - unfrozen   parameters have quickly diminished in GU so not   many updates happen later when new parameters   join . Based on this observation , we hypothesize   that the failure of GU is because the early - tuned   parameters have already converged in early itera-   tions and can not be re - activated later to adapt for   newly - unfrozen parameters .   To verify this hypothesis , we simply modify GU   to stop using early - tuned parameters in the previous   iteration and instead copy the weight from the pre-   trained BERT model . For simplicity , we term this   method as “ GU ( restart ) ” . Note that full fine - tuning   is just the last iteration of GU ( restart ) when all lay-   ers are unfrozen . We plot GU ( restart ) performance   in Fig . 1 , and observe that GU ( restart ) achieves   consistently much better performance than GU .   However , GU ( restart ) is much more unstable than   GU , and the best performance is only reached when   almost all layers have been tuned .   Unbalanced Gradients across Parameters To   investigate the cause of the convergence racing   problem , we analyze the distribution of parameters ’   gradients from different components , by plotting   the gradient norm in Fig . 3.Here we follow the   terminology in previous work ( Dodge et al . , 2020 ;   Mosbach et al . , 2020 ) that , if a fine - tuned model   checkpoint ca n’t beat the majority classifier , then   it is a “ Failed Run ” and “ Success Run ” otherwise .   Comparing Fig . 3b and Fig . 3a , we can find that   convergence racing still exists under normal full   fine - tuning . The success run wins by making all pa-   rameters update roughly following the same trends   and making gradient norms well - bounded .   Discussion on Fine - Tuning Stability From the   case study over GU , we observe that there is a con-   vergence racing problem between parameters of   model components , which would lead to incompe-4855   tent or unstable performance . Based on existing   studies , we argue that a robust PLM fine - tuning   method should satisfy the following principles :   1.Updating the full set of parameters , not just a   subset of layers .   2.No significant parameter updates to avoid   catastrophic forgetting problem .   3.Adjusting the convergence speed for parame-   ters to mitigate the convergence racing .   Mosbach et al . ( 2020 ) proposed to use small learn-   ing rate ( e.g. , 1e ) to mitigate catastrophic forget-   ting , together with longer fine - tuning process . How - ever , unbalanced gradients across different compo-   nents is another factor for catastrophic forgetting   andconvergence racing , which is not resolved by   only using small learning rates .   3 The Component - Wise Gradient Norm   Clipping Method   Bearing these principles in mind , we propose the   component - wise gradient norm clipping ( CWGNC )   method . The components are different parameters   serving different functionalities . For example , in   a Transformer - based neural architecture , compo-   nents can be the weight matrices of Key - Query-   Value in Transformer architectures , the weight and4856ApproachRTE MRPC COLA   Std Mean Max Std Mean Max Std Mean Max   Devlin et al . ( 2019 ) 4.5 50.9 67.5 3.9 84.0 91.2 25.6 45.6 64.6   + bias correction 4.0 70.6 75.5 2.1 89.2 92.2 11.1 59.2 64.1   Lee et al . ( 2019 ) 7.9 65.3 74.4 3.8 87.8 91.8 20.9 51.9 64.0   Mosbach et al . ( 2020 ) 2.7 67.3 71.1 0.8 90.3 91.7 1.8 62.1 65.3   CWGNC 1.3 73.1 75.1 0.6 90.5 91.5 1.7 62.2 65.0   bias terms in feed - forward layers , etc . For those op-   timizers which maintain first - order and/or second-   order bias correction terms ( e.g. , Adam ( Kingma   and Ba , 2015 ) and AdamW ( Loshchilov and Hutter ,   2018 ) as the popular optimizer in PLM literature ) ,   our proposed norm clipping operation happens be-   fore the bias correction terms computation and does   not interfere with normal bias correction process .   By clipping the gradient norm of each compo-   nent individually , we aim to balance the distribu-   tion of gradients across different components to   adjust their convergence speed , hence mitigating   the convergence racing problem .   4 Experiments   To evaluate our CWGNC method , we follow Mos-   bach et al . ( 2020 ) to run our CWGNC method over   25different runs and report aggregated results on   the validation set of three different datasets : RTE   ( Acc ) , MRPC ( F1 ) and COLA ( MCC ) . We report   the standard deviation ( Std ) , averaged performance   ( Mean ) and maximum performance ( Max ) . Hy-   perparameters are tuned on held - out data . More   implementation details are in Appendix B.   Baseline Setting For baselines , we first consider   the original BERT paper reported results ( Devlin   et al . , 2019 ) and we run original BERT fine - tuning   with bias correction to make a stronger baseline fol-   lowing the observation in ( Mosbach et al . , 2020 ) .   We also compare to Mixout regularization meth-   ods ( Lee et al . , 2019 ) and “ simple but hard - to - beat   baseline ” proposed by Mosbach et al . ( 2020 ) .   Experiment Results Experiment results are   shown in Table 1 . Here we can see our CWGNC   achieves significantly better performance in terms   of averaged performance and standard deviation .   Compared with previous state - of - the - art results   from ( Mosbach et al . , 2020 ) , our method only   needs to tune 5epochs and does not need to waitfor20epochs even on these small datasets so our   method indeed converges faster .   Our method is also robust to a wide range of   the selection of gradient norm clipping thresholds .   We show this in Table 2 on COLA dataset as we   find that fine - tuning methods are particularly un-   stable on COLA dataset . Here we see that within   a reasonable range of threshold ( < 1 ) , the model   performance would be mostly maintained and the   standard deviation is well controlled . If we use a   significantly larger threshold ( > = 1 ) , less control   is enforced by CWGNC and it will degrade to nor-   mal full model fine - tuning if we further increase   the threshold .   5 Conclusion   In this paper , we investigate the instability problem   of fine - tuning large pretrained language models . In-   spired by previous works , we first experiment with   gradual unfreezing methods , which should help   minimize the updates in top layers and ease the   catastrophic forgetting problem . However , further   experiment results do not support that and we find   it is because there is a convergence racing issue   between different parameters , namely that early-   converged parameters can limit the search space   for other parameters . Based on this finding , we   propose to do component - wise gradient norm clip-   ping and achieve significant improvement on aver-   aged performance , smaller standard deviation and4857quicker convergence speed . Our method is robust   to the selection of gradient norm clipping thresh-   old . In the future , we will try to study whether the   component racing problem also exists in pretrained   language models with different sizes and different   pretraining methods .   6 Limitations   In this paper we mainly work with one partic-   ularly popular large pretrained language model   BERT ( Devlin et al . , 2019 ) . While we believe our   empirical investigation and conclusion is widely   applicable to a wide range of current transformer-   based large pretrained language models , more ex-   periments and theoretical explanations are needed   for further research . Also , due to computational   resources limitation , we can not investigate whether   the most recent large models like T0 ( Sanh et al . ,   2022 ) are stable under fine - tuning . We follow   the evaluation protocols in previous works ( Dodge   et al . , 2020 ; Lee et al . , 2019 ; Mosbach et al . , 2020 )   to investigate the instability of fine - tuning on small   datasets including COLA , RTE and MRPC . But for   real challenging low - resource situation , we believe   it can be more complicated and more investigation   is needed .   Acknowledgements   This material is based on research sponsored by Air   Force Research Laboratory ( AFRL ) under agree-   ment number FA8750 - 19 - 1 - 1000 . The U.S. Gov-   ernment is authorized to reproduce and distribute   reprints for Government purposes notwithstanding   any copyright notation therein . The views and con-   clusions contained herein are those of the authors   and should not be interpreted as necessarily rep-   resenting the official policies or endorsements , ei-   ther expressed or implied , of Air Force Laboratory ,   DARPA or the U.S. Government .   References4858A Gradual Unfreezing Algorithm   The gradual unfreezing algorithm we implemented   following Howard and Ruder ( 2018 ) is shown in   Algorithm 1 .   Algorithm 1 Gradual Unfreezing Fine - Tuning   Require : A multi - layer Transformer PLM Mwith   itsL - layer parameters { θ,1≤i≤L } . The   maximum number of iterations T , the dataset   D , the optimizer Opt , the number of epochs for   each iteration E.   Ensure : 1≤T≤L   ˆR←ϕ ▷ ˆRrepresents to - be - tuned   parameters at l - th iteration .   while T≤Ldo   ˆR←ˆR∪ { θ }   forj= 1→Edo   L ← Forward ( M , D )   G←Backward ( L , L−T+ 1 , L )   ▷Only needs to compute the gradient   for top- Tlayers   Update ( Opt , G , ˆR )   ▷Apply Gover ˆR   Replace ( M,ˆR , L−T+ 1 , L )   ▷Replace the updated layers back to   make sure the updated parameters are involved   in next epoch forward process   end for   T←T+ 1   end while   B Implementation Details   Our codebase is based on Huggingface Transform-   ers ( Wolf et al . , 2020 ) example fine - tuning scripts   and will be released later . We tune models using   our method for 5epochs . For weight decay and   warm - up steps , we follow the settings original fine-   tuning method as described in ( Devlin et al . , 2019 ) .   We here report the result with clipping threshold   0.05as we empirically find it works better on held-   out dataset . We also show later that our method is   actually pretty robust to a wide range of threshold   picking in Table 2 .   C Gradient Update Measured by Cosine   Similarities   In the main text we measures the update via root-   mean - square difference . Here we show that if mea-   sured by cosine - similarity , we would obtain similarconclusion . Here , Fig . 4 is the cosine similarity   version for Fig . 2b . Fig . 5 is the cosine similar-   ity version for Fig . 2a . Note that because smaller   cosine similarity indicates more changes , in con-   trast to square - mean - root difference , we use the   component - wise minimum cosine similarity to rep-   resent the update at each layer.4859