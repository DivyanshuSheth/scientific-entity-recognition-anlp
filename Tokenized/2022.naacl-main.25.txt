  Sajad Sotudeh andNazli Goharian   IR Lab , Georgetown University , Washington DC 20057 , USA   { sajad , nazli}@ir.cs.georgetown.edu   Abstract   Many scientific papers such as those in arXiv   and PubMed data collections have abstracts   with varying lengths of 50–1000 words and   average length of approximately 200 words ,   where longer abstracts typically convey more   information about the source paper . Up to re-   cently , scientific summarization research has   typically focused on generating short , abstract-   like summaries following the existing datasets   used for scientific summarization . In domains   where the source text is relatively long - form ,   such as in scientific documents , such summary   is not able to go beyond the general and coarse   overview and provide salient information from   the source document . The recent interest to   tackle this problem motivated curation of scien-   tific datasets , arXiv - Long and PubMed - Long ,   containing human - written summaries of 400-   600 words , hence , providing a venue for re-   search in generating long / extended summaries .   Extended summaries facilitate a faster read   while providing details beyond coarse infor-   mation . In this paper , we propose T , an   extractive summarizer that utilizes the introduc-   tory information of documents as pointers to   their salient information . The evaluations on   two existing large - scale extended summariza-   tion datasets indicate statistically significant   improvement in terms of R and average   R ( F1 ) scores ( except in one case ) as   compared to strong baselines and state - of - the-   art . Comprehensive human evaluations favor   our generated extended summaries in terms of   cohesion and completeness .   1 Introduction   Over the past few years , summarization task has   witnessed a huge deal of progress in extractive ( Nal-   lapati et al . , 2017 ; Liu and Lapata , 2019 ; Yuan   et al . , 2020 ; Cui et al . , 2020 ; Jia et al . , 2020 ; Feng   et al . , 2018 ) and abstractive ( See et al . , 2017 ; Co-   han et al . , 2018 ; Gehrmann et al . , 2018 ; Zhang   et al . , 2019 ; Tian et al . , 2019 ; Zou et al . , 2020)Figure 1 : A truncated human - written extended sum-   mary . Top box : introductory information , bottom   box : non - introductory information . Colored spans are   pointers from introductory sentences to associated non-   introductory detailed sentences .   settings . Many scientific papers such as those in   arXiv and PubMed ( Cohan et al . , 2018 ) posses ab-   stracts of varying length , ranging from 50 to 1000   words and average length of approximately 200   words . While scientific paper summarization has   been an active research area , most works ( Cohan   et al . , 2018 ; Xiao and Carenini , 2019 ; Cui and Hu ,   2021 ; Rohde et al . , 2021 ) in this domain have fo-   cused on generating typical short and abstract - like   summaries ( Chandrasekaran et al . , 2020 ) . Short   summaries might be adequate when the source text   is of short - form such as those in news domain ;   however , to summarize longer documents such as   scientific papers , an extended summary including   400–600 terms on average , such as those found in   extended summarization datasets of arXiv - Long   and PubMed - Long , is more appealing as it conveys   more detailed information.325Extended summary generation has been of re-   search interest very recently . Chandrasekaran   et al . ( 2020 ) motivated the necessity of generating   extended summaries through LongSumm shared   task . Long documents such as scientific papers   are usually framed in a specific structure . They start   by presenting general introductory information .   This introductory information is then followed by   supplemental information ( i.e. , non - introductory )   that explain the initial introductory information in   more detail . Similarly , as shown in Figure 1 , this   pattern holds in a human - written extended sum-   mary of a long document , where the preceding sen-   tences ( top box inside Figure 1 ) are introductory   sentences and succeeding sentences ( bottom box   inside Figure 1 ) are explanations of the introduc-   tory sentences . In this study , we aim to guide the   summarization model to utilize the aforementioned   rationale in human - written summaries . We con-   sider introductory sentences as those that appear   in the first section of paper with headings such as   Introduction , Overview , Motivations , and so forth .   As such , all other parts of paper and their sentences   are considered as non - introductory ( i.e. , supple-   mentary ) . We use these definitions in the reminder   of this paper .   Herein , we approach the problem of extended   summary generation by incorporating the most   important introductory information into the sum-   marization model . We hypothesize that incorpo-   rating such information into the summarization   model guides the model to pick salient detailed   non - introductory information to augment the final   extended summary . The importance of the role   of introduction in the scientific papers was earlier   presented in ( Teufel and Moens , 2002 ; Arma ˘gan ,   2013 ; Jirge , 2017 ) where they showed such infor-   mation provides clues ( i.e. pointers ) to the objec-   tives and experiments of studies . Similarly , Boni   et al . ( 2020 ) conducted a study to show the impor-   tance of introduction part of scientific papers as   its relevance to the paper ’s abstract . To validate   our hypothesis , we test the proposed approach on   two publicly available large - scale extended summa-   rization datasets , namely arXiv - Long and PubMed-   Long . Our experimental results improve over the   strong baselines and state - of - the - art models . In   short , the contributions of this work are as follows:•A novel multi - tasking approach that incorpo-   rates the salient introductory information into   the extractive summarizer to guide the model   in generating a 600 - term ( roughly ) extended   summary of a long document , containing the   key detailed information of a scientific paper .   •Intrinsic evaluation that demonstrates statis-   tically significant improvements over strong   extractive and abstractive summarization base-   lines and state - of - the - art models .   •An extensive human evaluation which reveals   the advantage of the proposed model in terms   of cohesion and completeness .   2 Related Work   Summarizing scientific documents has gained a   huge deal of attention from researchers , although   it has been studied for decades . Neural efforts   in scientific text have used specific characteris-   tics of papers such as discourse structure ( Cohan   et al . , 2018 ; Xiao and Carenini , 2019 ) and citation   information ( Qazvinian and Radev , 2008 ; Cohan   and Goharian , 2015 , 2018 ) to aid summarization   model . While prior work has mostly covered the   generation of shorter - form summaries ( approx . 200   terms ) , generating extended summaries of roughly   600 terms for long - form source documents such   as scientific papers has been motivated very re-   cently ( Chandrasekaran et al . , 2020 ) .   The proposed models for the extended summary   generation task include jointly learning to predict   sentence importance and sentence section to ex-   tract top sentences ( Sotudeh et al . , 2020 ) ; utiliz-   ing section - contribution computations to pick sen-   tences from important section for forming the fi-   nal summary ( Ghosh Roy et al . , 2020 ) ; identify-   ing salient sections for generating abstractive sum-   maries ( Gidiotis et al . , 2020 ) ; ensembling of ex-   traction and abstraction models to form final sum-   mary ( Ying et al . , 2021 ) ; an extractive model with   TextRank algorithm equipped with BM25 as sim-   ilarity function ( Kaushik et al . , 2021 ) ; and incor-   porating sentences embeddings into graph - based   extractive summarizer in an unsupervised man-   ner ( Ramirez - Orta and Milios , 2021 ) . Unlike these   works , we do not exploit any sectional nor citation   information in this work . To the best of our knowl-   edge , we are the first at proposing the novel method   of utilizing introductory information of the scien-   tific paper to guide the model to learn to generate326summary from the salient and related information .   3 Background : Contextualized language   models for summarization   Contextualized language models such as B(De-   vlin et al . , 2019 ) , and RB ( Liu et al . , 2019 )   have achieved state - of - the - art performance on a   variety of downstream NLP tasks including text   summarization . Liu and Lapata ( 2019 ) were the   first to fine - tune a contextualized language model   ( i.e. , B ) for the summarization task . They   proposed BS — a fine - tuning scheme for   text summarization — that outputs the sentence   representations of the source document ( we use   the term source and source document interchange-   ably , referring to the entire document ) . The B-   SEmodel , which is built based on B-   S , was proposed for the extractive summariza-   tion task . It utilizes the representations produced   byBS , passes them through Transformers   encoder ( Vaswani et al . , 2017 ) , and finally uses   a linear layer with Sigmoid function to compute   copying probabilities for each input sentence . For-   mally , let l , l , ... , lbe the binary tags over the   source sentences x={sent , sent , ... , sent}of   a long document , in which nis the number of sen-   tences in the paper . The BSEnetwork   runs over the source documents as follows ( Eq . 1 ) ,   h = BertSum ( x )   h = Encoder(h )   p = σ(Wh+b)(1 )   where handhare the representations of source   sentences encoded by BSand Trasformers   encoder , respectively . Wandbare trainable pa-   rameters , and pis the probability distribution over   the source sentences , signifying extraction copy   likelihood . The goal of this network is to train a net-   work that can identify the positive sets of sentences   as the summary . To prevent the network from se-   lecting redundant sentences , BSuses Tri-   gram Blocking ( Liu and Lapata , 2019 ) for sentence   selection in inference time . We refer the reader to   the main paper for more details .   4 T : Intro - guided Summarization   In this section , we describe our methodology to   tackle the extended summary generation task . Our   approach exploits the introductory information .   of the paper as pointers to salient sentences within   it , as shown in Figure 2 . It is ultimately expected   that the extractive summarizer is guided to pick   salient sentences across the entire paper .   The detailed illustration of our model is shown   in Figure 3 . To aid the extractive summarization   model ( i.e. , right - hand box in Figure 3 ) which takes   in source sentences of a scientific paper , we utilize   an additional BSencoder called Introduc-   tory encoder ( left - hand box in Fig . 3 ) that receives   x = { sent , sent , ... , sent } , with mbeing   the number of sentences in introductory section .   The aim of adding second encoder in this frame-   work is to identify the clues in the introductory   section which point to the salient supplementary   sentences . The BSnetwork computes the   extraction probabilities for introductory sentences   as follow ( same way as in Eq . 1 ) ,   ˜h = BertSum ( x )   ˜h = Encoder(˜h )   ˜p = σ(W˜h+b)(2 )   in which ˜h , and ˜hare the introductory sentence   representations by BS , Transformers en-   coder , respectively . ˜pis the introductory sentence   extraction probabilities . Wandbare trainable   matrices .   After identifying salient introductory sentences ,   the representations associated with them are re-   trieved using a pooling function and further used to   guide the first task ( i.e. , right - hand side in Figure   3 ) as follows ,   ˜h = Select ( ˜h,˜p , k )   ˆh = MLP(˜h)(3)327   where Select ( · ) is a function that takes in all in-   troductory sentence representations ( i.e. , ˜h ) , and   introductory sentence probabilities ˜p . It then out-   puts the representations associated with top kin-   troductory sentences , sorted by ˜p . To extract top   introductory sentences , we first sort ˜hvectors based   on their computed probabilities ˜pand then we pick   up top khidden vectors ( i.e. , ˜h ) that has the high-   est probability . MLPis a multi - layer perceptron   that takes in concatenated vector of top introduc-   tory sentences and projects it into a new vector   called ˆh .   At the final stage , we concatenate the trans-   formed introductory top sentence representations   ( i.e. , ˆh ) with each source sentence representations   from Eq . 1 ( i.e. , hwhere ishows the ith paper   sentence ) and process them to produce a resulting   vector rwhich is intro - aware source sentence hid-   den representations . After processing the resulting   vector through a linear output layer ( with Wand   bas trainable parameters ) , we obtain final intro-   aware sentence extraction probabilities ( i.e. , p ) as   follows , r = MLP(h;ˆh )   p = σ(Wr+b)(4 )   in which MLPis a multi - layer perceptron , influ-   encing the knowledge from introductory sentence   extraction task ( i.e. , t)into the source sentence ex-   traction task ( i.e. , t ) . We train both tasks through   our end - to - end system jointly as follows ,   ℓ = ( α)ℓ+ ( 1−α)ℓ ( 5 )   where ℓ , and ℓare the losses computed for in-   troductory sentence extraction and source sentence   extraction tasks , αis the regularizing parameter   that balances the learning process between two   tasks , and ℓ is the total computed loss that is   optimized during the training .   5 Experimental Setup   In this section , we explain the datasets , baselines ,   and preprocessing and training parameters .   5.1 Dataset   We use two publicly available scientific extended   summarization datasets ( Sotudeh et al . , 2021).328 - arXiv - Long : A set of arXiv scientific pa-   pers containing papers from various scientific   domains such as physics , mathematics , computer   science , quantitative biology . arXiv - Long is in-   tended for extended summarization task and was   filtered from a larger dataset i.e. , arXiv ( Cohan   et al . , 2018 ) for the summaries of more than 350   tokens . The ground - truth summaries ( i.e. , ab-   stract ) are long , with the average length of 574   tokens . It contains 7816 ( train ) , 1381 ( validation ) ,   and 1952 ( test ) papers .   -PubMed - Long : A set of biomedical scien-   tific papers from PubMed with average summary   length of 403 tokens . This dataset contains 79893   ( train ) , 4406 ( validation ) , and 4402 ( test ) scien-   tific papers .   -LongSumm : The recently proposed Long-   Summ dataset for a shared task ( Chandrasekaran   et al . , 2020 ) contains 2236 abstractive and ex-   tractive summaries for training and 22 papers for   the official test set . We report a comparison with   BSEM using this data in Table   2 . However , as the official test set is blind , our   experimental results in Table 1 do not use this   dataset .   5.2 Baselines   We compare our model with two strong non - neural   systems , and four state - of - the - art neural summa-   rizers . We use all of these baselines for the pur-   pose of extended summary generation whose docu-   ments hold different characteristics in length , writ-   ing style , and discourse structure as compared to   documents in the other domains of summarization .   -LSA ( Steinberger and Je ¨zek , 2004 ): an extrac-   tive vector - based model that utilizes Singular   Value Decomposition ( SVD ) to find the semanti-   cally important sentences .   -LR ( Erkan and Radev , 2004 ): a widely   adopted extractive summarization baseline that   utilizes a graph - based approach based on eigen-   vector centrality to identify the most salient sen-   tences .   -BSE(Liu and Lapata , 2019 ): a con-   textualized summarizer fine - tuned for summa-   rization task , which encodes input sentence rep-   resentations , and then processes them through   a multi - layer Transformers encoder to obtaindocument - level sentence representation . Finally ,   a linear output layer with Sigmoid activation   function outputs a probability distribution over   each input sentence , denoting the extent to which   they are probable to be extracted .   -BSE - I ( Liu and Lapata , 2019 ):   aBSEmodel that only runs on the   introductory sentences as the input , and extracts   the salient introductory sentences as the summary .   -BSEM ( Sotudeh et al . , 2021 ):   an extension of the BSEmodel that   incorporates an additional linear layer with Sig-   moid classifier to output a probability distribution   over a fixed number of pre - defined sections that   an input sentence might belong to . The additional   network is expected to predict a single section   for an input sentence and is trained jointly with   BSEmodule ( i.e. , sentence extractor ) .   -B ( Lewis et al . , 2020 ): a state - of - the - art ab-   stractive summarization model that makes use   of pretrained encoder and decoder . B can   be thought of as an extension of BSin   which merely encoder is pre - trained , but decoder   is trained from scratch . While our model is an ex-   tractive one , at the same time , we find it of value   to measure the abstractive model performance in   the extended summary generation task .   5.3 Preprocessing , parameters , labeling , and   implementation details   We used the open implementation of B-   SEwith default parameters . To implement   the non - neural baseline models , we utilized Sumy   python package . Longformer model ( Beltagy   et al . , 2020 ) is utilized as our contextualized lan-   guage model for running all the models due to its   efficacy at processing long documents . For our   model , the cross - entropy loss function is set for   two tasks ( i.e. , t : source sentence extraction and   t : introductory sentences extraction in Figure 3 )   and the model is optimized through multi - tasking   approach as discussed in Section 3 . The model with   the highest R -2on validation set is selected   for inference . The validation is performed every   2k training steps . α(in Eq . 5 ) is set to be 0.5 ( em-   pirically determined ) . Our model includes 474M329trainable parameters , trained on dual GeForce GTX   1080Ti GPUs for approximately a week . We use   k= 5 for arXiv - Long , k= 8 for PubMed - Long   datasets ( Eq . 3 ) . We make our model implementa-   tion as well as sample summaries publicly available   to expedite ongoing research in this direction .   A two - stage labeling approach was employed   to identify ground - truth introductory and non-   introductory sentences . In the first stage , we used a   greedy labeling approach ( Liu and Lapata , 2019 )   to label sentences within the first section of a given   paper ( i.e. , labeling introductory sentences ) with   respect to their R overlapwith the ground-   truth summary ( i.e. , abstract ) . In the second stage ,   the same greedy approach was exploited over the   rest of sentences ( i.e. , non - introductory)with re-   gard to their R overlap with the identified   introductory sentences in the first stage . Our choice   ofR -2andR -Lis based on the fact that   these express higher similarity with human judg-   ments ( Cohan and Goharian , 2016 ) . We continued   the second stage until a fixed length of the sum-   mary was reached . Specifically , the fixed length of   positive labels is set to be 15 for arXiv - Long , and   20 for PubMed - Long datasets as these achieved the   highest oracle R scores in our experiments .   6 Results   6.1 Experimental evaluation   The recent effort in extended summarization and its   shared task of LongSumm ( Chandrasekaran et al . ,   2020 ) used average R ( F1 ) to rank the par-   ticipating systems , in addition to commonly - used   R -scores . Table 2 shows the performance   of the participated systems on the blind test set .   As shown , BSEM model outper-   forms other models by a large margin ( i.e. , with   relative improvements of 6 % and 3 % on R -   1and average R ( F1 ) , respectively ) ; hence ,   we use the best - performing in terms of F1 ( i.e. ,   BSEM model ) in our experiments .   Tables . 1 presents our results on the test sets of   arXiv - Long and PubMed - Long datasets , respec-   tively . As observed , our model statistically sig-   nificantly outperforms the state - of - the - art systems   on both datasets across most of the R vari - ants , except R -Lon PubMed - Long . The im-   provements gained by our model validates our hy-   pothesis that incorporating the salient introductory   sentence representations into the extractive summa-   rizer yields a promising improvement . Two non-   neural models ( i.e. , LSA andLR ) under-   perform the neural models , as expected . Compar-   ing the abstractive model ( i.e. , B ) with extrac-   tive neural ones ( i.e. , BSEandB-   SEM ) , we see that while there is rel-   atively a smaller gap in terms of R -1 , the   gap is larger for R -2 , and R -L. Inter-   estingly , in the case of B , we found that gen-   erating extended summaries is rather challenging   for abstractive summarizers . Current abstractive   summarizers including B have difficulty in ab-   stracting very detailed information , such as num-   bers , and quantities , which hurts the faithfulness   of the generated summaries to the source . This   behavior has a detrimental effect , specifically , on   R -2andR -Las their high correlation   with human judgments in terms of faithfulness has   been shown ( Pagnoni et al . , 2021 ) . Comparing   the extractive BSEandBSE-   M models , while BSM Eis   expected to outperfom BSE , it is ob-   served that they perform almost similarly , with   small ( i.e. , insignificant ) improved metrics . This   might be due to the fact that BSEM   works out - of - the - box when a handful amount of   sentences are sampled from diverse sections to   form the oracle summary as also reported by its   authors . However , when labeling oracle sentences   in our framework ( i.e. , Intro - guided labeling ) , there   is no guarantee that the final set of oracle sen-   tences are labeled from diverse sections . Over-   all , our model achieves about 1.4 % , 2.4 % , 3.5 %   ( arXiv - Long ) , and 1.0 % , 2.5 % , 1.3 % ( PubMed-   Long ) improvements across R score vari-   ants ; and 2.2 % ( arXiv - Long ) , 1.4 % ( PubMed-   Long ) improvements over F1 , compared to the   neural baselines ( i.e. , BSEandB-   SEM ) . While comparing our model   with BSE - I , we see the vital effect   of adding second encoder at finding supplemen-   tary sentences across non - introductory sections ,   where our model gains relative improvements of   9.62%-26.26%-16.09 % and 9.40%-5.27%-9.99 %   forR -1 , R -2 , R -Lon arXiv-   Long and PubMed - Long , respectively . In fact , the   sentences that are picked as summary from the in-330arXiv - Long PubMed - Long   Model   O 53.35 24.40 23.65 33.80 52.11 23.41 25.42 33.65   BSE - I 44.88 15.99 19.14 26.25 45.08 20.08 21.52 28.89   LSA 43.23 13.47 17.50 24.73 44.47 15.38 19.17 26.34   LR 43.73 15.01 18.62 25.41 48.63 20.37 22.49 30.50   BSE 48.42 19.71 21.47 29.87 48.82 20.89 23.37 31.03   BSEM 48.52 19.66 21.42 29.87 48.85 20.71 23.29 30.95   B 48.12 15.30 20.80 28.07 48.32 17.33 21.42 29.87   T(Ours ) 49.20∗20.19∗22.22∗30.54 49.32∗21.41∗23.67 31.47   troduction section are not comprehensive as such   they are clues to the main points of the paper . The   other important sentences are picked from the sup-   plementary parts ( i.e. , non - introductory ) of the pa-   per .   6.2 Human evaluation   While our model statistically significantly improves   upon the state - of - the - art baselines in terms of   R scores , a few works have reported the low   correlation of R with human judgments ( Liu   and Liu , 2008 ; Cohan and Goharian , 2016 ; Fab-   bri et al . , 2021 ) . In order to provide insights into   why and how our model outperforms the best-   performing baselines , we perform a manual anal-   ysis of our system ’s generated summaries , B-   SE ’s , and BSEM ’ s. For the   sake of evaluation , two annotators were asked to   manually evaluate two sets of 40 papers ’ ground-   truth abstracts ( 40 for arXiv - Long , and 40 for   PubMed - Long ) with their generated extended sum-   maries ( baselines ’ and ours ) to gain insights into   qualities of each model . Annotators were Electrical   Engineering and Computer Science PhD students   and familiar with principles of reading scientificpapers . Samples were randomly selected from the   test set , one from each 40 evenly - spaced bins sorted   by the difference of R -Lbetween two experi-   mented systems .   The evaluations were performed according to   two metrics : ( 1 ) Cohesion : whether the ordering   of sentences in summary is cohesive , namely sen-   tences entail each other . ( 2 ) Completeness : whether   the summary covers all salient information pro-   vided in the ground - truth summary . To prevent bias   in selecting summaries , the ordering of system-   generated summaries were shuffled such that it   could not be guessed by the annotators . Annotators   were asked to specify if the first system - generated   summary wins / loses or ties with the second system-   generated summary in terms of qualitative metrics .   It has to be mentioned that since our model is purely   extractive , it does not introduce any fact that is un-   faithful to the source .   Our human evaluation results along with Co-   hen ’s kappa ( Cohen , 1960 ) inter - rater agreements   are shown in Table 3 ( agr . column ) . As shown ,   our system ’s generated summaries improve com-   pleteness and cohesion in over 40 % for most of   the cases ( 6 out of 8 for win cases ) . Specifi-   cally , when comparing with BSE , we   see that 68 % , 80 % ( arXiv - Long ) ; and 60 % , 66 %   ( PubMed - Long ) of sampled summaries are at least   as good as or better than the corresponding base-   line ’s generated summaries in terms of cohesion   and completeness , respectively . Overall , across   two metrics for BSEandBSE-   M , we gain relative improvements over the   baselines : 25.6 % , 19.0 % ( cohesion ) , and 56.5%,331   ( a ) ( b )   ( a ) ( b )   46.7 % ( completeness ) on arXiv - Long ; and 23.1 % ,   13.5 % ( cohesion ) , and 27.7 % , 21.9 % ( complete-   ness ) on PubMed - Long . These improvements ,   qualitatively evaluated by the human annotators ,   show the promising capability of our purposed   model in generating improved extended summaries   which are more preferable than the baselines ’ . We   observe a similar improvement trend when com-   paring our summaries with BSEM ,   where 66 % , 77 % ( arXiv - Long ) ; and 58 % , 58 %   ( PubMed - Long ) of our summaries are as good as or   better than the baseline ’s in terms of cohesion and   completeness . Looking at the Cohen ’s inter - rater   agreement , the correlation scores fall into “ moder - ate ” agreement range according to the interpreta-   tion of Cohen ’s kappa range ( McHugh , 2012 ) .   6.3 Case study   Figure 4 ( a ) demonstrates an extended summary   generated from a sample arXiv - Long paper by our   model . The underlined sentences denote that the   corresponding sentences are oracle ( i.e. , summary-   worthy ) , the colored spans denote the pointers from   introductory information to non - introductory infor-   mation , and sentence numbers appear in brackets   following each sentence . As shown , our system   first identifies salient introductory sentences ( i.e. ,   [ s]and[s ] ) , and then augments them with im-   portant non - introductory sentences . Figure 4 ( b )   shows the R scores between pairs of intro-332ductory and non - introductory sentences . The edge   thickness signifies the strength of the R score   between a pair of sentences . For example , intro-   ductory sentence [ s]highly correlates with non-   introductory sentence [ s]as it has a stronger edge   ( s , s ) thickness . More specifically , [ s]has men-   tions of “ radiative line driving ” , “ properties of   the winds ” , “ possible generations of very massive   stars ” , and “ ionizing fluxes ” which maps to [ s ]   with semantically similar mentions of “ line driven   stellar winds ” , “ stellar wind properties ” , “ possi-   ble generations of very massive stars ” , and “ ioniz-   ing fluxes ” .   7 Error Analysis   To determine the limitations of our model , we fur-   ther analyze our system ’s generated summaries   and report three common defects , along with the   percentage of these errors among underperformed   cases . We found that ( 1 ) our end - to - end system ’s   performance is highly dependent on the introduc-   tory sentence extraction task ’s performance ( i.e. ,   tasktin Figure 3 ) as identification of salient in-   troductory sentences ( i.e. , oracle introductory sen-   tences ) sets up a firm ground to explore detailed   sentences from the non - introductory parts of the   paper . In other words , identification of non - salient   introductory sentences leads to a drift in finding   supplemental sentences from the non - introductory   parts . Our model often underperforms when it can-   not find important sentences from the introductory   part ( 65 % ) ; ( 2 ) in underperformed cases , our model   fails in selecting motivation , objective sentences   from the introductory part , and only identifies the   contribution sentences ( i.e. , describing paper ’s con-   tributions ) , such that the final generated summary   is composed of contribution sentences , rather than   objective sentences . This observation hurts the sys-   tem in cohesion and completeness ( 40 % ) ; and ( 3 )   as discussed , our model matches introductory sen-   tences with sentences from non - introductory parts   of the paper . Given that two sentences within a sci-   entific paper might conceptually convey the exact   same information , but are just paraphrased of each   other , our model samples both to form the final   summary as a high semantic correlation exists be-   tween them . This phenomenon leads to sampling   two sentences that convey the same informationwithout providing more details ; hence , information   redundancy ( 35 % ) .   8 Conclusion   In this work , we propose a novel approach to tackle   the extended summary generation for scientific doc-   uments . Our model is built upon the fine - tuned   contextualized language models for text summa-   rization . Our method improves over strong and   state - of - the - art summarization baselines by adding   an auxiliary learning component for identifying   salient introductory information of long documents ,   which are then used as pointers to guide the sum-   marizer to pick summary - worthy sentences . The   extensive intrinsic and human evaluations show the   efficacy of our model in comparison with the state-   of - the - art baselines , using two large scale extended   summarization datasets . Our error analysis further   paves the path for future reseacrh .   References333334335