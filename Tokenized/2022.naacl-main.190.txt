  James Y. Huang , Bangzheng Li , Jiashu Xuand Muhao Chen   University of Southern California   Los Angeles , California , USA   { huangjam,bangzhen,jiashuxu,muhaoche}@usc.edu   Abstract   Semantic typing aims at classifying tokens or   spans of interest in a textual context into seman-   tic categories such as relations , entity types ,   and event types . The inferred labels of seman-   tic categories meaningfully interpret how ma-   chines understand components of text . In this   paper , we present UST , a unified framework   for semantic typing that captures label seman-   tics by projecting both inputs and labels into   a joint semantic embedding space . To formu-   late different lexical and relational semantic   typing tasks as a unified task , we incorporate   task descriptions to be jointly encoded with the   input , allowing USTto be adapted to dif-   ferent tasks without introducing task - specific   model components . USToptimizes a mar-   gin ranking loss such that the semantic related-   ness of the input and labels is reflected from   their embedding similarity . Our experiments   demonstrate that USTachieves strong perfor-   mance across three semantic typing tasks : en-   tity typing , relation classification and event typ-   ing . Meanwhile , USTeffectively transfers   semantic knowledge of labels and substantially   improves generalizability on inferring rarely   seen and unseen types . In addition , multiple   semantic typing tasks can be jointly trained   within the unified framework , leading to a sin-   gle compact multi - tasking model that performs   comparably to dedicated single - task models ,   while offering even better transferability .   1 Introduction   Semantic typing is a group of fundamental natu-   ral language understanding problems that aim at   classifying tokens ( or spans ) of interest into se-   mantic categories . This includes a wide range of   long - standing NLP problems such as entity typing ,   relation classification , and event typing . Inferring   the types of entities , relations or events mentionedis not only crucial to the structural perception of   human language , but also plays an important role   in many downstream tasks such as entity linking   ( Onoe and Durrett , 2020 ) , information extraction   ( Zhong and Chen , 2021 ) and question answering   ( Yavuz et al . , 2016 ) .   Most traditional methods tackle semantic typing   problems by training task - specific multi - class clas-   sifiers with token or sentence representations from   language models to predict a probability distribu-   tion over a pre - defined set of classes ( Dai et al . ,   2021 ; Yamada et al . , 2020 ) . However , this ap-   proach comes with several limitations . First , these   models simply convert labels into indices , thus   completely ignoring the rich semantics carried by   the label text itself . For example , given “ Currently   Ritek is the largest producer of OLEDs . ” , know-   ing what the entity type company means would   naturally simplify the inference of “ Ritek ” is a com-   pany in this context . Second , models trained as   classifiers do not generalize well to class labels that   are rarely seen or unseen in the training data , as   these models rely on the abundance of annotated   examples to associate semantics to label indices .   In particular , since these classifiers are limited by   the pre - defined label set , they can not infer any un-   seen labels unless being re - trained or incorporated   with label mapping rules . As a result , these models   struggle to handle more fine - grained semantic typ-   ing tasks in real - world scenarios ( Choi et al . , 2018 ;   Chen et al . , 2020 ) where any free - form textual la-   bels may be used to represent the types , many of   which may also be unseen during training .   In contrast to the aforementioned traditional   paradigm for semantic typing , several studies have   explored alternative approaches such as prompt-   based learning ( Schick and Schütze , 2021 ; Ding   et al . , 2021 ) and indirect supervision from NLI   models ( Yin et al . , 2019 ; Sainz et al . , 2021 ) to   make more efficient use of label semantics . How-   ever , these methods usually require hand - crafted2642   templates or mapping between labels and language   model vocabulary that do not scale well to diverse ,   free - form labels across various semantic typing   tasks . Instead , we seek a generalizable approach   that captures label semantics while requiring mini-   mal effort to be adapted to a different task .   In this paper , we propose UST , a unified   framework for semantic typing that projects con-   text sentences and candidate labels into a shared   semantic embedding space . USTprovides a uni-   fied solution to two major categories of semantic   typing tasks , namely lexical typing ( e.g. , entity typ-   ing , event typing ) and relational typing ( relation   classification ) . By optimizing a margin ranking   loss , our model captures label semantics such that   positive labels are encoded closer to their respec-   tive context sentences than negative labels by at   least a certain similarity margin . Depending on the   task requirement , either top- kcandidate labels or   any candidate labels with similarity above a certain   threshold are given as the final predictions . Fur-   thermore , we add a task description to the end of   the context sentences to specify the task and to-   ken ( spans ) of interest , and use a single model for   encoding both context sentences and labels . This   simple technique allows us to unify different seman-   tic typing tasks without introducing separate task-   specific model components or learning objectives ,   while differentiating among distinct task prediction   processes during inference . USTdemonstrates   strong performance on three semantic typing bench-   marks : UFET ( Choi et al . , 2018 ) for ( ultra - fine )   entity typing , TACRED ( Zhang et al . , 2017 ) for   relation classification , and MA VEN ( Wang et al . ,   2020 ) for event typing , even achieving comparable   performance with a single model trained to solve   all three tasks simultaneously . The main contributions of this work are three-   fold . First , the proposed USTframework con-   verts distinct semantic typing tasks into a unified   formulation , where both input and label seman-   tics can be effectively captured in the same repre-   sentation space . Second , we incorporate a model-   agnostic task representation scheme to allow the   model to differentiate among distinct tasks in train-   ing and inference without introducing additional   task - specific model components . Third , UST   demonstrates substantial improvements in both ef-   fectiveness and generalizability on entity typing ,   relation classification and event typing . In addition ,   our unified framework makes it possible to learn   a single model for all three tasks , which performs   comparably to dedicated models trained separately   on each task .   2 Method   In this section , we present the technical details   ofUST , our unified framework for semantic   typing . We first provide a general definition of   a semantic typing problem ( § 2.1 ) , followed by a   detailed description of our model ( § 2.2 ) , training   objective(§2.3 ) , and inference ( § 2.4 ) .   2.1 Problem Definition   Given an input sentence sand a set of one or more   token spans of interest E={e , ... e } , e⊂s , the   goal of semantic typing is to assign a set of one or   more labels Y={y , ... y } , Y⊂ Y toEthat best   describes the semantic category Ebelongs to in the   context of s. Ydenotes the set of candidate labels ,   which may include a large number of free - form   phrases ( Choi et al . , 2018 ) or ontological labels   ( Zhang et al . , 2017 ) . In this paper , we consider   two categories of semantic typing tasks , lexical2643   typing of a single token span ( e.g. , entity or event   typing ) , and relational typing between two token   spans ( relation classification ) .   2.2 Model   Overview . As illustrated in Fig . 1 , USTlever-   ages a pre - trained language model ( PLM ) to project   both input sentences and the candidate labels into   a shared semantic embedding space , where the se-   mantic relatedness between the input and label is   reflected by their embedding similarity . This is ac-   complished by optimizing a margin ranking objec-   tive that pushes negative labels away from the input   sentence while pulling the positive labels towards   the input . This simple , unified paradigm allows our   model to rank candidate labels based on the affinity   of semantic representations with regard to the in-   put during inference . Meanwhile , our model is not   limited to a pre - defined label set , as any textual la-   bel , whether seen or unseen during training , can be   ranked accordingly as long as the model captures   its semantic representation . In order to specify the   task at hand along with the tokens ( or spans ) we   aim to classify , we add a task description to the end   of the input sentence . This allows our framework   to use unified representations from a single encoder   for both inputs and labels , as well as support the   inference of distinct semantic typing tasks without   introducing task - specific model components .   Task Description . To highlight the tokens ( or   spans ) we aim to type , we first enclose them with   special marker tokens indicating their roles ( enti-   ties , subjects , objects , or triggers ) . Next , we lever-   age the existing semantic knowledge in PLMs and   add a natural language task description to the end   of the input sentence to specify the task at hand   along with tokens ( or spans ) of interest . The gen-   eral format for lexical semantic typing is   and that of relational semantic typing is   Examples of different input formats ( including spe-   cial tokens and task descriptions ) can be found in   Tab . 1 . In addition , relational typing ( relation clas-   sification ) tasks may incorporate entity types from   NER models alongside input sentences . Entity type   information has been shown to benefit relation clas-   sification ( Peng et al . , 2020 ; Zhong and Chen , 2021 ;   Zhou and Chen , 2021a ) , and can be easily incor-   porated into our task description , as shown in the   given example .   Input Representation . We use a RoBERTa model   ( Liu et al . , 2019 ) to jointly encode the input sen-   tence and the task description . Given an input s   and its task description d , we concatenate sandd   into a single sequence , and obtain the hidden rep-   resentation of the < s > token as the input sentence   representation , denoted by u :   u = f ( [ s , d ] ) .   A traditional approach to semantic typing is to   train classifiers on top of the representations of   specific tokens of interest ( Wang et al . , 2021a ; Ya-   mada et al . , 2020 ) . In the case of relational typing   where two entities are involved , their representa-   tions are usually concatenated , leading to dimen-   sion mismatch with lexical typing tasks and requir-   ing a different task - specific module to handle . In-   stead , thanks to the introduction of task description ,   USTalways uses the universal < s > token rep-   resentation for both inputs and labels , and across   different semantic typing tasks .   Label Representation . Most semantic typing tasks   provide textual labels in natural language from2644which a language model can directly capture la-   bel semantics . Some relation classification datasets   such as TACRED use extra identifiers per : andorg :   to distinguish same relation type with different sub-   ject types . For example , per : parent refers to the   parent of a person , while org : parent represents the   parent of an organization such as a company . In   this case , we simply replace per : andorg : with   person andorganization respectively . The label   text is encoded by the exact same model used to   encode the input sentence . Given the label y , we   again take the < s > token representation as the label   representation , denoted by v :   v = f ( y ) .   2.3 Learning Objective   LetYbe the set of all candidates labels for a se-   mantic typing task . Given an input [ s , d]and the   positive label set Y⊂ Y , we first randomly sam-   ple a negative label y∈ Y\ Yfor each training   instance . Then , we encode the input [ s , d ] , positive   label yand negative label yinto their respective   semantic representations u , v , andv . USTop-   timizes a margin ranking loss such that positive   labels , which are more semantically related to the   input than negative labels , are also closer to the   input in the embedding space . Specifically , the loss   function for a single training instance is defined as :   L = max{c(u , v)−c(u , v ) + γ,0 } ,   where c(·)denotes cosine similarity and γis a non-   negative constant . The overall ( single - task ) training   objective is given by :   L=1   N / summationdisplay / summationdisplayL ,   where Sis the set of training instances for task t ,   Yis the set of all positive labels of s , andNis the   number of distinct pairs of training sentence and   positive label . In addition to the single - task setting   which optimizes an individual task - specific loss   L , we also consider a multi - task setting of UST   where it is jointly trained on different semantic   typing tasks and optimizes the following objective :   L=1   N / summationdisplay / summationdisplay / summationdisplayL.   where Tis the set of semantic typing tasks UST   is trained on , and Nis the total number of training   instances.2.4 Inference   USTsupports different strategies for inference   depending on the task requirement . If the number   of labels for each input is fixed , we simply retrieve   the top- kclosest candidate labels to the input as the   final predictions . Otherwise , all candidate labels   with similarity above a certain threshold are given   as predictions . Note that USTis not restricted   to a pre - defined label set , as any textual label in   natural language can be encoded by USTinto   its semantic representation and ranked accordingly   during inference .   3 Experiments   In this section , we evaluate USTon single - task   experiments on three semantic typing tasks : en-   tity typing ( § 3.1 ) , relation classification ( § 3.2 ) and   event typing ( § 3.3 ) . We then assess the general-   izability of USTby conducting zero - shot and   few - shot prediction , and study the effects of task   description ( § 3.4 ) . Finally , we train USTunder   multi - task setting to solve all three tasks simultane-   ously ( § 3.5 ) .   3.1 Ultra - fine Entity Typing   We first conduct experiments on the ultra - fine entity   typing task , which aims at predicting fine - grained   free - form words or phrases that describe the appro-   priate types of entities mentioned in sentences .   Dataset . We use the Ultra - Fine Entity Typing   ( UFET ) benchmark ( Choi et al . , 2018 ) , which in-   cludes 5,994 sentences split into 1,998 each for   train , dev and test . Each entity mention in UFET is   annotated with one or more free - form type labels ,   covering a set of 2,519 distinct words and phrases .   Following the original evaluation protocol , we re-   port macro precision , recall and F1 score on the   UFET test set .   Model . Since the number of ground truth labels for   each entity is not fixed , all candidate labels with   similarity above a certain threshold is given as the   final predictions . We tune the hyperparameters , in-   cluding the threshold , on the UFET dev set . We use   base and large versions of RoBERTa as encoders   for UST and UST respectively .   Baselines . UFET - biLSTM ( Choi et al . , 2018 )   learns context and mention representations by   combining pre - trained word embeddings with a   character - level CNN and a bi - LSTM . LabelGCN   ( Xiong et al . , 2019 ) adds a graph propagation layer2645   to capture label dependencies . LDET ( Onoe and   Durrett , 2019 ) learns a denoising model that au-   tomatically filters and relabels distant supervision   data for training . Box4Types ( Onoe et al . , 2021 )   introduces box embeddings to represent type hier-   archies and uses BERT as context and men-   tion encoder . LRN ( Liu et al . , 2021 ) uses an auto-   regressive LSTM to discover label structures , a   bipartite attribute graph to capture intrinsic label   dependencies , and a BERT as sentence en-   coder . MLMET ( Dai et al . , 2021 ) generatively   augments the training data with a masked language   model , and fine - tunes BERT on the augmented   training set .   Results . As shown in Tab . 2 , UST already   outperforms the SOTA baseline MLMET without   training on any augmented data by 0.2 % in F1   score . With a larger language model , UST   further improves F1 score by another 0.6 % . Since   UFET only provides a small set of human anno-   tated training data compared to its diverse label   set , all baselines except LRN incorporate distant   supervision data to alleviate data scarcity . UST ’s   superior performance on UFET demonstrates the   importance of capturing label semantics as an aux-   iliary supervision signal that is not fully exploited   by previous methods . This is especially beneficial   when annotated data are limited , and can alleviate   the model ’s reliance on augmenting training data .   In this way , USTalso achieves better generaliz-   ability to unseen and rarely seen labels , for which   we conduct a more detailed analysis on few - shot   and zero - shot UFET labels in § 3.4 .   3.2 Relation Classification   The goal of relation classification is to determine   the relation between a subject entity and an object   entity mentioned in a sentence .   Dataset . We run the experiments on TACRED   ( Zhang et al . , 2017 ) , a widely used benchmark for   this task that contains 106,264 sentences with en-   tity pairs labeled as one of the 41 relation types or   ano_relation type . TACRED provides 68,124 in-   stances for training , 22,631 for dev , and 15,509 for   testing . Following the original evaluation protocol ,   we report micro precision , recall and F1 score on   the TACRED test set .   Model Configuration . USTretrieves the can-   didate label closest to the input in the embedding   space as the final prediction . Since entities in TA-   CRED are also annotated with entity types , we   place the entity type labels in front of their corre-   sponding entity mentions in the task description to   provide additional information for relation classi-   fication , as shown in Tab . 1 . We tune the hyperpa-   rameters on the TACRED dev set .   Baselines . SpanBERT ( Joshi et al . , 2020 ) incor-   porates span prediction as an additional objective   for BERT pre - training . MTB ( Baldini Soares et al . ,   2019 ) introduces matching - the - blank training on   entity - linked text to connect relation representa-   tions among related instances . TANL ( Paolini   et al . , 2021 ) proposes a unified text - to - text frame-   work for structured prediction tasks based on T5   ( Raffel et al . , 2020 ) . K - Adapter ( Wang et al . ,   2021a ) learns adapter modules to infuse structured   knowledge into a RoBERTa model . LUKE   further trains RoBERTa on entity - annotated   corpus with an entity - aware self - attention mecha-   nism . BERT - CR ( Zhou and Chen , 2021b ) intro-   duces a co - regularization framework to improve   learning from noisy datasets with a BERT   model . IBRE ( Zhou and Chen , 2021b ) incorpo-   rates entity type information into mention mark-   ers in the sentence to boost the performance of2646RoBERTa .SP(Cohen et al . , 2020 ) formu-   lates relation classification as a two - way span pre-   diction problem , and uses ALBERT ( Lan et al . ,   2020 ) as encoder .   Results . As shown in Tab . 3 , UST   already outperforms several strong baselines   which are built on larger PLMs ( BERT   or RoBERTa ) , except for SP and IBRE .   UST further improves the performance   and establishes new SOTA on TACRED , outper-   forming the best baseline SP by 0.7 % in F1 . While   SP also leverages label semantics by framing rela-   tion classification as a two - way question answer-   ing problem , it requires hand - crafted question tem-   plates for each relation label and more significant   computational cost for answer span prediction . In   comparison , USTdirectly captures label seman-   tics from the label text itself , while offering supe-   rior performance and inference efficiency as labels   can be retrieved by simply computing embedding   cosine similarity .   3.3 Event Typing   Event typing aims at assigning an event type to an   event trigger that clearly indicates an event .   Dataset . We conduct the evaluation using MA VEN   ( Wang et al . , 2020 ) , a general - domain event extrac-   tion benchmark with 77,993/18,904/21,835 event   triggers for train / dev / test annotated with 168 dis-   tinct event types . MA VEN also provides a large   set of negative triggers , which includes all content   words ( nouns , verbs , adjectives , and adverbs ) la-   beled by a part - of - speech tagger but not annotated   as an event trigger . Since USTfocuses on se-   mantic typing and does not handle mention span   prediction , we train a BERT - CRF model to first   identify trigger candidates following Wang et al .   ( 2020 ) , and then predict an event type for each   trigger candidate using UST . Following the orig-   inal paper , we report micro precision , recall and F1   score on MA VEN test set .   Model Configuration . We retrieve the candidate   label with the highest similarity to the input as the   predicted event type . We tune the hyperparameters   on the MA VEN dev set .   Baselines . DMCNN ( Chen et al . , 2015 ) uses a   CNN with dynamic multi - pooling to obtain trig-   ger representations for classification . MOGANED   ( Yan et al . , 2019 ) proposes a multi - order GCN to   capture interrelation between event trigger and ar-   gument representations based on dependency trees .   DMBERT ( Wang et al . , 2019 ) improves DMCNN   by training a BERT model as sentence encoder   with dynamic multi - pooling . BERT - CRF stacks a   CRF layer on top of BERT to model multiple   event correlations in a single sentence . CLEVE   ( Wang et al . , 2021b ) proposes a contrastive learning   framework fine - tuned on large - scale corpus with   AMR structures obtained from AMR parsers , and   combines AMR graph representations from a GNN   and text representations from RoBERTa to   classify event types .   Results . As shown in Tab . 4 , USTis able to   improve event typing over BERT - CRF , and outper-   form all baselines except CLEVE . Note that in ad-   dition to being initialized from the same RoBERTa   model as UST , CLEVE is further fine - tuned on   large - scale corpus with AMR structures obtained   from a separate parsing model ( Xu et al . , 2020 ) that   also requires large human - annotated data to train .   This indicates much more expensive supervision   signals used by CLEVE . In contrast , USTef-   fectively captures the meaning of event types and   learns to classify event triggers by only fine - tuning   on MA VEN , while still achieving promising perfor-   mance without the need of any additional annotated   resources .   3.4 Analysis   In this section , we provide a detailed analysis to   better understand the generalizability of UST   and the effects of incorporating task description .   Specifically , we examine UST ’s performance   on few - shot and zero - shot entity typing on UFET,2647   zero - shot relation classification on FewRel ( Han   et al . , 2018 ) , and how USTperforms without   task descriptions .   Few - shot & Zero - shot Entity Typing . A large   portion of UFET test set labels have very few or   even no training instances . We focus on entity types   with no more than 10 instances in the training set ,   and compare the performance of UST with   the previous SOTA model MLMET on these few-   shot and zero - shot labels .   As shown in Fig . 2 , the advantage of USTover   MLMET becomes more evident for rarer labels .   For the most challenging zero - shot labels , UST   substantially outperforms MLMET by 7.2 % in F1   score , suggesting that USTis better generalized   to infer low - resource and unseen entity types .   Zero - shot Relation Classification . We conduct   experiments on FewRel ( Han et al . , 2018 ) , a widely   used benchmark for low - resource relation classifi-   cation . FewRel includes 64/16/20 non - overlapping   relation types for train / dev / test with 700 sentences   collected from Wikipedia for each relation type .   We evaluate USTunder the N - way- 0 - shot set-   ting , where the goal is to predict the correct relation   among Ncandidate relations without seen train-   ing examples . Following previous studies ( Cetoli ,   2020 ; Dong et al . , 2021 ) , we report 5 - way-0 - shot   and 10 - way-0 - shot accuracy on the FewRel dev set .   We compare USTwith following baselines :   REGRAB ( Qu et al . , 2020 ) proposes a bayesian   meta - learning method to infer the posterior distri-   bution of relation prototypes initialized with knowl-   edge graph embeddings . BERT - SQuAD ( Cetoli ,   2020 ) formulates zero - shot relation classification   as a question answering problem , and fine - tunes   a BERT QA model trained on SQuAD 1.1   ( Rajpurkar et al . , 2016 ) to predict relation types .   MapRE ( Dong et al . , 2021 ) proposes a contrastive   pre - training framework that learns input and re-   lation representations from large - scale relation-   annotated data . All baselines , as well as UST ,   are fine - tuned on the FewRel training set , and then   evaluated on the FewRel dev set with a new set of   relation types completely disjoint from that of the   training set .   As shown in Tab . 5 , USToutperforms the best   baseline MapRE by 0.5 % and 1.4 % in accuracy on   5 - way-0 - shot and 10 - way-0 - shot tasks without first   pre - training on any relation - annotated data . This   demonstrates that by effectively captures label se-   mantics , USTallows better knowledge transfer   to handle unseen relation types .   Effects of Task Description . We conduct an abla-   tion experiment on task descriptions using UST   to better understand their effects on downstream   tasks . As shown in Tab . 6 , the performance on TA-   CRED degrades much more significantly compared   to that on UFET and MA VEN after removing task   description . In lexical typing , the token span to be   classified tend to share similar semantics with its   type , and in many cases can be easily matched to   its type label without explicitly specifying the task .   In contrast , relation types are usually not seman-   tically similar to its subject and object , and task   description helps bridge this gap .   3.5 Multi - task Learning   With a unified task formulation , USTfacilities   learning a single model to jointly train on and si-   multaneously solve different semantic typing tasks .   For more balanced training , We train USTon2648   the combined training set of UFET , TACRED and   MA VEN , and report F1 performance on their re-   spective test sets by following their respective eval-   uation protocol . We also include performance of   single - task UST for comparison .   As shown in Tab . 7 , our multi - task model ob-   tain generally comparable performance to dedi-   cated USTmodels trained separately on each   of the three semantic typing tasks . Despite a slight   decrease in performance on some of the tasks ,   UST is still able to outperform several   strong baselines discussed earlier . Hence , UST   provides a possible solution for learning a compact ,   unified model with a joint semantic embedding   space across different semantic typing tasks . More-   over , this leads to a well - structured embedding   space that better allows zero - shot transfer to new   semantic typing tasks . To provide a preliminary   analysis on the potential of USTon cross - task   transfer , we evaluate both single - task and multi-   task USTmodels on FewRel dev set without   training on any FewRel data . While FewRel is also   a relation classification dataset like TACRED , 75 %   of the relation types in FewRel dev set do not exist   in TACRED . Results in Tab . 7 show that by jointly   training on different semantic typing tasks within   a unified framework , USTdemonstrate signifi-   cantly stronger transferability to the unseen FewRel   task compared to single - task variants . It would be   meaningful to see if incorporating more datasets   and tasks into USTwould further benefit cross-   task transfer , especially to tasks with limited data   available for training . We leave this as a direction   for further investigation .   4 Related Works   We present two lines of relevant research topics .   Each has a large body of work which we can only   provide as a highly selected summary .   Semantic Typing . Semantic typing tasks can begenerally categorized into lexical typing ( e.g. , en-   tity typing , event typing ) and relational typing ( or   classification ) . A large number of specialized ap-   proaches have been developed for individual se-   mantic typing tasks . For example , prior studies on   entity typing have exploited label dependencies and   hierarchies ( Xu and Barbosa , 2018 ; Xiong et al . ,   2019 ) , capturing label relations with knowledge   bases ( Dai et al . , 2019 ; Jin et al . , 2019 ) , as well as   automatic data augmentation and denoising tech-   niques ( Onoe and Durrett , 2019 ; Dai et al . , 2021 ) to   deal with fine - grained type vocabularies . Relation   classification has been tackled by modeling depen-   dency structures ( Zhang et al . , 2018 ) , learning span   representations ( Joshi et al . , 2020 ) , entity represen-   tations ( Yamada et al . , 2020 ) , and injecting external   knowledge into pre - trained language models ( Pe-   ters et al . , 2019 ; Zhang et al . , 2019 ; Wang et al . ,   2021a ) . Nevertheless , most previous methods have   formulated semantic typing as a multi - class classi-   fication problem without capturing label semantics .   Learning Label Semantics . Previous studies have   attempted formulating typing tasks into other tasks   that allow more effective learning of label seman-   tics . Following this idea , semantic typing tasks   have been reformulated as prompt - based learning   ( Ding et al . , 2021 ; Han et al . , 2021 ) , natural lan-   guage inference ( Yin et al . , 2019 ; Sainz et al . ,   2021 ) , question answering ( Levy et al . , 2017 ; Li   et al . , 2019 ; Du and Cardie , 2020 ) , and translation   ( Paolini et al . , 2021 ) . Another line of research that   is more relevant to our approach focuses on learn-   ing semantic label embeddings such that candidate   labels can be ranked based on their affinity with   the input in the embedding space . Semantic label   embeddings have been successfully applied to a   variety of tasks such as hierarchical text classifi-   cation ( Chen et al . , 2021 ; Shen et al . , 2021 ) and   intent detection ( Xia et al . , 2018 ) . In the context of   semantic typing tasks , Chen et al . ( 2020 ) propose   a learning - to - rank framework for multi - axis event   process typing with indirect supervision from la-   bel glosses . Chen and Li ( 2021 ) use a pre - trained   sentence embedding model to learn relation label   embeddings from label descriptions . Dong et al .   ( 2021 ) propose a contrastive pre - training frame-   work to learn input and relation representations   from large - scale relation - annotated data . Unlike   previous approaches , USTdoes not rely on exter-   nal label knowledge , training data or task - specific   model components . Instead , USTeffectively2649captures label semantics solely from label names ,   and unify different semantic typing tasks into a sin-   gle framework by incorporating task descriptions   to be jointly encoded with the input .   5 Conclusion   We propose UST , a unified framework for se-   mantic typing that exploits label semantics to learn   a joint semantic embedding space for both inputs   and labels . By incorporating model - agnostic task   descriptions , USTcan be easily adapted to differ-   ent semantic typing tasks without introducing task-   specific model components . Experimental results   show that USToffers both strong performance   and generalizability on entity typing , relation clas-   sification , and event typing . Our unified framework   also facilitates learning a single model to solve dif-   ferent semantic typing tasks simultaneously , with   performance on par with dedicated models trained   on individual tasks .   Acknowledgment   We appreciate the anonymous reviewers for their   insightful comments and suggestions . This ma-   terial is supported in part by the DARPA MCS   program under Contract No . N660011924033 with   the United States Office Of Naval Research , and by   the National Science Foundation of United States   Grant IIS 2105329 .   References265026512652   A Experiment Details   We run all single - task UST experiments on   NVIDIA RTX 2080Ti GPUs , and all UST   and multi - task experiments on NVIDIA RTX   A5000 GPUs . UST andUST use   base and large versions of RoBERTa as encoders   with 125 M and 355 M parameters respectively . We   conduct hyperparameter search within the follow-   ing range :   • learning rate : { 3e-6 , 5e-6 , 1e-5 , 2e-5 }   • Batch size : { 32 , 64 , 128 }   •Number of training epochs : { 50 , 100 , 200 , 500 ,   1000 }   • Ranking loss margin γ : { 0.1 , 0.2 , 0.3 }   We optimize our models using AdamW   ( Loshchilov and Hutter , 2019 ) with linear   learning rate decay . The best model checkpoints   are selected based on dev set performance . Tab . 8   lists common hyperparameters used across all   experiments . All datasets used in our experiments   are in English . More details of individual tasks and   experiments are provided below .   A.1 UFET   The UFET dataset is publicly available on its offi-   cial website . Tab . 9 shows the hyperparameters   and dev F1 score for UFET experiments.2653A.2 TACRED   The TACRED dataset we use is licensed by LDC . Tab . 10 shows the hyperparameters and dev F1   score for TACRED experiments .   A.3 MA VEN   The MA VEN dataset is publicly available via its   official github repository . Tab . 11 shows the hy-   perparameters and dev F1 score for MA VEN exper-   iments .   A.4 FewRel   The FewRel dataset is publicly available via its   official github repository . We report the average   accuracy of 10 runs on the dev set during evaluation .   Tab . 12 shows the hyperparameters for FewRel   experiments .   A.5 Multi - task Experiments   We conduct multi - task experiments on the com-   bined UFET , TACRED , and MA VEN training sets .   We up - sample UFET training set by a factor of   10 for more balanced training . Tab . 13 shows the   hyperparameters and dev set F1 for multi - task ex-   periments .   B Ethics Considerations   Our experiments are all conducted on openly avail-   able and widely used datasets . We do not augment   any information to those data in this research , hence   this research is not expected to introduce any addi-   tional biased information to existing information   in those data . However , the model may potentially   capture biases reflective of the pre - trained language   models and datasets we use for our experiments ,   in such biases have pre - existed in these pre - trained   models or datasets . This is a common problem for   models trained on large - scale data , and therefore   we suggest conducting a thorough bias analysis   before deploying our model in any real - world ap-   plications.2654