The paper is an exploratory analysis of multilingual word-level quality estimation using cross-lingual transformers. The proposed system, called Re2G (Retrieve, Rerank, Generate), combines neural initial retrieval and reranking into a BART-based sequence-to-sequence generation. The authors also use knowledge graph embeddings and evaluate the system using lexical-overlap metrics (ROUGE-1/2/L) and embedding-similarity metrics (BERTSCORE). The task evaluated is quality estimation, and the dataset used is not specified. The paper also mentions hyperparameters such as L and H, but their values are not provided. The evaluation results on the mentioned metrics are not mentioned, and there is no mention of specific baseline systems/methods. The paper incorporates ideas from existing models for zero-shot entity linking and introduces an entity linking model that works with arbitrary knowledge bases. Overall, the paper explores various aspects of multilingual quality estimation and entity linking.