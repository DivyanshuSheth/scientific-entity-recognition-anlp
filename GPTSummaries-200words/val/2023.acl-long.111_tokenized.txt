Summary : 
 
  In this research paper , the authors propose a training technique called Learning GoodTeacher Matters ( LGTM ) to enhance knowledge distillation in text classification tasks . They introduce the concept of distillation influence to quantify the impact of distillation from each training sample on the student 's generalization ability . By prioritizing samples that enhance the student 's generalization ability , LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark . The paper also discusses the limitations of existing knowledge distillation methods and provides detailed explanations of the proposed methods . Experimental results show that LGTM achieves state - of - the - art performance and is robust in various knowledge distillation settings . The research paper adheres to ethical considerations and does not involve human participants or data collection .
