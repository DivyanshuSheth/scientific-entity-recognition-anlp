Summary : 
 
  This research paper introduces a novel knowledge - informed encoder - decoder framework to generate explanations of biased and toxic speech . The proposed framework leverages three different sources of knowledge : expert knowledge , explicit knowledge from knowledge graphs , and implicit knowledge from large pretrained generative models . The authors evaluate their models on the SBIC dataset and the Implicit Hate Speech Corpus . The results show that the knowledge - informed models outperform baselines significantly , generating more detailed explanations of stereotypes in toxic speech . The MGEN models , which combine knowledge from multiple sources using simple mixture models , perform the best . They achieve higher scores on metrics such as BLEU , ROUGE - L , and BERTScore compared to other models . The authors also perform error analysis and ablation studies , providing insights into the strengths and weaknesses of different knowledge types . The research paper highlights the importance of explainable machine learning classifiers for the detection and understanding of toxic speech and provides promising results in generating detailed explanations .
