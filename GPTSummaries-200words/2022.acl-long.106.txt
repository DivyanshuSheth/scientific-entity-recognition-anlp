Summary:

This research paper investigates how the number of pretraining languages influences zero-shot performance on unseen target languages and whether model adaptation affects this relationship. The experiments are conducted on multilingual models pretrained on diverse and related languages. The evaluation is performed on three downstream tasks: part-of-speech tagging, named entity recognition (NER), and natural language inference (NLI). The findings suggest that increasing the number of pretraining languages improves performance up to a certain point, after which it plateaus. However, with model adaptation, pretraining on a larger number of languages often leads to further improvement. The authors also find that choosing a diverse set of pretraining languages is crucial. The experimental results highlight the impact of pretraining languages on zero-shot cross-lingual transfer and provide insights for model development. The research focuses on low-resource languages and evaluates the models on the GLUE benchmark dataset.