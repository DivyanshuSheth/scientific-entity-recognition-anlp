This research paper compares three transfer learning methods in natural language processing: training on an intermediate task before the target task (STILTs), using multi-task learning (MTL) to train on both tasks simultaneously (pairwise MTL), and training on all available tasks together (MTL). The study focuses on the GLUE dataset and finds that pairwise MTL performs better than STILTs when the target task has fewer instances than the supporting task, and vice versa. The size heuristic holds true in over 92% of applicable cases. The results also show that MTL is generally worse than the pairwise methods. The paper concludes that this analysis can help researchers make better decisions for NLP tasks.