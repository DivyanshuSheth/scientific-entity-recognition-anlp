Summary:
The paper analyzes reference-free evaluation metrics for natural language generation (NLG) systems, specifically for text summarization and dialog generation. The authors find that these metrics may rely on spurious correlations with measures like word overlap and length. They demonstrate that these errors can be mitigated by explicitly designing evaluation metrics to avoid spurious features. The authors evaluate recently proposed metrics, such as FactCC and DAE, on the tasks of text summarization and dialog generation, comparing them to spurious correlates like word overlap. They also propose an adversarially trained metric that achieves improved performance in ranking abstractive and faithful systems. The paper emphasizes the importance of analyzing these metrics across different distributions of systems and suggests future work to ensure robustness and better understanding of the metrics. The dataset used for evaluation includes datasets like CNN/DM, PersonaChat, and DailyDialog.