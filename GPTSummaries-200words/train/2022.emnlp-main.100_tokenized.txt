This research paper investigates the ability of vision - and - language models to track predicate - noun dependencies . The authors create a new multimodal task to evaluate this ability and test a range of state - of - the - art models . They find that the models ' performance on the task varies considerably , with some models performing well and others at chance level . The authors analyze the results and attribute the variability in performance to the quality of pretraining data and the use of fine - grained multimodal pretraining objectives . The paper highlights the importance of targeted and controlled evaluations for assessing the multimodal knowledge of such models . The models evaluated include LXMERT , UNITER , ViLBERT , ViLT , Oscar , VinVL , and CLIP . The evaluation metrics used are accuracy scores for image - text matching in a zero - shot setting . The task is focused on understanding predicate - noun dependencies on a controlled dataset derived from Open Images . The results indicate that some models perform better than others in capturing these dependencies , with LXMERT and UNITER showing the highest scores . The dataset used for evaluation consists of 2584 triplets with carefully selected images and pairs of sentences . The authors discuss limitations such as the small range of concepts evaluated and the subjective annotations of perceived gender . Code to reproduce the analyses is available at a GitHub repository .
