Summary:

The research paper proposes a method called Speech and Text joint Pre-Training (STPT) for speech translation and recognition. The method incorporates four subtasks: (1) self-supervised text to text (T2T) subtask, (2) self-supervised speech learning (SSL) subtask, (3) supervised speech to phoneme classification (S2P) subtask, and (4) supervised AED-based speech to text (S2T) subtask. The proposed method effectively fuses speech and text information into one model, achieving 1.7 to 2.3 BLEU improvement on the MST-C speech translation dataset and comparable Word Error Rates (WERs) to wav2vec 2.0 on the L speech recognition task. The pre-training configuration is adapted depending on the downstream task (ASR or ST) to minimize subtask interference. The experiments show that more supervised speech data in the pre-training stage leads to smaller WER. The study also compares the effectiveness of using masked KL divergence loss in SSL subtask compared to contrastive loss. Ablation studies demonstrate the importance of each sub-task in the pre-training process.