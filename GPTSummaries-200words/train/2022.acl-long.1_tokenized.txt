Summary : 
  AdapLeR is a novel method that reduces the computational cost of BERT models during inference while minimizing the loss in downstream performance . It dynamically eliminates less contributing tokens through layers to reduce the length and computational cost . A Contribution Predictor ( CP ) is trained for each layer to estimate token contribution using saliency scores . Experiments on diverse classification tasks show speedups of up to 22x during inference without sacrificing performance . AdapLeR also outperforms other strategies for selecting important tokens , such as saliency and attention . The method is evaluated on multiple datasets , including SST-2 , IMDB , MRPC , AG â€™s News , DBpedia , MNLI , QNLI , and HateXplain , and achieves significant improvements in both speed and accuracy . AdapLeR provides a more efficient approach for BERT models , making them more viable in resource - limited settings . Code for AdapLeR is available on GitHub .
