
Hung Le, Nancy F. Chen, Steven C.H. HoiSalesforce Research AsiaSingapore Management UniversityAgency for Science, Technology and Research (A*STAR)
{hungle,shoi}@salesforce.com, nfychen@i2r.a-star.edu.sg
Abstract
Designed for tracking user goals in dialogues,
a dialogue state tracker is an essential compo-
nent in a dialogue system. However, the re-
search of dialogue state tracking has largely
been limited to unimodality, in which slots and
slot values are limited by knowledge domains
(e.g. restaurant domain with slots of restau-
rant name and price range) and are defined by
specific database schema. In this paper, we pro-
pose to extend the definition of dialogue state
tracking to multimodality. Specifically, we in-
troduce a novel dialogue state tracking task
to track the information of visual objects that
are mentioned in video-grounded dialogues.
Each new dialogue utterance may introduce
a new video segment, new visual objects, or
new object attributes and a state tracker is re-
quired to update these information slots accord-
ingly. We created a new synthetic benchmark
and designed a novel baseline, Video-Dialogue
Transformer Network (VDTN), for this task.
VDTN combines both object-level features and
segment-level features and learns contextual
dependencies between videos and dialogues to
generate multimodal dialogue states. We op-
timized VDTN for a state generation task as
well as a self-supervised video understanding
task which recovers video segment or object
representations. Finally, we trained VDTN to
use the decoded states in a response prediction
task. Together with comprehensive ablation
and qualitative analysis, we discovered inter-
esting insights towards building more capable
multimodal dialogue systems.
1 Introduction
The main goal of dialogue research is to develop
intelligent agents that can assist humans through
conversations. For example, a dialogue agent
can be tasked to help users to find a restaurant
based on their preferences of price ranges and
food choices. A crucial part of a dialogue sys-
tem is Dialogue State Tracking (DST), which isresponsible for tracking and updating user goals
in the form of dialogue states, including a set
of(slot, value) pairs such as (price, “moder-
ate”) and(food, “japanese”) . Numerous machine
learning approaches have been proposed to tackle
DST, including fixed-vocabulary models (Ramadan
et al., 2018; Lee et al., 2019) and open-vocabulary
models (Lei et al., 2018b; Wu et al., 2019; Le
et al., 2020c), for either single-domain (Wen et al.,
2017) or multi-domain dialogues (Eric et al., 2017;
Budzianowski et al., 2018).
However, the research of DST has largely lim-
ited the scope of dialogue agents to unimodality. In
this setting, the slots and slot values are defined by
the knowledge domains (e.g. restaurant domain)
and database schema (e.g. data tables for restaurant
entities). The ultimate goal of dialogue research
towards building artificial intelligent assistants ne-
cessitates DST going beyond unimodal systems. In
this paper, we propose Multimodal Dialogue State
Tracking (MM-DST) that extends the DST task in a
multimodal world. Specifically, MM-DST extends
the scope of dialogue states by defining slots and
slot values for visual objects that are mentioned
in visually-grounded dialogues. For research pur-
poses, following (Alamri et al., 2019), we limited
visually-grounded dialogues as ones with a ground-
ing video input and the dialogues contain multiple
turns of (question, answer) pairs about this video.
Each new utterance in such dialogues may focus
on a new video segment, new visual objects, or
new object attributes, and the tracker is required to
update the dialogue state accordingly at each turn.
An example of MM-DST can be seen in Figure 1.
Toward MM-DST, we developed a synthetic
benchmark based on the CATER universe (Girdhar
and Ramanan, 2020). We also introduced Video-
Dialogue Transformer Network (VDTN), a neural
network architecture that combines both object-
level features and segment-level features in video
and learns contextual dependencies between videos3394
and dialogues. Specifically, we maintained the in-
formation granularity of visual objects, embedded
by object classes and their bounding boxes and in-
jected with segment-level visual context. VDTN
enables interactions between each visual object
representation and word-level representation in di-
alogues to decode dialogue states. To decode mul-
timodal dialogue states, we adopted a decoding
strategy inspired by the Markov decision process
in traditional DST (Young et al., 2010). In this strat-
egy, a model learns to decode the state at a dialogue
turn based on the predicted/ observed dialogue state
available from the last dialogue turn.
Compared to the conventional DST, MM-DST
involves the new modality from visual inputs. Our
experiments show that simply combining visual
and language representations in traditional DST
models results in poor performance. Towards
this challenge, we enhanced VDTN with self-
supervised video understanding tasks which re-
covers object-based or segment-based representa-tions. Benchmarked against strong unimodal DST
models, we observed significant performance gains
from VDTN. We provided comprehensive ablation
analysis to study the efficacy of VDTN models.
Interestingly, we also showed that using decoded
states brought performance gains in a dialogue re-
sponse prediction task, supporting our motivation
for introducing multimodality into DST research.
2 Multimodal Dialogue State Tracking
Traditional DST. As defined by (Mrkši ´c et al.,
2017), the traditional DST includes an input of di-
alogue Dand a set of slots Sto be tracked from
turn to turn. At each dialogue turn t, we denote the
dialogue context as D, containing all utterances
up to the current turn. The objective of DST is for
each turn t, predict a value vof each slot sfrom
a predefined set S, conditioned by the dialogue
context D. We denote the dialogue state at turn
tasB={(s, v)}|. Note that a majority3395of traditional DST models assume slots are condi-
tionally independent, given the dialogue context
(Zhong et al., 2018; Budzianowski et al., 2018; Wu
et al., 2019; Lee et al., 2019; Gao et al., 2019). The
learning objective is defined as:
ˆB= arg maxP(B|D, θ)
= arg max/productdisplayP(v|s,D, θ) (1)
Motivation to Multimodality. Yet, the above
definition of DST are still limited to unimodal-
ity and our ultimate goal of building intelligent
dialogue agents, ideally with similar level of in-
telligence as humans, inspires us to explore mulit-
modality. In neuroscience literature, several studies
have analyzed how humans can perceive the world
in visual context. (Bar, 2004; Xu and Chun, 2009)
found that humans can recognize multiple visual
objects and how their contexts, often embedded
with other related objects, facilitate this capacity.
Our work is more related to the recent study (Fis-
cher et al., 2020) which focuses on human capacity
to create temporal stability across multiple objects.
The multimodal DST task is designed to develop
multimodal dialogue systems that are capable of
maintaining discriminative representations of vi-
sual objects over a period of time, segmented by
dialogue turns. While computer science literature
has focused on related human capacities in intelli-
gent systems, they are mostly limited to vision-only
tasks e.g. (He et al., 2016; Ren et al., 2015) or QA
tasks e.g. (Antol et al., 2015; Jang et al., 2017) but
not in a dialogue task.
Most related work in the dialogue domain is
(Pang and Wang, 2020) and almost concurrent to
our work is (Kottur et al., 2021). However, (Kot-
tur et al., 2021) is limited to a single object per
dialogue, and (Pang and Wang, 2020) extends to
multiple objects but does not require to maintain
an information state with component slots for each
object. Our work aims to complement these di-
rections and address their limitations with a novel
definition of multimodal dialogue state.
Multimodal DST (MM-DST). To this end, we
proposed to extend conventional dialogue states.
First, we use visual object identities themselves
as a component of the dialogue state to enable the
perception of multiple objects. A dialogue state
might have one or more objects and a dialogue sys-
tem needs to update the object set as the dialoguecarries on. Secondly, for each object, we define
slots that represent the information state of objects
in dialogues (as denoted by (Fischer et al., 2020)
as “content” features of objects memorized by hu-
mans). The value of each slot is subject-specific
and updated based on the dialogue context of the
corresponding object. This definition of DST is
closely based on the above well-studied human
capacities while complementing the conventional
dialogue research (Young et al., 2010; Mrkši ´c et al.,
2017), and more lately multimodal dialogue re-
search (Pang and Wang, 2020; Kottur et al., 2021).
We denote a grounding visual input in the form
of a video Vwith one or more visual objects o.
We assume these objects are semantically differ-
ent enough (by appearance, by characters, etc.)
such that each object can be uniquely identified
(e.g. by an object detection module ω). The ob-
jective of MM-DST is for each dialogue turn t,
predict a value vof each slot s∈ Sfor each ob-
jecto∈ O. We denote the dialogue state at turn
tasB=|{(o, s, v)}| . Assuming
all slots are conditionally independent given dia-
logue and video context, the learning objective is
extended from Eq. (1):
One limitation of the current representation is the
absence of temporal placement of objects in time.
Naturally humans are able to associate objects and
their temporal occurrence over a certain period.
Therefore, we defined two temporal-based slots:
s ands, denoting the start time and end
time of the video segment that an object can be
located by each dialogue turn. In this work, we
assume that a dialogue turn is limited to a single
continuous time span, and hence, s ands
can be defined turn-wise, identically for all objects.
While this is a strong assumption, we believe it
covers a large portion of natural conversational
interactions. An example of multimodal dialogue
state can be seen in Figure 1.
3 Video-Dialogue Transformer Network
A naive adaptation of conventional DST to MM-
DST is to directly combine visual features extracted
by a pretrained 3D-CNN model. However, as3396
shown in our experiments, this extension of con-
ventional DST results in poor performance and
does not address the challenge of MM-DST. In
this paper, we established a new baseline, denoted
as Video-Dialogue Transformer Network (VDTN)
(Refer to Fig. 2 for an overview):
3.1 Visual Perception and Encoder
Visual Perception. This module encodes videos
at both frame-level and segment-level representa-
tions. Specifically, we used a pretrained Faster
R-CNN model (Ren et al., 2015) to extract ob-
ject representations. We used this model to output
the bounding boxes and object identifiers (object
classes) in each video frame of the video. For an ob-
jecto, we denoted the four values of its bounding
boxes as bb= (x, y, x, y)andoas the ob-
ject class itself. We standardized the video features
by extracting features of up to N= 10 objects
per frame and normalizing all bounding box co-
ordinates by the frame size. Secondly, we used
a pretrained ResNeXt model (Xie et al., 2017) to
extract the segment-level representations of videos,
denoted as z∈Rfor a segment m. Practi-
cally, we followed the best practice in computer
vision by using a temporal sliding window with
strides to sample video segments and passed seg-
ments to ResNeXt model to extract features. To
standardize visual features, we use the same strid-ing configuration N to sub-sample segments
for ResNeXt and frames for Faster R-CNN models.
Visual Representation. Note that we do not
finetune the visual feature extractors in VDTN
and keep the extracted features fixed. To trans-
form these features into VDTN embedding space,
we first concatenated all object identity tokens
OBJ<class> of all frames. An object identity to-
kenOBJ<class> is the code name of the object
class (e.g. a class of small blue metal cones) that
a visual object can be unique identified (See Fig-
ure 2). Frames are separated by a special token
FRAME<number> , where <number> is the tem-
poral order of the frame. This results in a sequence
of tokens Xof length L= (N+ 1)×
(|V|/N )where |V|is the number of video
frames. Correspondingly, we concatenated bound-
ing boxes of all objects, and used a zero vector
in positions of FRAME<number> tokens. We
denoted this sequence as X∈Rwhere
the dimension of 4is for the bounding box co-
ordinates (x, y, x, y). Similarly, we stacked
each ResNeXt feature vector by (N+ 1) for
each segment, and obtained a sequence X∈
R.
Visual Encoding. We passed each of Xand
Xto a linear layer with ReLU activation to map
their feature dimension to a uniform dimension d.3397We used a learnable embedding matrix to embed
each object identity in X, resulting in embed-
ding features of dimensions d. The final video input
representation is the summation of above vectors,
denoted as Z=Z+Z+Z∈R.
3.2 Dialogue and State Encoder
Dialogue Encoding. Another encoder encodes
dialogue into continuous representations. Given
a dialogue context D, we tokenized all dialogue
utterances into sequences of words, separated by
special tokens USR for human utterance and SYS
for system utterance. We used a trainable embed-
ding matrix and sinusoidal positional embeddings
to embed this sequence into d-dimensional vectors.
Flattening State into Sequence. Similar to the
recent work in traditional DST (Lei et al., 2018b;
Le et al., 2020b; Zhang et al., 2020), we are mo-
tivated by the DST decoding strategy following a
Markov principle and used the dialogue state of the
last dialogue turn Bas an input to generate the
current state B. Using the same notations from
Section 2, we can represent Binto a sequence of
o, s, and vtokens, such as “ OBJ4 shape cube
OBJ24 size small color red ”. This sequence is then
concatenated with utterances from D, separated
by a special token PRIOR_STATE . We denoted the
resulting sequence as Xwhich is passed to the
embedding matrix and positional encoding as de-
scribed above. As we showed in our experiments,
to encode dialogue context, this strategy needs only
a few dialogue utterances (that are closer to the cur-
rent turn t) andB, rather than the full dialogue
history from turn 1. Therefore, dialogue represen-
tations Zhave more compressed dimensions of
|X| ×dwhere |X|<|D|.
3.3 Multimodal Transformer Network
We concatenated both video and dialogue repre-
sentations, denoted as Z= [Z;Z].Z
has a length of L+Land embedding dimen-
siond. We pased Zto a vanilla Transformer
network (Vaswani et al., 2017) through multiple
multi-head attention layers with normalization (Ba
et al., 2016) and residual connections (He et al.,
2016). Each layer allows multimodal interactions
between object-level representations from videos
and word-level representations from dialogues.3.4 Dialogue State Decoder and
Self-supervised Video Denoising Decoder
State Decoding. This module decodes dialogue
state sequence auto-regressively, i.e. each token is
conditioned on all dialogue and video representa-
tions as well as all tokens previously decoded. At
the first decoding position, a special token STATE
is embedded into dimension d(by a learned embed-
ding layer and sinusoidal positional encoding) and
concatenated to Z. The resulting sequence is
passed to the Transformer network and the output
representations of STATE are passed to a linear net-
work layer that transforms representations to state
vocabulary embedding space. The decoder applies
the same procedure for the subsequent positions to
decode dialogue states auto-regressively. Denoting
bas the ktoken in B, i.e. token of slot, ob-
ject identity, or slot value, we defined the DST loss
function as the negative log-likelihood:
L=−/summationdisplay
logP(b|b, X, X)
Note that this decoder design partially avoids the
assumption of conditionally independent slots. Dur-
ing test time, we applied beam search to decode
states with the maximum length of 25 tokens in all
models and a beam size 5. An END_STATE token
is used to mark the end of each sequence.
Visual Denoising Decoding. Finally, moving
away from conventional unimodal DST, we pro-
posed to enhance our DST model with a Visual
Decoder that learns to recover visual representa-
tions in a self-supervised learning task to improve
video representation learning. Specifically, during
training time, we randomly sampled visual repre-
sentations and masked each of them with a zero vec-
tor. At the object level, in the mvideo frame, we
randomly masked a row from X(m)∈R.
Since each row represents an object, we selected a
row to mask by a random object index j∈[1, N]
such that the same object has not been masked in
the preceding frame or following frame. We denote
the Transformer output representations from video
inputs as Z∈R. This vector is passed to
a linear mapping fto bounding box features R.
We defined the learning objective as:
L=/summationdisplay1 ×l(f(Z), X)
where lis a loss function and 1 ={0,1}is
a masking indicator. We experimented with both3398L1 and L2 loss and reported the results. Similarly,
at the segment level, we randomly selected a seg-
ment to mask such that the preceding or following
segments have not been chosen for masking:
L=/summationdisplay1 ×l(f(Z), X)
4 Experiments
4.1 Experimental Setup
Dataset. In existing benchmarks of multimodal
dialogues such as VisDial (Das et al., 2017a) and
A VSD (Alamri et al., 2019), we observed that a
large number of data samples contain strong distri-
bution bias in dialogue context, in which dialogue
agents can simply ignore the whole dialogue and
rely on image-only features (Kim et al., 2020). An-
other observed bias is the annotator bias that makes
a causal link between dialogue context and output
response actually harmful (Qi et al., 2020) (as an-
notator’s preferences are treated as a confounding
factor). The above biases would obviate the need
for a DST task.
To address the above biases, Le et al. (2021b)
developed a Diagnostic Benchmark for Video-
grounded Dialogues (“DVD”), by synthetically cre-
ating dialogues that are grounded on videos from
CATER videos (Shamsian et al., 2020). The videos
contain visually simple yet highly varied objects.
The dialogues are synthetically designed with both
short-term and long-term object references. These
specifications remove the annotation bias in terms
of object appearances in visual context and cross-
turn dependencies in dialogue context.
Extension from DVD (Le et al., 2021b) .
We generated new dialogues following Le et al.
(2021b)’s procedures but based on an extended
CATER video split (Shamsian et al., 2020) rather
than the original CATER video data (Girdhar
and Ramanan, 2020). We chose the extended
CATER split (Shamsian et al., 2020) as it includes
additional annotations of ground-truth bounding
box boundaries of visual objects in video frames.
This annotation facilitates experiments with Faster-
RCNN finetuned on CATER objects and experi-
ments with models of perfect visual perception, i.e.
P(o|V, ω)≈1. As shown in (Le et al., 2021b),
objects can be uniquely referred in utterances based
on their appearance by one or more following as-
pects: “size”, “color”, “material”, and “shape”. We
directly reuse these and define them as slots in our
dialogue states, in addition to 2 temporal slots for
s ands. We denote the new benchmark as
DVD-DST and summarize the dataset in Table 1
(for more detail, please refer to Appendix B).
Baselines. To benchmark VDTN, we compared
the model with following baseline models, includ-
ing both rule-based models and trainable models:
•Q-retrieval (tf-idf) , for each test sample, di-
rectly retrieves the training sample with the
most similar question utterance and use its
state as the predicted state;
•State prior selects the most common tuple of
(object, slot, value) in training split and uses
it as predicted states;
•Object (random) , for each test sample, ran-
domly selects one object predicted by the vi-
sual perception model and a random (slot,
value) tuple (with slots and values inferred
from object classes) as the predicted state;
•Object (all) is similar to the prior baseline but
selects all possible objects and all possible
(slot, value) tuples as the predicted state;
•RNN(+Att) uses RNN as encoder and an MLP
network as decoder. Another variant of the
model is enhanced with a vanilla dot-product
attention at each decoding step;
•We adapted and experimented with strong uni-
modal DST baselines, including: TRADE (Wu
et al., 2019), UniConv (Le et al., 2020b) and
NADST (Le et al., 2020c).
We implemented these baselines and tested them
on dialogues with or without videos. When video
inputs are applied, we embedded both object and
segment-level features (See Section 3.1). The video
context features are integrated into baselines in the
same techniques in which the original models treat
dialogue context features.3399Training. We trained VDTN by jointly minimiz-
ingLandL . We trained all models using
the Adam optimizer (Kingma and Ba, 2015) with
a warm-up learning rate period of 1 epoch and the
learning rate decays up to 160 epochs. Models are
selected based on the average Lon the valida-
tion set. To standardize model sizes, we selected
embedding dimension d= 128 for all models, and
experimented with both shallow ( N= 1) and deep
networks ( N= 3) (by stacking attention or RNN
blocks), and 8attention heads in Transformer back-
bones. We implemented models in Pytorch and
released the code and model checkpoints. Refer
to Appendix C for more training details.
Evaluation. We followed the unimodal DST
task (Budzianowski et al., 2018; Henderson et al.,
2014a) and used the state accuracy metric. The
prediction is counted as correct only when all the
component values exactly match the oracle values.
In multimodal states, there are both discrete slots
(object attributes) as well as continuous slots (tem-
poral start and end time). For continuous slots,
we followed (Hu et al., 2016; Gao et al., 2017)
by using Intersection-over-Union (IoU) between
predicted temporal segment and ground-truth seg-
ment. The predicted segment is counted as correct
if its IoU with the oracle is more than p, where
we chose p={0.5,0.7}. We reported the joint
state accuracy of discrete slots only (“Joint Acc”)
as well as all slot values (“Joint Acc IoU@ p”). We
also reported the performance of component state
predictions, including predictions of object identi-
tieso, object slot tuples (o, s, v), and object
state tuples (o, s, v)∀s∈ S. Since a model
may simply output all possible object identities and
slot values and achieve 100% component accura-
cies, we reported the F1 metric for each of these
component predictions.
4.2 Results
Overall results. From Table 2, we have the fol-
lowing observations:
•we noted that simply using naive retrieval
models such as Q-retrieval achieved zero joint
state accuracy only. State prior achieved only
about 15% and 8% F1 on object identities and
object slots, showing that a model cannot sim-
ply rely on distribution bias of dialogue states.
•The results of Object (random/all) show that
in DVD-DST, dialogues often focus on a sub-
set of visual objects and an object perception
model alone cannot perform well.
•The performance gains of RNN models show
the benefits of neural network models com-
pared to retrieval models. The higher results
ofRNN(D) against RNN(V) showed the dia-
logue context is essential and reinforced the
above observation.
•Comparing TRADE and UniConv, we noted
that TRADE performed slightly better in com-
ponent predictions, but was outperformed in
joint state prediction metrics. This showed the
benefits of UniConv which avoids the assump-
tions of conditionally independent slots and
learns to extract the dependencies between
slot values.
•Results of TRADE, UniConv, and NADST all
displayed minor improvement when adding
video inputs to dialogue inputs, display-
ing their weakness when exposed to cross-
modality learning.
•VDTN achieves significant performance gains
and achieves the SOTA results in all compo-
nent or joint prediction metrics.
We also experimented with a version of VDTN
in which the transformer network (Section 3.3) was3400
initialized from a GPT2-base model (Radford et al.,
2019) with a pretrained checkpoint released by
HuggingFace. Aside from using BPE to encode
text sequences to match GPT2 embedding indices,
we keep other components of the model the same.
VDTN+GPT2 is about 36×bigger than our default
VDTN model. As shown in Table 2, the perfor-
mance gains of VDTN+GPT2 indicates the bene-
fits of large-scale language models (LMs). Another
benefit of using pretrained GPT2 is faster training
time as we observed the VDTN+GPT2 converged
much earlier than training it from scratch. From
these observations, we are excited to see more fu-
ture extension of SOTA unimodal DST models (Lin
et al., 2021; Dai et al., 2021) and large pretrained
LMs (Brown et al., 2020; Raffel et al., 2020), espe-
cially ones with multimodal learning such as (Lu
et al., 2019; Zhou et al., 2020), to MM-DST task.
Impacts of self-supervised video representation
learning. From Table 3, we noted that compared
to a model trained only with the DST objective
L, models enhanced with self-supervised video
understanding objectives can improve the results.
However, we observe that L1 loss works more con-
sistently than L2 loss in most cases. Since L2 loss
minimizes the squared differences between pre-
dicted and ground-truth values, it may be suscep-
tible to outliers (of segment features or bounding
boxes) in the dataset. Since we could not control
these outliers, an L1 loss is more suitable.
We also tested with L(tracking), in which
we used oracle bounding box labels during train-
ing, and simply passed the features of all objects to
VDTN. This modification treats the self-supervised
learning task as an object tracking task in which
all output representations are used to predict the
ground-truth bounding box coordinates of all ob-
jects. Interestingly, we found L(tracking) only
improves the results insignificantly, as compared
to the self-supervised learning objective L. This
indicates that our self-supervised learning tasks do
not strongly depend on the availability of object
boundary labels.
Finally, we found combining both segment-level
and object-level self-supervision is not useful. This
is possibly due to our current masking strategy that
masks object and segment features independently.
Therefore, the resulting context features might not
be sufficient for recovering masked representations.
Future work can be extended by studying a code-
pendent masking technique to combine segment-
based and object-based representation learning.
Impacts of video features and time-based slots.
Table 4 shows the results of different variants of
VDTN models. We observed that:
•Segment-based learning is marginally more
powerful than object-based learning.
•By considering the temporal placement of ob-
jects and defining time-based slots, we noted
the performance gains by “Joint Obj State Acc”
(Bvs.B\time ). The performance gains show
the interesting relationships between temporal
slots and discrete-only slots and the benefits
of modelling both in dialogue states.
•Finally, even with only object-level features
X, we still observed performance gains from
using self-supervised loss L, confirming
the benefits of better visual representation
learning.3401
Ablation analysis by turn positions. Figure 3
reported the results of VDTN predictions of states
that are separated by the corresponding dialogue
positions. The results are from the VDTN model
trained with both LandL. As expected, we
observed a downward trend of results as the turn
position increases. We noted that state accuracy re-
duces more dramatically (as shown by “Joint Acc”)
than the F1 metrics of component predictions. For
instance, “Object Identity F1” shows almost stable
performance lines through dialogue turns. Inter-
estingly, we noted that the prediction performance
of dialogue states with temporal slots only deterio-
rates dramatically after turn 2onward. We expected
that VDTN is able to learn short-term dependencies
(1-turn distance) between temporal slots, but failed
to deal with long-term dependencies ( >1-turn dis-
tance) between temporal slots. In all metrics, we
observed VDTN outperforms both RNN baseline
and UniConv (Le et al., 2020b), across all turn po-
sitions. However, future work is needed to close
the performance gaps between lower and higher
turn positions.
Impacts on downstream response prediction
task. Finally, we tested the benefits of studying
multimodal DST for a response prediction task.
Specifically, we used the best VDTN model to pre-
dict dialogue states across all samples in DVD-DST.
We then used the predicted slots, including object
identities and temporal slots, to select the video
features. The features are the visual objects and
segments that are parts of the predicted dialogue
states. We then used these selected features as input
to train new Transformer decoder models which
are added with an MLP as the response prediction
layer. Note that these models are trained only with
a cross-entropy loss to predict answer candidates.
From Table 5, we observed the benefits of filtering
visual inputs by predicted states, with up to 5.9%
accuracy score improvement. Note that there
are more sophisticated approaches such as neural
module networks (Hu et al., 2018) and symbolic
reasoning (Chen et al., 2020) to fully exploit the
decoded dialogue states. We leave these extensions
for future research.
For more experiment results, analysis, and quali-
tative examples, please refer to Appendix D.
5 Discussion and Conclusion
Compared to conventional DST (Mrkši ´c et al.,
2017; Lei et al., 2018b; Gao et al., 2019; Le et al.,
2020c), we show that the scope of DST can be fur-
ther extended to a multimodal world. Compared
to prior work in multimodal dialogues (Das et al.,
2017a; Hori et al., 2019; Thomason et al., 2019)
which focuses more on vision-language interac-
tions, our work was inspired from a dialogue-based
strategy with a formulation of a dialogue state track-
ing task. For more comparison to related work,
please refer to Appendix A.
We noted the current work are limited to a syn-
thetic benchmark with a limited video domain (3D
objects). However, we expect that MM-DST task is
still applicable and can be extended to other video
domains (e.g. videos of humans). We expect that
MM-DST is useful in dialogues centered around a
“focus group” of objects. For further discussion of
limitations, please refer to Appendix E.
In summary, in this work, we introduced a novel
MM-DST task that tracks visual objects and their
attributes mentioned in dialogues. For this task,
we experimented on a synthetic benchmark with
videos simulated in a 3D environment and dia-
logues grounded on these objects. Finally we pro-
posed VDTN, a Transformer-based model with
self-supervised learning objectives on object and
segment-level visual representations.34026 Broader Impacts
During the research of this work, there is no hu-
man subject involved and hence, no ethical con-
cerns regarding the experimental procedures and
results. The data is used from a synthetically de-
veloped dataset, in which all videos are simulated
in a 3D environment with synthetic non-human vi-
sual objects. We intentionally chose this dataset
to minimize any distribution bias and make fair
comparisons between all baseline models.
However, we wanted to emphasize on ethical
usage of any potential adaptation of our methods
in real applications. Considering the development
of AI in various industries, the technology intro-
duced in this paper may be used in practical appli-
cations, such as dialogue agents with human users.
In these cases, the adoption of the MM-DST task
or VDTN should be strictly used to improve the
model performance and only for legitimate and au-
thorized purposes. It is crucial that any plan to
apply or extend MM-DST in real systems should
consider carefully all potential stakeholders as well
as the risk profiles of application domains. For
instance, in case a dialogue state is extended to hu-
man subjects, any information used as slots should
be clearly informed and approved by the human
subjects before the slots are tracked.
References3403340434053406
A Details of Related Work
Our work is related to the following domains:
A.1 Dialogue State Tracking
Dialogue State Tracking (DST) research aims to
develop models that can track essential information
conveyed in dialogues between a dialogue agent
and human (defined as hidden information state by
(Young et al., 2010) or belief state by (Mrkši ´c et al.,
2017)). DST research has evolved largely within
the domain of task-oriented dialogue systems. DST
is conventionally designed in a modular dialogue
system (Wen et al., 2017; Budzianowski et al.,
2018; Le et al., 2020b) and preceded by a Natural
Language Understanding (NLU) component. NLU
learns to label sequences of dialogue utterances
and provides a tag for each word token (often in
the form of In-Out-Begin representations) (Kurata
et al., 2016; Shi et al., 2016; Rastogi et al., 2017).
To avoid credit assignment problems and stream-
line the modular designs, NLU and DST have been
integrated into a single module (Mrkši ´c et al., 2017;
Xu and Hu, 2018; Zhong et al., 2018). These DST
approaches can be roughly categorized into two
types: fixed-vocabulary or open-vocabulary. Fixed-
vocabulary approaches are designed for classifica-
tion tasks which assume a fixed set of (slot, value)
candidates and directly retrieve items from this set
to form dialogue states during test time (Hender-
son et al., 2014b; Ramadan et al., 2018; Lee et al.,
2019). More recently, we saw more approaches
toward open-vocabulary strategies which learn to
generate candidates based on input dialogue con-
text (Lei et al., 2018b; Gao et al., 2019; Wu et al.,
2019; Le et al., 2020c). Our work is more related
to open-vocabulary DST, but we essentially rede-
fined the DST task with multimodality. Based on
our literature review, we are the first to formally
extend DST and bridge the gap between traditional
task-oriented dialogues and multimodal dialogues.A.2 Visually-grounded Dialogues
A novel challenge to machine intelligence, the in-
tersection of vision and language research has ex-
panded considerably in the past few years. Earlier
benchmarks test machines to perceive visual inputs,
and learn to generate captions (Farhadi et al., 2010;
Lin et al., 2014; Rohrbach et al., 2015), ground
text phrases and objects (Kazemzadeh et al., 2014;
Plummer et al., 2015), and answer questions about
the visual contents (Antol et al., 2015; Zhu et al.,
2016; Jang et al., 2017; Lei et al., 2018a). As an
orthogonal development from Visual Question An-
swering problems, we noted recent work that tar-
gets vision-language in dialogue context, in which
an image or video is given and the dialogue ut-
terances are centered around its visual contents
(De Vries et al., 2017; Das et al., 2017a; Chat-
topadhyay et al., 2017; Hori et al., 2019; Thomason
et al., 2019; Le et al., 2021b). Recent work has ad-
dressed different challenges in visually-grounded
dialogues, including multimodal integration (Hori
et al., 2019; Le et al., 2019; Li et al., 2021), cross-
turn dependencies (Das et al., 2017b; Schwartz
et al., 2019; Le et al., 2021a), visual understanding
(Le et al., 2020a), and data distribution bias (Qi
et al., 2020). Our work is more related to the chal-
lenge of visual object reasoning (Seo et al., 2017;
Kottur et al., 2018), but focused on a multi-turn
tracking task over multiple turns of dialogue con-
text. The prior approaches are not well designed
to track objects and maintain a recurring memory
or state of these objects from turn to turn. This
challenge becomes more obvious when a dialogue
involves multiple objects of similar characters or
appearance. We directly tackles this challenge as
we formulated a novel multimodal state tracking
task and leveraged the research development from
DST in task-oriented dialogue systems. As shown
in our experiments, baseline models that use atten-
tion strategies similar to (Seo et al., 2017; Kottur
et al., 2018) did not perform well in MM-DST.
A.3 Multimodal DST
We noted a few studies have attempted to integrate
some forms of state tracking in multimodal dia-
logues. In (Mou et al., 2020), however, we are not
convinced that a dialogue state tracking task is a
major focus, or correctly defined. In (Pang and
Wang, 2020), we noted that some form of object
tracking is introduced throughout dialogue turns.
The tracking module is used to decide which object3407
the dialogue centers around. This method extends
to multi-object tracking but the objects are only lim-
ited within static images, and there is no recurring
information state (object attributes) maintained at
each turn. Compared to our work, their tracking
module only requires object identity as a single-slot
state from turn to turn. Almost concurrent to our
work, we noted (Kottur et al., 2021) which formally,
though very briefly, focuses on multimodal DST.
However, the work is limited to the task-oriented
domain, and each dialogue is only limited to a sin-
gle goal-driven object in a synthetic image. While
this definition is useful in the task-oriented dia-
logue domain, it does not account for the DST of
multiple visual objects as defined in our work.
B DVD-DST Dataset Details
For each of CATER videos from the extended split
(Shamsian et al., 2020), we generated up to 10 turns
for each CATER video. In total, DVD-DST con-
tains more than 13kdialogues, resulting in more
130k(human, system) utterance pairs and corre-
sponding dialogue states. A comparison of statis-
tics of DVD-DST and prior DST benchmarks can
be seen in Table 6. We observed that DVD-DST
contains a larger scale data than the related DST
benchmark. Even though the number of slots in
DVD-DST is only 6, lower than prior state tracking
datasets, our experiments indicate that most cur-
rent conventional DST models perform poorly onDVD-DST.
CATER universe. Figure 4 displays the config-
uration of visual objects in the CATER universe. In
total, there are 3 object sizes, 9 colors, 2 materials,
and 5 shapes. These attributes are combined ran-
domly to synthesize objects in each CATER video.
We directly adopted these attributes as slots in dia-
logue states, and each dialogue utterance frequently
refers to these objects by one or more attributes. In
total, there are 193 (size, color, material, shape)
valid combinations, each of which corresponds to
an object class in our models.
Sample dialogues. Please refer to Figure 5, Ta-
ble 14 and Table 15.
Usage. We want to highlight that the DVD-DST
dataset should only be used for its intended purpose,
i.e. to diagnose dialogue systems on their tracking
abilities. Any derivatives of the data should be
limited within the research contexts of MM-DST.
C Additional Training Details
In practice, we applied label smoothing (Szegedy
et al., 2016) on state sequence labels to regularize
the training. As the segment-level representations
are stacked by the number of objects, we randomly
selected only one vector per masked segment to
applyL. We tested both L1 and L2 losses on
L . All model parameters, except pretrained
visual perception models, are initialized by a uni-
form distribution (Glorot and Bengio, 2010).3408
For fair comparison among baselines, all mod-
els use both object-level and segment-level feature
representations, encoded by the same method as
Describe in Section 3.1. In TRADE , the video rep-
resentations are passed to an RNN encoder, and
the output hidden states are concatenated to the dia-
logue hidden states. Both are passed to the original
pointer-based decoder. In UniConv andNADAST ,
we stacked another Transformer attention layer to
attend on video representations before the original
state-to-dialogue attention layer. We all baseline
models, we replaced the original (domain, slot) em-
beddings as (object class, slot) embeddings and
kept the original model designs.
Note that in our visual perception model, we
adopted the finetuned Faster R-CNN model used by
(Shamsian et al., 2020). The model was finetuned
to predict object bounding boxes and object classes.
The object classes are derived based on object ap-
pearance, based on the four attributes of size, color,
material, and shape. In total, there are 193 object
classes. For segment embeddings, we adopted the
ResNeXt-101 model (Xie et al., 2017) finetuned on
Kinetics dataset (Kay et al., 2017). For all models
(except for VDTN ablation analysis), we standard-
izedN= 10 andN = 12 to sub-sample
object and segment-level embeddings.Resources. Note that all experiments did not re-
quire particularly large computing resources as we
limited all model training to a single GPU, specifi-
cally on a Tesla V100 GPU of 16G configuration.
D Additional Results
Greedy vs. Beam Search Decoding. Table 7
shows the results of different variants of VDTN
models. We observed that compared to greedy
decoding, beam search decoding improves the per-
formance in all models. As beam search decoding
selects the best decoded state by the joint probabili-
ties of tokens, this observation indicates the benefits
of considering slot values to be co-dependent and
their relationships should be modelled. This is con-
sistent with similar observations in later work of
unimodal DST (Lei et al., 2018b; Le et al., 2020c).
Ablation analysis by component predictions.
From Table 8, we have the following observations:
(1) In ablation results by component predictions,
we noted that models can generally detect object
identities well with F1 about 80%. However, when
considering object and slot tuples, F1 reduces to
48−60%, indicating the gaps are caused by slot
value predictions. (2) By individual slots, we noted
“color” and “shape” slots are easier to track than3409
“size” and “material” slots. We noted that in the
CATER universe, the latter two slots have lower
visual variances (less possible values) than the oth-
ers. As a result, objects are more likely to share
the same size or material and hence, discerning ob-
jects by those slots and tracking them in dialogues
become more challenging.
Table 9 and 10 display the ablation results by
component predictions, using precision and recall
metrics. We still noted consistent observations as
described in Section 4. Notably, we found that cur-
rent VDTN models are better in tuning the correct
predictions (as shown by high precision metrics)
but still fail to select all components as a set (as
shown by low recall metrics). This might be caused
by the upstream errors coming from the visual per-
ception models, which may fail to visually perceive
all objects and their attributes.
Results by turn positions. Table 11 reported the
results of VDTN predictions of states that are sepa-
rated by the corresponding dialogue positions. The
results are from the VDTN model trained with both
LandL. As expected, we observed a down-
ward trend of results as the turn position increases.
Impacts of dialogue context encoder. In Table
12a, we observed the benefits of using the Markov
process to decode dialogue states based on the di-
alogue states of the last turn B. This strategy
allow us to discard parts of dialogue history that is
already represented by the state. We noted that theoptimal design is to use at least 1last dialogue turn
as the dialogue history. In a hypothetical scenario,
we applied the oracle Bduring test time, and
noted the performance is improved significantly.
This observation indicates the sensitivity of VDTN
to a turn-wise auto-regressive decoding process.
Impacts of frame-level and segment-level sam-
pling. As expected, Table 12b displays higher
performance with higher object limits N, which
increases the chance of detecting the right visual ob-
jects in videos. We noted performance gains when
sampling strides increase up to 24 frames. How-
ever, in the extreme case, when sampling stride
is 300 frames, the performance on temporal slots
reduce (as shown by “Joint State IoU@ p”). This
raises the issue to sample data more efficiently by
balancing between temporal sparsity in videos and
state prediction performance. We also observed
that in a hypothetical scenario with a perfect object
perception model, the performance improves sig-
nificantly, especially on the predictions of discrete
slots, although less effect on temporal slots.
Impacts of object-level representation. Table
13 reported the results when only segment-level
features are used. We observed that both VDTN
andRNN(V+D) are affected significantly, specifi-
cally by 24% and 3.1% “Joint Obj State Acc” score
respectively. Interestingly, we noted that RNN(V) ,
using only video inputs, are not affected by the re-
moval of object-level features. These observations3410
indicate that current MM-DST requires object-level
information. We expected that existing 3DCNN
models such as ResNeXt still fail to capture such
level of granularity.
Qualitative analysis. Table 14 and 15 display 2
sample dialogues and state predictions. We dis-
played the corresponding video screenshots for
these dialogues in Figure 5. To cross-reference
between videos and dialogues, we displayed the
bounding boxes and their object classes in video
screenshots. These object classes are indicated in
ground-truth and decoded dialogue states in dia-
logues. Overall, we noted that VDTN generated
temporal slots of start and end time such that the
resulting periods better match the ground-truth tem-
poral segments. VDTN also showed to maintain
the dialogue states better from turn to turn.
E Further Discussion
Synthetic datasets result in overestimation of
real performance and don’t translate to real-
world usability. We agree that the current state
accuracy seems to be quite low at about 28%. How-
ever, we want to highlight that state accuracy used
in this paper is a very strict metric, which only
considers a prediction as correct if it completelymatches the ground truth. In DVD, assuming the
average 10 objects per video with the set of at-
tributes as in Figure 4 (+ ‘none’ value in each slot),
we can roughly equate the multimodal DST as a
7200-class classification task, each class is a dis-
tinct set of objects, each with all possible attribute
combinations. Combined with the cascading er-
ror from object perception models, we think the
current reported results are reasonable.
Moreover, we want to highlight that the reported
performance of baselines reasonably matches their
own capacities in unimodal DST. We can consider
Object State F1 as the performance on single-object
state and it can closely correlate with the joint
state accuracy in unimodal DST (remember that
unimodal DST such as MultiWOZ (Budzianowski
et al., 2018) is only limited to a single object/entity
per dialogue). As seen in Table 2, the Object State
F1 results of TRADE (Wu et al., 2019), UniConv
(Le et al., 2020b), and NADST (Le et al., 2020c)
are between 46-50%. This performance range is
indeed not very far off from the performance of
these baseline models in unimodal DST in the Mul-
tiWOZ benchmark (Budzianowski et al., 2018).
Finally, we also want to highlight that like other
synthetic benchmarks such as CLEVR (Johnson3411
et al., 2017), we want to use DVD in this work as
a test bed to study and design better multimodal
dialogue systems. However, we do not intend to use
it as a training data for practical systems. The DVD-
DST benchmark should be used to supplement real-
world video-grounded dialogue datasets.
MM-DST in practical applications e.g. with
videos of humans. While we introduced MM-
DST task and VDTN as a new baseline, we noted
that the existing results are limited to the synthetic
benchmark. For instance, in the real world, there
would be many identical objects with the same
(size, color, material, shape) tuples, which would
make the current formulation of dialogue states
difficult. In such object-driven conversations, we
would recommend a dialogue agent not focus on all
possible objects in each video frame, but only on
a “focus group” of objects. These objects, required
to be semantically different, are topical subjects of
the conversations.
Say we want to scale to a new domain e.g. videos
of humans, the first challenge from the current
study is the recognition of human objects, which
often have higher visual complexity than moving
objects as in DVD. We also noted that it is im-
possible to define all human object classes as in
CATER object classes, each of which is unique by
its own appearance. To overcome this limitation,
we would want to explore multimodal DST with the
research of human object tracking, e.g. (Fernando
et al., 2018), and consider human object identities
uniquely defined per video. Another limitation is
the definition of slots to track in each human ob-
ject. While this requires careful considerations, for
both practical and ethical reasons, we noted several
potential papers that investigate human attributes
in dialogues such as human emotions (Wang et al.,
2021). Along these lines, we are excited to see in-
teresting adaptations of multimodal dialogue states
grounded on videos of humans.3412341334143415